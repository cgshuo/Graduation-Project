 Institute of Computing Science, Pozna  X  n University of Technology, Pozna  X  n, Poland 1. Introduction
Learning classi fi ers from imbalanced data has become a new research challenge in data mining [20]. A data set is considered to be imbalanced if one of target classes contains much smaller number of objects than the other classes. The under-represented class is called the minority class , while the remaining classes are referred to as majority classes . Many practical problems as text or Web pages categorization, biomedical data analysis, fi nancial risk prediction, technical diagnostics of equipment failures, or image recognition are often characterized by class imbalance [8,20,48]. Let us note that the minority class is usually of the primary interest in such problems and classi fi ers need to accurately recognize examples from this class. For instance in medical diagnosing of a dangerous disease, where patients with this disease are very rare comparing to normal cases, early and accurate diagnosis is crucial for timely and proper treatment. Class imbalance constitutes a dif fi culty for most learning algorithms, which assume even class distribution and are biased toward learning and recognition of the majority classes. As a result, examples from the minority class tend to be misclassi fi ed.

One should notice, however, that under-representation of the minority class is not the only and main source of dif fi culties. Some r ecent works suggest that the d egradation of the classi fi er performance is also related to other factors as decomposition of the minority class into small parts being small dis-juncts [23], extent of overlapping between classes [15,32,35], presence of noisy examples [32] and combination of these factors with small size of a data set [22,23].

Learning from imbalanced data has received growing research interest in the last decade and several specialized methods have already b een proposed, see [8,20,48] for a review. According to the literature these methods can be categorized in two groups. The fi rst group includes classi fi er-independent methods that rely on transforming the original data to change the distribution of classes, e.g., by re-sampling. The other group involves modi fi cations of a learning phase, classi fi cation strategies or adaptation of cost sensitive learning.
 Two of the authors of this paper have already worked on focused pre-processing methods [32,43,44]. Thus, in our research we are particularly interested in this group of methods. The most well known fo-cused methods are SMOTE for selective over-sampling of the minority class [6], and NCR for removing examples from the majority classes [29]. Following critical observations on some of their limitations Stefanowski and Wilk proposed a new method for selective pre-processing, called SPIDER, which com-bined fi ltering and over-sampling of imbalanced data [43]. Previous experiments showed that SPIDER was competitive to SMOTE and NCR [44] with resp ect to accuracy of recogni zing examples from the minority class (expressed by sensitivity). On the other hand, the same experiments also demonstrated an undesirable behavior of SPIDER and other methods on several data sets with respect to majority classes. The improvement of sensitivity for the minority class was associated with too large decrease of the speci fi city for this class (it translated into deteriorated recognition of objects from the majority classes).

In our view improving sensitivity at the cost of speci fi city is a serious weakness of SPIDER and other pre-processing methods. As pointed out by other authors, it is equally important to improve sensitivity of a classi fi er and to keep its speci fi city and overall accuracy at an acceptable level [27]. As a result, values of both these measures should not drop too much comparing to a classi fi er induced from data without pre-processing. There are several aggregated measures that evaluate trade-off between classi fi cation performance for the minority and majority classes  X  we discuss them in Section 5. However, we claim that the geometric mean of sensitivity and speci fi city in a very precise and natural way re fl ects the postulate of keeping the above trade-off. Moreover, it is very well suited for use with deterministic classi fi ers (for explanation and comparison to the AUC measure see [47]) and it was extensively used in previous research on the SPIDER method.

Responding to the requirement of preserving acceptable speci fi city and overall accuracy we directed our attention to adaptive ensembles of classi fi ers . This learning method iteratively constructs a set of so-called component or base classi fi ers  X  which are of the same type (e.g. decision trees)  X  learned from diversi fi ed learning samples. Learning samples are iteratively changed (either by weighting or by special importance sampling) to emphasize these examples that have been dif fi cult to classify in previous iterations [28] and adding component classi fi ers to the ensemble is controlled by optimizing overall accuracy. However, when the ensemble is constructed from imbalanced data, learning examples in each iteration are s till coming for the data set t hat is predominated by th e majority cl asses, thus misclassi fi ed examples may be still biased toward these classes.

According to our intuition, a solution to this problem is to use the SPIDER method to pre-process learning samples during subsequent iterations. This should increase the number of examples of the minority class, and possibly remove some examples from the majority classes, thus improving sensitivity of component classi fi ers. Moreover, the main control strategy based on optimizing overall accuracy should keep speci fi city at a reasonable level. Finally, combination of these two factors should result in improved geometric mean for the ensemble classi fi er. Let us remark that a similar idea of using adaptive ensembles was applied in the SMOTEBoost algorithm [7], where the basic SMOTE method was successfully integrated with changing weights of learning examples inside the boosting procedure.
We decided to choose a speci fi c adaptive ensemble of classi fi ers called Ivotes . It was originally intro-duced by Breiman in [5]. A comparison of Ivotes with other types of ensembles of classi fi ers that may be considered in our setting is presented in Section 4. Results reported in the related literature show that Ivotes gives similar classi fi cation results as more popular boosting [5]. We expect that these results propagate to the case of imbalanced data.

Another reason to choose Ivotes comes from our experience with this method. In our previous re-search [3] we successfully combined Ivotes with C4.5 and MODLEM algorithms [39,42]. Let us remind that C4.5 and MODLEM were also used in previous experiments with SPIDER on several imbalanced data sets [44]. We already know from our previous studies that SPIDER and Ivotes improve perfor-mance of C4.5 and MODLEM classi fi ers when they are used separately. This work provides a thorough experimental study showing how classi fi cation performance is in fl uenced by incorporating SPIDER into Ivotes (with component classi fi ers learned either by C4.5 or MODLEM).

Yet another argument supporting the choice of Ivotes is associated with our earlier research on abstain-ing component classi fi ers from predicting a class label for uncertain examples. Results from [3] show that such modi fi cation of the classi fi cation strategy in Ivotes improves overall accuracy. Therefore, we plan to check whether this feature could also help in recognizing the minority class. Indeed, abstaining component classi fi ers are well suited to the aggregation technique employed in Ivotes which is similar to voting used in the classical bagging. The fi nal reason to rely on Ivotes is that unlike boosting classi-fi ers [38] it was not designed with weak component classi fi ers (i.e., classi fi ers that are able to achieve accuracy only slightly better than random guessing) in mind, thus, it better fi ts symbolic component classi fi ers that we use.

Summarizing, the main aim of this paper is to present a new framework for dealing with imbalanced data based on incorporating SPIDER into the Ivotes ensemble. We evaluated its performance experimen-tally on several imbalanced data sets and we compared it to the performance of single classi fi ers also combined with SPIDER. We considered tree-based and rule-based classi fi ers induced by the C4.5 and the MODLEM algorithms respectively, as according to previous studies they are sensitive to class imbal-ance [43,44]. Moreover, we checked how SPIDER affected the performance of Naive Bayes classi fi ers that are based on different principles than decision rules and trees.

This paper follows our preliminary study [4], however, it signi fi cantly extends it. First, we con-ducted new experiments including more learning algorithms, more data sets characterized by different imbalance-related factors, and additional evaluation measures. We also considered conditions where the new framework might improve performance. Finally, from the methodological perspective we paid more attention to the in fl uence of abstaining.

The paper is organized as follows. In Section 2 we brie fl y discuss the most related focused pre-processing methods. Then, the SPIDER method and its previous experimental evaluations are presented in Section 3. A new framework for integrating SPIDER with Ivotes ensemble is described in Section 4. Its experimental evaluation is given in Section 5. The paper concludes with a discussion in Section 6. 2. Related works
In the following review we brie fl y describe only these methods, which are mostly related to our pro-posal; for more extensive reviews see, e.g., [8,14,20,48]. We start with focused re-sampling methods that were inspiration for SPIDER and that resulted from poor experience with simple random re-sampling. Several authors showed that the random under-sampling or over-sampling were not suf fi ciently good at improving recognition of imbalanced classes. In particular, under-sampling may potentially remove some important examples and simple over-sampling may lead to over fi tting.

Kubat and Matwin in their paper on one-side sampling claim that characteristics of mutual positions of learning examples is a source of dif fi culty for learning classi fi ers from imbalanced data [27]. They focus attention on noisy majority class examples located inside the minority class and borderline examples. According to their approach, such examples are removed from the majority classes, while the minority class is kept unchanged. As result of such  X  X ocused X  under-sampling ambiguous regions around the minority class are  X  X leaned X . Moreover, some examples from  X  X afer X  regions of the minority class can be also discarded. Then, the Nearest Cleaning Rule (NCR) method was introduced in [29]. This method is based on the Edited Nearest Neighbour Rule (ENNR) and it removes these examples from the majority classes that are misclassi fi ed by its k nearest neighbours. Experimental results con fi rmed that both methods improved the sensitivity of the minority class.

A very well-known representative of focused over-sampling is SMOTE (Synthetic Minority Over-sampling Technique) introduced by Chawla et al. [6], which considers each example from the minority class and generates new synthetic examples along the lines between this example and some of its ran-domly selected k nearest neighbours (also belonging to the minority class). Experiments reported in [6] with C4.5 trees, Ripper rules and Naive Bayes classi fi ers showed that SMOTE improved recognition of the minority class. Moreover, its combination with under-sampling of the majority class was able to achieve better results than other under-sampling methods as ENNR alone  X  see, e.g., [1].

Although SMOTE and NCR showed to be promising in experimental evaluation, they also demon-strated several shortcomings that became motivations for introducing SPIDER  X  we will further discuss them in the next section. We should note that recently some researchers have tried to propose various generalizations of SMOTE following similar critical observations  X  see discussion in [20]. Two most interesting generalizations of SMOTE are Borderline SMOTE that takes into account the different na-ture of examples from the minority class, and Safe-Level SMOTE, where also the distribution of the majority class is considered while generating synthetic examples from the minority class. Both methods are described in [20].

The other related research topic is integration of focused re-sampling methods with ensemble clas-si fi ers. Among proposed approaches, SMOTEBoost [7] is closest to our framework. It introduces the original version of SMOTE to the AdaBoost.M2 ensemble  X  learning sets created in each iteration are pre-processed by oversampling of the minority class with synthetic examples. Experiments carried out with RIPPER rule learning algorithm on 4 data sets demonstrated improvements in recognition of the minority class and better overall performance with respect to F-measure. These experiments also demonstrated that introducing SMOTE inside boosting resulted in better performance than applying it independently to the original data set, and then running AdaBoost on such transformed data. Another integrated approach is the DataBoost-IM method [19] that combines a different technique for generating synthetic examples with AdaBoost.M1. Some authors claim that techniques combining ensembles with focused re-sampling are quite complicated and computationally expensive [20], and simpler oversam-pling could be also used in adaptive ensembles.

Finally, let us note that learning from imbalanced data has been also considered within cost-sensitive learning. Proposed approaches rely on modifying example weighting strategies inside boosting. The reader is referred to a review in [14,20] where modi fi cations of AdaBoost: AdaC1, and AdaCost are described.
 3. SPIDER method Critical analysis of undesirable properties of well-known focused re-sampling methods, especially NCR and SMOTE, became a starting point for developing of the SPIDER method [43]. NCR and in particular one-side sampling are strongly biased toward cleaning overlapping regions between classes and interior areas of the majority class. Learning examples from the minority class may be easier as dif fi cult examples from the majority classes have been discarded. However, both methods may remove too many examples from the majority classes. Such greedy  X  X leaning X  de fi nitely leads to the increased sensitivity for the minority class, however, too extensive changes in the majority classes may deteriorate the ability of an induced classi fi er to recognize examples from these classes.

One of the main shortcomings of SMOTE is the overgeneralization problem. SMOTE blindly gener-alizes regions of the minority class without checking positions of the nearest examples from the majority classes. This strategy is particularly problematic in the case of skewed class distribution where the mi-nority class is very sparse in comparison to the majority classes. In such situation SMOTE may increase overlapping between classes by locating synthetic examples from the minority class among existing ex-amples from the majority classes. Moreover, the number of synthetic examples generated by SMOTE has to be globally parametrized , thus reducing the fl exibility of the approach. Results of experimental studies with simulated data sets [15] imply that an ef fi cient method should be rather focused on local distributions of dif fi cult examples than being controlled by a global parameter. Let us also note that ac-cording to experimental results reported in the lite rature the value of this parameter strongly in fl uences SMOTE X  X  performance and its proper tuning requires a computationally costly procedure (iterative test-ing of various possible values). Finally, random introduction of synthetic examples by SMOTE may be dif fi cult to justify in some domains where it is important to preserve a link between original data and a constructed classi fi er in order to explain suggested decisions.

The SPIDER method relies on the local characteristics of examples (i.e., characteristics of their lo-cal neighborhood) and distinguishing between different types of examples. We generally distinguish between two types of examples  X  safe and not-safe . Safe examples should be correctly classi fi ed by a constructed classi fi er, while not-safe ones are likely to be misclassi fi ed and require special processing. We discover the type of an example by applying the nearest neighbour rule (NNR) with the heteroge-neous value distance metric (HVDM) [ 49]  X  i.e., the distance is calcu lated with the Euclidean distance metric for numerical features and with the value distance metric [12] for qualitative features. According to NNR an example is safe if it is correctly classi fi ed by its k nearest neighbours, otherwise it is not-safe .
More precise categorization of an example is based on the analysis of its neighbors from the other classes (i.e., different than the class of a considered example). If an example is not-safe and its nearest examples belong to other classes, then this example is identi fi ed as certain-noisy and we interpret it as a rare case located deeply inside the other classes. Such example should be treated in a different way than a not-safe example with some neighbors from the same class  X  this one is likely located in an overlapping region between classes. Unlike related methods that distinguish the type of examples in the minority class only, SPIDER identi fi es the nature of examples in all classes.

SPIDER assumes two decision classes  X  the minority class c min and the majority class c maj .Ifan original data set contains several majority classes they are collapsed together. The same approach should be applied when there are several minority classes  X  they need to be combined together into a single class before invoking the method. SPIDER consists of two main phases  X  identi fi cation and pre-processing . In the fi rst phase examples are fl agged according to the  X  X ocal X  characteristics of their nearest neighbors. Not-safe examples from c maj are also marked as candidates for possible transformation (removal or relabeling). In the second phase not-safe examples from both classes are processed. Depending on the pre-processing option and the type of examples SPIDER ampli fi es selected examples from c min , relabels selected not-safe examples from c maj (i.e., their class is changed to c min ), and fi nally removes remaining not-safe examples from c maj from a resulting data set.

SPIDER is presented in detail in Algorithm 1. In t he pseudo-code we use the following auxiliary functions (in all these functions we employ HVDM [49] to identify the nearest neighbours of a given example):  X  correct ( S , x , k )  X  classi fi es example x using its k nearest neighbors in set S and returns true or  X  flagged ( S , c , f )  X  identi fi es and returns a subset of examples from set S that belong to class c  X  knn ( S , x , k , c , f )  X  identi fi es and returns these examples among the k nearest neighbors of x in set  X  amplify ( S , x , k , c , f ) X  X mpli fi es example x by creating its | knn ( S , x , k , c , f ) | copies and
Pre-processing options of SPIDER de fi ne three different techniques for the second phase: weak ampli-fi cation ( weak ), weak ampli fi cation and relabeling ( relabel ), and strong ampli fi cation ( strong ). They all involve modi fi cation of the minority and majority classes, however, the degree and scope of changes vary between options. Weak ampli fi cation is the simplest and less greedy modi fi cation of the minority class. It focuses on not-safe examples from c min and slightly over-samples them by adding as many of their copies as there are safe examples from c maj in their neighborhoods. The second option  X  rela-bel  X  changes these noisy examples from c maj which could be interpreted as noisy outliers located more deeply inside the minority class. For the last option  X  strong  X  the degree of ampli fi cation of not-safe examples from c min could be higher depending on the analysis of an extended neighborhood. Much more thorough description of the method is provided in [43,44].

An experimental evaluation of SPIDER applied together with C4.5 and MODLEM algorithms was described in [44], where SPIDER was compared to competitive methods (NCR and SMOTE) and basic classi fi ers used without any pre-processing. The results showed that although NCR often led to the high-est increase of sensitivity, at the same time it signi fi cantly deteriorated speci fi city and overall accuracy. SPIDER was the second best with respect to improving the sensitivity of the minority class (improve-ment was more visible for MODLEM), and similarly to SMOTE it did not deteriorate the recognition of the majority classes as much as NCR. However, these results also demonstrated that evaluated methods were not good enough at keeping speci fi city and overall accuracy at satisfactory level comparing to basic classi fi ers. In case of SPIDER and possible pre-processing options, weak resulted in the best speci fi city and accuracy (often at the cost of sensitivity), strong resulted in a good balance between speci fi city and sensitivity and relabel improved sensitivity at the cost of speci fi city. Given these results and the aim of this work, in the experiment described in Section 5 we focused on the fi rst two options and discarded the last one. Finally, recent experiments with arti fi cial simulated data showed that SPIDER and NCR outperformed random oversampling methods if there was large overlapping region between the minority and majority classes [32]. SPIDER was also more accurate than NCR when the minority class contained enough noisy examples located inside the majority class close to the decision boundary. 4. Imbalanced Ivotes ensemble
Undesirable behavior of the SPIDER method and the requirement of preserving acceptable speci fi city and overall accuracy described in Section 1 guided our attention to ensemble methods. Let us remind Algorithm 1: SPIDER that an ensemble (also called a multiple classi fi er or a committee) is a set of single base (component) classi fi ers whose individual predictions are combined by an aggregation technique to produce a fi nal classi fi cation decision. Such combined classi fi ers should better classify new (or testing) examples than their component classi fi ers used independently. Indeed numerous experimental evaluations con fi rmed that the use of multiple classi fi ers led to improving classi fi cation accuracy in many problems, for some reviews see, e.g., [28]. Construction of ensembles has been approached in many ways  X  the most popular ones are bagging and boosting that manipulate learning data in order to get diversi fi ed learning samples and then apply the same learning algorithm over these samples to generate diversi fi ed classi fi ers.
Boosting is an example of a so-called adaptive ensemble which iteratively develops combined clas-si fi ers incrementally adding one classi fi er at a time. Such ensembles are able to adapt to examples that have been dif fi cult to learn in previous iteration by either changing weights of these examples (this is a standard approach in AdaBoost) or increasing pr obability of their random selection (as in Ivotes) in subsequent iterations when new component classi fi ers are constructed. The process of adding new component classi fi ers is usually controlled by a stopping cond ition associated with dropping the testing error of the current ensemble, or with reaching the assumed size of the ensemble (maximum number of component classi fi ers).

We are interested in boosting mainly due to its ab ility to change focus of lear ning component classi fi ers towards  X  X if fi cult X  examples in the learning data. Such adaptation property does not hold for bagging which is based on learning from bootstrap samples drawn independently from the initial learning data. In case of imbalanced data, this adaptation property means that dif fi cult examples from the minority class could be made more important during learning. However, as the original learning data can be still highly predominated by the majority class, the number of misclassi fi ed examples from this class could be still quite high comparing to minority ones and these examples would be used to learn subsequent component classi fi ers, thus introducing a bias. Therefore, simple application of adaptive ensembles may be not suf fi cient to balance the recognition of the minority and majority classes. We hypothesize that the solution to this problem is to use focused re-sampling methods as SPIDER to transform each learning sample inside subsequent iterations. 4.1. Integrating SPIDER with Ivotes
In Section 1 we have justi fi ed our choice of Ivotes as an adaptive ensemble. Generally speaking, it represents the idea of pasting small votes introduced by Breiman [5]. Breiman proposed using so-called pasting votes where many component classi fi ers were trained on relative small subsets of the original training data. The name  X  X asting small votes X  comes from the fact that this procedure takes small pieces of the learning data (learning samples), grows a classi fi er from each small piece, and then pastes these classi fi ers together.

Ivotes follows the idea of adaptive construction of an ensemble by using importance sampling which increases the likelihood of selecting  X  X if fi cult X  examples to a learning sample. In the fi rststageallexam-ples from the original data set have the same proba bility of being selected. They are selected randomly (with replacement) until the learning sample contains required number of examples (denoted by n ). This learning sample is used to construct the fi rst component classi fi er. In the following stages each example is classi fi ed by out-of-bag component classi fi ers (i.e., these classi fi ers for which the example was not in the learning sample). To produce the class label for a classi fi ed example predictions of component classi fi ers are aggregated by majority voting. If the example is misclassi fi ed, then it is included in the new learning sample. Oth erwise, it is accepted with probability dependent on the current estimate of generalization error. Drawing is repeated until the learning sample contains n examples. The rationale for such sampling procedure is that the new learning sample should contain about equal numbers of in-correctly and correctly classi fi ed examples. Once the new learning sample is created, a new component classi fi er is constructed. The Ivotes algorithm stops when generalization error stops decreasing.
This speci fi c type of adaptive ensemble proved to be effective in experiments with symbolic com-ponent classi fi ers  X  see research of Breiman with trees and also summary of other experiments (Chap-ter 7.1.3 in [28]). We have previously applied Ivotes with C4.5 and MODLEM [3]. The results allowed us to conclude that in terms of predi ctive accuracy Ivotes ensembles i n this setting are at least compa-rable to other ensembles of classi fi ers. What counts in favor of Ivotes is better interpretability resulting from a simple but meaningful aggregation of responses of component classi fi ers. Assignment to class made by Ivotes corresponds directly to the majority of votes of component classi fi ers. Other ensemble classi fi ers may use more complicated aggregation procedures, e.g., weighting of responses of component classi fi ers in case of boosting.

We propose to integrate Ivotes ensemble with SPIDER to obtain a multiple classi fi er more focused on the minority class. We claim that due to the idea of adaptation toward dif fi cult, misclassi fi ed learning examples the most natural place for such an integration is changing the result of importance sampling. Thus, the resulting Imbalanced Ivotes (shortly called IIvotes ) algorithm differs from original Ivotes mainly in the construction of the learning sample S i (see Algorithm 2). In each iteration, after the new learning sample S i is created, we additionally pre-process it with SPIDER to amplify examples from the minority class and weaken examples from the majority classes (some problematic examples from the majority classes may be removed). The pre-processed learning sample is then used to build a new base classi fi er C i . Let us remark that due to the construction of the ensemble and its general controlling Algorithm 2: IIvotes criterion (generaliza tion error/accuracy ) we still expect th at it should suf fi ciently balance the sensitivity and speci fi city for the minority class.

In this study we use three different approaches to construct component classi fi ers in the IIvotes en-semble. The fi rst two  X  induction of decision trees and rules  X  come from our earlier experiments with SPIDER [43,44]. We have chosen Quinlan X  X  C4.5 as a tree induction algorithm [33]. Following the literature on using trees with imbalanced data we induce unprunned decision trees. Decision rules are induced by the MODLEM algorithm. It was originally introduced by Stefanowski in [39] . Similarly to some well known algorithms, as CN2 [9], it also follows a sequential covering schema and generates an unordered minimal set of rules. Its advantages are related to processing of numerical values of attributes (without pre-discretization), missing values and imperfect descriptions of examples. MODLEM can be adapted to handle inconsistent or noisy examples either by rule pruning or rough approximations. In order to create a classi fi er the generated set of rules needs to be combined with a speci fi c classi fi cation strategy  X  we will discuss it in the next subsection. More details of MODLEM can be found in [18, 40,42]. Finally, in order to study also non-symbolic probabilistic approaches as the third base classi fi er we have chosen Naive Bayes. It is based on applying Bayes X  theorem with strong (naive) independence between features. 4.2. Abstaining rule ensembles Another interesting feature of the Ivotes ensemble, that we intend to verify in combination with SPI-DER, is abstaining , i.e., refraining from predicting class labels when the classi fi er is uncertain as to a new example. Such an example may be located either in the boundary region between classes or very far from any class region. Some techniques as threshold classi fi ers producing distributions of membership to several class, e.g., neural networks, may naturally abstain from classi fi cation if none of predictions exceeds a preferred threshold.

The concept of abstaining in ensembles could be considered at two levels [3]. At the fi rst level  X  which is considered by some researchers, see e.g. [37], abstaining may occur when the fi nal decision of the ensemble is established. At the second level, component classi fi ers may also refrain from prediction  X  this is more related to our research, and according to the best of our knowledge it has not been considered by other researchers.

Abstaining provides a rule ensemble with the ability which is typical for single decision rules. This ability results from the fact that a single rule classi fi es an example only when it covers this example (or in other terms, attribute-value description of the example satis fi es the condition part of the rule). Let us remind that single rules cover only a bounded part of the problem as opposed to, e.g., decision trees. Thus, a single rule has the ability to refrain from predicting, when it has insuf fi cient information on a new example. On the other hand, the set of rules does not need the ability to refrain that is typical for a single rule. Most rule-based classi fi ers are designed to always assign a class label for a new example, e.g., by using ordered priority lists of rules with the default class label [33] or specialized strategies for solving ambiguous con fl icts with unordered rules [16,31]. We showed that this urge to always assign a class label is not necessary in case of an ensemble of rule-based classi fi ers. More precisely, our experiments showed that introducing abstaining to the component rule-based classi fi ers in Ivotes ensemble improved classi fi cation accuracy [3]. 1 However, we have not tested this property with respect to class imbalance yet.

Let us explain more precisely how abstaining is considered with component rule-based classi fi ers generated by MODLEM. The set of induced rules needs to be combined with a speci fi c classi fi cation strategy to constitute a classi fi er. As we have mentioned above many of existing strategies are based on matching the description of a new example to rules  X  we follow Grzymala X  X  proposal of voting rules [16] which was successfully applied with MODLEM (see some reviews in [42]). As MODLEM produces an unordered set of rules, all rules are tested for such matching.

There exist three possible situations: only one rule matches an example, multiple rules match the example and no rule matches the example. Solving the second con fl icting situation is based on voting of matching rules with their supports. The total support for class K is de fi ned as: sup( K )= m i =1 sup( r i ) , where r i is a matched rule that indicates K , m is the number of these rules, and sup( r i ) is the number of learning examples satisfying both condition and decision parts of r i . A new example is classi fi ed to the class with the largest total support.

In the case of no match, so called partial matching is considered, where at least one of rule conditions is satis fi ed by a value of the corresponding attribute in the the description of a new example x .Inthis case, a matching factor match( r i ,x) is introduced as a ratio of conditions matched by the example x to where p is the number of partially-matched rules, and example x is assigned to the class with the largest support.

In [3] we proposed to adapt the above strategy to abstaining component classi fi ers by switching off the partial matching phase. It corresponds to our observation that induced rules establish an area of expertise for each component classi fi er (i.e., a subspace of problem space that is covered by its rules). If an example completely matches a rule from the given component classi fi er, it may be treated as being close to this area. Otherwise, in case when it is not matched by any rule, it is far from the area of expertise and it can be classi fi ed as unknown. Moreover, since components of adaptive ensembles of classi fi ers are generated from diversi fi ed samples it is more likely that their areas of expertise do not overlap. From technical perspective, in our ensemble framework unknown answers are implemented as zero votes in the aggregation phase.

Let us notice, that the above described Grzymala voting classi fi cation strategy is de fi nitely biased toward the majority classes as rules from these classes are usually supported by more learning examples than rule indicating the minority class [18,45]. We discuss more precisely this bias in Section 6 and refer the reader to some other solutions. So, we expect that reducing this bias by introducing abstaining should improve recognitio n of the minority class. 5. Experiments The main aim of our experiments was to evaluate the ability of the new IIvotes framework (i.e., Ivotes combined with SPIDER) to improve balance between the recognition of the minority and majority classes in comparison to SPIDER applied alone. This balance could be evaluated in different ways [20]. One possibility is employing ROC (Relative Opera ting Characteristics) curves. Area under the ROC curve (AUC) measures the classi fi cation potential of a classi fi er. The AUC has important statistical property  X  it is equivalent to the probability that the classi fi er will rank a randomly chosen positive example (i.e., example from the minority class) higher than a randomly chosen negative one (i.e., from example from the majority classes). Though the advantages of AUC seem evident, one of its properties made its application in our experiments dif fi cult. It is known that the AUC calculated for probabilistic than AUC of deterministic ones [13]. Quite similar opinion on better suitability of AUC for threshold classi fi ers than for fully deterministic ones is expressed in [47]. Since we considered deterministic (C4.5 and MODLEM) and probabilistic classi fi ers (Naive Bayes), AUC would not have allowed for a proper comparison of their performance and we decided to use other measures.

Looking for measures more suitable for experiments and having better intuitive meaning we directed our interest to sensitivity and speci fi city. They are de fi ned for a binary classi fi cation given in a confu-sion matrix [27]. The confusion matrix, presented in Table 1, represents possible outcomes of binary classi fi cation.

Sensitivity (also called recall rate in information retrieval) measures the proportion of actual positives which are correctly classi fi ed, and speci fi city measures the proportion of negatives which are correctly classi fi ed. Given the confusion matrix we can calculate sensitivity and speci fi city according to the following formulas: where | X | denotes cardinality of a set.

Geometric mean of sensitivity and speci fi city (G-mean in short) was proposed in [27]. G-mean relates to a point on the ROC curve and the idea behind this measure is to balance the accuracy for the majority and the minority classes: Maximizing G-mean turned out to be a simple but effective solution to our problem and we employed this measure in our experiments as the primary and most important evaluation criterion.
We also decided to use the F-measure (traditionally used in information retrieval) as the secondary criterion. It is a harmonic mean of precision and recall. Recall is computed with the same formula as sensitivity and precision can be calculated as:
Finally, we additionally used sensitivity as the last (and least important) criterion, as we did not want to promote a classi fi er that was characterized by high G-mean resulting from high speci fi city and relatively low sensitivity.

To evaluate the IIvotes framework we compared the performance of IIvotes with two SPIDER pre-processing options ( weak and strong ) to the performance of single classi fi ers combined with the same SPIDER pre-processing. Moreover, for completeness we included single classi fi ers and Ivotes ensem-bles with no-preprocessing in the comparison. We used three learning algorithms to construct compo-nent classi fi ers  X  Naive Bayes for probabilistic classi fi ers, C4.5 (J48 from WEKA) for decision trees and MODLEM for decision rules (MODLEM was applied together with Grzymala X  X  LERS strategy for classifying new examples [16]). C4.5 and MODLEM were run without pruning to get more precise description of the minority class. SPIDER was used with k =3 neighbors. Our earlier experiments with SPIDER combined with single classi fi ers [32] showed that increasing the value of k to 5 did not lead to signi fi cant differences in classi fi cation performance. The same value of k was also successfully used with the related NCR method [29]. Finally, the size of sample n in IIvotes was set to 50% of the size of learning set based on our experience from previous experiments. The experiments were carried out medical case studies ( abdominal pain ). We selected data sets that were characterized by varying degrees of imbalance and that were used in other related works. Let us notice that for some of these data sets minority class ratio is rather high (e.g. breast cancer or pima ). However, they are also char-acterized by other factors as overlapping decision classes or presence of noise examples which makes learning more dif fi cult (see the discussion in Section 1).
All experiments were run with a strati fi ed 10-fold cross-validation repeated fi ve times for better re-producibility of results and reduce possible variance of estimating average of the measures. We present average values of G-mean, F-measure, and sensitivity. G-mean values for C4.5, MODLEM and Naive Bayes are presented in Tables 3, 8 and 12. Values of F-measure are given in Tables 5, 10 and 14. These are followed by by values of sensitivity in Tables 6, 11, and 15. In all these tables, for each data set, we marked with bold the best result of a single classi fi er and the best result of an ensemble classi fi er. Moreover, the best result for each data set (over all classi fi ers) is marked with asterix.
Let us fi rst consider the G-mean results of SPIDER used with a single C4.5 classi fi er and with an Iv-otes/IIvotes ensemble of classi fi ers. These results are presented in Table 3. One can notice that SPIDER almost always produced better results than the base single or ensemble classi fi er. At least with respect to single classi fi ers this observation is consistent with the earlier experimental evaluation of SPIDER [44]. The more important observation concerns comparison of particular options of SPIDER used in IIvotes C4.5 ensemble or with the single C4.5 classi fi er, i.e. comparing results in appropriate pairs of columns in Table 3. It is clearly visible that IIvotes C4.5 performed better than single C4.5 with SPIDER. This improvement was seen for both pre-processing options ( weak or strong ). On the other hand, it is rather dif fi cult to decide which of these options is better with respect to the G-mean criterion only.
We performed the Wilcoxon test to get a better insight in these results. In this test [24], null-hypothesis is that the medians of G-mean for the two compared classi fi ers on all data sets are equal. The p -values resulting from this test are presented in Table 4. Their analysis implies there was no signi fi cant difference between strong and weak pre-processing for single C4.5 and Ivotes C4.5 classi fi ers. It is also visible that there was no signi fi cant difference between single C4.5 or Ivotes C4.5 used without SPIDER. When SPIDER was applied, it signi fi cantly improved C4.5 regardless of the pre-processing option. Moreover, IIvotes C4.5 classi fi ers were signi fi cantly better than single C4.5 with SPIDER.

The values of F-measure resulting from the same experiment with C4.5 allow us to make more decisive observations with respect to SPIDER pre-processing options. These results are presented in Table 5. The effect of using SPIDER with a single classi fi er is less visible in this case. On the other hand, SPIDER with the weak option produced the best results for IIvotes C4.5 ensembles.

We fi nish the analysis of results related to C4.5 with sensitivity. As it was mentioned before, we expected the classi fi ers to demonstrate good bala nce in prediction accuracy between classe s resulting in high values of G-mean and F-measure. Additionally, we did not expect high values of these two measures to be followed by a signi fi cant drop in sensitivity. The values of sensitivity are presented in Table 6. The fi rst observation is that SPIDER always improved the results. Moreover, better sensitivity was obtained for IIvotes C4.5 than for a single C4.5 with SPIDER. In case of single classi fi ers, there was no visible difference between the strong and weak options. For IIvotes ensembles, however, better sensitivity was obtained for the strong option. As we have shown in the analysis of Tables 3, 5 and 6, IIvotes C4.5 with the weak option was the best classi fi er with respect to G-mean and F-measure, and IIvotes C4.5 with the strong was better from the point of view of sensitivity. Such behavior may be explained by the characteristic of this option, i.e., informed oversampling of the minority class that leads to improved performance of classi fi ers for this class.

We started the MODLEM-related analysis with comparing two variants of IIvotes MODLEM ensem-bles  X  with abstaining and without abstaining. Results for G-means are given in Table 7. They clearly show the superiority of the abstaining variant, therefore we selected it for further comparison with sin-gle MODLEM classi fi ers (due to the applied classi fi cation strategy single MODLEM classi fi ers were non-abstaining).
 Results of G-mean for single MODLEM classi fi ers and their abstaining ensembles are given in Table 8. Similarly to what we observed for C4.5, the results of single and ensemble classi fi ers with SPIDER were better then without it  X  the only exception was the vehicle data set. For single MODLEM with SPIDER, it is hard to see difference between the strong and weak options. On the other hand, in case of IIvotes MODLEM the strong option gave better results more frequently. Finally, IIvotes MODLEM, regardless of the pre-processing option, gave better results more frequently than a single MODLEM with SPIDER.
 Again, we performed the Wilcoxon test on G-mean values to get more insight into MODLEM results. Its results are presented in Table 9. They show that IIvotes MODLEM ensembles were better than corresponding single MODLEM classi fi ers with SPIDER for both pre-processing options. Moreover, IIvotes MODLEM, due to abstaining, became an evident winner over single MODLEM (with or without SPIDER).

The values of F-measure presented in Table 10 are in concordance with values of G-mean from Ta-ble 8. They favor IIvotes with the strong option. IIvotes with the weak option also performed better than single MODLEM with SPIDER (regardless of the selected option).
IIvotes MODLEM with the strong option was also the best classi fi er according to values of sensitivity, which are presented in Table 11. For single MODLEM classi fi ers with SPIDER, the strong option was also better than the weak one.

According to the analysis conducted for MODLEM, we can state that IIvotes constructed better clas-si fi ers than single MODLEM with SPIDER regardless of the pre-processing option, and IIvotes with the strong option resulted in the best classi fi er.

As mentioned earlier, we decided to consider Naive Bayes (further denoted shortly as NB) in our experiments in order to see how SPIDER and IIvotes worked with non-symbolic probabilistic classi-fi ers. Although NB relies on a strong assumption about independence of attributes, it performs surpris-ingly well when this assumption is violated (i.e., for disjunctions and conjunctions of concepts) [11]. Furthermore, Rish demonstrated that NB reaches its best performance when attributes are completely independent or when they are functionally dependent [36]. Like the other classi fi ers considered in this experiment (C4.5 and MODLEM), NB is not suited for imbalanced data, as the prior probability of the majority classes overwhelms conditional probabilities for speci fi c attributes [25], although these proba-bilities can be relatively well established from imbalanced data [21]. Let us remark that several different groups of techniques have been studied to address this de fi ciency  X  they include re-sampling of a learning set (either random or more informed) [30,46], considering misclassi fi cation costs during learning [25], adjusting the decision threshold [34], and combinations of the mentioned techniques [26].

G-mean for NB classi fi ers is presented in Table 12. Similarly to C4.5 and MODLEM the re-sults show that applying SPIDER improved the results  X  the only exceptions were abalone and abdominal-pain for single classi fi ers and vehicle for ensembles. The strong option seemed to work better for a single NB, while the weak option did better with IIvotes NB. This observed synergy of NB and SPIDER con fi rms with what has been already published in literature and is consistent with re-sults reported in [15]. In particular Garcia et al. demonstrated that a NB classi fi er can take advantage of  X  X ense X  overlapping region between the minority and the majority classes, where the local imbalance is skewed towards the minority class (thus the local imbalance is opposite to the global one) [15]. SPIDER is capable of constructing such  X  X ense X  regions by amplifying examples from the minority class. Comparison of results presented in Tables 3, 8 and 12 reveals other interesting observations. First, for eight data sets ( abdominal-pain , cleveland , breast-cancer , ecoli , german , hepatitis , solar-flare , yeast ) a single NB (combined with SPIDER with the exception of the fi rst data set) was better not only than corresponding single C4.5 and MODLEM classi fi ers, but also than their best performing Ivotes/IIvotes variants. Similar behavior of NB has been already pre-sented in literature . In [11] Domingos and Pazzani described an e xperimental study on real-life data sets (some of these sets were used in our experiments as well, e.g., breast-cancer , hepatitis , solar-flare ) where they compared the performance of NB to C4.5 and CN2 [9]. According to their results, NB performed signi fi cantly better (however, in terms of the total accuracy) than C4.5 and CN2 for half of the tested data sets. In additional study reported in the same paper and comparing the performance of NB to C4.5 on arti fi cial data sets the authors showed that the former classi fi er was more accurate for smaller data sets (around 1000 examples) and that the cross point increased with the number of attributes. In fact, the sizes of all data sets mentioned above (with the exception of yeast ) vary from 155 ( hepatitis ) to 1066 ( solar-flare ). Such a superiority of NB may be attributed to its lower variance (error induced by the size of a learning set) in comparison to tree-and rule-based classi fi ers, what results in its better stability and performance on smaller data sets [11]. On the other hand, for a few data sets ( abalone , car , vehicle ) symbolic classi fi er (single and ensemble) were much better  X  constructing ensemble NB classi fi ers improved the results, however, demonstrated per-formance was still worse than the results of C4.5 and MODLEM ensemble classi fi ers. Two out of these data sets were among the largest ones considered in our experiment (1728 examples in car , and 4000 in abalone ) what further supports results from [11].

Moreover, single NB combined with SPIDER demonstrated better ability to handle the class imbal-ance than similar C4.5 and MODLEM classi fi ers  X  for fi ve data sets such a combination performed better than IIvotes ensemble, while single MODLEM with SPIDER was better for two data sets, and single C4.5 with SPIDER just for one data set. Furthermore, for all the data sets mentioned in the previous paragraph IIvotes NB resulted in at most minor performance improvements in comparison to single NB with SPIDER. This somehow limited usefulness of the IIvotes framework may be attributed to the ef-fects of ensemble classi fi cation (i.e., the underlying principle of IIvotes that result of classi fi cation is an aggregation of votes of component classi fi ers) on NB. According to results of the experimental evalu-ation presented in [2] similar ensemble (boosting) classi fi cation decreases the bias (i.e., approximation error) of NB and increases its variance. Therefore the positive in fl uence of ensemble classi fi cation on NB may be weaker than on decision trees or decision rules.
The above observations are to some extent supported by results of the Wilcoxon test presented in Ta-ble 13. Single NB with SPIDER (either with weak or strong options) was signi fi cantly better than single NB with no pre-processing. Moreover, single NB with the strong option was signi fi cantly better than Ivotes NB. Finally, IIvotes NB with the weak option was signi fi cantly better than Ivotes NB, single NB with no pre-processing, and single NB with SPIDER and the same option. Comparison of IIvotes NB with the strong option to single NB with the same SPIDER option was not conclusive.

The observations made so far for NB are in concordance with the results of F-measure presented in Table 14. The only signi fi cant difference is that SPIDER and IIvotes did not work well on the solar-flare data set.

As for the previously analyzed classi fi ers, we fi nish with the results of sensitivity in Table 15. These results clearly show that the strong option led to the best values of sensitivity for single classi fi ers and IIvotes as well. However, this fi nding contradicts observations for IIvotes and G-mean and F-measure where the weak option was promoted.

Analysing the above results for NB, we can still claim that incorporation of SPIDER into an ensemble was a useful solution, although it did not lead to such signi fi cant changes in performance as in case of previous learning algorithm. Moreover, SPIDER improved results when combined with single classi-fi ers, as well as within IIvotes  X  the former combination outperformed single and ensemble C4.5 and MODLEM classi fi ers on several data sets.

Finally to provide additional comparison of all analyzed classi fi ers from a single point of view, we de-cided to construct win-tie-loose tables for G-mean and F-measure measures. These results are presented in Tables 16 and 17.

Considering all presented results (including the win-tie-loose tables) we can conclude that generally the IIvotes framework (i.e., incorporating SPIDER into Ivotes) improved classi fi cation performance in comparison to using SPIDER with single classi fi ers. However, there were some differences between pre-processing options. IIvotes with the weak option was always better than a single classi fi er with SPIDER and the same option. This also holds for IIvotes with the strong option, when we consider G-mean measure. This conclusion is also true with respect to F-measure, but only for symbolic classi fi ers. According to F-measure, single NB with SPIDER and the strong option performed better than IIvotes with the same option.

Another important issue that was investigated in the experiments is the scalability of IIvotes ensem-bles. The results of this analysis are summarized by average numbers of component classi fi ers detected for different variants of IIvotes ensembles with strong option  X  they are presented in Table 18. These averaged number of classi fi ers comes from the same repeated 10 -fold cross validation experiments that were the basis of previously presented analysis. According to our observations, the respective average values obtained for Ivotes and IIvotes with weak option are similar to the ones presented in the table. The highest average numbers of component classi fi ers in Table 18 are marked with bold. One can easily notice that the number of generated component classi fi ers clearly depends on the data set. The highest observed average number is below 30 (C4.5 component classi fi ers on car data set). The lowest average number is above 10 (MODLEM component classi fi ers on breast cancer data set). Nevertheless, considering different learning algorithms the number of component classi fi ers on the same data set are comparable. There is one visible exception to this general observation for abalone data set, where en-sembles with NB component classi fi ers were much smaller than MODLEM and C4.5 ensembles. Let us remind that according to previous experiments, IIvotes with NB component classi fi ers performed poorly on this data set. Moreover, an additional observation with respect to the dependence of the average num-ber of component classi fi ers in ensemble and type of component classi fi er can be made. It seems that the average number of component classi fi ers in ensembles constructed with C4.5 is slightly, but constantly, higher than in case of MODLEM and NB. The differences are, however, not signi fi cant enough to make sound statements. All these observations allow us to formulate more detailed expectation of the cost of using IIvotes ensemble classi fi er instead of a single classi fi er on the considered data sets. On the other hand, one must remember that the component classi fi ers in IIvotes are constructed on portions of original data set  X  which could slightly reduce these computational costs for larger data sets. 6. Conclusions
In this paper we proposed a new IIvotes framework for constructing ensemble classi fi ers from im-balanced data. IIvotes integrates the SPIDER method for selective data pre-processing with the Ivotes adaptive ensemble. More speci fi cally, SPIDER is applied in each iteration of the Ivotes algorithm to pre-process a learning sample (created according to importance sampling principles) that is then used to construct a new component classi fi er. Such an integration is aimed at obtaining a better balance between the sensitivity and speci fi city measures for the minority class (expressed by the geometric mean of both measures) than in case of SPIDER combined with a single classi fi er. On the one hand, using SPIDER to pre-process speci fi c learning samples improves sensitivity of derived component classi fi ers, although it is associated with some drop of speci fi city. Such improvement of sensitivity (and deterioration of speci fi city) can be controlled by selecting appropriate pre-processing options. Let us also remind that the controlling mechanism of IIvotes (based on monitoring generalization error) ensures that overall accuracy (and thus speci fi city) is kept at a reasonable level.

IIvotes was evaluated in a series of experiments where we compared the new framework to single clas-si fi ers combined with SPIDER (for completeness we also included Ivotes and single classi fi ers with no-preprocessing in this comparison). In the experiments we considered both symbolic and non-symbolic classi fi ers  X  decision trees (induced with C4.5), decision rules (created with MODLEM) and Naive Bayes classi fi ers. As indicated above, we used the G-mean measure as the primary evaluation crite-rion. The overall goal of this experimental evaluation was to verify that IIvotes improves performance in comparison to single classi fi ers combined with SPIDER. Comparison of single and ensemble classi-fi ers combined with speci fi c SPIDER pre-processing ( weak and strong )con fi rmed our expectations  X  incorporating SPIDER into ensemble resulted in improved performance (with respect to G-mean and F-measure) for both pre-processing options. It was especially evident for symbolic classi fi ers (C4.5 and MODLEM) where differences in performance between single and ensemble classi fi ers with the same SPIDER option were statistically signi fi cant according to the Wilcoxon test. In case of Naive Bayes such difference was signi fi cant only for the weak pre-processing option. This could be explained by good capab ilities of single N aive Bayes combined with SPIDER to h andle imbalanced data (according to our observations for quite many data sets it outperformed the other two single classi fi ers) that made it less susceptible to bene fi ts of IIvotes. Finally, the experimental results did not allow for unequivocal selection of the most effective pre-processing option for IIvotes  X  the impact of a selected option onto the performance varied across different learning algorithms used to construct component classi fi ers.
Our experimental evaluation also highlighted advantages of abstaining rule ensembles that outper-formed their non-abstaining variants and demonstrated much better balance between sensitivity and speci fi city in terms of G-mean. Let us remind that component classi fi ers in the IIvotes ensemble use unordered rule sets and the Grzymala classi fi cation strategy that relies on voting with rule supports to solve con fl icting situations (i.e., multiple match and no match). Such voting scheme is biased towards the majority classes, as rules from these classes are stronger (supported by larger number of learning examples) and more general than rules for the minority class. Thus, without changing this classi fi cation strategy the minority class is likely to be outvoted when no match or multiple match occurs resulting in incorrect prediction. This was already noticed in [17] and a proposed solution to address this problem for a single classi fi er was to strengthen rules for the minor ity class by introducing additional support multipliers in the formu la for the total support sup( K ) . Yet another option could be to use another rule induction scheme for generating richer set of rules for the minority class that in the standard sequential covering (like in MODLEM). Such an option has been discussed in [45]. However, in this study we have not testing these modi fi cations, which could be a topic for further research with abstaining in presence of class imbalance.

This problem also occurs in an ensemble, as errors (related to the outvoted minority class) made by component classi fi ers are propagated to the ensemble fi nal decision possibly deteriorating its sensitivity (due to class imbalance limited impact on overall accuracy). This is evident when an example from the minority class is correctly classi fi ed according to fully matched rules by one of the component classi fi ers, and at the same time it is not matched to any of the remaining component classi fi ers. The remaining classi fi ers are likely to misclassify such an example due to biased voting in Grzymala X  X  strategy, and these incorrect predictions may outweigh the correct one when establishing the fi nal outcome of the ensemble. Refraining from making a prediction by component classi fi ers when no rule has been matched solves such situations, and this was demonstrated in experiments where abstaining clearly improved performance of the IIvotes ensemble, thus con fi rming our intuition and earlier observations. Acknowledgments
The research has been partially supported by the Ministry of Science and Higher Education of Poland, grant no. N N519 441939. Authors are also grateful to anonymous referees for their useful remarks. References
