 Task 1 of the 2006 KDD Challenge Cup required class ification of pulmonary embolisms (PEs) using variables derived f rom computed tomography angiography. We present our ap proach to the challenge and justification for our choices. W e used boosted trees to perform the main classification task, but modified the algorithm to address idiosyncrasies of the scoring criteria. The two main modifications were: 1) changing the depen dent variable in the training set to account for multiple PEs per patient, and 2) incorporating neighborhood information through augm entation of the set of predictor variables. Both of these resu lted in measurable predictive improvement. In addition, we discuss a statistically based method for setting the classifi cation threshold. I.5.1 [ Pattern Recognition ]: Models  X  statistical classification, nonstandard loss function, boosted trees, Real AdaBoost, multiple instance learning. Task 1 of the 2006 KDD Cup involved learning to cla ssify candidate sites as pulmonary embolisms (PEs), or no t, using 116 predictor variables measured by computed tomography angiography. A training sample included 3038 candi date sites over 46 patients. For 38 patients with at least on e PE, there were 363 positive sites forming 142 PEs (a single PE cou ld include one or more than one site). In simplified terms, the goal was to identify the l argest possible percentage of PEs (PE sensitivity) for a test sampl e of 21 patients, without exceeding any of the specified limits of 2, 4, and 10 false positives (FPs) per patient for three subtasks. Fo r more details about the data and tasks, see the 2006 KDD Cup web site [5] or the article by Lane in this issue. The largest challenge in this problem was to addres s the nonstandard scoring criteria. The criteria differe d in two important ways from those for problems based on (we ighted) classification error.  X  The PE sensitivity criterion gave no extra credit f or  X  The hard limits for the numbers of FPs, above which a We approached the problem in two stages. First, we tried to develop an algorithm that would automatically order candidates to optimize expected PE sensitivity for any given numb er of false positives. We used boosted trees X  X ith a non standa rd dependent variable and an enhanced set of predictor variables derived from the results of prior boosting steps. We present ou r algorithm and its development in Sections 2 to 4. Second, given our ordering, we tried to determine how far down the list we coul d go while maintaining an acceptable risk of violating one or more of the subtask limits (see Section 5). Task 2 had the same format except that the goal was to correctly classify at least one positive candidate site for e ach patient with one or more positive sites. We focus our presentat ion on Task 1, except for a brief discussion of Task 2 in Section 6. Our basic tool for this entire problem was boosted trees, which we chose for three reasons. First, we expected booste d trees to be at least competitive among available classifiers for t his type of problem X  X ne with many predictor variables and littl e prior information about whether relationships were additi ve or monotonic, much less linear. According to Hastie e t al. [4], in 1996, Breiman called AdaBoost the  X  X est off-the-she lf classifier in the world X  (see also [1]). Second, many of the available predictor variables are very skewed or have large o utliers. Because trees use only the ranking of the predictor variables, this reduces the influence of outliers and the need for transformations. Third, like many other classifiers, boosting focuse s on optimizing classification error rather than the desired criter ion, PE sensitivity. However, because boosting involves fitting a series of models, it seemed to offer an ideal setting for modifications designed to improve performance on an alternative criterion. This section describes a boosted trees model that w e refer to as the base classifier . Section 3 illustrates the need to modify this classifier in order to improve performance on the T ask 1 scoring criteria. In Section 4, we describe modifications to the base classifier used in our submission to the KDD Challe nge Cup. We created 6-node regression trees using a least sq uares error criterion, with no pruning. The dependent variable for the trees was set at +1 (for PEs) and -1 (for non PEs). We u sed the raw 116 predictor variables without any transformations , since trees are invariant to monotonic transformations of the i nput variables. Each tree produced real-valued predictions in the r ange [-1, +1]. Trees were aggregated using Real AdaBoost [3, 6], w hich extends the original AdaBoost algorithm to allow real-value d predictions at each boosting step. Consider a single candidate with outcome y and vector of explanatory variables x i . Suppose that at step t , the tree produces prediction h t ( x i ). Real AdaBoost produces a series of scores which can be used for classification or, more gener ally, to order candidates. The key to boosting is that the sample is reweighte d at each step to focus estimation on the hard to classify cases. Specifically, the weight for case i at step T is proportional to exp [ -y Small values of the  X  X hrinkage X  parameters {  X  t } serve to keep the estimator from learning too much from any one tree. Although boosting generally does a surprisingly good job of not over fitting, that can still be a problem if the algorithm contin ues for too many steps. Sufficiently small values of the {  X  t } in combination with early stopping provide a means to avoid over fittin g. We used 60 boosting steps and set  X  t = 0.15 for all steps. Although users of boosting often base binary classi fication simply on the sign of the final boosting score, H T ( x i ), classification can be based on comparison of H T ( x i ) with any real number. Because Task 1 calls for creating classifiers with a variet y of false positive rates, we assess each boosting model using a variet y of cut points c  X  X ith each candidate i classified as positive if and only if H ( x i ) &gt; c. Because the goal of Task 1 is to maximize PE sensit ivity within fixed FP limits, it was important to base model sel ection for the base classifier, as well as subsequent modification s, on those criteria. Consequently, we used graphs of estimate d PE sensitivity as a function of FPs per patient (see S ection 4 for example graphs) as our central model evaluation too l. In analyses for model selection, both quantities in the graphs were estimated using five-or ten-fold cross validation (CV). For example, for ten-fold CV, patients were grouped int o sets of 4 or 5 patients. Candidate models were fit ten times, l eaving out a different set of patients each time. PE sensitivit y and FP rates were computed for the patients omitted from the mod el fit and aggregated across the ten groups. Results presente d in this paper are based on subsequent 46-fold CVs of the original training data, where patients were omitted one at a time. Due to time constraints, we performed relatively li ttle exploration of alternative forms for the base classifier. For example, we did not evaluate alternatives to six for the number of nodes in the trees. Based on an arbitrary initial choice of  X = 0.15, we did evaluate a wide range of alternatives for the number of boosti ng steps. Performance improved up to a range of values and de graded beyond that range. Although it was difficult to pi npoint an optimum number of steps, a value of 60 seemed to pr ovide sufficient learning without obvious signs of over f itting. In practice [2], it could be better to use a smaller v alue of  X , with correspondingly more steps. However, very limited testing of this idea did not demonstrate noticeable improvement, so we retained the original parameters. At first glance the list of 116 predictor variables seemed ripe for some type of variable reduction, because large grou ps of variables were reported to measure related things (e.g., shap e, intensity). However, we did not attempt any variable reduction due to the lack of information about what each variable actual ly measured or any insight about which variables were important fo r classification. We note that Real AdaBoost is a la rge-margin learning algorithm that generalizes well on a large number of predictor variables, particularly in combination wi th early stopping. We did try a couple ideas for new predictors based on patterns in the data. First, because several predictor variabl es (most notably the  X  z  X  coordinate) seemed to be measured on different sc ales in one hospital (number 21) than in others, we created a dummy variable for that hospital. Second, the variable  X  x  X  seemed to measure position moving laterally across the chest, with a value of 269 somewhere near the center. Consequently, we cr eated a new variable | x  X  269| as a measure of distance from the center. Neither predictor stood out as particularly importan t in early modeling explorations, so both were dropped. We also tested using a large number of new predicto r variables based on averages of values for sets of nearest nei ghbors and residuals from those averages. Consider nearest ne ighbor sets of size 10 (we also tried 3 and infinity). For a give n variable V and nearest to i and V i r = V i -V i m . Although many of the variables created in this manner looked promising in terms of frequency of inclusion in the trees, all attempts to include add itional variables like these degraded results from cross validation. Consequently, we abandoned these predictor variables as well. Predictor variable labeled  X  X hape neighbor Feature N X  (for N = 5, 6, 8, 11, and 33) dominated the resulting model. T hose five variables accounted for 67 of the 300 splits across 60 trees, with each involved in at least ten splits (number 6 had 23 splits). Only two other predictor variables  X  X imple intensity sta tistic 2 X  and  X  X hape feature 4 X  reached the ten split level. As mentioned in the Task 1 problem statement  X  X his problem is a multiple-instance problem, where each positive exam ple has multiple instances X  [5]. The scoring criteria plac e a premium on finding at least one candidate from as many PEs as possible. It is just as important to identify PEs of degree 1 (i.e. , PEs with a single candidate) as is it to identify PEs of high degree. Indeed, slightly more than half the PEs in the training dat a had degree 1. Table 1 shows 46-fold cross validated results from the base classifier calibrated to have exactly 4.0 FPs per p atient (similar patterns occurred for other cut points). Overall, the base classifier correctly detected 52 percent of positive candidate s and 75 percent of PEs at this cutoff. Notably, performance in terms of candidate sensitivity was best for PEs with degree one or two, although the difference was not statistically signi ficant. Even though the classifier performed best at identi fying individual candidates from PEs with low degree, PE sensitivity was best for high-degree PEs because those PEs offered multiple chances for identification. Of 35 PEs not detected by the base classifier, 27 had degree one and another 6 had degree two; only 2 had degree three or greater. The final column of Table 1 show s that for high degree PEs, a large proportion of positive classifi cations were redundant. Consequently, it appeared that it would be very val uable to modify the base classifier to do better on low degr ee PEs, even at the risk of sacrificing some power to identify high degree PEs. This would be especially valuable if mostly redunda nt positives were reclassified. Degree 6-15 13 125 50.4 100 4.85 Total 142 363 51.8 75.4 1.76 This section describes two modifications to boosted trees: modification of the dependent variable for positive candidates and enhancements to the set of predictor variables at s elected boosting steps based on the results up to that point. We pr esent cross validation results to illustrate the impact of thes e two innovations. To try to focus the classifier on detecting candida tes from low degree PEs, we modified the standard dependent vari able for the regression trees at each boosting step. For positi ve candidates only, we replaced y i = +1 with y i = +1/(PE degree). Note that this new dependent variable required no changes to our c ode that implements Real AdaBoost for regression trees. One motivation for the form of this proposal is bas ed on the characterization by Friedman et al. [3] of boosting as additive logistic regression with loss function L( y , H (x)) = e like a weight in boosting. In that sense, the modi fied dependent variable defined above adjusts the data to give eac h PE the same weight. The hope was to improve candidate sensitiv ity for PEs of degree one or two, even at the risk of worse candid ate sensitivity for high degree PEs. Of course, there was no guara ntee that information in the predictor variables could discri minate candidates in low degree PEs from one in high degre e PEs. Figure 1 shows cross validated PE sensitivity versu s the number of FPs per patient as the cutoffs are varied for th e base classifier (solid line) and the classifier with the modified d ependent variable (dashed line). This simple change to the dependent variable generally improved cross validated PE sensitivity b y about 5 to 7 percentage points over much of the target range for Task 1. Figure 1. Cross validated Task 1 performance for ba se classifier and MDV classifier Table 2 compares classification results, by PE degr ee, for the modified dependent variable (MDV) classifier with t hat for the base classifier (BC), with both constrained to have exactly 4.0 FPs Degree 
Table 2. Comparison of classification results for the base classifier (BC) and classifier with modified depend ent variable anticipated. Sensitivity improved dramatically for PEs of degree 1; the MDV classifier correctly detected 47 of 64 p ositives, compared with 37 of 64 for the base classifier. Wh ile candidate sensitivity declined noticeably for all higher PE d egrees, the adverse impact on PE sensitivity was small, resulti ng in non detection of only two additional PEs of degree two. Overall, while candidate sensitivity declined by 3.0 percent age points, PE sensitivity rose by 5.6 points. The two classifiers described above treat the obser vations as if they are independent. We did not provide them with any information to indicate which candidates were from the same patient or which positive candidates shared the sam e PE. Consequently, the MDV classifier X  X  ability to discr iminate between low degree and high degree PEs appears to h ave derived purely from characteristics of individual candidate s. We do note that many of the most important predictor variables associated with individual candidates bear names like  X  X hape n eighbor feature N X , so that they may actually contain infor mation useful about nearby sites. There is substantial structure in the make up of PE s that might be exploited. For example, among pairs of positive ca ndidate sites from the same patient, at least 75 percent of those within 35 units of each other (based on Euclidean distance) came fr om the same PE, while fewer than 25 percent did for pairs separ ated by 75 or more units. Given appropriate data about each candidate X  X  neigh bors, if any exist, a classifier might be better able to discrim inate between candidates from low degree and high degree PEs. Fo r example, the MDV classifier might learn that it is crucial t o correctly classify positive candidates without any neighbors that appear to be positive, but less important to do so for candid ates with one or more close neighbors that are already being classif ied as positive. To improve the opportunity for the model to disting uish between high and low degree PEs, we augmented the list of e xplanatory variables at selected boosting steps ( T = 33, 36, 39, ... , 60) with five new variables defined specifically at those st eps. For neighborhoods of size 10, 20, 40, 60, and 100, we c omputed the difference between H T -1 ( x i ) for the candidate and the maximum value of the corresponding score for any other site within the neighborhood (this difference was set to a large po sitive constant for candidates with no neighbor). Negative values o f these differences suggest candidates that are either nega tive or, if positive, are members of multiple degree PEs. In c ontrast, candidates from degree 1 PEs are likely to have pos itive differences scores. When included in the list of predictor variables, o ne of these five new variables almost always defined the first split in the tree, and sometimes multiple splits. To avoid over fitting, we limited inclusion of these variables to later boosting step s and to every third tree. Figure 2 compares cross validated results for this classifier (short dashed line) with those for the previous two classi fiers. This enhancement appears to increase PE sensitivity by a couple percentage points on average for FP rates of 5 or h igher. Because of this improvement, we used the MDV classifier wit h the enhanced set of predictor variables in our submissi on for Task 1. Figure 2. Cross validated Task 1 performance for ba se classifier, MDV classifier, and MDV classifier with added predictors In this subsection, we describe a promising alterna tive classifier, even though we ultimately did not use it as part of our KDD Cup submission. Although the MDV classifier did very well at shifti ng the focus towards candidates from low degree PEs, there might still be gain available from trying to focus a classifier on one or two candidates from high degree PEs. Classifying four or five sit es in close proximity is unlikely to increase PE sensitivity co mpared with picking the one or two  X  X est X  sites. Consequently, instead of treating all members of a multiple degree PE interc hangeably, as the MDV classifier does, it might help to try to id entify one candidate, or a few, for training a classifier. Fo r multiple instance learning, Viola et al. [7] suggested that classification performance can be improved by focusing on a few critical examp les in a  X  X ag X  (PE, for our purposes). With this issue in mind, we tried to redistribute t he +1 shared across values of the dependent variable for a PE, w ith more going at time T to candidates with larger values of the current sc ore, H (e.g., a PE with degree 3 might have outcomes of 0. 7, 0.2, and 0.1). Specifically, if cases 1 to k were members of the same PE, we created a variable modified dependent variable (VMDV) for a =  X . As T grows, the values of y i for a single PE tend to diverge from 1/ k . This classifier involves a tradeoff. While focusin g weight on few cases from any particular PE may tend to reduce red undant positive classifications, it also risks decreasing the effective sample size available for learning how to discrimin ate positives from negatives. Figure 3 compares results for the VMDV classifier ( solid line) against those for the MDV classifier X  X ith both usin g the same enhanced set of predictor variables. Overall, resu lts were quite similar. However, there is mild evidence of a cros s over near 5 FPs per patient, with VMDV doing better for low val ues of the FP rate and MDV doing better for high values. This su ggests that VMDV may have the most promise when reducing FPs to a minimum is critically important. In contrast, VMDV may lose too much power when the limit on FPs is less stringent.
 Because the VMDV classifier did not demonstrate sub stantial improvement at any point on the curve, we used the classifier described in Section 4.2 in our submission for Task 1. Figure 3. Cross validated Task 1 performance for th e MDV and VMDV classifiers, both with added predictors To decide how many candidates to take from the orde red list created above, we used 46-fold cross validation on the training data to compute nearly unbiased estimates for FP pr obabilities as a function of the boosted scores (using the base cl assifier). We fit a logistic regression of PE status on a natural cub ic spline with 4 knots [4] of the boosted score excluding the same p atient. To obtain FP probability estimates for the test dat a, we applied the base classifier to the test data and then used the logistic regression described above to compute an estimated FP probabil ity, p each candidate in the test data. Summing these est imated probabilities { p i } yields estimates for the expected number of cumulative FPs at any point in the ordered list. Table 3 illustrates this process for a subset of ca ndidates from the test data set. The rows of the table are ordered b y decreasing values of the boosted scores, using the classifier described in Subsection 4.2. Note that the values of p i do not increase monotonically, because those values are based on th e base classifier. The column labeled  X  X um. p i  X  estimates the cumulative number of FPs through a particular candidate on the ordered list. Similarly, summing estimated values of the nominal variances, { p (1-p i )}, yields nominal estimates for the variances of t hese cumulative numbers of FPs under the assumption of independence. However, our analysis found intra pa tient correlation in PE status that was not explained by the base classifier. Near the threshold for Task 1, we estim ated that the true variance for the cumulative number of FPs was about twice the nominal variance. The last two columns of Tabl e 3 show  X  X orrected X  variances and standard deviations that account to the intra patient correlation. For Task 1a, the disqualification limit was 2.0 FPs per patient, or 46 across the 23 patients in the test data. Howeve r, as the last column of Table 3 makes clear, we needed to aim wel l below 46 FPs in order to have a high probability of passing this task. We ended up setting a threshold at 40.00; that is, we classified candidates with scores of -0.56 or higher as positi ve, and all others as negative. This threshold corresponded to aiming about 1.0 standard deviations below the target number of FPs for Task 1. For Tasks 1b and 1c, we set cut points corresp onding to 3.51 and 9.34 FPs per patient, both of which corresponde d to approximately 1.5 estimated standard deviations bel ow the subtask targets. We chose those thresholds based o n a goal of qualifying on all three subtasks with probability a bout 0.75 (ignoring bootstrapping). Boosted Score p i 
Table 3. Illustration of the process for setting t he threshold With hindsight, it is clear that the use of bootstr ap samples for scoring substantially increased the variance of the number of FPs at any cut point beyond that associated with the ra ndom sampling of 23 test patients. Although disqualification on any particular bootstrap sample was no longer fatal, the penalty w as still quite severe. Consequently, qualifying for a very high p roportion of bootstrap samples was needed to score well on Task 1, so even more conservative cut points might have been approp riate. In theory, all the methods developed and tested for Task 1 transferred seamlessly to Task 2, where the goal wa s to correctly classify at least one positive site for each patient with one or more PEs. The modified dependent variable for positive PEs became +1 / (# of positive candidates for the patient). F or the enhanced set of predictor variables, we dropped the neighbor hood of size 10 and added one covering the whole patient. Our results for Task 2 were substantially worse tha n for Task 1. One reason may be that we did not conduct any model evaluations specific to this task. In practice, discrimination between patients with few versus many positive sites may be a lot di fferent that discrimination between PEs with low versus high deg ree. Also, because we had not tuned our models at all for Task 2, we set more aggressive thresholds of: 1.82, 3.67, and 9.6 5 FPs per patient. As noted above, this adjustment was proba bly directly opposite what we should have done. We thank R. Bharat Rao and Siemens Medical Solution s for providing the data and Terran Lane for orchestratin g this fascinating and challenging problem. [1] Breiman, L. Arcing classifiers (with discussion and [2] Friedman, J. H. Greedy function approximation: a gr adient [3] Friedman, J., Hastie, T., and Tibshirani, R. Additi ve logistic [4] Hastie, T., Tibshirani, R., and Friedman, J. The Elements of [5] KDD Cup 2006. Data description, task specification , rules. [6] Schapire, R. E., and Singer, Y. Improved boosting [7] Viola, P., Platt, J, and Zhang, C. Multiple instan ce boosting 
