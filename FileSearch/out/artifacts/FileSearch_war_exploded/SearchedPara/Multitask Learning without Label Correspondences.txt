
Novi Quadrianto 1 , Alex Smola 2 , Tib  X  erio Caetano 1 , S.V.N. Vishwanathan 3 , James Petterson 1 In machine learning it is widely known that if several tasks are related, then learning them simulta-neously can improve performance [1 X 4]. For instance, a personalized spam classifier trained with data from several different users is likely to be more accurate than one that is trained with data from a single user. If one views learning as the task of inferring a function f from the input space X to the output space Y , then multitask learning is the problem of inferring several functions f i : X i 7 X  Y i simultaneously. Traditionally, one either assumes that the set of labels Y i for all the tasks are the same (that is, Y i = Y for all i ), or that we have access to an oracle mapping function g i,j : Y i 7 X  Y j . However, as we argue below, in many natural settings these assumptions are not satisfied. Our motivating example is the problem of learning to automatically categorize objects on the web into an ontology or directory. It is well established that many web-related objects such as web direc-tories and RSS directories admit a (hierarchical) categorization, and web directories aim to do this in a semi-automated fashion. For instance, it is desirable, when building a categorizer for the Yahoo! directory 1 , to take into account other web directories such as DMOZ 2 . Although the tasks are clearly related, their label sets are not identical. For instance, some section heading and sub-headings may be named differently in the two directories. Furthermore, different editors may have made differ-ent decisions about the ontology depth and structure, leading to incompatibilities. To make matters worse, these ontologies evolve with time and certain topic labels may die naturally due to lack of interest or expertise while other new topic labels may be added to the directory. Given the large label space, it is unrealistic to expect that a label mapping function is readily available. However, the two tasks are clearly related and learning them simultaneously is likely to improve performance. This paper presents a method to learn classifiers from a collection of related tasks or data sets, in which each task has its own label dictionary, without constructing an explicit label mapping among them. We formulate the problem as that of maximizing mutual information among the labels sets. We then show that this maximization problem yields an objective function which can be written as a difference of concave functions. By exploiting convex duality [5], we can solve the resulting optimization problem efficiently in the dual space using existing DC programming algorithms [6]. Related Work As described earlier, our work is closely related to the research efforts on multitask learning, where the problem of simultaneously learning multiple related tasks is addressed. Several papers have empirically and theoretically highlighted the benefits of multitask learning over single-task learning when the tasks are related. There are several approaches to define task relatedness. The works of [2, 7, 8] consider the setting when the tasks to be learned jointly share a common subset of features. This can be achieved by adding a mixed-norm regularization term that favors a common sparsity profile in features shared by all tasks. Task relatedness can also be modeled as learning functions that are close to each other in some sense [3, 9]. Crammer et al. [10] consider the setting where, in addition to multiple sources of data, estimates of the dissimilarities between these sources are also available. There is also work on data integration via multitask learning where each data source has the same binary label space, whereas the attributes of the inputs can admit different orderings as well as be linearly transformed [11].
 The remainder of the paper is organized as follows. We briefly develop background on the maximum entropy estimation problem and its dual in Section 2. We introduce in Section 3 the novel multi-task formulation in terms of a mutual information maximization criterion. Section 4 presents the algorithm to solve the optimization problem posed by the multitask problem. We then present the experimental results, including applications on news articles and web directories data integration, in Section 5. Finally, in Section 6 we conclude the paper. Here we briefly summarize the well known duality relation between approximate conditional maxi-mum entropy estimation and maximum a posteriori estimation (MAP) [5, 12]. We will exploit this where p ( y | x ) is a conditional distribution on the space of labels Y . Let x  X  X and assume the existence of  X  ( x,y ) : X  X  Y 7 X  H , a feature map into a Hilbert space H . Given a data set ( X,Y ) := { ( x 1 ,y 1 ) ,..., ( x m ,y m ) } , where X := { x 1 ,...,x m } , define Lemma 1 ([5], Lemma 6) With the above notation we have Although we presented a version of the above theorem using Hilbert spaces, it can also be extended to Banach spaces. Choosing different Banach space norms recovers well known algorithms such as ` or ` 2 regularized logistic regression. Also note that by enforcing the moment matching constraint exactly, that is, setting = 0 , we recover the well-known duality between maximum (Shannon) entropy and maximum likelihood (ML) estimation. For the purpose of explaining our basic idea, we focus on the case when we want to integrate two data sources such as Yahoo! directory and DMOZ. Associated with each data source are labels Y = { y 1 ,...,y c }  X  Y and observations X = { x 1 ,...,x m }  X  X (resp. Y 0 = { y 0 1 ,...,y 0 c 0 }  X  Y 0 and X 0 = { x 0 1 ,...,x 0 m 0 }  X  X 0 ). The observations are disjoint but we assume that they are drawn from the same domain, i.e., X = X 0 (in our running example they are webpages).
 If we are interested to solve each of the categorization tasks independently, a maximum entropy estimator described in Section 2 can be readily employed [13]. Here we would like to learn the two tasks simultaneously in order to improve classification accuracy. Assuming that the labels are different yet correlated we should assume that the joint distribution p ( y,y 0 ) displays high mutual information between y and y 0 . Recall that the mutual information between random variables y and y is defined as I ( y,y 0 ) = H ( y ) + H ( y 0 )  X  H ( y,y 0 ) , and that this quantity is high when the two variables are mutually dependent. To illustrate this, consider in our running example of integrating Yahoo! and DMOZ web directories, we would expect there is a high mutual dependency between section heading  X  X omputer &amp; Internet X  at Yahoo! directory and  X  X omputers X  at DMOZ directory although they are named somewhat slightly different. Since the marginal distributions over the labels, p ( y ) and p ( y 0 ) are fixed, maximizing mutual information can then be viewed as minimizing the joint entropy This reasoning leads us to adding the joint entropy as an additional term for the objective function of the multitask problem. If we define then we have the following objective function Intuitively, the above objective function tries to find a  X  X imple X  distribution p which is consistent with the observed samples via moment matching constraints while also taking into account task relatedness. We can recover the single task maximum entropy estimator by removing the joint entropy term (by setting  X  = 0 ), since the optimization problem (the objective functions as well challenges in solving (5): In the following section we discuss in further detail how to address these two challenges, as well as the resulting optimization problem obtained, which can be solved efficiently by existing convex solvers. The concave convex procedure (CCCP) works as follow: for a given function f ( x ) = g ( x )  X  h ( x ) , where g is concave and  X  h is convex, a lower bound can be found by This lower bound is concave and can be maximized effectively over a convex domain. Subsequently one finds a new location x 0 and the entire procedure is repeated. This procedure is guaranteed to converge to a local optimum or saddle point [16].
 Therefore, one potential approach to solve the optimization problem in (5) is to use successive linear lower bounds on H ( y,y 0 ) and to solve the resulting decoupled problems in p ( y | x ) and p ( y 0 | x 0 ) separately. We estimate the joint entropy term H ( y,y 0 ) by its empirical quantity on x and x 0 with the conditional independence assumption (in the sequel, we make the dependency of p ( y | x ) on a parameter  X  explicit and similarly for the dependency of p ( y 0 | x 0 ) on  X  0 ), that is and similarly for H ( y,y 0 | X 0 ) . Each iteration of CCCP approximates the convex part (negative joint yields optimization problems in p ( y | x i ) and an analogous problem in p ( y 0 | x 0 i ) : min The above objective function is still in the form of maximum entropy estimation, with the lineariza-impose an additional maximum entropy requirement on the  X  X ff-set X  observations p ( y | x 0 i ) , as after all we also want the  X  X implicity X  requirement of the distribution p on the input x 0 i . We can of course weigh the requirement on  X  X ff-set X  observations differently.
 While we succeed in reducing the non-concave objective function in (5) to a decoupled concave ob-jective function in (10), it might be desirable to solve the problem in the dual space due to difficulty in handling the constraint in (10b). The following lemma shows the duality of the objective function in (10). The proof is given in the supplementary material.
 Lemma 2 The corresponding Fenchel X  X  dual of (10) is The above dual problem still has the form of logistic regression with the additional evidence terms from task relatedness appearing in the log-partition function. Several existing convex solvers can be used to solve the optimization problem in (11) efficiently. Refer to Algorithm 1 for a pseudocode of our proposed method.
 Initialization For each iteration of CCCP, the linearization part of the joint entropy function re-quires the value of  X  and  X  0 at the previous iteration (refer to (9)). At the beginning of the iteration, we can start the algorithm with a uniform prior, i.e. set p ( y ) = 1 / | Y | and p ( y 0 ) = 1 / | Y 0 | . Algorithm 1 Multitask Mutual Information Input: Datasets ( X,Y ) and ( X 0 ,Y 0 ) with Y 6 = Y 0 , number of iterations N Output:  X  ,  X  0
Initialize p ( y ) = 1 / | Y | and p ( y 0 ) = 1 / | Y 0 | for t = 1 to N do end for return  X   X   X  N ,  X  0  X   X  0 N To assess the performance of our proposed multitask algorithm, we perform binary n-task ( n  X  { 3 , 5 , 7 , 10 } ) experiments on MNIST digit dataset and a multiclass 2-task experiment on the Reuters1-v2 dataset plus an application on integrating Yahoo! and DMOZ web directory. We detail those experiments in turn in the following sections. 5.1 MNIST Datasets MNIST data set 3 consists of 28  X  28 -size images of hand-written digits from 0 through 9 . We use a small sample of the available training set to simulate the situation when we only have limited number of labeled examples and test the performance on the entire available test set. In this task, respectively. To simulate the problem that we have distinct label dictionaries for each task, we consider the following setting: in the 3-task problem, the first task has binary labels { +1 ,  X  1 } , where label +1 means digit 8 and label  X  1 means digit 9 and 0 ; in the second task, label +1 means digit 9 and label  X  1 means digit 8 and 0 ; lastly in the third task, label +1 means digit 0 and label  X  1 means digit 8 and 9 . Similar one-against-rest grouping is also used for 5-task, 7-task and 10-task problems. Each of the tasks has its own input x .
 Algorithms We couldn X  X  find in the literature of multitask learning methods addressing the same problem as the one we study: learn multiple tasks when there is no correspondence between the output spaces. Therefore we compared the performance of our multitask method against the baseline given by the maximum entropy estimator applied to each of the tasks independently. Note that Section 3) and thus a simple strategy of multilabel prediction with union of label sets corresponds to our baseline. For both ours and the baseline method, we use a Gaussian kernel to define the implicit feature map on the inputs. The width of the kernel was set to the median between pairs of observations, as suggested in [17]. The regularization parameter was tuned for the single task estimator and the same value was used for the multitask. The weight on the joint entropy term was set to be equal to 1.
 Pairwise Label Correlation Section 3 describes the multitask objective function for the case of the 2-task problem. For the case when the number of tasks to be learned jointly is greater than 2 , we experiment in two different ways: in one approach we can define the joint entropy term on the full joint distribution, that is when we want to learn jointly 3 different tasks having label y , y 0 and y 00 , more computationally efficient way, we can consider the joint entropy on the pairwise distribution instead. We found that the performance of our method is quite similar for the two cases and we report results only on the pairwise case.
 Results The experiments are repeated for 10 times and the results are summarized in Table 1. We find that, on average, jointly learning the multiple related tasks always improves the classification Table 1: Performance assessment, Accuracy  X  STD. m ( m 0 ) denotes the number of training data points (number of test points). STL: single task learning; MTL: multi task learning and Upper Bound: multi class learning. Boldface indicates a significance difference between STL and MTL (one-sided paired Welch t-test with 99.95% confidence level).
 accuracy. When assessing the performance on each of the tasks, we notice that the advantage of learning jointly is particularly significant for those tasks with smaller number of observations. 5.2 Ontology News Ontologies In this experiment, we consider multiclass learning in a 2-task problem. We use the Reuters1-v2 news article dataset [18] which has been pre-processed 4 . In the pre-processing stage, the label hierarchy is reorganized by mapping the data set to the second level of topic hier-archy. The documents that only have labels of the third or fourth levels are mapped to their parent category of the second level. The documents that only have labels of the first level are not mapped onto any category. Lastly any multi-labelled instances are removed. The second level hierarchy consists of 53 categories and we perform experiments on the top 10 categories. TF-IDF features are used, and the dictionary size (feature dimension) is 47236 . For this experiment, we use 12500 news articles to form one set of data and another 12500 news article to form the second set of data. In the Table 2: Yahoo! Top Level Categorization Results. STL: single task learning accuracy; MTL: multi task learning accuracy; % Imp.: relative performance improvement. The highest relative improvement at Yahoo! is for the topic of  X  X omputer &amp; Internet X  , i.e. there is an increase in accuracy from 48.12% to 52.57%. Interestingly, DMOZ has a similar topic but was called  X  X omputers X  and it achieves accuracy of 75.72%.
 Topic MTL/STL (% Imp.) Topic MTL/STL (% Imp.) Arts 56.27/55.11 (2.10) News &amp; Media 15.23/14.83 (1.03) Business &amp; Economy 66.52/66.88 (-0.53) Recreation 68.81/67.00 (2.70) Computer &amp; Internet 52.57/48.12 (9.25) Reference 26.65/24.81 (7.42) Education 62.48/63.02 (-0.85) Regional 62.85/61.86 (1.60) Entertainment 63.30/61.37 (3.14) Science 78.58/79.75 (-1.46) Government 24.44/22.88 (6.82) Social Science 31.55/30.68 (2.84) Health 85.42/85.27 (1.76) Society &amp; Culture 49.51/49.05 (0.94) Table 3: DMOZ Top Level Categorization Results. STL: single task learning accuracy; MTL: multi task learning accuracy; % Imp.: relative performance improvement. The improvement of multitask to single task on each topic is negligible for DMOZ web directories. Arguably, this can be partly explained as DMOZ has higher average topic categorization accuracy than Yahoo! and there might be more knowledge to be shared from DMOZ to Yahoo! than vice versa.
 on each set to form training and test sets. We run a maximum entropy estimator independently, for the second set. We then learn the two sets of the news articles jointly and in the first test set, we achieve accuracy of 93 . 81% . For the second test set, we achieve an accuracy of 93 . 31% . This experiment further emphasizes that it is possible to learn several related tasks simultaneously even though they have different label sets and it is beneficial to do so.
 Web Ontologies We also perform an experiment on the data integration of Yahoo! and DMOZ web directories. We consider the top level of the Yahoo! X  X  topic tree and sample web links listed in the directory. Similarly we also consider the top level of the DMOZ topic tree and retrieve sampled web links. We consider the content of the first page of each web link as our input data. It is possible that the first page that is being linked from the web directory contain mostly images (for the purpose of attracting visitors), thus we only consider those webpages that have enough texts to be a valid input. This gives us 19186 webpages for Yahoo! and 35270 for DMOZ. For the sake of getting enough texts associated with each link, we can actually crawl many more pages associated with the link. However, we find that it is quite damaging to do so because as we crawl deeper the topic of the texts are rapidly changing. We use the standard bag-of-words representation with TF-IDF weighting as our features. The dictionary size (feature dimension) is 27075. We then use 2000 web pages from Yahoo! and 2000 pages from DMOZ as training sets and the remainder as test sets. Table 2 and 3 summarize the experimental results. From the experimental results on web directories integration, we observe the following: We presented a method to learn classifiers from a collection of related tasks or data sets, in which each task has its own label set. Our method works without the need of an explicit mapping between the label spaces of the different tasks. We formulate the problem as one of maximizing the mutual information among the label sets. Our experiments on binary n -task ( n  X  { 3 , 5 , 7 , 10 } ) and mul-ticlass 2 -task problems revealed that, on average, jointly learning the multiple related tasks, albeit with different label sets, always improves the classification accuracy. We also provided experiments on a prototypical application of our method: classifying in Yahoo! and DMOZ web directories. Here we deliberately used small amounts of data X  X  common situation in commercial tagging and classification. This shows that classification accuracy of Yahoo! significantly increased. Given that DMOZ classification was already 4.5% better prior to the application of our method, this shows the method was able to transfer classification accuracy from the DMOZ task to the Yahoo! task. Furthermore, the experiments seem to suggest that our proposed method is able to discover implicit label correlations despite the lack of label correspondences.
 Although the experiments on web directories integration is encouraging, we have clearly only touched the surface of possibilities to be explored. While we focused on the categorization at the top level of the topic tree, it might be beneficial (and further highlight the usefulness of multitask learning, as observed in [2 X 4, 9]) to consider categorization at deeper levels (take for example the second level of the tree), where we have much fewer observations for each category. In the extreme case, we might consider the labels as corresponding to a directed acyclic graph (DAG) and encode the feature map associated with the label hierarchy accordingly. One instance as considered in [19] is to use a feature map  X  ( y )  X  R k for k nodes in the DAG (excluding the root node) and associate with every label y the vector describing the path from the root node to y , ignoring the root node itself.
 Furthermore, the application of data integration which admit a hierarchical categorization goes be-yond web related objects. With our method, it is also now possible to learn classifiers from a collec-tion of related gene-ontology graphs [20] or patent hierarchies [19].
 Acknowledgments NICTA is funded by the Australian Government as represented by the Depart-ment of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program. N. Quadrianto is partly supported by Microsoft Re-search Asia Fellowship. [1] R. Caruana. Multitask learning. Machine Learning , 28:41 X 75, 1997. [2] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature [3] Kai Yu, Volker Tresp, and Anton Schwaighofer. Learning gaussian processes from multiple [4] Rie Kubota Ando and Tong Zhang. A framework for learning predictive structures from mul-[5] Y. Altun and A.J. Smola. Unifying divergence minimization and statistical inference via convex [6] T. Pham Dinh and L. Hoai An. A D.C. optimization algorithm for solving the trust-region [7] G. Obozinski, B. Taskar, and M. I. Jordan. Multi-task feature selection. Technical report, U.C. [8] Remi Flamary, Alain Rakotomamonjy, Gilles Gasso, and Stephane Canu. Svm multi-task [9] Theodoros Evgeniou, Charles A. Micchelli, and Massimiliano Pontil. Learning multiple tasks [10] K. Crammer, M. Kearns, and J. Wortman. Learning from multiple sources. In NIPS 19 , pages [11] Shai Ben-David, Johannes Gehrke, and Reba Schuller. A theoretical framework for learning [12] M. Dud  X   X k and R. E. Schapire. Maximum entropy distribution estimation with generalized reg-[13] Nadia Ghamrawi and Andrew McCallum. Collective multi-label classification. In CIKM  X 05: [14] A.L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation , 15:915 X  [15] A. J. Smola, S. V. N. Vishwanathan, and T. Hofmann. Kernel methods for missing variables. In [16] Bharath Sriperumbudur and Gert Lanckriet. On the convergence of the concave-convex pro-[17] B. Sch  X  olkopf. Support Vector Learning . R. Oldenbourg Verlag, Munich, 1997. Download: [18] David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. RCV1: A new benchmark collection [19] Lijuan Cai and T. Hofmann. Hierarchical document categorization with support vector ma-[21] J. M. Borwein and Q. J. Zhu. Techniques of Variational Analysis . CMS books in Mathematics.
