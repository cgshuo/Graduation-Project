 NICK CAMPBELL 1. Introduction The latest keyword in speech technology research is  X  X motion X . For decades now, we have been producing and improving methods for the input and output of speech signals by computer, but the market seems slow to take up these technologies. This is not to say that speech technology is not being used, and there are already many applications where computers mediate in human spoken communications, but in only a few limited domains. In spite of the early promises for human X  X omputer voice-based interactions, the man or woman in the street is yet to make much use of this technology in their daily lives. The technology appears to have fallen short of its promise.  X  X motion X ? Perhaps because the current technology is based so much upon written text as the core of its processing. Speech recognition is evaluated by the extent to which it can  X  X ccurately X  transliterate a spoken utterance; and speech synthesis is driven, in the majority of case, from input text alone. Yet text is a very different type of medium from speech. Text lives on, while speech decays quickly in time. Text is optimised for visual input, relying on differences in e.g., font and layout so that its structure is obvious at a glance, and allows scanning up and down the page, back and forth along the lines, in a way that is independent of time.
 converted into speech by a process of media conversion, just as speech can be transcribed and converted into text; but what is lost in the process? Reading aloud is a very difficult task; a task in which most people perform very poorly. It involves translating the visual text-based information into a time-decaying signal that conveys the same propositional and attitudinal content. This requires rendering the syntactic and semantic structure, through the use of prosody, into a form that preserves the often very complicated propositional content. For news-readers and schoolteachers alike, this task requires extensive training and practice. Yet speech  X  X omes naturally X  to almost everybody, and is perhaps the most popular medium of human communication. Why the prob-lem? Perhaps the solution can be best approached by first looking at the dif-ferences between read speech and its conversational counterpart. 1.1. Conversational speech Human speech is a complex information source that signals many levels or layers of complementary information, and that can best be described in terms of three basic components: linguistic, paralinguistic, and extralinguistic. Though all three are expressed simultaneously, they each appear to be per-ceived or processed separately. We normalise across age and sex of the speaker to perceive the linguistic content of each utterance independently of, but in conjunction with, the characteristics of the voice and the interpretation cues coming from the speaking style.
 action, in addition to the transfer of information, concerns control of the discourse flow and definition of the relationships between speaker and lis-tener. The  X  X ow X  and the  X  X hy X  of conversational speech are as important as the  X  X hat X , and the expression of affect is as common as the delivery of propositional content. Conversational speech is therefore processed on sev-eral levels at once; to determine not just what is being said, but how it should be perceived in the context of a given interpersonal relationship. 1.2. Read speech Read speech, on the other hand, is a more impersonal event; in which the reader expresses the content of the text almost independently of any 110 relationship with the listener. A text may be interpreted, but it is not gen-erated; the source of each utterance is external to the speaker, and the listener is an audience rather than an active participant in the communicative event, or media transformation.
 examples of such impersonal speech, and are typical applications for speech technology. The presenter X  X  job is simply to convey the message of the text, and no personal interaction between speaker and listener is expected, although in the case of a news  X  X nchor X , an element of authority or person-ality may be added. 1.3. Computer speech Based primarily on research carried out using read X  X peech corpora, com-puter-generated speech is currently well tuned for linguistic content, and the expression of syntactic relations, but the extra-linguistic or paralinguistic information is not yet well modelled, if at all. Speech recognition may accurately transcribe the text of an utterance, but it leaves no record about how it was expressed. The speaker-specific characteristics are normalised out; as is the speaking-style information and attitudinal cues. Speech synthesis can now accurately render an utterance in the recognisable voice of a given speaker, but there are currently few controls for the way it can be said. Research has been focussed on content rather than style, yet speaking-style often provides a rich source of information about how that content should be interpreted or situated in a given context. 2. Human Speech Processing Speech technology has learnt much from the sciences of linguistics and pho-netics about how the basic components of language fit together. It might turn next to neuroscience to learn how the components of speech are integrated for a fuller interpretation of the message as a whole, and for the role of speech prosody in particular. Little is known yet about how speech is processed in the human brain, but just as visual information is enhanced by stereoscopic input, so perhaps might speech be enhanced by binaural processing. 2.1. Binaural speech processing The auditory speech signal that enters the brain is processed first at the level of the olive, which functions to integrate the signals from both ears, but part of the signal from the right ear is also sent to the left hemisphere of the brain, and that from the left ear is sent to the right hemisphere of the brain. It is interesting to speculate on why this might be so. The speech sounds that we  X  X ear X  are filtered by the cochlea for frequency analysis at the lowest  X  X echanical X  level, and then by the different hemispheres of the brain at a higher  X  X erceptual X  level, to produce an image of the content that is  X  X nder-stood X  by the listener. We know that the right hemisphere is more attuned to a wider time-window of processing, being more sensitive to affect and emo-tion, and that the left hemisphere is more attuned to fine details of linguistic content (Ross, 1996, 1998). We do not yet know how these different levels of speech processing are combined, or bound, nor do we know what form the resulting image might take before an integrated understanding of the various levels of information in the speech signal can occur, but it seems that the contribution of each hemisphere may be complementary. 2.2. The roles of the two hemispheres Sensory and motor information is processed by distinct but interconnected regions of the cortex. Unlike computers, there is no  X  X entral processing unit X  in the brain that combines the separate streams of information from the various distributed processing regions, but instead the different regions each process their different types of information independently, and are simulta-neously activated (Toates, 2001).
 order cognitive behaviours such as planning, organisation, and monitoring of recent events, outcomes of actions and the emotional value of such actions (Tucker et al. , 1995). Several studies have confirmed that the understanding of propositional content activates the prefrontal cortex bilaterally, on the left more than on the right, and that, in contrast, responding to emotional prosody activates the right prefrontal cortex more. (e.g., Benowitz et al. , 1983; Blonder et al. , 1991; Bradshaw et al. , 1996).
 prosody.  X  X  X he ventral medial frontal regions are also important, perhaps because connections with the amygdala and other limbic structures give them a key role in the neural network for behavioural modulation based upon emotions and drives (Pandya and Yeterian, 1996) X  X .  X  X  X he frontal lobes are essential, with the right frontal lobe perhaps particularly critical, maybe because of its central role in the neural network for social cognition, including inferences about feelings of others and empathy for those feelings X  X  (Stuss et al. , 2001).
 different areas of the brain are simultaneously activated to provide a global percept of the social and emotional implications of an utterance along with 112 an image of its propositional or linguistic content. However, research into prosody for speech synthesis has concentrated almost exclusively on the linguistic uses of intonation and timing. We might infer from the above that when listening to computer speech, the stimulation of the right brain is considerably weaker than that of the left, because although the linguistic content of a synthesised utterance is adequate for recognition of its meaning, the paralinguistic information about its social implications is lacking. Simi-larly, in speech recognition technology, this information has been almost completely disregarded. 2.3. Paralinguistic speech processing One of the earliest inquiries into the neurology of speech prosody arose from experience with a patient suffering from acute Broca X  X  aphasia caused by a shrapnel wound to the left frontal area of the brain (Monrad-Krohn, 1947). Finding that prosody processing was intact, but linguistic processing im-paired, Monrad-Krohn X  X  work distinguished four main categories or func-tions of speech prosody: (i) intrinsic prosody , or the intonation contours which distinguish a (ii) intellectual prosody , for the placement of stress, which gives a sentence (iii) emotional prosody , for expressing anger, joy, and the other emotions, (iv) inarticulate prosody , which consists of grunts or sighs and conveys Ross elaborates:  X  X  X ialectal and idiosyncratic prosody are also to some de-gree subsumed by the term  X  X ntrinsic prosody X  and refer to regional and individual differences in enunciation, pronounciation and the stresses and pausal patterns of speech. Intellectual prosody imparts attitudinal informa-tion to discourse and may drastically influence meaning. Emotional prosody inserts moods and emotions, such as happiness, sadness, fear and anger, into speech. The term  X  X ffective prosody X  refers to the combination of attitudinal and emotional prosody. When coupled with gestures, affective prosody im-parts vitality to discourse and greatly influences the content and impact of the message. If a statement contains an affective-prosodic intent that is at vari-ance with its literal meaning, the former usually takes precedence in the interpretation of the message both in adults and to a lesser degree in children. For example, if the sentence  X  X  had a really great day X  is spoken with an ironic tone of voice, it will be understood as communicating an intent opposite to its linguistic meaning. The paralinguistic features of language , as exemplified by affective prosody, may thus play an even more important role in human communication than the exact choice of words X  X . (Ross, 2000; my italics) volves the making of inferences about feelings of others and having an empathy for those feelings. The  X  X ig-six X  emotions of anger, joy, fear, etc., (Ekman, 1972) that are the subject of much current speech research, may be better considered as an indicator of what the  X  X uman animal X  is experiencing in terms of drives and motivations, but not what is most influencing the  X  X uman social agent X  in the speech production process. It may be more appropriate to consider these basic types of emotion as incidental informa-tion in speech, since pure uncontrolled displays of anger and fear are ex-tremely rare in everyday conversational interactions. Our early socialisation training in public education and at home serves to ensure that the basic emotions are usually kept well under control in a social context. linguistic elements such as grunts and sighs to embellish discourse, is a reliable carrier of affective information, signalling to the listener the state-of-mind and attitudes of the speaker. We might consider the so-called inarticulate prosody to be the most articulate of all when it comes to the understanding or  X  X eading between the lines X  of interactive or conversational speech. 3. Data-based Research Whereas much research into the neuro-psychology of speech has been based on the study of lesions (e.g., Baum and Pell, 1999), observing what becomes disfunctional when damaged, the majority of speech technology research is based on the statistical analysis of corpora, or databases. The distinction between these two terms is not trivial, and the difference has had a profound effect upon our research.
 for ease of retrieval by computerised methods; a  X  X orpus X , on the other hand, is  X  X  X  collection of naturally-occurring spoken or written material in machine-readable form X  X  (Sinclair, 1991)  X  X  ... that are in themselves more-or-less representative of a language X  X  (McArthur and McArthur, 1992)  X  X  ... for the systematic study of authentic examples of language in use X  X  (Crystal, 1991). The important difference is that while both comprise an accumulation or 114 assemblage of texts or recordings which can be considered as representative of a genre, the former is usually  X  X onstructed X , and the latter  X  X btained X . More specifically, a database is purpose-built; a store of information which is structured from the beginning, while a corpus is a body of information from which knowledge can be derived. When designing speech databases, care is usually taken to exclude all inarticulate prosody, since it is associated with  X  X ll-formed X  speech. 3.1. Constructed data The early speech databases, reflecting an interest biased towards speech production processes rather than speech communication, were designed primarily for balance of phonetic content; usually being read lists of words or sentences to illustrate all combinations of the individual speech sounds in various contexts. Later databases, even those of so-called  X  X motional X  speech, usually consisted of lists of (often  X  X emantically neutral X ) sentences that were read in a controlled environment by professional or trained speakers spe-cifically for the purpose of analysis. The speech was allowed to vary only in the dimension to be studied. A typical procedure is described as  X  X  X he speakers were shown a sentence and an emotion label on the screen, after which they were asked to speak that particular sentence with that particular emotion. The four different emotion labels used were happiness, sadness, anger, and fear X  X  (Dellaert et al. , 1996). This type of  X  X motional X  prosody, although the first that comes to mind when the term is mentioned, may be more relevant to the realm of extralinguistic information than to any delib-erate or revealed communication strategies. When it is acted or produced at a prompt, it is not expressed as a contextualised or situated utterance, but simply generated as a sample. It may be good data, but it is not part of a corpus that we can learn from. It is not authentic, not naturally occurring, probably not even representative of normal situated speech, and does not help us to study  X  X anguage in use X  since it has never been  X  X sed X ; i.e., the mouth has moved, but not the heart.
 such recordings take on a permanence. Many are worked upon, before release, so that extraneous noises and  X  X erformance errors X  are cut; the  X  X mms X  and  X  X ahs X  edited out, silences, restarts and hesitations removed, so that what remains is a polished and refined version close to what the designers had in mind, but necessarily removed from the raw performance of living speech. Being text-based to begin with, these performances and their production process remove all but the text and the targeted differences from the resulting speech. The resulting technology illustrates the linguistic or text-related aspects of the speech signal well, but lacks much of the interpersonal information that is characteristic of spoken interaction. Even with databases of  X  X motional X  speech, the style is stereotypical; each target emotion may be recognised at levels significantly greater than chance on a forced-choice test, but none contains the rich information of naturally occurring speech communication. 3.2. Found data Collecting a corpus of  X  X atural X  interactive or conversational speech is not a simple task. As Labov discovered, people change when confronted with a microphone, and their speech becomes self-monitored. Conversations be-come less natural as the element of permanence enters in. Ethical and legal problems prevent the covert monitoring of speech, even for scientific re-search, and copyright restrictions govern the use of existing or broadcast materials (Roach et al. , 1998).
 (Labov, 1972) and now corpora of naturally-occurring speech are becoming available for wider research. We found from our analysis of the ESP (Expressive Speech Processing) corpus (Campbell, 2004), which now contains almost five years of daily conversational speech from a limited number of speakers, that there was remarkably little overt expression of the big-six emotions, but a great variety of different ways that speaking styles changed as a consequence of listener and subject differences. In particular, the  X  X runts X  and noises (so-called  X  X illers X (!)) that are usually filtered out of a custom-designed database or ignored in speech recognition were remarkably fre-quent, and appeared to be reliable indicators of what above we have called  X  X ight-brain information X , or affect (Campbell and Erickson, 2004). 4. Getting to the Heart of the Matter Speech technology has been driven by the needs of scientists and engineers to produce machines which are capable of processing human speech. It has evolved from heuristic methods based on experience and retrospective cog-nition, to more statistical processes based on large bodies of data. However, for very sound reasons of scientific balance and enquiry, much of the research has been based on studies of materials that are not representative at all of daily conversational speech. They were collected to illustrate speech processes but, being purpose-designed, were limited to only those aspects of speech considered to be relevant or worthy of analysis at the time. The criteria were biased towards linguistic or production models, and interpersonal speech communication was not considered to be of prime concern.
 linguistic processing and the right hemisphere better tuned for affective processing, then it is likely that, when listening to speech, the combination of 116 the reactions of the two hemispheres provides  X  X epth X  to a spoken utterance. If the prosody of an utterance is tuned only for linguistic content, as happens for computer speech synthesis at the present time, then that utterance will likely appear unnaturally  X  X hallow X . The call for  X  X motion X  in speech may be a reaction to this lack of  X  X epth X  in speech synthesis, but the extra information that is required is not that of raw emotional expression; rather it is an expectation of social information such as that which signals speaker X  X istener relations, and speaker-attitude and affect. 5. Conclusion This paper has presented a personal view of recent developments in speech technology research, with a focus on corpus-based speech processing, and has claimed that the current call for  X  X motion X  to be included in speech processing might be better phrased instead as one for the expression of affect and interpersonal relationships.
 Acknowledgements This work was supported in part by the Japanese Science &amp;Technology Agency, and was conducted at the Advanced Telecommunications Research Institute (ATR) with help from the NiCT, the National Institute of Information and Communications Technology. The author is grateful for the continuing support of ATR in this non-verbal speech processing work as part of the FEAST (Feature Extraction and Analysis for Speech Processing).
 References
