 Anomaly detection (also known as novelty or outlier detection) is an important component of many data stream mining applications. For example, in network intrusion detection, anom aly detection is used to det ect suspicious behaviour that deviates from normal network usage. Similarly, in environmental monitor-ing, anomaly detection is u sed to detect interesting events in the monitored environment, as well as to detect faulty sensors that can cause contamination of the collected data. A major challenge for anomaly detection in applications such as these is the presence of nonstationary behaviour in the underlying distribution of normal observations. In addition, the high rate of incoming data in such ap-plications adds another challenge to anomaly detection, as the anomalous data points should be detected in a computatio nally efficient way with high accuracy. Many anomaly detection techniques are based on an assumption of stationar-ity. However, such an assumption can r esult in low detection accuracy when the underlying data stream exhibits nonstationarity. In this paper, we present a novel method for robust anomaly detection for a special class of nonstationarity, known as switching data streams.

Many environments are intermittently changing, and thus exhibit piecewise stationarity. For example, sensors in an office environment may collect observa-tions with different underlying statistical distributions between day-time and night-time. We refer to the data from such environments as switching data streams. In addition to large-scale switching behaviour, there can also be finer-scale variation in the distribution of observations, e.g., variation during day-time observations. To address these challenges, we propose a novel approach to anomaly detection that can se lectively learn from previous time periods in order to construct a model of normal behaviour that is relevant to the current time window. This is achieved by maintaining a cluster model of normal behaviour in each previous time period, and then constructing an ensemble model of normal behaviour for the current period, based on the relevance of the cluster models from previous time periods to the current time period. This relevance-weighted ensemble model of normal behaviour can then provide a more robust model of normality in switching data streams.

While ensemble methods have been widely used in supervised learning [1,2], their use for unsupervised anomaly de tection in data streams is still an open research challenge. A key advantage of our approach is that it can improve the accuracy of anomaly detection in switching data streams. We have evaluated the effectiveness of our approach on large-scale synthetic and real sensor network data sets, and demonstrated its ability to adapt to the intermittent changes in switching data streams. We show that our approach achieves greater accuracy compared to a state-of-the-art anomaly detection approach for data streams. A number of surveys [3,4] have categorized various techniques for anomaly de-tection. Most focus only on static environments and do not consider dynamic behaviour. In dynamic environments an unknown volume of data (data streams) are produced. Hence the major challenge is how to detect anomalies in such en-vironments in the presence of dynamics in the distribution of the data stream. Based on existing surveys of anomaly detection in data streams [5,6], we cate-gorize these methods into Model based and Distance based approaches.

Model based approaches build a model over the data set and update it as data evolves with each incoming data point. In this category, some authors learn a probabilistic model. This requires a priori knowledge about the underlying distribution of data, which is not appropriate if the distribution is not known [7,8]. Other approaches use clustering algorithms to build a model for normal behaviour in data streams. However, t hey are not really designed for anomaly detection [9,10]. An exception is [11], i n which a segment based approach is pro-posed. It uses k -means as a base model and provides guidelines regarding how to use the proposed approach for anomaly detection. As a result, it assumes the number of clusters is known, which may not be the case in changing environ-ments (since the number of clusters may change over time). In addition, while these clustering techniques work for gra dually evolving data streams, they are not applicable in switching environments, which is our focus in this paper.
Distance based approaches are the second category of a nomaly detection tech-niques in data streams. We have two types of distance-based outliers:  X  X lobal X  and  X  X ocal X . Distance-based  X  X lobal X  outliers are first introduced by [12], where a data point x is a distance-based outlier if less than k data points are within a distance R from it. In [13] and [14], a sliding window is used to detect such global outliers. Since parameter R is fixed for all portions of the data, these approaches fail to detect anomalies in non-homogenous densities. In contrast, distance-based  X  X ocal X  outliers are data points that are outliers with respect to their k nearest neighbours without considering any distance R . Local outliers are first intro-duced in [15] and a measure of being an outlier -the Local Outlier Factor(LOF) -is assigned to each data point in a static environment. Later, in [16] and [6] two similar approaches are proposed to find local outliers in data streams. While the former made an assumption about the underlying distribution of data, the latter (Incremental LOF) extended [15], and could detect outliers in data streams by assigning the LOF to incoming data points and updating the k nearest neighbors. However, there are still some limitations with this approach in the presence of switching data streams: 1) It has problems with detecting dense drift regions in data streams, which results in false negatives. 2) In switching data streams, the distribution of normal data can change suddenly. Since incremental LOF keeps all of the history of the data points, it could not differentiate between different states, which again results in false negatives (e.g., a data point is an anomaly in one state while it is not an anomaly in another). 3) It is hard to choose the parameter value k in the presence of changing distribution environments. We propose to address these open problems for anomaly detection in switching data streams by using an ensemble approach.

Ensemble approaches have been shown to have benefits over using a single classifier, particularly in dynamic environments. So far, several ensemble learn-ing methods have been proposed for data stream classification [1,2]. In contrast, there is little work done on anomaly detection using ensemble techniques [17]. In a recent comprehensive survey paper [18], a categorization of outlier detection en-semble techniques is based on the constituent components (data/model) in these techniques. Nevertheless, none of these approaches are aimed at handling switch-ing data streams or even streaming data.

Since incremental LOF (iLOF) is the only anomaly detection technique in streaming data which handles different densities and is reported to detect changes in data distributions, we use it as a baseline to evaluate our proposed approach. In this section, we formally define our problem and proposed algorithm. 3.1 Problem Definition We begin by describing our notation for switching data streams. We consider the problem of anomaly detection in a switching data stream, where the underlying distribution of observations is only piecew ise stationary. That is, the monitored environment switches between a number of  X  X ormal X  states, such as day vs night in an episodic manner. Let the state of the system be a random variable over the domain of possible states S = { s 1 , ..., s D } , where the system has D normal states. The distribution of an observation X in state s d is drawn from a mixture of K d components. For example, each compon ent of the mixture distribution of state s d could be a multivariate Gaussian distribution with different means and covariance. Let = {  X  d,i ,i =1 , .., K d } denote the parameters corresponding to the K d components of state s d , e.g.,  X  d, 1 = {  X  d, 1 , X  d, 1 } for a multivariate Gaussian distribution. Further, let = {  X  d,i ,i =1 , .., K d } denote the mixture weights of the K d components of state s d , i.e., the prior probability that a ran-dom observation in state s d comes from component i is  X  d,i .

We observe a stream of observations, where X (1 : T ) denotes the sequence of observation vectors coll ected over the time period [ t 1 , ..., t T ]. At time t ,we receive a vector of observations X ( t )=[ x 1 ( t ) , .., x P ( t )] T corresponding to P different types of observation variables, e.g., temperature, pressure, humidity, etc, where X ( t )  X ! P . If the environment is in state s d at time t ,then X ( t )is sampled from one of the K d component distributions in d of state s d .
The monitored environment is initially in some random state s (1)  X  S at time t , and remains in that state until a later time t c 1 when it switches to a different state s (2)  X  S \ s (1) . It then remains in state s (2) until a later time t c 2 when it switches again to s (3)  X  S \ s (2) , and so on. Our aim is to detect anomalies in this data stream X . In order to detect anomalies, we require a model of  X  X ormal X  behaviour for the environment, given that the number of possible states, the number and mixture of components in each state, and the parameters of each component in each state are unknown a priori.

In order to learn our model of normal behaviour, we could estimate all of the different component parameters for all states by keeping all data X (1 : t ). However, this is impractical due to the memory requirements and it would mix all the states together when building the normal model. Alternatively we can learn only from the window of the w most recent observations X ( t  X  w +1: t ). While this is more computationally efficient, it only provides a limited sample of measurements from the current state. If the window size w is small, then this might yield a noisy estimate of the component distribution parameters.
The problem we address is how to find a b alance between these two extremes, by maintaining multiple models of normal behaviour based on previous data windows. Our expectation is that this will provide us with a balance between minimising the memory requirements for our anomaly detection scheme while maximising accuracy. However, the key challenge in achieving this balance is that not all previous windows will be relevant to the current window of observa-tions, due to the switching nature of the data stream. Consequently, we require a method to take into consideration the relevance of previous windows, so that we can construct a model of normal behaviour based on the current and previous windows. In particular, we need a way to weight the influence of previous win-dows on data in the current window, based on the degree of relevance of those previous windows. We consider different kinds of anomalies in the system: either localized in time, i.e., a burst of noise or a drift in the data stream, or uniformly distributed over the data stream. 3.2 Our Ensemble-Based Algorithm We now describe the main steps of our algorithm for ensemble anomaly detection in switching data streams. Algorithm 1 shows the pseudo code of the ensemble algorithm, comprising three main steps: Windowing , Weighting and Ensemble formation. We assume the previous data streams have already been clustered based on windows of w observations and the problem is how to detect anomalies in the most recent (current) window of w observations. 1. Windowing Step : In general, in applications that generate data steams, the ob-servations arrives sequentially. We consider a data stream that switches between different states, where each state comprises different component distributions. By breaking the whole data stream of observations into windows of size w ,we can extract the different underlying distributions of the data streams.
In this step, we cluster the current w observations by using an appropri-ate clustering algorithm, in order to find out the current underlying component distributions. We chose the HyCARCE clustering algorithm [19], which is a com-putationally efficient density-based hyperellipsoidal clustering algorithm that au-tomatically detects the number of clusters, and only requires an initial setting for one input parameter, i.e., the grid-cell size. In addition, it has a lower computa-tional complexity in comparison to existing methods. Hence, it is an appropriate clustering algorithm for our data stream analysis problem in which time is a vital issue and the number of clusters is unknown. Interested readers can find a detailed description and pseudo code for the HyCARCE algorithm in [19]. At the end of this step a clustering model is built that comprises a set of cluster boundaries, C  X  = { c  X , 1 , ..., c  X , | C ellipsoidal clusters as our decision boundaries. 2. Weighting Step : According to our windowing step, all of the previous w sized observation windows have already been clustered. The history is used to better estimate the underlying distribution of the current window. However, not all of the previous clusterings have the same level of importance, and some of them might be more relevant to the current window, as the data stream switches be-tween different states. Our solution, which is called the Weighting step ,isto assign weights to previous clustering models based on the similarity between the current and previous c lustering models.

In this step, the distance between each previous clustering model ( C j )andthe current clustering model ( C  X  )iscomputed.Astimecomplexityisamajorissue for anomaly detectio nondatastreams,a greedy approach is proposed to com-pute this distance. In this approach, the distance between each pair of cluster boundaries in C  X  and C j is computed based on their focal distance [20]. Focal distance is a measure of the distance between two hyperellipsoids considering their shapes, orientations and locatio ns. According to a recent work [20], this measure works well for computing the similarity between hyperellipsoids.
After determining the focal distance, the algorithm finds the minimum dis-tance among all computed distances, and assigns the relevant cluster boundaries as a matching pair. It continues this process, until all of the clusters in at least one of the two clustering models are each matched to a corresponding cluster in the other model. The sum of the found minimum distances is used as the dis-tance between two clustering models. Ob viously, if the models are less distant, they would be more similar. Therefore, th e distances are reversed at the end of the algorithm to show the similarity. The relevant pseudo code is described in Algorithm 2. Finally, in order to use the current clustering model in the ensemble step, the maximum assigned similarity among all previous clusterings is assigned to the current clustering. In this way, we can find a balance between the current window and previous windows for anomaly detection. 3. Ensemble Step : In this step, anomaly detection is performed based on the current and previous clusterings. As discussed in the previous subsection, not all historical models are useful, due to the changing environment. Hence, for each current observation, we check if it belongs to a hyperellipsoid in each of the clustering models according to the following definition: Definition 1. The observation X belongs to hyperellipsoid c j,i , if the Maha-lanobis distance between them is less than a threshold t 2 , where the Mahalanobis distance is: where  X   X  j,i and  X   X  j,i are the mean and covariance of hyperellipsoid c j,i respectively, and the threshold is: where t 2 is the inverse of chi-square statistic,  X  is the percentage of data covered by the ellipsoid, and P is the dimensionality of the observations.

Thereafter, the probability of being outlier is computed for the current obser-vation according to the formula in Algorithm 1 (line 16). The formula is based on a relevance voting. The algorithm computes this probability for all data points X in the current window. 3.3 Time Complexity The total number of observations (data points) N is divided into windows of size w .Let l be the average of number of ellipsoids in each C j . Therefore l is a function of K d ,d = { 1 , .., D } and K d N . The time complexity for the Windowing Step is O ( N ) as the complexity of HyCARCE is near linear with respect to the number of data points. The Weighting Step can be computed in O ( In the last step of the algorithm,  X  X elongs X  can be computed in O ( l )sothetime complexity of the Ensemble Step is O ( wl ). As a result, the time complexity of Algorithm 1 is O ( N ( of data points. Hence, our ensemble approach is computationally efficient. Time complexity of iLOF is also near to linear depending on the parameter k . In this section we aim to comp are the accuracy, sensitivity and specificity of our ensemble model with the iLOF algorithm on several dynamic environments. 4.1 Data Sets We use three different data sets to evaluate our approach.
 Synthetic Data Set: In order to generate synthetic data sets, we consider a state machine that can simulate a switching data stream from a changing envi-ronment. We assume there are only two different states ( S 1 ,S 2 ) and changing from one state to the other occurs periodically. The first state ( s 1 ) has three un-derlying component distributions  X  1 , 1 ,  X  1 , 2 and  X  1 , 3 with corresponding mixture distributions  X  2 , 1 and  X  2 , 2 with mixture weights  X  2 , 1 and  X  2 , 2 . Altogether the environment consists of five component distributions. The states are changing periodically based on a constant rate (every m = 100 samples). In addition, the distributions are hyperellipsoids and the initial mean and covariance is chosen randomly. As discussed earlier, from one instance of a state to the other, for the same state, we have some perturbations in the mean and covariance of the underlying distribution, i.e.,the parameters of  X  d,i can be slightly perturbed be-tween different occurrences of state s d . By using this state machine, we have generated 50,000 2-dimensional records. We generated 10 different data sets and averaged the final results. The inserted noise was 2%.
 Real Data Sets: In order to evaluate our algorithm over a real data set, we used two public available data sets. These data sets contain periodic measurements over day (state s 1 ) and night (state s 1 ). The first is the IBRL (Intel Berkeley Research Lab) data set 1 . A group of 54 sensors were deployed to monitor an of-fice environment, from Feb. 28th until Apr. 5th, 2004. Moreover, by visualising the data collected by all the sensors, we observed that sensor number 45 stopped working in the last two days causing a drift which is labeled as anomalies (4%). The sensors were collecting w eather information. We chose two features, humid-ity and temperature. The measurement s were taken every 31 seconds and there are about 50,000 records.

The second real data set is from the TAO (Tropical Atmosphere Ocean) project 2 , by the Pacific Marine Environmental Lab of the U.S. National Oceanic and Atmospheric Administration. This monitors the atmosphere in the tropical Pacific ocean. We have used the period of time from Jan. 1st until Sep. 1st, 2006 which is used in [13]. We chose three features, precipitation, relative hu-midity and sea surface temperature. Amo ng the different monitoring sites, we chose site:(2  X  N,165  X  E), since this site has all three features available for the mentioned period of time. The measure ments were taken every 10 minutes and there are about 37,000 records. This data set has some labels on the quality of measurements and we have used them for our evaluation. After visualization of the low quality data points (2%) and good quality ones, we find that in this data set the noise is a dense separate region from the normal observations. 4.2 Performance Measures In order to evaluate our algorithm in comparison to iLOF, we have used three performance measures: (1) Area under the ROC curve (AUC). (2) The accu-racy ( positives (anomalies) in the data set, N is the number of negatives (normal obser-vations), TP is the number of true positives (correctly reported anomalies) and TN is the number of true negatives (correctly reported normal observations). The optimal point on a ROC curve is the point with the maximum distance from diagonal line using the Youden Index ( max i is the sensitivity for the i th threshold and SP i is the specificity for the i th threshold. (3) The ratio of co rrectively detected anomalies sensitivity ( correctly detected no rmal observations specificity ( optimal points. 4.3 Results and Discussion We have compared our ensemble approach with iLOF on the three test data sets. For iLOF we have used the implementation provided in ELKI[21]. All measure-ments were normalized based on min-max normalization and we set  X  =0 . 99 in Equation 2 [19]. Moreover, we have studied how the performance of our approach varies over different window sizes. The window size w is initially set to 100 ob-servations, and then it is increased by increments of 500 observations until w is of the whole data set length (because we need at least two window sizes for voting). We also studied the effect of changing the number of nearest neighbours k in the iLOF algorithm, where k  X  X  3 , 5 , 10 , .., 200 } . Since the computational time of the iLOF algorithm increases with k , we set the upper bound to 200 to obtain a reasonable runtime.

We have computed the ROC curves for different window sizes w and number of neighbours k . Figure 1 depicts only the best ROC curves (thicker graph) and worst ones (thinner graphs) for simplicity for both approaches over three different data sets. The results show that in the two real data sets with dense outliers, our approach is better than iLOF by a large margin for both the best and worst curves (Figure 1a and 1b). In addition in Figure 1b our worst curve is even better than iLOF X  X  best curve. Howev er, in the synthetic data set in which the outliers are uniformly distributed over the data stream, both approaches have approximately the same best curves, and our approach has better AUC in comparison to iLOF in the worst case (Figure 1c). In order to make a more detailed comparison, we computed the optimal points of different ROC curves as we described in Section 4.2. The results of the optimal point X  X  accuracy, sensitivity and specificity are shown in Table 1.

The accuracy and specificity of the optimal points in our approach are higher than iLOF in all data sets. This means that iLOF produces more false negatives. In the IBRL and TAO data sets which are the real data sets, we have dense out-liers (either drift or a separate distribut ion) which yields the false negatives. As can be seen the minimum specificity of iLO F in both real data sets is extremely low. Moreover, since in all three data sets we have switching states our ensemble algorithm can perform better in terms of specificity, whereas iLOF keeps all the history of the data points, and it could not differentiate between different states, which again results in false negatives. The last three columns of the table show the optimal point X  X  sensitivity. The results show that we have much higher sensi-tivity in comparison to iLOF in the IBRL and TAO data sets and iLOF fails to detect anomalies with a considerably lower rate. However, in the synthetic data set with uniformly distributed anomalies, iLOF performs better in terms of finding anomalies in the average and maximum cases. However, the difference is small, and our ensemble method performs better i n terms of overall accuracy. Finally, considering the max, average and min in all measures in Table 1, our method per-forms more consistently, which shows the results are less dependent on choosing the window size w . Also, for iLOF the range between the max and min cases is larger, indicating that it is sensitive to the choice of the parameter k .
In summary, our approach outperforms iLOF on the two real data sets with dense anomalies. Moreover, it is better t han iLOF in accuracy and specificity on the synthetic data set, while iLOF only performs better in terms of sensitivity in the synthetic data set with uniformly distributed anomalies. In this paper we proposed a novel approach to the problem of anomaly detection in data streams where the environment changes intermittently. We introduce an ensemble based approach to construct a robust model of normal behaviour in switching data streams. Although there have been many supervised techniques based on ensemble models for outlier det ection in data streams, to the best of our knowledge there is no similar work that tackles the problem of using ensemble techniques for anomaly detection in data s treams. The empirica l results show the strength of our approach in terms of accuracy. This highlights several interesting directions for future research. First, we can explore alternatives to our greedy approach for comparing clusterings. Second, we will investigate approaches to minimize memory consumption. Third, we will investigate different methods for selecting appropriate window boundaries.
 Acknowledgments. The authors would like to thank National ICT Australia (NICTA) for providing funds and support.

