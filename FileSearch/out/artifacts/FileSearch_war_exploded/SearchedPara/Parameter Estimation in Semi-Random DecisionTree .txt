 Streaming data are a key type of data in business applications, such as website shopping transactions, Internet search requests, and telephone call records. In these continuous, high-volume and open-ended data streams, it is a challenge to perform learning with traditional inductive models or algorithms[11][12][13].
An inductive model of Semi-Random Decision Tree Ensembling (SRDTE) is presented in this paper. We have built our work on SRMTDS[10], where attributes are selected ra ndomly and a heuristic and semi-random strategy is adopted to estimate thresholds of numerical attributes and to generate multiple decision trees incrementally. There are thr ee different features in the strategies compared with SRMTDS. Firstly, it does not grow children branches at one node in the tree generation phase until there are real training instances passed. Secondly, a new Children-List data structure is designed for a quick search. These strategies are both more suitable for streaming data environments in terms of space and time. Lastly, SRDTE makes use of a voting mechanism with the ma-jority class for classification to improve predictive accuracy . (In order to address these differences, we call the develope d algorithm a new name of  X  X RDTE X  in-stead of the used one  X  X RMTDS X .)
Besides, in the analysis of the strength and correlation of classifiers, Breiman [3] pointed out that the generalization error for a model of random tree en-sembling is dependent on the strength of t he decision tree structure (i.e., the measure of classification accuracy of the classifier, denoted by s )andthemean correlation (or dependency, marked by  X  p ) between the base classifiers, which is applicable to classifier based ensembling including random trees[5]. Due to the semi-randomized method adopted in SRDTE, the value of s is nondeterminis-tic. Meanwhile, because the generation of a single tree is independent with each other, the immediate corollary is that the generalization error in SRDTE is al-most only related to the factor of s , which is mostly impact ed by the parameters of the initial height of tree-h 0 , the count of trees-N and the minimum number of split-instances-n min used in the Hoeffding Bounds inequality. To obtain their optimal values of these parameters for strengthening the abilities of SRDTE, extensive experiments and analysis hav e been done in order to take into account of their qualitative and quantitative relations, which is different from SRMTDS as well. The results demonstrate that SRDTE has a higher predictive accuracy estimated by the 0-1 loss function and performs better in time, space and robust-ness. To further validate its adaptability and feasibility oriented to real world data streams, we have collected on-line shopping data for mining implicit mer-chant relations and hidden action patterns. The performance study shows that SRDTE is efficient and effective, and pro vides a significant reference model of classification in rea l streaming data. Compared with SRMTDS, the model of SRDTE to be described in this section also creates N semi-random decision trees incrementally till an initial height of h . When the number of training examples satisfies a certain threshold, rescan leaves and change some of them with higher error rates of classification by de-cision nodes to further split. If the total space consumption of decision trees is over a predefined restriction, release spaces of leaves or nodes with numerical attributes, and cut off some sub-trees if n ecessary. In conclusion, its basic frame-work is illustrated as follows.

However, in contrast to SRMTDS, three diverse strategies in SRDTE are adopted, which are given in details respectively.
 Incrementally Generate Decision Trees After Seeing Training Data Instead of growing branches blindly till up to h 0 in advance for SRMTDS, only the necessary branches are created in SRDT E after the real training data reach, whose details are shown in the function of GenerateIncreTree . More specifically, an available training instance follows the branch of tree corresponding to the attribute X  X  value in the instance and traverses from its root to a leaf or a node with a numerical attribute at Step (2). Once the passed node has no split-attribute, select an available attribute of A j as the split-feature at Step (4). If A j is discrete, mark the node as discrete and generate m A j +1 children nodes ( m A j is the count of different attribute values of A j ); otherwise, denote the node as numerical and create only two children branches ( X  &lt;  X  X r X   X   X  X cut-point value of A j ) in Step (5). Repeat this process for each child node till up to the height of h 0 at Steps (6)  X  (7). Otherwise, if the current node is a leaf, update the information associated with the leaf including the total number of training instances, the distributions of class labels and attribute values at Steps (9)  X  (10); If it is a numerical node without a cut-point, install the split-test to solve a split-threshold based on the heuristic method of Hoeffding Bound inequality [7][15] at Steps (11)  X  (13) after the statistic count of instances reaches n min ,wherethe feasibility and efficiency for streaming data in the Hoeffding Bound inequality have been verified in related efforts[6][1][10] and the definition is given below.
Consider a real-valued random variable r whose range is R . Suppose we have made n independent observations of this variable, and computed their mean  X  r , which shows that, with probability 1 - X  , the true mean of the variable is at least  X  r - X  , i.e., where the minimum value of n is n min ,thevalueof R is set to the number of class labels of log 2 ( M ( classes )). Compute the information gain of candidate cut-point values of attribute A j and suppose the difference of two candidate cut-points with the highest information  X  H = H ( A j ( x )) -H ( A j ( y )). For a given  X  (the value of  X  is lower than a standard value of  X  in Formula (1). It implies that the top-two candidate cut-points with the highest information gain are either good or bad without obvious deviation, which is given as a tie bound of differ-ence values), if  X  H &gt; X  or  X  H  X   X &lt; X  , the attribute value of the x th cut-point with the highest gain will be selected as a final split-point and more details are seen in [10].
 The Structure of Children-List for Quick Search For quick search of the target nodes, an additional data structure of Children-List (marked as Clist ) is designed in SRDTE to point to different levels in one tree. For example, Clist [ i ] projects the i th level in a semi-random decision tree, which binds all nodes in this level and records their information. It is applied into the process of scaling up the trees and r eleasing the node space at Steps 5  X  9. Voting Mechanism for Classification After each semi-random tree has made its decision on test instances at Steps 10  X  12, the judgment for final class labels is made with a majority-class voting or Na  X   X ve Bayes in Step 13, whose robustness for data streams has been demon-strated both theoretically and experimentally in [4]. Compute the sum for each class label in the tree ensembling and choose the class label with the maximum value as the one of the current test instance.

Besides, to achieve the optimal performance of SRDTE, the theorem of gen-eration error mentioned above is worth em phasizing before we discuss the esti-mations of related parameters in the following sections, as shown in Formula (2). Evidently, the generalization error of GE in ensemble classifiers is directly pro-portional to  X  p and is in inverse proportion to s . Though the voting mechanism is adopted in the classification, the gen eration process of each decision tree in SRDTE is still independent of each other. Hence, the impact from  X  p is much less, which is similar with that in SRMTDS. As regards the value of s , the pa-rameters pertinent to the magnitude of trees refer to the ones in the inequality of Hoeffding Bounds, h 0 and N . In a similar analysis with SRMTDS, the prob-ability ensuring the optimum of an ensemble model in SRDTE is more than and h 0 , the less the generalization error is. Therefore, for the least value of GE , it is important to search for optimal parameter values. In this section, we first discuss the heuristic method to select the relevant param-eter values, then analyze the perform ances of SRDTE based on optimal estima-tion values. Last, we apply SRDTE to real shopping data from Yahoo! website for further verification of its efficiency an d effectiveness. In our experiments, we have also used several synthetic data sets from UCI[8] that are the representative ones with various types of attributes. Moreover, the different databases varying with the noise rates are generated by the LED data generator for the estima-tion on the robustness of SRDTE, where the training set of a varying number of examples starts from 100k to 1000k and the test set contains 250k examples. All experiments are performed on a P4, 3.00GHz PC with 1G main memory, running Windows XP Professional with the program platform of Visual C++ in a simulated parallel environment. 3.1 Parameter Estimation Parameter Analysis for  X  ,  X  and n min The Hoeffding Bound inequality adopted in SRDTE is applied to find better split thresholds at nodes with numerical attributes, which impacts the classification ability of individual trees. According to Formula 1, the smaller the value of  X  ,the more accurate the gener ated tree is; however, the larger the value of  X  will be. Thus, it is key to achieve a good trade-off of  X  and  X  and an important goal is to reduce the value of  X  for a lower error probability. Supposing the value of  X / R is 0.01, to ensure  X  = 0.001, it only requires n min = 345. Actually, the greater the value of n min the higher the probability of accuracy is. But it will lead to more computational complexity at numerical nodes. Therefore, the study on n min is much more significant for better res ults, which restricts the value of  X  .
Given  X  = 0.05, we set values of n min from 0.1k to 1k (1k = 1000) with a 0.1k variance each time, to ensure an invariant value of h 0 ; and run 100 times for each case and make use of k -single semi-random decision trees to vote ( k = 10). Experimental results are plotted in Figure 1 (where  X  X ax-Single X  and  X  X ayes-Single X  represent the values of 100-single trees on average in the classi-fication methods of majority-class and Na  X   X ve Bayes respectively,  X  X ax-Voting X  and  X  X ayes-Voting X  refer to average results of 10-votings), which demonstrate several characteristics. F irst, generally the voting result is in direct proportion to the average value of single trees and p erforms better. Second, the prediction accuracy of voting in  X  X ayes X  is best, in which the least improvement rate is also more than 6% averagely as compared with the results in  X  X ax X .

However, as the value of n min increases, the classification error presents in a light variance, which is limited to the bound of (0%, 3.54%). It is especially suitable for the case classified in  X  X ayes X . As we know that the larger value of n min indicates the more computation cost when splitting and the fewer instances collected at leaves, which probably resu lts in the reduction on predictive accu-racies. Thus, it is better to select a smaller value of n min .Thevastmajorityof experiments show that a candidate optimal value of n min is set to 0.2k. Analysis of Parameters h 0 and N Massive experiments are conducted in the cases that the value of h 0 varies from 2toMin(10, M (Attr)/2) with N = 100 and n min = 0.2k (the maximum bound of height is set to M (Attr)/2 here, which is a primary conclusion in [2]). And the error rates averaged in 100-single trees are listed in Table 1, which of the minimum values are high-lighted in bold. Due to the limited space, the figures of change trends are omitted but several conclusions observed from the experimen-tal results are given below. Firstly, the curves go with a stable tendency and the error rates of 100-single trees on average is approximately in direct proportion to the height of h 0 , especially for the classification results with  X  X ax X . Secondly, in the case with  X  X ayes X , the predictive accuracy improves continuously as the value of h 0 increases till up to a certain value. Obvious turning points of down-trend appear, which are probably candidate optimal values we need.

To find an optimal value of h 0 , not only should the pre dictive accuracy be en-sured; but also the space overhead should be considered. Thus, the space costs of one single tree in different values of h 0 are further estimated, as shown in Table 2. The results demonstrate that space overheads go up slowly with the increasing of h 0 for Covertype and Waveform-40, while it increases sharply for Connect. The analysis shows that if the value of h 0 is initialized to 6 or a larger value, the space consumption increases from 0.5 to 80 times in comparison to that with h 0 = 5, while the prediction accuracy only improves less than 1.5%. Thus, our preliminary conclusion is to select the value of h 0 =Min(5, M (Attr)/2) as an optimal height of the tree for any databases.
 Based on the optimal value of h 0 = 5, we consider the voting results from N -classifiers and initialize the value of N varying from 3 to 15. The curves of average results based on 10-runs are drawn in Figures 2  X  3(where vo refers to the classifier count). Evidently, with the increasing of the number of clas-sifiers, the voting results from 3-to 15-classifiers vary lightly, especially when classifying with  X  X ax X . According to the theory of generation error in random ensembling trees, the more the number of trees, the more the probability of rele-vant attributes to appear in one single tree. However, more classifiers for voting will result in a heavier overhead of spac e with a less improvement of accuracy; hence, the selection on the value of N also should consider the space cost and the requirement on accuracy contem porarily. Finally, we select N = 10, though the lowest error-rate occurs when the number of classifiers equals the upper bound of N = 15 in the case of classification with  X  X ax X . 3.2 Model Evaluation With the obtained optimal values of parameters from the above analyses ( N = 10, n min =0.2kand h 0 = 5), the performance of a s ingle semi-random decision tree in SRDTE is measured by the 0-1 loss function, in which misclassifications are the loss cost.
 Predictive Accuracy To study the effect of predictive accura cy, we have compared SRDTE with the VFDTc algorithm [1] improved from the state-of-the-art algorithm of VFDT [6] and made use of the  X  X egment Probability Estimation X (abbreviated as SPE )to visualize processes of classification in Figure 4. Where SPE refers that testing data are divided into K -segments averagely (suppose the instance number in one segment is C ; K =100) and each segment projects a score, i.e., the score of the i th segment is i / K . After finishing a classificatio n decision, count the number of accurate class labels (e.g., c ) and obtain a probability of estimation( c / C * i / K ). Hence, a point ( i / K , c / C * i / K ) of estimation is formed in the 2-dimension axes (where the x -axis is the score and the y -axis is the estimated probability). Ob-viously, the perfect line composed of these points is y = x .Themorethepoints approximate this curve, the better the ability of classification is. Figures 4-(c), (e) and (f) explicitly present that prediction ability of SRDTE outperforms VFDTc. The final accuracies have improved by the largest rate of 3.6% in the case with  X  X ax X  and increased from 1.5% to 10.9% in  X  X ayes X , which are demonstrated in Table 3 as well. However, in Figures 4-(a)  X  (b), the difference of classification between both algorithms is not explicit, which the average predictive probabili-ties with  X  X ax X  in SRDTE are lower than those in VFDTc by about 0.1% for Connect while it increases by 2% in the ca se with  X  X ayes X . Moreover, though the result of classification in SRDTE seems worse for Covertype in Figure 4-(d), its average predicative probability is still more than that of VFDTc by 0.8% and the final accuracy of classification w ith  X  X ayes X  is improved by 1.9%. Speed and Space In this subsection, the overheads of space and time are compared between SRDTE and VFDTc. The experimental results are shown in Table 4 and Figure 5. For the medium-sized databases in Table 4 ( TRT : training time and TCT : test time), both algorithms present an unremarkable derivation. The overhead of space demanded in VFDTc is heavier than that in SRDTE, as plotted in Figure 5-(a). Actually, it grows linearly and exceeds SRDTE as the sizes of databases increase, where the ascending rate is from 0.29-to 1.04-times. With respect to the total time consumption in Figure 5-(b), we can observe explicitly that both of the costs in the training time are directly proportional to the sizes of databases. Besides, the overheads of total time in SRDTE with  X  X ax X  are least in the test time carrying a little variance; while the total time consumed in VFDTc increases rapidly, which is due to the fact that all of the available attributes are required computation for the best split-feature.
 Robustness It is inevitable to be affected from noise in the classification of data streams. Thus, to inspect the robustness to the noise for SRDTE compared with VFDTc, a large number of experiments are estimated on the LED database with discrete attributes only, which owns 17 irrelevant attributes varying with noise rates in attribute values. Experimental results given in Table 5 demonstrate that the performance of  X  X RDTE-Bayes X  on anti-noise is the best almost maintaining the accuracy of 100%, which of the reason is similar to that of [10]. However, in the case with  X  X ax X , though the predictive accuracies in SRDTE are higher than those in VFDTc with a rate of 7.03% on average, it does not perform well in consideration of the demand on accu racy from user. We try to increase the values of h 0 and an interesting observation fro m further experiments is that the accuracy in  X  X RDTE-Max X  could be improved continuously with the increasing of h 0 . Thus, an additional experimental conclusion is given now. For several special databases with a large number of irrelevant attributes or more discrete attributes, if the space restriction is satisfied, the initial height of tree should approach to the value of Max(5, M (Attr)/2) in the case of classification with  X  X ax X .

Moreover, we observe with surprise th at the accuracy of  X  X FDTc-Byaes X  is the highest when the noisy rate is up to 30% compared with the results in the case of lower noise rates. A reason is cau sed that the distribution of class labels in LED is even. If the noise rate amounts to a certain value, a new distribution of attribute values would be formed possibly, which adapts to the case of Na  X   X ve Bayes. Actually, a similar accuracy will keep up until the noise rate is up to 37%. After that value, the accuracy of cl assification in  X  X FDTc-Bayes X  is much worse, which only approaches to 10.03%.
 Application We had a try to apply SRDTE into mining streaming shopping data from Yahoo! website via its web services interface. The database used in our experiments is sampled from their shopping database of product offers and merchants, includ-ing 113k records of transaction items with 23 attributes. The features contain the information of products ( Price , NumRating , ShippingCost , etc.) and related information of merchants, which consist in attributes of ( NumRatingsOfMer-chant , PriceSatisfaction-Rating , IsCertifiedMerchant , etc.) (See [9] for more de-tails). In order to mine the relations between credibility of merchants and pos-sible factors, we define the attribute of OverallRating with different scores as our class labels, which are divided into five values. We make use of 2/3 of total records as the training data and the rest as the test data. The experimental study shows that our model could judge merchants X  credibility in a higher accu-racy, which is up to 76.49% increased by 32.17% compared with that of VFDTc in the case with  X  X ax X . Meanwhile, when classifying with  X  X ayes X , its accuracy is improved by 47.90% in comparison to VFDTc with the error rate of 54.08%, which is described in Figure 6. Besides, Table 6 lists the estimation results on the time and space costs, which demonstrate that the total space or time cost to deal with these data is much less for SRDTE; especially in the method of Na  X   X ve Bayes, the maximum time consumption of one single tree is less than one minute. Fan et al. proposed Random Decision Trees (RDTs) [2] in 2003. The model of RDTs selects the splitting feature randomly without any heuristics and has good performances on space, runtime and the r obustness (also seen in [14]), which is more suitable for medium and large databas es with discrete attr ibutes. However, it requires discretization on numerical attributes beforehand so it does not adapt to process data streams in real time directly. Hu et al X  X  work[10] in 2007 was on the classification of data streams based on RDTs, which adopts a new method to deal with numerical attributes of data streams in the inequality of Hoeffding Bound and introduces the Na  X   X ve Bayes method for better predictive accuracy. However, some important factors of the impact on the model abilities, such as the initial height of a tree, the count of trees and several parameters used in the Hoeffding Bound inequality, are not analyzed systematically and further mechanisms still need to be developed. Besides, this algorithm does not take into account of real-time streaming data.
 In this paper, we have proposed a model of Semi-Random Decision Tree Ensem-bling (SRDTE) for classification on streaming data. It generates each semi-random decision tree incrementally after seeing the training instances, and takes a new data structure for a quick search of target objects. Meanwhile, it adopts the voting mechanism of majority class, which has improved the predictive accuracy largely. Besides, the optimal values of significant parameters, which have an impact on the model X  X  abilities, are estimated in an extensive study. Based on these obtained optimal values, we have evaluated SRD TE on its accuracy, run time, space cost and the robustness. Our experimental results show that SRDTE has better per-formance compared to VFDTc. Also, a n experiment to apply SRDTE to some Yahoo! shopping data demonstrates that SRDTE is efficient and effective on real data streams. Our future work is to obtain massive data via the Yahoo! Shopping Web Services Interface to find potential patterns of transactions continuously with consideration of concept drift in real shopping data.

