 1. Introduction
Collaborative filtering aims at predicting a test user X  X  ratings for new items based on a collection of other like-minded users X  ratings information. It assumes that users sharing the same ratings on past items tend to agree on new items. Up to now, researches on collaborative filtering can mainly be categorized into two types: memory-based and model-based . Memory-based algorithms, including user-based ( Breese, Heckerman, &amp; Kadie, 1998; Herlocker, Konstan, Borchers, &amp; be identified. Then the unknown rating is predicted by combining the known rating of the neighbors. However, there exist two fundamental challenges for memory-based methods. The first one is sparsity . These algorithms rely upon exact matches of two user/item vectors, which cause the algorithms to sacrifice recommender system X  X  coverage and accuracy. More con-cretely, since the correlation coefficient is only defined between customers who have rated at least one product in common, or the items which have been co-purchased, then many pairs of customers/items will have no correlation at all. As a result, memory-based recommender systems cannot accurately compute the neighborhood and identify the items to recommend, items can be quite large (often millions of users and thousands of items). This may slow down the recommendation procedure significantly since K nearest neighbor based algorithms will require too much computations in this case (Sarwar et al., 2001 ).

Different from memory-based approaches, model-based approaches need to establish a model using training examples that can predict the unknown ratings of a test user. Examples within this category include decision tree (Breese et al., usually many free parameters to tune.

In this paper, we provide a novel framework for collaborative filtering, which circumvents the problems of traditional memory-based and model-based approaches by applying orthogonal nonnegative matrix tri-factorization ( ONMTF )(Ding, Li, ings via a linear combination. Our algorithm possesses the following superiorities: (1) The sparsity problem has been circumvented due to the application of matrix factorization. (2) The scalability problem can be greatly alleviated by the application of ONMTF technique, since the users and items are (3) Our method can fuse the prediction results of different types of algorithms (user-based approach and item-based The rest of this paper is organized as follows. We first give a brief summary of related work on collaborative filtering in results of experiments are presented in Section 5, followed by our conclusions in Section 6. 2. Related work
As introduced in Section 1, there are mainly two classes of collaborative filtering algorithms: memory-based and model-based. The memory-based approaches have been widely used in practical systems (Linden, Smith, &amp; York, 2003 ), including
Karypis, 2004; Sarwar et al., 2001 ). The key to memory-based approaches is to compute the similarities between pairwise users or items. Many methods have been developed for this task, such as correlation-based, cosine-based and adjusted-co-sine ( Sarwar et al., 2001 ). Compared to model-based approaches, the memory-based approaches do not need to train a model and do not have too many parameters to be tuned. Recently, Wang, de Vries, and Reinders (2006) suggested a unifying user-based and item-based collaborative filtering approach by similarity fusion which improves the performances on sparse data.
However, their method is based on the traditional memory-based algorithms which have to search for the whole data set to identify the neighbors of a test user or item. Therefore, it still suffers from the scalability problem. compact model and circumvent the scalability problem to some extent. The dimensionality reduction methods can alleviate the data sparsity problem. However, just as pointed out by Huang, Chen, andZeng (2004) and Xue et al. (2005) , some poten-tially useful information may be lost during the dimensionality reduction process. Besides, one disadvantage of the model-based approaches is that there are often many parameters to be tuned such as the number of clusters and the final dimen-sionality of the space in which the data is embedded.

Recently, researchers have proposed some hybrid approaches in order to combine the strengths of memory-based ap-proaches and model-based approaches (Pennock, Horvitz, Lawrence, &amp; Giles, 2000; Xue et al., 2005). For example, Xue et ever, they only employ the user-based approaches. Our framework in this paper extends the ideas in Wang etal. (2006) and
Xue et al. (2005) and naturally integrates clustering techniques (model-based), user-based approaches and item-based ap-proaches (memory-based) by orthogonal nonnegative matrix tri-factorization.
 kind of methods mine the content information contained in the items which can be incorporated into traditional collabora-tive filtering techniques, so they can overcome some shortages of using collaborative filtering methods alone. However, as pointed out by Xue et al. (2005) , the content information may be different or expensive to acquire in many cases.
The other technique that is closely related to this paper is the co-clustering approach, which solves the problem of simul-taneously clustering rows and columns of a data matrix. Dhillon (2001) and Zha et al. (2001) individually developed a co-clustering approach by using the bipartite graph formulation and spectral methods, but that demanded each row cluster to be associated with a column cluster. In the past years, co-clustering based on information theory has attracted more and more attention. Tishby, Pereira, and Bialek (1999) first introduced an information bottleneck framework for one-sided clus-tering. Later, Slonimand Tishby (2000) gave an agglomerative hard clustering version of the information bottleneck method, which could cluster documents after clustering words. El-Yaniv and Souroujon (2001) further extended the framework and mation-theoretic co-clustering technique by minimizing the loss of mutual information, and Banerjee, Dhillon, Ghosh, Mer-ugu, and Modha (2004) extended the information-theoretic co-clustering and suggested a generalized maximum entropy co-clustering approach by appealing to the minimum of Bregman information principle. Recently, Ding et al. (2006) gave a novel co-clustering approach based on nonnegative matrix factorization, and Long, Zhang, and Yu (2005) also provided a similar co-clustering method called co-clustering by block value decomposition. George and Merugu (2005) considered model-based approach. In this paper, we use ONMTF rather than the method in George and Merugu (2005) to co-cluster, to the information-theoretic co-clustering methods in Dhillon et al. (2003) and El-Yaniv and Souroujon (2001) by empirical studies. 3. Orthogonal nonnegative matrix tri-factorization
The nonnegative matrix factorization ( NMF ) is first brought into machine learning and data mining fields by Lee andSeung (1999, 2001), and has been widely used in pattern recognition, multimedia, text mining and bioinformatics ( Ding et al., 2006). Recently, Ding et al. (2005, 2006) proved the equivalence between NMF and K -means/spectral clustering, and ex-tended NMF to ONMTF which could simultaneously cluster rows and columns of an input matrix.

Now let X  X  briefly review the basic idea of NMF . Given a nonnegative matrix X , NMF aims to find two nonnegative matrix factors U and V such that rank matrix factorization.

Furthermore, if the orthogonality of the matrix factors is required, we can obtain the following orthogonal NMF . where kk is Frobenius norm, i.e. k A k X  column of X , and each column of U is a cluster centroid).

In a recent paper Ding et al. (2006) derived the following orthogonal nonnegative matrix tri-factorization ( ONMTF ): cates which cluster every column of X belongs to when clustering X  X  X  columns.
 The optimization problem (3) can be solved using the following update rules (Ding et al., 2006 ) decreasing, i.e. the update rule (6) ensures that k X USV T k 2 can converge to a local minimum.

In the next section, we will show how to apply the ONMTF technique to collaborative filtering. 4. Our framework
X : x Let where the vector u j indicates the j th user X  X  ratings to all items.
 Likewise, the user-item matrix X can be decomposed into column vectors based approaches: the former corresponds to user-based and the later corresponds to item-based. 4.1. Memory-based approaches
In our method, we choose the cosine similarity as the user similarity measure. Mathematically, the cosine similarity be-tween the j 1 th user and the j 2 th user is the cosine of the angle between these two user vectors. Formally,
Subsequently, the test user X  rating for an unknown item can be given by where u j is the average rating of the j th user, and u h 2f the most similar K -users of u j g .

In addition, we select the adjusted-cosine similarity as the item similarity measure. Different from cosine-based similar-item and the m 2 th item is defined by item is calculated by where i h 2f the most similar K -items of i m g .
 4.2. Clustering techniques for collaborative filtering
In our framework, the user-item matrix X is first approximated by using orthogonal nonnegative matrix tri-factorization, corresponding item vector and each column vector of US denotes a item-cluster centroid.

Now we will introduce how our method identifies K nearest neighbors of test users or test items. Traditional methods usually need to search for the whole database, which will definitely suffers from the scalability problem as the growth of selection as an example (the item X  X  neighbor selection is similar): (2) Choose K neighbors of the test user in the candidate set by using ordinary user-based methods. Apparently, in the previous two steps, pre-selection directly determines the search space and the subsequent prediction.
After the K most similar neighbors of a test user or a test item having been decided, we can apply Eqs. (8) and (10) to obtain two individual predictions for an unknown rating. 4.3. Prediction fusion
Purely using user-based or item-based algorithms for collaborative filtering often gives poor predictions, especially when improve the performances by fusing the results of user-based and item-based predictions.

In addition, since USV T already makes an estimation for all unknowing ratings in the user-item matrix X , we should also consider the prediction of ONMTF itself.

By linearly combining the previous three different types of predictions, we can obtain the final prediction result as coefficients.

Therefore, our prediction model can be viewed as the weighted sum of three types of predictors, in which k and d control probabilistic fusion framework by using the independence assumption on different types of ratings and Bayes rule. Obvi-ously, the principled probabilistic interpretation can be easily endowed with our framework.

Finally, we summarize our algorithm (called CFONMTF )in Table 1 . 5. Experiments 5.1. Dataset
We used the MovieLens 1 data set to evaluate our algorithm. The MovieLens data set is composed of 943 users and 1682 items (1 X 5 scales), where each user has more than 20 ratings. For conveniently comparing with collaborative filtering algorithms listed in Wang et al. (2006) and Xue et al. (2005) , we also extracted a subset which contained 500 users with more than 40 observed ratings are used to predict the held out ratings.
 Additionally, we randomly selected 5, 10 and 20 items rated by test users in the observed set, which were called Given5 , predict the held out ratings.

It should be noted that before factorizing the user-item matrix X , we tried three different treatments to unknown ratings ( x zero leads to the worst prediction performance, and the latter two approaches provide similar results of prediction. In our experiments, we replace the unknown ratings with the average rating of the corresponding user. 5.2. Evaluation metric
There are several types of measures for evaluating the performance of collaborative filtering methods ( Sarwar et al., 2001). For consistency with the experiments in Wang et al. (2006) and Xue et al. (2005) , we choose the mean absolute error ( MAE ) as evaluation metric. The MAE is calculated by averaging the absolute deviation between predicted values and true values. Formally, where N is the number of tested ratings. The lower the MAE , the better the performance. 5.3. Parameters tuning 5.3.1. Number of clusters
In the ONMTF X  X  USV T , the column number of U : k is the number of user clusters and the column number of V : l is the fusion method.

Fig. 2 shows MAE of CFONMTF when k or l varies. We use ML _ 300 dataset for training, and try 10 different values of k or l (2, 5, 10, 20, 30, 40, 50, 60, 70, 80). The best value of k or l is about 20 for ML _ 300 .

As shown in Fig. 2 , the number of clusters does affect the performance of our method. When the number of clusters be-information so general that some similar neighbors can be missed during the pre-selection. 5.3.2. Size of neighbors
Traditional memory-based methods need to calculate the similarities between the test user/item and all the training the K nearest neighbors are found by searching for the candidate set. The size of candidates and the number K of neighbors may affect the prediction accuracy.

The first experiment on ML _ 300 shows how the percentage of pre-selected neighbors affects the performance of our method. Here we select the percentage of pre-selected neighbors rather than the number of candidate clusters C because stable when the percentage of pre-selected neighbors reaches around 30%, which guarantees that the pre-selection can find the major similar neighbors of a test user or item.

The second experiment on ML _ 300 shows the performance as a function of the size of neighbors. Here, we suppose that the size of user neighbors is equal to the size of item neighbors. Fig. 4 shows MAE of CFONMTF when the size of neighbors K varies. We observe that the optimal accuracy can be achieved when K varies between 20 and 40, so we choose 20 as the size of neighbors. 5.3.3. Combination coefficients
As shown in Eq. (11), k and d reflect the weights of three different models: ONMTF , user-based and item-based. We con-ducted two experiments on ML _ 300 to identify the optimal combination coefficients.
 the predictions between user-based and item-based. We observe that the optimal value of d is approximately between 0.5 for item-based methods ( d = 0), so the optimal value of d emphasizes the user-based methods.

Second, We fix d to be 0.6 and continue to test the property of k . From Fig. 6 , we observe that the optimal value of k is approximately between 0.2 and 0.4. The optimal value of k is a trade-off between ONMTF and memory-based approaches. On the whole, our fusion framework integrates the complementary information from three different methods: ONMTF , user-based and item-based. Generally speaking, the complementary information can improve the prediction accuracy. Our experiments demonstrate that the fusion method is indeed superior to any individual approach. 5.4. Scalability
Relative to traditional memory-based methods, our fusion algorithm adds a process: simultaneously clustering rows and columns of a user-item matrix (i.e. ONMTF ), but the neighbor search space can be reduced. Consider a simple fusion scheme that just linearly combines user-based and item-based methods (Wang et al., 2006 ). Formally, The scheme is called SF1 in Wang et al. (2006) . We conducted an experiment on ML _ 300 that compared the execution time of CFONMTF and SF1 under the same computational environment.
As shown in Fig. 8 , the execution time for CFONMTF on a PC with a 2.0 GHz Intel PIV CPU and a 1 GB Memory is far shorter steps ( Fig. 7 shows the convergence speed of ONMTF on ML _ 300 ), and its cost of execution time is much less than that of searching for the left 70% of the whole data set. Therefore, the execution time is greatly reduced. 5.5. Sparsity Data sparsity has an important effect on the performance of collaborative filtering approaches. We did an experiment on ML _ 300 that showed our algorithm X  X  tolerance to sparsity.

Fig. 9 shows the performances of SF1 and CFONMTF when the sparsity of the data set ML _ 300 varies. We selected the 10%, 20%, 40%, 60%, 80%, 100% of the known ratings from the whole data set at random respectively. As shown in Fig. 9 , CFONMTF mance of collaborative filtering algorithms based on memory-based alone. In our scheme, ONMTF itself can be considered as a background model which provides the complementary information and improve the prediction accuracy when only sparse data is available. From the experiment we conclude that our fusion framework is quite suitable for sparse data. 5.6. Performance comparison with other methods
Finally, we compared our fusion scheme CFONMTF with state-of-the-art collaborative filtering algorithms, i.e. similarity methods in Given5 , and is just a little worse than SF2 in Given10 and Given20 (only in few cases, also a little worse than
SCBPCC and MMMF ). But as discussed in Section 2, SF2 suffers from the scalability problem while our method successfully tween computation efficiency and prediction accuracy.

In summary, our algorithm CFONMTF includes two phases: first, it does ONMTF to the user-item matrix (in this phase, the known ratings. Clearly, the main computational burden of CFONMTF lies in matrix factorization and neighbor search. ONMTF can be done offline and the user-item matrix can be updated periodically (just like Google updates its webpage pool). More-a user comes, we just need to draw its neighbors from the memory and predict its unknown ratings online, which takes very this problem. 6. Conclusions
In this paper, we represented a novel fusion framework for collaborative filtering. The model-based approaches (i.e. clus-tering techniques here) and the memory-based approaches (i.e. user-based and item-based here) are naturally assembled via ONMTF . Our method greatly mitigates the two fundamental problems: sparsity and scalability by using the proposed hybrid ing and resolves the sparsity and scalability problems. In the future, we will investigate new co-clustering techniques and develop better fusion models.
 Acknowledgement This research was supported in part by the National Natural Science Foundation of China under the Grant No. 60835002. References
