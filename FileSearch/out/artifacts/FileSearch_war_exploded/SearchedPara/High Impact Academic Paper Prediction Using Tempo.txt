 Predicting promising academic papers is useful for a variety of parties, including researchers, universities, scientific coun-cils, and policymakers. Researchers may benefit from such data to narrow down their reading list and focus on what will be important, and policymakers may use predictions to in-fer rising fields for a more strategic distribution of resources. This paper proposes a novel technique to predict a paper X  X  future impact ( i.e., number of citations) by using temporal and topological features derived from citation networks. We use a behavioral modeling approach in which the temporal change in the number of citations a paper gets is clustered, and new papers are evaluated accordingly. Then, within each cluster, we model the impact prediction as a regression problem where the objective is to predict the number of ci-tations that a paper will get in the near or far future, given the early citation performance of the paper. The results of empirical evaluations on data from several well-known cita-tion databases show that the proposed framework performs significantly better than the state of the art approaches. H.3.4 [ Systems and Software ]: Information Networks; H.3.7 [ Digital Libraries ]: Dissemination; I.2.6. [ Learning ]; I.5.3 [ Clustering ] Algorithms; Design; Experimentation; Verification.
 Citation count prediction; clustering; time series; regression; network analysis.  X 
This researcher has been supported by TUBITAK grant number 2215.

In today X  X  academia, publish or perish policy results in an enormous body of publications. Hence, given the limited time, researchers have to be selective while putting a paper into their reading list, as well as prioritizing the articles in that list. Ideally, many would be more interested in reading papers that are likely to have high impact in their fields so that they can get ahead of their peers in contributing to an emerging field, and possibly become a leading figure in that area. However, it is almost impossible to decide whether a paper would really make a high impact ahead of time be-fore reading it (even after reading it, it would be challeng-ing to mark such papers). In order to tackle with reading list building and prioritization challenge, researchers prac-tice some implicit rough filters, such as following top publi-cation venues and prominent researchers constantly in their field. The implicit assumption is that high impact papers would be published in top venues and/or by prominent re-searchers. This assumption holds true in many cases, but even with such rough filters, the number of candidate pa-pers to read may still be in the order of hundreds (if not in thousands) per year.

In addition, for policy makers and funding agencies, it may be essential to determine which papers will gain more attention. This is because such information may help them foresee which fields are more likely to be important in the future so that they can allocate resources more strategically. A direct approach for determining the future impact of a pa-per or field is to use expert knowledge. However, this is time consuming and human effort is not scalable to keep up with the current rate of academic production. Also, this method involves personal opinions of experts which may be subjec-tive and prone to differ significantly. In addition, expert opinions are shown to be fallible for numerous times, as it is difficult to estimate the future impact of a paper just based on its content. One example of this is on the use of Neural Networks [16] for machine learning problems. In early 80s, Neural Networks were very popular in the field of machine learning, until they were replaced by linear classifiers, such as Support Vector Machines [23]. Until the popularity of the Neural Networks diminished, it may be highly likely that an expert would underrate the development in linear classifiers. Thus, the evaluation would be biased.

Hence, there is a need for automated tools that can accu-rately predict high impact papers (and, indirectly, the corre-sponding future hot research fields) shortly after their pub-lication. In this paper, we propose an impact prediction framework for academic papers in which we model the be-havior of the papers in terms of their citation performance during their initial lifetime. Given the citation behavior of a paper in the first few years after it gets published, our model assigns the paper to a cluster, hence captures its cita-tion behavior. Our model then uses the paper X  X  topological properties (such as various centrality measures within the citation network) to improve the prediction accuracy. The main novelty of our approach is the use of behavioral models that are defined by clustering the citation time series of the papers to analyze their citation patterns.

Measuring the impact of a publication is usually done in an appeal to popularity fashion, in which the number of cita-tions a paper gets is used to quantify the attention that the paper gets; hence, the impact of the paper in the academic world [21]. In spite of the studies that point out the problems of the citation-based analysis of impact [13], the number of citations has been widely used as a measure of impact of publications or scientists [7, 9]. Hence, the impact of a pa-per in our approach is also based on the number of citations that a paper already got or will get. Previous research [2] shows that the number of citations in the initial years after the publication of a paper is a good indicator of that paper X  X  citation performance in the long term. Hence, citation per-formance of a paper during its early life-time is a promising feature, and has been employed previously [14,17]. However, none of the previous research has focused on a paper X  X  cita-tion behavior which captures the patterns in citation count changes of a paper. More specifically, the citation behavior of a paper tells about whether the number of citations in-creases steadily, or it saturates after some time, or whether the paper seems to get no citations at all at the beginning, and its citation count explodes later, and so on. Figure 1 shows three different types of citation patterns, where x axis represents the time and y axis represents the cumulative number of citations. In Figure 1a, the number of citations forms a straight line, since the number of citations over time does not increase, and the paper ends up having zero cita-tions at the end of the analysis period. Figures 1b and 1c show a more steady increase, followed by a saturation pe-riod. The difference between Figures 1b and 1c is in terms of the scale of the graph, i.e., the number of citations that the papers get at the end of the analysis period: the paper in Figure 1b ends up having around 85 citations, while the paper in Figure 1c gets around 230 citations. These three patterns indicate two important parameters about the cita-tion behavior of a paper: (i) the scale, which corresponds to the number of citations, and (ii) the temporal change with respect to the number of citations. While considering the current citation behavior of a paper, one may take into account these two parameters.

It is relatively straightforward to employ the number of ci-tations as a predictor of future importance, when the prob-lem is considered as a regression task. However, it is not as intuitive to incorporate the citation behavior into such a regression-based prediction scheme. For this problem, we propose to cluster papers based on their citation behaviors ( i.e., change of the number of citations over time), and as-sign a polynomial to each cluster for regression. Given a new paper, we assign the paper to a cluster by using its citation behavior in the initial stages after its publication, and per-form citation prediction based on the polynomial associated with the paper X  X  cluster.

In addition to the temporal citation patterns, topological properties of papers in the citation network are shown to be helpful in predicting future importance of a paper [17, 21]. Hence, we also use topological properties of a paper, such as betweenness centrality, closeness centrality, PageRank, and eigenvector centrality in the citation network for improving prediction performance of our model.

Experiments in two well-known datasets reveal that our model outperforms the state of the art by a significant mar-gin. Also, we show that using behavioral models, rather than considering all papers to have the same citation behavior improves the prediction performance significantly. We also provide several sensitivity analyses on the parameters we use in our model.

Contributions. Our contributions in this paper are as follows:
Organization. The rest of the paper is organized as fol-lows: In Section 2, we review the related work, and present the features and models that have been used for predicting a paper X  X  importance. Then, in Section 3, we present the proposed scheme. We also present a complexity analysis in this section. In Section 4, various experiments that we per-formed will be presented to validate the proposed scheme. Several parameter sweeps will also be presented. Then, in Section 5, we conclude by summing up the overall findings and giving insights about future work.
Impact prediction for academic papers has been an active research area. Most of the literature uses features that are extracted from graph structure [21, 24]. [2] states the im-portance of the citations in the initial years after the publi-cation of a paper, hence it is commonly used [5, 14, 17]. [3] models citation behaviors of researchers in computer science, and defines two publication popularity phases, namely, the population growth phase and the population decay phase. However, none of the papers mentioned here uses clustering in terms of citation behavior to have a better prediction. In our model, we exploit the differences between the cita-tion behavior of papers, and cluster similar papers together for training regression models for each cluster, rather than training a single regression model for all papers.

In addition to number of citations, measures of network centrality, such as clustering coefficient, average shortest path length, and betweenness centrality are used [21]. The idea in [21] is that there is a pattern of topological features between the papers that get high number of citations. For testing their hypothesis, they analyzed the correlation be-tween the number of citations in the future and the topolog-ical measures stated above. Similar to these work, our model uses topological features. However, the way our model uses topological features is to use it as a means of improvement, rather than using it as the main component used for predic-tion. Various topological features are also studied in [20].
In addition to temporal and topological features described above, a variety of contextual features are used. A reason-able indicator of a paper X  X  impact is the previous works of the paper X  X  authors. People in academia tend to cite papers that are published by celebrities , i.e., people who are well-known in these fields. Hence, an author with high number of citations is more likely to get citations in his/her follow-ing papers. This phenomenon was taken into account in [5]. Some other works consider the content of the paper. [24] uses contextual features, in addition to a large pool of other features, to predict for citation number prediction in Arnet-Miner dataset. [25] uses metadata information of papers, and they found out that text features significantly improve the prediction performance, compared to the baseline methods.
Models that are employed for the impact prediction prob-lem are also worth mentioning. One approach is to consider citations in an information diffusion context, in which cita-tions are spread like a disease. Preferential attachment [4] is another model that is used to study the problem, espe-cially in the context of link prediction [12]. The concept of preferential attachment suggests that new nodes favor con-nections to existing nodes that are highly connected. There exist other models proposed for multidisciplinary networks that are based on structural holes, but these are outside the scope of our study.

The next section is devoted to the explanation of our pro-posed method.
In this section, we present the temporal and topological features that are used to analyze the paper X  X  current situ-ation, and how these features are employed to predict the future characteristic of the paper. Then we present the re-gression model that performs the prediction, based on the extracted features.
Given paper X  X  time series of citation counts and topolog-ical features; predict citation count in future. Formally we can define this problem as below.

Let G be directed citation graph of papers. Where nodes are papers and edges p  X  q means p cites q Let p be the paper that we want to predict citation count.
Let C p t be citation count time series of a paper p where t=0,1,2,3,...
 Let F p be the topological feature set of a paper p where
F p = { f p 1 ,f p 2 ,f p 3 ,f p 4 ,... } . An example of features might be f p 1 = PageRank ( p,G ), f p 2 = Closeness ( p,G ) and f Eigenvector ( p,G ).

Predict citation count c t  X  given ( C p t ,F p ,t  X  ) where t time in future.
We consider the citation pattern of each paper as a time series, and investigate the relationship among different time series. We do this by defining paper types in the context of incoming citation performance. We first create snapshots of the network with a constant interval determined by consid-ering the properties of the dataset, and in each snapshot, we compute the number of citations for each paper for six years after the publication of paper for training purposes. Then, we construct a distance matrix D between all the papers that are used in the training, which is defined as follows: where X i and X j are citation time series for paper i and paper j , and k is the snapshot id starting from the publi-cation of the paper. Here, the sizes of both time series are assumed to be the same, hence Euclidean distance [10] alone is used for computing the overall distance. However, it may not be the case in real life scenarios: for each paper, there exist a time interval where it has no citations, since the pa-pers that cite a paper will go through a peer-review process which usually takes months, if not years. Hence, one might want to consider the citation behavior only after the first citation (if exists) by considering the related timespan. To this end, one may extend the scheme above by employing Dynamic-Time Warping (DTW) [19]. DTW can be used to compare time series that vary in time. Hence, using DTW makes it possible to compare papers even their citation be-havior has differences caused by different peer-review lengths among the papers that cite each paper.

After D matrix is calculated, we apply spectral cluster-ing [18] to determine citation models. Spectral clustering involves the application of eigen decomposition over the dis-tance (or similarity) matrix. The resulting eigenvectors are considered as cluster labels. We expect the citation behav-iors to act as an indicator of citations in the future, given its citation performance in the first few( usually two or three in practice) years after the publication.

One important aspect that needs to be determined is the number of clusters, each of which correspond to a differ-ent behavioral model. If less number of clusters is used, the model results in underfitting, coming short in correctly modeling the citation behavior. On the other hand, having too many clusters is likely to overfit, which is another source of error. There are several options available for determining the number of clusters in given data. A rule of thumb is to use p n 2 as the number of clusters, where n is the num-ber of data points [15]. There also exist different types of predictions that are based on the rate of change in cost func-tion ( i.e., the elbow method), or information criterion. The idea in our model is to choose the number of clusters that minimizes the prediction error in our training dataset.
Figure 2 shows two of the clusters when the number of clusters are equal to 5. In each subfigure, the upper plot corresponds to individual citation behaviors, while the mid-dle plot corresponds to the average of the cluster, and the bottom plot is the predicted polynomial for that cluster.
Note that the normalization plays an important role in the preprocessing stage prior to clustering, since our goal in the clustering is to capture the behavior, rather than to record the number of citations. We perform normalization by dividing the number of citations in each instant by the number of citations after time used for clustering; hence, the last element of a normalized sequence always becomes 1. The numbers of citations at the end of the timespan used for clustering are kept separate from the behavior, and used during the prediction stage.

When a new paper arrives, the paper is assigned to the cluster that minimizes the distance between its mean ci-tation behavior of the cluster and the paper X  X  normalized citation behavior, that is:
In addition to the clustering scheme that is described in the previous section, it is possible to use topological fea-tures as improvement terms. In this context, improvement is used to introducing information obtained from the topo-logical properties of a paper with the intend of improving prediction accuracy. We either increase or decrease the pre-dicted citations by a sum of topological features.
 The way that we employ topological features is as follows: We first calculate topological features, such as betweenness centrality, closeness centrality, PageRank, and eigenvector centrality, and create a f i vector out of these features. Af-ter f i vector is calculated for each paper, we calculate the following score for each feature: where  X  j is the mean and  X  j is the standard deviation of training samples for j th feature, and  X  i j is the multiplier of i th test sample for j the feature. Normalizing the fea-tures with respect to the distribution of these features along the training samples makes it possible for us to approach the problem in an anomaly detection sense. The anoma-lous samples, i.e., the samples that deviate more from the mean, are considered as advantageous features in terms of topology. The sum of these values are then multiplied with a carefully tuned  X  parameter to improve citation predic-tions. The tuning is performed based on the change in the prediction error in the training data(see Figure 3). feature name formula
Betweenness centrality P i 6 = j 6 = k totalShortestPath jk
Closeness centrality closeness ( i ) = P i 6 = j 1 PageRank PR ( i ) = 1  X  d
Eigenvector centrality eigen ( i ) = 1
As discussed in the above sections, the evaluation of a paper is twofold: (i)a paper is first assigned to a cluster, and then (ii)the topological features are computed for the paper. The way we combine these two is as follows: Given a paper for which the prediction is to be performed, we first compare the citation behavior of the paper for a short timespan ( e.g., 2-3 years) with mean citation behavior of all clusters. As in Equation 2, the cluster that minimizes the distance is chosen as the cluster of the paper. Then, for the rest of the prediction timeline, the chosen cluster X  X  polynomial is used for prediction ( polynomial regression ). Since the polynomial for each cluster is determined based on normalized citations, it only captures the behavior, and it should be amplified with the number of citations the paper has after the first window. Then, we introduce the effect of topological features, which we use as improvement terms. We use a carefully-tuned  X  parameter to adjust the effect of  X  values on prediction. The resulting prediction, y 0 , is calculated as follows: The plots are based on citation data from Arxiv HEP-TH dataset where y i is the initial prediction without topological fea-tures. The  X  value may be tailored for each feature, or may be global, which results in equal weighing of topological fea-tures. We used it as a global since all features are equally important.
If the algebraic solution is used for spectral clustering, it requires computing the eigenvectors, which is O ( n 3 ), where n is the number of papers to be clustered, which is imprac-tical when the number of samples grow. Hence, we use a greedy algorithm that approximates spectral clustering [6]. The algorithm we use is based on sparse similarity values, and the complexity of constructing the similarity matrix is O ( n 2 ), where n is the number of training data points ( i.e., papers). The complexity for spectral clustering is where m is the Arnoldi length in using the eigensolver, n is the number of data points, t is the number of desired nearest-neighbors in the algorithm ( t &lt;&lt; n ), and k is the number of desired clusters [6]. m is often set to be several times larger than k [6]. The importance of training complexity further diminishes due to the fact that clustering is done once, and the constructed model is used for many times.

For testing, assigning a paper to a cluster is O ( kw ), where w is the window size used to compare papers. Using the MATLAB implementation of [6] that runs on commercially-available hardware, training of 2 , 000 samples can be per-formed in around 1 second. Similarly, testing can also be performed in real-time.

The next section is devoted to experiments that we per-formed to evaluate the performance of our proposed frame-work.
In this section, we first present a comparison of our method to the state of the art in various datasets. We then elaborate on our method by presenting sensitity analyses. Throughout the experiments, we used root-mean squared error (RMSE), the coefficient of determination ( R 2 ), and correlation ( r ) as error metrics, depending on the work that we compare our results with. The reported results are averaged over 10 runs( i.e. , k-fold cross validation with k=10 ).
Throughout the experiments, we used three datasets. The first dataset is Arxiv HEP-TH (high energy physics the-ory) dataset [8, 11], which contains citation information for 27 , 700 papers that were published between 1992 and 2003, with all papers belonging to high energy physics theory. The second dataset we use is ArnetMiner dataset [22], which con-tains 1,511,035 papers and 2,084,019 citation relationships. Lastly, we used CiteSeerX [1] dataset which consists of over 2 million papers and 40 million citations. ArnetMiner and CiteSeerX datasets are used for comparison with the state of the art, while Arxiv is used to perform a detailed evaluation of our method, since it has less amount of missing values.
We compared our method to the two most recent works as representative studies [21] [5] of the state of the art from the literature. The first comparison was performed against [24] on ArnetMiner dataset [22]. In the experiments section of [24], the results were reported on various regression methods such as linear regression, regression trees, and SVR for t = 1, t = 5, and t = 10 years, where t is the time for which the number of citations is predicted. For the sake of brevity, we used the best result for each t . We used the citation information for first three years after the publication.
The error metric used by [24] is the coefficient of determi-nation ( R 2 ), which is defined as follows: where y is the mean of the observed data (actual number citations at time t ), y i is each observed value and f i each predicted value.

Table 2 indicates that our method outperforms the method used in [24], for t = 1, t = 5, and t = 10. The proportional difference between two methods is more apparent with t = 1 and t = 10, compared to t = 5. In order to show the positive effect of clustering, the results for regression where clustering is not performed are included as baseline. For the clustered case, the number of clusters is 7 and polynomial degree is 3. Window size is determined to be 2 years.
 1 year 0.683 0.784 -9.9 5 years 0.752 0.800 -1.028 10 years 0.786 0.842 0.66 Table 2: R 2 comparison between our method and [24]. The second set of comparison were performed on Cite-SeerX dataset against the results of [5]. They used the cor-relation coefficient r between the predicted and observed values as the error metric, where r is defined as follows: where x is the predicted citations and y is the actual cita-tions.

The comparison of results is presented in Table 3. Note that their method used the month data, in addition to the year data(which is not available in CiteSeer dataset) and predicted the number of citations for 4 . 5 years. Since month data is not available to our method for CiteSeer dataset, we provide comparison of results up to 4 years. Similar to Ar-netMiner dataset, the number of clusters is 7 and polynomial degree is 3.
 Figure 5: RMSE error vs window size used for classification
Table 3 indicates that our method outperforms [5] when the used window is up-to two years, while both methods provide similar performance when the length of the used window converges to the predicted window.
 Table 3: Correlation coefficient ( r ) comparison between our method and [5].

In the next subsections, we present the sensitivity analy-ses. The following results were obtained from experiments performed over Arxiv HEP-TH dataset.
The first study was performed to see the effect of clus-tering (if any), and how the number of clusters affects the accuracy of predictions. We first evaluated a non-clustered prediction, which we considered as baseline, and then in-creased the number of clusters gradually. The results of this study can be seen in Figure 4. Note that introducing cluster-ing significantly decreases the prediction error, which shows the effectiveness of clustering. The baseline predictor, which does not use clustering, results in an RMSE error of 28 . 889, while the predictor with 6 clusters results in 12 . 365. How-ever, it can be seen that increasing the number of clusters results in overfitting after some point, which increases the prediction error.
The second experiment was performed on the window size that we use to predict a paper X  X  citation behavior. The win-dow size is of great importance since using a very large win-dow size would make the scheme impractical, requiring the citation records over long years, in which case, the citations of a paper is already stabilized. On the other hand, using a very small window size would be noise-prone, since it takes variable amount of time for papers to get their first citations. Figure 5 shows the average RMSE error for varying window sizes.

Looking at Figure 5, it can be seen that the error con-stantly decreases until a window size of 42 months, after which increasing the window size does not make any sig-nificant contribution. Taking the necessities of today X  X  aca-demic world into consideration, we concluded that a window size of 24 months is suitable(which is what we use in our ex-periments), since having a long window size will decrease the usefulness of the scheme, especially in fields like computer science.
Another goal of our study is to identify a point for which the number of citations stabilize. This makes it possible to determine prediction interval which limits the range of prediction. In this experiment, we trained our model by using the citation records of 10 years. The prediction was also performed over a 10 years interval. Figures 6a and 6b show two of the three models that are observed in long term. The model that is not shown in the above figure corresponds to the papers that got no citations in the whole timespan. The first model, which is shown in Figure 6a, corresponds to papers that stabilize after some time. These papers possibly lost their connection with the state-of-the-art, hence they are less cited or no more cited at all. Obviously, this needs to be verified based on data, which is part of our future work. On the other hand, model in Figure 6b corresponds to the papers that sustain a steady growth in terms of the number of citations. These papers are more likely to be seminal papers, i.e., papers that preserve their popularity even after years. The plots presented above show that these two types can be easily discriminated by using the data of 10 years, which allows for the possibility of long-term citation prediction. However, our analysis on this topic is limited due to constraints introduced by the dataset, which covers citation data from only 11 years, hence making a long-term analysis impossible. Using different datasets that cover a longer timespan for further analysis is part of our future work.
An other experiment was performed to see how prediction error changes over time. Figure 7 shows the prediction error over time.

Looking at Figure 7, it can be seen that the prediction error increases as the predicted interval becomes larger up until 40 months. After that point, the prediction error de-clines. This is possibly caused by degree of the regression. A quick solution to decrease error might be to increase the order of polynomial. However, we noticed that increasing the degree of polynomial results in higher prediction error, since a polynomial of higher degree is more likely to overfit.
We proposed an academic impact prediction framework based on the first years X  citations and topological position of a paper. Our model uses time series approach to pre-dict the number of citations, which is successfully employed before. Additionally, our model makes use of the citation behavior, i.e., the pattern in the increase of the number of citations. In the training phase, the papers are clustered according to citation behaviors. Then, when a new paper arrives, it is assigned to a cluster and the prediction is per-formed accordingly. We also employ topological features of the paper, such as various centrality measures, to increase the prediction performance. Using topological features were also used before, however, we use them as a contributor to the prediction task, rather than using them as the main fea-tures. The experimental evaluation shows the high accuracy and robustness of our framework. Also, several parameter sweeps are performed to see the effect of parameters.
As part of future work, we are planning to include contex-tual features described in the related work section to further improve our prediction framework. Also, we are planning to use a better weighing method for employing topological fea-tures to decrease prediction error. Finally, we plan to extend our prediction interval and prediction context further by us-ing datasets that span a longer amount of time and a variety of disciplines.
We would like to thank Prof. C. Lee Giles and his team from Pennsylvania State University for sharing the Cite-SeerX data with us. [1] CiteSeerX Digital Library. [2] J. Adams. Early citation counts correlate with [3] S. Bani-Ahmad and G. Ozsoyoglu. On popularity [4] A.-L. Barab  X asi and R. Albert. Emergence of scaling in [5] C. Castillo, D. Donato, and A. Gionis. Estimating [6] W.-Y. Chen, Y. Song, H. Bai, C.-J. Lin, and E. Y. [7] E. Garfield. The history and meaning of the journal [8] J. Gehrke, P. Ginsparg, and J. Kleinberg. Overview of [9] J. E. Hirsch. An index to quantify an individual X  X  [10] E. F. Krause. Taxicab geometry. The Mathematics [11] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graphs [12] D. Liben-Nowell and J. Kleinberg. The link-prediction [13] M. H. MacRoberts and B. R. MacRoberts. Problems [14] J. Manjunatha, K. Sivaramakrishnan, R. K. Pandey, [15] K. V. Mardia, J. T. Kent, and J. M. Bibby.
 [16] W. S. McCulloch and W. Pitts. A logical calculus of [17] D. McNamara, P. Wong, P. Christen, and K. S. Ng. [18] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral [19] H. Sakoe and S. Chiba. Dynamic programming [20] X. Shi, J. Leskovec, and D. A. McFarland. Citing for [21] N. Shibata, Y. Kajikawa, and K. Matsushima.
 [22] J. Tang, D. Zhang, and L. Yao. Social network [23] V. Vapnik. The nature of statistical learning theory . [24] R. Yan, J. Tang, X. Liu, D. Shan, and X. Li. Citation [25] D. Yogatama, M. Heilman, B. O X  X onnor, C. Dyer,
