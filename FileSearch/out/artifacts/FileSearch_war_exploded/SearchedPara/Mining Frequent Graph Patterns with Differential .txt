 Discovering frequent graph patterns in a graph database offers valu-able information in a variety of applications. However, if the graph dataset contains sensitive data of individuals such as mobile phone-call graphs and web-click graphs, releasing discovered frequent patterns may present a threat to the privacy of individuals. Dif-ferential privacy has recently emerged as the de facto standard for private data analysis due to its provable privacy guarantee. In this paper we propose the first differentially private algorithm for min-ing frequent graph patterns.

We first show that previous techniques on differentially private discovery of frequent itemsets cannot apply in mining frequent graph patterns due to the inherent complexity of handling structural infor-mation in graphs. We then address this challenge by proposing a Markov Chain Monte Carlo (MCMC) sampling based algorithm. Unlike previous work on frequent itemset mining, our techniques do not rely on the output of a non-private mining algorithm. In-stead, we observe that both frequent graph pattern mining and the guarantee of differential privacy can be unified into an MCMC sam-pling framework. In addition, we establish the privacy and utility guarantee of our algorithm and propose an efficient neighboring pattern counting technique as well. Experimental results show that the proposed algorithm is able to output frequent patterns with good precision.
 K.6.5 [ Management of Computing and Information Systems ]: Security and Protection Differential privacy; graph pattern mining
Frequent graph pattern mining (FPM) is an important topic in data mining research. It has been increasingly applied in a va-riety of application domains such as bioinformatics, cheminfor-matics and social network analysis. Given a graph dataset D = { D 1 ,D 2 ,...,D n } , where each D i is a graph, let gid ( G ) be the set of IDs of graphs in D which contain G as a subgraph. G is a frequent pattern if its count | gid ( G ) | (also called support ) is no less than a user-specified support threshold f . Frequent subgraphs can help the discovery of common substructures, and are the building blocks of further analysis, including graph classification, clustering and indexing. For instance, discovering frequent patterns in social interaction graphs can be vital to understand functioning of the so-ciety or dissemination of diseases.

Meanwhile, publishing frequent graph patterns may impose po-tential threat to privacy, if the graph dataset contains private infor-mation of individuals. In many applications, each graph (rather than a node) is associated with an individual and may be sensi-tive. For example, the click stream during a browser session of a user is typically a sparse subgraph of the underlying web graph; in location-based services, a database may consist of a set of trajecto-ries, each of which corresponds to the locations of an individual in a given period of time. Other scenarios of frequent pattern mining with sensitive graphs may include mobile phone call graphs [23] and XML representation of profiles of individuals. Therefore, ex-tra care is needed when mining and releasing frequent patterns in these graphs to prevent leakage of private information of individu-als.

Recently, the model of differential privacy [9] was proposed to restrict the inference of private information even in the presence of a strong adversary. It requires that the output of a differentially pri-vate algorithm is nearly identical (in a probabilistic sense), whether or not a participant contributes her data to the dataset. For the prob-lem of frequent graph mining, it means that even an adversary who is able to actively influence the input graphs cannot infer whether a specific pattern exists in a target graph. Although tremendous progress has been made in processing flat data (e.g. relational and transactional data) in a differentially private manner, there has been very little work (discussed in Section 7) on differentially private analysis of graph data, due to the inherent complexity in handling the structural information in graphs.

In this paper we propose the first algorithm for privacy-preserving mining of frequent graph patterns that guarantees differential pri-vacy. Recently several techniques [3, 17] have been proposed to publish frequent itemsets in a transactional database in a differ-entially private manner. It would seem attractive to adapt those techniques to address the problem of frequent subgraph 1 mining. Unfortunately, compared with private frequent itemset mining, the private FPM problem imposes much more challenges. First, graph datasets do not have a set of well-defined dimensions (i.e., items ), which is required by the techniques in [17]. Second, counting graph patterns is much more difficult than counting itemsets (due to graph
We use  X  X raph pattern X  and  X  X ubgraph X  interchangeably. isomorphism), which makes the size of the output space not imme-diately available in our problem. This prevents us from applying the techniques in [3]. We will explain the distinctions between [3, 17] and our work with more details in Section 2.3.
 Contributions. The major contributions of this paper are summa-rized as follows: 1. For the first time, we introduce a differentially private algo-2. Our approach provides provable privacy and utility guaran-3. In order to propose a neighboring pattern more efficiently in 4. We conduct an extensive experimental study on the effective-
Frequent graph pattern mining (FPM) aims at discovering the subgraphs that frequently appear in a graph dataset. Formally, let D = { D 1 ,D 2 ,...,D n } be a sensitive graph database which con-tains a multiset of graphs. Each graph D i  X  D has a unique identifier that corresponds to an individual. Let G = ( V,E ) be a (sub)graph pattern, the graph identifier set gid ( G ) = { i : G  X  D i  X  D} includes all IDs of graphs in D that contain a subgraph isomorphic to G . We call | gid ( G ) | the support of G in D . The FPM algorithm can be defined either as returning all subgraph pat-terns whose supports are no less than a user-specified threshold f , or as returning the top k frequent patterns given an integer k as in-put. One can easily convert one version to the other. All graphs we consider in this paper are undirected, connected and labeled. Note that each node has a label and multiple nodes can have the same label.
Differential privacy [9] is a recent privacy model which provides strong privacy guarantee. Informally, a data mining or publishing procedure is differentially private if the outcome is insensitive to any particular record in the dataset. In the context of graph pattern mining, let D , D 0 be two neighboring datasets , i.e., D and D in only one graph (by adding or removing an individual), written as ||D X  X  0 || = 1 . Let D n be the space of graph datasets containing n graphs.
 gorithm A is  X  -differentially private if for all neighboring datasets D , D 0  X  X  n , and any set of possible output O  X  Range ( A ) : The parameter  X  &gt; 0 allows us to control the level of privacy. A smaller  X  suggests more limit posed on the influence of a single graph. Typically, the value of  X  should be small (  X  &lt; 1 ).  X  is usu-ally specified by the data owner and referred as the privacy budget . In section 5.1 our discussion is related to a weaker notion called (  X , X  ) -differential privacy [8], which allows a small additive error factor of  X  .
 ized algorithm A is (  X , X  ) -differential private if for all neighbor-ing datasets D , D 0  X  D n , and any set of possible output O  X  Range ( A ) : A popular technique in applying differential privacy is the Laplace mechanism [9], which adds noise following Laplace distribution to the numeric output of a function. Applying the Laplace mechanism in our problem means adding noise to the support of all possible patterns and selecting the patterns with the highest noisy supports. However, this would be infeasible since it is computationally pro-hibitive to enumerate all possible patterns in any non-trivial sized graph mining problem.
 Exponential Mechanism. A general technique of applying dif-ferential privacy is the exponential mechanism [20]. It not only supports non-numeric output but also captures the full class of dif-ferential privacy mechanisms. The exponential mechanism consid-ers the whole output space and assumes that each possible output is associated with a real-valued utility score. By sampling from a distribution where the probability of the desired outputs are ex-ponentially amplified, the exponential mechanism (approximately) finds the desired outputs while ensuring differential privacy.
Formally, given input space D n and output space X , a score function u : D n  X X  X  R assigns each possible output x  X  X a score u ( D ,x ) based on the input D  X  D n . The mechanism then draws a sample from the distribution on X which assigns each x a probability mass proportional to exp(  X u ( D ,x ) / 2 X  u ) , where  X  u = max  X  x, D , D 0 | u ( D ,x )  X  u ( D 0 ,x ) | is the sensitivity of the score function. Intuitively, the output with a higher score is expo-nentially more likely to be chosen. It is shown that this mechanism satisfies  X  -differential privacy [20].
 T HEOREM 1. [20] Given a utility score function u : D n  X X  X  R for a dataset D , the mechanism A , gives  X  -differential privacy.

The exponential mechanism has been shown to be a powerful technique in finding private medians [6], mining private frequent itemset [3, 17] and more generally adapting a deterministic algo-rithm to be differentially private [22]. Our Diff-FPM algorithm works by carefully applying the exponential mechanism. In this process we must overcome several critical challenges, which are identified next.
There has been work [3, 17] on mining frequent itemsets in a transaction dataset under differential privacy. However, the shift from transactions to graphs poses significant new challenges. In [17], transaction datasets are viewed as high-dimensional tabular data, and the proposed approach projects the input database onto lower dimensions. However, graph datasets do not have a well de-fined set of items , i.e., dimensions, which renders the approach in [17] inapplicable in our FPM problem. In [3], two methods are pro-posed which make use of a notion of truncated frequency. However, those methods cannot be used in our problem due to the following fundamental challenges: Support Counting. Obtaining the support of a graph pattern is much more difficult than counting itemsets. An itemset pattern can be represented by an ordered list or a bitmap of item IDs Checking the existence of an itemset in a transaction only takes O (1) time, while checking whether a subgraph pattern exists in a graph is NP-complete due to subgraph isomorphism.
 Unknown Output Space. The output space X in our problem con-tains a finite number of graph patterns which may or may not exist in the input dataset. Under differential privacy, any pattern in the output space should have non-zero probability to be in the final out-put. The probability of sampling a pattern x from the output space is where C = P x  X  X  exp(  X u ( x ) / 2 X  u ) is the normalizing constant according to Theorem 1. The most straightforward way to compute C requires enumerating all the patterns in the output space. In [3], a technique is proposed to apply the exponential mechanism with-out enumerating if the size of the output space is known. However, unlike [3], in which the output space size can be obtained by sim-ple combinatorics (i.e., m l patterns of size l given an alphabet of size m ), the size of the output space X in our problem is not im-mediately available (due to graph isomorphism 2 ) , which prohibits us from applying exponential mechanism directly. Therefore we cannot apply the same techniques as in [3].

Given the analysis above, we need to develop new ways to over-come the issue of an unknown |X| . Note that although the global information on the output space is not accessible, we do have the local information on any specific pattern  X  given any pattern x , we can immediately calculate its utility score u ( x ) . In addition, the unknown normalizing constant C is common to all patterns. That is, given any pair of patterns x 1 ,x 2 , the ratio of probability mass  X  ( x 1 ) / X  ( x 2 ) is available without knowing the exact probabilities, according to Eq.(1). Such scenarios, where one needs to draw sam-ples from a probability distribution known up to a constant fac-tor, also arise in statistical physics when analyzing dynamic sys-tems, where Markov Chain Monte Carlo (MCMC) methods are of-ten used. Inspired by that, our idea is to perform a random walk based on locally computed probabilities. By carefully choosing the neighbor and the probability of moving in each step using the Metropolis-Hastings (MH) method [24], the random walk will con-verge to the target distribution, from which we can output samples. Next we discuss the details of our Diff-FPM algorithm.
The key challenge of handling graph datasets is the unknown output space when applying the exponential mechanism. The Diff-FPM algorithm meets the challenge by unifying frequent pattern
A detailed analysis on the size of the output space can be found in the full version [26]. mining and applying differential privacy into an MCMC sampling framework. The main idea of Diff-FPM is to simulate a Markov chain by performing an MCMC random walk in the output space. Our goal is that when the random walk reaches its steady state, the stationary distribution of the Markov chain matches the target distribution  X  in Eq.(1). In Section 3.2.1 we will explain in detail how to apply the Metropolis-Hastings (MH) method in our problem to achieve this goal. Before that, we need to define the state space in which we perform the random walk.
 Partial Order Full Graph. To facilitate the MH-based random walk in the output space, we define the Partial Order Full Graph (POFG) as the state space of the Markov chain on which the sam-pling algorithm run the simulation. Each node in POFG corre-sponds to a unique graph pattern and each edge in POFG represents a possible  X  X xtension X  (add or remove one edge) to a neighboring pattern. Naturally, each node in the POFG has three types of neigh-bors: sub-neighbor (by removing an edge), super-backward neigh-bor (by connecting two existing nodes) and super-forward neigh-bor (by adding and connecting to a new node).

E XAMPLE 1. Figure 1 shows a simple graph dataset contain-ing 3 graphs and its POFG. The dashed patterns have support smaller than 2 in the dataset. Pattern A  X  A  X  C has two sub-neighbors, one super-backward neighbor and several super-forward neighbors (only one shown in Figure 1(b)). Self-loops and multi-edges are not considered in this example and thus are excluded from the output space.

At a higher level, the random walk starts with an arbitrary pattern and proceeds to an adjacent pattern with certain probability in each step. Since the transition decision is made solely based on local information (related to the neighborhood of the current pattern), there is no need to construct the global POFG explicitly. When the random walk has reached its steady state, the probability of being in state x follows exactly the target distribution  X  ( x ) in Eq.(1). Then the current state is drawn as a sampled pattern. Since the frequent patterns have larger probabilities in the target distribution, they are more likely to appear in the final output.
The core of the Diff-FPM algorithm is a careful application of the MH method. The MH method is a Markov Chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a target probability distribution for which direct sampling is diffi-cult. It only requires that a function proportional to the probability mass be calculable.

Suppose we want to generate a random variable X taking values in X = { x 1 ,...,x |X| } , according to a target distribution  X  , with where all b ( x i ) are strictly positive, |X| is large, and the normal-izing constant C = P |X| i =1 b ( x i ) is difficult to calculate. The MH method first constructs an |X| -state Markov chain { X t ,t = 0 , 1 ,... } on X whose evolution relies on an arbitrary proposal transition ma-trix Q = q ( x,y ) in the following way: 1. When X t = x , generate a random variable Y satisfying 2. Given Y = y , let means that given a current state x , the next state is proposed ac-cording to the proposal distribution Q . q ( x,y ) is the probability mass of state y among all possible states given the current state is x . With probability  X  xy , the proposal is accepted and the chain moves to the new state y . Otherwise it remains at state x . It fol-lows that { X t ,t = 0 , 1 ,... } has a one-step transition probability matrix P :
It can be shown that for the above P , the Markov chain is re-versible and has a stationary distribution  X  , equal to the target dis-tribution. Therefore, once the chain has reached the steady state, the sequence of samples we get from the MH method should fol-low the target distribution.

E XAMPLE 2. Consider a random walk on the POFG illustrated in Figure 1(b). Suppose the current state of the walk is  X  X -A-D X  (pattern x ). Following the MH method, one of pattern x  X  X  neighbors needs to be proposed according to a proposal distribu-tion q ( x,y ) . For simplicity, in this example each neighbor has an equal probability to be proposed, i.e., q ( x,y ) = 1 / | N ( x ) | , where N ( x ) is the neighbor set of x . Assuming  X  X -D X  (pattern y) is pro-the probability of accepting the proposal is calculated as  X  ber between 0 and 1 to decide whether walking to pattern y or staying at x .

The description of the Diff-FPM algorithm above can be summa-rized in Algorithm 1. The input consists of the raw graph dataset D , a support threshold f and the privacy budget  X  =  X  1 +  X  top-k frequent patterns are desired, we first run non-private FPM algorithms such as gSpan [29] to get the support threshold f , i.e., the support of the k th frequent pattern. If one only needs k pat-terns whose supports are no less than a threshold, f can be directly provided to the algorithm. At a higher level, Algorithm 1 consists
Algorithm 1: Diff-FPM algorithm input : Graph dataset D , threshold f , privacy budget  X  1 output : A set S of k private frequent patterns for i = 1 to k do 2 Choose any pattern in the output space as seed pattern; 3 while True do 4 Propose a neighboring pattern y of current pattern x 5 Accept the proposed pattern with probability 6 if convergence conditions are met then 7 Add current pattern to S and remove it from the 8 break ; (Optional) for each pattern in S , perturb its true support by
Laplace mechanism with privacy budget  X  2 /k ; of two phases: sampling and perturbation. The sampling phase in-cludes k applications of the exponential mechanism via MH-based random walk in the output space.

Initially, we select an arbitrary pattern in the output space to start the walk (Line 2). At each step, we propose a neighboring pat-tern y of the current pattern x according to a proposal distribution (Line 4). The proposal distribution does not affect the correctness of the MH method, so we defer the details to Section 3.2.3. The proposed pattern is then accepted with probability  X  xy as in the MH-algorithm (Line 5), where u (  X  ) is the score function with  X  u being the sensitivity of u (  X  ) . We explore the design space of the score function in the next paragraph. When the Markov chain has converged (see Section 3.3 for convergence diagnostic), we output the current pattern and remove it from the output space (Line 6 to 8). We then start a new walk until k patterns have been sampled. Finally, if one wants to include the support of each output pattern as well, the count of each pattern is perturbed by adding Lap ( k/ X  noise (Line 9).
Choosing the utility score function is vital in our approach as it directly affects the target distribution. A general guideline is that the patterns with higher supports should have higher utility scores in order to have larger probabilities to be chosen according to expo-nential mechanism. Under this guideline, given an input database D , the most straightforward choice is to let u ( x, D ) = | gid ( x ) | for any pattern x . In this case, the sensitivity  X  u is exactly 1 since the support of any subgraph pattern may vary by at most 1 with the addition or removal of a graph in the dataset. This is also the score function we use in the experiment.
Although in theory the proposal distribution can be arbitrary, it can significantly impact the efficiency of the MH method by affect-ing the mixing time (time to reach steady state). A good proposal distribution can improve the convergence speed by increasing the accept rate  X  xy in the MH method. On the contrary, if the proposed pattern is often rejected, the chain can hardly move forward. It has been suggested that one should choose a proposal distribution close to the target distribution [11]. In our problem setting, it is prefer-able to make a distinction between the patterns having support no less than f (referred as frequent patterns) and those whose supports are lower (referred as infrequent patterns). Given a current state x , we denote the set of frequent neighbors of x as N 1 ( x ) and the set of infrequent neighbors as N 2 ( x ) . Since | N 2 ( x ) | is usually larger than | N 1 ( x ) | , we will balance the probability mass assigned to N and N 2 ( x ) by introducing a tunable parameter  X  (0 &lt;  X  &lt; 1) . Our heuristic based proposal distribution is formally described below: In the experiment we use  X  &gt; 0 . 5 such that a frequent pattern has a higher probability to be proposed than an infrequent pattern. If any of N 1 ( x ) or N 2 ( x ) is empty, its probability mass will be re-distributed (by setting  X  = 0 or  X  = 1 respectively). Note that the choice of the proposal distribution does not impact the privacy and utility guarantee of Diff-FPM .
In line 6 to 8 of Algorithm 1, after the convergence conditions are met and a sample pattern g is outputted, we need to exclude g from the output space by connecting g  X  X  neighbors and removing g in the POFG. In our implementation this is done by replacing g by all the neighbors of g whenever g appears in some pattern X  X  neighborhood. Note that we do not output multiple patterns when the chain has converged. This is because once a pattern is sampled, it should be excluded from the output space and thus have zero probability to be chosen. Therefore adjustment to the output space is necessary after each sample. For the same reason we do not run multiple chains at once.
The theory of MCMC sampling requires that samples are drawn when the Markov chain has converged to the stationary distribution, which is also our target distribution  X  . The most straightforward way to diagnose convergence is to monitor the distance between the target distribution  X  and the distribution of samples  X   X  . In practice, however,  X  is often known only up to a constant factor. To deal with this problem, several online diagnostic tests have been developed in the MCMC literature [11] and used in random walk based sampling of graphs [12].

Online diagnostics rely on detecting whether the chain has lost its dependence on the starting point. We adopt a standard conver-gence test called the Geweke diagnostic [10]. The Geweke diagnos-tic takes two non-overlapping parts (usually the first 0.1 and last 0.5 proportions) of the Markov chain and see if they are from the same distribution. Specifically, let X be a sequence of samples of our metric of interest and X 1 ,X 2 be the two non-overlapping subse-quences. Geweke computes the Z -score: Z = E ( X 1 )  X  E ( X With increasing number of iterations, X 1 and X 2 should move fur-ther apart and become less and less correlated. When the chain has converged, X 1 and X 2 should be identically distributed with Z  X  N (0 , 1) by law of large numbers. We can declare conver-gence when Z has continuously fallen in the [  X  1 , 1] range. Since the samples in our problem are graph patterns rather than a scalar, we may need to monitor multiple scalar metrics related to different properties of the sampled pattern and declare convergence when all these metrics have converged.

We need to acknowledge that these convergence diagnostic tools from the MCMC literature are heuristic per se . Verifying the con-vergence remains an open problem if the distribution of samples is not directly observable. Even so, Diff-FPM still achieves (  X , X  ) -differential privacy if there exists a small distance between the tar-get and simulation distributions, as we will show in Lemma 2 in Section 5.

Algorithm 2: The EEN algorithm input : Pattern x , graph dataset D , support threshold f output : N 1 ( x ) , N 2 ( x )
Initialize N 1 , N 2  X  X  X  ( x omitted for brevity);
Find membership bitmap B x using VF2 isomorphism test;
Populate sub-neighbors N b , super-back neighbors N p back super-forward neighbors N p fwd ; / * Explore sub-neighbors N b * / if sum ( B x )  X  f then N 1  X  N 1  X  N b ; else for x 0  X  N b do 6 if SUB _ IS _ FREQ ( x 0 ,B x ) then N 1  X  N 1  X  X  x 0 } ; 7 else N 2  X  N 2  X  X  x 0 } ; / * Explore super-back neighbors N p back * / if sum ( B x ) &lt; f then N 2  X  N 2  X  N p back ; else 10  X  x 0  X  N p back , initialize dictionary H [ x 0 ] = 0 ; 11 for i  X  1 to |D| do 12 Find set M of all mappings between D i and x ; 14 if H [ x 0 ] &lt; f and |D| X  i + H [ x 0 ]  X  f then 15 Let ( u,v ) be the back edge, i.e., 16 for m  X  X  do 17 if m ( u ) ,m ( v ) are adjacent in D i then 18 H [ x 0 ]  X  H [ x 0 ] + 1 ; 19 break; 21 if H [ x 0 ]  X  f then N 1  X  N 1  X  X  x 0 } ; 22 else N 2  X  N 2  X  X  x 0 } ;
Explore super-forward neighbors N p fwd similarly as N p back details in [26]; return N 1 ,N 2 ;
We have discussed so far the core of the Diff-FPM algorithm and seemingly it could be run straightforwardly. However, without cer-tain optimization, the computation cost might render the algorithm impractical to run. The most costly operation in the Diff-FPM algo-rithm is proposing a neighbor of the current pattern x . According to the proposal distribution in Eq.2, this requires knowledge on the support of each pattern in x  X  X  neighbors N ( x ) . Due to the fact that subgraph isomorphism test is NP-complete, obtaining the support of each neighbor might become a computation bottleneck.

To overcome this problem, we have developed an efficient al-gorithm (called EEN) to explore the neighborhood of a pattern by observing the connection between neighboring patterns and their isomorphic mappings.
The task of neighbors exploration can be described as: given a pattern x , find the set of frequent neighbors N 1 ( x ) and infrequent neighbors N 2 ( x ) , as in the proposal distribution (Eq.2). A naive way to populate N 1 ( x ) and N 2 ( x ) is to test each neighbor of x against the graph dataset D . However, this is extremely inefficient since | N ( x ) | X |D| isomorphism tests are required, where |D| is the number of graphs in D . A basic optimization would be using the monotonic property of frequent patterns: if x is a frequent pattern, any subgraph of x should be frequent too; likewise, an infrequent pattern X  X  super-graph must be infrequent. However, explicit iso-morphism testing is still required for exploring the super-neighbors of x if x is frequent or x  X  X  sub-neighbors if x is infrequent.
The EEN algorithm is able to further reduce the number of iso-morphism tests. Observing that x and y only differ in one edge for all y  X  N ( x ) , the main idea of is to re-use the isomorphic mappings between x and D i  X  D and examine whether any of the isomorphic mappings can be retained after extending an edge. The EEN algorithm is formally presented in Algorithm 2 and is described in the following.

Algorithm 2 takes pattern x , graph dataset D and support thresh-old f as input and returns N 1 ( x ) and N 2 ( x ) . First, pattern x is tested against each graph in D and the result is stored in B { i | x  X  D i ,D i  X  X } , which is the set of IDs of graphs containing pattern x (line 2). The subgraph isomorphism algorithm we use is the VF2 algorithm [5]. Next we populate three types of neighbors of x : sub-neighbors N b , super-back neighbors N p back forward neighbors N p fwd (line 3), and handle them differently. Explore sub-neighbors (line 4 to 7). For N b , if x is frequent, the entire set N b should be frequent. If x is infrequent, each pattern in N b is examined by the boolean sub-procedure S UB _ IS S
UB _ IS _ FREQ takes a sub-neighbor x 0 of x and B x as input and returns whether x 0 is frequent. First we find B E = T e  X  x intersection of ID sets of all edges in pattern x 0 . Then subgraph isomorphism test is only needed for the graphs D i The set of IDs of graphs that succeed the test together with B comprise B x 0 . Finally the procedure returns the frequentness of x by comparing f and the size of B x 0 .
 Explore super-back neighbors (line 8 to 22). For N p back infrequent, the entire N p back must be infrequent. Otherwise, we test whether x 0  X  N p back is a subgraph of D i for each D i . In this part, the EEN algorithm does not require any additional subgraph iso-morphism test at all. This is achieved by re-using the isomorphism mappings between the base pattern x and D i and reasoning upon that. In line 12 we find all the subgraph isomorphism mappings computing B x in line 2 as part of the VF2 algorithm. Note that the subgraph isomorphism package we use is complete, i.e., it can return all the mappings. Suppose x is extended to x 0 by connect-ing node u and v (line 15). If any of the isomorphism mappings m  X  X  is preserved with the edge extension (i.e., m ( u ) and m ( v ) are adjacent in D i ), x 0 must be a subgraph of D i . Otherwise if none of the mappings can be preserved, x 0 is not a subgraph of D
In the above process, we use a dictionary H to keep track of the number of graphs in D so far that contains x 0 as a subgraph, i.e., H [ x 0 ] maintains |{ D i | x 0  X  D i }| for the D i tested so far. Line 14 ensures that the isomorphism extension test is only performed when H [ x 0 ] has not reached f .
 Explore super-forward neighbors. For N p fwd , the algorithm is similar to the procedures of exploring super-back neighbors, except that the extension test is now on a forward edge instead of a back edge. The details are available in [26] due to space limit.
The proof of the lemmas and theorems in this section can be found in [26].
In this part we establish the privacy guarantee of Diff-FPM . We show both the sampling and perturbation phases preserve privacy, and then we use the composition property of differential privacy to show the privacy guarantee of the overall algorithm.

In the sampling phase, our target probability distribution  X  ( D ,  X  ) drawn directly from this distribution, it would achieve strict differential privacy due to the exponential mechanism. Since we use MCMC based sampling, the distribution of the samples  X   X  ( D ,  X  ) will approximate  X  ( D ,  X  ) , i.e. the two distributions are asymptot-ically identical. In real simulation, there may be a small distance between the two distributions. To quantify the impact on privacy when a small error is present, we use the total variation distance [24] to measure the distance of the two distributions at a given time: which is the largest possible difference between the probabilities that  X  (  X  ) and  X   X  (  X  ) can assign to the same event.

Let A ( D ) denote the process of sampling one pattern according to Algorithm 1 (Line 4 to 8). The privacy guarantee that A ( D ) offers is described by the following lemma:
L EMMA 2. Let  X  (  X  ) and  X   X  (  X  ) denote the target distribution and the distribution of samples from A ( D ) respectively. Suppose ||  X   X  (  X  )  X   X  (  X  ) || TV  X   X  , procedure A ( D ) gives (  X  1 k , X  ) -differential privacy, where  X  =  X  (1 + e  X  1 /k ) .
 Note that  X  is a function of simulation time t . The following lemma describes the asymptotic behavior and the speed of convergence of the chain :
L EMMA 3. [24] If a Markov chain on a finite state space is irreducible and aperiodic, and has a transition kernel P and sta-tionary distribution  X  (  X  ) , then for x  X  X  , for some  X  &lt; 1 and M &lt;  X  . And
It means  X  is decreasing at least at a geometric speed and approx-imates to zero when the simulation is running long enough.
Since the sampling process in Algorithm 1 consists of k succes-sive applications of exponential mechanism based on random walk, we need the following well-known composition lemma to provide privacy guarantee for the entire sampling phase.

L EMMA 4. [19] Let A 1 ,..., A t be t algorithms such that A satisfies  X  i -differential privacy, 1  X  i  X  t. Then their sequential composition  X  X  1 ,..., A t  X  satisfies  X  -differential privacy, for  X  = P
Equipped with the results in previous lemmas, we are able to provide the privacy guarantee for Algorithm 1.

T HEOREM 5. Algorithm 1 satisfies  X  -differential privacy.
Because neighboring inputs must have similar output under dif-ferential privacy, a private algorithm usually does not return the exact answers. In the scenario of mining top-k frequent patterns, the Diff-FPM algorithm returns a noisy list of patterns which is close to the real top-k patterns. To quantify the quality of the out-put of Diff-FPM , we first define two utility parameters, following [3]. Recall that f is the support of the k th frequent pattern, and let  X  be an additive error to f . Given 0 &lt;  X  &lt; 1 , we require that with probability at least 1  X   X  , (1) no pattern in the output has true support less than f  X   X  and (2) all patterns having support greater than f +  X  exist in the output. The following theorems provide the utility guarantee of Diff-FPM . A score function u ( x ) = | gid ( x ) | is assumed.

T HEOREM 6. At the end of the sampling phase in Algorithm 1, for all 0 &lt;  X  &lt; 1 , with probability at least 1  X   X  , all patterns in set S have support greater than f  X   X  , where  X  = 2 k  X  and M is an upper bound on the size of output space.
 The following theorem provides the upper bound of noise added to the true support of each output pattern.

T HEOREM 7. For all 0 &lt;  X  &lt; 1 , with probability of at least 1  X   X  , the noisy support of a pattern differs by at most  X  , where
In this section, we evaluate the performance of Diff-FPM through extensive experiments on various datasets. Since this is the first work on differentially private mining of frequent graph patterns, the quality of the output is compared with the result from a non-private FPM algorithm and the accuracy is reported. In this section we consider the scenario of mining the top-k frequent patterns. Datasets. The following three datasets are used in our experiment: DTP is a real dataset containing DTP AIDS antiviral screening dataset 3 , which is frequently used in frequent graph pattern min-ing study. It contains 1084 graphs, with an average graph size of 45 edges and 43 vertices. There are 14 unique node labels and all edges are considered having the same label.

The click dataset consists of 20K small tree graphs (4 nodes and 3 edges on average) obtained by a graph generator developed by Zaki [30]. To a certain extent, this synthetic dataset simulates user click graphs from web server logs [30], which is a suitable type of data requiring privacy-preserving mining. All the tree graphs in this dataset are sampled from a master tree.
 The above two datasets contain graphs that are relatively sparse. To test our algorithm on dense graphs, we also use a dataset con-taining 5K graphs, in which the average node degree is 7. Each graph contains 10 vertices and 35 edges on average. The graph gen-erator [4] we use is specially designed for generating graph datasets for evaluation of frequent subgraph mining algorithms. The size of this graph dataset is comparable to the largest datasets used in pre-vious works [29, 15].
 Utility metrics. We evaluate the quality of the output of Diff-FPM by employing the following three utility metrics: Precision , Support Accuracy and nDCG 4 . Precision is defined as the fraction Precision = | True Positives | /k . This is the complementary mea-sure of the false negative rate used in [3]. The true top-k patterns are obtained by a non-private graph mining algorithm (gSpan [29] in our experiment). The measure of precision reflects the percent-age of desired/undesired patterns in the output, yet it cannot in-dicate how good or bad the output patterns are in terms of their supports. For example, if f = 1000 , it is much more undesirable if a pattern with support 10 appears in the output compared to a pat-tern with support 980, even though the precision may be the same in these two cases. We first define the relative support error (RSE) http://dtp.nci.nih.gov/docs/aids/aids_data.html http://en.wikipedia.org/wiki/Discounted_cumulative_gain as RSE = ( S true  X  S out ) /kf , where S true and S out are the sum of the supports of the real top-k patterns and sum of the supports of the sampled patterns respectively. This measure reflects the average deviation of an output pattern X  X  support with respect to the support threshold f . In the plots, the support accuracy is reported, which equals 1  X  RSE . nDCG is a commonly used metric to compare two ranked lists. This metric is accumulated from the top of the result list to the bottom with the weight of each result discounted at lower ranks. In our problem setting, the top-k patterns are un-ordered. Still, nDCG is able to reveal whether any important pattern is miss-ing in the output.

All experiments were conducted on a PC with 3.40GHz CPU with 8GB RAM. The random walk in the Diff-FPM algorithm has a small memory footage due to its Markovian nature. We imple-mented our algorithm in Python 2.7 with the JIT compiler PyPy speed up. The default parameters of  X  = 0 . 5 ,  X  = 0 . 8 and k = 15 were used unless specified otherwise. In the experiment we do not release the noisy supports of the patterns in the output (line 9 in Al-gorithm 1), so all the privacy budget is used in the sampling phase. Comparison of neighbor exploration methods. In Section 4.1 we proposed the EEN algorithm to efficiently explore the neigh-borhood of a pattern. We now compare it with two other meth-ods: a naive approach which finds the support of each neighbor of the current pattern x and a basic approach which uses the mono-tonic property of frequent patterns (see Section 4.1). Figure 2(a) shows the average iteration time in logarithm of the three meth-ods over three datasets. In each iteration, a neighboring pattern is proposed and then accepted or rejected according to the MH algo-rithm. Clearly, EEN takes significantly less time in each iteration than the other methods in both datasets, reducing the iteration time by at least an order of magnitude compared to the naive approach. Thus all subsequent results are presented with EEN enabled. Run time and scalability. Figure 2(d) illustrates the average time taken to output one frequent pattern as the size of the dataset in-creases. For the full datasets, click takes 20 seconds, DTP takes about 1 minute and dense sits in the middle, although the click dataset contains 20K graphs compared to only 1K in the DTP . It indicates that the size of each individual graph and the size of the neighborhood have a larger impact on the run time than the total number of graphs in the dataset (note that DTP has 14 labels and thus a larger neighborhood of a pattern compared to dense ). For scalability, all datasets are observed to have linear scale-up in time as the size of graph dataset increases.
 Utility result. To test the quality of the output by Diff-FPM , we ex-amine the utility metrics introduced above under various parameter settings.

First, Figure 2(b) and Figure 2(c) show the precision and SA when we increase the size of the graph dataset from 10% to 100% . An increasing trend of the output quality can be clearly observed here. This is in line with our expectation because achieving dif-ferential privacy is more demanding in a small dataset  X  the larger the number of records in the database, the easier it is to hide an individual record X  X  impact on the output. For all three full datasets, Diff-FPM is able to achieve at least 80% on both precision and SA . Figure 3(a) shows the precision when varying privacy budget  X  . With a very limited budget (  X  = 0 . 1 ), only about 30% of samples http://pypy.org
The data point for dense at 10% is absent since the smallest dataset size can be generated is 1K. are from the real top-k patterns for DTP and dense . This is in-evitable due to the privacy-utility tradeoff. As more privacy budget is given, the precision of Diff-FPM increases fast. At  X  = 0 . 5 , the precisions from all datasets have reached 80%. Further increase in privacy budget does not provide significant benefit on the pre-cision. We observed a similar trend in the support accuracy plot (Figure 3(b)), with less dramatic changes for  X  from 0.1 to 0.5.
Figures 3(c) and 3(d) illustrate the impact of the number of pat-terns in the output. Recall that in each round of sampling, a budget of  X /k is consumed (cf. proof of Theorem 5). Given a certain privacy budget, the more patterns to output, the less privacy bud-get each sample can use. Thus we expect the average quality of the output to drop as k increases, which is confirmed in the result. Meanwhile, the support accuracy of the output holds well with the increasing number of output, which can be seen in Figure 3(d).
We also report the nDCG of the output with respect to differ-ent privacy levels in Figure 4. It can be seen that given moderate amount of privacy budget, the nDCG of the output remains larger than 0.8, suggesting close resemblance (especially on the several most frequent patterns) between the true top-k and the top-k we found.
 Convergence analysis. A decision we have to make is when to stop the random walk and output a sample. In Section 3.3 we in-troduced Z-score based Geweke diagnostic, which compares the distribution at the beginning and end of the chain. Since MCMC is typically used to estimate a function of the underlying random vari-able instead of structural data like graphs, we need to choose some properties of the patterns which we will monitor using the Geweke test. The three metrics we use in the experiment are the number of neighbors N ( x ) , the number of frequent neighbors N the number of nodes in the pattern | x | . Figure 5 shows the conver-gence traces of a sample run with K = 20 and  X  = 0 . 5 on the DTP dataset. Each curve corresponds to the Z-score of a chain over the number of iterations. It can be seen that the Markov chain we de-sign has pretty fast convergence rate thanks to the tuning of the pro-posal distribution. For each chain, convergence is declared when the Z-scores of all three metrics have fallen within the [  X  1 , 1] range for 20 iterations continuously. In Figure 5, this happens around 150 iterations for most chains.
Data Mining with Differential Privacy. There exist two ap-proaches to differentially private data mining. In the first approach, the data owner releases an anonymized version of the dataset under differential privacy. And the user has the freedom of conducting any data mining task on the anonymized dataset. We call this the  X  X ublishing model X . Examples include releasing anonymized ver-sion of contingency tables [28], data cubes [7] and spatial data [6]. The general idea in these work is to release tables of noisy counts (histograms) and study how to ensure they are sufficiently accurate for different query workloads. In the other approach, differential privacy is applied to a specific data mining task, such as social rec-ommendations [18] and frequent itemset mining [3]. The problem addressed in this paper falls into this category.

Privacy-Protection of Graphs. The aforementioned works on differentially private data mining all deal with structured data. For graph data, there is plenty of research effort [1] to anonymize a so-cial network graph to prevent node and edge re-identification. But most of them focus on modifying the graph structure to satisfy k -anonymity, which has been proved to be insufficient [1]. Recently, several works [16, 13, 25, 14, 21] emerge to provide private anal-ysis of graph data. Two types of differential privacy have been introduced to handle graph data: node differential privacy and edge differential privacy. It is still open whether any nontrivial graph statistics can be released under node differential privacy due to its inherent large sensitivity (e.g., removing a node in a star graph may result in an empty graph). Hay et al. [13] consider the problem of releasing the degree distribution of a graph under a variant of edge differential privacy. More recently, Karwa et al. [16] pro-pose algorithms to output approximate answers to subgraph count-ing queries, i.e., given a query graph H (e.g. a triangle, a k -star), returning the number of edge-induced isomorphic copies of H in the input graph. Unfortunately, their work does not support the case when H is an arbitrary subgraph yet.

In contrast, we have a different problem setting from [16]. First, like [3], our privacy-preserving algorithm is associated with a spe-cific and more complicated data mining task. Second, we consider a graph database containing a collection of graphs related to indi-viduals.

Graph Pattern Mining. Finally, we briefly discuss relevant works on traditional non-private graph pattern mining. Earlier works which aim at finding all the frequent patterns in a graph database usually explore the search space in a certain manner. Represen-tative approaches include a priori -based (e.g. [15]) and pattern growth based (e.g. gSpan [29]). Recent works aim at mining signif-icant or representative patterns with scalability. One way of achiev-ing this is through random walk [2], which also motivates our use of MCMC sampling for privacy preserving purpose. Another re-motely related work is [27], which connects probabilistic inference and differential privacy. It differs from this work by focusing on inferencing on the output of a differentially private algorithm.
In this paper we have presented a novel technique for differen-tially private mining of frequent graph patterns. The proposed solu-tion integrates the process of graph mining and privacy protection into an MCMC sampling framework. Moreover, we have estab-lished the theoretical privacy and utility guarantee of our algorithm. Experiments on both synthetic and real datasets show good preci-sion and support accuracy with moderate amount of privacy budget. We also notice the drop in utility with the increase of the number of outputs or the decrease in dataset size, which is inevitable under the requirement of differential privacy.
 Acknowledgments. This research is partially supported by the National Science Foundation under the award CNS-0747247.
