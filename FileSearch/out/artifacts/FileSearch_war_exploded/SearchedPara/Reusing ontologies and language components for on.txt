 1. Introduction
Ontology construction is a central research issue for the Semantic Web. Ontologies provide a way of formalizing human knowledge to enable machine interpretability. Creating ontologies from scratch is, however, usually tedious and costly.
When the Semantic Web requires ontologies that express Web page content, the ontology engineering task becomes too expensive to be done manually. Many Semantic Web ontologies may have overlapping domain descriptions because many
Web sites (or pages) contain information in common domains. It is inefficient to redo ontology engineering for pre-explored domains. These issues illustrate the importance of automated ontology reuse for the Semantic Web.

Ontology reuse involves building a new ontology through maximizing the adoption of pre-used ontologies or ontology components. Reuse has several advantages. First, it reduces human labor involved in formalizing ontologies from scratch.
It also increases the quality of new ontologies because the reused components have already been tested. Moreover, when two ontologies share components through ontology reuse, mapping between them becomes simpler because mappings be-tween their shared components are trivial. One can also simultaneously update multiple ontologies by updating their com-monly shared components. Ontology maintenance overhead can thus be accordingly reduced.

Despite the many advantages of (automated) ontology reuse, the topic is not well explored in the literature. There are many reasons for this. Before the advent of the Semantic Web, few ontologies existed. Due to the difficulty of constructing opment. With the advance of Semantic Web technologies, the number of ontologies has significantly increased recently.
When the use of ontologies in Semantic Web applications improves system performance, more people will appreciate the advantage in using ontologies. In the meantime, most existing ontologies are hard to reuse. The benefits of manual ontology reuse are often unclear since the overhead of seeking and understanding existing ontologies by humans may be even greater than simply building an ontology from scratch. At the same time, many existing ontologies simply do not support effectively automated ontology reuse. The corresponding information in these ontologies is hard to retrieve for automated ontology reuse.

The work we describe below 1 offers three contributions for automated ontology reuse. We first sketch the state of the art in ontology reuse (Section 2). We then present our generic ontology reuse architecture and our implementation (Section 3). Next, we discuss experimental results obtained by using our implementation on real-world examples, as well as five lessons we have learned from this work (Section 4). We conclude with possible future directions (Section 5). 2. Related work
Ontology reuse has been studied for years. Most of the earlier research focuses on the study of reusable ontology repos-demonstration only. X  They concluded that reusing an ontology was  X  X  X ar from an automated process X  at that time.
With the growth of Semantic Web research, more and more ontologies have been created and used in real-world appli-cations. Researchers have started to address more of the ontology reuse problem. Typically, there are two strands of study: libraries showed that it was difficult to manage heterogeneous ontologies in simple repositories. Standardized modules may significantly improve the reusability of ontologies. One major purpose of modular ontology research concerns the reusability of ontologies [3 X 5] . There are, however, fewer ontology reuse studies quantifying how modular ontologies may improve the efficiency of ontology reuse. Hence one of our purposes is to argue for the use of modular ontologies in real-world, auto-mated ontology reuse experiments.

Meanwhile, there are also several studies on practical ontology reuse. Noy and Musen [7] introduced  X  X  X raversal views X  that define an ontology view, through which a user can specify a subset of an existing ontology. This mechanism enables users to extract self-contained portions of an ontology describing specific concepts. Stuckenschmidt and Klein [8] described another process for partitioning very large ontologies into sets of meaningful and self-contained modules through a struc-ture-based algorithm. Of course, working in the opposite direction is also possible. Pinto and Martins [9] assemble, extend, and integrate small existing ontologies into a large domain ontology by using publicly available ontologies. They discuss two different domains in general terms though not quantifying the size or complexity of the domains or the performance of their ontology reuse process.

Alani et al. [6] coined a new term for reusing existing ontologies: ontology winnowing. The intuition of their research is that individual Semantic Web applications more profitably use smaller customized ontologies rather than larger general-purpose ontologies. They therefore described a method for culling out X  X hich they called winnowing X  X seful application-specific information from a larger ontology. Subsequent work of theirs [10] proposes a generic ontology reuse infrastructure that combines ontology search, ranking, segmentation, mapping, merging, and evaluation. Though technical and experimen-tal details lack in this position paper, this work could well complement our own. In particular, once users build small domain ontologies with our system, they might channel the newly constructed results to Alani X  X  system to assure their future reusability.

A common implicit assumption in all these practical ontology reuse studies is that source ontologies must be reusable for a target domain. Although this assumption simplifies the problem, it does not address the general situation. Besides our work, to the best of our knowledge, the only research that has addressed (albeit implicitly) the domain-specific ontology re-use problem is by Bontas and her colleagues [11]. Their case studies on ontology reuse identified difficulties due to end-user unfamiliarity with the complex source structure. Although this assessment is reasonable, we found a further reason for the difficulty they encountered. Even though source ontologies often declare a target domain, the corresponding information is irretrievable for automated ontology reuse. This is the real bottleneck for automated ontology reuse.

Finally, others concentrate their research on the user X  X  context for ontology reuse. For example, Qin et al. [12] stress the tionship to ontological structure in reuse. While we recognize the value of this and related lines of investigation, we focus here mainly on the ontology reuse processes that a more general contextualized approach could subsume. 3. Automated ontology reuse
Fig. 1 sketches our generic architecture for automated ontology reuse. The reuse procedure takes at least two inputs: nat-ural language (NL) documents and source ontologies. NL documents express the projected domains and they can encompass different types. Typical NL documents could include collections of competency questions [13] or collections of sample Web pages [14].
In this architecture, ontology reuse consists of three sequential steps: concept selection, relation retrieval, and constraint discovery. These correspond to the three fundamental components in ontologies: concepts, relationships, and constraints. The concept selection process identifies reusable ontology concepts from source ontologies based on the descriptions in NL documents. NL documents must contain sufficient information for a system to identify all the necessary domain concepts.
The identification methodologies vary with respect to different types of NL documents. The relation retrieval process re-trieves relationships among selected concepts from the previous step. These relationships can be automatically gathered from source ontologies or perhaps even (optionally) recoverable from the NL documents. Fig. 1 represents these optional requirements with dotted lines. The constraint discovery process discovers constraints for previous selected concepts and relationships. An ontology reuse system should be able to gather existing information about constraints from source ontol-ogies or even perhaps from NL documents.

After these three sequential steps, the system composes the selected concepts, relationships, and constraints together into a unified ontology. Human experts then inspect and revise these auto-generated ontologies.

We have implemented a prototype automated ontology reuse system based on this generic architecture. Our system reuses existing ontologies to create small domain ontologies within the scope of describing individual Web pages. In the rest of this section we describe the system in more detail. 3.1. Preparation of input
We first take a small set of sample Web pages as input NL documents, and pre-process them to focus on the content that reflects the domain of interest. This involves removing extraneous information such as advertisements, side bars, and links to various other pages. Similarly, many pages might combine several records together; in such cases each record is factored out and stored as a separate document. We do some of these processes automatically and others manually; information about how we automate this document preprocessing work is available elsewhere [15,16] . Only the main body of each page re-mains, which constitutes the focus of interest for readers.

Proper preparation of source ontologies is also essential for ontology reuse automation. Poorly integrated source ontologies create a very complex ontology integration problem during final composition of the output ontology. Two op-tions exist: either we can directly adopt a single large-scale ontology, or we can manually pre-integrate several small ones. For simplicity, we chose the first option (and will discuss the second option later). Specifically, we adopted the
MikroKosmos ( l K) ontology [17], a large-scale ontology containing more than 5000 hierarchically-arranged concepts (excluding instances). These broad-coverage concepts describe various domains, a desideratum for flexible experimenta-tion. The l K ontology has an average of 14 inter-concept links/node, providing rich interpretations for the defined concepts.

We adopted the DTD for the XML version of l K ontology as our standard ontology input XML DTD. Because the l K ontol-ogy is represented as a directed graph, mapping any other graph-based knowledge sources, either directly or indirectly, to l K is straightforward.
 rather has an associated lexicon base that supports the ontology matching. Since the original lexicons were not available for this work, we created our own separate lexicon base. This was done by pre-integrating the leaf concepts from the l K ontol-ogy with external lexicon dictionaries and declarative data recognizers. Most of these lexicons and data recognizers are col-lected from the Web. For example, for the ontology concept C cities of the independent countries in the world. Since we collected information from varied resources, we found that synonym identification became critical for the performance of ontology reuse. We therefore adopted WordNet synonym resource in this work; see Lesson 5 for related work involving a hierarchical terminology resource.
To the extent possible, we rely on available resources for specifying lexical content to be used in concept discovery and matching. However, we have found it useful to develop our own library of data frames (i.e. data recognizers) used for this purpose. These data frames consist of dictionaries of lexical items where enumeration is necessary; otherwise their lexical content is specified via regular expressions were possible. Associated with these definitions are contextual keywords and other informative clues that participate in string matching operations for concept matching.

Although this source ontology preparation process is quite involved, it is a one-time effort. This integrated ontology source thus become static and constant for all downstream ontology reuse applications. 3.2. Ontology reuse process
Fig. 2 shows how our system extracts an appropriate sub-domain from a larger, integrated source ontology by executing concept selection, relation retrieval, and constraint discovery. Since any ontology can be viewed as a conceptual graph, our algorithm is implemented to find nodes, edges, and specific constraints in graphs. 3.2.1. Concept selection
We have implemented the concept selection process as three concept recognition procedures (which could be executed in parallel) followed by a concept disambiguation procedure. In particular, the three recognition procedures involve matching concept names and concept values.

Concept name matching associates content in NL documents with concept names in source ontologies. We have imple-mented three concept selection strategies: S1 compare a string with the name of a concept; S2 compare a string with values belonging to a concept; and S3 apply data-frame recognizers to recognize a string.

S1 and S2 are concept recognition procedures (which could be executed in parallel) and S3 is a concept disambiguation procedure. Concept name matching associates content in NL documents with concept names in source ontologies in the preparation stage described above.

For example, consider the sentence  X  X  X fghanistan X  X  capital is Kabul and its population is 17.7 million. X  Concept matching would associate the word  X  X  X apital X  with the ontology concepts C  X  X  X abul X  with the concept C APITAL-CITY .
 The process is similar for Web page content. In Fig. 3 a, S1 matches the string  X  X  X apital X  in a Web page with the concepts C
Fig. 3 b shows the kind of matches S2 provides.  X  X  X fghanistan X  is an instance of a name of a country and  X  X  X abul X  is an in-stance of a capital city. Hence we select the object sets C
Fig. 4 illustrates S3 , which selects concepts based on regular-expression recognizers in the data-frame library. Two data frame patterns match the  X  X 17.7 million X  in the training record. One pattern associates with the concept P other associates with P RICE . Note that the knowledge sources preprocessing has already integrated the concepts in the data-frame library with the concepts in the l K ontology. We thus add the associated two l K concepts into the potential candidate list of object sets.
 Clearly, matches occasionally entail incorrect conceptual interpretations. For example, in Fig. 3 a, S1 selects both F
To resolve conflicts, we have implemented three heuristics, namely: (1) same string only one meaning , (2) favor longer over shorter , and (3) context decides meaning . We sketch each in turn.

Same string only one meaning : A given string in a document record may match more than one concept. Such a string, how-ever, can have only one meaning for each occurrence. Therefore, only one concept can be the correct selection. For our pur-ument, both S1 and S2 , or both S1 and S3 select a concept, we accept it as a correct selection. ognizes the same string then is deemed an error, and the conflict resolution procedure removes it from the list of candidate concepts. Fig. 5a illustrates this heuristic.
 Although both F INANCIAL -C APITAL and C APITAL -CITY match  X  X  X apital X  in the document via S1 , we know we cannot have both. Since C APITAL -CITY also matches  X  X  X apital X  by S2 , we accept C Fig. 5 b shows another example. Although S3 suggests both the concept P the system accepts the concept P OPULATION as the correct selection based on the additional suggestion from S1 because of the string  X  X  X opulation X .
 of B 0 , the selection process chooses concept B over A because the meaning of B 0 overrides the meaning of A 0 . In Fig. 6 a, S2 recognize the word  X  X  X ronze X  as an instance of A instance of S PORT-ARTIFACT . Because  X  X  X ronze X  is a proper substring of  X  X  X ronze medal X , the system chooses the concept S ARTIFACT instead of the concept A LLOY .

Context decides meaning : Sometimes we can use the context around a target string to identify one of multiple matching concepts for a string. Fig. 6b shows an example.

In order to decide between the potential candidates P OPULATION data-frame library as a context keyword. Since we find the keyword  X  X  X opulation X  close to the occurrence of a data value for the concept P OPULATION and do not find any context keywords of the concept P provides these contextual keywords. 3.2.2. Relation retrieval
The crux of the relation retrieval process is about finding appropriate edges between two concept nodes. An obvious res-olution is to find all possible paths between any two candidate concepts. It would be easier for users to reject inapplicable edges rather than to add new relations. But this resolution entails serious performance difficulties. Hence we must seek an alternative resolution.

Different paths in an ontology graph refer to relations with different meanings. In addressing the problem of ontology generation, it is almost never necessary to find all possible paths; for a generated ontology we typically only want one. From our studies we have found that in general a shorter path represents a closer or more straightforward relationship between two concepts. An extra-long path often means a very uncommon relation between the two concepts within the domain.
Hence it is reasonable to set a threshold length to reduce the search space and thus the complexity of the edge-searching algorithm.

In the implementation, we adapted Dijkstra X  X  algorithm [18]. Although the original algorithm computes only the shortest stra X  X  algorithm has polynomial time complexity and the threshold length is fixed and finite, the time complexity of this up-dated algorithm is also polynomial.

To illustrate, consider the concepts N ATION ,C OUNTRY-NAME along with identified instances (for the last two), namely Afghanistan and Kabul. Relation retrieval begins by choosing a node (for example N ATION ) from the l K ontology. We then perform an exhaustive and recursive search to find all reachable nodes and their distance from that node, up to some thresholded distance. When other previously identified concepts are reached, a relationship can be set up between them. For example, both C N
ATION , so corresponding relations are established. Occasionally paths of equal length may extend from a concept to more than one ancestor (or descendent) node; in such cases we arbitrarily choose the first one for practical purposes. However, the discussion in Lesson 5 focuses on why further processing is justified in such cases, and how we have in fact discovered useful information when allowing multiple paths to be pursued.
 After this edge-searching procedure, the system performs a subgraph detection procedure to finalize the target domain.
Quite often, the edge-searching procedure results in multiple unconnected subgraphs. Normally, two separate subgraphs represent two independent domains. However, since we focus only on narrow-domain applications and assume that the
NL documents represent this domain, we expect only one subgraph. Such would be the case with the concepts mentioned in the previous paragraph; only one subgraph in the political entities domain would result. Since we also assume that the NL documents are data-rich, we can assume that the largest subgraph of concepts is the subgraph of interest. This subgraph becomes the conceptual model instance for the ontology we generate. Obviously this also helps boost the accuracy and per-formance of the system by reducing the possible inventory of concepts to be processed.

We use a standard algorithm to find the largest subgraph, which is the subgraph containing the largest number of con-usually we obtain one large subgraph and either no other subgraphs or only a few other small subgraphs, each often having just one node and no edges. 3.2.3. Constraint discovery
Ontologies involve numerous types of constraints; this paper cannot possibly enumerate them all or discuss the methods for reusing constraints. For our purpose in demonstrating automated ontology reuse, we limited our study to cardinality con-which render the automatic discovery process particularly interesting. Generating many other constraints, such as hasValue (specifying value instances of particular concepts), is straightforward via our approach. Based on the data recognizers, we can immediately obtain the mapping between extracted instances and their concepts. The generation of these purely descriptive constraints is usually easier than producing constraints without precise quantitative scales such as cardinality. Hence here we address arguably the most difficult constraint generation process: cardinality constraint generation.
We have implemented a cross-counting algorithm to discover cardinality constraints from NL documents. number and a maximum number [ min : max ]. The cross-counting algorithm counts the instantiated numbers of paired con-cepts, from which the system can decide these minimum and maximum numbers. For example, suppose that in document D 1 concept A is instantiated by a 1, and there are no instantiations for concept B in the same document. In another document
D 2, however, concept A is instantiated by the same a 1 and concept B is instantiated by b 2. With these two documents, we can not always have an instance of B appearing at the same time. The details of this algorithm are presented elsewhere [14].
Fig. 7 shows an example of discovering a [1:1] participation constraint. Suppose that we have two NL documents as this figure shows. For each record, we recognize three concepts: C resolution process described earlier we determine that N ATION application. We retrieve three binary relationship sets: NATION has COUNTRY-NAME has AGRICULTURAL-PRODUCT . Since N ATION is the primary concept, the minimum participation constraint for N By construction, the primary concept appears only once in each record. The concept C each record, and for a different instance of C OUNTRY-NAME ticipation constraint for both concepts to 1. Hence the system generates the object set NAME[1:1] .

For the relationship set NATION has AGRICULTURAL-PRODUCT , the system generates the maximum participation con-straint for N ATION to be because each record contains more than one instance of A at least one instance  X  X  X heat X  in both the records. That is, the same instance of A instances of N ATION . The system therefore also generates the maximum participation constraint for A
Hence, the system generates NATION[0: ] has AGRICULTURAL-PRODUCT[1: ] For the relationship set NATION has LANGUAGE , Record 1 has three instances of L of L ANGUAGE while none of these instances is the same. Thus, each N belongs to only one N ATION . Hence, the system generates
Note that this last example is not totally correct. People know that more than one nation can use the same lan-guage. For example, both the United States and the United Kingdom use English as their native language. Significant bias may result from a lack of sufficient examples. Thus, we need a large enough collection of NL documents to gen-erate correct participation constraints. However, even a huge unsupervised selected document base may not totally solve this problem. Supervised selection of NL documents can overcome this problem, but the tradeoff requires more human effort.
 3.2.4. Ontology refinement
After finding concepts, relations, and constraints, composing them together into an ontology is straightforward. The result probably will not precisely describe the target domain, so in a final stage a human revises the resulting ontology manually. name the inappropriate content, (3) modify the incorrect content, and (4) add the missing content. In general, a preferred ontology reuse procedure will produce outputs requiring less revision operations on (3) and (4), especially the latter. It is in general much easier for users to reject unexpected components than to add something totally new into an ontology on their own initiative. Based on this refinement perspective, our ontology reuse system preserves as much useful information as possible, minimizing the need for addition by users. 4. Experiments and discussions
This section describes a series of experiments with our ontology reuse system; full details on the experimental method-ture automated ontology reuse studies.
 Lesson 1. Ontology coverage is best specified by the leaf concepts.

For ontology reuse, the coverage of an ontology is the reusable domain described by an ontology. Users for ontology reuse could conceivably believe that one can straightforwardly determine the coverage of an ontology by its root definition. For example, when the root concept of an ontology is B OOK , this ontology should cover the domain of books; when the root con-A (i.e. everything), we began our study with the expectation that we could reuse the l K ontology to describe arbitrary domains.

Our initial experiments were not promising: usually we either got no result or the composed ontologies were outside the characterized by its root definition. Instead, often ontology developers do not properly circumscribe the domain, and in such cases a significant portion of the domain is often not reusable.

More precisely, the true reusable domain or coverage of an ontology is primarily determined by the union of its leaf-level concepts, a subset of the root-specified domain. For example, if a N C simple observation, though critical for ontology reuse research, seems not to have been documented in the prior ontology reuse literature.
 Lesson 2. Extend ontology coverage with lexicons and data recognizers.

To improve the degree of reusability of existing ontologies, we want to boost the coverage of an ontology so that it is clo-new concepts can be evaluated by an ontology reuse program.

To boost the applicable coverage of our source ontology during the source-ontology preparation stage, we associated lex-icons and data recognizers with the leaf-level concepts. We have named the result  X  X  X nstance recognition semantics X , or for-mal specifications that identify instances of a concept C in ordinary text [20]. These are essential to automating ontology reuse.

We further populate the ontology with some upper-level ontology concepts. For example, prior to June 3, 2006 Monte-negro was not an independent nation, so the original l K ontology did not have a leaf concept M portion of the ontology becomes non-reusable for many situations involving Montenegro after June 3, 2006. It is a very com-plicated issue to obtain permission and then properly modify an ontology that is created by somebody else. For the purpose of automated reuse, however, we developed a simple and effective (though imperfect) alternative. We simply bind a lexicon have not formally specified Montenegro as a country in the ontology, we have rendered the original source ontology reusable for situations involving the new country Montenegro. In the new generated ontology, instead of a specific concept M EGRO as an independent nation, we can correctly generate an upper-level concept X  X  become applicable in this new generated domain ontology. With such a technique, we artificially boost the applicable cov-erage of the source ontology.

In our experiments we augmented lexicons and data recognizers for leaf-level concepts in the l K ontology and their superclasses up to 2 levels above (on average). The union of these augmented concepts and their relations composes the applicable coverage of the source ontology in our experiments.
 Lesson 3. For known target domains, ontology reuse is already possible and even valuable.

After having prepared the source ontology, we started our real experiments. Based on Lesson 1, we decided to focus our experiments on several selected domains rather than on arbitrary domains. We want human inspection to assure that the projecting domains have significant overlap with the applicable coverage of our source ontology. In particular, we chose experiments in three narrow domains: car advertisements, apartment rentals, and nation descriptions. This paper only briefly summarizes our results; see [14] for details.

First we list some basic settings and statistics of our experiments. Each of the three target domains contains a dozen to twenty concepts. For each domain, we feed four to seven cleaned sample Web pages (NL documents) to the ontology reuse system. The source ontology had been pre-integrated and augmented by its applicable coverage. In order to evaluate the per-formance of our outputs, we had human experts separately create ontologies for each target domain. We adopted the hu-man-created ontologies as a gold standard to which the automatically generated ontologies were compared for precision and recall.

In general, we obtained low precision results. In the three target domains, the best precision was 48% for concept gener-ation, 14% for relation generation, and 10% for cardinality constraint generation. The news is not all bad. Low precision im-plies the need for more rejections of corresponding components within a generated ontology. For humans, as mentioned earlier, rejecting inappropriate ontology components is much easier than adding new ontology ones. Hence our strategy is to favor greater recall values (i.e. less addition) over greater precision values (i.e. less rejection).
We updated the traditional recall calculation equation as follows: where the numerator is the number of component types (i.e. either concept, relationship, or constraint) correctly reused in a generated ontology; the denominator is the number of component types contained in input sources (both from NL docu-ments and source ontologies). We use this formula because not everything defined in the human-created ontology is also identifiable by the inputs. For example, human experts defined a concept F from the source l K ontology. Hence it is impossible for a system to reuse an non-pre-existing concept. To be more equitable, our recall calculation must eliminate this type of error.

With the new formula, in the three test domains our worst recall values were 83% (concept generation), 50% (relation generation), and 50% (cardinality constraint generation). All the best recall values were close or equal to 100%. Our ontology reuse system performs quite well even though it still is a prototype. The recall values show that we may reduce at least half of the human effort in ontology construction through ontology reuse when a target ontology is properly contained in the applicable coverage of the source ontology. Considering the expense of training professional ontologists and the time they need to build and tune ontologies, 50% already represents substantial savings. There are many ways to further improve the performance of the system. Already, though, our experiments demonstrate that ontology reuse is no longer  X  X  X ar from an automated process X  [2].
 Lesson 4. Ontology modularization facilitates and accelerates automated ontology reuse.

During our experiments, another metric studied was running time. In general the system took about 1000 s to resolve all the ontology components with respect to about 50 X 100 candidate concepts on a Pentium 800 MHz single processor machine.
This execution time is rather short compared to the time required for manually creating an ontology of the same scale. Our benchmark showed that almost 90% of the execution time was spent on the relation retrieval process. Though we may fur-ther improve this time performance by optimizing our implementation, the problem lies mainly in the magnitude of the source ontology (over 5000 concepts and over 70,000 relationships to explore).

Reducing the execution time of relation retrieval should be possible by using modular ontologies rather than a single large-scale one. Modular ontologies are usually small and designed to be self-contained. An ontology module is self-con-tained if all of its defined concepts are specified in terms of other concepts in the module, and do not reference any other concepts outside the module. As soon as several major concepts in a module are selected as candidate concepts, an ontology reuse system may decide to directly reuse the entire module rather than perform a costly relation retrieval algorithm. Hence the execution time for relation retrieval can be significantly reduced.

To pursue this issue, we manually pruned several comparatively independent clusters of ontology components from our source ontology and used them as individual modules. Since these clusters originated from a previously unified ontology, we did not need to further integrate them. The same experiments were re-run with these multiple  X  X  X odular X  ontologies. On average the system took less than 300 s X  X aving more than 70% of run time X  X o resolve all the ontology components for about 50 X 100 candidate concepts. Because these pruned clusters were not true, self-contained modular ontologies, the perfor-mance in terms of precision and recall decreased in this experiment. Saving execution time by replacing a large unified ontol-ogy with multiple small modular ontologies is thus a convincing strategy. By using truly well-designed modular ontologies, our ontology reuse system achieves both higher precision and recall values, as well as faster run-time performance. Lesson 5. Sample documents may help us mine  X  X  X atent X  knowledge from text documents.

We also carefully studied our low-precision experimental results. Many reused concepts and relations that were gener-ated fell beyond the scope of the expert-created ontologies. Yet they were not all meaningless or useless. On the contrary, we found that useful information X  X atent in the document but beyond the topic directly at hand X  X ould be gleaned from the re-sults. Such latent information is not really what people cannot find, but is easily overlooked by human readers. We believe that automated ontology generation may provide various industrial and research communities an attractive solution for seeking valuable latent information.
Of course, the generation of reasonable ontologies depends crucially on how well the knowledge base addresses the do-main of interest. A perfect fit is not likely, so we rely on extensible knowledge bases which allow ontology builders to gen-erate ontologies even in the presence of only partial coverage. In this section we sketch an experiment that explored extending the default l K ontology with two other knowledge sources and testing it on a novel application domain.
One resource was Eurodicautom, a large-scale terminology bank we only focused on the English terms, discarding those in other languages. Since Eurodicautom was not in XML format at the time, we first converted entries into the TBX lexicon/termbase structure. 7
The TBX termbase itself is not helpful for this task, though, because it merely defines terms with only minimal hierarchi-cal or relationship information. However, an associated resource called Lenoch places each Eurodicautom term into a more completely specified conceptual hierarchy. Because Lenoch codes define a hierarchy for Eurodicautom terms, we were able to integrate them together and arrange them in the l K XML format. Fig. 8 shows some examples of Lenoch codes, and Fig. 8 shows corresponding samples from the l K XML encoding. More details on the Eurodicautom, Lenoch, and l K integration are documented elsewhere [21].

We ran our tool on various US Department of Energy (DOE) abstracts vance was only interested in the generic information about these abstracts, such as the theme of a document and the number of erated some concepts and relations indicating that crude oil prices dropped in the year 1986. Although this was not what the information could be very valuable. Fig. 9 shows a very small portion of the output ontology generated by the system using the updated knowledge base.

Within the generated ontology, of course, there are several dozen posited relationship sets; some are spurious but in fact several are correct. For example, a novel relationship is appropriately posited between the concept C concept introduced via the integration of Eurodicautom) and the concept N and capture of such interactions between concepts is possible via the procedure outlined in this paper. It should be noted that the focus of this experiment was to test whether the system could properly integrate new sources and generate mean-ingful relationships between the new added concepts and the original ones. A larger question is whether the DOE abstracts are well suited for treatment by the system, and here we are skeptical since they do not contain extensive data-rich, narrow-domain information that we assume as input. 5. Conclusion and future work
We have presented an automated ontology reuse approach. Although we only applied our system to reuse the l K ontol-ogy, our methodology supports automated ontology reuse in general. Informed by our experiments on real-world examples, we have summarized five lessons that are constructive for future exploration of ontology reuse studies. In essence, we con-clude that ontology reuse is no longer  X  X  X ar from an automated process X  [2].

In the meantime, a few critical problems remain to be solved. One is to automatically decide whether a target domain is within the reusable coverage of an integrated source ontology. If the majority of a target domain lies outside the source ontology, ontology reuse becomes nothing but extra overhead.

We also need to experiment with applying modular ontologies for ontology reuse. Until now, the research on modular ontologies is still at the stage of theoretical analysis. We need practical study cases to push this research field forward.
The study of instance recognition semantics should be paired with modular ontology research to improve the reusability of modular ontologies.
 Logic-based approaches have been used with other ontologies; Grau et al. [22] extract particular modules within existing
OWL ontologies by detecting the internal logic of the ontology based on derivational proofs involving the notion of syntactic locality . Their experiments with large, well-known ontologies such as the Gene Ontology yield scores of 85% accuracy for concept reuse in general cases. Though the scoring criteria are somewhat unclear and there logic approach with our own data extraction ontology reuse system to combine the benefits of both techniques.
Last but not least, mining latent information through ontology reuse is an interesting research topic. More exploration on this topic may bring many benefits to users, especially in the business domain.

So far there are few published studies on automated ontology reuse research. We hope that our results draw more atten-tion to this field of research.
 Acknowledgements
This work was funded in part by US National Science Foundation Information and Intelligent Systems grants for the TIDIE (IIS-0083127) and TANGO (IIS-0414644) projects. Part of the work was also supported by the European Commission under the projects DIP (FP6-507483), SUPER (FP6-026850), and MUSING (FP6-027097), by the Austrian BMVIT/FFG under the FIT-IT Semantic Systems project myOntology (Grant No. 812515/9284), and by a Young Researcher X  X  Grant from the University of Innsbruck. We are also grateful to Sergei Nirenburg for providing a copy of l K for this work.

References
