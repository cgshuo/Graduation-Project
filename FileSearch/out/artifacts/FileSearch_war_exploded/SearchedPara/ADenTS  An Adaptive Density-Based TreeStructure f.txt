 Answering aggregate queries in data cubes approximately and efficiently is one of the most essential techniques for data warehousing and data mining. Consider a database of a charitable organization with 2 dimensions age and salary. Each data point in the age-salary space corresponds to a donator and the attribute value represents the amount of money the person donated. Moreover, consider for instance in a 3-dimensional geographical database, geologists may be particu-larly interested in a geological feature such as readings of snowfall or wind speed at different locations. Typically, aggregate functions include COUNT, SUM, AV-ERAGE, MAX, MIN, MEDIAN, etc.
 the course of cube modelling. But real applications often call for methods that can support different aggregations in a single cube. For example, for the database of charitable organization mentioned above, we may have aggregate queries as follows: what is the total number of people whose age is between 40 and 50 , whose salary is above $10 K , and whose donation is between $1 K and $2 K and please give the average amount of money donated by people whose donation falls in the range $5 K  X  $10 K and $15 K  X  $20 K .
 structure by pre-computation of data cubes. Several techniques are presented for efficient aggregate query approximation. Generally speaking, our method has the following properties: (1) it is able to answer a variety of types of aggregate queries within a single data cube; (2) while most traditional algorithms in this category do not allow a user to specify a query range over the value dimension, our algorithm does not have such constraint; (3) empirical evidence indicates that our method leads to good accuracy in query processing.
 related work in this area. Section 3 and 4 include the algorithms for tree construction and query approximation. Next we discuss in section 5 a heuris-tic approach for the tree construction. Experimental results are presented in Section 6 . Existing algorithms for aggregate query approximation include quadtree struc-techniques [ 5 ], and wavelet decomposition [ 1 , 6 ].
 R-tree like) multi-dimensional index structure that answers aggregate queries like COUNT, SUM, MIN, MAX, AVG by selectively traversing the nodes of the tree in a top-down fashion. Nodes at different levels of the tree correspond to space partitioning in different granularities. An essential idea of the approach is that it provides 100% intervals of confidence on the value of the aggregate. Furthermore, the algorithm introduces a tree node traversal policy in order to reduce the uncertainty of the aggregate value as fast as possible. However, this algorithm sometimes cannot provide good aggregate estimation because the interval of confidence is too wide and it suffers from information redundancy.
 ability density function or kernel density estimators to represent a data set. For example, gaussian mixture model [ 5 ] is utilized for such representation. Given a fixed gaussian model number k , the algorithm generates k clusters represented by gaussian models based on the data set. A high compression ratio is achieved. Nevertheless, the types of queries it can answer are limited and the quality of the model is completely dependent upon the similarity between the actual prob-ability distribution of the points and gaussian distribution.
 sity of multi-dimensional datasets, i.e. to answer COUNT queries. It creates buckets of variable size and allows them to overlap. In order to make the data points in a bucket uniformly distributed, the algorithm removes a proper num-ber of points from the dense grids and therefore the density of the data point in space becomes smoother. The method is able to achieve high accuracy while storing comparatively small number of values. To the best of our knowledge, our method, unlike all previous ones, addresses the problem of answering multiple types of aggregate queries in a single data cube by maintaining an effective index structure. 3.1 Basic Definitions and Notations We denote a d -dimensional data set consisting of n data points to be D ( n, d ). R loc . We also regard P d  X  1 as in the R value domain, representing the value at-tribute associated with the point. Generally, users are interested in the value attribute and therefore aggregate queries are performed over this dimension. maintain five fields: Area , Count , Max , Min , and Distrb . Given a tree node N , We have Area ( N ) corresponds to a (hyper-)rectangular area and Area ( N )  X  R loc . We denote PSet ( N ) as a set of data points associated with node N . PSet ( N ) is a subset of the data points falling in Area(N). How PSet is calcu-lated will be discussed in the following section. Formally, we have the followings: P  X  PSet ( N ) } . For a node N , we also maintain Distrb ( N ) in order to store the probability distribution function over the value dimension R value of PSet ( N ) . 3.2 Algorithm The main algorithm for building up the tree structure can be divided into two major steps. The first step is the adaptive construction of density binary tree, which updates the Area field for every node. The second step is to iteratively update all tree nodes with Count , Min , Max , and Distrb . Adaptive Construction of the Density Binary Tree. Two parameters h and p are specified initially. Here h represents the maximum height of the tree and p specifies a density threshold for the creation of a new node. The outline of the step is shown in figure 3 .
 Bottom-Up Updating. Nodes at a same level have two properties: (1) they have same shape (figure 1 ); (2) their density Count is greater than the threshold. Thus for any node N , Area ( N ) can be regarded as one of the populated grids picked up from a particular partitioning of the ( d  X  1)-dimensional space R loc . be derived from GENHIST [ 2 ]. Given a populated grid N , we compare its den-sity with their surrounding ones. If its density is higher than its neighbors, we randomly remove a certain number of data points from Area ( N ) to make sure that the density of the remaining points in Area ( N ) is equal to the average density of the surrounding grids. We call this process as  X  X igh Bump Removal X  (figure 2). Intuitively, the dense areas become sparser and the density of the new data set comes to be smoother in R loc . After we obtain PSet for the node N , Count , Min , Max , and Distrb can be easily acquired according to the previ-ous definitions. The distribution function can be specified either by users or by implementing unbiased parameter estimation. An aggregate query consists of three parts: (1) a query type identification specify-ing the type of aggregate query over the value dimension, (2) a ( d  X  1)-dimensional user query region Q (  X  R loc ), and (3) a specified range of the value dimension denoted as T (  X  R value ). For example, for the charitable organization database, a query can be { COUNT, Q (40  X  50 , 10 K +) , T (1 K  X  2 K ) } . 4.1 COUNT There are four kinds of relation between Q and Area ( N ( i ) ) [ 4 ], as shown in figure 5 . For case (d), we simply return 0 and stop traversing. Otherwise, based in the R loc space, an estimated aggregate value E ( i ) count corresponding to the is returned as the approximation of the COUNT query.
 4.2 SUM and AVERAGE The SUM and AVERAGE queries differ from the COUNT queries in that dif-ferent estimated aggregate values are calculated. As for SUM, we have E ( i ) sum = R 4.3 MAX and MIN Due to the continuous property of the value dimension, it is unnecessary to specify a range T for MAX or MIN queries. Without loss of generality, we discuss how MAX queries are answered.
 case (d). Otherwise, when no correct answer can be guaranteed, the problem of expected maximum value among the m values? Assume that the n data points have value V 1 ,  X  X  X  , V n and V i  X  V i +1 (1  X  i  X  n ), then a straightforward solution However, such computational cost is extremely high. Note that the E ( i ) max ( m ) is a function that monotonically increases with respect to m , and E ( i ) max (1) = E max ( max ( V 1 )) = E tion information stored in each node into consideration and does not require us to pre-compute V 1 ,  X  X  X  , V n . The final approximation for the MAX query is E max = max { E 4.4 MEDIAN Probability distribution information is very useful in the approximation of many other types of aggregate queries. Typically, we discuss how MEDIAN queries R The goal of  X  X igh bump removal X  process in algorithm ADTreeUpdate is to improve the uniformity of the density of the whole data set. Nevertheless, if two children nodes with a same parent are denser than their neighboring grids, there will be interference between the two corresponding adjacent high bumps. To minimize the problem, we take the advantage of the binary tree structure and update every pair of adjacent dense children simultaneously instead of updating them one by one. This process is called  X  X ouble-bump removal X , as shown in figure 7 .
 depicted in figure 6 . Suppose that N 1 and N 2 are two nodes with the same parent. The original algorithm will result in Count ( N 1 ) = 1000  X  200  X  7+600 will be better with Count ( N 1 ) = 800 and Count ( N 2 ) = 400. 6.1 Methodology We applied ADenTS to real database obtained from the US Forest Cover Type and synthesized query workloads. The implementation of our algorithm is on an IBM 1.5GHz CPU and 256MB of DDR main memory with Windows XP and Microsoft Visual C++ 6.0.
 and each supported type of query, 1000 random queries are created with average selectivity 1% to form a query workload. Queries with data point selectivity of less than 0 . 1% are disregarded because small selectivity would seriously degen-erates the effectiveness of all algorithms. The accuracy is measured by relative error. Note that the relative error of MIN, MAX queries should be calculated by RelativeError = 6.2 Comparison with MRA-Tree and GENHIST Due to the apparent difference of the type of queries three algorithms can sup-port, we categorize the experimental results into three groups according to: (1) queries that can be only answered by ADenTS ; (2) queries that can be an-swered by both ADenTS and MRA-Tree ; and (3) queries that can be answered by ADenTS and GENHIST . accuracy for the 3-and 4-dimeniosnal datasets and all supported queries with ADenTS , i.e. COUNT, SUM, AVG, MAX, MIN, MEDIAN. For case (2), it is stated that MRA-Tree does not support queries with a specified value range and that it does not support MEDIAN queries, either. Therefore, we generate four types of queries with no specified range over the value dimension so that they can be answered by MRA-Tree . In figure 11 we demonstrate that for COUNT, SUM, AVG, and MAX queries, our method can produce results better than MRA-Tree in 3 to 5 dimensional data sets. This is due to the fact that in ADenTS the uniform assumption can be better satisfied than in the simple grid partitioning in MRA-Tree . Furthermore, unlike MRA-Tree , the data points related to the nodes in ADenTS are disjoint and contains no information redundancy. In case (3), GENHIST can answer only COUNT queries. The way our method approximate COUNT queries is essentially similar to GENHIST with respect to the bump removing procedure. However, because ADenTS introduces the process of double-bump removal, it outperforms GENHIST slightly, as shown in figure 9 . We also present the relation of the number of tree nodes with respect to the splitting density threshold  X  . The three curves drawn in figure 10 display that, as the splitting density threshold is decreased, more and more nodes with relatively less data points will be created. In this paper, we presented ADenTS , an effective tree structure for aggregate query answering in multi-dimensional data cubes. The main target of our method is to support various types of aggregate queries without loss of accuracy. Briefly speaking, The tree is built through an adaptive density-based approach in top-down and bottom up fashion. Our method inherits the advantages of both the Multi-Resolution Aggregate tree ( MRA-Tree ) structure and the histogram tech-nique GENHIST . In addition, it outperforms them by answering more kinds of queries within a single data structure and meanwhile achieving good accuracy, which can be well demonstrated in the experimental evaluation.

