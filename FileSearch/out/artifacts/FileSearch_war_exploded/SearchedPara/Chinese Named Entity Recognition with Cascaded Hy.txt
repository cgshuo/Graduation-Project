 Named entity recognition (NER) involves the identifica-tion and classification of certain proper nouns in text, such as person names (PERs), locations (LOCs), orga-nizations (ORGs), miscellaneous names (MISCs), tem-poral, numerical and monetary phrases. It is a well-established task in the NLP community and is regarded as crucial technology for many NLP applications, such as information extraction, question answering, information retrieval and machine translation.

Compared to European-language NER, Chinese NER seems to be more difficult (Yu et al. , 2006). Recent ap-proaches to Chinese NER are a shift away from man-ually constructed rules or finite state patterns towards machine learning or statistical methods. However, rule-based NER systems lack robustness and portability, and machine learning approaches might be unsatisfactory to learn linguistic information in Chinese NEs. In fact, Chinese NEs have distinct linguistic characteristics in their composition and human beings usually use prior knowledge to recognize NEs. For example, about 365 of the highest frequently used surnames cover 99% Chi-nese surnames (Sun et al. , 1995). For the LOC  X   X  /Beijing City X ,  X   X  /Beijing X  is the name part and ing them into a highly accurate classifier. Each weak clas-sifier searches for the hypothesis in the hypotheses space that can best classify the current set of training examples. Based on the evaluation of each iteration, the algorithm re-weights the training examples, forcing the newly gen-erated weak classifier to give higher weights to the exam-ples that are misclassified in the previous iteration. We use the AdaBoost.MH algorithm (Schapire and Singer, 1999) as shown in Figure 1, an n-ary classi-fication variant of the original well-known binary Ad-aBoost algorithm (Freund and Schapire, 1997). It has been demonstrated that Boosting can be used to build language-independent NER models that perform excep-tionally well (Wu et al. (2002), Wu et al. (2004), Carreras et al. (2002)). In particular, reasonable Chinese NER results were still obtained using Boosting, even though there was no Chinese-specific tuning and the model was only trained on one-third of the provided corpora in SIGHAN bakeoff-3 (Yu et al. , 2006). A Markov Network (also known as Markov Random Field) is a model for the joint distribution of a set of variables (Pearl, 1988). It is composed of an undirected graph and a set of potential functions. A First-Order Knowledge Base (KB) (Genesereth and Nislsson, 1987) is a set of sentences or formulas in first-order logic. A Markov Logic Network (MLN) (Richardson and Domin-gos, 2006) is a KB with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it species a ground Markov Net-work containing one feature for each possible grounding of a first-order formula in the KB, with the correspond-ing weight. The weights associated with the formulas in an MLN jointly determine the probabilities of those for-mulas (and vice versa) via a log-linear model . An MLN defines a probability distribution over Herbrand interpre-tations (possible worlds), and can be thought of as a tem-plate for constructing Markov Networks. The probabil-ity distribution over possible worlds x specified by the ground Markov Network M L,C is given by P ( X = x ) = where F i is the formula in first-order logic, w i is a real number, n i ( x ) is the number of true groundings of F i in x , x { i } is the true value of the atoms appearing in F i , and  X  3.1 Learning Weights Given a relational database, MLN weights can in princi-ple be learned generatively by maximizing the likelihood of this database. The gradient of the log-likelihood with where  X  F i is the set of worlds where F i holds, and P ( x | M L,C ) is given by Equation 1. The ques-tion of whether a knowledge base entails a formula F in first-order logic is the question of whether P ( F | L
KB ,C KB ,F ) = 1 , where L KB is the MLN obtained by assigning infinite weight to all the formulas in KB , and C KB ,F is the set of all constants appearing in KB or F .
The most widely used approximate solution to proba-bilistic inference in MLNs is Markov chain Monte Carlo (MCMC) (Gilks et al. , 1996). In this framework, the Gibbs sampling algorithm is to generate an instance from the distribution of each variable in turn, conditional on the current values of the other variables. One way to speed up Gibbs sampling is by Simulated Tempering (Marinari and Parisi, 1992), which performs simulation in a gener-alized ensemble , and can rapidly achieve an equilibrium state. Poon and Domingos (2006) proposed MC-SAT, an inference algorithm that combines ideas from MCMC and satisfiability. Even though the Boosting model is able to accommodate a large number of features, some NEs, especially LOCs, ORGs and MISCs are difficult to identify due to lack of linguistic knowledge. For example, some ORGs are possibly mistagged as LOCs and/or MISCs. We incor-porate heuristic human knowledge via MLNs to validate the Boosting NER hypotheses. We extract 151 location salient words and 783 organization salient words from the LDC Chinese-English bi-directional NE lists compiled from Xinhua News database. We also make a punctua-tion list which contains 19 items. We make the following assumptions to validate the Boosting results:  X  Obviously, if a tagged entity ends with a location  X  If a tagged entity is close to a subsequent location  X  Heuristically, if a series of consecutive tagged en- X  Similarly, if there exists a series of consecutive  X  Entity length restriction: all kinds of tagged entities We extracted all the distinct NEs (4,475 PERs, 2,170 LOCs, 2,823 ORGs and 614 MISCs) from the 15,000-sentence training corpus. An MLN training database, which consists of 10 predicates and 44,810 ground atoms was built. A ground atom is an atomic formula all of whose arguments are ground terms (terms con-taining no variables). For example, the ground atom location(  X  ) conveys that  X   X  /Beijing City X  is a LOC.
 During MLN learning, each formula is converted to Conjunctive Normal Form (CNF), and a weight is learned for each of its clauses. The weight of a clause is used as the mean of a Gaussian prior for the learned weight. These weights reflect how often the clauses are actually observed in the training data.
 We validated 352 Boosting results to construct the MLN testing database, which contains 1,285 entries and these entries are used as evidence for inference. Infer-ence is performed by grounding the minimal subset of the network required for answering the query predicates. We applied 3 MCMC algorithms: Gibbs sampling (GS), MC-SAT and Simulated Tempering (ST) for inference and the comparative NER results are shown in Table 1. The cascaded hybrid model greatly outperforms the Boosting model. We obtained the same results using GS and ST algorithms. And GS (or ST) yields slightly better results than the MC-SAT algorithm. In this paper we propose a cascaded hybrid model for Chinese NER. We incorporate human heuristics via MLNs, which produce a set of weighted first-order clauses to validate Boosting NER hypotheses. To the best of our knowledge, this is the first attempt at using MLNs for the NER problem in the NLP community. Experi-ments on People X  X  Daily corpus illustrate the promise of our approach. Directions for future work include learning the structure of MLNs automatically and using MLNs for information extraction and statistical relational learning (e.g., entity relation identification).

