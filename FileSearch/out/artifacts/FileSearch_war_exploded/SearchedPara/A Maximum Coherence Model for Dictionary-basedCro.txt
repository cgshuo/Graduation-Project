 One key to cross-language information retrieval is how to efficiently resolve the translation ambiguity of queries given their short length. This problem is even more challeng-ing when only bilingual dictionaries are available, which is the focus of this paper. In the previous research of cross-language information retrieval using bilingual dictionaries, the word co-occurrence statistics is used to determine the most likely translations of queries. In this paper, we pro-pose a novel statistical model, named  X  X aximum coher-ence model X , which estimates the translation probabilities of query words that are consistent with the word co-occurrence statistics. Unlike the previous work, where a binary de-cision is made for the selection of translations, the new model maintains the uncertainty in translating query words when their sense ambiguity is difficult to resolve. Further-more, this new model is able to estimate translations of multiple query words simultaneously. This is in contrast to many previous approaches where translations of individ-ual query words are determined independently. Empirical studies with TREC datasets have shown that the maximum coherence model achieves a relative 10% -40% improvement in cross-language information retrieval, comparing to other approaches that also use word co-occurrence statistics for sense disambiguation.
 H.3.3 [ Information storage and Retrieval ]: Information Search and Retrieval X  Retrieval Models Algorithms, Performance, Experimentation maximum coherence model, co-occurrence statistics, cross-language information retrieval Copyright 2005 ACM 1-59593-034-5/05/0008 ... $ 5.00.
To overcome the language barrier in cross-language infor-mation retrieval (CLIR), either queries or documents are translated into the language of their counterparts. Usu-ally it is simpler and more efficient to translate queries than to translate documents because queries are generally much shorter than documents. Given its short length, how to disambiguate translations of a query has become a challeng-ing problem in cross-language information retrieval. Many approaches, such as statistical translation models [7, 11, 22] and relevance language models [12, 13, 14, 17], rely on parallel bilingual corpora for query translation disambigua-tion. Often, they learn an association between the words in the language of queries and the language of documents from a bilingual corpus, and apply the association to dis-ambiguate translations of queries. However, it is usually not only time consuming but also expensive to acquire large parallel bilingual corpora, particularly for minor languages. Due to the increasing availability of machine readable dic-tionaries, much of the research effort in CLIR has been put into the dictionary-based approaches.

The simplest approach toward dictionary-based CLIR is to use all the translations of query words provided by the dictionary equally [5, 6]. This amounts to no sense disam-biguation for query words. Other approaches try to resolve the translation ambiguity by measuring the coherence of a translation word to the entire query. Typically, the coher-ence score of a translation word is computed using word co-occurrence statistics. Given a query, a translation of a query word is assigned with a high coherence score when it co-occurs frequently with the translations of other query words. The selection of translation words are then deter-mined by their coherence scores: in approaches [2, 5, 6, 8, 10, 12], for each query word, only the translation with the highest coherence score is selected; in approaches [15, 16], a translation word is selected when its coherence score exceeds a certain threshold. In both approaches, a selection strategy is used, namely for each query word, a binary decision has to be made as to which translation(s) of the word should be used for the translation of the query. Given the short length of queries and the large variance existing in mapping information across different languages, such binary decisions are usually difficult, if not impossible, to make. We call this problem  X  translation uncertainty problem  X . Another prob-lem with the selection-based approaches is that the transla-tion of one query word is usually determined independently from the translations of others, which we call  X  translation independence assumption  X . This assumption is reflected in the calculation of coherence scores. Usually, the coherence score of a translation word is computed as the sum of simi-larities to all the translations of query words provided by the dictionary. As a result, coherence scores are estimated inde-pendently from the choice of translations for query words, which leads the selection of translations for different query words to be independent.

In this paper, we propose a novel statistical model, named  X  maximum coherence model  X . It estimates the transla-tion probabilities of query words by maximizing the overall coherence of the corresponding query, which we call  X  maxi-mum coherence principle  X . In particular, the proposed model explicitly addresses the two problems mentioned above: to resolve the translation uncertainty problem, the maximum coherence model maintains the uncertainty in translating queries through the estimation of translation probabilities for query words; to drop the translation independence as-sumption, the new model estimates the translation prob-abilities for all query words simultaneously. To speed up the computation, a quadratic programming technique is em-ployed to efficiently solve the related optimization problem. Our empirical studies with TREC datasets have shown that the maximum coherence model outperforms the selection-based approaches with relative improvements ranging from 10% to 40%.

The rest of the paper is structured as follows: Section 2 briefly reviews the related work in selection-based ap-proaches for query translation disambiguation. Section 3 describes our maximum coherence model, and the proce-dure for solving the related optimization problem. Section 4 presents the experimental results. Section 5 concludes this work.
One of the major factors that can potentially degrade the effectiveness of dictionary-based cross-language information retrieval is the ambiguity in translating query words [3, 8]. In the efforts to resolve this translation ambiguity, several recent studies [2, 5, 6, 8, 10, 12, 15, 16] have suggested the strategy of translation selection by exploiting word co-occurrence patterns. Usually a similarity measurement be-tween two translation words is defined in the form of word co-occurrence statistics. With the word similarities, we can then measure the coherence of a translation word with re-gard to a query. Only translation words with high coherence scores will be selected for the translation of the query.
Ideally, for each query word, we should select the trans-lation(s) that is consistent with the selected translations for other query words. Apparently, this becomes a  X  X hicken-egg X  problem since the selection of translations for one word is determined by the translations selected for other words. Thus, due to the computational concern, most selection-based approaches [1, 8, 9] adopted an approximate solution. For each query word, it selects the translation that is most consistent with all the translations provided by the dictio-nary for all query words, including both the selected and the unselected translations. Typically, a translation selec-tion strategy can be formulated into the following algorithm: Approximate Translation Selection Algorithm 2. For each set S i
The definition of similarity between two words in the above algorithm can take various forms of co-occurrence statistics, such as Dice similarity (as in [1]), mutual information (as in [15, 16]) or its variants (as in [8, 9]). In addition to select-ing the most likely translation for each query word, other selection-based approaches have been tried, such as select-ing the best N translations [6] and selecting translations by a predefined threshold [15, 16].
 Apparently the above approximate algorithm is not ideal. In particular, the coherence score for a translation is com-puted with regard to both selected and unselected transla-tions. As a result of such an approximation, translation of different query words are determined independently, which leads to the translation independence problem as discussed in the introduction section. In the proposed model, by for-mulating the problem of translation selection into a quadratic programming problem, we are able to efficiently estimate the translations for all query words simultaneously . Fur-thermore, in contrast to the selection-based approaches that make binary decision for each translation, the new model employs soft probabilities for representing both selected and unselected translations. This is particularly useful when bi-nary decisions are hard to make, for instance, all the trans-lations of a query word have very similar coherence scores.
The essential idea of the maximum coherence model is to learn a set of translation probabilities for query words from word co-occurrence statistics that maximizes the over-all coherence of the corresponding query. This is referred as  X  X aximum coherence principle X . In the following subsec-tions, we will first describe the proposed statistical model and the definition of the overall coherence for a query, fol-lowed by a description of the procedure that solves the re-lated optimization problem efficiently.

Before starting the discussion of the proposed model, we would like to introduce the notations that is used throughout this paper. Similar to other CLIR papers,  X  X ource language X  refers to the language of queries, and  X  X arget language X  refers to the language of documents. In order to differentiate the source language from the target language, a superscript s is used for any variable related to the source language and a superscript t is used for any variable related to the target language. Let a query of the source language be denoted tinct words in q s . Let m t be the total number of distinct translations provided by the dictionary for all the words in query q s . Let matrix T represent the part of the bilingual dictionary related to query q s , i.e., T = [ t k,j ] m s  X  m t element t k,j in T is 1 if the j -th word of the target language appears as a translation in the dictionary for the k -th word in the source language and 0 otherwise. Also we use r k to denote the set of translations provided by the dictionary for a word w s k in the query q s .
To address the problem of translation uncertainty, the new model introduces translation probabilities to capture the un-certainty in translating queries.

Let p k,j denote the probability of translating a word w s the source language into a word w t j in the target language, given the context of query q s . It is defined as which satisfies By aggregating the translation probabilities for all the words in q s , we can define a matrix for translation probabilities:
With the translation probabilities p k,j , we can now define a statistical retrieval model for CLIR [11, 13]. In particular, we estimate Pr( d t | q s ), i.e., the probability for a document d in the target language to be relevant to a query q s in the source language. By the Bayes X  law, this probability can be approximated as The last step is based on the assumption that document prior Pr( d t ) follows a uniform distribution.

Taking the logarithm of Pr( d t | q s ), we have log Pr( d t | q s )  X  log Pr( q s | d t )
Here Pr( w t | d t ) is a monolingual language model for docu-ment d t in the target language; Pr( w t | w s ) is the probability for translating query word w s into w t ; and Pr( w s | q monolingual language model for query q s in the source lan-guage, which can also be seen as the weight assigned to the query word w s . For the sake of simplicity, we assume equal weights for all query words in the source language .
The crucial part of our model is to determine the transla-tion probabilities for a given query. To this end, we propose the maximum coherence model that automatically learns translation probabilities for query words from word co-occur-rence statistics. The key of this learning procedure is to first define the overall coherence for a query, and then efficiently identify the set of translation probabilities that maximizes the overall coherence measurement. Using the translation probabilities introduced in the previous subsection, we can now define a probabilistic measurement for the overall co-herence for a query q s , i.e., where s j,j 0 is a similarity measurement between word w t and w t j 0 that can be derived from word co-occurrence statis-tics. In the previous studies of selection-based approaches, several metrics have been used for similarity s j,j 0 , including the mutual information [8, 16], or the Dice similarity [1, 2]. In this paper, we adopt the mutual information metric for similarity measurement, which is defined as Pr( w t j ) is the unigram probability for word w t j , and Pr( w is the joint probabilities for word w t j and w t j 0 to co-occur in same documents. Both probabilities can be acquired by simply counting the term frequency of single words and the frequency of co-occurrence between two words.

Using the matrix notation, the expression for the overall coherence can be simplified as where e = [1 , 1 , . . . , 1] T and S = [ s t j,j 0 ] m t  X  m t principle of maximum coherence, the optimal set of trans-lation probabilities is acquired by maximizing the overall coherence Co ( q s ; T ), i.e., To avoid unstable results, similar to logistic regression [18] and support vector machine [4], a regularizer is introduced into the above objective function, which is expressed as where 1 is a matrix whose elements are all 1. Trace ( PP T P Similar to the uninformative priors used in the Bayesian learning, the goal of this regularizer is to reflect our prior knowledge of translation probabilities  X  without context, we assume that all translations provided by a bilingual dic-tionary are equally likely to be selected. By combining the regularizer with the coherence measurement, we now have a regularized optimization problem, i.e., where C p is a constant that balances the contribution be-tween the coherence measurement and the regularizer. It is determined empirically in our experiments. Note that, in the above formalization, the translation probabilities for all query words are estimated simultaneously through the computation of P . This is in contrast to the selection-based approaches, in which the selection of translations for indi-vidual query words are determined independently.
The optimization problem in (11) is in fact a standard quadratic programming (QP) problem [19]. To write it in an explicit QP form, we define where  X  represents kronecker product . It is easy to derive that Using Equation (12) -(18), the optimization problem in (11) can be rewritten in a standard form of the QP problem The quadratic programming is a well studied optimization problem and can be solved efficiently. In our experiment, we use the QP package in MATLAB [21].
For the QP problem formulated in (19), the problem size appears to be large because the number of variables in vector q is m s m t , i.e., the product between the number of unique query words and the number of distinct translation words provided by the dictionary. But, notice that in the con-straint (21), q , i.e., the upper bound of translation proba-bilities, is a concatenation of translation vectors t i obtained from dictionary T . Given that most query words only have a few translations, most of the elements in the bilingual dic-tionary T will be zeros. As a result, most elements in the upper bound vector q are zeros, which leads to the zero values for the corresponding translation probabilities in q . Hence, the number of non-zero translation probabilities in q is no more than the total number of translations provided by the bilingual dictionary for the query words, which is usually much smaller than the product m s m t . Thus, the computa-tion cost of the maximum coherence model is modest for real CLIR practice, if not overestimated.
The goal of this experiment is to examine the effective-ness of the proposed model for cross-language information retrieval. In particular, three research questions will be ad-dressed in this empirical study: 1. Is the proposed maximum coherence model effective for 2. How important is it for a query disambiguation algo-3. How important is it to remove the translation indepen-
All our experiments are retrieval of English documents using Chinese queries. The document collections used in this experiment are from TREC ad hoc test collections, including AP88-89 164,835 documents from Associated Press(1988, WSJ87-88 83,480 documents from Wall Street Journal (1987, DOE1-2 226,087 documents from Department of Energy
In addition to the homogeneous collections listed above, we also tested the proposed model against heterogeneous collections that are formed by combining multiple homoge-neous collections. In particular, two heterogeneous collec-tions are created: collection AP88-89 + WSJ87-88, and col-lection AP89 + WSJ87-88 + DOE1-2. In a heterogeneous collection, words are more likely to carry multiple senses than words from a homogeneous collection, which will in-crease the difficulty for an automatic algorithm to disam-biguate the senses of query words using the pairwise word similarities. The SMART system [20] is used to process doc-ument collections. Each document is first parsed into tokens
DOE1-2 collection is not used as one of the homogeneous datasets in our experiments because DOE1-2 collection pro-vides no relevant documents for a majority of the queries used in this experiment. It is only used to create heteroge-neous collections by combining with the other two homoge-neous collections. with stop words removed, and then tokens are stemmed us-ing the Porter algorithm. Finally, each document is repre-sented as a bag of stemmed words. Since our goal is to illus-trate the advantage of the proposed statistical model, we did not apply more sophisticated procedures for text analysis in our experiment, such as phrase identification.
 Our queries come from a manual Chinese translation of TREC-3 ad hoc topics (topic 151-200). To fully examine the effectiveness of the proposed model, we test it against both the long Chinese queries and the short Chinese queries. A short Chinese query is created by translating the  X  X itle X  field of an English query into Chinese; a long Chinese query is formed by combining the Chinese translations of both the  X  X itle X  field and the  X  X escription X  field in an English query. The average length of short Chinese queries is 9.64 Chinese characters, and 30.72 Chinese characters for long queries. Since most of its words in a short query are highly rele-vant to the topic of the query, we would expect that query disambiguation approaches based on word similarities will work well. In contrast, a long query usually include words either irrelevant or only slightly relevant to its topic. As a result, even a translation word that is coherent with the translations of many query words may not necessarily be a good candidate for selection. Hence, a long query usually poses a more challenging problem than a short query for a translation disambiguation algorithm based on word simi-larity information. Finally, the relevance judgments for the original English queries are used as the relevance judgments for their Chinese translations.

The Chinese-English dictionary used in our experiments comes from Linguistic Data Consortium (LDC, http://www.-ldc.upenn.edu), which consists of translations for 53061 Chi-nese words. Since our experiments do not involve the process-ing of English phrases, for any English phrase that is the translation of a Chinese word, we simply treat it as a bag of words.

To evaluate the effectiveness of the proposed method, we implement two baseline models that use translation selection methods. The first baseline model selects the most likely translation for each query word, which we call  X  X ESTONE X . The details of this model has been described in section of re-lated work. The second model, which we call  X  X LLTRANS X , makes no efforts for translation disambiguation by simply including all the translations provided by the dictionary for query words into the final query translation. Finally, for easy reference, we use the abbreviation  X  X AXCO X  for our maximum coherence model. The constant C p for the regu-experience.
Table 1 lists the average precision across 11 recall points for both the homogeneous collections and the heterogeneous collections. As indicated in Table 1 the proposed model (i.e.,  X  X AXCO X ) is able to outperform the two baseline mod-els for both short queries and long queries across all four different collections. Furthermore, we plot the precision-recall curves for both the short queries and the long queries in Figure 1 and Figure 2, respectively. As illustrated in Figure 1 and 2, for all four collections, the precision-recall curves of the maximum coherence model always stay above the curves of the other two models. Based on these results, we conclude that the maximum coherence model performs substantially better than the other two selection-based ap-proaches for cross-language information retrieval.
A further examination of results in Table 1 gives rise to the following observations: 1. In general, the retrieval accuracy for heterogeneous 2. A better retrieval is achieved for short queries than for 3. The  X  X ESTONE X  method does not consistently out-To demonstrate the uncertainty in query translation, in Figure 3, we list the translation probabilities for three Chi-nese words that are estimated by the maximum coherence model. As we can see, a significant variance exists in the distribution of translation probabilities across different Chi-nese words. The first example in the figure shows an almost figures are for short queries, and the right two are for long queries. Figure 3: Examples of translation probabilities esti-mated by the maximum coherence model. uniform distribution over all translations, while the third one illustrates a very skewed distribution. Meanwhile, the second example provides a distribution that is neither uni-form nor totally skewed. These three examples illustrate the  X  translation uncertainty problem  X , which we have addressed in previous sections. Furthermore, the diversity in the dis-tribution of translation probabilities makes it difficult for a selection-based approach to perform well over all differ-ent cases. For example, the  X  X ESTONE X  method is able to work well for the third example but will fail in the first one. On the other hand, the  X  X LLTRANS X  method would be perfect for the first example but not for the third one. Base on the above analysis, we conclude that it is important to capture the translation uncertainty and the diversity of translation uncertainty in a probabilistic model.
To illustrate the impact of the translation independence assumption on query translation disambiguation, consider the example in Figure 4. This query consists of four Chinese words, and the English translations for each Chinese are pro-vided by the dictionary are listed in the second column. For the purpose of illustration, the original English query is also included at the bottom of the figure. The English trans-lations selected by the  X  X ESTONE X  method are listed in the third column, marked by small crosses. The translation probabilities from Chinese words to their English transla-tions estimated by the maximum coherence model are listed in the last column.

Comparing to the original English query, we see that the  X  X ESTONE X  method makes incorrect translation selection for both the first and the second Chinese words. For the first one, the correct English translation should be  X  X nde-Figure 4: An example of query translation, using the  X  X ESTONE X  method and the maximum coherence model. (English words in italicized font are removed as stop words.) pendent X , instead of  X  X tand (alone) X  2 . The better trans-lation for the second Chinese word should be  X  X ublish X  in-stead of  X  X ress X . One reason for such mistakes is that in the  X  X ESTONE X  method, the coherence score of a translation is computed based on all the English translations provided by the dictionary for the Chinese words in the query. Thus, the coherence score of one translation word is completely inde-pendent from the selection of other translations. Since both  X  X tand X  and  X  X ress X  are common in English, their overall coherence scores turn out to be larger than the coherence scores of other words, which lead them to be selected by the  X  X ESTONE X  method. In contrast, in the maximum coherence model, the estimation of translation probabilities for one word is dependent on the estimation of translation probabilities for other words. As a result, the maximum coherence model is able to adjust the mistakes by assign-ing significant amounts of probability mass to the correct translations. For example, for the first Chinese word, the maximum coherence model is able to assign a probability to the correct English translation  X  X ndependent X  comparable to the probability assigned to the translation  X  X tand (alone) X .  X  X lone X  is removed as a stop word and does not count in the translation. It is listed only for clarity purpose.
In this paper, we propose a novel statistical model for cross-language information retrieval, named  X  X aximum co-herence model X . It utilizes word co-occurrence statistics for estimating translation probabilities that are effective for query disambiguation. Compared to the selection-based ap-proaches, the merits of the maximum coherence model are twofold: 1) It preserves the translation uncertainty through the estimation of translation probabilities; 2) It estimates the translations for all query words simultaneously. Em-pirical results under various scenarios have shown that the proposed model is able to perform substantially better than the existing selection-based approaches.

In the future, we plan to improve the robustness of the maximum coherence model with regard to the query noises, which has led to significant degradation in the retrieval ac-curacy in our experiments. [1] M. Adriani. Dictionary-based clir for the clef [2] M. Adriani. Using statistical term similarity for sense [3] L. Ballesteros and W. B. Croft. Phrasal translation [4] C. J. C. Burges. A tutorial on support vector [5] W. Daelemans, K. Sima X  X n, J. Veenstra, and [6] M. W. Davis. New experiments in cross-language text [7] M. Federico and N. Bertoldi. Statistical cross-language [8] J. Gao, J.-Y. Nie, E. Xun, J. Zhang, M. Zhou, and [9] J. Gao, M. Zhou, J.-Y. Nie, H. He, and W. Chen. [10] D. A. Hull and G. Grefenstette. Querying across [11] W. Kraaij, J.-Y. Nie, and M. Simard. Embedding [12] W. Kraaij, R. Pohlmann, and D. Hiemstra.
 [13] V. Lavrenko, M. Choquette, and W. B. Croft.
 [14] V. Lavrenko and W. B. Croft. Relevance based [15] A. Maeda, F. Sadat, M. Yoshikawa, and S. Uemura. [16] S. H. M. Myung-Gil Jang and S. Y. Park. Using [17] J.-Y. Nie and M. Simard. Using statistical translation [18] K. Nigam, J. Lafferty, and A. McCallum. Using [19] W. M. P.E. Gill and M. Wright. Practical [20] G. Salton, editor. The SMART retrieval system . [21] The Mathworks. http://www.mathworks.com/. [22] J. Xu and R. Weischedel. TREC-9 cross-lingual
