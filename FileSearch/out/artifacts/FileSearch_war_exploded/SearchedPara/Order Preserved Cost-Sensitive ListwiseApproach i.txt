 Learning to rank is a popular research area in machine learning and information retrieval(IR). In this paper, we focus on its application to document retrieval in IR. When applied to document retrieval, learning to ranking aims to learn a real-valued ranking function in training. In testing, the ranking function assigns a rank score for each document of a query, and sorts the documents in descending order of rank scores. Several approaches have been proposed, and are summarized into three categories: the pointwise, the pairwise and the listwise approach.
The listwise methods are attracted remarkable attention due to their high performance on the empirical datasets. Th e representative listwise methods in-clude ListNet[1], RankCosine[2] and ListMLE[3]. In ListNet, the entropy is used to evaluate the similarity between the ranked list and the ground truth list. Nevertheless, the cosine similarity is used in the RankCosine. The ranked list is generated by sorting the documents in descending order of rank scores. The ground truth list is obtained from the descending order of document relevance level. ListMLE maximizes the likelihood probability of the ground truth list. Meanwhile, the ranking order on the top of the ranked list is very important in IR, and the correct ranking list of all the documents is usually not needed. For instance, users care much more about the top ranked documents than the whole ranked list in the search engine. As a result, two variants of listwise approach are proposed to emphasize ranking order on the top of the ranked list. They are top-k consistency of ranking approach[4] an d cost-sensitive listwise ranking approach [5]. The top-k ListMLE [4] maximizes the likelihood probability of permutation of the top k documents based on plackett-Luce model[6]. The cost-sensitive ListMLE[5] sets large coefficients( i.e., weights) for the documents with higher ranks in the listwise loss function.

The effectiveness of cost-sensitive lis twise ranking approach[5] is verified on several benchmark datasets. However, [5] does not study the two important properties: order preservation and gener alization. The two properties should be cared about because they can guide to develop a better ranking method. The order preservation in ranking indicates whether the loss function can indeed represent the loss in ranking. Intuitively, suppose there are two ranked lists p 1 and p 2 with respect to the given query. When p 1 is superior to p 2 ,thevalueof the loss function on p 2 should be no less than that on p 1 . The generalization represents the expected loss for an unseen query.

In this paper, we establish a framework f or order preserved cost-sensitive list-wise ranking approach. The framework yiel ds the constraints of coefficients for the order preservation of the cost-sensit ive listwise methods. It is validated that the cost-sensitive ListMLE[5] violates the constraints. In addition, the gener-alization of the order preserved cost-sensitive listwise approach is proven. As an example, the cost-sensitive ListMLE is modified into the order preserved cost-sensitive ListMLE, referred to OPC S.ListMLE. Experimental results show OPCS.ListMLE outperforms ListNet[1], Top-k ListMLE[4] and cost-sensitive ListMLE[5].

The rest of the paper is organized as fo llows. The cost-sensitive listwise ap-proach is shortly introduced in Section 2. Section 3 describes the framework for order preserved cost-sensitive listwis e approach. Section 4 pr esents the experi-mental results. The last section draws the conclusion. The purpose of the cost-sensitive listwise approach[5] was to emphasize the rank-ing order on the top of the ranked list. To achieve the target, [5] made use of the idea of cost-sensitive learning in the listwise loss function. More precisely, the documents in the listwise loss function were set with coefficients ( i.e., weights). The coefficients were computed based on the IR evaluation measure NDCG[7]. NDCG evaluates the performance about ranking order on the top of the ranked list. Moreover, a cost-sensitive ListML E method was proposed, referred to CS-ListMLE. Its loss function on the query q = { ( x i ,y i ) } n i =1 was defined as: where x i denote the feature vector of the document d i . y i is the relevance level of the document d i . f is the ranking function, and f ( x i ) is the rank score of the document d i .  X  denotes an increasing function. For the sake of describing simply, in the paper the documents have been ranked in descending order of document relevance level, i.e., y 1 y 2 ... y n . The coefficients Z,  X  j and  X  j,t satisfy the following conditions.
 In the paper, the cost-sensitive listwise approaches are referred to the methods whose loss function are defined in (1). The loss functions of all the cost-sensitive listwise approaches are proved to be bounded by functions being (1) plus a constant. The proof is easily verified for the cost-sensitive variants of ListNet and ListMLE. Regarding the cost-sensitive variant of RankCosine, the proof can be verified with the inequality  X  3.1 Conditions of Order Preservation for Cost-Sensitive Listwise The framework first gives the conditions of the order preservation for the cost-sensitive listwise approach on the two ranked lists p 1 and p 2 . In the ranked list p , the rank scores of the documents are f ( x 1 ) ,...,f ( x n ). The rank list p 2 is obtained by just exchanging the values of f ( x s )and f ( x t )in p 1 only. loss function L defined in (1) on p 2 is no less than that on p 1 when the three following constraints are satisfied. 1. Z&gt; 0  X  i, j  X  i  X  00  X   X  i,j  X  1 2. j =1 , 2 ,...,s  X  1  X  j,s  X   X  j,t If p 1 is superior to p 2 , the theorem 1 demonstrates the constraints of the coeffi-cients for the order preservation on the p 1 and p 2 . The proof of the theorem 1 is based on the principle: the value of each item in the loss function L on p 2 is no less than the value of the same item on p 1 . The proof is omitted since it is easily verified. However, in theorem 1 the hypothesis is sufficient but not necessary for the conclusion. In other words, there is a case where the conclusion is obtained but the loss function L is not order preserved on the p 1 and p 2 .Thecaseisthe value of the loss function L being constant. In that case, the value of the loss function on the p 2 is no less than p 1 even though the p 1 is inferior to p 2 .Thus, the coefficients should be restricted to prevent the value of the loss function being constant. The sufficient and necessa ry constraints of the coefficients for order preservation of the cost-sensitive listwise approach are deduced: 1. Z&gt; 0 2.  X  1  X   X  2 ...  X   X  n  X  0 4. L = constant of ListNet[1], ListMLE[3] and Top-k ListMLE[4]. However, the coefficients in the cost-sensitive ListMLE[5] defined in (2) violate the third constraint. Thus, the cost-sensitive ListMLE can be modified i nto the order preserved cost-sensitive ListMLE by accommodating the coefficients  X  j,j for all j . 3.2 Generalization for Order Preserved Cost-Sensitive Listwise There are many coefficients complying wit h the constraints for the order preser-vation of the cost-sensitive listwise approach. Thus, it is necessary to study the effect of the coefficients to the loss on th e unseen query. The generalization of the order preserved cost-sensitive listwise approach is proven.
 Theorem 2. Suppose the training set consists of N queries q 1 ,q 2 ,...,q N drawn independently according to an unknown but fixed probability distribution X X  Y ,where X X  R n  X  d denotes the input space of n documents represented by d dimensional feature vectors, and let Y be the output space of the permutation of the documents. Each feature vector has norm bound R and model parameters w  X  R d has norm bound B. Let the loss function L be a lipschitz function with constant l. The expected loss of an unseen query Q is bounded with probability at least 1  X   X  where L ( w, q ) denotes the loss on the query q with model parameters being w . The detail of the proof is referred to [8]. The lipschitz constant l for the order preserved cost-sensitive listwise loss function is l  X  Z  X  exp (  X  ( BR )  X   X  (  X  BR ))  X  The three key points implied in the theorem 2 can guide to develop better order preserved cost-sensitive listwise al gorithms. They are listed as follows: 1. The major influence coefficients to the generalization are Z,  X  1 , X  2 ,..., X  n . 2. The document pair &lt;d i ,d j &gt; can be emphasized with large value of  X  i,j , 3. Since the model parameter w has norm bound B , the second term 3 lBR n N To obtain good generalization, the empirical risk on the training set and the penalty on the model complexity should be minimized together. Hence, a novel loss function for the order preserved cost-sensitive listwise approach on the query is defined by where the coefficients satisfy the conditi ons of the order preservation for cost-sensitive listwise approach.  X  is an auxiliary variable. When the value of  X  is large, the L penalizes much for the model parameter w with large value. The above statement can be deduced from the following approximation: Compared to the cost-sensitive listwise loss function defined in (1), there are two advantages about the loss function L defined in (5). On the one hand, L has the property of order preservation. The order preservation makes L match the NDCG[7] metric better. On the other hand, L achieves a better generalization error by penalizing the model complexity. Since [5] proved that loss function defined in (1) is the upper bound of NDCG loss, it is deduced that L will achieve less NDCG loss for the unseen query. 3.3 A Case: Order Preserved Cos t-Sensitive ListMLE Approach As case study, the cost-sensitive ListMLE[5] is modified into the order pre-served cost-sensitive ListMLE method, called OPCS.ListMLE. The loss function of OPCS.ListMLE on a query is defined in (5). In the experiments, let  X  ( z )= z . The values of the coefficients in (5) are calculated based on the values of the coefficients in the loss function of CS-ListMLE( i.e., cost-sensitive ListMLE).
With the abuse of notation, z i ,  X  i,j and  X  i,s,t denote the coefficients Z ,  X  j and  X  s,t in the CS-ListMLE loss function on the query q i . z i ,  X  i,j and  X  i,s,t denote the coefficients Z ,  X  j and  X  s,t in the OPCS.ListMLE loss function on the query q . The values of the coefficients z i ,  X  i,j and  X  i,s,t is listed in Table 1. The loss function of OPCS.ListMLE is optimized by Stochastic Gradient Descent(SGD) method. Since the loss function is conv ex, the model parameter converges to a global optimum[9].
 The experiments are conducted on the two datasets OHSUMED and TD2003 in Letor2.0 1 . The experiments validate whether OPCS.ListMLE obtains high per-formance about the ranking order on the top of the ranked list. NDCG@ k (N@ k ) is used as evaluation measures with k taking 1, 3, 5 and 10. The baselines are summarized into three categories: state-of-the-art methods[10,11], representative listwise approaches[1,2,3] and Top-k ranking algorithms[4,5]. The methods Top-k ListMLE[4], CS-ListMLE@ k [5] and OPCS.ListMLE@ k focus on the ranking order of the top k documents in the ranked list. 4.1 Parameter Setting Three parameters are manually tuned in the OPCS.ListMLE loss function. Two parameters are the learning rate  X  and tolerance factor  X  in the gradient descent method. The last parameter is the auxiliary variable  X  .Ifthevalueof  X  is large, the OPCS.ListMLE penalizes more for the model parameter with larger value. In the experiments,  X  takes 0.01 and 0.001, and  X  takes 1e-05 and 1e-06. The choices of  X  are 10, 20, 30, 40 and 50. The combinations of all values of the parameters are tried in the experiments. There are 2  X  2  X  5 experiments conducted for each fold in a dataset. The three parameters are selected on the validation set. 4.2 Experiment Results The performances of OPCS.ListMLE and CS-ListMLE on the two datasets are reported in Table 2. The OPCS.ListMLE@ k ( k =5, 10) outperforms the CS-ListMLE@ k ( k =5, 10) on the dataset OHSUMED at all evaluation measures. The OPCS.ListMLE@1 achieves higher ranking accuracy than all versions of CS-ListMLE on the dataset TD2003. Moreover, OPCS.ListMLE@1 significantly outperforms CS-ListMLE on the two datasets at NDCG@1. We conduct t-test on the improvement of OPCS.ListMLE@1 over CS-ListMLE in terms of NDCG@1. The improvement of NDCG@1 over CS-ListMLE@ k ( k =3,5,10) on the dataset OHSUMED is statistically significant(p-value &lt; 0.05). There is no statistically significant difference on the dataset TD2003.

We take OPCS.ListMLE@1 as an example to compare the ranking accuracy of OPCS.ListMLE with the other baselines. The experimental results are il-lustrated in Table 3. On the one hand, OPCS.ListMLE@1 obtains the highest NDCG@1 among the methods on the dataset OHSUMED. The t-test on the im-provement of OPCS.ListMLE@1 over the baselines in terms of NDCG@1 is con-ducted. The improvements over Ranking SVM[10], AdaRank[11] and ListNet[1] are statistically significant. On the other hand, OPCS.ListMLE@1 outperforms all the baselines on the dataset TD2003 at all evaluation measures, especially the NDCG@1. The t-test results show that OPCS.ListMLE@1 statistically out-performs Ranking SVM, AdaRank and ListNet. Note that the t-test on the improvement of NDCG@1 over ListMLE, RankCosine and Top-k ListMLE on the two datasets is not conducted. Their performances at each fold on the two datasets are not published.

Experimental results demonstrate that the OPCS.ListMLE achieves higher ranking accuracy on the two dataset. However, the OPCS.ListMLE is a vari-ant of the CS-ListMLE with coefficients normalization. The reason why the OPCS.ListMLE performs better is: the ord er preservation an d generalization are incorporated into the cost-sensitive listwise methods. It demonstrates the effectiveness of the order preserved cost-sensitive listwise approach. To develop better cost-sensitive listwis e ranking methods, we study the two impor-tant properties: order pres ervation and generalization. We establish a framework for order preserved cost-sensitive listwise ranking approach. The framework yields the constraints of the coefficients for order preservation of the cost-sensitive list-wise approach. In addition, the generalization of the order preserved cost-sensitive listwise approach is proven. We then analyze the effect of the coefficients and model complexity to the generalization error. A ccording to the theore m of generalization, a novel loss function for order preserved cost-sensitive listwise approach has been proposed. The loss function not only is order preserved but also penalize the model complexity by an auxiliary variable. Moreover, the order preserved cost-sensitive ListMLE method is proposed, called OPCS.ListMLE. Experiment results on the benchmark datasets show the effectivenes s of the order preserved cost-sensitive listwise methods.
 Thanks to the anonymous reviewers, esp ecially regarding writing and experi-ments. This work was supported by the National Natural Science Foundation of China under gra nt 60673009 and the Fundamental Research Funds for the Central Universities under gra nt 65010571 at NanKai University.

