 Deep neural network representati ons play an important role in computer vision, speech, computational linguistics, robotics, reinforcement learning and many other data-rich domains. In thi s talk I will show that learning-to-learn and compositionality ar e key ingredients for dealing with knowledge transfer so as to so lve a wide range of tasks, for dealing with small-data regimes, and for continual learning. I will demonstrate this with several exampl es from my research team: learning to learn by gradient descent [1 ], neural programmers and interpreters [3], and learning communication [2]. Deep Learning; Recurrent Neural Networks; Compositionality; Transfer Learning; Learning to Learn, Abstraction, Algorithm Induction Nando de Freitas is a machine learning professor at Oxford University and a senior staff research scientist at Google DeepMind. He is a fellow of the Canadian Institute For Advanced Research (CIFAR) in the successful Neural Computation and Adaptive Perception program, and an action editor for the Journ al of Machine Learning Research. 
