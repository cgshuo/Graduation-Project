 Big Data Pipelines decompose complex analyses of large data sets into a series of simpler tasks, with independently tuned components for each task. This modular setup allows re-use of components across several different pipelines. However, the interaction of independently tuned pipeline components yields poor end-to-end performance as errors introduced by one component cascade through the whole pipeline, affecting overall accuracy. We propose a novel model for reasoning across components of Big Data Pipelines in a probabilistically well-founded manner. Our key idea is to view the interaction of components as dependencies on an underlying graphical model. Different message passing schemes on this graphical model provide various inference algorithms to trade-off end-to-end performance and computational cost. We instantiate our framework with an efficient beam search algorithm, and demonstrate its efficiency on two Big Data Pipelines: parsing and relation extraction.
 I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation, Theory Big Data Pipelines, Modular Design, Probabilistic Inference
Benevolent pipelines propagate knowledge.  X  Adapted from a quotation in the Sama Veda.

Unlocking value from Big Data is a hard problem. The  X  X hree V X  X  X  of the data (volume, velocity, and variety) require setting up sophisticated Big Data Pipelines  X  workflows of tasks such as data extraction and transformation, feature Figure 1: Big Data Pipeline for relation extraction. Figure 2: Object Detection &amp; Recognition pipeline. creation, model construction, testing, and visualization. In order not to have to develop components for each task from scratch, data scientists build Big Data Pipelines using exist-ing software components, for example, a natural language parser or a text classifier.

An example of a Big Data Pipeline for relation extraction from text is given in Figure 1. After sentence splitting and tokenization, a part-of-speech (POS) tagger feeds into a syntactic parser, which is the basis for a dependency parser. Finally, the POS tags and parses provide features for the relation extractor. The components of this pipeline are also used in pipelines for other natural language analysis tasks like joint entity and relation extraction [30], sentiment analysis [23] or opinion extraction [5]. Another example of a Big Data Pipeline from computer vision [14] is shown in Figure 2, where low-level filters (e.g., edge detectors) eventually feed into an object recognizer (e.g., an SVM) after several transformation and pooling operations. Similarly, such pipelines also exist in other domains, ranging from robotics [1] to computational biology [31].
 The speed of plug-and-play development of Big Data Pipelines comes with a big drawback: the composition and resulting execution of the pipeline is rather ad-hoc. In par-ticular, we lack a model for global reasoning about the un-certainty that is associated with the predictions of each task in the pipeline, even though components maintain and could provide a wealth of relevant information. In the default ap-proach, where each stage merely passes its canonical output to the next stage, any error in the early stages can lead to cascading and unrecoverable errors in later stages. Each pipeline component makes a locally optimal choice without consideration that it is only one of the steps in a long chain. In this paper, we present a formal model of inference over Big Data Pipelines in a probabilistically well-founded manner. Our model accounts for and utilizes the uncertainty in the predictions at all stages of the pipeline. We propose inference algorithms that allow us to trade off computational efficiency versus overall quality of the pipeline by optimizing how much uncertainty is propagated. Our inference algorithms have an elegant formulation in relational algebra which may be of in-dependent importance due to its connections to probabilistic databases and data provenance [2, 29].
 In particular, the paper makes the following contributions. First, we formalize pipeline inference as a graphical model in which nodes are processing components and edges de-note data flow between components. With this model, we have a formal foundation for reasoning across a whole Big Data Pipeline (Section 2). Second, we propose inference algorithms that resolve uncertainty across the pipeline, in-cluding a beam search algorithm that adaptively increases the amount of information that is propagated. We show that these inference algorithms can be formulated in relational algebra well-suited for probabilistic database implementation (Section 3). Third, in a comprehensive experimental evalu-ation, we demonstrate how our approach gracefully trades off the quality of predictions and computational efficiency of inference on two real-world tasks: a Syntactic Parsing pipeline and a Relation Extraction pipeline (Section 4). We conclude with a discussion of our approach in Section 5 and its connection to related work in Section 6.
Given an input x , how should we compute the output y of a pipeline with multiple components? The answer to this question depends on the API that each component exposes. In the most basic API, we may only assume that a component takes input x (e.g., sequence of words) and outputs an answer y (1) (e.g., sequence of part-of-speech tags). We call this y (1) the canonical output . With this simplistic API for each component, it is unclear how cascading errors can be avoided.
Fortunately, many pipeline components permit a more refined API in two respects. First, a component  X  provide a confidence score, interpreted as the conditional probability Pr ( y | x, X  i ), for the returned output y (1) a component can not only return the canonical output y (1) which most naturally is the maximizer of the conditional probability i.e. y (1) = argmax y Pr ( y | x, X  i ), but also all of the top-k highest scoring predictions with their respective conditional probabilities: In contrast to the basic API that only returns the canonical output, we call this API the Top-k API.

Many standard components (e.g., POS taggers, parsers) already provide this functionality. Even non-probabilistic models (e.g., SVMs) can typically be transformed to produce reasonable conditional probability estimates [25].
The Top-k API allows us to interact with components as if they were truly black boxes. We do not assume we have access to  X  i , nor the form of the internal probabilistic model. This is important in the Big Data application setting where re-training pipeline components for a new domain imposes a significant scalability bottleneck, and plug-and-play development of pipelines is desirable.

We now explore how the information provided by this extended interface can be used to not only make pipeline predictions more reliable and robust, but also to provide probabilistic outputs for the pipeline as a whole.
A note about notation: We denote specific values using lower-case letters and the set of all possible values by upper-case letters. This is consistent with the usual notation for random variables. For instance, we denote any general output of a component by Y , and a specific output by y .
To derive a general model of pipeline inference, let us for now assume that each component, when provided an input x , exposes the full distribution Pr ( Y | x, X  i ) through the API. Equivalently, one may view this as providing the top-k , but for a large enough value of k that enumerates all possible y . Under this API for each component, what should the final pipeline output be and what confidence should the pipeline assign to this output?
Assume that each component  X  i in the pipeline is a trained or hand-coded model. We shall use s x,i ( y ) as shorthand for Pr ( y | x, X  i ). When it is clear from the context which component and input is being talked about, we shall drop the subscript to yield s ( y ).

The schematic of a black box component is illustrated in Figure 3. It is very natural to view input X and out-put Y as random variables, and the component  X  specifies the conditional dependencies between X and Y . Note that this is a general definition of a component, encompassing deterministic rule-based transformations (the conditional dis-tribution is such that all the probability mass is concentrated on the mapping of the input x ) as well as generative and discriminative probabilistic models.

To provide intuition for the inference problem, first consider a pipeline of only two components,  X  1 and  X  2 , where the output of  X  1 is piped to the second component. This is illustrated in Figure 4. Let the input to the pipeline be X , the outputs of  X  1 and  X  2 be Z 1 and Y respectively. From Figure 3, we know that this is equivalent to a graphical model with three nodes X,Z 1 ,Y . The law of joint probability gives us where (1) follows from the conditional independence induced by the graphical model. Thus, for a particular input x , the ideal output according to Equation 1 would be: We call y Bayes the Bayes Optimal output since it considers all possible intermediate outputs in determining the prediction.
Going beyond the composition of two components, any general pipeline can be viewed as a Directed Acyclic Graph, where the nodes are the black box components and the edges denote which outputs are piped as inputs to which components. The basic idea illustrated above for a linear two-step pipeline can be extended to general pipelines such as the one illustrated in Figure 5. Any general pipeline structured as a DAG admits a topological ordering. Denoting the input to the pipeline as X , eventual output Y , and the output of each intermediate stage i in this topological ordering as Z then any acyclic pipeline is a subgraph of the graph shown in Figure 5. From our modeling assumption, this DAG is an equivalent representation of a directed graphical model: each of X,Z 1 ,Z 2 ... Y are nodes denoting random variables, and the components enforce conditional independences. The graphical model corresponding to Figure 5 makes the fewest independence assumptions across stages.
 Analogous to the two stage pipeline from above, the Bayes Optimal prediction y Bayes for general pipelines is y
X Computing the Bayes Optimal y Bayes is infeasible for real-world pipelines because it requires summing up over all pos-sible intermediate output combinations z 1 , ... z n  X  1 . We will therefore explore in the following section how this intractable sum can be approximated efficiently using the information that the Top-k API provides.
Even the naive model of passing only the top output to the next stage can be viewed as a crude approximation of the Bayes Optimal output. We call this procedure the Canonical Inference algorithm. For simplicity, consider again the two component pipeline, such that the naive model results in The optimization over y in (4) is unaffected if it is multiplied by the term Pr ( z 1 (1) | x, X  1 ). Once the summation in the marginal in (1) is restricted to z 1 (1) , the sum reduces to Hence, this objective provides a lower bound on the true Pr ( y | x ). It is easy to see that this holds true for general pipeline architectures too, where the canonical inference procedure proceeds stage-by-stage through the topological ordering. For i = 1 ,...,n , define:
In this sense, canonical inference is optimizing a crude lower bound to the Bayes Optimal objective (again, the terms Pr ( z 1 (1) | x, X  1 ) etc. do not affect the optimization over y ). This suggests how inference can be improved, namely by computing improved lower bounds on Pr ( y | x ). This is the direction we take in Section 3, where we exploit the Top-k API and explore several ways for making the computation of this bound more efficient.
Canonical inference and full Bayesian inference are two extremes of the efficiency vs. accuracy tradeoff. We now ex-plore how to strike a balance between the two when pipeline components expose the Top-k API. In particular, given input x suppose each component  X  is capable of producing a list of k outputs h y (1) ,y (2) ,...y ( k ) i along with their conditional probabilities h Pr( y (1) | x, X  ) ,... Pr( y ( k ) | x, X  ) ing order). We now outline three algorithms, characterize their computational efficiency, and describe their implemen-tation in terms of relational algebra. This interpretation enables direct implementation of these algorithms on top of traditional/probabilistic database systems.
The natural approach to approximating the full Bayesian marginal (Eq. 3) when computation is limited to a few sam-ples is to choose the samples as the top-k realizations of the random variable, weighted by their conditional probabilities. We call this Top-k Inference, which proceeds in an analo-gous fashion to canonical inference, stage-by-stage through the topological ordering. However, inference now becomes more involved. Each component produces a scored list, and each item in the lists generated by predecessors will have a corresponding scored list as output.

Denote the list of outputs returned by the component  X  i (each of which are possible values for the random variable Z in the equivalent graphical model) as { z i } k . Note that this set depends on the inputs to  X  i , but we omit this dependence from the notation to reduce clutter. This gives rise to the following optimization problem for identifying the eventual output of the pipeline. y
X A simple extension of top-k inference is to have a parameter k for each component  X  i that allows variable length lists of outputs { z i } k i for different components in the pipeline. We refer to these parameters collectively as ~ k .

Consider again the simple two stage pipeline in Figure 4. Each component now operates on relational tables and generates relational tables. We initialize the table T 0 the input to the pipeline x : The subscripts in the attributes refers to the component ID, IX denotes the index into the scored list of outputs produced by the component, VALUE is the actual output and SCORE is its conditional score. Now, given a parameter vector ~ = ( k 1 ,k 2 ), the first component will generate the table T Note that IX 0 carries no additional information, if component 1 is the only successor to component 0 (as is the case in this example). However, in a general pipeline, we need columns like IX 0 to uniquely identify inputs to a component.
The second component now takes the table T 1 and produces the final table T 2 .

The final output for top-k inference y TopK is generated by grouping the values T 2 . VALUE 2 , aggregating by SCORE 2 and selecting the element with largest sum. For linear pipelines (Figure 5 without the curved edge dependencies), this procedure is summarized in Algorithm 1. The linear pipeline has the nice property that each component has ex-actly one successor, so we need not store additional informa-tion to trace the provenance of each intermediate output. We assume each component provides the interface GenTopK ( ID , k , INPUT ) which returns a table with attributes VALUE ID SCORE ID that has k tuples. For the n -stage linear pipeline, performing top-k inference with parameters ~ k will yield a final table T n with at most k 1 k 2 ... k n tuples. There may be fewer tuples, since any output generated by different com-binations of intermediate outputs results in only one tuple.
For top-k inference in general pipelines, the algorithm needs to maintain additional information to uniquely identify each input that a component processes. Specifically, for a component  X  i , let the indices of its parents in the DAG be denoted by J . We need to construct a table that  X  process, which has all compatible combinations of outputs in tables { T j } j  X  J . This procedure is outlined in Algorithm 2. Let the lowest common ancestor of two parents of  X  i (say,  X  p and  X  q ) be denoted by  X  m . If  X  p and  X  q do not share an ancestor, m defaults to 0. The only compatible output combinations between  X  p and  X  q are those that were generated from the same output from  X  m . We store columns IX  X  in our tables to identify precisely this fact: T i traces all ancestors of the component  X  i . Hence, the natural join of tables T and T q will match on the IX attributes corresponding to Algorithm 1 Top-k Inference for Linear Pipelines.
 Require: GenTopK ( id,k,x ) , ~ k
T 0  X  CreateTable( V ALUE 0 : X ; SCORE 0 :1) for i = 1 to n do end for return Max ( T n .SCORE n ) Algorithm 2 Creating Compatible Inputs.
 Require: J 1: p  X  max J 2: T  X  T p 3: A  X  J \ p 4: repeat 5: q  X  max A 6: r  X  LowestCommonAncestor ( p,q ) 7: T  X  T 1 T q 8: T.SCORE r  X  T.SCORE p  X  T q .SCORE q 9: A  X  A  X  X  r }\{ p,q } 10: p  X  r 11: until | A | X  1 12: return T their common ancestors up to m , as shown in lines 6-7 in Algorithm 2. Furthermore, we need to update the probability mass for each compatible output combination. From (6), we see that the product of the scores in T p and T q will have the contribution from their common ancestors double-counted. Hence, in line 8 of the algorithm, we divide out this score. With this algorithm, and the understanding that Gen-TopK ( ID , k , INPUT ) augments its returned table with IX information, inference proceeds exactly as the algorithm for the linear pipeline (Algorithm 1) with T i  X  1 replaced by the output of Algorithm 2 given Parents ( i ) as input. It should now be clear why we always check the Successors ( i ) before aggregating tuples in a table: Such an aggregation step loses provenance information (in the form of IX attributes) which is necessary for correct recombination later in the pipeline. The top-k inference procedure described above follows Equation (6) exactly. However, this can result in a multi-plicative increase in the sizes of the intermediate tables, with the eventual output requiring aggregation over potentially k k 2 ... k n outputs. This may become infeasible for even moderate pipeline sizes and modest choices for ~ k .
One way to control this blowup is to interpret the param-eters ~ k as limits on the sizes of the intermediate tables. In particular, at each stage i of the pipeline, we sort table T Algorithm 3 Adaptive Deepening.
 Require:  X , [ s 1 ,...,s k ] d  X  0 repeat until d  X  k  X  1 or r &lt;  X  return [ s 1 ,...,s d ] according to the SCORE i and only retain the top k i tuples. This pruning can be implemented as the Transform function of Algorithm 1. We refer to this inference method as Beam-M Inference. This strategy provides an optimization objective that is a scalable intermediate between canonical inference and top-k inference.
Since it may be difficult to set the parameters ~ k limiting the beam width a priori, we now explore an adaptive inference method. Intuitively, the beam width k i of component  X  should depend on the uncertainty in its prediction. If that component had perfect confidence, doing canonical inference for this step suffices to yield the Bayes optimal solution since all of the probability mass is concentrated on a single prediction. If a component is less confident, the beam width should be larger. A larger beam is computationally more intensive but promises to be a more robust estimate of the Bayes optimal objective.

This leads to the operational goal of adapting the beam width to ensure that each result list includes all predictions that have high probability, but ideally not many more so that efficiency does not suffer. This can be operationalized by looking for sharp drops in the conditional probability as the algorithm goes down the scored list of predictions generated by a component. To control this process, we not only have the maximum beam width ~ k like in top-k inference, but also a vector of parameters ~  X  = [  X  1 , X  2 ,..., X  n ] that specify the drop threshold for each component. As outlined in Algorithm 3, we only deepen the beam (up to the maxima specified by ~ k ), if the next prediction in the top-k list has a sufficiently large (as specified by ~  X  ) conditional probability relative to the previous element of the list. We typically set the value of ~  X  to be the same for all components. Adaptive Inference also proceeds through the topological ordering as in Algorithm 1, with AdaptiveDeepening (  X  i ,k i ) (Algorithm 3) being used as the Transform function.
The following experiments evaluate the proposed inference methods on three pipelines.

Parse Pipeline: The syntactic parsing pipeline is an important NLP task and a common sub-pipeline in many application problems. The pipeline consists of two stages, namely POS tagging followed by parsing. We use the bench-mark Wall Street Journal Treebank dataset [20] with the standard setup: POS tagger and parser with models trained on Sections 2-21, the remaining sections are used for evalu-ation. This leads to a test set of 9370 sentences. We used two popular off-the-shelf components for this pipeline: the HMM-based LingPipe system [4] to predict POS tags and the phrase grammar-based Enju parser [26]. There was no modification required to use these systems, as they both al-ready expose interfaces to get the top k outputs. In addition to the standard Bracketed F1-measure [24], we also report 0/1-loss performance ( i.e., the number of sentences with the correct parse tree predicted) denoted as # Cor .

Relation Extraction (RelEx) Pipeline: We built a three-stage Relation Extraction (RE) pipeline similar to the one from Figure 1, which adds the RE stage at the end of the POS/parsing pipeline. We use the benchmark ACE 2004 dataset, which contains sentences labeled with entity-pairs along with the corresponding relation type. As in previous studies [22], the labeled relation types are collapsed into seven pre-defined categories. To extract relations we used the RECK relation extraction system [22], which employs an SVM with a convolution kernel. Given a sentence, its POS tag sequence, and its parse tree, and a pair of entities, the SVM predicts the relation between them. RECK predicts the class with the highest score if it exceeds a fixed pre-defined threshold  X  . Since RECK uses the Stanford parser [15], so do we. Lingpipe was used for POS tagging.

A large fraction of this dataset can be trivially classified using just the sentence tokens. Thus in order to better study the impact of pipeline inference, we use a subset of the data which consists of the  X  X ifficult X  examples: all examples where the predicted relation differs for at least one of the candidate (POS sequence, parse tree) values. The candidate set consists of the top 9 POS tag sequences predicted by LingPipe and the top 5 parses predicted by the Stanford Parser for each tag sequence (our results are stable to a broad choice of these numbers). This leads to a set of 11016 examples.

Mean average precision (MAP) over the difficult examples is used as the performance measure, computed by the ranking scores of all examples for each of the seven relations. To compare with results from the literature, we also report the precision of the classification (using the same pre-defined threshold for prediction  X  as in [22]).

Synthetic Data (Synth) Pipeline: We also report re-sults for a synthetic pipeline, since this allowed us to investi-gate a wide range of scenarios, such as the amount of noise at each stage of the pipeline. To generate this data, we used the following model.

Each black box in the pipeline is represented by two dis-tributions: the true distribution Pr (  X | X  ) (instantiated as a Dirichlet distribution Dir (  X  )) mapping inputs to outputs and the learned distribution  X  Pr (  X  |  X  ). The learned distri-bution is a mixture of the true distribution and a Dir (  X  ) noise distribution. This captures the notion of noisy training without having to materialize a training dataset or algorithm. The default mixture weight for the noise was set to 10%. The true label for an example is obtained as the Bayes Optimal value using the true distributions.

By default, we use a three-stage pipeline. The first black box takes in one of 500 input values and outputs one of 50 outputs. Using the above as input, the second black box outputs one of 20 outputs, which is used in the final stage to generate one of 10 possible outputs. We report 0/1 accuracies for these final outputs, averaged over 500 synthetic pipelines drawn from Dir (  X  ).
We now show the results of our experimental study. In the remainder, we distinguish between performance of the inference, which is measured by problem-specific quality
Data Top-1 Beam-M Parse 83.0 83.9 84.4 85.3 85.7 85.9 RelEx 35.1 35.5 35.7 36.3 36.7 37.0 Synth 25.4 30.2 34.5 42.2 49.5 74.2 measures and computational cost , which we measure by the number of calls to the black box components in the pipeline.
The first experiment verifies whether Top-k inference in-deed provides better prediction performance than the canon-ical Top-1 inference. The results for the three pipelines are shown in Table 1. ~ k is set so that each component returns the same number of outputs. All measures are on a 0-100 scale  X  the larger the better.

We can see that going from Top-1 to even just Top-2 has a substantial impact on all the pipelines; for instance, we see a  X  3% improvement in the MAP score for Relation Extraction. With increasing k , the performance continues to improve monotonically.

Closer inspection reveals that this difference is largely due to the errors made by the Top-1 inference in the early stages of the pipeline. On the Parse pipeline, for example, using the True POS tag sequence (available from the labeled test set), instead of the Lingpipe output, we obtain a significantly higher F-1 score of 89 . 1% even when using top-1 in the second stage. Similarly, in the Synth pipeline, the Bayes-Optimal solution using the noisy learned distributions  X  Pr (  X | X  ) leads to an accuracy of 90 . 1%. Using Top-k inference narrows the gap between the naive Top-1 baseline and these skylines by overcoming early-stage errors in the pipeline.
How should we set k in the pipeline  X  should we use the same k for all components or should k vary by component? To answer this question, we experimented with the two real-world pipelines using up to nine POS tag sequences and five parse trees. The results are shown in Tables 2 and 3. We can see that using a larger k helps more for tagging than parsing. For instance, using the top-5 POS tag sequences and the top parse tree leads to larger improvements than using the top-5 parse trees and the top POS tag sequence across both pipelines. This indicates that the gains observed are mainly due to  X  X ixing X  errors in early stages of the pipeline. Sending more outputs from the early stages provides a mechanism for the pipeline to recover the true eventual output even if the top intermediate output is incorrect. Thus including a sufficiently good POS sequence in the top-k improves the confidence of the correct parse. This effect is particularly pronounced in the two-stage parsing pipeline (compared to the three-stage RelEx pipeline) as here fixing errors in the POS tag sequence has a larger impact on the eventual output.
Data Top 1 Top-k Beam-M Parse 83.4 84.7 85.3 85.9 84.7 85.3 85.9 RelEx 34.3 35.3 35.5 35.4 35.5 35.4 35.8 Synth 24.7 33.0 40.0 52.5 28.7 31.9 37.7 Table 5: The effect of noise in the learned models.
These performance improvements exist for nearly every combination of k values for both datasets. In particular, using only the top-1 parse tree but all the top-9 tag sequences leads to more than 250 additional sentences parsed correctly and a gain of  X  10% in precision for the RelEx pipeline. Thus, if we have limited resources, we should use different values of k throughout the pipeline. Next we investigate if the computationally cheaper Beam-M inference also improves over Top-1 inference. We use a fixed beam width M across all stages of the pipeline. Table 4 shows the results of varying M for the three pipelines. As with Top-k inference, we find that deepening the beam by even one ( M = 2) leads to improvements on all pipelines. Further increasing M provides additional gains. Again, it seems helpful to pass more information at earlier stages of the pipeline. Comparing k = 3 and M = 7 for the RelEx Pipeline shows that passing on more (3 vs 7) POS tag sequences helps despite passing fewer (9 vs 7) parse trees.

Note that the amount of information that is passed be-tween components is constant in Beam-M , while it can grow exponentially with pipeline length for Top-k . Scalability can be further improved by reducing the beam for components that are computationally intensive.
Our results indicate that the inference algorithms can trade off computational cost and performance by varying k or M . Algorithm 3 automatically selects the beam size individually for each pipeline component on a per-example basis. We ran Algorithm 3 for different values of  X   X  [0 . 001 , 0 . 999]. For each run, we set identical values of  X  for all components in the pipeline. We measured the computational cost in terms of the sum of black box calls.

Figure 6 shows the tradeoff achieved by this deepening strategy. In all pipelines, we find a diminishing return in per-formance gain when we increase computation. This means that we can achieve significant improvement in performance with a small computational overhead as compared to the canonical Top-1 approach. As the computational cost is monotone in  X  , we can estimate the value of  X  that fits within the available computational budget (and maximizes perfor-mance) by profiling over a small sample of examples. Other beam deepening strategies lead to similar results, but are omitted for brevity. Our cost model assumed a uniform cost across pipeline components. However, more sophisticated cost models could plausibly be incorporated and further extensions to adaptive deepening are discussed in Section 5.
How robust are Top-k and Beam-M inference algorithms to errors in the predictions of the components? To answer this question, we experimented using different sources of noise and error in the learned models. A common source of error is that models may have been trained on out-of-domain
Length Top 1 Top-k Beam-M 2 48.7 57.4 65.1 76.1 57.4 65.1 76.1 3 24.4 35.0 45.1 62.3 30.6 36.4 46.1 4 17.2 29.4 44.2 64.9 21.6 26.6 34.5 5 16.0 33.3 46.0 71.6 20.1 22.5 31.8
Table 7: Effect of black box complexity for Synth. data. Thus on the Parse pipeline (which is tested on financial newswire text), we increased the error rate of the Lingpipe POS tagger by training it on the literary Brown corpus [11]. Another source of error is the use of less powerful learning techniques. As an example of this case, on the RelEx pipeline we degraded the final RE stage by using the CK 1 instead of the CK 1 +SSK kernel from [22], a weaker kernel, in the SVM classifier. In the Synth pipeline, we increased the noise component in the learned distribution mixture of all pipeline components to 30%.

Table 5 shows the results comparing the different inference methods in the presence of increased error and noise. While the performance of different inference methods drops com-pared to the low error/noise settings, we find that the Top-k and Beam-M inference methods still significantly outperform the canonical Top-1 inference method. Furthermore, the per-formance for both proposed inference techniques improves on increasing the value of k in Top-k or M in Beam-M as already observed in the low error/noise settings. In general, the trends from our earlier experiments continue to hold, showing the robustness of our inference methods.
In our last set of experiments, we explore the effects of increasing pipeline length and complexity.
 Table 8: Popular NLP modules for tagging and pars-ing that provide the Top-k API
First, we varied the length of the synthetic pipeline by adding more components at the end of the pipeline. The results are shown in Table 6. As expected, the performance for all methods drops as we increase the length. However, we still find that the two proposed inference methods signifi-cantly outperform the canonical method, and they maintain their trends across varying pipeline lengths.

Second, another source of complexity in pipelines is the difficulty of correct inference in the individual components. To study this effect, we changed the Dirichlet distribution parameter  X  that was used to generate the distributions for each of the black boxes. The results are shown in Table 7. We find that while the task gets harder as we increase  X  , the Top-k and Beam-M inference methods maintain their advantage compared to the Top-1 canonical inference.
Generality of our methods: Our DAG formalism is general enough to subsume most pipelines used in practice. For the general case of graphs with cycles (corresponding to pipelines with feedback), there still exists an underlying undirected graphical model, and message passing schemes (like Loopy Belief Propagation) may correspond to natural procedures for pipeline inference. In cases where feedback is limited to a maximum number of iterations, the underlying graph can be  X  X nrolled X  to a DAG, and our approach applies.
Since the Top-k API makes very mild assumptions about the components, it is directly applicable in pipelines across several domains, such as vision and robotics. We experi-mented with NLP pipelines in particular as they offer an attractive experimental test-bed to demonstrate our approach since the community has clearly structured linguistic tasks into well defined sub-tasks, and has agreed upon performance measures for each sub-task. Additionally there exist a variety of NLP software that provide the Top-k API. For example, the Parse ( left ), RelEx ( middle ) and Synth ( right ) pipelines. Table 8 lists commonly used software for tagging and pars-ing which provide this functionality. Relation extraction is most commonly solved as a supervised multi-class predic-tion problem, which trivially yields top-k output along with conditional probabilities.

Applicable even without the Top-k API: In our ex-position, we assumed that black boxes provide the Top-k API. However our methods can still be used when this assumption is violated, albeit with small modifications. For example, some software provide joint probabilities Pr ( Y,X |  X  ) in-stead of conditional models. These can still be incorporated into our framework via normalization, which can be done efficiently for many learning models. When components do not model a probabilistic distribution and only provide the canonical output, we can still obtain a proxy for the Top-k in our method, by using redundant techniques/software that solve the same problem. This is not uncommon as seen in Table 8. Techniques such as Boosting or Mixture-of-Experts provide reliable proxies for the confidence scores assigned by redundant components for each output. Another alter-native, that is applicable when model parameters can be re-learned, is to use variants of the same model obtained via training from different data samples or other such parameter perturbations.

Alternatives to Adaptive Inference: The Transform operator in Algorithm 1 is just one way to limit computational complexity during inference. An alternative transformation of the intermediate output table T i is to pass along tuples so as to capture a fixed mass of the conditional probability. This heuristic allows us to bound the gap between the inference objective and the Bayes Optimal objective.
 Inference in a pipeline should be a goal-oriented process. When deciding whether to deepen the beam at a particular component, we would ideally want to know the sensitivity of the eventual output of the pipeline to additional intermediate outputs. We can create adaptive inference strategies that operate in phases. In the first phase, one may propagate limited amounts of information throughout the pipeline to reliably estimate the sensitivity of different components. This can then help determine how to deepen the beams. A version of this adaptive beam deepening heuristic that performs sensitivity analysis of just the successor component (rather than the end of the pipeline) showed promising empirical results.
 Limitations of Top-k Inference: The Top-k and Fixed Beam approaches (as well as the canonical approach) are not impervious to badly designed pipelines. For example, in a two stage pipeline, if the second component ignores the output of the first component when making its prediction, generating additional outputs from the first component cannot improve overall performance. Sensitivity analysis and adaptive beam deepening promise to address this limitation.

Applying our ideas in monolithic software pipelines can be challenging. These software, which typically deploy fine-tuned instantiations of pipeline components, have been de-signed to combat the problem of cascading errors in pipelines. For instance, some relation extraction software bundle to-gether a hand-tuned parser and POS tagger rather than accepting inputs from off-the-shelf components. This makes these systems brittle to changes in the evaluation domain and prevents potential improvements from incorporating bet-ter models developed for early stage components. Thus in our experimental set-up, we made a conscious effort to have a modular pipeline throughout, with different off-the-shelf components plugged in for different tasks.

Our approach can also fail if the Bayes Optimal prediction is not the ideal goal. For example, if an early component encounters a catastrophic failure on an input, and the en-tire conditional distribution is  X  X rong X , the Bayes optimal prediction need not be the correct eventual output. In light of our work, this provides interesting insight into model de-velopment efforts. During training, it may be more prudent to spend modeling effort on limiting catastrophic failures across the input space, than tweaking parameters to make the correct output appear at the top of the scored prediction lists rather than a few places below.

Scalability of Inference: Pipeline inference is embar-rassingly parallel: each instance in the test set is usually processed independently. Hence, the overall cost of inference is linear in the size of the test set. Thus approaches that bring down the linear cost of testing are complementary to our efforts at improving the quality of predictions.
Bridging DBs and Big Data Pipelines: The rela-tional algebra formulation of our inference procedures draws connections with probabilistic databases and data prove-nance, and could be used as computationally-efficient infer-ence methods for these problems as well.
In different domains, pipelines take different shapes. Li et al. [16] have developed a model of shallow and broad pipelines for scene understanding in computer vision, while NLP pipelines are typically deep e.g., Machine Translation, Semantic Role Labeling, Relation Extraction and Opinion Extraction [27, 10, 5]. In Natural Language Processing, UIMA [12], GATE [8] and more recently CURATOR [6] provide several components that can be plugged into a con-figurable pipeline. Our approach to improve inference in pipelines is orthogonal to their implementations, and can be incorporated into each of these frameworks seamlessly.
Beam search has been used commonly in combination with algorithms for efficient inference [28]. To propagate the outputs in the beam efficiently, a number of efficient structures have been devised to compactly encode certain families of distributions [21, 17]. Efficient encodings for top-k lists can improve the scalability of our approach as well.
Adaptively sub-sampling data while training models with bounded loss guarantees during inference is a way to trade-off training efficiency with inference fidelity [9]. Researchers have attempted to migrate code to FPGAs and GPUs [7] and devised parallel implementations of expensive analysis tasks [18, 19]. Our approach is complementary to these efforts, and promises to trade-off efficiency and performance during inference in a graceful manner.

An alternative to propagating the top-k outputs would be to sample from the posterior distribution of probabilis-tic models [10]. However we recognize that most practical components do not support such an operation. Other work alleviates the cascading error problem by routing the eventual outputs back as inputs to the pipeline [16]. These iterative methods do not have clear guarantees on convergence or on the quality of inference. Finally, other groups have recog-nized that canonical inference ignores the scores s ( y ) that components provide. These scores can be incorporated as fea-tures for downstream tasks [3] or used to iteratively constrain outputs of upstream tasks in subsequent passes [13]. Another approach to the cascading error problem is performing joint inference across multiple stages of the pipeline, as done for joint entity and relation extraction [30]. However, this is limited to certain kinds of probabilistic models and to small pipelines, due to the complexity of joint inference.
We propose a probabilistic graphical model that allows principled reasoning about Big Data Pipelines. The model provides a well-founded interface between pipeline compo-nents that accounts for uncertainty, while maintaining modu-larity that enables reuse of pipeline components. Abstracting pipeline components behind an API that is already widely supported by off-the-shelf components, the paper demon-strated how Top-k inference, Beam search, and Adaptive beam selection can provide improved prediction performance and robustness of the overall pipeline. More generally, we anticipate that this probabilistic model of Big Data Pipelines will be useful in many other ways (e.g., the global tuning of pipelines based on partial training data). This work was sup-ported in part by NSF Awards IIS-1012593 and IIS-0911036.
