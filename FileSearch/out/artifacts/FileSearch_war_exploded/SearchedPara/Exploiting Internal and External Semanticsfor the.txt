 Clustering of short texts, such as snippets, presents great challenges in existing aggregated search techniques due to the problem of data sparseness and the complex semantics of natural language. As short texts do not provide sufficient term co-occurrence information, traditional text represe nta-tion methods, such as  X  X ag of words X  model, have several limitations when directly applied to short text tasks. In th is paper, we propose a novel framework to improve the per-formance of short text clustering by exploiting the interna l semantics from the original text and external concepts from world knowledge. The proposed method employs a hierar-chical three-level structure to tackle the data sparsity pr ob-lem of original short texts and reconstruct the correspond-ing feature space with the integration of multiple semantic knowledge bases  X  Wikipedia and WordNet. Empirical eval-uation with Reuters and real web dataset demonstrates that our approach is able to achieve significant improvement as compared to the state-of-the-art methods.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clustering Algorithms, Experimentation Clustering, Short Texts, Syntactic Structure, Semantic Kn owl-edge Bases
Aggregated search aims to gather the search results from various resources and present them in a succinct format to improve the robustness and usability of the systems. One of the key issues in state-of-the-art aggregated technolog y is  X  X ow should the information be presented to the user? X  Traditionally, browsing through search results in the form of a ranked list is inconvenient for users to effectively loca te their interests. To address this problem, many research and commercial aggregated search systems, such as DIGEST [26] and Clusty 2 , provide clustering of relevant search results to make the information more systematic and manageable. Consequently, these systems facilitate users X  quick grasp ing of their interests by examining the overview of the subtopic s provided by the clustering module.
 Short texts, such as the snippets, product descriptions, QA passages and image captions etc., have played impor-tant roles in current Web and IR applications. Successful processing short texts is essential for aggregated search s ys-tems. However, unlike standard texts with lots of words and their statistics, short texts, which only consist of a few phrases or 2 X 3 sentences, especially present great chal -lenges in clustering. They do not provide sufficient word co-occurrence or context shared information for effective sim-ilarity measure [23], which is the basis of clustering meth-ods [15]. Therefore, the conventional texts clustering met h-ods may fail to achieve satisfactory results when they are directly applied to short text tasks [21].

To tackle the data sparseness problem, several methods have been proposed in the literatures. One is the basic rep-resentation of texts, called surface representation [18, 1 9], exploits phrases in the original texts from different aspect s to preserve the contextual information. However, NLP tech-niques, such as parsing, are not employed as it is time con-suming to apply such techniques to analyze the structure of standard text in detail. As a result, the methods fail to per-form deep understanding of the original text. Another limi-tation of such methods is that they did not use world knowl-edge, which has been found to be useful in dealing with the semantic gap in text representation [9]. For example, the first snippet returned by Google using  X  X un X  as the query does not contain any words or phrases related to  X  X racle  X , while we learn that these two companies are highly relevant from Sun X  X  homepage via the link of the snippet. Because they have no common words or phrases, this snippet can not be successfully clustered into the  X  X racle X  related clu s-ters. Thus, one obvious approach is to enrich the contexts of basic text segments by exploiting world resources and such methods have been found to be effective in narrowing the se-mantic gap in different tasks. Urena et al. [28] showed that the integration of WordNet effectively improved the perfor-http://www.yr-bcn.es/sigir08
Previously known as Vivisimo. http://www.clusty.com/ mance of text classification and word sense disambiguation tasks. Gabrilovich and Markovitch [11] proposed to com-pute text semantic relatedness by representing the meaning of text as a weighted vector of Wikipedia-based concepts.
In this paper, we present a novel framework to improve the clustering of short texts by incorporating both the rich internal and external semantics. Internal semantics aim to provide a deep understanding of the original short texts and external semantics incorporate the concepts derived from t he world knowledge to reduce the semantic gap. Our framework consists of three steps. First, we obtain the internal seman -tics based on hierarchical representation of the original s hort text by applying NLP techniques. Original features and seed phrases are carefully extracted from different levels of the hi-erarchical structure. Original features serve as part of the feature space for clustering and seed phrases provide a solid basis for feature generation in next step. Second, a phrase selection approach is introduced to eliminate the informa-tion duplicate among seed phrases by measuring the seman-tic similarity between them. Based on the seed phrases , we employ a feature generation strategy that leverages multi-ple resources and utilizes the advantages of each knowledge base, i.e. Wikipedia and WordNet, to generate high qual-ity external semantics. These external semantics serve as the external features , in conjunction with the original fea-tures generated from the first step to construct the feature space for clustering. Finally, a feature selection method i s proposed to avoid negative impact of the huge number of fea-tures from world knowledge. In our method, based on the hierarchical structure, we elaborately separate the phras es in the original short texts from different granularity to con -struct the original features and seed phrases . The latter are used as the basis for generating the external features im-plied in the world resources. In this way, both the internal and external information are better utilized based on the hierarchical structure in the proposed framework, and thei r contribution are extensively exerted. The empirical resul ts on two datasets using different clustering algorithms show the effectiveness of our proposed method.

The rest of this paper is organized as follows: Section 2 introduces related work. Section 3 presents the architec-ture of our proposed framework. Details of the proposed approaches are given in Sections 4, 5 and 6. Experimental results are presented in Section 7. Section 8 concludes the paper with directions for future work.
Many methods have been proposed to improve the repre-sentation of standard text for clustering and classificatio n. These methods can generally be divided into two categories. One category is the traditional text representation meth-ods called surface representation. Terms [19], name enti-ties [18] and phrases [3] were extracted from the original text to construct the feature space. Another category is to enrich the text representation based on  X  X ag of words X  model by generating external features from linguistic and collaborative knowledge bases. Hotho et al. [14] observed that additional features from WordNet can improve cluster-ing results. Gabrilovich and Markovitch [9, 10] analyzed th e documents and mapped them onto the ontology concepts of Wikipedia and ODP ( X  X pen Directory Project X ), which in turn induced a set of features that augment the standard  X  X ag of words X . The experimental results of integrating col -laborative knowledge bases show improvements as compared to the  X  X ag of words X  baselines in different tasks.
However, these approaches have several inherent limita-tions. The surface representation based techniques encoun ter the common semantic gap problem due to the lack of world knowledge [9]. As background knowledge, Dave et al. [7] utilized WordNet synsets independently as additional fea-tures for document representation and found that the per-formance of clustering decreased in his experiments; while the use of collaborative knowledge bases by current systems are limited to the user-defined categories and concepts in those repositories [15].

Several clustering techniques were employed to place the search engine snippets to their highly relevant topic-cohe rent groups. Some of the methods [5, 13] first clustered the snip-pets and then summarized each cluster to generate a cluster label. In contrast, other methods [30, 31] first extracted some common phrases from the set of snipeets as the clus-ter label and then created clusters according to these key phrases. However, these two kinds of methods highly rely on the common key phrases appearing in the texts and ig-nore the implicit semantic relationship between the phrase s.
World knowledge bases have been found useful in improv-ing the short text representation. Kohomban and Lee [17] built a word sense disambiguation system to tackle the data scarcity problem. This system trains the classifier using grouped senses for verbs and nouns according to the top-level synsets from WordNet and is able to effectively pool the training cases across senses within the same synset. Sa-hami et al. [25] addressed the data sparseness by leveraging web search results to provide greater context for short text s. This method shows the effectiveness for suggesting related queries to search engine users in a large-scale system. Re-cently, some methods were proposed to tackle the problems of data sparseness and semantic gap in short texts clusterin g or classification by exploiting world knowledge. Somnath et al. [1] proposed a method to enrich short texts represen-tation with additional features from Wikipedia. Although this method only used the titles of Wikipedia articles as additional external features, it showed improvement in the accuracy of short texts clustering . Phan et al. [23] pre-sented a framework for building classifiers that deal with short texts from Web and achieved significant quality en-hancement. The underlying idea of the framework is to col-lect a large-scale external data collection, and then build a classifier on both labeled data and external semantics for each classification task.
In this Section we introduce the proposed framework that aims to improve the clustering of short texts by exploiting the internal and external semantics. The workflow consists of three consecutive phases, including Hierarchical Resol u-tion, Feature Generation and Feature Selection, as shown in Figure 1. In our framework, internal semantics represent the features from the original text by employing the three-level hierarchical structure, while external semantics re pre-sent the features derived from external knowledge bases.
For ease of illustration, in Figure 1, we present an example for reconstructing feature space of a Google snippet, which describes a famous movie  X  X he Dark Knight X .
Hierarchical Resolution Phase Short texts have the characteristics of sparsity, noisy and non topic-focus due to their limited length. When using  X  X ag of words X  model to represent short texts, it neglects contextual information and hence often leads to synonymy and polysemy problems [15]. In another way, the X  X ag of phrases X  model in representation of short text is insufficient to provide enough term occurring information for clustering [16]. Thus a method which can better make use of the limited original text is necessary for such tasks. Many NLP techniques have achieved great suc-cess by using parser to analyze a sentence [4]. Therefore, in -spired by the structure of parsing tree, we resolve the short text in a hierarchical view to extract the three-level inter nal semantics by employing NLP techniques. Each level of in-ternal semantic features have their different characterist ics, and together, they preserve the syntactic structure of shor t text from multiple aspects. From the hierarchical structur e, we extract the original features which serve as part of the feature space for clustering, and seed phrases , which provide an informative basis for generating external semantics in t he next phase.

Feature Generation Phase Internal semantics are ex-tracted from the original texts in hierarchical resolution phase, however, they are still inadequate for the representation o f short texts due to the semantic gap. It is difficult to deter-mine whether two texts are semantically similar only by con-sidering their original term co-occurring information. Th ere-fore, we propose to employ feature generation techniques to enrich their representation space by leveraging repositor ies of world knowledge.

Feature generation consists of two steps, the construction of basic features and the generation of external features . The basic features should be informative for generating divers e external semantics and effective in avoiding producing nois y or redundancy features. Therefore, we first employ a simi-larity measure algorithm to eliminate duplicates in the seed phrases and use the remaining ones as the basic features for feature generation. For each seed phrase , we employ rules to measure whether WordNet or Wikipedia is more appro-priate as semantic knowledge base. For Wikipedia pages, we not only use the explicit concepts, such as the titles and links terms, but also extract the hidden topics [23] to be incorporated into the external semantics. Also, the lexica l features generated by WordNet are added as a complement.
Feature Selection Phase Features generated from world knowledge are unstructured and the huge number of exter-nal features leads to the  X  X urse of dimensionality X , which brings in negative impact to the feature space for clusterin g. Therefore, feature selection step is employed to refine the unstructured features derived from Wikipedia and to ensure that the reconstructed feature space is compact and effectiv e for clustering.

In next Sections, we will present the details of these three phases.
Original text normally contains precise and valuable infor -mation. The snippet in Figure 1 contains information on the description ( X  X he best American film X ), director ( X  X hristo -pher Nolan X ) and title ( X  X he Dark Knight X ) of the film. However, the  X  X ag of words X  approach, which ignores the contextual information of the text, is not able to capture the rich semantics of this short text. On the other hand, using text segments generated by spliter or chunker to rep-resent the text is too sparse to locate the centroids of infor -mation from the noisy text [21]. Therefore, we propose to exploit the internal semantics of short texts, which not onl y preserves the contextual information but also avoids data sparsity. A snippet typically comprises two or three sentences. In NLP tasks, people usually employ parsing to mine the syn-tactic structure contained in the sentences [4]. Figure 2 il -lustrates an example of the syntax tree 3 for the snippet in Figure 1. From this syntactic structure, we can see the snip-pet contains three important components, including Syn-tax Node(S, sentence), Branch(VP, NP verb phrase &amp; noun phrase) and Leaf(words) of the tree. To preserve the origi-nal information conveyed by the snippet, we should make full use of these three components from the syntax tree. Under this scenario, we propose a method to decompose the orig-inal text into a three-level top-down hierarchical structu re  X  segment level , phrase level and word level .
 Segment Level: From the observation of short texts in Web applications, they do not always comprise fully struc-tured sentences but segments like  X  X ul 18, 2008 X  as shown in Figure 2. We broadly define a text segment or sentence
Due to limitation of space, we present only part of the syntax tree in Figure 2. DT The as a single unit in this level. The text is split into segments with the help of punctuations. Each segment contributes to provide a subtopic or one aspect of the original text.
Segment level features are informative and have been found to be beneficial in generating quality external features [1]. However, the features at this level are generally ambiguous and often fail to convey the exact information to represent the short text [33]. Therefore, we need to further exploit the internal semantics contained in the original text from the other two components of the syntax tree.

Phrase Level: When people speed-read through a text, they do not fully parse the sentence but instead look for  X  X ey phrases X  [20]. Thus, shallow parsing [12] is adopted to divide sentences into a series of words that together com-pose a grammatical unit, mostly NP(noun phrase), VP(verb phrase) and PP(preposition phrase). The output of shallow parsing to the snippet in Figure 1 is as below: After stemming and removal of stop-words from the phrases generated by the shallow parser, the NP and VP chunks are employed as phrase level features.

Phrase level features are informative and more focused on one concept. They can readily be mapped to the rel-evant articles in world knowledge bases. Inevitably, there will be many information duplicates between segment level and phrase level features. We rely on the seed phrase se-lection step to eliminate the redundant features, preservi ng only those that genuinely characterize the text. Employing features at this level independently to represent text is to o sparse due to the limited length of short text. Therefore, we incorporate word level features to better represent the short text.

Word Level: The cost of full parsing to analyze the sen-tence is expensive in both time and resources. Therefore, we do not use traditional syntactic technologies to obtain key words but decompose the phrase level features directly. We choose the non-stop words contained in NP and VP chunks to build the word level features.

Features at this level further remove the meaningless words in the short texts, thus offering a more effective feature spac e than X  X ag of words X  X odel. They serve as additional features to represent the text and tackle the data sparseness to some degree. The negative characteristic of word level features is that they are too general to generate meaningful external features .
The three-level features from different granularities not only preserves the word ordering information in the short text, but also avoids data sparseness problem to some ex-tent. As mentioned before, the segment level features are not appropriate to represent the text due to their sparse-ness. Thus, we extract features at phrase level and word level to compose our original feature set. First, the phrase level features, which contain the original contextual infor-mation implied in the hierarchies, are exploited to tackle one of the main impediments in NLP  X  polysemy. For ex-ample, the word  X  X night X  X ay be regarded as  X  X an to whom the sovereign has given a rank of honour X  or  X  X an raised to honourable military rank X . The phrase  X  X he Dark Knight X  builds relationship with the movie and clearly indicates th at the word  X  X night X  here refers to the second meaning. An-other example is that although every single word in  X  X uly 18 2008 X  is general and meaningless, when we consider this phrase as a unit, it is possible to build connection with top-ics of  X  X ovie X  and  X  X he dark knight X  from some top ranking results returned by Google. Second, the word level features contribute as a complement to avoid the problem of data sparseness. The features from two levels of the hierarchica l structure support each other to comprise the original feature space.
Consider again the snippet in Figure 1, even by mining the original text, it is still inadequate to build the seman-tic relationship with other relevant concepts. For example ,  X  X he Dark Knight X  and  X  X atman X  are different names of one movie, but they cannot be linked as the same concept with-out additional information from external knowledge. To nar -row the semantic gap, we propose to mine external features to enrich the text representation.

In this Section, we present two steps of feature generation: the extraction of seed phrases from the internal semantics and the generation of external features from seed phrases .
Among internal semantics, features at segment level and phrase level are informative to cover the key subtopics de-scribed in the short texts. We thus use features at these two levels to construct the seed phrases . However, there are redundancies between these two kinds of features as phrase level features are in a way derived from segment level fea-tures. For example, the segment level feature  X  X hristopher Nolan X  X  The Dark Knight is revelatory visceral X  generates three phrase level features [NP Christopher Nolan X  X ], [NP The Dark Knight] and [NP revelatory visceral]. If we employ all these features as seed phrases , they would produce many duplicate information between the segment level feature and [NP The Dark Knight]. Therefore, we propose to measure the semantic similarity between phrase level features and its parent segment level feature to eliminate information redun-dancy.

Several methods have been proposed to calculate the se-mantic similarity between words [27] or associations [2] us ing web search. However, along with the increasing scale of the web, the page counts provided by some commercial search engines are not so reliable [6]. Thus instead of simply using the search engine page counts, we propose a phrase-phrase semantic similarity measure algorithm using co-occurrenc e double check in Wikipedia to reduce the semantic duplicates . For Wikipedia we download the XML corpus [8], remove xml tags and create a Solr 4 index of all XML articles.
Let P denotes a segment level feature, P = { p 1 , p 2 , . . . , p where p i denotes the phrase level feature contained in P. The segment level feature is too sparse to calculate its frequency directly. Therefore, we calculate the semantic similarity be-tween p i and { p 1 , p 2 , . . . , p n } as InfoScore ( p p  X  which has the largest similarity with other features in P will be removed as the redundant feature.

Given two phrases p i and p j , we use p i and p j separately as query to retrieve top C Wikipedia pages from the built index. The total occurrences of p i in the top C Wikipedia pages retrieved by query p j is denoted as f ( p i | p j ); and we define f ( p j | p i ) in a similar manner. The total occurrences of p i in the top C Wikipedia pages retrieved by query p i is denoted as f ( p i ), and similarly for f ( p j ). The variants of three popular co-occurrence measures [6] are defined as below: where WikiDice is a variant of the Dice coefficient. where WikiJaccard is a variant of the Jaccard coefficient. where WikiOverlap is a variant of the Overlap(Simpson) co-efficient. http://lucene.apache.org/solr/ scores are normalized into values in [0 , 1] range using the linear normalization formula defined below: where k is from 1 to n 2 2 . We again define W J ij and W O a similar manner for WikiJaccard and WikiOverlap respec-tively. A linear combination is then used to incorporate the three similarity measures into an overall semantic similar ity between two phrases p i and p j , as follows: where  X  and  X  weight the importance of the three similar-ity measures. As text clustering is an unsupervised method, where we do not have any labeled data to tune the param-eters. We thus empirically set  X  and  X  to equal weight.
For each segment level feature, we rank the information score defined in Equation 6 for its child node features at phrase level .

Finally, we remove the phrase level feature p  X  , which del-egates the most information duplicate to the segment level feature P , and it is defined as:
Wikipedia, as background knowledge, has a wider knowl-edge coverage than WordNet and is regularly updated to re-flect recent events. On the other hand, as the construction of WordNet follows theoretical model or corpus evidence, it contains rich lexical semantic knowledge [32]. Under thi s scenario, we take Wikipedia as the principle semantic knowl -edge source and WordNet as the secondary one.

Gabrilovich and Markovitch [10], as well as Hu et al. [15] preprocessed the Wikipedia data to collect Wikipedia con-cepts. Preprocessing of Wikipedia ignores the valuable con -textual information of Wikipedia plain texts and always en-counters problems when mapping the original text to ap-propriate concepts. Therefore, in this study we preserve th e original pages of Wikipedia with the built-in Solr index as described in Section 5.1. The external feature generation algorithm is illustrated in Figure 3. Given a seed phrase , we first employ heuristic rules to distinguish which knowledge base is more appropriate. If the phrase contains more than one non-stopword, then this phrase is regarded as containing enough information to reflect one aspect or subtopic of the short text. For this kind of phrase, we can retrieve accurate Wikipedia pages with the help of Solr search engine. On the other hand, phrase that has one or zero non-stopword is regarded as too general to generate accurate concepts from Wikipedia. We thus use WordNet as complement to deal with such phrases from lexical aspect.

In order to retrieve the appropriate pages from the large scale Wikipedia corpus, we derive queries based on seed
Algorithm 1 : GenerateFeatures( S ) input : a set S of seed phrases output : external features EF
EF  X  null for seed phrase s  X  S do return EF phrase arising from segment level or phrase level separately. As the key information of seed phrases from phrase level is more focused, we build the  X  X ND X  query which requires the retrieved pages to contain every term in the phrase. On the other hand, the seed phrases from segment level are infor-mative but sparse, we thus build  X  X R X  query 5 which means there is no guarantee that the retrieved Wikipedia pages wil l contain every term in the phrase. We use these two kinds of queries to retrieve the top  X  articles from the Wikipedia corpora. Similar to previous work [1], we extract titles and bold terms (links) from the retrieved Wikipedia pages to serve as part of the external features . To discover the intrin-sic concepts hidden in the plain texts, we adopt an effective key phrase extraction algorithm  X  Lingo [22], which uses algebraic transformations of the term-document matrix and implements frequent phrase extraction using suffix arrays. The key phrases extracted from the Wikipedia pages are added to the external feature space. For example, we may obtain extrinsic concepts X  X atman X  X or the phrase X  X he Dark Knight X  and the intrinsic concepts like  X  X ireworks X  or  X  X oke r X  by mining the related pages.

If the phrase contains only one non-stopword (e.g.  X  X n his car X ), WordNet synsets are applied to extract similar concepts (e.g.  X  X tuo X ,  X  X utomobile X ,  X  X utocar X ) of the sub -stantive ( X  X ar X ).

With this scheme, we can easily tackle the phrase sense synonymy from both the semantic and lexical aspects. The semantic synonymy of phrases is handled by the external features generated from Wikipedia, while the similar  X  X eigh-bors X  from WordNet synsets help to tackle the lexical syn-onym. For example, the phrase  X  X atman X  is generated by  X  X he Dark Knight X  X nd they are highly semantically related; and we can also distinguish that  X  X n his car X  talks about the same scene as X  X n his automobile X  X ith the help of the Word-Net.
As the construction of Wikipedia follows the non-binding
For more details about  X  X ND X  and  X  X R X  query syntax, please refer to http://wiki.apache.org/solr /SolrQueryS yn-tax guidelines and the data quality is only under social control by the community [32], it often leads to noise in the cor-pus. Meanwhile, a single text may generate a huge number of features. These overzealous external features bring ad-verse impact on the effectiveness and dilute the influence of valuable original information. Therefore, we conduct feat ure filtering to refine the unstructured or meaningless features and apply feature collection to avoid aggravating the  X  X urs e of dimensionality X .

Feature Filtering: We formulate empirical rules to re-fine the unstructured features obtained from Wikipedia page s, some typical rules are as follows:
External Feature Collection: We have obtained n 1 original features in Section 4.2, and now collect n 2 exter-nal features to construct n 1 + n 2 dimension feature space for clustering. The number of external features we need to collect is determined by: where  X  is the fraction of external features to the whole fea-ture space in an interval [0 , 1]. In the extreme cases,  X  = 0 means the feature space contains no external features and  X  = 1 indicates the features in the feature space are all from external features .

First, tf-idf weights of all the generated features are cal-culated. One seed phrase s i (0 &lt; i  X  m ) may generate k diversity of the external features , we select one feature for each seed phrase . Thus m features are collected as follows:
Second, the top n 2  X  m features are extracted from the remaining external features based on their frequency. These frequently appearing features, together with the features from the first step, to construct the n 2 external features .
Finally, the feature space for clustering is constructed wi th the combination of original features and the collected exter-nal features .
In this Section, we present empirical evaluation results to assess the effectiveness of our proposed framework for short texts clustering. In particular, we conduct experiments on two datasets using six different text representation method s and the results show that our approach is more effective than the state-of-the-art methods. Moreover, some factors that appear to affect the experiment are further discussed. Table 1: Statistics of query length from Google Trends during Nov. 26th 2007  X  Nov. 25th 2008 Table 2: The selected hot queries in Web Dataset
Since the average length of 1000 snippets crawled from the Web is 21.72, we define texts that contain no more than 50 words (including stop words) as short texts. To evaluate our methods in web applications, two test collections are employed in our experiment as the benchmark datasets.
Reuters-21578 6 : We remove the texts which contain more than 50 words and filter those clusters with less than 5 texts or more than 500 texts. Thus it leaves 19 clusters com-prising 879 texts. The number of texts in each cluster ranges from 6 (the cluster  X  X ncome X ) to 438 (the cluster  X  X cq X ).
Web Dataset: This dataset is built to simulate a real web application. As the users X  interests are varied, we choo se queries of different length according to the statistics of Go ogle Trends 7 during Nov. 26th 2007 to Nov. 25th 2008, as shown in Table 1. Ten hot queries of diverse topics are selected fro m Google Trends according to the percentage of query length. The selected queries are as shown in Table 2. Top 100 snip-pets for each query are retrieved via GoogleAPI to build a 10-category Web Dataset with 1000 texts.
In this experiment, we employ a clustering package from the freely available machine learning software Weka3 [29] and the number of clusters are predefined. Two clustering algorithms, K-means and EM are employed in this study to verify the effectiveness of six different text representat ion methods, as defined below: http://daviddlewis.com/resources/testcollections/re uters21578 http://www.google.com/trends
We evaluate the performance using F 1 measure and Aver-age Accuracy .

F 1 measure: A combination of both precision and recall that measures the extent to which a cluster contains only objects of a particular class and all objects of that class.
Average Accuracy: A statistical measure of how well a classification test correctly identifies or excludes a condi tion Table is defined as follows: where TP, TN, FP, FN are defined in Table 3.

In Table 3, TP (true positive) denotes that two texts are manually labeled with the same class and clustered into same cluster; FN (false negative) denotes that two texts are man-ually labeled with different classes but clustered into same one. TN (true negative) and FP (false positive) are defined in a similar manner.
As mentioned in Section 5.1, we retrieved top C Wikipedia pages to compute the semantic similarity between two phrase s. Given the large scale of Wikipedia corpus and the ranking schema provided by Solr search engine, our experimental re-sult is not sensitive to the number of retrieved documents [6 ]. Similarly we found the performance is independent of the value of the top  X  Wikipedia pages used in Section 5.3. As a result, we empirically set the value C = 100 and  X  = 20 in our experiment.

In Equation 5, WikiDice , WikiJaccard and WikiOverlap were combined to measure the similarity between two phrases using equal weights. We conducted extensive experiments on different parameter settings and found that assigning equal weights to combine the measures always show the best re-sults. We conjecture that it is because these three measures are all based on the occurrences of phrases in Wikipedia and the correlation coefficients of the measures are similar [2]. Therefore, we set  X  =  X  = 1 3 in our experiments.
As discussed in Section 6,  X  controls the influence of ex-ternal features to the whole feature space, which is crucial in our experiment. We empirically set  X  as 0 . 5 for SemKnow , which means that the external features have the same num-ber as the original features . The effect of external features in the experiment will be discussed in Section 7.5.
Experimental results of the six text representation meth-ods on both data collections using the two clustering algo-rithms are respectively reported in Table 4 and Table 5. In the Tables, AveAccuracy denotes the Average Accuracy for each method and Impr represents the percentage improve-ment of the methods as compared to BOW method. In the experiment, each result denotes an average of 10 test runs by randomly choosing the initial parameters for the clusterin g method.

From Tables 4 and 5, we observe that BOW+WN , BOW+Wiki and BOW+Know augment the performance of BOW model on both datasets using the two clustering algorithms. The performance of BOW is improved by incorporating exter-nal features generated from WordNet, Wikipedia and their combination respectively. It demonstrates the benefit and potential of integrating world knowledge into the represen -tation of short texts. BOW+Wiki achieves much better performance than BOW+WN ; we conjecture that it is be-cause of the huge up-to-date concept space in Wikipedia, which is more appropriate for the snippets that occurred re-cently. BOW+Know further improves the performance over BOW+Wiki , demonstrates the benefit of judicious integra-tion of both Wikipedia and WordNet as knowledge sources.
We note that BOF also achieves better performance as compared with BOW model. We believe that this is be-cause of the integration of internal semantics from hierar-chical resolution of the original text. It, however, achiev es the least augments with respect to Baseline as compared to BOW+WN , BOW+Wiki and BOW+Know . We believe that it is mainly due to the lack of external knowledge.
Comparing our proposed SemKnow with the other five methods, it achieves the best F 1 measure and Average Ac-curacy scores on both datasets using k-means and EM clus-tering algorithms. The highest improvement with respect to BOW is obtained on the Web Dataset using k-means algo-rithm. We apply t-test to compare the performance between SemKnow and all the Baselines . The results demonstrate that our method significantly outperforms the state-of-the -art methods with the p -value &lt; 0 . 01. Among the meth-ods that integrate semantic concepts from Wikipedia and WordNet, SemKnow obtains superior performance as com-pared to BOW+Know . We believe that the improvement stems from the ability of the hierarchical structure in bett er utilizing the characteristics of features at different leve ls to generate high quality semantics for clustering.
In order to better understand the effect of external fea-tures , we conduct experiments using BOF with different sizes of external features on both datasets. As clustering is unsupervised, it is difficult to obtain prior knowledge. Thus , we evaluate the clustering performance by setting the frac-tion of external features  X  from 0 to 1 at increments of 0 . 1. The results are presented in Figures 4 and 5. The BOF line depicts the performance of  X  X ag of original features  X , while the SemKnow curve shows the performance of our proposed framework which integrates both original features and ex-ternal features .
 Figures 4 and 5 depict the results on the Reuters and Web Dataset using k-means clustering algorithm. In Figure 4, the curve of SemKnow reaches a peak at  X  = 0 . 2. As the fraction of external features grows, the performance of SemKnow de-clines. When  X  &gt; 0 . 5, SemKnow performs worse than BOF , which means the external semantics begin to bring in nega-tive impact on the feature space. In Figure 5, the SemKnow achieves the best performance at  X  = 0 . 3 and the trend of curve is similar to that in Figure 4. When the fraction is close to 1, the performance of SemKnow is close to BOF . Similar phenomena have been observed for EM algorithm; we omit the results owing to lack of space.

In general, SemKnow achieve the best performance at a small external feature set size of  X  = 0 . 2 or  X  = 0 . 3. The possible explanation should be the increasing noise accom-panied with the larger external feature set. This result also demonstrates the utility of external feature selection str at-egy. As seen from the figures, SemKnow arrives at the best performance with a different fraction of external features on different datasets, while the clustering algorithm does not appear to be a major factor to the overall performance, we can infer that different data distributions influence the per -formance of our framework.

In Table 6, we summarize the performance of BOW and our method with optimal parameter  X  . As compared to BOW , we can achieve 12 . 35% and 30 . 39% improvement us-Figure 4: Impact of the parameter  X  on Reuters using K-means algorithm Figure 5: Impact of the parameter  X  on Web Dataset using K-means algorithm ing k-means , 15 . 40% and 16 . 56% improvement using EM , on the two datasets respectively. The improvement of optimal  X  on SemKnow is much higher than that in Tables 4 and 5 with respect to BOW . It infers that although the average performance of our method is satisfactory, it can be greatly improved by optimizing the important parameter  X  . There-fore, it is possible to manually label some data to optimize the parameter settings when the application has higher re-quirements in clustering accuracy.
To further analyze the reasons why our proposed method augments the performance of clustering as compared with the previous methods, we illustrate with the feature space for the snippet of Figure 1, as shown in Table 7. In the Table, the features on the upper section of the first row are from word level of internal semantics, and those in the lower section of first row are from phrase level . The exter-nal features with dagger are explicit concepts available in Wikipedia articles and features with star are mined from the plain texts of Wikipedia articles. As compared to the previous methods, our framework further removes the fea-ture X  X ikely X  X rom the word level features because it is not NP nor VP chunks. This feature is not stop-word but is appar-ently meaningless for text representation. Several concep ts, such as the date ( X  X uly 18 2008 X ), director ( X  X hristopher Table 7: Feature space for the snippet in Figure 1 Nolan X ), name of the movie ( X  X he Dark Knight X ) and com-ments ( X  X evelatory visceral X ), are extracted to construct the original features and they can better tackle the polysemy problem as discussed in Section 4.2. Meanwhile, the ex-ternal features, especially the features generated by the f ull extraction of Wikipedia such as the Internet movie database ( X  X MDB X ), title-role in the movie ( X  X oker X ) and another nam e of the movie ( X  X atman X ) handle the synonymy from lexical and semantic aspects well as discussed in Section 5.3. With better handling of the two impediments, namely synonymy and polysemy in NLP, our method achieves satisfactory per-formance in the clustering of short texts.
In this paper, we proposed a novel framework to augment the clustering accuracy of short texts by exploiting the in-ternal and external semantics. The internal semantics are extracted by resolving the original texts with a three-leve l hierarchical view and the external semantics are built with the explicit and implicit concepts derived from multiple se -mantic knowledge bases. The combination of internal and external semantics well tackled the problems of data sparse -ness and semantic gap in short texts. Empirical evaluations demonstrated that our framework significant outperformed all the baselines including previously proposed knowledge -based short text clustering methods on two datasets. There are a number of interesting extensions of this work. As this work is for aggregated search, the efficiency of the whole framework should be optimized for real applications. Moreover, we will explore more tasks in NLP and informa-tion retrieval using the internal and external semantics ge n-erated by our proposed framework. [1] S. Banerjee, K. Ramanathan, and A. Gupta.
 [2] H.-H. Chen, M.-S. Lin, and Y.-C. Wei. Novel [3] H. Chim and X. Deng. Efficient phrase-based [4] M. Collins. Three generative, lexicalised models for [5] D. R. Cutting, D. R. Karger, and J. O. Pedersen. [6] B. Danushka, M. Yutaka, and I. Mitsuru. Measuring [7] K. Dave, S. Lawrence, and D. M. Pennock. Mining the [8] L. Denoyer and P. Gallinari. The wikipedia xml [9] E. Gabrilovich and S. Markovitch. Feature generation [10] E. Gabrilovich and S. Markovitch. Overcoming the [11] E. Gabrilovich and S. Markovitch. Computing [12] J. Hammerton, M. Osborne, S. Armstrong, and [13] M. Hearst and J. Pedersen. Reexamining the cluster [14] A. Hotho, S. Staab, and G. Stumme. Wordnet [15] J. Hu, L. Fang, Y. Cao, H. Zeng, H. Li, Q. Yang, and [16] F. Keller, M. Lapata, and O. Ourioupina. Using the [17] U. S. Kohomban and W. S. Lee. Learning semantic [18] G. Kumaran and J. Allan. Text classification and [19] D. Lewis and W. Croft. Term clustering of syntactic [20] T. Marinis. Psycholinguistic techniques in second [21] D. Metzler, S. Dumais, and C. Meek. Similarity [22] S. Osinski, J. Stefanowski, and D. Weiss. Lingo: [23] X.-H. Phan, L.-M. Nguyen, and S. Horiguchi.
 [24] M. F. Porter. An algorithm for suffix stripping. [25] M. Sahami and T. Heilman. A web-based kernel [26] M. Sushmita, S.;Lalmas. Using digest pages to [27] E. Terra and C. Clarke. Frequency estimates for [28] L. Urena-Lopez, M. Buenaga, and J. Gomez.
 [29] I. Witten and E. Frank. Data Mining: Practical [30] O. Zamir and O. Etzioni. Grouper: a dynamic [31] H. Zeng, Q. He, Z. Chen, W. Ma, and J. Ma. Learning [32] T. Zesch, C. Muller, and I. Gurevych. Extracting [33] C. Zhang, N. Sun, X. Hu, T. Huang, and T.-S. Chua.
