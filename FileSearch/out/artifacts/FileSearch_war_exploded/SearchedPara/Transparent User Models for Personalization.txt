 Personalization is a ubiquitous phenomenon in our daily on-line experience. While such technology is critical for help-ing us combat the overload of information we face, in many cases, we may not even realize that our results are being tailored to our personal tastes and preferences. Worse yet, when such a system makes a mistake, we have little recourse to correct it.

In this work, we propose a framework for addressing this problem by developing a new user-interpretable feature set upon which to base personalized recommendations. These features, which we call badges , represent fundamental traits of users (e.g.,  X  X egetarian X  or  X  X pple fanboy X ) inferred by modeling the interplay between a user X  X  behavior and self-reported identity. Specifically, we consider the microblog-ging site Twitter, where users provide short descriptions of themselves in their profiles, as well as perform actions such as tweeting and retweeting. Our approach is based on the insight that we can define badges using high precision, low recall rules (e.g.,  X  X witter profile contains the phrase  X  X pple fanboy X  X ), and with enough data, generalize to other users by observing shared behavior. We develop a fully Bayesian, generative model that describes this interaction, while allow-ing us to avoid the pitfalls associated with having positive-only data.

Experiments on real Twitter data demonstrate the effec-tiveness of our model at capturing rich and interpretable user traits that can be used to provide transparency for per-sonalization.
 G.3 [ Mathematics of Computing ]: Probability and Statis-tics  X  Work done while at Microsoft Research, Cambridge, UK. personalization, Twitter, graphical models, transparency
Whether we are reading news, searching the Web or con-necting with our friends on a social network, personalization plays an important X  X f often hidden X  X ole in shaping our ex-perience. As the scale of online content grows, the ability to tailor information to the tastes and preferences of individ-ual users is becoming critical for maintaining a positive user experience. For example, different users may prefer news ar-ticles from different blogs, diners with different tastes may trustdifferentrestaurantreviews,andsoon.

However, along with the promise of personalization come many challenges, both technical and social, that hinder its potential:
In this work, we seek to address these challenges by mak-ing personalization more transparent . In other words, users should know, (1) when personalization is happening, and, (2) how they are perceived by the system (with the ability to correct this perception as necessary).

We provide this transparency by representing each user as a set of interpretable, explainable attributes (e.g.,  X  X eg-http://www.tivo.com Figure 1: An example Twitter profile, showing an anonymized user X  X  self-reported description. Here, if we are interested in the  X  X pple fanboy X  badge, we can observe this user X  X  actions on Twitter to help us figure out what it means to be an Apple fanboy. etarian, X   X  X ipster, X  or  X  X pple fanboy X ) that we learn from user behavior. It is important that both the meaning of these attributes and whytheywereassigned to the user be readily apparent. Following the paradigm made popular by location-aware social networks such as Foursquare 2 , we refer to these attributes as badges .

In particular, we consider the microblogging site Twitter and we associate each badge with a characteristic label (e.g.,  X  X pple fanboy X ) that a user might use to describe himself in his Twitter profile. For any user, we can observe his or her profile and determine whether or not it contains a particular label. For example, the user profile in Figure 1 contains the label  X  X pple fanboy, X  which we might associate with an Apple fanboy badge. It is important to note that there is a probabilistic relationship between labels and the badges they correspond to. For example, most users who adore Apple products will not explicitly identify themselves with  X  X pple fanboy X  in their Twitter profiles 4 . Nevertheless, we wish to use the actions of those self-identified Apple fanboys to help us learn what it means to be one. We can then hope to predict which other users might also be Apple fanboys, even if they don X  X  identify themselves as such.

Moreover, in this paper, we take the view that the set of possible badges (and their corresponding labels) are defined apriori in a supervised manner. Specifically, we assume we are given some set of possible badges (e.g., as in Table 1), and wish to infer : (1)theirpresenceorabsenceforeachuser, and, (2) how they manifest themselves in terms of Twitter behavior.

In the remainder of this paper, we describe how we learn badges from user activity on Twitter, using a Bayesian frame-work to explicitly model uncertainty. We show experimental results on real Twitter users, and present both quantitative evidence and qualitative anecdotes demonstrating the effec-tiveness of our method.
We describe each user as a set of latent badges that, collec-tively, explain the user X  X  behavior. The fundamental prob-lem we seek to solve is: how do we infer the badges for each user based on observed actions and labels?
For each user u , we observe two binary vectors: 1. The label vector  X  ( u ) ,with  X  ( u ) i = 1 indicating that http://www.foursquare.com http://www.twitter.com
This stands in contrast to Foursquare, where badges are deterministic (e.g., five check-ins at an airport guarantee the  X  X et-setter X  badge). 2. The action vector a ( u ) ,where a ( u ) j =1ifuser u is ob-
We model these observations as probabilistically arising from a latent set of badges b ( u ) ,where b ( u ) i  X  X  0 whetherornotuser u has badge i . In particular, we elect to define a generative X  X ather than discriminative X  X odel; while the high precision labels may provide us with positive training examples, their low recall leads to no meaningful negative examples. Moreover, if a user chooses to decline a badge that we predict for him (e.g., he might not really be an Apple fanboy), this simply corresponds in our model to observing the latent variable b ( u ) i = 0. Additionally, we note that our model differs from traditional unsupervised latent variable models, such as topic models [2], in that the badge labels provide identifiability that we would not otherwise achieve. Thus, for example, if we define the label for badge i to correspond to those users with  X  X unner X  in their Twitter profile, then the actions explained by badge i will always correspond to (our view of) runners, which is a property we do not get with fully unsupervised topic models, such as latent Dirichlet allocation [3].
Given a particular user X  X  badge assignments, the genera-tive process for labels encodes our intuition that each label  X  i is a high precision, low recall indicator of the presence or absence of a badge. Specifically,  X  X igh precision X  here means that it is very unlikely for someone without badge i (i.e., with b ( u ) i = 0) to use the corresponding label (i.e., in his profile, while  X  X ow recall X  indicates that many users most vegetarians on Twitter do not describe themselves as  X  X egetarian X  in their Twitter profiles, it is much more rare (but not impossible) for non-vegetarians to have the word  X  X egetarian X  in their profiles.

As such, we model label  X  ( u ) i as being apriori present with a true positive rate  X  T i and false positive rate  X  F i  X  T i &lt; 1and  X  F i  X  0). Formally, we have: p ( given the user X  X  badge b ( u ) i . In other words, the presence of a badge does not necessarily imply its appearance in a user X  X  profile, and it is precisely these badges that we aim to infer.
We assume that the observed actions a ( u ) j  X  X  0 , 1 } of a user u can be explained by one or more of his latent badges i . In the Twitter domain, possible actions j might include auser re-tweeting some author, or using a particular hashtag .
For each possible badge i and action j , there is a prob-ability s ij  X  ij of associating them; it is decomposed into a context-specific rate  X  ij  X  (0 , 1) and a sparsity prior { 0 , 1 } .The s i variables for a badge i act as a mask, delineat- X  Beta (  X   X  , X   X  )  X  Beta (  X  T , X  T )  X  Beta (  X  F , X  F )  X  Beta (  X   X  , X   X  ) s |  X  i  X  Bernoulli (  X  i )  X   X  Beta (  X   X  , X   X  )  X  Beta (  X   X  , X   X  ) |  X  i  X  Bernoulli (  X  i ) ing which actions can be explained by this particular badge, and their sparsity is controlled by a badge-specific prior Given that s ij =1,thevariable  X  ij represents the proba-bility that a user with badge i undertakes action j ,inthe absence of any other badges. For example, if a user only has the  X  X unner X  badge active, and s runner , # runkeeper =1,mean-ing that the  X  X unner X  badge can explain tweeting #runk-eeper , then our user will tweet #runkeeper with probability  X 
As a user may have more than one badge active that can explain a particular action, we combine their influence in a noisy-or fashion, indicating that a user performs an action if at least one of his badges induce him to do so. Moreover, it is plausible that a user X  X  behavior is influenced not just by his particular attributes, but by the environment at large, and thus we assume a background model  X  bg ,j ,actingasa badge that every user shares, that has some probability of explaining every action.

Thus, formally, action a ( u ) j is observed if it is explained by either the background or at least one of the badges of user u , which we can write as follows 5 :
Keeping with a proper Bayesian approach, we specify prior distributions on our badge assignments b ,rates  X  T and  X  F sparsity masks s and action probabilities  X  encoding our modeling assumptions.

First, as some badges are more prevalent than others (e.g., there are likely more vegetarians on Twitter than machine learning enthusiasts), we assume that each badge assignment i is drawn from a beta-distributed prior rate  X  i ,shared across all users for each badge i .

Second, to encode that we expect the false positive rate to
We note that, by assuming the influence of the badges to be independent of each other for a particular user, we can write this  X  X t least one X  clause as the complement of a product of complements. While this assumption may be violated in practice, we posit that the computational savings we achieve by this simplification will outweigh the induced bias. be considerably lower than the true positive rate, we place separate beta priors on  X  T and  X  F , setting the hyperparam-eters accordingly.

Third, as we want each badge i to explain only a sparse set of actions, we place badge-specific beta-distributed prior rates  X  i from which we sample s ij , allowing different badges to have different degrees of sparsity.

Finally, we place vague beta priors on the action proba-bilities  X  seeking to learn these primarily from data.
A depiction of our graphical model and a summary of the full generative process can be found in Figure 2.
Given our model and the observations from each user, we wish to infer the latent badge assignments b ,aswellaswhich actions are explained by each badge (and to what degree). As computing the exact posterior probabilities in a graphical model such as this is intractable, we employ Markov Chain Monte Carlo (MCMC) methodology and estimate the poste-rior probabilities on b , s and  X  by deriving a Gibbs sampler with interleaved Metropolis-Hastings steps. In particular, we derive a col lapsed Gibbs sampler, marginalizing out  X   X  ,  X  T and  X  F , leaving only the variables of interest. This results in the following sampler: 1. Sample b . We sample each badge assignment b ( u ) i for 2. Sample s . We sample the binary variable s ij from its Table 1: The 31 badges we defined for our exper-iments, as specified by their corresponding labels. 3. Sample  X  i . To sample  X  ij , we first write its conditional 4. Sample  X  bg . We sample the background action prob-
Further details of our sampling algorithm, including com-plete derivations of all conditional distributions, can be found in the supplemental material [4].
We evaluate our model on a data set of approximately seven million active Twitter users by monitoring Twitter X  X   X  X irehose X  stream in early August 2011, recording users with non-empty profiles. We scanned through these seven mil-lion users X  X hich at the time of collection represented ap-proximately 3.5 percent of all Twitter users X  X nd manually defined a set of 31 badges by specifying a label for each one, based on the occurrence of a particular phrase or word in each user X  X  Twitter profile. For example, we define a  X  X ege-tarian X  badge by specifying: Twitter profile.
 Table 1 contains a full listing of our 31 badges. We note that while these badges were defined as a proof of concept by the authors, a real personalization system would include a principled method for defining a large quantity of badges, as discussed further in Section 5.

Of the seven million users in our data set, we identified 376,916 that had at least one of the 31 labels in their profiles. We took this subset of users and monitored the firehose for one week, from 5 August 2011 to 12 August 2011, collect-ing every tweet and retweet by these users. This resulted in a set of approximately two million tweets. From these tweets, we extracted all unique hashtags (e.g., # runkeeper ) and retweeted users (e.g., @MacRumors ), defining a vocabu-lary of actions. We culled this vocabulary to remove any actions belonging to just a single user, leaving us with a fi-nal vocabulary of 32,030 actions (broken down into 18,003 hashtags and 14,027 retweets), performed by 75,880 differ-ent users. The most common action over this time period was the hashtag # londonriots , referring to the riots that took place in the British capital during the week of our data collection. Moreover, Figure 3(a) shows how many users in the data set described themselves using each of the 31 labels, with the most common being  X  X rtist. X 
Finally, we note that our model is not dependent on Twit-ter, as both the labels and the actions could be defined to take advantage of any other user signals one has access to, including location, shopping patterns, clicks, query logs and so on 6 . Twitter is, however, a convenient open platform for experimentation.
We ran our sampler on the data set described above, esti-mating posterior probabilities of badge assignments ( b )and badge definitions (  X  and s ) under our modeling assump-tions. For each iteration, our sampler has time complexity O ( B ( F + N )). Our implementation in the F# functional programming language achieves a runtime of approximately 3.5 minutes per sample, which is the time it takes to make a single, complete pass over more than 3.3 million random variables. Our hyperparameter settings and initialization condition are detailed in the supplemental material [4].
In an effort to compare our methodology to a state-of-the-art alternative, we wanted a model that: (1) can represent multiple labels per user; (2) provides a mechanism for in-terpreting the definitions of each badge; and, (3) can proba-bilistically infer badge assignments, especially in cases where the corresponding label is not present for the particular user.
We found the most suitable comparison technique to be the labeled latent Dirichlet allocation (labeled LDA) model of Ramage et al. [13]. This model makes a slight, but impor-tant, modification to the original LDA model, by assuming that each document is labeled with one or more tags, with each tag associated with one and only one topic. Thus, e.g., a document labeled with tags 1, 2 and 5 is assumed to have been generated from topics 1, 2 and 5, and no others. Like our model, labeled LDA provides a level of identifiability not obtained in traditional unsupervised approaches.

In order to adapt labeled LDA to our setting, we first as-sociate a tag (and thereby a topic) with each badge, as well as an additional tag corresponding to a background topic. In our particular example, this leads to 32 unique tags. We then run labeled LDA twice: once for learning badge defi-
For instance, we can imagine generalizing  X  X abels X  to in-stead represent any high-precision, low-recall rule that we intend on associating with a badge, not necessarily based on the content of a user X  X  Twitter profile. us a 5% significance threshold. nitions, and once for inferring badge probabilities for each user. 7 This two-stage approach contrasts with our model X  which performs both functions simultaneously X  X nd is neces-sary here because the labeled LDA model does not allow us to specify uncertainty in the label assignments.

In the first run of labeled LDA, we assign each user tags corresponding to the labels present in his or her user pro-file, as well as the background tag. For example, a user with the word  X  X unner X  in his Twitter profile would have to be modeled by only two topics: the one corresponding to the  X  X unner X  label, and the background topic. This first run learns a topic corresponding to each of the badges, giv-ing the probability that each badge explains each action in our vocabulary. However, as each topic is a multinomial distribution over actions, its probabilities must sum to one, leading to qualitative differences with the badges learned from our model. First, for a given badge i in our model, the probability of each action  X  ij lies in the set (0 , 1), and are conditionally independent of each other given their prior. This allows several actions to have high probability of being explained by the same badge, if that is what can best model the user data. Second, by explicitly modeling sparsity using the s variables, a badge is not forced to explain actions it is only weakly associated with, simply for the sake of get-ting its distribution to sum to one. This is a characteristic not only of labeled LDA, but of all such topic models. Fig-ure 3(b) shows the difference in the sparsity of our badge definitions when compared to labeled LDA.

After learning the topics with the first run of labeled LDA, we keep them fixed and infer the badge assignments, this time giving all 32 tags to each user, allowing for badges to be inferred beyond the ones corresponding to observed labels. However, as before, because labeled LDA provides no explicit model of sparsity, and is modeling a multinomial distribution, every user will, in expectation, be assigned to
In both cases, we use the implementation provided by Ra-mage and Rosen in the Stanford Topic Modeling Toolbox, using the default hyperparameter settings and the CVB0 inference algorithm [14]. one badge, but this probability mass will be spread over all 32 topics, even if they are all unlikely.

We take the badges learned and inferred by our model and compare it to those from labeled LDA, evaluating both the interpretability of the badge definitions and the correctness of the badge assignments.
One important desideratum from our problem descrip-tion is that, whichever model we use, if we are to bring transparency to the personalization process, we must pro-vide users with meaningful and interpretable answers when they ask,  X  X hy did I get badge X? X  A convenient way to visualize badge definitions is via word clouds, with the size of an action proportional to its weight in the badge, con-cisely summarizing what it means to have a particular badge. Specifically, in our model, the  X  X eight X  of action j in badge i refers to the quantity s ij  X  ij , while in labeled LDA, the weight is the probability of action j coming from topic i
Figure 4 shows six examples of badges learned from run-ning our model on the Twitter data set described above. 8 These badges do an excellent job of describing actions that are consistent with their definitions, and are precisely the types of explanations we would hope to expose to the user. For instance, the  X  X unner X  badge in Figure 4(d) explains the action # runkeeper , which is a hashtag automatically tweeted when using a particular smartphone application 9 that helps manage and track a user X  X  workouts.

However, looking at Figure 4(f), which we learn by gen-eralizing from the actions of self-described  X  X ednecks, X  we find that while some actions are expected for such a badge (e.g., # teaparty and # tcot 10 ), others are more surprising, (e.g., # p2 , a popular hashtag among progressives). In fact, looking at other hashtags here such as # obama ,# debt and # syria , we see that we actually learn a more general badge, referring to American politics and global affairs, rather than
A full visualization of all 31 badges learned from our model can be found with the supplemental material [4]. http://www.runkeeper.com  X  X op Conservatives on Twitter X  the size of a word is proportional to the action probability one narrowly focused on rednecks. A plausible explanation for this phenomenon can be found in Figure 3(a), where we see that  X  X edneck, X  associated with label 14, appears in the Twitter profiles of very few users X  X erhaps too few to effec-tively learn what it means to be a  X  X edneck X  on Twitter.
Figure 5 shows two other badges corresponding to la-bels present in very few users. The first example, Fig-ure 5(a), is a more extreme form of overgeneralization than the X  X edneck X  X adge, as we see the idea of a wine lover trans-lating to actions representing enjoyable (and often expen-sive) interests and activities, such as # swarowski ,# travel and # jewelry . Figure 5(b), on the other hand, shows the extreme situation where the original label X  X n this case for  X  X uby on Rails X  X  X s present in so few users that the badge can be completely overwhelmed by a more popular topic. Here, we see that this badge has been taken by actions re-lating to the London riots, which was the most prevalent news item in our data.

When we look at the topics learned by labeled LDA, we find that they also represent interpretable badge descrip-tions. However, as we described earlier in this section, we do not find the same sparsity that we achieve using our model, because topic modeling approaches assume each topic is a distribution over the entire vocabulary. This contrast is made clear in Figure 6, where we see an extremely sparse badge representation for Apple fanboys, as learned by our model, compared to a much denser distribution over actions, learned by labeled LDA. The badge we learn focuses on a few informative actions, such as following @MacRumors ,whereas the topic learned using labeled LDA includes, in addition to many Apple-related actions, many actions that are just tangentially related (e.g., # runkeeper ).
The fundament al goal of this work is not just to pro-duce interpretable badges, but to accurately assign badges to users based on their actions. After all, badges are only useful for personalization if we infer them correctly. We expect our model to significantly outperform labeled LDA here, as we explicitly model the uncertainty relating badges and their corresponding labels, which labeled LDA does not.
In order to quantitatively measure our performance in this area, and compare it to that of labeled LDA, we re-trained both models on the Twitter data set, this time hold-ing out a random tenth of present labels, which we treat as ground truth labels that we seek to recover. Specifically, of all badge-user pairs ( i, u ) corresponding to present labels i = 1 (of which there are 83,020), we select 10% uniformly at random, and hold them out. We then take the perturbed data set and run both labeled LDA (two stages, as before) and our model, leading to estimated posterior probabilities of badge assignments, b ( u ) i .

In order to compare the two models as fairly as possi-ble, we take each user and rank his or her badges from most probable to least probable, and see where the held out points ( i, u ) appear. When ranking badges for a user u using our model, we rank them by descending posterior probability of i = 1. When ranking based on the labeled LDA model, we rank the topics for each user by decreasing topic propor-tion. If label i was held out for user u , then the better of the two models will rank badge i closer to the top. 11 Fig-ure 7(a) demonstrates that our model significantly outper-forms labeled LDA on this metric, with the held out badges appearing, on average, approximately four positions higher in the ranking. Moreover, we hypothesize that the more ac-tive a user is (i.e., the more actions we observe for her in our data set), the better we will do in predicting the held out badge, as we will have more information to base our infer-ence on. This prediction is confirmed in Figure 7(b), where we see a six position difference in the ranking separating the most active from the least active users.

Moving beyond this quantitative comparison, we observe several qualitative properties of our inferred badges that pro-vide anecdotal support for our model X  X  effectiveness. First, as we model each user X  X  badge assignments as a binary vec-tor, we can use our samples to estimate the posterior marginal probability on pairs of badges appearing together in the same user, as predicted by our model. These results, shown in the annotated matrix in Figure 8, indicate that the hot spots of badge co-occurrence correspond to pairs of badges that we would expect to see together. In particular, the top four pairs of badges, ranked by decreasing posterior proba-bility, are:
We note that this is a fairer comparison than directly mea-suring the posterior badge assignment probability, since the labeled LDA probabilities are constrained to sum to one, giving us an unfair advantage. By ranking the badges, we avoid this problem. Figure 5: Two cases showing current limitations of our model that arise when we have labels present in only a small number of people. Figure 6: Two word clouds for the  X  X pple fanboy X  badge, contrasting the sparsity of our learned badge (a) versus the corresponding labeled LDA topic (b). 1. Entrepreneur and jQuery 2. Feminist and  X  X ondon riots X  (originally the  X  X uby on 3. Feminist and X  X merican politics X (the generalized X  X ed-4. Photographer and  X  X ondon riots. X  The matrix shows many other correspondences, for instance, tying together pop icons Taylor Swift and Lady Gaga.
Another view of our inferred badge assignments can be ob-tained by selecting a population of Twitter users from our data set, and visualizing their collective badge profile in ag-gregate. Here, the size of a badge label in each word cloud is proportional to its mean posterior probability across the en-tire population of interest. For example, we can compute the mean probability that someone with the word  X  X onservative X  in his Twitter profile is assigned the  X  X ntrepreneur X  badge. Figure 9(a) shows such a word cloud for precisely this pop-ulation of conservative Twitter users from our data set. We see a heavy focus on the redneck badge among this popula-tion, since it X  X  plausible that those with the word  X  X onserva-tive X  in their Twitter profiles are apt to tweet about Amer-ican politics. Interestingly, this is true even though conser-vatives are more likely to describe themselves as  X  X eachers X  or  X  X ntrepreneurs X , as shown in Figure 9(b), potentially in-dicating that an interest in politics plays a larger role in influencing Twitter behavior than, say, being a teacher.
Finally, we can compare the inferred badge profiles of two different populations, and look at the differences between them. Naturally, to contrast with the  X  X onservative X  popu-lation of Twitter users, we also gather all users in our data set with the word  X  X iberal X  in their Twitter profile. We can than take our estimate of the mean posterior probability for each badge in each of the two populations, and visualize the difference between them, which we display in Figures 9(c) Figure 7: Results showing that, on a held-out set of labels, (a) our algorithm is better able to recover (approximately) ground truth badges than labeled LDA, and, (b) more active users get better predic-tions. (Error bars indicating standard error are too small to be visible.) (conservatives -liberals) and 9(d) (liberals -conservatives). For example, if we look at the word X  X eminist X  X n Figure 9(d), its size is proportional to our estimate of the mean posterior see, e.g., that the  X  X eminist X  badge is the one that is most likely to occur in a liberal and not a conservative.
Since Zaslow X  X  article on TiVo was published in 2002 [16], many forms of personalization have appeared or intensified on the Web, and they operate with varying degrees of trans-parency: http://www.pandora.com http://blog.pandora.com/press/ pandora-company-overview.html http://www.google.com/intl/en/privacy/ Figure 8: A matrix depicting the posterior marginal probability of badge co-occurrence, as estimated by our model. Darker squares represent higher proba-bilities. The badges are ordered in the same manner as presented in Table 1. http://googleblog.blogspot.com/2011/10/ increasing-transparency-and-choice-with.html http://www.bing.com Figure 9: These four word clouds depict the badges learned for two populations of people: those with the word  X  X onservative X  in their Twitter profiles (254 such users in our data set), and those with the word  X  X iberal X  in their profiles (327 of them). (a) Shows the mean badge probabilities for self-described conservatives. The size of a badge name is proportional to the mean probability of that badge as estimated by our model. (b) Shows the proportion of  X  X onservatives X  that have each of the 31 labels present in their profiles. (c) These badges are more likely for self-described conserva-tives than self-described liberals, and the size of each badge label is proportional to the absolute differ-likely for liberals than conservatives. (Recall from Figure 5(b) that the  X  X uby on Rails X  badge is in-stantiated as a  X  X ondon riots X  badge.) While this is certainly not an exhaustive list, and personal-ization capabilities are routinely being added and removed from these sites and others, the websites mentioned repre-sent a large portion of user interaction on the Web (e.g., Google and Bing search amounted to 85 percent of all web searches from the United States in September 2011 17 , while over 500 million users log on to Facebook per day at the time of writing 18 ), demonstrating the importance of study-ing methods for making personalization more transparent.
From the perspective of methodology, personalization by inferring latent user features has come a long way. While col-laborative filtering through fac torizing a user-item matrix, and variants thereof, is extremely successful as a backbone of recommender systems [7], the latent features don X  X  have any interpretable meaning. Interpretability often means dis-cretization. In this vein, Porteous et al. extended matrix fac-torization with discrete class allocations [11], and although we are not aware of its existence, it is entirely feasible to enforce labels to certain class allocations.

When the domain of interest includes user-generated con-tent, like blogs and tweets, latent topic models have fre-quently provided a successful modeling framework. Unfor-tunately, as is the case with matrix factorization techniques, the topics learned in such unsupervised models do not lend http://www.comscore.com/Press_Events/Press_ Releases/2011/10/comScore_Releases_September_2011_ U.S._Search_Engine_Rankings http://newsroom.fb.com themselves to interpretability, as they are not identifiable, a problem we avoid by associating each badge with a la-bel. Topic models that incorporate supervision at the topic level, such as labeled LDA, explained in Section 3.2, and the more recent work by Andrzejewski et al. [1], provide a mechanism for such identifiability. For instance, labeled LDA was recently used to model another axis of personal-ization on Twitter, mapping posts as informative, personal status updates, or inter-user social communication [12].
In this paper we have presented a Bayesian inference al-gorithm to learn both a descriptive model and predictor for badges based on user activity on a micro-blogging site (Twitter). In our work, a badge is seeded by a label X  X ore generally, a high precision, low recall rule X  X ased on self-reported user information, while our predictive model for badges explicitly relies on the presence or absence of user actions. Both these modeling decisions contribute greatly to the transparency of the prediction and we believe trans-parency will be a critical building block for personalization systems that users will find acceptable and suitable.
We have shown empirically that our model outperforms state-of-the-art models such as labeled LDA in terms of pre-dictive performance, while producing interpretable descrip-tions of badges.

There are a number of open questions and challenges that need to be addressed in future work.
Despite these current limitations and future challenges, our work introduces a promising framework that we believe will be successful in bringing transparency to a myriad of personalized services on the Web. First author was partially supported by ONR grants MURI N000141010934, N000140810752 and PECASE N000141010672. [1] D.Andrzejewski,X.Zhu,M.Craven,andB.Recht.A [2] D. M. Blei and J. Lafferty. Topic models. In [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [4] K. El-Arini, U. Paquet, R. Herbrich, J. Van Gael, and [5] E.B.Fox. Bayesian Nonparametric Learning of [6] A. Frigessi, P. Di Stefano, C. Hwang, and S. Sheu. [7] Y. Koren, R. M. Bell, and C. Volinsky. Matrix [8] J. Liu. Peskun X  X  theorem and a modified discrete-state [9] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, [10] E. Pariser. The Filter Bubble . Viking, 2011. [11] I. Porteous, A. Asuncion, and M. Welling. Bayesian [12] D. Ramage, S. Dumais, and D. Liebling.
 [13] D. Ramage, D. Hall, R. Nallapati, and C. D. Manning. [14] D. Ramage and E. Rosen. Stanford topic modeling [15] A. J. Smola and S. Narayanamurthy. An architecture [16] J. Zaslow. If TiVo thinks you are gay, here X  X  how to
