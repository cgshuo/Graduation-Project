 Query auto-completion (QAC) is a common feature in mod-ern search engines. High quality QAC candidates enhance search experience by saving users time that otherwise would be spent on typing each character or word sequentially.
Current QAC methods rank suggestions according to their past popularity. However, query popularity changes over time, and the ranking of candidates must be adjusted ac-cordingly. For instance, while ha lloween might be the right suggestion after typing ha in October, ha rry potter might be better any other time. Surprisingly, despite the importance of QAC as a key feature in most online search engines, its temporal dynamics have been under-studied.

In this paper, we propose a time-sensitive approach for query auto-completion. Instead of ranking candidates ac-cording to their past popularity, we apply time-series and rank candidates according their forecasted frequencies .Our experiments on 846K queries and their daily frequencies sampled over a period of 4.5 years show that predicting the popularity of queries solely based on their past frequency can be misleading, and the forecasts obtained by time-series modeling are substantially more reliable. Our results also suggest that modeling the temporal trends of queries can significantly improve the ranking of QAC candidates. H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval X  Search Process, selection process Algorithms, Measurement, Experimentation Query auto-completion, temporal ranking, time-series
Query auto completion (QAS) is a feature incorporated in essentially every search engine, where the goal is to save user time by predicting user X  X  intent and suggesting possible other queries matching the first few keystrokes typed. The filtering of related candidates (query suggestions) is typically based on string matching, and the ranking is ideally based on the likelihood of the filtered candidate being the query which the user has in mind. The latter challenge (ranking) is the focus of this paper, for which the goal is to sort queries according to their expected popularity. The common prac-tice is to use past frequencies as proxy for future popularity [2, 8]. However, those approaches assume that user intent is static and does not change over time. Simple aggregation can overshadow the temporal trends that could potentially provide valuable signals for better ordering of QAC candi-dates.

Consider the example in Figure 1 where a user has typed di in Google query box on Sunday , February 12th, 2012. At the first glance, knowing that di ctionary is generally a more frequent query than di sney , it might be difficult to notice how the ranking might be improved. However, looking at the daily trends for these queries in Figure 2 reveals that disney is more popular on weekends. Hence, given that the first snapshot was taken on a Sunday, swapping disney and dictionary couldleadtoabetterrankingatthetimeofthis query. Similar observations can be made for different gran-ularity of time-spans. For instance, previous work [4] has shown that users are more likely to search for entertainment-related queries at night, while queries related to personal finance are more common in the morning. Therefore, the ranking of QAC candidates can vary dynamically even ac-cording to the time of the day.

In this paper, we add time-sensitivity to ranking auto-completion candidates. We consider the temporal variations of query popularity in ranking QAC suggestions, that are normally shadowed by aggregation. Rather than summariz-ing the entire query history in one aggregated number as expected popularity, we rely on shorter but more frequent aggregation of data, and model the overall query trends by time-series. We show that the expected popularity values produced by our time-sensitive approaches are closer ap-proximations to what will be observed later in the logs, and are more effective for ranking QAC suggestions.

The contributions of this work are three-fold: First, we study the shadowing problem that is caused by ignoring the temporal variations of query frequency in data aggregation. We investigate several aggregation options and show that query popularity predictions based on shorter but more re-cent data are more accurate than those produced based on aggregation over longer periods. This simple and counter-Figure 1: Google auto-completion candidates after typing di on Sunday, February 13th, 2012.
 Figure 2: Daily frequencies for queries dictionary (red) and disney (blue) during January 2012 accord-ing to Google Trends (the snapshot was taken on Monday, 13-Feb-2012). Among the two queries, dis-ney is more popular on weekends, while dictionary is issued more commonly by users on weekdays. intuitive finding, can be useful in scenarios where applying more sophisticated time-series models is feasible. Second, we introduce a time-sensitive query auto-completion model in which the expected popularity of each query suggestion is forecasted by time-series, and dynamically varies depend-ing on the time of query. Finally, we perform a large scale analysis over 4.5 years of sampled log data, and evaluate our predictions on daily and monthly intervals. Our experimen-tal results indicate that accurate query popularity forecasts are essential in generating high quality QAC rankings. Query auto-completion. Auto-completion during typing is a familiar feature and has been utilized in several applica-tions ranging from early UNIX Shells to modern text editors and web browsers.

Previous work on auto-completion can be grouped into two main categories; the first group (also referred to as predictive auto-completion [8]) deploys information retrieval and NLP techniques to generate and rank candidates on the fly as the user enters new words and characters [13, 18, 27]. For instance, Grabski et al. [18], and Bickel et al. [6] stud-ied sentence completion based on lexicon statistics of text collections. Fan et al. [17] ranked AC candidates according to a generative model learned by Latent Dirichlet Alloca-tion (LDA) [7]. White and Marchionini [33] developed a real-time query expansion system that produces an updated list of candidates based on the top-ranked documents as the user types new words in the search box.

In the second group of AC techniques  X  including this work  X  candidates are pre-generated and stored in tries and hash tables for efficient lookup . The list of candidates is updated by new lookups with each new input from the user. The filtering of candidates in this group is typically on the basis of exact prefix matching. 1
The ranking in lookup techniques is based on static scores that are assigned to candidates according to their impor-tance. For example, in a product engine such as amazon.com with products names as candidates, static scores can be as-signed according to popularity, price or the review scores of products [8]. In the context of web search, the most con-ventional approach is to rank candidates (query suggestions) according to their past popularity. In the most related study to our work, Bar-Yossef and Kraus [2] referred to this base-line as MostPopularCompletion (MPC) and suggested that it can be regarded as an approximate maximum likelihood estimator. The authors also proposed a context-aware tech-nique in which the default static scores for the candidates are combined with contextual-scores based on recent session history to compute the final ranking.

We also take MPC [2] as our QAC ranking baseline and show that it can be improved significantly by considering the temporal characteristics of queries. Our work is orthogonal to all aforementioned techniques; for instance, using error-tolerant data structures that support fuzzy matching [8, 20] can expand the list of candidates that are eligible for our temporal modeling. Our work can be also combined with the hybrid framework of Bar-Yossef and Kraus [2] to improve the ranking of candidates with no or little context. Time-sensitive search. The world is constantly changing and the dynamics of daily life are reflected on the web in the forms of fresh content and new information needs. Different aspects of freshness in web search have been explored; Jones and Diaz [21] and Li and Croft [24] extended the language modeling framework to incorporate the time extracted from document time-stamps. Berberich et al. [5] matched ex-plicit temporal expressions in queries with the time-stamps mentioned in documents in a query likelihood approach for ranking. Their work was extended by Kanhabua and N X rv  X  ag [22] that used the content of top-ranked documents for ap-proximating the most related time-interval for the query.
Elsas and Dumais [16] showed that there is a strong con-nection between the rate of content changes in a document and its relevance. They proposed a probabilistic ranking model in which the term weights vary according to their temporal characteristics. Relatedly, Efron [15] investigated using linear time-series models for term weighting. Dai and Davison [11] introduced a time-sensitive version of PageR-ank based on multiple crawls and showed that it can be used as an effective static score for document ranking. Dong et al. [14] and Dai and Davison [12] integrated freshness in learning to rank by introducing new ways of optimizing for both freshness and relevance.

Kulkarni et al. [23] classified queries into different cate-gories based on their change of popularity over time. The authors showed that monitoring the query popularity and content updates can reveal useful signals for detecting the change in query intent. Metzler et al. [26] classified queries with implicit temporal intent (e.g. halloween) according to their previous occurrences in the logs along with explicit temporal expressions (e.g. halloween 2010). Shokouhi [31]
Recently, Chaudhuri and Kaushik [8] and Ji et al. [20] proposed flexible fuzzy matching models that are tolerant to small edit-distance differences between the query (prefix) and candidates. leveraged time-series decomposition techniques for classify-ing seasonal queries.

Chien and Immorlica [9] demonstrated that queries with similar temporal patterns (e.g. halloween and pumpkin) can be semantically related despite no lexical overlap. Liu et al. [25] introduced a unified model for forecasting query fre-quency in which the forecast for each query, is influenced by the frequencies predicted for similar and correlated queries. Vlachos et al. [32] developed a compressed representation for time-series and proposed a model for detecting significant bursts in query frequencies. Baraglia et al. [3] investigated the impact of aging on query flow graphs and in the pres-ence of evolving query trends. They argued that generating the graphs from scratch may not be feasible and described an incremental approach for more efficient updates. Alfon-seca et al. [1] clustered queries according to their frequency time-series. They suggested that their approach can be used for query suggestion and query categorization. Zhang et al. [34] reranked documents based on their time-stamps in snip-pets and suggested that their approach can improve search effectiveness for temporal queries.

Radinsky et al. [29] have performed an analysis of pre-dictability of different user behaviors (such as query clicks, URL clicks and query-URL clicks) using time-series analy-sis and a learning model. Similarly, Cho and Varian [10] and Shimshoni et al. [30] study the general applicability of time-series analysis for modeling query trends. However, those studies have been performed on small sets of queries (  X  10,000 queries) and of short periods of time (6 months), disallowing identification of long term yearly seasonality. In our work, we perform rigorous analysis of a large scale cor-pus of 846K queries over a period of 4.5 years for the purpose of better query suggestions.

While it is clear that freshness has been considered in several areas of web search, to the best of our knowledge, this is the first time that the temporal trends of queries are modeled and used for QAC ranking.
In a typical QAC scenario, the user is presented with a list of query suggestions that their prefix matches the text en-tered in the search box. In this paper, we refer to the latest text in the search box as prefix (e.g.  X  X i X  in Figure 1). The prefix is updated as the user enters new characters and will eventually match the final query once the query is submit-ted. For each prefix P , the list of candidates C ( P ) consists of all previous queries that start with P . 2 The list of can-didates is dynamically updated at run-time with each new character typed by the user.

Ideally, the candidates should be ranked based their like-lihood of matching the query that the user has in mind. In the absence of any contextual information, this likelihood can be set according to the wisdom of crowds by using the general frequency in the past as a proxy for the expected popularity in future. The most common approach is to ag-gregate the query frequencies over a query log, and use these aggregated values for ranking QAC suggestions. Bar-Yossef and Kraus [2] referred to this general form of QAC ranking as MostPopularCompletion (MPC) where,
Without the loss of generality we ignore more advanced fuzzy matching techniques [8, 20] in our work.
 here, f ( q ) denotes the number of times the query q occurs in a previous search log Q . Under the MPC model, the can-didate scores do not change as long as the same query log Q is used. Each query is represented by a single value as its popularity which is computed by counting the number of times that it has occurred in the past. As long as Q is not replaced by a fresher log, the assumption is that the aggregated frequencies can be regarded as reasonable ap-proximations for future popularity. However, as mentioned earlier, the query popularity may change over time and the candidate lists must be adjusted consequently to account for recent changes.

While frequent updates of query statistics may address the freshness problem to some extent, it is not clear how often the logs must be updated to provide the best approxima-tions for future popularity. Without the right update pol-icy, trend effects will be disregarded. For example, a query such as Sarah Burke that gained high popularity in January 2012, but was not queried as often in the past, might get lower ranking if compared to the query Sarah Palin ,which has high volume, since it was queried for many years, despite being relatively less popular in January 2012. Furthermore, aggregation methods ignore seasonal effects. For instance, at the time of this writing on 13th February 2012  X  a day be-fore the Valentine X  X  day  X  Google suggests v erizon wireless as top candidate for v and does not suggest any Valentine X  X  related candidate in the QAC ranking (the top plot in Fig-ure 3). The query frequency trends in the bottom plot of Figure 3 however clearly show that v alentines day is tem-porally more relevant for suggestion. As another example, weekly patterns such as those shown in Figure 2 for dis-ney and dictionary disappear in monthly aggregations. In summary, the past is not always a good proxy for future par-ticularly for trendy and seasonal queries. Today X  X  frequency for query dictionary is not necessarily the best estimate for its frequency tomorrow.

We propose TS, a time-sensitive QAC ranking model in which the default aggregated candidate scores in Equation 1 are replaced with forecasted values computed by time-series modeling of query history. Here, the score of each candi-date at time t is determined according to its predicted value calculated using time-series models that capture trend, sea-sonality. Hence, our time-sensitive QAC ranking model can be formalized as,
TS ( P ,t ) = arg max where P is the entered prefix, and C ( P ) represents its list of QAC candidates, and  X  y t ( q ) denotes the estimated frequency of query q at time t . As a result, the quality of a candidate list generated by our TS model is directly influenced by the accuracy of predictions obtained by time-series modeling. Note that, our time-sensitive model becomes similar to the context-sensitive QAC ranking of Bar-Yossef and Kraus [2] if one considers time as  X  X ontext X . The focus of our work is on the time-sensitivity of query suggestions, and on how we can enhance QAC to handle this aspect. We leverage techniques from time-series analysis to model temporal query behavior Figure 3: (Top) The auto-completion candidates ranked by Google on 13th February 2012, a day be-fore Valentine X  X . (Bottom) The query frequencies for valentines day vs. verizon wireless since 2004. (Section 3.1), and demonstrate how these models can be learnt from query logs to do better QAC ranking (Section 3.2).
A time-series consists of a sequence of data points in suc-cessive time order and with uniform intervals. Hence, an-nual salaries, the daily rate of CO2 emissions and any other sequence of numbers collected at uniform intervals can be represented by a time-series. In practice, time-series analy-sis is often used to model the temporal changes in data and to forecast future trends.

At any given time, the Exponential smoothing methods [19] apply a weighted average over the frequencies of pre-vious data points to forecast the upcoming trends. In the simplest form  X  also known as Single exponential smoothing  X  X hetime-seriesissmoothedasfollows, where y t and  X  y t respectively denote the actual and smoothed values for the data at time t ,and  X  is the smoothing param-eter ranging between 0 and 1. The recursive equation above can be also used to provide the forecast for the next interval ( X  y t +1 ). That is, While single exponential smoothing may produce reasonable forecasts for stationary time-series, it performs poorly in the presence of a trend in time-series. To remedy this problem, Double exponential smoothing methods extend the previous model by introducing a trend variable F t , Here, parameter F t models the linear trend of time-series at time t ,while  X  1 and  X  2 are smoothing parameters. As in previous equations, y t and  X  y t represent the actual and Figure 4: The black line shows monthly frequency values for query spring flowers between Sep X 06 X  Jun X 11. The green curve depicts the predicted val-ues for the next 24 months (Jul X 11 X  X un X 13) based on triple exponential smoothing. Data from bing.com . smoothed values at time t . The forecast for the next interval based on double exponential smoothing can be obtained by: That is, the value of the next data point at time t +1is predicted by linear combination of the smoothed value at time t and the latest trend parameter F t . Hence, a negative F t sets a smaller predicted value for the next time interval ( t + 1), compared to the most recently observed value y t .
Double exponential smoothing does not capture the fre-quency variations due to potential seasonality (periodicity) in the data. Triple exponential smoothing generalizes the previous techniques by explicitly modeling the seasonality. The updated model  X  also known as HoltWinters smooth-ing  X  is often expressed as: where S t captures the seasonality of data at time t ,and  X  specifies the length of periodic cycle (e.g. 12 for annual cycles when the data points represent monthly frequencies). The smoothing parameters  X  1 ,  X  2 ,and  X  3 vary between 0 and 1 and can be optimized using standard techniques such as maximum likelihood. The forecast for the value of the time-series at the next interval is given by:
Figure 4 illustrates an example of applying triple exponen-tial smoothing for forecasting future trends. The underlying data comprises of monthly frequency values for query spring flowers sampled from logs of bing.com between September 2006 and June 2011. The green curve shows the forecast for July 2011  X  June 2013 (  X  =12).
In this work we utilize a hill-climbing optimization tech-nique called limited-memory BFGS (L-BFGS), which is a limited memory variation of the Broyden-Fletcher-Goldfarb-Shanno (BFGS). The technique estimated the direction by solving the following Newton equation: where B k is the approximation of the Hessian matrix at step k ,and f = 2 is the sum of the prediction error rates. The initial values B 0 and x 0 are guessed, and a line search in the direction at step k is then used to find the next point x and a new value of the matrix B k +1 is estimated [28].
Our first set of experiments focuses on forecast accuracy while in the second part we evaluate the effectiveness of QAC rankings. For each task, we borrow a couple of metrics from information retrieval and statistics for measurement. Forecast accuracy. Mean absolute error (MAE) is widely used in statistics to measure the accuracy of forecasts and is defined as follows, where y is the true value and  X  y is the prediction. MAE is an unbounded measure and is not strongly resilient to outliers. Therefore, its is often used along with other metrics such as Symmetric mean absolute percentage error (SMAPE) to diagnose the forecast variation. SMAPE is defined as, In contrast to mean absolute error, SMAPE is bounded between 0 and 1, although it penalizes under-and over-estimations unequally.
 QAC quality. In the absence of any contextual and per-sonalized information, the ground-truth QAC ranking for a prefix P is the list of all queries that start with P (or are somehow filtered for the prefix) ordered according to their true popularity. At any given time t , the true popularity val-ues are set according to the observed query frequency values at that time. Obviously, this information is not available to QAC ranking models at runtime, and they have to rely on historical data at time t  X  1 and before to rank candidates as close as possible to this unobserved ground-truth ( G ). We measure the quality of QAC lists by computing their Spearman correlation (  X  ) against this gold standard G .We also employ MRR to specifically measure the mean recipro-cal rank of the best ground-truth candidate (top-ranked in G ) in QAC rankings.

Small differences in query frequency can cause significant changes in the ordering of QAC candidates. Both MRR and Spearman correlation penalize swapping two queries with similar frequencies the same way they penalize swapping a pair with a substantial gap in popularity. Consider a sce-nario where the user has typed hall in the search box in October, and the QAC model has to rank the following can-didates: hall oween , hall oween party games ,and hall oween party ideas . The first candidate is orders of magnitude more popular than the other two which more or less have simi-lar frequencies. Clearly, pairwise swappings that involve the first candidate should be rewarded or penalized more heavily than the pairwise swapping of the other pair.

To make our evaluation more robust against the potential noise caused by insignificant differences, we first take a log of  X  true/predicted  X  query frequencies and then round them to the closest integer across all our QAC ranking evaluations.
Our experimental data comprises of all queries that were submitted to bing.com by users  X  with a US-based IP ad-dress  X  between January 1st, 2007 and June 30th, 2011. We dropped queries that never appeared more than 28 times in at least one of the months during this period. In total 846,432 queries and their daily frequencies were extracted.
The quality of QAC candidates can vary depending on the amount of historical data available for each query. The effectiveness of time-series techniques for modeling the query trends is also sensitive to the amount of training data and the length of seasonal cycle (  X  in Equation 12). Therefore, we use our sampled query statistics to construct two testbeds with distinctive characteristics.
 generated from daily query frequency values between Jan-uary 2nd, 2011, and June 30th, 2011 (180 days). For each query at time (day) t , we use the frequencies at all previous intervals (previous daily frequencies) to fit the time-series models. We consider the last 30 days (June 2011) as our testing period for reporting the results. We set the  X  value in time-series models to 7 which would allow them to capture potential weekly cycles in query frequency.
 the daily frequency values between January 2007 and June 2011 in monthly buckets . Hence, the history of each query is represented by 54 data points one for each month covered during our sampling time-frame. We report our evaluation results on the first 6 months of 2011. Here, we set the  X  parameter in our time-series models to 12, to allow modeling of potential annual cycles.

Compared to the previous testbed, there are fewer data points for time-series modeling of query trends, and there is less variance and sparsity in query frequencies.
In an oracle list of QAC suggestions, candidates are sorted in descending order of their true popularity. Since this ground-truth information is unavailable at runtime, QAC ranking models order candidates according to their expected (predicted) popularity inferred from previous logs.
Our TS auto-completion method models the entire query history by time-series and forecasts the future popularity accordingly. The MCP method [2] that we use as our base-line assumes that the aggregated frequency of a query over past search logs is a reasonable approximation for its future popularity. Surprisingly, little is known about the validity of this assumption and its impact on the ranking of QAC candidates. Furthermore, the trade-off between the recency (and size) of the query log used for aggregation, and the accuracy of outcome predictions has not been investigated.
In this section, we evaluate the accuracy of various meth-ods for predicting the future query popularity, and in the Table 1: The forecast error rates obtained by dif-ferent methods on the D-W testbed. The query fre-quencies are predicted once for each of the 30 days in the testing period (June 2011). The best performing method in each row is specified by an underline .All (TS vs. P  X  ) and (TMS vs. P  X  ) pairwise differences are detected as statistically significant by the t-test ( p&lt; 0 . 01 ).
 next section we measure the impact of these predictions on the quality of QAC rankings.
 Forecast quality. MCP considers the past query frequency as expected popularity for future. For this, the query fre-quencies are aggregated over a past query log. We explore several aggregation options by averaging the query frequen-the entire query history. We refer to latter form of aggrega-tion as P h and denote the others by P k where k is the num-ber of previous intervals used for averaging. We study the accuracy of these aggregated baselines in predicting future query popularity, and compare them against the forecasts produced by our time-series modeling of query trends (TS).
Table 1 includes the forecast error rates of different meth-ods on the D-W testbed. The frequency of each query is forecasted once for each of the 30 days in our testing pe-riod (June 2011). At each time t ,the P k baselines use the average frequency of the past k days as their prediction. P h computes the average over the entire query history be-fore t . The TS model also uses the entire previous history but produces its forecasts by triple exponential smoothing (  X  = 7). For now, ignore the TMS column as we will get to it later. The numbers show that our TS predictions are better than all aggregated baselines on both metrics; all dif-ferences are detected as statistically significant by the t-test ( p&lt; 0 . 01). Among the aggregated baselines, MAE favors P 1 while SMAPE picks P h as the best model. The discrep-ancy suggests that averaging over the entire history provides more robust predictions overall but may fail more noticeably on outliers.

We take a closer look at the daily error rates produced by the best three methods ( P 1 , P h , and TS) in Figure 5. The results are consistent with the overall numbers; the MAE rates have higher variance and P h is more robust than P 1 although it may produce greater absolute errors. P h per-forms worst over the weekends and P 1 errors are highest on Saturdays and Mondays. This can be explained by the fact that search traffic is generally lower on the weekends, and grows again on Mondays. P 1 is constantly one step behind to react to these changes and P h is too stale in general.
We repeated our analysis on the M-A testbed and summa-rized the results in Table 2. All methods produce 6 forecasts for each query, one for every month in our testing period Jan X  X une 2011. The P k baselines use the average frequency of the past k months as their forecast while P h computes the average over the entire query history since January 2007. The TS model also uses the entire previous history but pro-duces its forecasts by triple exponential smoothing (  X  = 12). Once again, ignore the values under the TMS column as Table 2: The forecast error rates obtained by dif-ferent methods on the M-A testbed. The query fre-quencies are forecasted once for each of the 6 months in the testing period (Jan X  X un 2011). The best performing method in each row is specified by an underline . Except for (MAE, k =1 ), all (TS vs. P  X  ) and (TMS vs. P  X  ) pairwise differences are detected as statistically significant by the t-test ( p&lt; 0 . 01 ).
SMAPE
MAE Figure 6: Daily SMAPE (top) and MAE (bottom) rates for P 1 , P h , TS and TMS predictions on the M-A testbed (January X  X une 2011). they will be described later. The numbers show that with the exception of P 1 , our TS predictions are better than all aggregated baselines on both metrics; all differences are de-tected as statistically significant by the t-test ( p&lt; The difference between P 1 and TS is statistically significant on SMAPE but not so according to MAE. The competitive performance of P 1 on this testbed can be explained by sev-eral reasons; compared to the daily frequency values used the in D-W testbed, the data here is aggregated in monthly partitions. The monthly query frequencies are less sparse and have lower variance. Furthermore, the TS model has about 6 times more data points in the D-W testbed which allows it to fit the query trends better.

Figure 6 illustrates the monthly error rates for different methods. While the trends are consistent with the overall numbers, it is interesting to note the higher error rate of for January which is due to the change of year and end of holiday season.
SMAPE Tue, 14th Wed, 15th Thu, 16th
MAE two most effective methods according to the overall metrics and inspected their performance on per-query basis.
On the D-W testbed, lower error rates for TS versus P 1 are most apparent in queries with highly periodic frequency trends. Figure 7 depicts the actual and predicted frequencies for queries nascar and irish lottery as two examples from this category. The P 1 gains over TS often come from queries that were either periodic and highly popular but suddenly lost their demand (e.g. recently ended tv shows), or those that face an unexpected spike in popularity (e.g. celebrity names). Figure 8 shows two of these examples for queries american idol and ginger lee .

We also grouped the TS weekday winners by extracting the queries that their TS predictions are better than P 1 those weekdays, consistently over the entire testing period (June 2011). The top winners for each day are listed in Table 3. 3 As expected all these queries have periodic trends and are related to intents that have weekly cycles. The list is dominated by queries related to lottery (e.g. euromillion), tv shows (e.g. zero punctuation) and those targeting weekly deals (e.g. dominos) or updates (e.g. week in pictures). There were no weekday losers apart from queries that were related to americal idol and dancing with the stars , the two tv shows that their latest series ended in May 2011, just before the start of our testing time period.

Similar patterns can be observed on the M-A testbed. TS is more effective in predicting the frequency of queries with seasonal trends, while it fails to adapt quickly when there is a sudden change in the query popularity. Figures 9 and 10
For presentation purposes, we excluded queries that have high overlap with those already included in the list. For example, abc family is dropped when abc is already included. Table 3: A sample of queries where TS performs substantially better than P 1 consistently on certain weekdays during the testing period (June 2011).
 Mon dear prudence; fedex tracking; gmail;  X  X  X  Tue euromillion; abc; daily show; dominos  X  X  X  Wed mega millions; publix; zero punctuation  X  X  X  Thu louisiana lottery; footy tips; olg;  X  X  X  Fri week in pictures; nascar; cinemex  X  X  X  Sat irish lottery; wclc; toto;  X  X  X 
Sun wisconsin unemployment; mail on sunday; ncesc;  X  X  X  contain four examples of significant wins and losses for TS versus P 1 .
 shown that when it comes to query frequency forecast mod-els, there is no one size fits all method that always wins. For queries with an unexpected spike in particular, the time-series models tend to overfit when trying to optimize their smoothing parameters. Motivated by this observation, we propose a temporal model selector that dynamically decides which forecast model to choose at each time-interval for the query. For this purpose, we consider a segment of query historyasthe validation period and decide between differ-ent models according to their performance on this segment. Thevalidationprocesscanbedesignedindifferentways. For instance, one can pick the model that has the lowest error rate on the entire validation set. Alternatively, it is pos-sible to train a classifier that takes in monthly error rates and other features for supervised model selection. We de-cided to take a simple approach and left more sophisticated  X  Figure 7: (Top) The actual daily frequencies and the predicted values by P 1 (SMAPE  X  0 . 19 )andTS (SMAPE  X  0 . 06 ) for query nascar in June 2011. (Bottom) Same data for query irish lottery by P 1 (SMAPE  X  0 . 52 ), and TS (MAE  X  0.09). TS is more successful in modeling the periodic trends. models for future work. At each time interval t ,wetestthe performance of  X  P 1 and MS  X  forecast models at times t  X   X ,t  X  2  X , ..., t  X  k X  as long as they are included in the validation segment. We then pick the model that wins more often on the selected months. We break any tie by backing off to the model that has lower SMAPE on the entire valida-tion set. In nutshell, the validation step reduces the risk of using time-series forecast models that overfit their smooth-ing parameters to fit an unexpected recent burst. Note that the corrections achieved by model selection still cannot cover the opposite cases such as american idol where the popular-ity suddenly drops.

On the D-W testbed, we used the daily frequencies in May 2011, and on the M-A testbed we used the monthly values between 2009 X 2010 as our validation sets. The overall error Figure 8: (Top) The actual daily frequencies and the predicted values by P 1 (MAE  X  0 . 08 ) and TS (MAE  X  0 . 81 ) for query american idol in June 2011. (Bot-tom) Same data for query ginger lee by P 1 (SMAPE  X  0 . 26 ), and TS (SMAPE  X  0.40). P 1 reacts to quick changes of popularity more effectively. rates obtained by TMS on these testbeds can be respectively found in Tables 1 and 2. On the M-A testbed, temporal se-lection boosts both metrics. On the D-W testbed TMS leads to further improvements in MAE but it negatively affects SMAPE. The daily and monthly error rates in Figures 5 and 6 are consistent with the overall numbers.

In summary, our experiments suggest that despite the common assumption, aggregated query frequencies may not provide a good proxy for future popularity. Although aggre-gation based on fresher data is generally more effective, it still fails to predict the popularity of periodic queries. Our forecasts based on time-series analysis, and temporal model selection consistently outperform the aggregated baselines on both testbeds. In the next section, we show how improve-ments in forecast accuracy leads to better QAC ranking.
We pick P 1 as the best aggregated model to form the best case for MostPopularCompletion (MCP) [2] baseline in our QAC experiments. At each time t (day/month), we rank the QAC candidates according to their frequency at time t  X  1 and compare them against the oracle ranking generated by the true frequency values at time t . We also use the forecasts produced by the TS and TMS models to generate two time-sensitive QAC rankings for each query.
 QAC Candidates. We extract the QAC prefixes and can-didates from the same query log described in Section 4. For each prefix P at time t , we match all the queries that start with P and rank them according to their true frequency at time t to generate the ground-truth ranking. We filter out all prefixes that have fewer than 3 characters, and those that have less than 5 candidates. For each forecast method at time t , we rank the top-20 ground-truth candidates based on the data available at time t  X  1 and before. Those ground-truth candidates that are observed for the first time in the logs at time t and did not exist before are omitted. Results. Table 4 contains the evaluation results for ranking QAC candidates based on different prediction models. On the D-W testbed 30 rankings are generated for each pre-fix, one for each day in June 2011. Similarly, on the M-A testbed, each prefix is used to generate 6 QAC rankings, one for each month during January X  X une 2011. The numbers in the table are averaged across all queries and over the en-tire testing period. All pairwise differences are detected as statistically significant for both evaluation metrics.
The MRR and Spearman (  X  ) results in Table 4 closely follow patterns to observed for SMAPE error rates in our previous experiments. On the D-W testbed in Table 5 we sawthatTShadlowererrorratesthanTMSandtheyboth produced significantly more accurate predictions compared to
P 1 . The same can be observed when comparing the QAC lists generated based on their forecast values. TS has the edge over TMS while P 1 performs significantly poorer than both. Similarly, the SMAPE numbers in Table 6 suggested that TMS produces the most accurate forecasts on the M-A testbed. Here the results in Table 4 confirm that indeed these more accurate forecasts lead to higher quality QAC rankings.

We also computed the Pearson coefficient ( r ) between the average forecast error rates of the top five QAC suggestions and the final  X  and MRR values computed for those rank-ings. As expected, the Pearson coefficient suggests a nega-tive correlation between the quality of QAC rankings and the average forecast errors of the top five candidates ( r  X  X  X  for SMAPE-Spearman and r  X  X  X  0 . 21 for SMAPE-MRR).
Once again, time-series modeling is a key factor for bet-ter QAC ranking. Time-sensitive models such as TS and TMS are able to choose between big east conference and big east tournament , or between when to plant tulips and when to plant tomatoes depending on the time of query, while methods based on aggregated data constantly fall behind.
We proposed a new time-sensitive query auto-completion model in which the ranking of query suggestions varies ac-cording to their predicted popularity at the time of query. Table 4: The effectiveness of QAC rankings pro-duced according to different forecast models on the D-W (left) and M-A (right) testbeds. The best per-forming method in each experiment is specified by an underline . All pairwise differences are detected as statistically significant by the t-test ( p&lt; 0 . 01 ).
We leveraged time-series techniques to predict the query popularity and showed that such methods consistently out-perform different baselines that use aggregated data. We also demonstrated that predictions based on aggregated fre-quency over a long period are usually worse than those made on smaller but more recent data. Finally, we showed that us-ing more accurate query popularity predictions produced by time-series modeling leads to higher quality auto-completion query suggestions.

As feature work, a mixture model produced by fitting sev-eral time-series models simultaneously (with different values of  X  ) may improve the quality of predictions. Last but not least, similar analysis can be done for different levels of gran-ularity such as hourly intervals. [1] E. Alfonseca, M. Ciaramita, and K. Hall. Gazpacho [2] Z. Bar-Yossef and N. Kraus. Context-sensitive query [3] R. Baraglia, F. M. Nardini, C. Castillo, R. Perego, [4] S.M.Beitzel,E.C.Jensen,A.Chowdhury, [5] K. Berberich, S. Bedathur, O. Alonso, and [6] S. Bickel, P. Haider, and T. Scheffer. Learning to [7] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [8] S. Chaudhuri and R. Kaushik. Extending [9] S. Chien and N. Immorlica. Semantic similarity  X   X  [10] H. Cho and H. Varian. Predicting the present with [11] N. Dai and B. D. Davison. Freshness matters: in [12] N. Dai, M. Shokouhi, and B. D. Davison. Learning to [13] J. J. Darragh, I. H. Witten, and M. L. James. The [14] A. Dong, R. Zhang, P. Kolari, J. Bai, F. Diaz, [15] M. Efron. Linear time series models for term [16] J. L. Elsas and S. T. Dumais. Leveraging temporal [17] J. Fan, H. Wu, G. Li, and L. Zhou. Suggesting [18] K. Grabski and T. Scheffer. Sentence completion. In [19] C. C. Holt. Forecasting seasonals and trends by [20] S. Ji, G. Li, C. Li, and J. Feng. Efficient interactive [21] R. Jones and F. Diaz. Temporal profiles of queries. [22] N. Kanhabua and K. N X rv  X  ag. Determining time of [23] A. Kulkarni, J. Teevan, K. M. Svore, and S. T. [24] X. Li and W. B. Croft. Time-based language models. [25] N. Liu, J. Yan, S. Yan, W. Fan, and Z. Chen. Web [26] D. Metzler, R. Jones, F. Peng, and R. Zhang. [27] A. Nandi and H. V. Jagadish. Effective phrase [28] P. L. R. Byrd and J. Nocedal. SIAM Journal on [29] K.Radinsky,K.M.Svore,J.Teevan,S.T.Dumais, [30] Y. Shimshoni, N. Efron, and Y. Matias. On the [31] M. Shokouhi. Detecting seasonal queries by time-series [32] M. Vlachos, C. Meek, Z. Vagena, and D. Gunopulos. [33] R. W. White and G. Marchionini. Examining the [34] R. Zhang, Y. Chang, Z. Zheng, D. Metzler, and J.-y.
