 Sparse linear models have become a popular framework for dea ling with various unsupervised and supervised tasks in machine learning and signal processing . In such models, linear combinations of small sets of variables are selected to describe the data. Re gularization by the  X  1 -norm has emerged as a powerful tool for addressing this combinatorial variab le selection problem, relying on both a well-developed theory (see [1] and references therein) and efficient algorithms [2, 3, 4]. The  X  1 -norm primarily encourages sparse solutions, regardless o f the potential structural relation-ships (e.g., spatial, temporal or hierarchical) existing b etween the variables. Much effort has recently been devoted to designing sparsity-inducing regularizati ons capable of encoding higher-order infor-mation about allowed patterns of non-zero coefficients [5, 6 , 7, 8, 9, 10], with successful applications in bioinformatics [6, 11], topic modeling [12] and computer vision [9, 10].
 By considering sums of norms of appropriate subsets, or groups , of variables, these regulariza-tions control the sparsity patterns of the solutions. The un derlying optimization problem is usually difficult, in part because it involves nonsmooth components . Proximal methods have proven to be While the settings where the penalized groups of variables do not overlap or are embedded in a tree-shaped hierarchy [12] have already been studied, regulariz ations with general overlapping groups have, to the best of our knowledge, never been addressed with proximal methods.
 This paper makes the following contributions: computed with a fast and scalable procedure by solving a quadratic min-cost flow problem. efficiently, which enables us to compute duality gaps for the corresponding optimization problems. subtraction to estimation of hierarchical structures for d ictionary learning of natural image patches. We consider in this paper convex optimization problems of th e form where f : R p  X  R is a convex differentiable function and  X  : R p  X  R is a convex, nonsmooth, sparsity-inducing regularization function. When one knows a priori that the solutions of this learn-ing problem have only a few non-zero coefficients,  X  is often chosen to be the  X  1 -norm (see [1, 2]). When these coefficients are organized in groups, a penalty enc oding explicitly this prior knowl-edge can improve the prediction performance and/or interpr etability of the learned models [13, 14]. Denoting by G a set of groups of indices, such a penalty might for example ta ke the form: coefficients of w indexed by g in G , and the scalars  X  g are positive weights. A sum of  X  2 -norms is also used in the literature [7], but the  X   X  -norm is piecewise linear, a property that we take advantage of in this paper. Note that when G is the set of singletons of [1; p ] , we get back the  X  1 -norm. If G is a more general partition of [1; p ] , variables are selected in groups rather than individually . When the groups overlap,  X  is still a norm and sets groups of variables to zero together [ 5]. The latter setting has first been considered for hierarchies [7, 11, 15], and then extended to general group structures [5]. 1 Solving Eq. (1) in this context becomes challenging and is th e topic of this paper. Following Jenatton et al. [12] who tackled the case of hierar chical groups, we propose to approach this problem with proximal methods, which we now introduce. 2.1 Proximal Methods In a nutshell, proximal methods can be seen as a natural exten sion of gradient-based techniques, and they are well suited to minimizing the sum f +  X   X  of two convex terms, a smooth function f  X  X ontinuously differentiable with Lipschitz-continuous g radient X  and a potentially non-smooth function  X   X  (see [16] and references therein). At each iteration, the fu nction f is linearized at the current estimate w 0 and the so-called proximal problem has to be solved: The quadratic term keeps the solution in a neighborhood wher e the current linear approximation holds, and L &gt; 0 is an upper bound on the Lipschitz constant of  X  f . This problem can be rewritten as with  X   X  ,  X /L , and u , w 0  X  1 L  X  f ( w 0 ) . We call proximal operator associated with the regulariza-tion  X   X   X  the function that maps a vector u in R p onto the (unique, by strong convexity) solution w  X  of Eq. (3). Simple proximal methods use w  X  as the next iterate, but accelerated variants [3, 4] are also based on the proximal operator and require to solve prob lem (3) exactly and efficiently to enjoy their fast convergence rates. Note that when  X  is the  X  1 -norm, the solution of Eq. (3) is obtained by soft-thresholding [16]. The approach we develop in the rest of this paper extends [12] to the case of general overlapping groups when  X  is a weighted sum of  X   X  -norms, broadening the application of these regularizations to a wider spectrum of problems. 2 In this section, we show that a convex dual of problem (3) for g eneral overlapping groups G can be reformulated as a quadratic min-cost flow problem . We present an efficient algorithm to solve it exactly , as well as a related algorithm to compute the dual norm of  X  . We start by considering the dual formulation to problem (3) introduced in [12], for the c ase where  X  is a sum of  X   X  -norms: Lemma 1 (Dual of the proximal problem [12]) Given u in R p , consider the problem Without loss of generality, 3 we assume from now on that the scalars u j are all non-negative, and we constrain the entries of  X  to be non-negative. We now introduce a graph modeling of prob lem (4). 3.1 Graph Model Let G be a directed graph G = ( V, E, s, t ) , where V is a set of vertices, E  X  V  X  V a set of arcs, s a source, and t a sink. Let c and c  X  be two functions on the arcs, c : E  X  R and c  X  : E  X  R + , where c is a cost function and c  X  is a non-negative capacity function . A flow is a non-negative function on arcs that satisfies capacity constraints on all arcs (the v alue of the flow on an arc is less than or equal to the arc capacity) and conservation constraints on a ll vertices (the sum of incoming flows at a vertex is equal to the sum of outgoing flows) except for the so urce and the sink.
 We introduce a canonical graph G associated with our optimization problem, and uniquely cha rac-terized by the following construction: (i) V is the union of two sets of vertices V u and V gr , where V u contains exactly one vertex for each index j in [1; p ] , and V gr contains exactly one vertex for each group g in G . We thus have | V | = |G| + p . For simplicity, we identify groups and indices with the ver tices of the graph. (ii) For every group g in G , E contains an arc ( s, g ) . These arcs have capacity  X  X  g and zero cost. (iii) For every group g in G , and every index j in g , E contains an arc ( g, j ) with zero cost and infinite capacity. We denote by  X  g j the flow on this arc. (iv) For every index j in [1; p ] , E contains an arc ( j, t ) with infinite capacity and a sarily have  X   X  j = P g  X  X   X  g j .
 Examples of canonical graphs are given in Figures 1(a)-(c). The flows  X  g j associated with G can now be identified with the variables of problem (4): indeed, the s um of the costs on the edges leading to the sink is equal to the objective function of (4), while th e capacities of the arcs ( s, g ) match the constraints on each group. This shows that finding a flow minimizing the sum of the costs on such a graph is equivalent to solving problem (4).
 When some groups are included in others, the canonical graph c an be simplified to yield a graph with a smaller number of edges. Specifically, if h and g are groups with h  X  g , the edges ( g, j ) for j  X  h carrying a flow  X  g j can be removed and replaced by a single edge ( g, h ) of infinite capacity and equivalent to the one of Figure 1(c). This does not change the optimal value of  X   X   X  , which is the quantity of interest for computing the optimal primal varia ble w  X  (a proof and a formal definition of these equivalent graphs are available in a longer technic al report [17]). These simplifications are useful in practice, since they reduce the number of edges in t he graph and improve the speed of the algorithms we are now going to present. Figure 1: Graph representation of simple proximal problems with different group structures G . The three indices 1 , 2 , 3 are represented as grey squares, and the groups g, h in G as red discs. The source is linked to every group g, h with respective maximum capacity  X  X  g ,  X  X  h and zero cost. Each other arcs in the graph have zero cost and infinite capacity. T hey represent inclusion relationships in-between groups, and between groups and variables. The gr aphs (c) and (d) correspond to a special case of tree-structured hierarchy in the sense of [12]. Thei r min-cost flow problems are equivalent. 3.2 Computation of the Proximal Operator Quadratic min-cost flow problems have been well studied in th e operations research literature [18]. One of the simplest cases, where G contains a single group g (  X  is the  X   X  -norm) as in Figure 1(a), can be solved by an orthogonal projection on the  X  1 -ball of radius  X  X  g . It has been shown that such a projection can be done in O ( p ) operations [18, 19]. When the group structure is a tree as in Figure 1(d), the problem can be solved in O ( pd ) operations, where d is the depth of the tree [12, 18]. 4 The general case of overlapping groups is more difficult. Hoc hbaum and Hong have shown in [18] that quadratic min-cost flow problems can be reduced to a specific parametric max-flow problem, for which an efficient algorithm exists [20]. 5 While this generic approach could be used to solve Eq. (4), we propose to use Algorithm 1 that also exploits the f act that our graphs have non-zero costs only on edges leading to the sink. As shown in the technical re port [17], it has a significantly better performance in practice. This algorithm clearly shares som e similarities with existing approaches in network flow optimization such as the simplified version of [20] presented in [21] that uses a divide and conquer strategy. Moreover, we have discovered a fter that this paper was accepted for publication that an equivalent algorithm exists for minimi zing convex functions over polymatroid sets [22]. This equivalence, however, requires a non-trivi al representation of structured sparsity-inducing norms with submodular functions, as recently poin ted out by [23].
 Algorithm 1 Computation of the proximal operator for overlapping group s. 1: Inputs: u  X  R p , a set of groups G , positive weights (  X  g ) g  X  X  , and  X  (regularization parameter). 2: Build the initial graph G 0 = ( V 0 , E 0 , s, t ) as explained in Section 3.2. 3: Compute the optimal flow:  X   X   X  computeFlow ( V 0 , E 0 ) . 4: Return: w = u  X   X   X  (optimal solution of the proximal problem).
 Function computeFlow ( V = V u  X  V gr , E ) 1: Projection step:  X   X  arg min  X  P j  X  V 2: For all nodes j in V u , set  X  j to be the capacity of the arc ( j, t ) . 3: Max-flow step: Update (  X   X  j ) j  X  V 4: if  X  j  X  V u s.t.  X   X  j 6 =  X  j then 5: Denote by ( s, V + ) and ( V  X  , t ) the two disjoint subsets of ( V, s, t ) separated by the minimum 8: end if The intuition behind this algorithm is the following: The fir st step looks for a candidate value for  X   X  = P g  X  X   X  g by solving a relaxed version of problem Eq. (4), where the con straints k  X  g k 1  X   X  X  g are dropped and replaced by a single one k  X   X  k 1  X   X  P g  X  X   X  g . The relaxed problem only depends on  X   X  and can be solved in linear time. By calling its solution  X  , it provides a lower bound k u  X   X  k 2 2 / 2 on the optimal cost. Then, the second step tries to find a feasi ble flow of the original problem (4) such that the resulting vector  X   X  matches  X  , which is in fact a max-flow problem [24]. If  X   X  =  X  , then the cost of the flow reaches the lower bound, and the flow is optimal. If  X   X  6 =  X  , the lower bound is not achievable, and we construct a minimum ( s, t ) -cut of the graph [25] that defines two disjoints sets of nodes V + and V  X  ; V + is the part of the graph that could potentially have received more flow from the source (the arcs between s and V + are not saturated), whereas all arcs linking s to V  X  are saturated. At this point, it is possible to show that the v alue of the optimal min-cost flow on all arcs between V + and V  X  is necessary zero. Thus, removing them yields an equivalent optimization problem, which can be decomposed into two inde pendent problems of smaller sizes and solved recursively by the calls to computeFlow ( V + , E + ) and computeFlow ( V  X  , E  X  ) . A formal proof of correctness of Algorithm 1 and further detai ls are relegated to [17].
 The approach of [18, 20] is guaranteed to have the same worst-case complexity as a single max-flow algorithm. However, we have experimentally observed a sign ificant discrepancy between the worst case and empirical complexities for these flow problems, ess entially because the empirical cost of each max-flow is significantly smaller than its theoretical c ost. Despite the fact that the worst-case of our graphs and has proven to be much faster in our experimen ts (see technical report [17]). Some implementation details are crucial to the efficiency of the algorithm: possible to process them independently in order to solve the global min-cost flow problem. for solving our max-flow problems, using classical heuristi cs that significantly speed it up in practice (see [24, 26]). This algorithm leverages the conce pt of pre-flow that relaxes the defini-tion of flow and allows vertices to have a positive excess. It c an be initialized with any valid pre-flow, enabling warm-restarts when the max-flow is called several times as in our algorithm. flow step. Adding these additional constraints leads to bett er performance when the graph is not well balanced. This modified projection step can still be com puted in linear time [19]. 3.3 Computation of the Dual Norm tity to study sparsity-inducing regularizations [5, 15, 27 ]. We use it here to monitor the convergence of the proximal method through a duality gap, and define a prop er optimality criterion for prob-The duality gap for problem (1) can be derived from standard F enchel duality arguments [28] and it is equal to f ( w ) +  X   X ( w ) + f  X  (  X   X  ) for w ,  X  in R p with  X   X  (  X  )  X   X  . Therefore, evaluating the duality gap requires to compute efficiently  X   X  in order to find a feasible dual variable  X  . This is equivalent to solving another network flow problem, based on the following variational formulation: and the capacities on the arcs ( j, t ) , j in [1; p ] , are fixed to  X  j . Solving problem (5) amounts to finding the smallest value of  X  , such that there exists a flow saturating the capacities  X  j on the arcs leading to the sink t (i.e.,  X   X  =  X  ). The algorithm below is proven to be correct in [17]. Algorithm 2 Computation of the dual norm. 1: Inputs:  X   X  R p , a set of groups G , positive weights (  X  g ) g  X  X  . 2: Build the initial graph G 0 = ( V 0 , E 0 , s, t ) as explained in Section 3.3. 3:  X   X  dualNorm ( V 0 , E 0 ) . 4: Return:  X  (value of the dual norm).
 Function dualNorm ( V = V u  X  V gr , E ) 2: Max-flow step: Update (  X   X  j ) j  X  V 3: if  X  j  X  V u s.t.  X   X  j 6 =  X  j then 4: Define ( V + , E + ) and ( V  X  , E  X  ) as in Algorithm 1, and set  X   X  dualNorm ( V  X  , E  X  ) . 5: end if 6: Return:  X  . Our experiments use the algorithm of [4] based on our proxima l operator, with weights  X  g set to 1 . 4.1 Speed Comparison We compare our method (ProxFlow) and two generic optimizati on techniques, namely a subgradient descent (SG) and an interior point method, 6 on a regularized linear regression problem. Both SG and ProxFlow are implemented in C++ . Experiments are run on a single-core 2 . 8 GHz CPU. We con-sider a design matrix X in R n  X  p built from overcomplete dictionaries of discrete cosine tr ansforms (DCT), which are naturally organized on one-or two-dimensi onal grids and display local corre-lations. The following families of groups G using this spatial information are thus considered: (1) every contiguous sequence of length 3 for the one-dimensional case, and (2) every 3  X  3 -square in the two-dimensional setting. We generate vectors y in R n according to the linear model y = Xw 0 +  X  , where  X   X  N (0 , 0 . 01 k Xw 0 k 2 2 ) . The vector w 0 has about 20% percent nonzero components, ran-domly selected, while respecting the structure of G , and uniformly generated between [  X  1 , 1] . In our experiments, the regularization parameter  X  is chosen to achieve the same sparsity as w 0 . For problem (1) can be cast either as a quadratic (QP) or as a conic program (CP), we show in Figure 2 the results for both formulations. Our approach compares fa vorably with the other methods, on addition, note that QP, CP and SG do not obtain sparse solutio ns, whereas ProxFlow does. We have also run ProxFlow and SG on a larger dataset with ( n, p ) = (100 , 10 6 ) : after 12 hours, ProxFlow and SG have reached a relative duality gap of 0 . 0006 and 0 . 02 respectively. 7 Figure 2: Speed comparisons: distance to the optimal primal value versus CPU time (log-log scale). 6 Figure 3: From left to right: original image y ; estimated background Xw ; foreground (the sparsity pattern of e used as mask on y ) estimated with  X  1 ; foreground estimated with  X  1 +  X  ; another foreground obtained with  X  , on a different image, with the same values of  X  1 ,  X  2 as for the previous image. For the top row, the percentage of pixels matching the ground truth is 98.8% with  X  , 87.0% without. As for the bottom row, the result is 93.8% with  X  , 90.4% without (best seen in color). 4.2 Background Subtraction Following [9, 10], we consider a background subtraction tas k. Given a sequence of frames from a fixed camera, we try to segment out foreground objects in a new image. If we denote by y  X  R n a test image, we model y as a sparse linear combination of p other images X  X  R n  X  p , plus an error term e in R n , i.e., y  X  Xw + e for some sparse vector w in R p . This approach is reminiscent of [29] in the context of face recognition, where e is further made sparse to deal with occlusions. The term Xw accounts for background parts present in both y and X , while e contains specific, or foreground , objects in y . The resulting optimization problem is min w , e 1 2 k y  X  Xw  X  e k 2 2 +  X  k w k 1 +  X  2 k e k 1 , with  X  1 ,  X  2  X  0 . In this formulation, the  X  1 -norm penalty on e does not take into account the fact that neighboring pixels in y are likely to share the same label (background or foreground), which may lead to scattered pieces of foregrou nd and background regions (Figure 3). We therefore put an additional structured regularization t erm  X  on e , where the groups in G are all the overlapping 3  X  3 -squares on the image. A dataset with hand-segmented evalua tion images is used to illustrate the effect of  X  . 8 For simplicity, we use a single regularization parameter, i .e.,  X  1 =  X  2 , chosen to maximize the number of pixels matching the ground truth. We consider p = 200 images with n = 57600 pixels (i.e., a resolution of 120  X  160 , times 3 for the RGB channels). As shown in Figure 3, adding  X  improves the background subtraction results for the two tes ted videos, by encoding, unlike the  X  1 -norm, both spatial and color consistency. 4.3 Multi-Task Learning of Hierarchical Structures In [12], Jenatton et al. have recently proposed to use a hiera rchical structured norm to learn dictio-naries of natural image patches. Following this work, we see k to represent n signals { y 1 , . . . , y n } of dimension m as sparse linear combinations of elements from a dictionary X = [ x 1 , . . . , x p ] in R m  X  p . This can be expressed for all i in [1; n ] as y i  X  Xw i , for some sparse vector w i in R p . In [12], the dictionary elements are embedded in a predefined tree T , via a particular instance of the structured norm  X  ; we refer to it as  X  tree , and call G the underlying set of groups. In this case, each signal y i admits a sparse decomposition in the form of a subtree of dict ionary elements. Inspired by ideas from multi-task learning [14], we propose to learn the tree structure T by pruning irrelevant parts of a larger initial tree T 0 . We achieve this by using an additional regularization term  X  joint across the different decompositions, so that subtrees of T 0 will simultaneously be removed for all signals y i . In other words, the approach of [12] is extended by the follo wing formulation: min where W , [ w 1 , . . . , w n ] is the matrix of decomposition coefficients in R p  X  n . The new regular-overall penalty on W , which results from the combination of  X  tree and  X  joint , is itself an instance of  X  with general overlapping groups, as defined in Eq (2).
 To address problem (6), we use the same optimization scheme a s [12], i.e., alternating between X and W , fixing one variable while optimizing with respect to the oth er. The task we consider is the denoising of natural image patches, with the same dataset an d protocol as [12]. We study whether learning the hierarchy of the dictionary elements improves the denoising performance, compared to standard sparse coding (i.e., when  X  tree is the  X  1 -norm and  X  2 = 0 ) and the hierarchical dictionary learning of [12] based on predefined trees (i.e.,  X  2 = 0 ). The dimensions of the training set  X  50 000 patches of size 8  X  8 for dictionaries with up to p = 400 elements  X  impose to handle large graphs, with | E | X  X  V | X  4 . 10 7 . Since problem (6) is too large to be solved many times to sele ct the regularization parameters (  X  1 ,  X  2 ) rigorously, we use the following heuristics: we optimize mo stly with the currently pruned tree held fixed (i.e.,  X  2 = 0 ), and only prune the tree (i.e.,  X  2 &gt; 0 ) every few steps on a random subset of 10 000 patches. We consider the same hierarchies as in [12], involving between 30 and 400 dictionary elements. The regularization parameter  X  1 is selected on the validation set of 25 000 patches, for both sparse coding (Flat) and hierarchical dic tionary learning (Tree). Starting from the tree giving the best performance ( in this case the largest one, see Figure 4), we solve problem (6) following our heuristics, for increasi ng values of  X  2 . As shown in Figure 4, there is a regime where our approach performs significantly b etter than the two other compared methods. The standard deviation of the noise is 0 . 2 (the pixels have values in [0 , 1] ); no significant improvements were observed for lower levels of noise. Figure 4: Left: Hierarchy obtained by pruning a larger tree o f 76 elements. Right: Mean square error versus dictionary size. The error bars represent two s tandard deviations, based on three runs. We have presented a new optimization framework for solving s parse structured problems involving sums of  X   X  -norms of any (overlapping) groups of variables. Interesti ngly, this sheds new light on connections between sparse methods and the literature of ne twork flow optimization. In particular, the proximal operator for the formulation we consider can be cast as a quadratic min-cost flow problem, for which we propose an efficient and simple algorit hm. This allows the use of accelerated gradient methods. Several experiments demonstrate that ou r algorithm can be applied to a wide class of learning problems, which have not been addressed before w ithin sparse methods.
 Acknowledgments This paper was partially supported by the European Research Council (SIERRA Project). The au-thors would like to thank Jean Ponce for interesting discuss ions and suggestions. References
