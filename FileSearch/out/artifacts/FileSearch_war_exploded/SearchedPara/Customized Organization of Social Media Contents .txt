 With the popularity of social media platforms such as Face-book and Twitter, the amount of useful data in these sources is rapidly increasing, making them promising places for in-formation acquisition. This research aims at the customized organization of a social media corpus using focused topic hi-erarchy. It organizes the contents into different structures to meet with users X  different information needs (e.g.,  X  X Phone 5 problem X  or  X  X Phone 5 camera X ). To this end, we intro-duce a novel function to measure the likelihood of a topic hierarchy, by which the users X  information need can be in-corporated into the process of topic hierarchy construction. Using the structure information within the generated topic hierarchy, we then develop a probability based model to i-dentify the representative contents for topics to assist users in document retrieval on the hierarchy. Experimental re-sults on real world data illustrate the effectiveness of our method and its superiority over state-of-the-art methods for both information organization and retrieval tasks. H.0 [ Information Systems ]: General; H.3.3 [ Informa-tion Storage and Retrieval ]: Information Search and Retrieval customized information organization; focused topic hierar-chy; information need; social media corpus
Thi s work was done when the first author was a visiting student in National University of Singapore. Corresponding author.
 Fi gure 1: The focused topic hierarchy for  X  X Phone 5 problem X , where different representative contents are collected for the four specific nodes of problem (highlighted with colors) accordingly.
With the rapid growth of contents in social media sources like Twitter 1 , Facebook 2 and Yahoo! Answers 3 , users are able to access huge amounts of data from these sources to find their desired information. However, the information overload and noise in social media contents limit their po-tential usefulness. To tackle this problem, organizing the social media contents into a general topic hierarchy has been shown to be an effective solution [23] [24] [29], in which the user is presented with an in-depth overview of his/her de-sired topics in the form of hierarchically organized document clusters.

The information need of different users may vary greatly, ranging from an overall description of a topic, e.g.,  X  X Phone 5 X , to contents about its particular aspects, for which a gen-eral topic hierarchy may not be applicable. For example, given a user information need on  X  X Phone 5 problem X , a fo-cused topic hierarchy as shown in Figure 1 should be more preferable, as compared to a general hierarchy of iPhone 5 (e.g., the one in Wikipedia 4 which contains subsections like  X  X istory X  and  X  X ale X ), Moreover, even if the topic hierarchy is presented, it could still be time-consuming for users to ma nually find useful contents in it. To this end, the ability to identify representative contents for each node on the hier-archy is necessary. For example, the representative contents for the node problem under battery in Figure 1 should focus only on iPhone 5 X  X  battery problem.

Generally, there are two major research challenges to or-ganize information using focused topic hierarchies.
In this paper, we propose a novel method for the cus-tomized organization of social media contents using focused topic hierarchies. Given the user X  X  information need, e.g.,  X  X Phone 5 problem X , we first use a propagation algorithm to collect the potentially useful topics, e.g., battery and solu-tion . Next, we devise a function to estimate the likelihood of a topic hierarchy and use it to integrate the obtained topics into a topic hierarchy that fits both the given social media corpus and the user X  X  personal information need. Finally, in order to further assist users to search on the hierarchy, we propose a probability based ranking model to identify the representative contents for topic nodes using both content and source based features of documents. To summarize, the main contributions of our approach are two fold:
The rest of this paper is organized as follows. In section 2 we introduce our related work. In section 3 we will for-mulate our research problems. In section 4 we describe our proposed framework and its three modules. In section 5 we evaluate the performance of the proposed method using re-al world data and compare it with state-of-the-art methods. Finally, we summarize the paper and outline the future work in section 6.
To organize the information in a text corpus efficiently has been studied by many researches [2] [5][12][27][29] be-fore. Compared to the early works [20][27] that proposed to split the corpus into shallow clusters, many recent ap-proaches tend to organize the corpus into cluster hierar-chies which could provide users with more in-depth view of the corpus and improve the search experience. Specifi-cally, methods like agglomerative hierarchical clustering [6] [9][28], hierarchical LDA [2][15][23] were all adopted in this task. Recently, approaches like [1][16] proposed to further improve the performance of common hierarchical clustering algorithms using partially known hierarchies. Besides, re-searchers [3][24] also used time-line analysis techniques to enable evolutionary hierarchical clustering on ever-changing contents.
Constructing concepts into taxonomies is useful for the or-ganization of the underlying knowledge in a given domain. Previous approaches like [17][19][21][25] used is-a relations between terms to connect them into taxonomies. Besides, multi-branch clustering algorithms [4][13][22] were also pro-posed for hierarchy generation. In [22], a generative model was used to build up a topic hierarchy by splitting the topic set into smaller clusters as its children on the hierarchy it-eratively. In [13], the authors proposed to use the bayesian rose tree clustering algorithm to automatically determine the depth and width of the resultant topic hierarchy. Be-sides, as shown in [14], linked data in Freebase were also shown to be helpful in identifying the underlying structures of topics. In a recent approach [8], word embeddings tech-nique was also adopted in hierarchy generation tasks.
Similar to our proposed method, some researches [26][29] attempted to generate topic hierarchies from social media contents. However, since they didn X  X  consider the users X  in-formation need as an input, they could construct only one general topic hierarchy for a given corpus. As a result, for a given social media corpus, they were not able to provide satisfactory results for users with various information needs.
In this research, we aim to realize the customized topic hierarchy generation and use it for information organization and retrieval tasks.
Given a social media source set S = { s 0 ; s 1 ; ::: } , we define a social media corpus D = the source s i and d i = ( c i ; s i ), in which c i is the document X  X  content and s i indicates its source. Next, we define the topic set T ( D ) for D as T ( D ) = { t 1 ; t 2 ; ::: } , in which t noun phrase and indicates a subtopic in D . For example, if the contents in D are all about iPhone 5, the corresponding T (
D ) may contain subtopics like battery and price .
We formulate a user information need q , or information need for short, as q = { t r ; t 1 ; t 2 ; ::: } , in which t the root topic, e.g.,  X  X Phone 5 X , that the user is interested in iPhone 5 on which the specific information is requested by the user.

Generally, user information needs can express the search intents of many keyword queries. Moreover, queries like  X  X -Phone 5 problem X  can even be directly interpreted into in-fo rmation needs, e.g., { iPhone 5, problem } . However, the process of transforming between the queries and informa-tion needs is beyond the scope of this paper. For the sake of consistency, we use the information need as the input of our proposed framework in the rest of this paper.
In this research, a focused topic hierarchy is used to or-ganize a social media corpus based on a user X  X  information need. Given an information need q = { t r ; t 1 ; t 2 ; ::: cial media corpus D ( t r ) in which all the contents are relevant to the root topic t r , a focused topic hierarchy H q consists of the following two components:
Generally, a focused topic hierarchy H q can organize the social media contents for the information need q with hier-archically structured topics. For example, given the focused topic hierarchy for { iPhone 5, problem } , its topic node set may contain topics such as battery drain and problem , where battery drain is a subtopic of problem and the blog  X  X Phone 5 battery life is significantly reduced by ... X  will be assigned to battery drain as its relevant content. Using the focused topic hierarchy, users can directly obtain their required in-formation by clicking through the connected topics on the hierarchy and reading the linked social media contents.
However, to generate a proper focused topic hierarchy that best meets an information need is not trivial. Generally, there are three research problems that we need to address in this study: (1) How to discover the potentially useful topics for an in-formation need? (2) How to obtain the optimal topic structure to organize the users X  desired information? (3) How to distinguish the representative contents for topics that directly meet the users X  requirements?
In order to tackle the three problems, in this paper we introduce a three step pipeline as shown in Figure 2, in which at each step we solve one of the problems sequential-ly. Specifically, given an information need q = { t r ; t and the social media corpus D ( t r ), we first collect q  X  X  rele-vant topics from T ( D ( t r )). Next we construct the optimal topic hierarchy H q for the given information need using the obtained topics. Finally, we assign each topic node on H q with its most representative documents in D ( t r ) as its rele-vant content set.

In next subsection, we will first show how we collect the social media corpus and model it using automatically ex-tracted topics. The followed three subsections will describe the details of each module in Figure 2.
 Fi gure 2: The proposed framework for the cus-tomized organization task.
The volume of social media contents increases rapidly. In this approach, we obtain topics and topic relations from the crawled social media contents incrementally and utilize them to model the resultant corpus. It is worth noting that the following corpus modeling process can be done off-line since it is not dependent on any specific user information need.
From a social media corpus D , we use a TF-IDF based keyword extraction method to generate the topic set T ( D Next, for each topic t in T ( D ), we further use a salience score ( t )  X  R + to measure its importance. Generally, a topic with higher salience score should be more important in the corpus. For example, we may have ( wifi ) = 0 : 59 and ( thing ) = 0 : 12 on the corpus for iPhone 5, indicating that wifi is more important than thing here. In practice, we use the method proposed in [27] to estimate the topic salience scores with multiple corpus-derived features.
Topics in T ( D ) could be correlated with each other through various relations. In the paper, we investigate the use of two topic relations, i.e., the topic relevance and subtopic relation.
The topic relevance ( t i ; t j )  X  [0 ; 1] indicates the strength of semantic relatedness between two topics t i and t j . For ex-ample, we may have ( network ; wifi ) = 0 : 65 and ( screen ; wifi ) = 0 : 17, indicating that network is more relevant to wifi than screen . In this research, we adopt the method in [10] to esti-mate the topic relevance scores based on topic co-occurrence.
The subtopic relation strength ( t m ; t n )  X  [0 ; 1], indicates the likelihood that t n is a subtopic of t m . For example, we may have ( network ; wifi ) = 0 : 89 and ( wifi ; network ) = 0 : 07, indicating that wifi is more likely to be a subtopic of network . We utilized the method in [29] to estimate ( t i ; t j ) using heuristic rules.
Given an information need q = { t r ; t 1 ; t 2 ; ::: } and the so-cial media corpus D ( t r ), our first task is to collect a subset T q  X  T ( D ( t r )), in which only the topics that fit the spe-cific information need are included. We first use a simple co -occurrence based method to construct a raw candidate topic set T 0 q , which includes all the topics in T ( D ( t co-occur with the subtopics in q . Next, we propose a graph-based label propagation algorithm to refine T 0 q using topics X  semantic relatedness.

Regarding topics in T ( D ( t r )) as vertexes, we construct a topic graph by bridging each topic pair t i and t j with a undirected edge weighted by their topic relevance score ( t i ; t j ). We first assign an initial weight w ( t ) for each topic t on the graph as follows, recall that ( t ) is the salience score of the topic t in the given corpus.

Based on the resultant topic graph, Equation 2 is used to propagate the topic weights between tightly related topics.
In Equation 2,  X  w ( t ) and  X ( t ) indicate the impact of the topic itself and that from its neighbors in the propagation, respectively. Note that when estimating  X ( t ), we only re-tain the topic t  X  X  influence to its nearest k neighbor set, i.e.,  X  ( t ), so that the propagation can be limited to a dense sub-graph, hence keeping out the potential noisy topics in T ( D ( t r )).

For the same purpose, in Equation 3, a zero-weighted topic t can obtain non-zero initial weight only if there is a topic t where t i  X   X  k ( t j ) and w ( t j )  X  = 0. In Equation 4, the transmitting probability from t i to t j , i.e., ( t i ; t zero only if t j  X   X  k ( t i ).

Finally, we rank the topics using their final topic weights and collect the top ranked non-zero topics to form the fo-cused topic set T q .
For an information need q = { t r ; t 1 ; t 2 ; ::: } , there are vari-ous ways to integrate the topics in T q into a topic hierarchy H . In this paper, we propose to use the following function L (
H )  X  X  + to measure the likelihood that the hierarchy H can fit a given information need.
 in which e ( v i ; v j ) is an edge from v i to v j on H , indicating that v j is a subtopic of v i on the hierarchy. w ( e ( v R + is the weight of e ( v i ; v j ), indicating the likelihood of this edge.

Given Equation 5, if all the edge weights on the hierar-chy are properly estimated, we can find the optimal focused topic hierarchy straight-forwardly. In this research, we in-troduce the following two assumptions to measure an edge X  X  weight, e.g., w ( e ( v i ; v j )) from two perspectives. The information need's perspective: w ( e ( v i ; v j )) is only relevant to the importance of v i and v j for the given infor-mation need and their subtopic relation strength. For exam-ple, given the information need { iPhone 5 device } , the edge e ( camera ; lens ) should be higher weighted than e ( camera ; photo ) since: (1) the subtopic lens is more relevant to the informa-tion need, and (2) the subtopic relation between camera and lens is also stronger.
 The taxonomy structure's perspective: w ( e ( v i ; v j only relevant to the fitness of the edge e ( v i ; v j ) on the current if it is part of a path X  network  X  problem  X  battery drain  X  X n the hierarchy, it should be weighted low since battery drain is irrelevant to network . On the contrary, for the path  X  battery  X  problem  X  battery drain  X , this edge should be weighted high.
 In order to combine the two assumptions, we define a path L = the hierarchy and v j L j is any non-root node on the hierarchy. We then estimate w ( L ), i.e., the path weight of L as follows. in which t k = t ( v k ) and t k +1 = t ( v k +1 ).
We can see that Equation 6 involves both the information need-related factors (i.e., w ( t k ), w ( t k +1 ) and ( t and the taxonomy-related factors (i.e., the path from the root to v j L j ). As a result, w ( L ) can be used to combine the above two assumptions mathematically. Finally, for each edge e ( v i ; v j ) on the hierarchy, we estimate the edge weight w ( e ( v i ; v j )) as the weight of the path on the hierarchy that ends with this edge and has the maximum path weight, re-sulting in the following equation:
Generally, the likelihood of a topic hierarchy can be cor-related with the user X  X  information need through the esti-mation of w ( e ( v i ; v j )), in which the more the topic hierar-chy meets the information need, the higher likelihood it can obtain. This is one of the major differences between our method and previous approaches on general topic hierarchy construction [25][29].
Using the proposed likelihood function, we propose Algo-rithm 1 for topic hierarchy generation. Generally, the al-gorithm first sets the root of the resultant hierarchy at t Next, it runs iteratively for iterations ( Line 2 ). In each iteration, the algorithm selects a specific topic and add it to a specific position on the current hierarchy so that the likelihood of the resultant hierarchy is maximized.
In practice, although we can find the optimal topic to insert by enumeration, to find the optimal position on the hierarchy for the insertion is not trivial. To this end, we denote v new as the candidate new node and V i 1 q as the to pic node set of the current hierarchy. Then for each topic node v old in V i 1 q , we first link v new with it using a new subtopic relation exists( Line 6 -13 ), resulting in  X  H i 1
The resultant  X  H i 1 q could be problematic. For example, it may contain conflicting nodes and undirected cycles, which could hinder the usage of the generated topic hierarchy. In this research, we propose to solve the problems by pruning q using the following two functions.

The RemoveDup ( : ) function ( Line 14 ) is used to solve the following two conflicts caused by the existence of dupli-cated nodes for a same topic on  X  H i 1 q .
The OptimumBranching ( : ) function ( Line 15 ) aims to break the undirected cycles, which happens when there ex-ists an undirected path like v new  X  v old 2  X  :::  X  v old q . To solve this problem, we run the optimum branch-ing algorithm [7] on  X  H i 1 q , which aims to obtain a tree from q which has the highest sum of edge weights. Note that in Equation 5 the likelihood of a topic hierarchy is defined as the sum of the edge weights, the OptimumBranching ( : ) function can guarantee that its output topic hierarchy is the one with the highest topic hierarchy likelihood.

Finally, note that we introduce a parameter to limit the maximum depth of the resultant topic hierarchy ( Line 17 ). The intuition behind is simple, that users usually do not prefer too complex topic hierarchies. Generally, both and the mentioned parameters , i.e., the limit of the resultant topic hierarchy X  X  size, can be tuned using human generated topic hierarchies like Amazon product categorization 5 .
In this section, we propose to organize the social media contents in D ( t r ) for a given information need q by assigning them to their relevant nodes on the generated topic hierarchy H . Specifically, we first collect the relevant documents for each topic in T q . Next, for each topic node on the hierarchy, we rank its relevant documents using a probability based model and select the top ranked ones to form its relevant content set.
For each topic t  X  T ( D ( t r )), there could be many docu-ments in the social media corpus that focus on it. Without loss of generality, we assume that we have a topic extrac-tion function T opic ( d )  X  2 T ( D ( t r )) to reveal the focuses of documents, then a relevant document for the topic t can be defined as a document that contains t as one of its focused Al gorithm 1 Topic Hierarchy Construction Algorithm Inpu t: T q , the focused topic set; Output: H q , the resultant topic hierarchy; 1: Initiate : set the hierarchy root at t r , resulting in 2: for i=1; i  X  ; i++ do 3: for each t s  X  X  q do 4: create a new node v new , where t ( v new ) = t s ; 7: if ( t ( v new ) ; t ( v old ))  X  = 0 then 9: end if 10: if ( t ( v old ) ; t ( v new ))  X  = 0 then 12: end if 13: end for 14:  X  H i 1 q = RemoveDup(  X  H i 1 q ); 16: end for 18: end for 19: H q  X  H q : to pics. Next, denoting D ( t )  X  D ( t r ) as the relevant docu-ment set for topic t , it can be obtained as follows.
In practice, a simple TF-IDF based keyword extraction method is employed as the T opic ( : ) function. It is noted that D ( t ) is only defined for topics, not topic nodes on topic hierarchies.
Obtained from social media sources like Twitter, the rel-evant document set could be very large and noisy for many topics. In this step, in order to present the users with more compact and precise information on each topic node v on the hierarchy, our goal is to identify the top N representa-tive documents in D ( t ( v )) to form the relevant content set of v , i.e., c ( v ).

Generally, there are two factors that we should consider here. First, when determining a document X  X  representative-ness for a topic node, besides the corresponding topic term, we should also consider its position on the hierarchy. For example, the representative documents for the topic prob-lem on the edge lens  X  problem could be very different from those on the edge battery  X  problem . On the other hand, the document X  X  source may also affect its potential useful-ness. Take tweets as the example, although they are usually noisy and less informative than blogs or cQAs for formal topics like the policy of  X  X arack Obama X , they can also be useful when encountering timely topics such as release date and price of  X  X Phone 5s X .
 In this research, given a topic node v and a document d  X  D ( t ( v )), we propose to use a probability p ( d | v ) to indicate the document d  X  X  representativeness for v . Recall that the document d = ( c; s ), in which c indicates the content of d and s indicates the source that d belongs to. Assuming that c and s are independent, we can decompose p ( d | v ) into t wo parts as in Equation 9. By doing so, the content and source based features of d can be combined to estimate the document X  X  representativeness.
In Equation 9, p ( c | v ) indicates the probability of the docu-ment X  X  content c given the topic node v . As discussed before, it could be relevant to both the topic t ( v ) and its immedi-ate ancestors on the hierarchy. Denote F ( v ) as the set of v and its immediate ancestors, let d ( v i ; v j ) be the distance between v i and v j on the hierarchy, we employ the Okapi BM25 function [18] okapi ( :; : ) to estimate p ( c | v ) as follows. in which Z is a constant that guarantees p ( c | v )  X  [0 ; 1] and  X  [0 ; 1] is a decay factor which controls the impact of v  X  X  distant ancestors on p ( c | v ). Note that when = 0, no structure information on the hierarchy will be considered. More details on the impact of will be shown in Section 5.5.2.

On the other hand, the second factor in Equation 9, i.e., p ( s | v ) indicates the impact of the document X  X  source to its representativeness. In this research, we further expand it using Bayesian rules, resulting in Equation 11.
In Equation 11, p ( s ) is the a priori probability of source s , which can be interpreted as the overall content quality of the social media source s . The intuition behind this is that doc-uments from high quality sources (e.g., blogs and cQAs) are usually more preferable by users. In practice, we estimate this probability as the average salience score of the topics extracted from this source, i.e., p ( s )  X  in which T ( D s ) indicates the topic set of D s and f req ( t ) is the frequency of the topic t in D s .

Next, the probability p ( v | s ) in Equation 11 can be regard-ed as the supportiveness of the source s to topic t ( v ). In this paper, we formulate it as the frequency of t ( v ) in D s t document set of t ( v ) in source s . The intuition behind this is that if a topic is frequently discussed in a source, the contents in this source could be useful to describe this topic.
Finally, for each topic node v on the topic hierarchy, its relevant content set c ( v ) can be generated by aggregating the top ranked N documents by p ( d | v ) from its relevant document set D ( t ( v )).
As shown in Table 1, we crawled blogs, cQAs and tweets to build up the test social media corpus. In order to demon-strate the performance of the proposed method in open-domain applications, the data we crawled is focusing on 9 root topics, which belong to various domains such as digital products, politicians, corporations and tv series. For each root topic, we crawled the blogs by querying Google Blog search 6 and collecting the first 250 returned blogs. For cQAs and tweets, we used the Yahoo! Answer API and Twitter API to obtain real-time data from the data stream 7 .
After pre-processing the crawled data, 395 : 6 subtopics are extracted for each root topic on average. Among these top-ics, it could be very time-consuming for users to manually find the relevant topics for their information needs. More-over, the average amount of relevant documents per topic is 84 : 9 (could be thousands for popular topics like microsoft office ), making it also impossible for users to go through all of them. Generally, the above observations demonstrate the necessity of our proposed task in this paper.
For the 9 root topics, we manually created 14 information needs for evaluation as listed in Table 2. They were selected by the following process: First, we submitted each root top-ic to search engines and collected the phrases in the  X  X elated search X  panels as candidates. Next, for each root topic, we manually selected one or two phrases (e.g.,  X  X Phone 5 prob-lem X ) from the candidate set that have hierarchical subtopics to construct the test information needs.
In this section, we first show the effectiveness of the pro-posed method in focused topic discovery. We compare our method ( T hiR ) with 2 baselines: (1) Topic Salience ( Sal ) [27], in which T q was generated by ranking the topics in by their salience scores, and (2) Topic Relevance ( Rel )[11], in which Markov random walk was adopted for topic weight propagation while the topics X  initial weights were equally set as 1. For all methods, we collected their top 50 topics as their results.

We compare all the methods against the gold standard topic sets, which were generated as follows: First, four un-dergraduate students were asked to label each topic pro-duced by the compared methods as relevant or irrelevant to the information need. Next, only the topics that were la-beled as relevant by all the annotators were added into the gold standard sets. Fi gure 3: Recall comparison of different methods for focused topic discovering.

As to the parameters in the proposed method, we find that when k is around 10 for  X  k ( : ), our resultant focused topic sets can cover most of the gold standards. This observation is reasonable since the amount of tightly relevant topics for any given topic is usually limited. Finally, we set k = 10 and set the iteration number to 5 as in [11]. The recall of all methods is reported in Figure 3, which can be calculated set.
From the result we can see that on average, the proposed method outperforms both baseline methods significantly (t-test, p-value &lt; 0.05) by 48 : 3%, 20 : 0%, respectively. Com-pared to the Sal method, the latter two methods obtain better results in most cases, indicating the effectiveness of the topic weight propagation in focused topic discovery. On the other hand, the topics X  salience scores are also useful for this task, resulting in 20 : 1% performance improvement of our method over the Rel method.
In this Section, we evaluate the performance of the pro-posed topic hierarchy construction method against manually created gold standards. For each information need, the gold standard hierarchy was constructed as follows: First, given the resultant focused topic set from Section 5.3, six under-graduate students were asked to connect the topics therein into a topic hierarchy independently. They then resolved the conflicts through discussions and came up with the final gold standard. Note that in order to prevent the gold standard hierarchies to become too complex to use, they are required to contain less than 40 nodes. On average, the gold standard hierarchies have 29 : 6 nodes, 30 : 6 edges and the depth is 4 : 2.
We use precision, recall and F1 score to measure the per-formance of the compared methods: Denoting E and E g as the edge sets of an output hierarchy and the gold standard, the metrics are calculated as: precision (pre.) = j E \ E
In this section, we first verify the usefulness of the two assumptions introduced in Section 4.4. To this end, we com-pare the proposed method ( T HiR ) with its two variants: (1) BooleanEdge , in which we change Equation 6 into w ( L ) = the users X  information need is discarded, and (2) MaxAll-Sub , in which we change Equation 7 into w ( e ( o i ; o j ( t ( o i ) ; t ( o j )) so that no taxonomy information is injected into the model. According to the average size and depth of the gold standard hierarchies, we set the parameter and as 30 and 5, respectively for all the compared methods. Fi gure 4: Comparison of F1-scores between the pro-posed methods and its two variations.
 Figure 4 illustrates the comparison results on F 1 scores. We can see that the full model ( THiR ) outperforms its two variants significantly (t-test, p-value &lt; 0.05), in which the average improvements against the BooleanEdge and MaxAll-Sub methods are 45.1% and 25.6%, respectively. From this we can conclude that, both of the two assumptions are im-portant for the estimation of a topic hierarchy X  X  likelihood, hence help to achieve better performance in topic hierarchy construction.
In this section, we compare the proposed method with three state-of-the-art methods: (1) Snow's Method [21], which introduced a probability model to obtain the most probable topic hierarchy from a given topic set; (2) Yu's Method [26], which designed an information function to guild the topic hi-erarchy construction using semantic distance between topics and (3) Zhu's Method [29], in which a graph based iterative algorithm was adopted for topic hierarchy construction. For the implementation of all the baseline models, the subtopic relation strength was used in the probability, semantic dis-tance and edge weight calculations, accordingly. Finally, al-though neither of the three baseline methods are developed for focused topic hierarchy construction, since we provide them the same focused topic sets as used in our method, they are still comparable to our method on the performance of topic hierarchy generation.

The experimental results are shown in Table 3, from which we can see that the proposed method outperforms all the three state-of-the-art methods by 41 : 6%, 64 : 8% and 60 : 7%, respectively. The reasons are two fold: First, compared to Snow's and Yu's Methods , the proposed method is more robust to the insertion errors due to our greedy algorith-m framework, which also explains why we can obtain the highest precision on most test samples. On the other hand, although Zhu's Method also employed a greedy algorithm, significant in t-test, p-value &lt; 0.05. our method can make better decisions in each iteration be-cause we optimize the topic selection and insertion uniform-ly, therefore we can find the global optimal solution for every iteration.

In Figure 5 we show a case study in which we compare the generated topic hierarchy for { NBA, players } of our method and that of the best baseline method, i.e., Zhu's Method . From the result, another advantage of our method is demonstrated: since the proposed method allows duplicat-ed topic nodes for a topic, it can provide every NBA player a subtopic career on the hierarchy correctly. However, since all the baseline methods only allow career to emerge once on the hierarchy, their performance on recall are low.
In this section, we evaluate the performance of the pro-posed method on representative document identification. The data set used in this experiment is generated as follows: First, for each focused topic hierarchy generated in Section 5.4, we selected 2 topic nodes, resulting in the 28 test topic nodes as shown in Table 4. Generally, the test nodes were carefully chosen so that they vary in many aspects, e.g., n-ode depth, node frequency and number of subtopic nodes.
Next, for each test topic node, its relevant documents were collected using the method as described in Section 4.5.1. Without loss of generality, we set N = 20, thus only the top 20 documents from each method are collect-ed as the relevant content set of the test topic nodes. To evaluate the performance of each method, we asked four undergraduate students to annotate each document in its results with a score as 5/2/0, indicating whether it is repre-sentative / relevant / irrelevant to the corresponding topic n-ode. For conflicts in the annotation results for a document, we adopted the lowest one as its final score. Finally, for all compared methods in the following sections, we use nDCG to measure their performance, which has a higher value when the documents of higher scores are ranked higher. le ns, problem, battery life, camera, wifi, camera, price, china, fran ce, shop, review, character, developer, player, story, actor, T able 4: List of the test topic nodes ordered by the positions of their corresponding information needs in Table 2.
 Fi gure 5: The comparison of the topic hierarchy gen-erated for f NBA, players g by Zhu X  X  method and our method.
In this Section, we first evaluate the impact of the topic hierarchy X  X  structure on representative document selection. In practice, we vary the parameter in Equation 10 from 0 to 1 and observe the trend of the average nDCG on all the test topic nodes. Intuitively, a larger would lead to larger impact of the structure information on the resultant relevant content sets.

The experimental results are presented in Figure 6. We can see that when = 0, where the structure information is not used, the nDCG is poor, indicating that the topic Fi gure 6: The nDCG performance of the proposed method for different . hierarchy X  X  structure is an essential factor for representa-tive contents identification. Besides, we also observe that the nDCG increases when increases and the maximum nDCG is achieved when  X  [0 : 1 ; 0 : 3], where we achieve a 31 : 1% improvement on nDCG @20 as compared to that when = 0. Based on this observation, we chose = 0 : 2 in our method.
In this section, to demonstrate the contributions of dif-ferent features in the proposed ranking model, we compare our full model ( T HiR ) with its four variations: (1) BM 25, which ranks the documents by their BM25 scores given on-ly the test topic node; (2) T HiR src , which simplifies the full model by setting p ( s | o ) in Equation 9 as 1, thus the source-based features are discarded; (3) T HiR sup , which enhances T HiR src by setting p ( s | o ) = p ( s ), i.e., adding the sources X  a priori probabilities; and (4) T HiR pro , which enhances T HiR src by setting p ( s | o ) = p ( o | s ), i.e., adding in the supportiveness of different sources.
 T able 5: The nDCG performance of different meth-ods in representative document selection.  X  indi-cates the improvement is significant against the first three methods in t-test, p-value &lt; 0.05.
 The performance of all methods are shown in Table 5. From the results we can see that the full model can achieve satisfying performance ( nDCG @ N &gt; 0 : 8) in identifying the representative contents for a given information need. More-over, the full model also outperforms all the other method-s, indicating that all the features introduced in this paper are useful to help finding the representative documents for topic nodes. Specifically, compared to BM 25, T HiR src gains 7 : 5% improvement on nDCG @20 since it consider-s the structure of the whole hierarchy. The T HiR sup and T HiR pro methods further improve the performance by 5 : 4% and 30 : 7% respectively on nDCG @20 over the T HiR src method by the introduction of the social media source-based features.
In this section we investigate the usefulness of the focused topic hierarchy on document retrieval task. By treating each information need, e.g., { iPhone 5, problem } as a keyword query, e.g.,  X  X Phone 5 problem X , the topic hierarchy can be used to improve its retrieval results on the social media cor-pus in two ways. First, if the hierarchy contains multiple nodes for the given subtopic, e.g., problem of camera and problem of battery , we can combine the representative doc-uments of them together to generate a more comprehen-sive search results. Second, if the corresponding node bears many subtopic nodes, e.g., diagnosis of problem and solution of problem , we can also use the subtopics X  representative documents to offer the user with more detailed information for the given query.

For the quantitative evaluation, we generate our test data set as follows: First, we aggregated together all the repre-sentative contents of topic nodes that meet the above two conditions for each test information need. Second, we ranked these contents again using the estimated probability and col-lected the top ranked 50 documents as the final search re-sults. It is noted that the annotation process here is similar to that described in Section 5.5.1, except that the annota-tors were provided with the corresponding information need, i.e., query, instead of a topic node on the topic hierarchy to represent the user X  X  search intent. In Table 6 we report the average nDCG performance of our method ( T HiR ) and a BM25 based document retrieval model ( BM 25), for which the information need is used as the input query.
 T able 6: The performance comparison of our method and a BM25 based method on query based document retrieval.

From the result we can see that the proposed method outperforms the BM25 based model by 23.1% on average nDCG . The reasons for the improvement are twofold. First, the relevant topics on the topic hierarchy can assist the re-trieval model with more information, hence enabling it to understand the query better and return more relevant con-tents. Second, the diversity of the retrieved documents is also increased. Take the information need { iPhone 5 prob-lem } as an example, compared to the search results of the B-M25 model which only include the contents about iPhone 5 X  X  major problems like battery problem, the results of T HiR method can also cover various problems of iPhone 5, includ-ing the minor ones such as the wifi and ios problems.
In this paper, we proposed a novel method for customized social media organization using focused topic hierarchies, in which the social media contents can be organized into differ-ent structures to meet with different users X  personal informa-tion needs. We developed rigorous methods to incorporate the user X  X  information need into the focused topic discovery and topic hierarchy construction process. To further assist users to search on the hierarchy, we developed a probability b ased model to obtain the representative contents for each topic node. The evaluation results demonstrated the effec-tiveness of our method for both information organization and retrieval tasks. In the future, we will try to enhance the present framework with data from knowledge bases and social networks. It is also interesting if we can apply it to more sophisticated tasks like question answering. This research is supported by the National Basic Research Program (973 Program) under grant No.2012CB316301 &amp; 2013CB329403, the National Science Foundation of China project under grant No.61332007 and No.61272227, the Ts-inghua University Initiative Scientific Research Program (with No.20121088071) and the Singapore National Research Foun-dation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office. [1] K. Bade and A. N  X  urnberger. Creating a cluster [2] D. M. Blei, T. L. Griffiths, M. I. Jordan, and J. B. [3] D. Chakrabarti, R. Kumar, and A. Tomkins.
 [4] S.-L. Chuang and L.-F. Chien. A practical web-based [5] M. Danilevsky, C. Wang, F. Tao, S. Nguyen, G. Chen, [6] I. Davidson and S. Ravi. Agglomerative hierarchical [7] J. Edmonds. Optimum branchings. Journal of [8] R. Fu, J. Guo, B. Qin, W. Che, H. Wang, and T. Liu. [9] B. C. Fung, K. Wang, and M. Ester. Hierarchical [10] X. Han and J. Zhao. Structural semantic relatedness: [11] J. He, V. Hollink, and A. de Vries. Combining implicit [12] A. C. K  X  onig and E. Brill. Reducing the human [13] X. Liu, Y. Song, S. Liu, and H. Wang. Automatic [14] O. Medelyan, S. Manion, J. Broekstra, A. Divoli, [15] D. Mimno, W. Li, and A. McCallum. Mixtures of [16] Z.-Y. Ming, K. Wang, and T.-S. Chua. Prototype [17] R. Navigli, P. Velardi, and S. Faralli. A graph-based [18] S. E. Robertson, S. Walker, S. Jones, M. M.
 [19] M. Sanderson and B. Croft. Deriving concept [20] U. Scaiella, P. Ferragina, A. Marino, and [21] R. Snow, D. Jurafsky, and A. Y. Ng. Semantic [22] C. Wang, M. Danilevsky, N. Desai, Y. Zhang, [23] J. Wang, C. Kang, Y. Chang, and J. Han. A [24] X. Wang, S. Liu, Y. Song, and B. Guo. Mining [25] H. Yang and J. Callan. A metric-based framework for [26] J. Yu, Z.-J. Zha, M. Wang, K. Wang, and T.-S. Chua. [27] H.-J. Zeng, Q.-C. He, Z. Chen, W.-Y. Ma, and J. Ma. [28] Y. Zhao and G. Karypis. Evaluation of hierarchical [29] X. Zhu, Z.-Y. Ming, X. Zhu, and T.-S. Chua. Topic
