 Recent sequential pattern mining methods have used the minimum description length (MDL) principle to define an encoding scheme which describes an algorithm for mining the most compressing patterns in a database. We present a novel subsequence interleaving model based on a probabilis-tic model of the sequence database, which allows us to search for the most compressing set of patterns without designing a specific encoding scheme. Our proposed algorithm is able to efficiently mine the most relevant sequential patterns and rank them using an associated measure of interestingness. The efficient inference in our model is a direct result of our use of a structural expectation-maximization framework, in which the expectation-step takes the form of a submodu-lar optimization problem subject to a coverage constraint. We show on both synthetic and real world datasets that our model mines a set of sequential patterns with low spurious-ness and redundancy, high interpretability and usefulness in real-world applications. Furthermore, we demonstrate that the quality of the patterns from our approach is comparable to, if not better than, existing state of the art sequential pattern mining algorithms.
Sequential data pose a challenge to exploratory data anal-ysis, as large data sets of sequences are difficult to visualise. In applications such as healthcare (patterns in patient paths [10]), click streams (web usage mining [18]), bioinformatics (predicting protein sequence function [27]) and source code (API call patterns [30]), a common approach has been se-quential pattern mining , to identify a set of patterns that commonly occur as subsequences of the sequences in the data.

A natural family of approaches for sequential pattern min-ing is to mine frequent subsequences [2] or closed frequent subsequences [26], but these suffer from the well-known prob-lem of pattern explosion, that is, the list of frequent subse-quences is typically long, highly redundant, and difficult to understand. Recently, researchers have introduced methods to prevent the problem of pattern explosion based on the minimum description length (MDL) principle [12, 25]. These methods define an encoding scheme which describes an al-gorithm for compressing a sequence database based on a library of subsequence patterns, and then search for a set of patterns that lead to the best compression of the database. These MDL methods provide a theoretically principled ap-proach that results in better patterns than frequent subse-quence mining, but their performance relies on designing a coding scheme.

In this paper, we introduce an alternate probabilistic per-spective on subsequence mining, in which we develop a gen-erative model of the database conditioned on the patterns. Then, following Shannon X  X  theorem, the length of the op-timal code for the database under the model is simply the negative logarithm of its probability. This allows us to search for the set of patterns that best compress the database with-out designing a specific coding scheme. Our approach, which we call the Interesting Sequence Miner (ISM) 1 , is a novel se-quential pattern mining algorithm that is able to efficiently mine the most relevant sequential patterns from a database and rank them using an associated measure of interesting-ness. ISM makes use of a novel probabilistic model of se-quences, based on generating a sequence by interleaving a group of subsequences. It is these learned component subse-quences that are the patterns ISM returns.

An approach based on probabilistic machine learning brings a variety of benefits, namely, that the probabilistic model al-lows us to declaratively incorporate ideas about what types of patterns would be most useful; that we can easily com-pose the ISM model with other types of probabilistic models from the literature; and that we are able to bring to bear powerful tools for inference and optimization from proba-bilistic machine learning. Inference in our model involves approximate optimization of a non-monotone submodular objective subject to a submodular coverage constraint. The necessary partition function is intractable to construct di-rectly, however we show that it can be efficiently computed using a suitable lower bound. The set of sequential patterns under our model can be inferred efficiently using a structural expectation maximization (EM) framework [8]. This is, to our knowledge, the first use of an expectation-maximization scheme for the subsequence mining problem.

On real-world datasets (Section 4), we find that ISM re-turns a notably more diverse set of patterns than the recent MDL methods SQS and GoKrimp (Table 2), while retain-https://github.com/mast-group/sequence-mining ing similar quality. A more diverse set of patterns is, we suggest, especially suitable for manual examination during exploratory data analysis. Qualitatively, the mined patterns from ISM are all highly correlated and extremely relevant, e.g. representing phrases such as oh dear or concepts such as reproducing kernel hilbert space . More broadly, this new per-spective has the potential to open up a wide variety of future directions for new modelling approaches, such as combining sequential pattern mining methods with hierarchical models, topic models, and nonparametric Bayesian methods.
Sequential pattern mining was first introduced by Agrawal and Srikant [2] in the context of market basket analysis, which led to a number of other algorithms for frequent sub-sequence, including GSP [23], PrefixSpan [22], SPADE [29], and SPAM [3]. Frequent sequence mining suffers from pat-tern explosion : a huge number of highly redundant frequent sequences are retrieved if the given minimum support thresh-old is too low. One way to address this is by mining frequent closed sequences, i.e., those that have no subsequences with the same frequency, such as via the BIDE algorithm [26]. However, even mining frequent closed sequences does not fully resolve the problem of pattern explosion. We refer the interested reader to Chapter 11 of [1] for a survey of frequent sequence mining algorithms.

In an attempt to tackle this problem, modern approaches to sequence mining have used the minimum description length (MDL) principle to find the set of sequences that best sum-marize the data. The GoKrimp algorithm [12] directly mines sequences that best compress a database using a MDL-based approach. The goal of GoKrimp is essentially to cover the database with as few sequences as possible, because the dictionary-based description length that is used by GoKrimp favours encoding schemes that cover more long and frequent subsequences in the database. In fact, finding the most com-pressing sequence in the database is strongly related to the maximum tiling problem, i.e., finding the tile with largest area in a binary transaction database.

SQS-Search (SQS) [25] also uses MDL to find the set of sequences that summarize the data best: a small set of in-formative sequences that achieve the best compression is mined directly from the database. SQS uses an encoding scheme that explicitly punishes gaps by assigning zero cost for encoding non-gaps and higher cost for encoding larger gaps between items in a pattern. While SQS can be very effective at mining informative patterns from text, it can-not handle interleaving patterns, unlike GoKrimp and ISM, which can be a significant drawback on certain datasets e.g. patterns generated by independent processes that may fre-quently overlap.

In related work, Mannila and Meek [15] proposed a gener-ative model of sequences which finds partial orders that de-scribe the ordering relationships between items in a sequence database. Sequences are generated by selecting a subset of items from a partial order with a learned inclusion probabil-ity and arranging them into a compatible random ordering. Unlike ISM, their model does not allow gaps in the gener-ated sequences and each sequence is only generated from a single partial order, an unrealistic assumption in practice.
There has also been some existing research on probabilis-tic models for sequences, especially using Markov models. Gwadera et al. [9] use a variable order Markov model to iden-tify statistically significant sequences. Stolcke and Omohun-dro [24] developed a structure learning algorithm for HMMs that learns both the number of states and the topology. Landwehr [13] extended HMMs to handle a fixed number of hidden processes whose outputs interleave to form a se-quence. Wood et al. developed the sequence memoizer [28], a variable order Markov model with a Pitman-Yor process prior. Also, Nevill-Manning and Witten [20] infer a context-free grammar over sequences using the Sequitur algorithm.
In this section we will formulate the problem of identifying a set of interesting sequences that are useful for explaining a sequence database. First we will define some preliminary concepts and notation. An item i is an element of a uni-verse U = { 1 , 2 ,...,n } that indexes symbols. A sequence S is simply an ordered list of items ( e 1 ,...,e m ) such that e  X  U  X  i . A sequence S a = ( a 1 ,...,a n ) is a subsequence of another sequence S b = ( b 1 ,...,b m ) , denoted S a  X  S there exist integers 1  X  i 1 &lt; i 2 &lt; ... &lt; i n  X  m such that a = b i 1 ,a 2 = b i 2 ,...,a n = b i n (i.e., the standard definition of a subsequence). A sequence database is merely a list of sequences X ( j ) . Further, we say that a sequence S is sup-ported by a sequence X in the sequence database if S  X  X . Note that in the above definition each sequence only con-tains a single item as this is the most important and pop-ular sequence type (cf. word sequences, protein sequences, click streams, etc.). 2 A multiset M is a generalization of a set that allows elements to occur multiple times, i.e., with a specific multiplicity # M (  X  ) . For example in the multiset M = { a,a,b } , the element a occurs twice and so has multi-plicity # M ( a ) = 2 .
Our aim in this work is to infer a set of interesting sub-sequences I from a database of sequences X (1) ,...,X ( N ) Here by interesting , we mean a set of patterns that are use-ful for helping a human analyst to understand the important properties of the database, that is, interesting subsequences should reflect the most important patterns in the data, while being sufficiently concise and non-redundant that they are suitable for manual examination. These criteria are inher-ently qualitative, reflecting the fact that the goal of data mining is to build human insight and understanding. To quantify these criteria, we operationalize the notion of inter-esting sequence as those sequences that best explain the un-derlying database under a probabilistic model of sequences. Specifically we will use a generative model, i.e., a model that starts with a set of interesting subsequences I and from this set generates the sequence database X (1) ,...,X ( N ) . Our goal is then to infer the most likely generating set I un-der our chosen generative model. We want a model that is as simple as possible yet powerful enough to capture cor-relations between items in sequences. A simple such model is as follows: iteratively sample subsequences S from I and randomly interleave them to form the database sequence X . If we associate each subsequence S  X  I with a probability  X  , we can sample the indicator variable z S  X  Bernoulli (  X 
Note that we can easily extend our algorithm to mine se-quences of sets of items (as defined in the original sequence mining paper [2]) by extending the subsequence operator  X  to handle these more general  X  X equences X . and include it in X if z S = 1 . However, we may wish to in-clude a subsequence more than once in the sequence X , that is, we need some way of sampling the multiplicity of S in X . The simplest way to do this is to change our generating dis-tribution from Bernoulli to e.g. Categorical and sample the multiplicity z S  X  Categorical (  X  S ) where  X  S is now a vec-tor of probabilities, with one entry for each multiplicity (up to the maximum in the database). We define the generative model formally in the next section.
As discussed in the previous section, we propose a simple directed graphical model for generating a database of se-quences X (1) ,...,X ( N ) from a set I of interesting sequences. The generative story for our model is, independently for each sequence X in the database: 1. For each interesting sequence S  X  I , decide indepen-dently the number of times S should be included in X , i.e., sample the multiplicity z S  X  N 0 as where  X  S is a vector of multiplicity probabilities. For clar-ity we present the Categorical distribution here but one could use a more general distribution if desired. 2. Set S to be the multiset with multiplicities z S of all the sequences S selected for inclusion in X : 3. Set P to be the set of all possible sequences that can be generated by interleaving together all occurrences of the sequences in the multiset S , i.e.,
Here by interleaving we mean the placing of items from one sequence into the gaps between items in another whilst maintaining the orders of the items imposed by each sequence. 4. Sample X uniformly from P , i.e., Note that we never need to construct the set P in practice, since we only require its cardinality during inference, and we show in the next section how we can efficiently compute an approximation to |P| . We can, however, sample from P efficiently by merging subsequences S  X  S into X one at a time as follows: splice the elements of S , in order, into X at randomly chosen points (here by splicing S into X we mean the placing of items from S into the gaps between items in X ). For example, S = { (1 , 2) , (3 , 4) } will generate the set of tion distribution between subsequences in our model, but we choose not to do so because we want to force the model to use I to explain the sequential dependencies in the data.
Given a set of interesting sequences I , let z denote the vector of z S for all sequences S  X  I and similarly, let  X  denote the list of  X  S for all S  X  X  . Assuming z ,  X  are fully determined, it is evident from the generative model that the probability of generating a database sequence X is where |  X  S | is the length of  X  S and [ z S = m ] evaluates to 1 if z S = m , 0 otherwise. Intuitively, it helps to think of each  X  S as being an infinite vector and each S  X  X  as being augmented with a Kleene star operator, so that, for exam-ple, one can use (1 , 2)  X  and (3)  X  to generate the sequence (1 , 2 , 1 , 3 , 2) .

Calculating the normalization constant |P| is problem-atic as we have to count the number of possible distinct sequences that could be generated by interleaving together subsequences in S . This is further complicated by the fact that S is a multiset and so can contain multiple occurrences of the same subsequence, which makes efficient computa-tion of |P| impractical. However, it turns out that we can compute a straightforward upper bound since |P| is clearly bounded above by all possible permutations of all the items in all the subsequences S  X  S , and this bound is attained when S contains only distinct singleton sequences without repetition. Formally, Conveniently, this gives us a non-trivial lower-bound on the posterior p ( X, z |  X  ) which, as we will want to maximize the posterior, is precisely what we want. Moreover, the lower bound acts as an additional penalty, strongly favouring a non-redundant set of sequences (see Section 4.2).
Now assuming the parameters  X  are known, we can infer z for a database sequence X by maximizing the log of the lower bound on the posterior p ( X, z |  X  ) over z : This is an NP-hard problem in general and so impractical to solve directly in practice. However, we will show that it can be viewed as a special case of maximizing a submodular function subject to a submodular constraint and so approx-imately solved using the greedy algorithm for submodular function optimization. Now strictly speaking the notion of a submodular function is only applicable to sets, however we will consider the following generalization to multisets:
Definition 1. (Submodular Multiset Function) Let  X  be a finite multiset and let N 0  X  denote the set of all possible multisets that are subsets of  X  , then a function f : N 0 is submodular if for for every C  X  D  X   X  and S  X   X  with #
C ( S ) = # D ( S ) it holds that Let us now define a function f for our specific case: let T be the multiset of supported interesting sequences, i.e., sequences S  X  I s.t. S  X  X with multiplicity given by the maximum number of occurrences of S in any partition of X . Now, define f : N 0 T  X  R as f ( C ) : = X and g ( C ) : = | X  S  X  X  S | . We can now re-state (3.1) as: Find a non-overlapping multiset covering C  X  T that maximizes f ( C ) , i.e., such that g ( C ) = g ( T ) and f ( C ) is maximized. Note that g ( T ) = | X | by construction. Now clearly g is monotone submodular as it is a multiset coverage function, and we will show that f is non-monotone submodular. To see that f is submodular observe that for C  X  D , # C ( S ) = #
D ( S ) which is precisely Definition 1. To see that f is non-monotone observe that f ( C X  X  S } )  X  f ( C ) = ln whose sign is indeterminate.

Maximizing the posterior (3.1) is therefore a problem of maximizing a submodular function subject to a submodular coverage constraint and can be approximately solved by ap-plying the greedy approximation algorithm (Algorithm 1). The greedy algorithm builds a multiset covering C by re-peatedly choosing a sequence S that maximizes the profit f ( C  X  X  S } )  X  f ( C ) of adding S to the covering divided by the number of items in S not yet covered by the covering g ( C X  X  S } )  X  g ( C ) = | S | . In order to minimize CPU time spent solving the problem, we cache the sequences and coverings for each database sequence as needed.
 Algorithm 1 Greedy Algorithm Input: Database sequence X , supported sequences T
Initialize multiset C  X  X  X  while g ( C ) 6 = | X | do end while return C
Note that while there are good theoretical guarantees on the approximation ratio achieved by the greedy algorithm when maximizing a monotone submodular set function sub-ject to a coverage constraint (e.g. ln | X | + 1 for weighted set cover [5, 7]) the problem of maximizing a non-monotone submodular set function subject to a coverage constraint has, to the best of our knowledge, not been studied in the literature. However, as our submodular optimization prob-lem is an extension of the weighted set cover problem, the greedy algorithm is a natural fit and indeed we observe good performance in practice.
 Algorithm 2 Hard-EM Input: Set of sequences I and initial estimates  X  (0) k  X  0 do
Remove from I sequences S with  X  S 0 = 1 return I ,  X  ( k )
Given a set of interesting sequences I , consider now the case where both variables z ,  X  in the model are unknown. In this case we can use the hard EM algorithm [6] for parame-ter estimation with latent variables. The hard-EM algorithm in our case is merely a simple layer on top of the inference algorithm (3.1). Suppose there are N database sequences X (1) ,...,X ( N ) with multisets of supported interesting se-quences T (1) ,..., T ( N ) , then the hard EM algorithm is given in Algorithm 2 (note that k X k F denotes the Frobenius norm and  X  S 0 is the probability that S does not explain any se-quence in the database). To initialize  X  , a natural choice is simply the support (relative frequency) of each sequence.
We infer new sequences using structural EM [8], i.e., we add a candidate sequence S 0 to I if doing so improves the optimal value p of the problem (3.1) averaged across all database sequences. Interestingly, there are two implicit reg-ularization effects here. Firstly, observe from (3.1) that when a new candidate S 0 is added to the model, a corresponding term ln  X  S 0 quences that S 0 does not support. For large sequence databases, this amounts to a significant penalty on candidates in prac-tice. Secondly, observe that the last term of (3.1) acts as an additional penalty, strongly favouring a non-redundant set of sequences.

To get an estimate of maximum benefit to including can-didate S 0 , we must carefully choose an initial value of  X  that is not too low, to avoid getting stuck in a local opti-mum. To infer a good  X  S 0 , we force the candidate S 0 to explain all database sequences it supports by initializing  X  0 = (0 , 1 ,..., 1) T and update  X  S 0 with the probability corresponding to its actual usage once we have inferred all the coverings. Given a set of interesting sequences I and cor-responding probabilities  X  along with database sequences X (1) ,...,X ( N ) , each iteration of the structural EM algo-rithm is given in Algorithm 3 below.

Occasionally the Hard-EM algorithm may assign zero probability to one or more singleton sequences and cause the greedy algorithm to not be able to fully cover a database sequence X using just the interesting sequences in I . In this case we simply re-seed I with the necessary singletons. Fi-nally, in practice we store the set of candidates that have been rejected by Structural-EM and check each poten-tial candidate against this set for efficiency.
The Structural-EM algorithm (Algorithm 3) requires a Algorithm 3 Structural-EM (one iteration) Input: Sequences I ,  X  , optima p ( i ) of (3.1)  X  X ( i ) do while p 0  X  p {until one good candidate found}
I  X  X  X  X  S 0 } return I ,  X  0 method to generate new candidate sequences S 0 that are to be considered for inclusion in the set of interesting sequences I . One possibility would be to use the GSP algorithm [23] to recursively suggest larger sequences starting from single-tons, however preliminary experiments found this was not the most efficient method. For this reason we take a slightly different approach and recursively combine the interesting sequences in I with the highest support first (Algorithm 4). In this way our candidate generation algorithm is more likely to propose viable candidate sequences earlier and in practice we find that this heuristic works well.
 Algorithm 4 Candidate-Gen Input: Sequences I , cached supports  X  , queue length q if @ priority queue Q for I then end if
Pull highest-ranked candidate S 0 from Q return S 0
Our complete interesting sequence mining (ISM) algo-rithm is given in Algorithm 5. Note that the Hard-EM Algorithm 5 ISM (Interesting Sequence Miner) Input: Database of sequences X (1) ,...,X ( N )
Initialize I with singletons,  X  with their supports while not converged do end while return I ,  X  parameter optimization step need not be performed at ev-ery iteration, in fact it is more efficient to suggest several candidate sequences before optimizing the parameters. As all operations on database sequences in our algorithm are trivially parallelizable, we perform the E and M -steps in both the hard and structural EM algorithms in parallel.
Now that we have inferred the model variables z ,  X  , we are able to use them to rank the retrieved sequences in I . There are two natural rankings one can employ, and both have their strengths and weaknesses. The obvious approach is to rank each sequence S  X  I according to its probabil-ity under the model  X  S , however this has the disadvantage of strongly favouring frequent sequences over rare ones, an issue we would like to avoid. An alternative is to rank the retrieved sequences according to their interestingness under the model, that is the ratio of database sequences they ex-plain to database sequences they support. One can think of interestingness as a measure of how necessary the sequence is to the model: the higher the interestingness, the more sup-ported database sequences the sequence explains. Thus in-terestingness provides a more balanced measure than prob-ability, at the expense of missing some frequent sequences that only explain some of the database sequences they sup-port. We define interestingness formally as follows.
Definition 2. The interestingness of a sequence S  X  X  re-trieved by ISM (Algorithm 5) is defined as and ranges from 0 (least interesting) to 1 (most interesting). Any ties in the ranking can be broken using the sequence probability p ( S  X  X ) = p ( z S  X  1) = 1  X   X  S 0 .
There is a close and well-known connection between prob-abilistic modelling and the minimum description length prin-ciple used by SQS and GoKrimp (see MacKay [14],  X 28.3 for a particularly nice explanation). Given a probabilistic model p ( X |  X  , I ) of a single database sequence X , by Shan-non X  X  theorem the optimal code for the model will encode X using approximately  X  log 2 p ( X |  X  , I ) bits. So by find-ing a set of patterns that maximizes the probability of the data, we are also finding patterns that minimize description length. Conversely, any encoding scheme implicitly defines a probabilistic model. Given an encoding scheme E that as-signs each transaction X to a string of L ( X ) bits, we can define p ( X | E )  X  2  X  L ( X ) , and then E is an optimal code for p ( X | E ) . Interpreting the previous subsequence mining methods in terms of their implicit probabilistic models pro-vides interesting insights into these methods.

The encoding of a database sequence used by SQS can be interpreted as a probabilistic model p ( X, z |  X  , I ) , where the SQS analog of p ( X, z |  X  , I ) is similar to (3.1) with along with additional terms that correspond to the descrip-tion lengths for indicating the presence and absence of gaps in the usage of a sequence S . Additionally, SQS contains an explicit penalty for the encoding of the set of patterns I , that encourages a smaller number of patterns. In a proba-bilistic model, this can be interpreted as a prior distribution p ( I ) over patterns. There is also a prior distribution on the content of the patterns, similar to a unigram model, which encourages the patterns to contain more common elements.
Similarly, GoKrimp uses a variant of the above model, where instead we have In addition, the description length used by GoKrimp also has a gap cost that penalizes sequences with large gaps. GoKrimp employs a greedy heuristic to find the most com-pressing sequence: an empty sequence S is iteratively ex-tended by the most frequent item that is statistically depen-dent on S . ISM, by contrast, iteratively extends sequences by the most frequent sequence in its candidate generation step which enables it to quickly generate large candidate sequences (Section 3.6). We did consider performing a sta-tistical test between a sequence and its extending sequence, however this proved computationally prohibitive.

The differences between these models and ISM are:  X  Interleaving. SQS cannot mine subsequences that are in-terleaved and thus struggles on datasets which consist mainly of interleaved subsequences (for illustration, see
Section 4.4). GoKrimp handles interleaving using a pointer scheme that explicitly encodes the location of the subse-quence within the database. In ISM, the partition function |P| allows us to handle interleaving of subsequences with-out needing to explicitly encode positions, and also serves as an additional penalty on the number of elements in the subsequences used to explain a database sequence.  X  Gap penalties. Both SQS and GoKrimp explicitly pun-ish gaps in sequential patterns. Adding such a penalty would require only a trivial modification to the algorithm, namely, updating the cost function in Algorithm 1. We did not pursue this as we observe excellent results without it (Section 4).  X  Encoding the set of patterns. Both SQS and GoKrimp con-tain an explicit penalty term for the description length of the pattern database, which corresponds to a prior distri-bution p ( I ) over patterns. In our experiments with ISM, we did not find in practice that an explicit prior distri-bution p ( I ) was necessary for good results. It would be possible to incorporate it with a trivial change to the ISM algorithm, in particular, when computing the score im-provement of a new candidate in the structural EM step.  X  Encoding pattern absence. Also, observe that, if we view
ISM as an MDL-type method, not only the presence of a pattern, but also the absence of it is explicitly encoded (in the form of  X  S 0 in (3.1)). As a result, there is an implicit penalty for adding too many patterns to the model and one does not need to use a code table which would serve as an explicit penalty for greater model complexity.
In this section we perform a comprehensive quantitative and qualitative evaluation of ISM. On synthetic datasets we show that ISM returns a list of sequential patterns that is largely non-redundant, contains few spurious correlations and scales linearly with the number of sequences in the dataset. On a set of real-world datasets we show that ISM Figure 1: ISM scaling as the number of sequences in our synthetic database increases. finds patterns that are consistent, interpretable and highly relevant to the problem at hand. Moreover, we show that ISM is able to mine patterns that achieve good accuracy when used as binary features for real-world classification tasks.
 Datasets We use ten real-world datasets in our numer-ical evaluation (see Table 1). The Alice dataset consists of the text of Lewis Carrol X  X  Alice in Wonderland, tokenized into 1 , 638 sentences using the Stanford Document Prepro-cessor [17] with stop words and punctuation deliberately retained. The Gazelle dataset consists of 59 , 601 sequences of clickstream data from an e-commerce website used in the KDD-CUP 2000 competition [11]. The JMLR dataset con-sists of 788 abstracts from the Journal of Machine Learning Research and has previously been used in the evaluation of the SQS and GoKrimp algorithms [12, 25]. Each sequence is a list of stemmed words from the text with stop words re-moved. The Sign dataset is a list of 730 American sign lan-guage utterances where each utterance contains a number of gestural and grammatical fields [21]. The last six datasets listed in Table 1 were first introduced in [19] to evaluate classification accuracy when mined sequential patterns are used as features. The datasets were converted from time in-terval sequences into sequences of items by considering the start and end of each unique interval as distinct items and ordering the items according to time.
 ISM Results We ran ISM on each dataset for 1 , 000 itera-tions with a priority queue size of 100 , 000 candidates. The runtime and number of non-singleton sequential patterns re-turned by ISM is given in the right-hand side of Table 1. We Table 1: Summary of the real datasets used and ISM results after 1 , 000 iterations.  X  excluding singleton subsequences. Figure 2: Precision against recall for each algorithm on our synthetic database, using the top-k patterns as a threshold. Note that SQS is a single point at the top-left and GoKrimp has near zero precision and recall. Each plotted curve is the 11-point interpolated precision 3 . also investigated the scaling of ISM as the number of se-quences in the database increases, using the model trained on the Sign dataset from Section 4.1 to generate synthetic sequence databases of various sizes. We ran ISM for 100 it-erations on these databases and one can see in Figure 1 that the scaling is linear as expected. All experiments were per-formed on a machine with 16 Intel Xeon E5-2650 2 . 60 Ghz CPUs and 128GB of RAM.
 Evaluation criteria We will evaluate ISM along with SQS, GoKrimp and BIDE according to the following criteria: 1. Spuriousness  X  to assess the degree of spurious correlation in the mined set of sequential patterns. 2. Redundancy  X  to measure how redundant the mined set of patterns is. 3. Classification Accuracy  X  to measure the usefulness of the mined patterns. 4. Interpretability  X  to informally assess how meaningful and relevant the mined patterns actually are.
The sequence-cover formulation of the ISM algorithm (3.1) naturally favours adding sequences to the model whose items co-occur in the sequence database. One would therefore ex-pect ISM to largely avoid suggesting sequences of uncor-related items and so return more meaningful patterns. To verify this is the case and validate our inference procedure, we check if ISM is able to recover the sequences it used to generate a synthetic database. To obtain a realistic synthetic database, we sampled 10 , 000 sequences from the ISM gen-erative model trained on the Sign dataset (cf. Section 3.2). We were then able to measure the precision and recall for each algorithm, i.e., the fraction of mined patterns that are generating and the fraction of generating patterns that are mined, respectively. Figure 2 shows the precision-recall curve for ISM, SQS, GoKrimp and BIDE using the top-k mined sequences (according to each algorithms ranking) as a threshold. One can clearly see that ISM was able to mine al-most all the generating patterns and almost all the patterns mined were generating, despite the fact that the generated database will contain many subsequences not present in the i.e., the interpolated precision at 11 equally spaced recall points between 0 and 1 (inclusive), see [16],  X 8.4 for details. original dataset due to the nature of our  X  X ubsequence inter-leaving X  generative model. This not only provides a good val-idation of ISM X  X  inference procedure and underlying genera-tive model but also demonstrates that ISM returns few spu-rious patterns. For comparison, SQS returned a very small set of generating patterns and GoKrimp returned many pat-terns that were not generating. The set of top-k patterns mined by BIDE contained successively less generating pat-terns as k increased. It is not our intention to draw con-clusions about the performance of the other algorithms as this experimental setup naturally favours ISM. Instead, we compare the patterns from ISM with those from SQS and GoKrimp on real-world data in the next sections.
We now turn our attention to evaluating how redundant the sets of sequential patterns returned by ISM, SQS, GoKrimp and BIDE actually are. A suitable measure of redundancy for a single sequence is the minimum edit distance between it and the other mined sequences in the set. Averaging this distance across all sequences in the set, we obtain the av-erage inter-sequence distance (ISD). Similarly, we can also calculate the average number of sequences containing other mined sequences in the set (CS), which provides us with an-other measure of redundancy. Finally, we can also look at the number of unique items present in the set of mined se-quences which gives us an indication of how diverse it is. We ran ISM, SQS, GoKrimp and BIDE on all the datasets in Table 1 and report the results of the three aforementioned redundancy metrics on the top 50 non-singleton sequential patterns for each algorithm in Table 2. One can see that on average the top ISM sequences have a larger inter-sequence distance, smaller number of containing sequences and larger number of unique items, clearly demonstrating they are less redundant than SQS, GoKrimp and BIDE. Predictably, the top BIDE sequences are the most redundant, with an aver-age inter-sequence distance of 1 . 00 .
A key property of any set of patterns mined from data is its usefulness in real-world applications. To this end, in keeping with previous work [12], we will focus on classifi-cation tasks as they are some of most important applica-tions of pattern mining algorithms. Specifically we will con-sider the task of classifying sequences in a database using mined sequential patterns as binary features. We therefore performed 10 -fold cross validation using a Support Vector Machine (SVM) classifier on the six classification datasets from Table 1 with the top-k patterns mined by ISM, SQS, GoKrimp and BIDE as features. We used the linear clas-sifier from the libSVM library [4] with default parameters. Additionally, we used the top-k most frequent singleton pat-terns as a baseline for the classification tasks. The resulting plots of k against classification accuracy for all the datasets and algorithms are given in Figure 3. One can see that the patterns mined by SQS perform best, exhibiting the highest classification accuracy on four out of the six datasets, closely followed by ISM and GoKrimp, which performs surprisingly well considering it struggles to return more than 50 patterns. All three consistently outperform BIDE and the singletons baseline which exhibit similar performance to each other. We therefore conclude that the sequential patterns mined by ISM can indeed be useful in real-world applications.
For the two text datasets in Table 1 we can directly in-terpret the mined patterns and informally assess how mean-ingful and relevant they are.
 JMLR Dataset We compare the top-20 non-singleton pat-terns mined by ISM, SQS, GoKrimp and BIDE in Table 3. It is immediately obvious from the table that the BIDE pat-terns are almost exclusively permutations of frequent items and so uninformative. For this reason we omit BIDE from consideration on the next dataset. The patterns mined by ISM, SQS and GoKrimp are all very informative, contain-ing technical concepts such as support vector machine and commonly used phrases such as state (of the) art . Alice Dataset We compare the top twenty-20 non-singleton patterns mined by ISM, SQS and GoKrimp in the first three columns Table 4. This time, one can clearly see that the patterns mined by ISM are considerably more informative. They contain collocated words and phrases such as mock turtle and oh dear , correlated words such as as spoke and off head , as well as correlated punctuation such as ( ) and  X   X  . Both SQS and GoKrimp on the other hand mine collo-cated words with spurious punctuation and stop words, e.g. prepending the to nouns and commas to phrases. To further illustrate this notable difference, we also show the top-20 non-singleton patterns that are exclusive to each algorithm (i.e., found by ISM but not SQS/GoKrimp, etc.) in the last three columns of Table 4. One can clearly see that GoKrimp has the least informative exclusive patterns, predominantly combinations of stop words and punctuation, SQS mostly prepends and appends informative exclusive patterns with punctuation and stop words, whereas ISM is the only algo-rithm that just returns purely correlated words. Note that SQS in particular struggles to return patterns such as bal-anced parentheses, since it punishes the large gaps between them and cannot handle interleaving them with the patterns they enclose. Here we can really see the power of the statis-tical model underlying ISM as it is able to discern spurious punctuation from genuine phrases.
 Parallel Dataset Finally, we consider a synthetic dataset that demonstrates the ability of ISM to handle interleav-ing patterns. Following [12], we generate a synthetic dataset where each item in the sequence is generated by five inde-pendent parallel processes, i.e., each process i generates one item from a set of five possible items { a i ,b i ,c i ,d shows consistently good performance, comparable to SQS and GoKrimp. she herself !  X   X   X  she herself ?  X   X   X  mock turtle , and , and ( ) the mock turtle , said . looked at ,  X  said alice i n X  X   X  she at once the white rabbit what ? had been the queen i  X  X  oh dear ,  X  you know looking at the white rabbit !  X  if  X  X  i  X  X e oh , ! had back ,  X  alice . such thing minute or two i  X  just when ; and to herself  X  X e seen there was alice ; she at once i  X  X  what ? going into  X   X  the . more than she had the queen soon found soo  X  oop of the e  X  e  X  it : Figure 4: Recall for each algorithm on the synthetic par-allel dataset, using the top-k (first-k for SQS) patterns as a threshold. Note that SQS maintains a recall level of 0 . 6 for the remaining patterns (up to k = 403 , not shown for clarity). der. In each step, the generator chooses i at random and generates an item using process i , until the sequence has length 1 , 000 , 000 . The sequence is then split into 10 , 000 sequences of length 100 . For this dataset we know that all mined sequences containing a mixture of items from different processes are spurious. This enables us to calculate recall, i.e., the fraction of processes present in the set of true pat-terns mined by each algorithm. We plot the recall for the top-k patterns mined by ISM and GoKrimp in Figure 4 and the first-k patterns mined by SQS (as it was still running after seven days). One can see that while ISM and GoKrimp are able to mine true patterns from all processes, SQS only returns patterns from 3 of the 5 processes.
In this paper, we have taken a probabilistic machine learn-ing approach to the subsequence mining problem. We pre-sented a novel subsequence interleaving model, called the Interesting Sequence Miner, that infers subsequences which best compress a sequence database without having to de-sign a MDL encoding scheme. We demonstrated the efficacy of our approach on both synthetic and real-world datasets, showing that ISM returns a more diverse set of patterns than previous approaches while retaining comparable qual-ity. In the future we would like to extend our approach to the many promising application areas as well as considering more advanced techniques for parallelization.
 This work was supported by the Engineering and Physical Sciences Research Council (grant number EP/K024043/1).
