 This paper addresses the problem of learning to classify texts by exploiting information derived from clustering both train-ing and testing sets. The incorporation of knowledge result-ing from clustering into the feature space representation of the texts is expected to boost the performance of a classi-fier. Experiments conducted on several widely used datasets demonstrate the effectiveness of the proposed algorithm es-pecially for small training sets.
 I.2.6 [ Artificial Intelligence ]: Learning; H.3.3 [ Information Search and Retrieval ]: Clustering Algorithms Text classification, clustering
Text classification is one of the first applications of ma-chine learning and applies to the general problem of super-vised inductive learning: given a set of training documents, classified manually to one or more predefined categories, the classifier learns to automatically classify new unseen docu-ments, about which there is no prior knowledge. To boost its performance, we seek for new information about the dis-tribution of the documents to be classified. This information comes from clustering both training and testing sets and is embodied in the data in the form of meta -information.
Clustering has been used in the literature of text classi-fication either as an approach for dimensionality reduction or as a technique to enhance the training set. In the sec-ond case, the enhancement is achieved either by extending the feature vectors of the training examples with new fea-tures derived from clustering or by expanding the training set with new examples derived from a large set of unlabeled data (see [1, 5, 7] for an example of each case).
In this article, a new algorithm combining clustering and classification is proposed. Our motivation relies on the ex-pectation that any prior knowledge about the nature of the and Reuters-52 subsets 1 of Reuters-21578 2 , and WebKB 3 . For WebKB the settings in [2] are followed. For the rest of the corpora no feature selection is done, both stemming and stopword-removal are used.

A binary classifier is constructed for each class of the ex-panded dataset, a linear kernel is selected and the weight C of the slack variables is set to default.
 A four-fold cross validation is applied to each experiment. The algorithm runs four rounds with different samples from the training set. These samples are selected randomly and uniformly by dividing the training set in smaller and equal parts and by preserving the same proportion of documents per category with that of the original dataset. All testing documents are used. The precision/recall breakeven point (BEP) is used as a measure of performance.
Several experiments are conducted with one or more meta -features added on the expansion step. To provide a base-line for comparison, results from the standard SVM clas-sifier without the clustering and expansion steps are also presented.

Table 1 shows the results of the experiments. The SVM combined with clustering (CL-SVM) constantly outperforms the standard SVM on all datasets raising the average of the breakeven points up to 15% in some cases. For the sake of brevity, the results from the standard TSVM are omit-ted. However, it should be noted that the TSVM combined with clustering has a similar behaviour also leading to an improved performance compared to the standard TSVM.
In figure 1, we demonstrate the impact of the size of the training set for the 20Newsgroup dataset. This graph shows that the advantage of using clustering as a former step to classification is largest in the case of small training sets. When the size of the training set increases, the performance of the CL-SVM approaches that of the standard SVM. The rest of the collections have a similar behaviour.
This algorithm stood out in a spam-filtering task [4] in the challenge competition of the ECML 2006 Conference.
To conclude, the use of clustering to boost classification seems to be a promising approach with several extensions that should be further investigated. Such an extension, cur-rently under examination, is the case where each cluster
Available at http://www.gia.ist.utl.pt/~acardoso/ datasets/ .
Available at http://www.daviddlewis.com/resources/ testcollections/reuters21578/ .
Available at http://www.cs.cmu.edu/afs/cs.cmu.edu/ project/theo-20/www/data/ .
