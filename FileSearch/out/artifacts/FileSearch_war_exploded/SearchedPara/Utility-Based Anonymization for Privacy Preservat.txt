 Privacy becomes a more and more serious concern in applicati ons involving microdata. Recently, efficient anonymization ha s attracted much research work. Most of the previous methods use global r e-coding, which maps the domains of the quasi-identifier attri butes to generalized or changed values. However, global recoding ma y not always achieve effective anonymization in terms of discern ability and query answering accuracy using the anonymized data. Mor e-over, anonymized data is often used for analysis. As well acc epted in many analytical applications, different attributes in a data set may have different utility in the analysis. The utility of at tributes has not been considered in the previous methods.
 In this paper, we study the problem of utility-based anonymiza-tion . First, we propose a simple framework to specify utility of a t-tributes. The framework covers both numeric and categorica l data. Second, we develop two simple yet efficient heuristic local r ecod-ing methods for utility-based anonymization. Our extensiv e perfor-mance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidim ensional global recoding methods in both discernability and query an swer-ing accuracy. Furthermore, our utility-based method can bo ost the quality of analysis using the anonymized data.
 H.2.8 [ Database Applications ]: Data Mining Security, Algorithms, Performance Privacy preservation, data mining, k-anonymity, utility, local re-coding Recently, privacy becomes a more and more serious concern in ap-plications involving microdata , which refers to data published in its raw, non-aggregated form [17]. One important type of pri vacy attack is re-identifying individuals by joining multiple p ublic data sources. For example, according to [15], more than 85% population of the United States can be uniquely identified us ing their zipcode, gender, and date of birth.
 To protect privacy against this type of attacks, k-anonymit y was proposed [12; 15]. A data set is k -anonymous ( k  X  1) record in the data set is indistinguishable from at least other records within the same data set. The larger the value o f the better the privacy is protected.
 Since the concept of k-anonymity has been proposed, efficien t meth-ods for anonymization has attracted much research work. A fe w k-anonymization algorithms have been developed. We shall r eview the related work briefly in Section 2.2. Generally, to achiev e k-anonymity, those methods generalize or suppress the quasi-identifier attributes , which are the minimal set of attributes in the table that can be joined with external information to re-identify indi vidual records.
 Information loss is an unfortunate consequence of anonymiz ation. In order to make the anonymized data as useful as possible, it is required to reduce the information loss as much as possibl e. A few models have been proposed to measure the usefulness of anonymized data. For example, the discernability model [4] tries to minimize the number of tuples that are indistinguishable , as long as they satisfy the k-anonymity requirement.
 In this paper, we study the problem of k-anonymization and fo cus on two interesting issues: anonymization using heuristic local re-coding and utility-based anonymization . Many recent methods (e.g., [4; 8; 9]) use global recoding , which maps the domains of the quasi-identifier attributes to gener alized or changed values. In other words, the data space is partitione d into a set of (non-overlapping) regions. The anonymization maps all tuples in a region to the same generalized or changed tuple. F or ex-ample, Figures 1(b) demonstrates a 3 -anonymization using global recoding for the table in Figures 1(a), where (age, zipcode) is the quasi-identifier. Tuples R 3 an R 4 in Figures 1(a) are identical. They are mapped to the same generalized tuple in global recod ing. In contrast, local recoding maps (non-distinct) individual tuple to generalized tuples. For example, Figure 1(c) shows a 3 -anonymiza-tion using local recoding of the same table in Figures 1(a). T he two identical tuples, R 3 and R 4 , are mapped to different generalized tuples in local recoding. Clearly, global recoding can be re garded as a specific type of local recoding.
 Interestingly, from Figure 1, we can observe that local recoding may achieve a less information loss than global recoding . In our example, the two generalized tuples in global recoding have the sizes of intervals 8 and 5 in age, and 1 and 0 in zipcode, respec-tively. In local recoding, the sizes of intervals are 6 and and 1 and 2 in zipcode, respectively. By intuition, smaller the sizes 53711 53711 53711 not part of the quasi-identifier. of intervals in the generalized tuples, less information lo ss in the anonymization.
 Can we use local recoding to achieve less information loss in anony-mization effectively? Generally, optimal k-anonymity is NP-hard [10; 2]. In this paper, we propose two simple yet efficien t heuristic algorithms using local recoding for k-anonymiza tion. Our extensive empirical study on both real data sets and synthet ic data sets show that our method outperforms the state-of-the-art global recoding method in both the discernability and the accuracy of query answering. Anonymized data is often for analysis and data mining. As wel l recognized in many data analysis applications, different a ttributes may have different utility. For example, consider anonymiz ing a data set about patients for disease analysis. Suppose in ord er to achieve k-anonymity, we can generalize from a five-digit ful l zip-code to a four-digit prefix (e.g., from 53712 to 5371  X  ). Alterna-tively, we can also generalize attribute age to age groups (e .g., from 23 to [20 , 30] ). In many cases, the age information is critical to dis-ease analysis, while the information loss on the accurate lo cation is often acceptable (a four digit prefix in fact still identifies a relatively local region). Thus, the age attribute has more utility than the zip-code attribute, and should be retained as accurately as poss ible in anonymization.
 Can we make the anonymization utility aware? Utility of attributes has not been considered by previous anonymization methods. In this paper, we propose a model for utility-based anonymization . We consider both numeric data and categorical data with and wit hout hierarchies. We present a simple method to specify utility o f at-tributes and push them into the heuristic local recoding ano nymiza-tion methods. Our experimental results show that the utilit y-based anonymization improves the accuracy in answering targeted queries substantially.
 The rest of the paper is organized as follows. In section 2, we recall the notions related to anonymization, and review the relate d work. We present our utility specification framework in Section 3. Our heuristic local recoding methods are developed in Section 4 . An extensive performance study on both real data sets and synth etic data sets is reported in Section 5. The paper is concluded in S ec-tion 6. Consider a table T = ( A 1 , . . . , A n ) . A quasi-identifier is a mini-mal set of attributes ( A i T that can be joined with external information to re-identify indi-vidual records. In this paper, we assume that the quasi-iden tifier is specified by the administrator based on the background knowl edge. Thus, we focus on how to anonymize T to satisfy the k-anonymity requirement.
 Formally, given a parameter k and the quasi-identifier ( A A there exist at least another ( k  X  1) tuples t 1 , . . . , t those k tuples have the same projection on the quasi-identifier, i.e ., t and all other tuples indistinguishable from t on the quasi-identifier form an equivalence class . We call the class the group that generalized.
 Given a table T with the quasi-identifier and a parameter problem of k-anonymization is to compute a view T  X  that has the same attributes as T such that T  X  is k-anonymous and T  X  is as close to
T as possible according to some quality metric. We shall discu ss the quality metrics soon.
 changed, to keep our discussion simple but without loss of ge neral-ity, hereafter we consider only the attributes in the quasi-identifier. That is, for table T ( A 1 , . . . , A n ) in question, we assume A ) is the quasi-identifier. K-anonymization was proposed by Samarati and Sweeney [11; 1 3; 15; 14]. Generally, data items are recoded in anonymization . Here, we regard suppression as a specific form of recoding that reco des a data item to null value (i.e., unknown).
 Two types of recoding can be used [17]: global recoding and lo cal recoding, as described and demonstrated in Section 1.1. Man y pre-vious methods use global recoding. In [11; 13], full-domain gener-alization , a specific type of global recoding, was developed, which maps the whole domain of each quasi-identifier attribute to a more general domain in the domain generalization hierachy. Full -domain generalization guarantees that all values of a particular a ttribute still belong to the same domain after generalization.
 To achieve full-domain generalization, two types of partit ioning can be applied. First, single-dimensional partitioning [4 ; 7] divides an attribute into a set of non-overlapping intervals, and ea ch in-terval will be replaced by a summary value (e.g., the mean, th e median, or the range). On the other hand, (strict) multidime nsional partitioning [9] divides the domain into a set of non-overla pping multidimensional regions, and each region will be generali zed into a summary tuple.
 Generally, anonymization is accompanied by information lo ss. Var-ious models have been proposed to measure the information lo ss. For example, the discernability model [4] assigns to each tuple penalty based on the size of the group that t is generalized, i.e., the number of tuples equivalent to t on the quasi-identifier. That is, Alternatively, the normalized average equivalence class size metric was given in [9]. The intuition of the metric is to measure how well the partitioning approaches the best case where each tu ple is generalized in a group of k indistinguishable tuples. That is, The quality of anonymization can also be evaluated based on i ts usefulness in data analysis applications, such as classific ation [6; 16].
 The ideal anonymization should minimize the penalty. Howev er, theoretical analysis [2; 10; 9; 3; 1] indicates that the prob lem of optimal anonymization under many non-trivial quality mode ls is NP-hard. A few approximation methods were developed [3], su ch as datafly [14], annealing [18], and Mondrian multidimensio nal k-anonymity [9]. Interestingly, some optimal methods [4; 8] w ith exponential cost in the worst case were proposed. The experi mental results in those studies show that they are feasible and can a chieve good performance in practice. Without loss of generality, in this paper we assume that gene raliza-tion is used in anonymization. That is, when a tuple is genera lized, the ranges of the group of tuples that are generalized are use d to represent the generalization, as illustrated in Figure 1. I f other rep-resentations such as mean or median are used, the definitions can be revised straightforwardly and our methods still work. In previous methods, the quality metrics, such as the discer nabil-ity metric and the normalized average equivalence class siz e met-ric discussed in Section 2.2, mainly focus on the size of grou ps in anonymization. In an anonymized table, when each group of tu ples sharing the same projection on the quasi-identifier has k penalty metrics are minimized. However, such metrics may no t lead to high quality anonymization.

E XAMPLE 1 (Q UALITY METRICS ). Suppose we want to achieve 2 -anonymity for the six tuples shown in Figure 2. is the quasi-identifier. The six tuples can be anonymized in t hree groups: { a, b } , { c, d } , and { e, f } . In this anonymization scheme, both the discernability metric C DM and the normalized average equivalence class size metric C AV G are minimized.
 Let us consider the utility of the anonymized data. Suppose e ach group is generalized using the range of the tuples in the grou p. That is, a and b are generalized to ([10 , 20] , [60 , 70]) generalized to ([20 , 50] , [20 , 50]) ; and e and f are generalized to ([50 , 60] , [10 , 15]) .
 In order to measure how well the generalized tuples approxim ate the original ones, for each tuple we can use the sum of the inte r-val sizes on all attributes of the generalized tuple to measu re the uncertainty of the generalized tuples. That is, U ( a ) = U ( b ) = 10 + 10 = 20 . Similarly, we get U ( c ) = U ( d ) = 60 U ( e ) = U ( f ) = 15 . The total uncertainty of the anonymized table is the sum of the uncertainty of all tuples, i.e., P t  X  T U ( t ) = 20 + 20 + 60 + 60 + 15 + 15 = 190 tuition, the uncertainty reflects the information loss. The less the uncertainty, the less information is lost.
 On the other hand, we may anonymize the tuples in two groups: { a, b, c } are generalized to ([10 , 20] , [50 , 70]) , and generalized to ([50 , 60] , [10 , 20]) . In fact, the data set is mous, which is better than 2 -anonymous in terms of privacy preser-vation. Moreover, the total uncertainty in this anonymizat ion is 150 , lower than the 2 -anonymity scheme.
 However, this anonymization scheme has a higher penalty tha n the 2-anonymous scheme in both the discernability metric C DM the normalized average equivalence class size metric C other words, optimizing the quality metrics on group size ma y not always lead to anonymization that minimizes the informatio n loss. Can we have a quality metric that can measure the utility of th e anonymized data? Such a utility-based metric should capture the following two aspects. We introduce the concept of certainty penalty to capture the uncer-tainty caused by generalization. First, let us consider the case of numeric attributes. Let table with quasi-identifier ( A 1 , . . . , A n ) , where all attributes are numeric. Suppose a tuple t = ( x 1 , . . . , x n ) is generalized to tuple t = ([ y 1 , z 1 ] , . . . , [ y n , z n ]) such that y i  X  x On attribute A i , the normalized certainty penalty is defined as where | A i | = max t  X  T { t.A i } X  min t  X  T { t.A i } is the range of all tuples on attribute A i .
 Let each attribute A i be associated with a weight w i to reflect its utility in the analysis on the anonymized data. Then, the weighted certainty penalty of a tuple is given by Clearly, when all weights are set to 1 and all attributes have ranges [0 , 1] , the weighted certainty penalty is the L 1 norm distance be-tween points (max t  X  G { t.A 1 } , . . . , max t  X  G { t.A (min t  X  G { t.A 1 } , . . . , min t  X  G { t.A n } ) , where group that t belongs to.
 Our utility-based metric is given by the total weighted cert ainty penalty on the whole table. That is, Distance is often not well defined on categorical attributes , which makes measuring utility on categorical attributes difficul t. In some previous methods (e.g., [8; 9]), it is assumed that a total or der exists on all values in a categorical attribute. In many applicatio ns, such an order may not exist. For example, sorting all zipcodes in t heir numeric order may not reflect the utility properly. Two regio ns may be adjacent but their zipcodes may not be consecutive.
 More often than not, hierarchies exist in categorical attri butes. For example, zipcodes can be organized into hierarchy of region s, cities, counties, and states.
 Let v 1 , . . . , v l be a set of leaf nodes in a hierarchy tree. Let the node in the hierarchy on the attribute such that u is an ancestor of v 1 , . . . , v l , and u does not have any descendant that is still an ancestor of v 1 , . . . , v l . u is called the closest common ancestor of v , . . . , v l , denoted by ancestor ( v 1 , . . . , v l ) nodes that are descendants of u is called the size of u , denoted by size ( u ) .
 Can we use the hierarchy information to measure the utility o n cat-egorical attributes? Consider a categorical attribute of domain { a, b, c, d, e, f, g } pose a hierarchy exists on the attribute as shown in Figure 3. The values appear in the leaf nodes in the hierarchy tree.
 Intuitively, if we generalize tuples having values b and c mized tuples have good utility on this categorical attribut es, since b and c share the same parent in the hierarchy. On the other hand, putting a and f into the same generalized group may have poor utility on the attribute since the common ancestor of a and away from f .
 One may wonder whether the shortest distance between u and in the hierarchy tree can be used as the certainty penalty. Un fortu-nately, it does not work well. Consider Figure 3 again. Intui tively, generalizing d and e together is better than generalizing together, since the closest common ancestor of d and e is in a hi-erarchical level lower than the closest common ancestor of d . However, the shortest distance between d and e is 5 , while the shortest distance between a and d is only 4 . If we use the shortest distance as the guide, then merging a and d is better than merging d and e . In other words, the shortest distance may be misleading. To measure the utility of merging two values x and y into the same generalized group, we can observe that the critical factor i s for the closest common ancestor u of x and y , how many other values are also the descendants of u . The smaller the number, the smaller the uncertainty introduced by the generalization.
 Based on the observation in Example 2, we define the certainty penalty on categorical attributes as follows.
 Suppose a tuple t has value v on a categorical attribute A it is generalized in anonymization, the value will be replac ed by a set of values { v 1 , . . . , v l } , where v 1 , . . . , v ples on the attribute in the same generalized group. We define the normalized certainty penalty of t as follows.
 where | A | is the number of distinct values on attribute we assume that each leaf node is of the same importance. The de f-inition can be straightforwardly extended by assigning wei ghts to internal nodes to capture the more important leaf nodes and i nternal hierarchical structures. Limited by space, we omit the deta ils here.
E XAMPLE 3. Let us consider the cases discussed in Example 2 again. Putting a and d together in a group has penalty 1 , and putting d the case of a and d .
 Putting things together, for a table consisting of both nume ric and categorical attributes, the total weighted normalized cer tainty penalty is the sum of the weighted normalized certainty penalty of al l tu-ples. That is, where NCP A a numeric or categorical attribute.
 Given a table T , a parameter k , the weights of attributes and the hierarchies on categorical attributes, the problem of optimal utility-based anonymization is to compute a k-anonymous table T  X  such that the weighted normalized certainty penalty on T  X  is minimized. The previous studies show that the problem of optimal k-anon ymity is NP-hard under various quality models. The utility-based model we propose here is a generalization of the suppression model . We have the following results on the complexity.
 quasi-identifier has only categorical attributes. The prob lem of op-timal utility-based k-anonymization is NP-hard for k  X  2 . Input: a table T , parameter k , weights of attributes, and Output: a k -anonymous table T  X  ; Method: 1: Initialization: create a group for each tuple; 2: WHILE there exists some group G such that | G | &lt; k 3: FOR each group G such that | G | &lt; k DO { 4: scan all other groups once to find group G  X  such 5: merge groups G and G  X  ; 6: FOR each group G such that | G | X  2 k DO 7: split the group into  X  | G | 8: generalize and output the surviving groups; Proof sketch. We can show that the suppression model used in [2] is a special case of the weighted normalized certainty penal ty de-fined here, where all weights are set to 1 and all hierarchies have only two levels: the detailed values and suppression. The le mma follows from the result in [2].
 Following from the lemma, we have the following result.

T HEOREM 1 (C OMPLEXITY ). The problem of optimal utility-based anonymization is NP-hard.
 In fact, for a table consisting of only numeric attributes, t he prob-lem is still NP-hard. Limited by space, we omit the details he re. In this section, we develop heuristic methods for utility-b ased ano-nymization. We propose two greedy algorithms. The first meth od conducts a bottom-up search, while the second one works top-down. To maximize the utility of the anonymization of a tuple, we ma y  X  X luster X  the tuples locally according to the weighted cert ainty penalty. Those compact clusters having at least k tuples can be generalized. This idea leads to our bottom-up method.
 At the beginning, we treat each tuple as an individual group. In each iteration, for each group whose population is less than merge the group with the other group such that the combined gr oup has the smallest weighted certainty penalty. The iteration goes on until every group has at least k tuples. The algorithm is shown in Figure 4.
 The bottom-up algorithm is a greedy method. In each round, it merges groups such that the resulted weighted certainty pen alty is locally minimized. In one iteration, if one group is merged w ith multiple groups, it is possible that the group becomes large r than In order to avoid over-generalization, if a group has more th an tuples, then the group should be split. It is guaranteed that in the resulted table, each group has up to (2 k  X  1) tuples.
 Please note that, unlike many previous methods that try to mi ni-mize the average number of tuples per group, our algorithms t ry to Input: a table T , parameter k , weights of attributes, Output: a k -anonymous table T  X  ; Method: 1: IF | T | X  k THEN RETURN ; 2: ELSE { 3: partition T into two exclusive subsets T 1 and T 2 such 4: IF | T 1 | &gt; k THEN recursively partition T 1 ; 5: IF | T 2 | &gt; k THEN recursively partition T 2 ; 6: adjust the groups so that each group has at least k tuples;
Figure 5: The framework of the top-down greedy search method . reduce the weighted certainty penalty, which reflects the ut ility of the anonymized data. At the same time, they also keep the numb er of tuples per group small.
 To understand the difference between our method and the prev ious methods, let us check the case in Figure 2. The bottom-up meth od generates two groups: { a, b, c } and { d, e, f } , as expected in Ex-ample 1. Although it does not minimize the average group size , it optimizes the utility of the anonymized data  X  the informati on loss is better than any 2 -anonymous scheme in this example. Moreover, as a byproduct, the result is 3 -anonymous, which means a stronger protection of privacy.
 After the k -th round, the number of tuples in a group is at least Therefore, by at most  X  log k tuples, and thus the generalized groups satisfy the k -anonymity requirement. The complexity of the algorithm is O (  X  log on table T .
 The bottom-up method is a local recoding method. It does not s plit the domain. Instead, it only searches the tuples. Different groups may have overlapping ranges. Moreover, in the step of splitt ing, several tuples with the identical quasi-identifier may be sp lit into different groups. The major cost in the bottom-up method is to search for the clo sest groups (Step 4 in Figure 4). In the bottom-up method, we have to use a two-level loop to conduct the search. We observe, if w e can partition the data properly so that the tuples in each par tition are local, then the search of the nearest neighbors can be spe d up. Motivated by this observation, we develop the top-down appr oach. The general idea is as follows. We partition the table iterat ively. A set of tuples is partitioned into subsets if each subset is m ore local. That is, likely they can be further partitioned into s maller groups that reduce the weighted certainty penalty. After th e parti-tioning, we merge the groups that are smaller than k to honor the k -anonymity requirement.
 To keep the algorithm simple, we consider binary partitioni ng. That is, in each round, we partition a set of tuples into two subset s. The algorithm framework is shown in Figure 5.
 Now, the problem becomes how we can partition a set of tuples into two subsets so that they are compact and likely lead to sm all weighted certainty penalty. We adopt the following heurist ic. We form two groups using the two seed tuples that cause the highe st certainty penalty if they are put into the same group, and ass ign the other tuples into the two groups according to the two seed tup les. Technically, we want to find tuples u, v  X  T that maximize NCP ( u, v ) . u and v become the seed tuple of groups G , respectively.
 The cost of finding u, v such that NCP ( u, v ) is maximized is O ( | T | 2 ) . To reduce the cost, we propose a heuristic method here. We randomly pick a tuple u 1 . By scanning all tuples once, we can find tuple v 1 that maximizes NCP ( u 1 , v 1 ) . Then, we scan all tuples again, find tuple u 2 that maximizes NCP ( u 2 , v ation goes on a few rounds until NCP ( u, v ) does not increase sub-stantially. Our experimental results on both the real data s ets and the synthetic data sets show that the maximal weighted certa inty penalty converges quickly. By up to 3 rounds, we can achieve of the maximal penalty. By up to 6 rounds, we can achieve more than 98 . 75% of the maximal penalty. In practice, we can choose a small integer as the number of rounds to find the seed tuples. Once the two seed tuples are determined, two groups G u and are created. Then, we assign other tuples to the two groups on e by one in a random order. For tuple w , the assignment depends on NCP ( G u , w ) and NCP ( G v , w ) , where G u , G v are the groups formed so far. Tuple w is assign to the group that leads to lower uncertainty penalty.
 If at least one group has k or more tuples, then the partitioning is conducted. The top-down method is recursively applied to th ose groups having at least k tuples.
 We have a postprocessing step to adjust for those groups with less than k tuples. If one group G has less than k tuples, we apply the local greedy adjustment similar to the bottom-up approach. That is, we consider two alternatives. First, we can find a set G  X  of tuples in some other group that has more than (2 k  X  X  G | ) such that NCP ( G  X  G  X  ) is minimized. Second, we compute the increase of penalty by merging G with the nearest neighbor group of G . By comparing the two penalty measures, we decide whether G  X  is moved to G or G is combined with its nearest neighbor group. Such adjustments should be done until every group has at leas t tuples, i.e., the k -anonymity requirement is satisfied.
 In worst case, the partition depth is bounded by O ( | T | ) step of partition, it takes O ( m ) time cost to partition the in the current set into two subsets. Thus, the overall partit ioning cost is O ( | T | 2 ) . After the top-down partitioning, in the worst case, we may have to adjust  X  | T | Thus, the cost of adjustment is O ( | T | 2 ) in the worst case. However, in practice, the number of groups that are smaller than k less than the worst case. As shown in our experiments, the top -down method is clearly faster than the bottom-up method.
 The top-down method is also a local recoding method, since in the adjustment step, similar to the bottom-up method, two tuple s iden-tical in the quasi-identifier may be assigned to two differen t groups. To evaluate the two heuristic methods proposed in this paper , we conducted an extensive empirical study using both real data sets and synthetic data sets. We compare three methods: the mondarian multidimensional k -anonymization method [9], the bottom-up method and the top-down method developed in this paper. According to [9], the mondar ian multidimensional k-anonymization method (called MultiDi m for short hereafter) is so far the best method in both quality (me asured by the discernability penalty) and efficiency. The general i dea of the method is a top-down greedy search that is similar to buil ding kd-trees [5]. At each step, it chooses a dimension to split th e data set at the median of the dimension. Heuristically, the dimen sion with the widest normalized range of values is chosen.
 We measure the quality of the anonymization using three crit eria: the certainty penalty, the discernability penalty, and the error rate in query answering. The certainty penalty proposed in this pap er mea-sures the utility of the anonymization. The discernability penalty is a de facto standard measure on anonymization quality used in many previous studies. The error rate measures how effectiv e the anonymized data sets are in query answering.
 All our experiments were conducted on a PC with a Pentium P4 2. 0 GHz CPU and 512 MB main memory, running Microsoft Windows XP. All the algorithms were implemented by us in Microsoft Vi sual C++ version 6.0. The Adults census data set from the UC Irvine machine learnin g repository has become a de facto benchmark for k-anonymizat ion. The data set was configured as described in [4]. The salary cla ss attribute was dropped, and the tuples with missing values we re re-moved. The resulting data set contains 30 , 162 tuples.
 Since the MultiDim method does not handle hierarchies on cat egor-ical attributes but treats a categorical attribute as a disc rete numeric attribute, we configured the data set for MultiDim as it was us ed in [9]. For the bottom-up method and the top-down method pro-posed in this paper, we used age and education levels as numer ic data, and use the other attributes as categorical attribute s. We used the two hierarchies in Figure 6 on attributes work-class and marital-status. On other categorical attributes, a simple two-leve l hierarchy is applied: the values are the leaf nodes and the root is ALL (i .e., suppression). All weights were set to 1 .
 Figure 7 shows the certainty penalty of the anonymization of the three methods with respect to different k values. As expected, since the bottom-up method and the top-down method focus on the cer -tainty penalty, but the MultiDim method does not, the anonym iza-tion generated by the bottom-up method and the top-down meth od has a clearly lower certainty penalty. The gap is stable, abo ut 2  X  10 4 .
 Figure 8 compares the discernability penalty of the anonymi zation generated by the three methods with respect to different val ues of Interestingly, although the bottom-up and the top-down met hods do not explicitly focus on reducing the discernability penalt y, they out-perform the MultiDim method. Please note that the discernab ility penalty in the figure is drawn in the logarithmic scale. The re sults show that optimizing the utility and the reducing the discer nability are not conflicting with each other. In fact, the two methods a lso try to keep the size of groups same when they reduce the certai nty penalty. Grouping tuples locally can bring us benefit on redu cing both the certainty penalty and the discernability penalty. Interestingly, the anonymized data sets generated by the bo ttom-up method and the top-down method are comparable in both the certainty penalty and the discernability. This is not unexp ected since the two methods greedily group tuples locally to achie ve k-anonymity.
 To test the effectiveness of query answering using the anony mized data, we generate workloads using SUM and COUNT aggregate Figure 7: Certainty penalty on data set Adults. queries, respectively. Each workload has 1 , 000 random queries. Each COUNT query involves all the attributes, and each SUM query involves all but the age attribute that is used to compute the sum. The ranges of the attributes are selected randomly. For a cat egorical attribute, a query carries either a random categorical valu e, or a set of values that are summarized by an internal node in the hiera rchy as the range. This is consistent with the settings in [9]. Figure 9 shows the results on two workloads of aggregate func tions COUNT and SUM , respectively, with respect to different k Clearly, the bottom-up method and the top-down method outpe r-form the MultiDim method substantially. The results can be e x-plained in two aspects. First, the utility-driven anonymiz ation put tuples that are similar to each other into groups. Thus, the g eneral-ized groups often have small ranges, and can answer queries m ore accurately. Second, our methods handle categorical attrib utes bet-ter than the MultiDim method. The hierarchies are considere d in the anonymization. This contributes to the query answering quality strongly.
 Figure 10 shows the runtime of the three methods. As the trade -off, the bottom-up and the top-down methods consumes more runtim e than the MultiDim method. The top-down method is about 5-6 times slower than MultiDim, and is much faster than the botto m-up method. The runtime of the three methods is not sensitive to difference in the efficiency can be explained by their comple xity. While the MultiDim method has the complexity O ( | T | log | T | ) bottom-up and the top-down methods have complexity O ( | T | To test the performance of the three methods more thoroughly , we generated synthetic data sets in two types of distributions : uniform distribution and Gaussian distribution. The dimensionali ty and the number of tuples may vary according to the needs of experimen ts. By default, a data set has 10 , 000 tuples and each attribute is in the domain of integer with range [1 , 16] . Again, by default the weights are set to 1 . Figures 11 and 12 show the certainty penalty with respect to the synthetic data sets with uniformly distribution and Gau ssian distribution, respectively. In the uniform distributed da ta, the Mul-tiDim method and the top-down method are comparable, and the top-down method is better when k is small. The bottom-up method performs poorly. The reason is that with uniform distributi on, the kd-tree like construction in the MultiDim method can partit ion the data set evenly into groups with hyper-rectangle bounding b oxes so that each group is balanced and achieves low penalty. The s ame happens to the top-down method as well. In the bottom up metho d, the groups formed by merging may be in irregular shape and thu s may lead to high certainty penalty.
 In data sets with Gaussian distribution, both the top-down m ethod and the bottom-up method work better than the MultiDim metho d. Figure 11: Certainty penalty with respect to k , on synthetic data sets with uniform distri-bution (dimensionality = 4).  X  = 1 . 0 ). Figure 14: Certainty penalty with respect to dimensionality, on synthetic data sets with Gaussian distribution (  X  = 1 . 0 , k = 10 ). The advantage is clear. With bias data, local search and loca l re-coding may have good chance to find local clusters that lead to low certainty penalty.
 It is interesting to test the certainty penalty with respect to the de-gree of bias in data. Figure 13 shows the results. The top-dow n method is consistently the best. When the data is severely bi ased, the MultiDim method performs poorly. But when the data becom es less biased, the MultiDim method catches up with and even out pe-forms the bottom-up method, but is still worse than the top-d own method.
 Figure 14 shows the certainty penalty with respect to variou s di-mensionality. The top-down method and the bottom-up method are comparable, and the top-down method is slightly better. The MultiDim method has a high certainty penalty in high dimensi onal data. Please note that, as the dimensionality increases, th e certainty penalty generally increases accordingly since each attrib ute con-tributes to the certainty penalty. The bottom-up and the top -down methods try to reduce the penalty in the anonymization proce dure and thus may achieve good results.
 We also test the quality of the anonymization using the disce rnabil-ity penalty measure. Figures 15, 16, 17, and 18 show the resul ts on the cases in Figures 11, 12, 13, and 14, respectively. The r e-sults using the discernability penalty measure are consist ent with the results reported in [9].
 From the results, we can observe that the bottom-up method an d the top-down method have similar performance, and achieve l ess discernability penalty than the MultiDim method in all case s. This is consistent with the results on the real Adults data set. From this set of experiments, we conclude that the bottom-up and the top-down methods often have similar performance in anon ymiza-tion quality, measured by both the certainty penalty and the dis-cernability. The anonymization quality using those two met hods are often better than the MultiDim method. To test the utility in query answering, we use a uniformly dis -tributed data set with 4 attributes, and set k = 10 . We assign weights 8 , 4 , 2 , and 1 to attributes A 1 , A 2 , A 3 , and tively. That is, the information loss in attribute A 1 is strongly un-desirable.
 We generate 4 groups of random queries on attribute combinations A , A 1 A 2 , A 1 A 2 A 3 , and A 1 A 2 A 3 A 4 , respectively. The average error rates of the queries in each group is shown in Figure 19. For comparison, we also conduct the same queries on anonymizati on that do not consider the weights.
 As can be seen, the effect of utility-based anonymization is signif-icant. The anonymization using the weighted top-down or bot tom-up methods answers the queries on A 1 , A 1 A 2 , and A 1 accurately than the non-weighted methods. When all attribu tes are involved in a query, the weighted methods may lose some accur acy as the trade-off.
 We also test the average error rates using the anonymized dat a to Figure 17: Discernability penalty with re-spect to  X  , on synthetic data sets with Gaus-sian distribution (dimensionality=4, k = 10 ).  X  = 1 . 0 , k = Figure 20: Query answering error rate, on synthetic data sets with Gaussian distribution (dimensionality=4,  X  = 1 . 0 ). answer aggregate queries. Figure 20 shows the results. In th is ex-periment, we assign the default weight 1 to every attribute, and test two aggregate functions SUM and COUNT . The average error rate is computed from 1 , 000 random queries. The methodology is the same as the experiment reported in Figure 9 and the experimen ts reported in [9].
 The results show that both the bottom-up and the top-down met h-ods achieve lower error rate than the MultiDim method when not large, since local recoding often groups tuples with sma ll cer-tainty penalty. When k is large, the top-down method has the best performance, and is clearly better than the other two method s. The advantages of the bottom-up and the top-down methods in anonymization quality do not come for free. The trade-off is the longer computation time. Figure 21 shows the results on scal ability. The complexity of the MultiDim method is O ( | T | log | T | ) than that of the bottom-up and the top-down methods. Thus, th e MultiDim method is more scalable. However, since anonymiza -tion is typically an offline, one-time task, quality can be a m ore important concern than the runtime. On the other hand, the di f-ference between the top-down method and the MultiDim method is not dramatic. In our experiments, even when the data set sc ales up to 100 , 000 tuples, the runtime of the top-down approach is just less than 6 times slower than that of the MultiDim method. The top-down method is substantially faster than the bottom -up method. As analyzed in Section 4, splitting in the top-down m ethod is much faster than merging in the bottom-up method.
 A critical step in the top-down method is to choose two seed tu -ples. We used a heuristic method as described in Section 4. Fi g-ure 22 shows the effectiveness of the heuristic. We used a tho rough method to compute the pair of tuples of the largest certainty penalty. Then, we used the heuristic method to compute seed tuples tha t are far away, and compare their certainty penalty with the maxim um. As shown, with a small number of iterations, our heuristic gi ves very good approximation to the maximum. Thus, in our impleme n-tation, we conduct 3 iterations to obtain the seed tuples. The extensive experiments using both real data sets and synt hetic data sets show that, in terms of utility and discernability, the bottom-up method and the top-down method developed in this paper oft en achieve better anonymization in quality than the MultiDim m ethod, the state-of-the-art approach. The top-down method is bett er than the bottom-up method.
 The trad-off of high anonymization quality is the runtime. T he MultiDim method is more efficient. However, the runtime of th e top-down method is not far away from that of the MultiDim meth od in practice. Moreover, for anonymization, the computation time is often a secondary consideration yielding to the quality. As privacy becomes a more and more serious concern in applica -tions involving microdata, good anonymization is importan t. In this paper, we showed that global recoding, which is often used in pre-vious methods, may not achieve effective anonymization in t erms of discernability and query answering accuracy. Moreover, the util-ity of attributes has not been considered in the previous met hods. Consequently, we study the problem of utility-based anonymiza-tion . A simple framework was given to specify utility of attribut es, and two simple yet efficient heuristic local recoding method s for utility-based anonymization were developed. Our extensiv e perfor-mance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidim ensional global recoding methods in both discernability and query an swer-ing accuracy. Furthermore, our utility-based method can bo ost the quality of analysis using the anonymized data.
 Utility-based anonymization is important in application a nd may lead to a few interesting problems for future study. For exam ple, given a utility specification (e.g., the maximum expected er ror rate), what is the best anonymization that can be achieved? On the ot her hand, if different tuples have different privacy-preservi ng require-ments, how can we come up with an anonymization scheme that balance the utility and the privacy preservation? We sincerely thank the reviewers for their very careful and c on-structive comments.
 This research was supported by the Shanghai Raising Star Pro -gram Grant 05QMX1405, the National Natural Science Founda-tion of China Grants 69933010 and 60303008, the NSERC Grants 312194-05 and 614067, the NSF Grant IIS-0308001, and the RGC Earmarked Research Grant of HKSAR CUHK 4120/05E. All opin-ions, findings, conclusions and recommendations in this pap er are those of the authors and do not necessarily reflect the views o f the funding agencies. [1] C. C. Aggarwal. On k-anonymity and the curse of dimension -[2] G. Aggarwal, T. Feder, K. Kenthapadi, R. Motwani, R. Pani -[3] G. Aggarwal, T. Feder, K. Kenthapadi, R. Motwani, R. Pani -[4] R. J. Bayardo and R. Agrawal. Data privacy through optima l [5] J. L. Bentley. Multidimensional binary search trees use d for [6] B. C. M. Fung, K. Wang, and P. S. Yu. Top-down special-[7] V. S. Iyengar. Transforming data to satisfy privacy con-[8] K. LeFevre, D. J. DeWitt, and R. Ramakrishnan. Incognito : [9] K. LeFevre, D. J. DeWitt, and R. Ramakrishnan. Mondrian [10] A. Meyerson and R. Williams. On the complexity of [11] P. Samarati. Protecting respondents X  identities in mi crodata [12] P. Samarati and L. Sweeney. Generalizing data to provid e [13] P. Samarati and L. Sweeney. Protecting privacy when dis clos-[14] L. Sweeney. Achieving k-anonymity privacy protection us-[15] L. Sweeney. K-anonymity: a model for protecting pri-[16] K. Wang, P. S. Yu, and S. Chakraborty. Bottom-up gener-[17] L. Willenborg and T. deWaal. Elements of Statistical Dis-[18] W. E. Winkler. Using simulated annealing for k-anonymi ty.
