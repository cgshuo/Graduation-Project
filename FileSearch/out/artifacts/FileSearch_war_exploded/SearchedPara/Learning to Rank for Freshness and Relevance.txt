 Freshness of results is important in modern web search. Failing to recognize the temporal aspect of a query can negatively affect the user experience, and make the search engine appear stale. While freshness and relevance can be closely related for some topics (e.g., news queries), they are more independent in others (e.g., time in-sensitive queries). Therefore, optimizing one criterion does not necessarily improve the other, and can even do harm in some cases.
We propose a machine-learning framework for simultaneously optimizing freshness and relevance, in which the trade-off is auto-matically adaptive to query temporal characteristics. We start by il-lustrating different temporal characteristics of queries, and the fea-tures that can be used for capturing these properties. We then intro-duce our supervised framework that leverages the temporal profile of queries (inferred from pseudo-feedback documents) along with the other ranking features to improve both freshness and relevance of search results. Our experiments on a large archival web corpus demonstrate the efficacy of our techniques.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Performance Temporal profiles, Query classification, Freshness ranking
The query stream seen by a web search engine and the interpreta-tion of those queries change over time. Previous analysis has shown that web logs clearly reflect daily events in user queries [6]. For ex-ample, during seasonal events such as Halloween, there are always spikes in the frequency of related queries such as  X  X alloween X ,  X  X alloween costumes X  and  X  X umpkins X . For many of the queries that correspond to events, the best answer may change over time (e.g., the latest SIGIR conference homepage for the query  X  X igir conference X ). In more extreme cases, the major intent behind the same query can temporally vary; for instance, the query  X  X S open X  is more likely to be targeting the tennis open in September, and the golf tournament in June. Kulkarni et al. [22] referred to this class of temporally ambiguous queries as shift topics.

News events, depending on their significance, can cause enor-mous growth in frequency of related queries. 1 It is also not uncom-mon for news events to change the general meaning of a query. For example, the query  X  X pad X  which could be treated as a misspelling for  X  X pod X  in 2009, suddenly turned into a valid query with several related websites in 2010. 2 Therefore, making search engine results appear current and fresh is important to satisfy users X  ever-changing information needs.

In this paper, we focus on improving the ranking of results for queries based on their temporal profiles. Of course, the impor-tance of the temporal profiles of queries extends beyond web re-sult ranking; advertisement rankers have to address similar prob-lems; related search and auto-complete suggestions must provide users with fresh and relevant alternatives to their queries; verti-cal search [11] ranking and triggering can be affected by temporal changes, and in general, the entire search experience can be influ-enced according to the temporal aspect of a query.

Learning ranking functions that can respond effectively to di-verse temporal dynamics of queries is challenging. One of the dif-ficulties is that traditional machine learning ranking algorithms fail to consider the interaction between freshness and relevance. While relevance clearly quantifies the topical matchability between query and web pages, freshness can be interpreted in different ways. For certain temporal queries such as breaking news, freshness is more meaningful when the actual page content reflects new infor-mation. Whereas, for non-temporal (time-insensitive) queries, it makes more sense to interpret freshness as the recency of page maintenance with respect to the time point of generating ranking lists (suppose web pages contain such information). Therefore, these two interpretations for freshness may be correlated to some extent but are not the same, considering that pages updated recently tend to record fresh information. It is worthwhile pointing out that both explanations can be part of the overall quality of search results that influences user search experience. In this work, the definition of freshness is sensitive to query temporal characteristics, varying
For example, the traffic caused by queries related to Michael Jack-son X  X  death in 2009, was so huge that Google mistook it as an attack (Source: Google Blog, 26 Jun 2009). It is probably still the case that some people mistype ipod as ipad. However, this group no longer represents the majority. on whether human editors (judges) can identify temporal intents concealed within queries. (See Section 4 for details.)
For certain temporal queries such as breaking news, relevance and freshness are highly correlated. Therefore, a ranker optimized for returning fresh documents may produce satisfactory results. However, for queries that are not usually time-sensitive (e.g.,  X  X ace-book X ,  X  X achine learning X ), paying too much attention to freshness may significantly hurt ranking effectiveness in terms of relevance. Among common ranking features, clicks, anchor-text and histori-cal data might be the most powerful for answering time-insensitive queries. For temporal queries however, other features such as the rate of content change in documents may provide better signals [22]. Therefore, a ranker optimizing either freshness or relevance only may not be flexible enough to deal with the temporal dynamics of queries effectively.

To address this issue, previous work [3, 12] suggested train-ing separate rankers for different classes of queries. The query is first classified according to its temporal profile, and then is sent to the appropriate ranker that has been optimized for either relevance or freshness. The main disadvantage of classification-based tech-niques is that selecting a wrong ranker due to misclassification can significantly degrade the performance.

We propose a machine learning model that optimizes freshness and relevance simultaneously. Our flexible framework allows train-ing multiple rankers with different optimization functions, and runs each query against all rankers with the weights varying according to the query X  X  temporal profile. This is in contrast with existing solutions that suggest selecting one ranker per query, and conse-quently has a lower risk of poor performance when queries are mis-classified. In addition, instead of splitting the labeled data to train separate rankers, our technique leverages the entire data in training all rankers. To the best of our knowledge, this is the first attempt to incorporate the trade-off between freshness and relevance into a single ranking framework.

Our work can be regarded as an extension to the family of divide and conquer (DAC) techniques for ranking [2]. In DAC, queries are clustered based on their feature representations, and separate rankers are trained with each for one cluster simultaneously. At test time, the query is compared against the generated cluster cen-troids and is ranked under all rankers with the weights depending on query-cluster similarity values. We follow a similar path since DAC enables specialized ranker training by considering query fea-tures, but we incorporate multiple criteria (freshness and relevance) into ranking optimization. We also modify the DAC loss function by introducing a new query-document importance factor that em-phasizes certain documents during training, and leads to further im-provements in the results. Our experiments on a large web archive demonstrate that the rankers trained by our techniques can achieve better relevance and freshness compared to state-of-the-art alterna-tives. The contributions of this paper are four-fold: 1. We extend an existing learning to rank framework to opti-2. We introduce a new loss function that emphasizes certain 3. We investigate the correlation between freshness and rel-4. We introduce hybrid NDCG , a new variant of NDCG that
The remainder of this paper is organized as follows. We review prior work in Section 2, and continue by introducing our criteria-sensitive ranking specialization framework in Section 3. Features are summarized in Section 4. We describe our experimental results in Section 5 followed by a more thorough discussion in Section 6. Section 7 concludes the paper and suggests directions for future research.
Pairwise learning to rank techniques have been widely studied in recent years [5, 15, 19]. They cast learning to rank as a preferential relation learning problem. Given a query and a pair of associated documents, if one is more relevant than the other, then it is boosted in the training process to get a higher rank. In most early work, the query type information was ignored in ranking, which limits the ef-fectiveness of ranking functions. For instance, navigational queries target specific websites, while informational queries have a broader range of relevant answers. Hence, their ranking models could be optimized in different ways that depend on query intent [21].
Query-dependent loss/ranking functions were introduced to ad-dress these issues [2, 3, 16]. The general idea is to adopt a query-dependent loss or ranking function based on the query type (class). Geng et al. [16] proposed a k -Nearest Neighbor based method which trains a query-dependent ranking function for each query based on its nearest neighbors in the training set.

Bian et al. [3] achieved better results by learning both mul-tiple ranking functions (by minimizing query-dependent rank-ing risks) and query categorization (navigational, informational, transactional) simultaneously. Although the query-dependent loss function has been found superior to the query-dependent ranking method of Geng et al. [16], it still leaves a few issues unaddressed: (1) query categorization and taxonomies may not be available or could be too noisy; (2) external taxonomies may not necessarily provide the best way of splitting queries for training specialized rankers; and (3) such categories may not be fine-grained enough for training and ranking purposes. To overcome these problems, Bian et al. [2] proposed a divide-and-conquer framework (DAC) for ranking specialization and instantiated it on RankSVM [19]. work [2] can be summarized in three main steps: (1) identifying ranking-sensitive query categories, (2) learning topic-specific rank-ing models via minimizing a global ranking risk, and (3) running each unseen query against all ranking models and merging the out-puts to produce the final ranked list.

In the first step (divide), queries are categorized (clustered) in a soft way to form ranking-sensitive topics. The queries in each clus-ter have similar ranking characteristics and similar ranking feature discriminativity. Bian et al. [2] suggested using the ranking fea-tures aggregated from the top-ranked pseudo-feedback documents that are generated by a reference ranking model to represent queries for clustering. In this way, the queries that have similar features are clustered together. The number of query clusters (topics) can be pre-defined or could be determined according to gap statistics [27].
In the second step ( conquer ), a unified learning method is used to train multiple ranking models, one for each cluster (topic). The au-thors applied a global loss aggregated across all ranking topics, and trained multiple rankers simultaneously by minimizing the global ranking risk. Each query contributes to training all ranking models though with the different weights that are determined by the prob-abilities of belonging to each cluster.

In the last step, each unseen query is submitted to all ranking models and a weighted combination is used to merge the final re-sults.

While most previous work focused on optimizing relevance, we propose an extended framework which optimizes freshness and relevance simultaneously in a more adaptive way. We enhance query representations by adding criteria-sensitive features that can capture different aspects (relevance, freshness) of query-document pairs. Each query is categorized according to both temporal and relevance features, and the final ranking is produced by merging the results generated from several different ranking models. Multiple criteria ranking. Training ranking models for mul-tiple criteria beyond relevance, such as diversity, freshness, and ef-ficiency, has been the subject of many recent papers [12, 13, 17, 28]. Dong et al. X  X  work on recency ranking [12, 13] is among the closest to our work; they consider freshness in instance labeling for training effective ranking models. They argued that freshness is especially important for breaking news queries and demoted the relevance labels of stale pages for training. Empirical experiments demonstrated that such demotion can result in significant improve-ments on both relevance and freshness. We similarly generate hy-brid labels for documents based on their relevance and freshness grades, and show that the labels generated by our strategy are more effective than those demoted for training. Despite this resemblance, our optimization tasks are fundamentally different; Dong et al. [12, 13] studied learning single adaptive or over-weighting rankers that can be trained with an imbalanced amount of training data for fresh-ness and relevance primarily from the perspective of ranking adap-tation. We investigate multi-criteria ranking problem in a divide and conquer framework with balanced distribution of training data, and emphasize adaptive balance between different criteria. that capture the dynamics of queries, web pages, hyperlinks, and user interaction to improve search quality has been widely stud-ied. Several methods focused on complementing content-based matching by utilizing the knowledge of query temporal charac-teristics [1, 10, 20, 23, 24]. Typically this includes: (1) profiling query temporal characteristics, e.g., generating a temporal distribu-tion over pseudo-feedback documents or based on query popularity over time; and (2) emphasizing documents whose temporal charac-teristics are close to the query X  X  temporal profile, e.g., enhancing document representation by adding temporal dimension and then incorporating temporal based matching into search process.
Elsas and Dumais [14] incorporated the dynamics of content changes into document language models and showed that their en-hanced representations can improve retrieval effectiveness. Dai and Davison [8] exploited the frequency of web content and hyperlink changes over time for better estimation of web authorities. Dong et al. [13] used Twitter data to detect and rank fresh documents.
In this section, we introduce our criteria-sensitive divide-and-conquer ranking framework (denoted as CS-DAC) that incorpo-rates the balance between relevance and freshness into training cus-tomized rankers that optimize both freshness and relevance. CS-DAC framework. A typical ranking function f with  X  pa-rameters takes a query-document feature vector X as input and pro-duces ranking scores of documents.
The common goal of learning to rank systems is to find a rank-ing model f  X  that takes query-document feature vectors as input, and produces a document ranking X  X s close as possible to the ora-cle ranking of documents according to their relevance labels y  X  X y minimizing the ranking risk aggregated from the loss L of all train-ing queries. f  X  = arg min By considering query differences in the DAC framework, we es-sentially cluster 3 training queries based on their ranking character-istics, and train one ranker per cluster. Each query contributes to learning all rankers with different importance based on its topical affinity to query clusters. Each ranker f  X  i is learned via: where Q is the training query set, and I ( q,i ) is the importance of query q with respect to the i th ranking model.

To account for relevance and freshness simultaneously, we pro-pose to use hybrid labels that are generated based on freshness and relevance judgments. 4 For this purpose, we exploit a weighted har-monic mean function which maps relevance and freshness grades (i.e., y R q,d and y F q,d on the query-document pair &lt;q,d&gt; ) to a single equivalent numerical score e y q,d for training f  X  i . We believe har-monic mean is appropriate here since (1) it heavily biases towards the minimum score; (2) it is more sensitive when y R q,d and y close; and (3) it has been shown as a good optimization metric for tasks such as learning to rank for efficiency [28] and classification. Formally, e y q,d,i is defined as: where parameter  X  i sets the trade-off between relevance and fresh-ness for each ranker, and is learned during training. Allowing dif-ferent values of  X  for rankers enables a flexible framework where each ranker can assign different weights to freshness and relevance. It also means that each query-document pair may affect the pair-wise learning of each ranker differently. 5 Therefore, we factorize query-document pair importance as follows: where, D q is the set of preferential query-document pairs with re-spect to query q , and U 0 ( q,i,d 1 ,d 2 ) is the importance of &lt;d in training for query q with respect to the i th ranking model. For simplicity, we assume &lt;q,d 1 &gt; and &lt;q,d 2 &gt; are independent, and so factorize the importance of the preferential pair U 0 ( q,i,d as follows.
 where U ( q,i,d 1 ) is the importance of query-document pair
We use query cluster, topic and category interchangeably.
Generating hybrid labels (single aggregate objective functions), is a simple form of multi-criteria optimization [26].
Similar ideas can be applied to list-wise and point-wise ranking learning algorithms. &lt;q,d 1 &gt; in training for query q with respect to the i model. 6 Ensemble ranking. Given an unseen query q 0 , we first profile its query characteristics, and then calculate its distances to the cen-troids of existing query clusters c 1 , c 2 , ... , c n . The trained rank-ing functions are then scored according to the normalized distance between the query and their corresponding clusters (a.k.a. query importance I ), given by: The query q 0 is run against all n rankers (one for each cluster), and the final results  X  q 0 are produced according to the ensemble ranking of their outputs. That is, where f  X  i is the i th ranking model, X q 0 is the query-document fea-ture vectors for query q 0 , and  X  i is the feature weights.
The CS-DAC framework summarized in Equation 3 consists of three main factors: query importance ( I ), ranker-specific query-document importance ( U ), and the loss function ( L ). We continue by describing each of these items.
 Query importance ( I ). In the divide step of the DAC frame-work, the query space is split into a few clusters based on criteria-sensitive features. These are the features that are extracted from the top-ranked documents of a basic reference ranker (BM25 [25] in our work) for the query. We will provide more details about these features in Section 4.

The I ( q,i ) values provide a Binomial distribution over each of criteria-sensitive query clusters, and specify the importance of dif-ferent ranking functions. We use Gaussian Mixture model as soft k -means clustering to group queries into clusters. The importance of query q with respect to the i th cluster is thus given by: where p q and c i respectively denote the feature vectors of query q and the centroid of the i th cluster, and Q represents the set of training queries. Therefore, I ( q,i ) is scaled between [0 , 1] , and is inversely proportional to the distance between query feature vector p and cluster centroid c i .
 Document importance ( U ). In pairwise learning to rank methods, the importance of a document with label y during training depends on the number of times it is compared to other documents with different labels. Due to the ranker-specific value of  X  which is set during training, a query-document pair with the same relevance and freshness grades can get unequal hybrid labels under different rankers, and hence may contribute unequally in training various rankers. Besides, centralizing hybrid label distribution within each query cluster stabilizes the correlation between freshness and rele-vance, which further emphasizes the effect of  X  i in Equation 2. To factorize these impacts, we introduced the U component in Equa-tion 3. We estimate the importance of a query-document pair with label y q,d by the likelihood of visiting that label in the training
The independence assumption is unrealistic, but we believe it is not unreasonable because if two query-documents pairs are impor-tant, then so is their preferential pair. dataset, under the assumption that the importance of a hybrid la-bel is proportional to the ratio of query-document pairs with that label in the training dataset. We define the document importance U as below.
 where Y i is the space of labels for ranker i , and Q denotes the training query set. The number of documents with and without label y are represented by N ( q,i, y ) and N ( q,i,  X  y ) . Equation 5 can be regarded as a function of the unique hybrid label y is denoted as w ( y q,d ) for short.

There are two potential problems with this type of normalization: (1) additional inter-label dependencies may arise from comparing common labels (e.g., y a and y b , versus y b and y c ), and, (2) overem-phasizing certain documents inevitably introduces bias in ranking. To overcome these issues, we exploit a random walk approach to determine U (instead of Equation 5) that has the effect of smooth-ing document importance values.

To perform a random walk, we first construct a fully connected bipartite graph G ( V,E ) (one graph per ranker) in which each node (state) v stands for a unique hybrid label y (associated with the weight w ( y ) ), and each edge e is associated with a weight com-puted according to the number of times the labels of the connected nodes compare with each other during training. At each step, the random walk surfer jumps to a random node with probability d (selection among random nodes is proportional to w ( y ) values) or follows some connected edge with probability 1  X  d (the se-lection among connected edges is proportional to the weights on edges). The value of d can be pre-defined or set during the training and validation. When d equals 1, the probability that the random surfer reaches every node (state) is proportional to the direct com-parison between preferential query-document pairs with different hybrid labels. Whereas, d = 0 suggests document importance en-tirely propagates through indirect comparison between preferential query-document pairs. Parameter d actually controls the extent that such propagation (from indirect comparison) influences the com-putation of document importance. We analyze the importance of U , with and without smoothed probabilities in Section 6. Loss function ( L ). The core of each ranker in our CS-DAC framework is a loss function that is trained for hybrid labels (Equa-tion 2). We follow Bian et al. [3] and use RankSVM [19] as our basic learning algorithm although it is important to note that the framework is flexible and not restricted to any particular learning technique.

RankSVM [19] is designed to maximize the margin between positively and negatively labeled documents in the training data by minimizing the number of discordant pairs. The RankSVM opti-mization problem is defined as: where the non-negative slack variable  X  q,i,j is used to approximate the NP-hard optimization solution by minimizing the upper bound P  X  q,i,j . Parameter C sets the trade-off between the training error and the margin size. The query-document feature vectors for doc-uments i and j are respectively represented by X q i and X notation y q i y q j implies that the document i is ranked higher than document j with respect to query q in the training dataset ( i has the same or higher relevance than j ).

CS-DAC modified the RankSVM loss function by incorporating query importance ( I ) and document importance ( U ). Formally, the i th ranking model of CS-DAC is optimized via: where  X  q,j,k is the slack variable and parameter C sets the trade-off between training error and the margin size.

In CS-DAC, several rankers are trained simultaneously, and each ranking function f  X  k (see Equation 3) is optimized using the CS-DAC loss function and hybrid labels. The  X  values are tuned via hill climbing based on the hybrid NDCG values of the final ranking lists merged from different rankers. That is, each ranker is trained on different values of  X  and the best combination of rankers is cho-sen by hill climbing on the training and validation data. Here, hy-brid NDCG extends the commonly used evaluation metric NDCG [18] to take hybrid labels for evaluation, since this new freshness-sensitive metric can take into account both freshness and relevance into a single measurement, aiming to quantify the overall search quality. Formally, we define hybrid NDCG as below: where Z n is the oracle discounted cumulative gain at ranking cut-off n , that bounds the NDCG values between 0 and 1. The y y
F values X  X lso known as gains  X  X re assigned according to the relevance and freshness labels of documents. Parameter  X  specifies the trade-off between relevance and freshness and is set to 0.5 in our experiments. Note that  X  = 1 turns hybrid NDCG into typ-ical relevance-based NDCG, while setting  X  to zero, makes it the same as the NDCF metric [13]. Dai and Davison [8] also adopted NDCG with freshness labels, although they did not refer to it as NDCF. While other combination forms may better fit the search utility that quantifies comprehensive users X  satisfaction, we leave the best definition of hybrid NDCG for future work. Testbed data. Standard learning to rank datasets only contain relevance judgments for query-document pairs without any infor-mation regarding their freshness.

Therefore, we built a new testbed based on a large archival web corpus. Our dataset contains 158 million unique URLs and 12 bil-lion links from the .ie domain, covering the time span from Jan-uary 2000 to December 2007 (one snapshot per month and 88 in total). We removed pages with less than five snapshots, and only kept the remaining 3.8 million unique pages with 435 million links in total.

We choose April 2007 as our time point of interest for ranking evaluation. We constructed two temporal and non-temporal query sets, each containing 90 queries. While the query size is small, the queries in the temporal set are manually selected from Google Trends suggestions for Ireland, which were popular during April 2007 . 7 For the non-temporal set, we first randomly sampled queries www.google.com/trends Figure 1: The STL decomposition [7] of a time series into sea-sonal, trend and remainder components. The data is generated from the click histogram of the query jingle bells in a commer-cial search engine. from a 2006 MSN query log (i.e., generating a representative query sample from a real-world search log), and then automatically fil-tered out about 10% of them that were detected as potentially tem-poral by a commercial classifier. The classifier has high precision (almost all Google Trend queries are detected as temporal), and uses several years of the query-frequency history extracted from the query logs of a major commercial search engine.
 Judgments and metrics. We have an average of 71 URLs per query judged by one or more participants from Amazon Mechani-cal Turk. 8 Given a query-URL pair, the judges were instructed to assess the quality of the URL with respect to both relevance and freshness. For relevance, the selection was among highly relevant , relevant , borderline , not relevant and not related , which was fur-ther translated to integer gains ranging from 4 to 0. For freshness, editors were instructed to judge the URL freshness for the given query according to our chosen point in time (April 2007). could select between very fresh , fresh , borderline , stale , and very stale , which we transferred into { 4 , 3 , 2 , 1 , 0 } . Judges were also required to provide the confidence of their judgements by choos-ing between high , medium and low . Table 2 shows the guideline of query-URL pair judgments used by Mturk workers. Judgments with low confidence were resubmitted for labeling. The standard deviations of relevance and freshness judgements on a random sam-ple of 76 query URL pairs among three judgers are 0.88 and 1.02 respectively.

Freshness and relevance are evaluated by hybrid NDCG , and so when  X  = 0 or  X  = 1 , this corresponds to NDCF [13] and NDCG, respectively.
 Ranking features. The features used by RankSVM for ranking can be grouped into non-temporal and temporal features. The non-temporal features (summarized in Table 1) include several com-monly used text-similarity scores such as BM25 [25], and language http://www.mturk.com
Admittedly, judging for freshness according to an arbitrary time in past could be a difficult task. However, the choice was dictated to us by the time span of our dataset. and anchor-text fields are respectively represented by B, T, H and A. modeling [31], computed over different fields of documents (head-ing, title, body). The list also includes a few well-known link-based static features such as the number of inlinks and PageRank [4].
The temporal ranking features are generated by measuring the changes in the contents of documents with respect to their previ-ous snapshots. For this purpose, we build a time series of each document X  X  content changes, by going through the entire time span and comparing the TFIDF similarity of the document at each point with the previous and next versions. We generate separate time se-ries for different document fields (heading, title, body), and use STL seasonal-trend decomposition [7] to decompose each time series  X  into trend ( T ), seasonal ( S ) and remainder ( R ) components.
The same steps are repeated to decompose the time series gen-erated based on link and page activities (create, remove, update) [8]. Figure 1 depicts an example of STL decomposition on a time series. In this instance, the time series (data) is generated from the frequency distribution of the query jingle bells in the logs of a commercial search engine. The same decomposition can be ap-plied to a sequence of TFIDF scores, PageRank values or any other type of time series data. We use the output of STL decomposition for different time series to generate our temporal ranking features as summarized in Table 3. The slope of  X  captures the speed of content changes, and has been suggested to be an effective feature for ranking [9]. The amplitude feature can measure the scale of content changes, and the position feature Rp (  X  ) is calculated with respect to the distance to the nearest peak in the time series. The confidence features are computed according to the distribution of S  X  and T  X  values after decomposition. We also employ the Timed PageRank of Yu et al. [30] as our temporally-sensitive static-rank feature.
 are used to cluster queries and assign the weights in each corre-sponding ranking function. We follow the approach taken by Bian et al. [2] and used the  X  top-ranked documents returned by a ref-erence ranker (BM25 [25]) to generate our clustering features. We set the value of  X  to 15 in all our experiments. Once the pseudo-feedback documents are gathered, we compute the average value of each ranking feature over them and use the final mean value as a clustering feature. The feature importance is computed by training a reference RankSVM model for hybrid NDCG (  X  = 0 . 5 ) on the training dataset.
 Baseline methods. We compare the effectiveness of our CS-DAC with four baselines: In SinR, we train a single RankSVM ranker with all features. This could be regarded as a weak baseline that has no form of query cat-egorization, and has been shown to perform more poorly than the Table 3: Temporal ranking features used by RankSVM in the CS-DAC framework and baseline methods. The features (ex-cept for TPR) are produced from the STL decomposition [7] of time series generated from the content changes in title, body, heading, anchor, and page/link activities [8].
 other baselines in previous work [2, 16]. Nevertheless, we report its results because it represents one of the most common learning to rank architectures.

The SepR baseline is representative for the family of query-dependent loss function methods [2, 3, 16], in which the loss function is determined according to the temporal aspect of the query. Separate RankSVM rankers are trained for temporal and non-temporal queries, and each query is tested on the correct ranker for its type. Note that using the correct query type information X  which is generally unavailable without manual effort X  X eans that the performance numbers for this baseline are unaffected by poten-tial query type misclassification, and therefore are overstated.
Dong et al. [12] investigated several techniques for ranking op-timization with imbalanced amount of training data for freshness and relevance. Among their methods the over-weighting approach was most effective. The over-weighting model combines relevance and freshness labeled data to train a single ranker. This is similar to SepR except that the training pairs of the criterion with fewer labels are over-weighted. Dong et al. [12] used GBrank [32] as their ranking model. However, we modify the over-weighting loss function to RankSVM for consistency with the other methods in our experiments as follows:  X  y where Q T and Q N denote the sets of queries from Google Trends and MSN query log. N T and N N are respectively the number of preferential pairs of query-documents in each of those sets.  X  is a parameter that controls the balance of Google Trends queries vs. MSN queries, ranging over [0,1].  X  represents the feature weights within the ranking model.

Our last experimental baseline is TopicalSVM [2] which is the state-of-the-art in the family of divide and conquer techniques. TopicalSVM trains all rankers using a global loss function, and does not factorize the query-document importance U in contrast to CS-DAC.
We start our experiments by investigating the performance of our baseline techniques optimized for different goals. We then pick the best-performing baselines and compare them against CS-DAC. In all our experiments we run 5-fold cross-validation in which the first three folds are used for training, and the remaining two folds are Table 4: Freshness comparison on the temporal (top) and non-temporal (bottom) query sets. All methods are trained using the hybrid labels and the evaluation is based on the freshness rat-ings (y F ). Symbols  X  ,  X  , and  X  respectively denote statistically significant differences according to a single-tailed student t-test (p-value&lt;0.05) over the SepR, TopicalSVM and Over-weighting baselines.
 SepR 0.378 0.360 0.372 0.408 TopicalSVM 0.365 0.355 0.365 0.402 Over-weighting 0.340 0.348 0.363 0.404 CS-DAC 0.398  X  0.364 0.376 0.411 CS-DAC( U ) 0.416  X   X   X  0.379  X  0.388 0.400 SepR 0.348 0.411 0.434 0.475 TopicalSVM 0.355 0.408 0.430 0.485 Over-weighting 0.335 0.408 0.434 0.480 CS-DAC 0.427  X   X   X  0.454  X   X   X  0.473  X   X   X  0.510 X   X 
CS-DAC( U ) 0.452  X   X   X  0.466  X   X   X  0.488  X   X   X  0.527  X   X   X  used for validation and testing. The number of ranking functions (clusters) in CS-DAC and TopicalSVM to are set to three ( k = 3 ), since preliminary results demonstrate CS-DAC and TopicalSVM perform the best when k = 3 and k = 4 (slightly outperforms the case when k = 3 ) respectively.
 investigate the performance of baseline techniques when trained for one of four optimization goals: 1. Relevance (Rel): The baselines are trained using relevance 2. Freshness (Fre): The baselines are trained using freshness 3. Hybrid labels (Hyb): The baselines are trained using hybrid 4. Demoted labels (Dem): Dong et al. [12, 13] suggested de-
The final results of each optimized ranker are evaluated sepa-rately for freshness and relevance using NDCG with corresponding labels.

Figure 2 shows the performance of baseline techniques on the non-temporal query set (sampled from the MSN logs). As ex-pected, when evaluating using the relevance labels ( y R ), it is more effective to optimize for relevance (Rel) rather than freshness (Fre). Similarly, optimizing for freshness produces results that have bet-ter NDCF values. The methods optimized for demoted (Dem) and hybrid (Hyb) labels consistently outperform those that are opti-mized for either freshness or relevance. The results also suggest that our hybrid labels are better for improving both relevance and freshness compared to the demoted labels of Dong et al. [12, 13]. are the standard deviations of performance across five cross-validation folds. Among the baselines, SinR has overall the poorest performance which is consistent with previous observations [3]. TopicalSVM, over-weighting and SepR show similar effectiveness while the lat-ter might be considered marginally better X  X ot surprising given that we use correct query type information in SepR.

We repeat the analysis on the temporal query set and the results are illustrated in Figure 3; as in the previous experiment, SinR has the lowest performance on both sets of labels while the other meth-ods show similar effectiveness. Compared to the experiments on the non-temporal query set, there is less variation in performance when optimized for different types of labels. Our investigations re-vealed that this is due to high correlation between relevance and freshness labels on the temporal set. The Pearson X  X  correlation be-tween relevance and freshness labels on the temporal query set is 0 . 912  X  0 . 004 , statistically significantly higher than 0 . 429  X  0 . 021 for the non-temporal set.

Based on the summarized results, we choose hybrid labels for training rankers in our remaining experiments. We also drop SinR as it consistently showed inferior effectiveness compared to all other methods.
 with freshness y F labels (NDCF [13]) to compare the performance of CS-DAC with the baselines on both temporal (Google Trends) and non-temporal (MSN logs) query sets. We report the results for CS-DAC in the presence and absence of the query-document importance factor ( U ) described in Equations 3 and 5. We respec-tively refer to these two versions as CS-DAC( U ) and CS-DAC.
Table 4 includes the NDCF results on both query sets. The over-weighting baseline performs worst than the other methods. This is not surprising given that over-weighting is originally designed for scenarios with imbalanced training data [12], and the fact that it does not leverage any type of query classification or clustering. Consistent with the observations in the previous section, SepR and TopicalSVM produce similar results on the temporal queries, while they are both outperformed by CS-DAC. Introducing the U factor leads to further improvements in performance particularly at higher cutoffs. On non-temporal queries, TopicalSVM and SepR and over-weighting show similar effectiveness while CS-DAC consistently outperforms all baselines significantly. It is interesting to observe that CS-DAC improvements over the baselines are larger on the non-temporal query set. This can be explained by two reasons: (1) the documents returned for temporal queries tend to be fresher on average than those returned for the non-temporal ones, and (2) the high correlation between relevance and freshness labels in this set leads to more effective learning by reducing impact of potential noise in clustering and hybrid labels.
 analysis, and compare the NDCG values of different techniques as Table 5: Relevance comparison on the temporal (top) and non-temporal (bottom) query sets. All methods are trained using the hybrid labels and the evaluation is based on the freshness rat-ings (y R ). Symbols  X  ,  X  , and  X  respectively denote statistically significant differences according to a single-tailed student t-test (p-value&lt;0.05) over the SepR, TopicalSVM and Over-weighting baselines.
 SepR 0.373 0.359 0.375 0.411 TopicalSVM 0.342 0.354 0.365 0.408 Over-weighting 0.355 0.351 0.368 0.411 CS-DAC 0.385 0.365 0.377 0.417 CS-DAC( U ) 0.401  X  X  0.375 0.389 0.426  X  SepR 0.481 0.517 0.532 0.562 TopicalSVM 0.490 0.508 0.521 0.566 Over-weighting 0.476 0.510 0.538 0.570 CS-DAC 0.493 0.520 0.541 0.574
CS-DAC( U ) 0.509 0.522 0.541 0.574 measured by the relevance labels ( y R ) in Table 5. For non-temporal queries, the CS-DAC results are marginally better than the base-lines, although none of the differences are statistically significant. On the temporal query set, SepR has the edge over the other base-lines while CS-DAC outperforms the three of them at all cutoff values. Adding the U factor significantly improves the results for NDCG@1 and NDCG@10. As in the NDCF numbers on this query set, the NDCG values could be also affected by the high correlation between freshness and relevance.
We showed that our CS-DAC method could significantly im-prove both freshness and relevance of the results compared to state-of-the-art baselines. In this section, we investigate the impact of random walk smoothing in improving the query-document factor U for training. We also compare CS-DAC and the baselines in terms of hybrid NDCG by assigning various weights to relevance and freshness. Finally, we report the most effective features ac-cording to our experiments for ranking temporal and non-temporal queries.
 earlier how original query-document importance values can be smoothed by random walk, where the probability d of random jumping can be tuned during training and validation. Figure 4 shows how choosing different fixed values for d may affect the re-sults. On the non-temporal query set, different degrees of smooth-ing have little advantage over no smoothing ( d = 0 ). On the tem-poral query set however, random-walk helps to smooth inter-label dependencies, and hence improves the results on both freshness and relevance.
 training for hybrid NDCG (  X  = 0 . 5 ) was effective for improv-ing both freshness and relevance. Here, we provide the evalua-tion results on hybrid NDCG, the metric we used for optimizing the ensemble ranking. Although we used  X  = 0 . 5 for training, we report the evaluation results for different values of  X  in Fig-ure 5 to account for scenarios where freshness and relevance are Figure 4: The impact of changing the random jump proba-bility d during smoothing of the query-document importance values U . The results are evaluated on temporal (left) and non-temporal (right) queries using both relevance and freshness la-bels. weighted differently. The results are consistent with our previous experiments; CS-DAC outperforms the baselines, and the weight-ing between freshness and relevance is less important for temporal queries. Increasing the  X  value grows the overall hybrid NDCG almost monotonically because the relevance-based NDCG values are generally greater than those computed based on the freshness labels. It is worthwhile of pointing out that this observation does not suggest that ranking performance benefits the most when only optimizing for relevance.
 Feature Analysis. CS-DAC relies on several temporal and non-temporal features for query clustering and document ranking. We examined all cross-validation folds to find the features that are as-signed with highest weights during training. Among the temporal features, the confidence values for the seasonality CS(  X  ), and regu-larity Cr(  X  ) of STL decompositions were generally the most effec-tive. Furthermore, the features generated from the time-series de-composition of changes in anchor-text and inlinks were more suc-cessful than those similarly produced based on other fields (e.g., title, body, heading). The effectiveness of temporal link-based fea-tures for improving relevance and freshness has been also acknowl-edged previously by Dai and Davison [8].

Between the non-temporal features, BM25 and language model-ing scores had the highest weights and were most effective when computed over the body and title text. In this paper, we proposed a learning to rank approach (CS-DAC) for optimizing for relevance and freshness simultaneously. We extended the state-of-the-art in divide and conquer ranking [3] by adding two new key elements; first, instead of optimizing for relevance labels, we generated and used hybrid labels based on rel-evance and freshness grades. Second, we introduced a new query-document importance factor ( U ) that allows each ranker to set dif-ferent importance to relevance and freshness. Compared with tradi-tional metasearch engines, divide-and-conquer ranking frameworks generate merged ranking lists on the model level instead of the result level. It enables automatic identification of effective rank-ing features for individual type of queries. Our experiments on a large web archive demonstrated that CS-DAC can improve both relevance and freshness compared to existing baselines. In the fu-ture, we plan to compare with data fusion methods, i.e, training two separate rankers with each utilizing judgments based on one criterion (relevance or freshness), and then merging results into a single ranking list. We also plan to consider the document diversity Figure 5: Hybrid NDCG5 values for different values of  X  on the temporal (top) and non-temporal (bottom) query set. Similar trends were found for NDCG at different cutoff values. problem that is especially important for answering time-sensitive queries, e.g, a news-related query may have many news articles which may incur document duplication.

We studied the correlation between relevance and freshness grades, and its implications on the training effectiveness. Our re-sults revealed high correlation between relevance and freshness la-bels in temporal queries, suggesting that the choice of document labels is less important for training on that set. We modeled doc-ument importance by the likelihood of visiting each unique hybrid label, and surprisingly found that it can improve ranking perfor-mance, especially for temporal queries. However, in what way that such document weighting strategies influence ranking performance is still unclear. We will leave it as one of our future work.
Our work can be considered as the simplest form of multi-objective (multiple-criteria) optimization [26], where multiple ob-jective functions (freshness, relevance) are combined to form a sin-gle optimization goal (hybrid labels). These kinds of aggregated functions require the weight of each objective to be known in ad-vance (  X  in our case), and are incapable of finding all optimal so-lutions. Deploying more sophisticated multi-objective optimization techniques may lead to more significant improvements in relevance and freshness. Further work includes adopting other learning to rank architectures such as boosted decision trees [29] for multi-objective optimization of freshness and relevance.
 We thank the anonymous reviewers for their useful comments. We also thank Jiang Bian for valuable discussion on TopicalSVM model and Jingjing Liu for helpful comments on the judgments for ranking evaluation. This work was supported in part by a grant from the National Science Foundation under award IIS-0803605.
