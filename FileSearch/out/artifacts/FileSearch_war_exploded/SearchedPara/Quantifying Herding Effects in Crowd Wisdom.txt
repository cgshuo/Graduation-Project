 In many diverse settings, aggregated opinions of others play an increasingly dominant role in shaping individual decision making. One key prerequisite of harnessing the  X  X rowd wis-dom X  is the independency of individuals X  opinions, yet in real settings collective opinions are rarely simple aggregations of independent minds. Recent experimental studies document that disclosing prior collective opinions distorts individuals X  decision making as well as their perceptions of quality and value, highlighting a fundamental disconnect from current modeling efforts: How to model social influence and its im-pact on systems that are constantly evolving? In this paper, we develop a mechanistic framework to model social influ-ence of prior collective opinions (e.g., online product rat-ings) on subsequent individual decision making. We find our method successfully captures the dynamics of rating growth, helping us separate social influence bias from inherent val-ues. Using large-scale longitudinal customer rating datasets, we demonstrate that our model not only effectively assesses social influence bias, but also accurately predicts long-term cumulative growth of ratings solely based on early rating trajectories. We believe our framework will play an increas-ingly important role as our understanding of social processes deepens. It promotes strategies to untangle manipulations and social biases and provides insights towards a more reli-able and effective design of social platforms.
 H.2.8 [ Database Management ]: Database Applications X  Data mining ; J.4 [ Social and Behavior Sciences ]: Soci-ology Algorithms, Experimentation Crowd wisdom; social influence; herding effect
With the explosive growth of information, our decisions are increasingly relying on aggregated opinions contributed by others, with the belief that the aggregations over a large population can successfully harness the  X  X isdom of crowds X  [22]. Indeed, rooting back to Galton [8], many studies have shown that collective opinions of a group are often closer to the truth than the answer of an individual to a ques-tion. While the crowd wisdom applies usefully to a spectrum of domains, ranging from product or service recommenda-tion [10] and crowdsourcing [5, 20, 26] to stock markets and political elections [22], one key prerequisite of harnessing the crowd wisdom is the independency of individuals X  opin-ions [22]. Indeed, most if not all of the times, individuals are exposed to others X  opinions before forming and expressing their own. As concrete examples, we go to the theater after checking reviews of the movies online; we download songs from the hit list; we purchase products or go to restaurants after researching what others think about them. As a result, the market does not simply aggregate pre-existing individ-ual preferences, but rather creates an environment rich in social influence.

Thanks to the availability of Web-based experiments, re-cent studies offered convincing evidence that social influence exerts important but counterintuitive effects on collective judgement [16, 19]. Indeed, through carefully designed con-trol experiments in different settings, these studies demon-strate that disclosing prior collective opinions distorts indi-viduals X  decision making as well as their perceptions of qual-ity and value, creating herding effects that are irrational and pervasive, yet consequential to market outcome. Despite the significance of these results in experimental settings, there has been no quantitative framework to model social influence and its impact on systems that are constantly evolving. In-deed, models on collective intelligence, from majority voting to collaborating filtering to crowdsourcing [5], all assume in-dependent crowds, representing a critical gap between mod-eling frameworks and empirical insights.

Here we develop a mechanistic framework to model social influence of prior collective opinions (e.g., product ratings) on subsequent individual decisions, namely, H erding E ffect A ware R ating D ynamics Model ( Heard ). Using 28 million ratings spanning over 18 years on over 1.7 million products from Amazon [15] as an exemplary case, we demonstrate that our method successfully captures the dynamics of rat-ing growth across different product categories, allowing us to separate social biases introduced by prior ratings from the true values inherent to products. We further show that, comparing with competing methods, our framework not only effectively detects the presence of social biases and gauges less biased values for any given product, but also accurately predicts the long-term cumulative growth of ratings through a scalable estimation model solely based on early rating tra-jectories. As a result, Heard can also make testable pre-dictions of collective response to artificial manipulations in rating systems, assisting in further testings through more systematic experiments.

To the best of our knowledge, this work represents one of the first few quantitative framework to model social influ-ence biases introduced from prior opinions. We believe our method is of fundamental importance to studies of social processes, promotes new strategies in untangling manipula-tions and biases within social environments, and provides significant insights towards design of platforms that aggre-gate individual opinions, from electoral polling to market analysis to product recommendation.

The remainder of the paper will proceed as follows. Sec-tion 2 surveys relevant literature. Section 3 details the model design of Heard and develops efficient inference algorithms to fit the model. Section 4 presents a scalable algorithm to predict the future rating growth based on Heard . Section 5 empirically evaluates the proposed models and algorithms. The paper is concluded in Section 6.
In this section, we review three categories of related work, namely, social network induced influence, measuring social influence in experimental settings, and effect of semantics of prior opinions.

Social networks have attracted significant interest, partly due to the availability of large datasets in many domains. One active line of enquiry in social network studies is how be-havior [1, 2], opinion [7], and information [13, 24, 3] spreads through social networks. It is conceivable that microscopic social interactions could induce influence that is visible on an aggregated level [25]. In a way, the process of generating collective opinions is similar to consensus formation [11]. For example, individuals may change their opinions after learn-ing about what their friends think. This is supported by experimental results by Lorenz et al. [14], in which they demonstrated that even mild social interactions can signifi-cantly bias simple estimation tasks. Therefore, Das et al. [4] proposed a social sampling method that takes into account individuals X  influence from their social neighbors and arrives at a de-biased estimation of collective opinions. While this line of research shows that social interactions can exert influ-ence on overall outcome, their focus on networks inevitably distinguishes themselves from our work. Indeed, often times, the population responsible for collective opinions are not in-teractive. You choose a restaurant, go watch a movie, or purchase a book, because of the opinions or reviews authored by people you do not know. Therefore, our work focuses on how to model social influence on a macroscopic level and hence predict the outcome of crowd wisdom.

On the other hand, there have been a number of experi-mental studies on measuring social influence within a popu-lation, thanks to the emergence of Web-based experiments. For example, Salganik et al. [19] implemented a music lab, where individuals download and rate songs with or with-out information about how good the songs are, and they demonstrated that increasing social influence could result in differential outcomes for songs of similar quality. Muchnik et al. [16] ran a large-scale randomized experiment on a red-dit like website, finding that prior ratings created significant bias in individual rating behavior, from turnout to binary choices. These studies confirmed experimentally that dis-closing prior ratings can create strong herding effects that are irrational and pervasive, leading to significant bias that is consequential to collective outcome. At the same time, they also highlight a fundamental gap between experimental insights and modeling efforts. Our work directly addresses this gap: To the best of our knowledge, this work is among the first few attempt to quantitatively model the herding effects in crowd wisdom and develop effective mechanisms to factor out such bias in estimation.

Finally, there have been a number of interesting studies into the semantics of collective opinions, such as that ana-lyze the text and social aspects of product reviews [10, 15, 21, 9]. While they are useful for review spam detection, customer sentiment analysis, product recommendation, and more, insights extracted from semantic features are, how-ever, not mechanistic, hence not capable of projecting the full rating trajectories. Nevertheless, these studies are com-plementary to our work, in a sense that the useful semantic features learned can be integrated into our model in forms of prior belief of model parameters. Indeed, one shall see in next sections that incorporating such text and social infor-mation into the rating growth model would be a promising future direction.

Our work also draws connections to other modeling ef-forts. The design of our herding effects model is inspired by the multi-neuron coupled spiking phenomena [17]. The ex-ponential additive generative mechanism has been applied proposing a more general form of generative model and de-veloping scalable inference algorithms to fit the model.
In this section, we detail the design of the Heard model and present efficient inference algorithms to fit the model. Concretely, we draw an analogy to the coupled spiking phe-nomena in a multi-neuron system to model the dynamics of rating growth and fit the model parameters using maximum likelihood estimation.
Without loss of generality, we consider a discrete K -level rating system, which is extensively used by today X  X  online retailers; for example, Amazon adopts a one-to-five star rat-ing system. Consider the sequence of ratings regarding a specific product, with r i  X  { 1 , 2 ,...,K } being the i -th rat-ing. We assume the first ( i  X  1) ratings form the history proportion of level-k ratings among the first ( i  X  1) ratings. Clearly, P K k =1 x i,k = 1 for i &gt; 1 and x 1 is an all-zero vec-tor. We intend to model how disclosing such rating history would influence individual rating behavior on r i .
Intuitively, the generation of a new level-k rating is driven by multiple factors, including: the intrinsic product quality, the occurrence of preceding level-k ratings, and the history of other ratings. We can draw a close analogy to the spiking activities of a multi-neuron system [17]: the response (i.e., spike) generated by a neuron is jointly determined by the stimulus strength and the preceding spikes of this neuron Figure 1: Illustration of HEARD model. The occur-rence of a new level-k rating is jointly influenced by (i) the intrinsic quality of product, (ii) the preceding level-k ratings, and (iii) the history of other ratings. and correlated neurons. We therefore introduce an additive generative model to describe the distribution of the i -th rat-ing r i over different levels:
This conditional distribution describes the likelihood of observing a level-k rating given rating history x i general formulation, we have:
These factors are then integrated in an exponential func-tion, as illustrated in Figure 1.

Note that here we ignore the time dimension in our model because various external factors may abruptly influence the temporal dynamics of rating growth, e.g., low price promo-tion, emergence of new products, advertisements, etc. Let  X  = [  X  1 ,  X  2 ,...,  X  K , u ] represent all the parameters. Both  X  and magnitude function f (  X  ) are estimated from data; in particular, f (  X  ) is estimated from an infinite dimen-sional functional space. Next we elaborate their inference.
We assume regarding a specific product, a temporally or-dered sequence of N ratings { r i } N i =1 has been observed. Note that while we focus on the case of a single product for ease of presentation, the extension to multiple products is straight-forward. For notational simplicity, we introduce a set of indicator variables y i  X  X  0 , 1 } K with y i,k = 1 if r ( i )
In the following, we use f i as a short notation of f ( i ). 0 otherwise. Then the log-likelihood of parameters  X  given this rating sequence is expressed as: L (  X  ) = 1
We estimate the model parameters by minimizing the pe-nalized log-likelihood function, which is defined as: where the first term represents the negative log-likelihood, the second term is a regularizer with  X  being the balance pa-rameter to prevent overfitting, and || X || F denotes the matrix Frobenius norm. In particular, R ( f ) is a penalty term pre-ferring smooth functions. Without prior knowledge, we use of f (  X  ).

While L  X  (  X  ) appears similar to the softmax regression; it contains the integral of an unknown function and meanwhile all the parameters are coupled, which makes it difficult to directly apply off-the-shelf optimization methods (e.g., co-ordinate descent). Next we propose an iterative algorithm which optimizes L  X  (  X  ) by (i) constructing a surrogate func-tion to decouple the parameters and (ii) applying an Euler-Lagrange equation to fit the unknown function.

More specifically, let  X  ( n ) = [  X  ( n ) 1 ,  X  ( n ) 2 ,...,  X  note the current parameter setting. We construct the follow-bound of L  X  (  X  ): Q (  X  ;  X  ( n ) ) = 1
It is noted that Q (  X  ;  X  ( n ) ) possesses the following desir-able properties (details in Appendix): which imply that if  X  ( n +1) = arg min  X  Q (  X  ;  X  ( n ) must have L  X  (  X  ( n ) )  X  L  X  (  X  ( n +1) ). Therefore, minimizing Q (  X  ;  X  ( n ) ) with respect to  X  at each iteration will ensure that L  X  (  X  ) decreases monotonically.
 The formulation above has the advantage that we can de-rive the closed form solution of  X  for arg min  X  Q (  X  ;  X  Specifically, by deriving the derivatives of Q (  X  ;  X  ( n ) respect to  X  k and  X  k,k 0 and set them to zero, we obtain their update rules as follows:  X  Next we derive the update rule for magnitude function f (  X  ) by optimizing it in an infinite dimensional functional space. reformulate the problem of minimizing Q (  X  ;  X  ( n ) ) with re-spect to f (  X  ) as follows: where terms A i and B i are defined below: Abusing the notation a little, we introduce two functions: A ( t ) = A t I { t  X  N  X  t  X  N } and B ( t ) = B t I { t  X  N  X  t  X  where I { X } is the indicator function which returns 1 if the predicate is true and 0 otherwise.
 Then the solution of the objective function as defined in Eqn.(6) must satisfy the Euler-Lagrange equation [27] (proof referred to Appendix): where g 00 (  X  ) is the second order derivate of g (  X  ).
Due to the discrete nature of the functions A ( t ) and B ( t ), we solve this differential equation numerically using a Sei-dal type iteration. Specifically, we discretize the differential equations over intervals of length 1:
Clearly from the equations above we can efficiently solve f for i = 1 , 2 ,...,N . We may then perform curve fitting to extrapolate the values of f i for i &gt; N .
 To set a proper starting point for optimization, we consider the degenerated case where the prior ratings have no effect on individual rating behaviors. Under this assumption, we have the following setting: Meanwhile we initialize  X  1 ,  X  2 ,...,  X  K randomly.
Putting everything together, Algorithm 1 sketches the procedure of model inference. After initialization, it iter-ates between updating parameters  X  and solving magnitude function f (  X  ) until the objective function converges. Algorithm 1: Inference of HEARD Model Input : rating history { r i } N i =1
Output : setting of parameters  X  and function f // initialization initialize  X  and f according to Eqn.(8); compute statistics { x i } N i =1 ; // iterative optimization while not converged yet do return setting of  X  and f ;
In this section we show that equipped with the aforemen-tioned Heard model, we are able to answer a set of funda-mental questions, including: Debiasing: What is the intrinsic quality of a product af-Prediction: Based on its rating history, can we predict the What-If Analysis: Given its current ratings, how would Next we detail how to answer these questions.
To the first question, recall that the Heard model defined in Eqn.(1) comprises two additive components, namely, the intrinsic distribution and the herding effect distributions. The background intrinsic distribution as controlled by pa-rameters {  X  k } is assumed related to the true quality of a product. Therefore, once we have estimated {  X  k } from the rating history of a product, we can then  X  X ebias X  the collec-tive ratings by factoring out the components attributed to the herding effects.

More concretely, abusing the notation a little, let  X  = [  X  1 , X  2 ,..., X  K ] &gt; . Without the herding effects, each rating is generated by the following unconditional categorical dis-tribution: which represents the intrinsic rating of the product.
The straightforward solution to estimating  X  of a given product is to directly fit the model parameters using its rat-ing history as in Algorithm 1, which however may lead to overfitting. Instead, we introduce an  X  X ut-of-sample X  exten-often follow similar patterns for products of the same cate-gory (e.g., books). We therefore use the rating histories of a bulk of products in the same category to train category-level parameters {  X  k } and magnitude function f (  X  ). For the query product, we fix {  X  k } and f (  X  ) and focus on learning product-level parameter  X  . As shown in Algorithm 2, this procedure is similar to Algorithm 1, except that at each iteration we only need to update  X  .
 Algorithm 2: Out of Sample Extension
Input : rating history { r i } i of query product, setting of
Output : setting of  X  for given product // initialization initialize  X  according to Eqn.(8); compute statistics { x i } i ; // iterative optimization while not converged yet do return  X  ;
Another interesting question one may pose is: given the current rating history of a product, is it possible to predict the distribution of it future ratings? While it is of theoretical interest to discuss the statistical convergence properties of the rating distribution as the number of ratings approaches infinity; in real settings, most products during their lifetimes receive only limited number of ratings. We thus focus on a more concrete question as follows:
Given the first N ratings of a product, can we characterize the distribution of its next M ratings?
Let us first consider the herding effects-agnostic case, in which each rating is independently generated by the categor-ical distribution as defined in Eqn.(9). Under this assump-tion, the next M ratings follow a multinomial distribution; specifically, the expected number of level-k rating is given by M X  k with variance M X  k (1  X   X  k ).

Next we incorporate the herding effects. Recall that the distribution of the first ( i  X  1) ratings is given by x i also corresponds to the history for the i -th rating. The tran-sition probability from x i to x i +1 can be described as below: where e k is a 1-of-K vector with the k -th element being 1.
Note that this transition rule essentially specifies a non-stationary Markov chain in which both the state space and the transition probability change from step to step. This setting is not amenable to exact inference; we thus resort to Monte Carlo methods [18].

Algorithm 3 sketches our prediction model. Starting with current rating distribution x N +1 estimated from the given rating history, it iteratively samples the next rating distribu-tion using the transition rule in Eqn.(10). Let { x ( i ) be the set of samples of target distribution x N + M +1  X  x distribution. We can prove that for given thresholds and  X  , if the sample size L satisfies the following condition: then |  X  x N + M +1  X  x N + M +1 | X  1 with probability at least 1  X   X  , where 1 denotes a K -dimensional all-ones vector (details given in Appendix).

It is also noted that Algorithm 3 features the complexity of O 1 2 log( 1  X  ) MK , thereby scaling up to large M . Algorithm 3: Prediction of Rating Growth
Input : rating history { r i } N i =1 , prediction range M ,
Output : estimation of rating distribution x N + M +1 // initialization estimate  X  and f by Algorithm 1; compute required sample size L by Eqn.(11) ; // random sampling for i = 1 , 2 ,...L do
The Markovian nature of the Heard model also enables us to perform the  X  X hat-if X  analysis. Concretely, given the current rating distribution x i , one may arbitrarily change x to another distribution x 0 i to reflect any artificial conditions one wishes to  X  X nject X  in (e.g., a burst of 50 five-star ratings due to certain promotion campaigns). Staring from this new state x 0 i and applying the prediction method above, one may then gauge the consequences of the injected conditions by predicting the trends of future rating growth.

Such what-if analysis is especially valuable for a range of applications including market profitability estimation, bud-geted advertising, and fraudulent manipulation detection.
In this section we present an empirical evaluation on the efficacy of the proposed models and algorithms.
We start with introducing the datasets and the alternative techniques to be evaluated.
 We evaluate different models using the real customer rat-ing data collected from Amazon , which spans a period of approximately 18 years, including around 35 million ratings regarding about 2.4 million products [15]. In particular, we
Average Perplexity 0 0 10 20 30
Difference of Avg. Ratings 0 0.1 0.2 0.3 0.4 0.5  X 0.5 0.5 1.5 2.5 f(n) focus on the products in four major categories: Books , Mu-sic , Movies &amp; TV , and Electronics , which cover over 72% of the total number of products in the collection. The statistics of this rating dataset is summarized in Table 1. It is noticed that these four categories demonstrate fairly diverse charac-teristics, for example, with average rating entropy ranging from 0.56 to 0.96.
 For comparison purposes, besides the Heard model, we im-plemented two additional rating growth models:
We implemented all the models and associated algorithms in Matlab and conducted the experiments on a Linux box running 3.5GHz Intel i7 CPU and 16GB RAM. The default parameter setting is:  X  = 1,  X  = 0 . 05, and = 0 . 01.
In this first set of experiments, we intend to evaluate the validity of different rating growth models. For each product in the dataset, we partition its temporally ordered sequence of ratings into two subsequences as the training (i.e., rating history) and testing parts respectively. We use the rating history to train the rating growth models and let them pre-dict the  X  X uture X  ratings in the testing set. We compare their accuracy in both short-term and long-term prediction. In short-term prediction, we vary the length of rating history (as the proportion of the entire rating sequence of a product) and measure the average perplexity of the prediction of the next 50 ratings by different models.

The results are shown in Figure 2. It is noticed that across all four product categories, Heard and Heard c outperform Img in terms of prediction accuracy. In particular, when only limited data (e.g., 30%) is available for training, the accu-racy of Img can be arbitrarily bad. This is attributed to the fact that the prediction of Img relies on the overall statistics of the rating history of each product, which however has not emerged yet at this early stage. In contrast, Heard leverages the rating histories of all the products in the same category to fit the model, thereby achieving high accuracy even when facing limited training data. This desirable property makes Heard especially valuable for early-stage prediction, as we will discuss shortly.
 It is also noticed that Heard achieves higher accuracy than Heard c but with marginally larger variance. This is con-sistent with the fact that Heard adopts a more complicated model than Heard c , which enables Heard to model a wider range of herding effects but at the cost of slightly higher variance. Figure 5: Heat maps of parameters {  X  k,k 0 } for each product category.
 In long-term prediction, we select the products with at least 500 ratings and fix the length of rating history (for training) as 200. We then apply each model to predicting the rat-ing distribution after the next M ratings ( M is referred to as the prediction range ). The accuracy is measured by the difference between predicted and actual average ratings.
The performance of different models is illustrated in Fig-ure 3, wherein we vary prediction range M from 100 to 300. It is observed that compared with Img and Heard c , the prediction accuracy of Heard is much less sensitive to the setting of M . This can be explained as follows. First, the prediction of Img depends on the simple statistics (i.e., frac-tion of ratings at different levels), which however may fluc-tuate significantly over a large time span. Second, as M increases, the change of the strength of herding effects can no longer be ignored as Heard c does.

We can thus conclude that Heard achieves reliable accu-racy in both short-term and long-term prediction tasks, im-plying that Heard faithfully captures the growth dynamics of product ratings.
Next, equipped with the Heard model as the analytical tool, we conduct a quantitative study on the herding effects observable in real customer rating data. More concretely, for each product category, we apply Algorithm 1 to fitting the model and examine the herding effects as characterized by the estimated magnitude function f (  X  ) and category-level parameters {  X  k } k .
 Recall that f ( n ) specifies the strength of herding effects as a function of the number of historical ratings n . Figure 4 illus-trates the estimated f ( n ) for each product category. We fur-ther apply curve fitting to f ( n ) with an exponential model a  X  exp ( b  X  n )  X  1 ( a and b are parameters). Interestingly, the magnitude functions in all four categories tightly follow the 0 0.2 0.4 0.6 0.8 1 exponential curves, despite their slightly different parameter settings of a and b .

This finding entails multi-fold implications: First, it con-firms our intuition that the strength of herding effects evolves with the cumulative number of historical ratings. Second, it also echoes the results documented by existing experimental studies (e.g., [19]) on the nonlinear relationship between the predicability of individual behaviors and external influence. Third, most importantly, it provides a formula to explicitly quantify the strength of herding effects. For example, com-paring the curves for the categories of Books and Movies &amp; TV , it is observed that the herding effects is stronger in the category of Movies &amp; TV , that is, customers are more easily to be influenced by prior ratings when purchasing Movies &amp; TV products. Such information can be valuable for applica-tions such as targeted advertising.
 Now we proceed to examining parameters {  X  k } . Recall that these parameters dictate the mutual influence between the ratings at different levels, concretely, with  X  k,k 0 how preceding level-k 0 ratings may positively excite or neg-atively inhibit the generation of level-k ratings.
Figure 5 illustrates the heat maps of {  X  k } estimated for each product category. While each category has its unique traits, certain common patterns are observed. First, high ratings (e.g., five-star ratings) tend to stimulate new high ratings while inhibiting the generation of low ratings. Sec-ond, high ratings are more impactful than low ratings in influencing other ratings. These observations are consistent with the finding of the asymmetric herding effects of positive and negative prior opinions as reported in [16].
As discussed in Section 4, equipped with Heard , we are able to perform various analytical tasks. In this set of ex-periments, we showcase how Heard helps answer two fun-damental questions: (i) exposing the rating inherent to the quality of a product (i.e.,  X  X ntrinsic rating X ) by factoring out the herding effects from collective ratings, and (ii) perform-ing predicative, what-if analysis by incorporating artificial conditions into the rating growth dynamics model.
 To understand the issue that the simple aggregated (or ex-ternal) rating of a product deviates from its true quality, we apply Heard to estimate the intrinsic ratings as in Sec-Figure 7: Two sample products with similar intrinsic ratings but with different rating growth histories, leading to significantly distinct external ratings. tion 4.1 and then measure for each product the difference between its intrinsic and external average ratings.
Figure 6 shows the cumulative proportion of products with respect to the difference between intrinsic and external rat-ings in each category. It is observed that in all the cases, over 50% products have their external ratings deviate at least 0.5 from their intrinsic ratings, which is significant considering that Amazon uses a five-level rating system.

Endowed with the capability of exposing the intrinsic rat-ing of a product, we can then compare the true quality of two products without being misguided by their external ratings. Figure 7 showcases such an example, in which the dynam-ics of the average external ratings of two sample products is depicted. Despite that they differ significantly in their ex-ternal ratings (about 0.9), their intrinsic ratings are indeed fairly similar as shown in the right plot. This is explained by that sample product 2 experiences a sequence of low ratings at the early stage of its history, which considerably changes the dynamics of its rating growth. With the help of Heard however, we are able to maximally debias this type of influ-ence caused by the herding effects.
 As introduced in Section 4.3, the Markovian nature of the Heard model enables us to perform predicative,  X  X hat-if X  analysis by artificially incorporating desired conditions into the prediction model and analyze the consequences using simulation. For example, before deciding whether to invest in a promotion campaign for a product, market analysts may Figure 8: What-if analysis by incorporating artificial conditions into prediction model. wish to estimate the long-term influence of the burst of high ratings due to the promotion.

Figure 8 shows one concrete example. We pick two sample products respectively from the categories of Movies &amp; TV and Books , which have fairly close average ratings thus far. Now, assuming the promotion takes effect, we inject 50 five-star ratings into their rating histories. As shown in the right panel of Figure 8, the prediction by Heard tells us: while both products experience similar short-term bursts in their popularity, in the long run the promotion has much longer-lasting influence on the sample product from the category of Books . It is clear that this provides valuable information for the decision making of market analysts.
In the last set of experiments, we evaluate the scalability of
Heard . Specifically, for model inference, we measure the average execution time per product by Heard under vary-ing length of rating history (for training); meanwhile, for future rating prediction, we measure its average execution time under varying setting of prediction range.

The results are depicted in Figure 9. It is observed that the execution time of Heard grows approximately linearly with the length of rating history and the range of prediction. This also confirms our theoretical analysis on the complexity of Algorithm 1 and Algorithm 3. We can thus conclude that Heard scales up to large rating datasets.
This paper presented a quantitative framework to gauge the herding effects in collective opinions of individuals. We proposed Heard , a mechanistic modeling framework for the growth dynamics of online product ratings, which explicitly accounts for the herding effects of prior customer opinions. Using massive customer rating datasets, we demonstrated the efficacy of Heard in capturing the dynamics of rating growth, quantifying social influence and debiasing collective ratings, and further performing what-if analysis against arti-ficial manipulations. Heard is not limited to product rating systems. Indeed, the mechanistic nature of Heard makes it applicable for modeling the herding effects in other domains where social influence plays a role, from crowdsourcing and recommender systems to electoral polling.

This work also opens up several directions for future in-vestigations. For example, recent work has shown that the temporal dynamics of collective response to a publication follows rather reproducible patterns, as citations can be cap-tured by a mechanistic model [23]. Hence, incorporating the temporal dynamics in the rating growth model can be fruit-Figure 9: Average execution time per product by HEARD in model inference and rating prediction. ful and could potentially shed new light on the nature of crowd wisdom. Furthermore, our framework is orthogonal to studies on the text and social aspects of product reviews and collective opinions, suggesting a rather promising direc-tion by combining the two approaches. Lastly, the model makes falsifiable predictions for collective response against artificial manipulations, making it a viable candidate to as-sess and guide experimental studies, results of which could feed back to and improve the model with more accurate and realistic predictions.
 This research was sponsored by the U.S. Army Research Laboratory and the U.K. Ministry of Defense and was ac-complished under Agreement Number W911NF-06-3-0001. The views and conclusions contained in this document are those of the authors and should not be interpreted as rep-resenting the official policies, either expressed or implied, of the U.S. Army Research Laboratory, the U.S. Government, the U.K. Ministry of Defense or the U.K. Government. The U.S. and U.K. Governments are authorized to reproduce and distribute reprints for Government purposes notwithstand-ing any copyright notation hereon. The authors would like to thank J. McAuley and J. Leskovec for sharing the Ama-zon review dataset. [1] N. A. Christakis and J. H. Fowler. The spread of [2] N. A. Christakis and J. H. Fowler. The collective [3] P. Cui, S. Jin, L. Yu, F. Wang, W. Zhu, and S. Yang. [4] A. Das, S. Gollapudi, R. Panigrahy, and M. Salek. [5] A. P. Dawid and A. M. Skene. Maximum likelihood [6] J. Eisenstein, A. Ahmed, and E. P. Xing. Sparse [7] J. H. Fowler, N. A. Christakis, Steptoe, and D. Roux. [8] F. Galton. Vox populi. Nature , 75(7):450, 1907. [9] G. Ganu, N. Elhadad, and A. Marian. Beyond the [10] M. Hu and B. Liu. Mining and summarizing customer [11] S. Judd, M. Kearns, and Y. Vorobeychik. Behavioral [12] B. Krishnapuram, L. Carin, M. A. T. Figueiredo, and [13] D. Liben-Nowell and J. Kleinberg. Tracing [14] J. Lorenz, H. Rauhut, F. Schweitzer, and D. Helbing. [15] J. McAuley and J. Leskovec. Hidden factors and [16] L. Muchnik, S. Aral, and S. J. Taylor. Social influence [17] J. W. Pillow, J. Shlens, L. Paninski, A. Sher, A. M. [18] C. P. Robert and G. Casella. Monte Carlo Statistical [19] M. J. Salganik, P. S. Dodds, and D. J. Watts. [20] V. S. Sheng, F. Provost, and P. G. Ipeirotis. Get [21] H. Sun, A. Morales, and X. Yan. Synthetic review [22] J. Surowiecki. The Wisdom of Crowds . Anchor, 2005. [23] D. Wang, C. Song, and A.-L. Barab  X asi. Quantifying [24] D. Wang, Z. Wen, H. Tong, C.-Y. Lin, C. Song, and [25] T. Wang, M. Srivatsa, D. Agrawal, and L. Liu. [26] D. Zhou, J. C. Platt, S. Basu, and Y. Mao. Learning [27] K. Zhou, H. Zha, and L. Song. Learning triggering We first prove that the objective function L  X  (  X  ) as defined in Eqn.(2) and its surrogate function Q (  X  ;  X  ( n ) ) as defined in Eqn.(3) satisfy the following relationships: First, according to the definition of Eqn.(1), we have:
We focus on the log-sum-exponential term log P k exp(  X  i,k and apply the following quadratic upper bound [12]: for any vectors u  X  R K and v  X  R K , log X In our context, we replace the log-sum-exponential term of L (  X  ) with its upper bound and substitute u k with  X  i,k and v with  X  ( n ) i,k , which then leads to the result of Q (  X  ;  X  L (  X  ).
 upper bound above is exact when u = v .
 To derive Eqn.(7), it is first noticed that the optimization problem in Eqn.(6) can be rewritten as: where F ( f,f 0 ) is defined by:
F ( f,f 0 ) = A t I { t  X  N } f ( t ) 2 + B t I { t  X  N } f ( t ) +
According to Euler-Lagrange equation, the solution of this problem satisfies the following differential equation:
By substituting F with the definition above, we get the differential equation in Eqn.(7).
 Here we derive the number of samples required for the given thresholds and  X  as in Eqn.(11). Without loss of generality, consider the k -th element of x N + M +1 , x N + M +1 ,k . Following the Hoeffding X  X  inequality, we have:
Notice that  X  x N + M +1 is an unbiased estimator of x N + M +1 of the inequality above to  X  , we obtain Eqn.(11).
