 Search advertising shows trends of vertical extension. Ver-tical ads, including product ads and local search ads, are proliferating at an ever increasing pace. They typically offer better ROI to advertisers as a result of better user engage-ment. However, campaigns and bids in vertical ads are not set at the keyword level. As a result, the matching between user query and ads suffers low recall rate and the match qual-ity is heavily impacted by tail queries. In this paper, we pro-pose an ad retrieval framework for retail vertical ads, based on query rewrite using personal history data to improve ad recall rate. To insure ad quality, we also present a relevance model for matching rewritten queries with user search in-tent, with a particular focus on tail queries. In addition, we designed and implemented a GPU-based system to acceler-ate the training of the relevance model to meet production performance constraints. Finally, we carry out extensive ex-periments on large-scale logs collected from Bing, and show significant gains in ad retrieval rate without compromising ad quality.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.7.0 [ Computing Methodology ]: Document and Text Processing X  General Algorithms; Design; Experimentation ad retrieval; personalization; tail queries; GPU
Online advertising is a multi-billion dollar business, and the market sees continuing double digit growth since its in-ception. Search engine companies are the dominant player in this business. They typically display sponsored ad listings on the top or side of a search engine result page (SERP), in response to a user query. A common revenue model for on-line advertising is  X  X ost per click X  where the advertiser pays only if the advertisement is clicked. So the expected rev-enue is proportional to the total number of user clicks and payment per click for each click, which is given as, where impression is the number of ads shown to the user, click-through rate (CTR) is the average ad clicks per ad display, and CPC is cost per click for the advertiser.
Recent years have seen rapid growth of retail vertical ads, and an increasing number of advertisers start to adapt to this emerging campaign paradigm. Retail ads usually pro-vide richer information than traditional text ads, which in-clude specific product information like an image, title, price, promotional message, and store or business name etc. Among all types of retail ads, product ads attract most attention and more importantly they are comparable with traditional text ads in terms of revenue generation. In this paper, we will focus our discussion on product ads, even though the techniques described are also generalizable to solve the low recall challenge for other type of retail vertical ads.
Figure 1 shows an example of product ads. Comparing with text ads, product ads have shown significant better user engagement and advertiser satisfaction. As shown in Figure 1, search users can see the exact products the advertisers offer before they even reach advertiser X  X  sites, which leads to more clicks and higher Return Of Investment(ROI) for advertisers.

Different from the traditional text ads, product ads re-quire no keyword match (this is true for other types of retail vertical ads as well). Whenever a user enters a search query with commercial intent, the search engine retrieves relevant items from the ad inventory. This makes it very easy for ad-vertisers to promote their entire product inventory. On the other hand, search engines are having much greater respon-sibility on ad relevance and recall: it is a search task on its own in the product inventory. A key challenge for vertical Figure 1: An example of product ads from Bing.com, as highlighted in the red dash box. Search query:  X  X ucci guilty X . ads is to match high quality ads with user intent (e.g. prod-uct in mind to buy), while maximizing ad recall. The current pressing issue is low recall rate and hence low auction den-sity due to the limited number of listings. Moreover, we have observed that ads for specific commercial queries outperform general queries often for product ads. An example is that ads served to query  X  X alloween costume X  outperforms  X  X os-tume X  overall in terms of CTR. Intuitively, a more specific product query leads to more accurate product ads presented by the search engine. These queries are often less frequently searched due to their specificity (i.e. tail queries). Their ads inventory are also sparse. How to effectively retrieve relevant ads for these tail queries is another challenge.
In this paper, we propose an ad retrieval framework based on query rewrite using personal data. This work addresses the issues of low recall rate in product ad retrieval through user intent matching from queries. The idea is to retrieve ads using both the original query and multiple of its rewrites. The system would only increase recall because the original query is always used for ad retrieval. The rewritten queries are generated from user history data in the hope that such personal data would reveal some of the user preferences. Intuitively the system would work in the following scenarios. 1) Specify queries with user preferred brand or merchant, for example, watches -&gt; watches louis vuitton , watches amazon . The production optimized ads system does not show ads for queries with low commerce intent, such as  X  X atches X , to ensure quality. This rewrite will increase ad recall and at the same time improve relevance. 2) Build connections between user query and purchasing intent. For instance, rewriting costume to halloween costume would help user narrow down their search, assuming halloween is in the search history. 3) Identify the right term for ad engine. For example, typo correction: mackbook pro -&gt; macbook pro .

The key challenge comes from how to identify the similar-ity between the current search query and user histories to be used for rewrite. While lexicon based similarity functions, such as cosine similarity using n-gram features, are easy to implement and can detect similar queries that comprise a subset of same tokens/words, the method is not robust (to stemming, misspelling and etc.). In addition, they cannot match semantically similar queries with different word com-positions, such as apple music player and ipod. Further-more, identifying relevance of rare queries is hard.
We propose a hybrid relevance model to tackle the above problems. At query level, we construct a similarity function based on successive search queries within a single search ses-sion across all the users. To better understand tail queries, the second layer of the model breaks down queries into words, and matches words in the product category space using product click-through data.
To the best of our knowledge, this is the first work propos-ing a novel ad retrieval system and reporting extensive re-sults in the scope of retail vertical ads. Another fundamen-tal innovation comes from how we use personal data. Unlike most of other personalization works, our usage of personal data aims primarily at increasing ad recall, rather than pre-cision. The major contributions of our work are listed as follows.
The rest of this paper is organized as follows. In Section 2, we describe the general framework of our ad retrieval sys-tem. We then, in Section 3, discuss the relevance model in detail, which we believe is the most important part of the overall system. In Section 4, we present a solution to resolve a practical concern of training large scale complex model efficiently, using GPU. Extensive experiment results are reported in Section 5. Section 6 presents related works and Section 7 concludes the paper and points out the limi-tations.
In this section, we give a high-level overview of the ad retrieval system. Figure 2 shows a diagram of the system architecture, where the new components for query rewriting are drawn within the dashed box.

In the current production system, original queries are di-rectly sent to the ad index, and it is up to the search engine to (1) retrieve ads with keywords matching the query; (2) estimate relevance scores for the retrieved ads; (3) return ads with the top relevance scores.

Our ad retrieval system feeds both the original query and multiple of its rewrites to the ad index which ultimately returns product ads to display, in the hope of boosting ad recall. All the rewrites are generated from user histories to capture user preferences and personalize ads. To ensure the relevance of the ads, all query candidates would have to pass through a commerce intent classifier, and the rewritten queries must meet the relevance requirement with regards to the original query according to our relevance model. Then, selected candidates are ranked according to the commerce intent score and relevance measure score, and top queries are selected. Finally the selected rewritten queries and the original one are sent to the ad index to retrieve ads.
This framework requires minimal modification to the cur-rent online production system. The procedure is largely iso-lated from the existing indexing and ranking routines. At the same time, the scheme is arguably the most effective and intuitive one to follow. It tackles the problem from the beginning of the work flow, and naturally acts as a personal assistant of the user to help specify, expand and formulate intent during the search session.

We here describe the procedure that rewritten queries are generated and how they are filtered (by commerce classifier) and ranked before hitting the ad index. We will describe the relevance model itself in greater detail in Section 3.
We generate personalized rewritten queries from two ma-jor user histories: search query history and browsing history from Microsoft Bing. We believe there are other sources of personal data, such as cookies and user profile. However, we will try them in the future. We call the incoming query to be personalized current query or original query, and the past query from search history the history query or the organic history query.

Query history log records the interaction behavior from a set of different users, U = { u 1 ,u 2 ,...,u N } . It stores a sequence of queries Q i = { q it 1 ,q it 2 ,...,q it M } from user u where t m  X  T u i are timestamps when the query is submitted. Similarly, browsing history log records data from the same set of user U , and maintains a list of user browsing urls URL i = { url it 1 ,url it 2 ...url it K } with timestamp t
We further segment user history into sessions. The con-cept of session has been extensively used in search/IR re-search[1]. The idea is to group together queries/url with the same user intent. This is a very rough clustering. However, studies have shown that it provides strong hints of whether queries are related by simply exploring temporal distances between queries. We use this as one of the features in the scoring and ranking function. Specifically, session is defined as follows.

Definition of Session: Given user u i  X  X  history Q i ( URL and a fixed timeout interval  X  , a session S it is a set of con-secutive queries from Q i (urls from URL i ) starting at time t , such that  X  consecutive q ij and q ik , | t ij  X  t ik
The definition implies that sessions { S it } T 0 is a set of dis-joint partition of Q i ( URL i ). Typically timeout interval is set to be 30 minutes.

In the following, we describe how to generate query rewrites using the history log data.
History search queries up to one week before the current query are used to initialize the rewritten candidate pool. All the queries go through a spam filter that screen out queries with irregular patterns/robot queries and are normalized be-fore further processing. Both organic history search query and simple expansions of current query based on history search queries are included. The expansion rule is described as follows. For each current and history query pair, first, we break down the normalized queries into words/phrases; in most cases, tokens are just words separated by spaces. Then, we find a set of tokens that appear in history query but not in current query. Finally all tokens in this difference set are appended to original query to form the expanded candidate.

The two types of candidates complement each other in certain ways. Organic candidates explore the user preference with no constrains, in the hope to find conceptually related queries expressed in different words that match matter to the ad index. While expanded candidates are supposed to enrich the current query with additional user preference such as merchant or brand. As we will discuss later, both candidates need to pass a relevance threshold with the current query so that we don X  X  recommend Nike ads when user is looking for computers.
Browsing history is used as another source of personal-ization signal. We use the user clicked urls of organic page views from Bing. Different from query history, urls can not be directed used as rewritten query. So we apply the fol-lowing Query Explorer Platform (QEP) service to translate urls to queries.

Query Explorer Platform : QEP is the entire query/url click graph of Bing over the past 18 months, updated daily. It is segmentable by month, current week, and current day. All data is stored in RAM, distributed across over 100 ma-chines, and is queryable at very reasonable speed.

We translate each url into queries that lead to the most clicks of the url. Top 3 queries are picked for each url. Af-ter translating into queries, we apply the same technique as for history search queries to generate browsing history candidates.
Commerce intent classifiers use machine learning to score a query X  X  likelihood of containing purchase intent on a scale from 0.0 to 1.0. The primary purpose of the classifier is to ensure that ads are triggered only when user needs it. The classifier takes positive and negative training data sets as inputs, and it scores the ngrams to maximize the query scores in the positive dataset, and to minimize the query scores in the negative dataset.
We use a simple linear model to rank the query candi-dates that pass both commerce intent threshold and rele-vance threshold. Each candidate is assigned a score accord-ing to the following formula, where f i is the i th feature and w i is the weight of the corre-sponding feature. The following features are included in the ranking model, The weights of the model are tuned according to experts ex-periences. A more systematic parameter learning procedure should be applied after more supervised data is collected. This is left to future work.
In this section, we develop a hybrid model to measure the relevance between the original query and rewritten candi-dates. The goal is that given a query pair input, output a number from 0 . 0 to 1 . 0 that measures the relevance of this pair of query, i,e. the QuerySimScore and WordSimScore in-troduced in the last section. We first introduce the baseline model [14] for semantic matching at query level, then discuss word level model based on product category decomposition to handle tail queries.

There are two major challenges for judging the relevance of two queries.
In Query-Level Matching, query pairs are generated from successive search queries issued by a single user within a single session. That is pair ( q ij ,q ik ) s.t.q ij  X  S and assume t ij &lt; t ik without loss of generality,  X  1  X  l  X  M over users across all the sessions. So that the data consists of all query pairs and frequencies of appearance (again across users and sessions).

The initial query pair set are quite noisy, and some are more semantically related than others. Therefore we use the likelihood ratio test to score each pair of queries. Denote the random variable of query q 1 appears in the first place of the query pair as X q 1 and the second place shows q 2 X 2 . We formulate the conditional probability as Bernoulli models [14]. Specifically, Under the above model, the likelihood of the corpus regard-ing ( p 1 ,p 2 ) is where n 1 is the count of q 1 showing up in the first place, and k 1 is among those the count q 2 succeeding q 1 . Similarly n 2 is the total number of query pairs whose first place is not q , and k 2 counts all the q 2 occurrences without q 1 showing up ahead of it.

We formulate a hypothesis test for this model. The null hypothesis is that query pair ( q 1 ,q 2 ) are not relevant which implies that Pr ( X q 2 | X q 1 ) = Pr ( X q 2 | X  X q 1 ). Formally, the hypothesis is,
The likelihood ratio for this test is,
The maximum likelihood is achieved with p = k 1 + k 2 n the numerator and p 1 = k 1 n Taking the logarithm of the likelihood ratio gives, where We further rescale the log-likelihood ratio to the range [0 , 1] using the following formula, The formula is inspired by the logistic function.

The likelihood ratio scores for all the history query pairs are pre-computed, and the tuple (query pair, score) is stored in a memory-based hash table to support real-time look-up.
The query level model above only covers less than 15% of all the incoming query pairs to be scored. All the other queries are scored zero because the system has never seen the pairs before and the pairs are considered to be unrelated. So the recall rate is low. To solve this problem, we break down query into words and construct a similarity function based on word product categories.

The query product decomposition is inspired by the re-cent development of topic models, especially LDA. LDA is believed to be superior to most other matrix factorization techniques such as SVD [3]. The idea is to represent each query in the product category space, and compare query vectors for relevance score. The product category space is a compression of the product space, the relationship is like topic v.s. document in LDA.

The first novelty comes from the way we build the prod-uct word matrices. Product is a standalone entity in the data base, each data entry consists of product title, descrip-tion and vendor information etc. Instead of only looking at product title and description, we leverage on the use of click-through data from Bing logs. In the log data, each product is associated with a list of search queries that lead to the click of the product. For each click log i for product p , we construct a matrix D 1 i where the p th row is the uni-gram representation of the queries and all the other rows empty. Finally the product matrix is produced according to where D 2 and D 3 are product -title matrix and product -description document respectively.
 There are several benefits of using the product-click data. First it automatically connects user intent and user search pattern with the product, and queries are represented in a compressed product space (category) after factorization. Second, there is too much noise in the title and description text data, click data concentrates on key terms and it also reflects the current search trend. Therefore the model re-quires frequent update to keep  X  X resh X .

To build similarity function directly from matrix D is not feasible. The product dimension is in the hundreds of thou-sand, real time retrieval is not practical and performance is a huge concern. In addition, the accuracy of the measure
Figure 3: Category model for word level matching is unlikely to be good either, because the product matrix is very sparse and lots of products are highly correlated. We apply the LDA model to the product-word matrix. The product matrix is factorized into a product-category matrix that summarizes the category component of a prod-uct, and a word-category matrix that represents word in a lower-dimension space. Specifically, The decomposition is also illustrated in Figure 3, where D,P,V are m  X  n,k  X  m,k  X  n matrices respectively. The cat-egorical representation of the each query is then estimated using MLE of the query, which is the average sum of the normalized word vectors. That is where w q is the bag-of-word representation (uni-gram) of the query.

Then the relevance score is obtained by computing the cosine similarity between the two queries.

The relevance measure is readily implementable in the cur-rent system. The vocabulary matrix is precomputed and stored in a memory-base hash table. It also gets frequent updates to keep track of the searching trend. Figure 4 illus-trates the framework for this relevance measure.

Figure 4: online computation of WordSimScore
We train both models using the data described in Section 5.1. To validate the accuracy of the model, we sent a random subset of query pairs to human judges. Figure 5 shows the ROCs for the two models and a baseline model. The baseline model computes the cosine similarity between query uni-gram feature vectors.

As illustrated in Figure 5, both models outperform the baseline by a significant margin. They are able to achieve 75%-80% precision given 15% false positive rate (FPR). Fur-thermore, it is interesting to see word-level matching oper-ates better at the low FPR range; this is partially due to its word-based similarity measure and its robustness in han-dling misspellings, stemming and synonyms etc. However, query-level matching performs better when tolerating more false alarm; this reflects the category model, albeit more precise, is more conservative than the collaborative model.
The product category model for word-level matching in-troduced in the previous section takes tremendous amount of time to train -usually in days when using standard li-brary [3] for even reasonably small topic dimensions (say k = 50). This is not practical in the production environ-ment, because we want to train multiple models and select the best one, and tune the model frequently to ensure qual-ity. To speed up model training, we resort to GPU accelera-tion to solve the problem. We acknowledge other works [26, 16, 21, 17] that accelerates topic modeling, however, they are either not open-sourced or requires additional infrastructure (customized cluster computing resources) to the production system.
Graphic Processors have seen much service in the scien-tific computing community, but recently Canny et al. [7, 6] demonstrate their use as general computing accelerators, especially in machine learning. GPUs contrast with CPUs in many ways, but the most important for us are: 1) Much higher levels of parallelism -the number of cores is in the thousands. 2) Significantly higher memory bandwidth. 3) Low-level support for transcendental functions (exp, log etc).
We describe here the implementation of LDA that fully utilize the benefits of GPU acceleration. The two tricks we use are a) a matrix formula for LDA update, b) a custom matrix kernel called sampled dense-dense matrix multiply (SDDMM). Both try to expose as much parallelism as pos-sible to the system, and the matrix update formula enables batch update (instead of operating on one product at a time) which introduce additional speed up.
We develop a matrix version of the variational bayes up-date equation in [3]. First we add subscripts j for the j product, so P ij is the variational topic parameter for cate-gory i in product j . Then we define: The update formula from Figure 6 of [3] can now be written: where C wj is the count of word w in product j , and  X  i the prior distribution. Most such counts are zero, since C is typically very sparse. The above sums have been written with w ranging over word values instead of word positions as per the original paper. This shows that LDA factorizations can be computed with bag-of-words representation without explicit word labels in each position. M is the vocabulary size, and k is the number of topics. Writing the above in matrix form: where the quotient of C by V T  X  C F is the element-wise quo-tient. Only terms corresponding to nonzeros of C (words that actually appear in each document) need to be com-puted, hence the denominator is a SDDMM operation as we will describe later. The quotient results in a sparse matrix with the same nonzeros as C , which is then multiplied by V . The dominant operations in this update are the SD-DMM, and the multiplication of V by the quotient. Both have complexity O ( kc ) where c is the number of nonzeros of C . There is also an M-step update of the topic-word param-eters (equation 9 of [3]) which can be expressed in matrix form and has the same complexity.
The sampled dense-dense matrix multiplication (SDDMM) is written , where A and B are respectively m  X  p and p  X  n dense ma-trices, S is an m  X  n sparse matrix, and  X  is the element-wise (Hadamard) product. S &gt; 0 denotes a matrix which is 1 at non-zeros of S and zero elsewhere. P is also an m  X  n sparse matrix with the same nonzeros as S . Its values are the el-ements of the product AB evaluated at the nonzeros of S , and zero elsewhere. As described earlier, SDDMM is a bot-tleneck operation in LDA. S is a input data matrix (words x products etc.) and is extremely sparse. Direct evalua-tion of AB is impractical. Naive (C/Java) implementation of SDDMM only achieves about 1 Gflop, while CPU-and GPU-assisted custom kernels achieve around 9 Gflops (8C), 40 Gflops (1G) and 140 Gflops (4G) respectively. Since SD-DMM is the bottleneck for LDA, the speedups from custom kernels lead to similar speedups in overall LDA performance.
We compare the LDA runtime using GPU with the C++ implementation by Blei et al [3]. The later is the most usable open source LDA implementation albeit its mediocre perfor-mance. Results show that our implementation is constantly 1000x faster than theirs at varying model sizes. This implies that our GPU system can train the model at the time scale of minutes instead of days.
We carry out extensive experiments on large-scale user log data collected from Bing, and we report the performance of our system in this section.
There are three sources of logs we use to evaluate our system:
In addition, we also use scraping data to measure adver-tisement performance. In scraping, we hit the production index of Bing with rewritten queries and collect the results of the set of ads displayed. Scraping data covers the queries that do not exist in logs.
The most important performance indicators for ad re-trieval are coverage and depth. Coverage is defined as the ratio between the number of user queries that show at least one ad and the total number of queries processed. Let a ( q denote the number of ads shown for query issued by user u at time t , then coverage is computed as follows,
Depth measures the average number of ads per query for those queries that show at least one ad. The following for-mula computes ads depth.
Figure 6 demonstrates the improvement in ad coverage of our system. Statistics of ad depth are plotted in Figure 7. We compare three configurations of the proposed system with the current production system. The first configuration uses search query log only, while the second one also incor-porates browsing history. Both configurations filter queries using the semantic matching model only. The last config-uration uses both source of history data and applies the word-level matching for query matching. The performance numbers of all configurations are relative to the baseline, of which all the numbers are normalized to 1.

As illustrated in Figure 6, we show significant performance improve in coverage. With the baseline relevance model, we are able to achieve 20% coverage increase. While we show a 60% coverage improvement when incorporating the word-level matching.

We also observe performance gains in ad depth in Fig-ure 7. It is interesting to see that the second configuration retrieves the most ads among queries that trigger ads at all. This, however, does not necessarily mean it is superior than the last configuration. Because the word-level match-ing enables the system to process more tail query which only match a small subset of ads more precisely, while the base-line relevance model just throws those queries away.
The effect of length of personal history used is presented in Figure 6, 7 and 8. Figure 8 is plotted for the third con-figuration only. As we can see, the longer the history we use, the more percentage of queries and users we are able to cover and the better performance improvement we achieve.
Ad impression coverage improves almost linearly with the days of history used. This means we can potentially benefit more from using longer personal history data. However, the performance requirement of production system set a con-straints on how far back we can look at for each individual user X  X  history. This parameter will be tuned lively in pro-duction.
 Figure 8: Comparison of different lengths of per-sonal history used.
 Figure 9: Contribution from 3 different methods, in terms of relative ad coverage improvement.

The contribution of each individual component of the sys-tem is presented in Figure 9. For example, the word-level matching section of the plot shows the percentage of ad cov-erage increase between using word-level matching and not using it. In the figure, we observe a diminishing return of the usage of browsing history: the longer history we use, the less useful is browsing history. We can conclude that brows-ing history is a good complement when user history is lim-ited, but does not add much value when user search history is plenty. Figure 9 also confirms that the use of word-level matching contributes a significant part to the overall per-formance gain, partly because it matches tail queries which the baseline relevance model ignores.
We sent the ads returned for each user query to human judges. Each user-query-ad tuple is judged to be either rele-vant (1) or not (0) given user search/browsing histories, and the precision is computed as the percentage of relevant ads among all pairs. Figure 10 compares the precision of the ads generated by the original query, and the first ranked candi-date query to the third one. All the ad relevance are judged with the original query, not the candidates. As illustrated in Figure 10, the ad quality degrades less than 10 % up to the third ranked candidate.

We further measure the CTR and CY on the test data set using product ad logs. CTR is defined as the total user clicks as a percentage of total ads served across all users, and click yield (CY) is the total clicks generated for all user queries. The CTR and CY estimation is under the two following assumptions: 1) the clicks for different users can be carried over; 2) the clicks for different queries triggered same ads can be carried over across ads. We are not able to use user level clicks because the click data is highly sparse, measurement would be biased without user/ad level aggregation. Again we report the relative performance compared to baseline. The results are illustrated in Figure 11. As we can see, the CTR remains largely unchanged, and the CY gain is propor-tional to the increase of coverage. It is helpful to clarify that there are two factors that present negative impacts on our CTR simulation result. First, the clicks used from retail log do not reflect user preferences. Second, the average num-ber of ads retrieved using our system is between 10 to 20 and we only have up to 6 spots to host ads on the SERPs. Therefore, we can select the ads with high CTRs from the initial pool in production, which will increase the CTR in practice. We will leave the online evaluation of CTR and CY to future works.

The research that is most relevant to our work falls into two main areas: sponsored search ads and query rewrite. Many topics in traditional information retrieval are also re-lated.
Many work has been proposed to improve the performance of sponsored search in recent years[30, 20, 12, 24, 23]. Most of the approaches published in the literature so far are tai-lored for traditional text ads. However, advertising at ver-tical domain (product ad, local ad) presents unique chal-lenges in comparison with its traditional counter-part: the campaigns and bids are not set a the keyword level, and the matching between a query and ads is up to search engine at the level of user intent. As a result, existing works are either not applicable to vertical ads or a poor match to solve the key challenge of low recall and intent inference in the verti-cal domain. To our knowledge, this work is the first attempt to address such issues in vertical ads. We present an query augmentation based approach using personal data.

Much of the work on sponsored search personalization focuses on improving the quality and relevance of ads to increase click-through rate [1, 9, 11, 25]. Several research group have investigated personalizing ad results using search history and browsing history [27, 2]. Many have proposed models to predict user click probability given personal data[24, 10]. This paper is different from the previous work in a fundamental way. We utilize personal history to enrich the query pool according to user preferences, and then send mul-tiple queries to the ads index to increase recall.
Many systematic frameworks for query expansion exist in information retrieval [31, 14, 15, 22], and some apply the rel-evance measure to improve web search and ad quality [5, 4, 23]. A more complete review of query rewrite and its appli-cations can be found in [8]. However, most of the previous work use traditional information retrieval approaches and are either not scalable or erroneous when dealing with tail queries. In this paper, we choose to follow the framework in [14] as baseline, since several recent works [28, 19, 13] have found this useful. We further propose a product category to match user intent in the product domain and address the issue of tail queries. Several research papers [27, 18] have investigated topical representations of user X  X  search interest for web search, but they focus on the analysis of general search topics instead of commerce intent.

Developing relevance models and similarity functions for tail queries [29, 32] is an ongoing research topic. How-ever, the computation required in the previous works ex-ceeds the capacity of typically CPU-based systems (cluster is also not a solution because of the curse of heavy communi-cation of model components) and does not meet production constraints where models are expected to get updated in a hourly basis. We propose a GPU-based system to accelerate training of our model, and the system is potentially exten-sible to solve other models as well.
This paper introduces an innovative framework for prod-uct ad retrieval (and retail vertical ad in general), proposes a category-based intent matching model with its GPU-based implementation, and shows significant gains over existing production system. To our knowledge, this work is the first attempt to resolve the key challenges presented in online vertical advertisement, so we primarily focus on addressing first order issues. However, we do recognize the limitations of this work. We expect to live tuning the parameters of the ranking model and other key inputs of the system, such as number of candidates to select, and see more results on key performance indicators including CTR, CY and eventually revenue per thousand impressions (RPM) in future online experiments. [1] Paul N Bennett, Ryen W White, Wei Chu, Susan T [2] Mikhail Bilenko and Matthew Richardson. Predictive [3] David M Blei, Andrew Y Ng, and Michael I Jordan. [4] Andrei Broder, Peter Ciccolo, Evgeniy Gabrilovich, [5] Andrei Broder, Marcus Fontoura, Vanja Josifovski, [6] John Canny and Huasha Zhao. Bidmach: Large-scale [7] John Canny and Huasha Zhao. Big data analytics [8] Claudio Carpineto and Giovanni Romano. A survey of [9] Ye Chen, Dmitry Pavlov, and John F Canny.
 [10] Haibin Cheng and Erick Cant  X u-Paz. Personalized click [11] Ayman Farahat and Michael C Bailey. How effective is [12] Dustin Hillard, Stefan Schroedl, Eren Manavoglu, [13] Jeff Huang and Efthimis N Efthimiadis. Analyzing [14] Rosie Jones, Benjamin Rey, Omid Madani, and Wiley [15] Victor Lavrenko and W Bruce Croft. Relevance based [16] Zhiyuan Liu, Yuzhou Zhang, Edward Y Chang, and [17] Mian Lu, Ge Bai, Qiong Luo, Jie Tang, and Jiuxin [18] Zhongming Ma, Gautam Pant, and Olivia R Liu [19] Nitin Madnani and Bonnie J Dorr. Generating phrasal [20] H Brendan McMahan, Gary Holt, D Sculley, Michael [21] Xuan-Hieu Phan and Cam-Tu Nguyen. Gibbslda++: [22] Filip Radlinski, Andrei Broder, Peter Ciccolo, Evgeniy [23] Hema Raghavan and Dustin Hillard. A relevance [24] Matthew Richardson, Ewa Dominowska, and Robert [25] Theo Rohle. Desperately seeking the consumer: [26] Alexander Smola and Shravan Narayanamurthy. An [27] Mirco Speretta and Susan Gauch. Personalized search [28] Peter D Turney, Patrick Pantel, et al. From frequency [29] Jingfang Xu and Gu Xu. Learning similarity function [30] Wei Vivian Zhang, Ye Chen, Mitali Gupta, Swaraj [31] Wei Vivian Zhang, Xiaofei He, Benjamin Rey, and [32] Ke Zhou, Xin Li, and Hongyuan Zha. Collaborative
