 Random eld models are a popular probabilistic frame work for representing comple x dependencies in natural image data. The two predominant types of random eld models correspond to generati ve versus discriminati ve graphical models respecti vely . Classical Mark ov random elds (MRFs) [2] follo w a traditional generati ve approach, where one models the joint probability of the observ ed image along with the hidden label eld over the pix els. Discriminati ve random elds (DRFs) [11 , 10], on the other hand, directly model the conditional probability over the pix el label eld given an observ ed image. In this sense, a DRF is equi valent to a conditional random eld [12 ] dened over a 2-D lattice. Follo wing the basic tenet of Vapnik [18 ], it is natural to anticipate that learning an accurate joint model should be more challenging than learning an accurate conditional model. Indeed, recent experimental evidence sho ws that DRFs tend to produce more accurate image labeling models than MRFs, in man y applications lik e gesture recognition [15 ] and object detection [11 , 10, 19, 17].
 Although DRFs tend to produce superior pix el labellings to MRFs, partly by relaxing the assumption of conditional independence of observ ed images given the labels, the approach relies more hea vily on supervised training. DRF training typically uses labeled image data where each pix el label has been assigned. Ho we ver, it is considerably more dif cult to obtain labeled data for image analysis than for other classication tasks, such as document classication, since hand-labeling the indi vidual pix els of each image is much harder than assigning class labels to objects lik e text documents. Recently , semi-supervised training has tak en on an important new role in man y application areas due to the abundance of unlabeled data. Consequently , man y researchers are now working on developing semi-supervised learning techniques for a variety of approaches, including generati ve models [14 ], self-learning [5], co-training [3], information-theoretic regularization [6, 8], and graph-based trans-duction [22 , 23, 24]. Ho we ver, most of these techniques have been developed for uni variate classi-cation problems, or class label classication with a structured input [22 , 23, 24]. Unfortunately , semi-supervised learning for structured classication problems, where the prediction variables are interdependent in comple x ways, have not been as widely studied, with few exceptions [1, 9]. Current work on semi-supervised learning for structured predictors [1, 9] has focused primarily on simple sequence prediction tasks where learning and inference can be efciently performed using standard dynamic programming. Unfortunately , the problem we address is more challenging, since the spatial correlations in a 2-D grid structure create numerous dependenc y cycles. That is, our graphical model structure pre vents exact inference from being feasible. Kumar et al [10 ] and Vish-wanathan et al [19 ] argue that learning a model in the conte xt of approximate inference creates a greater risk of the over-tting and over estimating.
 In this paper , we extend the work on semi-supervised learning for sequence predictors [1, 9], partic-ularly the CRF based approach [9], to semi-supervised learning of DRFs. There are several adv an-tages of our approach to semi-supervised DRFs. (1) We inherit the standard adv antage of discrimina-tive conditional versus joint model training, while still being able to exploit unlabeled data. (2) The use of unlabeled data enhances our ability to avoid parameter over-tting and over-estimation in grid based random elds even when using a learner that uses only approximate inference methods. (3) We are still able to model spatial correlations in a 2-D lattice, despite the fact that this introduces dependenc y cycles in the model. That is, our semi-supervised training procedure can be interpreted as a MAP estimator , where the parameter prior for the model on labeled data is governed by the conditional entrop y of the model on unlabeled data. This allo ws us to learn local potentials that capture spatial correlations while often avoiding local over-estimation. We demonstrate the rob ust-ness of our model by applying it to a pix el denoising problem on synthetic images, and also to a challenging real world problem of segmenting tumor in magnetic resonance images. In each case, we have obtained signicant impro vements over current baselines based on standard DRF training. We formulate a new semi-supervised DRF training principle based on the standard supervised for -mulation of [11 , 10]. Let x be an observ ed input image, represented by x = f x is a set of the observ ed image pix els (nodes). Let y = f y pix els of an image. For simplicity we assume each component y Y = f 1 ; 1 g . For example, x might be a magnetic resonance image of a brain and y is a realization Y would be the set of pre-dened pix el cate gories (e.g. tumor versus non-tumor). A DRF is a con-ditional random eld dened on the pix el labels, conditioned on the observ ation x . More explicitly , the joint distrib ution over the labels y given the observ ations x is written
Here N potential at pix el i , which quanties the belief that the class label is y vextor h spatial correlations among neighboring pix els (here, the ones at positions i and j ), such that is the pre-dened feature vector associated with observ ation x . Z ( x ) is the normalizing factor , also kno wn as a (conditional) partition function, which is
Finally , = ( w ; ) are the model parameters. When the edge potentials are set to zero, a DRF yields a standard logistic regression classier . The potentials in a DRF can use properties of the observ ed image, and thereby relax the conditional independence assumption of MRFs. Moreo ver, the edge potentials in a DRF can smooth discontinuities between heterogeneous class pix els, and also correct errors made by the node potentials.
 Assume we have a set of independent labeled images, D l = ( set of independent unlabeled images, D u = from the combined set of labeled and unlabeled examples, D l [ D u .
 The standard supervised DRF training procedure is based on maximizing the log of the posterior probability of the labeled examples in D l A Gaussian prior over the edge parameters is assumed and a uniform prior over parameters w . Here p ( ) = N ( ; 0 ; 2 I ) , where I is the identity matrix. The hyperparameter 2 adds a regular -ization term. In effect, the Gaussian prior introduces a form of regularization to limit over-tting on rare features and avoid degenerac y in the case of correlated features.
 There are a few issues regarding the supervised learning criteria (3). First, the value of 2 is critical to the nal result, and unfortunately selecting the appropriate 2 is a non-tri vial task, which in turn mak es the learning procedures more challenging and costly [13 ]. Second, the Gaussian prior is data-independent, and is not associated with either the unlabeled or labeled observ ations a priori. Inspired by the work in [8] and [9], we propose a semi-supervised learning algorithm for DRFs that mak es full use of the available data by exploiting a form of entr opy regularization as a prior over the parameters on D u . Specically , for a semi-supervised DRF , we attempt to nd that maximizes the follo wing objecti ve function
The rst term of (4) is the conditional lik elihood over the labeled data set D l , and the second term is a conditional entrop y prior over the unlabeled data set D u , weighted by a tradeof f parameter . The resulting estimate is then formulated as a MAP estimate.
 The goal of the objecti ve (4) is to minimize the uncertainty on possible congurations over parame-ters. That is, minimizing the conditional entrop y over unlabeled instances pro vides more condence to the algorithm that the hypothetical labellings for the unlabeled data are consistent with the su-pervised labels, as greater certainty on the estimated labellings coincides with greater conditional lik elihood on the supervised labels, and vice versa. This criterion has been sho wn to be effecti ve for uni variate classication [8], and chain structured CRFs [9]; here we apply it to the 2-D lattice case. Several factors constrain the form of training algorithm: Because of overhead and the risk of diver-gence, it was not practical to emplo y a Ne wton method. Iterati ve scaling was not possible because the updates no longer have a closed form. Although the criticism of the gradient descent' s principle is well tak en, it is the most practical approach we will adopt to optimize the semi-supervised MAP formulation (4) and allo ws us to impro ve on standard supervised DRF training.
 To formulate a local optimization procedure, we need to compute the gradient of the objecti ve (4) with respect to the parameters. Unfortunately , because of the nonlinear mapping function ( : ) , we are not able to represent the gradient of objecti ve function as compactly as [9], which was able to express the gradient as a product of the covariance matrix of features and the parameter vector . the node parameters w is given by 1 where the rst term in (5) is the gradient of the supervised component of the DRF over labeled data, and the second term is the gradient of conditional entrop y prior of the DRF over unlabeled data. Given the lattice structure of the joint labels, it is intractable to compute the exact expectation terms in the abo ve deri vatives. It is also intractable to compute the conditional partition function Z ( x ) . Therefore, as in standard supervised DRFs, we need to incorporate some form of approximation. Follo wing [2, 11, 10], we incorporate the pseudo-lik elihood approximation, which assumes that the joint conditional distrib ution can be approximated as a product of the local posterior probabilities given the neighboring nodes and the observ ation Using the factored approximation in (7), we can reformulate the training objecti ve as Here, the deri vative of the second term in ( 8 ), with respect to the potential parameters w and , can be reformulated as a factored conditional entrop y, yielding
Note that @ entrop y and feature expectations can be computed in terms of local conditional distrib utions. This al-lows us efciently to approximate the global conditional entrop y over unlabeled data. Note that there may be an over-smoothing issue associated with the pseudo-lik elihood approximation, as mentioned in [10 , 19]. Ho we ver, due to the fast and stable performance of this approximation in the supervised case [2, 10] we still emplo y it, but belo w sho w that the over-smoothing effect is mitigated by our data-dependent prior in the MAP objecti ve (4). As a result of our formulation, the learning method is tightly coupled with the inference steps. That is, for the unlabeled data, X perform inference steps for each node i and its neighboring nodes N iterati ve conditional modes (ICM) [2], and is given by where, for each position i , we assume that the labels of all of its neighbors y 0 2 N could alternati vely compute the mar ginal conditional probability P ( y for each node using the sum-product algorithm (i.e. loop y belief propagation), which iterati vely propagates the belief of each node to its neighbors. Clearly , there are a range of approximation methods available, each entailing dif ferent accurac y-comple xity tradeof fs. Ho we ver, we have found that ICM yields good performance at our tasks belo w, and is probably one of the simplest possible alternati ves. Using standard supervised DRF models, Kumar and Hebert [11 , 10] reported interesting experi-mental results for joint classication tasks on a 2-D lattice, which represents an image with a DRF model. Since labeling image data is expensi ve and tedious, we belie ve that better results could be further obtained by formulating a MAP estimation of DRFs by also using the abundant unlabeled image data. In this section, we present a series of experiments on synthetic and real data sets using our novel semi-supervised DRFs(SSDRFs). In order to evaluate our model, we compare the results with those using maximum lik elihood estimation of supervised DRFs [11 ]. There is a major rea-son that we consider the standard MLE DRF from [11 ] instead of the parameter regularized DRFs from [10 ]: that is, we want to sho w the dif ference between the ML and MAP principles without using any regularization term that can be problematic [10 , 13].
 To quantify the performance of each model, we used the Jaccard score J = T P TP denotes true positi ves, FP false positi ves, and FN false negati ves. Although there are man y accurac y measures available, we used this score to penalize the false negati ves since man y imaging tasks are very imbalanced: that is, only a small percentage of pix els are in the  X positi ve X  class. The tradeof f parameter , , was hand-tuned on one held out data set and then held x ed at 0.2 for all of the experiments. 5.1 Synthetic image sets Our primary goal in using synthetic data sets was to demonstrate how well dif ferent models clas-sied pix els as a binary classication over a 2-D lattice in the presence of noise. We generated 18 synthetic data sets, each with its own shape. The intensities of pix els in each image were indepen-dently corrupted by noise generated from a Gaussian N (0 ; 1) . Figure 1 sho ws the results of using supervised DRFs, as well as semi-supervised DRFs. [10 , 19] reported over-smoothing effects from the local approximation approach of PL while our experiments indicate that the over-smoothing is caused not only by PL approximation, but also by the sensiti vity of the regularization to the pa-rameters. Ho we ver, using our semi-supervised DRF as a MAP formulation, we have dramatically impro ved the performance over standard supervised DRF .
 Note that the rst row in Figure 1 sho ws good results from the standard DRF , while the oversmoothed outputs are presented in the last row. Although the ML approach may learn proper parameters from some of data sets, unfortunately its performance has not been consistent since the standard DRF' s learning of the edge potential tends to be overestimated. For instance, the last row sho ws that overestimating parameters of the DRF segment almost all pix els into a class due to the complicated edges and structures containing non-tar get area within the tar get area, while semi-supervised DRF performance is not degraded at all. Ov erall, by learning more statistics from unlabeled data, our model dominates the standard DRF in most cases. This is because our MAP formulation avoids the overestimate of potentials and uses the edge potential to correct the errors made by the node potential. Figure 2(a) sho ws the results over 18 synthetic data sets. Each point abo ve the diagonal line in Figure 2(a) indicates SSDRF producing higher Jaccard scores for a data set. Note that our model stably con verged as we increased the ratio ( nU =nL ) of unlabeled data sets in our learning, Figure 1: Outputs from synthetic data sets.
 as in Figure 2(b), where nU denotes the number of unlabeled images and nL the number of labeled images. Similar results have also been reported in simple single variable classication task [8]. 5.2 Brain Tumor Segmentation We have applied our semi-supervised DRF model to the challenging real world problem of seg-menting tumor in medical images. Our goal here is to classify each pix el of an magnetic resonance (MR) image into a pre-dened cate gory: tumor and non-tumor . This is a very important, yet notori-ously dif cult, task in sur gical planning and radiation therap y which currently involv es a signicant amount of manual work by human medical experts.
 We applied three models to the classication of 9 studies from brain tumor MR images. For each study 2 , i , we divided the MR images into D L three modalities available  X  T1 , T2 , and T1 contr ast . Note that each modality for each slice has 66 ; 564 pix els.
 As with much of the related work on automatic brain tumor segmentation (such as [7, 21]), our training is based on patient-specic data, where training MR images for a classier are obtained from the patient to be tested. Note that the training sets and testing sets for a classier are disjoint. Specically , LR and DRF tak es D L tak es D L We segmented the  X enhancing X  tumor area, the region that appears hyper -intense after injecting the contrast agent (we also included non-enhancing areas contained within the enhancing contour). Table 1 and 2 present Jaccard scores of testing D U the standard supervised DRF impro ves over its degenerate model LR by 1% , semi-supervised DRF signicantly impro ves over the supervised DRF by 11% , which is signicant at p &lt; 0 : 00566 using a paired example t test . Considering the fact that MR images contain much noise and the three modalities are not consistent among slices of the same patient, our impro vement is considerable. Figure 3 sho ws the segmentation results by overlaying the testing slices with segmented outputs from the three models. Each row demonstrates the segmentation for a slice, where the white blob areas for the slice correspond to the enhancing tumor area. We have proposed a new semi-supervised learning algorithm for DRFs, which was formulated as MAP estimation with conditional entrop y over unlabeled data as a data-dependent prior regular -ization. Our approach is moti vated by the information-theoretic argument [8, 16] that unlabeled examples can pro vide the most benet when classes have small overlap. We introduced a simple ap-proximation approach for this new learning procedure that exploits the local conditional probability to efciently compute the deri vative of objecti ve function. We have applied this new approach to the problem of image pix el classication tasks. By exploiting the availability of auxiliary unlabeled data, we are able to impro ve the performance of the state of the art supervised DRF approach. Our semi-supervised DRF approach shares all of the benets of the standard DRF training, including the ability to exploit arbitrary potentials in the presence of dependenc y cycles, while impro ving accurac y through the use of the unlabeled data. The main dra wback is the increased training time involv ed in computing the deri vative of the condi-tional entrop y over unlabeled data. Ne vertheless, the algorithm is efcient to be trained on unlabeled data sets, and to obtain a signicant impro vement in classication accurac y over standard supervised training of DRFs as well as iid logistic regression classiers. To further accelerate the performance with respect to accurac y, we may apply loop y belief propagation [20 ] or graph-cuts [4] as an infer -ence tool. Since our model is tightly coupled with inference steps during the learning, the proper choice of an inference algorithm will most lik ely impro ve segmentation tasks.
 Ackno wledgments This research is supported by the Alberta Ingenuity Centre for Machine Learning, Cross Cancer Institute, and NSERC. We gratefully ackno wledge man y helpful suggestions from members of the Brain Tumor Analysis Project, including Dr. A. Murtha and Dr. J Sander .
 [1] Y. Altun, D. McAllester , and M. Belkin. Maximum mar gin semi-supervised learning for struc-[2] J. Besag. On the statistical analysis of dirty pictures. Journal of Royal Statistical Society . [3] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In COL T , [4] Yuri Bo ykov, Olga Veksler , and Ramin Zabih. Fast approximate ener gy minimization via graph [5] G. Celeux and G. Go vaert. A classication EM algorithm for clustering and two stochastic [6] A. Corduneanu and T. Jaakk ola. Data dependent regularization. In O. Chapelle, B. Schoelk opf, [7] C. Garcia and J.A. Moreno. Kernel based method for segmentation and modeling of magnetic [8] Y. Grandv alet and Y. Bengio. Semi-supervised learning by entrop y minimization. In NIPS 17 , [9] F. Jiao, S. Wang, C. Lee, R. Greiner , and D Schuurmans. Semi-supervised conditional random [10] S. Kumar and M. Hebert. Discriminati ve elds for modeling spatial dependencies in natural [11] S. Kumar and M. Hebert. Discriminati ve random elds: A discriminati ve frame work for con-[12] J. Laf ferty , F. Pereira, and A. McCallum. Conditional random elds: Probabilistic models for [13] C. Lee, R. Greiner , and O. Za  X ane. Efcient spatial classication using decoupled conditional [14] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. Text classication from labeled and [15] A. Quattoni, M. Collins, and T. Darrell. Conditional random elds for object recognition. In [16] S. Roberts, R. Ev erson, and I. Rezek. Maximum certainty data partitioning, 2000. [17] A. Torralba, K. Murphy , and W. Freeman. Conte xtual models for object detection using [18] V. Vapnik. Statistical Learning Theory . John-W iley, 1998. [19] S.V .N. Vishw anathan, N. Schraudolph, M. Schmidt, and K. Murphy . Accelerated training of [20] J. Yedidia, W. Freeman, and Y. Weiss. Generalized belief propagation. In NIPS 13 , pages [21] J. Zhang, K. Ma, M.H. Er, and V. Chong. Tumor segmentation from magnetic resonance [22] D. Zhou, O. Bousquet, T. Na vin Lal, J. Weston, and B. Sch  X  olk opf. Learning with local and [23] D. Zhou, J. Huang, and B. Sch  X  olk opf. Learning from labeled and unlabeled data on a directed [24] X. Zhu, Z. Ghahramani, and J. Laf ferty . Semi-supervised learning using gaussian elds and
