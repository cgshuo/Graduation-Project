 Incremental deterministic parsing has received in-creased awareness over the last decade. Process-ing linguistic data linearly is attractive both from a computational and a cognitive standpoint. While there is a rich research tradition in statistical parsing, the predominant approach derives from chart pars-ing and is inherently non-deterministic.

A deterministic algorithm will incrementally ex-pand a syntactic/semantic derivation as it reads the input sentence one word/token at the time. There are a number of attractive features to this approach. The time-complexity will be linear when the algorithm is deterministic, i.e. it does not allow for later changes to the partial derivation, only extensions to it. For a number of applications, e.g. speech recognition, the ability to process input on the fly per word, and not per sentence, can also be vital. However, there are inherent challenges to an incremental parsing algo-rithm. Garden paths are the canonical example of sentences that are typically misinterpret due to an early incorrect grammatical assumption. (1) The horse raced past the barn fell.
 The ability to reevaluate an earlier grammatical as-sumption is disallowed by a deterministic parser. Optimistic Backtracking is an method designed to locate the incorrect parser decision in an earlier stage if the parser reaches an illegal state, i.e. a state in which a valid parse derivation cannot be retrieved. The Optimistic Backtracking method will try to lo-cate the first incorrect parsing decision made by the parser, and replace this decision with the correct transition, and resume parsing from this state. Incremental deterministic classifier-based parsing algorithms have been studied in dependency pars-ing (Nivre and Scholz, 2004; Yamada and Mat-sumoto, 2003) and CFG parsing (Sagae and Lavie, 2005). Johansson and Nugues (2006) describe a non-deterministic implementation to the depen-dency parser outlined by Nivre and Scholz (2004), where they apply an n -best beam search strategy.
For a highly constrained unification-based for-malism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. Ninomiya et al. (2009) suggest an algorithm for determinis-tic shift-reduce parsing in HPSG. They outline two backtracking strategies for HPSG parsing. Their ap-proach allows the parser to enter an old state if pars-ing fails or ends with non-sentential success, based on the minimal distance between the best candidate and the second best candidate in the sequence of transitions leading up to the current stage. Further constraints may be added, i.e. restricting the number of states the parser may backtrack. This algorithm is expanded by using a beam-thresholding best-first search algorithm, where each state in the parse has a state probability defined by the product of the prob-abilities of the selecting actions that has been taken to reach the state. Optimistic Backtracking is in this paper used to evaluate CuteForce, an incremental deterministic HPSG parser currently in development. Simi-lar to MaltParser (Nivre et al., 2007), it employs a classifier-based oracle to guide the shift-reduce parser that incrementally builds a syntactic/semantic HPSG derivation defined by LinGO English Re-source Grammar (ERG) (Flickinger, 2000).
 Parser Layout CuteForce has a more complex transition system than MaltParser in order to facil-itate HPSG parsing. The sentence input buffer  X  is a list of tuples with token, part-of-speech tags and HPSG lexical types (i.e. supertags (Bangalore and Joshi, 1999)).

Given a set of ERG rules R and a sentence buffer  X  , a parser configuration is a tuple c = (  X ,  X ,  X ,  X  where:  X   X  is a list of tuples of word forms W ,  X   X  is the current input position in  X   X   X  is a stack of passive edges instantiating a The stack of passive edges  X  makes up the full HPSG representation of the input string if the string is accepted. Transition System The shift-reduce parser has four different transitions, two of which are param-eterized with a unary or binary ERG rule, which are added to the passive edges, hence building the HPSG structure. The four transitions are:  X  ACTIVE  X  (adds an active edge to stack  X  , and  X  UNIT ( R 1 )  X  (adds unary passive edge to  X  in- X  PASSIVE ( R 2 )  X  (pops  X  and adds binary pas- X  ACCEPT  X  (terminates the parse of the sen-Derivation Example Figure 1 is a derivation ex-ample from Redwoods Treebank (Oepen et al., 2002). We note that the tree derivation consists of unary and binay productions, corresponding to the UNIT ( R 1 ) and PASSIVE ( R 2 ) parser transitions. Further, the pre-terminal lexical types have a le suf-fix, and are provided together with the terminal word form in the input buffer for the parser. Parsing Configuration Mode CuteForce can op-erate in three different oracle configurations: HPSG Unification mode, CFG approximation mode and unrestricted mode.

In HPSG Unification mode, the parser validates that no oracle decisions lead to an invalid HPSG derivation. All UNIT and PASSIVE transitions are an implicit unification. For each parsing stage, the parsing oracle returns a ranked list of transitions. The highest-ranked transition not violating a unifi-cation constraint will be executed. If no transition yields a valid unification, parsing fails for the given sentence.
 In CFG mode, a naive CFG approximation of the ERG is employed to guide the oracle. The CFG ap-proximation consists of CFG rules harvested from the treebanks used in training the parser  X  for this purpose we have used existing Redwoods treebanks used in training, and augmented with derivations from WikiWoods, in total 300,000 sentences. Each ERG rule instantiation, using the identifiers shown in Figure 1 as non-terminal symbols, will be treated as a CFG rule, and each parser action will be val-idated against the set of CFG rules. If the parser action yields a CFG projection not found among the valid CFG rules in the CFG approximation, the CFG filter will block this transition. If the parser arrives at a state where the CFG filter blocks all further tran-sitions, parsing fails.

In unrestricted mode, the oracle chooses the high-est scoring transition without any further restrictions imposed. In this setting, the parser typically reaches close to 100 % coverage  X  the only sentences not covered in this setting are instances where the parser enters an infinite unit production loop. Hence, we will only evaluate the parser in CFG and Unification mode in this paper. Optimistic Backtracking can be added as an overlay to a transition-based parser in order to evaluate the parser in non-deterministic mode. The overlay has a linear time-complexity. This backtracking method is, to the best of our knowledge, the only method that applies ranking rather than some probability-based algorithm for backtracking. This aspect is critical for classification-based parsing oracles that do not yield a probability score in the ranking of candidate transitions.

Treating backtracking as a ranking problem has several attractive features. It may combine global and local syntactic and semantic information related to each candidate transition, contrary to a probabilis-tic approach that only employs the local transition probability. Utilizing global information also seems more sound from a human point of view. Consider sentence (1), it X  X  first when the second verb ( fell ) is encountered that we would re-evaluate our original assumption, namely that raced may not be the head verb of the sentence. That fell indeed is a verb is surely relevant information for reconsidering raced as the head of a relative clause.

When the parser halts, the backtracker will rank each transition produced up until the point of fail-ure according to which transition is most likely to be the first incorrect transition. When the best scoring transition is located, the parser will backtrack to this position, and replace this transition with the pars-ing oracle X  X  second-best scoring transition for this current parsing state. If the parser later comes to another halt, only the transitions occurring after the first backtrack will be subject to change. Hence, the backtracker will always assume that its last back-track was correct (thus being Optimistic ). Having allowed the parser to backtrack unrestrictedly, we could theoretically have reached close to 100 % coverage, but the insights of parsing incrementally would have become less pronounced.

The search space for the backtracker is n  X  m where n is the number of candidate transitions, and m is the total number of parser transitions. In Op-timistic Backtracking we disregard the m dimension altogether by always choosing the second-best tran-sition candidate ranked by the parsing oracle, as-suming that the second-ranked transition in the given state actually was the correct transition. Hence we reduce the search-space to the n -dimension. In this paper, using CuteForce as HPSG parser, this as-sumption holds in about 80-90 % of the backtracks in CFG mode, in HPSG Unification mode this num-ber is somewhat lower. 4.1 Baseline As a baseline for identifying the incorrect transition, we use a strategy inspired by Ninomiya et al. (2009), namely to pick the candidate transition with the min-imal probability difference between the best and the second best transition candidate. However, since we do not have true probability, a pseudo-probability is computed by taking the dot product of the fea-ture vector and weight-vector for each best-scoring (P) and second-best scoring (P2) candidate transi-tion, and use the proportion of the second-best score over the joint probability of the best and second-best
In our development test set of 1794 sentences, we ran the parser in CFG and HPSG unification mode in deterministic and non-deterministic mode. The baseline results are found in Table 1 (CFG-BL) and Table 2 (UNI-BL). In CFG mode (Table 1), we ob-tain a 51.2 % reduction in parsing failure. In unifica-tion mode (Table 2) the parser is much more likely to fail, as the parse derivations are guaranteed to be a valid HPSG derivation. Baseline backtracking yields a mere 10 % reduction in parsing failures. 4.2 Feature Model Each candidate transition is mapped to a feature vector that provides information about the transi-tion. The task for the ranker is to identify the first incorrect transition in the sequence of transitions. The feature model used by the ranker employs fea-tures that can roughly be divided in three. First, the transition-specific features provide information on the nature of the candidate transition and surround-ing transitions. Here we also have features related to the pseudo-probability of the transition (provided by the parsing oracle), and the oracle score distance be-tween the best-scoring and second-best scoring tran-sition for each given state. Secondly we have fea-tures related to the last token that was processed by the parser before it reached an invalid state, and the information on the incomplete HPSG derivation that was built at that state. These features are used in combination with local transition-specific features. Third, we have features concerning the preliminary HPSG derivation in the actual state of the transition. Feature Types The list of transitions T = t t ject to backtracking upon parsing failure. The fea-ture types used by the backtracker includes:  X  the pseudo-probability of the best scoring (P)  X  the transition category of the current transition  X  the probability proportion of the second best  X  the transition number in the list of applicable  X  the last lexical tag and part-of-speech tag that  X  the head category of the HPSG derivation and  X  the lexical tag relative to the current position in
The backtracker is trained as a linear SVM us-ing SV M rank (Joachims, 2006). Totally, the feature vector maps 24 features for each transition, includ-ing several combinations of the feature types above. In this paper we trained CuteForce with data from Redwoods Treebank, augmented with derivations from WikiWoods (Flickinger et al., 2010). The test set contains a random sample of 1794 sentences from the Redwoods Treebank (which was excluded from the training data), with an average length of 14 tokens. Training data for the backtracker is extracted by parsing derivations from WikiWoods determin-istically, and record transition candidates each time parsing fails, labeling the correct backtracking can-didate, backtrack to this point, and resume parsing from this state. 5.1 Results The first column (CFG-NB and UNI-NB) in Table 1 and 2 indicates the scores when the parser is run in deterministic mode, i.e. without backtracking. The second and third column contain results for baseline and Optimistic backtracking. Coverage refers to the proportion of sentences that received a parse. Pre-cision refers to the backtracker X  X  precision with re-spect to identifying the incorrect transition among the candidate transitions.  X  BT Cand is the aver-age number of candidate transitions the backtracker ranks when trying to predict the incorrect transition, and  X  BT Cand,1st is the number of candidates at the initial point-of-failure. Exact Matches is the to-tal number of parse derivations which are identical to the gold standard.

For Ms per Sent (milliseconds per sentence) it should be said that the code is not optimized, es-pecially with respect to the HPSG unification algo-however give a good indication on how the compu-tational costs vary between the different configura-tions.
 5.2 CFG approximation The number of failed sentences is greatly reduced when backtracking is enabled. Using baseline back-tracking, the reduction is 51.2 %, whereas Opti-mistic backtracking has a 59.1 % reduction in parse failures. Further, Optimistic Backtracker performs substantially better than baseline in identifying in-correct transitions.

The average number of candidate transitions ranged from 26 to 30 for the baseline and Optimistic backtracking strategy. It X  X  interesting to observe that even with a success rate of about 1/5 in identifying the incorrect transition, the coverage is still greatly increased. That backtracking manages to recover so many sentences that initially failed, even if it does not manage to identify the incorrect transition, would seem to indicate that even when mistaken, the backtracker is producing a good prediction. On the other hand, the exact match score does not improve the same way as the coverage, this is directly related to the fact that the backtracker still has relatively low precision, as only a perfect prediction would leave the parser capable of deriving an exact match.
The success rate of about 0.23 in picking the in-correct transition in a set of in average 30 candidates indicates that treating the backtracking as a ranking problem is promising. The precision rate in itself is however relatively low, which serves as an indica-tion of the difficulty of this task. 5.3 HPSG Unification In unification mode the we see no substantive dif-ference between deterministic mode, and baseline and Optimistic backtracking, and practically no im-provement in the quality of the parses produced. In Table 2 we see that the only striking difference between the figures for the parser in backtracking mode and deterministic mode is the efficiency  X  the time consumption is increased by approximately a factor of 3. 5.4 Conclusion The findings in this paper are specific to CuteForce. It is however very likely that the results would be similar for other deterministic HPSG parsers.
In CFG mode, the number of failed parses are more than halved compared to deterministic mode. It is likely that further increase could be obtained by relaxing constraints in the Optimistic algorithm.
In Unification mode, we experienced only a slight increase in coverage. By relaxing the Optimistic constraints, the time-complexity would go up. Con-sidering how little the parser benefited from back-tracking in unification mode with Optimistic con-straints, it seems implausible that the parser will improve considerably without a heavy relaxation of the constraints in the Optimistic algorithm. If do-ing so, the attractive features of the parser X  X  inher-ently deterministic nature will be overshadowed by a very large number of backtracks at a heavy compu-tational cost. Hence, it X  X  hard to see that such a semi-deterministic approach could have any advantages over other non-deterministic HPSG parsers neither in computational cost, performance or on a cogni-tive level. The author would like to thank Stephan Oepen (Uni-versity of Oslo) and Joakim Nivre (Uppsala Uni-versity) for their valued input and inspiring feed-back during the writing of this paper, and in the PhD project. Experimentation and engineering was made possible through access to the TITAN high-performance computing facilities at the University of Oslo (UiO), and we are grateful to the Scientific Computation staff at UiO, as well as to the Norwe-gian Metacenter for Computational Science.

