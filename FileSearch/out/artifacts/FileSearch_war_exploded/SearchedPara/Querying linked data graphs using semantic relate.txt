 1. Introduction
Linked Data [1] has emerged as a de-facto standard for publishing data on the Web. With it comes the potential for a paradigmatic change in the scale in which users and applications reuse, consume and repurpose data. Linked Data, however, provided by search engines in the Web of Documents were fundamental in the process of maximizing the value of the information available on the Web, approaching the Web to the casual user.

However the approaches used for searching the Web of Documents cannot be directly applied for searching/querying data, since from the perspective of structured/semi-structured data consumption, users expect expressive queries, where they can structure and terms used in the data representation. In the Web scenario, where data is spread across multiple and highly
Ideally from the users' perspective, they should be abstracted away from the data representation. Additionally, from the of natural language for search and query tasks was previously investigated by Kauffman [2].

This work focuses on the investigation of a fundamental type of query mechanism for Linked Data: the provision of vocabulary fills an important gap in the spectrum of search/query services for the Linked Data Web, allowing users to expressively query the contents of distributed linked datasets without the need for a prior knowledge of the vocabularies behind the datasets.
 The rationale behind the proposed approach is to use large volumes of semantic information embedded in a comprehensive
Web corpora such as Wikipedia to guide the semantic matching between query and dataset terms, and the navigational process over the Linked Data Web. The distributional information provides a commonsense associative quantitative semantic model which is used as a semantic relatedness measure between the user query terms (which reflects his/her information needs) and dataset terms.

This paper is structured as follows. Section 2 briefly introduces Linked Data and Entity corpus-based semantic relatedness ) are used to build the query mechanism. Section 4 covers the evaluation of the approach, and finally, Section 7 provides a conclusion and future work. 2. Entity  X  Attribute  X  Value (EAV) databases, RDF(S) &amp; Linked Data
The Linked Data Web uses the Resource Description Framework (RDF) to provide a structured way of publishing information describing entities and its relations on the Web. RDF allows the definition of names for entities using Universal Resource the definition of named relations between entities, and the definition of named attributes of entities using literal values.
Linked Data consists a set of principles [1] for publishing structured data as RDF on the Web, allowing RDF datasets to be through HTTP (this process is called dereferenciation). Linked Data allows users to navigate across datasets on the Web, by following the links (triples) connecting entities in different datasets.

The core elements of the semantic model of the typed labeled graph behind RDF(S) for Linked Data assume that: (i) instances have associated properties connecting them with other instances or values. These core elements do not include blank nodes, reification and containers, valid elements in RDF which are not recommended for the publication of RDF as Linked Data.
The EAV model can be defined by a sparse matrix and is associated with open/dynamic schema databases. EAV can be seen as the more abstract data model behind RDF(S). An EAV data model is composed of three core elements: -entity : the element being described. In RDF(S) the entity maps to an instance or class . -attribute : the attribute definition. In RDF(S) an attribute maps to a predicate . -value : The value assigned to an attribute. In RDF(S) it maps to an object which can be a resource or literal value . EAV/CR are used interchangeably.

Open schema and entity-centric data integration are core characteristics of the EAV model which are central to Web data, but also on new data environments with high data heterogeneity, complexity and variability. In these scenarios the traditional structured query mechanisms which demands users to understand the schema or vocabulary in the dataset, limit the ability of data consumers to query the data. 3. Query processing approach 3.1. Motivation
The central motivation behind this work is to propose a Linked Data query mechanism for casual users maximizing flexibility contention is that the combination of these elements provides the support for the construction of a natural language query mechanism for Linked Data with an additional level of vocabulary independency .
The query types covered in this work concentrate on the following query graph patterns: [1]. instance  X  predicate  X  [ instance|value ] [2]. instance 0  X  predicate 0 ... predicate n  X  [ instance [3]. class  X  type  X  instance
These graph patterns are at the core of the RDF data model. Most of the corresponding SPARQL graph patterns matching the natural language queries from the Question Answering over Linked Data test collection [14] include these patterns. Query not covered in the scope of this work.

The remainder of this section describes the proposed approach and its main components. 3.2. Approach &amp; outline
The query mechanism proposed in this work receives as an input a natural language query and outputs a set of triple paths , into a high level architecture.

The query processing approach starts by determining the key entities present in the natural language query. Key entities are Section 3.3.
 over the natural language query and by transforming the generated Stanford dependency structure into a PODS ( Section 3.4 ).
Taking as an input the list of URIs of the pivot entities and the partial ordered dependency structure, the query processing done by computing the semantic relatedness measure between the query terms and terms corresponding to dataset elements in the Linked Data Web. The semantic relatedness measure, combined with a statistical threshold, which determines the will be explored in the Linked Data Web.

Fig. 2 depicts the core part of the query processing for the example query dbpedia:Barack_Obama ( dbpedia-owl:spouse , dbpedia-owl:writer , dbpedia-owl:child , threshold are further explored (dereferenced). The matching process continues until all query terms are covered.
In the example, after the matching between wife and dbpedia-owl: spouse is defined (2), the object pointed by the matched algorithm then navigates to the last node of the PODS, university , dereferencing dbpedia:Princeton_University and dbpedia:
Harvard_Law_School (5), matching for the second time with their type class (6). Since the semantic relatedness between the relatedness measure between the query terms and the vocabulary terms. The proposed algorithm works as a semantic best-effort query approach , where the semantic relatedness measure provides a semantic ranking of returned triples.
The output of the algorithm is a list of ranked triple paths, triples following from the pivot entity to the final resource the user for further complementary exploration by navigation over the answer set. The query mechanism described above was implemented in a prototype named Treo , the word for direction in Irish. 3.3. Entity recognition and entity search
The query processing approach starts by determining the set of key entities (pivot candidates) that will be used in the generation of the partial ordered dependency structure and in the determination of the final pivot entity. The process of is based on Conditional Random Fields sequence models [6] trained in the CoNLL 2003 English training dataset [8], covering the query terms. The POS tags are used to determine pivot candidates which are not named entities (typically mapping to as the main entity.
 a list of URIs representing the entity Barack Obama in different datasets (e.g. http://dbpedia.org/resource/Barack_Obama ). 3.4. Query parsing
The semantic relatedness spreading activation algorithm takes as one of its inputs a partial ordered dependency structure of an RDF representation. Partial ordered dependency structures are derived from Stanford typed dependencies [5] which represents a set of bilexical relations between each term of a sentence, providing grammatical relation labels over the dependencies. Additional details covering Stanford dependencies can be found in [5].

The query parsing module builds PODSs by taking as inputs both Stanford dependencies and the detected named entities/ pivots and by applying a set of operations over the original Stanford dependencies. These operations produce a reduced and ordered version of the original elements of the query. The pivots and named entities combined with the original dependency structure determine the ordering of the elements in the structure.

De
D ( V , E )of Q is defined by applying the following operations over T :  X  merge adjacent nodes V K and V K +1  X  T where E K , K +1  X   X  eliminate the set of nodes V K and edges E K  X  T where E K  X  replicate the triples where E K  X  {cc, conj, preconj}. where the edge labels advmod , amod , etc . represent the speci dependencies).

The traversal sequence should maximize the likelihood of the isomorphism between the partial ordered dependency structure the partial ordered dependency structure returned by the query parser is: Barack Obama 3.5. Semantic relatedness use of a semantic relatedness measure as the activation function in the matching process between query terms and vocabulary terms. The proposed approach is highly dependent on the quality of the semantic relatedness measure. This section briefly describes the basic concepts behind the semantic relatedness measures used in the algorithm.

The problem of measuring the semantic relatedness and similarity of two words can be stated as follows: given two words A and B, determine a measure of (A, B) which expresses the semantic proximity between words A and B. The notion of semantic similarity is associated with taxonomic (is-a) relations between words, while semantic relatedness represents more general queries over the Linked Data Web. In the example query, the relation between pure similarity analysis would not detect appropriately the semantic proximity between these two words. In the context of semantic query by spreading activation, it is necessary to use a relatedness measure that: (i) can cope with terms crossing based on a comprehensive knowledge base.
 Existing approaches for semantically querying and searching Semantic Web/Linked Data knowledge bases are mostly based on WordNet similarity measures or on exploring ontological relations in the query dataset (see Related Work section).
WordNet-based similarity measures [11] are highly dependent on the structure and scope of the WordNet model, not addressing the requirements above. Distributional relatedness measures [16,11] are able to meet the previous requirements, providing approaches to build semantic relatedness measures based on word co-occurrence patterns present in large Web corpora.
Wikipedia Link-based Measure (WLM) [17], and Explicit Semantic Analysis (ESA) [16] which achieved high correlation with human assessments.

The following sections introduce the basic principles behind distributional semantics and the two semantic relatedness measures used in this work. 3.5.1. Corpus-based semantic relatedness &amp; distributional semantics
Distributional semantics is built upon the assumption that the context surrounding a given word in a text provides important information about its meaning [47]. A rephrasing of the distributional hypothesis states that words that co-occur in similar models are naturally represented by Vector Space Models (VSMs), where the meaning of a word is represented by a weighted concept vector.

However, the proper use of the simplified model of meaning provided by distributional semantics implies understanding its in meaning between words, allowing a comparative quantification on the semantic proximity between two words. This differential analysis can be used to determine the semantic relatedness between words. Therefore, the applications of the semantic relatedness and similarity measures between pair of words is one instance in which the strength of distributional the provision of vocabulary independent queries. 3.5.2. Explicit Semantic Analysis (ESA)
Explicit Semantic Analysis (ESA) [16] is a distributional semantic approach built from Wikipedia corpora. ESA provides an inverse document frequency (TF/IDF) weighting scheme. In this space each article is represented as a vector where each ranked article titles, which define a concept vector associated with the query terms (where each vector component receives a disambiguation [16]. The ESA semantic relatedness measure between two terms or text fragments is calculated by computing the cosine similarity between the concept vectors representing the interpretation of the two terms or text fragments. 3.5.3. Wikipedia Link Measure (WLM)
The WLM measure is built based on the link structure between Wikipedia articles. The process of creation of the WLM relatedness measure starts by computing weights for each link, where the significance of each link receives a score. This relatedness computation. The weight score is defined below:
The reader is directed to [17] for additional details on the construction of the relatedness measure. 3.6. The semantic relatedness spreading activation algorithm 3.6.1. Introduction history in cognitive psychology, artificial intelligence and, more recently, on information retrieval.
The idea of spreading activation has its origins associated with the modeling of the human semantic memory in cognitive networks, a graph of interlinked concepts which contains some of the elements under the RDF(S) specifications [4]. applications of spreading activation in information retrieval. 3.6.2. The spreading activation algorithm and ranges are defined in the terminological level, type terms require an a priori instance dereferenciation to collect the associated types. The relatedness computation process between the next query term k and a neighboring node n takes the maximum of the relatedness score between properties p , types c and instance terms i :
Nodes above the semantic relatedness score threshold determine the node URIs which will be activated (dereferenced). The higher discrimination and it is defined as a function of the standard deviation a(I) of a node I is defined as: where I is the current node instance,  X  ( r ) is the mean of the relatedness values associated with each node instance, standard deviation of the relatedness values and  X  is an empirically determined constant. The value of value found for  X  was 2.185. In case no node is activated for the first value of of 0.9 until it finds a candidate node above a(I) .
 same point in the partial ordered dependency structure, in a different pivot in the Linked Data Web (working as an entity reconciliation step). The stop condition for the spreading activation is defined by the end of the query. The final semantic relatedness spreading activation algorithm is defined below: D ( V G , E G ): partial ordered dependency graph W ( V W , E W ): Linked Data graph
A ( V A , E A ): answer graph pivots : set of pivots URI's for all p in pivots do end for 4. Evaluation released for the Question Answering for Linked Data (QALD 2011) workshop [14] containing queries over DBpedia 3.6. From the original query set, 5 queries were highly dependent on comparative operations (e.g. queries were substituted with 5 additional queries exploring more challenging cases of query The reader can find additional details on the data used in the evaluation and the associated results in [15]. a semantic best-effort approach, where the semantic relatedness activation function works both as a ranking and as a cut-off precision we considered a correct answer a triple path containing the URI for the answer. For the example query used in this article, the triple path containing the answer Barack Obama  X  precision and recall). The QALD dataset contains aggregate queries which were included in the evaluation. However, since the answer.

Table 1 shows the relevance metrics collected for the evaluation for each query. For the Wikipedia Link Measure (WLM) semantic relatedness, the final approach achieved a mean reciprocal rank = 0.614, average precision = 0.487, and average recall = 0.57 and 70% of the queries were either completely or partially answered ( recall query approach. The first category, PODS Error , contains errors which were determined by a difference between the PODS associated pivot is a class yago:HostCitiesOfTheSummerOlympicGames ).

Relatedness Error includes queries which were not addressed due to errors in the relatedness computation process, leading to an incorrect matching and the elimination of the correct answer (Q11, Q12). The fourth category, Excessive Dereferenciation
Timeout Error covers queries which demanded a large number of dereferenciations to be answered (Q31, Q40). In the query Q40, last category covers small errors outside previous categories or combined errors in one query (Q32, Q39, Q43) ( Table 2 ).
The relatedness measure was able to cope with non-taxonomic variations between query and vocabulary terms, showing high relatedness mean is 2.81  X  ( r )).

The evaluation on this paper concentrates on the use of a link-based semantic model/relatedness measure (WLM). The reader is directed to [36,35] for results involving the use of a term based distributional model (Explicit Semantic Analysis).
From the perspective of query execution time an experiment was run using an Intel Centrino 2 computer with 4 GB RAM. No parallelization or indexing mechanism outside entity search was implemented in the query mechanism. The average query execution time for the set of queries which were answered was 635 s with no caching and 203 s with active caches. 5. Discussion: the semantic search pattern 5.1. Motivation
The problem of abstracting users from the vocabularies of knowledge bases (where these vocabularies can be database schemas or logical models) is a recurrent problem in computer science. The Treo approach can be abstracted into a semantic search pattern which can be applied to scenarios beyond natural language queries over Linked Data. In this section we pattern. 5.2. Semantic model
The query mechanism assumes a double-layered semantic model where the labeled typed graph model layer is complemented by a distributional semantic model layer. 5.2.1. Entity  X  Attribute  X  Value (EAV) data model layer
In the Entity  X  Attribute  X  Value (EAV) data layer the core semantic assumptions are: i. Assumption of a minimal entity  X  relation  X  entity or entity entities) or classes (category names), and relations and properties represent an attribute associated with an entity. be resolved in the datasets, instead of relying on numerical identifiers' aliases or truncated labels. 5.2.2. Corpus-based semantic model layer
Consists in the construction of a large-scale commonsense corpus-based semantic layer which is used to semantically interpret the elements in the EAV data layer. By allowing the computation of semantic relatedness over elements in the graph elements according to a reference corpus. The corpus-based knowledge can be used to semantically match user queries to the data, providing a comprehensive semantic matching solution. Fig. 4 depicts the two-layered semantic model of the approach.
Despite having as motivation the introduction of a lightweight semantic relatedness measure, the use of link structures to such as ESA [16], can provide a more comprehensive and transportable semantic model, since they are not dependent on the existence and quality of a link-structure in the source corpus.

Queries over the graph layer can use distributional semantic relatedness measures as a ranking function. An analysis of the suitability of semantic relatedness as a semantic ranking function is provided in [38].
 space model (VSM) named T-Space. The construction of an approach based on a vector space model provides a distributional structured semantic index for semantic search over data graphs. The construction of a structured index instead of using navigational queries also provides an improvement on the performance of the query mechanism [40]. 5.3. The core pattern
The key characteristic and strength of the proposed approach lie on the fact that it uses a simplified, comprehensive and of vocabulary independency between users' query terms and dataset terms. In this section we revisit the key elements of the approach, analyzing how each part of the query processing approach contributes to the creation of semantic search/matching pattern which can be applied over databases that can be transformed into an EAV model. 5.3.1. Entity recognition &amp; entity search
The performance of the semantic relatedness-based search step is dependent on the entity recognition and search step, which instances or classes. 5.3.2. Corpus-based (distributional) semantic relatedness
A corpus-based semantic model provides an automatic solution to allow a comprehensive semantic matching in the context of entity relations. It is important to emphasize that the semantic model can (and in most of the cases should) be defined by be used in the creation of the semantic model. 5.3.3. Determine a threshold/ fi lter unrelated results and for each semantic relatedness measure being used. 6. Related work
We approach four main categories of related work: natural language interfaces &amp; QA systems for the Semantic Web , search engines for structured data , spreading activation applied to information retrieval and query mechanisms with structural 6.1. Natural language interfaces &amp; QA systems Different natural language query approaches for Semantic Web/Linked Data datasets have been proposed in the literature.
Most of the existing query approaches using semantic approximations are based on WordNet and on the use of ontological/ taxonomic information present in the datasets.

PowerAqua [18] is a question answering system focused on natural language questions over Semantic Web/Linked Data datasets. PowerAqua uses PowerMap to match query terms to vocabulary terms. According to Lopez et al. [22], PowerMap is a hybrid matching algorithm comprising terminological and structural schema matching techniques with the assistance of large scale ontological or lexical resources . PowerMap uses WordNet-based similarity approaches as a semantic approximation strategy.

NLP-Reduce [20] approaches the problem from the perspective of a lightweight natural language approach, where the natural level.
 with synonyms.
 ORAKEL [41] is a natural language interface focusing on the portability problem across different domains. For this purpose,
ORAKEL implements a lexicon engineering functionality, which allows the creation of explicit frame mappings. Instead of allowing automatic approximations, ORAKEL focuses on a precise manually engineered model. Exploring user interaction techniques, FREyA [23] is a QA system that employs feedback and clarification dialogs to resolve ambiguities and improve the domain lexicon with users' help. FREyA delegates part of the semantic matching and disambiguation process to users. User feedback enriches the semantic matching p rocess by allowing manual entries of query
Compared to existing natural language interfaces approaches, Treo provides a query mechanism which explores a more comprehensive and automatic semantic approximation technique which can cope with the variability of the query matching on the heterogeneous environment of Linked Data on the Web. Additionally, its design supports querying dynamic and and avoiding the construction of a SPARQL query and by focusing on a semantic best-effort/semantic approximate approach. 6.2. Search engines
Information retrieval based approaches for Linked Data range from search mechanisms focusing on providing an entity-centric search functionality to mechanisms aiming towards more flexible query mechanisms. In the entity-centric search space Sindice vary from keyword-based to hybrid queries (mixing keyword search with structural elements in the database). Sindice does not apply a more principled technique for addressing the vocabulary problem.

Semplore [42] is a search engine for Linked Data which uses a hybrid query formalism, combining keyword search with
Semplore reuses the IR engine's merge-sort based Boolean query evaluation method and extends it to answer unary tree-shaped queries.

Dong &amp; Halevy [43] propose an approach for indexing triples allowing queries that combine keywords and structure. The introduced in the index using the same strategy. Schema-level synonyms are handled using synonym tables. Both approaches
By avoiding major changes over existing search paradigms, these approaches can inherit the implementation of optimized structures used in the construction of traditional indexes.

Compared to existing search approaches Treo focuses on providing a solution focused on the increase of vocabulary independency while keeping the query expressivity. 6.3. Spreading activation
More recently the interest in spreading activation models had migrated to information retrieval, mostly in the context of information provided by an existing knowledge base, previous search activities or current query context. In the context of associative retrieval, associations among information items are usually represented as a graph and the search mechanism in associative retrieval techniques in the information retrieval area due to the difficulty in building associative graphs. The recent emergence of the Linked Data Web is likely to motivate new investigations in associative spreading activation techniques.
 propagated to highly connected nodes) and distance constraints. Katifori et al. [32] describe a spreading activation based approach over ontologies for task and activity oriented Personal Information Management. Schumacher et al. [33] describes a
Desktops.
Differently from existing approaches, the spreading activation model proposed in the Treo approach uses a corpus-based search step included in the spreading activation process (as the entry point, and then as an entity reconciliation step). 6.4. Query mechanisms with structural approximations A set of approaches focus on the problem of introducing approximations on SPARQL-like conjunctive queries for Semantic queries based on the relaxation and progressive restoration of query constraints. Similarly, Oren et al. [45] introduces an an evolutionary algorithm. Hurtado et al. [46] introduce a logic-based approximate query mechanism where RDF(S) constraints are used to logically relax the schema-level constraints.
 while keeping query expressivity. 7. Conclusion &amp; future work
This paper proposes a natural language query mechanism for Linked Data focusing on improving the vocabulary problem, a novel combination for querying Linked Data is proposed, based on entity search, spreading activation and
Wikipedia-based semantic relatedness. The approach was implemented in the Treo prototype and was evaluated with an extended version of the QALD query dataset containing 50 natural language queries over the DBpedia dataset, achieving an a comprehensive semantic matching mechanism to support higher levels of vocabulary independency. The proposed approach was designed for querying live distributed Linked Data but it can be extended to databases which can be mapped to an Entity during the experiments, the incorporation of a more sophisticated post-processing mechanism and the investigation of performance optimizations for the approach.
 Acknowledgments The work presented in this paper has been funded by Science Foundation Ireland under Grant No. SFI/08/CE/I1380 (Lion-2).
References
