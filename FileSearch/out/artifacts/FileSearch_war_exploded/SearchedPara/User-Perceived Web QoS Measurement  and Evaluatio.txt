 The amount of web traffic in networks grows at a fast pace as more businesses are using the web to provide customers with information about their products and services. Web technology is the foundation of a wonderful communications medium, it provides a web X  X  performance should keep up with increased demands and expectations. Because of the web X  X  popularity many web-based applications run into performance bottlenecks that drastically decrease the throughput and the usability of the content delivered over the web. 
Web QoS refers to the capability of a web site to provide better service to end users over various technologies. The degree of satisfaction of the user is generally expressed in non-technical terms. Users are not concerned with how a particular service the web site is provided, or with any of the aspects of the network X  X  internal design, but only quality is defined in terms of system response delay  X  service availability and presentation quality. Response delay is the most important issue and has several definitions depending on the type of web service envisaged. Typically, response delay found that long delays increase user frustration, and decrease task success and efficiency[1]. In another study, Bhatti et al. found that users tolerate different levels of through 5 seconds,  X  X verage X  in the interval 5 through 11 seconds and  X  X ow X  for delays longer than 11 seconds, and users who experience long delays are likely to abort active sessions prematurely[2]. For service providers on the Internet, high availability to the attractive to the customers. Presentation quality is concerned with user X  X  terminal. 
QoS is a crucial factor to the success of a web application on the market. The user X  X  satisfaction with web site response quality influences how long the user stays at the site, and determines the user X  X  future visits. Performance measurement is an important way of evaluating this quality. Measuring web services quality is how long it takes to search and get information from the web site. However, due to many uncertainties of Internet traditional client/server measurement. A web server is forced to delay the response to a request when some resources necessary for processing the request are busy. Three possible bottleneck resources were identified: HTTP protocol processing, reading data from disk and transmission of data on the network downlink. 
How to construct a web performance measurement system with reliable users. We present an user-perceived web QoS measurement and evaluation system(WQMES) based on active probing technique, offer an performance evaluation criteria based on performance aggregation to assess web QoS. Content providers can distributed deployment of web site for customers and make appropriate decisions for optimizing site performance. overview of some of the literature related to web performance measurement and evaluation; section 2 presents the implementation of WQMES and related techniques; section 3 presents the experiments and results; the last section gives a brief summary of this research. and industry have responded to this trend both by developing optimizations for servers and by developing mechanisms to test their performance. QoS issues in web services have to be evaluated from the perspective of the providers of Web services and from the scheme which attempts to provide a solution based on server utilization. The technique uses a combination of DNS rotation and HTTP URL redirection for load balancing. SPAND determines network characteristics by making shared  X  passive measurements client requests to the server with the best observed response time in a geographically distributed Web server cluster[4]. Sandra et al. use tcping to probe median bandwidth advantage that the client-side selection scheme has an overall network vision, in terms of congestion of the links involved in the data transfer between server and client, that measured end-to-end web performance on 9 client sites based on the PROCOW infrastructure[7]. Li and Jamin use a measurement based approach to provide proportional bandwidth allocation to web clients by scheduling requests within a web server[8]. Their approach is not able to guarantee a given request rate or response time, workloads. Shen et al. define a performance metric called quality-ware service yield as a composition of throughput, response time, etc., and propose application-level request service differentiation in the Ask Jeeves sear ch engine. Their main focus is to provide developed for web performance measurement, such as WebBench  X  Httperf, etc. 
Along with the works mentioned above, most of the measurement works focus on measurement and analysis of signal metric. We have not seen the combination of multi metrics. A performance evaluation criteria was proposed based on performance evaluate performance. This paper use WQMES to measure and evaluate performance of four web sites from the end user X  X  perspective. In this section, the architecture and implementation of our prototype WQMES was proposed. A detailed introduction to the implementation of WQMES and its related technologies was also given. 3.1 System Design and Implementation There are two popular techniques for measuring the web performance. The first approach, active probing uses machines from fixed points in the Internet to periodically request one or more URLs from a target web service, record end-to-end performance characteristics, and report a time-varying summa ry back to the web service. The second approach, web page instrumentation, associates code with target web pages. The code, after being downloaded into the client browser, tracks the download time for individual objects and reports performance back to the web site. 
WQMES uses first active probing technology. We use TCP SYN packet to connect web site, if connect, Get command was sent to get the first level html page from the web through thread pool with multiple threads. (1) Manager Module: Responsible for customizing task set(including probing rule (2) Graph Visualizing Module: It fetches the corresponding data from Database (3) Performance Evaluation Module: It uses the performance evaluation criteria to (5) Probing Engine: Probing engine executes performance probing with multiple thread-per-request. Data Distributing maintenances the thread pool by prespawning a fixed number of threads at start-up to service all incoming requests. Probing requests can execute concurrently until the number of simultaneous requests exceed the number of threads in the pool. At this point, additional requests must be queued until a thread becomes available. 3.2 Performance Evaluation Criteria Based on Performance Aggregation Web QoS criteria includes conventional metrics such as throughput  X  delay  X  loss and jitter, as well as new QoS criteria based on utilization  X  reliability and security. 
Four performance metrics are used here: delay  X  delay jitter  X  loss rate and utilization. We give a simple introduction to them. 
Delay: Web delay corresponds to response delay that includes DNS delay  X  connection delay  X  server delay and network transmission delay. Delay jitter: Delay jitter here is the variation of the web response delay. specified time interval. 
Utilization: Here we use the probability of successful download sublinks to denote it. 
Aggregation is an action combining contents from a number of different data sources. There are so many web performance metrics. Some of them are correlative, until now there is no rule to combine these performance metrics to one single  X  aggregation combining four of these metrics to reflect QoS of four webs from the end user X  X  perspective. The aim of performance aggregation for web QoS is to satisfy a sort of QoS rule and combine all the single metrics using a mathematical formula to bring a single quantitative result. We give a precise definition of performance aggregation as follows. performance metrics. Performance aggregation is a n operation  X  on Y and M operation  X  on X . We express performance aggregation criteria as follows: Formula (2) depicts the performance aggregation criteria for web i during period j . be adjusted according to different kinds of performance requirements. formula (2) to relative performance evaluation formula(3). where M is the total number of webs. reflect the performance between different webs. 
We use relative performance metrics to evaluate performance between different webs. An integrated solution that covers several performance metrics is proposed by viewpoint. We use WQMES to measure and evaluate web performance. Probing packets were sent www.onlinedown.net as the main destination web site. Inside the main web site, four image sites were selected as destinations: Beijing  X  Nanjing  X  Wuxi and Guangzhou. Our focus is on validating WQMES, so we change Beijing  X  Nanjing  X  Wuxi and Guangzhou to A  X  B  X  C and D out of order. Our study is based on continuing measurement for a 6-hours period from June 29,2005 12:00 to June 29,2005 18:00, using WQMES to measure and evaluate web performance between four image sites. Unregulated active measurement traffic can cause an unpredictable negative impact on the actual application traffic. In order not to influence Internet traffic, sending probing Performance data was collected over a 6-hour time period and was analyzed and processed by performance evaluation criteria to compare the web performance between four image site. Figure 2 shows connection delay comparison of four web sites. From delay comparison of four web sites: DR C  X  DR D  X  DR B  X  DR A . We find that the connect time is little in proportion to response time. And Response time is more influenced by the transmission time  X  waiting time for each link  X  thread number running the same time for sublinks and volume of the page. 
The value of loss and utilization changes so little  X  so we didn X  X  give the comparison chart of them. Figure 4 shows normalization form of relative performance aggregation criteria comparison of A  X  B  X  C and D using formula (3) under subscribed case  X  =0.4  X   X  =0.2  X   X  =0.2 and  X  =0.2. From Figure 4 we got the conclusion: NRPAC A &gt; NRPAC B &gt; NRPAC C &gt; NRPAC D . We found that NRPAC is influenced mostly by response delay, loss  X  jitter and utilization also have influence on it. 
WQMES tests the web sites with simple active probe technology, the web sites are measured from the end user X  X  viewpoint. We stress, however, that the point of our analysis is not to make general claims about certain webs being better or worse than others, rather to show the utility of WQMES. In this article, we proposed the WQMES for web QoS measurement and evaluation. We use WQMES to measure performance of four webs from the end-user viewpoint, experiment results show that probing and evaluating to different web sites of the same contents have different answers. We find that the connection delay is little in proportion delay  X  waiting time for each link  X  thread number running the same time for sublinks and volume of the page. User-perceived web QoS is influenced mostly by response response delay at application level. It also can be used for server placement/selection, our methodology is effective in measuring and evaluating web QoS. Acknowledgements. This work has been supported by National Science Foundation of China under the grant No.60203021, the National  X 863 X  High-Tech Program of China under the grant No.2002AA142020. 
