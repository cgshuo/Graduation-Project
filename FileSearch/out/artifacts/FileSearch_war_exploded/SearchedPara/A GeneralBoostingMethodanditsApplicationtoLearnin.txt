
Zhaohui Zheng y Hongyuan Zha ? Tong Zhang y Oli vier Chapelle y Keke Chen y Gordon Sun y comple x loss functions arising from a variety of machine learning problems. by our proposed general boosting method which we now describe. We consider the follo wing general optimization problem: form of the risk functional R : where  X  For example, each function  X   X  wise comparisons:  X  is preferred to h problems:  X  [ x 1 ; : : : ; x k ] that span( C ) , i.e., h 2H can be expressed as h ( x ) = P j a j h j ( x ) with h j 2C . Friedman [8] proposed a solution when the loss function in (2) can be expressed as which he named as gradient boosting. The idea is to estimate the gradient r  X  gence analysis can be obtained. We  X rst rewrite (2) in the more general form: where N  X  P m now on we identify the function h with the vector [ h ( x function of N variables.
 we can expand R ( h ) around h where h 0 lies between h replace it by a quadratic upper -bound where W is a diagonal matrix upper bounding the Hessian between h r j =  X  [ rR ( h k )] j =w j step we try to minimize an upper bound R on this idea is listed in Algorithm 1 for the loss function in (5). expansion, which would be closer to a Ne wton type method. Algorithm 1 Greedy Algorithm with Quadratic Approximation
Input : X = [ x let h for k = 0 ; 1 ; 2 ; : : : end the form (4); we propose a natural extension to the ones of the form (5). y much as possible the set of preferences, i.e., h ( x same time h ( z T of a ranking function h , parameter  X  and would lik e to enforce that h ( x w The optimization problem we seek to solv e is h  X  = argmin function class. Note that R depends only on the values h ( x using the general boosting frame work discussed in section 2. Q For simplicity let us assume that each feature vector x otherwise we need to compute appropriately formed averages. We consider ponents of the negative gradient corresponding to h ( z negative gradient corresponding to h ( x Both of the abo ve equal to zero when h ( x equals to for x the abo ve deri vations in the follo wing algorithm.
 Algorithm 2 Boosted Ranking using Successi ve Quadratic Approximation (QBRank)
Start with an initial guess h to Gradient Boosting Trees (GBT) as proposed in [8].
 R values as the tar get value for each distinct x preference learning method which learns pair -wise preferences based on SVM approach. D in the query q appears in the anchor -te xts of the document d , etc. vectors and the corresponding labels form training set L remaining data by queries, and construct a training set L query-document pairs and a test set L We use L d grade than d pairs in total. We denote the preference data as P E which we considered as the true preferences. to function. use the follo wing variation of DCG, DCG to higher value of the weight.
 P absolute grade dif ference between each pair h x GBT through cross validation. We did not retune them for QBrank. E using labeled data L the same labeled data: P To this end, we considered the follo wing six experiments for comparison: 1) GBT using L using L P Table 1 presents the precision at K % on data P labeled training data L metric.
 The DCG-5 for RankSVM using P of x x 1 &gt; f x 2 ; x 3 g &gt; x 4 QBrank. functions. We introduce a few de X nitions.
 De X nition 1 C is scale-in variant if 8 g 2C and  X  2 R ,  X g 2C . De X nition 2 k g k De X nition 3 Let h 2 span ( C ) , then k h k upper bounds can also be established.
 the Hessian of R . Assume that C is scale-in variant. Let  X  h 2 span( C ) . Let  X  s normalized step-size , a
If we choose  X  s on k  X  h k lar , for for some  X xed s &gt; 0 , we can choose p 2  X  k  X  q k  X  h k W;X max(0 ; R (0)  X  R (  X  h )) =M s  X  3 ,  X  for Algorithm 1.

