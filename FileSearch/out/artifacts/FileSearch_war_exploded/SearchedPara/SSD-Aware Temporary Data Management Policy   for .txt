 been widely adopted in personal and server computers [2, 3]. 
Since the price per gigabyte of SSD is much more expensive than HDD, enterprise temporary data management policy to take advantage of fast random reads and reduce structures for locating data in memory or SSD, and reclaiming SSD space quickly. 
In this paper, we propose a novel temporary data management policy called STDM the effectiveness of STDM, we implement a prototype system based on PostgreSQL contributions of this work are summarized as follows.  X 
We observe the process of temporary data generation and compare system performance improvements on TPC-H benchmark of two approaches: one is time of I/O operations on temporary data has a direct impact on the response time show that traditional temporary data management policy doesn X  X  suit for SSD.  X  improve the performance of processing complex queries by writing temporary data to SSD in an append-only fashion. And with other efficient measures, STDM can take advantage of fast random reads and reduce random writes on SSD.  X  performance on TPC-H benchmark. Experiment result shows that STDM can manage temporary data efficiently and get obvious performance improvement over original PostgreSQL. 4, we describe the basic framework and key components of STDM. Section 5 contains In recent years, SSD is expected to gr adually replace HDD as the primary permanent database performance at the aspect of optimizing query processing with SSD. 
Some works focus on optimizing query algorithms and page layout, RARE-join is from hStorage-DB, STDM uses SSD to store temporary data only, rather than mixing them with data of query results. give answers to critical business questions. query number of the 22 queries; the second row represents the number of temporary by the query. The file nu m generated files in tempora r of temporary files depend s they varies from tens to t proportion to its file num b merging small sorted runs a
Traditional temporary d friendly to secondary stora g managing such a large n operating systems restrict t h they need memory to ma n software will be able to cr e hand, because random I/O lifetimes of these files, as t h of file system will fall badl y into a large file to reduc e approach and optimize it f o Query 2 3 5 Number 15 317 19 Size(MB) 988 2530 65 0
It is generally expected the size of shared buffer, w h results can be cached and t works well for OLTP a between HDD-only and SSD-only is much smaller than the I/O performance disparity between HDD and SSD. This provides conclusive evidence that traditional temporary data management policy doesn X  X  suit for SSD and then cannot make full use of SSD. 4.1 Basic Framework SSD serves as the secondary storage medium, which is used to store temporary data. LTF. 4.2 System Components hash table, TF, TF mapping table and TF bit map. random writes may degrade I/O performance on SSD significantly, the write buffer is should be cached in main memory. It will be discussed in more detail in Section 5. 2) Write buffer hash table: Because temporary data blocks may be accessed before hashing key and the corresponding block position in the write buffer as value. When When requesting a data block, STDM checks this hash table firstly to detect whether cleared simultaneously. TF. Write operations on it are invoked by swapping the in-memory write buffer. We I/O performance degeneration is unacceptable in our system, we abandon this method. performance. can be found immediately. TF mapping table is an in-memory hash table which uses time. the procedure is detailed in the next section. 4.3 Data Management Policy on SSD memory (line 8). remove the LTF from the TF mapping table (line 10). 4) TF space reclamation logic: Because of our append-only approach and the limit space occupied by these blocks can be reused. benchmark; at last we investigate impacts of the write buffer and the TF sizes. 5.1 Experimental Setup of main memory, Linux 3.2.0, a Seagate 15K RPM 146 GB HDD, a Samsung 840 Pro Series 128GB SATA III SSD. 5.2 Prototype Implem e We implement a prototy p memory data structures ha v data blocks, a bit map for r blockID to data cached in fileID to all data blocks b e temporary data manageme n The original logic of BufF i in HDD directly. We have m put an entry into the write deal with swapping the w r original logic of BufFileD u HDD. It also has been m o table or the TF mapping ta b 5.3 TPC-H Evaluation In our experiments, we use 1) Performance of ST D demonstrate the effectiven e which corresponding total d fixed at 256KB; the TF si z The experiment results are submitted by a single strea m concurrent sessions. 
According to the results performance by approxim a and by approximately 14 p and SSD-only are defined i is more obvious than po w throughput test issues a l transform multiple random random reads of SSD. T h temporary data. 2) Impact of various w r buffer sizes on system per f size and the write buffer si z Fig. 5. We can see that t degenerates when we incr e small write buffer size , s w SSD I/Os will be needed. O size , operators in the temp o swapping is finished. Thu s should not be too small or t 3) Impact of various T F TPC-H performance can performing TF space recl a have already reached their l I/O cost for loading them b size and various TF sizes, one hand, the result ma t performance gets better. H o the TF size is larger than 2 to make most blocks wi t reclamation invoked. Thus, by increasing the TF siz e workloads. So, it is unnece s In this paper, we verified temporary data has a direc t traditional temporary data temporary data manageme n query processing by buffe r SSD in an append-only fas h reduces random writes o over traditional approach. Doctoral Program of Higher Education(No. 20130004130001), and the Fundamental University( No. 11XNL010). 
