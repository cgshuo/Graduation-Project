 Classification methods which rely upon the graph Laplacian (see [3, 20, 13] and references therein), have proven to be useful for semi-supervised learning. A key insight of these methods is that unla-beled data can be used to improve the performance of supervised learners. These methods reduce to the problem of labeling a graph whose vertices are associated to the data points and the edges to the similarity between pairs of data points. The labeling of the graph can be achieved either in a batch [3, 20] or in an online manner [13]. These methods can all be interpreted as different kernel methods: ridge regression in the case of [3], minimal semi-norm interpolation in [20] or the per-ceptron algorithm in [13]. This computation scales in the worst case cubically with the quantity of unlabeled data, which may prevent the use of these methods on large graphs.
 In this paper, we propose a method to improve the computational complexity of Laplacian-based learning algorithms. If an n -vertex tree is given, our method requires an O ( n ) initialization step and after that any m  X  m block of the pseudoinverse of the Laplacian may be computed in O ( m 2 + mS ) time, where S is the structural diameter of the tree. The pseudoinverse of the Laplacian may then be used as a kernel for a variety of label prediction methods. If a generic graph is given, we first approximate it with a tree and then run our method on the tree. The use of a minimum spanning tree and shortest path tree is discussed.
 It is important to note that prediction is also possible using directly the graph Laplacian, without computing its pseudoinverse. For example, this may be achieved by solving a linear system of time [18], where E is the edge set. However, computation via the graph kernel allows for multiple prediction problems on the same graph to be computed more efficiently. The advantage is even more striking if the data come sequentially and we need to predict in an online fashion.
 To illustrate the advantage of our approach consider the case in which we are provided with a small subset of ` labeled vertices of a large graph and we wish to predict the label of a different subset of p vertices. Let m = ` + p and assume that m n (typically we will also have ` p ). A practical application is the problem of detecting  X  X pam X  hosts in the internet. Although the number of hosts in the internet is in the millions we may only need to detect spam hosts from some limited domain. If the graph is a tree the total time required to predict with the kernel perceptron using our method will be O ( n + m 2 + mS ) . The promise of our technique is that, if m + S n and a tree is given, it requires O ( n ) time versus O ( n 3 ) for standard methods.
 To the best of our knowledge this is the first paper which addresses the problem of fast prediction in semi-supervised learning using tree graphs. Previous work has focused on special prediction methods and graphs. The work in [5] presents a non-Laplacian-based method for predicting the labeling of a tree, based on computing the exact probabilities of a Markov random field. The issue of computation time is not addressed there. In the case of unbalanced bipartite graphs [15] presents a method which significantly improves the computation time of the pseudoinverse to  X ( k 2 ( n  X  k )) , where k is the size of a minority partition. Thus, in the case of a binary tree the computation is still  X ( n 3 ) time.
 The paper is organized as follows. In Section 2 we review the notions which are needed in order to present our technique in Section 3, concerning the fast computation of a tree graph kernel. In Section 4 we address the issue of tree selection, commenting in particular on a potential advantage of shortest path trees. In Section 5 we present the experimental results and draw our conclusions in Section 6. In this paper any graph G is assumed to be connected, to have n vertices, and to have edge weights. The set of vertices of G is denoted V = { 1 ,...,n } . Let A = ( A ij ) n i,j =1 be the n  X  n symmetric weight matrix of the graph, where A ij  X  0 , and define the edge set E ( G ) := { ( i,j ) : A ij &gt; 0 , i &lt; j } . We say that G is a tree if it is connected and has n  X  1 edges. The graph Laplacian is the n  X  n matrix defined as G = D  X  A , where D is the diagonal matrix with i -th diagonal element D ii = P n j =1 A ij , the weighted degree of vertex i . Where it is not ambiguous, we will use the notation G to denote both the graph G and the graph Laplacian and the notation T to denote both a Laplacian of a tree and the tree itself. The Laplacian is positive semi-definite and induces above semi-norm is G + , the pseudoinverse of matrix G , see e.g. [14] for a discussion. As the graph is connected, it follows from the definition of the semi-norm that the null space of G is spanned by the constant vector 1 only.
 The analogy between graphs and networks of resistors plays an important role in this paper. That is, the weighted graph may be seen as a network of resistors where edge ( i,j ) is a resistor with measured between vertex i and j in this network and may be calculated using Kirchoff X  X  circuit laws or directly from G + using the formula [16] The effective resistance is a metric distance on the graph [16] as well as the geodesic and struc-tural distances. The structural distance between vertices i,j  X  V is defined as s G ( i,j ) := min {| P ( i,j ) | : P ( i,j )  X  X } where P is the set of all paths in G and P ( i,j ) is the set of edges in a particular path from i to j . Whereas, the geodesic distance is defined as d G ( i,j ) := min { P ( p,q )  X  P ( i,j )  X  pq : P ( i,j )  X  X } . The diameter is the maximum distance between any two points on the graph, hence the resistance, structural, and, geodesic diameter are defined as R Note that, by Kirchoff X  X  laws, r G ( i,j )  X  d G ( i,j ) and, so, R G  X  D G . In this section we describe our method to compute the pseudoinverse of a tree. 3.1 Inverse Connectivity geodesic distance, as for example if there are k edge disjoint paths of geodesic distance d between two vertices, then the effective resistance is no more than d k . Thus, the more paths, the closer the vertices.
 In the following, we will introduce three more global measures of connectivity built on top of the resistance R tot = P i&gt;j r G ( i,j ) , which is a measure of the inverse connectivity of the graph: the smaller R tot the more connected the graph. The second quantity is R ( i ) = P n j =1 r G ( i,j ) , which is used as a measure of inverse centrality of vertex i [6, Def. 3] (see also [17]). The third quantity is G ii , which provides an alternate notion of inverse centrality.
 Summing both sides of equation (2.1) over j gives where we used the fact that P n j =1 G + ij = 0 , which is true because the null space of G is spanned by the constant vector. Summing again over i yields where we have used P n i =1 R ( i ) = 2 R tot . Combing the last two equations we obtain 3.2 Method Throughout this section we assume that G is a tree with corresponding Laplacian matrix T . The principle of the method to compute T + is that, on a tree there is a unique path between any two vertices and, so, the effective resistance is simply the sum of resistances along that path, see e.g. [16, 13] (for the same reason, on a tree the geodesic distance is the same as the resistance distance). We assume that the root vertex is indexed as 1 . The parent and the children of vertex i are denoted by  X  ( i ) and  X  ( i ) , respectively. The descendants of vertex i are denoted by The method is outlined as follows. We initially compute R (1) ,...,R ( n ) in O ( n ) time. This in turn see, with these precomputed values, we may obtain off-diagonal elements G + ij from equation (2.1) by computing individually r T ( i,j ) in O ( S T ) or an m  X  m block in O ( m 2 + mS T ) time. Initialization We split the computation of the inverse centrality R ( i ) into two terms, namely R ( i ) = T ( i ) + S ( i ) , where T ( i ) and S ( i ) are the sum of the resistances of vertex i to each descendant and non-descendant, respectively. That is, We compute  X  ( i ) and T ( i ) , i = 1 ,...,n with the following leaves-to-root recursions  X  ( i ) := by computing  X  (1) then T (1) and caching the intermediate values. We next descend the tree caching each calculated S ( i ) with the root-to-leaves recursion It is clear that the time complexity of the above recursions is O ( n ) . Computing an m  X  m block of the Laplacian pseudoinverse Our algorithm (see Figure 1) computes the effective resistance matrix of an m  X  m block which effectively gives the kernel (via equation (2.1)). The motivating idea is that a single effective re-sistance r T ( i,j ) is simply the sum of resistances along the path from i to j . It may be computed by separately ascending the path from i  X  X o X  X oot and j  X  X o X  X oot in O ( S T ) time and summing the resistances along each edge that is either in the i  X  X o X  X oot or j  X  X o X  X oot path but not in both. However we may amortize the computation of an m  X  m block to O ( m 2 + mS T ) time, saving a factor of min( m,S T ) . This is realized by additionally caching the cumulative sums of resistances along the path to the root during each ascent from a vertex.
 We outline in further detail the algorithm as follows: for each vertex v i in the set V m = { v 1 ,...,v m } we perform an ascent to the root (see line 3 in Figure 1). As we ascend, we cache each cumulative re-sistance (from the starting vertex v i to the current vertex c ) along the path on the way to the root (line 11). If, while ascending from v i we enter a vertex c which has previously been visited during the as-The computational complexity is obtained by noting that every ascent to the root requires O ( S T ) steps and along each ascent we must compute up to max( m,S T ) resistances. Thus, the total com-plexity is O ( m 2 + mS T ) , assuming that each step of the algorithm is efficiently implemented. For this purpose, we give two implementation notes. First, each of the effective resistances computed by the algorithm should be stored on the tree, preventing creation of an n  X  n matrix. When the computation is completed the desired m  X  m Gram matrix may then be directly computed by gath-ering the cached values via an additional set of ascents. Second, it should be ensured that the  X  X or loop X  (line 6) is executed in  X ( | visited ( c )  X  { p } X  X  * ( p ) | ) time by a careful but straightforward implementation of the visited predicate. Finally, this algorithm may be generalized to compute a p  X  ` block in O ( p` + ( p + ` ) S T ) time or to operate fully  X  X nline. X  Let us return to the practical scenario described in the introduction, in which we wish to predict p vertices of the tree based on ` labeled vertices. Let m = ` + p . By the above discussion, computation of an m  X  m block of the kernel matrix T + requires O ( n + m 2 + mS T ) time. In many practical applications m n and S G will typically be no more than logarithmic in n , which leads to an appealing O ( n ) time complexity. In the previous discussion, we have considered that a tree has already been given. In the follow-approximating tree. We will consider both the minimum spanning tree (MST) as a  X  X est X  in norm approximation; and the shortest path tree (SPT) as an approximation which maintains a mistake bound [13] guarantee.
 Given a graph with a  X  X ost X  on each edge, an MST is a connected n -vertex subgraph with n  X  1 edges such that the total cost is minimized. In our set-up the cost of edge ( i,j ) is the resistance  X  ij = 1 A ij , therefore, a minimum spanning tree of G solves the problem where T ( G ) denotes the set of spanning trees of G . An MST is also a tree whose Laplacian best approximates the Laplacian of the given graph according to the trace norm, that is, it solves the problem (4.1) and (4.2) have the same solution follows by noting that the edges in a minimum spanning tree are invariant with respect to any strictly increasing function of the  X  X osts X  on the edges in the original graph [8] and the function  X   X   X  1 is increasing in  X  .
 The above observation suggests another approximation criterion which we may consider for finding a spanning tree. We may use the trace norm between the pseudoinverse of the Laplacians, rather than the Laplacians themselves as in (4.2). This seems a more natural criterion, since our goal is to approximate well the kernel (it is the kernel which is directly involved in the prediction problem). It is interesting to note that the quantity tr( T +  X  G + ) is related to the total resistance. Specifically, we is a convex function of the graph Laplacian. However, we do not know how to minimize R tot ( T ) over the set of spanning trees of G . We thus take a different route, which leads us to the notion of shortest path trees . We choose a vertex i and look for a spanning tree which minimizes the inverse centrality R ( i ) of vertex i , that is we solve the problem Note that R ( i ) is the contribution of vertex i to the total resistance of T and that, by equations (3.1) and (3.2), R ( i ) = nT + ii + R tot n . The above problem can then be interpreted as minimizing a trade-off between inverse centrality of vertex i and inverse connectivity of the tree. In other words, (4.3) encourages trees which are centered at i and, at the same time have a small diameter. It is interesting to observe that the solution of problem (4.3) is a shortest path tree (SPT) centered at vertex i , namely a spanning tree for which the geodesic distance in  X  X osts X  is minimized from i to every other vertex in the graph. This is because the geodesic distance is equivalent to the resistance distance on a tree and any SPT of G is formed from a set of shortest paths connecting the root to any other vertex in G [8, Ch. 24.1].
 Let us observe a fundamental difference between MST and SPT, which provides a justification for approximating the given graph with an SPT. It relies upon the analysis in [13, Theorem 4.2], where the cumulative number of mistakes of the kernel perceptron with the kernel K = G + + 11 &gt; was upper bounded by ( k u k 2 G + 1)( R G + 1) for consistent labelings [13] u  X  { X  1 , 1 } n . To explain our argument, first we note that when we approximate the graph with a tree T the term k u k 2 G is always decreasing, while the term R G is always increasing by Rayleigh X  X  monotonicity law (see for example [13, Corollary 3.1]). Now, note that the resistance diameter R T of an SPT of a graph G is bounded by twice the geodesic diameter of the original graph, Indeed, as an SPT is formed from a set of shortest paths between the root and any other vertex in G , for any pair of vertices p,q in the graph there is in the SPT a path from p to the root and then to q which can be no longer than 2 D G .
 To further discuss, consider the case that G consists of a few dense clusters each uniquely labeled and with only a few cross-cluster edges. The above mistake bound and the bound (4.4), imply that a tree built with an SPT would still have a non-vacuous mistake bound. No such bound as (4.4) holds for an MST subgraph. For example, consider a bicycle wheel graph whose edge set is the union of n spoke edges { (0 ,i ) : i = 1 ,...,n } and n rim edges { ( i,i + 1 mod n ) : i = 1 ,...,n } with costs on the spoke edges of 2 and on the rim edges of 1. The MST diameter is then n + 1 while any SPT diameter is  X  8 .
 At last, let us comment on the time and space complexity of constructing such trees. The MST and SPT trees may be constructed with Prim and Dijkstra algorithms [8] respectively in O ( n log n + | E | ) time. Prim X  algorithm may be further speeded up to O ( n + | E | ) time in the case of small integer weights [12]. In the general case of a non-sparse graph or similarity function the time complexity is  X ( n 2 ) , however as both Prim and Dijkstra are  X  X reedy X  algorithms their space complexity is O ( n ) which may be a dominant consideration in a large graph. In this section, we present an experimental study of the feasibility of our method on large graphs (400,000 vertices). The motivation for our methodology is that on graphs with already 10,000 ver-tices it is computationally challenging to use standard graph labeling methods such as [3, 20, 13], as they require the computation of the full graph Laplacian kernel. This computational burden makes the use of such methods prohibitive when the number of vertices is in the millions. On the other hand, in the practical scenario described in the introduction the computational time of our method scales linearly in the number of vertices in the graph and can be run comfortably on large graphs (see Figure 2 below) and at worst quadratically if the full graph needs to be labeled.
 The aims of the experiments are: ( i ) to see whether there is a significant performance loss when using a tree sub-graph rather than the original graph, ( ii ) to compare tree construction methods, specifically the MST and the SPT and ( iii ) to exploit the possibility of improving performance through ensembles of trees. The initial results are promising in that the performance of the predictor with a single SPT or MST is competitive with that of the existing methods, some of which use the full graph information. We shall also comment on the computational time of the method. 5.1 Datasets and previous methods We applied the Fast Prediction on a Tree (FPT) method to the 2007 web-spam challenge developed at the University of Paris VI 1 . Two graphs are provided. The first one is formed by 9,072 vertices and 464,959 edges, which represent computer hosts  X  we call this the host-graph . In this graph, one host is  X  X onnected X  to another host if there is at least one link from a web-page in the first host to a web-page in the other host. The second graph consists of 400,000 vertices (corresponding to web-pages) and 10,455,545 edges  X  we call this the web-graph . Again, a web-page is  X  X onnected X  to another web-page if there is at least one hyperlink from the former to the latter. Note that both graphs are directed. In our experiments we discarded directional information and assigned a weight of either 1 to unidirectional edges and of w  X  { 1 , 2 } to the bidirectional edges. Each vertex is either labeled as spam or as non-spam. In both graphs there are about 80% of non-spam vertices and 20% of spam ones. Additional tf-idf feature vectors (determined by the web-pages X  html content) are provided for each vertex in the graph, but we have discarded this information for simplicity. Following the web-spam protocol, for both graphs we used 10% of labeled vertices for training and 90% for testing.
 We briefly discuss some previous methods which participated in the web-spam challenge. Abernathy et al. [1] used an SVM variant on the tf-idf features with an additional graph-based regularization term, which penalizes predictions with links between non-spam to spam vertices. Tang et al. (see [7]) used a linear and Gaussian SVM combined with Random Forests on the feature vectors, plus new features obtained from link information. The method of Witschel and Biemann [4] consisted of iteratively selecting vertices and classifying them with the predominant class in their neighborhood (hence this is very similar to label propagation method of [20]). Bencz  X  ur et al. (see [7]) used Naive Bayes, C4.5 and SVM X  X  with a combination of content and/or graph-based features. Finally, Filoche et al. (see [7]) applied html preprocessing to obtain web-page fingerprints, which were used to obtain clusters; these clusters along with link and content-based features were then fed to a modified Naive Bayes classifier. 5.2 Results Experimental results are shown in Table 1. We report the following performance measures: ( i ) average accuracy when predicting with a single tree, ( ii ) average accuracy when each predictor Figure 2: AUC and Accuracy vs. number of trees (left and middle) and Runtime vs. number of labeled vertices (right). aggregate predictive value given by each tree. In the case of the host-graph, predictions for the aggregate method were made using 81 trees. MST and SPT were obtained for the weighted graphs with Prim and Dijkstra algorithms, respectively. For the unweighted graphs, every tree is an MST, so we simply used trees generated by a randomized unweighted depth-first traversal of the graph and SPT X  X  may be generated by using the breadth-first-search algorithm, all in O ( | E | ) time. In the table, the tag  X  X gg. X  stands for aggregate and the  X  X idir X  tag indicates that the original graph was modified by setting w = 2 for bidirectional edges. In the case of the larger web-graph, we used 21 trees and the modified graph with bidirectional weights. In all experiments we used a kernel perceptron which was trained for three epochs (e.g. [13]).
 It is interesting to note that some of the previous methods [1, 4] take the full graph information into account. Thus, the above results indicate that our method is statistically competitive (in fact better than most of the other methods) even though the full graph structure is discarded. Remarkably, in the case of the large web-graph, using just a single tree gives a very good accuracy, particularly in the case of SPT. On this graph SPT is also more stable in terms of variance than MST. In the case of the smaller host-graph, just using one tree leads to a decrease in performance. However, by aggregating a few trees our result improves over the state of the art results.
 In order to better understand the role of the number of trees on the aggregate prediction, we also ran additional experiments on the host-graph with t = 5 , 11 , 21 , 41 , 81 randomly chosen MST or SPT trees. We averaged the accuracy and AUC over 100 trials each. Results are shown in Figure 2. As it can be seen, using as few as 11 trees already gives competitive performance. SPT works better than MST in term of AUC (left plot), whereas the result is less clear in the case of accuracy (middle plot). Finally, we report on an experiment evaluating the running time of our method. We choose the web-graph ( n = 400 , 000 ). We then fixed p = 1000 predictive vertices and let the number of labeled computation of the diagonal elements of the kernel) and initialization plus prediction times were measured in seconds on a dual core 1.8GHz machine with 8Gb memory. As expected, the solid curve, corresponding to initialization time, is the dominant contribution to the computation time. We have presented a fast method for labeling of a tree. The method is simple to implement and, in the practical regime of small labeled and testing sets and diameters, scales linearly in the number of vertices in the tree. When we are presented with a generic undirected weighted graph, we first extract a spanning tree from it and then run the method. We have studied minimum spanning trees and shortest path trees, both of which can be computed efficiently with standard algorithms. We have tested the method on a web-spam classification problem involving a graph of 400,000 vertices. Our results indicate that the method is competitive with the state of the art. We have also shown how performance may be improved by averaging the predictors obtained by a few spanning trees. Further improvement may involve learning combinations of different trees. This may be obtained following ideas in [2]. At the same time it would be valuble to study connections between our work and other approximation methods such as those in in the context of kernel-methods [9], Gaussian processes [19] and Bayesian learning [11].
 Acknowledgments. We wish to thank A. Argyriou and J.-L. Balc  X  azar for valuable discussions, D. Athanasakis and S. Shankar Raman for useful preliminary experimentation, D. Fernandez-Reyes for both useful discussions and computing facility support, and the anonymous reviewers for useful comments. This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-506778, by EPSRC Grant EP/D071542/1 and by the DHPA Research Councils UK Scheme.

