 1. Introduction
Multi-document summarization (MDS) (Carbonell &amp; Goldstein, 1998; Elhadad &amp; McKeown, 2001; Radev &amp; McKeown, 1999) tackles the information overload problem by providing a condensed and coherent version of a set of documents.
Among a number of sub-tasks involved in MDS including sentence extraction, topic detection, sentence ordering, informa-tion extraction, and sentence generation most MDS systems have been based on an extraction method, which identifies important textual segments (e.g. sentences or paragraphs) in source documents. It is important for such MDS systems to determine a coherent arrangement for the textual segments extracted from multi-documents, in order to reconstruct the text structure for summarization.

A summary with improperly ordered sentences confuses the reader and degrades the quality/reliability of the summary itself. Barzilay, Elhadad, and McKeown (2002) has provided empirical evidence to show that the proper order of extracted sentences significantly improves their readability. Lapata (2006) experimentally shows that the time taken to read a sum-mary strongly correlates with the arrangement of sentences in the summary.

For example, consider the three sentences shown in Fig. 1 , selected from a reference summary in Document Understand-ing Conference (DUC) 2003 dataset. The first and second sentences are extracted from the same source document, whereas the third sentence is extracted from a different document. Although all three sentences are informative and talk about the refers to Category 5 storms , described in sentence 2. A better arrangement of sentences in this example would be 3 X 2 X 1.
In single document summarization, where a summary is created using only one document, it is natural to arrange the extracted information in the same order as in the original document. In contrast, for multi-document summarization, we need to establish a strategy to arrange sentences extracted from different documents. Ordering extracted sentences into a coherent summary is a non-trivial task. For example, identifying rhetorical relations (Mann &amp; Thompson, 1988 ) in a docu-ment has been a difficult task for computers. However, our task of constructing a coherent summary from an unordered set of sentences is even more difficult. Source documents for a summary may have been written by different authors, have dif-ferent writing styles, or written on different dates, and based on the different background knowledge. We cannot expect a set of extracted sentences from such a diverse set of documents to be coherent on their own.

The problem of information ordering is not limited to automatic text summarization, and concerns natural language gen-content determination, discourse planning, sentence aggregation, lexicalization, referring expression generation, and ortho-graphic realization. Among those, information ordering is particularly important in discourse planning, and sentence aggre-language text about the given concept. For example, consider the case where generating game summaries, given a database containing statistics of American football. A sentence ordering algorithm can support a natural language generation system by helping to order the sentences in a coherent manner.

In this paper, we propose four criteria to capture the association of sentences in the context of multi-document summa-rization for newspaper articles. These criteria are then integrated into one criterion by a supervised learning approach. We propose a bottom-up approach in arranging sentences, which repeatedly concatenates textual segments until the overall segment with all sentences is arranged. Moreover, we investigate several automatic evaluation measures for the task of sen-tence ordering in multi-document summarization because subjective evaluations of sentence orderings is time consuming and difficult to reproduce. The proposed method outperforms existing sentence ordering algorithms, and shows a high cor-relation (Kendall X  X  s of 0.612) with manually ordered sentences. Subjective evaluations made by human judges reveal that the majority of summaries (ca. 64%) produced by the proposed method are coherent (i.e. graded as perfect or acceptable ).
Among the semi-automatic evaluation measures investigated in our experiments, we found that Kendall X  X  s coefficient has the best correlation with subjective evaluations. 2. Related work
Existing methods for sentence ordering are divided into two approaches: making use of chronological information ( McKe-
A newspaper usually disseminates descriptions of novel events that have occurred since the last publication. For this reason, the chronological ordering of sentences is an effective heuristic for multi-document summarization ( Lin &amp; Hovy, 2001;
McKeown et al., 1999 ). Barzilay et al. (2002) proposed an improved version of chronological ordering by first grouping sen-tences into sub-topics discussed in the source documents, then arranging the sentences in each group chronologically.
Okazaki et al. (2004) proposed an algorithm to improve the chronological ordering by resolving the presuppositional information of extracted sentences. They assume that each sentence in newspaper articles is written on the basis that pre-suppositional information should be transferred to the reader before the sentence is interpreted. The proposed algorithm first arranges sentences in a chronological order, and then estimates the presuppositional information for each sentence by using the content of the sentences placed before each sentence in its original article. The evaluation results show that the proposed algorithm improves the chronological ordering significantly.

Lapata (2003) presented a probabilistic model for text structuring and its application in sentence ordering. Her method computes the transition probability from one sentence to the next in two sentences, from a corpus based on the Cartesian product using the following features: verbs (precedent relationships of verbs in the corpus), nouns (entity-based coherence by keeping track of the nouns), and dependencies (structure of sentences). Lapata (2006) also proposed the use of Kendall X  X  duced by an algorithm and by a human. Although she has not compared her method with chronological ordering, it could be applied to generic domains, not relying on the chronological clue specific to newspaper articles.

Barzilay and Lee (2004) proposed content models to deal with the topic transition in domain specific text. The content models are implemented by Hidden Markov Models (HMMs), in which the hidden states correspond to topics in the domain of interest (e.g. earthquake magnitude or previous earthquake occurrences), and state transitions capture possible informa-tion-presentation orderings. The evaluation results showed that their method outperformed Lapata X  X  approach by a wide margin. They did not compare their method with chronological ordering as an application of multi-document summarization.

Ji and Pulman (2006) proposed a sentence ordering algorithm using a semi-supervised sentence classification and histor-ical ordering strategy. Their algorithm includes three steps: the construction of sentence networks, sentence classification, and sentence ordering. First, they represent a summary as a network of sentences. Nodes in this network represent sentences in a summary, and edges represent transition probabilities between two nodes (sentences). Next, the sentences in the source documents are classified into the nodes in this network. The probability p  X  c ing to a node c k in the network, is defined as the probability of observing s sentence network. Finally, the extracted sentences are ordered to the weights of the edges. They compare the sentence ordering produced by their method against manually ordered summaries using Kendall X  X  s . Unfortunately, they do not com-pare their results against the chronological ordering of sentences, which has been shown to be an effective sentence ordering strategy in multi-document news summaries.

As described above, several good strategies/heuristics to deal with the sentence ordering problem have been proposed. In order to integrate multiple strategies/heuristics, we have formalized them in a machine learning framework, and have con-sidered an algorithm to arrange the sentences using the integrated strategy. 3. Method
We define notation a b to represent that sentence a precedes sentence b . We use the term segment , to describe a se-quence of ordered sentences. When segment A consists of sentences a
The two segments A and B can be ordered as either B after A ,or A after B . We define the notation A B to show that seg-ment A precedes segment B .

Let us consider a bottom-up approach in arranging the sentences. Starting with a set of segments initialized with a sen-tence for each, we concatenate two segments, with the strongest association (discussed later) of all possible segment pairs, into one segment. Repeating the concatenating will eventually yield a segment with all sentences arranged. The algorithm is considered as a variation of agglomerative hierarchical clustering, with the ordering information retained at each concate-nating process.

The underlying idea of the algorithm, a bottom-up approach to text planning, was proposed by Marcu (1997) . Assuming ordering and clustering, thereby ensuring that the resultant discourse tree was well-formed.

Unfortunately, identifying the rhetorical relation between two sentences has been a difficult task for computers ( Marcu, association of the two segments (sentences) are defined. Hence, we introduce a function f  X  A B  X  to represent the direction and strength of the association of two segments, A and B , with different directions, e.g. f  X  A B  X  and f  X  B A  X  , are not always identical by our definition, Fig. 2 shows the process of arranging four sentences obtain a new segment,
Then, we search for the segment pair with the strongest association. Supposing that f  X  C D  X  has the highest value, we concatenate C and D to obtain a new segment, Finally, comparing f  X  E F  X  and f  X  F E  X  , we obtain the final sentence ordering,
Algorithm 1. Sentence ordering algorithm. 1: P f X  s 1  X  ;  X  s 2  X  ; ... g 2: while j P j &gt; 1 3:  X  p a ; p b  X  argmax p i ; p j 2 P ; p i  X  p j f  X  p i 4: for s 2 p b 5: p a p a s 6: end for 7: P P nf p b g 8: end while 9: return P
Algorithm1 presentsthe pseudocodeof the sentenceorderingalgorithm.Algorithm1 takesa set of extractedsentences S as input, and returns a single segment of ordered sentences. First, for each sentence s only.Subsequently,wefindthetwosegments, p a and p b inthesetofsegments P ,thathavethemaximumstrengthofassociation (Line No. 3). The for loop in Lines 4 X 6 then appends the sentences in segment p
No.5denotesthisappendingoperation.Wethenremovethesegment p
In the above description, we have not defined the association of two segments. We define four criteria to capture the asso-with a Support Vector Machine (SVM) (Vapnik, 1998 ) classifier. 3.1. Chronology criterion arranged in the chronological order of publication timestamps. A newspaper usually deals with novel events that have oc-curred since the last publication. Consequently, the chronological ordering of sentences has shown to be particularly effec-tive in multi-document news summarization. As already discussed in Section 2, previous studies have proposed sentence ordering algorithms using chronological information. Publication timestamps are used to decide the chronological order among sentences extracted from different documents. However, if no timestamp is assigned to documents, or if several doc-uments have the identical timestamp, the chronological ordering does not provide a clue for sentence ordering. Inferring temporal relations among events (Mani, Schiffman, &amp; Zhang, 2003; Mani &amp; Wilson, 2000 ) using implicit time references form of timestamps, we define the strength of association in arranging segments B after A , measured by a chronology crite-rion f chro  X  A B  X  in the following formula:
Here, a m represents the last sentence in segment A , b 1 of sentence s in the original document. The chronological order of segment B arranging after A is determined by comparing the last sentence in the segment A and the first sentence in the segment B .

The chronology criterion assesses the appropriateness of arranging segment B after A if sentence a than sentence b 1 , or if sentence a m appears before b 1 ument, preferring the original order in the source document has proven to be effective for single document summarization sentence a m and b 1 are published on the same day, but appear in different articles, the criterion assumes the order to be a score of zero for this condition in formula (8), the chronological criterion guarantees that sentence orderings which con-tradicts with the definition of chronological ordering are not produced.

In addition to the formulation of chronology criterion defined by formula (8), in our preliminary experiments we tried cation dates in days. The chronological distances in a summary are normalized to values in range [0,1] by dividing from the maximum value of chronological distances. However, we did not find any significant improvement in the sentence order-ings produced by this alternative approach in our experiments. Therefore, we only consider the simpler version of chrono-logical criterion defined in formula (8). 3.2. Topical-closeness criterion
A set of documents discussing a particular event usually contains information related to multiple topics. For example, a set of newspaper articles related to an earthquake typically contains information about the magnitude of the earthquake, its location, casualties, and rescue efforts. Grouping sentences by topics has shown to improve the readability of a summary disfluencies is ( b ) X ( a ) X ( c ).

The topical-closeness criterion deals with the association of two segments, based on their topical similarity. The criterion sure the topical closeness of two sentences, we represent each sentence by using a vector. First, we remove stop words (i.e. functional words such as and, or, the ) from a sentence and lemmatize verbs and nouns. Second, we create a vector in which each element corresponds to the words (or lemmas in the case of verbs and nouns) in the sentence. Values of elements in the vector are either 1 (for words that appear in the sentence) or 0 (for words that do not appear in the sentence). We define the topical-closeness of two segments A and B as follows: the sentences. For sentence b 2 B ; max same as by segment A . 3.3. Precedence criterion
In extractive multi-document summarization, only the important sentences that convey the main points discussed in source documents are selected to be included in the summary. However, a selected sentence can presuppose information from other sentences that were not selected by the sentence extraction algorithm. For example, consider the three sentences shown in Fig. 4 , selected from a summary on hurricane Mitch. Sentence ( a ) describes the after-effects of the hurricane, to estimate precedence relations. For example, assuming that in the source document where sentence ( a ) was extracted, there exist a sentence that is similar to sentence ( b ), we can conclude that sentence ( b ) should precede sentence ( a )in the summary.

To formally define the precedence criterion, let us consider the case illustrated in Fig. 5 , where we arrange segment A be-fore B . Each sentence in segment B has the presuppositional information such as background information or introductory facts that should be conveyed to a reader in advance. Given sentence b 2 B , such presuppositional information may be pre-sented by the sentences appearing before the sentence b in the original article. However, we cannot guarantee whether a sentence-extraction method for multi-document summarization chooses any sentences before b for a summary, because the extraction method usually determines a set of sentences within the constraint of summary length that maximizes infor-mation coverage and excludes redundant information.

The precedence criterion measures the substitutability of the presuppositional information of segment B (e.g. the sentences appearing before sentence b ) as segment A . This criterion is a formalization of the sentence ordering algorithm proposed by Okazaki et al. (2004) .

We define the precedence criterion in the following formula: source document, then P b is the empty set. For each sentence p in set P and sentences a in segment A . Cosine similarity between sentences are computed exactly as described in the topical-close-ness criterion. We find the maximum similarity between p and any sentence from segment A . Finally, we average the sim-ilarity scores by dividing from the number of sentences in segment B . Fig. 5 shows an example of calculating the precedence criterion for arranging segment B after A . We approximate the presuppositional information for sentence b by sentences P of sentences in P b and A , formula (10) is interpreted as the average similarity of the precedent sentences 8 P segment A . 3.4. Succession criterion
In extractive multi-document summarization, sentences that describe a particular event are extracted from a set of source articles. Usually, there exist a logical sequence among the information conveyed in the extracted sentences. For example, in propose succession criterion to capture the coverage of information for sentence ordering in multi-document summarization. The succession criterion assesses the coverage of the succeeding information for segment A by arranging segment B after
A : article from which a was extracted). For each sentence s in set S
Fig. 6 shows an example of calculating the succession criterion to arrange segments B after A . We approximate the informa-tion that should follow segment A by the sentences in segments S measure how much segment B covers this information. The succession criterion measures the substitutability of the suc-ceeding information, (e.g. the sentences appearing after the sentence a 2 A ) such as segment B . 3.5. SVM classifier to assess the integrated criterion
We described four criteria for measuring the strength and direction of association between two segments of texts. How-produce a summary. Thus, we use summaries created by humans as training data to find the optimum combination of the proposed criteria so that the combined function fits to the human-made summary. We integrate the four criteria: chronol-the integrated association strength based on four values, f the integration task as a binary classification problem, we employ a Support Vector Machine (SVM) to model the function.
We partition a human-ordered extract into pairs each of which consists of two non-overlapping segments. Let us explain the partitioning process taking four human-ordered sentences, a b c d shown in Fig. 7 . Firstly, we place the partition-instance as follows.

Given a pair of two segments A and B , arranged in an order A B , we obtain a positive training instance (labeled as +1) by computing a four dimensional vector (Fig. 8 ) with the following elements: f f succ  X  A B  X  . Similarly, we obtain a negative training instance (labeled as 1) corresponding to B A . We use a manually or-dered set of summaries and assume an ordering A B as a positive sentence ordering (for training purposes) if in a manually ordered summary the sentence A precedes the sentence B . For such two sentences A and B , we consider the ordering B A as a negative sentence ordering (for training purposes). Accumulating these instances as training data, we construct a binary classifier modeled by a Support Vector Machine. The SVM classifier yields the association direction of two segments (e.g. A B or B A ) with the class information (i.e. +1 or 1).

We assign the association strength of two segments by using the posterior probability that the instance belongs to a po-definition of formula (2)). Because SVM is a large-margin classifier, the output of an SVM is the distance from the decision hyperplane. However, the distance from a hyperplane is not a valid posterior probability. We use sigmoid functions to con-vert the distance into a posterior probability (see Platt (2000) for a detailed discussion on this topic). 4. Evaluation 4.1. Outline of experiments This section outlines the numerous experiments that we conduct to evaluate the proposed sentence ordering algorithm. sentence ordering algorithm and not a complete text summarization system. For example, a typical extractive multi-docu-ment summarization system would first select sentences from a given set of documents, and then order the selected sen-tences to produce a coherent summary. However, the proposed method does not extract any sentences from a set of documents, but assumes that sentence extraction has already completed, and only focuses on ordering those extracted sen-tences. Therefore, it cannot be directly compared with text summarization algorithms which perform sentence extraction such as maximum marginal relevance (MMR) (Carbonell &amp; Goldstein, 1998 ). Consequently, we compare the proposed sen-tence ordering algorithm with previously proposed sentence ordering algorithms for multi-document summarization. In our experiments, all sentence ordering algorithms are given the same set of extracted sentences (a set of sentences are extracted
Our experiment dataset is described in Section 4.2. First, we subjectively evaluate the summaries produced by the pro-posed method and numerous other sentence ordering algorithms in Section 4.3. Specifically, we compare the proposed agglomerative clustering-based sentence ordering algorithm ( AGL ) with six other sentence ordering algorithms: random ordering ( RND ), human-made ordering ( HUM ), chronological ordering ( CHR ), topical-closeness ordering ( TOP ), precedence ordering ( PRE ), and succedence ordering ( SUC ). Here, RND and HUM respectively correspond to the lower and upper baselines of sentence ordering. Details of these sentence ordering algorithms are presented later in Section 4.2. We asked three human judges to independently rate each sentence ordering produced by the different sentence ordering algorithms using four are detailed in Section 4.3.

Section 4.4 introduces three semi-automatic evaluation measures for sentence ordering. Specifically, we define Kendall X  X 
Kendall X  X  s and Spearman coefficient are used in previous work on evaluating sentence orderings. Average continuity is a novel metric that we propose in this paper. All three semi-automatic evaluation measures compare a sentence ordering pro-duced by a system under evaluation against a human-made reference sentence ordering. Moreover, in Section 4.5 we extend the semi-automatic evaluation measures to incorporate more than one reference orderings. We present the experimental results of our semi-automatic evaluation in Section 4.6. A good semi-automatic evaluation measure must have a high degree of correlation with subjective gradings provided by humans. In Section 4.7, we compare the correlation between semi-auto-matic evaluation measures defined in the paper with subjective gradings. Finally, in Section 4.8 we experimentally evaluate by the proposed method. Specifically, we train and test the proposed sentence ordering algorithm by holding out each cri-terion at a time and measure the difference in performance using semi-automatic evaluation measures. 4.2. Experiment dataset
We evaluated the proposed method using the 3rd Text Summarization Challenge (TSC-3) corpus.
Challenge is a multiple document summarization task organized by the  X  X  X ational Institute of Informatics Test Collection for IR Systems X  (NTCIR) project. 4 TSC-3 dataset was introduced in the 4th NTCIR workshop held in June 2 X 4, 2004. The TSC-3 dataset contains multi-document summaries for 30 news events. The events are selected by the organizers of the
TSC task. For each topic, a set of Japanese newspaper articles are selected using some query words. Newspaper articles are selected from Mainichi Shinbun and Yomiuri Shinbun, two popular Japanese newspapers. All newspaper articles in the dataset have their date of publication annotated. Moreover, once an article is published, it is not revised or modified. Therefore, all sentences in an article bares the time stamp of the article.

Although we use Japanese text summaries for experiments, it is noteworthy that there are no fundamental differences between Japanese and English text summarization. In fact, popular summarization algorithms originally designed for English text summarization, such as the maximum marginal relevance (MMR) (Carbonell &amp; Goldstein, 1998 ), have been successfully employed to summarize Japanese texts (Mori &amp; Sasaki, 2002 ).

For each topic, the organizers of the TSC task provide a manually extracted set of sentences. On average, a manually ex-tracted set of sentences for a topic contains 15 sentences. The participants of the workshop are required to run their multi-document summarization systems on newspaper articles selected for each of the 30 topics and submit the results to the workshop organizers. The output of each participating system is compared against the manually extracted set of sentences multi-document summarization.

In order to construct the training data applicable to the proposed method, we asked two human subjects to arrange the extracts. The two human subjects worked independently and arranged sentences extracted for each topic. They were pro-vided with the source documents from which the sentences were extracted. They read the source documents before ordering sentences in order to gain background knowledge on the topic. From this manual ordering process, we obtained 30  X  topics  X  2  X  humans  X  X  60 sets of ordered extracts. Table 1 shows the agreement of the ordered extracts between the two subjects. The correlation is measured by three metrics: Spearman X  X  rank correlation, Kendall X  X  rank correlation, and average continuity. Definitions of these automatic evaluation measures are described later in Section 4.4. The mean corre-lation values (0.74 for Spearman X  X  rank correlation and 0.69 for Kendall X  X  rank correlation) indicate a strong agreement in sentence orderings made by the two subjects. In 8 out of the 30 extracts, sentence orderings created by the two human sub-jects were identical.

We applied the leave-one-out method to the proposed method, to produce a set of sentence orderings. In this experiment, the leave-out-out method arranges an extract by using an SVM model trained from the rest of the 29 extracts. Repeating this proposed method, we prepared six sets of sentence orderings produced by different baseline algorithms. We outline the se-ven algorithms (including the proposed method): Agglomerative ordering ( AGL ) is an ordering arranged by the proposed method.
 Random ordering ( RND ) is the lowest anchor, in which sentences are arranged randomly.
 Human-made ordering ( HUM ) is the highest anchor, in which sentences are arranged by a human subject.
Chronological ordering ( CHR ) arranges sentences with the chronology criterion defined in formula (8). Sentences are arranged in chronological order of their publication date (i.e. sentences belonging to articles published earlier are ordered ahead of sentences belonging to articles published later. Among sentences belonging to the same source article, we order them according to the order in which they appear in the article. Chronological ordering cannot define an order for sen-tences belonging to articles with identical publication dates/times. Ordering among such sentences are decided randomly.

Topical-closeness ordering ( TOP ) arranges sentences with the topical-closeness criterion defined in formula (9). Ties are resolved randomly.

Precedence ordering ( PRE ) arranges sentences with the precedence criterion defined in formula (10). Ties are resolved randomly.

Succedence ordering ( SUC ) arranges sentences with the succession criterion defined in formula (11). Ties are resolved randomly.

The last four algorithms (CHR, TOP, PRE, and SUC) arrange sentences by the corresponding criterion alone, each of which uses the association strength directly without integrating other criteria. These orderings are expected to show the perfor-mance of each criterion, and their contribution to the sentence ordering problem. 4.3. Subjective grading
Evaluating sentence orderings is a challenging task. Intrinsic evaluation, which involves human judges to rank a set of semi-automatic evaluation measures described in Section 4.4.

We asked three human judges to rate sentence orderings according to the following criteria.
Acceptable An acceptable summary is one that makes sense, and is unnecessary to revise even though there is some room for
Unacceptable An unacceptable summary is one that leaves much to be improved and requires overall restructuring rather than
To avoid any disturbance in rating, we inform the judges that the summaries were made from a same set of extracted sentences, and that only the ordering of sentences is different. Furthermore, the judges were given access to the source doc-uments for each summary. Fig. 9 shows a summary that obtained a perfect grade. The ordering 1 X 4 X 5 X 6 X 7 X 8 X 2 X 3 X 9 X 10 was assigned an acceptable grade, whereas 4 X 5 X 6 X 7 X 1 X 2 X 3 X 8 X 9 X 10 was given a poor grade. A random ordering of the 10 sen-tences 4 X 7 X 2 X 10 X 8 X 3 X 1 X 5 X 6 X 9 received an unacceptable grade.

We conduct subjective grading on random ordering (RND), chronological ordering (CHR), topical-closeness ordering (TOP), precedence ordering (PRE), succession ordering (SUC), the proposed sentence ordering algorithm (AGL), and the human-made orderings (HUM). It is noteworthy that the TOP, PRE, and SUC criteria cannot be used to produce a total ordering of sentences on their own, because they cannot decide the first sentence in a summary. To produce a sentence ordering using these criteria on their own, we used them as the strength of association in Algorithm 1. For example, to produce a sentence ordering using concordance (Kendall X  X  W ), which assesses the inter-judge agreement of overall ratings, reported a higher agreement between duced in Section 3, CHR has the largest number of perfect orderings. Although CHR and AGL orderings have roughly the same number of perfect orderings (ca. 24% for CHR and 27% for AGL), the AGL algorithm gained more acceptable orderings (37%) than the CHR algorithm (29%). When we counted both perfect and acceptable summaries, the ratio for the proposed AGL algorithm was 64%, while the CHR reached only 53%. This result suggests that the proposed method successfully incorporated the chro-nological ordering with other criteria. However, a huge gap between AGL and HUM orderings was also found. In particular, the judges rated 27% AGL orderings as perfect , while the figure rose as high as 77% for HUM orderings. 4.4. Methods for semi-automatic evaluation
Even though subjective grading consumes much time and effort, we cannot reproduce the evaluation afterwards. Auto-matic evaluation measures are particularly useful when evaluations must be performed quickly and repeatedly to tune an ing Spearman X  X  rank correlation coefficient, and Kendall X  X  rank correlation coefficient (Kendall X  X  s ), to compare a sentence ordering produced by a system against a manual ordering. In this section, we briefly survey the existing semi-automatic evaluation measures (specifically, Kendall s and Spearman rank correlation coefficient), and introduce average continuity as an alternative evaluation measure for sentence orderings.
 relation coefficient (Kendall, 1938 ) (also known as Kendall X  X  s ) is defined as follows: ative positions in both p and r ). For example, in Fig. 11 between T [ 1,1]. It takes the value 1 if the two sets of orderings are identical, and 1 if one is the reverse of the other.
Likewise, Spearman X  X  rank correlation coefficient  X  r s  X  between orderings p and r is defined as follows: ple shown in Fig. 11 is 0. Spearman X  X  rank correlation, r tained for two identical orderings, and the r s computed between an ordering and its revers is 1.

In addition to Spearman X  X  and Kendall X  X  rank correlation coefficients, we propose an average continuity metric, which ex-tends the idea of the continuity metric (Okazaki et al., 2004 ), to continuous k sentences.

A text with sentences arranged in proper order does not interrupt the process of a human reading from one sentence to the next. Consequently, the quality of a sentence ordering produced by a system can be estimated by the number of contin-uous sentence segments that it shares with the reference sentence ordering. For example, in Fig. 11 the sentence ordering produced by the system under evaluation  X  T e v al  X  has a segment of four sentences  X  a b c d  X  , which appears exactly find to be coherent.

This is equivalent to measuring a precision of continuous sentences in an ordering against the reference ordering. We de-fine P n to measure the precision of n continuous sentences in an ordering to be evaluated as,
Here, N is the number of sentences in the reference ordering, n is the length of continuous sentences on which we are eval-uating, and m is the number of continuous sentences that appear in both the evaluation and reference orderings. In Fig. 11 , we have two sequences of three continuous sentences (i.e.  X  a b c  X  and  X  b c d  X  ). Consequently, the precision of three continuous sentences P 3 is calculated as:
The Average Continuity (AC) is defined as the logarithmic average of P the logarithm from becoming zero in case P n is zero. We set k  X  4 (i.e. more than five continuous sentences are not included orderings share no continuous sentences and one when the two orderings are identical.

Let us compute the average continuity between the two orderings T there are three segments of two consecutive sentences (i.e. in formula (14), for n  X  2 ; m  X  3) between T are two segments of three consecutive sentences between T 0.67 (i.e. 2/(5 3 + 1)). Finally, we observe that there exist a single segment of four consecutive sentences (i.e. a b c d ) five or more consecutive sentences. We then use formula (16) to compute average continuity between T follows: between T e v al and T ref .

The underlying idea of formula (16) is similar to that of BLEU metric ( Papineni, Roukos, Ward, &amp; Zhu, 2002 ), which was developed for the semi-automatic evaluation of machine-translation (MT) systems. BLEU compares the overlap between a translation produced by an MT system and a translation created by a human using n -grams (of words). If the two translations (by the MT system and by the human) share a large number of n -grams, then the MT system receives a high BLEU score. BLEU first computes the precision for n -grams with different lengths and then computes the geometric mean of all precision values using a formula similar to formula (16) used to compute average continuity. Between two orderings of the same set of sen-age continuity measures the overlap between a summary produced by a system and a human-made ordering of those sentences using segments of continuous sentences instead of n -grams of words. 4.5. Using multiple reference orderings to evaluate sentence orderings
There can be more than one way to order a set of sentences to create a coherent summary. Therefore, to evaluate a sen-tence ordering produced by an algorithm, we must compare it with multiple reference orderings produced by different hu-man annotators. However, all evaluation measures described in Section 4.4 compare a system ordering against one reference ordering. In this section, we modify the evaluation measures introduced in Section 4.4 to handle more than one reference ordering.

When comparing a system ordering against multiple reference orderings using the Kendall X  X  s , we consider a sentence b . For example, let us consider the case shown in Fig. 12 in which we compare a system ordering, T orderings T A and T B . If we only use T A as the reference, then pair  X  e ; a  X  in T between a system ordering p and a set f r 1 ; ... ; r t g of t multiple reference orderings is defined as,
Here, N is the number of sentences being ordered, and V t the  X  p  X  i  X  &lt; r k  X  j  X  X  inequalities is satisfied. It returns the value zero if none of the inequalities are satisfied.
We modify the Spearman X  X  rank correlation coefficient (formula (13)) by replacing the term under summation by using the minimum distance between the corresponding ranks of a system ordering, and any one of the reference orderings. The revised formula is given by,
We consider continuous sentence sequences between a system ordering and multiple reference orderings, to extend the average continuity (formula (16)). Specifically, when computing m (the number of continuous sentences that appear in both a system and reference ordering) in formula (14), we compare the continuous sequences of sentences in the system ordering accepted. For example, in Fig. 12 , the sequence e a in T when computing P 2 along with a b ; b c , and c d . Once P to compute the average continuity.
 4.6. Results of semi-automatic evaluation
Table 2 reports the resemblance of orderings produced by six algorithms to the two human-made ones, using the mod-optimum parameter values (i.e. C = 32 and gamma = 0.5) for the radial basis functions (RBF) kernel in SVM. RBF kernel with those parameter values are used for all experiments. libSVM
As seen from Table 2 , the proposed method (AGL) outperforms the rest in all evaluation metrics. Among the different baselines compared, chronological ordering (CHR) appeared to play the most major role. Succession criterion (SUC) performs slightly better than precedence criterion when compared using the Kendall coefficient and the Spearman coefficient. How-ever, the two methods are indistinguishable using average continuity. Topical-relevance criterion (TOP) reports the lowest performance among the four criteria introduced in the paper. Random ordering gained almost zero in all three evaluation metrics. The one-way analysis of variance (ANOVA) verified the effects of different algorithms for sentence orderings with ferences among these algorithms. The Tukey HSD test revealed that AGL was significantly better than the rest. Even though we could not compare our experiment with the probabilistic approach (Lapata, 2003 ) directly due to the difference of the text corpora, the Kendall coefficient reported a higher agreement than Lapata X  X  experiment (Kendall = 0.48 with lemmatized nouns and Kendall = 0.56 with verb X  X oun dependencies).

If two sentences a and b appear next to each other in both a reference sentence ordering and in a sentence ordering under evaluation, then we say that sentences a and b are continuous . A coherent sentence ordering must share many segments of continuous sentences with a reference sentence ordering. Average continuity measure assigns high scores to sentence order-ings that share many segments with a reference ordering. Fig. 13 illustrates the behavior of precision P (14)) with the length of the segment n (measured by the number of sentences in a segment) for the six methods compared in
Table 2 . The number of segments with continuous sentences becomes sparse (i.e. lesser) for a higher length n value. There-fore, the precision values decrease as the length n increases. Although RND ordering reported some continuous sentences for lower n values, no continuous sentences could be observed for the higher n values. The four criteria described in Section 3 cision for any length n , while CHR obtained the second highest precision values. The drop of P fore, we used geometric mean of P n instead of arithmetic mean to compute average continuity in formula (16). A sentence ordering produced by the proposed algorithm is shown in Fig. 14 . English translations are given for the extracted original
Japanese sentences. 4.7. Correlation between subjective gradings and semi-automatic evaluation measures
In Section 4.4, we defined three evaluation measures: Kendall X  X  s , Spearman X  X  rank correlation coefficient  X  r continuity. Ideally, a semi-automatic evaluation measure should have a good agreement with subjective gradings. For this purpose, we measure the correlation between the elicited gradings in Section 4.3, and the values reported by the semi-auto-matic measures described in Section 4.4.

First, we order each set of extracted sentences using three different methods: random ordering, chronological ordering, and the proposed method. For the 30 topics in our dataset, this procedure yields 90  X  30 3  X  summaries. We then compute topic as described in Section 4.5. To compare the subjective grades with semi-automatic evaluation measures, we assign judges graded each of these 90 summaries individually, each summary is assigned with three subjective grades. The final subjective score (grade) for a summary is computed as the average of the score (grade) given by each judge for that summary
For example, the score of a summary with grades Poor , Acceptable , and Unacceptable , is computed as (0.25 + 0.75 + 0.50)/ 3 = 0.5.

In Figs. 15 X 17 , we plot semi-automatic evaluation measure against their corresponding subjective scores for the summa-ries. We compute Pearson correlation coefficient between each semi-automatic evaluation measure and subjective scores. Pearson correlation coefficient ranges from [0,1]. The highest correlation with subjective scores is observed for Kendall X  X  s (0.8501), followed by Spearman X  X  r s (0.7889), and the average continuity (0.7643). We believe the higher correlation ob-tained for the Kendall X  X  s justifies the use of it as a measurement for sentence ordering for multi-document summarization.
Our experimental results are in line with the proposal made by Lapata (2006) to use Kendall X  X  s to evaluate the information ordering tasks. In her experiments, she elicited judgments for 64 orderings (corresponding to 8 texts) from 179 human sub-jects. The Pearson correlation coefficient between Kendall X  X  s and subjective gradings was 0.45. 4.8. Effect of individual criterion on the proposed algorithm The proposed sentence ordering method uses four criteria: chronology, topical-relatedness, precedence, and succession. time. The difference in performance when a particular criterion is removed from the algorithm, can be considered as a mea-sure of the importance of that criterion. Table 3 shows the performance of the proposed method when each of the criteria is removed. We show the difference in performance within brackets, as measured by the evaluation metrics described in Sec-tion 4.5. As seen from Table 3 , removing the chronology criterion results in the largest decline in performance. Removal of discuss the same topic, thereby improving the continuity of information related to a topic. Overall, from Table 3 , it can be seen that all four criteria discussed in the paper positively contribute to the proposed sentence ordering algorithm. 5. Conclusion
We presented a bottom-up approach to arrange sentences extracted for multi-document summarization. We defined four sentence ordering criteria based on previously proposed ordering heuristics, and introduced succedence : a novel sentence ordering criterion that we proposed for this task. Each criterion expresses the strength and direction of the association be-tween two text segments. We utilized support vector machines to integrate the four criteria. Using the trained SVM, we agglomeratively clustered the extracted sentences to produce a total ordering.

The subjective evaluation of the sentence orderings produced by the proposed method outperformed all baselines includ-ing previously proposed heuristics, such as the chronological ordering of sentences. In fact, 64% of the sentence orderings produced using the proposed method were graded as either perfect or acceptable by human judges. Moreover, our semi-automatic evaluation using Spearman, Kendall rank correlation coefficients, and average continuity, showed that the pro-posed method performs significantly better than the baselines. In particular, the proposed method reported a Kendall X  X  s coefficient of 0.612 with human-made sentence orderings, whereas the Kendall X  X  s coefficient reported by the previously proposed chronology ordering was only 0.587. Among the four sentence ordering criteria considered in the proposed meth-od, chronology ordering was the most influential. The removal of chronology criterion reduced the performance of the pro-posed method by 0.249 measured in Kendall X  X  s coefficient. We compared the scores produced by the semi-automatic evaluation measures with human gradings, and found that Kendall X  X  s showed a high correlation with human gradings. The correlation between subjective scores and Kenall X  X  s coefficient was 0.8501, whereas the same respectively between
Spearman coefficient and average continuity were 0.7889 and 0.7643. As a future direction of this study, we intend to ex-plore the possible applications of the proposed method in concept-to-text generation ( Reiter &amp; Dale, 2000 ). Acknowledgments
We used Mainichi Shinbun and Yomiuri Shinbun newspaper articles, and the TSC-3 test collection. We would like to thank NTCIR workshop organizers for providing us with the data.
 Appendix A In this Appendix A we illustrate the proposed method using a set of sentences extracted from actual newspaper articles.
Consider the six sentences shown in Fig. 18 extracted from the three documents shown in Fig. 19 (document A ), Fig. 20 (doc-ument B ), and Fig. 21 (document C ). All documents are selected from the TSC-3 dataset which was used in the experiments in
Section 4.2. The documents are actual newspaper articles selected from Mainichi and Yomiuri newspapers. All documents are originally written in Japanese and we have provided English translations alongside with the original sentences. More-over, we have assigned a sequentially numbered IDs to the sentences in each source document for the ease of reference.
For example, the sentences in document A are numbered A 1 in Japan after the second world war. The airplane after completing 33 years of service marked its final flight in December 1998. Documents A (Fig. 19 ) and C (Fig. 21 ) are published on the same date.

For the six sentences shown in Fig. 18 , the proposed sentence ordering algorithm first creates six segments with each seg-ment containing exactly one sentence. We use the bracket notation to indicate a segment. For example, the segment created by sentence A 1 is written as ( A 1). We then compare all possible pair-wise orderings of those six segments using the four criteria: chronology  X  f chro  X  , topical-closeness  X  f topic in Fig. 18 we generate 24 orderings of segment pairs. Each ordering is represented by a four dimensional feature vector as described in Section 3.5. Using the trained SVM model (the training procedure is detailed in Section 3.5) we obtain the strength of association for each sentence ordering. The proposed sentence ordering algorithm then selects the ordering with the highest strength of association. We then fix the ordering between the two sentences in the selected ordering and form a new segment using those sentences. The above process is repeated with the newly formed segment and the remaining seg-ments until we obtain a single segment with all six sentences ordered. This process can be seen as a hierarchical agglomer-ative clustering algorithm as described in Section 3.

For the sentences shown in Fig. 18 , we show the best two segments selected at each step to form a new segment in Table criteria chronology, precedence and succession report a value of 1. Topical-closeness criterion has a low value because there are few content words in common between sentences A 1 and A 5. Next, the newly formed segment  X  A 1 A 5  X  and the remain-cause sentence A 5, the last sentence in segment  X  A 1 A 5  X  , appears after the sentence A 3 in document A . However, prece-dence criterion reports a value of 1 in this ordering because sentence A 1 precedes sentence A 3 in document A . In step cause the A 3 (the last sentence in segment  X  A 1 A 5 A 3  X  ) and sentence C 3 are both extracted from documents which are published on the same day (i.e. 09-12-1998). Chronology criterion is undefined and returns a value of zero as defined in Eq. (8) when two sentences are extracted from different documents which are published on the same day. In the fifth step, sen-tences B 3 and B 4 are ordered into a single segment. Both B 3 and B 4 are extracted from the same source document (document
B ) and appear in that order in the source document. Therefore, all three criteria: chronology, precedence and succession re-port a value of 1 for the ordering B 3 B 4. In the final step, segment  X  B 3 B 4  X  is attached to the end of segment  X  A 1 A 5 A 3 C 3  X  to form the final sentence ordering, A 1 A 5 A 3 C 3 B 3 B 4. This algorithm is guaranteed to pro-duce a unique sentence ordering in a finite number of steps, because at each step the total number of segments is reduced by exactly one, and only the ordering with the highest strength of association is considered.
 References
