 Traditional information retrieval approaches are usually based on term matching. For example, the vector space model (VSM) [1], probability-based method such as Baeza-Yates approach [2], time series technique-based method such as hidden Markov model (HMM) [3], language-based method [1,4] and so on. However, those approaches are suffered from the problem of word usage diversity, namely, words sets of words, which will make the retrieval performance degrade severely. 
For solving the problem, the concept matching method is proposed. In contrast to term matching methods, it retrieves text documents semantically relevant to the query Analysis (LSA) model and the Probability Latent Semantic Analysis (PLSA) are two contrast, PLSA relies on the likelihood function of multinomial sampling and aims at point, in this paper we propose a PLSA-based method to analyze web pages that are latent factors between the two co-occurrence data for building user profile. 
Nowadays, the most general method for implementing personalization through For example, Micro Speretta [8] and Fang Liu [9] utilized user X  X  query history, respectively. Perkowitz [11] utilized the classify techniques, Mobasher et al. [12, 13] web personalization. 
These methods have been proven to be efficient, but they have to face up the problem that a user had multiple crossed interests. Moreover, they can not reveal the underlying characteristics of user usage information . 
Addressing those problems aforementioned, we proposed the method adopting the algorithm based on Probability Latent Semantic Analysis (PLSA) to cluster user X  X  web pages and construct user profile for implementing personalized recommendation. The rest of this paper is organized as follows. Section 2 gives a short overview of PLSA. Section 3 describes the user profile constructing, learning and updating algorithm, and how to implement the personalized recommendation. The experiments and evaluation are given in Section 4. Finally, the paper is concluded in Section 5. The Probabilistic Latent Semantic Analysis (PLSA) model is a statistical latent [14, 15,16,17]. The core of PLSA is an aspect model [5]. It is assumed that there exist relationship between the hidden factors and the two sets of object can be estimated by the Expectation-Maximization (EM) algorithm. PLSA represents the joint probability of a document d and a word w based on a latent class variable z : 
The probability that a word w in a particular document d is explained by the latent class corresponding to z is estimated during the E-step as: 
And the M-step consists of: In order to avoid overtraining, Hoffman [5] proposed the method using tempered Expectation Maximization (TEM) for fitting the algorithm. Thus the E-step is modified in Eq.(4) by introducing a control parameter  X  , as follows: that sub-simplex, which is identified as a probabilistic latent semantic space [7]. latent topics, which consists of four hierarchies: the first level denotes different topics, the second level denotes the different web pages under their corresponding topic attributes, the third level is the weights of each topic respectively, and the fourth level is the sequentially visited times. Utilizing the visited times sequentially of each topic, we construct a user interest decay factor which is represented with Fibonacci function. The Fibonacci function can be represented as: will be decreased by Fib [5], namely, weight = weight -Fib [5]. each topic not visited times as 0. The structure of user profile is shown as table 1. 3.1 Building User Profile Based on PLSA Algorithm 1. from D ; 2) A predefined threshold  X  ; topic and the different topics have their uniform weight). 
The computing process includes E-step an d M-step, and the E-step is as follows: get the final probabilities of P ( topic k | D ); categories have the corresponding weight; 
Noted that among the topics in user profile, there may be the overlapping  X  that means that a page maybe belongs to several categories (topics). 
According to Algorithm1, we can construct the initial user profile as Table 1. 3.2 Learning and Updating User Profile probability distribution of query Q over the latent topics. In order to decrease quickly interest decay factor. In detail, on the foundation of initial weight in user profile, each user issue a query, according to the topic probability distribution of query, we add 1 to the corresponding visited topic weight, represented as: weight = weight +1 is visited, the corresponding weight will be added. The user profile learning and updating algorithm is described as Algorithm2. predefined threshold  X  ; step just as Eq.(7) and Eq.(8), only replace the D , d with Q , q respectively. Where specific latent topics topic k , and the M-step just as Eq.(9) and Eq.(10), only replace probability value P ( topic k | Q ) is obtained; times]; those topics; 3.3 Implementing Pers onalized Recommendation The detailed algorithm is described as Algorithm 3. multi topics; pages have the same weight with corresponding the topic, record these web pages different topics; topic 1 and topic 2 , representing as web pages to user. classification algorithm. In each part, we also compared the results with baselines. 
The data set is based on a random sample of users visiting CTI web server (http://www.cs.depaul.edu). CTI denotes a department of Computer Science, Telecommunications and Information Systems of DEPAUL University. It contains a two week period during April of 2002 web log files. The original data contains a total of 20950 sessions and 5446 users. This dataset is referred to as the  X  X TI data X . 4.1 The Performance Evaluation including 181 users, whose valid sessions are more than 10. For each user X  X  sessions, we construct the user profiles through identifying latent topics in each web page they each user X  X  sessions into four parts and a dopt 4-fold cross-validation for experiments. 
Take user id=2885 as the example, we constructing his user profile. Firstly, only list user id= 2885 parts profile as Table 2. We download the ready-made software package of LSA [18] and the software SVDLIBC [19] library to perform the SVD transformation. To implement LSA method, we extract keywords from web pages and execute Singular Value Decomposition (SVD). In order to simplify the compare process, we only truncate topics. For PLSA, we only select the first 10 biggest weight topics. 
From the view of evaluating user profile performance, the effectiveness of our method is evaluated by the precision of the recommendation. For evaluating it, we use a metric called Recommendation Ratio in the context of top-N recommendations. For then compare the recommendations with the pages in the test sessions, and a match is considered a Recommendation Success. We define the Recommendation Ratio as the total number of recommendation web pages to divide the numbers of success recommendation. The results are depicted in Fig.1. 4.2 Precisions of Clus ter Document Evaluation In order to make a comparative experiment to evaluate the accuracy of cluster the web centroid of each cluster to build aggregate profile. We adopt the evaluation criteria presented in [12], which is named Weighted Average Visit Percentage (WAVP). This evaluation method is assessing each user profile individually according to the likelihood that a user who visits web page in the shows the comparative result. based method behaves better than the baseline in recommendation accuracy and the average improvement of performance is 7.3% when top 10 recommendations. In Fig. 2, the experimental result inspects that the PLSA-based method rises about 17.6% than the baseline when cluster number is 8. In this paper, we presented an approach based on Probability Latent Semantic Analysis to construct user profile for pers onalized recommendation. In the process of profiles in terms of those underlying factors. In order to explicitly display those latent factors, we made a hypothesis that late nt factor space was latent topic class. Experimental results proved that our approach was more effective than the other typical approaches to construct user profile. we conducted are all in objective datasets, which means that we can only objectively evaluate the whole performance, which decide we can X  X  judge how well our method in time, under PLSA framework, we will combine other classify methods or cluster methods to explore personalized recommendation. 
