 We present a new efficient algorithm for top-N match retrieval of sequential patterns. Our approach is based on an incremental ap-proximation of the string edit distance using index information and a stack based search. Our approach produces hypotheses with average edit error of about 0.29 edits from the optimal SED result while using only about 5% of the CPU computation. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  retrieval models, search process. Algorithms, Performance, Experimentation A* search, Stack decoder, String Edit Distance, String Matching We address the problem of efficiently identifying the top scoring set of sentences from a large data base of sentences using an in-cremental approximation of the string edit distance (SED) given a query sentence. Our motivation is the translation memory domain (e.g. [6]) where the goal is to identify the closest matching sen-tence from a large database of translation pairs given a query sentence in a source language in order to reduce the load of the translation engine, if the match is close enough. Sentences in this domain are typically short (cons isting of less than 20 words). The key idea in our approach is to approximate the SED which consists of the sum of edits costs (insertions, deletions and substi-tutions) under an optimal alignment using instead position ad-justed similitude counts coming from an index. We also propose a way to carry out the search usi ng a stack but without computing string alignments explicitly. This is desirable because, in general, index-based approaches are com putationally advantageous when the search is carried out over large databases. Our search strategy is similar to Jelinek [1] and Paul [4] with these differences: (1) we rely on an inverted index with position information rather than HMM state observations. (2) We ap-proximate the SED instead of the observation likelihoods. (3) Our approach approximates string alignment scores. (4) The evidence is considered in order of decreas ing rarity. Compared to Navarro and Baeza-Yates [2] our approach is based on a simpler structure, we do not partition the query pattern and we search terms in de-creasing rarity order. The SED between sentences A and B consists of the sum of the cost of the edit operations given the optimal alignment  X  the costs of the k th insertion, deletion or substitution operations are denoted by k  X  , k  X  and k  X  , respectively): Using dynamic programming (DP) one can simultaneously obtain the optimal alignment and the associated minimum total distance between A and B . The computational cost of na X ve DP is O(nm) , ( cfr. [3]). When the comparison is performed between string A and each sentence in the database } ... , { 2 1 k B B B =  X  exhaustively computing SED using na X ve DP is ) ( m n O  X  case m is the average sentence length in  X  , and cardinality. In translation memory domains | |  X  is typically large (millions of sentences) while n and m are relatively small. In order to approximate the SE D using index-de rived counts we manipulate the SED from a distance into a similitude score. For this we define the non-negative complemented SED (denoted as SED*) between A and the j th sentence j B in  X  : When A=B j then the n SED 2 * = . This is two times the total num-ber of observed evidence counts that are obtainable using the index (if we increment the score by one per observation). We can express the SED* in terms of two components: one provided by the counts directly identified using the index (which are equal to n minus deletions). We call this the observable evidence. The other term arises from the total number of spurious terms (n minus in-sertions). This is the unobservable evidence (the index returns no counts for these word-sentence combinations). Then: When estimating SED* for a collection of strings and a query we keep track and update the SED* for each sentence in sum of the observable and unobser vable evidence as each of the We denote the observable evidence (n minus deletions) up to term a as g(i). While h(i) is the estimate up to term a i of unobservable terms (amortized deletions). The SED* i up to the i th term is: We want g(n) to approximate the actual insertion related score: As the evidence is incrementally introduced we update g(i): The incremental contribution of term i a to g(i) is computed based on the linear distances between the position of that term and the previously considered term 1  X  i a in the query and hypothesis: The incremental contribution of a term should only be introduced question and the previously observe d evidence. For example, if A = X  X ast run X  and B = X  X un fast X ; terms a 1 = X  X ast X  and a should not both contribute towards the observable score of B . In the case of h(i) the increment depends on the amortized ex-pected deletions is: Where A B j  X  denotes the cardinality of the set of terms occur-ring both in the hypothesis and in the query, and m is To understand the process of searching for the top match from a large corpus using SED* estimates, we depart from a conceptual formulation in which alignments between the words in the query A and words in each of the sentences B j are represented as unique paths in a tree with edges corres ponding to alignment associations between a i and b j . Each root-to-leaf path represents an implicit alignment between A and B j . Partial paths with common prefixes represent hypotheses with comm on observable evidence sets. Our goal is to find the root-to-leaf path that maximizes the total SED* score as evidence is incrementally introduced. This conceptualiza-tion is useful to motivate the use of A * search [5] to conduct best-first tree search using our incr emental SED* approximation as described below: Algorithm: StackSED* Figure 1. StackSED* Algorithm During search we do not explicitly build the whole search tree, but rather dynamically extend hy potheses representing top partial paths in a hypothesis stack (similar to [1, 2]) and prune low per-forming ones. Our approach uses these heuristics: (1) Terms in A are introduced in increasing frequency order (decreasing rarity). (2) We consider only the top n terms in terms of rarity. (3) The lowest scoring hypotheses are pruned in each iteration. We performed experime nts based on a translation memory data-base. The task consisted of id entifying the closest match from a database consisting of 1,000,000 se ntences given a query. The test set consisted of 5,745 query sent ences. The baseline computation of the SED using DP (without backtrack step) for the test set con-sumed a median of 0.56 (x10^6) CPU cycles/sentence. Table 1 shows the average edit error dist ance between the closest sentence and the top 1 stack hypothesis (a nd in parenthesis, the median CPU cycles per sentence) for several stack and rarity cutoff con-figurations. With a stack with maximum depth of 400 and con-sidering only the top 4 terms per query, we speed up the computa-tion approximately twenty-fold while still obtaining results that have edit scores that are 0.29 edit points higher per sentence on average from the top match. Table 1. Avg. edit error and median CPU /sentence (in paren-250 0.36(0.01) 0.30(0.03) 0.34(0.07) 0.43(0.18) 400 0.36(0.01) 0.29(0.03) 0.34(0.08) 0.43(0.19) 600 0.36(0.01) 0.30(0.04) 0.36(0.09) 0.47(0.19) 900 0.36(0.01) 0.32(0.04) 0.38(0.10) 0.51(0.19) We introduced a new approach for efficient approximate string match using an inverted index us ing order information. Through a stack search and heuristics our approach reduces the search com-putation while producing near-op timal hypotheses. It can be shown that the average complexity of our approach (as imple-mented) is approximately ) (log nl S O , where l is the average number of sentence occurrences for the rarest terms in the query and |S| is the size of the stack. [1] F. Jelinek (1969), Fast sequential decoding algorithm using a [2] G. Navarro and R. Baeza-Yates (2000). A hybrid indexing [3] G. Navarro (2001), A guided t our to approximate string [4] D. Paul, (1992), An efficient A* stack decoder algorithm for [5] S. Russel and P. Norvig, (1995) Artificial Intelligence: A [6] E. Sumita, (2001). Example-base d machine translation using 
