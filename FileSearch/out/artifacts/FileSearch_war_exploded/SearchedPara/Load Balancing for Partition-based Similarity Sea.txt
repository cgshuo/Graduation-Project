 All pairs similarity search, used in many data mining and information retrieval applications, is a time consuming pro-cess. Although a partition-based approach accelerates this process by simplifying parallelism management and avoid-ing unnecessary I/O and comparison, it is still challenging to balance the computation load among parallel machines with a distributed architecture. This is mainly due to the variation in partition sizes and irregular dissimilarity rela-tionship in large datasets. This paper presents a two-stage heuristic algorithm to improve the load balance and shorten the overall processing time. We analyze the optimality and competitiveness of the proposed algorithm and demonstrates its effectiveness using several datasets. We also describe a static partitioning algorithm to even out the partition sizes while detecting more dissimilar pairs. The evaluation results show that the proposed scheme outperforms a previously de-veloped solution by up to 41% in the tested cases.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Clustering, Search Process Keywords: All-pairs similarity search; load balancing; par-titioning; competitiveness analysis
All Pairs Similarity Search (APSS) [9], which identifies similar objects in a dataset, is used in many applications including collaborative filtering based on user interests or item similarity [1], search query suggestions [22], web mir-rors and plagiarism recognition [23], coalition detection for advertisement frauds [20], spam detection [11, 17], cluster-ing [7], and near duplicates detection [16]. The complexity of na  X   X ve APSS can be quadratic to the dataset size. Previ-ous researches on expediting similarity computing developed filtering methods [9, 27, 4], inverted indexing [19, 21], or par-titioning and parallelization techniques [3]. It is shown in [3] that APSS can be performed through a number of indepen-dent tasks where each compares a partition of vectors with other candidate vectors. This approach outperforms other approaches [19, 8] by an order of magnitude because of the simplified parallelism management and aggressive elimina-tion of unnecessary I/O and comparison.

Given a large number of data partitions, we need to assign them to parallel machines and decide the direction of sim-ilarity comparison due to the symmetric property of com-parison. Load imbalance can hugely affect scalability and overall performance. In this paper, we explore load balanc-ing issues and develop a two-stage assignment algorithm to improve the execution efficiency of APSS. The first stage constructs a preliminary load assignment over tasks. The second stage refines the assignment to be more balanced. Our analysis shows that the developed solution is competi-tive to the optimum with a constant ratio. We further im-prove the dissimilarity detection ability in the static parti-tioning step, while producing partitions with relatively even sizes to facilitate the load balancing step.

The paper is organized as follows. Section 2 reviews some background and related work. Section 3 discusses the design framework. Section 4 introduces the two-stage load assign-ment algorithm and its performance properties. Section 5 presents the improved data partitioning scheme. Section 6 is the experimental evaluation. Section 7 concludes this paper and the appendix lists the proofs of the analytic results.
Following the work in [9], the APSS problem is defined as follows. Given a set of vectors d i = { w i, 1 ,w i, 2 ,  X  X  X  ,w where each vector contains at most m features and may be normalized to a unit length, the cosine-based similarity between two vectors is computed as: Two vectors d i ,d j are considered similar if their similarity score exceeds a threshold  X  , namely Sim ( d i ,d j )  X   X  . The time complexity of APSS is high for a big dataset. There are application-specific methods applied for data preprocessing. For example, text mining removes stop-words or features with extremely high document vector frequency [7, 13, 19].
There are several groups of optimization techniques de-veloped in the previous work to accelerate APSS.

Dynamic computation filtering . Partially accumu-lated similarity scores can be monitored at runtime and dis-similar document pairs can be detected dynamically without complete derivation of final similarity scores [9, 27, 21]. Similarity-based grouping in data pre-processing .
 The search scope for similarity can be reduced when po-tentially similar vectors are placed in one group. One can use an inverted index [27, 19, 21] developed for informa-tion retrieval [7]. This approach identifies vectors that share at least one feature as potentially similar, so certain data traversal is avoided. Similarly, the work in [25] maps feature-sharing vectors to the same group for group-wise parallel computation. This technique is more suitable for vectors with low sharing pattern, otherwise it suffers from excessive redundant computation among groups. Locality-sensitive hashing (LSH) can be considered as grouping similar vectors into one bucket with approximation [15, 24]. This approach has a trade-off between precision and recall, and may intro-duce redundant computation when multiple hash functions are used. A study [4] shows that exact comparison algo-rithms can deliver performance competitive to LSH when computation filtering is used. In partition-based APSS [3], dissimilar vectors are identified in the static partitioning step. The APSS problem is then converted to executing a set of independent tasks each compares one partition with some of the other partitions. These tasks can be executed in parallel with much simplified parallelism management.
Load balancing and scheduling . Exploiting parallel resources over thousands of machines for scalable perfor-mance is important and challenging. Load balancing is con-sidered in the context of search systems for index serving [6, 18]. A recent study [26] introduces a division scheme to im-prove load balance for dense APSS problems using multiple rounds of MapReduce computation. In order to minimize the communication overhead while maintaining the compu-tational load balance, in this paper, we focus on load bal-ancing of APSS with record-based partitioning. The general load balancing and scheduling techniques for clusters and parallel systems have been extensively addressed in previ-ous work. A simple greedy policy [14] that maps a ready task to a computation unit once it becomes idle is widely adopted (e.g. [10]). Scheduling for MapReduce systems such as Hadoop [12, 28] has followed the greedy policy to execute queued tasks on available cores and exploit data locality whenever feasible. Assuming that parallel tasks are sched-uled following such a greedy policy, we address how these tasks should be formed considering scalability and efficiency.
This section gives an overview of the partition-based frame-work [3] and presents the load balancing problem.
The framework for Partition-based Similarity Search (PSS) consists of two phases. The first phase divides the dataset into a set of partitions. During this process, the dissimilar-ity among partitions is identified so that unnecessary data I/O and comparisons among them are avoided. The second phase assigns a partition to each task at runtime and each task compares this partition with other potentially similar partitions. These tasks are independent when running on a set of parallel machines. Figure 1 depicts the whole process.
Dissimilarity-based partitioning identifies dissimilar vec-tors without explicitly computing the product of their fea-tures. One approach [3] utilizes the following inequality that calculates the 1-norm and  X  -norm of each vector: Figure 1: Illustration of partition-based similarity search. Task T k Read all vectors from assigned partition P k Build inverted index of these vectors
Conduct self-comparison among vectors in P k repeat until all non-dissimilar partitions are fetched The partitioning algorithm sorts the vectors based on their 1-norm values first. It then uses the sorted list to iden-tify dissimilar pairs ( d i , d j ) satisfying inequality k d dissimilarity-based partitioning.

Once the dataset is separated into v partitions, v inde-pendent tasks are scheduled. Each task is responsible for a partition and compares this partition with all potentially similar partitions. We assume that the assigned partition for each task fits the memory of one machine as the data par-titioning can be adjusted to satisfy such condition. Other partitions to be compared with may not fit the remaining memory and need to be fetched gradually from a local or remote storage. In a computing cluster with a distributed file system such as Hadoop, tasks can seamlessly fetch data without concerning about the physical locations of data.
Figure 2 describes the function of each task T k in partition-based similarity search. Task T k loads the assigned parti-tion P k and produces an inverted index to be used during the partition-wise comparison. Next, T k fetches a number of vectors from potentially similar partitions and compares them with the local partition P k . Fetch and comparison is repeated until all candidate partitions are processed.
We formalize the load assignment problem as follows. The data partitioning phase defines a set of v partitions and their potentially similar relationship. This can be represented as a graph, called a similarity graph defined next.
 Definition 1 Similarity graph ( G ): Let G be an undi-rected graph where each node represents a data partition and each edge indicates potential similarity relationship between the two partitions it connects.

Since the similarity result of two vectors is symmetric, comparison between two partitions P i and P j should be only conducted by one of the corresponding tasks T i or T load assignment algorithm determines which task performs this comparison. The load assignment process converts the undirected similarity graph into a directed graph in which the direction of each edge indicates which task conducts the corresponding comparison. We call this a comparison graph and it is defined as follows.
 Definition 2 Comparison graph ( D ): Let D be a di-rected graph where each node represents a data partition. An edge e i,j from partition P i to P j indicates that task T compares P j with P i . Figure 3: (a) An undirected similarity graph; node weights are partition sizes. (b) A directed comparison graph for (a); node weights are the corresponding task cost. (c) Another comparison graph for (a).

Comparison graph D contains the same set of nodes and edges as the corresponding similarity graph G , except that the edges in D are directed. The directed edges reveal the data flow direction when comparing two potentially similar partitions. Figure 3(a) illustrates a similarity graph with seven nodes. P 1 is potentially similar to P 2 , P 4 and P instance. The comparison between P 1 and P 2 can be per-formed by either T 1 or T 2 . The numbers marked inside the graph nodes are partition sizes, proportional to the num-ber of vectors in the partition. Figures 3(b) and 3(c) show two comparison graphs with different load assignments. The number marked inside a comparison graph node is the cor-responding task cost and we explain the cost model below.
The cost function of each task consists of computation cost and data I/O cost. For each task defined in Figure 2, the computation cost includes the cost of an inverted index look-up, multiplication and addition, and memory/cache ac-cesses. While a thorough cost model involves memory hier-archy analysis [2], the overall computation cost can be ap-proximated as proportional to the size of the corresponding partition P i multiplied by the size of the potentially similar partitions to be compared with. The data I/O cost occurs when fetching P i and other partitions from local or remote machines, and also when storing the detected similarity re-sults on disk. Since the start-up I/O cost and transmission bandwidth difference to the local or remote storage are rel-atively small, the I/O cost is approximately proportional to the size of the partitions involved. Note that the runtime scheduling that maps tasks to machines is affected by data locality. As we discuss later, the computation cost is domi-nating in APSS and thus the I/O cost difference caused by data locality is not sufficient enough to alter our optimiza-tion results in terms of competitiveness to the optimum.
Define the cost of task T i corresponding to partition P i comparison graph D as:
Cost ( T i ) = f ( P i ,P i ) + f c ( P i ) + X where f ( P i ,P i ) is the self comparison cost for partition i and is quadratically proportional to the size of P i . f ( P is the comparison cost between partition i and j . It sat-tional to the size of P i multiplied by size of P j is the I/O and communication cost to fetch partition P from local and/or remote storage and output the results of self-comparison. f c ( P j ) is the cost to fetch partition P and output the similar pairs between P i and P j . For Fig-ures 3(b) and 3(c), f ( P i ,P j ) is a multiplication of the sizes of P i and P j , and f c ( P i ) is estimated as 10% of the size of P . In Figure 3(c), Cost ( T 5 )=67.1 because f ( P 5 ,P f ( P 5 ,P 4 )=30, f c ( P 5 )=0.6 and f c ( P 4 )=0.5.
Different edge direction assignments can lead to a large variation in task weights. Let Cost ( D ) = max P i  X  D Cost ( T For example, in Figure 3(b) Cost ( D )=86.7 based on Cost ( T In Figure 3(c) Cost ( D )=67.1. Deriving a comparison graph that minimizes the maximum cost among all tasks is a key strategy in our design. As the load is shifted from the heavi-est task to the other tasks, better load balancing is achieved.
A circular mapping solution in [3] compares a partition with half of other partitions, if they are potentially similar. When the number of partitions is odd, task T i compares P i with partitions P j where j belongs to the set: i % v + 1 , ( i + 1)% v + 1 ,  X  X  X  , ( i + v  X  3 2 )% v + 1. Figure 3(b) shows the circular solution for the similarity graph in Figure 3(a). T 1 is assigned to compare with partitions from P 2 to P hence the edge is directed from P 2 and P 4 to P 1 . Similarly, the comparison between P 1 and P 5 is assigned to P 5 . The circular approach is reasonable when the distribution of node connectivity and partition sizes is not skewed. In practice, that is often not true.
 Table 1: Distribution statistics for partition size and parallel execution time with circular load assignment.

Table 1 shows the variance of partition sizes and task costs in three datasets. The largest partition size could be many times larger than the average partition size and the stan-dard deviation compared to the average size is also high. Additionally, the similarity relationship among partitions is highly irregular. Some partitions have lots of edges in simi-larity graph while others have sparse connections. Circular load assignment treats all partitions equally regardless of such variations and as a result, a task could be assigned all the comparison loads while its counterpart tasks are very light. Column 5 of Table 1 shows the maximum divided by average task cost using circular assignment.

The ultimate goal of load assignment is to schedule com-putation to parallel machines with minimum job completion time. Since undirected edges in a similarity graph creates uncertainty in task workload, the key question here is what to optimize. Will balancing the task costs computed from the comparison graph help speedup the runtime execution without knowing the allocated computing resource in ad-vance? In the next section, we discuss our optimization strategy and present a two-stage assignment algorithm.
Our algorithm for load assignment consists of two stages to derive a comparison graph with balanced load among tasks. The design considers uneven partition sizes and irreg-ular dissimilarity relationship. The derived tasks are sched-uled at runtime to q cores and the tasks with reduced varia-tion in sizes contribute to better performance after schedul-ing. We will show that such a strategy can produce a so-lution competitive to the optimal solution for scheduling a similarity graph on a given number of cores. We discuss the two-stage algorithm in the following two subsections.
The purpose of Stage 1 of this algorithm is to produce an initial load assignment such that tasks with small par-titions conduct more comparisons. This stage performs v steps where v is the total number of partitions in the given similarity graph. Each step identifies a partition, determines the direction of its similarity edges, and adds this partition along with these directed edges to comparison graph.
More specifically, each step works on a subgraph of the original undirected graph G , called G k at step k . G the original graph G . At step k , the algorithm identifies partition P x with the lowest potential computation weight ( PW ). The potential computation weight for task T x based on subgraph G k is defined as: It represents the largest possible computation weight for task T x given the undirected edges in G k . G k +1 is derived from G k by removing the selected partition P x and its edges in G . These edges connecting P x in G k are chosen to point to P x in the generated directed graph.
 F igure 4: The first two steps in Stage 1 in the right figure, along with the PW values in the left table.

Figure 4 illustrates the first two steps in Stage 1. The left part of the figure lists the initial PW values of each node, as well as the corresponding values after the first step and second step. Partition P 2 has the lowest PW value initially and is selected at Step 1. Edges connecting P 2 all directed to P 2 in the formed directed graph. The PW values of the partitions adjacent to P 2 are changed from G to G 2 . Step 2 identifies P 6 as the the lowest PW in G removing it and its edges from G 2 . Finally the outcome of Stage 1 produces a comparison graph shown in Figure 5(a).
The cost of a task at Step k is considered to be determined if its corresponding partition has been selected before Step k . Otherwise, a task has a potential cost that equals to PW value plus possible I/O cost. Figure 6 shows the standard deviation of task costs at the first 200 steps using Cost ( T this task is determined, or its potential computation weight Figure 5: (a) The assignment produced in Stage 1. (b) The first refinement step in Stage 2: reversing edge e 5 , 4 F igure 6: Monotonic decrease of the cost standard deviation in the first 200 steps in Stage 1 for Twitter dataset. The values are normalized by the average task computation cost. PW if it is undetermined. The step-wise trend illustrates that Stage 1 gradually reduces the variation of task costs.
Stage 1 pushes the computation load to the tasks with potentially low weight. This technique works better when partitions have highly skewed sizes since the lightest parti-tions absorb as much workload as possible. However, this greedy heuristic may cause some tasks to carry an excessive amount of computation. Another issue is that Stage 1 does not consider data I/O and communication cost, so the effect of optimization might be weakened. Hence, we introduce Stage 2 to further refine the assignment produced by Stage 1 and mitigate the aforementioned weakness.
Stage 2 conducts a number of refinement steps to reduce the load of the heavy tasks by gradually shifting part of their computation to their lightest neighbors. It performs the following procedure: 1. Find the task with the highest assigned cost Cost ( T
Identify one of P x  X  X  incoming neighbors, say P y , with the lowest cost among these neighbors, and reverse the direc-tion of this edge from e y,x to e x,y . Such a reversion causes a cost increase for T y and a cost decrease for T x . However, if the new cost of T y becomes the same or larger than the original cost of T x , this edge reversion is rejected. When an edge reversion is rejected, we continue with the incom-ing neighbor that has the second lowest cost. Repeat this process until a suitable neighbor is found so that the edge reversion successfully reduces Cost ( T x ). If all incoming neighbors of P x are probed but no flip reduces Cost ( T x successfully, mark Cost ( T x ) as non-reducible. 2. Repeat the above step for the task with the highest weight after the update. If such a task is non-reducible, try the reducible task with the next highest weight. If all nodes are marked non-reducible or the number of itera-tions tried reaches a predefined limit, the algorithm stops. Figure 5(b) depicts the first refinement upon the output of Stage 1. The first edge probed in Figure 5(a) is e 5 , 4 T 4 has the highest cost and T 5 has the lowest cost among all incoming neighbors of P 4 (i.e. P 1 and P 5 ). The reversion boosts T 5 to be the task with the highest assigned weight, ready for the next probe. Since the flip of any incoming edge to P 5 does not further reduce Cost ( T 5 ), we do not flip. Finally, Stage 2 produces a comparison graph as shown in Figure 3(c) with Cost ( D )=67 . 1.
We do not know how the optimum scheduling solution dynamically maps tasks to machines at runtime as shown in Figure 7. However, we can use a bound analysis to show that our heuristic approach performs competitively in a con-stant factor compared to the optimum. We first address the load balancing issue without awareness of the machine loca-tion. Network distances impact the I/O and communication cost, but this cost is relatively less significant compared to computation load imbalance in PSS. Define This ratio represents the overhead ratio of I/O and commu-nication involved in each task compared to its computation. In our experiments as shown in Table 3, I/O overhead is rel-atively small. Given this computation-dominating setting, for a cluster of machines with multiple CPU cores, we will simply view that the whole cluster has q cores without differ-entiating their machine location. The overhead in accessing data locally or remotely is captured in ratio  X  .

Theorem 1 shows the result of two-stage load assignment algorithm is competitive to the smallest possible cost with-out knowing the number of cores available. Theorems 2 and 3 characterize the competitiveness of the algorithm to the optimum when the similarity graph is scheduled to q cores. The theorem proofs are listed in the appendix.
 Theorem 1 Define Cost min ( G ) as the smallest cost of a comparison graph derived from a given similarity graph G . The two-stage load assignment algorithm produces a com-parison graph D with Cost ( D ) competitive to Cost min Their relative ratio satisfies
The above result shows that the tasks produced by the two-stage algorithm have a fairly balanced cost distribu-tion. As illustrated in Figure 7, a simple runtime scheduling heuristic is to assign tasks to idle computing units when-ever they become available [14]. For example, the Hadoop MapReduce [12] scheduler works by assigning ready tasks in a greedy fashion with the best effort of preserving data locality. Once the central job tracker detects the availability of a task tracker, it assigns a ready task to the task tracker as long as there exists an unassigned task. When deciding which task to assign, it favors the tasks processing data lo-cal to or close to the machine of the task tracker. What is the performance behavior of our comparison tasks scheduled under such a greedy policy?
The next theorem shows that under a greedy scheduler, the tasks produced by the two-stage algorithm perform com-petitively compared to an optimum solution.
 Figure 7: Greedy execution of v tasks at runtime on a cluster of machines with q cores.
 Theorem 2 The two-stage load assignment with a greedy scheduler produces a solution with job completion time PT competitive to the optimal solution with completion time PT Their relative ratio for dedicated q cores satisfies
Our analysis in the appendix shows that with computation-dominating tasks and a greedy scheduling policy, the upper bound of execution time is affected by the weight of the heaviest task. This supports our load balancing optimiza-tion that targets the minimization of the maximum task weight during load assignment.

Stage 1 may produce an unbalanced initial assignment in which some nodes absorb too much computation, especially in dense graphs. Stage 2 mitigates this issue with a sequence of refinements. The following theorem illustrates that for a fully connected graph, our approach delivers a near-optimal solution, and it can be inferred from the proof that the re-finement process carried out in Stage 2 is the main reason that this goal is accomplished.
 Theorem 3 The two-stage load assignment with a greedy scheduler is competitive to the optimum for a fully connected similarity graph with equal partition sizes and equal compu-tation costs in self-comparison and inter-partition compari-son. Their relative ratio satisfies
This section presents an improved partitioning method for Phase 1 of partition-based similarity search presented in [3]. The goal of this improvement is twofold: 1) to detect more dissimilarity among partitions to avoid unnecessary data I/O and comparison, and 2) to reduce the size gap among partitions and facilitate the load balancing process.
To identify more dissimilar vectors without explicitly com-puting the product of their features, we use H  X  older X  X  inequal-ity to bound the similarity of two vectors: where 1 r + 1 s = 1. k X k r and k X k s are r -norm and s -norm values. r -norm is defined as
With r = 1, s =  X  , the inequality becomes Sim ( d i ,d j k d i k 1 k d j k  X  , which is a special case introduced in [3].
If the similarity upper-bound is less than  X  , such vec-tors are not similar and comparison between them can be avoided. The algorithm that produces partitions following H  X  older X  X  inequality is described as follows. 1. Divide all vectors evenly to produce l consecutive layers
L 1 , L 2 ,  X  X  X  ,L l such that all vectors in L k have lower r -norm values than the ones in L k +1 . 2. Subdivide each layer further as follows. For the i -th layer
L i , divide its vectors into i disjoint sublayers L i, 1  X  X  X  , L i,j . With j &lt; i , members in sublayer L i,j are ex-tracted from L i by comparing with the maximum r -norm value in layer L j :
This partitioning algorithm has a complexity of O ( n log n ) for n vectors and can be easily parallelized. Each sublayer is considered as a data partition and these partitions have dissimilarity relationship with the following property. Proposition 1 Given i &gt; j , vectors in sublayer L i,j not similar to the ones in any sublayer L k,h where k  X  j and k  X  h .
 Figure 8: Dissimilarity relationship among data partitions.
Figure 8 illustrates the dissimilarity relationship among these sublayers as partitions and each pointing edge repre-sents a dissimilarity relationship. For example, L i, 2 is not similar to L 1 , 1 ,L 2 , 1 , or L 2 , 2 in the top two layers.
To facilitate load balancing in the later phase, we aim at creating more evenly-sized partitions at the dissimilarity de-tection phase. One way is to divide the large sublayers into smaller partitions. Its weakness is that it introduces more potential similarity edges among these partitions, hence the similarity graph produced becomes denser, more communi-cation and I/O overhead are incurred during runtime. An-other method targets at approximately the same L i,j for any i  X  j using a non-uniform layer size. For example, let the size of layer L k be proportional to the index value k , following the fact that the number of sublayers in L k is k in our algorithm. The main weakness of this approach is that less dissimilarity relationships are detected as the top layers become much smaller.

We adopt a hierarchical partitioning that identifies large sublayers, detects dissimilar vectors inside these sublayers, and recursively divides them using the procedure discussed in Section 5.1. The recursion stops for a sublayer when reaching a partition size threshold. Each partition inherits the dissimilar relationship from its original sublayer. The new partitions together with the undivided sublayers form the undirected similarity graph G ready for load assignment. We have implemented our algorithms in Java using Hadoop MapReduce. Prior to the comparison computation, records are grouped into dissimilar partitions and this partitioning step including norm value sorting is parallelized. The cost of parallel partitioning is relatively small and is roughly 3% of the total parallel execution time in our experiments. During the load balancing step, the two-stage algorithm defines the comparison direction among potentially similar partitions, generates a comparison graph stored in a distributed cache provided by Hadoop, and derives a set of parallel tasks de-fined in Figure 2.

Hadoop runtime scheduler monitors the load of live nodes in the cluster and assigns a PSS task to the first idle core. Such a dynamic and greedy scheme can absorb potential skewness in data that fluctuates the actual computational cost. Theorem 2 reflects the competitiveness of PSS tasks scheduled under Hadoop greedy policy. During execution, each task loads the assigned partition with a user-defined reader, obtains a list of partitions to be compared with from the comparison graph file, and loops through the partition list to conduct partition-wise comparison.
The following datasets are used: 1) Twitter dataset con-taining 100 million tweets with 18.5 features per tweet on av-erage after pre-processing. Dataset includes 20 million real user tweets and additional 80 million synthetic data gen-erated based on the distribution pattern of the real Twit-ter data but with different dictionary words. 2) ClueWeb dataset containing about 40 million web pages, randomly selected from the ClueWeb collection [5]. The average num-ber of features is 320 per web page. We choose 40M records because it is already big enough to illustrate the scalabil-ity. 3) Yahoo! music dataset (YMusic) used to investigate the song similarity for music recommendation. It contains 1,000,990 users rating 624,961 songs with an average feature vector size 404.5.
 A MD/df-limit 1 .27 7 97  X  4 .55 7 ,286  X  6 .23 Table 2: Sequential time in hours on AMD Opteron 2218 2.6GHz and Intel X5650 2.66GHz processors (  X  =0.8).
Experiments are conducted on a cluster of servers each with 4-core AMD Opteron 2218 2.6GHz processors and 8G memory and a cluster with Intel X5650 6-core 2.66GHz dual processors and 24GB of memory per node. Rows 3 and 4 of Table 2 list the sequential execution time in hours for three benchmarks with different input sizes when running PSS with static partitioning and two-stage load balancing. We set similarity threshold  X  as 0.8 throughout our experi-ments unless otherwise specified. The values marked with are estimated by sampling part of its computation tasks and considering the fact that computation load grows quadrat-ically as problem size grows. From the results in Rows 3 and 4, APSS is a time consuming process. Even for a Twit-ter dataset with 20M tweets, the entire dataset can fit in the memory; but it still takes many days to produce the re-sults. Parallelization can shorten the job turnaround time and speedup iterative data analysis and experimentation.
Stop words are removed in the Twitter and ClueWeb in-put datasets; additional approximated preprocessing may be applied to reduce sequential time significantly if the trade-off in accuracy is acceptable [13, 19]. For example, the bottom row of Table 2, marked as  X  X f-limit X , lists the sequential time on an AMD core after removing features with their vector frequency exceeding an upper limit proposed in [19]. After sampling a 8M ClueWeb dataset, 49 words with document frequency above 200,000 are excluded in web page compar-ison and the sequential time is shortened by 11x. Using this df-limit strategy reduces the sequential time by 35.3x or more for Twitter and by 5.1x for YMusic. In the rest of this section, we report performance without using approxi-mated preprocessing such as df-limit.

Noted that the algorithms discussed in this paper con-duct exact similarity comparison with no approximation for a given input dataset. One may consider using approxi-mation such as LSH mapping which roughly groups similar vectors into buckets and our algorithm can be applied within each big bucket produced by LSH. This is beyond the scope of this paper and we will investigate this in the future work.
In the rest of this section, we mainly report performance on the AMD cluster because a larger Intel cluster environ-ment was less available for us to conduct experiments. Our evaluation has the following objectives: 1) Demonstrate the problem complexity and the execution scalability of tasks produced by the two-stage load balancing method. We re-port the speedup over the sequential time as we scale the number of cores. 2) Assess the effectiveness of the proposed two-stage optimization compared with the circular load bal-ancing scheme. 3) Evaluate the performance of the general-ized static partitioning algorithm in detecting dissimilarity and narrowing the size gaps among partitions. Figure 9: X axis is the number of cores used. Left Y axis is speedup. Right Y axis is parallel execution time.

Figure 9 shows the speedup and parallel time for process-ing 40M ClueWeb dataset, 100M Twitter dataset and 625K YMusic dataset when varying the number of AMD cores. Due to the time constraint in our shared cluster environ-ment, we report the average execution time of multiple runs after randomly selecting 10% of ClueWeb parallel tasks and 20% of Twitter tasks. Such a sampling methodology follows the one used in [19]. Speedup is defined as the sequential time of these tasks divided by the parallel time. The perfor-mance of our scheme scales well as the number of CPU cores increases. The efficiency is defined as the speedup divided by the number of cores used. For the two larger datasets, the efficiency is about 83.7% for ClueWeb and 78% for Twit-ter when 100 cores are used. When running on 300 cores, the efficiency can still reach 75.6% for ClueWeb and 71.7% for Twitter. The decline is most likely caused by the in-creased I/O and communication overhead among machines in a larger cluster. Efficiency for YMusic with 31.95 hour se-quential time are 76.2% with 100 cores and 42.6% with 300 cores. There is no significant reduction of parallel time from 200 cores to 300 cores, remaining about 15 minutes. The problem size of this dataset is not large enough to use more cores for amortizing overhead. Still parallelization shortens search time and that can be important for iterative search experimentation and refinement.

We also calculate the average time for comparing each pair of vectors normalized by their average length in a dataset. Namely Parallel time  X  No of cores No of pairs wise comparison time is about 1.24 nanoseconds for Twitter and 0.74 nanoseconds for ClueWeb using 300 AMD cores given  X  = 0 . 8. Varying the number of cores affects due to the difference in parallel efficiency. Varying  X  also affects be-cause it changes the results of dissimilarity-based partition-ing and graph structure. This number can become smaller if approximated preprocessing is adopted [13, 19].

To confirm the choice of partition-based search, we have also implemented an alternative MapReduce solution to ex-ploit parallel score accumulation following the work of [19, 8] where each mapper computes partial scores and distributes them to reducers for score merging. The parallel score ac-cumulation is much slower because of the communication overhead incurred in exploiting accumulation parallelism. For example, to process 4M Twitter data using 120 cores, parallel score accumulation is 19.7x slower than partition-based similarity search which has much simpler parallelism management and has no shuffling between mappers and re-ducers. To process 7M Twitter data, parallel score accu-mulation is 25x slower. As a sanity check, we also esti-mate the normalized pair-wise comparison times reported in [19]. To compare 90K vectors with 4.59 million MED-LINE abstracts using at most 60 terms per vector on about 120 cores each with 2.8GHz CPU, it takes a MapReduce solution called PQ [19] 448 minutes with approximated pre-processing, meaning about 130.1 nanoseconds to compare each normalized vector pair.
 Cl ueWeb 3 00 2 .1% 1 .9% 7 .8% 8 8.2% Table 3: Cost of static partitioning and runtime cost distri-bution of PSS in parallel execution.

Table 3 shows that static partitioning which is also paral-lelized takes 2.1% to 3% of the total parallel execution time. The table also shows the time distribution in terms of data I/O and CPU usage for similarity comparison. Data I/O is to fetch data and write similarity results in the Hadoop distributed file system. This implies that the computation cost in APSS is dominating and hence load balance of the computation among cores is critical for overall performance.
The experiments discussed in the previous sub-section have adopted the two-stage load balancing and improved static partitioning. In the rest of this section, we assess the im-pact of optimization using 100 AMD cores for 20M Twitter, 300 cores for 8M ClueWeb, and 20 cores for YMusic. We choose these sizes for faster experimentation while the per-formance impact of optimization for larger sizes is similar. Figure 10: (a) Parallel time reduction contributed by Stages 1 and 2 compared to the circular assignment. (b) Maximum task cost and standard deviation over the average task cost with circular assignment or with two-stage assignment.
Figure 10(a) shows the improvement percentage in par-allel time using two-stage load assignment compared to the baseline circular assignment. Parallel time with two-stage assignment is about 23.2 hours for Twitter, 14 hours for ClueWeb, and 1.7 hours for YMusic respectively. The fig-ure also marks the improvement percentage contributed by Stage 1 and Stage 2 respectively. The overall improvement from the two-stage load assignment is 41% for Twitter, 32% for ClueWeb, and 27% for YMusic. Stage 1 contributes a large portion of the total improvement. Stage 2 contributes about 4% for Twitter, 12% in ClueWeb, and 10% for YMu-sic. Similarity graphs of ClueWeb and YMusic are denser and Stage 1 can be too aggressive in making the light parti-tions absorb too much comparison computation. Hence, the refinements in Stage 2 become more effective in such cases.
To examine the weight difference across all tasks, Fig-ure 10(b) shows the maximal task weight with circular map-ping or with the two-stage balancing method divided by the average task cost. It also lists the cost standard deviation divided by the average task cost. The larger these two ratios are, the more severe load imbalance is. Compared to circular mapping, the two-stage assignment reduces the Max./Avg. ratio by 32.2%, 23.5%, and 25.5% for Twitter, ClueWeb, and YMusic datasets respectively. For Std. Dev./Avg. ratio, the reduction is 42.4%, 34.0%, and 28.2% respectively.
Figure 11 provides a comparison of the improved data partitioning with different r -norms. Y axis is the percentage of pairs detected as dissimilar. r =1 reflects the results in [3]. For ClueWeb, 19% of the total pairs under comparison are detected as dissimilar with r =3 while only 10% for r =1. For Twitter, the percentage of pairs detected as dissimilar is 34% for r =4 compared to 17% for r =1. The results show that choosing r as 3 or 4 is most effective. We have used the best r value for partitioning each dataset.
Figure 11: Improved partitioning with different r -norms.
As discussed in Section 5.2, the initial layer size selection affects the size variation of the final partitions. Figure 12 gives a comparison of using uniform layer size and using non-uniform size with the marked r -norm settings. The uniform-sized layers yields better results. For ClueWeb, the uniform layers detect 2.6x as many dissimilar pairs compared to the non-uniform layers. Thus we opt for the uniform layers and recursively apply hierarchical partitioning to even out the sizes of sublayers.

Table 4 shows the effectiveness of recursive hierarchical data partitioning. The ratio of standard deviation of parti-tion sizes over the average size drops by 9.7% for Twitter, 22.3% for ClueWeb, and 3.7% for YMusic. The relatively even workload benefits the task load balancing process and reduces parallel execution time by 5% to 18% additionally. Table 4: Change of partition sizes and parallel time with or without the recursive hierarchical partitioning.
The main contribution of this paper is a two-stage load balancing algorithm for efficiently executing partition-based similarity search in parallel. The analysis provided shows its competitiveness to the optimal solution. This paper also presents an improved and hierarchical static data partition-ing method to detect dissimilarity and even out the parti-tions sizes. Our experiments demonstrate that the two-stage load assignment improves the circular assignment by up to 41% in the tested datasets. The improved static partition-ing avoids more unnecessary I/O and communication and reduces the size gaps among partitions with up to 18.23% end-performance gain in the tested cases.

Static partitioning can be processed efficiently in paral-lel, and could be further extended to handle incremental updates. Another future work is to study a hybrid scheme that integrates approximate methods such as LSH with our exact method for larger datasets when a trade-off between speed and accuracy is acceptable.
 Acknowledgments . We thank Paul Weakliem, Kadir Diri, and Fuzzy Rogers for their help with the computer cluster, and Xifeng Yan,  X  Omer E  X gecio  X glu, and the anony-mous referees for their insightful comments. This work is supported in part by NSF IIS-1118106 and Kuwait Univer-sity Scholarship. Equipment access is supported in part by the Center for Scientific Computing at CNSI/MRL under NSF DMR-1121053 and CNS-0960316. Any opinions, find-ings, conclusions or recommendations expressed in this ma-terial are those of the authors and do not necessarily reflect the views of the National Science Foundation.
 Theorem 1
Proof. Let Cost 1 ( D ) be the value of Cost ( D ) after Stage 1. Refinements in Stage 2 do not increase Cost ( D ) and thus Cost ( D )  X  Cost 1 ( D ). We just need to show that Stage 1 can reach a solution competitive to Cost min ( G ). Namely Cost 1 ( D )  X  2(1 +  X  ) Cost min ( G ) .

Let D i be a directed graph with all nodes  X  G i and all edge orientations determined through the steps from G i to G v  X  1 in stage 1, given a total of v partitions and D G = G .

We use an induction to prove this theorem. The induction goes from D v  X  1 to D 1 , reversing to the creation process in Stage 1. Towards the end of Stage 1, subgraph G v  X  1 two nodes left, and at most one edge between them. Choos-ing the partition with the smaller computation weight to perform the inter-partition comparison will add some com-munication and I/O cost, but leads to the balanced solution in this special case. Thus Cost 1 ( D v  X  1 ) = Cost min Figure 13: Illustration of D k and D k +1 for induction proof.
Our induction assumption is that the solution for sub-graph D k +1 is competitive. Namely Cost 1 ( D k +1 )  X  2(1 +  X  ) Cost min ( G k +1 ) . We want to show the solution for D k competitive. Figure 13 illustrates subgraphs D k and D k +1 Note that subgraph D k and G k both have v  X  k +1 nodes and without loss of generality, these partition nodes are called P ,P k +1 ,  X  X  X  ,P v . Cost min ( G k ) satisfies Cost min ( G k )  X  Also notice that graph G k +1 is a subgraph of G k , then Also following the definition of  X  and the setting of Cost ( T in Stage 1 of two-stage load assignment, With the induction assumption and the above three inequal-ities, the outcome of Stage 1 with respect to D k satisfies Cost 1 ( D k ) = max { Cost 1 ( D k +1 ) ,Cost ( T k ) } Therefore
Cost ( D )  X  Cost 1 ( D ) = Cost 1 ( D 1 )  X  2(1 +  X  ) Cost Theorem 2
Proof. First we examine the Gantt chart of the schedule from time 0 to PT q , identifying the total computation and I/O cost, and the idle time. Define the total computation cost as  X  = P P is the comparison graph generated by two-stage load assign-ment. Then the total computation and I/O cost is bounded by  X  (1 +  X  ). Since the scheduling algorithm assigns a task whenever there is an idle core available, the total idle time in all q cores from time 0 to time PT q is at most ( q  X  1) Cost ( D ). Then max( Cost ( D ) ,  X 
Given an optimal schedule for similarity graph G on q cores, a comparison graph can be derived. Let Cost opt ( G ) be the largest task cost in this comparison graph. Notice The optimal solution satisfies Following Theorem 1, Thus Theorem 3
Proof. Assume that the number of partitions v is an odd number and we show that all tasks formed have equal weights. The optimality for an even number v can be proved similarly.

Since all nodes have the same self-comparison cost, the same cost to compare with others, and the same cost for communication and data I/O, the cost of each task is pro-portional to the number of incoming edges for the corre-sponding node in D . We claim that every node at the end of load assignment has v  X  1 2 incoming edges in comparison graph D , namely it compares with v  X  1 2 neighbors.
We prove by contradiction. If some nodes have the num-ber of incoming edges different from v  X  1 2 , then some nodes must have more than v  X  1 2 incoming edges while some other nodes must have less than v  X  1 2 edges since the total number of edges is v ( v  X  1) 2 for a fully connected graph. Assume the heaviest nodes P x has more than v  X  1 2 incoming edges, and there exists an incoming edge from node P y with the number of incoming edges less than or equals to v  X  1 2  X  1. Figure 14 illustrates an example with contradiction.

Given all partitions have the equal size, Stage 2 of load as-signment should not have stopped since it could reverse the edge between T x and T y , causing the decrease of Cost ( T while Cost ( T y ) does not exceed the new value of Cost ( T That is a contradiction.
 Thus each task T i formed fetches from its v  X  1 2 neighbors. Tasks have the same weight, leading to a perfect task dis-tribution among q cores. Without loss of generality, we use comparison, inter-partition comparison, and data I/O re-spectively for all tasks. Then
PT q = v The above upper bound without factor 1 +  X  is the lower bound for any schedule including the optimum. Thus the solution derived is within 1 +  X  of the optimum.

