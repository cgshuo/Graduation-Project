 Methods for fusing document lists that were retrieved in re-sponse to a query often use retrieval scores (or ranks) of doc-uments in the lists. We present a novel probabilistic fusion approach that utilizes an additional source of rich informa-tion, namely, inter-document similarities. Specifically, our model integrates information induced from clusters of simi-lar documents created across the lists with that produced by some fusion method that relies on retrieval scores (ranks). Empirical evaluation shows that our approach is highly effec-tive for fusion. For example, the performance of our model is consistently better than that of the standard (effective) fusion method that it integrates. The performance also tran-scends that of standard fusion of re-ranked lists, where list re-ranking is based on clusters created from documents in the list.

Fusing document lists that we re retrieved from a corpus in response to a query so as to compile a single result list is a long studied task [9]. The lists are often produced by using multiple query representations, document representations or ranking functions [9]. Many effective fusion methods are based on the premise that documents that are highly ranked in many of the lists are likely to be relevant [13, 23, 9, 12, 2, 29].

However, different retrieved lists can contain different rele-vant documents [10, 14, 5, 3]. Hence, fusion methods based on the premise just mentioned can fall short in such sce-narios. A case in point, a relevant document that appears only in one list, and which is ranked low in this list, will be ranked low in the final result list. Nevertheless, if the docu-ment content is similar to that of other relevant documents  X  as implied by the cluster hypothesis [42]  X  and those are highly ranked in (many of) the lists, then the document can be  X  X ewarded X .

Accordingly, we present a novel probabilistic approach to fusion that lets similar documents across the lists pro-vide relevance-status support to each other. Our model integrates information produced by some standard fusion method, which relies on retrieval scores (ranks) of docu-ments in the lists, with that induced from clusters that are created from similar documents across the lists.

Empirical evaluation performed using TREC data demon-strates the effectiveness of our model. For example, the model posts performance that is consistently better than that of the standard fusion method that it integrates for three such effective fusion methods.

Our approach is also more effective than standard fusion of cluster-based re-ranked lis ts. In other words, re-ranking each list using clusters created from documents in the list so as to improve relevance estimates in the list [45, 25, 21]; and then, using standard fusion to aggregate the re-ranked lists, is shown to be less effective than our approach that uses clusters created from documents across the lists. We further demonstrate the merit s of using across-list created clusters by showing that they can contain a (much) higher percentage of relevant documents than that in clusters cre-ated from each list separately.

Conceptually, our contributions are twofold. From a fu-sion perspective, we show that using information induced from clusters created from documents across the lists can help to substantially improve over methods that use only retrieval scores/rank informat ion. From a cluster-based re-trieval perspective, we address the multiple retrieved lists setting in which documents may appear in several lists with different retrieval scores. In contrast, previous work on cluster-based retrieval addressed the single retrieved list set-ting [45, 25, 21, 26, 46, 27, 18, 31].
Let q , d ,and C denote a query, a document, and a corpus of documents respectively. We assume that the document lists L 1 ,...,L m , each of which contains k documents, were retrievedinresponseto q by m retrievals performed over C . The retrievals could be based, for example, on differ-ent representations of q , different document representations and/or different ranking functions [9]. We use C L def = to denote the set of documents that appear in the lists.
Our goal is to utilize information from the lists L 1 ,...,L so as to assign a positive fusion score, F ( d ; q ), to d ( we set F ( d ; q ) def =0for d  X  X  L . The score should reflect d  X  X  presumed relevance to the information need underlying q .
Standard fusion methods assign F ( d ; q ) based on the re-trieval scores (or ranks) of d in the lists in which it ap-pears [9]. Specifically, documents that are highly ranked in many of the lists are often  X  X ewarded X  [13]. For example, if S i ( d )is d  X  X  positive retrieval score in L i , and assuming that S i ( d ) [13] scores d by the sum of its retrieval scores: The CombMNZ method [13, 22] further rewards docu-ments that appear in many lists: The Borda method [47], on the other hand, considers only rank information. Specifically, d is scored by the number of documents not ranked higher than it in the lists:
A rich source of additional information that can be used to estimate d  X  X  relevance, and which is not utilized by standard fusion methods, is inter-document similarities . For example, suppose that d  X  X  content is highly similar to that of many documents that appear in the highest ranks of the lists. Pre-sumably, then, d and these documents discuss the same top-ics/aspects. Hence, d should potentially be rewarded even if, in the extreme case, it appears in only one list in which it is ranked low. On the other hand, fusion methods that rely only on retrieval scores (ranks) will  X  X enalize X  d and rank it low in the final result list.

Thus, inspired by the cluster hypothesis [42], which states that  X  X losely associated documents tend to be relevant to the same requests X , we opt to devise a fusion method that lets similar documents across the lists provide relevance-status support to each other. To that end, we consider a set Cl ( of document clusters created from C L (the set of all docu-ments in the lists) using some clustering algorithm; c will be used to denote a cluster. These clusters could potentially be viewed as representing query related  X  X spects X , by the virtue of the way they are created, that is, from documents retrieved in response to the query. Hence, our goal becomes integrating cluster-based information with information used by standard fusion methods  X  re trieval scores and/or ranks of documents in the lists.
Our goal is to estimate p ( d | q ), the probability that d is relevant to the information need underlying q . Inspired by work on cluster-based retrieval in the language-modeling framework [20, 19], we use clusters as proxies for documents to estimate this probability. However, in contrast to these approaches [20, 19], which focus on a single retrieved list, we do not use language-model estimates for probabilities of the form p ( x | y ) in the derivation to follow. Rather, we ex-ploit the rich information provided by the occurrences and retrieval scores of documents in the lists to devise estimates. Thus, the resultant cluster-based fusion model that we de-rive is different  X  in the use of probabilities and in the formation of their estimates  X  from the single-list models by which its development was inspired.
 Model derivation. We can write the probability for d  X  X  rel-evance as:
To estimate p ( d | c, q ), we use a linear mixture governed by a free parameter  X  : X  p ( d | c, q ) def =(1  X   X  ) p ( d [20]; we use the notation  X  p (  X  ), here and after, to denote esti-mates of (model) probabilities. Using  X  p ( d | c, q )inEquation 1, along with assuming that p ( c | q ) is a probability distribu-tion over Cl ( C L )  X  the universe of clusters that we consider, and then applying some probabilistic algebra, results in our ClustFuse algorithm: F
ClustFuse uses a two component mixture model to score d . The first is the original probability of relevance, p ( d from which we back off to using cluster-based information. The second component, which uses clusters as proxies for documents,  X  X ewards X  d if it is strongly associated with clus-ters c (as measured by p ( d | c )) that presumably contain a high percentage of information pertaining to q (as measured by p ( c | q )).

The remaining task for instantiating a specific algorithm from Equation 2 is deriving estimates for p ( d | q ), p ( c p ( d | c ). To that end, we assume some standard fusion method (e.g., one of those mentioned above, CombSUM, CombMNZ, or Borda) that assigns d with a score F ( d ; q )thatisbased on d  X  X  retrieval scores (ranks) in the lists. The estimates we present are not committed to a specific standard fusion method.
 Estimates. Using probability rules, we can write: p ( d | form prior distribution for documents ( p ( d )), we get that where F ( d ; q )is d  X  X  standard fusion score. Recall that F ( d ; q ) is by definition 0 for documents d not in C L .Thus,weget that is d  X  X  normalized standard-fusion score. Note that  X  p ( d a probability distribution over the entire corpus.
Similarly, assuming a uniform prior for clusters, the proba-bility that cluster c contains information pertaining to q can be written as: p ( c | q )= p ( q | c ) P like to exploit the fact that c contains documents that might have multiple appearances in the lists to be fused. Hence, we estimate p ( q | c ) based on the standard-fusion scores of c  X  X  constituent documents. Recent work on representing sets of documents, specifically, clusters of similar documents, has demonstrated the merits of a product-based (geometric) representation [27, 31]. Such representation was advocated using arguments based on information geometry [31]. Ac-cordingly, we set  X  p ( c | q ) is a probability distribution over Cl ( C L ters we use in the evaluation in Section 4 contain the same number of documents, there is no bias incurred by using the product of fusion scores of documents in a cluster. 1
To estimate p ( d | c ), the document-cluster association strength, we, again, assume a uniform document prior. Thus, we get that p ( d | c )= p ( c | d ) P we set  X  p ( c | d ) def =0for d  X  X  L . That is, we assume no association between documents not in C L and clusters of documents from C L . This assumption echoes those used in work on re-ranking a single retrieved list based on clus-ters of documents in the list [45, 25, 21]. (Refer to Section 3 for further details.) Our next task is then to estimate p ( c | d )for d  X  X  L . To that end, we use the mean inter-document-similarity between d and c  X  X  constituent docu-ments:  X  p ( c | d )  X  1 | c | documents in c ; sim (  X  ,  X  ) is the inter-document-similarity estimate used to create the clusters. (See Section 4.1 for details). Thus, we arrive to: where d  X  X  L .Asaresult, X  p ( d | c ) is a probability distribution over the corpus.

It is important to note that d (  X  X  L )doesnothavetobea member of c to have a non-zero association strength,  X  p ( d with c . Thus, even if we use a hard clustering technique, doc-uments could be deemed associated (to some degree) with clusters to which they do not belong, yielding a soft clus-tering effect. The merits of such association approach were demonstrated in work on cluster-based retrieval [19]. ClustFuse in a nutshell. Using the estimates just described in Equation 2 yields the following fusion principle, which addresses the motivation for utilizing inter-document simi-larities. Document d is rewarded based on its: (i) standard-fusion score ( F ( d ; q )), which reflects the extent to which d is highly ranked in (many of) the lists, and (ii) similarity ( X  p ( d | c )) to clusters c that contain documents highly ranked in many of the lists (as measured by  X  p ( c | q )).
We use ClustFuseCombSUM , ClustFuseCombMNZ , and ClustFuseBorda to denote the implementations of ClustFuse with the standard fusion methods described above. For  X  = 0, ClustFuseCombSUM, ClustFuseCombMNZ, and ClustFuseBorda amount to CombSUM, CombMNZ, and Borda, respectively (see Equations 2 and 3); i.e., ClustFuse reduces to the standard fusion method that it utilizes. Higher values of  X  result in more weight put on cluster-based information.
We found that using the product of retrieval scores of doc-uments in a cluster in Equation 4 yields fusion performance that is somewhat better than that of using the geometric mean of the scores. The performance is also superior to that of using the arithmetic mean (or sum) of the scores. The latter finding is in line with reports on using clusters from a single retrieved list to re-rank it [27, 31]. The ClustFuse method utilizes the estimate  X  p ( c | q )from Equation 4 for the probability that cluster c contains infor-mation pertaining to q . Naturally, the estimate should be high for clusters containing a high percentage of relevant documents. To study the extent to which  X  p ( c | q ) reflects this percentage in c , we take an approach that was used in work on ranking clusters created from a single list [26, 21, 18].
We use  X  p ( c | q ) to rank the clusters in Cl ( C L ). Then, all  X  documents of the highest ranked cluster are positioned at the top of the result list. (All clusters contain the same number,  X  , of documents.) To neutralize within-cluster or-dering effects, we only use the precision at  X  measure (p@  X  ) to evaluate the resultant retrieval performance; this preci-sion is the percentage of relevant documents in the highest ranked cluster. We use ClustRank to denote this retrieval approach, which, formally, scores d with 1 if d is a member of the highest ranked cluster, and with 0 otherwise; Clus-tRankCombSUM , ClustRankCombMNZ ,and Clus-tRankBorda denote the implementations of ClustRank with the standard fusion methods.
Most fusion approaches utilize retrieval scores or ranks of documents but not the document content [13, 44, 23, 1, 9, 12, 2, 29, 24, 34]. Our ClustFuse method, which can incorpo-rate these approaches, utilizes also information induced from clusters of similar documents. In Section 4.2 we show that ClustFuse outperforms the standard fusion method it incor-porates; specifically, CombSUM, CombMNZ and Borda.
Clusters of documents can potentially be created based on document snippets , rather than the entire document content, if the content is not (quickly) available [48]. Document snip-pets, and other content-based features, were used for fusion [6, 41, 4, 30, 33], but inter-snippet (document) similarities were not utilized in these approaches.

Similarities between document titles were used for merg-ing lists retrieved from non-ove rlapping corpora in response to a query [36]. In contrast to our approach, which oper-ates on a single corpus, retrieval scores were not integrated with these similarities, and clusters were not utilized. Doc-ument clusters were also used for selecting terms for query expansion in federated search [35]. In contrast, ClustFuse uses clusters to rank documents based on the original query. Inter-document similarities were also used to re-rank a re-trieved list using a second retrieved list [28]. However, clus-ters were not utilized. Furthermore, this asymmetric fusion (re-ranking) approach cannot be naturally extended to fus-ing several retrieved lists [28].

There is work studying the potential (e.g., for improving result interfaces) of using cl usters created from documents retrieved from several collections [7]. Specifically, cluster-based and document-based retrieval are contrasted wherein the former is based on using clusters that are known to con-tain a high percentage of relevant documents. In Section 4.2 we present a similar study applied for the single corpus case that we address here. In contrast to our work, a specific document (or cluster) ranking method was not proposed [7].
There is recent work on using a graph-based approach for fusion that utilizes inter-document similarities [16]. In contrast to ClustFuse, document clusters were not used. In Section 4.2 we further discuss this work, and demonstrate the performance merits of ClustFuse with respect to the pro-posed graph-based method.

The clusters used by ClustFuse are query specific [45], as they are created from documents retrieved in response to the query. Previous work utilizing query-specific clusters has fo-cused on re-ranking a single retrieved list using information induced from clusters of documents within the list [45, 25, 21, 26, 46, 27]. In contrast, ClustFuse fuses lists using infor-mation induced from clusters created from documents across the lists. Furthermore, ClustFuse also exploits the fact that documents might have multiple occurrences in the lists as is manifested in their standard-fusion scores.

We note that ClustFuse can, in fact, be used to re-rank a single retrieved list. The standard-fusion document score used by ClustFuse in this case is simply the retrieval score of the document in the list. This specific single list imple-mentation echoes a previous cluster-based ranking method [20]. However, while this method [20] uses language-model-based estimates for the document-query and cluster-query  X  X atch X , ClustFuse utilizes retrieval scores, however induced, for these estimates. Furthermore, we show in Section 4.2 that independently re-ranking each of the retrieved lists us-ing this variant of ClustFuse, and then using standard fusion to merge the re-ranked lists, yields performance that is infe-rior to that of the original ClustFuse implementation, which exploits information induced from across-list clusters.
There has been some work on identifying clusters of doc-uments from the same retrieved list that contain a high percentage of relevant documents [25, 21, 26, 27, 18, 31]. The estimate used by ClustFuse for the amount of query pertaining information in a cluster, which we explore as a cluster-based fusion method at its own right (ClustRank), is reminiscent of some of these methods [27, 18]; specifically, by the virtue of using the document-query  X  X atch X  of the cluster X  X  constituent documents. However, in contrast to previous methods, ClustFuse uses the standard-fusion score of documents rather than a retrieval score in a single list. Furthermore, while it was shown that there are clusters con-taining a high relevant-document percentage when created from a single retrieved list [15, 40, 32], we show in Section 4.2 that clusters created across multiple retrieved lists can contain a much higher relevant-document percentage.
We measure inter-document similarities using a previously proposed language-model-based estimate that was shown to be effective [20, 21]. Specifically, let p [  X  ] d (  X  unigram, Dirichlet-smoothed, language model induced from document d ;  X  is the smoothing parameter which is set to 1000 [49]. The similarity between documents d 1 and d 2 is defined using the KL divergence:
To cluster the set C L of documents in the retrieved lists, we use a simple nearest-neighbors-based approach [14, 20, 21, 26, 39, 27]. Some previous work [19] demonstrated the merits of using this clustering approach with respect to other clustering methods for cluster-based document retrieval us-ing a single retrieved list. For each d (  X  X  L ) we define a cluster that contains d and the  X   X  1documents d in C L ( d = d ) that yield the highest sim ( d, d ); note that the resultant clusters overlap. As relatively small clusters are known to be most effective for cluster-based retrieval [20, 21, 39, 27], we set  X  = 10, unless otherwise specified. (The performance of ClustFuse with  X  = 5 was inferior to that of using  X  = 10.)
We use TREC data for experiments: (i) the ad hoc track of trec3 (50 queries; 741,856 documents), which is based on news articles; (ii) the Web track of trec10 (50 queries; 1,692,096 documents); and, (iii) the robust track of trec12 (50 queries; 528,155 documents), which is a challenging bench-mark [43]. We apply tokenization, Porter stemming, and stopword removal (using the INQUERY list) to documents using the Lemur toolkit 2 , which is used for experiments.
Fusion methods are quite effective when the lists to be fused are relatively short [38, 41, 3]. Furthermore, utilizing similarities between top-retrieved documents is most effec-tive when the number of documents considered is relatively small [11, 21]. Hence, to maintain the number of documents to be ranked ( |C L | ) relatively small, we fuse three lists, each of which is composed of the k (= 20) highest ranked doc-uments in a submitted run in a track. In Section 4.2.3 we show that the resultant relative performance patterns are quite consistent for k  X  X  10 , 20 , 30 , 40 , 50 } . The three runs to be fused, unless otherwise stated, are randomly selected from all submitted runs in a track (both automatic and man-ual). We use run1 , run2 ,and run3 , to denote the runs in descending order of MAP(@ k ) performance. We use 20 sam-ples of randomly selected triplets of runs and report average performance over the samples. In Sections 4.2.6 and 4.2.7 we study the case of fusing the three most effective runs in atrack.
 The CombSUM and CombMNZ methods, utilized by the ClustFuse and ClustRank algorithms, require inter-list com-patibility of retrieval scores. To that end, we normalize the retrieval score of a document in a list with respect to the sum of all retrieval scores in the list. If retrieval scores are negative, which is due to using logs, we use the exponent of a score for normalization,
To evaluate retrieval performance, we use MAP(@ k )and the precision of the top 5 and 10 documents (p@5 and p@10, respectively). Statistically significant differences of perfor-mance are determined using the two-tailed paired t-test at a 95% confidence level [37]. 3
The ClustFuse method incorporates a single free param-eter,  X  . (ClustRank does not incorporate free parameters.) The value of  X  (  X  X  0 , 0 . 1 ,..., 1 } ) is set using leave-one-out cross validation performed over the entire set of queries in a track; performance is optimized in the learning phase with respect to MAP. In other words, the performance for a query is that attained using a value of  X  that maximizes MAP per-formance over all other queries in the track.
 A note on efficiency. The computational overhead posted by our methods with respect to standard fusion approaches is not significant. The clustering of a few dozen documents from the retrieved lists can be performed quickly (e.g., based on document snippets) as was shown in work on clustering www.lemurproject.org
A statistical significance test for a pair of methods in the case of 20 random samples of triplets of runs is employed upon the average (over the 20 samples) performance of the methods for the queries. cluster-based information. Note: figures are not to the same scale. the results of Web search [48]; our methods are not commit-ted to a specific clustering approach. Similar efficiency con-siderations were echoed in work on cluster-based re-ranking of a single document list [45, 25, 21]; and, in work on fusion based on document content [41, 36]. In Table 1 we present the performance numbers of Clust-Fuse. We can see that ClustFuse outperforms the standard fusion method that it incorporates in all relevant compar-isons (track  X  evaluation measure). Many of these improve-ments are substantial and statistically significant. It is also worth noting that in many cases for trec10 the standard fu-sion methods underperform run1 (the most effective of the three runs), while ClustFuse almost always improves over run1 for trec10; sometimes, to a statistically significant de-gree. Moreover, in the very few cases that ClustFuse is out-performed by run1 (mainly, MAP for trec12), these perfor-mance differences are not statistically significant. The stan-dard fusion methods, on the other hand, post in all cases for trec12 performance that is statistically significantly worse than that of run1. These findings attest to the merits of ClustFuse that integrates information induced from docu-ment clusters (created across the lists) with retrieval scores (or rank) information used by the standard fusion methods.
We next explore the effect of varying the value of  X  on the performance of ClustFuse. (Refer back to Equation 2.) For  X  = 0, ClustFuse amounts to the standard fusion method that it incorporates; higher values of  X  correspond to more weight put on cluster-based information. For  X &gt; 0 standard fusion scores of documents are used (also) for estimating the amount of query-pertaining information in a cluster. Figure 1 depicts the MAP-performance curves.

We see in Figure 1 that for  X &gt; 0, and for all three tracks, the performance of ClustFuse is better than that of the stan-dard fusion method that it incorporates, i.e., ClustFuse with  X  =0. Evenfor  X  =0 . 1, which amounts to assigning a relatively small weight to cluster-based information, some of the performance gains over not using this information (  X  = 0) are substantial; furthermore, the improvements are quite large for  X   X  X  0 . 6 , 0 . 7 , 0 . 8 } . Further increasing  X  can result in performance decrease (e.g., for trec10 and trec12) that attests to the importance of integrating the standard fusion score assigned to the document with information in-duced from clusters to which it is similar.
Insofar, we had our methods, and the reference compar-isons, fuse lists of k = 20 documents each. We now turn to study the effect of the list size, k ,onfusionperformance. As noted above, fusion methods, so as methods that utilize inter-document similarities, are quite effective when operat-ing over relatively short lists. In our case, the size of the document set to be ranked ( |C L | ) is determined by the num-ber of lists to be fused (three), the number of documents in each list ( k ), and the overlap between the lists. In Fig-ure 2 we present the MAP@ k performance of ClustFuseC-ombMNZ, and that of the standard fusion method that it integrates (CombMNZ), where k  X  X  10 , 20 , 30 , 40 , 50 } use ClustFuseCombMNZ as a representative, as it is the focus of a comparison with some previous work on utiliz-ing inter-document similarities for fusion that we present in Section 4.2.7.) For reference, we present the performance of run1  X  the most MAP@ k -effective run among the three to be fused.

As we can see in Figure 2, ClustFuseCombMNZ outper-forms CombMNZ for all values of k over all three tracks; the relative improvements are often quite substantial. Fur-thermore, while CombMNZ underperforms (to quite a sub-stantial degree) run1 for all values of k and for all three tracks, ClustFuseCombMNZ improves over run1 for trec3 and trec10 (but slightly underperforms it for trec12). These findings further attest to the benefits in using cluster-based information for fusion. We also note that the overall perfor-mance patterns are somewhat similar for k  X  20; yet, the rel-ative improvement of ClustFuseCombMNZ over CombMNZ for k = 20 can be somewhat lower than that for higher val-ues of k (e.g., for trec10). Thus, we will continue to focus in the evaluation to follow on k =20soastopresenta conservative picture of the performance our approach.
Previous work on cluster-based retrieval has focused on the single retrieved list case. Specifically, there is much work on re-ranking a retrieved list using clusters of documents in the list [45, 25, 21, 46, 27, 19]. Our approach, on the other hand, fuses several retrieved lists using clusters created across the lists. Thus, we next explore the merits for fusion of using information induced from clusters created across the lists with respect to that induced from clusters created from each list. To that end, we study a fusion paradigm that is based, in spirit, on principles underlying the cluster-based re-ranking approaches just mentioned.

First, we cluster the documents in each run. Then, we use ClustFuse to re-rank a run using its clusters. 4 The mo-tivation is to improve the relevance estimates in a run; the resultant re-ranking of run i is denoted Clust(run i ). The re-ranked runs are then fused using one of the standard fusion methods F (  X  X  CombSUM,CombMNZ,Borda } ); the fusion results are denoted F(Clust(run { i } )). The resultant perfor-mance is compared with that of ClustFuseF  X  our origi-nal fusion methods that utilize clusters created across the runs. The performance numbers, all based on a leave-one-ClustFuse only requires a retrieval score for each document, F ( d ; q ), and information about clusters and inter-document similarities. When employed over a single list, ClustFuse echoes a previous cluster-based (re-)ranking method [20]. and Borda) are marked with  X  X  X ,  X  X  X ,  X  X  X , and  X  X  X , respectively. out cross-validation performed for  X  as described above, are presented in Table 2.

Our first observation based on Table 2 is that ClustFuse is somewhat effective in re-ranking a single list. Indeed, in a majority of the relevant comparisons, the performance of Clust(run i ) is better than that of run i ; sometimes, the dif-ferences are also statistically significant. Now, fusing the re-ranked runs with the standard fusion methods yields ad-ditional (often substantial) performance improvements with respect to run2 and run3, but not with respect to run1. (Re-fer to CombSUM(Clust(run { i } )), CombMNZ(Clust(run { i } and Borda(Clust(run { i } )).)
We can also see in Table 2 that the performance of fus-ing the cluster-based re-ranked runs is always worse  X  of-ten, to a statistically significant degree  X  than that of our ClustFuse approach. (Compare CombSUM(Clust(run { i } )) with ClustFuseCombSUM, CombMNZ(Clust(run { i } )) with ClustFuseCombMNZ, and Borda(Clust(run { i } )) with Clust-FuseBorda.) Furthermore, there are many cases wherein the ClustFuse-based methods improve over run1 (the best per-forming run among the three to be fused) in a statistically-significant manner, while fusion of re-ranked runs does not. These findings attest to the merit s of utilizing clusters cre-ated across the runs as is done by ClustFuse.
The ClustFuse method, which was the focus of the evalu-ation insofar, utilizes an estimate for the presumed amount of query-pertaining information in a cluster. (Refer back to Equations 2 and 4.) The estimate is based on the standard-fusion scores of documents in the cluster. To evaluate the quality of this estimate, we devised ClustRank. This method ranks clusters based on the estimate, and positions the con-stituent documents of the highest ranked cluster at top of the result list. Hence, for clusters of  X  documents the resul-tant precision at  X  (p@  X  ) of ClustRank is the percentage of relevant documents in the highest ranked cluster. We study the performance of ClustRank with clusters of  X  =5and  X  = 10 documents in Table 3. For reference comparisons, we use OptCluster and OptCluster(run i ) : the clusters that contain the highest percentage of relevant documents among those created across the lists, and among those cre-ated from run i , respectively. In addition, we present the p@  X  performance of the standard fusion methods (CombSUM, CombMNZ and Borda) and the ClustFuse implementations.
We can see in Table 3 that the optimal clusters among those created across the lists (OptCluster) contain a very high percentage of relevant documents. If these clusters are automatically identified, then the resultant precision-at-top-ranks performance is by far better than that of all other methods considered. Furthermore, the percentage of rele-vant documents in these clusters is consistently higher than that in optimal clusters that are created from documents in a run (OptCluster(run i )). The latter finding further motivates the use of clusters created across the lists. Yet, identifying the optimal cluster in each run yields much better perfor-mance than that of the run, which is in accordance with previous reports on clustering a single retrieved list [18].
We can also see in Table 3 that ClustRank almost al-ways outperforms  X  and in several cases, statistically sig-nificantly so  X  the standard fusion methods. Furthermore, the highest ranked cluster by ClustRank, which can be com-posed of documents from the three runs, often contains higher percentage of relevant documents than that in the optimal cluster for run3 (OptCluster(run3)); however, this percent-age is still much lower than that in the optimal clusters for run2 and run1. In addition, while ClustRank almost always outperforms run1 for trec3 and trec10 (although statistically significantly so only in a single case), the reverse holds for trec12. As all other fusion methods, ClustRank outperforms run2 and run3 in a statistically significant manner.
Finally, as Table 3 shows, ClustRank is consistently less effective than ClustFuse. This is not surprising as Clus-tRank uses only the highest ranked cluster, while ClustFuse utilizes information from all clusters, and further integrates this information with standard fusion scores.

All in all, the conclusion rising from the analysis above is the following. While ClustRank is a relatively reasonable method for ranking clusters based on the presumed percent-age of relevant documents that they contain, more effective methods are called for. Evidently, the room for improve-ment, demonstrated by the numbers for the optimal clusters, is huge. We leave this challenge for future work; specifically, adapting estimates for the amount of within-cluster query-pertaining information that were proposed for the single-list setting to the multiple-lists setting that we address here. Using such estimates in ClustFuse can potentially help to improve its performance.
Heretofore, the runs to be fused were randomly selected from all those available for a track. We now turn to study the effectiveness of ClustFuse in fusing the three most MAP(@ k = 20)-effective runs in a track; these are denoted, in descending order of MAP performance, run1, run2 and run3. Naturally, this is not a realistic retrieval setting as in practice the qual-ity of retrieval is not known. Nevertheless, this is a challenge for any fusion approach.

We can see in Table 4 that both the ClustFuse methods and the standard fusion approaches that they incorporate are effective in fusing the most effective runs in a track. In-deed, the performance transcends that of the three runs in almost all relevant comparisons (track  X  evaluation mea-sure). Furthermore, in most re levant comparisons, Clust-Fuse outperforms the standard fusion method. While the performance differences are statistically significant for only a single relevant comparison, there are quite a few cases for trec3 in which ClustFuse improves over one of the runs in a statistically significant manner, and the standard fusion method does not; the reverse happens for a single relevant comparison in the table.
 We also see that the relative improvements posted by ClustFuse over the standard fusion method when fusing the best performing runs are smaller than those posted for fus-ing random runs. (Compare Table 4 with Table 1). Further-more, in a small number of cases ClustFuse is outperformed (although not to a statistically significant degree) by the standard fusion method when fusing the best performing runs, while this never happens for the random runs.
A possible explanation of these findings, following a recent report [17], can be made based on Figure 3. Figure 3 shows that the percentage of relevant documents, of those that ap-pear in the three runs, that appear only in a single run is often higher for random runs than for the best runs; thus, the relevant-documents overlap is higher for the best runs than for the random runs. This finding can help explain the fact that the standard fusion methods that depend on this overlap are much more effective for the best runs than for the random runs  X  e.g., compare their performance with that of run1 in Tables 4 and 1; especially, for trec12 for which the difference in the percentages for random and best runs presented in Figure 3 ( k = 20) is very high. Accord-ingly, our approach can improve over the standard fusion methods more for random runs than for the best runs as it differences between ClustFuseCombMNZ and GraphFuse. can help address a low relevant-documents overlap by using inter-document similarities; and, since the standard fusion methods are already quite effective for the best runs.
Finally, we hasten to point out that in both settings (ran-dom runs and best runs) ClustFuse outperforms the stan-dard fusion method that it incorporates in most relevant comparisons; and, posts more statistically significant im-provements over the runs to be fused.
As mentioned in Section 3, there is some recent work on utilizing inter-document-similarities for fusion using a graph-based approach [16]. Specifically, a graph is con-structed from document instances in the lists, and edge weights are based on inter-document similarities and re-trieval scores of documents. The stationary distribution of a Markov chain defined over the graph is used to rank doc-uments. The best performing graph-based method reported (BagDupMNZ) [16], which we refer to here as GraphFuse , amounts to CombMNZ if inter-document-similarities are not utilized. As our ClustFuseCombMNZ method also amounts to CombMNZ if inter-document-similarities are not utilized (i.e., cluster-based information is not used), we turn to com-pare the performance of ClustFuseCombMNZ with that of GraphFuse. As is the case for the  X  parameter of Clust-FuseCombMNZ, we use leave-one-out cross validation for setting the values of the two free parameters that Graph-Fuse incorporates (namely, graph out degree and edge weight smoothing factor); the search ranges for the parameter val-ues are those originally reported [16]. The performance numbers for fusing randomly selected runs and the best-performing runs are presented in Table 5. 5 We see in Table 5 that for the random-runs setting Clust-FuseCombMNZ outperforms GraphFuse to a statistically significantly degree for 5 out of 9 relevant comparisons (track  X  evaluation measure), while the reverse holds for a single relevant comparison. (Refer to the  X  X  X  marks.) Furthermore,
The performance numbers of GraphFuse that we present (for the best-runs setting) are different than those originally reported [16] for the following reasons. While we use leave-one-out cross validation to set free-parameter values (us-ing MAP for the performance-optimization criterion), those were not learned in the original report. Rather, the best po-tential average performance with respect to free-parameter values (optimized for average p@5) was presented. for trec10 ClustFuseCombMNZ outperforms CombMNZ in a statistically significant manner for all three evaluation mea-sures, while GraphFuse does so only for p@10. However, for trec12 GraphFuse is somewhat more effective than Clust-FuseCombMNZ, although not statistically significantly so. For the best-runs setup, ClustFuseCombMNZ outperforms GraphFuse in almost all relevant comparisons. Although the performance differences are not statistically significant, ClustFuseCombMNZ posts more statistically significant im-provements over the runs, especially for trec3. Furthermore, for trec12, GraphFuse underperforms CombMNZ in a statis-tically significant manner for all three evaluation measures, while ClustFuseCombMNZ outperforms CombMNZ for two evaluation measures; moreover, in contrast to GraphFuse, ClustFuseCombMNZ is never outperformed by CombMNZ in a statistically significant manner.
 All in all, we see that both ClustFuseCombMNZ and Graph-Fuse that utilize inter-document-similarities, although using different approaches, are highly effective for fusion. Yet, ClustFuseCombMNZ is somewhat more effective  X  and ro-bust, with respect to relative improvements over CombMNZ  X  than GraphFuse, which attests to the potential merits of using document clusters to exploit inter-document similari-ties. In addition, we note that an interesting venue for future work is using graph-based methods in ClustFuse. A case in point, a method similar in spirit to GraphFuse was used to find clusters containing a high percentage of relevant docu-ments in the single retrieved list setting [18]. Adapting the method to the multiple lists setting, and integrating it in ClustFuse, is a challenge we plan to address.
We presented a novel approach to fusing document lists retrieved in response to a query. The approach is based on integrating information used by standard fusion methods (i.e., retrieval scores or ranks) with that induced from clus-ters of similar documents created across the lists. Specif-ically, our model, which can incorporate various standard fusion methods, lets similar documents across the lists pro-vide relevance-status support to each other. Empirical eval-uation shows that our model outperforms the standard fu-sion method that it integrates. In addition, we showed that our cluster-based fusion approach outperforms standard fu-sion of re-ranked lists, wherein list re-ranking is based on clusters of documents from the list. We also showed that clusters created from documents across the lists can contain a much higher percentage of relevant documents than that in clusters created from documents in a list  X  a finding which further motivates our approach.
 Acknowledgments We thank the reviewers for their com-ments. This paper is based upon work supported in part by the Israel Science Foundation under grant no. 557/09, and by IBM X  X  SUR award. Any opinions, findings and conclu-sions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors.
