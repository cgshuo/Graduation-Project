 To our knowledge, all existing collaborative filtering tech-niques need to find neighbouring relationship between users or items by using some kind of similarity measurement in some feature space. However, a hypothesis hidden behind most existing works is that the neighbouring or similar re-lationship between users remains static over the whole item sets, which is not true in reality. Users who share similar opinions on some items may have totally different opinions on other items. Users can form many clusters in terms of their opinions on a set of items, However, these clusters may collapse and a new cluster structure will be built in terms their opinions on the new item sets. Analogously, clusters of items formed based on their popularity among a group of users would be disintegrated when encounter a new group of users. In a nutshell, user cluster structure varies across the item sets, and vice versa, item cluster structure also varies across the user sets.
 To deal with this collapse problem, we strive to find block structures embedded in the rating matrix in this paper. Block structure is used to characterize the interaction be-tween users and items. Each block consists of many users and items, users involved in the block share similar taste on all the items in the same block. On the other hand, all item-s in block share similar popularity among the users in this block. Every block is a meaningful colony resided by users and items, which reflects the fit degree of users X  interests and items X  attributes and the interaction between them. This paper proposes a general framework of collaborative  X  (Does NOT produce the permission block, copyright information nor page numbering). For use with ACM PROC ARTICLE-SP.CLS. Supported by ACM. filtering based on pairwise user-item blocking and its imple-mentation, which involves the unified feature extraction of users and items, the alignment of users to items, and clus-tering items based on the aligned feature representation so as to extract the block structure we expect. At last, exist-ing collaborative filtering algorithms are used to learn the latent factor at the global and block level and further make prediction on the unknown rating in the rating matrix. We develop an implementation of this framework. Experiment evidences show that the recommendation performance can be improved with utilization of these block structures. Collaborative Filtering, Dynamically Similar, Matrix Block-ing With the explosive growth amount of content available on the web, customers are suffering from the problem of in-formation overload, which has motivated the development of recommendation system. Recommendation system aims at recommending ones from a tremendous number of items to users that might interest them according to their histor-ical behaviors and item attributes or some other informa-tion. Collaborative Filtering(CF), as the most successful recommendation technique, has been very popular in both academia and industry because it is easy-to-implement and highly effective. There are mainly three kinds of collabo-rative filtering techniques, memory-based CF, model-based CF and hybrid approach.
 Memory-based CF has been adopted by many commercial systems such as Amazon. Memory-based CF first measures similarities between the active user and other users(user-based [8, 15]), or, similarities between the active item and other items(item-based [10, 16] ), to select some neighbors. Then it makes prediction by taking the weighted average of these neighbors X  ratings. This kind of method only utilizes the predictive power of ratings located in the row or column of the rating matrix corresponding to the active user or item. Model-based approach usually builds a model to predict un-known ratings according to some machine learning methods. Clustering techniques are usually used in model-based CF in order to reduce the number of similar candidates. Some model-based CF algorithms adopt dimensionality reduction and feature extraction methods to represent the user and item so as to measure the similarity between users and items accurately.
 As we all know, most of existing collaborative filtering meth-ods need to measure similarity between users and items in some feature space. We find that similarity is a basic prob-lem in collaborative filtering. Similarity can be divided into two kinds, independent similarity and dependent similarity. Independent similarity means that there is no interaction between user and item similarity. Independent similarity plays an important role in the early development of collab-orative filtering. However, user similarity only exists over some items because there can not be two users hold the same opinion over all items in the system. It is possible that user similarity over partial item sets may be covered by us-er similarity computed over all the items. Item similarity should be computed over a certain user set in the same idea. So the relationship between user similarity and item similar-ity should be close and mutually equal. Users and items can not be simply divided separately as existing works. On the other hand, dependent similarity represents that the similar-ity between users and items is interdependent and mutually decided by each other.
 To our delight, we are the first one to propose the idea that user similarity and item similarity are dynamic dependent on each other mutually. Block structure is used to describe us-er and item clusters and characterize their interdependence. We put forward a general framework of collaborative filter-ing algorithm based on pairwise user-item block structure. This framework includes extracting the unified feature of users and items, user alignment based on items, improved spectral clustering based on items and finally some exist-ing collaborative filtering techniques are used to predict un-known ratings. We also give a kind of detail implementation of this framework. Simulation results show that our method can effectively improve the performance of recommendation algorithm. Most existing CF algorithms assume user similarity and item similarity are independent. Although some works have real-ized user similarity and item similarity are mutually influ-enced, they think user similarity is static when faced with different items. In reality it is not the case. In this sec-tion, we introduce the framework of our pairwise user-item blocking process.
 Users can form clusters when faced with a item set. This cluster structure may collapse and form a new cluster struc-ture when faced with a new item set. Fig.1 shows this pro-cess. The five users form two clusters towards the left movie. However, when faced with a new movie, the five users form three clusters. In a word, user cluster structure is always changing towards different item sets. User similarity and item similarity are constantly changing and mutually de-pended on each other. In this paper, we try to explore the block structure hid in the rating matrix. Each block consists Figure 1: User cluster structure changes when faced with different item sets. of users and items. The block structure is used to represen-t user and item structure and portray their dependence on each other. Users in each block hold similar opinions on the items in the same block. Items in each block are rated similarly by users in the same block. Each block is a mean-ingful structure shared by users and items, which reflects the combination of users X  interests and items X  attributes in reality. We hope the final block structure is similar to Fig.2. Each block is a sub-matrix of the original whole rating ma-trix, rows of the sub-matrix represent users in this block and columns of the sub-matrix represent items in this block. The difference of our block structure and co-clustering is that the edge of the sub-matrix is not always aligned, which shows that the co-occurrence of user and item clustering is changing constantly.
 we present a general framework in the following to extract our expected block structure from the rating matrix. The framework is divided into 6 steps. 1. Extract a unified low-dimensional feature representa-2. Divide users into groups based on each item. For each 3. Cluster items depending on user division result based 4. Feature learning. There are two factors which may 5. Predication. In this step, the combination of global In this section, we present a detail implementation of the general framework mentioned in the previous section. In order to extract a unified low-dimensional representation of users and items we need to reexpress users and items in a same space. The higher rating of an user to an item, the closer they are in the feature space. On the contrary, the lower rating of an user to an item, the farther they are in the feature space. Enlightened by Laplacean Eigenmaps and Lo-cality Preserving Projections(LPP), we define our objective function:  X  q u  X  R k is the low-dimensional representation vector of user u . p i  X  R k is the low-dimensional representation vector of item i . is an indicator function, if the condition is true, it equals 1 otherwise it is 0. r is used to determine the distance of users and items. If the rating of an user to an item is larger than r , we hope their features are as close as possible, and the distance with the rating are inverse. Missing values won X  X  affect the solution to this objective function because the value of the objective function remains unchanged even when R ui = 0. However, Equ.1 has poor generalization performance, so we revise the object function as follows:  X   X  is used to control the degree of regularization.  X  = = = = tr ( Q T R row 2 Q ) + tr ( P T R col 2 P )  X  2 tr ( Q T = tr ([ Q T P T ] We also know tr ( Q T R 2 P ) = tr ( P T R T 2 Q ). Equ.2 can be transformed into the following formulation: where,
X = [ R 1 ] ui = R ui ( R ui  X  r ), R row 1 is a diagonal matrix. The u -th element on the main diagonal of R row 1 is is also a diagonal matrix, the i -th element in the main di-agonal is each element in the main diagonal is the sum of elements in the corresponding rows. R col 2 is also a diagonal matrix, each element in the main diagonal in the sum of elements in the corresponding columns. I is an unit matrix. X  X  R ( m + n )  X  k the former m rows represents user features and the latter n rows represents item features.
 Any multiple of the optimal solution X  X  to Equ.3 is also an optimal solution. It is also an optimal solution if it was multiplied by any number on the right, which means column orthogonal transformation does not change the optimality of the equation. What X  X  more, we adopt column orthogonal constraint to get a unique to avoid the situation when X = 0. So our formal formulation for the unified feature extraction objective function is: We can easily know M 2 is a semi-definite matrix and M 1 + I is positive definite matrix.
 Equ.4 is a standard form of the Trace Ratio optimization problem. This problem often occurs in the lower dimension of literatures [18, 19]. Recently [12] proposes a method to solve Trace Ratio problem directly, which is more accurate and efficient than other works. The matrix M 2  X  ( M 1 + I ) is very large and highly sparse, EIGFP [11] is used to get the largest eigenvalues of the matrix. Our algorithm to solve equation 4 is shown in Algorithm1.
 Algorithm 1 algorithm for Trace Ratio maximization 1: Select initial unitary matrix X , and compute 2: while not convergence do 3: compute k eigenvectors associated with k largest 5: end while After getting an unified representation of users and items, we cluster users into groups based on each item. The distance between users and items shows the degree of users X  love for items. For each item, users are grouped into clusters and then we examine the similarity of the cluster results on each item and combine similar items into one cluster. What X  X  unique in this paper is that we use a standard form of repre-sentation to express the cluster results which are useful for us to compare results. The standard representation consists of values on all the standard units. In this paper, the stan-dard unit is a real interval. We divide interval [0 ; 1] into n intervals, each interval is called a bucket. The distribution of users on buckets to represents the cluster results relative to each item. Users belong to buckets based on the following condition: bucket [ u ][ i ] represents the bucket number of user u towards item i . The distance between users and items may vary in a wide range, so we need to map the distance into the standard interval [0 ; 1]. Function f is the standardization function in this paper which is used to measure the closeness degree between user u and item i . In this paper, we have: f is called bucket mapping function, is the bucket reso-lution parameter. If we divide a movie into three buckets, corresponding to like so much, okay and dislike respectively. Then each item can be represented by the distribution of users among the buckets as shown in Fig.3. Each item can be represented by users X  clustering results. Item i can be represented by: B il represents user sets in the bucket l towards item i . If item i and j have similar user sets in the bucket B l , then item i and item j will be combined into one cluster. Jaccard similarity is used to measure the similarity between sets. It is possible that users in some bucket is the whole user set of the system, which is really a big set. It is extremely time-consuming if we compare all the elements between two sets one by one. It costs time O ( n 2 m 2 n b ). We have to explore efficient method to compute similarity between items. Min-Hashing is used by Google news recommendation system [4] to compute Jaccard similarity between items. Many exist-ing works [1,2,14] also use MinHashing to compute Jaccard similarity. In this paper, we also use Min-Hashing to com-pute the Jaccard similarity between user sets on each bucket. Hash function is used to simulate random permutation. Algorithm2 illustrates the detail of the similarity computing process. In this algorithm, P is the smallest prime number larger than m . N o ( u ) is the original serial number of user u . The final similarity matrix is high-dimensional and sparse. So sparse format is used to store the matrix.
 Algorithm 2 Compute Jaccard similarity on the bucket l by Min-Hashing 1: Number all the users, { 1 ; 2 ; 3 ; : : : } . Let k = 1 X c X n 2: while k  X  K h do 3: Randomly generate an integer ranges in [1 ; m ]. 4: Generate the k -th signature of each item i h l ( i; k ): 5: k ++. 6: end while 7: for each item i do 8: for each item j do 9: b Jaccard l ( i; j ) = ( 10: end for 11: end for Items can be grouped into clusters after getting the sim-ilarity between items. Although there are many excellent cluster algorithms, spectral clustering is used in this paper. Spectral clustering maps objects to points in an undirected graph, similarity between objects is equivalent to the edge weight. Then some suitable graph partitioning algorithms are designed. The most famous spectral cluster algorithms are ratio-cut [7] and normalized-cut [17].
 We improve spectral clustering algorithm in this paper to make it more suitable for collaborative filtering. Graph G = ( V; W ) is divided into V 1 , V 2 , V 1 The division cost is: An item is more influential and important if it owns more known ratings. So our objective is to minimize the following function: (1  X  ) nas ( V 1 ; V ) + R ( V 1 ) + assoc ( V 1 ; V ) = T i is the number of existing ratings owned by item i . T is the number of all existing ratings. is an adjustable parameter. The edge is standardized so that they all range in [0 ; 1], which has no effect on the final optimal solution. In other words, the following formulation defines the weight of each vertex: Suppose The object function in [5] is: weight ( V i ) = define cluster size. Suppose q represents a kind of division, then we can revise our objective Equ.9 as: L is the Laplacian matrix of the graph, L = D  X  W , D is a diagonal matrix. The i -th element in the main diagonal is the sum of elements in i -th row in W . Our objective is to minimize Q ( V 1 ; V 2 ). So our objective is: The solution is generalized eigenvalues: If the number of defined clusters is k l , we only need to find the top 10 minimum eigenvalues. Then we can get the final cluster results by doing k-means clustering on these eigen-values.
 Fig.4 is the block structure example we extract after doing our improved spectral clustering on items. The two movies X  similarity in the bucket  X  X islike X  is 0.25, it is the highest similarity in this example. So the two movies are in the
Figure 4: Block structure after clustering items. same cluster in this bucket. At the same time, users who dislike these two movies are also in the same cluster. Then the users and items in this cluster form a block structure as shown in this figure. There are two factors which affect users X  opinions on items. One is users X  own interests and items X  own attributes, which we call global feature. The other is the influence caused by neighbors, which we call local feature. Global feature is learned from the whole rating matrix, local feature is learned within each block. Matrix factorization has a good performance on extracting global feature. Another advantage of matrix factorization is that it does not use any similarity measurement. So matrix factorization is used to learn the global feature of users and items.
 Similar to RSVD, we learn a k -dimensional feature p u for each user and a k -dimensional feature q i for each item. In reality, people X  X  ratings on items are always not in the same scale, so we adopt Improved Regularized SVD [13]. The prediction formulation is: b u represents the rating scale of user u , b i represents the scoring scale of item i , is the mean value of all known ratings. Our objective function is: Stochastic gradient descent method can be used to solve this equation. The derivation of unknown variable in this function is listed in the following: Follow the discipline of stochastic gradient descent, we have the following iterative formulas: lrate is learning rate. The following iterative formula can be deduced if we let r u;i  X  b r u;i = e ui and load each known rating successively rather than load them all in a time.  X  We first use all known ratings to train the first dimensional feature, the second dimensional feature comes next. So the iteration number k represents the dimension. We decide to use RSVD to learn local block feature. After we learn the global feature, we need to continue analyze the residual ratings. The final rating prediction formulation is: b = block ( u; i ) is the block number which the user and item are in. p b u  X  R k , q b i  X  R k is computed by learning the resid-ual ratings in the block which ( u; i ) in. At the first step, we make predictions according to the global features learned from all the known ratings. If both the training ratings and testing ratings minus the global prediction ratings, we can get the residual ratings. Each residual rating associates with a block. Then we can treat each block as a small matrix, lo-cal block features are learned by doing RSVD on the residual rating in each block. In this section, we conduct experiments to locate optimal pa-rameters settings of our methods and conduct comparisons between our proposal algorithm and other collaborative fil-tering techniques. We use MovieLens ml-100k as our experimental data set. This data set consists of 100,000 ratings by 943 users on 1682 movies. These ratings take value from 1(lowest rating) to 5(highest rating). Each user rated at least 20 movies. We adopt 5-fold cross validation, which means the data set is divided into five disjoint splits. One split is used for test and the rest are used for training, in turn. We always report the average results, respectively in terms of several evaluation metrics discussed in the following.
 There are many metrics to evaluate the performance of a rec-ommendation system, which can be divided into three class-es: predictive accuracy metrics, classification accuracy met-rics, and rank accuracy metrics [9]. Cacheda et al [3] takes predictive accuracy metrics MAE and classification accura-cy metrics Precision and Recall together into account and obtain two new metrics, GIM and GPIM. GIM computes the absolute errors committed by the prediction on the rel-evance of the user-item pairs that are really relevant and GPIM computes that are thought to be relevant according to the prediction of recommendation system. Analogously, in this paper, we compute ordinary RMSE, GPIR and GIR. GPIR computes errors committed by the prediction for the pairs which are relevant thought by recommendation sys-tem. GIR computes errors committed by the prediction for truly relevant user-item pairs.
 First, we let test denote the set consisting of all the pairs ( u; i ) in the test set, then If the actual rating is not less than R g , then the user actually loves this item. If the prediction rating is not less than P then our algorithm predicts that the user loves this item. GIM and GPIM is computed as follows: Similarly, GIR and GPIR is computed as follows: In order to locate optimal parameters in our method, we have conducted a series of experiments. Note, we fix k = Figure 5: Distribution of Euler distance between us-er and item feature. 10 ; = 0 : 002 ; r = 3 : 5. To make Algorithm1 stop iterat-ing under appropriate guideline, we set this algorithm stops distance between user feature and item feature we extrac-t. The horizontal axis is the Euler distance between each user and each item, the vertical axis represents frequency. We can conclude from this figure that most of the distances are located at 10  X  6 , and the majority of them are close to 0. Bucket mapping function in Equ.5 is used to allocate each user a bucket number according to the distance be-tween the user and an item. is an important parameter for the construction of our block structure. Fig.6 shows the bucket mapping function curve under different . We can deduce the range of parameter . The block structure can be further deduced under different alpha . Figure 6: Bucket mapping function under different Min-hashing is used to compute the Jaccard similarity be-tween items quickly. First we need to compute the signature of each item set and then similarity is computed based on the signature similarity. Modulo function is used as hash func-tion in this paper. The parameter P in Algorithm2 is set 1901, it is the smallest prime number larger than 1682. The number of signatures can affect the estimation error of item similarity. To balance the estimation error and computing time, we experiment the impact of signature length on the estimation error of item similarity as Fig.7 shows. The hori-zontal axis represents the length of signature. = 5000000, the number of bucket is 20. The vertical axis in Fig.7 rep-resent the average error between the real similarity and the estimated similarity in each bucket. We can find that when the signature length is 300 the estimation error is the small-est. So in our experiment we set signature length 300. Figure 7: The estimation error of similarity by using Min-Hashing In the following we first use the standard normalized-cut algorithm to extract a feature for each item, then k-means clustering algorithm is used to cluster these features. In or-der to balance the computing speed and the ability to carry information, we choose to compute the 10 minimal eigenval-ues every time when solve Equ.10. We run k-means 10 times and elect the one with minimal SSE as our final result. Up to now, we can start to extract the block structure. We use improved regularized SVD to learn the global fea-ture, and stochastic gradient descent is used to solve Equ.11. To determine the stopping criterion, RMSE(Fig.8(a)) and MAE(Fig.8(b)) after each iteration is recorded. We can find a local minimum point among the iterations, the stopping point should be there. Figure 8: RSVD, the iteration of stochastic gradient descent.
 At this step we need to extract training set and test set within each block. Blocks with few users or items should be deleted. In this paper, Block with users fewer than 5 or items fewer than 5 won X  X  be learned within block. Then we con-duct in-block learning independently in each block. The di-mension of p b u , q b i are both 10. Fig.9 shows the performance improvement based on our block structure by using the s-tandard normalized-cut algorithm. Because some blocks are not learned within block, so Global in Fig.9 represents the global prediction error of the existing ratings which have be learned within block. Global+Block represents the predic-tion error of the combination of global prediction and local prediction within block. The measurements are computed based on many blocks together rather than the average val-ue. We can conclude from Fig.9 that RMSE and MAE have been improved, GIR and GIM are greatly improved, GIPR Figure 9: Performance comparison before and after block. Figure 10: Performance comparisons by adopting improved spectral cluster. anf GPIM have little improvement. So the rating predic-tion is more accurate after the within block learning. We also find that our algorithm has the best performance when = 3000000.
 To make spectral clustering suitable for collaborative filter-ing, we propose an improved spectral clustering(Equ.7) al-gorithm to cluster items. In collaborative filtering, items who own more ratings must have larger influence. We can use = 0 : 01 to control the degree. Fig.10 shows the per-formance comparisons by using our improved spectral clus-tering. We can see that GIR, MAE and GIM have been greatly improved, while other measurements have little im-provement. It proves that our method can predict users X  really loved movies more accurately. It is very important to improve users X  experience and improve users X  satisfaction with the recommendation system. What X  X  more, there will be more and more ratings added in the system along with time, so there will be new ratings in the rating matrix, which can help us judge users X  preference accurately. Our method has good expansibility, when new ratings come, we don X  X  need to train the global features, we only need to train the local within block feature which the new ratings are in. In this section, we compare our method with typical collab-orative filtering algorithms RSVD [6] and IRSVD [13]. We randomly divide the data set MovieLens ml-100k two times and conduct experiments respectively. 80% of the record-s are used as training data, the rest is used as test data. Comparison results are shown in Tab.1 and Tab.2.
 PBCF-nc in Tab.1 and Tab.2 represents using normalized-cut algorithm to cluster items in our pairwise user-item block collaborative filtering algorithm(PBCF, Collaborative Fil-tering Based on User-item Pairwise Blocking). PBCF-inc represents using improved normalized-cut algorithm to clus-ter items. RSVD and IRSVD both set K = 30, lrate = 0 : 005, = 0 : 02. In PBCF-nc, = 3000000, = 0. In PBCF-inc, = 3000000 ; = 0 : 01.
 We can find that the performance of PBCF-inc is greatly im-proved than other algorithms. In the two random divisions, PBCF-inc has slight improvements or unchanged compared with RSVD and IRSVD over the measurements (RMSE, MAE, GPIR, GPIM). However, PBCF-inc has significan-t improvement compared with RSVD and IRSVD over the measurements GIR and GIM. GIR and GIM specially mea-sure the prediction accuracy that items which users really love. In other words, our method can predict what users re-ally love more accurate, so that the recommendation system can recommend items that users indeed love, which can pro-mote users X  satisfaction with our recommendation system. In this paper, we think that the user cluster structure is always changing when faced with different items. At the same time, item cluster structure is also changing when faced with different users. So we propose that user similarity and item similarity should be dynamic dependent on each other mutually. We try to discover the hidden block structure in the rating matrix. A block structure is used to characterize the interaction between uses and items. Each block consists of many users and items. Users in each block hold similar opinions on the items in that block.
 We propose a general framework of pairwise user-item block structure algorithm. This framework includes the unified feature extraction of users and items. Divide users into groups based on each item. Cluster items according to user division results and then extract our expected block struc-ture. At last we use existing collaborative filtering algo-rithms to learn global and local within block feature and then predict unknown ratings. We also give a detail imple-mentation of this framework. The unified feature extraction problem is transformed into a standard Trace Ratio opti-mization problem. Min-Hashing is used to compute item similarity for its efficiency. We propose an improved spec-tral clustering to cluster items to make it more suitable for the collaborative filtering background and then the block structure is extracted. After getting the block structure, we use Improved Regularized SVD and RSVD to learn the global feature and local within block feature.
 Experiments shows that our method can efficiently improve the performance of a recommendation system. After ex-tracting blocks from the original rating matrix, we can great-ly improve the performance of CF algorithms. Especially we can predict what users really love in reality accurately, which can promote users X  satisfaction with our recommendation system. What X  X  more, our method can deal with changing data efficiently. When new ratings come, we only need to retrain the feature within the block which the new ratings located. We don X  X  need to retrain their global feature. [1] A. Z. Broder. On the resemblance and containment of [2] A. Z. Broder, M. Charikar, A. M. Frieze, and [3] F. Cacheda, V. Carneiro, D. Fern  X andez, and [4] A. S. Das, M. Datar, A. Garg, and S. Rajaram. [5] I. S. Dhillon. Co-clustering documents and words [6] S. Funk. Netflix update: Try this at home. [7] L. Hagen and A. B. Kahng. New spectral methods for [8] J. L. Herlocker, J. A. Konstan, A. Borchers, and [9] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and [10] G. Karypis. Evaluation of item-based top-n [11] J. H. Money and Q. Ye. Algorithm 845: Eigifp: a [12] T. T. Ngo, M. Bellalij, and Y. Saad. The trace ratio [13] A. Paterek. Improving regularized singular value [14] A. Rajaraman and J. D. Ullman. Mining of massive [15] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [16] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [17] J. Shi and J. Malik. Normalized cuts and image [18] H. Wang, S. Yan, D. Xu, X. Tang, and T. Huang. [19] S. Yan, D. Xu, B. Zhang, and H.-J. Zhang. Graph
