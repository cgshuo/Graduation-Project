 Quoc V. Le quocle@cs.stanford.edu Marc X  X urelio Ranzato ranzato@google.com Rajat Monga rajatmonga@google.com Matthieu Devin mdevin@google.com Kai Chen kaichen@google.com Greg S. Corrado gcorrado@google.com Jeff Dean jeff@google.com Andrew Y. Ng ang@cs.stanford.edu The focus of this work is to build high-level , class-specific feature detectors from unlabeled images. For instance, we would like to understand if it is possible to build a face detector from only unlabeled images. This approach is inspired by the neuroscientific conjecture that there exist highly class-specific neurons in the hu-man brain, generally and informally known as  X  X rand-mother neurons. X  The extent of class-specificity of neurons in the brain is an area of active investigation, but current experimental evidence suggests the possi-bility that some neurons in the temporal cortex are highly selective for object categories such as faces or hands ( Desimone et al. , 1984 ), and perhaps even spe-cific people ( Quiroga et al. , 2005 ).
 Contemporary computer vision methodology typically emphasizes the role of labeled data to obtain these class-specific feature detectors. For example, to build a face detector, one needs a large collection of images labeled as containing faces, often with a bounding box around the face. The need for large labeled sets poses a significant challenge for problems where labeled data are rare. Although approaches that make use of inex-pensive unlabeled data are often preferred, they have not been shown to work well for building high-level features.
 This work investigates the feasibility of building high-level features from only unlabeled data. A positive answer to this question will give rise to two significant results. Practically, this provides an inexpensive way to develop features from unlabeled data. But perhaps more importantly, it answers an intriguing question as to whether the specificity of the  X  X randmother neuron X  could possibly be learned from unlabeled data. Infor-mally, this would suggest that it is at least in principle possible that a baby learns to group faces into one class because it has seen many of them and not because it is guided by supervision or rewards.
 Unsupervised feature learning and deep learning have emerged as methodologies in machine learning for building features from unlabeled data. Using unlabeled data in the wild to learn features is the key idea be-hind the self-taught learning framework ( Raina et al. , 2007 ). Successful feature learning algorithms and their applications can be found in recent literature using a variety of approaches such as RBMs ( Hinton et al. , 2006 ), autoencoders ( Hinton &amp; Salakhutdinov , 2006 ; Bengio et al. , 2007 ), sparse coding ( Lee et al. , 2007 ) and K-means ( Coates et al. , 2011 ). So far, most of these algorithms have only succeeded in learning low-level features such as  X  X dge X  or  X  X lob X  detectors. Go-ing beyond such simple features and capturing com-plex invariances is the topic of this work.
 Recent studies observe that it is quite time intensive to train deep learning algorithms to yield state of the art results ( Ciresan et al. , 2010 ). We conjecture that the long training time is partially responsible for the lack of high-level features reported in the literature. For instance, researchers typically reduce the sizes of datasets and models in order to train networks in a practical amount of time, and these reductions under-mine the learning of high-level features.
 We address this problem by scaling up the core compo-nents involved in training deep networks: the dataset, the model, and the computational resources. First, we use a large dataset generated by sampling random are 200x200 images, much larger than typical 32x32 images used in deep learning and unsupervised fea-ture learning ( Krizhevsky , 2009 ; Ciresan et al. , 2010 ; Le et al. , 2010 ; Coates et al. , 2011 ). Our model, a deep autoencoder with pooling and local contrast nor-malization, is scaled to these large images by using a large computer cluster. To support parallelism on this cluster, we use the idea of local receptive fields, e.g., ( Raina et al. , 2009 ; Le et al. , 2010 ; 2011b ). This idea reduces communication costs between machines and thus allows model parallelism (parameters are dis-tributed across machines). Asynchronous SGD is em-ployed to support data parallelism. The model was trained in a distributed fashion on a cluster with 1,000 machines (16,000 cores) for three days.
 Experimental results using classification and visualiza-tion confirm that it is indeed possible to build high-level features from unlabeled data. In particular, using a hold-out test set consisting of faces and distractors, we discover a feature that is highly selective for faces. This result is also validated by visualization via nu-merical optimization. Control experiments show that the learned detector is not only invariant to translation but also to out-of-plane rotation and scaling. Similar experiments reveal the network also learns the concepts of cat faces and human bodies.
 The learned representations are also discriminative. Using the learned features, we obtain significant leaps in object recognition with ImageNet. For instance, on ImageNet with 20,000 categories, we achieved 15.8% accuracy, a relative improvement of 70% over the state-of-the-art. Our training dataset is constructed by sampling frames from 10 million YouTube videos. To avoid duplicates, each video contributes only one image to the dataset. Each example is a color image with 200x200 pixels. A subset of training images is shown in Ap-pendix A. To check the proportion of faces in the dataset, we run an OpenCV face detector on 60x60 randomly-sampled patches from the dataset (http://opencv.willowgarage.com/wiki/). This exper-iment shows that patches, being detected as faces by the OpenCV face detector, account for less than 3% of the 100,000 sampled patches In this section, we describe the algorithm that we use to learn features from the unlabeled training set. 3.1. Previous work Our work is inspired by recent successful algo-rithms in unsupervised feature learning and deep learning ( Hinton et al. , 2006 ; Bengio et al. , 2007 ; Ranzato et al. , 2007 ; Lee et al. , 2007 ). It is strongly influenced by the work of ( Olshausen &amp; Field , 1996 ) on sparse coding. According to their study, sparse coding can be trained on unlabeled natural im-ages to yield receptive fields akin to V1 simple cells ( Hubel &amp; Wiesel , 1959 ).
 One shortcoming of early approaches such as sparse coding ( Olshausen &amp; Field , 1996 ) is that their archi-tectures are shallow and typically capture low-level concepts (e.g., edge  X  X abor X  filters) and simple invari-ances. Addressing this issue is a focus of recent work in deep learning ( Hinton et al. , 2006 ; Bengio et al. , 2007 ; Bengio &amp; LeCun , 2007 ; Lee et al. , 2008 ; 2009 ) which build hierarchies of feature representations. In partic-ular, Lee et al ( 2008 ) show that stacked sparse RBMs can model certain simple functions of the V2 area of the cortex. They also demonstrate that convolutional DBNs ( Lee et al. , 2009 ), trained on aligned images of faces, can learn a face detector. This result is inter-esting, but unfortunately requires a certain degree of supervision during dataset construction: their training images (i.e., Caltech 101 images) are aligned, homoge-neous and belong to one selected category.
 3.2. Architecture Our algorithm is built upon these ideas and can be viewed as a sparse deep autoencoder with three im-portant ingredients: local receptive fields, pooling and local contrast normalization. First, to scale the autoencoder to large images, we use a simple idea known as local receptive fields ( LeCun et al. , 1998 ; Raina et al. , 2009 ; Lee et al. , 2009 ; Le et al. , 2010 ). This biologically inspired idea proposes that each fea-ture in the autoencoder can connect only to a small region of the lower layer. Next, to achieve invari-ance to local deformations, we employ local L2 pool-ing ( Hyv  X arinen et al. , 2009 ; Le et al. , 2010 ) and local contrast normalization ( Jarrett et al. , 2009 ). L2 pool-ing, in particular, allows the learning of invariant fea-tures ( Hyv  X arinen et al. , 2009 ; Le et al. , 2010 ). Our deep autoencoder is constructed by replicating three times the same stage composed of local filtering, local pooling and local contrast normalization. The output of one stage is the input to the next one and the overall model can be interpreted as a nine-layered network (see Figure 1 ).
 The first and second sublayers are often known as fil-tering (or simple) and pooling (or complex) respec-tively. The third sublayer performs local subtractive and divisive normalization and it is inspired by bio-logical and computational models ( Pinto et al. , 2008 ; Lyu &amp; Simoncelli , 2008 ; Jarrett et al. , 2009 ). 2 As mentioned above, central to our approach is the use of local connectivity between neurons. In our experi-ments, the first sublayer has receptive fields of 18x18 pixels and the second sublayer and the second sub-layer pools over 5x5 overlapping neighborhoods of fea-tures (i.e., pooling size). The neurons in the first sub-layer connect to pixels in all input channels (or maps) whereas the neurons in the second sublayer connect first sublayer outputs linear filter responses, the pool-ing layer outputs the square root of the sum of the squares of its inputs, and therefore, it is known as L2 pooling.
 Our style of stacking a series of uniform mod-ules, switching between selectivity and toler-ance layers, is reminiscent of Neocognition and HMAX ( Fukushima &amp; Miyake , 1982 ; LeCun et al. , 1998 ; Riesenhuber &amp; Poggio , 1999 ). It has also been argued to be an architecture employed by the brain ( DiCarlo et al. , 2012 ).
 Although we use local receptive fields, they are not convolutional: the parameters are not shared across different locations in the image. This is a stark difference between our approach and pre-vious work ( LeCun et al. , 1998 ; Jarrett et al. , 2009 ; Lee et al. , 2009 ). In addition to being more biolog-ically plausible, unshared weights allow the learning of more invariances other than translational invari-ances ( Le et al. , 2010 ).
 In terms of scale, our network is perhaps one of the largest known networks to date. It has 1 billion train-able parameters, which is more than an order of magni-tude larger than other large networks reported in liter-ature, e.g., ( Ciresan et al. , 2010 ; Sermanet &amp; LeCun , 2011 ) with around 10 million parameters. It is worth noting that our network is still tiny com-times larger in terms of the number of neurons and synapses ( Pakkenberg et al. , 2003 ). 3.3. Learning and Optimization Learning: During learning, the parameters of the second sublayers ( H ) are fixed to uniform weights, whereas the encoding weights W 1 and decoding weights W 2 of the first sublayers are adjusted using the following optimization problem
Here,  X  is a tradeoff parameter between sparsity and reconstruction; m, k are the number of examples and pooling units in a layer respectively; H j is the vector of weights of the j -th pooling unit. In our experiments, we set  X  = 0 . 1.
 This optimization problem is also known as recon-struction Topographic Independent Component Anal-ysis ( Hyv  X arinen et al. , 2009 ; Le et al. , 2011a ). 4 The first term in the objective ensures the representations encode important information about the data, i.e., they can reconstruct input data; whereas the second term encourages pooling features to group similar fea-tures together to achieve invariances.
 Optimization: All parameters in our model were trained jointly with the objective being the sum of the objectives of the three layers.
 To train the model, we implemented model parallelism by distributing the local weights W1, W2 and H to different machines. A single instance of the model partitions the neurons and weights out across 169 ma-chines (where each machine had 16 CPU cores). A set of machines that collectively make up a single copy of the model is referred to as a  X  X odel replica. X  We have built a software framework called DistBelief that manages all the necessary communication between the different machines within a model replica, so that users of the framework merely need to write the desired up-wards and downwards computation functions for the neurons in the model, and don X  X  have to deal with the low-level communication of data across machines. We further scaled up the training by implementing asynchronous SGD using multiple replicas of the core model. For the experiments described here, we di-vided the training into 5 portions and ran a copy of the model on each of these portions. The models com-municate updates through a set of centralized  X  X aram-eter servers, X  which keep the current state of all pa-rameters for the model in a set of partitioned servers (we used 256 parameter server partitions for training the model described in this paper). In the simplest implementation, before processing each mini-batch a model replica asks the centralized parameter servers for an updated copy of its model parameters. It then processes a mini-batch to compute a parameter gra-dient, and sends the parameter gradients to the ap-propriate parameter servers, which then apply each gradient to the current value of the model parame-ter. We can reduce the communication overhead by having each model replica request updated parame-ters every P steps and by sending updated gradient values to the parameter servers every G steps (where G might not be equal to P). Our DistBelief software framework automatically manages the transfer of pa-rameters and gradients between the model partitions and the parameter servers, freeing implementors of the layer functions from having to deal with these issues. Asynchronous SGD is more robust to failure than stan-dard (synchronous) SGD. Specifically, for synchronous SGD, if one of the machines goes down, the entire training process is delayed; whereas for asynchronous SGD, if one machine goes down, only one copy of SGD is delayed while the rest of the optimization can still proceed.
 In our training, at every step of SGD, the gradient is computed on a minibatch of 100 examples. We trained the network on a cluster with 1,000 machines for three days. See Appendix B, C, and D for more details re-garding our implementation of the optimization. In this section, we describe our analysis of the learned representations in recognizing faces ( X  X he face detec-tor X ) and present control experiments to understand invariance properties of the face detector. Results for other concepts are presented in the next section. 4.1. Test set The test set consists of 37,000 images sam-pled from two datasets: Labeled Faces In the Wild dataset ( Huang et al. , 2007 ) and ImageNet dataset ( Deng et al. , 2009 ). There are 13,026 faces sampled from non-aligned Labeled Faces in The Wild. 5 The rest are distractor objects randomly sampled from ImageNet. These images are resized to fit the visible areas of the top neurons. Some example images are shown in Appendix A. 4.2. Experimental protocols After training, we used this test set to measure the performance of each neuron in classifying faces against distractors. For each neuron, we found its maximum and minimum activation values, then picked 20 equally spaced thresholds in between. The reported accuracy is the best classification accuracy among 20 thresholds. 4.3. Recognition Surprisingly, the best neuron in the network performs very well in recognizing faces, despite the fact that no supervisory signals were given during training. The best neuron in the network achieves 81.7% accuracy in detecting faces. There are 13,026 faces in the test set, so guessing all negative only achieves 64.8%. The best neuron in a one-layered network only achieves 71% ac-curacy while best linear filter, selected among 100,000 filters sampled randomly from the training set, only achieves 74%.
 To understand their contribution, we removed the lo-cal contrast normalization sublayers and trained the network again. Results show that the accuracy of best neuron drops to 78.5%. This agrees with pre-vious study showing the importance of local contrast normalization ( Jarrett et al. , 2009 ).
 We visualize histograms of activation values for face images and random images in Figure 2 . It can be seen, even with exclusively unlabeled data , the neuron learns to differentiate between faces and random distractors. Specifically, when we give a face as an input image, the neuron tends to output value larger than the threshold, 0. In contrast, if we give a random image as an input image, the neuron tends to output value less than 0. 4.4. Visualization In this section, we will present two visualization tech-niques to verify if the optimal stimulus of the neuron is indeed a face. The first method is visualizing the most responsive stimuli in the test set. Since the test set is large, this method can reliably detect near optimal stimuli of the tested neuron. The second approach is to perform numerical optimization to find the op-timal stimulus ( Berkes &amp; Wiskott , 2005 ; Erhan et al. , 2009 ; Le et al. , 2010 ). In particular, we find the norm-bounded input x which maximizes the output f of the tested neuron, by solving: Here, f ( x ; W, H ) is the output of the tested neuron given learned parameters W, H and input x . In our experiments, this constraint optimization problem is solved by projected gradient descent with line search. These visualization methods have complementary strengths and weaknesses. For instance, visualizing the most responsive stimuli may suffer from fitting to noise. On the other hand, the numerical optimization approach can be susceptible to local minima. Results, shown in Figure 3 , confirm that the tested neuron in-deed learns the concept of faces.
 4.5. Invariance properties We would like to assess the robustness of the face de-tector against common object transformations, e.g., translation, scaling and out-of-plane rotation. First, we chose a set of 10 face images and perform distor-tions to them, e.g., scaling and translating. For out-of-plane rotation, we used 10 images of faces rotating in 3D ( X  X ut-of-plane X ) as the test set. To check the ro-bustness of the neuron, we plot its averaged response over the small test set with respect to changes in scale, 3D rotation (Figure 4 ), and translation (Figure 5 ). 6 The results show that the neuron is robust against complex and difficult-to-hard-wire invariances such as out-of-plane rotation and scaling.
 Control experiments on dataset without faces: As reported above, the best neuron achieves 81.7% ac-curacy in classifying faces against random distractors. What if we remove all images that have faces from the training set? We performed the control experiment by running a face detector in OpenCV and removing those training images that contain at least one face. The recognition accuracy of the best neuron dropped to 72.5% which is as low as simple linear filters reported in section 4.3 . Having achieved a face-sensitive neuron, we would like to understand if the network is also able to detect other high-level concepts.
 We observed that the most common objects in the YouTube dataset are body parts and pets and hence suspected that the network also learns these concepts. To verify this hypothesis and quantify selectivity prop-erties of the network with respect to these concepts, we constructed two datasets, one for classifying hu-man bodies against random backgrounds and one for classifying cat faces against other random distractors. For the ease of interpretation, these datasets have a positive-to-negative ratio identical to the face dataset. The cat face images are collected from the dataset de-scribed in ( Zhang et al. , 2008 ). In this dataset, there are 10,000 positive images and 18,409 negative images (so that the positive-to-negative ratio is similar to the case of faces). The negative images are chosen ran-domly from the ImageNet dataset.
 Negative and positive examples in our human body dataset are subsampled at random from a benchmark dataset ( Keller et al. , 2009 ). In the original dataset, each example is a pair of stereo black-and-white im-ages. But for simplicity, we keep only the left images. In total, like in the case of human faces, we have 13,026 positive and 23,974 negative examples.
 We then followed the same experimental protocols as before. The results, shown in Figure 6 , confirm that the network learns not only the concept of faces but also the concepts of cat faces and human bodies. Our high-level detectors also outperform standard baselines in terms of recognition rates, achieving 74.8% and 76.7% on cat and human body respectively. In comparison, best linear filters (sampled from the train-ing set) only achieve 67.2% and 68.1% respectively. In Table 1 , we summarize all previous numerical re-sults comparing the best neurons against other base-lines such as linear filters and random guesses. To un-derstand the effects of training, we also measure the performance of best neurons in the same network at random initialization.
 During the development process of our algorithm, we also tried several other algorithms such as deep autoen-coders ( Hinton &amp; Salakhutdinov , 2006 ; Bengio et al. , 2007 ) and K-means ( Coates et al. , 2011 ). In our im-plementation, deep autoencoders are also locally con-nected and use sigmoidal activation function. For K-means, we downsample images to 40x40 in order to lower computational costs.
 We also varied the parameters of autoencoders, K-means and chose them to maximize performances given resource constraints. In our experiments, we used 30,000 centroids for K-means. These models also employed parallelism in a similar fashion described in the paper. They also used 1,000 machines for three days. Results of these baselines are reported in the bottom of Table 1 . We applied the feature learning method to the task of recognizing objects in the ImageNet dataset ( Deng et al. , 2009 ). After unsupervised training on YouTube and ImageNet images, we added one-versus-all logistic classifiers on top of the highest layer. We first trained the logistic classifiers and then fine-tuned the network. Regularization was not employed in the logistic classifiers. The entire training was carried out on 2,000 machines for one week. We followed the experimental protocols specified by ( Deng et al. , 2010 ; Sanchez &amp; Perronnin , 2011 ), in which, the datasets are randomly split into two halves for training and validation. We report the performance on the validation set and compare against state-of-the-art baselines in Table 2 . Note that the splits are not identical to previous work but validation set perfor-mances vary slightly across different splits. The results show that our method, starting from scratch (i.e., raw pixels), bests many state-of-the-art baselines with hand-engineered features. On ImageNet with 10K categories, our method yielded a 15% relative improvement over previous best published result. On ImageNet with 20K categories, our method achieved a 70% relative improvement over the highest other result of which we are aware (including unpublished results known to the authors of ( Weston et al. , 2011 )). In this work, we simulated high-level class-specific neu-rons using unlabeled data. We achieved this by com-bining ideas from recently developed algorithms to learn invariances from unlabeled data. Our implemen-tation scales to a cluster with thousands of machines thanks to model parallelism and asynchronous SGD. Our work shows that it is possible to train neurons to be selective for high-level concepts using entirely unla-beled data. In our experiments, we obtained neurons that function as detectors for faces, human bodies, and cat faces by training on random frames of YouTube videos. These neurons naturally capture complex in-variances such as out-of-plane and scale invariances. Using the learned representations, we obtain 15.8% ac-curacy for object recognition on ImageNet with 20,000 categories, a significant leap of 70% relative improve-ment over the state-of-the-art.
 Acknowledgements: We thank Samy Bengio, Adam Coates, Tom Dean, Mark Mao, Peter Norvig, Paul Tucker, Andrew Saxe, and Jon Shlens for helpful discussions and suggestions.

