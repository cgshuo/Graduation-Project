 In this paper we introduce a problem class which we term activity monitoring. Such problems typically in-volve monitoring the behavior of a large population of entities for interesting events requiring action. Exam-ples include the tasks of fraud detection, computer in-trusion detection, network performance monitoring, cri-sis monitoring, some forms of fault detection, and news story monitoring. These applications may differ greatly in representation. For example, the data streams be-ing monitored may be streams of numbers, mixed-type feature vectors, or free-text documents. However, prob-lems in this class share significant characteristics that differentiate them from other KDD problems. 
The goal of activity monitoring is to issue alarms ac-curately and in a timely fashion. Standard KDD tech-niques such as classification, regression and time series analysis are useful as solution components, but these techniques do not completely address the goal. Al-though activity monitoring applications have received much attention individually, to our knowledge they have never been generalized and analyzed in a common framework. We present a framework within which these tasks have a natural expression. This framework cod-ifies similarities of the tasks and highlights significant differences. 
We begin by discussing briefly cellular phone fraud detection as a domain to illustrate some of the issues in activity monitoring. We define the problem formally and present an evaluation methodology. Then we ex-plain some important differences between types of ac-tivity monitoring tasks. The resulting framework com-prises the problem definition, the evaluation methodol-ogy, and the categorization of activity monitoring tech-niques. We believe this framework is a significant con-tribution because it helps to understand activity moni-toring better in general. To demonstrate this, we show that it aids in understanding how methods from fraud detection can apply directly to the seemingly different problem of creating stock market alerts based on news stories, yet the same methods do not seem to work well on the problem of computer intrusion detection, which seems closely related to fraud detection. Cellular phone fraud detection is a typical activity monitoring problem. The task is to scan a large set of accounts, examining the calling behavior of each, and to issue an alarm when an account appears to have been defrauded. This can be done in many ways, such as profiling users X  behavior, looking for known fraud patterns, etc. The goal is to identify fraudulent activity as soon as possible without creating too many false alarms [7]. 
Calling activity may be represented in various ways, but is usually described with call records. Each call record is a vector of features, both continuous (e.g., CALLDURATION) and discrete (e.g., CALLING-CITY). However, there is no inherent primitive representation in this domain. Calls can be aggregated by time, for example into call-hours or call-days. Though it is not obvious, they can also be broken up into a finer grainsize: more detailed intra-call information is available, such as the delays between button presses, switch hand-offs during each call, etc. to identify fraudulent activity as soon as possible. In one intervention scenario, when an alarm is issued, a human fraud analyst must check the account carefully and decide what to do. Therefore, it is important that the system not create too many false alarms. The precise definitions of  X  X s soon as possible X  and  X  X oo many false alarms X  vary with the amount of fraud, the size of the workforce, and other factors. arise: monitoring domains. Perhaps surprisingly, most prior work in such domains ignores one or more of these is-sues, even though they are exhibited in the correspond-ing application. Section 5 argues that standard evalu-ation metrics such as classification accuracy, cost, and even ROC analysis are insufficient for evaluating activ-ity monitoring systems. In the next section we intro-duce a formalism and a framework that can accommo-date these complexities. 
We define the general problem as follows. Let 27 be a set of data streams, where each data stream Di E V is an ordered set of data items {di,i, . . . , di,k;} (ki need not equal lcj, for i # j-that is, the length of the episodes need not be equal). We will be liberal in what are considered data-numeric measurements, vectors of symbolic features, text, etc. Each data stream represents information about a single entity in the population being monitored, for example, customers, users, network equipment, nuclear reactor components, or even potential investment opportunities. For the rest of this paper we only refer to one data stream at a time, so for simplicity we omit the subscript i, and refer instead to a data stream D and its elements {dj},j = l,... ,Ic. ing monitored, and comprise the evidence upon which alarms are to be issued. The data may be measurements taken directly from the entity, such as the temperature of an engine part at a point in time. They may be de-scriptions of the entity X  X  behavior, such as transactions billed to an account. They may also be indirect descrip-tions of the behavior from a third party, such as news stories about a particular company. The dj X  X  are totally ordered in time, so j &gt; lc H time(dj) &gt; time(dk). The dj X  X  are not necessarily evenly spaced in time. data streams in order to detect the occurrence of interesting behavior, which we refer to as positive activity. Episodes of positive activity need not be similar to each other, nor do is the non-positive activity in one data stream necessarily similar to that of any other. Let r be a point in time denoting the onset of positive activity. For this paper, we assume that each 
D contains at most one r, and that if there is a period of positive activity, it is at the end of D. Although real data streams may contain multiple periods of positive activity, we will see that this assumption is not overly restrictive. For the data stream D, r designates the beginning of a contiguous subsequence D, = (dPl, . . . ,dP,) such that time(di) 2 7 ti di E 
D,. Because the positive activity is necessarily at the end of D, p, = k. Note that the positive activity is separate from the existence of the activity X  X  evidence: 
D, may be the empty sequence. The goal of an activity monitor is to give an indication that the sequence is exhibiting positive activity; such an indication is called an alarm. Formally, an alarm cr represents the point in time when it is issued. 
A monitor may issue multiple alarms for any D. For completeness, a D that contains no positive activity implies r = co; if a monitor never alarms on a D, we consider cr = co. 
There is a crucial difference between this problem formulation and those used in previous work. The goal in activity monitoring is not to identify completely all the positive activity, nor to classify each dj as positive 
Figure 1: A data stream D with alarms (~1 and crz. or negative. Rather, the goal is to identify in a timely fashion that positive activity has begun. Alarming earlier may be more beneficial, but after a first alarm, a second alarm on the same sequence may add no value. 
To accommodate these considerations, we define two task-dependent functions: a score function s and a false alarm function f. Let so(r,a, H, 0) be a score function which returns the value (benefit) of an alarm cr on a given subsequence, with respect to a given r. The variable H is a sequence of the alarms that have already occurred on the sequence. We can now define the positive activity in terms of s: it is the subsequence of D for which s.o(r, time(dj), (),D) &gt; 0. 
A symmetric function f~ can be defined for false alarm penalties. Let f~(a, H, 0) be the cost of a false alarm on sequence D, with H being the sequence of false alarms already issued. 
We make several simplifications to these functions for presentation. Within a given domain we can eliminate the D subscript, although it is important to remember that the score function depends on the domain and on the particular sequence of activity. For this paper we will assume that for most domains the first alarm is significant but subsequent alarms add no value, so the score function returns a non-zero value only if H is the empty sequence. Thus we remove H from s and assume an implicit clause within s: Similarly, for this paper we assume that the cost of a false alarm is constant and does not depend on history, so f reduces to a variable representing this cost. 
The goal of activity monitoring is two-fold. A tech-nique should maximize the expected value of s(r, CY, D) for any given alarm, while minimizing the number of false alarms. Adopting standard terminology, Q is ~(7, (Y, D) &gt; 0. If r &lt; 00 and there does not exist an (Y such that r 5 (Y &lt; oo then the interesting period was missed. 
It is important to understand that the concept of  X  X rue negative X  is not well defined in activity monitor-ing. In practice, one can create definitions based on the particular representation used, but in principle there are infinitely many possible  X  X rue negatives, X  because of the continuity of time. This observation has been made before, for example in classification for visual pattern recognition [2], and we will return to it when we discuss the evaluation framework. The problem formulation may be clarified by an example from cellular phone fraud detection [7].l Each D E D represents a customer account, comprising a sequence of cellular phone calls (the dj). Each dj is a time-stamped vector of features representing details of the call.  X  X ositive activity X  is cellular cloning fraud. 
In this domain there are several realistic ways to define a score function s(r, Q, D) to measure the benefit of alarming at cr. One possibility is to count the number of fraudulent calls that would have been made had the fraud not been caught. These are the calls after (Y: 
A more elaborate score function calculates the cost of the airtime of these fraudulent calls: 
S(T,Q, D) = aircost x c airtime where callstart is the start time of a given call, and air-cost is the (constant) cost of a unit of call airtime. Our previous work assumed a monetary cost of $.40 per minute for airtime, and a fixed cost of $5 for a false alarm, so aircost = 0.4 and f = 5; 
Our assumption of at most one 7 per episode is not problematic in practice. If a customer account is defrauded twice, the sequence can be decomposed into two single-r sequences without affecting the evaluation. More generally, as long as the value of s depends on only one T, the subsequence can be decomposed into separate episodes for the purpose of evaluation. There may be cases where s would combine value from multiple r X  X , but we are aware of no non-contrived examples. We have argued elsewhere [12, 131 that Receiver Operating Characteristic (ROC) analysis, a method three classifiers. from signal detection theory, is the appropriate metric for analyzing classifiers under imprecision. ROC analysis depicts the tradeoff between true positive classifications and false positive ones, which is similar to the goal of activity monitoring. For activity monitoring we use ROC analysis with two minor modifications, which for this paper we call AMOC (Activity Monitor Operating Characteristic). rate, meaning here the percentage of the negative examples classified as positive. Since we have no fixed notion of a  X  X egative example, X  we define the false-alarm rate to be the number of false alarms, normalized by the distance metric used in s, for example, the expected number of false alarms per unit time. This is analogous to the FROC technique found in visual pattern recognition [2], which also assumes no fixed notion of a negative example so false alarms per unit area are measured. Normalizing false-alarm rate allows us to compare methods that cannot be compared under standard ROC analysis. For example, in fraud detection a transaction classifier and an account-day classifier have different notions of negative example, so their standard false-positive rates cannot be compared directly. However, their normalized false-alarm rates can, because each produces a certain number of false alarms per hour. The second modification to ROC analysis is that on the Y-axis, rather than plotting the true positive rate, AMOCs plot the expected value of 47, a, 0). [12] but are tailored to activity monitoring. An example highlights the differences. Consider a fraud detection problem where fraud must be caught within five hours of its onset. The score function is binary, where the benefit of fraud detection is 1 only if detection is within five hours: The false alarm rate is normalized per hour. is a fundamental difference between AMOC and ROC curves. The figure shows an AMOC curve for a hypothetical fraud detector. How good is the detection? 
In fact, this curve shows the performance of classifiers that alarm randomly with different frequencies (0.1 per hour, 0.2 per hour, etc.). Unlike ROC curves, for which the diagonal y = 2 depicts the performance of random guessing, in AMOCs performance depends upon the definition of s. Even with a simple binary definition of s, an AMOC is not necessarily the same as an ROC. is important for evaluating activity monitoring. The figures show ROC and AMOC curves for three classifiers on an activity monitoring domain. The ROC curves in b show that classifier 3 dominates the other two in ROC space [12], so its instance classification performance is generally superior. From this we might conclude that it is unconditionally best. However, the AMOC curves in c show that classifier 2 performs better for activity monitoring if maximal detection score is much more important than false alarm rate. curve from a probabilistic classifier. As with ROC curves, to generate AMOCs it is not necessary to specify thresholds on the classifier X  X  continuous output. 
Given an assignment of probabilities to examples, the examples are sorted decreasing by their assigned probabilities. Then the sorted list is traversed, updating 
Ftotal = F; 
Figure 3: Algorithm for generating an AMOC curve from a set of alarms the score and the false-alarm rate after each example. Each update produces a point on the AMOC curve; this point represents the performance of the classifier that would result from placing a threshold just below the corresponding probability. One list traversal generates points for all thresholds. Because we have made a simplifying assumption that any true alarms after the first have no contribution to the score, we need deal only with one other alarm (Hi) as history. This keeps the algorithm X  X  time complexity to O(n). 3.4 Key differences between methods There are many similarities between activity monitoring problems. A framework for a problem class also should aid in understanding intraclass differences; for example, which differences are superficial and which are fundamental. We now focus on one distinction: the nature of the modeling and monitoring process. There are two high-level classes of methods: l A profiling method builds a model of normal l A discriminating method builds a model of inter-ods for simple classification [14]. Here the distinction is based on a fundamental asymmetry of activity mon-itoring problems: normal activity is common and posi-tive activity is rare. Therefore, methods other than su-pervised learning algorithms can play a significant role. 
For example, modeling normal activity with a regres-sion curve or a probability distribution may be useful for judging typicality of subsequent activity. consider activity to be uniform over all Di from those that model each individual Di. Adding this second distinction yields four distinct categories of methods: 1. 2. 3. 4. tion only with respect to normal activity. For discrimi-nating models, this distinction may be made with re-spect to the positive activity as well. For example, in monitoring device behaviors, particular devices may have idiosyncratic failure modes that must be modeled. 
However, such individualized positive activity is un-usual; positive activity typically either is too scarce to be modeled individually, or simply is not related to any particular data stream. ficient. In intrusion detection, misuse detection is a simple example:  X  X efining intrusions ahead of time and watching for their occurrence X  [9, p.263. As another example, consider monitoring news stories for interest-ing investment opportunities. Noticing that today X  X  news text differs from prior news text is not particu-larly promising, but a discriminating model of  X  X seful investment news X  could be quite valuable. ful than discriminating? Detecting non-malicious intru-sions is one example. For this task a model of positive activity may not exist, so discriminating may be im-possible. An instance of profiling, anomaly detection  X  X bserves the activity of subjects and generates profiles for them that represent their behavior X  [9, p.161. Profil-ing is much more useful than discriminating in the bur-geoning application of news story topic detection and tracking [l]. When positive activity is simply the intro-duction of new news topics, discriminating via modeling positive activity may be of little use. of techniques, but in practice they often are interwoven. 
For example, the DC-l system [7] builds fraud detectors automatically by first using normal and positive activity to construct dimensions along which profiling will be effective. Then it creates profiling monitors that model each account X  X  normal activity with respect to a single dimension. Finally it uses the outputs of the profiling monitors (indicating how far from normal the current activity is) as inputs to a discriminator that was trained to distinguish positive deviations from normal deviations. we call change detection, which is based on a model of transitions that occur from normal to positive activity in each individual account. To be useful, a change model must be able to refer to a profile of normal activity for each account. Change detection may be effective when there is little commonality within positive activity, but significant differences between normal and positive activity. For example, in fraud detection, there may be no times of day or locations that indicate fraud, but for a given account changes from its typical time of day or location usage may be very reliable indicators. Change detection capitalizes (in the modeling) on the temporal nature of activity monitoring problems. ends of a spectrum of groupings of data streams. In many applications, Di X  X  can be clustered into useful groups such that profiling or discriminating may be made more effective. For example, for phone-fraud detection, customers often can be grouped usefully as high-volume users, nine-to-five users, emergency-only users, etc. This paper does not address the problem of grouping data streams. 
We have claimed that this framework allows activity monitoring tasks to be viewed as a single type of prob-lem, and that the distinctions that arise help to under-stand this type of problem. We now provide a demon-stration that the framework facilitates understanding of activity monitoring across different domains. related news alerts, which is superficially very different from fraud detection, but similar as an activity moni-toring task. Once representational issues are addressed, we can apply a system designed for fraud detection to this seemingly different problem. We then use the con-cepts of the framework to explain why this is the case. detection. Though it is superficially similar to fraud detection, the framework suggests that within the class they are fundamentally different. We show that, as would be predicted, the fraud detection system unmodified does not perform particularly well, but that a simple modification, suggested by the framework, performs better. either news monitoring or intrusion detection. Rather, these demonstrations serve to illustrate the value of viewing activity monitoring as a problem class. Before discussing the two domains, we review briefly the DC-l system which will be used in the experiments. 
DC-l was designed previously to create detectors for cellular phone fraud by mining data on the behavior of cellular phone accounts. It is described in detail elsewhere [7]; only its salient aspects will be covered here, cast in the activity monitoring framework. 
First it generates rules that distinguish fraudulent calls from legitimate ones. This can be done using either uniform discriminating modeling (e.g., learning a classifier from positive and negativecalls) or change modeling (e.g., learning rules to distinguish fraudulent changes in behavior within individual accounts). These rules are then used to create profiling monitors. Each monitor models the behavior of each account with respect to one rule and, when monitoring, describes how far the account X  X  activity is from its typical behavior. 
Finally, DC-l weights the monitor outputs to maximize the effectiveness of the resulting fraud detector. To do so, the outputs of the monitors are provided as features to a standard learning program along with the desired output (an account-day X  X  correct class: fraud or non-fraud). The result is a weighted sum of the outputs of fourth compare cd] income quarter liscal earnings per diluted fiscal quarter ended expenses months ended today reported consensus quarter earnings year ended repurchase below analyst for quarter shortfall said [it] expects same period revenues increase over per share the monitors, along with a threshold on the sum. 
In use, the monitors view each day X  X  calls from a given account, and each monitor generates a number indicating how unusual that account-day looks for the account. The numeric outputs from the monitors are treated as evidence and are combined by the detector. When the detector has enough evidence of fraudulent activity on an account, based on the indications of the monitors, it generates an alarm. 4.2 News Story Monitoring The goal of this task is to scan news stories associated with a large number of companies and to issue alarms on specific companies when their stocks are about to exhibit positive activity. We collected stories and stock prices for approximately 6000 companies over a three month period. Each story was tagged with a list of companies to which it pertained; each company X  X  stories constituted a different data stream, and an  X  X nteresting event X  (positive activity) was defined to be a 10% change in the company X  X  stock price (a price  X  X pike X ) in either direction. The activity monitoring goal is to minimize the number of false alarms and to maximize the number of correctly predicted price spikes. 
This domain seems very different from fraud detec-tion. The data items are represented as free text rather than as predefined feature vectors of numeric and dis-crete attributes. The news stories are not as clearly associated with positive activity as are cellular phone calls, which are carefully tagged by a cellular company X  X  billing system. Also, the alarms of this domain are fundamentally  X  X pportunistic X  rather than failure-or crisis-driven. A researcher attacking this problem would likely see it as very dissimilar to fraud detection, and would probably would seek an information retrieval so-lution, using precision and recall as evaluation metrics. 
However, generating stock alerts is easily cast as an activity monitoring problem. Each news story constitutes a dj of its associated company (D). We heuristically associate with a price spike any story appearing from midnight the day before the spike up until lo:30 AM of the day of the spike.2 With the prior midnight as r, the scoring function s for an alarm (Y can be defined as: 
The news story domain is similar to fraud detection with respect to building discriminating models. Specifi-cally,  X  X ositive X  news stories can be differentiated from  X  X ormal X  stories using DC-l X  X  data mining. Because news story text is superficially quite different from cel-lular call records, the representation had to be adjusted before DC-l could be applied. Each story was lexically analyzed and reduced to its constituent words. The words were stemmed, then a stop list was applied to re-move common noise words. The final representation for a story was a set of its processed words and bi-grams. The reduced stories were each labeled as positive or neg-ative using the heuristics discussed above. DC-l then learned indicators of positive activity (discriminating models) from these stories. Some specific indicators are shown in Table 1. These indicators were used to form a DC-l monitor (as described above). 
Figure 4 shows the performance of different activity monitors applied to this domain. Each curve was generated by lo-fold cross-validation. Random shows the performance of monitors that alarm randomly on stories with varying probabilities (each probability gives one curve point). DC-l shows the performance of the DC-l monitor. 
Consider this problem in terms of the distinctions made in Section 3.4. Because of the large number of features (approximately lo5 words and bi-grams appeared more than once), and because the news is always changing, we would expect little behavioral consistency at the word level. Consequently, we would predict that profiling would add little compared to discriminating; specifically, that the profiling done by DC-l adds little to the activity monitoring performance on this task. The curve labeled DC-l without profiling demonstrates this: the performance indeed decreases little when profiling is turned off, at which point DC-l just scans using the learned classifier. 4.3 Intrusion Detection Intrusion detection is a field of computer security concerned with detecting attacks on computers and 
Figure 4: Performance of several methods for news story monitoring computer networks [8, 91. Within intrusion detection, anomaly detection systems characterize behavior of individual users and issue alarms of intrusions, based on anomalies in behavior. and difficult to obtain, we chose a variant of this task common in computer intrusion research [6, 10, 151. In this variant, the goal is to predict, based on commands typed, when the user of a given account is not the actual legitimate user. With such a task, typically one user at a time is chosen to be the  X  X egitimate X  user and one to be the  X  X ntruder. X  The task then becomes one domain we used a dataset of Unix commands taken from about 8000 login sessions collected from a population of 77 users.3 toring. Each user constitutes a D with an associated stream of sessions dj . Each session is a set of Unix com-mands. False positives were normalized per session. We consider an alarm effective only if it occurs within the first five intrusion sessions, so the s function was defined as: 
S(T,cqD) 1 = if 0 5 1 {di E D, 1 time(di) 5 a}1 &lt; 5 between command sets used by various users. Table 2 shows some of the change indicators extracted from the Davison-Hirsh dataset. Although these commands may seem mundane, some indicate common distinctions among Unix users: emacs vs vi, exit vs logout, and 
Figure 5: Performance of several methods for intrusion detection 
Table 2: Indicators from change-modeling Unix users. text processing (gs) vs code development, (gee and Each curve was generated by lo-fold cross-validation. 
Random represents the effect of issuing random alarms with varying probabilities (as above). DC-l generated indicators such as those above to generate features for distinguishing users, and used these indicators to form its profiling monitor. As shown in Figure 5, DC-l detects intrusions better than random. of DC-l for fraud detection, its performance on intru-sion detection was disappointing. We believe that the reasons for the performance are clarified by viewing the two problems within the activity monitoring frame-work. In cellular fraud detection, for which DC-l was designed, discriminating positive activity is an impor-tant component because fraudulent behavior often is quite different from legitimate behavior. Superficially, detection intrusions is very similar to detecting cellular fraud. However, note that there is no distinguishable intruder behavior because any user can fill the role of an intruder with respect to some other user. Discrimi-native modeling is impracticable because there is no dis-tinct positive activity. Profiling should be much more important. filer based on the x2 statistical test, which models each 60 user X  X  normal behavior as a probability distribution across the commands they use. As predicted, the x2 technique outperformed the DC-l technique. A com-parison of this simple profiler to existing intrusion de-tection methods might be revealing. 
Much work in KDD has involved monitoring activity for unusual behavior; for example, fraud detection [3, 4, 71, intrusion detection [6, 10, 111, and network monitoring [16, 171. Indeed it is this wealth of prior work on closely related problems that makes the formulation of a general class worthwhile. It is important to examine how activity monitoring problems have been framed in prior work, in order to compare them with the framework we propose. classification problem in which the effectiveness of alarms is evaluated based on simple classification accuracy or error rate. However, such measures are particularly inappropriate for problems such as activity monitoring where one class is rare and where the cost of a miss and the cost of a false alarm are not equal [13]. ing the task as a cost-sensitive classification problem [3, 71. Unfortunately, in activity monitoring domains it is usually difficult to specify costs precisely. More im-portantly, cost-sensitive classification relies on a speci-fication of class priors, which can be even more difficult to determine precisely. In fact, the work cited uses pri-ors known to be inaccurate. It is difficult to generalize from such results without special studies weakening the basic, framing assumption of knowledge of class priors. that make explicit the tradeoff between hits and false alarms, including frameworks that use ROC curves, precision/recall curves, and others [6, lo]. If done well, this addresses the problem of imprecision in knowledge of costs and of class distributions. share a basic flaw, which our prior work on fraud detection admitted: The flaw is that they ignore the fundamental sequential nature of the problem and, thereby, the goal of timely classification. We have concentrated on classification-based frameworks because they are the most prevalent; corresponding simplifications are found in activity monitoring work rooted in other disciplines. framework for their study of activity preceding failures in a telecommunications network. Their framework models the sequential nature of the problem as time-stamped data streams, defines the goal as identifying a  X  X are event , X  notes that there is a window of the data stream where (in our terminology) s(r,cr,D) &gt; 0, and looks at the tradeoff between hits and false alarms. 
Their framework can be expressed within ours by appropriate definitions of r and s(r,cr,D), but ours applies more generally. In particular, their framework requires that the event to be identified be one of the dj. Our framework allows the event initiating the activity to be separate from the data; for example, some action of a company triggers news agencies to report on the action. More fundamentally, Weiss and 
Hirsh require that the activity precede the event in question. While this is appropriate for their application (network performance degrades before a failure), we found it hard to extend in general (predictive activity does not precede a case of fraud). We prefer to recast such problems: r marks the beginning of the period prior to the failure for which detection will be useful, and activity contained therein would be considered positive in the application. For example, there may be an underlying occurrence that causes (and therefore precedes) the performance degradation; the failure itself, just a further manifestation of the underlying cause, may be a key factor in the definition of s. In other cases, such as those described by Weiss and Hirsh, r can be defined as a fixed offset prior to the hard failure, representing  X  X he maximum amount of time prior to the target event for which a prediction is considered correct X  [17, p. 3591. work is to allow fundamentally different approaches to be compared. For example, many fraud detection meth-ods focus on individual transactions [7, 31, and eval-uation typically measures classifications of individual transactions. Other methods [7] aggregate transactions into account-days and measure differences in activity between them. A classification-based framework makes performance on different representations (transactions versus account days) difficult to compare. However, they can be compared within the activity monitoring framework. quence an interesting change in behavior has occurred. 
This differs from the typical goals of time series anal-ysis, such as predicting the next value of a series or characterizing the functional form of the series. HOW-ever, we are not claiming that any of these methods is inappropriate as part of the solution to a activity mon-itoring problem. Indeed the opposite is true: all these methods, standard time-series prediction, HMMs, clas-sification models, and others, are candidate tools for solving an activity monitoring problem. Activity monitoring is worth studying as a general KDD problem. Explicitly differentiating this class of problems should benefit KDD both scientifically and practically. We believe it is an ideal problem for KDD because it requires contributions from (at least) the three main constituent subfields: statistics, for expertise on time-series problems; databases, because of the tremendous amount of data typically involved; and machine learning, because of the representational complexity. We hope this paper has laid valuable groundwork. We thank Tom Dietterich for his encouraging words regarding generalizing our work on fraud detection. We thank Brian Davison for providing the Unix command data used in the computer intrusion experiments. [l] ALLAN, J., PAPKA, R., AND LAVRENKO, V. On-[2] BURL, M., ASKER, L., SMYTH, P., FAYYAD, U., [3] CHAN, P., AND STOLFO, S. Toward scalable learn-[4] Cox, K., EICK,~., WILLS, G., AND BRACHMAN, [5] DAVISON, B. D., AND HIRSH, H. Predicting [6] DUMOUCHEL, W., AND SCHONLAU, M. A fast [7] FAWCETT, T., AND PROVOST, F. Adaptive fraud [9] KUMAR, S. A Pattern Matching Approach to [lo] LANE, T., AND BRODLEY, C. Approaches [ll] LEE, W., STOLFO, S., AND MOK, K. Mining [12] PROVOST, F., AND FAWCETT, T. Analysis and [13] PROVOST, F., FAWCETT, T., AND KOHAVI, R. [14] RUBINSTEIN, Y. D., AND HASTIE, T. Discrimi-[15] RYAN, J., LIN, M.-J., AND MIIKKULAINEN, [16] SASISEKHARAN, R., SESHADRO, V., AND WEISS, [17] WEISS, G., AND HIRSH, H. Learning to predict 
