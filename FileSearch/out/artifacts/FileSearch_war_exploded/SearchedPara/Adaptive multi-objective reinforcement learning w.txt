 1. Introduction
In this paper, we focus on computing a consistent traf fi con fi guration at each junction that optimizes multiple perfor-mance indices (i.e., multi-objective traf fi c signal control). Traf signal control can be viewed as a multi-objective optimization problem. The multi-objective function can have a global objective for the entire road network or there may be different objectives for the different parts of the road network (e.g., maximize safety especially in residential and schools areas), or even different times of the day for the same part of the road network.

Construction of a new infrastructure is expensive, thus the generally acceptable solution is to improve the utilization of the existing resources by moving towards Intelligent Transportation Systems ( ITS ) for traf fi c management and control. Traf is a set of methods that are used to enhance the traf fi c network performance by, for example, controlling the traf fi c fl minimize congestion, waiting times, fuel consumption and avoid accidents. Traf fi c control generally includes the following compo-nents; controlling the traf fi c signals in urban areas, ramp-metering in highways, enforcing variable speed limits (according to vehicles types), supporting the drivers with route guidance based on the up-to-date traf fi c status using some kind of navigation systems (e. g., GPS), enforcing overtaking rules, and using driver-assistance systems (e.g., adaptive cruise control). In this paper, we particu-larly focus on controlling traf fi c signals in urban areas.
Another two important components of the ITS are traf fi c modeling and traf fi c simulation. Traf fi c modeling is the formula-tion of rigorous mathematical models that represent the various dynamics of the traf fi c system. This includes drivers 0 behavior in acceleration, deceleration, lane changing, phenomena such as rubbernecking, and behavior change under different weather conditions. Traf fi c simulation is the virtual emulation of the traf system on digital computers. Traf fi c simulators are used for experimentation and validation of the underlying traf fi c models and traf fi c control mechanisms.

Intelligent traf fi c control has many challenges that include the continuing increase in the number of vehicles (it is expected that 70% of the people worldwide will live in urban areas by 2050,
Pizam, 1999 ), the high dynamics and non-stationarity of the traf network, and the nonlinear behavior of the different components of the control system.

Nowadays, the different types of transportation means (spe-ci fi cally vehicles in urban areas) have major problems that governments are facing in both developing and developed countries. Traf fi c of vehicles in urban areas, speci fi many problems that include increase of traf fi ccongestion, psychological stress of drivers that affects their behavior leading to a high rate of accidents, considerable time losses, and a high rate of vehicle emissions which severely affects the environ-ment. Those problems have a considerable negative effect on the country economy. Thus, in this paper, the proposed traf signal controller tackles most of those problems (e.g., minimizes the waiting time of vehicles) a swillbeshownbytheperfor-mance evaluation in Section 7 .

In 2010, traf fi c costs (based on time loss and fuel consumption) about $115 billion in the US based on 439 urban areas ( Schrank et al., 2011 ). In the same year, 32,885 people died in accidents in the US ( U.S. Department of Transportation, 2012 ). In Egypt, traf problems are responsible for more than 25,000 accidents in 2010 with more than 6000 deaths per year ( CAPMAS ). Deaths per million driving kilometers in Egypt is about 34 times greater than in the developed countries ( Abbas, 2004 ). This value is about 3 times greater than countries in the Middle East region ( Abbas, 2004 ). The authors expect that this value is much worse in 2011 2013 due to the political upheaval in Egypt.

Recently, some computer science tools and technologies have been used to address the traf fi c signal control complexities. Among these is the MAS framework whose characteristics are similar in nature to the traf fi cproblem( Shoham and Leyton-Brown, 2010; De-
Oliveira and Camponogara, 2010 ). Such characteristics include dis-tributivity, autonomy, intelligibility, on-line learnability, and scalabil-ity. In particular, the formulation of the traf fi c signal control problem as a multi-agent reinforcement learning (MARL) con fi guration is very promising (as proposed in Bazzan, 2009 ).

In the current paper, we adopt a MARL framework in a cooperation-based con fi guration to comply with the distributed nature and complexity of the problem. Our work is a signi extension of the framework developed by Wiering (2000) and
Wiering et al. (2004) .Wiering 0 s controller, namely TC-1, repre-sents a pioneering step in the use of real-time reinforcement learning framework in modeling traf fi c signal control. TC-1 out-performs traditional controllers (e.g., random, fi xed time, longest queue, most cars). Moreover, TC-1 has proved its effectiveness and ef fi ciency when being applied to large scale traf fi
In contrast, other controllers based on reinforcement learning, e. g., Thorpe and Anderson (1996) and Abdulhai et al. (2003) suffer from exponential state-spaces when applied to large scale traf networks. In addition, many latter researchers, e.g., Houli et al. (2010) , Kuyer et al. (2008) , Schouten and Steingr X ver (2007) , I et al. (2006) ,and Steingr X ver et al. (2005) , use TC-1 as a bench-mark for performance evaluation. Each of these controllers contribute to TC-1 from a different prospective. For instance, in
Schouten and Steingr X ver (2007) , the authors overcome the partial observability of the traf fi c state-space, while we assume that the state-space is fully-observable , i.e., the agent can perfectly sense its environment.
 problems in which TC-1 fails to adapt with. This includes: (1) stable adaptation to the limited-time congestion periods (using Bayesian proba bility interpretatio n), (2) advanced reward formulation to adapt with the continuous-time continuous-space simulation platform, and (3) using a multi-objective reward formulation in an additive manner to optimize multiple perfor-mance indices.
 troller in comparison with two adaptive control strategies which are also based on AI methods: Self-Organizing Traf fi cLights(SOTL) ( Cools et al., 2008 ) (that outperforms a traditional green wave controller) and a Genetic Algorithm (GA) ( Wiering et al., 2004 ). control framework with the following characteristics: (1) inher-ently distributed through the use of a vehicle-based multi-agent system ; there are two types of agents: traf fi c junction agents (active computing agents) which are responsible for the decision making process (i.e., deciding on the proper traf fi c signal con according to the information collected from the vehicle agents; vehicle agents (passive agents) which support the decision making process by communicating the necessary information to the junction agents, (2) online sequential decision making framework where decisions are taken in real-time for signal splitting based on multiple optimization criteria; the core of the applied mechanism is based on Dynamic Programming (DP) which is very-well suited for sequential decision making tasks; the real-time optimization and decision making is done incrementally by integrating the online learning with DP through the use of reinforcement learning, (3) effectively and ef fi ciently handle the inherent complexity of the problem, the uncertainties involved, the incompleteness of information, the absence of a rigorous modeling of the traf volume and the general dynamics: through the use of stochastic and statistical tools to predict the unknown parameters and provide an up-to-date model of the current traf fi c conditions, (4) adaptive system in the sense that it responds effectively to the road dynamics (variations in traf fi c demand, changing weather conditions, etc.): through the use of a Bayesian approach for estimating the parameters of the underlying Markov Decision
Process (MDP) and the use of an adaptive cooperative hybrid exploration technique, and (5) higher con fi dence in the validity of the proposed traf fi c signal controller: through the use of a more realistic simulator as a testbed that is achieved by implementing the IDM acceleration model ( Treiber et al., 2000 ) in the GLD vehicle traf fi c simulator ( Wiering et al., 2004 ); moving from the unrealistic discrete-time discrete-space simulation platform to a continuous-time continuous-space one.
 listic in the sense that the fi rst waiting vehicle jumps once the traf signalturnsgreen.Now,byapplyingthemorerealisticIDMaccelera-tion model, the vehicle takes the normal time to decelerate when a traf fi c signal turns red and accelerates back again to cross the junction when the signal turns green. This behavior, on the other side, causes some kind of sign oscillation whe n being applied on the underlying RL model as will be shown later in Section 4 (which we called the
Zeno phenomenon 1 ) which results from the very slow acceleration of back vehicles when the traf fi c signal is just turning green. et al. (2012a,b) and Khamis and Gomaa (2012) .Inthispaper,we provide a more detailed description and improvements on the multi-objective function. Such improvements boost the performance of the multi-objective controller, particularly when being compared to the underlying single objective one. In addition, we present a novel cooperative hybrid exploration that is more adaptive to the changing dynamics in road conditions, and improves the trip waiting time of vehicles during transient periods. We also present a survey of the state-of-the-art work.

The remaining part of this paper is organized as follows. The related work of urban traf fi c signal controllers is discussed in
Section 2 . A background on the adopted traf fi c signal control and simulation models is presented in Section 3 . The proposed frame-work including the improvements on the traf fi c signal control and simulation models is presented in Section 4 .Traf fi cnon-stationarity is tackled by two models: MDP parameter estimation using the
Bayesian probability interpretation and a novel adaptive coopera-tive hybrid exploration technique. These two models are presented in Section 5 . Our multi-objective RL traf fi c signal control framework is discussed in Section 6 . Section 7 presents the experiments conducted under this framework. This section includes the results of the experiments, discussion about these results, and how those results can be validated. Finally, Section 8 concludes the paper and proposes some directions for future work. 2. Related work
There have been several approaches proposed in the literature for traf fi c signal control. The two broad classes of these controllers are traditional control paradigms and adaptive control paradigms. On the one hand, the simplest intuitive type of traf fi c control is to allow every traf fi cdirectiontopassfora fi xed amount of time. This of course ignores the dynamics and the high variability of the traf network. Thus, this strategy can result in very poor utilization of the traf fi csystemandinef fi cient usage of the available resources. On the other hand, traf fi c signal controllers based on robust models, e.g., petri-nets ( Febbraro et al., 2004; List and Cetin, 2004 ), Model
Predictive Control (MPC) ( De-Oliveira and Camponogara, 2010; Lin et al., 2011 ), etc., are hard to design and require a complete match with the actual traf fi c network dynamics for optimal traf control. In particular, as mentioned in Rezaee et al. (2012) ,any uncertainty or mismatch in the network model will result in a suboptimal performance of the MPC. Hence, these models are rigid and non-adaptive to non-modeled variations.

Some traf fi c signal controllers are based on the dynamic programming algorithmic paradigm, e.g., Heung et al. (2005) and
Sen and Head (1997) . DP is inherently a paradigm for sequential decision making hence it is very well suited to the nature of traf signal control. However, most traf fi c signal controllers based on DP are applied on an isolated junction, thus it does not take into account the inter-dependability between the different parts of the traf fi c network. In addition, most traf fi c prediction is based on historical traf fi c data that is taken in the same time of the day during which traf fi c is being controlled, e.g., Sen and Head (1997) . 2.1. AI-based traf fi c signal controllers
Modern traf fi c signal controllers tend to be more adaptive to the current traf fi c conditions than traditional controllers (e.g., fi xed-time controllers). That is if a change occurs in the network dynamics (due to accidents, rush hours, etc.) those traf fi controllers change accordingly the traf fi c signal con fi the way that optimizes the various performance indices (e.g., waiting time, queue lengths, etc.).

These controllers are mainly based on arti fi cial intelligence (AI) approaches, speci fi cally based on machine learning (ML) techniques.
There are two broad classes of the ML techniques; parametric and non-parametric .Ontheonehand, non-parametric ML techniques can be used to implicitly capture the control model from the training data. On the other hand, parametric ML techniques fi nd the optimal estimated value for the control model parameters (e.g., cycle time, offsets, splits, etc.) based on the training data.

For instance, parametric learning models are robust in the sense that there is no need for a complete mathematical model of the environment. Such controllers include arti fi cial neural networks, e.g., Smith and Chin (1995) , Srinivasan et al. (2006) , fuzzy logic, e.g., Gokulan and Srinivasan (2010) , Wenchen et al. (2012) , evolutionary algorithms, e.g., Lertworawanich et al. (2011) , S X nchez-Medina et al. (2010) . However, most of these approaches have the same problem of being only applied on small scale traf networks. Moreover, most controllers are hard to be applied on large scale traf fi c networks due to computational space and time constraints.

Generally, most of the previous works that are based on ML paradigms are non-adaptive in the sense that the dynamics of the environment is assumed to be non-changing (i.e., stationary). Particularly, after reaching steady state, the above learning algo-rithms can effectively converge to reasonable optimal con tion. However, if the road conditions change (due to rush hours, weather conditions, etc.), these methods fail to adapt to the new conditions, hence the performance indices might overshoot. In our traf fi c signal control framework, we handle the traf stationarity using: (1) Bayesian probability interpretation for estimating the parameters of the MDP (this estimation was found to be more stable, robust, and adaptive to the changing environ-ment dynamics) and (2) a novel adaptive cooperative exploration technique. We discuss these approaches in detail in Section 5 . 2.2. RL-based traf fi c signal controllers
The application of RL in the context of traf fi c signal control is pioneered by Thorpe and Anderson (1996) . This approach is based on a State-Action-Reward State-Action (SARSA) RL algorithm. This approach is based on a junction-based state-space representation which represents all possible traf fi c con fi gurations around a junction. In particular, each junction learns a Q -value that maps all possible traf fi c con fi gurations to total waiting times of all vehicles around the junction. As mentioned in Steingr X ver et al. (2005) , this representation quickly leads to a very large state-space, because there are many possible con fi gurations of vehicles waiting in the ingoing lanes of any junction. Most RL-based traf signal controllers proposed in the literature have junction-based full state representation (e.g., Abdulhai et al., 2003; El-Tantawy and Abdulhai, 2012; Medina and Benekohal, 2012 ). This suffers from the curse of dimensionality, the state-action space is esti-mated at the size of 10 101 (as mentioned in Prashanth and Bhatnagar, 2011 ).

In our work, we adopted a different approach that is a vehicle-based state-space representation ( Wiering, 2000 ). In this repre-sentation, the number of states will grow linearly in the number of lanes and vehicles 0 positions and thus will scale well for large networks. The traf fi c signal decision is made by combining the estimated gain (e.g., waiting time) of all vehicles around a junction. Note that each vehicle does not have to represent its estimated gain itself (this can be done by the traf fi c junction) but the representation is vehicle-based .

In El-Tantawy and Abdulhai (2012) , Medina and Benekohal (2012) , the authors proposed Q -learning algorithms for traf signal control with explicit coordination mechanisms among neighboring junctions. However, both works are based on junction-based state-space representation which consumes large space as discussed earlier. In addition, the latter work ( Medina and Benekohal, 2012 ) uses the max-plus algorithm which is computa-tionally demanding.
 These approaches are based on model-free RL algorithms (e.g.,
SARSA, Q -learning) in which the learning process is not guided by a state transition probability model. Although less computations per traf fi c signal decision are required by model-free RL methods relative to model-based ones, the convergence time is much smaller in model-based RL methods because the learning process is guided by a state transition probability model. The model-free RL methods may be more convenient in some domains, e.g., robotics applications where the computation and power capabilities of robots may be limited, while the number of iterations required for reaching the optimal policy is not demanding in applications lacking real-time decision making,e.g.,minesweepingusingrobots.Hence,we fi nd that model-based RL methods (e.g., value iteration) are more convenient for traf fi c signal control in which investing more computations per traf fi c signal decision is not a deman ding issue (considering the computation capabilities of junct ion agents) while reaching faster to the optimal learned values of traf fi csignalcon fi gurations is demand-ing in real-time traf fi csignalcontrol.

In this paper, we adopted the version of model-based RL presented in Wiering (2000) . This particular version proves its effectiveness when being applied to large scale traf fi c networks.
In Kuyer et al. (2008) , the authors extended Wiering RL model for traf fi c signal control ( Wiering, 2000 ) by using max-plus and coordination graphs. This work implements an explicit coordina-tion mechanism between the learning junction agents. The max-plus algorithm is used to estimate the optimal joint action by sending the locally optimized messages between the neighboring junctions. However, as mentioned in El-Tantawy and Abdulhai (2012) , the max-plus algorithm is computationally demanding and therefore the agents report their current best action at anytime even if the action found so far is sub-optimal.

RL approach using a local adaptive round robin phase switching model at each junction. Each junction collaborates with neighbor-ing junctions in order to learn appropriate phase timing based on traf fi c patterns. In Richter et al. (2007) , the authors exploited the natural actor-critic algorithm which is based on four RL methods, i.e., policy gradient, natural gradient, temporal difference, and least-square temporal difference. The authors extended the state-space of the agent to include the state of other agents to control a 10 10-junction grid. In Arel et al. (2010) , a distributed traf method using ML-based neural networks has been proposed. In this approach, RL is used to control only the central junction in a network of 5 junctions while the other 4 junctions use the longest-queue-fi rst algorithm and collaborate with the central agent by providing it with the local traf fi c statistics. However, due to the large state-space of junction-based methods, neural networks are used for better searching the state-space. 2.3. Wiering-based traf fi c signal controllers we use the GLD traf fi c simulator ( Wiering et al., 2004 ), see Fig. 1 . The GLD simulator was initially based on a very simple discrete-time discrete-space model of traf fi c dynamics. Three previous extensions to the GLD traf fi c simulator have been implemented with simple acceleration models. The fi rst extension is due to Cools et al. (2008) that proposes a simple rule-based acceleration model based on the distance to the front vehicle. The second extension is due to
Schouten and Steingr X ver (2007) that allows the vehicles to change their speed following either a Uniform or a Gaussian distribution.
The third extension is due to Kuyer et al. (2008) who implement the same technique of Gaussian distribution using different values of speed thresholds. All the three extensions are inherently discrete with respect to both the time and space domains.

An important concern in any traf fi c simulator is the generation of populations of vehicles at different parts of the traf (i.e., simulating the traf fi c demand). Two extensions have been added to the GLD in this context. Escobar et al. (2004) assume fi xed generation frequency over extended periods of time, the generation frequency can be changed over non-overlapping inter-vals, the schedule of such change is speci fi ed in an XML
Steingr X ver et al. (2005) implement the same technique through a screen graphical interface.

Two extensions have been added to the GLD to achieve traf green waves. The fi rst is implemented by Escobar et al. (2004) .
This work proposes a very simple rule-based method for imple-menting green waves which depends on successive green signals over consecutive junctions with offsets. These offsets are deter-mined based on the average speed of vehicles between the junctions. This is implemented over fi xed periods of time. Since only the two opposite directions of the main road can have green waves simultaneously, traf fi c in the side roads will be delayed even when the traf fi c fl ow on the main road is very low. The second extension was implemented by Cools et al. (2008) . They propose a more robust rule-based technique for implementing green waves. The integrity of a platoon of vehicles is achieved by preventing the tail of the platoon from being cut (when switching the traf fi c signal), while allowing the division of long platoons (in case there is a demand on the intersecting lanes) in order to prevent platoons from growing too much.

Our traf fi c signal control framework handles the drawbacks of the previously mentioned extensions to the GLD: acceleration model, traf fi c demand simulation, green wave implementation, etc. as will be shown in Section 4 . 2.4. Multi-objective based traf fi c signal controllers
To the best of our knowledge, few learning-based approaches are existing for multi-objective urban traf fi c signal control, e.g.,
Lertworawanich et al. (2011) . On the one hand, the majority of these methods are based on either neuro-fuzzy or Multi-Objective
Genetic Algorithms (MOGA). However, as mentioned in Faye et al. (2012) , the use of fuzzy logic is not suf fi cient to represent the real-time traf fi c uncertainties. Also, neural networks and genetic algorithms require many computations and their parameters are dif fi cult to be determined. In addition, as mentioned in Liu (2007) , traf fi c signal control methods based on fuzzy logic are more suitable to control traf fi c at an isolated intersection. Also, evolu-tionary algorithms such as genetic algorithms will spend huge time to converge to the optimal traf fi c signal decision for large scale networks. On the other hand, some traf fi c signal controllers that are junction-based , e.g., Abdulhai et al. (2003) implement RL models in which the reward is a function in both the total delay and the queue length. However, as mentioned previously, junction-based methods suffer from exponential state-space.
 In Wiering (2000) , the author proposes two controllers called
TC-2 and TC-3. The number of vehicles waiting in the queue at the next traf fi c signal is considered in the Q -function. The state representation is the same as in TC-1 (the original model of Wiering). However, as mentioned in Steingr X ver et al. (2005) , the proposed Q -function leads to an unusual adaptation of the real-time dynamic programming update in Eq. (1) . In addition, the Q  X  s ; a  X  0 s usually will not converge but instead keep oscillating between different values.

Houli et al. (2010) present a multi-objective RL traf fi c signal control model. However, the traf fi c adaptation is done of activating one objective function at a time according to the current number of vehicles entering the network per minute.

Steingr X ver et al. (2005) present two traf fi c signal controllers, namely State Bit for Congestion (SBC) and Gain Adapted by Congestion (GAC). Traf fi c junctions take into account congestion information from neighboring junctions. This extension allows the agents to learn different state transition probabilities and value functions when the outgoing lanes are congested (i.e., optimizes the fl ow rate while optimizing the primary objective; trip waiting time). However, adding a new bit to indicate the degree of congestion in the next lane increases the state-space and slows the learning process. On the contrary, in our model, the state-space representation is the same in size as the underlying traf signal controller ( Wiering, 2000 ). GAC ( Steingr X ver et al., 2005 ) does not learn anything permanent about congestion, also this approach cannot be easily generalized. In Section 3 , we present the underlying traf fi c signal control and simulation models for our multi-objective traf fi c signal control framework. 3. Background: traf fi c signal control and simulation models 3.1. Wiering RL traf fi c signal control model
In Khamis et al. (2012a,b) and Khamis and Gomaa (2012) ,we adopted the RL model developed by Wiering (2000) for traf signal control. Each junction is controlled by an active 2 agent that learns a policy for signal splitting through a guided trial-and-error life interaction process with the environment to online optimizing some criteria (e.g., minimizing the waiting time of vehicles). This approach is vehicle-based , that is, the state of the system is local and microscopic.

In Wiering 0 s approach, the state of the vehicle at a particular junction consists of the following pieces of information: (1) the traf fi c light of the lane in which the vehicle is moving or waiting, denoted tl , (2) the position in which the vehicle is currently at, denoted p , and (3) the destination towards which the vehicle is traveling, denoted des . In a real-world application, drivers/vehicles can send the information required by the junction controller agent (i.e., position and destination) for the junction to estimate the vehicle gain from the traf fi c signal decision. This can be achieved using some kind of sensors (e.g., sensors in smart phones) through a Vehicle-to-Infrastructure (V2I) communication protocol.
This approach is essentially a model-based value-iteration tech-nique where the state transition probability is continually estimated to guide the learning and optimization process. The state transi-tion probability is represented by a lookup table Pr  X  s ; is the action of the traf fi c signal (i.e., red or green) that causes the vehicle to move from state s to the next state s 0 . These probabilities are estimated based on the frequentist interpretation of probabil-ity: Pr  X  s ; a ; s 0  X  X  C  X  s ; a ; s 0  X  = C  X  s ; a  X  where C  X  s of transitions  X  s ; a ; s 0  X  and C  X  s ; a  X  counts the number of times a vehicle was in state s and action a was taken. In Khamis et al. (2012a) ,weusedthe Bayesian probability interpretation to estimate the parameters of these probabilities. This estimation was found to be more stable, robust, and continuously adaptive to the changing environment dynamics. We discuss this approach in Section 5 .
The original model ( Wiering, 2000 ) optimizes the cumulative waiting time of all vehicles till arriving at their destinations. Thus, the Q -function represents the estimated waiting time for a vehicle at state s until it arrives to its destination in case the action of the current traf fi c signal is a and is given by
Q  X  s ; a  X  X   X  where  X  is a discount factor (0 o  X  o 1) that discounts the in of the previously learned V -values and ensures that the Q -values are bounded. The reward function R  X  s ; a ; s 0  X  is the immediate scalar reward. In the single objective controller proposed in the original same position, otherwise equals 0. In Khamis et al. (2012b) and
Khamis and Gomaa (2012) , we proposed a more elaborate design for the reward function that is well-suited for a multi-objective traf fi c signal control framework. The proposed multi-objective reward function is discussed in Section 6 .

The V -function represents the estimated average waiting time for a vehicle at state s till leaving the traf fi c network regardless of the current traf fi c signal action and is given by
V  X  s  X  X   X 
The controller at each junction sums up the gains Q  X  s ;
Q  X  s ; green  X  of all vehicles waiting at the current junction and chooses the traf fi c signal con fi guration (consistent green lights on all directions of the junction) with the maximum cumulative gain. In the proposed multi-objective traf fi c signal control frame-work, we adopt the same gain de fi nition of vehicles.

The possible traf fi c signal con fi gurations (i.e., possible phases) represent the consistent green lights on all directions of the junction that do not cause any possible accidents between the crossing vehicles. Consider a junction controlling the traf between 4 intersecting roads. Each road consists of 4 lanes, in which the ingoing lanes per each road are one lane for turning left and one lane for going straight or turning right. According to this setting, there exist 8 possible traf fi c signal con (4 possible con fi gurations for the traf fi c signals of each road to be green for left and straight/right directions and 4 possible con urations for the traf fi c signals of each opposite roads to be green for left and straight/right directions).

For a fi xed time controller, all possible phases should at least be green once within a cycle. In our multi-objective framework, we do not estimate the optimal phase length, but rather, at each time step, the junction agent chooses (based on the current traf situation) either to extend the current phase or to begin another possible traf fi c signal con fi guration. In addition, the decision is based on all vehicles in the lane (i.e., not only the vehicles queued at the traf fi c signals), this setting is much consistent with the nature of the multi-objective function, i.e., formulation and eva-luation of some objectives, e.g., average trip time, average speed of vehicles, etc. 3.2. GLD traf fi c signal simulation model
In order to examine the proposed traf fi c signal control frame-work, some experimentation platform is needed, that is a traf c simulator . In our work, we chose to extend the moreVTS vehicle traf fi c simulator ( Cools et al., 2008 ) that is based on the GLD traf signal simulation platform ( Wiering et al., 2004 ). This is due to the following reasons: (1) the GLD is a widely used open source traf simulator, e.g., used by Cools et al. (2008) , Steingr X ver et al. (2005) ,
Kuyer et al. (2008) , and Prashanth and Bhatnagar (2011) , (2) the ability to compare the proposed traf fi c signal controller with other major traf fi c signal controllers implemented over the GLD, (3) col-lecting statistics from a set of performance indices that are already available in the GLD with the ability to add new performance indices, and (4) the visual ability to edit/create traf fi and schedule traf fi c demands through a graphical interface, see Fig. 1 . 4. Proposed framework contains severe drawbacks resulting from oversimpli fi cations that we fi xed in our previous work. We brie fl y mention our fi and refer the reader to the original papers for more details ( Khamis et al., 2012a,b; Khamis and Gomaa, 2012 ). 4.1. Continuous-time and continuous-space simulation platform that is based on cellular automata in which each road is repre-sented by discrete cells. A road cell can be occupied by a vehicle or can be empty. In Khamis et al. (2012a) , we implemented the more realistic IDM acceleration model ( Treiber et al., 2000 ) that is used to simulate, in continuous-time and continuous-space, the accel-eration and deceleration of vehicles. The vehicle acceleration d v = d t depends on (1) the current velocity 4 v , (2) the distance to the front vehicle s , and (3) the difference in velocity  X  positive when approaching the front vehicle; the acceleration is given by d v d t  X  a 1 s n  X  s 0  X  min 0 ; vT  X  v acceleration when the road is free a  X  1  X  v = v 0  X   X  , and the braking deceleration when there is a front vehicle a  X  X  s n = s  X  framework that need to be synchronized: (1) the IDM modeler time, (2) the traf fi c signal controller time, and (3) the GLD simulator time. The 3 clocks are synchronized every  X  t as follows.
First, the IDM modeler updates the state of all vehicles in the entire traf fi c network where the new positions are calculated as follows: speed new  X  speed old  X  acceleration IDM  X  t ; position new  X  position old speed new  X  t :  X  4  X 
Note that in the GLD, the vehicles position values are decreasing as vehicles move from its source nodes towards the junctions. This clari fi es the negative sign in the position update, Eq. (4) . After-wards, the simulator gathers all the needed statistics from the traf fi c network such as the average waiting time and the average queue length. The controller updates the state transition of each updates the traf fi c network screen visualization. Afterwards, the traf fi c signal controllers decide on the new actions at all junctions of the network by calculating how every traf fi c signal should be switched. The new traf fi c signal con fi gurations are applied by switching the traf fi c signals to their appropriate values. Finally, the simulator schedules the next state for the next time step (e.g., new vehicles join the network following the scheduled traf fi 4.2. IDM impact on the RL traf fi c signal control model
In Khamis and Gomaa (2012) , we analyzed and fi xed some crucial problems that appeared in the original RL traf fi control model ( Wiering, 2000 ), particularly when applying the
IDM acceleration model. As a result of the control being still discrete in nature, many IDM state transitions (potentially in correspond to one state transition with respect to the control (the controller perceives the lane as an extension of discrete cells whereas the IDM views it as a continuous stretched line  X  that the vehicle position is a part of the controller state de
As a result, some ambiguity appears in the de fi nition of the reward function R  X  s ; a ; s 0  X  . In particular, if the reward value is depending on the distance traveled by the vehicle, then there will be different immediate reward values for the same controller state transition. We solved this problem by averaging the reward values gained over time.

Another issue is the sign oscillation problem (a Zeno phenom-enon) that results from the in fi nitesimally slow acceleration of back vehicles when the traf fi c signal is just turning green. In this decreases the cumulative gain and accordingly forces the traf signal to switch back to red (too early) before any vehicle can cross the junction. We solved this issue by giving those stationary vehicles some penalty smaller than the one given when the traf signal is red, e.g., R  X  s ; a ; s 0  X  for back stationary vehicles when the signal is green equals 0.3 instead of one. 5 4.3. Traf fi c demand probability distributions
The traf fi c demand in the GLD traf fi c simulation model ( Wiering et al., 2004 ) is implemented by generating a uniform random number at every simulation time step and checking its value against a fi xed traf fi c demand rate A  X  0 ; 1 . In order to allow for variability and non-stationarity, we have implemented in the
GLD varying probability distributions of the inter-arrival times of the input vehicles in Khamis et al. (2012a) . 4.4. Exploration policy
In the underlying traf fi c signal control model ( Wiering, 2000 ), a random traf fi c signal con fi guration can be chosen with a small probability  X   X  0.01 for the exploration of the state-action space. In
Khamis and Gomaa (2012) , we also used  X  -exploration, though we found that it is better to start initially with a high exploration rate (where there is still no much knowledge about the optimal gain values to be exploited) and decrease the exploration rate gradually in time; the exploration rate was given by  X  t  X  exp  X  t = is the current simulation time step and k t is the Boltzmann temperature factor that decays by time till being fi xed at the value of 1. In the current paper, we propose a novel hybrid exploration technique that uses softmax exploration to better respond to transient periods (e.g., due to congestion at rush hours). This exploration technique is discussed in detail in Section 5 . 4.5. Fixing the next states de fi nition in the GLD
The implementation of the underlying traf fi c signal control model loops on all the possible next states s 0 according to the free positions ahead of a vehicle at state s in the current time step. Particularly, this implementation assumes the next states by discretizing the free distance between the vehicle and the front one. Thus, the sum of the transition probabilities of these next states is not a must to be equal to 1 because the probability should be calculated and updated based on the actually experienced next states. Hence, this implementation is improper and in Khamis and Gomaa (2012) we instead loop on all the next states that are actually experienced (e.g., by other vehicles) starting from the same state s . The sum of these state transition probabilities equals 1. 4.6. New performance indices
The main performance measure in the GLD depends on the average delay of the vehicles. The junction delay of a vehicle is calculated as follows: Junction delay  X  X  Time step the vehicle crosses the junction In Khamis et al. (2012b) ,wede fi ned the proper junction waiting time of a vehicle as follows: Junction waiting time  X  Time step the vehicle crosses the junction where joining the junction waiting queue is counted once the vehicle speed drops beyond a speci fi c threshold, 0.36 km/h ( Khamis and Gomaa, 2012 ).
 In Khamis et al. (2012b) , we criticized the inef fi ciency of the GLD performance indices. The original average trip waiting time (ATWT) proved to be insuf fi cient because all vehicles not arrived yet to their destinations (for any reason, e.g., due to congested traf fi c) are not incorporated in the statistics. We include all vehicles even those that have not yet arrived to their destinations by adding for those vehicles the expected trip waiting time V ( s )to the total waiting time they have experienced so far. The total waiting time that a vehicle has experienced equals the summation of the waiting times at the junctions that the vehicle has already crossed in Eq. (6) . We call this policy the co-learning technique for calculating the performance indices. We have also implemented the co-learning average trip time (ATT). For more details and mathematical derivations of the co-learning performance indices, the reader is referred to Khamis et al. (2012b) . Despite we have implemented as well the co-learning average junction waiting time (AJWT) version, it is not logically meaningful as the co-learning technique for calculating the performance indices is more con-venient to the trip-based statistics (using the expected remaining value till the end of the trip).

In the original GLD, the vehicles waiting in edge nodes (due to overfull ingoing lanes) do not enter the traf fi c network and consequently are not incorporated in many performance measures (e.g., ATWT, ATT, etc.). We solved this problem by rejecting the vehicles that are queued in edge nodes and use the percentage of rejected vehicles ( Khamis et al., 2012b ) as a more reasonable performance index. Moreover, we added the relative throughput performance index ( Khamis et al., 2012b ) in the GLD. This performance index equals the total number of arrived vehicles divided by the total number of entered vehicles. In addition, we added the average speed performance index ( Khamis et al., 2012b ).
This performance index equals the total distance traveled by all vehicles (either have arrived or have not arrived yet) divided by the total time spent in the network.

In order to evaluate the performance of the green wave objective, we added the average number of trip absolute stops performance index ( Khamis and Gomaa, 2012 ). Once the vehicle joins the waiting queue (i.e., its speed drops beyond 0.36 km/h, as mentioned earlier), we count 1 vehicle stop, and once the vehicle joins the next waiting queue after crossing the current junction, this count will be 2 vehicle stops. Since the vehicle stops increase the vehicle emission and oil consumption (as mentioned in Houli et al., 2010 ), we added the average number of vehicles trip stops performance index ( Khamis and Gomaa, 2012 ) to evaluate the performance of the fuel consumption objective. This performance index equals the sum of all vehicle stops in the whole trip divided by the number of arrived vehicles. 5. Handling traf fi c network non-stationarity 5.1. MDP parameters estimation using Bayesian probability interpretation
We use the Bayesian probability interpretation for estimating the unknown parameters of the MDP probabilities instead of the frequentist interpretation that was originally proposed in Wiering (2000) . In our approach, the current estimation becomes the prior for the next time step. This estimation is more stable and more adaptable to the changing environment dynamics. That is if a change occurs in the network dynamics (due to accidents, rush hours, etc.) the controller using this probability estimation can handle the traf fi cef fi ciently by the way that optimizes the various performance indices (e.g., waiting time, queue lengths, etc.) in the congested periods. The idea behind this state transition probability estimation is based upon the simple Bayes 0 rule: Let A and B be two events, then the posterior density of A given B has the following formula: Pr  X  A j B  X  X  Pr  X  B j A  X  Pr  X  A  X  = Pr  X  B  X  :  X  7  X 
Let P be a random variable representing an estimator of some unknown parameter. In the proposed traf fi c signal control frame-work, such a parameter can be either (1) one of the parameters of Pr  X  a j s  X  which is the posterior probability of taking action a given state s , or (2) one of the parameters of Pr  X  s 0 j s the transition probability of being in the next state s 0 given the state/action pair  X  s ; a  X  . Following, we give an example for illustration. Fix some state s , then Pr  X  a j s  X  has one parameter P for the probability of a  X  RED . For every time index t , let
I  X f j r t : state s is occupied at time j g . For every n  X j I , de fi ne the Bernoulli random variable X n as follows:
X  X 
That is X n is a sequence of Bernoulli random variables de at the time indices where the state s is occupied by a vehicle. When X n  X  1 is de fi ned, we estimate P by recursively applying the Bayesian inference rule as follows: Posterior  X  n  X  1  X  X  Likelihood  X  n  X  1  X  Prior  X  n  X  1  X  Then we have where  X  is the normalization factor. Solving the above recursive equation with the assumption that X n  X  1 are independent random variables,
For an easier differentiation, we fi nd
Differentiating with respect to P n  X  1 and equating to 0, where ln Pr  X  P n  X  1 j X n  X  1  X  and consequently Pr  X  P n  X  1 The posterior probability P n  X  1 as a function of n  X  1 is given by
P n  X  1  X  2  X  n  X  1  X  X  n  X  2  X   X  n  X  1 estimator P :
P  X  2 n  X  n  X  1  X   X  n 5.2. Adaptive cooperative hybrid exploration on both  X  -exploration and softmax exploration. In softmax exploration, the traf fi c signal decision is chosen proportionally to the gain values: exp  X  g i  X  =  X  g i exp  X  g i  X  , where g of the vehicles in the lanes of the traf fi c signal con fi number i . This hybrid exploration is more adaptive to the transient periods, particularly when a main road has very high congestion for some period of time (e.g., due to accidents or rush hours) while the side roads have much lower traf fi c demand. In this case, using  X  -exploration solely, leads to semi-permanent domination of the main road that causes long waiting times to the vehicles in the side roads. Thus, we propose at every time step that each junction decides whether to use the network-level  X  default  X   X  -greedy exploration (  X   X  0.01 as proposed in Wiering, 2000 ) or to use softmax exploration. We found that the softmax exploration gives better trip waiting time results in case the gain of some traf signal con fi guration exceeds the gain of any other con fi 20% of its value (i.e., domination that might lead to blockage of the other possible con fi gurations if  X  -greedy exploration is used). This hybrid exploration technique requires an explicit coordination between a junction agent and its neighboring junctions. A junction (or one of its direct neighbors) is said to be in a transient state if the cumulative gain of all vehicles in this junction keeps increasing (or decreasing) with 10% of its current value for 10 (or more) consecutive time steps. The cooperation is used to check if some junction is in a transient state, then this transient state will be most likely transferred soon to some neighboring junction; thus during this period it is preferable for the junction to use the softmax exploration. We have proposed another kind of cooperation in Khamis and
Gomaa (2012) that depends on transferring the learned Q-values (with some decaying cooperation factor) from the ingoing lanes of a junction to the outgoing lanes. This method leads to better performance in the transient period, however, we fi nd that the steady state is worse. The new cooperative hybrid exploration technique improves both the transient and steady state periods. 6. Multi-objective RL model for traf fi c signal control
As mentioned in Jin and Sendhoff (2008) , little work has been done in multi-objective RL with some exceptions, e.g., G X bor et al. (1998) , Mannor and Shimkin (2004) , and Natarajan and Tadepalli (2005) . Thus, the framework proposed in this paper is considered a novel contribution to the area of using multi-objective RL especially in the domain of traf fi c signal control.

In our model, we had two alternatives for implementing the multi-objective RL traf fi c signal control. The fi rst is to use a separate Q -function for each objective, the second is consolidating all rewards in one Q -function. We decided to use the second alterative that is more suitable for the vehicle-based approach where each vehicle has two representative values Q  X  s ; red  X  and
Q  X  s ; green  X  . In particular, similar to the underlying traf fi control model ( Wiering, 2000 ), s is the state of the vehicle and
Pr  X  s ; a ; s 0  X  is the state transition probability; both values are the same for the various objectives with respect to the same vehicle.
The innovative part in this model speci fi cally (and in the RL generally) is the design of the reward function . The consolidated reward values represent the core of the model which lead to the fi nal estimated gain of every vehicle which affects the decision of the traf fi c signal controller.
 The proposed multi-objective function is given by
Q  X  s ; a  X  X   X 
Let the distance traveled by the vehicle in the current time step be equal to  X  p (always positive). The fi rst reward represents the
ATWT (the same as the single objective of Wiering 0 s approach) and is given by R ATWT  X  s ; a ; s 0  X  equals 10 or 3 in case the traf is red or green respectively with  X  p C 0, otherwise equals 0.
The second reward represents the ATT. In this paper, we improve the ATT reward function that we previously proposed in
Khamis and Gomaa (2012) in order to better discriminate the reward values in case the traf fi c signal is red or green. For instance, if the vehicle waits at the current position, i.e.,  X  p C to higher ATT), then it will be penalized by the reward value.
In main roads, our controller enforces the ATT objective to dominate by using a stronger reward function: R ATT  X  s ; C  X  1 2  X  2 p  X  . In side roads (e.g., residential areas in which the main objective is to avoid accidents ), the controller uses a weaker ATT reward function: C ATT  X  1 2  X  p  X  . C ATT equals 10 or 10 in case the traf fi c signal is red or green respectively. Since the negative value when the traf fi c signal is green.

The third reward represents the AJWT. If the vehicle waits at the current junction, i.e., tl 0  X  tl (that leads to higher AJWT), then it will be penalized by the reward value. The AJWT reward function (the AJWT will increase if the current lane has red signal or is congested with green signal).

The fourth reward represents the fl ow rate (FR) in which we consider the spatial queuing that considerably affects neighboring junctions performances. If there is high congestion in the next lane, then the vehicle will be penalized by the reward value. The FR reward function is given by R FR  X  s ; green ; s 0  X  X  10 in case tl otherwise equals 0. Assume that the number of blocks 7 taken by the waiting vehicles in the next lane 8 and the length of the next lane to be N and L respectively. Let W  X  N = L , then the Congestion Factor (CF) is given by ( Houli et al., 2010 ) CF  X  s ; green ; s 0  X  X  is a threshold whose best value equals 0.8 (as mentioned in Steingr X ver et al., 2005 ). For instance, if N  X  9 m and L  X  10 m, then FR when the next lane is congested). If tl 0 a tl , CF  X  s decrease when the next lane at tl 0 is free. In this case, Q  X  s will decrease and thus the cumulative gain will increase (recall that a vehicle gain equals Q  X  s ; red  X  Q  X  s ; green  X  ) and accordingly the green phase length will be longer that allows more traf pass through, i.e., increasing vehicles fl ow rate.

The fi fth reward represents achieving a traf fi c green wave (GW) and is implemented by checking the following conditions: (1) the current lane is part of a main road , (2) the current traf green , and (3) the number of vehicles within distance  X  from the traf fi c junction is A  X  1 ;  X  , then R GW  X  s ; green ; equals 0. The best parameters values are  X   X  25 m (as proposed in Cools et al., 2008 ) and  X   X  3 vehicles. Unlike the original RL model ( Wiering, 2000 ) that considers only the gain of the waiting vehicles when taking a traf fi c signal decision, our controller considers as well the approaching vehicles. In this case, the red signals might switch to green even before the vehicles reach the junctions creating an emergent green wave (the vehicles need not slow down or stop at all). That occurs due to the increase of Q  X  s ; red  X  for the approaching vehicles.
 The sixth reward represents the accidents avoidance (AA). In this paper, we improve the safety reward function that we previously proposed in Khamis and Gomaa (2012) in order to better discriminate the reward values in case the traf fi red or green. The impact of an accident (i.e., vehicles moving with very slow speed or stationary at a short distance e beyond a green traf fi c signal) is propagated to the vehicles crossing the green signal. In this case, our controller uses a stronger AA reward function regardless of the road type: R AA  X  s ; a  X  1 =  X   X  2 p  X  1  X  X  . The best value of the short distance e beyond the traf fi c junction is 10 m (as proposed in Gershenson and Rosenblueth, 2009 ). In residential and schools areas, our controller alleviates driver 0 s aggressiveness by using the following AA reward function: C AA  X  1 =  X   X  p  X  1  X  X  . C AA equals 10 or 10 in case the traf fi c signal is red or green respectively. This reward function assures that Q  X  s ; green  X  will increase at high vehicle speeds that decreases the gain leading the traf fi c signal to switch to red (i.e., forces vehicles to decelerate that helps in accidents avoidance in residential and schools areas). Note that in the simulation envir-onment, the IDM acceleration model is a collision-free model ( Treiber et al., 2000 ). Thus, we cannot measure ef fi ciently the performance of the AA objective, e.g., by using the number of accidents performance index. However, other performance indices still can give good indication, e.g., average speed of vehicles .
The seventh reward represents forcing vehicles to move within moderate speed (MS) range of minimum fuel consumption. In this paper, we improve the fuel consumption reward function that we previously proposed in Khamis and Gomaa (2012) in order to better discriminate the reward values in case the traf fi red or green. If the distance traveled per time step (resulting in the motion from a controller state s to a next state s 0 ) is smaller or greater than the moderate speed limits (for main roads is 60 70 km/h and for side roads is 55  X  70 km/h), we set R MS  X  s C
MS or C MS , otherwise equals 0. C MS equals 10 or 10 in case the traf fi c signal is red or green respectively. 7. Experimentation 7.1. Symmetric network: horizontal main roads with vertical side roads
We use the traf fi c network in Fig. 1 for experimentation. This network consists of 12 edge nodes and 9 traf fi c signal nodes. There are 6 roads each of 2 lanes in each direction. The 3 horizontal roads are the main roads (where there is higher possibility of traf fi c green wave creation) each of length equals 1120 m (2 entry links each of 300 m, 2 links between intersections each of 200 m, and 3 junctions each of 40 m) and the 3 vertical roads are the side roads each of length equals 920 m (2 entry links each of 200 m, 2 links between intersections each of 200 m, and 3 junctions each of 40 m). The road lengths and generation rates are chosen to simulate a high congestion in main roads with less traf fi roads. This setting is made to show how the proposed traf controller can tackle the possible long waiting vehicles in side roads. We assume that all vehicles have equal length and number of passengers. The  X  discount factor is set to 0.9. The duration of each simulation time step is 0.25 s. The results of this experiment are averaged over 10 independent runs. Every run has a seed equals its starting computer clock time (in milliseconds) and consists of 100,000 time steps which are about 400 min.
As mentioned in Prashanth and Bhatnagar (2011) , the propor-tion of vehicles fl owing in a main road to those on a side road is in the ratio of 100:5 (this setting is close to real-life traf on many busy corridors and grid networks). Accordingly, we set the default generation rate of the main and side roads to 0.04 (576 vehicles per hour 9 ) and 0.002 ( C 30 vehicles per hour) respec-tively. We set the default weather condition in the main and side roads to normal rain and sandstorm respectively and the IDM desired velocity parameter v 0 to 108 km/h and 77 km/h respec-tively. For more details about the impact of weather conditions on the IDM acceleration model parameters, we refer the reader to
Khamis et al. (2012b) . We set the speed limit of the main and side roads to 60 km/h and 55 km/h respectively. For more details about the labeled roads and their characteristics, we refer the reader to Khamis and Gomaa (2012) .

In order to clarify the case where the vehicles in the side roads will wait for very long times, i.e., main road domination , when the controller uses  X  -greedy exploration, we schedule the destination frequency such that 90% of the traf fi c demand generated from the source edge node of a main road will exit from its destination edge node. The remaining 10% of the generated traf fi c demand will exit uniformly from the other 10 edge nodes. We use the same destination frequency for the side roads. In order to simulate the transient periods in the main roads, the traf fi c demand is drama-tically changed every 100 min where the distribution of the inter-arrival time is set to U  X  a  X  2 ; b  X  4  X  , i.e., at maximum a vehicle is generated every 2 time steps (7200 vehicles per hour) and at minimum a vehicle is generated every 4 time steps (3600 vehicles per hour), continued for a period of 5 min (this corresponds to extremely high congested traf fi c situation). In these periods, we set the weather condition to dry and the IDM desired velocity para-meter v 0 to 120 km/h. Dashed vertical lines clarify times at which changes occur in dynamics. 7.2. Results controller (using the Bayesian probability estimation) with hybrid exploration based on the transient state of the current and neighboring junctions (i.e., cooperation-based) versus the TC-1 controller ( Wiering, 2000 ) (single objective with frequentist prob-ability estimation using  X  -exploration). The former controller is represented by blue long dashes, while the latter controller is represented by red dashes. Note that the achievements added to the GLD traf fi c simulator are applied on all controllers for fair performance evaluation.
 comparison with two adaptive control strategies which are also based on AI methods: Self-Organizing Traf fi c Lights (SOTL) ( Cools et al., 2008 ) and a Genetic Algorithm (GA) ( Wiering et al., 2004 ).
Both controllers are already implemented in the GLD traf fi simulator, namely  X  SOTL platoon  X  and  X  ACGJ-1  X  respectively.
The SOTL controller turns a traf fi c signal to green if the time elapsed, since the signal turned red, reaches a certain threshold (  X  min  X  5 s). Given that the number of vehicles in the lane controlled by this traf fi c signal reaches another threshold ( vehicles) within a distance of 80 m from the red signal. In the intersecting lane (which will be switched to red), the integrity of a platoon of vehicles is maintained by preventing the platoon tail from being cut (platoon tail A  X  1 ;  X   X  3 vehicles ) within a distance  X   X  25 m from the green signal, while allowing the division of long platoons. The ACGJ-1 controller creates a genetic population every time step and tries to fi nd the optimal city-wide con fi parameters of this algorithm are as follows: mutation factor  X   X  0.05, population size s  X  200, and maximum number of gen-erations maxGen  X  100.
 effectiveness (MOEs): ATT, ATWT, average speed, average number of trip stops, average number of trip absolute stops, percentage of arrived vehicles, percentage of rejected vehicles (indicating net-work utilization), and the maximum queue length.
 adverse weather conditions), our controller signi fi cantly outper-forms the single objective controller. For the co-learning ATT , Fig. 2 , and the co-learning ATWT , Fig. 3 , the mean values are C 6 times lower respectively when using the multi-objective con-troller. Figs. 2 and 3 10 show that the multi-objective controller has much more stable response to the changing dynamics (occurring every 100 min). The response of the single objective controller to the transient periods is severe. Fig. 4 shows that the average speed of vehicles is C 8 times higher when using the multi-objective controller. This means lower congestion and faster arrival to destinations (that increases the driver 0 s satisfaction). Fig. 5 shows that when using the single objective controller, a vehicle stops at almost all junctions that the vehicle crosses before exiting the network ( C 3 junctions). Whereas, when using the multi-objective controller, a vehicle stops on average at only 1 junction. This creates a traf fi c green wave . Fig. 6 shows that the vehicle stops are C 22 times lower when using the multi-objective controller. This will save fuel consumption and consequently is more envir-onment friendly. Moreover, the number of vehicle stops can also be considered as a good measure of the total delays that encounter vehicles.

Fig. 7 shows that the mean value of the arrived vehicles percentage is higher by C 22% when using the multi-objective controller. This performance index is a good indicator of the network throughput , and accordingly the traf fi c fl ow rate.
Fig. 8 shows that the rejected vehicles percentage relative to all generated vehicles (i.e., generated but cannot join the network due to overfull ingoing lanes) is C 4 times lower when using the multi-objective controller. This performance index is a good indicator of the network congestion , and accordingly the network utilization .
Fig. 9 shows that the mean value of the maximum number of vehicles waiting at any junction in the entire network is lower by
C 10 vehicles when using the multi-objective controller. This performance index is a good indicator of the driver 0 s comfort (i.e., waiting in shorter queues).

We use the co-learning ATWT performance index in order to show the impact of using the cooperative hybrid exploration (discussed in
Section 5 ) on the long waiting times of vehicles in side roads when using the  X  -exploration solely. Fig. 10 compares the co-learning ATWT of the multi-objective controller with hybrid exploration based on the transient state of the current junction, the neighboring junctions, or the current-neighboring junctions versus the  X  -exploration. The mean value of the multi-objective controller with hybrid exploration based on the current-neighboring junctions is lower by C 10% than the multi-objective controller using  X  -exploration. 7.3. Validation
In order to better realize the contributions presented in this paper, here we give some insights about how the results presented in this paper can be validated. Firstly, the mathematical model of estimating the parameters of the MDP based on the Bayesian probability interpretation presented in Section 5 represents one sort of system validation. In Eq. (15) , the agent takes the whole history into consideration in the learning process and gives higher weight to the initial experiences than the most recent ones. Since non-stationarity in the traf fi c network (e.g., due to accidents, rush hours, etc.) lasts for some limited time (i.e., transient periods), the system performance will be more stable and not much affected with these abrupt changes.
 ( Q -function) formulation (Eq. (16) ) in which the various reward functions (even of the con fl icting objectives) work in harmony to optimize the fi nal value function. This is generally achieved by gain will increase (recall that a vehicle gain equals Q  X  s
Q  X  s ; green  X  ) and accordingly the green phase length will be longer that allows more deserving vehicles to cross the junction. traf fi c signal controller with the theoretically optimum solution may be computationally prohibitive . Our multi-objective traf signal controller is mainly based on online decision making .
Whereas, it is computationally demanding to compute the theore-tical optimum solution at every time step, e.g., using Little 0 slawof
Queueing Theory which may ignore some traf fi c related character-istics, e.g., the speed of vehicles and the inter-dependability between consecutive junctions, etc.. However, we can simply say that the theoretical optimum ATWT is zero . In addition, the theoretical optimum ATT can be calculated from the optimum average speed in main roads (equals 70 km/h C 20 m/s) and the average traveled distance (equals 1.12 km  X  1120 m). Note that the average traveled distance is calculated based on the destination frequencies (where 90% of the traf fi c demand generated from the source edge node of a main road will exit from its destination edge node.) Moreover, this average traveled distance complies with the average absolute number of vehicle stops, i.e., 3 stops, Fig. 5 . Thus, for the traf fi c network in Fig. 1 , the theoretical optimum ATT equals 1120 m C 20 m/s  X  56 s C 1 min. In comparison with the perfor-mance of our multi-objective traf fi c signal controller (the mean value of ATT C 4 min and the mean value of ATWT C 2 min) considering the dramatic change in the traf fi c demand every 100 min; our traf fi c signal controller yields very good results. objective controller is C 17 km/h. This value complies with the average speed in many mega cities which guarantees safety in urban areas. Moreover, this average speed value is not too low in the sense that it yields lower fuel consumption especially when being compared to the performance of other controllers, Fig. 4 .
In addition, the mean value of the average speed using our multi-objective controller (i.e., C 17 km/h) complies with the mean value of the ATT presented in Fig. 2 (i.e., C 4 min); given that the average traveled distance is 1120 m as mentioned previously.
This yields some kind of validation for the presented results. 7.4. Discussion
The proposed multi-objective traf fi c signal controller does not overshoot at all in transient periods in Figs. 2 and 3 . This is due to the triple effect of: (1) the reward function of the ATWT tackles the Zeno phenomenon discussed in Section 4 (giving stationary vehicles some penalty smaller than the one given when the traf signal is red). In addition, the reward function of the ATT is a function in the road type as discussed in Section 6 (in main roads, our controller enforces the ATT objective to dominate by using a stronger reward function), (2) using the Bayesian probability interpretation for estimating the parameters of the underlying
MDP which responds effectively to the traf fi c non-stationarity lasting for limited period of time. As mentioned in Section 5 , the current estimation becomes the prior for the next time step. This estimation is more stable and more adaptable to the changing environment dynamics, and (3) using the novel adaptive coopera-tive exploration technique (discussed in Section 5 ) in which the impact of any transient period is propagated between the neigh-boring junctions to avoid very long waiting times in side roads (i.e., main road domination).

Note that the objectives could be classi fi ed into three con ing groups: (1) ATWT, ATT, AJWT, FR, GW, (2) AA, and (3) MS.
In particular, to position our work in the scope of multi-objective reinforcement learning, we do not compute the Pareto front (that is computationally demanding), we rather use multi-objective scalar optimization (i.e., scalar addition for the rewards represent-ing the different objectives). For example, the Pareto front may include one optimal solution in which the trip time is minimized to the level that does not maximize the fuel consumption (in case a vehicle is moving too fast). The study of such points of optimality is subject to a future study.

Moreover, despite the proposed multi-objective traf fi csignalcon-troller is based on con fl icting objectives, the performance indices are not con fl icting .Forinstance,the number of vehicle stops is decreased when using our multi-objective controller that indicates lower fuel consumption, while the trip time is also decreased that indicates a possibility of higher fuel consumption. However, we ignore this possibility because in urban areas the trip time is scarcely decreased to the level at which high amount of fuel is consumed.

Table 1 presents the mean values of the various MOEs when adding the objectives incrementally. This gives a better view about the impact of adding the reward function of every objective on the various performance indices. One interesting conclusion is that the addition of every reward function almost affects the entire set of
MOEs, i.e., not just the corresponding MOE being optimized; this assures that machine learning is inherently a multi-objective task (as mentioned in Jin and Sendhoff, 2008 ). Moreover, this opens the door to a future study of the impact of every individual objective, i.e., instead of being added incrementally. In addition, one can examine the performance when changing the order of adding the reward function of every objective. Finally, those proposed experi-ments should be tried on various traf fi c patterns; this can clearly show the impact of every objective under the speci fi c conditions at which this objective optimally behaves.

Another issue worth discussing is studying the time complexity of the proposed multi-objective traf fi c signal control framework. On the one hand, in the work presented in this thesis, we did not optimize the execution time of the controller, e.g., using parallel programming techniques. However, the time complexity of the multi-objective controllerversusthesingleobjectiveoneiscomparable.Thisis mainly due to the scalar addition of the reward functions of the multi-objective controller. Thus , the high performance gain of the multi-objective controller (as shown by the various performance indices) does not come with a high computation cost. On the other hand, the proposed traf fi c signal controller is based on online learning and accordingly online decision making, thus there is no speci threshold for reaching a terminal state. This is mainly due to the continuous learning of the changing environment dynamics. 7.5. City center network: competing demands with non-parallel arterials
In this section, we apply the proposed traf fi c signal controller on a different traf fi c pattern , Fig. 11 (non-symmetric network with competing demands and not only parallel arterials). This traf network complies with the city center traf fi c network presented in Wiering et al. (2004) . The inner edge-nodes represent a city center.
The settings of this traf fi c network are similar to those in the main scenario traf fi c network, Fig. 1 ; the number of lanes in each direction, the length and the number of passengers of each vehicle, discount factor, and the duration of each simulation time step.
The horizontal and vertical roads highlighted by green are the main roads. The results of this experiment are averaged over 10 independent runs. Every run has a seed equals its starting computer clock time (in milliseconds) and consists of 50,000 time steps which are about 200 min.

The generation and destination rates are chosen to simulate competing demands where the default generation rate of all edge nodes is set to 0.01 (144 vehicles per hour). The default weather condition is set to light fog and the IDM desired velocity parameter to 90 km/h. We schedule the destin ation rate of every edge node to be equiprobable to the rest of the edge nodes, i.e., equals 1/8.
In order to simulate the transient periods at normal congestion periods (e.g., road users going to and leaving from the work), the traf fi c demand is changed every 100 min where the distribution of the inter-arrival time is set to 0.04 (576 vehicles per hour) continued for a period of 5 min. In these periods, we set the weather condition to normal rain and the IDM desired velocity parameter v 0 to 108 km/h. Dashed vertical lines clarify times at which changes occur in dynamics.

Figs. 12  X  15 show that the average junction waiting time ( AJWT ), the average number of trip stops , the percentage of rejected to generated vehicles , and the average number of vehicles waiting at any junction are better when using the multi-objective controller compared to the other controllers. 7.6. Average green light percentage
In order to determine how the proposed traf fi c signal controller deals with congestion speci fi cally and how it behaves generally, we added a new performance index to the GLD traf fi csimulator,thatis the average green light percentage 11 at each junction. This perfor-mance index represents the percentage of time that a speci c light con fi guration 12 at a speci fi cjunctionis green . For space limita-tions, we mention here the traf fi c signal operation at two junctions only (A and B), Fig. 16 , that are highlighted by red boxes in Fig. 11 .
Figs. 17  X  20 show that the average green light percentage at junction A using the proposed multi-objective controller is better than the other controllers. For instance, the proposed controller,
Fig. 17 , responds effectively to the transient periods (occurring every 100 min) where it gives larger green time percentage to the critical con fi guration, i.e., the second con fi guration towards the city center. Nevertheless, at the same time, the proposed controller gives good chances to the vehicles in the other signal con tions to cross junction A. On the one hand, the TC-1 controller,
Fig. 18 , and the ACGJ-1 controller, Fig. 19 , do not make the suf distinction to the critical con fi guration especially at transient periods . On the other hand, the SOTL controller, Fig. 20 ,over-discriminates the critical con fi guration, however, it almost blocks the third traf fi c signal con fi guration.
 junction B using the proposed multi-objective controller is better than the other controllers. For inst ance, the proposed controller,
Fig. 21 ,respondseffectivelytothe transient periods (occurring every 100 min) where it prioritizes the traf fi csignalcon fi gurations accord-ing to their directions to/from the city center with the following order: (1) the third con fi guration (the vehicles most probably are entering the city center), (2) the fi rst con fi guration (the vehicles are leaving the city center at rush hours), and then (3) the second con fi guration (the vehicles may enter, leave, or move around the city center). The TC-1 controller, Fig. 22 ,theACGJ-1controller, Fig. 23 ,and the SOTL controller, Fig. 24 , do not make such prioritization to the possible traf fi csignalcon fi gurations. 8. Conclusions and future work 8.1. Conclusions forcement learning system for traf fi c signal control based on a cooperative multi-agent framework. We show that using RL for solving control optimization problems in continuous state-space (speci fi cally in the traf fi c signal control domain) has some chal-lenges that affect the reward design of the model. In addition, we show that using the Bayesian probability interpretation to esti-mate the parameters of the MDP probabilities can result in a better response to the traf fi c non-stationarity. Traf fi c non-stationarity is simulated by changing the traf fi c fl ow and the traf fi resulting from changing the weather conditions.

Generally, the application of multi-objective RL optimization is still a challenging task, and particularly, in the domain of traf signal control. However, using an innovative reward design on a scalar-based form can greatly boost the various performance indices without the overhead of other computationally demanding techniques (e.g., using Max-plus, Pareto front optimization, etc.)
Moreover, we show that the application of new exploration techniques that are adaptive to the current traf fi c conditions can greatly affect the performance of the traf fi c signal controller.
Under the congested and free traf fi c situations, the proposed multi-objective traf fi c signal controller signi fi cantly outperforms the underlying single objective controller. For instance, the aver-age trip and waiting times are C 8 and 6 times lower respectively when using the multi-objective controller.

Finally, we show that the proposed traf fi c signal controller outperforms other controllers using the city center traf fi with competing demands . This traf fi c network is unlike the typical traf fi c pattern of main arterials and side roads (that leads to a main road domination ) where the proposed traf fi c signal controller optimally behaves. 8.2. Future work future work. For better organizing the suggested directions for future work, we categorize our ideas into future work in traf fi control model and future work in traf fi c signal simulation model. 8.2.1. Traf fi c signal control model
Firstly, we want to investigate the multi-objective optimization using the Pareto front approach. This will be a challenging task in the domain of reinforcement learning traf fi c signal control. For instance, we may separate the bene fi ts associated with each traf objective and identify the trade-offs associated with each one. We can introduce the objectives one by one and show how these objectives affect the various performance indices.

Secondly, we want to check the robustness/sensitivity of the proposed multi-objective traf fi c signal controller due to noisy input provided by sensors, i.e., partial observability of state-space.
In Schouten and Steingr X ver (2007) , the authors overcome the partial observability of the traf fi c state by estimating belief states and combining this with multi-agent variants of approximate
Partially Observable Markov Decision Process (POMDP) solution methods. It was shown that the state transition model and value function could be estimated effectively under partial observability .
Thirdly, we plan to control traf fi c signals in roundabouts .An initial idea is based on game-theory ; every vehicle in every approach (i.e., road in the roundabout) will play a game with other vehicles in the other approaches. The precedence of round-about crossing will be determined accordingly.

Fourthly, we want to check the role of further exploration techniques in enhancing the various performance indices (i.e., not only the trip waiting time of vehicles). Another possible improve-ment is generalizing the role of exploration in enhancing the performance when the congested periods are continued over an extended course of time or when no change in dynamics occur for a long period of time (despite these are rare cases).

Finally, our long-term goal is to implement and test the proposed controller on real traf fi cnetwork in Egypt. However, there are some challenges of deployment in Egypt, e.g., unlanned roads, chaotic driving behavior, etc. We can overcome the unlanned roads by state-space approximation to the vehicles 0 positions. For the chaotic driving behavior ,thetraf fi c signal controller can learn the non-stationarities due to the aggressive undisciplined driving behavior in Egypt. We need to integrate the traf fi c control system with sensors (through loop detectors in roads, cameras, and/or communication with vehi-cles using GPS/Wi-Fi sensors). 8.2.2. Traf fi c signal simulation model
Firstly, we need to examine the controller behavior when simulating an accident at some part of the traf fi c network (that need special handling from the traf fi c signal controller) while in another part of the network there is a free-fl owing traf
Secondly, we plan to use learning-based techniques to estimate the optimal values of the parameters of the IDM acceleration model based on the driving behavior in Egypt.

Finally, we need to use a more advanced traf fi c simulator (rather than the GLD) to simulate a real traf fi c network and examine the proposed controller behavior accordingly. One pro-posed solution is the integration of the proposed traf fi control framework with a 3 Dtraf fi c simulator that allows for human drivers as well as agent vehicles . This simulator has been developed as a collaboration of our research team with the Prendinger Laboratory in the National Institute of Informatics (NII), Tokyo, Japan.
 Acknowledgment This work is mainly supported by IBM PhD Fellowship and E-JUST Research Fellowship, and partially by Pharco Pharmaceuticals Cor-poration grant. The authors would like to thank the anonymous reviewers for their valuable comme nts and suggestions that helped improve this paper. Special thanks are due to Prof. Ahmed El-Mahdy, the director of the Smart City Facility at the Egypt-Japan University of Science and Technology (E-JUS T), for fruitful discussions. References
