 In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle . In this way, the active learner can achieve good performance with much fewer labels than passive learning . The number of these queried labels, which is necessary and sufficient fo r obtaining a good leaner, is well-known as the sample complexity of active learning.
 Many theoretical bounds on the sample complexity of active l earning have been derived based on the realizability assumption (i.e., there exists a hypothesis perfectly sepa rating the data in the hypothesis class) [4, 5, 11, 12, 14, 16]. The realizability assumption, however, rarely holds in practice. Recently, the sample complexity of active learning in the non-realizable case (i.e., the data cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17]. It is worth noting that these bounds obtained in the non-real izable case match the lower bound  X (  X  2  X  2 ) [19], in the same order as the upper bound O ( 1  X  2 ) of passive learning (  X  denotes the generalization error rate of the optimal classifier in the hypothesis class a nd  X  bounds how close to the optimal classifier in the hypothesis class the active learner has to g et). This suggests that perhaps active learning in the non-realizable case is not as efficient as tha t in the realizable case. To improve the sample complexity of active learning in the non-realizable case remarkably, the model of the noise or some assumptions on the hypothesis class and the data dist ribution must be considered. Tsybakov noise model [21] is more and more popular in theoretical anal ysis on the sample complexity of active learning. However, existing result [8] shows that obtainin g exponential improvement in the sample complexity of active learning with unbounded Tsybakov nois e is hard.
 Inspired by [23] which proved that multi-view setting [6] can help improve the sample complexity of active learning in the realizable case remarkably, we hav e an insight that multi-view setting will also help active learning in the non-realizable case. In thi s paper, we present the first analysis on the sample complexity of active learning in the non-realizable case under multi-view setting, where the non-realizability is caused by Tsybakov noise. Specificall y: -We define  X  -expansion , which extends the definition in [3] and [23] to the non-reali zable case, and  X  -condition for multi-view setting. -We prove that the sample complexity of active learning with Tsybakov noise under multi-view setting can be improved to e O (log 1  X  ) when the learner satisfies non-degradation condition. 1 This exponential improvement holds no matter whether Tsybakov noise is bound ed or not, contrasting to single-view setting where the polynomial improvement is the best possible achievement for active learning with unbounded Tsybakov noise. -We also prove that, when non-degradation condition does no t hold, the sample complexity of ac-tive learning with unbounded Tsybakov noise under multi-vi ew setting is e O ( 1  X  ) , where the order of 1 / X  is independent of the parameter in Tsybakov noise, i.e., the sample complexity is always e O ( 1  X  ) no matter how large the unbounded Tsybakov noise is. While in p revious polynomial bounds, the order of 1 / X  is related to the parameter in Tsybakov noise and is larger th an 1 when unbounded Tsy-bakov noise is larger than some degree (see Section 2). This d iscloses that, when non-degradation condition does not hold, multi-view setting is still able to lead to a faster convergence rate and our polynomial improvement in the sample complexity is better than previou s polynomial bounds when unbounded Tsybakov noise is large.
 The rest of this paper is organized as follows. After introdu cing related work in Section 2 and preliminaries in Section 3, we define  X  -expansion in the non-realizable case in Section 4. We analy ze the sample complexity of active learning with Tsybakov nois e under multi-view setting with and without the non-degradation condition in Section 5 and Sect ion 6, respectively. Finally we conclude the paper in Section 7. Generally, the non-realizability of learning task is cause d by the presence of noise. For learning the task with arbitrary forms of noise, Balcan et al. [2] propose d the agnostic active learning algorithm A 2 and proved that its sample complexity is b O (  X  2 complexity of the algorithm A 2 , Hanneke [17] defined the disagreement coefficient  X  , which depends on the hypothesis class and the data distribution, and prove d that the sample complexity of the algorithm which extends the scheme in [10] and proved that it s sample complexity is b O (  X   X  2  X  2 ) . Recently, the popular Tsybakov noise model [21] was conside red in theoretical analysis on ac-tive learning and there have been some bounds on the sample co mplexity. For some simple cases, where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sam-ple complexity is possible [4, 7, 18]. As for the situation wh ere Tsybakov noise is unbounded, only polynomial improvement in the sample complexity has been obtained. Bal can et al. [4] as-sumed that the samples are drawn uniformly from the the unit b all in R d and proved that the sample complexity of active learning with unbounded Tsybakov nois e is O  X   X  2 1+  X  (  X  &gt; 0 depends on Tsybakov noise). This uniform distribution assumption, ho wever, rarely holds in practice. Castro and Nowak [8] showed that the sample complexity of active lea rning with unbounded Tsybakov noise is b O  X   X  2  X  + d  X  2  X   X  1  X  ( &gt; 1 depends on another form of Tsybakov noise,  X   X  1 depends on the H  X  older smoothness and d is the dimension of the data). This result is also based on the strong uniform distribution assumption. Cavallanti et al. [9] assumed that the labels of examples are generated according to a simple linear noise model and in dicated that the sample complexity the algorithms or variants thereof in [2] and [13] can achiev e the polynomial sample complexity b O  X   X  2 1+  X  for active learning with unbounded Tsybakov noise. For acti ve learning with unbounded Tsybakov noise, Castro and Nowak [8] also proved that at leas t  X (  X   X   X  ) labels are requested to learn an  X  -approximation of the optimal classifier (  X   X  (0 , 2) depends on Tsybakov noise). This result shows that the polynomial improvement is the best possible achievement for active lea rning with un-bounded Tsybakov noise in single-view setting. Wang [22] in troduced smooth assumption to active learning with approximate Tsybakov noise and proved that if the classification boundar y and the underlying distribution are smooth to  X  -th order and  X  &gt; d , the sample complexity of active learning is b
O  X   X  2 d  X  + d ; if the boundary and the distribution are infinitely smooth, the sample complexity of active learning is O polylog ( 1  X  ) . Nevertheless, this result is for approximate Tsybakov noise and the assumption on large smoothness order (or infinite smooth ness order) rarely holds for data with high dimension d in practice. In multi-view setting, the instances are described with sev eral different disjoint sets of features. For the sake of simplicity, we only consider two-view setting in this paper. Suppose that X = X 1  X  X 2 is the instance space, X 1 and X 2 are the two views, Y = { 0 , 1 } is the label space and D is the the optimal Bayes classifiers in the two views, respectively . Let H 1 and H 2 be the hypothesis class in each view and suppose that c 1  X  X  1 and c 2  X  X  2 . For any instance x = ( x 1 , x 2 ) , the hypothesis h v  X  H v ( v = 1 , 2) makes that h v ( x v ) = 1 if x v  X  S v and h v ( x v ) = 0 otherwise, where S v is a subset of X v . In this way, any hypothesis h v  X  X  v corresponds to a subset S v of X v (as for how to combine the hypotheses in the two views, see Section 5). Cons idering that x 1 and x 2 denote the same instance x in different views, we overload S v to denote the instance set { x = ( x 1 , x 2 ) : x v  X  S v } without confusion. Let S  X  v correspond to the optimal Bayes classifier c v . It is well-known [15] that v = { x v :  X  v ( x v )  X  D is R ( h v ) = R ( S v ) = P r ( x pseudo-distance between the sets S v and S  X  v .
 Let  X  v denote the error rate of the optimal Bayes classifier c v which is also called as the noise rate in the non-realizable case. In general,  X  v is less than 1 2 . In order to model the noise, we assume that the data distribution and the Bayes decision boundary in eac h view satisfies the popular Tsybakov and all 0 &lt; t  X  1 / 2 , where  X  =  X  corresponds to the best learning situation and the noise is c alled bounded [8]; while  X  = 0 corresponds to the worst situation. When  X  &lt;  X  , the noise is called unbounded [8]. According to Proposition 1 in [21], it is easy to know tha t (2) holds. following lamma [1] which gives the standard sample complex ity for non-realizable learning task. Lemma 1 Suppose that H is a set of functions from X to Y = { 0 , 1 } with finite VC-dimension V  X  1 and D is the fixed but unknown distribution over X  X  Y . For any  X  ,  X  &gt; 0 , there is a Multi-view active learning first described in [20] focuses o n the contention points (i.e., unlabeled instances on which different views predict different label s) and queries some labels of them. It is motivated by that querying the labels of contention points m ay help at least one of the two views to learn the optimal classifier. Let S 1  X  S 2 = ( S 1  X  S 2 )  X  ( S 2  X  S 1 ) denote the contention points between S 1 and S 2 , then P r ( S 1  X  S 2 ) denotes the probability mass on the contentions points.  X   X   X  and  X   X   X  mean the same operation rule. In this paper, we use  X   X   X  when referring the excess error between S v and S  X  v and use  X   X   X  when referring the difference between the two views S 1 and S 2 . In order to study multi-view active learning, the properties o f contention points should be considered. One basic property is that P r ( S 1  X  S 2 ) should not be too small, otherwise the two views could be exactly the same and two-view setting would degenerate into single-view setting.
 In multi-view learning, the two views represent the same lea rning task and generally are consistent with each other, i.e., for any instance x = ( x 1 , x 2 ) the labels of x in the two views are the same. However, some of these agreed instances may be predicted dif ferent label by the optimal classifier S  X  , i.e., the instances in ( S 1  X  S 2  X  S  X  )  X  ( S 1  X  S 2  X  S  X  ) . Intuitively, if the contention points can convey some information about ( S 1  X  S 2  X  S  X  )  X  ( S 1  X  S 2  X  S  X  ) , then querying the labels of contention points could help to improve S 1 and S 2 . Based on this intuition and that P r ( S 1  X  S 2 ) should not be too small, we give our definition on  X  -expansion in the non-realizable case. Definition 1 D is  X  -expanding if for some  X  &gt; 0 and any S 1  X  X 1 , S 2  X  X 2 , (3) holds. We say that D is  X  -expanding with respect to hypothesis class H 1  X H 2 if the above holds for all S 1  X  X  1  X  X 1 , S 2  X  X  2  X  X 2 (here we denote by H v  X  X v the set { h  X  X v : h  X  X  v } for v = 1 , 2 ). T ) , for realizable learning task under the assumptions that th e learner in each view is never  X  X onfi-dent but wrong X  and the learning algorithm is able to learn fr om positive data only. Here T v denotes the instances which are classified as positive confidently in each view. Generally, in realizable learn-ing tasks, we aim at studying the asymptotic performance and assume that the performance of initial larger than P r ( T 1  X  T 2 ) . In addition, in [3] the instances which are agreed by the two views but are predicted different label by the optimal classifier can be de noted as T 1  X  T 2 . So, it can be found that Definition 1 and the definition of expansion in [3] are based on the same intuition that the amount of contention points is no less than a fraction of the amount of i nstances which are agreed by the two views but are predicted different label by the optimal class ifiers. In this section, we first consider the multi-view learning in Table 1 and analyze whether multi-view setting can help improve the sample complexity of activ e learning in the non-realizable case remarkably. In multi-view setting, the classifiers are ofte n combined to make predictions and many strategies can be used to combine them. In this paper, we cons ider the following two combination schemes, h + and h  X  , for binary classification: 5.1 The Situation Where S  X  1 = S  X  2 With (4), the error rate of the combined classifiers h i + and h i  X  satisfy (5) and (6), respectively. Here S i v  X  X v ( v = 1 , 2 ) corresponds to the classifier h i v  X  H v in the i -th round. In each round of multi-view active learning, labels of some contention poin ts are queried to augment the training data set L and the classifier in each view is then refined. As discussed in [23], we also assume that the learner in Table 1 satisfies the non-degradation condition as the amount of labeled training examples the region of S i 1  X  S i 2 .
 To illustrate the non-degradation condition, we give the fo llowing example: Suppose the data in X probability mass for simplicity. The positive class is the u nion of some clusters while the negative class is the union of the others. Each positive (negative) cl uster  X  v  X  in X v is associated with only x 3  X  v will only be in one of these  X  3  X  v  X  ). Suppose the learning algorithm will predict all instance s in each cluster with the same label, i.e., the hypothesis cla ss H v consists of the hypotheses which do not split any cluster. Thus, the cluster  X  v  X  can be classified according to the posterior probability P ( y = 1 |  X  v  X  ) and querying the labels of instances in cluster  X  v  X  will not influence the estimation of in this task. Note that the non-degradation assumption may n ot always hold, and we will discuss on this in Section 6. Now we give Theorem 1.
 Theorem 1 For data distribution D  X  -expanding with respect to hypothesis class H 1  X  H 2 ac-cording to Definition 1, when the non-degradation condition holds, if s =  X  2 log 1 8  X  log 1 and h s  X  , at least one of which is with error rate no larger than R ( S  X  ) +  X  with probability at least 1  X   X  .
 Here, V = max[ V C ( H 1 ) , V C ( H 2 )] where V C ( H ) denotes the VC-dimension of the hypothesis Q d  X  P r ( S i 1  X  S i 2  X  S  X  ) + P r ( S i 1  X  S i 2  X  S  X  ) +  X  P r ( S i 1  X  S i 2  X  S  X  ) + P r ( S i 1  X  S i 2  X  S  X  ) + As in each round some contention points are queried and added into the training set, the difference  X  C Thus, with (5) and (6) we have either R ( h s + )  X  R ( S  X  ) +  X  or R ( h s  X  )  X  R ( S  X  ) +  X  . From Theorem 1 we know that we only need to request and h s  X  , at least one of which is with error rate no larger than R ( S  X  ) +  X  with probability at least error rate is no larger than R ( S  X  ) +  X  . Fortunately, there are only two classifiers and the probabi lity of getting the right classifier is no less than 1 2 . To study how to choose between h s + and h s  X  , we give Definition 2 at first.
 Definition 2 The multi-view classifiers S 1 and S 2 satisfy  X  -condition if (8) holds for some  X  &gt; 0 . (8) implies the difference between the examples belonging t o positive class and that belonging to negative class in the contention region of S 1  X  S 2 . Based on Definition 2, we give Lemma 2 which provides information for deciding how to choose between h + and h  X  . This helps to get Theorem 2. labels we can decide correctly whether P r { x : x  X  S s 1  X  S s 2  X  y ( x ) = 1 } or P r { x : x  X  S 1  X  S Theorem 2 For data distribution D  X  -expanding with respect to hypothesis class H 1  X H 2 accord-ing to Definition 1, when the non-degradation condition hold s, if the multi-view classifiers satisfy  X  -condition, by requesting e O (log 1  X  ) labels the multi-view active learning in Table 1 will genera te a classifier whose error rate is no larger than R ( S  X  ) +  X  with probability at least 1  X   X  . From Theorem 2 we know that we only need to request e O (log 1  X  ) labels to learn a classifier with error rate no larger than R ( S  X  ) +  X  with probability at least 1  X   X  . Thus, we achieve an exponential improvement in sample complexity of active learning in the n on-realizable case under multi-view setting. Sometimes, the difference between the examples be longing to positive class and that be-longing to negative class in S s 1  X  S s 2 may be very small, i.e., (9) holds.
 If so, we need not to estimate whether R ( h s + ) or R ( h s  X  ) is smaller and Theorem 3 indicates that both h s + and h s  X  are good approximations of the optimal classifier.
 Theorem 3 For data distribution D  X  -expanding with respect to hypothesis class H 1  X  H 2 ac-cording to Definition 1, when the non-degradation condition holds, if (9) is satisfied, by request-ing e O (log 1  X  ) labels the multi-view active learning in Table 1 will genera te two classifiers h s + and h  X  which satisfy either (a) or (b) with probability at least 1  X   X  . (a) R ( h R ( h s  X  )  X  R ( S  X  ) + O (  X  ) ; (b) R ( h s + )  X  R ( S  X  ) + O (  X  ) and R ( h s  X  )  X  R ( S  X  ) +  X  . The complete proof of Theorem 1, and the proofs of Lemma 2, The orem 2 and Theorem 3 are given in the supplementary file. 5.2 The Situation Where S  X  1 6 = S  X  2 Although the two views represent the same learning task and g enerally are consistent with each other, sometimes S  X  1 may be not equal to S  X  2 . Therefore, the  X  -expansion assumption in Definition 1 should be adjusted to the situation where S  X  1 6 = S  X  2 . To analyze this theoretically, we replace S  X  by S  X  1  X  S  X  2 in Definition 1 and get (10). Similarly to Theorem 1, we get The orem 4. Theorem 4 For data distribution D  X  -expanding with respect to hypothesis class H 1  X H 2 accord-ing to (10), when the non-degradation condition holds, if s =  X  2 log 1 8  X  log 1 least one of which is with error rate no larger than R ( S  X  1  X  S  X  2 ) +  X  with probability at least 1  X   X  . ( V , k , C 1 and C 2 are given in Theorem 1.) harder than learning a classifier with error rate no larger th an R ( S  X  v ) +  X  . Now we aim at learning a classifier with error rate no larger than R ( S  X  1  X  S  X  2 ) +  X  . Without loss of generality, we assume no larger than R ( S  X  1  X  S  X  2 ) +  X  . Thus, we can neglect the probability mass on the hypothesis whose the discussion of Section 5.1, with the proof of Theorem 1 we g et Theorem 4 proved.
 with probability at least 1  X   X  . With Lemma 2, we get Theorem 5 from Theorem 4.
 Theorem 5 For data distribution D  X  -expanding with respect to hypothesis class H 1  X  H 2 ac-cording to (10), when the non-degradation condition holds, if the multi-view classifiers satisfy  X  -condition, by requesting e O (log 1  X  ) labels the multi-view active learning in Table 1 will genera te a classifier whose error rate is no larger than R ( S  X  1  X  S  X  2 ) +  X  with probability at least 1  X   X  . , i.e., P r ( S  X  1  X  S  X  2 )  X   X / 2 , we have Corollary 1 which indicates that the exponential improvement in the sample complexity of active learning with Tsybakov no ise is still possible.
 Corollary 1 For data distribution D  X  -expanding with respect to hypothesis class H 1  X H 2 ac-cording to (10), when the non-degradation condition holds, if the multi-view classifiers satisfy  X  -at least 1  X   X  .
 The proofs of Theorem 5 and Corollary 1 are given in the supple mental file. Section 5 considers situations when the non-degradation co ndition holds, there are cases, however, the non-degradation condition (7) does not hold. In this sec tion we focus on the multi-view active learning in Table 2 and give an analysis with the non-degrada tion condition waived. Firstly, we give Theorem 6 for the sample complexity of multi-view active lea rning in Table 2 when S  X  1 = S  X  2 = S  X  . Theorem 6 For data distribution D  X  -expanding with respect to hypothesis class H 1  X H 2 accord-learning in Table 2 will generate two classifiers h s + and h s  X  , at least one of which is with error rate no larger than R ( S  X  ) +  X  with probability at least 1  X   X  . ( V , k , C 1 and C 2 are given in Theorem 1.) Proof sketch. In the ( i + 1) -th round, we randomly query (2 i +1  X  1) m i labels from Q i and add them into L . So the number of training examples for S i +1 v ( v = 1 , 2) is larger than the number of for any  X  v . Setting  X  v  X  { 0 , 1 } , the non-degradation condition (7) stands. Thus, with the p roof of Theorem 1 we get Theorem 6 proved.
 Theorem 6 shows that we can request at least one of which is with error rate no larger than R ( S  X  ) +  X  with probability at least 1  X   X  . To guarantee the non-degradation condition (7), we only need t o query (2 i  X  1) m i more labels in the i -th round. With Lemma 2, we get Theorem 7.
 Theorem 7 For data distribution D  X  -expanding with respect to hypothesis class H 1  X H 2 accord-ing to Definition 1, if the multi-view classifiers satisfy  X  -condition, by requesting e O ( 1  X  ) labels the multi-view active learning in Table 2 will generate a classi fier whose error rate is no larger than R ( S  X  ) +  X  with probability at least 1  X   X  .
 Theorem 7 shows that, without the non-degradation conditio n, we need to request e O ( 1  X  ) labels to learn a classifier with error rate no larger than R ( S  X  )+  X  with probability at least 1  X   X  . The order of 1 / X  is independent of the parameter in Tsybakov noise. Similarl y to Theorem 3, we get Theorem 8 which indicates that both h s + and h s  X  are good approximations of the optimal classifier. Theorem 8 For data distribution D  X  -expanding with respect to hypothesis class H 1  X H 2 accord-ing to Definition 1, if (9) holds, by requesting e O ( 1  X  ) labels the multi-view active learning in Table 2 will generate two classifiers h s + and h s  X  which satisfy either (a) or (b) with probability at least R ( h s  X  )  X  R ( S  X  ) +  X  .
 As for the situation where S  X  1 6 = S  X  2 , similarly to Theorem 5 and Corollary 1, we have Theorem 9 and Corollary 2.
 Theorem 9 For data distribution D  X  -expanding with respect to hypothesis class H 1  X H 2 accord-with probability at least 1  X   X  .
 Corollary 2 For data distribution D  X  -expanding with respect to hypothesis class H 1  X H 2 accord-e O ( 1  X  ) labels the multi-view active learning in Table 2 will genera te a classifier with error rate no larger than R ( S  X  v ) +  X  ( v = 1 , 2 ) with probability at least 1  X   X  .
 The complete proof of Theorem 6, the proofs of Theorem 7 to 9 an d Corollary 2 are given in the supplementary file. We present the first study on active learning in the non-reali zable case under multi-view setting in this paper. We prove that the sample complexity of multi-vie w active learning with unbounded Tsy-bakov noise can be improved to e O (log 1  X  ) , contrasting to single-view setting where only polynomial improvement is proved possible with the same noise conditio n. In general multi-view setting, we prove that the sample complexity of active learning with unb ounded Tsybakov noise is e O ( 1  X  ) , where the order of 1 / X  is independent of the parameter in Tsybakov noise, contrast ing to previous polyno-mial bounds where the order of 1 / X  is related to the parameter in Tsybakov noise. Generally, th e non-realizability of learning task can be caused by many kin ds of noise, e.g., misclassification noise and malicious noise. It would be interesting to extend our wo rk to more general noise model. Acknowledgments This work was supported by the NSFC (60635030, 60721002), 97 3 Program (2010CB327903) and JiangsuSF (BK2008018). [1] M. Anthony and P. L. Bartlett, editors. Neural Network Learning: Theoretical Foundations . [2] M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In ICML , pages [3] M.-F. Balcan, A. Blum, and K. Yang. Co-training and expan sion: Towards bridging theory and [4] M.-F. Balcan, A. Z. Broder, and T. Zhang. Margin based act ive learning. In COLT , pages [5] M.-F. Balcan, S. Hanneke, and J. Wortman. The true sample complexity of active learning. In [6] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In COLT , [7] R. M. Castro and R. D. Nowak. Upper and lower error bounds f or active learning. In Allerton [8] R. M. Castro and R. D. Nowak. Minimax bounds for active lea rning. IEEE Transactions on [9] G. Cavallanti, N. Cesa-Bianchi, and C. Gentile. Linear c lassification and selective sampling [10] D. A. Cohn, L. E. Atlas, and R. E. Ladner. Improving gener alization with active learning. [11] S. Dasgupta. Analysis of a greedy active learning strat egy. In NIPS 17 , pages 337 X 344. 2005. [12] S. Dasgupta. Coarse sample complexity bounds for activ e learning. In NIPS 18 , pages 235 X  [13] S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnost ic active learning algorithm. In [14] S. Dasgupta, A. T. Kalai, and C. Monteleoni. Analysis of perceptron-based active learning. In [15] L. Devroye, L. Gy  X  orfi, and G. Lugosi, editors. A Probabilistic Theory of Pattern Recognition . [16] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selecti ve sampling using the query by [17] S. Hanneke. A bound on the label complexity of agnostic a ctive learning. In ICML , pages [18] S. Hanneke. Adaptive rates of convergence in active lea rning. In COLT , 2009. [19] M. K  X  a  X  ari  X  ainen. Active learning in the non-realizable case. In ACL , pages 63 X 77, 2006. [20] I. Muslea, S. Minton, and C. A. Knoblock. Active + semi-s upervised learning = robust multi-[21] A. Tsybakov. Optimal aggregation of classifiers in stat istical learning. The Annals of Statistics , [22] L. Wang. Sufficient conditions for agnostic active lear nable. In NIPS 22 , pages 1999 X 2007. [23] W. Wang and Z.-H. Zhou. On multi-view active learning an d the combination with semi-
