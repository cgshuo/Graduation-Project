 Current search systems are designed to find relevant articles , e s-pecially topically relevant ones, but the notion of relevance large-ly depends on search tasks . We study the specific task that scie n-tists are searching for worth-reading articles beneficial for their research. Our stu dy finds: users  X  perception of relevance and pref-erence of reading are only moderately correlated; current systems can effectively find readings that are highly relevant to the topic, but 36% of the worth-reading articles are only marginally relevant or even non-relevant. Our system can effectively find those worth-reading but marginally relevant or non-relevant articles by taking advantages of scientists  X  recommendations in social websites . H.2.8 [ Database Applications ]: scientific databases . Algorithms, Measurement, Performance, Experimentation. Scientific articles ; scientific readings; social search . to their main research f ields but also those from other fields. This makes automatically finding useful readings more difficult than simply matching articles with queries because : 1) many useful readings could come from topics that the scientists do not have adequate knowledge to formulate effective queries; 2) not all topi-cal ly relevant articles are worth reading; and 3) some useful rea d-ings may not contain query terms.
 lem should have the following key features : 1) the scientist s are not required to formulate queries related to the topics of the rea d-ings , but rather queries about the problems or research topics they are working on, which should be a relative ly easier task ; 2) the system can find not only readings that ar e topically relevant, but also those beyond the topics that the scientists are working on. Here we propose a method of find ing readings by looking at what scientists  X  peers are reading about in social websites, which ge n-erates a list of candidate reading s not limited to topically relevant articles. Specifically, we study the following research questions: vance and preference of reading? cles beyond the queries  X  topics? CiteULike, a social reference management website . In CiteULike, each user can maintain a collection of articles as the user  X  X  pe r-sonal library . Here we assume CiteULike users are scientists and their personal libraries are their collection s of useful readings . Previous studies found co-occurrences of articles in users  X  librar-ies can be used for clustering articles into research fields [2]. Our dataset includes titles and abstracts for 913,846 unique articles posted to CiteULike by users in 2010 (99.2% of all articles posted by users in 2010) and 54,402 users  X  personal libraries . ings by P ( R | q ), which is equivalent to P ( R , q ) in ranking . Further, we model P ( R , q ) as: what is the probability that a scientist wor k-ing on the problems described by q will read a reading R ? P ( R , q ) is calculated by two steps. First, we find a list of peer scientists by P ( q | u ) is calculated as (2) using expert finding  X  model 2  X  in [3]. In (2), L u is the list of articles in u  X  X  personal library , which is used to model u  X  X  expertise. Second, we let each peer scientist u vote for reading R by the probability P ( R | u , q ). We assume a peer scientist u will vote for R as a worth -reading article for q if R is in both L and the search results of query q (referred to as S q whole method is referred to as RUL, which is summarized in (1) . We can further set a cutoff value n in (3) so that only the top n retrieved results for query q are considered for voting (referred to as RUL n ). We use t wo a d hoc search models for comparison: s e-quential dependence model (SDM) and relevance model 3 (RM3 [4]). We also merge rankings of RUL n and SDM/RM3 by (4).
 ty member , 1 postdoc and 8 PhD students ). Every participant was asked to generate 3 queries, each of which described a research problem related to his/her research. Seven runs were pooled (depth=10) for each query : SDM, RM3 , RUL , and RUL 20, 50, 100, and 200. On average 31 articles were pooled for each query. The participant s need to answer the following questions: not relevant, 2-somewhat relevant, 3-relevant) yes, I want to read it, 3-no, I don't want to read it) don't like it) participant to select at most 3 articles he/she want ed to read first if he/she is only given limited time. We assign relevance score s ( r el ) to articles based on answer s to Q1 ( rel = 2 for  X  X elevant X ; rel = 1 for  X  somewhat relevant  X ; and rel = 0 for  X  X ot relevant X ) . For rea d-ing preference score ( read ), we consider two cases: if the partic i-pant has read the article ( Q2=1 ), we assign read score based on for  X  I don X  X  like it X ); if the participant has not read the article, we assign read = 2 to the three articles the participant selected to read read (Q2=2). Table 1 shows statistics of participant s X  judgments. For the 477 articles judged as worth-reading ( read  X  1), 173 (36%) are only marginally relevant or not relevant to the queries X  topics ( rel  X  1) . After removing 370 articles that are neither relevant to the topic nor worth-reading ( rel = 0; read = 0) , we find a moderate correlation ( r = 0.357) between articles  X  rel and read scores. This indicates the fact that good reading s are not necessarily relevant to the queries X  topics and vice versa . r ead = 2 166 40 6 212 r ead = 1 138 111 16 265 r ead = 0 13 76 370 459 ( rel = 2; read  X  1) for evaluation ( H RelRead ). As shown in Table 1, over 90% of the highly relevant articles ( rel = 2) are also worth -reading ( read  X  1). Current search systems (SDM and RM3) may perform well in finding HRelRead articles because the HRelRead readings are mostly also highly relevant articles. W e also evaluate by article relevance ( QRel ) for comparison. and HRelRead for evaluation . Ad hoc search models (SDM and RM3) performed very effective ly (nDCG@10  X  0.6 and 0.5). In comparison, RUL n performed worse than ad hoc search models , but is still very effective (when n = 20, nDCG@10  X  0.45). This is not surprising considering read and rel scores are highly correla t-ed in HRelRead readings. RUL n does not explicitly model topical relevance, but SDM and RM3, as two of the best ad hoc search models, model topical relevance very well. Thus, in general , ad hoc search models can effectively find the HRelRead readings , but their success may come from better modeling of topical rel e-vanc e rather than better modeling the task of finding readings. relevant or not relevant for evaluation ( referred to as MRelRead ; rel &lt;= 1, read &gt;= 1). Since s uch readings are not necessarily rele-vant, ad hoc search models may not perform effectively. The right part of Figure 1 shows evaluation results for MRelRead readings. It indicate s that ad hoc search algorithms (SDM and RM3) cannot perform as effective ly as they did in finding HRelRead readings (nDCG@10 &lt; 0.3 5). In comparison, when n = 20, RUL form significantly better than SDM (nDCG@10+13.36%; p &lt;0.05 ). Combining RUL n with SDM/ RM3 further improve d performance. the problems of finding MRelRead readings because in such cases article relevance and user  X  X  preference of reading diverge a lot. It seems unlikely to solve the problem by query expansion, as RM3 perform ed not much better than SDM . The reasons can be: 1) if the original query is not effective, it is unclear whether or not pseudo-relevance feedback can produce high quality expanded terms ; and 2) expanded terms may enrich representations of the query X  X  topic, but do not necessarily help match ing of cross-disciplinary articles . SDM, RM3 , RUL 20 , and RUL 20 +SDM/RM3 . It shows that RM3 improves and hurts nearly the same number of topics compared with SDM, but RUL 20 (+SDM/RM3) can not only improve topics hurt by RM3, but can also get as much as improvements i n topics improved by RM3. We find the reasons are related to the voting mechanisms in RUL n . O n the one hand, voting can reduce wrong expansions of topics. For example , for topic 28  X  science mapping intellectual structure  X , SDM performs effective but RM3 wrongly emphasizes on  X  X apping X  and matches articles such as  X  Genetic mapping in human disease  X . However, since peers found by RUL sults of RUL n . On the other hand, voting is also a useful way of finding good readings beyond topic al level . Still, for topic 28, the user wants to read the article titled  X  Scholarly research and infor-mation pract ices: a domain analytic approach  X , which is not ranked in top position by SDM or RM3 for the lack of query terms in title and abstract. Instead, RUL n vote s a high score for this article because it has been read by many scientists . We also find, however, RUL n works poorly if the topic is associated with peers with diversified interests , e.g. topic 7  X  collaborative infor-mation seeking X . search models can effectively find readings that are also topically relevant , because in such cases article relevance and preference of reading are highly correlated . In comparison, our system can more effectively find readings beyond the query  X  X  topic by taking a d-vantages of peer scientists  X  recommendations in s ocial websites . dation grant IIS -1052773 and III-COR 0704628. [1] http://www.sis.pitt.edu/~jjiang/data/ rul/ [2] J. Jiang, D. He, C. Ni. 2011. Social reference: aggregating [3] K. Balog, L . Azzopardi, M . de Rijke. 2006. Formal models [4] Y. Lv, C. Zhai. 2009. A comparative study of methods for 
