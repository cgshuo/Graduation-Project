 Eyal Even-Dar evend@cs.tau.ac.il Shie Mannor shie@mit.edu Yishay Mansour mansour@cs.tau.ac.il Reinforcement Learning (RL) has emerged in the re-cent decade as unified discipline for adaptive con-trol of dynamic environments (e.g., Barto &amp; Sut-ton, 1998). A common problem with many RL al-gorithms is a slow convergence rate, even for relatively small problems. For example, consider the popular Q-learning algorithm (Watkins, 1989) which is essentially an asynchronous stochastic approximation algorithm (Bertsekas &amp; Tsitsiklis, 1996). Generic convergence rate bounds for stochastic approximation (e.g., Borkar &amp; Meyn, 2000) or specific rates for Q-learning (see Kearns &amp; Singh, 1998; Even-Dar &amp; Mansour, 2001) are somewhat disappointing.
 The problem of finding optimal policies in Markov De-cision Processes (MDPs) was the subject of intensive research since the 1950 X  X . When the model is known, and learning is not required there are several standard methods for calculating the optimal policy -Linear Programming, Value Iteration, Policy Iteration etc., see Puterman (1994) for a review. Starting from Mac-Queen (1966) several algorithms that eliminate actions were proposed. When the MDP model is known Ac-tion Elimination (AE) serves two purposes: reduce the size of the action sets to be searched at every iteration; identify optimal policies when there is a unique opti-mal policy. (In Value Iteration this is the only way to reach optimal policy rather than  X  -optimal policy.) AE procedures became standard practice in solving large practical MDPs and are considered state-of-the-art. (See Puterman (1994) for more details.) In this paper we consider the learning aspect of AE when the model is not known.
 In many applications the computational power is avail-able but sampling of the environment is expensive. By eliminating sub-optimal actions early in the learning process, the total amount of sampling is reduced, lead-ing to spending less time on estimating the parameters of sub-optimal actions. The main motivation for ap-plying AE in RL is reducing the amount of samples needed from the environment. In addition to that, AE in RL enjoys the same advantages as in MDPs -conver-gence rate speedup and possibility to find an optimal policy (rather than  X  -optimal).
 We suggest a framework for AE in RL. The underly-ing idea is to maintain upper and lower estimates of the value (or Q) function. When the expected upper estimate of the return of a certain action falls below the expected lower estimate of another action, the ob-viously inferior action is eliminated. We suggest both, a model-based and a Q-learning style AE algorithms. The upper and lower bounds are based on a large devi-ations inequality, so that when an action is eliminated, it is eliminated with high probability.
 Stopping times that are based on generic convergence rate bounds (as in Even-Dar &amp; Mansour, 2001) are overly conservative. We suggest a stopping time based on the difference between the upper and lower bounds of the value (or Q) function. We show that if the difference is small, then the greedy policy with respect to the lower estimate is almost optimal. We define a Markov Decision process (MDP) as follows Definition 2.1 A Markov Decision process (MDP) M is a 4-tuple ( S, A, P, R ) , where S is a set of the ability from state i to state j when performing action a  X  A in state i , and R ( s, a ) is the reward received when performing action a in state s .
 A strategy for an MDP assigns, at each time t , for each state s a probability for performing action a  X  A , given includes the states, actions and rewards observed until time t  X  1. While executing a strategy  X  we perform at time t action a t in state s t and observe a reward r t (distributed according to R ( s, a )), and the next state s the sequence of rewards into a single value called the return . Our goal is to maximize the return. In this work we focus on the discounted return , which has a parameter  X   X  (0 , 1), and the discounted return of policy  X  is V  X  = observed at time t . We also consider the finite horizon return, V  X  = We assume that R ( s, a ) is non-negative and bounded by R max , i.e., for every s, a : 0  X  R ( s, a )  X  R max and for simplicity we assume that R ( s, a ) is determin-istic and note that all the results apply for stochastic rewards as well (under minor changes in the proof). This implies that the discounted return is bounded by V max = R max 1  X   X  ; for the finite horizon the return is bounded by HR max . We define a value func-tion for each state s , under policy  X  , as V  X  ( s ) = E policy  X  starting at state s , and a state-action value function Q  X  ( s, a ) = R ( s, a ) +  X  larly, we define the value functions for the finite hori-zon model.
 Let  X   X  be an optimal policy which maximizes the re-turn from any start state. This implies that for any  X   X  ( s ) = argmax a ( R ( s, a ) +  X  ( use V  X  and Q  X  for V  X   X  and Q  X   X  , respectively. We say that a policy  X  is  X  -optimal if k V  X   X  V  X  k  X   X   X  . We also define the policy Greedy ( Q ) as the policy that prescribes in each state the action that maximizes the For a given trajectory let: T s,a be the set of times in which we perform action a in state s and T s,a,s 0 be a subset of T s,a in which we reached state s 0 . Also, #( s, a, t ) is the number of times action a is performed Next we define the empirical model. Given that | T s,a | &gt; 0 we define the next state distribution as  X  pirical model and the reward can be chosen arbitrar-ily. We define the expectation of the empirical model notations we omit s, a in the notations  X  E s 0 whenever evident.
 We often use large deviation bounds in this paper. Since we assume boundedness we can rely on Hoeffd-ing X  X  inequality (We note that the boundedness as-sumption is not essential and can be relaxed.) Lemma 2.1 (Hoeffding, 1963) Let X be a set, D be valued functions defined on X with f i : X  X  [ a i , b i ] for i = 1 , ..., m , where a i and b i are real numbers sat-D . Then we have the following inequality P "  X   X  In this section we focus on model-based learning. In the model-based methods, we first learn the model, i.e., estimate the immediate reward and the next state distribution. Then by either value iteration or pol-icy iteration on the learned (empirical) model, we find the exact optimal policy for the empirical model. If enough exploration is done, this policy is an almost optimal policy for the real model. We note that there is an inherent difference between the finite horizon and the infinite discounted return. Technically, the finite horizon return is simpler than the discounted return, as one can apply the large deviation bounds directly. We provide model-based algorithms for both cases. 3.1. Finite Horizon Let us first recall the classical Value Iteration equa-tions for finite horizon: V
H ( s ) = max
V 0 ( s ) = max where V H ( s ) is the optimal value function for horizon H . Given the empirical model by time t we define the upper estimate V  X  , which will be shown to satisfy for high probability. For horizon H we define:
V H  X  ( s ) = max
V 0  X  ( s ) = max for some constant c &gt; 2. Similarly to the upper bound V  X  , a lower bound may be defined where the plus sign before the last element of Eq. (1) is replaced by a minus sign. We call this estimate the lower estimate V  X  . The following Lemma proves that V indeed an upper (lower) estimation for any horizon. (The proof appears in Appendix A.) Lemma 3.1 Every state s and for every finite horizon ity at least 1  X   X  .
 Consequently, a natural early stopping condition is to stop sampling when k V H  X  V H k  X  &lt;  X  . We do not pro-vide here an algorithm, however a detailed algorithm will be given in the following subsection. 3.2. Discounted Return -Infinite Horizon In this subsection, we provide an upper estimate of the value function V . The optimal value is the solution of the set of the equations: As in Subsection 3.1, we provide an upper value func-tion V  X  , which satisfies with high probability V  X  ( s )  X  V  X  ( s ). We define V  X  as the solution of the set of equa-tions: V  X  ( s ) = max a o and Q  X  as: Q  X  ( s, a ) = R ( s, a )+  X  Similarly, we define V  X  and Q V  X  ( s ) = max a Q The next lemma shows that with high probability the upper and lower estimations are indeed correct. (The proof is deferred to Appendix B.) Lemma 3.2 For every state s and action a with prob-Q The AE procedure is demonstrated in the following al-gorithm, which supplies a stopping condition for sam-pling the model and eliminates actions when they are clearly sub-optimal.
 Input : MDP M ,  X  &gt; 0,  X  &gt; 0 Output : A policy for M Choose arbitrarily an initial state s 0 , let t = 0, and let U 0 = { ( s, a ) | s  X  S, a  X  A } repeat until  X  ( s, a )  X  U | Q  X  ( s, a )  X  Q return Greedy ( Q Algorithm 1: Model-Based AE Algorithm A direct corollary from Lemma 3.2, is a stopping time condition to the Model-Based algorithm using the fol-lowing Corollary.
 Corollary 3.3 (Singh &amp; Yee, 1994) If  X  Q is a function a  X  A . Then for all s where  X   X  = Greedy (  X  Q ) .
 Corollary 3.4 Supposed the Model-Based AE Algo-rithm terminates. Then the policy,  X  , it returns is  X  -optimal with probability at least 1  X   X  . Proof: By Lemma 3.2 we know that with probability at least 1  X   X  for every s and a we have that Q least 1  X   X  the optimal action has not been eliminated in any state. Furthermore, any action b in state s that has not been eliminated satisfies Q  X  ( s, b )  X  Q Q  X  ( s, b )  X  Q  X  ( s, b )  X  Corollary 3.3. In this section we describe a model-free algorithm. We use two functions Q upper estimations on Q  X  , respectively. We use these functions to derive an asynchronous algorithm, which eliminates actions and supplies stopping condition. Let us first recall the Q-learning algorithm (Watkins, 1989). This algorithm requires space which is pro-portional to the space used by Q-learning and con-verges under the same conditions. The Q-learning al-gorithm estimates the state-action value function (for discounted return) as follows: where s 0 is the state reached from state s when per-We define the upper estimation process as:
Q t +1  X  ( s, a ) = (1  X   X  t ( s, a )) Q t  X  ( s, a ) +  X  where c &gt; 4 and s 0 is the state reached from state s when performing action a at time t , V t  X  ( s ) = max a Q t  X  ( s, a ) and Analogously, we define the lower estimate Q Input : MDP M ,  X  &gt; 0,  X  &gt; 0 Output : A policy for M For every state action ( s, a ): #(s,a) = 1 Choose an arbitrary initial state s repeat until  X  s  X  S  X  a  X  U ( s ) | Q ( s, a )  X  Q ( s, a ) | &lt; return Greedy ( Q ) where V  X  ( s ) = max a Q processes converge almost surely to Q  X  . (The proof appears in Appendix C.) Proposition 4.1 If every state-action pair is per-formed infinitely often then the upper (lower) estima-tion process, Q t  X  ( Q t one.
 The following Proposition claims that Q t  X  upper bounds Q  X  and Q t bility. (The proof appears in Appendix D.) Proposition 4.2 For every state action pair s, a and time t with probability at least 1  X   X  we have that Q  X  ( s )  X  Q  X  ( s )  X  Q We combine the upper and lower estimates to an algo-rithm, which eliminates sub optimal actions whenever possible. Furthermore, the algorithm supplies a stop-ping condition that assures a near optimal policy. The model free AE algorithm is described in Algorithm 2. A direct corollary from Proposition 4.2 is a stopping condition to the model free AE algorithm. The follow-ing corollary follows from Corollary 3.3 and its proof is similar to the proof of Corollary 3.4.
 Corollary 4.3 Suppose the Model-Free AE Algorithm terminates. Then the policy,  X  , it returns is  X  -optimal with probability at least 1  X   X  . In this section we show four types of MDPs in which the number of samples used by AE procedures is sig-nificantly smaller than the number of samples used by standard Q-learning and  X  -greedy Q-learning. Both model free AE algorithm and standard Q-learning choose the action in each state uniformly at ran-dom. In our experiments we focused on the steady state norm ( L 1 weighted by steady state probabilities) rather than the L  X  norm. We note that we use the steady state rather than the discounted steady state. We run AE Q-learning algorithm from Section 4 with the same input (for actions that were not eliminated) as a standard Q-learning algorithm. The following ex-periments were conducted: 1. A queueing system. The MDP represents a 2. Random MDPs. The random MDPs are ran-3. Howard X  X  automobile replacement prob-Extending the concept of action elimination to large state spaces is probably the most important direction. The extension to function approximation, which ap-proximate the value function, requires some assump-tions on the value (or Q) function approximation ar-chitecture. Following Kakade and Langford (2002) we can consider value functions that can be approximated under the infinity norm. For an example of such an al-gorithm see Ormoneit and Sen (2002). If convergence rate of the function approximation is provided, as in Ormoneit and Sen (2002), then an AE procedure can be derived as before.
 This research was supported in part by a grant from the Israel Science Foundation. S.M. was partially sup-ported by the MIT-Merrill Lynch Partnership. E.E was partially supported by the Deutsch Institute. Aiello, W. A., Mansour, Y., Rajagopolan, S., &amp; Rosen,
A. (2000). Competitive queue policies for diffrenti-ated services. In INFOCOM .
 Barto, A., &amp; Sutton, R. (1998). Reinforcement learn-ing . MIT Press.
 Bertsekas, D. P., &amp; Tsitsiklis, J. N. (1996). Neuro-dynamic programming . Belmont, MA: Athena Sci-entific.
 Borkar, V., &amp; Meyn, S. (2000). The O.D.E. method for convergence of stochastic approximation and re-inforcement learning. SIAM J. Control Optim. , 38 , 447 X 469.
 Even-Dar, E., &amp; Mansour, Y. (2001). Learning rates for Q-learning. Fourteenth Annual Conference on Computation Learning Theory (pp. 589 X 604).
 Hoeffding, W. (1963). Probability inequalities for sums of bounded random variables. Journal of the Amer-ican Statistical Association , 58 , 13 X 30.
 Howard, R. (1960). Dynamic programming and Markov decision processes . MIT press.
 Kakade, S., &amp; Langford, J. (2002). Approximately optimal approximate reinforcement learning. Pro-ceedings of the Nineteenth International Conference on Machine Learning (pp. 267 X 274). Morgan Kauf-mann.
 Kearns, M., &amp; Singh, S. P. (1998). Finite-sample convergence rates for Q-learning and indirect algo-rithms. Neural Information Processing Systems 10 (pp. 996 X 1002).
 Kesselman, A., Lotker, Z., Mansour, Y., Patt-Shamir,
B., Schieber, B., &amp; Sviridenko, M. (2001). Buffer overflow management in qos switches.  X  X CM Sym-posium on Theory of Computing X  (pp. 520 X 529). MacQueen, J. (1966). A modified dynamic program-ming method for Markov decision problems. J. Math. Anal. Appl. , 14 , 38 X 43.
 Ormoneit, D., &amp; Sen, S. (2002). Kernel-based rein-forcement learning. Machine Learning , 49 , 161 X 178. Puterman, M. (1994). Markov decision processes . Wiley-Interscience.
 Singh, S. P., &amp; Yee, R. C. (1994). An upper bound on the loss from approximate optimal-value functions. Machine Learning , 16 , 227 X 233.
 Watkins, C. (1989). Learning from delayed rewards . Doctoral dissertation, Cambridge University. We prove the claim by induction. For the base of the induction we have that for every state s V 0  X  ( s ) = V 0 ( s ) = R ( s, a ). Next we assume that the claim holds for i  X  k and prove for k + 1 and for every action a . By definition V k +1  X  ( s ) satisfies for every a that where the second inequality follows from the inductive assumption. Note that V k is not a random variable, so we can bound the last expression using Hoeffding X  X  inequality. We arrive at: P  X   X   X  Therefore, we have that with high probability the fol-lowing holds Using the union bound over all state-action pairs and all finite horizons k , we obtain that the failure proba-bility is bounded by  X / 2 for large enough c . Repeating the same argument for the lower estimate and applying the union bound completes the proof.
 Suppose we run a value iteration algorithm on the em-pirical model. Let V k  X  be the k th iteration of the value function algorithm, and let Q k  X  be the associated Q-function, that is Q k  X  ( s, a ) = R ( s, a ) +  X   X  E s 0 V not used in the algorithm.) We need to prove that Q  X  ( s, a )  X  Q  X  ( s, a ) for every s and a . Note that since the value iteration converges, Q k  X  converges to Q  X  . We prove by induction on the number of the iterations that if we take V 0  X  = V  X  then with high inequality and obtain that for every state action pair s, a P n  X   X  with probability 1  X   X / 2. For the induction step, we assume that the claim holds for i &lt; k and prove for k . Q  X  ( s, a )  X  Q duction that for every s , V  X  ( s ) = max a Q So that Q k  X   X  Q k  X  1  X   X  0. We conclude that P [ Q  X   X  Q  X  ]  X  1  X   X / 2. Repeating the same argument for the lower estimate, Q completes the proof.
 In order to show the almost sure convergence of the upper and lower estimations, we follow the proof of Bertsekas and Tsitsiklis (1996). We consider a gen-eral type of iterative stochastic algorithms , which is performed as follows: X where w t is a bounded random variable with zero ex-pectation and each H is a pseudo contraction mapping (See Bertsekas and Tsitsiklis (1996) for details). Definition C.1 An iterative stochastic algorithm is well behaved if: 1. The step size  X  t ( i ) satisfies (1) 2. There exists a constant A that bounds w t ( i ) for 3. There exists  X   X  [0 , 1) and a vector X  X  such that 4. There exists a nonnegative random sequence  X  t , We first note that the Q-learning algorithm satisfies the first three criteria and the fourth criteria holds trivially since u t = 0, thus its convergence follows (see Proposition 5.6 in Bertsekas &amp; Tsitsiklis, 1996). The upper estimate has an additional noise term, u t . If we show that it satisfies the fourth requirement, then the convergence will follow.
 Lemma C.1 The upper estimation algorithm is well behaved.
 Proof: In the convergence proof of Q-learning, it was shown that requirements 1 X 3 are satisfied, this implies that the upper estimates satisfies them as well. Now we let u t =  X  t = c verges to zero, thus Similar result holds for the lower estimate as well. We use induction on the time t to show that for every state-action pair the following holds
P For the induction basis we have that for every state-claim holds for k &lt; t and prove for t . Suppose that action a is performed at state s at time t , thus for every other state-action pair the claim holds. Let t i be the time were action a was performed at state s for the i th time. Let s i be the next state at time t i + 1. For s, a we have that, Q  X  ( s, a ) = where the last expression can be bounded with high probability using Hoeffding X  X  inequality which completes the induction by using the union bound. Repeating the same argument for the lower
