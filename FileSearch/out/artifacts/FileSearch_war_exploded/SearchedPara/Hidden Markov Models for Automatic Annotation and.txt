 This paper introduces a novel method for automatic anno-tation of images with keywords from a generic vocabulary of concepts or objects for the purpose of content-based im-age retrieval. An image, represented as sequence of feature-vectors characterizing low-level visual features such as color, texture or oriented-edges, is modeled as having been stochas-tically generated by a hidden Markov model, whose states represent concepts. The parameters of the model are es-timated from a set of manually annotated (training) im-ages. Each image in a large test collection is then automat-ically annotated with the a posteriori probability of con-cepts present in it. This annotation supports content-based search of the image-collection via keywords. Various as-pects of model parameterization, parameter estimation, and image annotation are discussed. Empirical retrieval results are presented on two image-collections  X  COREL and key-frames from TRECVID. Comparisons are made with two other recently developed techniques on the same datasets. H.3.3 [ Information Search and Retrieval ]: Retrieval Models; I.5.1 [ Models ]: Statistical; I.4.8 [ Scene Analy-sis ]: Object Recognition Design, Experimentation, Performance Hidden Markov models, Image &amp; video retrieval
The content of communications in the digital age is in-creasingly multi-modal in nature, with text, images and even speech or video being used in a single  X  X ocument. X  This is true of web pages on the Internet, reports at a work-place, and even interpersonal interactions, as witnessed by the file-contents of most personal computers. Content-based indexing and retrieval of multimedia is therefore becoming an increasingly important issue. Unlike search and retrieval of text documents, in which the modality in which the user naturally, or at least usually, specifies her information need is the same as the modality of the search collection, there is relatively little work in image retrieval based on textual queries. The major reason, of course, is that open domain image understanding remains a largely unsolved problem, and even detecting a single predetermined object in an im-age, such as a human face, is a fairly difficult problem (cf e.g. [13]). Yet, important progress has been made in the last few years in content-based image retrieval, as reported in [4, 2, 3, 7] and elsewhere.

We are not the first to observe that while the  X  X ure X  im-age understanding problem, i.e. the problem of recognizing all the objects in a given image, is very difficult due to sev-eral invariance issues, there are two aspects of the image search and retrieval problem which make it relatively much more tractable. One is the availability of side information in the accompanying text: images in multimedia documents are often accompanied by captions or other descriptive text which a search engine may use to guess the content of an image. This is indeed what is used, e.g. by Google Images and similar tools. The other is that the search problem is far more tolerant of erroneous inferences than is usually as-sumed necessary for object recognition in individual images: the system need not recognize with high certainty whether any particular image in a collection contains, say, a tiger, since it suffices if the inferred likelihood of tiger in images containing tigers is higher than its likelihood in images not containing tigers, no matter how small either likelihood is. These two observations lead to the research paradigm sub-scribed to in this paper. 1. We develop a generative stochastic model for images 2. We estimates parameters of our model from a manu-3. We measure the efficacy of our image annotation model We take the view that as long as the retrieved images are in the right rank-order, human users are very efficient at scanning thumbnails for the images they need.

The remainder of this paper is organized as follows. Sec-tion 2 formally describes the hidden Markov model (HMM) used for image annotation, and compares it with two other recently developed techniques for the same task. Section 3 delves into various issues in the design and parameter esti-mation for the model. Section 4 presents a series of experi-mental results on two image-collections, namely the COREL and the TRECVID datasets. We conclude with some dis-cussion in Section 5.
Let a collection of image+caption pairs be provided and consider the problem of developing a stochastic generative model that jointly describes each pair. Let I =  X  i 1 , . . . , i denote segments (regions) in an image, and C = { c 1 , . . . , c denote the objects (concepts) present in that image, as spec-ified by the corresponding label (caption) C . For each image ture, edges, shape and other salient visual features of the region 1 . Finally, let V denote the global vocabulary of the caption-words c n across the entire collection of images.
We propose to model the { x t } -vectors of an image I as a hidden Markov process [11], generated by an underlying unobserved Markov chain whose states s t take values in C . Specifically, each x t is generated according to some probabil-ity density function f (  X | s t ) given the state s t , where s is a Markov chain with a known initial state s 0 and transi-tion probabilities p ( s t | s t  X  1 ). Figure 1 illustrates an image with T = 6 rectangular regions with the caption { tiger , grass , water , ground } , and the state-transition diagram of the underlying Markov chain which generates these regions. Formally, for a state-sequence  X  s 1 , . . . , s T  X  = s T Note that observing the state sequence s t is equivalent to knowing the alignment of each image region i t with one of the words in the caption. Since this level of detail is usually not provided in a caption, a hidden Markov model (HMM) is an appropriate formalism for computing the joint likelihood: where x T 1 denotes the T -length sequence  X  x 1 , .., x T
The T segments may be object based, with each region cor-responding to one semantically distinct object, or they may be a simple rectangular partition of the image into fixed-size blocks. We have chosen to use fixed-size segments in our ex-periments based on a 4  X  6 or 5  X  7 partition of the image. Figure 1: Illustration of the State Transitions Graph and Output for an Image+Caption HMM.

The emission densities f and transition probabilities p of the HMM are estimated, given a training collection of image+caption pairs, to maximize the joint likelihood (1). In particular, we model the output density f (  X | c ) for each state c  X  V as a mixture of multivariate Gaussian densities on IR d : f ( x | c ) = where w m,c  X  X  are the mixture weights,  X  m,c the mean-vector and  X  m,c the diagonal covariance-matrix for the of the m -th mixture component of the state c . The transition probabil-ities are non-parametric, with the initial values being the uniform pmf for all permissible states in C (or in V as de-scribed below). Details of this maximum likelihood estima-tion procedure are standard (cf [11]) and therefore omitted.
Given an image+caption pair ( I, C ), the most likely align-ment of image regions with caption-words is This is sometimes called the Viterbi alignment. Finally, the marginal likelihood of an image may be computed as f ( x T 1 | s 0 ) = f ( x T 1 , V| s 0 ) = X When an image with no caption is provided, this marginal likelihood enables one to use the HMM to compute the con-ditional probability that a particular image region i t was generated by a particular concept c  X  V based on the total visual evidence x T 1 in the image I : p ( s t = c | x T 1 , s 0 ) = f ( x Finally, the probability of a particular concept c  X  V being present (somewhere) in an image may be calculated as Thus jointly modeling the image-features and caption-text as the realization of such a generative stochastic process, i.e. an HMM, provides for 1. estimating the parameters of the model from anno-2. aligning image-regions with caption-words in an im-3. computing the likelihood of a caption-word being present HMMs have several strengths, and a few limitations, as mod-els of stochastic processes, which our methodology inherits. A freely available toolkit, HTK [12], efficiently implements all the basic parameter estimation and probability calcula-tion algorithms needed for working with the model described above, and this is what we have used in the experiments de-scribed in Section 4 below.

Note that the model proposed above differs fundamentally from the use of HMMs by [9] for image annotation. In their work, different HMMs, one for each category (concept), are first estimated from all images labeled with that category. The states of the HMMs have no correspondence with any semantic concepts in an image. Only the likelihood (4) of each image in a test collection is computed under the HMM of every category. The actual assignment of index terms to a test image is done post hoc, based on (a few) categories under whose HMMs the test image is highly likely.
In some recent work, Duygulu et al [4] have presented the image annotation problem as a variant on the statisti-cal machine translation problem. In their formulation, the representations x t of the segmented image regions are dis-cretized and then treated as  X  X ords, X  replacing IR d with a vocabulary X of visual expressions (visterms). The training corpus of image+caption pairs is then treated as a corpus of aligned bi-lingual text, and statistical machine translation techniques are employed to infer a stochastic translation lex-icon p ( x | c ) between c  X  V and x  X  X . Given a test image I with visterms x T 1 , a probability is calculated for each con-cept c in the caption-vocabulary (in the simplest case) as Note that the estimation of p ( x | c ) in their case, particularly when using IBM Model-1, is identical in form to the em-bedded estimation of f ( x | c ) in the HMM case provided we set the transition probabilities to be uniform. In both cases, the alignment of individual image-regions to caption-words is treated as a hidden variable and the EM algorithm is employed. Thus the primary difference between our model and theirs is that the image features are modeled here as continuous-valued vectors, avoiding the need for quantiza-tion. A lesser difference is that unlike object-based seg-mentation in their case, we have used regular rectangles to segment the image.

In more recent work [6], the model of [4] has been investi-gated with rectangular regions, It will be shown in Section 4 that preserving the continuous-valued representation of the x  X  X  results in significant improvement in annotation and retrieval performance.
Manmatha et al [7, 8] have used a continuous version of the relevance model (CRM) to perform image annotation and retrieval. In their case, continuous-valued visual fea-tures { x t } are extracted from rectangular regions of the (test) image I and compared with the visual features ex-tracted from each training image J using a Gaussian kernel function. In particular, for a training image J with visual features { r 1 , . . . , r T } and caption { y 1 , . . . , y The parameter  X  is estimated from held-out data. This ker-nel then yields the probability of each concept being present in I as p ( c | I )  X  X where  X  P is computed from the caption of J . Note that the sum need only run over the training images which assign a nonzero  X  P -probability to the concept c . But for this respite, each test image needs to be compared, in principle, with every training image during annotation.

While, upon first glance, the continuous relevance model seems to differ fundamentally from the HMM paradigm pro-posed here, deeper analysis reveals remarkable similarities.
Each concept c  X  V is modeled in the HMM by a mixture of Gaussians. The maximization of the joint likelihood (1) results in the emission probabilities f ( x | c ) in the HMM be-ing estimated by a fractional assignment of each r t  X  J to one of the states y n ; i.e. image regions r t from all images labeled with c contribute to the estimate of the probability density function (pdf) f ( x | c ). Therefore, when the likeli-hood of a region x in a test image is computed, concepts whose pdf X  X  were estimated from  X  X imilar looking X  vectors r will have high a posteriori probability (6). In particular, if each state c  X  V has exactly as many Gaussian pdf X  X  in the mixture as the number of images containing c in their cap-tion, if the mixture weights w m are uniform, if each Gaussian is modeled by a mean  X  m,c = r t , and if the variance  X  is held constant, there is a remarkable similarity between the proba-bility calculations of the HMM and the continuous relevance model. As may be anticipated, the annotation performance of the HMM and relevance model are comparable, as will be shown in Section 4.

In a nutshell, the HMM performs an abstraction of the information presented in the paired image+caption training data. For each concept c  X  V , only the sufficient statistics (mean and covariance) of the image features are retained. By contrast, the relevance model retains the entire training corpus for comparison with the test image. Consequently, the HMM performs the test annotations very efficiently, since each image-feature vector needs to be compared only with the Gaussian-mixture representation of each concept, not each training image. This results in tremendous com-putational speed-ups compared to the relevance model . Our approach is also much more scalable as a consequence. In our current experiments (see Sec. 4) the average time re-quired to annotate an image with all the concepts is less than 2 seconds. Also the annotation is readily parallelized on multiple CPU X  X  leading to further speed-ups.
The aforementioned comparison with the relevance model ought to convince the reader that the HMM paradigm per-mits several design choices which need to be explored. We briefly describe some of the major issues in this regard in this section.
 Fixed vs Object-Based Image Segmentation : Before we begin, note that we partition each image into a fixed number of rectangular blocks. One problem with rectangu-lar partitioning is that the rectangular blocks might con-tain pixels belonging to different  X  X bjects X  (or concepts) whereas our statistical model assumes that it is generated by the state corresponding to a single object (concept). How-ever it was shown in [5] that models trained on rectangular partitions outperform those trained on object-based parti-tions. The decision to work with fixed blocks is also driven by pragmatic concerns: we were kindly provided the pro-cessed training and test data from the 2004 Johns Hopkins University Summer Workshop [6], where considerable effort had been made in extracting image-features for the entire COREL and TRECVID datasets, and since state-of-the-art image annotation and retrieval performance has been pub-lished on these datasets, it provides a convenient venue to make comparisons with related techniques.
 Visual Feature Vectors : The workshop team extracted color, texture and oriented-edge features of 30-dimensions (COREL) or 76-dimensions (TRECVID) from a rectangular partition of the images, and this is what we have used in our experiments. In case of the COREL images, there are 4  X  6 blocks per image ( T = 24)and in case of the TRECVID collection, it is 5  X  7 ( T = 35). The block size, we are told, is an ad hoc balance between two conflicting criteria: one would like each block to be sufficiently small to contain only one of the concepts with which the image is annotated, and at the same time one would like it to be as big as needed to cover the entire object, so that features x t that characterize the object may easily localized.
 Figure 2: Illustration of Unlabeled Objects in Im-ages: this image was labeled with sand , beach , trees and people but not sky .
 HMM Topology and Permitted Transitions : We be-gin the HMM-design discussion by noting that given a set of image+caption pairs, we construct, for each image, an HMM with as many states as the number of words in its caption. The states of an HMM are fully connected, permit-ting any state to follow any other state. This corresponds to permitting any concept to occur in the image next to any other concept. One could, of course, investigate the ex-ploitation of the spatial propensities of concepts such as sky and grass to appear in certain positions t in the image, and the propensity of certain concepts such as tiger and sky (not to) appear in adjacent blocks. But we have not done so in this preliminary investigation. Every state has an equal likelihood of being reached at every position t . Shared State HMMs : A caption c  X  V appearing in two different images, of course, must be modeled by the same state. Therefore the HMM for each image  X  X hares X  states from a common pool of |V| states. In HTK parlance, the states of the HMM of each training image are  X  X ied X  to the corresponding states of other HMMs. Note that |V| = 375 for the COREL dataset, and |V| = 75 for TRECVID, as provided to us by [6].
 Accounting for Unlabeled Objects : An additional con-cept, which we call null has been introduced into the vo-cabulary and is added to the caption of each image. The primary motivation for doing so was our observation that in several images, the annotators did not label some prominent concept in-spite of having a word in V corresponding to it. Figure 2 depicts an image in which the sky occupies more than a third of the image, and yet the image is annotated with sand , beach , trees and people but not sky . The null state is akin to the silence model used in automatic speech recognition. In both cases, since the stochastic model of the image or speech is a generative model, the entire image or speech sequence needs to be accounted for by the model, and adding an optional silence or null to the transcripts or caption results in better modeling of the data 2 .
Once the visual features of the images are extracted and the HMM for each training image defined, estimation of the HMM parameters proceeds in a standard manner as pre-scribed in [12]. In particular, we first use a single Gaussian
Unlike the speech recognition application, however, we did not include the null state in the calculation of (6) since a null has no direct benefit for image retrieval. pdf for each state c  X  V . Each Gaussian pdf is initialized with the common mean and variance computed from all the images in the training data. Several iterations of the EM al-gorithm are carried out, updating the means and variances of the HMMs. We note that the transition probabilities are not updated in the experiments reported below; however they have a negligible effect on the likelihood and therefore, hopefully, on the eventual annotation performance. Mixture Splitting : Once the single Gaussian pdf X  X  have been estimated, the pdf for each state c  X  V is replaced by a mixture of a pair of Gaussian pdf X  X  obtained by minor perturbation. Further iterations of the EM algorithm are then carried out to (re)estimate the Gaussian mixture pdf X  X . This procedure is standard in training speech recognition systems and the details are described in [12].

Note that if the image features x t that  X  X lign X  with any state are very well modeled by the mixture of a certain num-ber of Gaussian pdf X  X , i.e. there is no gain in likelihood by increasing the complexity of the pdf, then one may choose not to  X  X ix up X  the pdf of that state. Such a model-based stopping criterion has been implemented in the HTK tools, and the experimental results reported below are for this de-fault setting.
 Setting the Variance-Floor : One subtle consideration in estimating Gaussian pdf X  X  is the estimate of variance. If a large number of mixture components are used, the amount of data used to estimate each component may be small, and the corresponding estimate of variance may be too low. Besides being inappropriate as a statistical model, this gives rise to numerical problems (divide-by-zero) during the probability calculations. It is standard to impose a  X  X ariance floor X   X  a minimum value for the estimate of the variance. Typically, this is set to be a fraction of the total empirical variance of all the image data.
 Maximum Mixture Size : As long as the data-likelihood increases by increasing the number of mixture components in f ( x | c ), we continue to increase the mixture size of the output pdf X  X  of each state until a certain pre-specified max-imum mixture size is attained.
Once the output pdf X  X  f ( x | c ) of all the states c  X  V have been estimated from the annotated image+caption pairs, we proceed to construct the so called decoding HMM. This is simply a fully connected |V| state HMM, one state cor-responding to each concept in the vocabulary. Transition probabilities of the HMM are set to be uniform.

Given test image, we perform the Balm-Welch algorithm that computes the posterior probability of (6). Once this is done for each image in the test collection, we are ready for image retrieval experiments.

Note that the transition probabilities of the decoding HMM need not be uniform. A simple variation is to compute the co-occurrence statistics of the words in the concept vo-cabulary, and use that frequency to adjust the transition probabilities. For instance, plane and sky tend to co-occur much more often than plane and water , suggesting that p ( sky | plane ) could be set higher than p ( water | plane ). In a contrastive experiment, we set these probabilities to be proportional to the corresponding relative frequencies, with adequate smoothing to avoid zero-probabilities.

This setting of transition probabilities is akin to the lan-guage model in speech recognition, and an experiment with this setting is so named in the results below.
We present image retrieval results on two collections as de-scribed below, one of still images and another of key frames extracted from video.
 The COREL Image Dataset consists of 5000 images from 50 Corel Stock Photo CD X  X  provided to us by [4]. Each CD contains 100 images with a certain theme ( e.g. polar bears), of which 90 are designated to be in the training set and 10 in the test set, resulting in 4500 training images and a balanced 500-image test collection. We note that this collection and training/test division is also that used by [7] and [5]. As stated earlier, each image is divided into 24 rectangular regions, and 30 dimensional image features are extracted from it and provided to us by [6]. The vocabulary has |V| = 375 words.
 The TRECVID Dataset consists of key-frames extracted from Broadcast News video from several sources, such as ABC News and C-SPAN. A community-wide effort has re-sulted in 44,100 images from the video portion of the 2002 TREC SDR corpus to be annotated by a set 137 concepts arranged in a hierarchical tree. This collection has been divided into four parts by [1], and named  X  X oncept-train, X   X  X oncept-validate, X   X  X oncept-fusion-1 X  and  X  X oncept-fusion-2. X  Each key-frame has been divided into 35 rectangles, and 76-dim visual feature-vectors extracted for each image-block by researchers at the 2004 Johns Hopkins Summer Workshop [6]. The workshop team also culled the concept vocabulary to remove annotations with very low counts, etc., so that the resulting collection of 38K images has captions covered by a |V| = 75 word vocabulary. Of the 44K manually labeled images, roughly 35K are used to train the HMMs, and about 9K are held out for image retrieval experiments. We again acknowledge the generosity of the workshop team in letting us use their training and test data in our experiments. The TRECVID-2003 Dataset consists of about 32K key-frames, again from Broadcast News sources [10] and was used for multiple tasks in the 2003 benchmark tests con-ducted by the US National Institutes for Standards and Technology. One of the tasks, named  X  X eature detection X , corresponds to our image annotation task. Participants were asked to detect 17 different features (concepts) in each shot of the video. Relevance judgements were created post hoc by a pooling of the retrieved shots of participating systems in the usual manner. Of these 17 concepts, 6 were of the sort that we are unable to cover using the generic concept annotation we trained our HMMs for, e.g. Madeline Al-bright and Physical Violence. Of the remaining 11 concept, some, such as Buildings, correspond directly with a concept in our vocabulary, making it easy for us to retrieve images for them. Finally, some concepts, such as Sporting Events, may be synthesized by a union of events in our vocabulary, e.g. basketball, hockey, football, baseball, etc. We therefore present retrieval results for this subset of 11 concepts (or features).
 Evaluating Image Annotation : Since our goal is to eval-uate image annotation performance in the service of image retrieval, we follow [7] and construct single-word queries for Table 1: Effect of Mixture-Size on Mean Average Precision for 260 One-Word Queries on COREL.
 Variance-Floor is set to 0.01.
 Table 2: Effect of the Variance-Floor on Mean Aver-age Precision for 260 One-Word Queries on COREL.
 Number of mixture components is set to 10. each word in our concept vocabulary V . Thus there are 375 queries for the COREL dataset (of which 260 have at least one relevant image in the test collection) and 75 for the TRECVID dataset. We retrieve images from the appropri-ate manually-labeled test sets (500 images for COREL) and (9000 for TRECVID). All test images whose caption con-tains the query-word are deemed  X  X elevant X  to that query, and all other test images as irrelevant. Standard mean aver-age precision (mAP) is calculated over all queries using the standard ( trec eval ) protocols, and precision-recall curves are plotted where appropriate. Tables 1 and 2 show the mean average precision of the HMM based image annotation system on the 500 COREL images. Two primary design parameters are investigated: the maximum number of Gaussian mixture components in f ( x | c ), and the variance-floor set during re-estimation of the HMM parameters. All experiments use a word co-occurrence based language model during decoding.

It is clear from the two tables that performance improves up to a certain point by building more and more complex models by adding mixture components, and then begins to level off. It is also clear that a very conservative variance floor (1% of the global variance of all image blocks) hurts performance, and that individual concepts have sufficiently sharp densities that a variance floor of 0.1% is significantly better. The best performance is an mAP of 0.19.

We note for the sake of comparison that on the 260 word query set, the normalized CRM of [5] has a mAP of 0.26, which is significantly higher than that of the HMM, while the MT based model of [4] on the same training and test sets has an mAP of 0.15 [6].
The annotation performance for an HMM as a function of an increasing number of mixture components on the TRECVID dataset is shown in Table 3. It confirms the earlier conclu-sion based on the COREL data that up to a certain point, increasing model complexity improves performance. Note that the TREC training set is significantly larger than the COREL, and hence performance continues to increase even up to 20-Gaussian mixture densities. The improvement in mAP from 10 to 20 mixture components is significant at a p-value less than 0.0001, while that from 16 to 20 at 0.006.
Before proceeding to increase the number of mixture com-ponents beyond 20, it is fair to contemplate whether a con-cept which has been attested, say, 20 times in the training Table 3: Effect of Mixture-Size on Mean Average Precision for 75 One-Word Queries on TRECVID.
 Variance-Floor is set to 0.01.
 Table 4: Effect of the Variance-Floor on Mean Average Precision for 75 One-Word Queries on TRECVID. Number of mixture components is set to 20. images should be modeled by an HMM state with a mix-ture of more than 20 Gaussian pdf X  X . Of course, each im-age has up to 35 feature vectors and a concept may span more than one image block. In order to establish where this trade-off may lie, we undertook an controlled mixture growth experiment. With a variance floor of 0.001, we estimated 4 different HMMs. The output pdf f ( x | c ) for concept c was 1.  X  X ixed-up X  to a maximum of 20 pdf X  X  no matter how 2.  X  X ixed-up X  to a maximum of 1 pdf per 3 tokens of c , 3.  X  X ixed-up X  to a maximum of 1 pdf per 5 tokens of c , 4.  X  X ixed-up X  to a maximum of 1 pdf per 15 tokens of c . Note that the very frequent concepts are not affected by this, since many of them have more than 300 tokens. It is only the infrequent concepts whose maximum mixture size is progressively reduced in the successive experiments. Table 5 presents the results for the increasingly conservative model growth strategies.
 Table 5: Annotation Performance for Progressively Conservative Limits on the Maximum Number of Mixture Components for each Concept.
 mAP 0.153 0.179 0.184 0.185 Table 6: Annotation Performance with up to 100 Gaussian pdf X  X  for each Concept.

The reader should be convinced that the TRECVID dataset is large enough not to take a conservative view about model size in the range we are operating. Therefore, we went ahead and estimated an HMM system with up to 100 Gaussian pdf X  X  per concept (state), limited of course by the likelihood-gain based automatic limits implemented in HTK. The an-notation performance results of the final system are pre-sented in Table 6. We plot the 11-point precision-recall curve for the 100-Gaussian HMM system in Figure 3. The mAP Figure 3: Precision Recall Curve for 75 One-Word Queries on the TRECVID Dataset. of 0.185 for the 100-Gaussian system is significantly better than the 0.168 mAP for the 20-Gaussian system (p-value = 0.0012).

We note from [6] that the MT based system has a mean average precision of 0.13 and the Normalized CRM has 0.16 for identical training and test partitions and feature extrac-tion. The MT based system discritizes the x t  X  X  while the CRM uses the 76-dim features. We believe that the HMM based system is significantly better than either of them. The HMM based approach is therefore competitive with the best of the available techniques.

Figure 4 illustrates some of the more successful queries out of the 75 by showing the top 5 retrieved images, and the precision-recall curves for these queries. As the reader may notice, the images have several visually striking features, which the HMM has apparently managed to model even with such a primitive model. Note that one of the queries shown, ( weather news ), happens to be among the features prescribed in the TRECVID-2003 feature-detection task, as described next. We mapped 11 of the 17  X  X eature detection X  queries from TRECVID-2003 onto one or more concepts in our 75-word vocabulary, and performed image retrieval using the HMM developed on the TRECVID dataset. Some features, such as sporting events, were obtained by the union of concepts such as hockey, baseball, etc. , while others, such as weather, had direct analogues in V . The average precision for the 11 concepts is reported in Table 7 and Figure 5.

Also reported in Table 7 is the average precision of an almost comparable stage (BOU) from [1]. Note that BOU represents the best of three feature detectors, only one of which is comparable to our HMM based system. Since the
The authors of [5] state that the CRM attains a mean av-erage precision of 0.18 on this task using 30 dimensional features instead of 76-dim.
 Figure 5: Precision Recall Curve for 11 Features on TRECVID-2003. overall system submitted by [1] was the best performing fea-ture detection system in TRECVID-2003, we feel confident in claiming that the HMM based methods proposed here are competitive with the state of the art.
 Table 7: Feature Detection Performance on the TRECVID-2003 Feature-Detection Task.
We have presented a novel method for image annotation for the purpose of content based image retrieval, and evalu-ated in in several ways to establish that it is indeed compet-itive with or better than the state of the art. The system was implemented with publicly available tools and tested on standard datasets.
Much more is known in the speech recognition literature about training and decoding using hidden Markov models, including context-dependent modeling, unsupervised adap-tation, adaptive and discriminative training, and minimum error decoding, all of which remains to be applied to the image annotation and retrieval problem. Each of these tech-niques have resulted in significant improvements in recogni-tion accuracy on speech recognition, and indeed constitute the bulk of the progress in speech recognition research in the last decade. Adaptation, for instance, adjusts the mod-els to overall variations from image to image, discrimina-tive training replaces the maximization of the likelihood (1) with maximization of the ability to discriminate concepts, and context dependent modeling accounts for the influence of adjacent objects on the visual features of an object.
We expect that significant improvements in image anno-tation and retrieval performance will be obtained by intelli-gently importing these ideas from speech recognition.
The authors gratefully acknowledge the considerable assis-tance, particularly standardized image datasets and visual features, provided by the Johns Hopkins Summer Workshop team in conducting these experiments. This research was partially supported by the U.S. National Science Founda-tion via Grant No fense via Contract No Academy of Sciences via Grant No [1] A. Amir et al. IBM Research TRECVID-2003 Video [2] K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth, [3] D. M. Blei and M. I. Jordan. Modeling Annotated [4] P. Duygulu, K. Barnard, N. de Freitas, and [5] S. L. Feng, R. Manmatha, and V. Lavrenko. Multiple [6] G. Iyengar et al. Joint Visual-Test Modeling for [7] J. Jeon, V. Lavrenko, and R. Manmatha. Automatic [8] V. Lavrenko, S. L. Feng, and R. Manmatha.
 [9] J. Li and J. Z. Wang. Automatic Linguistic Indexing [10] NIST. In Proceedings of the TREC Video Retrieval [11] L. R. Rabiner. A tutorial on hidden Markov models [12] S. Young et al. The HTK Book . 2002. [13] P. Viola and M. Jones. Rapid object detection using a
