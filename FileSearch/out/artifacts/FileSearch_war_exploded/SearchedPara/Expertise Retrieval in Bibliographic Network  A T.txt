 Expert finding in bibliographic networks has received in-creased interests in recent years. This task concerns with finding relevant researchers for a given topic. Motivated by the observation that rarely do all coauthors contribute to a paper equally, in this paper, we propose a discriminative method to realize leading authors contributing in a scientific publication. Specifically, we cast the problem of expert find-ing in a bibliographic network to find leading experts in a research group, which is easier to solve. According to some observations, we recognize three feature groups that can dis-criminate relevant and irrelevant experts. Experimental re-sults on a real dataset, and an automatically generated one that is gathered from Microsoft academic search show that the proposed model significantly improves the performance of expert finding in terms of all common Information Re-trieval evaluation metrics.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  Search process; Retrieval models ; H.2.8 [ Database Management ]: Database applications X  Data mining Expertise retrieval, Learning to Rank, Discriminative mod-els
As the large portion of web provides information for var-ious kinds of real-world objects (i.e. entities), more search engines provide object level search result. Some instances of typical objects are products, peoples, papers and organi-zations. As one of the noteworthy resources in the world, searching the human expertise has recently attracted much attention in Information Retrieval (IR) community. Con-sidering experts as the web objects, expert finding is one of the challenging types of object level search, which concerns itself with ranking people who are knowledgeable in a given topic.

Initial approaches for expert finding have been proposed to identify experts in simple environments such as organi-zations [2] and universities [3]. While these approaches are quite effective in these simple domains, they are not appro-priate for complicated environments such as bibliographic networks [10], hierarchical organizations [14] and expert X  X  social networks [18]. Considering associated documents of each candidate as the main evidence of his or her expertise, initial approaches are not able to take into account other valuable expertise evidences in such complicated domains. These evidences can be social interactions between individ-uals, temporal behaviors of them and the document quality indicators (e.g. number of citation and type of document) which are usually independent of the content of the docu-ments.

Identification of knowledgeable persons in a specific aca-demic field could be of great value in many applications such as recognizing qualified experts to supervise new researchers, assigning a paper to reviewers [19], and expert team forma-tion [15]. As a complicated search domain, bibliographic networks contain various types of documents (e.g. confer-ence proceedings, journal articles), experts (e.g. students and supervisors) and relationships between them (e.g. co-author and citation relationships) that can be used to infer the expertise of the academic persons. Finding experts in a bibliographic network is a challenging task because of the following reasons: 1. In contrast with simple domains, there exist a huge 2. Besides the content of associated documents of a per-3. While scientific publications are usually a product of
Many approaches have been proposed and shown to be effective for expert finding. For example, document centric models [2, 10] are based on the assumption that the rele-vance of the textual context (i.e. associated documents) of a person adds up to the evidence of his or her expertness. Although these methods are beneficial for expert finding, they only consider the equivalent share for each expert can-didate mentioned in a scientific publication. Generally, the authors of a paper have different expertise level in the do-main of that paper. For instance, while the supervisor of a PhD project has a broader view of the research problem, other co-authors might be more involved in the detail of the project. As another example, contributing in an indus-trial project, the project managers have more expertise in the problem domain than the other team members. While recognizing the contribution of each author in a scientific publication has been noticed in many researches [1, 21], it is not considered in expert finding approaches.

Considering each paper as an evidence of expertise for its authors, in this paper we propose a discriminative learning algorithm to assign non-equal expertise score to authors of a document. Specifically, our model assigns more score to au-thors who are more dominant in the topic of the paper. We called our approach in this paper as X  X opic dominance X  X earn-ing. We examine the impact of various features to estimate the topic dominance of authors in a bibliographic network. Specifically, we consider three feature groups; namely, struc-tural, temporal, and activity-based features. We found that all of these feature groups are beneficial to determine the topic dominance of authors.

The rest of this paper is organized as follows. In Section 2 we review some related work on expertise search. Section 3 is devoted to a description of the background and pre-liminaries, and in Section 4 we detail our models of topic dominance learning. In Section 5, we define the experimen-tal setup and report the experimental results. Finally, we present the conclusions and future work in Section 6.
The inclusion of the expert finding task in TREC Enter-prise has attracted a significant amount of attention from 2005 to 2008 [8, 23, 6]. The main approaches of expert finding can be categorized into two groups: profile centric and document centric approaches [2]. While profile centric method (i.e. Model 1 in [2]) directly models the knowl-edge of an expert from associated documents, document centric method (i.e. Model 2 in [2]) first locates documents which are related to the topic and then finds the associ-ated experts. Although some variations of profile-centric methods (e.g. proximity-based [20]) performed better than document-centric methods on the TREC collections, docu-ment centric method is generally more effective and easier to implement than the profile centric method [5]. There-fore, most recent methods of expert finding are basically the extension of the document centric method.

The problem of expert finding in bibliographic networks has been introduced in [10]. In this research, the goal is to retrieve experts in specific academic domains based on their publications in the DBLP 1 bibliographic network. More re-cently, Deng et al. [11] proposed a new smoothing method based on the community context of each author and also a community-sensitive AuthorRank method for co-authorship networks. They observe that the community (i.e venue in which the paper is published) provides valuable and dis-tinctive information along with the documents of the ex-perts. They also studied the expertise ranking problem through modeling and exploiting heterogeneous network to-gether with the textual content information [9].

In all above mentioned approaches [9, 10, 11], if a doc-ument is associated with more than one expert candidate, then all of these candidates will get an equal score from that document. In contrast, some approaches consider a non-equal expertise scores for expert candidates occurred in a single document. For example, Serdyukov and Hiemstra [22] proposed an expert centric language model which assumes that the terms in a document are generated by those persons who are mentioned in it. For documents with more than one associated candidate, they proposed an EM-algorithm to re-fine the specific language model of each expert candidate. As another approach, Petkova and Croft [20] considered the proximity of query terms to expert X  X  name occurrences and give more score to the expert candidates whom their names are occurred near to the query terms.

Similar to these approaches, our model assigns non-equal scores for expert candidates associated with a document. While the proposed method in [22] is an unsupervised ap-proach and basically a profile-centric method, our model is a document-centric method which utilizes supervised learning to determine the author X  X  dominance in topics of a docu-ment. Besides the simplicity and the performance of the document centric methods in comparison with the profile centric method, our proposed model is able to employ var-ious features to learn author X  X  topic dominance. On the other hand, in bibliographic networks, the names of authors are usually mentioned in the same position of documents (just after the title); therefore, the proximity based method proposed in [20] cannot be useful to determine the topic dominance of each author in a document.

As a related line of research, Balog and Rijke [4] proposed three methods to estimate the association strength between a candidate expert and a document. The boolean method simply assigns zero/one association weights according to oc-currence of an expert X  X  name in a document; however, the frequency method assigns weights based on the occurrence number of an expert X  X  name in a document. They also pro-posed the semantic relatedness approach which assigns as-sociation weights based on the similarity of document to the expert X  X  profile. The boolean and frequency based weight-ing have better performance on TREC test collections in comparison with the semantic relatedness approach and are widely used in expert finding algorithms. Therefore, the boolean weighting scheme is used in expert finding algo-rithms for bibliographic networks[9, 10, 11]. It is worth men-tioning that in bibliographic networks, the name of experts are clearly occurred only once in their associated document (i.e. paper), therefore, the frequency approach is not ap-www.informatik.uni-trier.de/ ley/db plicable for expert finding in these networks. In all above methods, the association weights between a document and its authors are determined independent of each other. In contrast, our proposed model considers the relative topic dominance of authors and assigns non-equal weights to these associations.

While previous researches on expert finding estimate the expertise relevancy without regard to authors X  contribution on their associated document, some previous researches [21, 1] suggested that authors of a document should not nec-essarily get equal contribution. For example, Sekercioglu suggested that the contribution of k th coauthor should be 1 /k as much as the first author [21], and Abbas introduced weighted h-indices that take into account the weighted con-tributions of individual authors in multi authored papers [1]. While these approaches estimate the contribution of each ex-pert based on their names X  order, following the same idea, we used extensive set of features to learn the topic dominance of authors in their research publications.

As another related line of research, Fang et al. [12] pro-posed a discriminative learning framework for expert search. Their model is able to integrate a variety of document X  X  ev-idence and document association features. More recently, Macdonald and Ounis [17] proposed an approach to learn rankings for aggregate search tasks such as expert search. However, these methods did not take into account unequal expert-document association for authors of multi-author doc-uments.
As one of the efficient approaches of expert finding, document-centric method [2, 10] estimates the expertise of an expert candidate by summing the relevance of his or her associated documents. In this section, two main efficient methods of ex-pert ranking in bibliographic networks are described. The first method is Model 2 proposed by Balog et al. [2] and the second one is the weighted language model proposed by Deng et al. [10].
In this model, for a given query q , the relevance probabil-ity of each expert is determined by the following equation: in which, p ( e | q ) represents the score of candidate e for the given query q , p ( q | e ) is the probability of query q given the expert candidate e , p ( e ) is the prior probability of being expert for an expert candidate e ,and p ( q ) is the prior prob-ability of a given query q .As p ( q ) is a constant in expert ranking, it can be ignored from above equation. Therefore, relevance probability of each experts can be estimated by the probability of a query given expert candidate ( p ( q weighted by the prior probability that expert candidate e is an expert ( p ( e )): According to the document centric methods [2, 12], the prob-ability of a query given expert candidate can be estimated by the following equation: where D e indicates the subset of documents associated with the expert candidate e . In this equation, to simplify the calculations, it is assumed that expert candidate e is condi-tionally independent of the query q given document d (i.e. p ( q | d, e )  X  p ( q | d )). As a result, by substituting p ( q Equation (2) and expanding p ( d | e ), p ( e | q ) can be estimated as follows: where p ( d ) denotes the prior relevance probability of doc-ument d , p ( q | d ) is the probability of a query q given the document d ,and p ( e | d ) is the probability of the association between the document d and the expert candidate e .The probability p ( e | d ) provides a ranking of candidates associ-ated with a given document d , based on their contribution made to d [4]. According to equation (4), in order to rank candidate experts, we should estimate three probabilities, namely, prior probability of retrieval for document d (i.e. p ( d )), relevance probability of document d (i.e. p ( q association probability of document d and expert candidate e (i.e. p ( e | d )).

The relevance probability of document d to query q (i.e. p ( q | d ) in equation (4)) can be estimated by the document language model  X  d of document d : where p ( t |  X  d ) is the probability of term t given the document model  X  d ,and n ( t, q ) is the number of times that term t oc-curs in query q . In order to overcome zero probabilities, we use the JM-smoothing [24], so the probability p ( t |  X  d mated as p ( t |  X  d )=(1  X   X  ) p ( t | d )+  X p ( t ), where p ( t maximum likelihood estimation of the occurrence of term t in the document d ,and p ( t ) is the occurrence probability of the term t in the document repository. In our experiments, we follow Balog et al. [2] in setting the smoothing param-eter  X  =0 . 5. Balog et al. [2] assume the document prior probability p ( d ) is uniform, so it does not affect the rank-ing of experts. For a multi-author document d ,itassumed that each author has the same level of knowledge about the topics described in the document and therefore the associ-ation probability of document d and expert candidate e is estimated as follows: where n d is the number of authors of document d .Tosum up, by substituting equations (5) and (6) in equation (4), the final estimation of the Balog X  X  ranking is obtained by the following equation. In the rest of the paper, we refer to this method as the uniform language model (ULM).
The method described in Section 3.1 calculates the rele-vance probability between the query and expert candidate, but it ignores the prior relevance probabilities of the doc-uments. If we assume that two documents d 1 and d 2 have similar contents, then the query likelihoods are almost the same score to the authors of these documents. However, if these documents have different importance, we would prefer to rank the authors of the more important document higher than the less important one. This is the main idea of the weighted language model proposed by Deng et al. [10]. They introduced a weight factor w d to denote the importance of a document, which, theoretically, can be interpreted as being proportional to the document prior probability p ( d ). The weight factor is estimated using the citation count of doc-ument (i.e. paper) and is transformed by the natural loga-rithm. where c d ( c d  X  0) is the citation count of the document d and constant e is used to guaranty that the weight factor not to be less than 1. The final estimation of the weighted language model is: In the rest of the paper, we refer to this method as the weighted language model (WLM).
The document centric model introduced in section 3 is an effective model for finding experts in simple environments where each document usually associated with a single ex-pert candidate. In these environments, the boolean weight-ing model [4] can efficiently estimate the document expert association probability (i.e. p ( e | d ) in equation (4)) like fol-lows: Moreover, because the name of each author of a paper is occurred only once in that paper, the frequency based asso-ciation estimation method [4] has the similar behavior with boolean method in bibliographic networks. In contrast with expert finding in above mentioned environments, a substan-tial fraction of documents in bibliographic networks is asso-ciated to more than a single author. For example, in DBLP bibliographic network, more than 76 percent of documents (1,254,058 of 1,632,442 papers) 2 have more than one author. Therefore, the expert-document association probability (i.e. p ( e | d )) becomes an effective component to estimate expert relevancy. Surprisingly, previous methods (e.g. [10, 11, 9]) for expert finding in bibliographic networks, simply ignore the effect of this component and assign the same score to all authors of a multi-author document.
 Indeed, rarely do all coauthors contribute to a paper equally. Suppose a multi-author document d written by two authors a and a 2 , here, the question is which author is more domi-nant in the topic of the document d ? Which author should The dataset is gathered from the Arnetminer website. Figure 1: The co-authorship networks of top re-trieved authors for Information Retrieval (left fig-ure) and Face Recognition (right figure) topics. be ranked higher for a query relevant to document d ?Ob-viously, we would prefer to rank the author with more topic dominance higher than the rest authors of the document. To the best of our knowledge, current approaches for expert finding in bibliographic network do not take this factor into account. We observe that some suitable features can signif-icantly help estimating the topic dominance of authors in research publications. In the rest of this section, first we introduce some effective feature groups, and then propose our model for learning topic dominance of each author in multi-author documents.
Various features can be beneficial to estimate the topic dominance of author e in a document d . For example, re-search longevity of author, the expert X  X  authority in related research community in topic of paper d , and the research ac-tivity (e.g. diversity, quality of research, as well as semantic relatedness of the document d and the profile of expert e ) are some effective features in estimation of topic dominance of author e in document d .

Figure 1 indicates the co-author networks of top retrieved expert candidates using the ULM (introduced in Section 3.1) for two research topics (i.e. Information Retrieval and Face Recognition), in which relevant experts and non-relevant authors are illustrated with black and gray colors, respec-tively 3 . According to this figure, it is obvious that relevant expertstendtobeplacedinthecenteroftheco-authornet-work.

As another example, to quantify the research quality of authors, Figure 2 illustrates the logarithm of average of the sum of citations for relevant experts (black columns) and non-relevant authors (gray columns) retrieved by the ULM. It is clear that the relevant experts obtain a greater sum of citations than the non-relevant authors.

As another effective feature, the research longevity of an expert candidate can be a useful evidence to infer his or her expertise. As indicated in Figure 3, in most of the research topics/queries, the research longevity of relevant experts (black columns) is more than non-relevant authors (gray columns).

Note that in above examples, relevant experts and non-relevant authors are the top-rank candidates that are re-
We use Deng et al. test collection [10] to label relevant and non-relevant expert candidates. Figure 2: Average of sum of citations for relevant experts and non-relevant authors in logarithm scale. Figure 3: Average of research longevity of relevant experts and non-relevant authors. trieved by the ULM, and this approach is not able to discrim-inate these authors, because it ignores the effect of the men-tioned feature groups. According to above observations, we define three feature groups to discriminate relevant experts from non-relevant authors. These feature groups include: structural, temporal, and activity-based features. Although, these groups of features may have positive correlation with each other, each group is conceptually distinct from other groups.
In this section, we propose two discriminative methods to predict the topic dominance of each author in a multi-author document. The predicted value for the topic dominance of an author then can be plugged into the equation (4) to es-timate the relevancy of each expert candidate on a topic. For a given query, we cast the topic dominance estimation problem into a classification problem that treats the rele-vant experts in a multi-author document as positive data, and non-relevant authors as negative data. Formally, we use a topic dominance variable c  X  X  0 , 1 } to denote how much author e of document d has dominance on the topic of docu-ment d . Specifically, the probability of P  X  ( c =1 | e, d )canbe used as the topic dominance estimator of author e for docu-ment d , where  X  is the unknown parameters that should be learned using training data.

In order to train our model, for a given topic t ,wedi-vide the set of retrieved documents by ULM into two dis-tinct subsets. The first subset (i.e. D 1 t ) includes documents which have been written by at least one relevant expert to topic t (an author which is determined as a relevant expert for topic t in the test collection) and the second subset (i.e. D t ) includes all documents that all of their authors are non-relevant to the topic t . By definition of D 1 t and D D t = D 1 t  X  D 2 t where D t is the set of documents retrieved by the ULM for the topic t .
While there is no obvious evidence for the relative topic dominance of the authors of papers in the D 2 t set, we can assume that relevant experts have more topic dominance in comparison with non-relevant authors in the D 1 t set. There-fore, in our first attempt for building a learning model, we only use the documents of D 1 t .Foreachdocument d  X  D 1 t we produce the training set as T 1 = { ( e, d, e t ) | d  X  A ( d ) ,e t  X  X  0 , 1 }} , where A ( d ) denotes authors of document d , e indicates an author of it, and e t is the label of author e in the expert finding test collection for topic t .Specifically, e = 1 if author e is relevant expert for topic t .
 The members of T 1 set can be divided into positive and neg-ative instances. Given the relevance judgment e t for each expert e on a topic t , the likelihood L of training data is as follows. We assume that the relevance judgments e t are generated independently.

L = where A ( d m ) denotes the authors of document d m .We model P  X  ( c =1 | d m ,e ) by logistic functions on a linear com-bination of features. The unknown parameters  X  canthenbe estimated by maximizing the following log likelihood func-tion.  X   X  = argmax The estimated parameters can then be plugged back in P  X  ( c = 1 | d m ,e ). In the first learning model, we used only the papers in the D t set and ignored the papers in the D 2 t set. In order to avoid the bias and more precisely predict the topic domi-nance of each author, we can also use the papers in D 2 t As mentioned before, for papers in D 2 t set, we do not have any evidence that confirms the relative topic dominance of authors in a multi-author document. As a result, following the idea of Deng et al. [10], for each document d  X  D 2 t assume the equal topic dominance for all authors. In this case, the predicted topic dominance of an author in a given paper can be estimated by the following equation: where P  X  2 ( y =1 | d ) indicates the probability of paper d being amemberof D 1 t and P  X  1 ( c =1 | d, e, y = 1) is the strength of topic dominance of author e in paper d which has been written by at least one expert. In this learning model, the members of D 1 t as well as D 2 t set is used to generate the training set. Each training instance can be described using the following set:
T 2 = { ( e, d, y, e t ) | d  X  D t ,e  X  A ( d ) ,e t  X  X  0 , 1 Each member of T 2 (i.e. each training instance) has two la-bels. The y label indicates whether the paper d is a member of D 1 t ;if y = 1, then the label e t indicates the relevance of author e in topic t . Given the relevance judgment e t and the label y of each paper, the likelihood L of the training data is as follows. We assume that the relevance judgments e t generated independently (we assume the same assumption for y labels).
 Learning in this model is finding parameters  X  1 and  X  2 such that the above likelihood function is maximized. Accord-ing to the definition of equation (9), we have the following equations:
We model both P  X  1 ( c =1 | d m ,e,y =1)and P  X  2 ( y =1 by logistic functions on a linear combination of features. For-mally, they are parameterized as follows: where N f and N g are the count of features for functions P and P  X  2 respectively, and  X  ( x )= 1 1+exp(  X  x ) is the standard logistic function. In above equations,  X  i is the weight of the i th document feature g i ( d )and  X  j is the weight of the j author-document feature f j ( d, e ). The weight parameters can be learned by maximizing the conditional log-likelihood of the data (i.e. equation (10)). Because there is no ana-lytical solution, we use the BFGS Quasi-Newton for solving it.

Table 1 indicates the features used for learning the topic dominance of authors in multi-author publications (i.e. P 1 | d m ,e,y = 1)). In this table, each feature is defined for a document d and one of its authors. As mentioned before, we categorize the features into three main groups. Structural features are defined based on the position of an author in the co-authorship network of the venue (i.e. journal or confer-ence) in which paper d is published in (i.e. venue ( d )). For example, f 1 is the count of co-authors of an expert and f the PageRank [7] value of an author in venue ( d ). Moreover, f is the AuthorRank [16] value, which is a modification of PageRank to measure the authority of authors in venue ( d ). f 4 and f 5 are betweenness centrality measures (computed on weighted and un-weighted venue ( d )) that represents the Table 1: Author-document association features or-ganized in three groups extent to which an author lies on the paths between other authors and can also be interpreted as measuring the influ-ence an author has over the spread of information through the co-author network.

Temporal features are also defined for document and ex-pert pairs, and represent the research longevity of an expert. Specifically, f 6 indicates the number of papers an expert e published before document d and f 7 is the overall research longevity of expert e . As the last feature group, activity-based features indicate the diversity and research quality of authors associated with a document. Specifically, f 8 is the number of different venues that an author is participated in. f 9 and f 10 indicates the average and the sum of citation count of each author respectively. Finally, semantic relat-edness feature (i.e. f 11 ) is the similarity score of the expert candidate X  X  profile(i.e. concatenation of his or her previous papers) and the document X  X  language models. Features used for learning of P  X  2 ( y =1 | d m ) should indicate the quality of paper d m . We assume the quality of each paper is affected by the research quality of its authors. Therefore, we use the aggregate values of the features described in Table 1 to de-fine document features. For example, features g i 1 , g i g 3 are the minimum, maximum and the average value of features f i for all authors of the document d .

The expert-document association probability p ( e | d ) pro-vides a ranking of candidates associated with a given docu-ment d , based on their contribution made to d . This prob-ability is proportional to the prior probability of being ex-pert for an expert candidate e (i.e. p ( e | d )  X  p ( e )). There-fore, in addition to the features that indicate the association strength of a document-expert pair, utilizing features that illustrate expert X  X  prior relevance probability (i.e. f 7 and f 10 ) is also reasonable.

Our proposed model is capable to assign different scores for different authors of a single documents. Therefore, con-sidering each document as the relevant evidence, it is able to give more scores to more relevant candidates. After pre-dicting the topic dominance probability for each author of the document d , we normalize the association probabilities such that e  X  A ( d ) p ( e | d )=1.
An extensive set of experiments are designed to address the following questions of the proposed research:
In this section, we describe dataset and evaluation metrics.
We evaluate our models on the real-world DBLP bibliog-raphy data. Each DBLP record consists of several elements, such as  X  X uthor X ,  X  X itle X ,  X  X onference X  and  X  X ate of publish-ing X . Using these records, we could easily extract the co-author network and the mentioned features in Section 4. The citation and abstract information of papers were ob-tained from the Arnetminer [ ? ] 4 . We get totally 1,632,442 papers and 1,033,321 authors in our data collection.
Due to the scarcity of ground truth that can be exam-ined publicly, the evaluation of expert finding performance in such a large data collection is very challenging. Further-more, it is impractical to obtain expert ratings for all au-thors. Therefore, in order to measure the performance of the proposed models, we use the benchmark dataset pro-posed by Deng et al. [10]. This dataset contains 17 queries and a list of relevant experts for each topic that is manu-ally created. It cover both the broad and specific queries and has been widely used for evaluating expert finding task on bibliographic network [10, 11, 9]. However, this dataset contains a few number of queries. As a result, we also exam-ine the performance of our proposed models as well as the ULM and WLM on an automatically generated dataset that is crawled from the Microsoft academic search 6 (i.e. MAS). We select 87 queries including broad (e.g. Databases and Artificial Intelligence) and specific topics (e.g. Feature Ex-traction, Genetic Algorithm) in computer science which are distinct from the topics of Deng et al. [10] dataset 7 .For each topic, we gathered the top 50 experts from Microsoft academic search as relevant experts.

To test our proposed models, for the Deng et al. [10] dataset, in order to avoid overfitting for a given query, we http://arnetminer.org/lab-datasets/citation/ this dataset can be accessed from http://www.cs.uiuc.edu/ hbdeng/data/queryset.txt http://academic.research.microsoft.com/
The list of topics are indicated in Appendix 8 sample 8 out of 16 other available topics and train our learn-ing model using these topics. We repeat the sampling pro-cess 10 times and report the average of the measures. For the automatically generated dataset, we train our model us-ing all available topics of Deng et al. dataset, and test its performance for all topics of MAS dataset.
For the evaluation of task, several common IR metrics are employed in measuring the performance of our proposed models in different aspects. These metrics are precision at rank n ( p @ n ), Mean Average Precision ( MAP ), Mean Re-ciprocal Rank ( MRR ), bpref , and R-precision. p @ n mea-sures the fraction of the top n retrieved results that are relevant experts for the given query which is defined as
For a single query, AP is defined as the average of the p @ n values for all relevant documents as AP = N n =1 p @ n  X  rel where n is the rank, N is the number of expert candidate retrieved, and rel ( n ) is a binary function indicating the rel-evance of a given rank. AP emphasizes returning more rel-evant documents earlier, per query, and MAP is the mean value of the AP s computed for all queries. The MRR is the average of the reciprocal ranks of results for a set of queries defined as the precision at rank R where R is the num-ber of relevant candidates for the given query. Besides the measurement of precisions, bpref is a good score function that evaluates the performance from a different aspect, i.e., the number of non-relevant retrieved candidates. It is for-where r is a relevant candidate and n is a member of the first R candidates judged non-relevant as retrieved by the system.
To demonstrate how the expertise ranking performance can be improved by our proposed models, we implemented ULM and WLM. As mentioned in Section 4, we proposed two learning models (i.e. Model A and Model B ). Table 2 in-dicates the performance measures for Model A and Model B as well as ULM and WLM for both datasets. In this exper-iment, we follow the idea of WLM to define the prior rele-vance probability of a document proportional to the number of its citation, and our proposed models are learned by all features that are introduced in Section 4.

The queries in Deng dataset are more general/braoder, than the queries in MAS dataset. Therefore, it seems that the expert finding task on Deng dataset is easier than the MAS dataset. As a result, as indicated in Table 2, the overall performance of expert finding task on Deng dataset is bet-ter than the MAS dataset. According to this table, Model A improves all performance measures for Deng et al. dataset, but slightly reduces the MRR measure; and it improves all performance measures for MAS dataset. In contrast, Model B significantly 8 improves all performance measures in both datasets. As mentioned before, Model B uses the training in-
Student T-Test is used to indicate that our proposed mod-els X  have improved the baseline models significantly.To this purpose, for Deng dataset, we do the sampling 10 times and each time, 8 out of 17 topics are selected; and for MAS dataset the sampling is done 10 times and each sample con-stances from both D 1 t and D 2 t sets, but Model A only uses the training instances from the D 1 t set. Although, Model A can improve the expert ranking by estimating the topic dominance of authors, it is somehow biased for instances in D 1 t set. However, the Model B utilizes all possible evi-dences to learn the topic dominance of authors. In the rest of the paper, we examine the properties of the Model B in comparison with the ULM and WLM models.

We compare the discriminative power of each feature group in recognizing the relevant expert for a given query. Table 3 indicates the performance measures of WLM, Model B which is trained on each feature group separately, and the Model B which is trained by the all feature groups. According to this experiment, structural , temporal and activity-based feature groups can improve the WLM ranking in terms of all evaluation measures in both datasets. While structural and activity-based feature groups are more affective than the temporal feature group, they have almost the same dis-criminative power in recognizing relevant experts. Finally, learning the model by utilizing all of the feature groups can significantly improves expert ranking over WLM in terms of all measures for both datasets. Specifically, we found that the most effective features for recognizing the topic domi-nance are f 9 , f 8 , f 6 , f 1 , f 5 ,and f 3 respectively.
Table 4 reports the evaluation results of our proposed models as well as ULM and WLM in both datasets. In this experiment, we use two methods to determine the prior relevance probability of documents. In the first method, we use uniform prior probability for all documents (i.e. simi-lar to the ULM approach) and in the second method, we use the weight which is proportional to the citation count of each document (i.e. similar to WLM approach). As it is clear in Table 4, the proposed model can improve the corre-sponding baseline models independent of the method used for estimating the document prior probabilities. This obser-vation shows that, while the WLM approach focused on the document X  X  quality for improving expert ranking, our model measures the topic dominance of each author of a document and gives more score to the leading expert candidates in a document.

Finally, we turn to a topic-level analysis of the compar-isons illustrated in pervious experiments. We plot the dif-ferences in Average Precision between our proposed model and WLM (per query) in Figure 4 and 5. These figures show that our proposed model is substantially preferable than the WLM. According to Figure 4, it is obvious that our proposed model improves the average precision of WLM for all the queries except query 12 in Deng dataset. Interestingly, query 12 is  X  X anguage Model For Information Retrieval X  which is relatively a new field of research. Therefore, most of its rele-vant experts are young researchers that are less experienced than other authors. As mentioned before, our method tends to choose more experienced authors of a document as rele-vant experts. As a result, it is rational that our proposed model cannot improve average precision of query 12, but for all other queries in Deng dataset, our model can improve the average precision of WLM. As illustrated in Figure 5, the same pattern is observed in MAS dataset. Specifically, our proposed model improves the average precision for all queries except few queries (e.g. Web Ontology Language) which are relatively new field of research. tains 45 topics out of 87 topics. It should be noted that the chosen threshold for statistical significance is 0.05. Figure 4: Topic-level differences in average precision for Deng dataset, WLM vs. our proposed model. Figure 5: Topic-level differences in average precision for MAS dataset, WLM vs. our proposed model.

To gain a better insight into our proposed model, we chose  X  X mbedded System X  and  X  X rid Computing X  queries as the example cases to demonstrate the detailed results. The top-10 author lists ranked by ULM, WLM and our proposed model are shown in Table 5, where the relevant experts that are based on MAS dataset are illustrated by underline. Ac-cording to this experiment, the results of our proposed model make more sense than ULM and WLM. For example, con-sidering the query  X  X mbedded System X , we find that our proposed model can boost some relevant researchers such as  X  X rancky Catthoor X ,  X  X uigi Carro X  and  X  X uca Benini X  into top 10. Similarly, results of  X  X rid Computing X  query show the same observation. This is because of distinguishing rel-evant expert author from other authors of a document.
In this paper, we studied the expertise ranking problem through learning topic dominance of each author in a scien-tific publication. We proposed two discriminative learning models to recognize the leading experts in a research group. We consider three effective feature groups to discriminate relevant experts from irrelevant ones. The experimental re-sults show that our proposed models can significantly im-prove the quality of expert finding task on a real dataset and also on an automatically generated one crawled from the Microsoft academic search. The proposed method in this paper can be used to recognize leading researchers in research communities, industrial groups, and also organiza-tions. As a future work, we plan to implement our method in these environments. Table 2: Comparison of our two proposed models.  X  indicates the improvement is statistically significant (  X &lt; 0 . 05 .) is statistically significant (  X &lt; 0 . 05 ).
 is statistically significant (  X &lt; 0 . 05 ).
 Table 5: The top-10 experts retrieved by ULM, WLM, and our best proposed model (i.e. Model B (w-all)). Relevant experts are illustrated by un-derline.

