 literature in the last 20 year [4, 21. The algorithm partitions the data-points into k subsets such that all points in a given subset  X  X elong X  to some center. The algorithm keeps track of the centroids of the subsets, and proceeds in iterations. We denote the set of centroids after the i-th iteration by Cci). Before the first iteration the centroids are initialized to random values. 
The algorithm terminates when Cc X ) and C(i-l) are identical. In each iteration, the following is performed: 1. For each point x, find the center in Cci) which is 2. Compute C(i+l) by taking, for each center, the 
Our algorithms involve modification of just the code within one iteration. We therefore analyze the cost of a single iteration. Naive k-means performs a  X  X earest-center X  query for each of the R points. During such a query the distances in M-space to k centers are calculated. Therefore the cost is O(kMR). 
One fundamental tool we will use to tackle the problem is the kd-tree data-structure. A thorough discussion is out of the scope of this paper. We just outline its relevant properties, and from this point on will assume that a kd-tree for the input points exists. Further details about kd-trees can be found in [7]. 
We will use a specialized version of kd-trees called mrkd-trees, for  X  X ulti-resolution kd-trees X  [3]. Their properties are: l They are binary trees. l Each node contains information about all points For two points x, y we denote by d(x, y) their 
Euclidean distance. For a point x and a hyper-rectangle h we define closest(x, h) to be the point in h which is closest to x. Note that computing closest(x, h) can be done in time O(M) d ue to the following facts: l If x E h, then z is closest. l Otherwise, closest(x, h) is on the boundary of h. We define the distance d(x, h) between a point x and a hyper-rectangle h to be d(x, closest(x, h)). For a hyper-rectangle h we denote by width(h) the vector pax -/pin. 
Given a clustering 4, we denote by 4(x) the centroid this clustering associates with an arbitrary point x (so for k-means, $(x) is simply the center closest to x). We then define a measure of quality for 4: where R is the total number of points and x ranges over all input points. 
The k-means algorithm is known to stop at a local minimum of the distortion measure. It is also known to be too slow for practical databases. Much of the related work does not attempt to confront the algorithmic issues directly. Instead, different methods of subsampling and approximation are proposed. A way to obtain a small  X  X alanced X  sample of points by sampling from the leaves of a R* tree is shown in [5]. In WI, a simulated-annealing approach is suggested to direct the search in the space of possible partitions of the input points. A tree structure with sufficient statistics is presented in [12]. It is used to identify outliers and speed computations. However, the calculated clusters are approximations, and depend on many parameters. 
Our algorithms exploit the fact that instead of updating the centroids point by point, a more efficient approach is to update in bulk. This can be done using the known centers of mass and size of groups of points. Naturally, these groups will correspond to hyper-rectangles in the led-tree. To ensure correctness, we must first make sure that all of the points in a given rectangle indeed  X  X elong X  to a specific center before adding their statistics to it. This gives rise to the notion of an owner. 
We will omit the subscript C where it is clear from the context. The rest of this section discusses owners and efficient ways to find them. We start by analyzing a property of owners, which, by listing those centers Figure 1: Domination with respect to a hyper-rectangle. Liz is the decision line between centers c1 and c2. 
Similarly, Lis is the decision line between c1 and c3. ~12 is the extreme point in h in the direction c2 -cl, and ~13 is the extreme point in h in the direction c3 -cl. 
Since ~12 is on the same side of L12 as cl, c1 dominates c2 with respect to the hyper-rectangle h. Since ~13 is not on the same side of ,513 as cl, c1 does not dominate c3. which do not have it, will help us eliminate non-owners from our set of possibilities. Note that ownerc(h) is not always defined. For example, when two centers are both inside a rectangle, then there exists no unique owner for this rectangle. Therefore the precondition of the following theorem is that there exists a unique owner. 
The algorithmic consequence is that our method will not always find an owner, and will sometimes be forced to descend the Icd-tree, thereby splitting the hyper-rectangle in hope to find an owner for the smaller hyper-rectangle. 
Proof: Assume, for the purpose of contradiction, that c # argmin,tec d(c X , h) E c X . Then there exists a point in h (namely closest(c X , h)) which is closer to c X  than to c. A contradiction to the definition of c as owner(h). 0 
Equivalently, we can say that when looking for owner(h), we should only consider centers with shortest (as opposed to  X  X inimal X ) distance d(c, h). Suppose that two (or more) centers share the minimal distance to h. Then neither can claim to be an owner. 
Theorem 2 narrows down the number of possible own-ers to either one (if there exists a shortest distance cen-ter) or zero (otherwise). In the latter case, our algo-rithm will proceed by splitting the hyper-rectangle. In the former case, we still have to check if this candidate is an owner of the hyper-rectangle in question. As will become clear from the following discussion, this will not always be the case. Let us begin by defining a restricted form of ownership, where just two centers are involved. 
Definition 3 Given a hyper-rectangle h, and two cen-c1 dominates c2 with respect to h if every point in h is 
Observe that if some c E C dominates all other centers with respect to some h, then c = owner(h). A possible (albeit inefficient) way of finding owner(h) if one exists would be to scan all possible pairs of centers. 
However, using theorem 2, we can reduce the number of pairs to scan since c1 is fixed. To prove this approach feasible we need to show that the domination decision problem can be solved efficiently. rectangle h such that d(c X , h) &lt; d(c2, h), the decision problem  X  X oes c1 dominate c2 with respect to h? X  can 
Proof: Observe the decision line L composed of all points which are equidistant to c1 and c2 (see Figure 1). 
If c1 and h are both fully contained in one half-space defined by L, then c1 dominates c2. The converse is also true; if there exists a point x E h such that it is not in the same half-space of L as cl, then d(c X ,x) &gt; d(c2,x) and c1 does not dominate c2. It is left to show that finding whether c1 and h are contained in the same half-space of L can be done efficiently. Consider the vector v X  5 c2 -cl. Let p be a point in h which maximizes the value of the inner product (v,p). This is the extreme point in h in the direction v X . Note that v X  is perpendicular to L. If p is closer to c1 than it is to c2, then so is any point in h (p is the closest one can get to L, within h). If not, p is a proof that c1 does not dominate c2. 
Furthermore, the linear program  X  X aximize (21, p) such that p E h X  can be solved in time O(M). Again we notice the extreme point is a corner of h. For each coordinate i, we choose pi to be hpax if c X  &gt; ci, and hpin otherwise. 0 I 
We now describe a procedure to update the centroids in Cci). It will take into consideration an additional parameter, a hyper-rectangle h such that all points in h affect the new centroids. The procedure is recursive, with the initial value of h being the hyper-rectangle with all of the input points in it. If the procedure can find owner(h), it updates its counters using the center of mass and number of points which are stored in the Ld-node corresponding to h (we will frequently interchange h with the corresponding kd-node). Otherwise, it splits h by recursively calling itself with the children of h. The proof of correctness follows from the discussion above. 1. If h is a leaf: For each data point in h, find 2. Compute d(c, h) for all centers c. If there exists one 3. Call Update( hl , C). 4. Call UpdaFe( h, , C). 
We would not expect our Update procedure to prune in the case that h is the universal set of all input points (since all centers are contained in it, and therefore no shortest-distance center exists). We also notice that if the hyper-rectangles were split again and again so that the procedure is dealing just with leaves, this method would be identical to the original k-means. In fact, this implementation will be much more expensive because of the redundant overhead. Therefore our hope is that large enough hyper-rectangles will be owned by a single center to make this approach worthwhile. See Figure 2 for a visualization of the procedure operation. 
Our next algorithm is a refinement of the simple algorithm. The idea is to identify those centers which will definitely not be owners of the hyper-rectangle h. 
If we can show this is true for some center c, there is no point in checking c for any of the descendants of h, hence the term  X  X lacklisting X . Let c1 be a minimal-distance center to h, and let c2 be any center such that d(c2, h) &gt; d(cl, h). If c1 dominates c2 with respect to h, we have two possibilities. One, that c1 = owner(h). 
This is the good case since we do not need any more computation. The other option is that we have no owner for this node. The slow algorithm would have given up at this point and restarted a computation for the children of h. The blacklisting version notices that c1 dominates c2 with respect to h X  for any h X  contained in h. This is true by definition. Now, since the Figure 2 : Visualization of the hyper-rectangles owned by centers. The entire two-dimensional dataset is drawn as points in the plane. All points that  X  X elong X  to a specific center are colored the same color (here, K=2). The rectangles for which it was possible to prove that belong to specific centers are also drawn. Points outside of rectangles had to be determined in the slow method (by scanning each center). Points within rectangles were not considered by the algorithm. Instead, their number and center of mass are stored together with the rectangle and are used to update the center coordinates. descendants of h in the kd-tree are all contained in h, we can eliminate c2 from the Iist of possible centers at this point for all descendants. Thus the list of prospective owners shrinks until it reaches a size of 1. At this point we declare the only remaining center the owner of the current node h. Again, we hope this happens before h is a leaf node, otherwise our overhead is wasted. As an added bonus, the  X  X wnership X  property can help accelerate other computations. With the small price of storing, in each kd-node, the sum of the squared norms of all points of this node, one can use the exact same algorithm to compute the distortion measure defined in Equation 1. We omit the straightforward algebra. For other obtainable statistics see [12]. 4 Experimental Results We have conducted experiments on both real and randomly-generated data. The real data is preliminary SDSS data with some 400,000 celestial objects. The synthetic data covers a wide range of parameters that might affect the performance of the algorithms. Some of the measures are comparative, and measure the performance of our algorithms against both the naive algorithm and BIRCH [12]. Others simply test our fast algorithms X  behavior on different inputs. Due to space constraints, the details are omitted. The interested reader may find them at [ll]. 
Another way to accelerate clustering is to prune the search when only small error is likely to be incurred. 
We do this by not descending down the k&amp;tree when a  X  X mall-error X  criterion holds for a specific node. We then assume that the points of this node are divided evenly among all current competitors. For each such competing center c, we update its location as if the relative number of points are all located at closest (c, h). 
Our pruning criterion is: where n denotes the number of points in h, U is the  X  X niversal X  hyper-rectangle bounding all of the input points, i is the iteration number, and d is a constant, typically set to 0.8. For experimental results for this heuristic, see [ll]. 
The main message of this paper is that the well-known k-means algorithm need not be considered an impractically slow algorithm, even with many millions of records. We have described, analyzed and given empirical results for a new fast implementation of means. We have shown how a kd-tree of all the datapoints, decorated with extra statistics, can be traversed with a new, extremely cheap, pruning test at each node. Another new technique-blacklisting-gives a many-fold additional speed-up, both in theory and empirically. 
For datasets too large to fit in-core, the same traversal and black-listing approaches could be applied to an on-disk structure such as an R-tree, permitting tractable, exact k-means even for many billions of records. 
This method performs badly in high (&gt; 8) dimen-sions: it is not a clustering panacea, but possibly a wor-thy problem-specific tool for domains in which there is massively large amounts of low-dimensional data (e.g. astrophysics, geospatial data, and controls). We are also investigating whether AD-trees [9] could be used to give similar speed-ups on categorical data: an ad-vantage of AD-trees is that, subject to many caveats, they remain efficient up to hundreds of dimensions. 
Unlike previous approaches (eg., mrkd-trees for EM [S]) this new algorithm scales very well with the number of centers, permitting clustering with tens of thousands of centers. Why would we care about making exact k-means fast? Why not just use a fast non-k-means approximate clusterer? First, exact k-means is a well-established algorithm that has prospered for many years as a clustering algorithm workhorse. Second, it is often used to help find starting clusters for more sophisticated iterative methods such as mixture models. 
Third, running k-means on an in-memory sample of the points is a popular approximate clustering algorithm for monstrously large datasets. The techniques in this paper can make such preprocessing steps efficient. 
Finally, with fast k-means, we can afford to run the algorithm many times in the time it would usually take to run it once. This allows automatic selection of subsets of attributes upon which to cluster, to become a tractable, real-time operation. PI [f31 
