 Yiping Ke  X  James Cheng  X  Wilfred Ng Abstract Quantitative association rule (QAR) mining has been recognized an influential research problem over the last decade due to the popularity of quantitative databases and the usefulness of association rules in real life. Unlike boolean association rules (BARs), which only consider boolean attributes, QARs consist of quantitative attributes which contain much propose an information-theoretic approach to avoid unrewarding combinations of both the information between the attributes in a quantitative database and devise a normalization on the mutual information to make it applicable in the context of QAR mining. To indicate the strong informative relationships among the attributes, we construct a mutual information graph (MI graph), whose edges are attribute pairs that have normalized mutual information no less than a predefined information threshold. We find that the cliques in the MI graph represent a majority of the frequent itemsets. We also show that frequent itemsets that do not form a clique in the MI graph are those whose attributes are not informatively correlated significantly reduces the number of value intervals of the attribute sets to be joined during the mining process. Extensive experiments show that our algorithm speeds up the mining process by up to two orders of magnitude. Most importantly, we are able to obtain most of the high-confidence QARs, whereas the QARs that are not returned by MIC are shown to be less interesting.
 Keywords Quantitative databases  X  Association rules  X  Information-theoretic approach  X  Mutual information 1 Introduction ciation relationships among sets of attributes in business and scientific domains. In a QAR, tes. An example QAR in an employee database is { age[25, 40], gender[female] }  X  { salary[13500, 18700] }( supp = 0.03, conf = 0.8). The QAR states that  X 3% ( support ) of the employees are females aged between 25 and 40 and earning a salary of between $13 , 500 and $18 , 700 X , while  X 80% ( confidence ) of the female employees aged between 25 and 40 are earning a salary of between $13 , 500 and $18 , 700 X .

The problem of QAR mining [ 28 ] is: given a database, a minimum support threshold and a minimum confidence threshold , find all QARs with support and confidence no less than the given thresholds.

Due to the popularity of quantitative databases and the usefulness of association rules in QARs over the last decade. A common approach to the QAR mining problem is to transform of a quantitative or categorical attribute, the pair attribute , value is mapped to a boolean attribute, and then algorithms for mining BARs are applied. However, in many cases, the
Mining QARs by a generic BAR mining algorithm, however, is infeasible in most cases for the following reasons. First, QAR mining suffers from the same problem of a combinatorial in a QAR mining problem may not be as large as that in a BAR mining problem. However, the consecutive intervals. When we join the attributes in the mining process, the number of itemsets (i.e., a set of attribute , interval pairs) can become prohibitively large if the a QAR mining problem that a quantitative attribute has 200 intervals; however, there are ( 200  X  ( 200 + 1 )/ 2 ) 2 = 404 , 010 , 000 different combinations of intervals if we join two such attributes, which is equivalent to 404,010,000 candidate attribute sets in a BAR mining problem. This number further grows exponentially when more than two attributes are joined. As a result, effective techniques to prune the large search space of QAR mining are necessary in order to develop an efficient algorithm for the problem.

In this paper, we adopt an information-theoretic approach to address the two combinatorial explosions in QAR mining by investigating the relationships between the attributes. We first define an interaction graph to formally represent the relationships between the attributes in the mining problem. The vertices of the interaction graph correspond to the attributes in the mining problem, while an edge represents a pair of attributes appearing in the same QAR. Thus, the set of attributes that compose a QAR forms a clique (i.e., a complete subgraph) in the interaction graph.

We introduce a framework, called MIC (which stands for mutual information and clique), to mine the set of QARs. The MIC framework has three phases. The first phase prepares the the mutual information between each pair of attributes. Then, we propose a normalization on if their normalized mutual information is no less than a predefined minimum information that have strong informative relationships. We show that the MI graph can retain all or most facilitate the computation of frequent itemsets as well as to guide the generation of QARs.
Utilizing the cliques in the MI graph greatly alleviates both the combinatorial explosions of attribute sets and intervals in the QAR mining problem. Instead of joining the intervals a clique in the MI graph. Therefore, both the number of attribute sets and their intervals to be joined are significantly reduced. Moreover, the attributes in a clique of the MI graph are strongly informatively related as measured by normalized mutual information, thereby ensuring the quality of the QARs obtained.
 Our contribution. We study an information-theoretic approach that addresses the problem of QAR mining. Since the mutual information is able to capture the inherent co-occurrence relationships between the attributes, it is a good indicator for frequent itemsets and hence relationships between the attributes. Our extensive experiments show that compared with the state-of-the-art QAR mining algorithm [ 28 ], MIC speeds up the mining process by up to two orders of magnitude on both synthetic and real datasets. Most importantly, MIC obtains most of the QARs that have high confidence. We also show that the QARs that are not returned by MIC are insignificant by a formal measure [ 6 ] of interestingness for association rules. Organization. We give some preliminaries on QAR mining in Sect. 2 . We then introduce the MIC framework and describe the technical details in each phase of the framework. We conclude our paper in Sect. 7 . 2 Preliminaries In this section, we present the notions and basic concepts in the QAR mining problem. 1  X  j  X  m .An item , denoted as x [ l x , u x ] , is an attribute x associated with an interval [ l , u x ] ,where x  X  I and l x , u x  X  dom ( x ). We have l x = u x if x is categorical and l  X  u x if x is quantitative. An itemset is a non-empty set of items with distinct attributes. x [ A transaction T is a sequence v 1 ,v 2 ,...,v m ,where v j  X  dom ( x j ), for 1  X  j  X  m . apredefined minimum support threshold .

A quantitative association rule (QAR), r , is an implication of the form X  X  Y ,where X The support of r is defined as supp ( X  X  Y ). The confidence of r is defined as conf ( r )= given that T supports X .
 Problem description. Given a database D ,a minimum support threshold  X  (0  X   X   X  1), and a minimum confidence threshold c (0  X  c  X  1), the QAR mining problem is to find all the QARs with support and confidence no less than  X  and c , respectively.

Note that boolean association rules (BARs) [ 2 ] are a special case of QARs, where all the attributes are categorical attributes with boolean values.
 Example 1 Ta b l e 1 shows an employee database having ten transactions. I ={ age , gender , salary , education , service years } , among which age , salary and service years are quantitative attributes. An example item is age [ 25 , 30 ] .And Given  X  = 0 . 3and c = 0 . 6, age [ 25 , 30 ] X  gender [ M , M ] is a QAR since supp ( age [ 25 , 30 ] gender [ M , M ] ) = 0 . 3  X   X  and conf ( age [ 25 , 30 ] X  gender [ M , M ] ) = 3 Interaction graph In this section, we define an interaction graph to model the set of QARs obtained by QAR mining. Given a QAR mining problem P ,the interaction graph is defined as an undirected E set of all QARs in P . Thus, the interaction graph is a graph representation of Rules ( P ) . satisfies the minimum support threshold. Then, we can restore all the QARs based on the frequent itemsets.

The interaction graph represents the relationships between attributes in a QAR mining problem. Thus, if we can obtain the interaction graph prior to performing QAR mining, we can restrict the search space to a much smaller one that encompasses all QARs. More specifically, sets to produce the QARs. We show that most of the relationships of the attributes reflected in the interaction graph can be acquired by establishing a mutual information graph in the next section. 4 The MIC framework In this section, we introduce a framework, called MIC , for mining QARs. The MIC framework seamlessly incorporates the mutual information concept from information theory [ 27 ]into the context of QAR mining. We first give an overall description of the framework and then elaborate on the techniques in each phase. 4.1 Overall description There are three main phases in the MIC framework, as shown in Fig. 1 .  X  Phase I: Discretization. The domain of each quantitative attribute is partitioned into a  X  Phase II: MI graph construction. Based on the discretized database obtained in Phase I,  X  Phase III: Clique computation and QAR generation. We utilize the cliques in G MI
Here, we briefly introduce Srikant and Agrawal X  X  Mining approach [ 28 ] (denoted as SAM ), compare the performance of our MIC to that of SAM.

We can view the QAR mining problem at two conceptual levels: the attribute level that of the attributes. SAM directly operates on the interval level throughout the entire mining process. In other words, the pruning by the a priori property is performed on the intervals performance improvement, because once the attribute set is pruned, none of the intervals associated with the attribute set is considered in the subsequent mining process. However, will miss all frequent itemsets and QARs that are generated from the attribute set. MIC applies the concept of mutual information to perform pruning at the attribute level. Mutual information captures the informative relationships between the attributes, which have intervals are also pruned. Meanwhile, MIC also performs pruning at the interval level using the a priori property as does SAM. Thus, the search space of MIC is significantly smaller than that of SAM.
 The pruning at the attribute level in MIC may miss some QARs in the mining result. However, as evidenced by our experiments, MIC obtains most of the QARs that are of high confidence and we also show that the missing QARs are of very low interest [ 6 ], because the attributes in the same QAR are informatively related to each other. 4.2 Phase I: discretization This phase is a preprocessing step in the mining process. The purpose of discretization is to to deal with the continuous domain and to speed up the mining process. In this phase, the such that the order of the base intervals is preserved. During the mining process, each base integers. Thus, the raw values of the attributes are transparent to the mining algorithm in subsequent phases.

The discretization phase is a common preprocessing method in the QAR mining problem the database can be prepared according to the domain knowledge and then it passes through Phases II and III of the MIC framework to mine the QARs. However, in most cases, the domain knowledge is hard to obtain, which is the situation we consider in this paper. While a detailed discussion of discretization is not the focus of this paper, we remark that any discretization technique can be applied to this phase of the MIC framework. Here, we limit our discussion to the equidepth discretization technique used in SAM [ 28 ], which we com-pare with our approach. The equidepth discretization technique is proved to minimize the information loss caused by discretization in Srikant and Agrawal [ 28 ]. Equidepth partitions in each base interval is roughly the same. Note that the discretization is an information-information loss but the higher the computational cost to mine QARs. A smaller n results in more information loss. The following example helps to illustrate the idea of equidepth discretization. Example 2 Given the employee database in Table 1 and using the equidepth discretization method, the quantitative attributes, age , salary and service years , are discretized employee database is shown in Table 7 . 4.3 Phase II: mutual information graph construction In this section, we discuss in detail how we apply the concepts of entropy and mutual infor-mation that originates from information theory [ 27 ] in the context of QAR mining. 4.3.1 Entropy and mutual information Notation Let x and y be two random variables. Given v x  X  dom ( x )and v y  X  dom ( y ), we denote the probability parameters as follows:  X  p (v x ) : the probability of x taking the value v x . ( x [ v x ,v x ] y [ v y ,v y ] ) and p (v y | v x ) = conf ( x [ v x ,v x ] X  y [ v y ,v y ] ) . uncertainty in a random variable. Entropy and mutual information are closely related and we use entropy to interpret many of the fundamental properties of mutual information and to elaborate the semantics of normalized mutual information. The entropy of a random variable x , denoted as H ( x ) ,isdefinedas The conditional entropy of a random variable y given another variable x , denoted as H ( y | x ) ,isdefinedas We use the following example to illustrate the application of entropy in the context of QAR mining.
 p ( H ( education ) = 1 . 97. Thus, the attribute education exhibits a greater degree of uncertainty than gender ,since H ( education )&gt; H ( gender ) . Intuitively, we can say that we are more certain about the value of a gender instance than that of an education instance.

We can also compute H ( gender | education ) = 0, which indicates that given education , there is no uncertainty in gender . This may not be true in reality; howe-ver, in our designated database as shown in Table 1 , given the education of an employee, we can determine his/her gender. In contrast, we cannot determine education given gender since H ( education | gender ) = 1 . 09 &gt; 0, although we are now more certain about education as indicated by H ( education | gender )&lt; H ( education ) .
 Mutual information. Mutual information describes how much information one random variable tells about another one. The mutual information of two random variables x and y , denoted as I ( x ; y ) ,isdefinedas An important interpretation of mutual information comes from the following property. Property 1 I ( x ; y ) = H ( x )  X  H ( x | y ) = H ( y )  X  H ( y | x ) .

From Property 1 , the information that y tells us about x is the reduction in uncertainty greater the value of I ( x ; y ) , the more information x and y tell about each other. Example 4 Consider the discretized database in Table 7 .ByEq.( 3 ), we have I ( gender ; can verify that I ( gender ; education ) = H ( gender )  X  H ( gender | education ) = 0 . 88  X  0 = 0 . 88. Since we know for certain the value of gender given the value of education , the information that education tells us about gender is just the infor-mation that gender itself carries. We can also verify that I ( gender ; education ) = H ( education )  X  H ( education | gender ) = 1 . 97  X  1 . 09 = 0 . 88, which shows that the knowledge of gender causes a reduction of 1 . 09 in the uncertainty about education .
We first explore some properties of mutual information that are used to develop a norma-lization for mutual information. Detailed proof of the properties can be found in [ 10 ]. Property 2 I ( x ; y ) = I ( y ; x ) .

Property 2 suggests that mutual information is symmetric , which means that the amount of information x tells about y is the same as that y tells about x .
 Property 3 I ( x ; x ) = H ( x ) .

Property 3 states that the mutual information of a random variable x with itself is the entropy of x . Thus, the entropy is also called self-information .
 Property 4 I ( x ; y )  X  0.
 p each other.
 Property 5 I ( x ; y )  X  H ( x ) and I ( x ; y )  X  H ( y ) .

Property 5 gives the upper bound for mutual information. 4.3.2 Normalized mutual information Let M beameasureusedtoevaluate the strongness of the relationship between two attributes in a QAR mining problem. Given a predefined threshold  X  ,if M  X   X  ,wesaythatthetwo attributes are strongly related to each other; otherwise, we say that they are not strongly related. Ideally, M is a measure being able to identify attributes that do not constitute any candidate frequent itemsets in the mining process.

Defining M as the mutual information between the attributes seems to be an ideal approach because mutual information, by definition, naturally measures the information that one at-tribute tells about another. For two attributes appearing in the same QAR, the strongness of their relationship is reflected by their mutual information.
 bounded by the minimum of their entropy. Since the entropy of different attributes varies tes. For example, if we set  X  = 1inExample 4 , we will not join gender with education tion between gender and any other attributes, which is locally maximum. Therefore, it is very likely that joining gender and education will produce some frequent itemsets. But if  X  is smaller, we may include some pairs of attributes that do not constitute any significant QARs. They are included just because their mutual information is globally large compared
Second, Property 4 states that the mutual information of two attributes is a non-negative value, while a greater value indicates more information one attribute tells about the other. However, there is no unified scale for the mutual information measure. Thus, the threshold  X  is a problem since we cannot tell how strong the relationship between the attributes is. For example, if we set the minimum confidence threshold at 0.9, we know that the QARs obtained are of high quality. However, if we set  X  at 0.9, we do not know how much information the number  X 0.9 X  amounts to unless it is mapped to a unified scale.

To tackle the above-mentioned problems, we propose a normalization for mutual infor-mation.
 Definition 1 ( Normalized Mutual Information ) The normalized mutual information of two attributes x and y , denoted as I ( x ; y ) ,isdefinedas
Our idea is to normalize the mutual information between x and y by the maximal value of we can get rid of the localness and make the normalized mutual information a global measure. Now, we present some useful properties of the normalized mutual information.
 Property 6 I ( x ; y ) = I ( y ; x ) if I ( x ; x ) = I ( y ; y ) .
 I ( y ; x ) . Hence, if I ( x ; x ) = I ( y ; y ) ,then I ( x ; y ) = I ( y ; x ) .
Property 6 shows that, unlike mutual information, normalized mutual information is not symmetric.
 Property 7 0  X  I ( x ; y )  X  1.
 I ( x ; y )  X  1.

This property ensures that the value of normalized mutual information falls within the unit interval [ 0 , 1 ] .
 Property 8 I ( x ; y ) = H ( x )  X  H ( x | y ) H ( x ) .

Property 8 suggests the semantics of the normalized mutual information between x and y ,whichis the percentage of reduction in uncertainty about x due to the knowledge of y .
Thus, normalized mutual information gives the threshold  X  an intuitive meaning and makes it relatively independent of specific attributes. Now the threshold  X  indicates the minimum percentage of reduction in uncertainty about an attribute due to the knowledge of another attribute. We further illustrate this important point by the following example. Example 5 In Example 4 , when we say that the knowledge of gender causes a reduction of 1.09 in the uncertainty about education , we have little idea how much a reduction of 1.09 is. Now, we compute the normalized mutual information I ( education ; gender ) = I ( age ; service years ) = I( age ; service years ) H( age ) = 0.90 1.57 = 0 . 57.
We note that I ( gender ; education )&lt; I ( age ; service years ) but I ( gender ; education ) &gt; I ( age ; service years ) . This means that the percentage of uncertainty reduction of gender due to the knowledge of education is higher than that of age due to the knowledge of service years , although the mutual information of the former information.

We list the values of mutual information and normalized mutual information for all the 4.3.3 Mutual information graph construction x ,havea strong informative relationship with each other if I ( x i ; x j )  X   X  .
Given a QAR mining problem, we construct a Mutual Information graph ( MI graph ), which is a directed graph, G MI = ( V MI , E MI ) , where the set of vertices V MI = I and and represents the strong informative relationships between the attributes in a QAR mining problem. Example 6 Given the employee database in Table 7 and  X  = 0 . 5, we construct the corre-sponding G MI as shown in Fig. 2 a. For example, the attribute pair ( age , service years ) the knowledge of service years . In other words, if we know the value of service years , we can infer the value of age with a higher accuracy.
 of [ 0 , 1 ] , according to the user X  X  requirement of the strongness of the relationship between the attributes. One way to set the value of  X  , without any domain knowledge, is based on the density of the MI graph. The graph density is defined as the number of edges in the graph divided by the number of edges in the corresponding complete graph. We first specify a graph density d for the MI graph. Then, we set  X  to be the normalized mutual information database in Table 7 . Since there are five attributes, the corresponding complete graph has (5  X  5  X  5 = 20) edges. (Self-loops are not considered in the MI graph because the antecedent and the consequent of a QAR are disjoint.) We first compute all the values of normalized sort these values in descending order. If we specify the density d of the MI graph to be 20%, the derived MI graph has ( 20  X  20% = 4 ) edges. Therefore,  X  is set to be the fourth largest value of the normalized mutual information in the sorted list. 4.4 Phase III: clique computation and QAR generation In this final phase of MIC, we find all the cliques in G MI and simultaneously compute the set of frequent itemsets based on the cliques. We then generate the QARs from the frequent itemsets. 4.4.1 Clique computation and frequent itemset generation edges in G MI and consider its corresponding undirected graph  X  G MI . Figure 2 bshows  X  G MI that corresponds to G MI in Fig. 2 a.

As we have discussed in Sect. 3 , the attributes in a QAR (as well as in a frequent itemset) strong informative relationships, we can obtain most of the attribute sets that potentially considered to generate frequent itemsets. Meanwhile, we also check the support condition of the itemsets to make sure that they are frequent.

We compute all the cliques in  X  G MI and generate frequent itemsets using a prefix tree structure. Given  X  G MI , we construct a prefix tree level by level as follows. of the root at Level 1. Each node at Level 1 is labeled with the corresponding attribute name and is attached with a set of intervals whose support is no less than  X  . Consecutive base intervals are combined and also attached to the node as long as the support of the combined intervals are no less than  X  . However, the larger the range of a combined interval, the less specific is the meaning of the interval. For example, the interval [1,100] for the attribute age is trivial. To avoid the occurrence of too general combined intervals, a maximum support threshold  X  m [ 28 ] is specified as an upper bound of the support of a combined interval. In this way, the intervals are combined as long as their support is no greater than  X  m .
Algorithm 1 describes CliqueMine ( u ) , which recursively computes all the cliques con-in  X 
G MI (Line 3). If the edge exists, we create a new node w that has the same label as v and insert w into the tree as a child of u (Line 4). Note that each node is attached with a the set of frequent itemsets attached with u to release the memory (Line 9). Then, we call CliqueMine recursively for each child of u , until the tree cannot be further expanded (Lines 10 X 14).
 Algorithm 1 CliqueMine ( u )
In the prefix tree constructed by Algorithm 1 , each path from a child of the root at Level of k nodes. We prove this observation in the following lemma.
 Lemma 1 Let u 1 be a node at Level 1 in the prefix tree and u k a node at Level k. A path nodes in the k-clique.
 Proof We prove the lemma by induction on k . a 2-clique. u ( u k , u k + 1 ) exists in u a ( path P k + 1 .

It then follows directly from Lemma 1 that the attribute set of each node at Level k is represent a path u 1 ,..., u k in the prefix tree. This is because due to the checking of the itemset produced for the corresponding attribute set, even though there is a corresponding clique in  X  G MI .

We use the following example to illustrate how the computation of frequent itemsets can be guided by enumerating the cliques in  X  G MI .
 Example 7 Let  X  = 0 . 3and  X  m = 0 . 6. Figure 3 shows the prefix tree that we construct from the  X  G MI shown in Fig. 2 b. Each solid rectangle represents a node labeled with an attribute name, while each node in the prefix tree except the root node is associated with a set of intervals, which are the intervals of frequent itemsets. The intervals are shown in a dashed rectangle attached to the node. In Fig. 3 , we only show the intervals of three nodes paths in the prefix tree represent the cliques in  X  G MI .

We demonstrate the execution of Algorithm 1 on the subtree rooted at the node gender , which is the second child of the root. We begin with the first right sibling of gender ,that is, the node salary . Since the edge ( gender , salary )existsin  X  G MI , we create a new node labeled salary and add it as the first child of gender .
 Then, we join the set of intervals attached with gender and that attached with salary . The set of intervals attached with gender is { ([1,1]: 0.7) , ([2,2]: 0.3) }and that attached with salary is { ([1,1]: 0.3) , ([1,2]: 0.6) , ([2,2]: 0.3) , corresponding itemset. Note that the intervals [1,1] and [2,2] of salary are combined join of gender and salary produces five frequent 2-itemsets. Since these five 2-itemsets havethesameattributeset,{ gender , salary }, we attach their intervals, ([1,1][1,2]: 3) , ([1,1][2,2]:3) , ([1,1][3,3]:4) , ([2,2][1,1]:3) and ([2,2][1,2]: 3) with the child node salary of gender . Similarly, we create the node education as the second child of gender , with the set of intervals, { ([1,1][3,3]:3) , ([2,2] [1,1]:3) }, that are obtained by joining the intervals of gender and education .
We proceed to the next level and process the children of gender . Since there is an edge between salary and its right sibling education in  X  G MI , we create a new node labeled education as a child of salary . Note that the path gender , salary , education represents the 3-clique { gender , salary , education }in  X  G MI . We then perform the join on the intervals of salary and education at Level 2 and generate two frequent 3-itemsets.

In a similar way, we follow the clique enumeration process to generate all other frequent itemsets.

By enumerating the cliques in  X  G MI with a prefix tree structure, we limit the search space Without using the normalized mutual information concept, the search space is equivalent to a space is drastically reduced.

It is known that the complexity of enumerating all cliques in a graph is NP-complete [ 9 ]. However, we emphasize that utilizing the cliques in  X  G MI does not mean to solve the NP-complete problem. Instead, we seamlessly incorporate the clique enumeration into the computation of frequent itemsets, such that the only extra processing incurred on the com-exists in  X  G MI (as shown in Line 3 of Algorithm 1 ), which is a trivial operation.
We adopt diffset [ 34 ] on the prefix tree, so that we only scan the database twice: one for transaction IDs). All other frequent itemsets are then computed using the diffsets. We also remark that the first scan of the database also computes the normalized mutual information between the attributes. 4.4.2 QAR generation After the set of frequent itemsets is derived, we simply map each frequent itemset into a boolean itemset. Then, the algorithm for BAR generation in Agrawal and Srikant [ 3 ] can be trivially applied to generate the QARs. 4.5 Theoretical bounds for QARs In this section, we first study the theoretical bounds on the confidence of QARs for a given frequent itemset and the minimum information threshold. Then, we introduce the measure of interest as to further assess the quality of QARs. We further provide the theoretical bounds on the interest of QARs. 4.5.1 Theoretical bounds for the confidence of QARs We formalize the connections between the normalized mutual information, and the support and confidence of QARs. The significance of our result is twofold. First, we guarantee that any pair of attributes pruned by normalized mutual information cannot form a QAR with a confidence greater than the derived bound. Second, we ensure that the attributes retained in the MI graph generate QARs with confidence greater than the given bound.

Given two attributes x and y ,welet n x and n y denote the number of distinct values of x and y , respectively.
 ( y [ v y ,v y ] X  x [ v x ,v x ] ) has (a) an upper bound if I ( x ; y )&lt; X  , and (b) a lower bound if I ( x ; y )  X   X  . x [ v x ( x ) &gt;( 1  X   X ) . We start by deriving an upper bound for H numerator, leading to the inequality of i = 1&amp; j = 1 p (v x i ,v y j )  X  log p thedenominator,wehave p (v x i )  X  ( 1  X  p (v x 1 )) whenever i = 1.
  X  . Thus, it follows that: Finally, since we have H ( x | y ) H ( x ) &gt;( 1  X   X ) , it follows that So, we have the following upper bound for conf ( y [ v y 1 ,v y 1 ] X  x [ v x 1 ,v x 1 ] ) :
If we allow a looser upper bound, the above expression can be further simplified as follows: (i.e., p (v x 1 | v y 1 ) ) has a lower bound. Similar to the proof in Part(a), we first derive a lower bound for H ( x | y ) H ( x ) .
Equation ( 7 ) holds, since we apply the log sum inequality for the second term in the the conditional probability falls within the range [0,1].

If we allow a looser lower bound, the above expression can be further simplified as follows:
The following corollary shows that Theorem 1 can be generalized to the itemsets with intervals instead of single values.
 ( y [ I ( x ; y )  X   X  .
 Proof It directly follows from Theorem 1 , since the derived equations are based on proba-can simply sum up the probabilities of the composite values of a given interval to obtain the same bounds.
 The next corollary shows that Theorem 1 can also be generalized to the QARs.
 where Z is an itemset, we have conf ( y [ l y , u y ] X  x [ l x , u x ] Z )&lt; c. Proof By the definition of the confidence of a rule, we have the following expression:
Corollary 2 is important, since it shows that if the confidence of a rule has an upper bound, the confidence of all the rules formed by augmenting more items in the consequent of the rule also have the same upper bound. Therefore, the upper bound derived in the proof of Theorem 1 is not only limited to the rule having one single item in both antecedent and consequent, but also generally holds for the rules that have more items in the consequent. 4.5.2 Theoretical bounds for the interest of QARs statistical definition of dependence on X and Y , given as follows:
The range of the interest of an association rule is from 0 to  X  . Interest values above 1 indicate positive dependence, while values below 1 indicate negative dependence. An interest value of 1 implies that X and Y are independent, while the further the value is from 1, the greater is the positive or negative dependence between X and Y .

Similar to the results of Sect. 4.5.1 , we formalize the connections between the normalized mutual information, and the support and interest of QARs.
 Theorem 2 Let x [ v x ,v x ] y [ v y ,v y ] be a frequent itemset. Then, the interest, interest ( y [ v y ,v y ] X  x [ v x ,v x ] ) , has (a) an upper bound if I ( x ; y )&lt; X  , and (b) a lower bound if I ( x ; y )  X   X  .
 Proof To establish the result of Part (a), we refer to the proof of Theorem 1 .ByEq.( 6 ), we have
Therefore, it follows that
If we allow a looser upper bound, the above expression can be further simplified as follows: Similarly, in order to prove Part (b), by Eq. ( 7 ), we have
Therefore, it follows that
If we allow a looser lower bound, the above expression can be further simplified as follows:
The results in Theorem 2 can be further generalized to itemsets with intervals, as shown in the following corollary. We skip the proof since it is similar to that of Corollary 1 . ( y [ I ( x ; y )  X   X  .

The following corollary describes the connection between the confidence and the interest of a QAR.
 then interest ( y [ l y , u y ] X  x [ l x , u x ] )  X  c  X  Proof By the definition of the interest of a rule, we have the following expressions: a rule also has a lower bound that is related to the bound of confidence. Because of this connection, we can simply specify a confidence threshold for mining QARs, while we still of QARs, as what we are going to show in Sect. 5 . 4.6 Discussions on the interestingness of missing QARs The NMI measure, however, computes the dependency relationship between two attributes on the whole set of transactions and takes into account all values in the attribute domain. As a result, the NMI pruning may eliminate some QARs that are interesting locally within a small set of transactions (i.e., the QARs have low support values). This problem can also be seen from Eq. ( 8 ) in Theorem 2 :when  X  decreases, the upper bound of the interest of the missing QARs increases.

A possible solution to this problem is to allow the user to specify a maximum interest Eq. ( 8 ), we can derive a lower bound for the value of  X  as follows:
The above bound provides a useful reference for setting  X  in terms of  X  ,  X  m and  X  .Inthis way, we can avoid missing the QARs with interest higher than  X  by setting a suitable  X  . 5 Experimental evaluation We evaluate the performance of our MIC framework on both synthetic and real datasets. quality of the mined QARs. Recall that MIC operates on an MI graph that captures the strong informative relationships between the attributes, while SAM operates on the complete graph make a fair comparison, we test SAM by inputting a complete graph into our program, so that the performance improvement is indeed only due to the pruning as a result of using the MI graph. Thus, the SAM used in our experiment is not the a priori-like algorithm proposed the equidepth discretization with the number of base intervals n calculated by an equation to minimize the information loss, we also apply the same discretization in MIC. The equation partial completeness level. We choose K = 1 . 5 in the experiments as suggested in SAM. After generating all the frequent itemsets, we apply the rule generation algorithm in [ 3 ]to obtain the QARs. All the experiments are run on an XP machine with a 3.0 GHz Intel P4 and 2GB RAM. 5.1 The interest measure itemsets produced by MIC is a subset of that produced by SAM. Consequently, the set of QARs generated by MIC is also a subset of that generated by SAM. However, we emphasize that our method is not an approximation technique that improves the efficiency at the expense of accuracy. Instead, we show that MIC not only significantly outperforms SAM, but the rules we obtain are also of higher quality than that obtained by SAM, as measured using interest [ 6 ], which is a well-established measure for the interestingness of an association rule.
In particular, we show that the missing QARs , i.e., QARs that are missed by MIC but retur-ned by SAM, are rules whose attributes are of low dependency on each other. For example, Thus, it just happens to be the case that whenever we have x [ 1 , 1 ] ,wearelikelytohave y [ 1 , 1 ] as well. Such rules are not generated by MIC, because these attributes have very low normalized mutual information and are hence excluded from the MI graph.
 In our experiments, we first use support and confidence to obtain the high-confidence QARs. Then, we compute the mean and variance of the interest of the missing QARs. The maximum interest of missing QARs is also presented. We justify that most of the missing when computing their mean, variance and maximum. 5.2 Datasets and parameters In this section, we introduce the datasets and the parameters we are going to study in the subsequent subsections.
 We use both synthetic and real datasets to justify the effectiveness and efficiency of MIC. The synthetic datasets are generated by the IBM Quest Synthetic Data Generator [ 14 ]. We modify their code to generate three extra boolean attributes, using Functions 1 X 3 described datasets of sizes from 100 K to 1,000 K transactions as a scalability test for MIC. The four real datasets we test are chosen from the commonly used UCI machine learning repository datasets. The number of quantitative attributes of each dataset is given in the brackets.
In the following subsection, we first study the effect of  X  in the MIC framework. Mean-while, we also demonstrate the scalability of MIC by varying the size of synthetic datasets. Then, we study the effect of minimum support threshold  X  on real datasets.
 5.3 Experimental results 5.3.1 Experiments on synthetic datasets graph at 20, 15 and 10%, respectively. For each dataset, we generate four sets of QARs, at the minimum confidence threshold c = 0 . 7, c = 0 . 8, c = 0 . 9and c = 1, respectively.
Figure 4 a shows the running time for generating the frequent itemsets. For all the three values of  X  , MIC runs significantly faster than SAM. While the running time of SAM increases ( a ) (b) (c) ( d ) (e) linearly when the size of the dataset increases, the running time of MIC remains relatively stable. When the density of the MI graph decreases from 20 to 10% (i.e., the value of  X  increases), the running time of MIC decreases only slightly. The decrease in the running time is because the size of the MI graph is smaller for larger  X  and hence the search space pruned is also larger. However, the decrease in running time is small because the difference in the MI graphs of the three respective  X  is small. More specifically, the MI graph computed on the dataset with size of 1,000 k at density 15% only has three more edges than that at density 10%. These three edges consist of at least one categorical attribute which has only two distinct values. Thus, the number of itemsets that are produced from these three edges is also small.

Figure 4 b shows the ratio of the number of QARs obtained by MIC at density 15% to that obtained by SAM. On average, MIC obtains 80% of QARs that have a confidence over 0 . 7, while it obtains almost all QARs that have a confidence of 1. Most importantly, we show in Fig. 4 c,d that the mean of the interest of the missing QARs is approximately 1 in all cases, with a very small variance of less than 0.001. Figure 4 e further shows that the maximum that the attributes composing a missing QAR are independent of each other. 5.3.2 Experiments on real datasets to  X  . Therefore, when  X  increases from 0 . 1to0 . 3, the number of base intervals decreases maximum values of  X  m at which SAM does not run out of memory. We test three values of  X  by ensuring the density of the MI graph at 20, 15 and 10%, respectively. The values of  X  c = 0 . 9and c = 1, respectively.

Figure 5 a shows that MIC computes the frequent itemsets approximately two orders of magnitude faster than does SAM on the covtype dataset when the graph density is lower than 15%. When the graph density is 20%, MIC becomes slower but still significantly outperforms SAM. The dramatic improvement is because many of the quantitative attributes of this dataset have a large domain. MIC is able to remove the edges between those attributes that do not have a strong informative relationship, thereby drastically reducing the number of intervals to be combined. We also show in Fig. 5 b that only in the case when  X  = 0 . 1, MIC misses a MIC obtains exactly the same set of QARs as does SAM. We thus omit the figures for the interest measures due to the negligible number of missing QARs.
 ( a ) (b)
We notice an unexpected, slight increase in the running time of MIC when  X  becomes larger in Fig. 5 a, when the graph density is 15 and 10%. This is because in QAR mining, the number of frequent itemsets is also determined by  X  m , since a greater  X  m implies that more intervals can be combined to generate more frequent itemsets. Thus, the number of itemsets generated at  X  = 0 . 1and  X  m = 0 . 13 can be smaller than that generated at  X  = 0 . 15 and  X  at  X  = 0 . 1and  X  m = 0 . 13 can be less than that at  X  = 0 . 15 and  X  m = 0 . 18. Without using the MI graph, most of the time is spent on joining the unpromising intervals and the smaller the  X  , the more the time used. However, the MI graph of covtype at density lower than 15% almost prunes all irrelevant search space and thus the time spent on joining the unpromising intervals becomes insignificant. As for the graph density of 20%, the MI graph still consists of some edges that involve attributes with weak relationship. Thus, the running time of MIC MI graph can indeed capture the strong informative relationships between the attributes. Figure 5 a also reports the effect of the number of base intervals on the performance of MIC and SAM. When the number of base intervals increases, i.e.,  X  decreases, the running time of SAM increases rapidly, while the running time of MIC remains relatively stable at all values of  X  . This is because larger number of base intervals aggravates the problem of combinatorial explosions of attribute intervals. Without pruning the irrelevant search space, the performance of SAM is severely degraded by the increase in the number of base intervals. On the contrary, the performance of MIC is almost not affected by the increase in the number of base intervals since the generation of unpromising itemsets is avoided by the effective pruning.

Figure 6 a shows that MIC is on average six times faster than SAM on the letter-recognition dataset when computing the frequent itemsets. All the quantitative attributes in this dataset of number of base intervals but majorally by  X  m . This explains the abnormal trend of SAM in with low mutual dependency. Figure 6 b shows that the set of QARs obtained by MIC is over 90% of that obtained by SAM, except when  X  = 0 . 25, the percentage is slightly lower. This ( a ) (b) is because the number of QARs at  X  = 0 . 25 is the smallest among all values of  X  .Weomit the figures for the interest measures since the number of missing rules is small.
Figure 7 a shows the running time of MIC and SAM on the ann-thyroid dataset. MIC computes the frequent itemsets up to three orders of magnitude faster than SAM. We are not able to obtain the results of SAM and MIC-20% at  X  = 0 . 1 since they run out of memory due to the large number of base intervals.
 Figure 7 b shows that the set of QARs obtained by MIC is less than 1% of that obtained by SAM. This result is because SAM generates a prohibitively large number of QARs (up to 1 billion and consumes over 50 GB of space). By capturing the strong informative relationships of attributes, MIC produces a reasonable number of interesting QARs (about 60 K). Moreover, Fig. 7 c X  X  show that the missing QARs are indeed uninteresting, since the mean and the maximum interest are almost 1 and the variance of the interest is 0 in all cases. Figure 8 a shows the running time of MIC and SAM on the yeast dataset. On average, MIC computes the frequent itemsets four times faster than SAM. The improvement is not as The MI graph of a small dataset does not reflect the relationships between the attributes as good as does the MI graph of a large dataset, because larger datasets are statistically more stable.

Figure 8 b shows that the set of QARs obtained by MIC at the density of 15% is only about 20 X 50% of that obtained by SAM. However, Fig. 8 c,d show that the mean and variance of the interest of the missing QARs are 1 and 0 in all cases. Figure 8 e shows that the maximum interest of the missing QARs is 1 in all cases, except when  X  = 0 . 1, MIC misses four QARs with interest around 4.6. Thus, the results once again show that the QARs missed by MIC are of low interest. 5.4 Summary of experiments Since MIC outperforms SAM in all the experiments, we conclude that utilizing normalized mutual information indeed enables us to effectively reduce the number of attributes to be results reveal that the improvement of MIC for small datasets is not as significant as that for large datasets, for most QAR mining problems in practice, the datasets are large and their attributes have a large domain. MIC achieves remarkable performance on such datasets, as verified by the experiments on the large synthetic datasets and on the large real dataset covtype .

Another important finding is that the QARs returned by SAM but missed by MIC mostly have an interest value of 1, i.e., the attributes composing the missing QARs are independent on each other. Thus, in addition to the improvement in efficiency, the set of QARs mined by MIC is also of higher quality than that mined by SAM. ( a ) (b) (c) ( d ) (e) 6 Related work attribute in both the antecedent and the consequent of a QAR. Srikant and Agrawal [ 28 ] We are also aware of mining four variants of association rules in quantitative databases. tains certain uninstantiated attributes and the mining problem is to determine values for the uninstantiated attributes such that one measure (e.g., support, confidence or gain) is ma-ximized and another measure satisfies a predefined threshold. Inspired by the problem of image segmentation in computer vision, Fukuda et al. [ 11 ] propose a geometric method to compute the optimized region for association rules. However, the rules they produce are in Fukuda et al. [ 11 ] by allowing disjunctions over an arbitrary number of uninstantiated attributes. Another novel approach is proposed to use genetic algorithms to mine optimized interval for a quantitative attribute. However, their approach does not guarantee to produce and complete optimized rules. A multi-objective genetic algorithm based method is proposed based on a genetic algorithm to achieve optimizing both the support and confidence of a rule. Mining optimal association rules tackles a different problem from ours. It focuses on finding the optimal values of certain given attributes instead of mining general QARs without any constraint on the attributes.
 statistical measure (e.g., mean, variance) or an aggregate (e.g., min, max) of a quantitative than giving the interval information of the attributes, which is more detailed and intuitive.
The third type proposes a new representation of QARs based on half-spaces [ 25 ]. The antecedent and the consequent of a rule are a weighted sum of the attributes tested against a threshold. As a result, this type of rule is very complex and more suitable to scientific analyses.

The fourth type is privacy-preserving QARs [ 8 , 15 ] proposed recently. The problem is to mine QARs without revealing the private information of parties who share distributed data. Therefore, the mining algorithm mainly focuses on secure computation instead of the efficiency.
 We are also aware of different applications of normalized mutual information in literature. two attributes x and y is defined as 2  X  I ( x ; y ) max possible cluster labels and B represents possible category labels. Normalized mutual infor-tern requires all attributes in the pattern be strongly correlated, while normalized mutual information in this paper is directional because a QAR is an implication of the antecedent on the consequent. We emphasize that the work in this paper and the work in Ke et al. [ 18 ] are complementary to each other, since correlated patterns are in fact originally proposed as strongly correlated items, the contribution of this paper is to obtain the implication of one quantitative pattern on the high probability of the occurrence of another pattern, which is different from the scope of Ke et al. [ 18 ] 7 Conclusions In this paper, we present an MIC framework that adopts an information-theoretic approach to mine QARs. We propose the concept of normalized mutual information and then apply it to discover the informative relationships between the attributes in a QAR mining problem. Based on normalized mutual information, we construct an MI graph that captures the strong the true relationships between the attributes in QARs, we find that the cliques in the MI graph correspond to the potential frequent itemsets in the mining problem. We incorporate the enumeration of the cliques seamlessly into the computation of frequent itemsets. The clique enumeration limits the mining process to a smaller but more relevant search space, thereby significantly improving the mining efficiency. Our experimental results show that MIC speeds up the mining process for up to orders of magnitudes. More importantly, MIC obtains most of the high-confidence QARs, while the QARs that are not returned by MIC are shown to be of little significance based on the interest measure.

As an on-going work, we consider to incorporate the concept of near-clique, which is a clique except for one edge, for computing frequent itemsets into our framework. This may [ 12 ] in the context of QAR mining also deserve attention as a future work.
 References Authors Biography
