 Parsing bilingual texts (bitexts) is crucial for train-ing machine translation systems that rely on syn-tactic structures on either the source side or the target side, or the both (Ding and Palmer, 2005; Nakazawa et al., 2006). Bitexts could provide more information, which is useful in parsing, than a usual monolingual texts that can be called  X  X ilin-gual constraints X , and we expect to obtain more accurate parsing results that can be effectively used in the training of MT systems. With this mo-tivation, there are several studies aiming at highly accurate bitext parsing (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009).
This paper proposes a dependency parsing method, which uses the bilingual constraints that we call bilingual subtree constraints and statistics concerning the constraints estimated from large unlabeled monolingual corpora. Basically, a (can-didate) dependency subtree in a source-language sentence is mapped to a subtree in the correspond-ing target-language sentence by using word align-ment and mapping rules that are automatically learned. The target subtree is verified by check-ing the subtree list that is collected from unla-beled sentences in the target language parsed by a usual monolingual parser. The result is used as additional features for the source side dependency parser. In this paper, our task is to improve the source side parser with the help of the translations on the target side.

Many researchers have investigated the use of bilingual constraints for parsing (Burkett and Klein, 2008; Zhao et al., 2009; Huang et al., 2009). For example, Burkett and Klein (2008) show that parsing with joint models on bitexts im-proves performance on either or both sides. How-ever, their methods require that the training data have tree structures on both sides, which are hard to obtain. Our method only requires dependency annotation on the source side and is much sim-pler and faster. Huang et al. (2009) proposes a method, bilingual-constrained monolingual pars-ing, in which a source-language parser is extended to use the re-ordering of words between two sides X  sentences as additional information. The input of their method is the source trees with their trans-lation on the target side as ours, which is much easier to obtain than trees on both sides. However, their method does not use any tree structures on the target side that might be useful for ambiguity resolution. Our method achieves much greater im-provement because it uses the richer subtree con-straints.

Our approach takes the same input as Huang et al. (2009) and exploits the subtree structure on the target side to provide the bilingual constraints. The subtrees are extracted from large-scale auto-parsed monolingual data on the target side. The main problem to be addressed is mapping words on the source side to the target subtree because there are many to many mappings and reordering problems that often occur in translation (Koehn et al., 2003). We use an automatic way for generat-ing mapping rules to solve the problems. Based on the mapping rules, we design a set of features for parsing models. The basic idea is as follows: if the words form a subtree on one side, their corre-sponding words on the another side will also prob-ably form a subtree.
 Experiments on the translated portion of the Chinese Treebank (Xue et al., 2002; Bies et al., 2007) show that our system outperforms state-of-the-art monolingual parsers by 2.93 points for Chi-nese and 1.64 points for English. The results also show that our system provides higher accuracies than the parser of Huang et al. (2009).
 The rest of the paper is organized as follows: Section 2 introduces the motivation of our idea. Section 3 introduces the background of depen-dency parsing. Section 4 proposes an approach of constructing bilingual subtree constraints. Sec-tion 5 explains the experimental results. Finally, in Section 6 we draw conclusions and discuss future work. In this section, we use an example to show the idea of using the bilingual subtree constraints to improve parsing performance.

Suppose that we have an input sentence pair as shown in Figure 1, where the source sentence is in English, the target is in Chinese, the dashed undi-rected links are word alignment links, and the di-rected links between words indicate that they have a (candidate) dependency relation.

In the English side, it is difficult for a parser to determine the head of word  X  X ith X  because there is a PP-attachment problem. However, in Chinese it is unambiguous. Therefore, we can use the in-formation on the Chinese side to help disambigua-tion.

There are two candidates  X  X te X  and  X  X eat X  to be the head of  X  X ith X  as the dashed directed links in Figure 1 show. By adding  X  X ork X , we have two possible dependency relations,  X  X eat-with-fork X  and  X  X te-with-fork X , to be verified.

First, we check the possible relation of  X  X eat X ,  X  X ith X , and  X  X ork X . We obtain their corresponding words  X   X  (meat) X ,  X   X  (use) X , and  X   X  X  X  (fork) X  in Chinese via the word alignment links. We ver-ify that the corresponding words form a subtree by looking up a subtree list in Chinese (described in Section 4.1). But we can not find a subtree for them.

Next, we check the possible relation of  X  X te X ,  X  X ith X , and  X  X ork X . We obtain their correspond-ing words  X   X  (ate) X ,  X   X  (use) X , and  X   X  X  X  (fork) X . Then we verify that the words form a subtree by looking up the subtree list. This time we can find the subtree as shown in Figure 2.
Finally, the parser may assign  X  X te X  to be the head of  X  X ith X  based on the verification results. This simple example shows how to use the subtree information on the target side. For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008; Nivre and Kubler, 2006): transition-based (Nivre, 2003; Yamada and Matsumoto, 2003) and graph-based (McDonald et al., 2005; Carreras, 2007). Our approach can be applied to both parsing mod-els.

In this paper, we employ the graph-based MST parsing model proposed by McDonald and Pereira (2006), which is an extension of the projec-tive parsing algorithm of Eisner (1996). To use richer second-order information, we also imple-ment parent-child-grandchild features (Carreras, 2007) in the MST parsing algorithm. 3.1 Parsing with monolingual features Figure 3 shows an example of dependency pars-ing. In the graph-based parsing model, features are represented for all the possible relations on single edges (two words) or adjacent edges (three words). The parsing algorithm chooses the tree with the highest score in a bottom-up fashion.

In our systems, the monolingual features in-clude the first-and second-order features pre-sented in (McDonald et al., 2005; McDonald and Pereira, 2006) and the parent-child-grandchild features used in (Carreras, 2007). We call the parser with the monolingual features monolingual parser. 3.2 Parsing with bilingual features In this paper, we parse source sentences with the help of their translations. A set of bilingual fea-tures are designed for the parsing model. 3.2.1 Bilingual subtree features We design bilingual subtree features, as described in Section 4, based on the constraints between the source subtrees and the target subtrees that are ver-ified by the subtree list on the target side. The source subtrees are from the possible dependency relations. 3.2.2 Bilingual reordering feature Huang et al. (2009) propose features based on reordering between languages for a shift-reduce parser. They define the features based on word-alignment information to verify that the corre-sponding words form a contiguous span for resolv-ing shift-reduce conflicts. We also implement sim-ilar features in our system. In this section, we propose an approach that uses the bilingual subtree constraints to help parse source sentences that have translations on the tar-get side.

We use large-scale auto-parsed data to obtain subtrees on the target side. Then we generate the mapping rules to map the source subtrees onto the extracted target subtrees. Finally, we design the bilingual subtree features based on the mapping rules for the parsing model. These features in-dicate the information of the constraints between bilingual subtrees, that are called bilingual subtree constraints. 4.1 Subtree extraction Chen et al. (2009) propose a simple method to ex-tract subtrees from large-scale monolingual data and use them as features to improve monolingual parsing. Following their method, we parse large unannotated data with a monolingual parser and obtain a set of subtrees ( ST t ) in the target lan-guage.

We encode the subtrees into string format that is expressed as st = w : hid (  X  w : hid )+ 1 , where w refers to a word in the subtree and hid refers to the word ID of the word X  X  head ( hid =0 means that this word is the root of a subtree). Here, word ID refers to the ID (starting from 1) of a word in the subtree (words are ordered based on the positions of the original sentence). For example,  X  X e X  and  X  X te X  have a left dependency arc in the sentence shown in Figure 3. The subtree is encoded as  X  X e:2-ate:0 X . There is also a parent-child-grandchild re-lation among  X  X te X ,  X  X ith X , and  X  X ork X . So the subtree is encoded as  X  X te:0-with:1-fork:2 X . If a subtree contains two nodes, we call it a bigram-subtree. If a subtree contains three nodes, we call it a trigram-subtree.

From the dependency tree of Figure 3, we ob-tain the subtrees, as shown in Figure 4 and Figure 5. Figure 4 shows the extracted bigram-subtrees and Figure 5 shows the extracted trigram-subtrees. After extraction, we obtain a set of subtrees. We remove the subtrees occurring only once in the data. Following Chen et al. (2009), we also group the subtrees into different sets based on their fre-quencies. 4.2 Mapping rules To provide bilingual subtree constraints, we need to find the characteristics of subtree mapping for the two given languages. However, subtree map-ping is not easy. There are two main problems: MtoN (words) mapping and reordering, which of-ten occur in translation. MtoN (words) map-ping means that a source subtree with M words is mapped onto a target subtree with N words. For example, 2to3 means that a source bigram-subtree is mapped onto a target trigram-subtree.

Due to the limitations of the parsing algo-rithm (McDonald and Pereira, 2006; Carreras, 2007), we only use bigram-and trigram-subtrees in our approach. We generate the mapping rules for the 2to2, 2to3, 3to3, and 3to2 cases. For trigram-subtrees, we only consider the parent-child-grandchild type. As for the use of other types of trigram-subtrees, we leave it for future work.

We first show the MtoN and reordering prob-lems by using an example in Chinese-English translation. Then we propose a method to auto-matically generate mapping rules. 4.2.1 Reordering and MtoN mapping in Both Chinese and English are classified as SVO languages because verbs precede objects in simple sentences. However, Chinese has many character-istics of such SOV languages as Japanese. The typical cases are listed below: 1) Prepositional phrases modifying a verb pre-cede the verb. Figure 6 shows an example. In En-glish the prepositional phrase  X  X t the ceremony X  follows the verb  X  X aid X , while its corresponding prepositional phrase  X   X  (NULL)  X  X  (ceremony)  X  (at) X  precedes the verb  X   X  (say) X  in Chinese. Figure 6: Example for prepositional phrases mod-ifying a verb 2) Relative clauses precede head noun. Fig-ure 7 shows an example. In Chinese the relative clause  X   X  X  X  (today)  X  X  X  (signed) X  precedes the head noun  X   X  X  X  (project) X , while its correspond-ing clause  X  X igned today X  follows the head noun  X  X rojects X  in English.
 Figure 7: Example for relative clauses preceding the head noun 3) Genitive constructions precede head noun. For example,  X   X  X  X  (car)  X  X  (wheel) X  can be translated as  X  X he wheel of the car X . 4) Postposition in many constructions rather than prepositions. For example,  X   X  X  X  (table)  X  (on) X  can be translated as  X  X n the table X .
We can find the MtoN mapping problem occur-ring in the above cases. For example, in Figure 6, trigram-subtree  X   X  (NULL):3- X  (at):1- X  (say):0 X  is mapped onto bigram-subtree  X  X aid:0-at:1 X .
Since asking linguists to define the mapping rules is very expensive, we propose a simple method to easily obtain the mapping rules. 4.2.2 Bilingual subtree mapping To solve the mapping problems, we use a bilingual corpus, which includes sentence pairs, to automat-ically generate the mapping rules. First, the sen-tence pairs are parsed by monolingual parsers on both sides. Then we perform word alignment us-ing a word-level aligner (Liang et al., 2006; DeN-ero and Klein, 2007). Figure 8 shows an example of a processed sentence pair that has tree structures on both sides and word alignment links.
 Figure 8: Example of auto-parsed bilingual sen-tence pair
From these sentence pairs, we obtain subtree pairs. First, we extract a subtree ( st s ) from a source sentence. Then through word alignment links, we obtain the corresponding words of the words of st s . Because of the MtoN problem, some words lack of corresponding words in the target sentence. Here, our approach requires that at least two words of st s have corresponding words and nouns and verbs need corresponding words. If not, it fails to find a subtree pair for st s . If the corre-sponding words form a subtree ( st t ) in the target sentence, st s and st t are a subtree pair. We also keep the word alignment information in the tar-get subtree. For example, we extract subtree  X   X   X  (society):2- X  X  X  (fringe):0 X  on the Chinese side and get its corresponding subtree  X  X ringes(W 2):0-of:1-society(W 1):2 X  on the English side, where W 1 means that the target word is aligned to the first word of the source subtree, and W 2 means that the target word is aligned to the second word of the source subtree. That is, we have a sub-tree pair:  X   X  X  X  (society):2- X  X  X  (fringe):0 X  and  X  X ringe(W 2):0-of:1-society(W 1):2 X .

The extracted subtree pairs indicate the trans-lation characteristics between Chinese and En-glish. For example, the pair  X   X  X  X  (society):2- X   X  (fringe):0 X  and  X  X ringes:0-of:1-society:2 X  is a case where  X  X enitive constructions pre-cede/follow the head noun X . 4.2.3 Generalized mapping rules To increase the mapping coverage, we general-ize the mapping rules from the extracted sub-tree pairs by using the following procedure. The rules are divided by  X  = &gt;  X  into two parts: source (left) and target (right). The source part is from the source subtree and the target part is from the target subtree. For the source part, we replace nouns and verbs using their POS tags (coarse grained tags). For the target part, we use the word alignment information to rep-resent the target words that have correspond-ing source words. For example, we have the subtree pair:  X   X  X  X  (society):2- X  X  X  (fringe):0 X  and  X  X ringes(W 2):0-of:1-society(W 1):2 X , where  X  X f X  does not have a corresponding word, the POS tag of  X   X  X  X  (society) X  is N, and the POS tag of  X   X  X  X  (fringe) X  is N. The source part of the rule becomes  X  X :2-N:0 X  and the target part becomes  X  X  2:0-of:1-W 1:2 X .

Table 1 shows the top five mapping rules of all four types ordered by their frequencies, where W 1 means that the target word is aligned to the first word of the source subtree, W 2 means that the target word is aligned to the second word, and W 3 means that the target word is aligned to the third word. We remove the rules that occur less than three times. Finally, we obtain 9,134 rules for 2to2, 5,335 for 2to3, 7,450 for 3to3, and 1,244 for 3to2 from our data. After experiments with dif-ferent threshold settings on the development data sets, we use the top 20 rules for each type in our experiments.

The generalized mapping rules might generate incorrect target subtrees. However, as described in Section 4.3.1, the generated subtrees are verified by looking up list ST t before they are used in the parsing models. 4.3 Bilingual subtree features Informally, if the words form a subtree on the source side, then the corresponding words on the target side will also probably form a subtree. For
Table 1: Top five mapping rules of 2to3 and 3to2 example, in Figure 8, words  X   X  X  X  (they) X  and  X   X  X  X  (be on) X  form a subtree , which is mapped onto the words  X  X hey X  and  X  X re X  on the target side. These two target words form a subtree. We now develop this idea as bilingual subtree features.
In the parsing process, we build relations for two or three words on the source side. The con-ditions of generating bilingual subtree features are that at least two of these source words must have corresponding words on the target side and nouns and verbs must have corresponding words.

At first, we have a possible dependency relation (represented as a source subtree) of words to be verified. Then we obtain the corresponding target subtree based on the mapping rules. Finally, we verify that the target subtree is included in ST t . If yes, we activate a positive feature to encourage the dependency relation.
We consider four types of features based on 2to2, 3to3, 3to2, and 2to3 mappings. In the 2to2, 3to3, and 3to2 cases, the target subtrees do not add new words. We represent features in a direct way. For the 2to3 case, we represent features using a different strategy. 4.3.1 Features for 2to2, 3to3, and 3to2 We design the features based on the mapping rules of 2to2, 3to3, and 3to2. For example, we design features for a 3to2 case from Figure 9. The possible relation to be verified forms source subtree  X   X  X  X  (signed)/VV:2- X  (NULL)/DEC:3- X  X  X  (project)/NN:0 X  in which  X   X  X  X  (project) X  is aligned to  X  X rojects X  and  X   X  X  X  (signed) X  is aligned to  X  X igned X  as shown in Figure 9. The procedure of generating the features is shown in Figure 10. We explain Steps (1), (2), (3), and (4) as follows: Figure 10: Example of feature generation for 3to2 case (1) Generate source part from the source subtree. We obtain  X  X :2- X  /DEC:3-N:0 X  from  X   X   X  (signed)/VV:2- X  (NULL)/DEC:3- X   X  (project)/NN:0 X . (2) Obtain target parts based on the matched mapping rules, whose source parts equal  X  X :2- X  /DEC:3-N:0 X . The matched rules are  X  X :2- X  /DEC:3-N:0 = &gt; W 3:0-W 1:1 X  and  X  X :2- X  /DEC:3-N:0 = &gt; W 3:2-W 1:0 X . Thus, we have two target parts  X  X  3:0-W 1:1 X  and  X  X  3:2-W 1:0 X . (3) Generate possible subtrees by consider-ing the dependency relation indicated in the target parts. We generate a possible subtree  X  X rojects:0-signed:1 X  from the target part  X  X  3:0-W 1:1 X , where  X  X rojects X  is aligned to  X   X   X  (project)(W 3) X  and  X  X igned X  is aligned to  X   X   X  (signed)(W 1) X . We also generate another pos-sible subtree  X  X rojects:2-signed:0 X  from  X  X  3:2-W 1:0 X . (4) Verify that at least one of the generated possible subtrees is a target subtree, which is in-cluded in ST t . If yes, we activate this feature. In the figure,  X  X rojects:0-signed:1 X  is a target subtree in ST t . So we activate the feature  X 3to2:YES X  to encourage dependency relations among  X   X   X  (signed) X ,  X   X  (NULL) X , and  X   X  X  X  (project) X . 4.3.2 Features for 2to3 In the 2to3 case, a new word is added on the target side. The first two steps are identical as those in the previous section. For example, a source part  X  X :2-N:0 X  is generated from  X   X  X  X  (car)/NN:2- X   X  (wheel)/NN:0 X . Then we obtain target parts such as  X  X  2:0-of/IN:1-W 1:2 X ,  X  X  2:0-in/IN:1-W 1:2 X , and so on, according to the matched map-ping rules.

The third step is different. In the target parts, there is an added word. We first check if the added word is in the span of the corresponding words, which can be obtained through word alignment links. We can find that  X  X f X  is in the span  X  X heel of the car X , which is the span of the corresponding words of  X   X  X  X  (car)/NN:2- X  X  (wheel)/NN:0 X . Then we choose the target part  X  X  2:0-of/IN:1-W 1:2 X  to generate a possible subtree. Finally, we verify that the subtree is a target subtree in-cluded in ST t . If yes, we say feature  X 2to3:YES X  to encourage a dependency relation between  X   X   X  (car) X  and  X   X  X  (wheel) X . 4.4 Source subtree features Chen et al. (2009) shows that the source sub-tree features ( F src  X  st ) significantly improve per-formance. The subtrees are obtained from the auto-parsed data on the source side. Then they are used to verify the possible dependency relations among source words.

In our approach, we also use the same source subtree features described in Chen et al. (2009). So the possible dependency relations are verified by the source and target subtrees. Combining two types of features together provides strong discrim-ination power. If both types of features are ac-tive, building relations is very likely among source words. If both are inactive, this is a strong negative signal for their relations. All the bilingual data were taken from the trans-lated portion of the Chinese Treebank (CTB) (Xue et al., 2002; Bies et al., 2007), articles 1-325 of CTB, which have English translations with gold-standard parse trees. We used the tool  X  X enn2Malt X  2 to convert the data into dependency structures. Following the study of Huang et al. (2009), we used the same split of this data: 1-270 for training, 301-325 for development, and 271-300 for test. Note that some sentence pairs were removed because they are not one-to-one aligned at the sentence level (Burkett and Klein, 2008; Huang et al., 2009). Word alignments were gen-erated from the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007) trained on a bilin-gual corpus having approximately 0.8M sentence pairs. We removed notoriously bad links in { a, an, the } X {  X  (DE),  X  (LE) } following the work of Huang et al. (2009).
 For Chinese unannotated data, we used the XIN CMN portion of Chinese Gigaword Version 2.0 (LDC2009T14) (Huang, 2009), which has ap-proximately 311 million words whose segmenta-tion and POS tags are given. To avoid unfair com-parison, we excluded the sentences of the CTB data from the Gigaword data. We discarded the an-notations because there are differences in annota-tion policy between CTB and this corpus. We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data. For English unannotated data, we used the BLLIP corpus that contains about 43 million words of WSJ text. The POS tags were assigned by the MXPOST tagger trained on training data. Then we used the Base-line Parser to parse all the sentences in the data.
We reported the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of to-kens (excluding all punctuation tokens) with cor-rect HEADs. 5.1 Main results The results on the Chinese-source side are shown in Table 2, where  X  X aseline X  refers to the systems with monolingual features,  X  X aseline2 X  refers to adding the reordering features to the Baseline,  X  F
BI  X  refers to adding all the bilingual subtree features to  X  X aseline2 X ,  X  F src  X  st  X  refers to the monolingual parsing systems with source subtree features,  X  X rder-1 X  refers to the first-order mod-els, and  X  X rder-2 X  refers to the second-order mod-els. The results showed that the reordering fea-tures yielded an improvement of 0.53 and 0.58 points (UAS) for the first-and second-order mod-els respectively. Then we added four types of bilingual constraint features one by one to  X  X ase-line2 X . Note that the features based on 3to2 and 3to3 can not be applied to the first-order models, because they only consider single dependencies (bigram). That is, in the first model, F BI only in-cludes the features based on 2to2 and 2to3. The results showed that the systems performed better and better. In total, we obtained an absolute im-provement of 0.88 points (UAS) for the first-order model and 1.36 points for the second-order model by adding all the bilingual subtree features. Fi-nally, the system with all the features (OURS) out-performed the Baseline by an absolute improve-ment of 3.12 points for the first-order model and 2.93 points for the second-order model. The im-provements of the final systems (OURS) were sig-nificant in McNemar X  X  Test ( p &lt; 10  X  4 ). Table 2: Dependency parsing results of Chinese-source case
We also conducted experiments on the English-source side. Table 3 shows the results, where ab-breviations are the same as in Table 2. As in the Chinese experiments, the parsers with bilingual subtree features outperformed the Baselines. Fi-nally, the systems (OURS) with all the features outperformed the Baselines by 1.30 points for the first-order model and 1.64 for the second-order model. The improvements of the final systems (OURS) were significant in McNemar X  X  Test ( p &lt; 10 Table 3: Dependency parsing results of English-source case 5.2 Comparative results Table 4 shows the performance of the system we compared, where Huang2009 refers to the result of Huang et al. (2009). The results showed that our system performed better than Huang2009. Com-pared with the approach of Huang et al. (2009), our approach used additional large-scale auto-parsed data. We did not compare our system with the joint model of Burkett and Klein (2008) be-cause they reported the results on phrase struc-tures.
 We presented an approach using large automati-cally parsed monolingual data to provide bilingual subtree constraints to improve bitexts parsing. Our approach remains the efficiency of monolingual parsing and exploits the subtree structure on the target side. The experimental results show that the proposed approach is simple yet still provides sig-nificant improvements over the baselines in pars-ing accuracy. The results also show that our sys-tems outperform the system of previous work on the same data.

There are many ways in which this research could be continued. First, we may attempt to ap-ply the bilingual subtree constraints to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). Here, we may design new fea-tures for the models. Second, we may apply the proposed method for other language pairs such as Japanese-English and Chinese-Japanese. Third, larger unannotated data can be used to improve the performance further.

