 We propose NCE, an efficient algorithm to identify and ex-tract relevant content from news webpages. We define rele-vant as the textual sections that more objectively describe the main event in the article. This includes the title and the main body section, and excludes comments about the story and presentation elements.

Our experiments suggest that NCE is competitive, in terms of extraction quality, with the best methods available in the literature. It achieves F 1 = 90 . 7% in our test corpus con-taining 324 news webpages from 22 sites. The main advan-tages of our method are its simplicity and its computational performance. It is at least an order of magnitude faster than methods that use visual features. This characteristic is very suitable for applications that process a large number of pages.
 Categories and Subject Descriptors: H.3.m [Informa-tion Storage and Retrieval]: Miscellaneous General Terms: Algorithms, Experiments.
The evolution of web browser technology in the past decade has caused a large growth in the number of presentation el-ements on HTML web pages. Today, web content in most popular sites exploit these browser features, such as dynamic navigational menus, flashing logos, a multitude of ad blocks,  X 
Partially supported by CNPQ, (proc 470654/2007-4 and proc 304786/2006-3) and by FAPERJ (proc E-26/100.505/2007).
 rich headers, footers, and so on. This diversity and com-plexity of design options contributed to the dissemination of HTML publishing systems that hide these visual or auxiliary blocks from content authors by the use of layout templates. In fact, Gibson et al [6] estimate that layout presentation ele-ments constitute 40% to 50% of all internet content and that this volume has been increasing approximately 6% yearly.
One undesirable effect of this growth in presentation ele-ments is the increased difficulty in extracting relevant con-tent for information storage and retrieval applications. For example, tokens, phrases, and named-entities coming from advertising sections and footnotes become indistinguishable from those coming from the title and body sections of a news article. In fact, a common solution implemented in some extractors is to render the page, much like a browser would for human consumption, and exploit the derived vi-sual features during the extraction process [8, 14, 4]. While this approach usually provides good results, it comes at a high computational cost.

In this work, we are interested in developing an efficient algorithm to identify and extract relevant content from news webpages. We define relevant as the textual sections that more objectively describe the main event in the article. This includes the title and the main body section, and excludes comments about the story and presentation elements.
Our motivation is two-fold. First, we would like to apply our method to cluster and detect near-duplicate news stories as we crawl a high volume of webpages that contain them. Since our particular application relies on the analysis of the main event in a story, we see as beneficial to eliminate both layout and off-topic elements that are part of the same page. Second, because of the large volume of up-to-date articles we wish to maintain in our search index, we put a premium on both execution speed and extraction accuracy. One can then apply vectorization, then clustering algorithms only against the remaining on-topic textual elements.
 Our Contributions. Our main contribution is the News Content Extractor (NCE) heuristic, a fast and simple method to extract relevant content from news webpages. We under-stand that our method constitutes an interesting contribu-tion to this line of research due to the following reasons: Related Work. The problem of automatically extracting relevant information from web documents has been widely studied in the last years. For a survey of this area, we refer to [7] and the tutorial in [10]. For interesting domain inde-pendent approaches we refer to [12, 11, 3]. Since the design of a very general and effective method is a quite complex task, many works focus on specific domains.

News webpages have been an attractive domain to test extraction techniques [5, 9, 8, 14, 4]: first, because news are one of the main sources of up-to-date information on the Web and, second, because their content are constantly changing which makes it very difficult to manually handle them.
 In [14], Zheng et. al. propose a visual wrapper (V-Wrapper) that uses visual and other features from a DOM tree to train a machine learning model to classify each text passage (leaf of a DOM tree) as a title, body or other con-tent. They evaluate this approach through a 3-fold cross validation over a set of 295 pages from 16 sites obtaining F 1 close to 95%. In addition, they compare the performance of V -wrapper with that obtained by a wrapper based on the techniques presented in [9] and conclude that V -wrapper achieves better results. In [8, 4], high F 1 measures are also reported due to methods that employ visual features.
The results of [8, 14, 4] confirm the intuition that visual features are clearly useful to extract relevant blocks from news webpages. The major drawback of this approach is that it needs to render the webpages, which is a computa-tionally expensive task. This issue was also raised in [13], one of the first papers to use visual features for webpage segmentation. We should mention that none of these works report execution times.

In summary, we acknowledge that interesting approaches have been proposed and investigated for the problem of ex-tracting relevant content from news webpages. However, it is difficult to have a precise view on how NCE compares with the available methods in terms of extraction quality. This is mainly due to the lack of a standard metric and evaluation corpora. In terms of computational efficiency, our method seems to be much faster than those that employ visual features. In fact, our experiments with Gecko [2] and MSHTML [1], two well known rendering engines, indicate that the time required to extract basic visual features, as the height and the width of a block, is at least an order of magnitude larger than that required to run NCE .
 Our Metric. For a chunk of text C we use bag ( C ) to denote the bag of words associated with C . We use | bag ( C ) | to denote the number of (not necessarily distinct) elements in a set of words C .
 Let Rel ( D ) be the relevant content of a Web document D . For our purposes, the Recall and Precision of a chunk of text C with respect to D are defined as and the F 1-measure of C is the harmonic mean of Precision ( C,D ) and Recall ( C,D ).
Here, we detail the NCE heuristic. We need to introduce some definition and notations. Our method works over the DOM tree representation of news webpages. For a node u in a DOM tree T , the id of u is the number of nodes visited before u in a depth first search (DFS) over T ; we use T u denote the subtree of T rooted at u that includes u and all of its descendants; given a forest F (set of trees), text ( F ) stands for the text located in the nodes of F ; linkdensity ( F ) is used to denote the link density of a forest F i.e. the number of characters contained in tags of type &lt;a&gt; located given a positive real number  X  , we use F u, X  to denote the forest composed by the subtrees, with link density at most  X  , that are rooted at the children of u . More formally, F u, X  { T v | v is a child of u and linkdensitiy ( T v )  X   X  } . NCE strongly relies on the following assumption:
Assumption 1 (Separator Node Existence). For an arbitrary news webpage P , there exists a node u in the DOM tree associated with P and a positive real number  X  for which the F 1 -measure of the union of text chunks in the forest F is very high.

We observe that 137 out of 141 documents in our explo-ration corpus there exists a node u such that F u, X  provides F 1-measure higher than 90% when  X  = 0 . 4, as an example. In what follows, we explain the three phases of NCE . In our description we use two families of parameters: {  X  and {  X  j } . The values we set for  X   X  X  depend on some sim-ple features of the exploration corpus. To set the values of  X   X  X  we execute a simple optimization procedure over the exploration corpus. Due to space constraints, we defer the description of the procedure employed to set the parame-teres for the extended version of this paper.
 Phase 1: Separator Node Search. In the first phase, motivated by Assumption 1, NCE searches for a separator node in a DOM tree T . For that, it visits the leaves of T following a depth first search until it finds a leaf L , with at least  X  3 chars, for which there is an ancestor u that simul-taneously satisfies the following conditions: (i) u does not correspond to a paragraph tag; (ii) | text ( T u ) |  X   X  has at most one sibling u 0 for which | text ( T u 0 linkdensity ( T u 0 ) &lt;  X  1 ; (iv) u is the closest ancestor of L that simultaneously satisfies conditions (i)-(iii).
If NCE fails to find the desired leaf L then it returns the entire DOM tree. Otherwise, it returns the forest F u, X  4
Condition (i) above is motivated by the fact that the news body usually has many paragraphs so that stopping the search in a paragraph tag may lead to a considerable loss of relevant content. Condition (ii) guarantees that the block retrieved is large enough. Condition (iii) mitigates the risk of losing relevant content by avoiding the search for a sepa-rator node to stop at a node u if it has at least two siblings with some reasonable amount of text. We stop at the first leaf in the DFS because the first large text passage is usually the body of a news article.

By analyzing the results of Phase 1 in our exploration corpus we realize that it fails to remove comments from a reasonable number of documents. This observation moti-vates the introduction of Phase 2.
 Phase 2: Comments Removal. In the second phase, NCE tries to remove comments that may appear in the forest U obtained by the end of the first phase. In order to achieve this goal, NCE takes into account the following assumption:
Assumption 2. Comments are located after the body of a news webpage in a region that has repetitive patterns like dates, permalinks, etc.

In a nutshell, NCE searches for a set of closely located blocks that present similar content in the region of the web-page associated with U . If it succeeds then it discards all content that appear after the first of these blocks because it assumes that such a block demarcates the beginning of the region of comments.

In details, NCE proceeds as follows. 1. It visits the nodes of U through a DFS. If u is the 2. It builds a graph G = ( L,E ) where there is an edge 3. If G does not have a connected component with at
We should mention that in practice the construction of graph G stops as soon as the first connected component with at least 3 nodes is identified, saving a considerable amount of time. For the sake of fairness, we remark that our method faces difficulties if the page under consideration has either one or two comments because in this case we do not expect to find a connected component with at least 3 nodes. Phase 3: Title Search. At the third phase NCE searches for the title of the news since it may not be included in Phase 1. This search is guided by the following observations: for many web pages the title of the news is also the title of a page, commonly available in tag &lt;title&gt; ; the title usually appears before the body in a DFS; many of title words are, in general, included in the body of the news; and the title does not have many words.

More precisely, let U 0 be the forest obtained by NCE at the end of the second phase. NCE visits the nodes of T with id X  X  smaller than the first node of U 0 . In this process, it includes in a list L 0 each node u that is either associ-ated with a tag &lt;title&gt; or that simultaneously satisfies the conditions:  X  3  X | bag ( text ( T u )) | X   X  4 and | bag ( text ( T cess, the subtree rooted at the node of L 0 with the largest amount of text is included in the solution.

We shall mention that some of the ideas of our third phase appear in the last phase of [9].
In order to test our method, we built a test corpus of 324 news webpages (almost) uniformly distributed from 22 sites. These sites are disjoint from those in the training corpus em-ployed to develop our heuristic.
 Extraction Results. Columns 2, 3 and 4 of Table 1 present, respectively, the average recall, precision and F1 attained by NCE for each site. The last column presents for each site the average percentage of relevant content of its webpages or, in other words, the average precision, per site, achieved by a method that returns all the textual content (relevant or not) of a webpage. As an example, Recall = 96 . 1% for bradeton.com is obtained by taking the average of the re-call achieved by NCE for each document of our corpus that belongs to this site.

We observe that NCE achieves a very high recall for al-most all sites. On the other hand, there are some sites for which the precision is not so good as in watfordobserver.com where NCE fails to remove comments. Nevertheless, NCE managed to remove a considerable amount of noise since the average percentage of relevant content in this site increases from 16.9% to 54.3%.

NCE achieves, in average, Recall = 96%, Precision = 88 . 5% and F 1 = 90 . 7%. It is interesting to compare its av-erage precision (88 . 5%) with the average percentage (36 . 6%) of relevant content in the webpages of the corpus. This comparison suggests that the application of our extraction method increases the amount (in percentage) of relevant content by a factor of  X  2 . 5. The price of this significant gain is a rather small loss in terms of relevant content. Table 1: Average results per site: Recall, Precision, and F1 of NCE, F1 of V-Wrapper (V-W). The last column is the percentage of relevant content per site.
We also investigated whether we benefit from executing the second phase of NCE . For 300 out of the 324 news documents there were no significant differences in using the second phase. However, for the remaining 24 documents (  X  8%) of the test corpus, the F 1 achieved by NCE , with the second phase, is clearly superior.

Finally, we visually inspected the quality of results pro-vided by NCE . We have the following observations: (i) NCE loses relevant content located in the news body for less than 4% of the webpages; (ii) NCE retrieves the title for 95% of the news articles; and (iii) NCE managed to remove comments from more than 70% of the documents containing them.

It should be noted that we executed NCE with the pa-rameters X  values obtained through a local search procedure. In fact, we had already set the values of our parameters be-fore building the test corpus.
 Comparison with a Visual Wrapper. As good results have been reported for visual wrappers, we implemented the V-Wrapper proposed in [14] and compared it with NCE. We trained the V-Wrapper using our training corpus, and applied it to our test corpus.

The metric proposed in [14] considers the classification of leaf nodes in the DOM trees into relevant (title, body) or non-relevant. Under this metric, our implementation of V-Wrapper obtains F 1 = 85 . 6% on the classification of all leaf nodes in the test corpus. The results we observed are worse than those reported in [14]. Potential reasons to explain this difference are: (i) in [14], a 3-fold cross-validation is employed, while here we train the V-Wrapper using sites that do not occur in the test corpus (hold-out) (this way we have a fair comparison with NCE); and (ii) the engine used to produce the visual features for the V-Wrapper (Gecko [2]) does not render correctly some pages of our corpus.
The column 5 of Table 1 presents the average F1 obtained by the V-Wrapper for each site, according to our bag of words based metric. The V-Wrapper outperformed NCE only on three sites, which are exactly the three worst sites for NCE with respect to F1 measure. However we believe that there is room for improvement on the V-Wrapper re-sults by a better rendering of our corpus. Our results suggest that: (i) NCE provides competitive results when compared with the V-Wrapper, one of the best extractors available in the literature; and (ii) the visual features employed by the V-Wrapper are useful to remove comments. In fact, com-ments are commonly located away from the beginning of the news webpages. This is the main explanation for the better performance of V-Wrapper on the last 3 sites. Execution Time. We believe that NCE is much faster than methods that require visual features since a rendering phase is avoided. In order to provide some evidence for our belief we use both Gecko and MSHTML, the latter employed in [14], to measure the time required to compute the height and width of the visual blocks corresponding to DOM nodes. We observe that the time spent to compute them  X  a lower bound on the running time of visual wrappers  X  is at least an order of magnitude larger than that required to parse the HTML into a DOM tree and run the three phases of NCE .
