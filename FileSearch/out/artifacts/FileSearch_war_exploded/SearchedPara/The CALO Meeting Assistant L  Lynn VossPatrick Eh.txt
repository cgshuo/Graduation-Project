 Technologies that assist in making meetings more productive have a long history. The latest chapter in that history involves projects that integrate recent advances in speech, natura l language understanding, vision, and multimodal interaction technologies in an effort to produce tools that can perceive what happens at a meeting, extract salient events and in-teractions, and produce a record of the meeting that people can later consult or analyze. 
Research projects such as the ICSI Meeting Pro-ject (Janin et al. 2004) have sought to produce auto-mated and segmented transcripts from natural, multi-party speech as it occurs in meetings. Others, like the ISL Smart Meeting Room Task (Waibel et al. 2003), and the M4 and AMI projects (Nijholt, op den Akker, &amp; Heylen 2005), employ instrumented meeting rooms to collect multiple streams of behav-ior data and analyze the interactions of meeting par-ticipants to produce a rich and flexible record of their meeting activities, while also providing a sup-portive environment for collaboration. 
The CALO Meeting Assistant is similar to the lat-ter in that it collects multiple streams of information about the behaviors of people in meetings, and as-similates speech, movement , and note-taking behav-ior to create a rich representation of the meeting that can be analyzed and reviewed at many levels. How-ever, a primary aim of the CALO Meeting Assistant is to integrate its observations with those of a larger system of agents, which can assess the meeting data it collects in the context of the ongoing projects and workflow in the work lives of each of the meeting participants. Thus, our meeting assistant aims to reach beyond an intelligent room that understands only the activities of people in meetings, and at-tempts to understand their overarching concerns and interpret their behaviors from the perspective of what their meetings mean to them. 
That overarching system of agents is being devel-oped under the DARPA CALO (Cognitive Assistant that Learns and Organizes) Program, which seeks to produce machine learning technology in the form of personalized agents that support high-level knowl-edge workers in carrying out their professional ac-tivities. The CALO system handles a broad range of interrelated decision-making tasks that are tradition-ally resistant to automation; doing so partly by inter-acting with, being advised by, and learning from its users. The CALO system can take initiative on com-pleting routine tasks, and on assisting when the un-expected happens. 
CALO is designed from the ground up as a cogni-tive system. Whereas conventional, hand-coded soft-ware excels at a narrow set of capabilities in a particular domain, cognitive systems maintain ex-plicit, declarative models of their capabilities, ongo-ing activities, and operating environments. These models enable CALO to extend and improve its ca-pabilities through learning and adaptation. Cognitive systems are better equipped to cope with unexpected developments, learn to improve over time, and adapt to the contexts and requirements of different situa-tions. CALO also uses natural interfaces that enable simple, effective interactions with humans and other cognitive systems. 
The CALO Meeting Assistance Project is devel-oping capabilities to enable CALO to participate in group discussions and meetings. Unlike instru-mented  X  X ntelligent room X  meeting projects, this sys-tem is designed for users in an office environment with access to the Internet, a laptop, and some small, off-the-shelf peripheral devices (such as headsets, webcams, and digital writing devices) to capture speech, gestures, and handwriting. It aims to be un-obtrusive by leveraging cross-training, unsupervised learning, and lightweight supervision captured from normal user interaction (e.g., users reviewing and editing notes, or adding detected action items to a to-do list). 
These data are transparently processed at a central server location and redistributed, so the meeting as-sistant interacts seamlessly with other CALO desk-top functionalities, using a common ontology. The CALO Meeting Assistant helps its owners by capturing and interpreting meeting conversations and activities and, as appropr iate, retrieving relevant information. Information gleaned from a meeting can be incorporated in the respective owner X  X  CALO knowledge stores to, for example, track commit-ments and remember references to projects, people, places, and dates. An archive of each meeting pro-vides a searchable record for users, as well as a his-tory of training data for CALO X  X  learning components. Learning areas include the following: Speech processing  X  Automatic transcriptions are pro-duced from conversational speech among multiple speakers while adapting to speaker and background noise; recognizing prosodic cues; learning new vo-cabulary; and constructing person, role, and topic-specific language models. Visual recognition  X  Faces, gaze direction, gestures, and activities are detected, and detection is improved through lightly-supervised learning and unsuper-vised cross-training. Discourse understanding  X  Dialog moves are recog-nized, topics are segmented and grouped through supervised and unsupervised generative models, ac-tion items are detected, and discussions can be threaded across documents and email. Multimodal reinforcement  X  Pen, speech, and text in-puts combine to offer natural communications. Meeting activity  X  Speech and note-taking activities combine to provide cross-training for recognizing meeting phases, and for tracking agendas and docu-ment usage. We demonstrate how the CALO Meeting Assistant captures speech, pen, and other meeting data using an ordinary laptop; produces an automated tran-script; segments by topic; and performs shallow dis-course understanding to produce a list of probable action items arising from a single, pre-recorded meeting. We then demons trate a Meeting Rapporteur that provides a meeting summary and allows partici-pants to review and organize the meeting transcript, audio, notes, action items, and topics X  X ll while providing actions in a feedback loop that supports the meeting assistant X  X  semi-supervised learning process. Finally, we discuss the potential and current development of real-time capabilities that allow us-ers to interact with the meeting assistant during an ongoing meeting. 
