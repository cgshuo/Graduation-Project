 Relevance is a key concept in retrieval theory [7][14]. Among the formal relevance models that have been proposed, relevance-based language models is perhaps the most popular one [9][10][11]. Given a topic of interest t, relevance-based language models estimate the probability distribution p ( w | R t ). The distribution is estimated by using a set of training documents. Nonetheless, these relevance-based language models have a limitation; they make an overly-strict assumption that all tokens in each training document are generated by a single topic to which the document belongs. This assumption is obviously not true in many practical cases. The example below is a part of a Wall Street Journal article judged relevant to the topic  X  X achine translation X  (TREC topic 63). As we see, many portions of it are non-relevant to the topic.
 Alongside the development of relevance-based models, we observe a strong strand of research on Latent Dirichlet Allocation (LDA), that has been shown to be effective in many text-related applications [2][5][6][15]. LDA offers a strong the-oretical framework within which we may consider each document as generated by a mixture of multiple topics. However, the topics discovered by LDA from a corpus are synthetic. In other words, if experts identified topics manually for a corpus, then these may have little or no correspondence with the synthetic topics identified by LDA. From a different perspective, we may say that probabilistic topic models are unable to model the concept of relevance to given topics of interest. Thus, not surprisingly LDA has not found use in applications such as relevance feedback based query modification. Our work shows how this can be done.

In this paper, we propose an approach that bridges relevance-based language models and LDA. Our approach allows us to address the limitation of relevance-based language models, specifically thei r assumption that all tokens of a relevant document are equally relevant to a topic. We do this by estimating the relevance model using the multiple-topic framework of LDA. In essence, we consider that although a document d may be relevant to a given topic t , it could still have non-relevant portions. Some portions could pertain to background information shared by many documents. Other non relevant portions while specific to d may be on themes other than t . Specifically, each document d is hypothesized to be generated by a combination of three topics: the topic t to which it is relevant, a background topic b representing the general language in the document set, and a third topic t o ( d ) responsible for generating themes that though specific to d are neither b nor t . Because we consider this mixture of three topics, our model is able to identify just those portions of the document that are truly relevant to the topic t . In our work, these selected portions are the ones that contribute to the estimation of the relevance model p ( w | R t ). In this way, we utilize the Latent Dirichlet framework to solve for a limitation in relevance-based language models.
As in previous work in standard LDA [5][6][15], we also implement the infer-ence process using Gibbs sampling [1][3]. A secondary contribution of this paper is that we exploit the  X  X ag-of-words X  assumption in order to reduce the compu-tational complexity of the inference algorithm. Since token order in a document is not considered, we can re-arrange the tokens in any order that is convenient for the inference algorithm. In our case, we group tokens with the same stem into continuous segments because the topics of the tokens are sampled from the same distribution. That helps to reduce the running time of the sampling process. The proposed idea is also applicable for standard probabilistic topic models. 2.1 Notation A Vocabulary set (dictionary) V is a set of W possible words (terms) V = { word 1 ,word 2 ... word W } . A token is a specific occurrence of a word in a doc-ument. Document d is a sequence of N d tokens. A training set D t of a topic of interest t is a set of | D t | relevant (or pseudo relevant) documents: D t = { index and document index of the i th token. As we mentioned above, each doc-ument d in the training set of a topic of interest t is generated by a mixture of three topics x d = { b, t, t o ( d ) } ,where b denotes the background topic and t o ( d ) denotes a document-leveled topic covering other themes rather t also mentioned in d . The topic mixing proportion of the three topics in d is represented by  X  d,z = p ( z the vocabulary set denoted by:  X  z,w = p ( w | z )where1  X  w  X  W . 2.2 Model Description The proposed Latent Dirichlet relevance model is a generative model describing the process of generating relevant documents for K 0 given topics of interest. In this model, the language used to generate a document relevant to a topic of interest t is a combination of (1) the language reflecting the meaning of t it-self, (2) the language of a general background topic, (3) the language reflecting themes other than t that are also mentioned in the document. For example, in the domain of computer science research papers, suppose that the training set for the topic machine learning (ML) includes d 1 a document about applying ML to information retrieval (IR) and d 2 a document about ML tools for the banking industry . The general background topic would be responsible for common words in English and common words in the domain such as  X  X aper X ,  X  X ropose X ,  X  X p-proach X ... The distribution for topic ML, representing the meaning of ML, would likely give high probabilities to words like  X  X earning X , X  X raining X ,  X  X est X . . . Topic t o ( d 1 ) responsible for other themes in document d 1 would likely generate words erate words such as  X  X ank X ,  X  X ales X ,  X  X arketing X  that are related to the banking industry emphasis in d 2 . The process of generating relevant documents for K 0 topics of interest is formally described as follows: 1. Pick a multinomial distribution  X  b for the background topic ( b )froma W-dimensional Dirichlet distribution Dir (  X  ). 2. For each topic t in K 0 topics of interest: 2.1 Pick a multinomial distribution  X  t for t from the W-dimensional Dir (  X  ). 2.2 For each document d relevant to t : 2.2.1 Pick a multinomial distribution  X  t o ( d ) for the topic covering themes other than t that are also mentioned in d from the W-dimensional Dir (  X  ). 2.2.2 Pick a multinomial distribution  X  d from a 3-dimensional Dir (  X  ), each element of  X  d corresponds to a topic in x d = { b, t, t o ( d ) } 2.2.3 For each token in document d : 2.2.3.1 Pick a topic z among the three topics in x d from multinomial  X  d . 2.2.3.1 Pick a word from the corresponding multinomial distribution  X  z .
The graphical model using plate nota tion in Fig. 1 describes this process. In document d) are observable variables and denoted by shaded circles; z i (latent topic of a token i th ),  X  and  X  are hidden variables and denoted by un-shaded circles;  X  ,  X  are hyper-parameters of Dirichlet distributions. In our model, values of  X  ,  X  are pre-defined as in [15][16].

Observe that unlike standard LDA describing how all documents in a corpus are generated, our model describes how relevant documents for a set of given topics are composed. Consequentially, each given topic of interest is explicitly associated with a multinomial distribution over the vocabulary. Therefore, we are able to explicitly model relevance.

The advantage of our model compared to relevance-based language models is that our model considers two more components b and t o ( d ). The purpose of the background topic b is to explain words commonly appearing in training documents of all topics. That allows the distribution of the topic of interest t to be more discriminative. The purpose of t o ( d ) is to explain words frequently appearing in the particular document d , but not in other training documents of topic t . That prevents the distribution of topic t from wasting its probabil-ity mass on these extra document-specific features. Thus, the consideration of document-specific t o ( d ) topics minimizes the risk of t over-fitting the given set of training documents. We model the topic mixing proportion  X  d and topic-word distribution  X  z by latent variables which are assumed to be sampled from prior Dirichlet distributions. The explicit assumption about the prior sources of these variables provides complete generative semantics for the model [2][6][16]. Moreover, the mathematical property that the Dirichlet priors of p (  X  d |  X  )and p (  X  z |  X  ) are conjugate to their likelihoods (multinomial distributions) p ( z |  X  d ) in doc d } )and p (  X  z |  X , { w i | for all tokens generated by z } ) are also Dirichlet distributions. Mathematically that makes the inference feasible.
 2.3 Inference As in previous work on LDA, we also apply Gibbs sampling to infer latent vari- X  d } , { w ones by generating sequence of ( S + 1) samples, where each sample contains val-ues for all latent variables. The sampling algorithm is presented in Fig. 2. For the on all relevant sets, on the training set of t , and on document d , respectively.  X  d for each document d is the uniform distribution i.e.  X  For each of the following S samples (Step 2, Fig. 2), each latent variable is ran-domly sampled from its posterior distribution given current values of all other variables. Specifically, latent topic of token i th is sampled from its posterior dis-tribution that is estimated by using values of  X  and  X  in the previous sample (Step 2.1), where w i and d i are word index and document index of token i th .Af-ter sampling z i for every token, we update the values for  X  and  X  by maximum likelihood principle (Steps 2.2 and 2.3). Given the (S+1) samples, we ignore the first S X  samples (samples in the burn-in period), then select every P th samples (i.e. samples S X , (S X +P), (S X +2P). . . ) to estimate  X  z,w  X  = p ( word = w | topic = z ) for all topics. Those distributions are estimated by averaging over  X  ( s ) z,w in these selected samples.
 Reducing Complexity: In the inference algorithm (Fig. 2), z ( s +1) i is condi-tionally independent of any other z ( s +1) k ( k = i ) (Step 2.1). So, we can re-arrange the sampling order in Step 2.1 in any order without affecting the final results. Exploiting this observation, in the preprocessing step, we re-arrange tokens in each document such that tokens from the same stem are consecutive. Since the latent topics for these tokens are sampled from the same posterior distribution, the re-arranging could reduce the complexity by a factor of r = W d /N d ,where W d is the average number of distinct stems, and N d is the average number of distinct tokens in a document. 3.1 Pseudo-relevance Feedback In this section, we evaluate the effectiveness of our Latent Dirichlet relevance model (Dir Rel) on the task of pseudo-relevance feedback in comparison against a standard relevance-based language model (Rel LM). Our implementation of the Rel LM follows the description given in [7]. Our experiments are done using four corpora (Table 1). AP and WSJ contain newswire articles. For these corpora, we use 100 topics (title only) and partial judgments for these topics provided by TREC. 20 Newsgroup contains discussion posts. Each of the posts is labeled by one of 20 topics. Cora contains computer science abstract research papers. These papers are also manually assigned to topics. We use 20 topics for this corpus. For 20 Newsgroup and Cora, we have complete relevance judgments.
 All documents are stemmed using the Porter stemmer [12] and indexed using Lucene. We do not remove stop words in this experiment. For a simple retrieval baseline, we use all Lucene default parameter settings. From the results returned by Lucene, the top 50 documents for each query are used to train Rel LM and Dir Rel. In each case the top ranked 50 words, with the highest probabilities estimated by each model, are used to expand the original query. These parameter values (50x50) have been tuned for Rel LM in previous work. We also use these values for our model. Tuning these values specifically for our model could result in a better performance. We leave this for future work. The expanded query is rerun using Lucene. The performances of the baseline retrieval and pseudo-relevance feedback by the two models are shown for each dataset in Tables 2-5. We measure averages across topics of precision at top 10, top 100 and top 1000 ranked documents. We also measure average precision (averaged across topics to yield MAP) and the total number of relevant documents retrieved ( rel ret ). As expected with only a single exception both feedback models are consistently better than the no feedback Lucene baseline (row 1 of the tables) for all measures. The only exception is for 20 Newsgroup for P@10 w.r.t. our model. Focusing just on MAP (the fifth column), notations  X  and  X  indicate statistically significant over the baseline and Rel LM ( p  X  value &lt; 0 . 05 by the paired t-test). The improvements against baseline for the Rel LM are generally in the range of 10% to 43%, while for Dir Rel are in 23% to 100%. In 3 of the 4 cases, the MAP improvements for Dir Rel against Rel LM are around 10%. The best improvement is observed in the 20 Newsgroup dataset (40%). In terms of precision, for example the P@100 score, we find that Dir Rel is consistently better than Baseline and Rel LM in all cases. Thus on the whole, we find that Dir Rel is successful at achieving improvements over the Rel LM, and both feedback models are, as expected, better than the no feedback baseline. These results support our contention that a) relevant documents may contain portions that are not relevant to the topic of interest and b) it is possible to build more robust relevance models using the Latent Dirichlet framework.
 3.2 Perplexity The goal of both relevance-based language models and our Latent Dirichlet rel-evance model is to estimate the unknown true relevance distribution p ( w | t )of some topic of interest t . A traditional measure for comparing the two estimations is perplexity. Perplexity indicates how well estimated distributions predict a new sequence of tokens drawn from the true distribution. Better estimations of the true distribution tend to give higher probabilities to test tokens. As a result, they have lower perplexity, which means they are less surprised by these tokens.
In our experiment such ideal test data is not available. Instead, for each topic (query) t , we approximate the new sequence of relevant tokens by using a held out set of 50 actual relevant documents that do not appear in the training set. We remove stop words from a standard list and also rare words in these relevant documents. Then, we use the remaining tokens as test data. Given estimated distributions p RelLM ( w | t )and p DirRel ( w | t ) obtained from the previous experiment, we compute Perplexity (PPX) for each topic as follows: where N is the number of tokens in the test data. Table 6 shows the average per-plexity over 20 topics of Cora and 20 Newsgroup. We experiment on Cora and 20 Newsgroup since each topic of these corpora has hundreds of relevant documents. As we see, the perplexity of relevance distributions estimated by the proposed model is significantly lower than distributions estimated by relevance-based lan-guage models. The asterisk symbol (  X  ) means that the difference between the This indicates that our Rel Dir is better able to predict unseen test data from the true distribution as compared to Rel LM. Again, the key difference here is that our model considers each document to be generated by a mixture of topics and not just the relevant topic alone.
 In this section, we further analyze the key feature of our proposed model, i.e., the important fact that a document relevant to a given topic could also talk about other non-relevant themes and also have uninformative background terms. Our model X  X  strength is that it automatically extracts relevant terms and rules out non-relevant background terms and terms belonging to other themes in each document. We illustrate this ability with the example below. The following is a relevant document in the training set for the topic of information retrieval (IR). The document seems to be about image retrieval in the medical domain. (Note: to make it more readable, we restore the stemmed words to the original forms.) After running the inference algorithm described in Fig.2, our model determines the latent topic of each token as shown in the example. Bold tokens are inferred to be generated by IR topic (i.e. are relevant terms), italicized tokens are inferred to be generated by the background topic (i.e. are non-relevant terms), underlined tokens are inferred to be generated by t o ( d ) (and so also non-relevant to t). As we see all stop words as well as words popular in the domain such as  X  X resent X ,  X  X ethod X   X  X btain X  are inferred as background terms (recall that Cora contains computer science research papers). Most of the bold are really relevant to IR such as  X  X imilarity X   X  X etrieval X   X  X emantics X . The t o ( d ) terms identified by the model reflect the specific context of the document and contain almost nothing about the topic of IR.

A secondary hypothesis that we now explore is that the proportion of relevant (on topic) tokens in top retrieved documents is likely to be higher than in lower ranked ones. Analogously, the contributions of t o ( d ) topics in lower ranked doc-uments are likely to be more serious than in top ranked ones. To test this, we explore the contributions, in percentages, of the relevant topical component and the non-relevant component generated by t o ( d ) over top 100 retrieved docu-ments. We group the results by bins. Each bin contains 10 documents (i.e. the first bin in Fig. 3 includes the top 10 documents, the last bin includes documents from ranks 91  X  100). Fig. 3 shows the result averaged over 20 topics on Cora. We see that proportion of relevant tokens in the first bin is 19% higher than in the 10th bin. Similarly, the contribution of t o ( d ) topics in the last bin is 27% higher than in the first bin. The results on other datasets also have the same trend (not shown due to lack of space). Recall that the contribution proportions of topics in documents are modeled as a latent variable in our model, and are determined automatically by the inference algorithm. Our work proposed in this paper is related to two separate existing directions: relevance models, and probabilistic topic models.

Relevance-based language models [9], a popular approach for relevance mod-eling, expand a given topic (query) t to a multinomial distribution p ( w | R t )of observing a word w in documents relevant to t . The probabilities are estimated p ( w | D ) is a language model, and p ( D | R t )=1 / | R t | as assumed in previous work of Hiemstra et al. [7]. In our experiment, we use the same assumption for imple-menting relevance-based language models. A limitation of the relevance-based language models is that they are based on a strict assumption that if a docu-ment D is relevant to a topic, all tokens in the document are equally relevant to that topic. In [7][17], three-component mixture relevance models are proposed. Besides the relevance component ( R t ), the authors introduce two additional components to capture the background (b) and local features (d) in documents. However, the model assumption that the mixing proportions of the three compo-nents (  X  b , X  R t , X  d )areknowninadvanceandthesameforalldocumentsisnot reasonable. For instance, in the case where we use top 50 retrieved documents for the query t as the training set, the contribution of relevance component in the first document is likely to be higher than in the 50th document.

Another approach to alleviate the problem of noises in training documents is to build relevance model on passages (usually windows of text) instead of the whole documents (Liu et al. [11]). However, the way that documents are broken into passages is rather ad-hoc and corpus specific. Moreover, all tokens in each passage are still considered equally relevant. As in the WSJ and Cora example documents we show above, relevant and non-relevant terms appear together even within a sentence.
 In topic model literature, Hofmann [8] proposes probabilistic Latent Semantic Indexing (pLSI) modeling each document as a mixture of topics, where a topic is a multinomial distribution. Each word in a document is generated by a topic, and different words in the same document may be generated by different topics. Top-ics are automatically discovered from the corpus. One limitation of pLSI is that it is not clear how the mixing proportions for topics in a document are generated [2]. To overcome the limitation, Blei et al. [2] propose Latent Dirichlet Alloca-tion (LDA). In LDA, topic proportion of every document is a K-dimensional hidden variable randomly drawn from the same Dirichlet distribution, where K is the number of topics. Thus, generative semantics of LDA are complete [16]. LDA and its variants have been applied in many applications such as finding scientific topics [6], E-community discovery [18], mixed-membership analysis [5] and ad-hoc retrieval for representing document language model [4][6].
However, a common problem of both pLSI and LDA is their inability to model the concept of relevance, which is key in information retrieval [7][13][14]. Con-sequently, there is no explicit mapping between the resulting topics generated by pLSI or LDA and the topics in the prior knowledge of human beings. There-fore, the approach could not be applied directly for problems, such as relevance feedback for query modification and text classification, where topics (classes and queries) are provided upfront.

Compared to these two sets of approaches, our Latent Dirichlet relevance model has the following advantages. First, our model explicitly takes the key concept of relevance in account, as in the relevance models [9]. Second, our model could be able to identify relevant and non-relevant terms in training documents. Only relevant terms contribute to the estimation of relevance models. Third, our model possesses complete generative semantics by treating document-topic mix-ing proportion (  X  d ) and topic-word distribution (  X  z ) as hidden random variables sampled from Dirichlet distributions as in the original LDA [2]. As a result, we could exploit the Latent Dirichlet theoretical framework to automatically infer both these variables by taking semantic s of topics and content of each relevant document into account. This paper presents a Latent Dirichlet relevance model that combines the advan-tages of both relevance-based language models [9] and probabilistic topic models [2][15]. Crucially, our model relaxes the strict assumption of relevance-based lan-guage models that if a document is relevant to a topic, the entire document is relevant to that topic. This is done by automatically identifying the non-relevant parts in the document. Second, in the context of research on probabilistic topic models, our model explicitly considers the notion of relevance by starting with given topics and estimating their distributions over the corpus vocabulary. We also propose the idea of exploiting the assumption of exchangeability for the tokens in a document ( X  X ag-of-words X  assumption) to reduce the computational complexity of the learning algorithm. This idea is not only applicable to our Latent Dirichlet relevance models, but also to conventional LDA.

Our preliminary experiments on pseudo-relevance feedback show the effec-tiveness the proposed model. The results obtained by the model are consistently better across all of the four corpora than the results of the baseline retrieval (23%-100% improvement in terms MAP) and relevance-based language models (10%-40%). Our work on perplexity re-affirms the advantages of our model over relevance-based language models for the task of estimating the true unknown relevance model.

For future directions, we plan to apply the model for some other applications such as text classification without any human-labeled training data. Instead, we will use as training sets documents returned from a global search engine (e.g. Google) or an intranet search engine, retrieved by the topics themselves. The challenge of this approach is that there is a lot of noise (non-relevant portions) in the returned sets. The ability to automatically detect non-relevant parts in documents of our model is the key to tackling this challenge. Moreover, the background topic in our model could cover common word features of all given classes (topics of interest), so that each of these classes could spend its probability mass on its discriminative features that distinguish itself from the rest of the classes. The background topic could, therefore, increase the margins among the distributions of the classes. This idea is similar to SVM classification technique, but in our model it is not only applicable to case of two classes but also naturally applicable any set of classes.

