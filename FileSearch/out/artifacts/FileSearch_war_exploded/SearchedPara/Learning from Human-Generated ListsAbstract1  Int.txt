 Kwang-Sung Jun, Xiaojin Zhu { deltakam,jerryzhu } @cs.wisc.edu Burr Settles bsettles@cs.cmu.edu Timothy T. Rogers ttrogers@wisc.edu We present a probabilistic model describing the hu-man process of ordered list generation. For machine learning, such a model can enhance the way computers learn from people. Consider a user who is training a system to classify sports articles. She might begin by generating a list of relevant phrase  X  label rules (e.g.,  X  X ouchdown  X  football, home-run  X  baseball X ). Incor-porating such  X  X eature volunteering X  as training data in a machine learning algorithm has been an area of considerable research interest (Druck et al., 2008; Liu et al., 2004; Settles, 2011). Less effort has gone into modeling the human process of generating these lists, which (as we show) can be combined with these al-gorithms for further improvement. For cognitive psy-chology, such list-generation tasks are used to probe the structure of human memory and to diagnose forms of cognitive impairment. For instance, the  X  X erbal fluency X  task requires a subject to generate as many words of a given category (e.g.,  X  X ehicle X ) as possible within 60 seconds (e.g.,  X  X ar, plane, boat, ...  X ). Per-formance on this simple task is highly sensitive to even mild mental dysfunction.
 Learning from human-generated lists differs from more familiar machine learning settings. Unlike ranking, the human teacher does not have the collection of items in front of her to arrange in order. Unlike active learn-ing, she is not prompted with item queries, either. In-stead, she must generate the list internally via memory search. Such lists have two key characteristics: 1. Order matters. Items (e.g., vehicle names or phrase  X  label rules) that appear earlier in a list tend to be more  X  X mportant. X  This suggests that we can es-timate the  X  X alue X  of an item according to its position in the list. This relation is not deterministic, however, but stochastic. Very important items can appear later or be missing from any single list altogether. 2. Repeats happen. Humans tend to repeat items in their list even though they know they should not (see Table 1). Indeed, we will see that people tend to repeat items even when their lists are displayed right in front of them (e.g., Figure 1). Though this is a nuisance for some applications, the propensity to repeat can provide useful information in others.
 We propose a new sampling paradigm, sampling with reduced replacement (SWIRL), to model human list production. Informally, SWIRL is  X  X n-between X  sam-pling with and without replacement, since a drawn item is replaced but with its probability discounted. This allows us to model order and repetition simul-taneously. In Section 2, we formally define SWIRL and provide the maximum likelihood estimate to learn model parameters from human-generated lists. Though not in closed form, our likelihood function is convex and easily optimized. We present a ma-chine learning application in Section 3: feature vol-unteering for text classification, where we incorporate SWIRL parameters into down-stream text classifiers. We compare two frameworks: ( i ) Generalized Expec-tation (GE) for logistic regression (Druck et al., 2008) and ( ii ) informative Dirichlet priors (IDP) for na  X  X ve Bayes (Settles, 2011). We then present a psychology application in Section 4: verbal fluency, where SWIRL itself is used to classify healthy vs. brain damaged pop-ulations, and its parameters provide insight into the different mental processes of the two groups. Let V denote the vocabulary of items for a task, and z = ( z 1 ,...,z n ) be an ordered list of n items where z t  X  V and the z  X  X  are not necessarily distinct. The set of N lists produced by different people is written z We now formally define SWIRL. Assume that humans possess an unnormalized distribution over the items for this task. Let s i  X  0 be the  X  X nitial size X  of item i for i  X  V , not necessarily normalized. One would select item i with probability proportional to s i . Crit-ically, the size of the selected item (say, z 1 ) will be discounted by a factor  X   X  [0 , 1] for the next draw: s will be selected again in the future. To make it a full generative model, we assume a Poisson(  X  ) list length distribution. The process of generating a single list z is specified in Algorithm 1.
 Algorithm 1 The SWIRL Model 2.1. Relation to Other Sampling Paradigms Setting  X  = 1 recovers sampling with replacement, while  X  = 0 differs subtly from sampling without re-placement. Consider an  X  X rn-ball X  model where balls have |V| distinct colors. Let there be m i balls of color i , each with size s i . The probability of drawing a ball is proportional to its size. The chance that a draw has trast several sampling paradigms: Sampling without replacement and all colors have the same size s i = s j . If a draw produces a ball with color i then m i  X  m i  X  1 for the next draw. Let m = ( m 1 ,...,m |V| ) &gt; be the counts of balls in the urn and k = ( k 1 ...k |V| ) &gt; be the counts of balls drawn, then k follows the multivariate hypergeometric distribution mhypg( k ; m , 1 &gt; k ).
 Sampling without replacement when the sizes may be different. The distribution of k follows the multivariate Wallenius X  noncentral hypergeometric distribution mwnchypg( k ; m , s , 1 &gt; k ), which is a gen-eralization to the multivariate hypergeometric distri-bution (Wallenius, 1963; Chesson, 1976; Fog, 2008).  X  X oncentral X  means that the s i  X  X  may be different. Note that after drawing a ball of color i , we subtract s from the  X  X otal size X  of color i .
 SWIRL. Each color has only one ball: m i = 1, but the sizes s i may differ. Balls are replaced but trimmed: m i stays at one but s i  X   X s i . This results in a geomet-ric (rather than a P  X olya urn process-style arithmetic) change in that color X  X  size. We are interested in the probability of the ordered sequence of draws (i.e., z ) rather than just a count vector k . The salient differ-ences are illustrated by the following example. Example (Non-exchangeability) . Sampling without replacement is well-known to be exchangeable. This is not true for SWIRL. Let there be two colors V = { A,B } . Consider two experiments with matching to-tal size for each color: (Experiment 1) There are m 1 = a balls of color A and m 2 = b balls of color B . Let s 1 = s 2 = 1 , and perform sampling without replacement. Let z i be the color of the i th draw. It is easy to show that P ( z i = A ) = P ( z j = A ) = a a + b ;  X  i,j . (Experiment 2) There is m 1 = 1 ball of color A with size s 1 = a , and m 2 = 1 ball of color B with size s = b . We perform SWIRL with discounting factor  X  . Then P ( z 1 = A ) = a a + b , but P ( z 2 = A ) = (  X a  X a + b instance, when  X  = 0 we have P ( z 2 = A ) = b a + b . 2.2. Maximum Likelihood Estimate Given observed lists z (1) ... z ( N ) , the log likelihood is ` = where n ( j ) is the length of the j th list, and  X  is the Poisson intensity parameter. The key quantity here is where c ( i,j,t ) is the count of item i in the j th prefix counting and renormalization couples  X  and s , making the MLE difficult to solve in closed form.
 The  X  parameter is independent of s and  X  , so the MLE b  X  is the usual average list length. To simplify notation, we omit the Poisson and focus on the log likelihood of s and  X  , namely ` ( s , X  ) = X We transform the variables into the log domain which is easier to work with:  X   X  log s , and  X   X  log  X  . The log likelihood can now be written as ` (  X  , X  ) = X Due to the log-sum-exp form, ` (  X  , X  ) is concave in  X  and  X  (Boyd &amp; Vandenberge, 2004). Note the initial sizes s are scale invariant. We remove this invariance by setting s MFI = 1 where MFI is the most frequent item in z (1) ... z ( N ) . Equivalently,  X  MFI = 0. The complete convex optimization problem for finding the MLE of SWIRL is The MLE is readily computed by quasi-Newton meth-ods such as LBFGS, where the required gradient for (2) is computed by  X  X   X  X   X  X  where c ( i ) is the total count of occurrences for item i in the lists z (1) ... z ( N ) , for i  X  X  . 2.3. Maximum A Posteriori Estimate Additionally, it is easy to compute the MAP estimate when we have prior knowledge on  X  and  X  . Sup-pose we believe the initial sizes should be approxi-mately proportional to s 0 . For example, in the ve-hicle verbal fluency task, s 0 may be the counts of various vehicles in a large corpus. We can define  X  prior on  X  centered around  X  0 (equivalently, s follows a log-Normal distribution). Since this prior removes the scale invariance on  X  , we no longer need the constraint  X 
MFI = 0. Similarly, we may have prior knowledge of  X  . The MAP estimate is the solution to where  X  1 , X  2 are appropriate regularization terms to prevent overfitting. We now turn to a machine learning application of SWIRL: training text classifiers from human-volunteered feature labels (rather than documents). A feature label is a simple rule stating that the presence of a word or phrase indicates a particular class label. For example,  X  X ouchdown  X  football X  implies that doc-uments containing the word  X  X ouchdown X  probably be-long to the class  X  X ootball. X  Some prior work exploits a bag of volunteered features from users, where each has an equal importance. Although such feature la-bels help classification (Liu et al., 2004), the order of volunteered words is not taken into account. The or-der turns out to be very useful, however, as we will show later. Other prior work solicits labeled features through a form of feature queries : the computer, via unsupervised corpus analysis (e.g., topic modeling), proposes a list of high-probability candidate features for a human to label (Druck et al., 2008).
 Departing from previous works, we point out that the human teacher, upon hearing the categorization goal (e.g., the classes to be distinguished), can volunteer an ordered list of feature labels without first consulting a corpus; see Table 1(a,b). This  X  X eature volunteering X  procedure is particularly attractive when a classifier is promptly needed for a novel task, since humans can be recruited quickly via crowdsourcing or other means, even before a corpus is fully compiled. Another pos-sibility, which we recommend for practitioners 1 , is to treat feature volunteering as a crucial first step in a chain of progressively richer interactive supervision, followed by queries on both features and documents. Queries can be selected by the computer in order to build better classifiers over time. Such a combination has been studied recently (Settles, 2011).
 In this section, we show that feature volunteering can be successfully combined with two existing frameworks for training classifiers with labeled features: ( i ) Gener-alized Expectation (GE) for logistic regression (Druck et al., 2008) and ( ii ) informative Dirichlet priors (IDP) for multinomial na  X  X ve Bayes (Settles, 2011). We also show that  X  X rder matters X  by highlighting the value of SWIRL as a model for feature volunteering. That is, by endowing each volunteered feature with its size s i as estimated in Section 2, we can build better classi-fiers than by treating the volunteered features equally under both of these machine learning frameworks. 3.1. Generalized Expectation (GE) Let y  X  Y be a class label, and x  X  R |F + | be a vec-tor describing a text document using feature set F + , which is a super set of volunteered feature set F . Con-sider the conditional probability distributions realiz-able by multinomial logistic regression where Generalized Expectation (GE) seeks a distribution p  X   X   X   X  that matches a set of given reference dis-tributions feature f  X  X  is present. We will construct feature volunteering and compare it against other con-structions in the next section. Before that, we specify the matching sought by GE (Druck et al., 2008). We restrict ourselves to sufficient statistics based on counts, such that x f  X  x is the number of times feature f occurs in the document. Let U be an unlabeled corpus. The empirical mean conditional distribution on documents where x f &gt; 0 is given by GE training minimizes a regularized objective based on the KL-divergence of these distributions: p  X  = argmin In other words, GE seeks to make the reference and empirical distributions as similar as possible. 3.2. Constructing GE Reference Distributions Recall that in feature volunteering, the N human users produce multiple ordered lists z (1) ,..., z ( N ) Each item z in these lists is a f  X  y (feature  X  label) pair. It is possible that the same f appears multiple times in different z  X  X , mapping to different y  X  X  (e.g.,  X  X oalie  X  soccer X  and  X  X oalie  X  hockey X ). In this case we say feature f co-occurs with multiple y  X  X . For each list z , we split it into |Y| sublists by item labels. This produces one ordered sublist per class. We collect all N sublists for a single class y , and treat them as N lists generated by SWIRL from the y th urn. We find the MLE of this y th urn using (2), and normalize it to sum to one. We are particularly interested in the size estimates s y = { s f  X  y | f  X  F} . This is repeated for all |Y| urns, so we have s 1 ,..., s |Y| . We construct reference distributions using where F is the union of features appearing in the lists z (1) ,..., z ( N ) and s f  X  y = 0 if feature f is absent from the y th list. For example, imagine  X  X oalie  X  soccer X  appears near the top of a list, so s goalie  X  soccer is large (say 0.4),  X  X oalie  X  hockey X  appears much later in an-other list, so s goalie  X  hockey is small (say 0.1), and  X  X oalie X  is never associated with  X  X aseball X . Then  X  p In our experiments, we compare three ways of creat-ing reference distributions. GE/SWIRL is given by Equation (10). GE/Equal is defined as which is similar to (10), except that all features co-occurring with y have equal size. This serves as a baseline to investigate whether order matters. GE/Schapire is the reference distribution used in previous work (Druck et al., 2008): c p f ( y ) = where m is the number of distinct labels co-occurring with feature f in z (1) ,..., z ( N ) , and q is a smoothing parameter. We use q = 0.9 as in prior work. 3.3. Informative Dirichlet Priors (IDP) Another way to use feature volunteering is by training multinomial Na  X  X ve Bayes models, where feature  X  label rules are adapted as informative priors on feature pa-rameters. The class distribution p ( y ) is parametrized by  X  y , and the likelihood of a document x given a class label y is modeled as p ( x | y ) = Q f (  X  fy ) x f , where x is the frequency count of feature f and  X  fy is multino-mial parameter for feature f under class y . Dirichlet priors are placed on each class-conditional multinomial parameter, where the hyperparameter is denoted by d fy for phrase f under class y .
 We estimate  X  y by class proportion of labeled docu-ments, and  X  fy by posterior expectation as follows: where  X  fy is normalized over phrases to sum to one for each y . When learning from only labeled instances, p ( y | x )  X  X  0 , 1 } indicates the true labeling of instance x . When learning from both labeled and unlabeled instances, we run EM as follows. First, initialize  X  fy  X  X  by (13) using only the d fy hyperparameters. Second, repeatedly apply (13) until convergence, where the sec-ond summation term is over both labeled and unla-beled instances and p ( y | x ) for unlabeled instance x is computed using Bayes rule.
 3.4. Constructing IDP Priors d fy with SWIRL We compare two approaches for incorporating prior knowledge into na  X  X ve Bayes by feature volunteering. IDP/SWIRL sets the hyperparameters as follows: d fy = 1 + kn y s f  X  y , where f is a feature, k a parame-ter, and n y is the number of unique features in y  X  X  list. Again, we compute s f  X  y via SWIRL as in Section 3.2. Note that replacing s f  X  y with 1 { s f  X  y &gt; 0 } /n covers prior work (Settles, 2011). In this method, only the association between a feature f and a class y is taken into account, rather than relative importance of these associations. This baseline, IDP/Settles , sets d fy = 1 + k 1 { s f  X  y &gt; 0 } and serves to investigate whether order matters in human-generated lists. 3.5. Experiments We conduct feature volunteering text classification ex-periments in three different domains: sports (sports articles), movies (movie reviews), and webkb (uni-versity web pages). The classes, number of human participants ( N ), and the number of distinct list fea-tures they produced ( |F| ) are listed in Table 2. Participants. A total of 135 undergraduate stu-dents from the University of Wisconsin-Madison par-ticipated for partial course credit. No one participated in more than one domain. All human studies in this paper were approved by the institutional review board. Procedure. Participants were informed of only the class labels, and were asked to provide as many words or short phrases as they thought would be necessary to accurately classify documents into the classes. They volunteered features using the web-based computer interface illustrated in Figure 1 (shown here for the sports domain). The interface consists of a text box to enter features, followed by a series of buttons cor-responding to labels in the domain. When the partic-ipant clicks on a label (e.g.,  X  X ockey X  in the figure), the phrase is added to the bottom of the list below the corresponding button, and erased from the input box. The feature  X  label pair is recorded for each action in sequence. The label order was randomized for each subject to avoid presentation bias. Participants had 15 minutes to complete the task.
 Data cleaning. We normalized the volunteered phrases by case-folding, punctuation removal, and space normalization. We manually corrected obvi-ous misspellings. We also manually mapped different forms of a feature to its dictionary canonical form: for example, we mapped  X  X ay-up X  and  X  X ay up X  to  X  X ayup. X  The average (and maximum) list length is 39 (91) for sports, 20 (46) for movies, and 40 (85) for webkb. Most participants volunteered features at a fairly uniform speed for the first five minutes or so; some then exhausted ideas. This suggests the impor-tance of combining feature volunteering with feature and document querying, as mentioned earlier. This combination is left for future work.
 Unlabeled corpus U . Computing (8) requires an un-labeled corpus. We produce U for the sports domain by collecting 1123 Wikipedia documents via a shallow web crawl starting from the top-level wiki-category for the five sport labels (e.g.,  X  X ategory:Baseball X ). We produce a matching U for the movies and we-bkb domains from the standard movie sentiment cor-pus (Pang et al., 2002) (2000 instances) and the We-bKB corpus (Craven et al., 1998) (4199 instances), re-spectively. Note that U  X  X  are treated as unlabeled for training our models, however, we use each U  X  X  in a transductive fashion to serve as our test sets as well. Training with GE. We define the feature set for learning F + (i.e., the dimensionality in (6)) to be the union of F (volunteered phrases) plus all unigrams oc-curring at least 50 times in the corpus U for that do-main. That is, we include all volunteered phrases, even if they are not a unigram or appear fewer than 50 times in the corpus. |F + | for each domain is listed in Ta-ble 2. We construct the reference distributions accord-ing to GE/SWIRL , GE/Equal , and GE/Schapire as in section 3.2, and find the optimal logistic regres-sion models p  X  ( y | x ) by solving (9) with LBFGS for each domain and reference distribution.
 Training with IDP. We use the same F + in GE training, construct IDP hyperparameters according to IDP/SWIRL and IDP/Settles , and learn MNB classifiers as in section 3.3 using uniform  X  y . Following prior work, we apply one-step EM with k = 50.
 Feature Voting Baseline (FV). We also include a simple baseline for both frameworks. To classify a document x , FV scans through unique volunteered fea-tures for which x f &gt; 0. Each class y for which f  X  y exists in z (1) ,..., z ( N ) receives one vote. At the end, FV predicts the label as the one with the most votes. Ties are broken randomly. Accuracy of FV is measured by averaging over 20 trials due to this randomness. Results. Text classifiers built from volunteered fea-tures and SWIRL consistently outperform the base-lines. Classification accuracies of the different models under GE and IDP are shown in Table 3(a,b). For each domain, we show the best accuracy in bold face, as well as any accuracies whose difference from the best is not statistically significant 2 . Under the GE frame-work, GE/SWIRL is the best on movies and webkb, and is indistinguishable from the best on sports. Un-der the IDP framework, IDP/SWIRL consistently outperforms all baselines.
 The fact that both GE/SWIRL and IDP/SWIRL are better than (or on par with) the baselines under both frameworks strongly indicates that order mat-ters . That is, when working with human-generated lists, the item order carries information that can be useful to machine learning algorithms. Such informa-tion can be extracted by SWIRL parameter estimates and successfully incorporated into a secondary classi-fication task. Although dominated by SWIRL-based approaches, FV is reasonably strong and may be a quick stand-in due to its simplicity.
 A caveat: the human participants were only informed of the class labels and did not know U . Mildly amusing mismatch ensued. For example, the we-bkb corpus was collected in 1997 (before the  X  X o-cial media X  era), but the volunteered feature labels included  X  X acebook  X  student, X   X  X ropbox  X  course, X   X  X eddit  X  faculty, X  and so on. Our convenient but out-dated choice of U quite possibly explains the low ac-curacy of all methods in the webkb domain. One human list-generation task that has received de-tailed examination in cognitive psychology is  X  X erbal fluency. X  Human participants are asked to say as many examples of a category as possible in one minute with no repetitions (Glasdjo et al., 1999). 3 For instance, participants may be asked to generate examples of a semantic category (e.g.,  X  X nimals X  or  X  X urniture X ), a phonemic or orthographic category (e.g.,  X  X ords be-ginning with the letter F X ), or an ad-hoc category (e.g.,  X  X hings you would rescue from a burning house X ). Verbal fluency has been widely adopted in neurology to aid in the diagnosis of cognitive dysfunction (Mon-sch et al., 1992; Troyer et al., 1998; Rosser &amp; Hodges, 1994). Category and letter fluency in particular are sensitive to a broad range of cognitive disorders re-sulting from brain damage (Rogers et al., 2006). For instance, patients with prefrontal injuries are prone to inappropriately repeating the same item several times ( perseverative errors) (Baldo &amp; Shimamura, 1998), whereas patients with pathology in the anterior tem-poral cortex are more likely to generate incorrect re-sponses ( semantic errors) and produce many fewer items overall (Hodges et al., 1999; Rogers et al., 2006). Despite these observations and widespread adoption of the task, standard methods for analyzing the data are comparatively primitive: correct responses and differ-ent error types are counted, while sequential informa-tion is typically discarded.
 We propose to use SWIRL as a computational model of the verbal fluency task, since we are unaware of any other such models. We show that, though overly sim-plified in some respects, SWIRL nevertheless estimates key parameters that correspond to cognitive mecha-nisms. We further show that these estimates differ in healthy populations versus patients with temporal-lobe epilepsy, a neurological disorder thought to dis-rupt semantic knowledge. Finally, we report promising classification results, indicating that our model could be useful in aiding diagnosis of cognitive dysfunction in the future.
 Participants. We investigated fluency data gener-ated from two populations: a group of 27 patients with temporal-lobe epilepsy (a disorder thought to disrupt semantic abilities), and a group of 24 healthy controls matched to the patients in age, education, sex, nonver-bal IQ and working-memory span. Patients were re-cruited through an epilepsy clinic at the University of Wisconsin-Madison. Controls were recruited through fliers posted throughout Madison, Wisconsin.
 Procedure. We conducted four category-fluency tasks: animals, vehicles, dogs, and boats. In each task, participants were shown a category name on a computer screen and were instructed to verbally list as many examples of the category as possible in 60 sec-onds without repetition. The audio recordings were later transcribed by lab technicians to render word lists. We normalize the lists by expanding abbre-viations ( X  X ab X   X   X  X abrador X ), removing inflections ( X  X irds X   X   X  X ird X ), and discarding junk utterances and interjections. The average (and maximum) list length is 20 (37) for animals, 14 (33) for vehicles, 11 (23) for dogs, and 11 (20) for boats.
 Results. We estimated SWIRL parameters  X , s , X  us-ing (2) for the patient and control groups on each task separately, and observed that: 1. Patients produced shorter lists . Figure 2(a) shows that the estimated Poisson intensity  X   X  (i.e., the av-erage list length) is smaller for patients on all four tasks. This is consistent with the psychological hy-pothesis that patients suffering from temporal-lobe epilepsy produce items at a slower rate, hence fewer items in the time-limit. 2. Patients and controls have similar lexicon distribu-tions, but both deviate from word usage frequency . Fig-ure 2(b) shows the top 10 lexicon items and their prob-abilities (normalized s ) for patients ( s P ) and controls ( s C ) in the animals task, sorted by ( s Pi + s Ci ) / 2. s and s C are qualitatively similar. We also show corpus probabilities s W from the Google Web 1T 5-gram data set (Brants et al., 2007) for comparison (normalized on items appearing in human-generated lists). Not only does s W have smaller values, but its order is very different: horse (0.06) is second largest, while other top corpus words like fish (0.05) and mouse (0.03) are not even in the human top 10. This challenges the psychological view that verbal fluency largely follows real-world lexicon usage frequency. Both observations are supported quantitatively by comparing the Jensen-Shannon divergence (JSD) 4 between the whole distri-butions s P , s C , s W ; see Figure 2(c). Clearly, s P and s
C are relatively close, and both are far from s W . 3. Patients discount less and repeat more. Figure 2(d) shows the estimated discount factor  X   X  . In three out of four tasks, the patient group has larger  X   X  . Recall that a larger  X   X  leaves an item X  X  size relatively unchanged, thus increasing the item X  X  chance to be sampled again  X  in other words, more repeats. This is consistent with the psychological hypothesis that patients have a reduced ability to inhibit items already produced. Healthy vs. Patient Classification. We conducted additional experiments where we used SWIRL param-eters to build down-stream healthy vs. patient classi-fiers. Specifically, we performed leave-one-out (LOO) classification experiments for each of the four verbal fluency tasks. Given a task, each training set (mi-nus one person X  X  list for testing) was used for learn-ing two separate SWIRL models with MAP estima-tion (5): one for patients and the other for healthy participants. We set  X  1 =  X  and  X  0 = 1 due to the finding that both populations have similar lexicon dis-tributions, and  X  2 = 0. We classified the held-out test list by likelihood ratio threshold at 1 for the patient vs. healthy models. The LOO accuracies of SWIRL on animals, vehicles, dogs and boats were 0.647, 0.706, 0.784, and 0.627, respectively. In contrast, the major-ity vote baseline has accuracy 27 / (24 + 27) = 0 . 529 for all tasks. The improvement for the dogs task over the baseline approach is statistically significant 5 . Human list generation is an interesting process of data creation that deserves attention from the ma-chine learning community. Our initial foray into modeling human-generated lists by sampling with re-duced replacement (SWIRL) has resulted in two in-teresting applications for both machine learning (ef-fectively combining SWIRL statistics with modern feature-labeling frameworks) and cognitive psychology (modeling memory in healthy vs. brain-damaged pop-ulations, and predicting cognitive dysfunction). Learning from human-generated lists opens up several lines of future work. For example: ( i ) Designing a  X  X u-pervision pipeline X  that combines feature volunteering with feature label querying, document label querying, and other forms of interactive learning to build bet-ter text classifiers more quickly. ( ii ) Identifying more applications which can benefit from models of human list generation. For example, creating lists of photo tags on Flickr.com, or hashtags on Twitter.com, can be viewed as a form of human list generation conditioned on a specific photo or tweet. ( iii ) Developing a hierar-chical version of SWIRL, so that each human has their own personalized parameters  X  , s , and  X  , while a group has summary parameters, too. This is particularly at-tractive for applications like verbal fluency, where we want to understand both individual and group behav-iors. ( iv ) Developing structured models of human list generation that can capture and learn from people X  X  tendency to generate  X  X uns X  of semantically-related items in their lists (e.g., pets then predators then fish). The authors were supported in part by National Sci-ence Foundation grants IIS-0953219, IIS-0916038, IIS-1216758, IIS-0968487, DARPA, and Google. The pa-tient data was collected under National Institute of Health grant R03 NS061164 with TR as the PI. We thank Bryan Gibson for help collecting the feature vol-unteering data.
 Baldo, J. V. and Shimamura, A. P. Letter and cat-egory fluency in patients with frontal lobe lesions. Neuropsychology , 12(2):259 X 267, 1998.
 Boyd, S. and Vandenberge, L. Convex Optimization . Cambridge University Press, Cambridge UK, 2004. Brants, T., Popat, A.C., Xu, P., Och, F.J., and Dean, J. Large language models in machine translation.
In Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Natural Language Learning (EMNLP-CoNLL) , 2007.
 Chesson, J. A non-central multivariate hypergeometric distribution arising from biased sampling with ap-plication to selective predation. Journal of Applied Probability , 13(4):795 X 797, 1976.
 Craven, M., DiPasquo, D., Freitag, D., McCallum, A.,
Mitchell, T., Nigam, K., and Slattery, S. Learning to extract symbolic knowledge from the world wide web. In Proceedings of the Conference on Artifi-cial Intelligence (AAAI) , pp. 509 X 516. AAAI Press, 1998.
 Druck, G., Mann, G., and McCallum, A. Learning from labeled features using generalized expectation criteria. In Proceedings of the International ACM SIGIR Conference on Research and Development In Information Retrieval , pp. 595 X 602, 2008.
 Fog, A. Calculation methods for Wallenius X  noncen-tral hypergeometric distribution. Communications in Statistics -Simulation and Computation , 37(2): 258 X 273, 2008.
 Glasdjo, J. A., Schuman, C. C., Evans, J. D., Peavy,
G. M., Miller, S. W., and Heaton, R. K. Normas for letter and category fluency: Demographic correc-tions for age, education, and ethnicity. Assessment , 6(2):147 X 178, 1999.
 Hodges, J. R., Garrard, P., Perry, R., Patterson, K.,
Bak, T., and Gregory, C. The differentiation of semantic dementia and frontal lobe dementia from early alzheimer X  X  disease: a comparative neuropsy-chological study. Neuropsychology , 13:31 X 40, 1999. Liu, B., Li, X., Lee, W.S., and Yu, P.S. Text classifica-tion by labeling words. In Proceedings of the Confer-ence on Artificial Intelligence (AAAI) , pp. 425 X 430. AAAI Press, 2004.
 Monsch, A. U., Bondi, M. W., Butters, N., Salmon,
D. P., Katzman, R., and Thal, L. J. Comparisons of verbal fluency tasks in the detection of dementia of the alzheimer type. Archives of Neurology , 49(12): 1253 X 1258, 1992.
 Pang, B., Lee, L., and Vaithyanathan, S. Thumbs up: Sentiment classification using machine learn-ing techniques. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing (EMNLP) , pp. 79 X 86. ACL, 2002.
 Rogers, T.T., Ivanoiu, A., Patterson, K., and Hodges,
J.R. Semantic memory in alheimer X  X  disease and the fronto-temporal dementias: A longitudinal study of 236 patients. Neuropsychology , 20(3):319 X 335, 2006. Rosser, A. and Hodges, J.R. Initial letter and semantic category fluency in alzheimer X  X  disease, huntington X  X  disease, and progressive supranuclear palsy. jour-nal of Neurology, Neurosurgery, and Psychiatry , 57: 1389 X 1394, 1994.
 Settles, B. Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing (EMNLP) , pp. 1467 X 1478. ACL, 2011.
 Troyer, A.K., Moscovitch, M., Winocur, G., Alexan-der, M., and Stuss, D. Clustering and switch-ing on verbal fluency: The effects of focal fronal-and temporal-lobe lesions. Neuropsychologia , 36(6), 1998.
 Wallenius, K.T. Biased Sampling: The Non-Central
Hypergeometric Probability Distribution . PhD the-sis, Department of Statistics, Stanford University,
