 Some applications have to present their results in the form of ranked lists. This is the case of many information retrieval applications, in which documents must be sorted according to their relevance to a given query. This has led the in-terest of the information retrieval community in methods that automatically learn effective ranking functions. In this paper we propose a novel method which uncovers patterns (or rules) in the training data associating features of the document with its relevance to the query, and then uses the discovered rules to rank documents. To address typical problems that are inherent to the utilization of association rules (such as missing rules and rule explosion), the pro-posed method generates rules on a demand-driven basis, at query-time. The result is an extremely fast and effective ranking method. We conducted a systematic evaluation of the proposed method using the LETOR benchmark collec-tions. We show that generating rules on a demand-driven basis can boost ranking performance, providing gains rang-ing from 12% to 123%, outperforming the state-of-the-art methods that learn to rank, with no need of time-consuming and laborious pre-processing. As a highlight, we also show that additional information, such as query terms, can make the generated rules more discriminative, further improving ranking performance.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learn-ingtoRank Algorithms, Experimentation
This research was sponsored by UOL (www.uol.com.br) through its UOL Bolsa Pesquisa program, process num-ber 20080131200100, and partially supported by CNPq, CAPES,Finep,andFapemig.

The interest in ranking models, paradigms, and functions is not new, and is still an important research topic in many fields. In Information Retrieval, where often, documents must be sorted according to their relevance to a given query, ranking is paramount since it directly affects retrieval qual-ity. Several empirical ranking methods such as boolean mod-els, vector space models and probabilistic models have been proposed in the literature [3]. Due to the difficulty in empir-ically tuning the parameters of the ranking functions that are obtained from the above methods, state-of-the-art search engines are recently adopting alternate methods which are derived from machine learning techniques. These methods automatically learn effective ranking functions and are re-garded as learning to rank methods [22].

The task of learning to rank in information retrieval is defined as follows. We have as input the training data (re-ferred as D ), which consists of a set of records of the form &lt; q,d,r &gt; ,where q is a query (represented as a list of terms { t 1 ,t 2 ,...,t n } ), d is a document (represented as a list of fea-tures { f 1 ,f 2 ,...,f m } ), and r is the relevance of d to q .The relevance draws its values from a discrete set of possibilities (e.g., 0, 1 and 2). The training data is used to construct a model which relates features of the documents to their cor-responding relevance. The test set (referred as T ) consists of records &lt;q,d, ? &gt; for which only the query q and the document d are known, while the relevance of d to q is un-known. The model learned from the training data is used to produce an estimation or likelihood of relevance of such documents to the corresponding queries, which can be used to generate a final ranking.

Several learning to rank methods have already been pro-posed. They usually rely on techniques such as neural net-works [4], genetic programming [10] and support vector ma-chines [14, 25] to learn the ranking model. In this paper we propose an alternative to such methods, which is based on the utilization of association rules [1]. Instead of optimizing a specific target measure (i.e., MAP, NDCG or precision) the proposed method generates a model, R ,composedofrules of the form f j  X  ...  X  f k  X   X  r , which describe the training data by means of feature-relevance associations. These rules can contain any mixture of the available features in the an-tecedent and a relevance level in the consequent. Once the model is built, the rules composing it are directly used to estimate the relevance of documents in the test set.
The search space for rules is huge, and thus, computa-tional cost restrictions must be imposed during model gen-eration. Typically, a minimum support threshold (  X  min )is employed in order to select the most frequent rules to com-pose the model (i.e., rules occurring at least  X  min times in the training data). This strategy, although simple, has some problems. If  X  min is set too low, a large number of rules will be generated, and often most of these rules are useless for ranking documents in the test set (representing a wastage of computational resources). Otherwise, if  X  min is set too high, some important rules will not be included in R ,caus-ing problems if some documents in the test set contain rare features. Usually, there is no optimal value for  X  min ,thatis, there is no single value that ensures that only rules useful for ranking are included in R , while at the same time important rules are not missed. The proposed method deals with this problem by generating rules on a demand-driven basis, that is, the model generation process is delayed until a query is performed and the retrieved documents are informed. Then, several document-specific models, R d , are quickly generated at query-time, using an efficient rule caching mechanism. Since each model is specifically generated to rank document d , only useful rules are included in R d . Also, the chance of missing important rules for ranking document d is drasti-cally reduced, potentially increasing ranking performance.
Further improvements are still possible by enabling the use of additional information while generating the rules, namely the query terms. In this case, the proposed method generates a specific model, R q d , for each query/document pair. This model is composed of rules of the form q x  X  ... q  X  f j  X  ...  X  f k  X   X  r . This information may turn the gen-erated rules more discriminative and accurate.

To evaluate the effectiveness of the proposed method, we performed a systematic set of experiments using the LETOR benchmark collections (OHSUMED, TD2004, and TD2003) and several evaluation measures (MAP, NDCG and preci-sion). The results show that the proposed method is able to outperform all state-of-the-art learning to rank methods, with gains in MAP ranging from 12% to 123%. Ranking per-formance is improved even further if query terms are used during rule generation, indicating that this information is valuable for ranking documents. Furthermore, the proposed method is extremely fast, being able to assign a rank value to a document in roughly 0.0007 seconds, demonstrating the feasibility of learning to rank at query-time.
Several methods have been proposed on how to compose a ranking function for information retrieval. Zobel and Mof-fat, for example, presented more than one million possibili-ties to compute such functions [27]. Those possibilities take into account essentially a small number of features, such as term frequency, inverse document frequency, and document normalizations. Due to the growth in volume and popularity of the Web throughout the last decade, extra features have been proposed for improving retrieval, including those rela-tive to the document structure (e.g. title, anchor text, and URL) and features concerning the importance of a document based on link analysis (e.g., Page Rank, HITS authority and hub). Thus, learning to rank methods that consider and combine all sorts of features for effective document retrieval and automatic ranking have become a topic of interest.
Several methods based on machine learning techniques [19] have been proposed and applied for learning to rank in in-formation retrieval. According to Cao et al. [5, 6], the cur-rent methods fall into three categories: (i) point-wise, (ii) pair-wise and (iii) list-wise approaches. In the point-wise approach [7, 20], each training example is composed of a set of document features and its corresponding rank relative to a query. The learning process tries to map features into ranks. In the pair-wise approach [4, 5, 12, 13, 14, 15, 21, 23], each training example is composed of pairs of instances and the preference relation among them. In this case, the goal is to classify each pair into correctly or incorrectly ranked categories. Finally, in the list-wise approach [6, 24, 25], a list of documents are used as training instances. A ranking function is learned, and then used to sort documents.
Nallapati [20] proposed a formalization of the ranking task as a binary classification problem (i.e. documents are as-signed as relevant or irrelevant), exploring the use of clas-sifiers such as SVM and Maximum Entropy. Gao et al [13] proposed a discriminative model for ranking (LDM) to op-timize average precision. Herbrich et al. [14] proposed the Ranking SVM method, which is based on the pair-wise ap-proach. Joachims also applied SVM for learning ranking functions using click-through data for training [15]. Other approaches based on SVM include [5, 21, 25]. Burges et al. [4]proposedRankNet,whichisbasedonneuralnetworks.
 Tsai et al. [23] extended RankNet by proposing a fidelity loss function on the basis of the probabilistic ranking framework. Freund et. al [12] proposed RankBoost, a boosting approach for combining preferences. Another boosting-based method is presented in [24].

Other methods to discover ranking functions are based on genetic programming (GP) [16]. Fan et al. have pro-posed several approaches for discovering ranking functions using GP. In [8, 9] a method to automatically generate term-weighting schemes for different contexts (e.g., collections and users) was proposed. The work in [22] presented another GP approach, based on statistical information of the collection, documents, and queries. A combined component approach (CCA) for generating ranking functions was proposed in [2].
The method proposed in this paper uses a different strat-egy to generate ranking models. Instead of optimizing a target measure, the proposed method simply constructs a model which describes the training data using association rules. Then, the generated rules are used to estimate the relevance of documents in the test set. To avoid a combina-torial explosion, the method generates rules on a demand-driven basis, at query-time. As a result, only necessary rules are generated, making the method fast. By learning to rank at query-time, the proposed method can also use more spe-cific information, such as query terms, further improving ranking performance. The method is intuitive (easily un-derstood using a set of illustrative examples), but is also extremely effective, as will be shown in the experiments.
In this section we present a novel ranking method based on association rules. We start by defining association rules, and then we describe how these rules are used for estimating the relevance of documents and generating a ranking.
Association rules are patterns describing implications of the form X X   X  X  ,where X is the antecedent of the rule, and Y is its the consequent. Association rules were initially used for market basket analysis [1], however, more recently they were successfully used in classification tasks [17].
For sake of ranking, we are primarily interested in using the training data, D , to map features to relevance levels. In this case, rules have the form X X   X  r i , where the antecedent of the rule is a set of features and the consequent is a rele-vance level. Two measures are used to quantify the quality of a rule. The support of X X   X  r i , referred as  X  ( X X   X  the fraction of examples in D containing features X and rel-evance r i . The confidence of X X   X  r i , referred as  X  ( is the conditional probability of r i given X . To ensure that the rule represents a strong implication between X and r i a minimum confidence threshold (  X  min ) is employed during rule generation. Also, to avoid a combinatorial explosion while generating the rules, a minimum support threshold (  X  min ) is employed, so that only frequent rules are gener-ated. There are several efficient algorithms for rule genera-tion following the support/confidence paradigm [1, 26].
Consider the collection shown in Table 1, used as a run-ning example in this paper. There are three queries in the training data, and one query in the test set. For each query there are three retrieved documents, and each document is represented by three features  X  PageRank, BM25 and tf (which were normalized and discretized). Thus, there are nine examples in the training data, and three documents in the test set. Let  X  min =0 . 2and  X  min =0 . 67. In this case, the following rule-set (or model), R , is generated: 1. PageRank=[0.85-0.92]  X   X  r =1 (  X  =1.00) 2. PageRank=[0.74-0.84]  X   X  r =0 (  X  =0.67) 3. BM25=[0.56-0.70]  X   X  r =0 (  X  =0.67) 4. tf =[0.46-0.61]  X   X  r =1 (  X  =0.75)
Now, suppose we want to assign a rank to document 10 in the test set using R . Rule 2 is not applicable to document 10, because the feature in its antecedent (PageRank=[0.74-0.84]) is not present in document 10. Let R d be the set of all rules in R that are applicable to document d .Anaive strategy would be to select the rule with highest  X  value in R d , and apply the consequent of the selected rule as the pre-dicted relevance level. One major problem with this strategy is that it neglects all evidence coming from the other rules in R d . An alternative is to use all rules in R d to estimate the relevance of document d . This strategy has the advantage of using all available evidence in R d , potentially providing a better rank estimation.
As previously discussed, in order to better estimate the relevance of a document d , it is necessary to combine all rules in R d . Our strategy is to interpret R d as a poll, in whicheachrule X X   X  r i  X  X  d is a vote given by evidence X for relevance r i . Votes have different weights, depend-ing on the confidence of the corresponding rules (rules with higher  X  values weight more heavily). The weighted votes for relevance r i are summed and then averaged (by the total number of rules in R d that predict relevance r i ), forming the score associated with relevance r i , as shown in Equation 1:
Therefore, for a document d , the score associated with relevance r i is given by the average confidence of the rules predicting r i  X  X  d . Finally, the rank of d is estimated by a linear combination of the normalized scores associated with each relevance, as shown in Equation 2:
The value of rank is an estimation of the true relevance of document d using rules in R d , and ranges from r 0 to r where r 0 is the lowest relevance and r k is the highest one.
The main steps of relevance estimation using association rules are shown in Algorithm 1. To illustrate how this method works, suppose again that we want to rank docu-ment 10 using the rule-set shown in Section 3.1. Rules 1 and 4 predict relevance 1, while rule 3 predicts relevance 0. Thus, according to Eq. 1, the scores associated with relevances 0 and 1 are respectively s (0)=0.67 and s (1)= 1 . 00+0 . 75 Now, according to Eq. 2, the rank value of document 10 is
The next document to be ranked is document 11. How-ever, there is no applicable rule for this document ( R d 11 In order to generate applicable rules to document 11,  X  min should be lowered to 0.1, but in this case several useless rules will also be generated. Next we will present an alternative approach, which generates rules on a demand-driven basis, depending on the document being ranked. Algorithm 1 AR  X  X  Require: Examples in D and T , thresholds  X  min and  X  min Ensure: A rank value rank for each document d  X  X  1: R X  rules extracted from D|  X   X   X  min ,  X   X   X  min 2: for all pair ( d, q )  X  X  do 3: R d  X  rules X X   X  r i in R|X X  d 4: for all i | 0  X  i  X  k do 5: s ( r i )  X  6: end for 7: rank  X  0 8: for all i | 0  X  i  X  k do 9: rank  X  rank + r i  X  s ( r i ) P k 10: end for 11: end for
The method presented in the previous section may not perform well on complex search spaces, such as the ones observed in learning to rank problems. This is because it generates rules before documents to be ranked are known, and the difficulty in this case is in anticipating the rules that will be necessary for ranking documents in the test set. The common approach of using a single value of  X  min to restrict the space for rules can be hard, since important rules may be lost due to this absolute cut-off value.
In this section we present an alternate approach which generates rules exactly as needed to rank a specific doc-ument. Instead of generating a single rule-set, R ,from which non-applicable rules are then removed, the proposed approach directly generates only applicable rules from the training data, resulting in multiple rule-sets, where each is generated exclusively for document d in the test set.
Rule generation is delayed until a set of documents is con-sidered for a given query in the test set. Then, each individ-ual test document is used as a filter to remove irrelevant fea-tures and examples from the training data, D .Thisprocess generates a projected training data, D d ,whichisfocused only on the useful examples for ranking a specific document, d . Therefore, there is an automatic reduction of the size and dimensionality of the training data, since useless examples are not considered during rule generation 1 .Asaresult,for a given value of  X  min , important rules that are not frequent in the original training data, D ,maybecomefrequentinthe filtered/projected training data, D d , providing a better cov-erage of the examples 2 . Since a specific rule-set is generated for each document in the test set, in the end of the process several different models are generated. However, the rank-ing models that are generated from the projected training data ( R d ) are much simpler than the model that would be generated from the entire training data ( R ).
An example e  X  X  is useless for ranking d if e  X  d =  X  .That is, the example e is useless for ranking document d if e does not share any feature with d , since, in this case, no rule generated from e will be applicable to d .
Note that the cut-off value (which is  X  min  X |D d | )may change according to the size of D d . Thus, different docu-ments may imply in different cut-off values.
 Algorithm 2 AR  X  X  d Require: Examples in D and T , thresholds  X  min and  X  min Ensure: A rank value rank for each document d  X  X  1: for all pair ( d, q )  X  X  2: D d  X  X  after projected according to d 3: R d  X  rules extracted from D d |  X   X   X  min ,  X   X   X  min
The main steps of learning to rank at query time are shown in Algorithm 2 (please note that steps 4 to 11 are identical to those shown in Algorithm 1, and thus, they were omit-ted in Algorithm 2). To facilit ate the understanding of how this method works, let X  X  consider again the example in Ta-ble 1. Suppose that we want to rank document 11. The first step is to project the training data based on the features of document 11, forming D d 11 , which is shown in Table 2. As can be seen, only five (out of nine) examples contain useful information for ranking document 11. From D d 11 ,andfor  X  min =0.2 and  X  min =0.67, only two rules are found: 1. BM25=[0.36-0.55]  X  tf =[0.28-0.45]  X   X  r =0 (  X  =1.00) 2. PageRank=[0.51-0.64]  X   X  r =0 (  X  =1.00)
Both rules are applicable to document 11, since they were generated from D d 11 . Also, both rules predict relevance 0, and thus, according to Equation 1, s (0)= 1 . 00+1 . 00 2 s (1)=0. Finally, according to Equation 2, the rank value of Table 2: Training Data after being projected accord-ing to Document 11 (i.e., D d 11 ).

Thus, generating rules on a demand-driven basis has three main advantages. First, only useful rules are generated. Sec-ond, necessary rules are more likely to be included in the ranking model (since multiple cut-off values are employed), and third, there is an automatic reduction in size and di-mensionality of the training data (making rule generation faster).
Processing a rule has a significant computational cost, since this process involves accessing the training data multi-ple times. Different documents may need different rule-sets (models), but different rule-sets may share common rules. In this case, caching is effective in reducing work replication. Algorithm 3 AR  X  X  q d Require: Examples in D and T , thresholds  X  min and  X  min Ensure: A rank value rank for each document d  X  X  1: for all pair ( d, q )  X  X  2: D q d  X  X  after projected according to { d  X  q } 3: R q d  X  rules extracted from D q d |  X   X   X  min ,  X   X   X  4: for all i | 0  X  i  X  k 5: s ( r i )  X  6: end for
Our cache is a pool of entries, and it stores rules of the form X X   X  r i . Each entry has the form &lt;key, data&gt; ,where key = {X ,r i } and data = {  X  ( X X   X  r i ) , X  ( X X   X  r i plementation stores all cached rules in main memory. Before generating a rule X X   X  r i , the proposed method first checks whether this rule is already in the cache. If an entry is found with a key matching {X ,r i } , the rule in the cache entry is used instead of generating it. If it is not found, the rule is generated and then it is inserted into the cache.
When the cache is full, some rules have to be discarded to make room for other ones. The replacement heuristic is based on the support of rules. Specifically, the least frequent rule in the cache is the first to be discarded. There are two reasons to adopt this heuristic. First, the more frequent a rule is, the higher is the chance of using it later. Second, the cost associated with generating more frequent rules is higher than the cost associated with less frequent ones. We show empirically that caching rules is extremely effective.
So far we presented methods that explore only document features while generating rules. In this section we show how to enhance the quality of rules by using an additional ev-idence  X  the query terms. The basic idea is to explore potential associations between query terms and features in order to make rules more discriminative by including query terms in their antecedents. The main steps of this method are shown in Algorithm 3 (please note that steps 7 to 11 are identical to those shown in Algorithm 1, and thus, they were omitted in Algorithm 3). To illustrate how it works, please consider again the example shown in Table 1, and suppose we want to rank document 12 (with  X  min =0.2 and  X  min =0.67). After projecting D accordingtodocument12 (
D d 12 ), the following rules are found: 1. BM25=[0.22-0.35]  X   X  r =0 (  X  =1.00) 2. tf =[0.46-0.61]  X   X  r =1 (  X  =0.75)
Applying Equations 1 and 2, the rank value for document 12 is 0.43. If we allow the rules to also contain query terms, then an additional rule is also found from D q d 12 :
Now, applying Equations 1 and 2, the rank value increases to 0.47. In some cases, query terms can be a valuable infor-mation for ranking, as we will show in the next section.
In this section we present the experimental results for the evaluation of the proposed methods in terms of classification effectiveness and computational efficiency. Our evaluation is based on a comparison against current state-of-the-art methods for learning to rank. We first present the collec-tions employed in the evaluation, and then we discuss the effectiveness of the proposed methods in these collections.
LETOR [18] is a benchmark for research on learning to rank, released by Microsoft Research Asia 3 .LETORmakes available a package composed of three subset (OHSUMED, TD2003, and TD2004), evaluation tools and several baseline evaluation results (such as Ranking SVM [14], RankBoost [12], AdaRank [24], FRank [23], ListNet [6], and MHR [21]). To evaluate the performance of the proposed methods against these baselines, we used NDCG@ n ,P@ n ,andMAPmea-sures. Pre-processing involved only normalization and dis-cretization [11] of features in the training data.
Each subset contains a set of queries, features for query-document pairs, and the corresponding relevance judgments. Features cover a wide range of properties, such as term frequency, BM25, PageRank, HITS etc. In order to con-duct five-fold cross validation, each subset is arranged in five folds, including training, validation and test data.
The OHSUMED collection is a subset of MEDLINE that is a database on medical publications. OHSUMED has 106 queries. For each query is associated a number of documents and their respective relevance degree with respect to the query (i.e. definitely, possibly, or not relevant). In LETOR, there are a total of 16,140 query-document pairs with rele-vance judgments, and 25 extracted features.
 The TD2003 and TD2004 subsets were obtained from TREC 2003 and TREC 2004 collections (topic distillation tasks). There are 1,053,110 html documents and 11,164,829 hyperlinks. There are 50 and 75 queries for TD2003 and TD2004 respectively, and a total of 44 extracted features, including low-level and high-level content features, hyper-link features, and hybrid featu res. Relevance judgment for each query-document pair can be relevant or not relevant.
We start our analysis by evaluating the retrieval qual-ity of the proposed methods, referred hereafter as AR ( R AR ( R d ), and AR ( R q d ), which were described in Algo-rithms 1, 2 and 3, respectively. We used the validation set to obtain  X  min and  X  min values, which were set to 0.001 and 0.25, respectively. Tables 3, 4 and 5 show MAP numbers for OHSUMED, TD2003 and TD2004 subsets, respectively. The result for each trial is obtained by averaging partial re-sults obtained from each query in the trial. The final result is obtained by averaging the five trials. Improvements of the proposed methods over the best baseline are highlighted in bold. We conducted two sets of significance tests (t-test) on these improvements for each subset. The first set of signifi-cance tests was carried on the average of the results for each query. The second set of significance tests was carried on the average of the five trials. The values between parenthe-sis are the p-values corresponding to the comparison with the most competitive baseline in each subset.
Available at http://research.microsoft.com/users/LETOR/
Trial RR d R q d SVM MAP NDCG
For all subsets, the best overall results were always ob-tained by AR ( R q d ). This result is important, since it shows the great value of using query-specific information for sake of ranking. However, this additional information (i.e., query terms) used during model generation, makes unfair a direct comparison of AR ( R q d ) to other methods that do not use this information. So, next we only compare the results ob-tained by AR ( R )andAR( R d ) to the baseline results.
As can be seen in Table 3, all methods showed competitive results in the OHSUMED subset. The worst overall result was obtained by RankBoost (0.440), while the best result was obtained by AR ( R d ) (0.457). The main reason for so much competitiveness is that OHSUMED contains only few features, which are extracted basically from textual evi-dence, reducing the possibilities of improvements. Still, AR (
R d ) showed improvements (relative to the best baseline, which in this case was ListNet) in 4 trials, while AR ( R showed improvements only in the first two trials. The differ-ence in ranking performance between AR ( R )andAR( R d ) is mainly due to the missing rule problem, which happens when there is not a sufficient number of rules to be applied for some documents in the test set. This problem does not occur with AR ( R d ), which generates rules on a demand-driven basis, according to the need of the document being ranked. Similarly, we believe that other methods may also suffer from insufficient evidence during model generation, explaining the best performance obtained by AR ( R d ).
For TD2003, ListNet was again, the most competitive baseline. As shown in Table 4, AR ( R d )showedimprove-ments in 3 trials, specially in the first one. The overall im-provement ranges from 12% (relative to ListNet) to 123% (relative to AdaRank.MAP). In contrast to OHSUMED, TD2003 contains more and diverse features, making possible the achievement of more significant improvements. For TD2004, the most competitive baseline was Rank-Boost. As shown in Table 5, AR ( R d ) showed improvements in the first three trials, specially in the first one. Again, AR ( R d ) showed the best overall results, with overall im-provements ranging from 2.35% (relative to RankBoost) to 31.10% (relative to AdaRank.NDCG).
 The next set of experiments evaluates the effectiveness of AR ( R ), AR ( R d )andAR( R q d ), in terms of NDCG and precision. Figure 1 shows NDCG and precision numbers ob-tained from the execution of the evaluated methods. Again, AR ( R q d ) showed to be the best performer, but we will use AR ( R d ) to make a more fair comparison with the baselines.
For OHSUMED and TD2004, the results are very com-petitive, specially in terms of precision. In terms of NDCG, AR ( R d ) was able to provide a slight overall improvement over the baselines. Again, impressive improvements were obtained using the TD2003 subset. In terms of NDCG, im-provements range from 17.30% (relative to Ranking SVM) to 134.61% (relative to AdaRank.NDCG). In terms of pre-cision, improvements range from 15.38% (relative to Rank-Boost) to 130.67% (relative to AdaRank.NDCG).
The computational efficiency of methods that learn to rank at query-time  X  AR ( R d )andAR( R q d )  X  was evalu-ated through the total execution time, that is, the processing time spent in generating rules and ranking all documents in the test set. The experiments were performed on a Linux-based PC with a Intel Pentium III 1.0 GHz processor and 1.0 GBytes RAM. Figure 2 (left) depicts the times obtained from the execution of AR ( R d )fordifferentcachesizes. We varied the cache size from 0 to 150MB, and for each storage capacity we obtained the corresponding execution time. Clearly, execution time is very sensitive to cache size. Caches as large as 150MB are able to store all rules with no need of replacement, being the best configuration. In the OHSUMED subset, times varied from 243 to 11 seconds (corresponding to an average of 0.10 seconds per query). In the TD2003 subset, times varied from 17,504 to 53 seconds (an average of 0.94 seconds per query). But the most impres-sive result was obtained in the TD2004 subset, where times of AR ( R d ) varied from 38,176 to only 102 seconds (an aver-age of 1.36 seconds per query), an improvement of more than two orders of magnitude, showing that caching is extremely effective, and makes feasible learning to rank at query-time. The reason for the excellent caching performance is depicted in Figure 2 (right), which shows the fraction of documents demanding a specific rule k . For instance, the first rule in the x-axis was demanded for all documents in the test set. That is, the first rule appears in all rule-sets generated by AR ( R d ). Similarly, other rules appear frequently in differ-ent rule-sets, indicating that a large fraction of documents in the test set demands rules in common. Since these rules are cached, execution time is greatly reduced.

We have shown in Section 5.2 that query terms are valu-able for ranking, since the best performance was always ob-tained by AR ( R q d ). By repeating the previous experiment with varying cache sizes, but now using AR ( R q d ), we also observed that exploring query terms during rule generation does not incur in serious overhead. The increase in execution times varies from only 0.9% (TD2004) to 2.2% (TD2003), when query terms are used during rule generation. Figure 2: Left  X  Effect of Caching in Exec. Time. Right  X  Fraction of Documents demanding Rule k .
In this paper we propose and evaluate novel methods that learn to rank. The proposed methods introduce interest-ing innovations, such as the use of association rules and the ability to learn ranking models at query-time (enabling the better use of more discriminati ve information, such as query terms). By generating rules on a demand-driven basis, de-pending on the documents to be ranked, only the necessary information is extracted from the training data, resulting in fast and effective ranking methods. Experimental results, obtained using the LETOR benchmark, indicate that meth-ods that learn to rank at query-time outperform the state-of-the-art methods. The results also suggest that query terms are valuable information for sake of ranking. The running times of the proposed methods, which are able to rank a doc-ument in roughly 0.0007 seconds, is also worth mentioning, and makes feasible learning ranking models at query-time.
As future work, we intend to improve performance in two directions: (i) by combining the results obtained by multiple methods, and (ii) by exploring other query-specific informa-tion, such as query type (navigational, transactional etc.). [1] R. Agrawal, T. Imielinski, and A. Swami. Mining [2] H. Almeida, M. Gon  X  calves, M. Cristo, and P. Calado. [3] R. Baeza-Yates and B. R-Neto. Modern Information [4] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [5] Y. Cao, J. Xu, T. Liu, H. Li, Y. Huang, and H. Hon. [6] Z. Cao, T. Qin, T. Liu, M. Tsai, and H. Li. Learning [7] K. Crammer and Y. Singer. A new family of online [8] W. Fan, M. Gordon, and P. Pathak. Personalization of [9] W. Fan, M. Gordon, and P. Pathak. Discovery of [10] W. Fan, M. Gordon, and P. Pathak. Genetic [11] U. Fayyad and K. Irani. Multi interval discretization [12] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An [13] J. Gao, H. Qi, X. Xia, and J. Nie. Linear discriminant [14] R. Herbrich, T. Graepel, and K. Obermayer. Large [15] T. Joachims. Optimizing search engines using [16] J. Koza. Genetic Programming: On the programming [17] B. Liu, W. Hsu, and Y. Ma. Integrating classification [18] Y.Liu,J.Xu,T.Qin,W.Xiong,andH.Li.LETOR: [19] T. Mitchell. Machine Learning . McGraw Hill, 1997. [20] R. Nallapati. Discriminative models for information [21] T. Qin, X. Zhang, D. Wang, T. Liu, W. Lai, and [22] A. Trotman. Learning to rank. Information Retrieval , [23] M. Tsai, T. Liu, T. Qin, H. Chen, and W. Ma. FRank: [24] J. Xu and H. Li. Adarank: a boosting algorithm for [25] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A [26] M. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. [27] J. Zobel and A. Moffat. Exploring the similarity space.
