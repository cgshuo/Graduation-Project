 REGULAR PAPER Te -We i C h i a n g  X  Tienwei Tsai Abstract In this paper, an image retrieval method based on wavelet features is proposed. Due to the superiority in multiresolution analysis and spatial-frequency localization, the discrete wavelet transform (DWT) is used to extract wavelet fea-tures (i.e., approximations, horizontal details, vertical details, and diagonal details) at each resolution level. During the feature-extraction process, each image is first transformed from the standard RGB color space to the YUV space for the purpose of efficiency and ease of extracting the features based on color tones; then each component (i.e., Y, U, and V) of the image is further transformed to the wavelet domain. In the image database establishing phase, the wavelet coefficients of each image are stored; in the image retrieving phase, the system compares the wavelet coefficients of the Y, U, and V components of the query image with those of the images in the database, based on the weight factors adjusted by users, and find out good matches. To benefit from the user X  X achine interaction, a friendly graphic user interface (GUI) for fuzzy cognition is developed, allowing users to easily ad-just weights for each feature according to their preferences. In our experiment, 1000 test images are used to demonstrate the effectiveness of our system. Keywords Content-based image retrieval  X  Query by example  X  Discrete wavelet transform  X  Color space 1 Introduction With the spread of Internet and the growth of multimedia application, the require-ment of image retrieval is increased. Basically, image retrieval procedures can be roughly divided into two approaches: query-by-text (QBT) and query-by-example (QBE). In QBT, queries are texts and targets are images; in QBE, queries are images and targets are images. For practicality, images in QBT retrieval are of-ten annotated by words. When images are sought using these annotations, such retrieval is known as annotation-based image retrieval (ABIR). ABIR has the fol-lowing drawbacks. First, manual image annotation is time-consuming and there-fore costly. Second, human annotation is subjective. Furthermore, some images could not be annotated because it is difficult to describe their content with words. On the other hand, annotations are not necessary in a QBE setting, although they can be used. The retrieval is carried out according to the image contents. Such retrieval is known as the content-based image retrieval (CBIR) [ 8]. It becomes popular for the purpose of retrieving the desired images automatically. 1.1 Related work in CBIR In the past decade, many research prototype CBIR systems have been developed in universities and research laboratories. Some of these systems are as follows:  X  The Photobook System [ 15], developed by the MIT Media Lab, supports func- X  The VisualSeek system [ 16], developed at Columbia University, supports re- X  The SIMPLIcity System [ 21], developed at Stanford University, uses seman- X  The WALRUS system [ 14], developed at Watson Research Center, uses Probably the best-known ones are as follows:  X  The IBM QBIC system [ 5], developed at the IBM Almaden Research Center,  X  The Virage search engine [ 1], developed by the Virage Incorporation, supports and to define a similarity measure for comparing images. The features serve as an image representation in the view of a CBIR system. Typical methods for extract-ing features include color histograms [ 4, 17], which can be used to characterize the color composition of an image, regardless of its scale or orientation. Color histograms, however, do not contain any shape, location, or texture information. As a result, two images sharing similar color distribution may, in fact, be com-pletely unrelated semantically. Based on the observation that the discrete wavelet transform (DWT) is capable of capturing shape, texture, and location information in a single unified framework, we are motivated to develop a wavelet-based CBIR system, in which the most significant wavelet coefficients for an image are used as its feature vector.
 systems. An obvious drawback of such systems is that the segmentation algo-rithms are complex and computation intensive and the size of the search space is sharply increased due to exhaustive generation of subimages. In addition, the re-trieval performance is affected significantly because the segmentation results are often incorrect. Therefore, the automatic detection of subimages remains an elu-sive task. In our work, we resort to examine global features with the help of DWT multiresolution ability. The experimental results show that the proposed system still has a good feature granularity while decreasing the computation time. 1.2 The contribution of the proposed system Our work mainly focuses on the use of the DWT as a contribution to effective re-trieval in a CBIR system. We will first show the effective representation of images in the DWT domain of the YUV color space. Then, a set of weights are used to characterize the relative importance of the features in a query image, which plays an important role in the multiple passes of refining the retrieval. In addition, to fur-ther improve the performance measurement used for evaluating a CBIR system, a measure revised from efficacy is proposed. 1.3 Overview of the proposed CBIR system In the image database establishing phase, each image is first transformed from the standard RGB color space to the YUV space for the purpose of efficiency and ease of extracting the features based on color tones; then each component (i.e., Y, U, and V) of the image is further transformed to the wavelet domain. In the image retrieving phase, after the features (or the most significant wavelet coeffi-cients) of the query image have been extracted by the feature-extraction module, the similarity-measuring module compares the most significant wavelet coeffi-cients of the Y, U, and V components of the query image and those of the images in the database and find out good matches.
 query only. The retrieval result will be unsatisfactory if the query forms cannot easily represent the content [ 11, 18]. To overcome such problems, our approach allows users to conduct a query by transforming human cognition into numeri-cal data based on their impressions or opinions on the target. In our CBIR system, users can retrieve the desired images that are most similar to the query image from the aspect of the feature(s) the users are interested in via the incorporation of an in-teractive GUI provided by the system. The GUI allows users to adjust the weights for each wavelet feature according to their expectations. As such, the images that are most similar to the query image, or most meet the user X  X  expectations, can be retrieved from the image database.
 calculate the Euclidean distance between the feature vectors representing the two images. In our approach, we define a Euclidean-like distance function to indicate the dissimilarity (or distance) of an image to the query image. Then, the images in the database with the smallest distance to the query image can be found. As such, the desired images can be successfully retrieved from the image database. 1.4 Outline of the paper The remainder of this paper is organized as follows. The next section presents the proposed image retrieval system. Section 3 introduces the feature-extraction module. The similarity-measuring module and the GUI for fuzzy cognition are presented in Sects. 4 and 5. Experimental results are given in Sect. 6. Finally, conclusions are drawn in Sect. 7. 2 System architecture In this section, we first briefly address the CBIR retrieval problem and introduce the framework of our approach. Then we will get into more details of our approach in the following sections. CBIR often comprises both indexing [ 22] and retrieval [6]. Indexing may be subdivided into the steps of feature extraction and feature vector organization. Feature extraction finds out the suitable properties of interest and converts them into mathematical feature vectors. Feature vector organization intends to organize the feature vectors in the database into a structure optimized for searching efficiency. Retrieval is a process that supports the user interaction to retrieve the desired images from the database, which comprises user query fea-ture extraction, query search space strategy, and similarity-measuring method. The user query feature extraction will first convert the user X  X  exemplary image into a mathematical feature vector which is compatible with the feature vectors stored in the database. Afterwards, while conducting the retrieval, the CBIR system will use the search strategy to efficiently search the database of feature vectors for those near to the user query feature vector. Similarity measuring is the computation of the similarity score between a pair of images. It is typically defined by a distance function, e.g., Euclidean distance.
 module and (2) the similarity-measuring module. The architecture of the proposed system is illustrated in Fig. 1. To check if one image is similar to the other or not, we have to compare the features of the two images. Therefore, the feature-extraction module plays an important role in both the image database establish-ing phase and the image retrieving phase. During indexing, each image is first transformed from the standard RGB color space to the YUV space; then each component (i.e., Y, U, and V) of the image is further transformed to the wavelet domain. In the image retrieving process, after the most significant wavelet coef-ficients of the query image have been extracted by the feature-extraction module, the similarity-measuring module compares the most significant wavelet coeffi-cients of the Y, U, and V components of the query image and those of the images in the database and find out good matches. Besides, to obtain the benefits from the user X  X achine interaction, a GUI for fuzzy cognition is developed, allowing users to adjust weights for each feature according to their preferences. The details of each module are introduced in the following sections. 3 Feature extraction Features are functions of the measurements performed on a class of objects (or patterns) that enable that class to be distinguished from other classes in the same general category. To achieve that, we have to extract distinguishable and reliable features from the images. Before the feature-extraction process, the images have to be converted to the desired color space. 3.1 Color space A color space is the multidimensional space in which different dimensions rep-resent different components of color. It specifies how color information is rep-resented. The common used color spaces [ 3, 4] include RGB, CMY(K), YUV, YCrCb, MTM, HSV, HSB, HLS, CIE L  X  a  X  b  X  ,CIE L  X  u  X  v , etc. The RGB and CMY(K) color spaces are hardware-oriented. The RGB (red, green, and blue) color model is perhaps the simplest color space, which is used in most color CRT monitors. The CMYK (cyan, magenta, yellow, and black) space is a color space perceptually uniform. Alternative color spaces can be generated by transform-ing the RGB color space. For example, linear transformations of the RGB color spaces produce a number of important color spaces that include YUV (NTSC, PAL, and SECAM color television standard), YCrCb (JPEG digital image cod-ing standard and MPEG digital video coding standard). These linear transforms, each of which generates one luminance channel and two chrominance channels, were designed specifically to accommodate targeted display devices: YUV X  X olor television and YCrCb X  X olor computer display. Because these color spaces are not uniform, color distance does not correspond well to perceptual color dissim-ilarity. On the other hand, some user-oriented color models were developed ac-cording to human perception of colors, including MTM, HSV family, and CIE family. Humans feel colors through hue, saturation, and brightness percepts. One of the first attempts to organize human color observations in a model was car-ried out by Munsell at the beginning of the twentieth century [ 13]. It is conceived mainly for artists and is based on subjective observations rather than on direct measurement of hue, saturation, and brightness properties. Although one trans-formation from RGB to Munsell, named the mathematical transform to Munsell (MTM), was investigated, there does not exist a simple mapping from color points in RGB color space to Munsell color chips. HSV family color models, such as HLS (hue, lightness, and saturation), HSV (hue, saturation, and value), and HSB (hue, saturation, and brightness) are approximations of the Munsell system. These spaces, which have been used extensively in computer vision and computer graph-ics, are nonuniform color spaces and require nonlinear transformations from the RGB color space. CIE-based color spaces are the most complete color models used conventionally to describe all the colors visible to the human eye. They were developed for this specific purpose by the International Commission on Illumi-nation (Commission Internationale d X  X clairage). CIE L  X  a  X  b  X  (CIELAB) and CIE L u  X  v  X  (CIELUV) are generated by nonlinear transformation of the RGB space, with the goal of deriving uniform color spaces. However, they are inconvenient because of the necessary nonlinearity of the transformations to and from the RGB color space. As yet, the determination of the optimum color space is still an open problem.
 the image that is not or is only minimally visible, producing the smallest possible perceptible change to the image. To determine what information can be discarded, an elementary understanding of the human visual system (HVS) and the opponent color theory is required. The HVS processes color information by converting the red, green, and blue data from the cones into a luminance X  X hrominance space, with the luminance channel having approximately five times the bandwidth of the chrominance channel. Consequently, much more error in color (or chrominance) than in luminance information can be tolerated in compressed images. The oppo-nent color theory was first proposed by Hering [ 9]. He thought that the colors red, yellow, green, and blue are special in that any other color can be described as a mix of them, and that they exist in opposite pairs. That is, either red or green is perceived and never greenish red.
 space. The YUV model is based on the HVS and the opponent color theory of human vision and intends to approximate color differences as perceived by hu-mans. In our approach, the RGB images are first transformed to the YUV color space for two reasons: (1) efficiency and (2) ease of extracting the features based on the color tones. It is well known that the Y component of an image is much more significant than are the U and V components of the image; therefore, in the feature-extraction process, the wavelet coefficients kept for the U or V component can be much less than those kept for the Y component without sacrificing two much useful information for retrieval. Moreover, compared with the features de-rived from the RGB color space, the features derived from the YUV color space are more suitable for the purpose of screening the images based on a certain color tone. For example, if we need to verify whether an image X  X  color tone is red or not, it makes vain attempt to analyze the R component of the RGB image because most of the energy of the R component contributes to the luminance of the image; on the other hand, it is more effective to analyze the V component of the image, which eliminates the component that constitutes the luminance of the image. The details of RGB and YUV color spaces are introduced as follows. 3.1.1 RGB color space A gray-level digital image can be defined to be a function of two variables f ( x , y ) , where x and y are spatial coordinates, and the amplitude f at a given pair of co-ordinates is called the intensity of the image at that point. Every digital image is composed of a finite number of elements, called pixels, each with a particular lo-cation and a finite value. Similarly, for a color image, each pixel ( x , y ) consists of the intensity of the red, green, and blue color in the pixel, respectively. In CBIR, the RGB color model has a major drawback on the similarity measure. The light-ness and saturation information are implicitly contained in the R, G, and B values. Therefore, two similar colors with different lightness may have a large Euclidean distance in the RGB color space and are regarded as different. 3.1.2 YUV color space Originally used for PAL (European  X  X tandard X ) analog video, YUV is based on the CIE Y primary, and also chrominance. The Y primary was specifically de-signed to follow the luminous efficiency function of human eyes. Chrominance is the difference between a color and a reference white at the same luminance. The following equations are used to convert from RGB to YUV spaces: and luminance, the blue chrominance, and the red chrominance, respectively. From (2), it can be found that the blue chrominance U is obtained from removing the luminance component Y from the blue component B; the red chrominance V is obtained similarly. Figure 2 gives a sample image to illustrate the RGB and YUV color space. There are three objects in this image, namely, the red apple, the green lemon, and the blue hat. It can be found that to discriminate the R, G, B colors in the RGB color space is quite difficult even though the value of brightness of the objects in their dominant color component is larger; for instance, the value of the brightness of apple is larger in the R channel (see Fig. 2(b)). On the other hand, the blue and red colors can be identified easily from the U and V channels (see Fig. 2(f) and (g)). After converting from RGB to YUV, the features of each image can be extracted by the DWT. 3.2 DWT To extract distinguishable and reliable features from YUV color images, DWT can be applied to each component (i.e., Y, U, and V) of the images. Basically, each component of a YUV image can be regarded as a gray-level image and DWT are mainly used for gray-level images; therefore, the color images can be transformed to the wavelet domain to constitute the main features of the original images. information of light intensity in a gray scale code from 0 (black) to 255 (white). The intensity can be considered as a function of two variables f ( x , y ) ,where x and y are spatial coordinates, and the amplitude f at a given pair of coordinates is called the intensity of the image at that point. A gray-level image of size M  X  N can be decomposed into its wavelet coefficients by using Mallat X  X  pyramid al-gorithm [ 12] for multiresolution analysis (MRA), where a function is viewed at various levels of approximations or resolutions. By applying the MRA, we can di-vide a complicated function into several simpler ones and study them separately. Mathematically, it can be described as the following recursive equations: and decomposed. L and H are used to indicate low-and high-frequency components. H and G correspond to the low-pass and the high-pass filters, respectively. Ex-pression 2  X  1(1  X  2) denotes sampling along column (row), and k is the level of wavelet decomposition. Equations (4) X (7) indicate that any image signal can be decomposed in a specific wavelet domain. In fact, it can be found that by re-constructing the original signal with only the level in question (all other wavelet levels equivalent to zero) and then performing standard Fourier analysis on the reconstructed signal, it could be seen that the wavelet analysis is acting like a fil-ter on the signal, with each wavelet level corresponding to a different frequency band in the original signal. After wavelet decomposition, the object image en-ergy is distributed in different subbands, each of which keeps a specific frequency component. In other words, each subband image contains one feature. Intuitively, the feature at different subbands can be distinguished more easily than that in the original image.
 The output of high-pass filters LH ( 1 ) , HL ( 1 ) ,and HH ( 1 ) are three subimages with the same size as low-pass subimage LL ( 1 ) , representing different image details in different directions, i.e., horizontal details, vertical details, and diagonal details. LL ( 1 ) corresponds to the approximation under the first resolution level. Figure 4 shows a sample image to illustrate the wavelet decomposition. From this figure, we can find that the horizontal, vertical, and diagonal details of the image are ex-tracted to the LH , HL ,and HH subimages, respectively; moreover, the resolutions HH ( 1 ) . Therefore, by the wavelet decomposition, we can obtain not only the di-rectional features of the images, but also the features at various resolution levels. ferent subbands, each of which keeps a specific frequency component. In other words, each subband image contains one feature. It is concluded that the feature at different subbands can be distinguished more easily than that in the original image. 4 Similarity measurement In general, the accuracy of the retrieved results is directly proportional to the ac-curacy of the similarity measurement. A similarity (or distance) measure is a way of ordering the features from a specific query point. The retrieval performance of a feature can be significantly affected by the distance measure used. Ideally, we want to use a distance measure and feature combination that gives best retrieval performance for the collection being queried. In our experimental system, we de-fine a measure called the sum of squared differences (SSD) to indicate the degree of distance (or dissimilarity) between the query image and an image in the image ficients of the Y component of the query image Q and that of a database image X i under LL ( k ) subimage, respectively. Then, the distance between Q and X i under the Y component and LL ( k ) subimage can be defined as D the component Y, i.e., D Y ( Q , X i ) , can be defined as the weighted combination of these subimages: where K is the level of wavelet decomposition, and w YLL ( k ) is the weight of dis-wise, the distances between Q and X i under the component U and V, i.e., D nents are less important than the Y component, only the highest level subband and D 5 GUI for fuzzy cognition To benefit from the user X  X achine interaction, we develop a GUI for fuzzy cogni-tion, allowing users to adjust the weight of each feature more easily according to their preferences. Each weight can be regarded as the fuzziness of the cognition to the associated feature. Users can emphasize the features that are relatively im-portant based on their feelings or opinions. As such, users can retrieve the desired images that are most similar to the query image from the aspect of the feature(s) the users are interested in.
 imageis3;thatistosay,thevalueof K in (9) is 3. Since Y component is the most significant one among the Y, U, and V components, we keep more coefficients for Y component than for U or V component. Thus, the number of features derived weights that correspond to the 12 features can be adjusted by the scrollbars ap-peared in the lower right frame of the GUI as shown in Fig. 5.AsforUandV components, only the most significant two features, i.e., ULL ( 3 ) and VLL ( 3 ) ,are used in our system; the corresponding two weights are w U and w V , which can be adjusted by the GUI of the system. The scrollbars in the lower middle frame of the GUI (see Fig. 5) correspond to weights w Y , w U ,and w V in (12). You can use a guideline that assumes you are looking at the query image from a differ-ent distance to determine which wavelet level and which details (approximation, horizontal, vertical, or diagonal) should be emphasized in the query. The higher level decomposition corresponds the more distance from where the query image is being seen.
 and minimize the retrieval of irrelevant images. This aim can be approached by adjusting the weights of the distinguishing features of the query image. Basically, the impact of emphasizing a certain feature is twofold: (1) including the images that are very similar to the query image in terms of this feature and (2) excluding the images that are quite dissimilar to the query image in terms of this feature. Therefore, the function of the weight adjustment is to improve the retrieval effi-cacy based on the users X  observation or experience.
 the techniques widely used in optimization can be applied. There is a common philosophical background underlying the principle of optimization. When one ap-proaches a complex decision problem, involving the selection of values of a num-ber of variables, one should focus on a single objective (or some objectives) de-signed to qualify performance and measure the quality of the decision. In CBIR, In other words, the objective of the weight adjustment is to obtain the retrieved images with the highest efficacy. To obtain the best assignments of weights, one straightforward way is to enumerate all possible assignments of weights. However, although the optimal solution can be found through this kind of exhaustive search, it is very time-consuming and, therefore, impracticable. On the other hand, local search [ 7] is a good alternative. Local search searches for improvement within its local neighborhood using a testing for improvement and, if there is any improve-ment, takes an action for improvement. Intuitively, if the local search starts from a solution which is good from the viewpoint of the entire searched area, it is very likely to approach a near-optimal solution.
 YUV-DWT2 method uses adjustable weights. As will be illustrated later, YUV-DWT1 could be a starting point while searching for a global optimal solution. The weight adjustment embedded in the YUV-DWT2 method can be described as follows:
Step 1: Initially, assign the weights of the YUV-DWT2 the same values as those
Step 2: Based on the user X  X  observation, select the most distinguishing feature
Step 3: Increase the weight of the selected feature.
Step 4: If there is any improvement, then accept the new assignments; otherwise, YUV-DWT1. More specifically, the selected distinguishing feature can be the de-light feature (i.e., the feature of the query image, which the retrieved irrelevant images do not have) or the disliked feature (i.e., the feature of the retrieved irrele-vant images, which the query image do not have). Emphasizing the distinguishing features may include the relevant images or exclude the irrelevant images, both of which can improve the retrieved results. 6 Retrieval evaluation Our CBIR system is evaluated by performing retrieval experiments that carry out QBE on a color image database which was downloaded from the WBIIS database [20 ]. This test bed consists of a collection of 1000 color photographs depicting a variety of subjects, including butterflies, flowers, animals, scenery, building, and so forth. The system is developed on a personal computer (Intel Pentium 4 CPU 2.8 GHz, 512 MB DRAM) running Windows XP with MATLAB v. 6.5 installed. 6.1 Experiments for illustration To illustrate the effectiveness of our approach, we conducted a series of experi-ments using different type of feature sets. 6.1.1 Experiment 1 In the first experiment, a lake scene of size 128  X  85 is used as the query image. Figure 5 shows the main screen of our CBIR system. The retrieved results are the top ten images in similarity, where the items are ranked in the ascending order of the distance to the query image from the left to the right and then from the top to the bottom. The lower right frames in the figure consist of the scrollbars, by which users can adjust the weight of each feature at each wavelet level. The values of weights are between 0 and 1. In this case, weights w Y and w YLL ( 3 ) are set to 1, and the others are set to 0; in other words, the Y-component approximations at level 3 is used as the main feature. Figure 6 shows the retrieved results based on YLL ( 3 ) (i.e., luminance), YLL ( 3 ) + ULL ( 3 ) (i.e., luminance + blue chrominance), and YLL ( 3 ) + VLL ( 3 ) (i.e., luminance + red chrominance), respectively, using the same query image. As shown in Fig. 6(b), using the Y-component approximations at wavelet level 3 as the main feature is not good enough; the fifth to eighth retrieved images are obviously not similar to the query image from the viewpoint of color tone. Since the color tone of these dissimilar images is much redder than that of the blue-tone query image, one may consider emphasizing the weight of the feature based on the U component (or blue chrominance). Though the retrieved results YLL ( 3 ) as the main feature (see Fig. 6(c)), they are not as good as those using emphasizing the feature based on V component (or red chrominance) has a better result, which is quite short of our expectations. In fact, it can be explained from the viewpoint of screening. In other words, the features which appear in many images are not very useful for distinguishing a relevant image from a non-relevant one. In this scenario, the problem to find blue-tone images can be reduced to the problem of determining which images are not red-tone by setting a larger value of w V . 6.1.2 Experiment 2 In the second experiment, we use a cluster of flowers as the query image. The retrieved results based on different main features are shown in Fig. 7. It can be found that the best results are based on YLL ( 3 ) and ULL ( 3 ) features (see Fig. 7(c)). Just like emphasizing the feature based on red chrominance may be helpful for retrieving blue-tone images, emphasizing the feature based on blue chrominance may also be helpful for retrieving red-tone images. 6.1.3 Experiment 3 In this experiment, a celestial body is used as the query image. When YLL ( 3 ) is used as the main feature, the celestial bodies of various color can be retrieved asshowninFig. 8(b). Therefore, we can conclude that the YLL (or the wavelet approximations under luminance) feature corresponds to the shape feature. Obvi-ously, users can increase the weight of ULL ( 3 ) or VLL ( 3 ) in order to retrieve the celestial bodies that are not only similar to the query image in shape but also in color. The results based on YLL ( 3 ) + ULL ( 3 ) and YLL ( 3 ) + VLL ( 3 ) are shown in Fig. 8(c) and (d). 6.1.4 Experiment 4 effect of emphasizing directional features. Since the sunset scene is a horizontal-oriented image; intuitively, horizontal-oriented images can be retrieved by simply increasing the weight of the horizontal feature. As shown in Fig. 9(c), the retrieved (i.e., approximations). However, the retrieved result can be further improved by increasing the weight of the feature that the query image does not have (e.g., the vertical or diagonal feature). Figure 9(d) and (e) shows the retrieved results based on the vertical and diagonal details of the Y component at wavelet level 3. It can be found that the best retrieved result is obtained when the diagonal details are used as the main feature.
  X  the retrieved results can be improved via the incorporation of users;  X  the retrieved results can be improved, no matter positive or negative features w the YUV-DWT1 method. Though YUV-DWT1 cannot provide the best solution, its solution can serve as a starting point of the process toward the best solution. In other words, we can approach a global optimal solution by adjusting one weight each time on the basis of YUV-DWT1, which is coined the YUV-DWT2 method. Further comparisons between the two methods will be provided in the next sub-section. 6.2 Efficacy of image retrieval The evaluation of image retrieval is a difficult yet essential issue for the successful development of CBIR systems. The difficulty is arisen because of two major rea-sons: (1) the retrieval performance of a system highly depends on the agreement between the similarity measure used and human judgments of similarity and (2) neither a standard test database nor a standard performance measure is available. Two commonly used performance measures, the precision rate p and the recall rate r , can be applied for CBIR, which are defined as and where M is the total number of retrieved images, N r is the total number of relevant images in the database, and n r is the number of relevant images retrieved. It is assumed that the user inspects the first m images of the ranked list. So for every m = 1 , 2 ,..., M , precision p the number of relevant images is greater than the size of the answer list, recall is meaningless as a measure of the retrieval quality. To overcome this problem, a measure called efficacy is introduced [ 10]: where  X  M is the efficacy of retrieval for a given list of size M .If N r  X  M ,  X  M becomes the traditional recall of information retrieval; if N r &gt; M ,  X  M is indeed the precision of information retrieval. Since the appearance of images with differ-ent semantics may look alike, using efficacy may exclude some  X  X ood X , but not  X  X orrect X  retrieval. Therefore, we develop a modified version of efficacy: where s r is the total relevance score of the images retrieved. To assess the ground-truth relevance score of each retrieved image for each query, each target image in the collection is assigned a relevance score as follows: 1, if it belonged to the same class as the query image; 0.5, if partially relevant to the query image; and 0, otherwise. In the following experiment, we use the modified version of efficacy as the performance measure. A more effective retrieval method often shows a higher efficacy.
 ods, which are summarized in Table 1, are compared, using the modified efficacy as the performance measure. A set of query images, referring to images depicting bears, eagles, flowers, deer, and butterflies, are served as the benchmark queries. An instance of the retrieval result is given in Fig. 10 . The comparison results are summarized in Table 2. The number of retrieved images used for evaluating the efficacy, i.e., M , is set to 10. To avoid the subjectivity that may occur in the weight adjustment of the YUV-DWT2 method, the trials were repeated five times involv-ing different users to access the mean performance of the YUV-DWT2 method. It can be found that the YUV-DWT2 approach outperforms the other approaches. Since the color-histogram-based approaches, i.e., RGB-H and YUV-H, do not contain any shape, location, or texture information, some retrieved images shared similar color distribution but with completely unrelated semantics. The RGB-DCT approach captures the shape and texture information, but does not sufficiently cap-ture the color information in the images so that the efficacy is low. The efficacy of the YUV-DWT1 approach is slightly better than YUV-DCT X  X . It is also shown that, with the involvement of users, YUV-DWT2 performs best among the six CBIR methods. 7 Conclusions In this paper, a CBIR method that exploits wavelet features and fuzzy-cognition concepts is proposed. The DWT is applied to extract low-level features from the images due to its superiority in multiresolution analysis and spatial-frequency lo-calization. To achieve QBE, the system compares the most significant wavelet coefficients of the Y, U, and V components of the query image and those of the images in the database and find out good matches by the help of users X  cogni-tion ability. Since there is no feature capable of covering all aspects of an image, the discrimination performance is highly dependent on the selection of features and the images involved. Since several features are used simultaneously, it is nec-essary to integrate similarity scores resulting from the matching processes. An important part of our system is the implementation of a set of flexible weighting factors for this reason. For each type of feature, we will continue investigating and improving its ability of describing the image and its performance of similarity measuring.
 References Author Biographies
