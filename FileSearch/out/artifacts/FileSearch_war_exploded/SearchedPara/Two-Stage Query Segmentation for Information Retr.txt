 Modeling term dependence has been shown to have a signifi-cant positive impact on retrieval. Current models, however, use sequential term dependencies, leading to an increased query latency, especially for long queries. In this paper, we examine two query segmentation models that reduce the number of dependencies. We find that two-stage segmenta-tion based on both query syntactic structure and external information sources such as query logs, attains retrieval per-formance comparable to the sequential dependence model, while achieving a 50% reduction in query latency. Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Query Formulation General Terms: Algorithms, Experimentation, Theory Keywords: Query segmentation, term dependence, long queries
Modeling term dependencies can have a significant pos-itive impact on retrieval performance [1, 3, 4, 6, 7]. For instance, Metzler and Croft [6] show that a sequential depen-dence model , which uses all ordered adjacent pairs of query terms, is highly effective for retrieval on TREC corpora. Bai et al. [1] demonstrate similar results on a web search engine.
In the web search setting, where query latency is crucial, the computation of all sequential dependencies may become prohibitively expensive for long queries, as the number of de-pendencies grows linearly with the size of the query. Hence, it would be beneficial to reduce the number of dependencies, while still retaining the improvements in retrieval effective-ness brought on by modeling term dependencies.

Recent work on query segmentation [2, 3, 4, 5, 8] seeks to address this problem. For natural language queries (e.g., TREC topic descriptions), a segmentation can be obtained by a shallow syntactic parser [4]. For non-grammatical long queries, often encountered in web search (e.g.,  X  X ew york times square X  ), supervised segmentation that leverages in-formation from external sources such as query logs, web corpora and Wikipedia has been shown to be more effec-tive [2, 5, 8]. In fact, the segmentation techniques need not be mutually exclusive. As we show, a supervised segmenter trained on an external data source can be applied as a second stage for a better segmentation of long noun phrases. within the boundaries of a noun phrase chunk are depen-dent, and there are no dependencies between the chunks. The two-stage segmentation model, TDM , further segments the long 2 noun phrase chunks into finer-grained concepts. Second stage segmentation of the noun phrase chunks uses not only POS tags, but also features that were found to be useful for query segmentation [2, 8] (see Table 2).
In this section, we evaluate the retrieval performance at-tained by the proposed segmentation models. In our experi-ments, we use three TREC corpora: one newswire collection (ROBUST04) and two web collections (W10g and GOV2). Indexing and retrieval is done using the Indri 3 toolkit. The indexes are stemmed using a Porter stemmer. For comput-ing functions f O , f U in Eq. 1, we use Indri ordered window ( #OW1 ) and unordered window ( #UW8 ) operators, respectively.
The queries are stopped using a custom list of 52 stop-words, designed to remove some of the frequent stop pat-terns in description queries (e.g.,  X  X ind information X  ). In all retrieval experiments, Dirichlet smoothing with  X  = 1500 is used. Only the description portion of TREC topics is used.
Noun phrase chunks are extracted using a syntactic shal-low parser CRFChunker 4 trained on WSJ corpus. Second stage segmentation is done using a sequential multi-purpose chunker YamCha 5 . For training the second stage segmen-tation model we use a corpus of 500 pre-segmented noun phrases [2]. Table 2 details the features used in training.
Table 3 summarizes the retrieval efficiency and effective-ness of the proposed query segmentation models. For ef-ficiency, we report the CPU time required to run all the queries. For effectiveness, we report P@5, MAP and b-pref.
In terms of effectiveness, all segmentation models outper-form the independence model. SDM and TDM are the best performing methods and have no statistically significant dif-ferences, save for MAP on ROBUST04. TDM always out-performs NDM , verifying our assumption about the utility of applying the finer-grained second segmentation stage.
