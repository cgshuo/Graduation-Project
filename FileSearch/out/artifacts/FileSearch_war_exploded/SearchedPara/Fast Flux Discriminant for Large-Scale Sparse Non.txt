 In this paper, we propose a novel supervised learning method, Fast Flux Discriminant (FFD), for large-scale nonlinear clas-sification. Compared with other existing methods, FFD has unmatched advantages, as it attains the e ffi ciency and in-terpretability of linear models as well as the accuracy of nonlinear models. It is also sparse and naturally handles mixed data types. It works by decomposing the kernel den-sity estimation in the entire feature space into selected low-dimensional subspaces. Since there are many possible sub-spaces, we propose a submodular optimization framework for subspace selection. The selected subspace predictions are then transformed to new features on which a linear model can be learned. Besides, since the transformed features nat-urally expect non-negative weights, we only require smooth optimization even with the ! 1 regularization. Unlike other nonlinear models such as kernel methods, the FFD model is interpretable as it gives importance weights on the original features. Its training and testing are also much faster than traditional kernel models. We carry out extensive empiri-cal studies on real-world datasets and show that the pro-posed model achieves state-of-the-art classification results with sparsity, interpretability, and exceptional scalability. Our model can be learned in minutes on datasets with mil-lions of samples, for which most existing nonlinear methods will be prohibitively expensive in space and time. H.2.8 [ Database Management ]: Database Applications-Data Mining; J.3 [ Computer Applications ]: Life and Medical Sciences classification; interpretability; sparsity; submodularity
In supervised classification, there are several potentially competing needs. For example in biomedical applications, classifiers often need to be feature-sparse ,inordertoidentify leading risk factors for prevention and intervention of crit-ical medical conditions. In addition, the training sets can be very large, thus requiring good scalability ,buttheclas-sifier should also be expressive enough to model nonlinear feature interaction. Finally, and possibly most importantly, the classifier must also achieve high accuracy .Theserequire-ments are by no means specific to biomedical applications. In fact, they are representative for many machine learning application domains. Although several methods excel at var-ious aspects, to date none manages to capture all of them.
Linear classifiers [33] cover many of these needs: they can be highly scalable [8], learn weights that are naturally in-terpretable and, if paired with ! 1 regularization, can also be feature-sparse [7, 29]. Because of these strengths, they are acommonchoiceformachinelearningpractitioners. How-ever, linear classifiers fall short on at least two of the desired requirements: they cannot learn nonlinear decision bound-aries, which inherently limits their accuracy on di ffi cult or lower-dimensional datasets. Further, they cannot discover the relevance of features that are beneficial only through nonlinear interactions with each other.

Nonlinear classifiers can model such feature interactions and are not limited by linear decision boundaries, but they typically su ff er from di ff erent limitations. For example, deep neural nets [12] are highly nonlinear and scalable, but do not naturally incorporate feature sparsity and the decisions are hard to interpret. Similarly, tree ensembles, such as Random Forests [4], share the same weaknesses. The kernel trick [26] is a popular method to extend linear classifiers to learn nonlinear decision boundaries, but it also removes their natural interpretability and scalability.

To get the best out of both nonlinear methods (nonlin-ear separability) and linear methods (high e ffi ciency), a re-cent trend in data mining and machine learning is to replace heavy nonlinear machineries with simpler linear ones, and to achieve high accuracy by introducing certain nonlinearity in feature mapping .Thatis,theymaptheoriginalinput data x  X  R D to a feature vector  X  ( x )  X  R M and then learn alinearhyperplane w T  X  ( x ), w  X  R M [17,22,24,30]. A lin-ear classifier, trained on  X  ( x )  X  R M instead of the X  X aw X  X ea-tures x  X  R D ,canlearnnonlineardecisionboundariesinthe original input space while maintaining its high scalability. Unfortunately, however, this approach does not also main-tain its interpretability and feature-sparseness. The classi-fier weights can no longer be interpreted, which drastically limits the usefulness of this approach for many applications tied to the discovery of feature importance.

As our first contribution, we propose a novel algorithm, which we refer to as Fast Flux Discriminant (FFD). FFD is designed to combine all the strengths of linear and nonlin-ear supervised learning. It maintains the scalability, feature-sparsity and interpretability of linear classifiers, yet can learn non-linear decision boundaries, discover pairwise feature in-teractions and it matches the high accuracy of state-of-the-art nonlinear models.

At its core, FFD is based on kernel density estimation (KDE) [3]. The ultimate goal of (discriminative) super-vised learning is to estimate the conditional label distribu-tion p ( y | x ), for a label y of an input x  X  R D .Ifunlimited labeled data were available, this distribution could be esti-mated directly with KDE. In practice, however, such a na  X   X ve approach does often not work. The curse of dimensional-ity [3] makes the data requirements of KDE grow exponen-tially with the data dimensionality, thus quickly exceeding any realistic limitations, if the data is su ffi ciently high di-mensional. However, if the dimensionality is low, KDE is very e ff ective.

Chen et al. [6] make use of KDE as a feature transfor-mation. They point out that if features do not interact with each other, the conditional label distribution can be de-where [ x ] d denotes the d th dimension of x and the weights w d are estimated from the data. 1 Similar to Rahimi and Recht [24], this approach learns an explicit feature trans-formation that enables linear classifiers to learn non-linear decision boundaries. Unfortunately, it cannot discover non-linear feature interactions.

In this paper we follow this insight but relax the restric-tion that features cannot interact with each others. Instead, we assume that features can interact, but their interactions are limited to r " D features. Let A denote a set or  X  X ag X  of features, with | A |  X  r ,andlet[ x ] A denote the shortened in-put vector x with only those features in A .Weassumethat r is small enough, so that p ( y | [ x ] A )canstillbecomputed fast and accurately via KDE. FFD learns a new representa-tion x  X   X  ( x ), where each dimension of  X  ( x )correspondsto the conditional label distribution p ( y | [ x ] A )forsomefeature bag A .A linear classifier learned on the data  X  ( x )canlearn nonlinear decision boundaries and its weights indicate which feature  X  X ags X  are most important. If in addition the classi-fier is regularized to be sparse, we can identify which feature bags are in fact su ffi cient to make accurate predictions. Be-cause KDE is highly non-linear and approximates the true conditional distribution, FFD reliably learns the within-bag feature interactions.

One immediate challenge with this setup is the exponen-tially growing number, identify which such sets of features to include in our repre-sentation? Our second contribution is to formulate this as atractableoptimizationproblem. Weproposeanovelop-timization framework which maximizes the similarity cover-age and minimizes redundancy of the selected feature set.
This approximation is exact for w d =1ifthefeaturesare label conditionally independent, which is also often referred to as the Na  X   X ve Bayes assumption [3].
 Moreover, the optimization formulation contains cardinal-ity penalty for sparsity and interpretability. We formulate this selection problem as a combinatorial optimization with asubmodularobjective. Previouswork[9]provesthatfor this category of problems, there exists a 1/3-approximation bound in the worst case.

To further promote feature sparsity of the FFD model, we also employ ! 1 regularization. A disadvantage of typical ! regularization is that it is not di ff erentiable everywhere and requires non-smooth optimization techniques such as sub-gradient descent. However, since the KDE features in FFD model conditional label distribution, their weights are natu-rally expected to be non-negative. Based on this key insight, we add non-negativity constraints to the training objective and make it a smooth optimization problem even with ! 1 regularization. This enables us to employ fast smooth opti-mization algorithms.

Finally, we carry out extensive experiments on real-world datasets and show that the proposed FFD model achieves state-of-the-art classification results with exceptional scala-bility. FFD can be learned in minutes on datasets with mil-lions of samples, for which other nonlinear methods will be prohibitively expensive in space and time. FFD also demon-strates interpretability and results in very sparse models.
This paper is organized as follows. In Section 2, we present the notations and preliminaries. We describe the proposed FFD model in Section 3. We discuss related work in Section 4andpresentexperimentalresultsinSection5. Section6 gives conclusions.
In this section, we introduce the notations and review the density-based logistic regression (DLR) model [6], which is highly related to the proposed model.

Assume we are given a dataset D = { x i ,y i } , i =1 ,  X  X  X  ,N , x  X  R D where D is the number of attributes 2 ,andthelabel y  X  {  X  1 , 1 } .Lettheinputvectorbe x i =([ x i ] 1 ,  X  X  X  , [ x D can be further partitioned into two datasets D 1 and D  X  1 ,whichincludeallthedatapointswhoselabelsare y =1 and y =  X  1, respectively.

In the training phase, the DLR model first maps each training sample to a new feature space in a dimension-wise manner, i.e. x  X   X  ( x )where
Similar to logistic regression, DLR models the conditional probability of y given a sample x by a sigmoid function on the transformed features. where the parameter w can be learned via maximum likeli-hood estimation or equivalently the empirical risk minimiza-tion with logistic loss:
For simplicity, here we only discuss datasets with numerical features. The proposed method also applies to datasets with categorical and mixed features.
If we use the original attributes without feature mapping, i.e.  X  d ( x )=[ x ] d for d =1 ,  X  X  X  ,D ,Eq. (1)istheoriginal LR.

Unlike LR, DLR introduces a nonlinear feature mapping based on the following rationale. First, Eq. (1) can be re-written as:
Assuming conditional independence of the attributes given the class label y ,itcanbeshownthat[6] and setting w =1,wecannaturallyderivethefollowing feature transformation used by DLR. For each dimension d , the DLR feature mapping is: which is a logit transformation on the conditional probability of y given a single feature [ x ] d .

In order to compute the features  X  d ( x ) in (5), DLR es-timates p ( y | [ x ] d )bytreatingcategorialandnumericalat-tributes in di ff erent ways.
In the training phase, DLR first computes the feature mapping and then calls a standard LR package to learn w . In the testing phase, given a testing sample x ,theDLR model first transforms it to  X  ( x ) via KDE or counting de-pending on the feature type. The conditional probability of y given x is then calculated by Eq. (1).

Though DLR o ff ers good interpretability as it assigns a weight to each original dimension, it has some serious draw-backs. 1) DLR has high training and testing complexity. Al-though DLR is more e ffi cient in its training time, its feature computation is still expensive. For a dataset with N samples and D dimensions, DLR requires O( DN 2 )timefortraining and O( DN )timefortestingonesinglesample.Suchatest-ing cost is the same as kernel SVM, making it too expensive for applications where extensive testings are required. 2) DLR generates dense vectors and does not o ff er sparsity. 3) DLR assumes conditional independence of each dimension give the class label, which is often violated in practice and may su ff er from high correlation between dimensions. In this section, we propose our FFD model. Like DLR, FFD also performs a feature mapping based on density es-timation and then learns a linear machine after the feature mapping. However, FFD is significantly di ff erent from DLR and o ff ers a few salient advantages. It does not assume con-ditional independence and is able to capture the correla-tion between features. Also, it preserves the interpretability and explicitly promotes sparsity of the model. Moreover, the learning process of FFD is far more e ffi cient than DLR, leveraging on the fast computing of low-dimensional density estimation via histogram estimation.

In summary, the main steps of FFD include the following. 1. Subspace feature mapping, which generates features 2. Submodular subspace selection, which selects subspace 3. Model training, which learns a sparse and interpretable
All the above steps are designed to be highly e ffi cient and capable of scaling to large data. Below, we discuss the main components of FFD before putting them together.
This step generates features that can e ff ectively model the conditional probability p ( y | x ), which enables the nonlinear separability of the model. It is based on non-parametric den-sity estimation which does not make any parametric assump-tion of the data distribution and is particular suitable for large data since it can make full use of all the data samples. Here, we first describe the histogram-based density estima-tion in general for the full feature space, which is unrealistic due to the curse of dimensionality. But the same idea can be applied to and is very e ffi cient for low-dimensional sub-spaces. We then combine the predictions from all subspaces via a linear model and generate nonlinear classification de-cision boundary.
 First, each dimension is divided into equal-length bins. Let b d be the number of bins for the d th dimension. If [ x ] is categorical, b d is always the number of categories for this feature. If [ x ] d is numerical, b d is a parameter we need to set.

In the training phase, given the training dataset D = { x i ,y i } , i =1 ,  X  X  X  ,N ,weassigneachtrainingsampletothe corresponding bin. If [ x i ] d is a categorical feature, B ([ x the bin index for dimension d is the category index of [ x For a numerical feature, suppose the bins of the d th dimen-sion start at and end at The bin length l d is given by: Let B ([ x i ] d )bethebinindexfor[ x i ] d .Wehavethat Figure 1: Histogram estimation for a 2-D subspace. Left: a snapshot of a 2-D grid. Right: The propor-tion of y =1 in each grid cell based on counting.
Each data sample x i corresponds to a grid cell 3 (a vector of bin indices):
After assigning each training sample to the correspond-ing grid cell, a naive histogram-based density method would estimate the the probability of p ( y | x )bycountingthepro-portion of samples with di ff erent labels in grid cell B ( x ). This naive histogram-based density estimation is e ffi cient and does not make any assumption on the distribution of the underlying data. However, if the number of training samples is not enough, this estimation would su ff er from huge bias and variance. In addition, the number of grid cells grows exponentially with the number of dimensions, leading to the curse of dimensionality. As a result, this simple histogram method is not practical.

To overcome this problem, instead of directly modeling p ( y | x )bydensityestimationforthewholespace,FFDex-presses p ( y | x )byanumberofdensityestimationforlowdi-mensional subspaces. Each subspace contains a small num-ber (less than r )offeatures.Inessence,weassumethatfea-tures can interact with each other, but their interactions are limited to r " D features. We assume that r is small enough, so that the density estimation for r -dimensional subspaces can still be computed fast. For example, Figure 1 shows the histogram-based density estimation for a 2-dimensional sub-space. This subspace is discretized into a grid as shown on the left subfigure. The proportion of training samples with y = 1, i.e. p ( y =1 | B ), is then computed for each grid cell by simple counting as shown on the right subfigure.
To combine the density estimations from all subspaces, we convert the result from each subspace to a new feature and then apply a linear model on these new features. Specifically, FFD learns a new representation x  X   X  ( x ), where is a vector of M subspace features. For each m =1 ,  X  X  X  ,M , FFD uses the following feature
In this paper, the meanings of  X  X in X  and  X  X rid cell X  are interchangeable. We often refer to  X  X in X  in the context of a single dimension and  X  X rid cell X  otherwise Figure 2: Kernel smoothing for a 1-D histogram.
 The height of each bin is the proportion of y =1 in each bin based on counting. The purple dots are the new p ( y =1 | B ) for each bin after kernel smoothing. where A m  X  { 1 ,  X  X  X  ,D } , | A m |  X  r ,isasubsetoffeature dimensions and [ x ] A m =([ x ] d  X  A m )are x  X  X  values in the di-mensions included in A m .

Following the design of logistic regression, FFD models the following probability:
FFD estimates p ( y =1 | [ x ] A m ) via histogram-based den-sity estimation. Since the cardinality | A m |  X  r where r " D ,itavoidsthecurseofdimensionality.

For relatively small datasets, the histogram estimation for the subspace specified by A m is still unstable. First, if the length of bins is too small, there would be few data samples in each bin, resulting in inaccurate estimation. Second, the histogram estimation could be non-smooth for neighboring bins.

To address these issues, we propose to use a bin ker-nel smoothing technique, which allows the bins to a ff ect each other according to their mutual kernel. Suppose B = ( B 1 ,  X  X  X  ,B | A m | )isagridcellinthesubspacespecifiedby the dimensions in A m and denote G A m as the set of all the grid cells in this subspace. We have the following smoothed estimate for grid cell B where n B " ( y )isthethenumberoftrainingsampleswith label y in B # ,and N B " is the total number of training sam-ples in B # .Thekernelbetweentwogridcells B and B # is a Guassian kernel given by where h d &gt; 0isaparametercalledthe bandwidth of the kernel density function. A popular rule of thumb [27] for deciding the value of h d is as follow: where  X  d is the standard deviation of the training samples on d th dimension. Figure 2 illustrates the kernel smoothing for a1-dimensionalhistogram. Aswecanobserve, p ( y =1 | B ) in the third bin is originally very low due to the lack of data. However, after kernel smoothing, this value gets higher and the overall histogram becomes smoother.

In essence, the proposed bin kernel smoothing is an ap-proximation of KDE by discretizing each dimension into bins and treating all samples in the bin as located at the center of the bin. Not surprisingly, we can show that the approx-imation error approaches zero as the discretization is fine enough.

Proposition 1. As b d  X  X  X  ,theconditionalprobability estimated by (12) approaches the result by the Nadaraya-Watson estimator [3] where D 1 is the set of training samples with label y =1 and K A m ( x 1 , x 2 ) is a Guassian kernel function on variables specified by A m , namely,
The size of memory required to store all histogram infor-mation is O total number of grid cells required for the subspace speci-fied by A m .Since | A m | is always very small (e.g. less than 3), the memory cost will not be large. One advantage of FFD is that the size of the model does not increase as the training dataset gets larger. This is di ff erent from other pop-ular non-parametric models such as RBF-SVM and KNN. For example, in RBF-SVM all the support vectors should be stored. As the training set gets larger, the number of support vectors will also increase.

In addition, in the testing phase, the kernel matrix be-tween support vectors and testing samples needs to be com-puted, resulting in O ( N t N s )runningtimewhere N t is the size of the testing data and N s is the number of support vec-tors. In contrast, FFD has a linear test time O ( N t ), since for a given testing sample FFD only needs to retrieve the value stored in the histogram in order to compute  X  m .
Let U be the ground set containing all subspace candi-dates, i.e. all A m  X  { 1 ,  X  X  X  ,D } such that | A m |  X  r .The cardinality of U is one key challenge is to determine which subspaces in U to choose from. Here we propose a combinatorial optimization framework to address this problem. Suppose S  X  U is the set of subspaces we select, we propose to find S such that maximize where c i,j is the Pearson correlation between  X  i and  X  j on A i and A j ,respectively. a i is the training accuracy of the subspace estimation for G A i ,whichcanbecomputedef-ficiently by simply checking the number of mislabeled sam-ples in all grid cells of G A i .(  X  ,  X  ,  X  )areallnon-negative hyper-parameters.

The four components in (17) correspond to four di ff er-ent goals, respectively. 1) The first term, which is a cut function [10], maximizes the similarity coverage of the set U so that set S is a good representative of U .2)Thesecond term minimizes the pair-wise correlations within S to reduce the redundancy and relieve the problem of highly correlated features and co-linearity. 3) The third term maximizes the overall accuracy of histogram estimation of the selected sub-spaces. 4) The fourth term minimizes the cardinality of S for sparsity.

The maximization of (17) is a NP-hard problem. However, we show that (17) is a submodular maximization problem and good approximation bound can be achieved. We first introduce the concept of submodular set functions.
Definition 1. [10] Suppose U is the ground set, a set func-tion f :2 U  X  R is submodular if it satisfies the property of diminishing return: for every A, B  X  U , A  X  B ,andevery e  X  U  X  B ,wehavethat If equality always holds in (18), f is modular.

It has been shown that maximizing a submodular function without any constraints can achieve a 1/3-approximation bound using a deterministic local search (DLS) algorithm [9, 13]. That is, where f obj is the objective function in (17), S g is the solution by the DLS algorithm, and S  X  is the optimal solution. Note that this lower bound only occurs in the worst case. In practice, the performance is typically much better. Now we prove the submodularity of (17).

Theorem 1. The objective function in (17) is submodu-lar if c i,j  X  0 ,  X  i, j  X  U .

Proof. Let It has been shown that f c ( S )issubmodularif c i,j  X  0 ,  X  i, j  X  U [20]. Moreover, it is easy to verify that  X  modular function and that  X   X  | S | 2 is submodular function according to Definition 1. Since submodularity is preserved under non-negative linear combination of submodular func-tions [10], we see that the the objective function in (17) is submodular.

In practice, we find that most c i,j are non-negative, since each  X  m is an indicator of the label y and they are not likely to have negative correlation. For completeness, we have the following lemma to help guarantee the submodu-larity of (17).

Lemma 1. Let  X  =  X  min { min i,j { c i,j } , 0 } ,then f c  X  (1 +  X  ) | S | 2 is submodular where f c ( S ) is defined in (19).
Proof. Let e S be the | U |  X  1indicatorvectorofset S where U is the ground set, and e S ( i )=1if A i  X  S and 0 otherwise 4 (so e U is a vector whose elements are all 1). Let C be the correlation matrix of all elements in U where its element ( i, j )is c i,j .Werewritethefunctioninmatrixform as follows e
S ( i )isthe i th element in e S Suppose E is a | U |  X  | U | matrix with all elements being 1. It is easy to show that | S | 2 = e &amp; S Ee S .Thus,wehavethat Now we define According to the definition of  X  ,wehavethat  X   X  0and elements in C + are all non-negative. Thus,  X  (  X  +1) e &amp;  X  E ) e S is submodular due to the submodularity of nega-tive quadratic function [2]. In addition, we can see that e
Ce U +  X  | S | is modular. Thus, f c ( S )  X   X  (1 +  X  ) | S | submodular.

Theorem 2. When  X   X   X  ,theobjectivefunctionin(17) is submodular and the DLS algorithm has a 1/3-approximation bound in the worst case.

Proof. According to Lemma 1, (17) can be rewritten as $ where c + i, j are elements in C + defined in (22). Since c 0, we see that (23) has the same form as defined in Theorem 1if  X   X   X   X  0. Thus, it is submodular and has a 1/3-approximation bound [9].

In practice, we can first compute all the c i,j and  X  ,and then choose a  X  value such that  X   X   X  .Therefore,wecanal-ways guarantee the submodularity and optimization bound.
Since the dimensionality of subspaces should be relatively small to avoid the curse of dimensionality, in this paper we have a restriction that | A m |  X  2. So there are M = O ( D ( D +1) / 2) potential  X  m ( x )including1-dimensionaland 2-dimensional subspaces. After all those  X  m ( x )arecom-puted by smoothed histogram estimation and filtered by submodular subspace selection, we need to learn the weight vector w =( w 1 ,  X  X  X  ,w M ) 5 in (11). We require strong spar-sity on w ,whichisnotsatisfiedbythelearningframework in (2).

One simple alternative is to replace the ! 2 regularization with ! 1 regularization [11] which promotes sparsity. How-ever, the FFD model has a nice structure and can o ff er an even better solution. Let us rewrite (10) as  X  m ( x )=ln p ( y =1 | [ x ] A m ) where g ( z )=ln z 1  X  z is a logit function. We can observe that  X  m ( x )isanincreasingfunctionof p ( y =1 | [ x ] A addition,  X  m ( x )  X  0if p ( y =1 | [ x ] A m )  X  0 . 5andlessthan0 otherwise. Given the monotonic relation between p ( y =1 | x ) and  X  m ( x )in(11),theweight w m on  X  m ( x )issupposedto be non-negative under the assumption that p ( y =1 | [ x ] estimated by histogram is a weak learner. Given that w is
Now M is the number of  X  m after submodular subspace selection.
 Algorithm 1 The learning algorithm for FFD 1: for m =1 to M do ( Histogram estimation 2: Build grid G A m . 3: for i =1 to N do 4: Assign x i to the grid cell indexed by (9) 5: end for 6: for B  X  G A m do 7: Compute p ( y | B )by(12) 8: end for 9: end for 10: for i =1 to N do ( Computing  X  m ( x i ) 11: for m =1 to M do 12: Compute  X  m ( x i )by(10) 13: end for 14: end for 15: Select subspaces by solving (17) using the DLS algo-16: Learn w by solving (25) non-negative, we have that &amp; w &amp; 1 = formulation with ! 1 regularization becomes: minimize subject to w  X  0
Compared with other ! 1 norm method, the ! 1 norm in our framework is di ff erentiable for all feasible w .Suchasmooth optimization problem with simple bound constraints can be e ffi ciently solved by gradient-descent solvers.
Using (25), FFD also enforces more sparsity than tradi-tional ! 1 regularization. To see that, assume that the gra-dient of a positive w m is negative. When doing gradient descent on it, w m will tend to decrease its value. But due to the non-negative constraint, it cannot go below 0 and will thus end up at 0, which leads to even more sparse solution than ! 1 regularization.

The overall learning algorithm for FFD is shown in Algo-rithm 1. We make a few further comments about FFD.
 Cross validation. Before executing the complete algo-rithm of FFD as stated in Algorithm 1, all the hyperparame-ters should be pre-specified including  X  in (25) and (  X  ,  X  ,  X  ) in (17). Experts can set these hyper-parameters by good intuition. For non-experts, a typical way to tune them is utilizing a k -fold cross-validation where grid search is per-formed on all hyper parameters to minimize the validation error. However, this brute-force algorithm has a high com-putational cost. In addition, it does not make use of the validation errors during the grid search.

As a better solution,we use a recent Bayesian optimiza-tion technique [28] to tune the hyper-parameters in order to globally minimize the validation error. Specifically, the validation error is modeled as a sample from a Gaussian Process (GP). Each time, we sample the hyper-parameters (  X  ,  X  ,  X  ,  X  )whichminimizetheexpectationofitsvalidation error under the assumption of GP. Then, Algorithm 1 is run on this hyper-parameter sample to evaluate its actual cross-validation error. This sample with its validation error are then incorporated in the previous GP and forms a new GP. This process keeps running until a maximum iteration limit is reached. The best hyper-parameters are the one that have the minimum cross-validation error among all the samples. It has been shown that this Bayesian optimization method for cross validation largely outperforms normal grid search [28].
 Usefulness of submodular subspace selection. Though ! regularization encourages sparsity, the performance of lin-ear methods with ! 1 regularization (including logistic regres-sion) su ff ers if the input variables are highly correlated or even co-linear [11,14]. Thus, before training FFD by solving (25), it is necessary and important to do subset selection on all  X  m in order to reduce their correlation.
 Interpretability. Similar to LR, FFD can provide prob-ability of its prediction in addition to the predicted label. However, in LR model the weights on di ff erent features may not be directly comparable. For example, the measurement of blood pressure has a vastly di ff erent scale than that of height, and their weights are not comparable. In contrast, the weights w in FFD do not su ff er from this problem be-cause all  X  m are on the same scale as they model condi-tional probabilities. In addition, each  X  m ( x )isanindicator of p ( y =1 | [ x ] A m )becausetheyhaveamonotonicrelation-ship as shown in (10). With the sparsity of FFD, we can also identify which feature bags A m are in fact su ffi cient to make accurate predictions based on the weights. Since kernel density estimation for the subspaces is highly non-linear and approximates the true conditional distribution, FFD reliably learns the feature interactions.
Our FFD model is related to density-based logistic regres-sion (DLR) [6] which transforms the original raw features to anewfeaturerepresentationbyKDEandthentrainsalo-gistic regression model on the new feature vectors. However, it does not promote sparsity and is not capable of handling feature interaction. In addition, compared to DLR, the fea-ture transformation in FFD allows much faster computation, reducing the training time from O( DN 2 ) to O( DN )andthe testing time from O( DN )to O ( D ).

In general, FFD is related to recent works on learning lin-ear models with explicit nonlinear feature transformations. For example, random kitchen sinks (RKS) [24] transforms each data x into a finite-dimensional vector  X  ( x )  X  R M approximate the RBF-kernel function for any two inputs x is the RBF kernel function. This allows highly scalable linear classifiers in the transformed space to learn approximately the same decision boundaries as SVM with a RBF kernel in the original input space. The recent fastfood algorithm [17] further speeds up RKS using matrix approximation tech-niques and reduces the time and space complexities. Other feature mapping techniques include those based on random projection [1,15,18,23], polynomial approximation [21], and hashing [19,32]
Existing feature mapping techniques, when combined with linear classifiers, can achieve both nonlinear separability and higher scalability of linear classifiers. However, they cannot take advantage of the interpretability of linear classifiers. The feature mapping techniques such as Fourier transfor-Table 1: Comparison of the characteristics of di ff er-ent classifiers.
 Interpretable Yes No Yes No Yes E ffi cient Yes No No Yes Yes Nonlinear No Yes Yes Yes Yes
Sparse Yes No No No Yes mation, random projection, and hashing are all defined in a di ff erent space than that of the original features. As a re-sult, these models do not provide a weight for each original feature dimension and cannot o ff er a clear notion of inter-pretability. Moreover, all these works except for [18] do not explicitly support sparsity.
In this section, we conduct extensive experiments to eval-uate the proposed FFD model. We evaluate three versions of FFD.
In FFD-2 and FFD-sfo, for  X  m involving two dimensions, we picks the top 3 D out of D ( D  X  1) / 2such  X  m  X  X  accord-ing to their accuracies for histogram estimation, where D is the dimensionality of the training samples. For all the three methods, we set the number of bins for numerical features to 50, i.e. b d =50. Forsubmodularmaximizationin(17),we use the sfo toolbox developed by Andreas Krause [16]. To solve the sparse learning problem in (25), we use the min-Conf TMP function 6 .Thisfunctionissuperioratoptimiz-ing smooth objective functions subject to bound constraints, which is exactly our case.

Baseline methods .Wealsoconsiderfourothermethods for comparison. 1) Logistic regression with ! 1 regulariza-tion (LR) [11], implemented by Mark Schmidt [25]. 2) Sup-port vector machines with the RBF kernel (SVM-rbf). We use the LibSVM library [5]. 3) Density-based Logistic Re-gression (DLR) [6], a linear classifier with nonlinear feature mapping based on the naive Bayes assumption. 4) Random Kitchen Sink (RKS) [24], a linear classifier with nonlinear feature mapping that approximates the RBF kernel.
Acomparisonofthecharacteristicsofthesemodelsis given in Table 1. We can see that FFD is the only model that can support all of the desirable properties including interpretability, nonlinearity, e ffi ciency, and sparsity.
Cross validation . The hyper-parameters in all meth-ods are tuned via cross validation. The cross validations are performed by Bayesian optimization which has been proved to be far more e ffi cient and accurate than a simple grid search [28]. For the RKS model, we also cross validate all
Available at http://www.di.ens.fr/~mschmidt/Software/minConf Figure 3: The decision boundary of FFD-1 on a toy example. Left: FFD-1 with 20 bins for each dimen-sion. Right: FFD-1 with 100 bins for each dimen-sion. types of features including random Fourier features and ran-dom binning features.

Visualization of a toy example. We create a nonlin-early separable dataset to visualize the classification ability of FFD method and the e ff ect of using di ff erent number of bins, as shown in Figure 3. We can observe that the FFD-1canperfectlyseparatethetwoclasseswhileotherlinear methods cannot. We also see that, as the number of bins increases, the decision boundary gets smoother.

Comprehensive evaluation .Table2showsacompre-hensive comparison of all methods on various datasets. All these datasets are publicly available at the UCI repository or the LibSVM website 8 .Thedatasetsaresortedbythe number of samples in the datasets. The experiments are run on an o ff -the-shelve desktop with two 8-core Intel(R) Xeon(R) processors of 2.67 GHz and 128 GB of RAM. The implementations of all methods are in or through the inter-face of MATLAB TM .InorderforSVM-rbftorunasfastas possible, we set the cache size of LibSVM to 10 GB which is su ffi ciently large for all the tested datasets. From Table 2, we can observe the following facts.
In terms of accuracy, the performance of FFD methods is fairly strong in general. Datasets including checkboard, banana, mnist38 and cod-rna are well-known for its high non-linearity, which can be told by the poor performance of LR on these datasets. However, both FFD-2 and FFD-sfo perform almost as good as SVM-rbf and RKS, which demonstrates their superior nonlinear classification ability. And for datasets including splice and Adult, FFD methods largely outperforms all the other methods.

In terms of running time, we can see that FFD models are very e ffi cient in general, which is fairly comparable with and sometimes even better than the linear machines LR and RKS. For example, the Adult dataset has 14 features, among which 8 are categorical. Models such as LR, SVM-rbf and RKS should first convert the categorical features to numerical features before training. The most popular method, as recommended in [31], is using k binary numer-ical features to represent an k -category feature. For exam-ple, (red,blue,green) can be represented by (1,0,0), (0,1,0) https://archive.ics.uci.edu/ml/datasets.html http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ datasets/binary.html and (0,0,1). We observe that the original 14 features in-crease to 104 features at the end, which adds to the burden of training. In contrast, our FFD models naturally handle categorical features since each category is a bin in its dimen-sion and no kernel smoothing is needed, leading to superior time e ffi ciency.

We also see that FFD models are much more e ffi cient than nonlinear models such as SVM-rbf and DLR. For the cod-rna dataset with 0.27 million samples, it takes FFD models less than 1 second while SVM-rbf spends over 10 minutes and DLR cannot finish. The improvement is even more salient on the kddcup99 dataset which contains almost 5 million samples. We make kddcup99 a binary classified dataset by setting class 6 to a positive class and other classes to a nega-tive class, since class 6 contains 57% of the samples. It takes FFD models less than one minute while it is prohibitively large for both SVM-rbf and DLR.

However, though FFD-sfo generally has good accuracy and e ffi ciency, we also point out that the cost of submod-ular optimization is sensitive to the dimensionality of the datasets, and using submodular subspace selection may not be the most sensible for image datasets such as mnist38. However, even for the mnist38 dataset, FFD-sfo is much faster than SVM-rbf and DLR, while FFD-1 and FFD-2 are extremely fast.

In terms of practical usability, FFD models give inter-pretability and sparsity, while other nonlinear models in-cluding SVM-rbf and RKS do not. In summary, FFD is a clear winner considering all the aspects.
 Cases of highly correlated features. As stated in Section 3, linear methods including models with ! 1 regu-larization become more unstable and less accurate when highly correlated features or co-linearity exist. Although FFD methods are capable of classifying nonlinear datasets, they are linear models on  X  m and thus may also su ff er from this issue. In this case, FFD-sfo is better than FFD-2 since it rules out highly correlated  X  m by maximizing the submodu-lar objective function in (17). This e ff ect has been partially reflected in Table 2.

To demonstrate the advantage of FFD-sfo over FFD-2 in amoreobservableway,weconductanotherexperimentin the feature selection context. First, we make a new splice data set by duplicating each feature, which makes them co-linear. Then, we run FFD-2 and FFD-sfo on both the origi-nal and new datasets. For reference, we also run the LR and Lasso [11] models which are state-of-the-art feature selection methods. For FFD methods, one feature is considered se-lected if it is used by any  X  m whose w m / =0.Figure4shows the curves about accuracy versus number of selected fea-tures. First, we observe that FFD methods is much better than LR and Lasso since FFD o ff ers nonlinear separabil-ity. Second, although FFD-2 and FFD-sfo have comparable performance on the original dataset, FFD-sfo is way better than FFD-2 on the new dataset, demonstrating the ability of FFD-sfo in addressing the issue of highly correlated or co-linear features.

Real-world clinical prediction. In addition to evalua-tion on public benchmark datasets, we also test FFD models on a real-world clinical application. This is a collaboration with Barnes-Jewish Hospital, one of the largest hospitals in the US. The task is to predict potential ICU transfers for hospitalized patients based on 34 vital signs. The data col-in 2 hours or memory overflow.
 splice dataset with co-linear features. lection process can be found in [6]. On this dataset, FFD-sfo and FFD-2 have 95.37% and 94.59% accuracy, respectively. The results outperform the DLR model with a 93.26% ac-curacy which is found to be better than LR and SVM [6]. Moreover, FFD is the only model that can o ff er sparsity and interpretability. This is vitally important in clinical practice since healthcare personnel can be informed of the most im-portant risk factors and take proper actions for prevention and intervention.
Many applications in the big data era find existing classi-fiers inadequate. They often require not only high accuracy but also high e ffi ciency, sparsity, and interpretability. To date no classifier, linear or nonlinear, excel at all these as-pects. We have presented a novel Fast Flux Discriminant (FFD) model which delivers all these desirable properties. FFD learns a linear discriminant on top of a non-parametric feature mapping which captures nonlinear feature interac-tions. FFD addresses the curse of dimensionality by decom-posing the kernel density estimation in the original feature space into low-dimensional subspaces. We have also pro-posed a submodular optimization framework to select sub-spaces and to address the problem of highly correlated fea-tures and collinearity. Moreover, FFD attains feature spar-sity using ! 1 regularization. A nice feature of FFD is that its density-based features naturally have non-negative weights and hence allows for smooth optimization, which is not pre-viously possible for ! 1 regularization. Empirical results have shown that FFD delivers similar accuracy as state-of-the-art nonlinear classifiers, but with sparsity, interpretability, and much better scalability. To the best of our knowledge, this is the first classifier that possesses all these merits. Given its unprecedented combination of advantages, we believe FFD will become a popular general-purpose classification model for a large scope of real-world applications such as biomed-ical prediction. WC and YC are supported in part by the CNS-1017701, CCF-1215302, and IIS-1343896 grants from the National Science Foundation of the United States, a Microsoft Re-search New Faculty Fellowship, a Washington University URSA grant, and a Barnes-Jewish Hospital Foundation grant. KQW is supported by NSF grants 1149882, 1137211. [1] D. Achlioptas. Database-friendly random projections: [2] F. Bach. Learning with submodular functions: A [3] C. M. Bishop. Pattern Recognition and Machine [4] L. Breiman. Random forests. Machine Learning , [5] C.-C. Chang and C.-J. Lin. LIBSVM: A library for [6] W. Chen, Y. Chen, Y. Mao, and B. Guo.
 [7] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. [8] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [9] U. Feige and V. S. Mirrokni. Maximizing [10] S. Fujishige. Submodular Functions and Optimization: [11] T. Hastie, R. Tibshirani, and J. H. Friedman. The [12] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast [13] R. Iyer, S. Jegelka, and J. A. Bilmes. Fast [14] G. James, T. Hastie, D. Witten, and R. Tibshirani. [15] P. Kar and H. Karnick. Random feature maps for dot [16] A. Krause. Sfo: A toolbox for submodular function [17] Q. Le, T. Sarlos, and A. Smola. Fastfood -computing [18] P. Li, T. J. Hastie, and K. W. Church. Very sparse [19] P. Li and A. Konig. b-bit minwise hasing. In Proc. [20] H. Lin and J. Bilmes. Multi-document summarization [21] K. Lin and M. Chen. E ffi cient kernel approximation [22] O. Pele, B. Taskar, A. Globerson, and M. Werman. [23] N. Pham and R. Pagh. Fast and scalable polynomial [24] A. Rahimi and B. Recht. Random features for [25] M. Schmidt, G. Fung, and R. Rosales. Fast [26] B. Sch  X  olkopf and A. J. Smola. Learning with kernels . [27] B. W. Silverman and P. J. Green. Density Estimation [28] J. Snoek, H. Larochelle, and R. Adams. Practical [29] R. Tibshirani. Regression shrinkage and selection via [30] A. Vedaldi and A. Zisserman. E ffi cient additive kernels [31] C. wei Hsu, C. chung Chang, and C. jen Lin. A [32] K. Weinberger, A. Dasgupta, J. Langford, A. Smola, [33] G. Yuan, C. Ho, and C. Lin. Recent advances of
