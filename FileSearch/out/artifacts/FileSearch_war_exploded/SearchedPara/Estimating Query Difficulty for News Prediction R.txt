 News prediction retrieval has recently emerged as the task of re-trieving predictions related to a given news story (or a query). Pre-dictions are defined as sentences containing time references to fu-ture events. Such future-related information is crucially important for understanding the temporal development of news stories, as well as strategies planning and risk management. The aforemen-tioned work has been shown to retrieve a significant number of rele-vant predictions. However, only a certain news topics achieve good retrieval effectiveness. In this paper, we study how to determine the difficulty in retrieving predictions for a given news story. More precisely, we address the query difficulty estimation problem for news prediction retrieval. We pr opose different entity-based pre-dictors used for classifying queries into two classes, namely, Easy and Difficult . Our prediction model is based on a machine learn-ing approach. Through experiments on real-world data, we show that our proposed approach can predict query difficulty with high accuracy.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models ; H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation (effi-ciency and effectiveness) Algorithms, Experimentation, Performance Query difficulty estimation, Relevance ranking, News predictions, Future events
What will happen in the eurozone after the financial crisis? How will health care change in the post-genomic society? When can re-newable energy replace fossil fuels? These questions commonly arise when reading news stories, which reflect our anticipation and curiosity about the future. While future-related information helps people understand the temporal development of news stories, it can also be used for strategies planning to avoid/minimize disrup-tions, risks, and threats, or to maximize new opportunities. Can-ton [3] describes the describes the future trends that can influence our lives including an energy crisis, the global financial crisis, poli-tics, health care, science, securities, globalization, climate changes, and technologies. Knowing about the future related to such topics is not only demanded by individuals ,butalso organizations , e.g., business firms or official governments.

In this paper, we address the retrieval and ranking task defined in [11], so-called ranking related news predictions . The objective of the task is to retrieve and rank predictions related to a given news story (or a query). Predictions are defined as sentences mentioning future dates, for instance, Under the new rules, eurozone countries have to slash their budget deficits to a ceiling of 3% of GDP by next year ,or Cisco reported it expects mobile web video traffic to increase 250-fold between 2011 and 2015 . In the same study [11], they showed that nearly one third of news articles contain refer-ences to the future, as captured by time mentions of future dates in the articles.

While the approach proposed in [11] has been shown to retrieve a significant number of relevant predictions, the quality of result pre-dictions vary greatly for different topics. In other words, only a cer-tain type of queries achieves good retrieval effectiveness. Thus, we seek to improve the retrieval effectiveness for the worst performing queries, namely, entity queries, by studying query difficulty estima-tion . More precisely, we will focus on how to predict the quality of result predictions for a given topic or a news story, or estimating query difficulty for news prediction retrieval  X  to the best of our knowledge the first approach tackling this objective.

The main contributions in this paper are: 1) the first study of es-timating query difficulty for news prediction retrieval, 2) propos-ing different predictors used for estimating query difficulty, and 3) extensive experiments for evaluating the proposed predictors us-ing the New York Times Annotated Corpus in combination with queries selected from real-world future trends [3] and relevance as-sessments from [11].

The organization of the rest of the paper is as follows. In Sec-tion 2, we give an overview of related work. In Section 3, we de-scribe the task of ranking related news prediction and the models for annotated documents, predictions, and queries. Then, we ex-plain the problem of query difficulty estimation. In Section 4, we present a model for ranking result predictions. In Section 5, we propose novel predictors used for estimating query difficulty. In Section 6, we evaluate the proposed predictors and discuss results in detail. Finally, in Section 7, we conclude the paper.
The problem of query difficulty estimation [4] (also known as query performance prediction) has recently gained increasing in-terest from the IR community. Existing approaches to predicting query performance can be categorized wrt. two aspects [7]: 1) time of predicting (pre/post-retrieval) and 2) objective of task (difficulty, query rank, effectiveness). Pre-retrieval predictors work indepen-dently from a specific retrieval model and result documents, and such predictors are preferred to post-retrieval based methods be-cause they are based solely on query terms, collection statistics and possibly external knowledge, such as WordNet or Wikipedia.
By measuring the specificity of query terms, the effectiveness of a query can be estimated by assuming that the more specific a query, the better effectiveness it will achieve. In order to deter-mine the specificity, different heuristic-based predictors have been proposed, for example, the averaged length of a query [12], the av-eraged inverse document frequency [5] and the averaged inverse collection term frequency [8]. The summed collection query sim-ilarity [15] employs both term frequencies and inverse document frequencies. Another approach for estimating query difficulty is to measure query ambiguity . An example of an ambiguity based predictor is a set coherence score [9] measuring the ambiguity of a query by calculating the similarity between all documents that contain the query term.

The predictors presented above ignore term relatedness among query terms. To measure the relationship between two terms, point-wise mutual information (PMI) is computed as suggested in [7]. PMI measures the term relationship by observing co-occurrence statistics of terms in a document collection. Two PMI-based pre-dictors are proposed in [7] including the averaged PMI value and the maximum PMI value of all query term pairs. More detailed de-scriptions of different approaches to query difficulty estimation can be found in the book by Carmel and Yom-Tov [4], and references therein.

The problem of future information retrieval was first presented by Baeza-Yates [1]. He proposed to extract temporal mentions of future events from news articles, index and retrieve such informa-tion using a probabilistic model. A document score was computed by multiplying a keyword similarity and a time confidence, i.e., a probability that the future events will actually happen. Jatowt et al. [10] proposed an analytical tool for extracting, summarizing and aggregating future-related events from news archives using a clustering method. In recent work, Kanhabua et al. [11] proposed the novel task of ranking related news predictions with the main goal of improving the retrieval effectiveness of future information (as captured by mentions of future dates in news articles). They proposed a ranking model based on a learning-to-rank technique, which is learned using different f eatures. None of aforementioned work addresses the problem of query difficulty estimation for future information retrieval, which is the topic of this paper.
In this section, we briefly describe the task of ranking related news predictions . Then, we outlin e the models for annotated doc-uments, predictions, and queries. Finally, we describe how to per-form query difficulty estimation.
The task of ranking related news predictions was first proposed in [11]. Predictions can be automatically extracted from a temporal document collection , (e.g., news archives, company websites, fi-nancial reports, or blogs) using a series of annotation processes in-cluding tokenization, sentence extraction, part-of-speech tagging, named entity recognition, and tempor al expression extraction. The results are predictions or sentences annotated with named entities and future dates.

Instead of having a user X  X  information need explicitly provided, a query will be automatically generated from the news article be-ing read by the user. For example, a query can be top-m entities or top-n terms extracted from the news article. For a given news article, predictions will be retrieved and ranked by the degree of rel-evance. As defined in [11], a prediction is  X  X elevant X  if it is future information about the topics of the news article. Note that, there is no specific instructions about how the dates involved are related to relevance. However, predictions extracted from more recent docu-ments are assumed to be more relevant.
The document collection used in this work is a collection of news articles defined as C = { d 1 ,...,d n } . A news article is represented as a bag-of-words, d = { w 1 ,...,w n } . The publication time of d is denoted by the function time ( d ) . Each document d is associated to an annotated document  X  d composed of three parts:  X  d named entities  X  d e = { e 1 ,...,e n } , where each entity e person, location, or organization;  X  d t is a set of annotated temporal expressions  X  d t = { t 1 ,...,t m } and  X  d s is a set of sentences { s 1 ,...,s z } . Later in the paper, we will propose predictors used for estimating query difficulty, which are based on these annotated documents.
A prediction p is associated with its parent document d p p is extracted from, and each prediction p is represented as a sen-tence with multiple fields/values including: a prediction X  X  unique number ( ID ), the unique number of d p ( PARENT _ ID ), the title of d (
TITLE ), annotated entities p entity in p ( ENTITY ), future dates p in p ( FUTURE _ DATE ), the publication time of d p ( PUB sentence text of p ( TEXT ), and surrounding sentences of p (
A query q is extracted automatically from a news article d read, where q is composed of two parts: keywords q text , and the time of query q time . The keywords q text can be generated from d ways, resulting in three types of queries: 1) entity query (a list of top-m entities ranked by frequency), 2) term query ( top-n terms ranked by term weighting, i.e., TF-IDF), and 3) combined query (combining both top-m entities and top-n terms).

In this work, we are interested in entity queries only, where rep-resenting a query using top-m entities performed worst among other query types as shown in [11]. Thus, we seek to improve the re-trieval effectiveness for entity queries by performing query diffi-culty estimation during the retrieval stage so that particular actions can be taken to improve the overall performance. Consider a news article d q about  X  X resident Bush and the Iraq war X , the keyword part of an entity query q can be represented as q text = George Bush , Iraq , America . During retrieval, q matched with the ENTITY field of the predictions.

The time q time are two time constraints used for retrieving pre-dictions. First, only predictions that are future relative to the publi-cation time of query X  X  parent article, or time ( d q ) will be retrieved. Second, those predictions must belong to news articles published before time ( d q ) . Both time constraints are represented using a time interval, i.e., [ t b ,t e ] ,where t b is a beginning time point, t is an ending time point, and t e is greater than t b . In all cases, [ t min , time ( d and t max and t min are the maximum time in the future and the min-imum time in the past respectively. During retrieval, predictions will be retrieved by matching the first constraint with the field TURE _ DATE and the second constraint with the field PUB _
The task of query difficulty estimation can be viewed as a clas-sification problem. Queries will be labeled into predefined classes based on how well a particular ranking model performs. In this work, we consider two classes of queries: Easy and Difficult .The query difficulty can be determined using the retrieval effectiveness, such as, the Mean Average Precision (MAP). A query achieves the higher MAP wrt. a particular ranking model is considered the eas-ier query. On the contrary, the lower MAP a query achieves, the more difficult the query is.

The prediction quality is highly dependent on a retrieval model because the effectiveness is dependent on a specific retrieval ap-proach. In addition, the prediction quality also depends on a dataset and a document collection used for retrieval [4]. Thus, we take into account prediction robustness by employing several ranking mod-els (cf. Section 4) in determining the difficulty of a given query or topic. Those models can also be regarded as different runs.
We follow a similar approach for identifying classes of queries as presented in [13]. In order to label queries as Easy or Difficult , we use a condition for sp litting queries into tw o groups. For a given query q , we measure MAP wrt. all ranking models and determine whether the average of MAP (denoted avgMAP ) and the standard deviation of MAP (denoted stdMAP ) exceed the respective thresh-olds and  X  or not. if avgMAP ( q )  X  and stdMAP ( q )  X   X  then else end if
In this section, we will present features and two models used for ranking result predictions, which are based on a feature-based ranking model. The features can be categorized into two classes: 1) pre-retrieval and 2) post-retrieval, where both classes are ob-tained from entity information at different retrieval stages. Note that, the features to be presented are commonly employed in an en-tity ranking task [2, 6] but used in other context. In this work, we use entity-based features in order to capture the semantic similarity between q and p .

Pre-retrieval features are extracted from annotation data of a query article (a news article being read d q ) and thus independent from retrieval and the ranked list of result predictions. The features in this class include senPos , senLen , cntSenSubj , cntEvent , cntFu-ture , cntEventSubj , cntFutureSubj , timeDistEvent , timeDistFuture and tagSim . The first feature senPos gives the position of the 1 sentence where e occurs in d p . senLen gives the length of the first sentence of d that contains e . cntSenSubj is the number of sentences where e is a subject. cntEvent is the number of event sentences (or sentences annotated with dates) of e . cntFuture is the number of sentences with a mention of a future date. cntEventSubj is the number of event sentences where e is a subject. timeDistEvent is a measure of the distance between e and all dates in d p . timeDistFuture ( e, d p ) is the distance of e and all future dates in d p computed similarly to timeDistEvent . tagSim is the string similarity between e and an entity tagged in d is only applicable for a collection provided with manually assigned tags (e.g., the New York Times Annotated Corpus).

Post-retrieval features are, in contrast to the previous class, ex-tracted from the annotation data of result predictions including is-Subj and timeDist . isSubj ( e, p ) is 1 if e is a subject with respect to a prediction p ,and timeDist ( e, p ) is a distance of e and all future dates in p computed similarly to timeDistEvent .

A set of all features F presented previously are parameter-free, and their values will be normalized to range from 0 to 1. The de-tailed computation of different features can be found in [11].
We propose two different ranking models that linearly combine two normalized scores: where the mixture parameter  X  indicates the importance of term-based similarity and entity-based similarity. The term-based simi-larity S term ( q, p ) can be measured using any of existing text-based weighting functions, e.g., TF-IDF or a unigram language model. The entity-based similarity can be computed using the features pre-sented above as a single feature S single ( q, p ) , or a combination of multiple features S combined ( q, p ) . All similarity scores will be nor-malized, e.g., divided by the maximum scores, before generating the final scores: S ( q, p ) and S ( q, p ) .

The score for multiple features are computed by linearly com-bining the scores of three different features as follows. where each individual feature is a member of a set of all features: { f i ,f j ,f k } X  X  .  X  and  X  are mixture parameters giving a weight to the score of each feature, where  X  +  X &lt; 1 .
In this section, we present our methodology for estimating query difficulty, which is based on a machine learning approach. We will learn a classification model using features, so-called predictors ,in order to classify queries into two classes, i.e., Easy and Difficult .
We propose 10 post-retrieval predictors that are derived from an-alyzing top-k retrieved predictions: cntEntity , avgEntityPerPredict , distinctEntity , avgPredictPerEntity , cntPeople , percentPeople , cn-tOrg , percentOrg , cntLoc ,and percentLoc . Note that, these features used for predicting query difficult are different from those used for ranking result predictions (presented in Section 4).

Our proposed predictors are aimed at capturing the ambiguity of a query by analyzing entities (i.e., people, organization, and loca-tion) in the top-k retrieved predictions. To the best of our knowl-edge, the proposed predictors have never been employed in a simi-lar task before. The description of the proposed predictors is shown in Table 1. The actual values of all predictors will be calculated with respect to top-k retrieved predictions, where the k value will be varied in the experiments
The New York Times Annotated Corpus (with 1.8 million news articles from 1987 to 2007) was used as a temporal document col-lection. Documents were annotated and predictions were extracted using different NLP tools as follows. We extracted sentences and performed part-of-speech tagging using OpenNLP. The SuperSense tagger was used for named entity recognition and the TARSQI Toolkit was used for extracting temporal expressions. The Apache Lucene search engine was employed for both indexing and retriev-ing predictions.

We used future-related queries and relevance assessments from the previous work [11]. The dataset is composed of 42 query news articles related to future topics and 4,888 manually evaluated pairs of query/prediction. In this work, the actual queries or  X  X ntity queries X  used for retrieving predictions were extracted from these query news articles.

Parameters used in the experiments were set as follows. We rep-resented an entity query using the number of entities m =11as recommended in [11]. For the ranking models, we generated all possible runs by varying the values for  X  ,  X  ,and  X  from 0 to 1 by increment of 0.1. For a given query, we measured the retrieval effectiveness using the Mean Average Precision (MAP).

In order to label a query into two classes ( Easy and Difficult ), we determined whether avgMAP and stdMAP are greater than thresh-olds and  X  or not. In this work, we used =0 . 4 and  X  =0 . we observe empirically. The Weka implementation [14] was used for modeling the query difficulty prediction as a classifier, which was learned using several algorithms: decision tree, Na X ve Bayes, neural network and SVM, using 10-fold cross-validation with 10 repetitions. We measured statistical significance using a t -test with p&lt; 0 . 05 . In the tables, bold face indicates statistically significant difference from the respective baseline.

Classification results. The baseline method for query classifi-cation is the majority classifier. The accuracy of the baseline is 0.79%. Table 2 shows the accuracy of the best-performing classi-fication algorithm on each predictor. The combination of all pre-dictors is denoted ALL . We varied the number of top-k retrieved documents in order to study how a k -value affect the classification performance, namely, k =10 , 25 , 50 , 75 and 100 .

The results show that the predictor avgEntityPerPredict performs best almost in every k  X  X  value when comparing with other predic-tors. The other predictors do not achieve better accuracy among them. The combination of all features gives the best result with the accuracy of 0.92. Hence, the combined predictor can be used for estimating query difficulty with high accuracy. We do not observe any trend in performance for different k  X  X  values.
In this paper, we have proposed a machine learning approach to estimate query difficulty for the ranking related news predic-tion task. Our proposed predicto rs are based on entity informa-tion, which can be extracted from annotation data of news articles. Through experiments using real-world dataset, we showed that our proposed approach is able to predict two classes of query difficulty with high accuracy. [1] R. A. Baeza-Yates. Searching the future. In Proceedings of [2] K. Balog, L. Azzopardi, and M. de Rijke. A language [3] J. Canton. The Extreme Future: The Top Trends That Will [4] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty [5] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting [6] G. Demartini, A. P. de Vries, T. Iofciu, and J. Zhu. Overview [7] C. Hauff, L. Azzopardi, and D. Hiemstra. The combination [8] B. He and I. Ounis. Inferring query performance using [9] J. He, M. Larson, and M. de Rijke. Using coherence-based [10] A. Jatowt, K. Kanazawa, S. Oyama, and K. Tanaka.
 [11] N. Kanhabua, R. Blanco, and M. Matthews. Ranking related [12] J. Mothe and L. Tanguy. Linguistic features to predict query [13] A.-M. Vercoustre, J. Pehcevski, and V. Naumovski. Topic [14] I. H. Witten and E. Frank. Data Mining: Practical Machine [15] Y. Zhao, F. Scholer, and Y. Tsegay. Effective pre-retrieval
