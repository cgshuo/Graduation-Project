 Given a current news article, we wish to create a succinct query reflecting its content, which may be used to follow the news story over a period of days, or even weeks. In part, the need for succinct queries is occasioned by limitations of commercial social media search engines, which can perform poorly with longer queries. We start by applying established key phrase extraction methods to the article, creating an ini-tial set of candidate query terms. We then generate a series of probe queries, each a subset of these candidate terms, which we apply to search current social media streams. By analyzing the results of these probes, we rank and trim the candidate set to create a succinct query. We present an ex-perimental study of this method based on a collection of news articles taken from March-April 2014, with the result-ing succinct queries used to re-query social media one week later.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Query formulation Algorithms, Experimentation Search; News; Social media; Microblogs
Starting with a news article, or a similar source document, we wish to find related material in social media, or in a similar target resource. Prior research has approached this query by document problem [18] in a variety of ways. As one approach, we might extract key words and phrases from the source document, forming queries from these extracted terms. For example, given the news article  X  X ore than 100 we might extract the query  X  X ake Albert boat accident X , which may produce higher precision results than querying with more general terms, such as  X  X ganda X  or  X  X ongo X .
To query by document across blogs and other media, Yang et al. [18] employ a part-of-speech tagger to identify candi-date terms in the source document. They rank these terms using TF-IDF and mutual information, supplementing them with associated terms from Wikipedia. Similarly, Tsagkias et al. [15] explore query-by-document methods for linking news with social media. They employ multiple methods for extracting query terms from the source document, supple-menting them with terms extracted from social media posts that explicitly reference the news article. They then sep-arately execute the queries from each method, generating multiple ranked lists and merging them through late fusion.
Unfortunately, this approach can produce large and com-plex queries. For example, one method employed by Tsagkias et al. simply uses the full document as the query. Perhaps unsurprisingly, using full documents as queries produces the best results of any single method they explore, and it also provides a challenging baseline for evaluating late fusion.
The execution and fusion of multiple large and complex queries may not be feasible to support casual queries to find related material. Even if this process can be justified, re-querying to follow updates to an evolving news story re-quires the process to be repeated. Moreover, from a practi-cal standpoint, commercial social media search engines may provide poor external support for long queries, requiring this approach to be built into the engine itself.

As an alternative, we present an approach for generating succinct queries from a candidate set of extracted terms. These succinct queries (comprising perhaps four or five terms in total) provide a lightweight way to follow the story over hours, days, or even weeks. Starting with the candidate terms, we execute a series of probe queries over a sample of contemporaneous social media. By analyzing the results of the probes, and comparing them to the language of the source document, we rank the terms according to their abil-ity to retrieve related material, when used in combination with other terms. While we define our succinct query gener-ation algorithm in general terms, in this short paper we con-centrate on implementing each step using straightforward techniques, aiming to demonstrate the viability of probe queries as a method for ranking and trimming extracted terms. h ttp://www.cnn.com/2014/03/24/world/africa/ uganda-boat-capsizes-death-toll
The problem of finding additional documents related to a given source document has been a longstanding research topic within the information retrieval community [7,13,16]. Generally these methods assume that the search engine will directly support a X  X ind similar X  feature, although Dandan et al. [5] consider the generation of queries for identifying near-duplicate documents by querying against a search engine. In this paper, rather than finding similar documents within a given collection, we create succinct queries to efficiently search across collections and across time.

Other research considers the problem of trimming and re-weighting verbose queries, although without probe queries. Bendersky and Croft [2] train a classifier to recognize key words and phrases in queries consisting of a few sentences, as is typically seen in the topic descriptions from older TREC topics. Bendersky et al. [3] extended this approach to incor-porate pseudo-relevance feedback and information derived from external sources. Lease et al. [11] re-weight these terms using simple term features in a learning to rank framework. Xue et al. [17] use conditional random fields to model query subset distributions and select the terms to keep.
Closer to our work, Kumaran and Carvalho [10] analyze term subsets from TREC topic descriptions. They use var-ious query quality features to rank subsets, with some fea-tures based on the execution of subsets as probe queries. While we start with full documents and introduce a post-probe analysis step, we intend to expand our work with their ideas, extending them to social media.

Balasubramanian et al. [1] build on the work of Kumaran and Carvalho to improve the performance of longer Web queries, i.e., those with five terms or more. They employ query quality features to predict which term, if any, could be removed to improve the performance of these longer queries. Datta and Varma [6] further extend this work by probing with randomly selected subsets.

Other related work includes methods for term select in pseudo-relevance feedback [4]. Jiang and Allan introduce the notion of necessary and frequent terms [8]. Kumaran and Allan [9] examine interactive query reduction. Figure 1 presents our succinct query generation algorithm. While we express the algorithm in general terms, within this paper the primary source document is a news article taken from a mainstream news source, while the secondary source collection is sample of tweets from a three-day window start-ing at the date of the news article. This secondary source collection is used to execute probe queries. Tweets for the secondary source collection were gathered through the Twit-ter Streaming API, which produces a maximum yield of 1% of the total tweet stream. Twitter X  X  search engine cannot be used to execute probe queries, since it restricts the speed at which it accepts queries from a particular user.
Output from the algorithm is a ranked list of query terms intended to find social media content related to the news story. The term ranking is intended to reflect the expected value of the terms for this purpose. For the experiments reported in this paper, we form a succinct query from the top five terms of Q , which are then executed on the main Twitter search service.

Each step of this algorithm could be implemented in nu-merous ways. In this paper, we explore simple implemen-tations of these steps, with suggestions for future work pro-vided in our concluding discussion. Details appear below.
The algorithm could be further generalized by repeating steps 2-5 multiple times, with the results of each iteration applied to suggest new subsets for probing in the next itera-tion, and with each iteration improving the estimated rank-ing. New terms might be extracted from the probe rankings through pseudo-relevance feedback and added to the candi-date set. We leave the exploration of these ideas to future work.
To extract an initial candidate term set (step 1) we apply standard key phrase extraction methods to the news article. We first use pointwise K-L divergence [12,14] to rank terms appearing in the article X  X  full contents, where p i s the relative frequency of term t in the article and q the relative frequency of term t in the secondary collection. We take the top-20 terms from this ranking to form a set L , with the choice of 20 terms based on preliminary exper-iments over a set of pilot news articles. In later steps, we also use L as a simple language model for the news story.
As suggested by the results of Tsagkias et al. [15], the non-stopword terms in the headline H provide a solid base-line query for linking news to social media. For our initial candidate term set we use In our experiments, we also use terms from H as a baseline for evaluating our succinct query generation algorithm. Given that | T |  X  20, we do not probe with all 2 | T | subsets. Instead, we probe all pairs from T, which provided reason-able performance in our preliminary experiments. Probing all subsets of size three, four or five terms [10] might also be feasible, especially given that probes queries are executed over a subset of tweets. Random selection of subsets is an-other possibility [6].
We execute each probe query P i over the secondary source collection to produce a ranking  X  i corresponding to each purpose. We rank tweets using the PL2 divergence from ran-domness formula, with the restriction that all probe terms are required to appear in all the retrieved tweets.
Each  X  i consists of up to the top-50 documents returned by Terrier, with the choice of 50 documents based on our pre-liminary experiments. Since retrieved tweets are required to contain all probe terms, some probes produce less than 50 documents. Since we seek material related to the news arti-cle, rather than re-tweets and re-postings of the article, we apply near-duplicate detection before computing similarities in the next step. We also assume that tweets containing all terms from the headline merely repeat the original story, and these tweets are removed from the  X  i rankings.
We analyze the probes by computing the similarity be-tween the source document A and each  X  i (step 4): The computation of this similarity is critical to the perfor-mance of the algorithm, and we are actively working on im-provements in our ongoing research. For this paper, we com-pute similarity from two simple matching functions, both based on the un-weighted language model provided by L .
The first of these matching functions f 1 ( L  X  P i ,  X  i matches between terms in L  X  P i (i.e., L with the probe terms excluded) and the tweets in  X  i taken as a group. It returns the proportion of terms in L  X  P i appearing in  X  The second matching function f 2 ( L  X  P i ,  X  i ) compares each individual tweet in  X  i to L  X  P i . The number of match-ing terms is used to estimate a probability of relevance for each tweet. To compute these probabilities, we derived a h ttp://terrier.org Headline 0.391 0.334 0.287 0.321 0.353 S uccinct query 0.594 0.542 0.455 0.495 0.480 F igure 2: Experimental results. All improvements over the baseline are significant (two-sided paired t-test, p &lt; . 01 ). mapping between the number of matching terms and the estimated probability of relevance from the results of our preliminary experiments. These estimates are then summed and divided by |  X  i | to return an overall precision estimate for  X  i . To compute similarity, we take an unweighted linear combination of these two functions:
The similarity values ( s 1 , s 2 , ... ) essentially represent a sys-tem of equations, each parameterized by a pair of probe terms. If we imagine a latent variable associated with each term, our goal is to estimate values for these variables, rank-ing T according to these values to produce Q . We explored several approaches, which often produced similar rankings.
For this paper, we adopt a pagerank-like algorithm, al-lowing terms that produce high similarity value across many pairs to be properly recognized. We create a Markov chain with each term in T represented by a state. Transition prob-abilities are derived from the similarity values associated with each term pair. Let S ( t x , t y ) be the similarity value associated with a term pair { t x , t y } . We set the transition probability to Following the example of pagerank,  X  is a jump or telepor-tation probability, which we set to 0.01. We then apply the power method to determine the stationary distribution, which ranks the terms in T .
We evaluated our approach using a collection of news ar-ticles taken from March and April 2014, with the resulting succinct queries used to re-query social media one week later. As mentioned above, we worked with a set of pilot news ar-ticles to conduct preliminary experiments during the devel-opment of our succinct query generation algorithm. These articles were not re-used for the experiments reported in this section.

We developed a fresh test set based on news articles linked from Wikipedia X  X  news pages for March-April 2014. We use Wikipedia as a method for selecting articles to pro-vide breadth and to prevent our personal news preferences from unduly influencing the selection. For simplicity, we restrict the selection to articles from six high-quality main-stream news sources: BBC, CNN, Reuters, the Washington Post, the Guardian, and CBC. Together, these sources pro-vide coverage from a variety of perspectives across much of the English-speaking world. For some major events (e.g., MH370 and Crimea) we removed all but one related article to avoid having these events dominate the test set. A few other articles were removed for technical issues (e.g., parsing problems). This process produced a test set of 66 articles.
We applied our succinct query generation algorithm to e ach of these articles, taking the top-5 terms from Q as our succinct query. As a baseline for comparison, we ranked the terms from article X  X  headline ( H ) according to their IDF values in  X  , again taking the top-5 terms. Tsagkias et al. [15] identify the headline terms as providing a solid base-line for our task, outperformed only by a small number of their methods, which were either based on full articles or on substantial external resources. We executed the queries on Twitter X  X  commercial search service, restricted to English-language tweets. If a query produced less than 25 tweets, we removed the lowest ranking term and re-issued the query, ranking new tweets below existing tweets and repeating until 25 tweets were returned.

For each article, the tweets returned from both methods were merged and placed in random order for relevance as-sessment. The assessor first read the associated article and formulated a brief statement describing material that could be considered relevant up to one week later. The tweets were then judged in terms of this statement. Judgments were bi-nary, relevant or not. Tweets considered to be borderline were judged as not relevant. For this short paper, a single assessor performed all assessments.

Results are shown in Figure 2, which gives average values for several standard effectiveness measures. Although ERR and NDCG are designed for graded relevance values, they adapt naturally to binary values (i.e., two relevance grades). NDCG is computed down to depth 25 and normalized by assuming the collection contains an unlimited number of relevant tweets. All improvements over the baseline are sta-tistically significant ( p &lt; 0 . 01) under a two-sided paired t-test. Beyond statistical significance, we would expect these improvements, e.g., more than 50% in precision@5, to be practically significant, i.e., noticeable at the user level.
For the news article used as an example in the introduc-tion ( X  X ore than 100 Congolese refugees killed in boat ac-cident, Uganda says X ) the algorithm produced the ranked query Q = {  X  X lbert X  ,  X  X oat X  ,  X  X ake X  ,  X  X ccident X  ,  X  X apsized X  } . As with any query re-weighting method, there were misses as well as hits. For an article on President Obama X  X  new health secretary the query over-emphasizes the departing secretary Q = {  X  X athleen X  ,  X  X ebelius X  ,  X  X ecretary X  ,  X  X bama X  ,  X  X bamacare X  } , although many tweets were still relevant. In some cases, the generated query merely copied terms from the headline. And, of course, in others the headline outperformed it.
In this short paper, we explore probe queries as a method for extracting succinct queries from full documents. We apply our algorithm to the problem of linking mainstream news articles to social media. Our evaluation shows statisti-cally and practically significant improvements over baseline queries derived from the headlines of the news articles.
Our ongoing efforts focus on improvements to the simi-larity function in Equation 3, which must recognize related material while avoiding near-duplicate material, We are cur-rently adapting ideas from Kumaran and Carvalho [10]. Other directions for future work include dynamic probing and im-proved methods for subset selection. The integration of pseudo-relevance feedback would allow hashtags to be added to the succinct query, which may be important in a mi-croblogging context. We also plan to extend our evaluation with a broader range of assessments through crowdsourcing. [1] Niranjan Balasubramanian, Giridhar Kumaran, and [2] Michael Bendersky and W. Bruce Croft. Discovering [3] Michael Bendersky, Donald Metzler, and W. Bruce [4] Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and [5] Ali Dasdan, Paolo D X  X lberto, Santanu Kolay, and [6] Sudip Datta and Vasudeva Varma. Tossing coins to [7] Jeffrey Dean and Monika R. Henzinger. Finding [8] Jiepu Jiang and James Allan. Necessary and frequent [9] Giridhar Kumaran and James Allan. A case for [10] Giridhar Kumaran and Vitor R. Carvalho. Reducing [11] Matthew Lease, James Allan, and W. Bruce Croft. [12] Juan Martinez-Romo and Lourdes Araujo. Updating [13] Mark D. Smucker and James Allan. Find-similar: [14] Takashi Tomokiyo and Matthew Hurst. A language [15] Manos Tsagkias, Maarten de Rijke, and Wouter [16] W.John Wilbur and Leona Coffee. The effectiveness of [17] Xiaobing Xue, Samuel Huston, and W. Bruce Croft. [18] Yin Yang, Nilesh Bansal, Wisam Dakka, Panagiotis
