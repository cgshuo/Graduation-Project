 For Web applications that are based on user generated content the detection of text quality flaws is a key concern. Our research con-tributes to automatic quality flaw detection . In particular, we pro-pose to cast the detection of text quality flaws as a one-class clas-sification problem: we are given only positive examples (= texts containing a particular quality flaw) and decide whether or not an unseen text suffers from this flaw. We argue that common binary or multiclass classification approaches are ineffective in here, and we underpin our approach by a real-world application: we employ a dedicated one-class learning approach to determine whether a given Wikipedia article suffers from certain quality flaws. Since in the Wikipedia setting the acquisition of sensible test data is quite intricate, we analyze the effects of a biased sample selection. In ad-dition, we illustrate the classifier effectiveness as a function of the flaw distribution in order to cope with the unknown (real-world) flaw-specific class imbalances. Altogether, provided test data with little noise, four from ten important quality flaws in Wikipedia can be detected with a precision close to 1.
 Categories and Subject Descriptors : H.3.3 [Information Stor-age and Retrieval]: Information Search and Retrieval; H.5.3 [In-formation Interfaces and Presentation]: Group and Organization Interfaces X  Evaluation/methodology General Terms : Measurement, Algorithms, Experimentation
The machine-based assessment of text quality is becoming a topic of enormous interest. This fact is rooted, among others, in the increasing popularity of user generated Web content [3] and the (unavoidable) divergence of the delivered content X  X  quality. Most of the relevant literature on automatic text quality assessment deals with the classification of texts in predefined abstract quality schemes, see for instance [2, 6, 11]. Only a few approaches fo-cus on quality flaw detection and try to give precise indications in which respects a text needs improvement [1, 7]. A general find-ing of our literature review (from which we can show only a tiny excerpt here) is the fact that the detection of text quality flaws in general has not yet been operationalized.

The paper in hand focuses on detection issues, whereas our con-tributions are as follows: Firstly, in the remainder of this section, we argue that the detection of text quality flaws is essentially a one-class classification problem and give a respective problem defini-tion. Secondly, we employ a one-class machine learning approach to detect quality flaws in Wikipedia articles (Section 2). Thirdly, we perform comprehensive analyses to assess the effectiveness of our approach (Section 3).
 Problem Definition Let D be a set of text documents and let F be a set of text quality flaws. A document d  X  D can contain up to | F | flaws, where, without loss of generality, the flaws in F are considered as being uncorrelated. A classifier c hence has to solve the following multi-labeling problem: 1 where 2 F denotes the power set of F . A document d is represented by a feature vector d , called document model, where D denotes the set of document models for D .

Basically, there are two strategies to tackle multi-labeling prob-lems: (1) by multiclass classification, where a single classifier is learned on the power set of all classes, and (2) by multiple binary classification, where a specific classifier c i : D  X  { 1 , 0 } is learned for each class f i  X  F . Since the high number of classes under a multiclass classification strategy entails a very large number of training examples, the second strategy is favorable.

In most classification problems training data is available for all classes that can occur at prediction time, and hence it is appropriate to train a classifier c i with (positive) examples of the target class f and (negative) examples from the classes F \ f i . However, in the case of detecting text quality flaws an unseen document can either belong to the target class f i or to some unknown class that was not available during training. I.e., the standard discrimination-based classification approaches (binary or multiclass) are not applicable to learn a class-separating decision boundary: given a flaw f target class is formed by those documents that contain (among oth-ers) flaw f i  X  X ut it is impossible to model the  X  X o-class X  with docu-ments not containing f i . Even if many counterexamples were avail-able, they could not be exploited to properly characterize the uni-verse of possible counterexamples. As a consequence, we model the classification c i ( d ) of an document d  X  D with respect to a text quality flaw f i as the following one-class classification prob-lem: Decide whether or not d contains f i , whereas a sample of documents containing f i is given.
 The following example may serve as an additional illustration: Wikipedia articles should be written in a formal tone 2 , and hence
P ossibly existing correlations among the flaws in F will not affect the nature of the multi-labeling problem. http://en.wikipedia.org/wiki/Wikipedia:Tone#Tone  X  X nappropriate tone X  is a text quality flaw in this particular context. An even large sample of articles that suffer from this flaw can be compiled without problems (consider articles containing slang, jar-gon, etc.). However, it is impossible to compile a representative sample of articles that are written in a formal tone. Though there definitely exist outstanding articles written in a formal tone, they cannot be considered as a representative sample.

For an in-depth discussion of one-class classification and a sur-vey of respective methodologies see [14, 9]. Typical one-class problems in the information retrieval domain include typist recog-nition [8], authorship verification [10], plagiarism analysis [12], and anomaly detection [5].
In previous research we analyzed cleanup template messages in the English Wikipedia and compiled a set of quality flaws of Wi-kipedia articles that have been tagged by the community [1]. This analysis is restricted to a specific subset of cleanup template mes-sages. By applying the same approach without these restrictions to a more recent Wikipedia snapshot from January 2011 we extracted 388 quality flaws, which form the set F ; the 3 557 468 articles of the snapshot form the set D . We distinguish different subsets of D , see Figure 1: The set D  X  comprises 979 299 articles that have been tagged with at least one of the flaws from F , which corresponds to 27.5% of D . Notice that we have no knowledge about the articles in D \ D  X  i ; these articles either do not contain the flaw f not yet been evaluated with respect to f i . 3
We make two assumptions in order to estimate the actual fre-quency of a flaw f i : (1) each article in D  X  is tagged completely, i.e. with all flaws that it contains (Closed World Assumption), and (2) the distribution of f i in D  X  is identical to the distribution of f in D . Based on these assumptions we estimate the actual frequency of a flaw f i by the ratio of articles in D  X  i and articles in D  X  .
We model the quality flaws of an article d  X  D by a feature vector d , where each dimension in d quantifies a quality-specific characteristics of d . Our document model employs state-of-the-art features that have been proposed in the relevant literature [6, 11, 13] as well as new quality flaw predictors that quantify the usage of in-links, templates, lists, and special words, among others.
We employ a one-class classification approach as proposed by [8], which combines density estimation with class probability estimation. The idea is to use a reference distribution to model the probability P ( d | f  X  i ) of an artificial class f  X  i , and to generate (ar-a flaw f i let P ( f i ) and P ( f i | d ) be the a-priori probability and the class probability function respectively. According to Bayes X  theorem the class-conditional probability for f i is given as follows:
P ( f i | d ) is estimated by a class probability estimator (a classi-fier whose output is interpreted as probability). Since we are in a
A special case is the set D  X  , which will be discussed later on. one-class situation we have to rely on the face value of P ( d | f more specifically, P ( d | f i ) cannot be used to determine a maxi-mum a-posterior (MAP) hypothesis among the f i  X  F . As a con-sequence, given P ( d | f i ) &lt;  X  with  X  = 0 . 5 , the hypothesis that d suffers from f i could be rejected. However, because of the approx-imative nature of P ( f i | d ) and P ( f i ) the estimation for P ( d | f is not a true probability, and the threshold  X  has to be chosen em-pirically. In practice, the threshold  X  is derived from a user-defined target rejection rate, trr , which is the rejection rate of the target class training data.
We report on experiments to assess the effectiveness of our clas-sification approach in detecting ten of the most frequent quality flaws of Wikipedia articles. The evaluation treats the following is-sues: 1. Since a bias may not be ruled out when collecting outlier 2. Since users (Wikipedia editors) have different expecta-3. Since the true flaw-specific class imbalances in Wikipedia
Recall that no articles are available that have been tagged to not contain a quality flaw f i  X  F . Thus a classifier c i can be evaluated only with respect to its recall, whereas a recall of 1 can be achieved easily by classifying all examples into the target class of f der to evaluate c i with respect to its precision one needs a represen-tative sample of examples from outside the target class, so-called outliers. As motivated above, in a one-class situation it is not pos-sible to compile a representative sample, and one way out of the dilemma is the generation of uniformly distributed outlier exam-ples [14]. Here, we pursue two strategies to derive examples from outside the target class, which result in the following settings: 1. Optimistic Setting. Use of featured articles as outliers. This 2. Pessimistic Setting. Use of a random sample from D \ D  X 
The above settings address two extremes: classification under laboratory conditions (overly optimistic) versus classification in the wild (overly pessimistic). The experiment design is owing to the
T he hypothesis may hold in many cases but not always: the snap-shot comprises 13 featured articles that have been tagged with some flaw. We discarded these articles in our experiments. facts that  X  X o-flaw features X  cannot be stated and that the num ber of false positives as well as the number of false negatives in the set D  X  of tagged articles are unknown.
We use the English Wikipedia snapshot from January 15, 2011. The articles X  plain texts and wikitexts are extracted in a prepro-cessing step by processing the  X  X ages-articles X  XML dump on an Apache Hadoop cluster using Google X  X  MapReduce. Furthermore, a local copy of the Wikipedia database is established by import-ing the database dumps into a MySQL database. The plain texts, the wikitexts, and the local Wikipedia database form the basis to compute the features of our document model.
 Experiment Design The evaluation is performed for the set F  X   X  F of the ten most frequent quality flaws that show the fol-lowing three properties: they describe a single and specific quality aspect, they refer to an article as a whole, and they are not restricted to a particular domain, language, or user group (see Table 1). About 70% of the articles in D  X  suffer from these flaws. In the optimistic setting 1 000 outliers are randomly selected from the 3 128 featured articles in the snapshot. In the pessimistic setting 1 000 outliers are randomly selected for each flaw f i  X  F  X  from D \ D  X  i . We eval-uate our approach under both settings by applying the following procedure: For each flaw f i  X  F  X  the one-class classifier c and the respective 1 000 outliers, applying tenfold cross-validation. whereas testing is performed with the remaining 100 articles from i plus 100 outliers. Note that c i is trained exclusively with the training of c i is neither affected by the class distribution nor by the outlier selection strategy that is used in the respective setting.
The one-class classifier is built as follows: a class with artificial examples is generated, whereas the feature values obey a Gaussian distribution with  X  = 0 and  X  2 = 1 . The Gaussian distribution is employed in favor of a more complex reference distribution to un-derline the robustness of the approach. The proportion of the gen-erated data is 0 . 5 compared to the target class. As class probability estimators we apply bagged random forest classifiers with 1 000 decision trees and ten bagging iterations. A random forest is a col-lection of decision trees that differ with respect to their features, and a voting over all trees is run in order to obtain a classification decision (for further details see [4]).
 Operating Point Analysis For the major part of the relevant use cases precision is the determining measure of effectiveness; con-sider for instance a bot that autonomously tags flawed articles. The precision of the one-class classifier is controlled by the hyperpa-rameter target rejection rate. We empirically determine the optimal operating point for each of the ten flaws under both the optimistic and the pessimistic setting. Here, the optimal operating point corre-sponds to the target rejection rate of the maximum precision clas-sifier. Figure 2 illustrates the operating point analyses exemplary for the flaw Unreferenced : with increasing target rejection rate the recall value drops while the precision values increase. Observe that the recall is the same in both settings, since it solely depends on the target class training data. For the flaw Unreferenced the optimal op-erating points under the optimistic and the pessimistic setting are at a target rejection rate of 0.1 and 0.35 respectively (with precision values of 0.99 and 0.63).

The precision of a one-class classifier cannot be adjusted arbi-trarily since the target rejection rate controls only the probability
W ikimedia downloads: http://download.wikimedia.org/enwiki. threshold  X  for the classification decision. For instance, a target rejection rate of 0.1 means that a  X  is chosen such that 10% of the target class training data will be rejected, which results in a classi-fier that performs with an almost stable recall of 0.9. Increasing the target rejection rate entails an increase of  X  . However, if  X  achieves its maximum no further examples can be rejected, and hence both the precision and the recall remain constant beyond a certain target rejection rate (which is 0.4 for the flaw Unreferenced , see Figure 2). Results and Discussion Table 1 shows the performance values for the ten quality flaws. The values correspond to the performances at their optimal operating points; the performance is quantified as precision ( prec ) and recall ( rec ). We also report the area under ROC curves (AUC), which is important to assess the tradeoff be-tween specificity and sensitivity of a classifier: an AUC value of 0.5 means that all specificity-sensitivity-combinations are equiva-lent, which in turn means that the classifier is random guessing.
Under the optimistic setting four flaws can be detected with a nearly perfect precision. For the flaw Notability even the achieved recall value is very high, which means that this flaw can be detected exceptionally well. As expected, the effectiveness of the one-class classifiers deteriorates under the pessimistic setting. However, the classifiers still achieve reasonable precision values, and even in the noisy test set the flaw Orphan can be detected with a good preci-sion. Notice, however, that the expected performance in the wild lies in between the two extremes. For some flaws the effective-ness of the one-class classifiers is pretty low under both settings, including Original research . We explain this behavior as follows: (1) Either the document model is inadequate to capture certain flaw characteristics, or (2) the hypothesis class of the one-class classifi-cation approach is too simple to capture the flaw distributions.
The performance values in Table 1 presume a balanced class dis-tribution, i.e., the one-class classifiers are evaluated with the same number of flawed articles and outliers. The real distribution of flaws in Wikipedia is unknown (cf. Section 2), and we hence re-port precision values as a function of the class imbalance. Given the recall and the false positive rate ( fpr ) of a classifier for the bal-anced setting, its precision for a class size ratio of 1:n (flawed arti-cles : flawless articles) computes as follows:
The false positive rate is the ratio between the detected nega tive examples and all negative examples, and hence it is independent from the class size ratio; the same argument applies to the recall. Figure 3 shows the precision values as a function of the flaw distri-bution under the optimistic setting.

Observe that the expected precision values for the flaws Unref-erenced , Orphan , and Notability are still high. The flaw ratio of the flaw Unreferenced is 1:3, and thus the expected precision is close to that of the 1:1 ratio. The flaw Orphan can be detected with a pre-cision of 1, i.e., the false positive rate is 0, and hence the detection performance is independent of the class imbalance. Although the flaw ratio of the flaw Notability is 1:26, the expected precision is still about 0.9, which shows that the respective one-class classifier captures the characteristics of the flaw exceptionally well. The ex-pected precision values for those flaws with a flaw ratio 1:n where n &gt; 40 are lower than 0.2. Aside from conceptual weaknesses regarding the employed document model, the weak performance indicates also that the training set of the respective one-class clas-sifiers might be too small.
We treat the detection of text quality flaws as a process where for each known flaw an expert is asked whether or not a given doc-ument suffers from it; the experts in turn are operationalized by one-class classifiers. This approach is applied to detect text qual-ity flaws in the English Wikipedia. Our evaluation is based on a corpus comprising 10 000 human-labeled Wikipedia articles. We report on precision values close to 1 for four out of ten important quality flaws X  X resuming an optimistic test set with little noise and a balanced flaw distribution. Even for a class size ratio of 1:16 three flaws can still be detected with a precision of about 0.9.
We are convinced that the presented or similar approaches will help to simplify Wikipedia X  X  quality assurance process by spotting weaknesses within articles. Our current research on quality flaw de-tection in Wikipedia targets the investigation of tailored one-class classifiers for each flaw, as well as the development of flaw-specific document models that combine expert rules, multi-level filtering, and feature selection. [1] M. Anderka, B. Stein, and N. Lipka. Towards automatic quality [2] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne. [3] R. Baeza-Yates. User generated content: how good is it? In [4] L. Breiman. Random forests. Machine Learning , 45(1):5 X 32, [5] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: a [6] D. Dalip, M. Gon X alves, M. Cristo, and P. Calado. Automatic [7] L. Gaio, M. den Besten, A. Rossi, and J. Dalle. Wikibugs: using [8] K. Hempstalk, E. Frank, and I. Witten. One-class classification [9] V. Hodge and J. Austin. A survey of outlier detection [10] M. Koppel and J. Schler. Authorship verification as a one-class [11] N. Lipka and B. Stein. Identifying featured articles in Wikipedia: [12] B. Stein, N. Lipka, and P. Prettenhofer. Intrinsic plagiarism [13] B. Stvilia, M. Twidale, L. Smith, and L. Gasser. Assessing [14] D. Tax. One-Class Classification . PhD thesis, Delft University of
