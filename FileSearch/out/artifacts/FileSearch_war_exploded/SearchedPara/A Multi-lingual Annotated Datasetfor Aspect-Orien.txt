 Reviews of products and services that are sponta-neously produced by customers represent a source of unquestionable value not only for marketing strategies of private companies and organizations, but also for other users since their purchasing de-cisions are likely influenced by other customers X  opinions (Chevalier and Mayzlin, 2006).

Overall ratings (e.g., in terms of a five stars rat-ing scale), and also aspect-specific ratings (e.g., the Cleanliness or Location of a hotel), are the typi-cal additional information expressed by customers in their reviews. Those ratings help to derive a number of global scores to facilitate a first screen-ing of the product or service at hand. Notwith-standing, users who pay more attention to a par-ticular aspect (e.g., the Rooms of a hotel) remain constrained to manually inspect the entire text of reviews in order to find out the reasons other users argued in that respect. Methods for au-tomatic analysis of the aspect-oriented sentiment expressed in reviews would enable highlighting aspect-relevant parts of the document, so as to al-low users to perform a faster and focused inspec-tion of them.
 Previous work on opinion mining (Pang and Lee, 2008) has already faced the overall sentiment prediction (Pang et al., 2002), multiple aspect-oriented analysis (Hu and Liu, 2004), and fine-grained phrase-level analysis (Wilson et al., 2009). Most of the available opinion mining datasets con-tain only documents written in English, as this lan-guage is the most used on the Internet and the one for which more NLP tools and resources are avail-able. (Hu and Liu, 2004) worked on the summa-rization of reviews by means of weakly supervised feature mining. (T  X  ackstr  X  om and McDonald, 2011) used a finer-grained dataset in which global po-larity annotation is applied also to each sentence composing the document. Similarly did (Socher et al., 2013) with the Stanford Sentiment Tree-bank, which annotates each syntactically plausi-ble phrase in thousands of sentences using anno-tators from Amazon X  X  Mechanical Turk, annotat-ing the polarity of phrases on a five-level scale. (Lazaridou et al., 2013) performed a single-label polarity annotation of elementary discourse units of TripAdvisor reviews, adopting ten aspect labels. (Marcheggiani et al., 2014) did a similar annota-tion work, using sentences as the annotation ele-ments and adopting a multi-label polarity annota-tion, i.e., each sentence can be assigned to zero, one, or more than one aspect.

Cross-lingual sentiment classification (Wan, 2009; Prettenhofer and Stein, 2011) explores the scenario in which training data are available for a language that is different from the language of the test documents. Cross-lingual learning meth-ods have important practical applications, since they allow to build classifiers for many languages reusing the training data produced for a single lan-guage (typically English), probably giving up a bit of accuracy, but compensating it with a large save in terms of human annotation costs.

Multi-lingual datasets are beneficial to the re-search community both as a benchmark to ex-plore cross-lingual learning and also as resources on which to develop and test new NLP tools for languages other than English. Prettenhofer and Stein (2010) used a multi-lingual dataset focused on full-document classification at the global po-larity level. Denecke (2008) used a dataset of 200 Amazon reviews in German to test cross-lingual document polarity classification using an English training set. Klinger and Cimiano (2014) pro-duced a bi-lingual dataset (English and German), named USAGE, in which aspect expressions and subjective expressions are annotated in Amazon product reviews. In (Klinger and Cimiano, 2014) aspect expressions can be any piece of text that mentions a relevant property of the reviewed en-tity (e.g., washer, hose, looks ) and are not categor-ical label, as in our dataset. The USAGE dataset is thus more oriented at information extraction rather than at text classification applications. Banea et al. (2010) used machine translation to create a multi-lingual version of the information-extraction ori-ented MPQA dataset (Wiebe et al., 2005) on six languages (English, Arabic, French, German, Ro-manian and Spanish).

In this paper we present Trip-MAML, which al. (2014) with Italian and Spanish annotated re-views. We describe Trip-MAML and report ex-periments aimed at defining a first baseline. Both the dataset and the software used in experiments are publicly available at http://hlt.isti. cnr.it/trip-maml/ . We recall the annotation process adopted by Marcheggiani et al. (2014) for Trip-MA and the procedure we employed to extend it into Trip-MAML. We will use the national codes EN, ES, and IT, to denote the English, Spanish, and Ital-ian parts of the Trip-MAML dataset, respectively. Note that EN coincides with Trip-MA. 2.1 English Reviews The Trip-MA dataset was created by Marcheg-giani et al. (2014) by annotating a set of 442 reviews, written in English, randomly sampled from the publicly available TripAdvisor dataset of Wang et al. (2010), composed by 235,793 reviews. Each review comes with an overall rating on a dis-crete ordinal scale from 1 to 5  X  X tars X . The dataset was annotated according to 9 recurrent aspects fre-quently involved in hotel reviews: Rooms , Clean-Food , and Building . The last two are not officially rated by TripAdvisor but were added because they are frequently commented in reviews. Two  X  X atch-all X  aspects, Other and NotRelated , were also added, for a total of 11 aspect. Aspect Other denotes opinions that are pertinent to the hotel being re-viewed, but not relevant to any of the former nine aspects (e.g., generic evaluations like Pulitzer ex-ceeded our expectations ). Aspect NotRelated de-notes opinions that are not related to the hotel (e.g., Tour Eiffel is amazing ).

If a sentence is relevant to an aspect, the possi-ble sentiment label values are three: Positive , Neg-subjective evaluations that are not clearly polar-ized (e.g., The hotel was fine with some excep-tions ). 2.1.1 Annotation protocol Marcheggiani et al. (2014) relied on three human annotators to annotate each sentence of the 442 re-views with respect to polarities of opinions that are relevant to any of the 11 aspects. 73 reviews, out of 442, were independently annotated by all the annotators in order to measure the inter-annotator agreement, while the remaining 369 reviews were partitioned into 3 equally-sized sets, one for each annotator. Bias in the estimation of inter-annotator agreement was minimized by sorting the list of re-views of each annotator so that every eighth re-view was common to all annotators; this ensured that each annotator had the same amount of coding experience when labeling the same shared review. Table 1: Number of reviews, sentences, and sen-tences with at least one opinion annotation. 2.2 Spanish and Italian Reviews For the creation of ES and IT parts of the Trip-MAML dataset we followed the same annotation protocol of Marcheggiani et al. (2014), employ-ing teams of three native speakers as annotators for each language. We crawled the Spanish and Italian reviews from TripAdvisor by accessing its websites with the  X .es X  and  X .it X  domains, which mostly contains reviews in the national language. From that domains we downloaded the reviews for the 10 most visited cities in Spain and Italy, respectively. We downloaded 10 reviews for ev-ery hotel of each city, obtaining a total of 17,020 reviews for Spanish and 33,325 for Italian. For each dataset, 500 reviews were selected by ran-domly sampling 50 reviews for each city. We thus obtained 139 unique reviews for each annotator, plus 83 reviews which all three annotators inde-pendently annotated.

We decided to annotate the aspects that were ratable on TripAdvisor at the time of our crawl cation , and Sleep Quality ). Differently from the as-pect schema in EN, we included the new aspect Sleep Quality , and we did not consider the miss-ing aspects Check-in and Businnes , which are, in any case, the least frequent aspects in the Trip-MA dataset (see Table 2). We kept the additional as-still appear frequently in the reviews. We adopted the same 3-values sentiment label schema of EN, Following the same procedure adopted by Marcheggiani et al. (2014), the Spanish and Ital-ian annotator teams performed a preliminary an-notation session on reviews not included in the fi-nal dataset. This preliminary activity was aimed at aligning the annotators X  understanding about the labeling process for the different aspects, by shar-ing and solving any doubt that might arise during the annotation of some examples. 2.3 Statistics Table 1 shows that English reviews have, on av-erage, about double the number of sentences of Spanish and Italian reviews. This can be in part motivated by observing that the sentences in EN are, on average, 25% shorter than in ES and IT. Also, after a manual inspection of the data, we found that the EN part contains some reviews re-lated to long vacations in resorts, thus describ-ing in longer details the experience, while IT and ES reviews are mainly related to relatively short visits to classic hotels. However, the portion of opinionated sentences is similar across the three parts, indicating homogeneity in content, which is confirmed by the detailed aspect-level statistics re-ported in Table 2.

Both aspect and sentiment labels show imbal-anced distributions that follow similar distribu-tions across the three parts. The most frequent aspect in all collections is Other , followed by are among the least frequent ones. The average value of the Pearson correlation between the lists of the shared aspects ranked by their relative fre-quency, measured pairwise among the three parts, is 0.795, which indicates a good uniformity of content among the parts. In all the three parts, Pos-itive is the most frequent sentiment label, followed by Negative . Location is always the aspect with the highest frequency of positive labels. We measured the inter-annotator agreement in two steps. The F 1 score measures the agreement on aspect identification, regardless of the sentiment label assigned. Then symmetric Macro-averaged Mean Absolute Error ( sMAE M ) (Baccianella et al., 2009) measures the agreement on sentiment labels on the annotations for which the annotators agreed at the aspect level. Aspect NotRelated is not included in agreement evaluation, nor in the ex-periments of Section 4. sMAE M is computed be-tween each of the three possible pairs of annota-tors and then averaged to determine the agreement values reported in Table 3.

Agreement on aspect detection is higher for ES and IT than for EN. This difference is in part moti-vated by the fact that the two aspects that are miss-ing in ES and IT have low agreement on EN, and the novel Sleep Quality aspect has instead a high agreement. However, also on the other aspects there is, in general, a higher or equal agreement in ES and IT with respect to EN, indicating that the formers two were annotated in a more consistent way. The agreement on assignment of sentiment label is rather similar across the whole dataset. The experiments we present here are aimed at defining a shared baseline for future experiments. For this reason we chose a relatively simple setup that uses a simple learning model and minimal lin-guistic resources. We used a sentence-level Linear Chain (LC) Conditional Random Field (Lafferty et al., 2001) as described by Marcheggiani et al. (2014). With respect to the features extracted from text, we used three simple features types: word unigrams, bigrams, and SentiWordNet-based fea-tures, which consist of a Positive and a Negative feature extracted every time the review contains a word that is marked as such in SentiWordNet (Baccianella et al., 2010). To use SentiWord-Net on ES and IT, we used Multilingual Cen-tral Repository (Gonzalez-Agirre et al., 2012) and MultiWordNet (Pianta et al., 2002) to map senti-ment labels to Spanish and to Italian, respectively.
Experiments were run separately on the EN, ES, and IT parts, leaving cross-lingual experiments to future work. On each part we built five 70%/30% train/test splits, randomly generated by sampling the reviews annotated by single reviewers (we left out reviews annotated by all the reviewers, as we consider that part of the dataset more useful as a validation set for the optimization of methods tested in future experiments). We then run the five experiments and averaged their results. 4.1 Evaluation Measures As for the agreement evaluation (Section 3), we split the evaluation of experiments into two parts, aspect detection and sentiment labeling. For the sentiment labeling part we used simple Macro-averaged Mean Absolute Error ( MAE M , not the symmetric version) as the true dataset labels are the reference ones in this case, while in the anno-tator agreement case the two sets of labels have equal importance. 4.2 Results Experiments on ES and IT obtain better F 1 values than on EN, indicating that the observed higher human agreement can be also explained by a lower hardness of the task when working with Spanish ES F 1 .520 .668 .766 .782 .567 .730 -.416 .593 .215 -.584 IT F 1 .576 .747 .646 .770 .697 .757 -.254 .630 .087 -.574 and Italian.
 again confirming what has been observed on agreement. However, MAE M values on experi-ments are sensibly worse than those measured on agreement, possibly due to the fact that we used very basic features, with limited use of sentiment-related information. We have presented Trip-MAML a multi-lingual extension of Trip-MA, originally presented in (Marcheggiani et al., 2014). The extension pro-cess involved crawling and selecting the reviews for the two new languages, Spanish and Italian, and their annotation by a total of six native lan-guage speakers. We measured dataset statistics and inter-annotator agreement, which show that the new ES and IT parts we produced are consis-tent with the original EN part. We also presented experiments on the dataset, based on a linear chain CRFs model for the automatic detection of aspects and their sentiment labels, establishing a baseline for future research. Trip-MAML enables the ex-ploration of cross-lingual approaches to the prob-lem of multi-aspect sentiment classification. This work has been partially supported by ATTOS project (TIN2012-38536-C03-0) from the Spanish Government.

