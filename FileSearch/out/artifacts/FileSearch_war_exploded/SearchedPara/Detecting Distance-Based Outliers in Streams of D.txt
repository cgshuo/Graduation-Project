 In this work a method for detecting distance-based outliers in data streams is presented. We deal with the sliding win-dow model, where outlier queries are performed in order to detect anomalies in the current window. Two algorithms are presented. The first one exactly answers outlier queries, but has larger space requirements. The second algorithm is directly derived from the exact one, has limited memory requirements and returns an approximate answer based on accurate estimations with a statistical guarantee. Several experiments have been accomplished, confirming the effec-tiveness of the proposed approach and the high quality of approximate solutions.
 H.2.8 [ Database Management ]: Database Applications X  Data mining Algorithms, Performance Anomaly detection, data streams, distance-based outliers
In many emerging applications, such as fraud detection, network flow monitoring, telecommunications, data manage-ment, etc., data arrive continuously, and it is either unnec-essary or impractical to store all incoming objects. In this context, an important challenge is to find the most excep-tional objects in the stream of data.

A data stream is a large volume of data coming as an unbounded sequence, where, typically, older data objects are less significant than more recent ones, and thus should contribute less. This is because characteristics of the data may change during the evolution, and then the most recent behavior should be given larger weight.
 Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00.
Therefore, data mining on evolving data streams is often performed based on certain time intervals, called windows. Two main different data streams window models have been introduced in literature: landmark window and sliding win-dow [15].

In the first model, some time points (called landmarks) are identified in the data stream, and analysis are performed only for the stream portion which falls between the last land-mark and the current time. Then, the window is identified by a fixed endpoint and a moving endpoint.

In contrast, in the sliding window model, the window is identified by two sliding endpoints. In this approach, old data points are thrown out as new points arrive. In particu-lar, a decay function is defined, that determines the weight of each point as a function of elapsed time since the point was observed. A meaningful decay function is the step func-tion. Let W be a parameter defining the window size and let t denote the current time, the step function evaluates to 1 in the temporal interval [ t  X  W + 1 , t ], and 0 elsewhere.
In all window models the main task is to analyze the por-tion of the stream within the current window, in order to mine data stream properties or to single out objects con-forming with characteristics of interest.

In this work, the problem of detecting objects, called out-liers , that are abnormal with respect to data within the cur-rent window, is addressed. Specifically, the sliding window model with the step function as decay function is adopted.
There exist several approaches for the problem of singling out the objects mostly deviating from a given collection of data [8, 6, 17, 11, 16, 1, 22]. In particular, distance-based approaches [17] exploit the availability of a distance function relating each pair of objects of the collection. They identify as outliers the objects lying in the most sparse regions of the feature space.

Distance-based definitions [17, 24, 4] represent an use-ful tool for data analysis [18, 14, 21]. Given parameters k and R , an object is a distance-based outlier if less than k objects in the input data set lie within distance R from it [17]. Distance-based outliers have robust theoretical founda-tions, since they are a generalization of different statistical tests. Furthermore, they are computationally efficient, since distance-based outlier scores are a monotonic non-increasing function of the portion of the database already explored.
Two algorithms for detecting distance-based outliers in data streams are presented. The first one exactly answers outlier queries at any time, but has larger space require-ments. The second algorithm is directly derived from the exact one, has limited memory requirements and returns an approximate answer based on highly accurate estimations with a statistical guarantee.

The approach proposed in this work introduces a novel concept of querying for outliers. Specifically, previous work deals with continuous queries , that are queries evaluated continuously as data stream objects arrive; conversely, we deal with one-time query , that are queries evaluated once over a point-in-time (for a survey on data streams query models refer to [7]).

The underlying intuition is that, due to stream evolution, object properties can change over time and, hence, evaluat-ing an object for outlierness when it arrives, although mean-ingful, can be reductive in some contexts and sometimes misleading. On the contrary, by classifying single objects when a data analysis is required, data concept drift typical of streams can be captured. To this aim, it is needed to sup-port queries at arbitrary points-in-time, called query times , which classify the whole population in the current window instead of the single incoming data stream object. To our best knowledge this is the first work performing outlier de-tection on windows at query time.

The example below shows how concept drift can affect the outlierness of data stream objects.

The contribution of this work can be summarized as fol-lows: The rest of the work is organized as follows. Section 2 briefly surveys related outlier detection approaches and data stream algorithms. Subsequent Section 3 formally states the data stream outlier query problem which this work deals with. Section 4 describes both the exact and the approximate al-gorithm. Section 5 illustrates experimental results. Finally, Section 6 presents conclusions.
Distance-based outliers have been first introduced by Knorr and Ng [17]. Given parameters k and R , an object is a distance-based outlier if less than k objects in the input data set lie within distance R from it. This definition is a solid one, since it generalizes several statistical outlier tests.
Some variants of the original definition have been subse-quently introduced in literature. In particular, Ramaswamy et al. [24], in order to rank the outliers, introduced the fol-lowing definition: given k and n , an object o is an outlier if no more than n  X  1 other objects in the dataset have higher value for D k than o , where D k ( o ) denotes the distance of the k th nearest neighbor of o . Subsequently, Angiulli and Pizzuti [4, 5, 3], with the aim of taking into account the whole neighborhood of the objects, proposed to rank them on the basis of the sum of the distances from the k nearest neighbors, rather than considering solely the distance to the k th nearest neighbor. In this work we will deal with the definition provided in [17].

Knorr et al. [17, 19] presented two algorithms. The first one is a block nested loop algorithm that runs in O ( dN 2 time, where N is the number of points and d the number of dimensions of the data set, while the second one is a cell-based algorithm that is linear with respect to N , but exponential in d . The last method is fast only if d is small. On the other hand, the nested loop approach does not scale well. Thus, efforts for developing efficient algorithms that scale to large datasets have been subsequently made.
In [24] the authors presented two novel algorithms to de-tect outliers. The first assumes the dataset is stored in a spatial index, like the R  X  -tree [10]. The second algorithm, first partitions the input points using a clustering algorithm, and then prunes the partitions that cannot contain outliers.
Bay and Schwabacher [9], introduced the distance-based outlier detection algorithm ORCA. Basically, ORCA en-hances the naive block nested loop algorithm with a sim-ple pruning rule and randomization, obtaining a near linear scaling on large and high dimensional data sets. The CPU time of the this nested loop schema is often approximately linear in the dataset size.

Distance-based methods previously discussed are designed to work in the batch framework, that is under the assump-tion that the whole data set is stored in secondary mem-ory and multiple passes over the data can be accomplished. Hence, they are not suitable for data streams. While the majority of the approaches to detect anomalies in data min-ing consider the batch framework, some researchers have attempted to address the problem of online outlier detec-tion.

In [27] the SmartSifter system is presented, addressing the problem from the viewpoint of statistical learning the-ory. The system employs an online discounting learning al-gorithm to learn a probabilistic model representing the data source. Every time a datum is input, SmartSifter evaluates how large the datum is deviated relative to a normal pat-tern. In [2] the focus is on detecting rare events in a data stream. Rare events are defined on the basis of their devi-ation from expected values computed on historical trends. In [25] a distributed framework to approximate data distri-butions coming from a sensor network is presented. Kernel estimators are exploited to approximate data distributions. The technique is used to report outlying values in the union of the readings coming from multiple sensors.

The last work applies the outlier technique presented with two outlier definitions, i.e. distance-based and MDEF-based [23]. However, it must be said that this method is specifi-cally designed to support sensor networks. Moreover, all the techniques above discussed, detect anomalies online as they arrive, and one-time queries are not supported.
Next, the formalism employed in this work will be pre-sented. First of all, the formal definition of distance-based outlier is recalled [17].
 Definition 3.1 (Distance-Based Outlier).
 Let S be a set of objects, obj an object of S , k a positive inte-ger, and R a positive real number. Then, obj is a distance-based outlier (or, simply, an outlier ) if less than k objects in S lie within distance R from obj .
 Objects lying at distance not greater than R from obj are called neighbors of obj . The object obj is not considered a neighbor of itself.

A data stream DS is a possible infinite series of objects . . . , obj t  X  2 , obj t  X  1 , obj t , . . . , where obj observed at time t . We will interchangeably use the term identifier and the term time of arrival to refer to the time t at which the object obj t was observed for the first time.
We assume that data stream objects are elements of a metric space on which is defined a distance function. In the following we will use d to denote the space required to store an object, and  X  to denote the temporal cost required for computing the distance between two objects.

Given two object identifiers t m and t n , with t m  X  t n window DS [ t m , t n ], is the set of n  X  m + 1 objects obj obj t m +1 , . . . , obj t n . The size of the window DS [ t n  X  m + 1.
 Given a window size W , the current window is the window DS [ t  X  W + 1 , t ], where t is the time of arrival of the last observed data stream object.

An expired object obj is an object whose identifier id is less than the lower limit of the current window, i.e. such that id &lt; t  X  W + 1.

Now, we are in the position of defining the main problem we are interested in solving.
 Definition 3.2 (Data Stream Outlier Query).
 Given a data stream DS , a window size W , and fixed pa-rameters R and k , the Data Stream Outlier Query is: return the distance based outliers in the current window. The time t , which a data stream outlier query is requested at, is called query time .

In the following the neighbors of an object obj that pre-cede obj in the stream and belong to the current window are called preceding neighbors of obj . Furthermore, the neigh-bors of an object obj that follow obj in the stream and belong to the current window are called succeeding neighbors of obj .
According to Definition 3.1, an inlier is an object obj having at least k neighbors in the current window. In other words, let  X  be the number of preceding neighbors of obj and  X  be the number of succeeding neighbors of obj , obj is an inlier if  X  +  X   X  k .

Let obj be an inlier. Since during stream evolution objects expire, the number of preceding neighbors of obj decreases. Therefore, if the number of succeeding neighbors of obj is less than k , obj could become an outlier depending on the stream evolution. Conversely, since obj will expire before its succeeding neighbors, inliers having at least k succeed-ing neighbors will be inliers for any stream evolution. Such inliers are called safe inliers .
 In this section the algorithm STORM, standing for STream OutlieR Miner, is described. Two variants of the method are presented.

When the entire window can be allocated in memory, the exact answer of the data stream outlier query can be com-puted. In the above setting, the algorithm exact-STORM is able to exactly answer outlier queries at any time (Section 4.1).

Nevertheless, interesting windows are often so large that they do not fit in memory, while in some applications only limited memory can be allocated . In this case approxima-tions must be employed. The algorithm approx-STORM is designed to work in the latter setting by introducing effec-tive approximations in exact-STORM (Section 4.2). These approximations guarantee highly accurate answers with lim-ited memory requirements (Section 4.3).

After having described the two methods, temporal and spatial costs required to obtain exact and approximate an-swers will be stated (Section 4.4).
The algorithm exact-STORM is shown in Figure 1. This algorithm consists of two procedures: the Stream Manager and the Query Manager . The former procedure receives the incoming data stream objects and efficiently updates a suitable data structure that will be exploited by the latter procedure to effectively answer queries.

In order to maintain a summary of the current window, a data structure, called ISB (standing for Indexed Stream Buffer ), storing nodes is employed (nodes are defined next). Each node is associated with a different data stream object.
ISB provides a method range query search , that, given an object obj and a real number R  X  0, returns the nodes in ISB associated with objects whose distance from obj is not greater than R . We will describe the implementation of ISB in Section 4.4.

Now, the definition of node is provided. A node n is a record consisting of the following information: The Stream Manager takes as input a data stream DS , a window size W , a radius R , and the number k of nearest neighbors to consider.

For each incoming data stream object obj , a novel node n curr is created with n curr .obj = obj . Then a range query search with center n curr .obj and radius R is performed in ISB, that returns the nodes associated with the preceding neighbors of obj stored in ISB.

For each node n index returned by the range query search, since the object obj is a succeeding neighbor of n index .obj , the counter n index . count after is incremented. Moreover, since the object n index .obj is a preceding neighbor of obj , the list n curr . nn before is updated with n index .id .
If the counter n index . count after becomes equal to k , then the object n index .obj becomes a safe inlier. Thus, it will not belong to the answer of any future outlier query. De-spite this important property, a safe inlier cannot be dis-carded from ISB, since it may be a preceding neighbor of a future stream object. Finally, the node n curr is inserted into ISB. This terminates the description of the procedure Stream Manager .

In order to efficiently answer queries, when invoked by the user, the Query Manager performs a single scan of ISB. In particular, for each node n of ISB, the number prec neighs of identifiers stored in n. nn before associated with non-expired objects is determined. This is accomplished in O (log k ) time by performing a search in the list n. nn before of the iden-tifier closest to the value t  X  W + 1, that is the identifier of the oldest object in n. nn before belonging to the current window.

The number succ neighs of succeeding neighbors of n.obj is stored in count after . Thus, if prec neighs + succ neighs  X  k then the object n.obj is recognized as an inlier, otherwise it is an outlier and is included in the answer of the outlier query.
Figure 2 shows the algorithm approx-STORM. The exact algorithm requires to store all the window objects. If the window is so huge that does not fit in memory, or only lim-ited memory can be allocated, the exact algorithm could be not employable. However, as described in the following, the algorithm described in the previous section can be readily modified to reduce the space required.
 With this aim, two approximations are introduced.
Firstly, in order to severely reduce the space occupied, we do not store all the window objects into ISB. In particu-lar, objects belonging to ISB can be partitioned in outliers and inliers. Among the latter kind of objects there are safe inliers, that are objects that will be inliers in any future stream evolution. As already observed, despite safe inliers cannot be returned by any future outlier query, they have to be kept in ISB in order to correctly recognize outliers, since they may be preceding neighbors of future incoming objects.

However, as shown in subsequent Section 4.3, it is suffi-cient to retain in ISB only a fraction of safe inliers to guaran-tee an highly accurate answer to the outlier query. Thus, in order to maintain in ISB a controlled fraction  X  (0  X   X   X  1) of safe inliers, the following strategy is adopted.
During stream evolution, an object obj of the stream be-comes a safe inlier when exactly k succeeding neighbors of obj arrive. At that time, if the total number of safe in-liers into ISB exceeds  X W , then a randomly selected object of ISB is removed. The random selection policy adopted guarantees that safe inliers surviving into ISB are uniformly distributed in the window.

To answer one-time queries both outliers and non-safe in-liers, which are objects candidate to become outliers if the stream characteristics change, have to be maintained in ISB. Note that, meaningful combinations of the parameters R and k are only those for which the number of outliers, and hence of non-safe inliers, amounts to a negligible fraction of the overall population. Thus, the number of nodes in ISB can be assumed approximately equal to  X W. In the follow-ing section it will be discussed how to compute an optimal value for  X  in order to obtain a statistical guarantee on the approximation error of the estimation of the number of pre-ceding neighbors of each data stream object.

The second approximation consists in reducing the size of each node by avoiding storing the list of the k most re-cent preceding neighbors. This is accomplished by storing in each node n , instead of the list n.nn before , just the frac-tion n.fract before of previous neighbors of n.obj observed in ISB at the arrival time n.id of the object n.obj . The value n.fract before is determined as the ratio between the number of preceding neighbors of n.obj in ISB which are safe inliers and the total number of safe inliers in ISB, at the arrival time of n.obj .
 At query time, in order to recognize outliers, a scan of ISB is performed, and, for each stored node n , the number of neighbors of n.obj in the current window has to be evaluated. Since only the fraction n.fract before is stored now in n , the number of preceding neighbors of n.obj in the whole window at the current time t has to be estimated.

Let  X  be the number of preceding neighbors of n.obj at the arrival time of n.obj . Assuming that they are uniformly dis-tributed along the window, the number of preceding neigh-bors of n.obj at the query time t can be estimated as Note that n. fract before does not give directly the value  X  , since it is computed by considering only the objects stored in ISB and, thus, it does not take into account removed safe inliers preceding neighbors of n.obj . However,  X  can be safely (see next section) estimated as Summarizing, the number of preceding neighbors of n.obj at the query time t can be estimated as Recall that to classify objects the sum between the esti-mated number of its preceding neighbors and the number of succeeding neighbors is computed. It is worth to point out that the number of succeeding neighbors is not estimated, since n.count before provides the true number of succeeding neighbors of n.obj . Therefore as stream evolves, the above sum approaches the true number of neighbors in the window.
Now, we discuss how to set the parameter  X  in order to obtain safe bounds on the approximation error of the esti-mation.

Let W be the window size, let w be the number of safe inliers in ISB, let  X  be the exact number of preceding neigh-bors of an object at its time of arrival, let e  X  be the number of preceding neighbors of the object in ISB which are safe inliers at its time of arrival, and let  X  denote the ratio  X 
In order to determine an optimal value for  X  , a value for w , such that e  X  w is a good approximation for  X  , has to be deter-mined. Formally, the following property has to be satisfied. For given  X  &gt; 0 and 0 &lt;  X  &lt; 1 , we want to have:
Since ISB contains a random sample of the window safe inliers, a safe bound for w can be obtained from the well known Lyapounov Central Limit Theorem .

This theorem asserts that, for any  X  where  X (  X  ) denotes the cumulative distribution function of the normal distribution.
Thus, if w is large enough, then the following relation holds: Now, the result that will allow us to get the needed safe bound for w can be formally presented.

Theorem 4.1. For any  X  &gt; 0 and 0 &lt;  X  &lt; 1 , if w satisfies the following inequality then it satisfies (1) .
 Theorem 4.1 is a direct consequence of the central limit the-orem (see [26] for details).

Now, the bound stated in the above theorem is exam-ined. Although the provided bound depends on the un-known value  X  , it can be safely applied by setting  X  to 1 Therefore, in order to satisfy (1), being w =  X W , it is suffi-cient to set  X  to the value It is worth to note that the bound for w given by expression (2) does not depend on the window size W . Furthermore, since in expression (3) the unknown value  X  is safely set to 1 whenever  X  is different to 1 2 the property (1) is guaranteed for values of  X  and  X  better than those used to compute w . In particular, the two following inequalities hold: In the first inequality,  X   X  is obtained from the following equa-tion: whereas, in the second one,  X   X  is obtained from Note that, if the true value for  X  is 1 2 , then  X   X  =  X  and  X  =  X  .
 Equation (5) can be rewritten as It follows that the maximum error err we can make in com-puting  X  is This value provides the maximum error made in estimating the total number of neighbors of an object when it arrives.
Now, we are interested in determining when the above error will cause a misclassification, i.e. when an inlier (resp., an outlier) will be estimated as an outlier (resp., an inlier).
For an object obj having identifier id , when it arrives, the number of true preceding neighbors is  X W . As stream evolves, some preceding neighbors expire, and, assuming they are uniformly distributed along the window, in the por-tion of the stream preceding obj in the current window their number becomes  X  ( W  X  t + id ).

To correctly classify obj , if the sum between the number  X  of preceding neighbors of obj and the number  X  of succeeding neighbors is greater (resp. smaller) than k in the current window, then also the estimated value for  X  plus the number  X  of succeeding neighbors should be greater (resp. smaller) than k . Formally, let W = W  X  t + id be the number of objects of the current window preceding the object obj with identifier id , let  X  =  X  W be the true value of preceding neighbors of obj in the current window, let  X  be the number of its succeeding neighbors, and let 2 W X  error err . If  X  +  X  is greater than k , then the following inequality should hold: Assuming that the distribution of succeeding neighbors is the same as the distribution of preceding neighbors,  X  can be approximated to  X  ( W  X  W ), where ( W  X  W ) is the portion of the stream in the current window succeeding obj . Thus, for an inlier is recognized with probability (1  X   X  ).
Analogously, if  X  +  X  &lt; k , starting from with the same assumption stated above, we obtain that for an outlier is recognized with probability (1  X   X  ).
It can be concluded that, if an object obj has more than  X  up W or less than  X  down W neighbors, it is correctly classi-fied with probability 1  X   X  .

Contrariwise, if the number of neighbors of obj is in the range [  X  down W,  X  up W ] then we have an estimation error at most equal to 2 W X  classification. Both the error and the range are small and directly proportional to  X  . Moreover, they depend on the current time t . As t increases, the error goes to zero and the range tends to be empty.

Before concluding, it is worth to recall that object classi-fication depends also on the number of succeeding neighbors of the object, whose true value is known.
In this section the implementation of ISB is detailed, and then, temporal and spatial costs of STORM are analyzed. Implementation details. The ISB data structure is a pivot-based index [12]. It performs proximity search in any metric space making use of a certain number of objects (also called pivots ) selected among the objects stored into the in-dex. Distances among pivots and objects stored into the index are precalculated when a novel pivot is generated. When a range query with center obj and radius R is sub-mitted to the index, the distances between the pivots and the query object obj are computed. The precalculated dis-tances are exploited to recognize the index objects lying at distance greater than R from obj . For each index ob-ject obj 0 , if there exists a pivot p , such that | dist( obj, p )  X  dist( obj 0 , p ) | &gt; R , then, by the triangular inequality, it is known that dist( obj, obj 0 ) &gt; R , and obj 0 is ignored. Oth-erwise, obj 0 is marked as a candidate neighbor and if the distance dist( obj 0 , obj )  X  R then obj 0 is returned as a true neighbor of obj . By using this kind of technique the range query search returns the list of neighbors of obj . The per-formances of pivot-based indexes are related to the number of pivots employed. In particular the larger is the number of pivots, the more accurate is the list of candidates, and then the lower is the number of distances computed. Neverthe-less, the cost of querying and building the index increases. In this work the number of pivots used is logarithmic with respect to the index size. In order to face concept drift, the older pivot is periodically replaced with an incoming object.
Now, the temporal cost of operations to be executed on the ISB data structure is analyzed. Recall that the cost of computing the distance between two objects is  X . Assume that the number of nodes stored in ISB is N .

The cost of performing a range query search corresponds to the cost of computing the distances between an object and all the pivots plus the cost of determining true neighbors. Since the number of pivots we used is logarithmic in the size of the index, the former cost is O ( X  log N ). As for the latter cost, let  X  (0  X   X   X  1) be the mean fraction of index objects marked as candidate neighbors when the radius is set to R . In order to determine if a candidate is a true neighbor, its distance from the query object has to be computed. Then, the cost is O ( X   X N ). Supposing that the latter cost is always greater than the former one, the total cost for the range query search is O ( X   X N ).

The cost of removing an object from the index is constant, since it practically consists in flagging as empty the entry of an array.

Finally, as for the insertion of an object obj into ISB, it requires to compute the distances among obj and all the pivots. However, since insertion is always performed after the range query search, these distances are already available and, then, the insertion cost is constant, too.
 Spatial analysis. For exact-STORM, ISB stores all the W objects of the current window and, for each of them, the list nn before of the k most recent preceding neighbors, and two integer numbers. Recall that each object requires space d and each list requires space k .

For approx-STORM, assuming meaningful combinations of the parameters, ISB stores approximately  X W objects of the current window and, for each of them, a counter of pre-ceding neighbors, and two integer numbers.

Summarizing, the spatial cost of the algorithm STORM is Temporal analysis. The procedure Stream Manager con-sists of four steps (see Figures 1 and 2). The cost of the step 1 corresponds to the cost of removing an object from ISB, which, from the discussion above, is constant. Also step 2 has a constant cost.
 Step 3 performs a range query search, whose cost is O ( X   X N ). For each of the  X N objects returned by the search, the exact-STORM performs an ordered insertion into the list nn before , that costs O (log k ) (see Section 4.1), and executes some other operations having constant cost. Contrariwise, the approx-STORM possibly removes a node from ISB and executes some other operations. Both the removal and the other operations have constant cost.

Finally, step 4 inserts a node into ISB and hence has con-stant cost. Summarizing, the cost of the procedure Stream Manager is The procedure Query Manager consists of a scan of the ISB structure. For each node n , the exact-STORM computes prec neighs in time O (log k ) (see Section 4.1) by determining the number of identifiers in n. nn before associated with non-expired objects. Conversely, the approx-STORM performs only steps having constant cost. Summarizing, the cost of the procedure Query Manager is
In this section, we present results obtained by experiment-ing the proposed techniques on both synthetic and real data sets.
 First of all, the data set employed are described. The Gauss data set is a synthetically generated time sequence of 35 , 000 one dimensional observations, also used in [23]. It consists of a mixture of three Gaussian distributions with uniform noise.

We also used some public real data from the Pacific Ma-rine Environmental Laboratory of the U.S. National Oceanic &amp; Atmospheric Administration (NOAA). Data consist of temporal series collected in the context of the Tropical At-mosphere Ocean project (TAO) 1 . This project collects real-time data from moored ocean buoys for improved detec-tion, understanding and prediction of El Ni  X no and La Ni  X na,
See http://www.pmel.noaa.gov/tao/ . which are oscillations of the ocean-atmosphere system in the tropical Pacific having important consequences for weather around the globe. The measurements used in experiments have been gathered each ten minutes, from January 2006 to September 2006, by a moored buoy located in the Tropical Pacific.

We considered both a one and a three dimensional data stream. The Rain data set consists of 42 , 961 rain measure-ments. The TAO data set consists of 37 , 841 terns (SST, RH, Prec), where SST is the sea surface temperature, mea-sured in units of degrees centigrade at a depth of 1 meter, RH is the relative humidity, measured in units of percent at a height of 3 meters above mean sea level, and Prec is the precipitation, measured in units of millimeters per hour at a height of 3.5 meters above mean sea level. The three attributes were normalized with respect to their standard deviation.

Finally, we employed the 1998 DARPA Intrusion Detec-tion Evaluation Data [13], that has been extensively used to evaluate intrusion detection algorithms. The data consists of network connection records of several intrusions simulated in a military network environment. The TCP connections have been elaborated to construct a data set of 23 numeri-cal features. We used 50 , 000 TCP connection records from about one week of data.

In all experiments, the window size W was set to 10 , 000 and the parameter k was set to 0 . 005  X  W = 50. The param-eter R was selected to achieve a few percent of outliers in the current window ( R = 0 . 1 for Gauss , R = 0 . 5 for Rain , R = 1 for TAO , and R = 1 , 000 for DARPA ).

Furthermore, an outlier query was submitted every one hundred objects. Measures reported in the sequel are aver-aged over the total number of queries submitted. The first query was submitted only after having observed the first W data stream objects.
 The classification accuracy of the method was evaluated. It is worth to recall that exact-STORM exactly detects distance-based outliers in the current window. Thus, the answer re-turned by this algorithm was used to evaluate the quality of the approximate solution returned by approx-STORM.
The precision and recall measures were employed. Preci-sion represents the fraction of objects reported by the algo-rithm as outliers that are true outliers. Recall represents the fraction of true outliers correctly identified by the algorithm.
Figure 3 shows precision (dark bars, on the left) and recall (light bars, on the right) achieved by approx-STORM on the four considered data sets, for increasing values of  X  , that is  X  = 0 . 01,  X  = 0 . 05, and  X  = 0 . 10.

Interestingly, on the Gauss data set the method practi-Table 1: Elaboration time per single object [msec]. cally returned all and only the true outliers. This is because in this data set outliers are represented by noise which is well separated from the data distribution. Notice that other methods were not able to obtain this very good classification result.

As for the data sets from the TAO Project, since outliers there contained are associated to large oscillations of earth parameters, they lie on the boundary of the overall measure-ment distribution and are not completely separated from the rest of the population. Thus, there exists a region of tran-sition where the approximate algorithm can fail to exactly recognize outliers (see below in this section for an evalua-tion of the characteristics of objects on which classification errors are made).

It is clear by the diagrams that by augmenting the pa-rameter  X  the precision tends to decrease while the recall tends to increase. This can be explained since by using a small sample size the number of nearest neighbors tends to be overestimated. Anyway, the classification accuracy was very good, e.g. precision 0 . 965 and recall 0 . 942 on the Rain data set, and precision 0 . 948 and recall 0 . 935 on the TAO data set, for  X  = 0 . 05.

The DARPA data set represents a challenging classifica-tion task due the considerable number of attributes it is composed of. The precision-recall trade-off previously ob-served is confirmed also on this data set. Moreover, the classification accuracy is of remarkable quality: for  X  = 0 . 05, precision 0 . 947 and recall 0 . 956 were achieved.
Table 1 reports the time (in milliseconds) employed by approx-STORM for various values of  X  (first three columns), and by exact-STORM (fourth column) to process an incom-ing data stream object 2 . Approx-STORM guarantees time savings with respect to exact-STORM which are in most cases proportional to the parameter  X  . Differences in perfor-
Experiments were executed on a Core 2 Duo based machine having 2GB of main memory. mances among the various experiments are justified by the different characteristics of the data sets, and among them, particularly, by the mean fraction  X  of objects falling in the neighborhood of radius R of data stream objects.

Figure 4 shows the distribution of the number of near-est neighbors associated with objects of the Rain data set which are misclassified by exact-STORM. These diagrams are interesting to comprehend the nature of the misclassi-fied objects returned and the quality of the approximation. From left to right, diagrams are associated with increasing values of  X  .

The abscissa reports the number of nearest neighbors, while the ordinate the cumulated absolute frequency of mis-classified objects. Two cumulated histograms are included in each diagram, one concerning outliers and the other con-cerning inliers.

Light bars (on the left) represent the mean number of outliers which are reported as inliers. Thus, these misclas-sifications concern the recall measure. Specifically, a bar of position k 0 and height h 0 represents the following informa-tion: among the objects having at most k 0 ( &lt; 50) nearest neighbors (and hence outliers), on the average, h 0 of them have been recognized as inliers.

Dark bars (on the right) represent the mean number of inliers which are reported as outliers. Thus, these misclassi-fications concern the precision measure. Specifically, a bar of position k 0 and height h 0 represents the following infor-mation: among the objects having at least k 0 (  X  50) nearest neighbors (and hence inliers), on the average, h 0 of them have been recognized as outliers.

These diagrams show that for small sample sizes the num-ber of errors is biased towards the outliers, due to the over-estimation effect. Moreover, more interestingly, they show the nature of the misclassified objects. Indeed, as predicted by the analysis of Section 4.3, for an object the probabil-ity of being misclassified greatly decreases with the distance | k 0  X  k | between the true number k 0 of its nearest neighbors and the parameter k . Indeed, the majority of the misclas-sified inliers have a number of neighbors close to k . For example, when  X  = 0 . 05, almost all the misclassified outliers have at most 60 neighbors (compare this value with k = 50). The quality of the approximate answer is thus very high. Although these objects are not outliers according to Defini-tion 3.1, from the point of view of a surveillance application, they could be as interesting as true outliers, since they any-how lie in a relatively sparse region of the feature space.
In this work the problem of detecting distance-based out-liers in streams of data has been addressed. The novel data stream outlier query task was proposed and motivated, and both an exact and an approximate algorithm to solve it were presented. Also, bounds on the accuracy of the estima-tion accomplished by the approximated method. Finally, experiments conducted on both synthetic and real data sets showed that the proposed methods are efficient in terms pro-cessing time, and the approximate one is effective in terms of precision and recall of the solution.
 Acknowledgments. The authors would like to thank the TAO Project Office for making available the collected mea-surements. [1] C. C. Aggarwal and P.S. Yu. Outlier detection for [2] Charu C. Aggarwal. On abnormality detection in [3] F. Angiulli, S. Basta, and C. Pizzuti. Distance-based [4] F. Angiulli and C. Pizzuti. Fast outlier detection in [5] F. Angiulli and C. Pizzuti. Outlier mining in large [6] A. Arning, C. Aggarwal, and P. Raghavan. A linear [7] Brian Babcock, Shivnath Babu, Mayur Datar, Rajeev [8] V. Barnett and T. Lewis. Outliers in Statistical Data . [9] S. D. Bay and M. Schwabacher. Mining distance-based [10] N. Beckmann, H.-P. Kriegel, R. Schneider, and [11] M. M. Breunig, H. Kriegel, R.T. Ng, and J. Sander. [12] Edgar Ch  X avez, Gonzalo Navarro, Ricardo A.
 [13] Defense Advanced Research Projects Agency DARPA. [14] E. Eskin, A. Arnold, M. Prerau, L. Portnoy, and [15] Lukasz Golab and M. Tamer  X  Ozsu. Issues in data [16] W. Jin, A.K.H. Tung, and J. Han. Mining top-n local [17] E. Knorr and R. Ng. Algorithms for mining [18] E. Knorr and R. Ng. Finding intensional knowledge of [19] E. Knorr, R. Ng, and V. Tucakov. Distance-based [20] Donald Knuth. The Art of Computer Programming, [21] A. Lazarevic, L. Ert  X oz, V. Kumar, A. Ozgur, and [22] S. Papadimitriou, H. Kitagawa, P.B. Gibbons, and [23] Spiros Papadimitriou, Hiroyuki Kitagawa, Phillip B. [24] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient [25] S. Subramaniam, T. Palpanas, D. Papadopoulos, [26] O. Watanabe. Simple sampling techniques for [27] Kenji Yamanishi, Jun ichi Takeuchi, Graham J.
