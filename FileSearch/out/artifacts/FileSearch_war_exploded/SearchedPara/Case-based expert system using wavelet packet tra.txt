 1. Introduction
Automotive engine ignition systems vary in construction, but are similar in basic operation. All have a primary circuit that causes a spark in the secondary circuit. This spark must then be delivered to the correct spark plug at the proper time. Conditions in the ignition system and in the cylinder affect the ignition pattern (i.e. scope pattern) in the secondary circuit. An automo-tive oscilloscope, also known as a scope meter, is considered a valuable tool for detecting engine and ignition problems by displaying the scope patterns for analysis of the operation of an ignition system. The scope patterns can reflect the conditions within the ignition system and help pinpoint their causes of failure, such as narrow spark-plug gaps, open spark-plug cables, a shorted ignition coil, etc. In a typical diagnosis, the scope patterns are usually matched against a normal pattern. Fig. 1 shows some examples of scope patterns and their corresponding engine faults. necessary when an automotive scope meter for engine trouble diagnosis is used. The pickup-clamp is usually connected to the ignition system to capture its spark ignition patterns. Capturing ignition pattern usually requires intervention of the mechanic to indicate the start and end points of the pattern. The captured pattern would then be compared with samples from the hand-books ( Liu and Chu, 2005 ; Crouse and Anglin, 1993 ) for diagnosis.
The diagnosis is always based on domain knowledge and user experience because the samples from the handbooks are for reference only. However, the ignition patterns are time-varying and non-stationary. Different engine models produce various amplitude and duration for the same ignition system trouble.
Even on the same engine, different sizes of ignition patterns under different engine operating conditions may be produced, which increases the difficulty for human diagnosis. Moreover, the igni-tion patterns of many engine faults are very similar, leading to increased difficulty in identifying the patterns correctly. After diagnosis, the corresponding parts in the ignition system will be disassembled for intensive investigation (this is called a trial). In view of the inaccuracy of human diagnosis, identifying a fault based on ignition patterns, several trials for disassembling and assembling of engine parts are necessary, which incurs a large amount of time and effort by the mechanic. A computer-based pattern classification system is proposed to aid an automotive mechanic for this problem.

Currently, there is very little research in the literature ( Vong and Wong, 2011 ) on computer-aided ignition pattern analysis for automotive engine ignition system trouble-diagnosis. The devel-opment of an intelligent system for engine ignition system diagnosis is a promising area of research.
 Machine learning methods ( Xue and Zhu, 2009 ; Kotsiantis, 2007 ; Whiteson and Whiteson, 2009 ) play an important role in the pattern recognition, such as multi-layer perceptions (MLP) and support vector machines (SVM). A major focus of machine learning research is to automatically learn recognition of complex patterns and make intelligent decisions based on sampled data. In this research, if the computer-aided diagnostic system is designed merely based on captured signals, the accuracy will likely be poor due to the excessive similarity between the captured signals. Applying this rationale, machine learning methods combined with domain knowl-edge are proposed in this research . However, most of the machine learning methods can only provide a user with a single solution. To increase the chance of locating the faults, a case-based reasoning (CBR) expert system is proposed, which can generate multiple diagnoses, instead of the single most probable one generated by traditional network-based cl assifiers such as MLP and SVM ( Fig. 2 ).
This kind of CBR expert system helps a mechanic make a final decision. From this viewpoint, CBR paradigm fits the requirements of the expert system better. In addition, CBR can overcome the problem of incremental and decremental knowledge update as compared to inductive learning ( Smyth and Cunningham, 1995 ) such as MLP and SVM. Moreover, when adding or removing a case, the maintenance of CBR classifier is relatively easy.

One of the main difficulties of building a CBR diagnostic system is to define the clear and effective case representation in the case library. In the current research, it is unwise to directly put the captured signals in the case representation. The ignition patterns captured by the scope meter are very similar and their differences can only be compared under extr acted features. Similar work extracting effective representation from sensor signals can be found in ( Funk and Xiong, 2008 ) and hence wavelet packet transform (WPT) is proposed for the purpose of feature extraction. However, the number of the extracted WPT features for an ignition signal is very large (about 18,000 points) so that the computational cost of building and running the diagnostic system will be very high. For this reason, kernel principal component analysis (KPCA) is proposed to perform a dimension reduction while retaining most of the pattern information of the ignition signals. Finally, the set of features processed by WPT and KPCA along with the domain knowledge of the signals constitutes the case representation of the CBR diagnostic system. CBR is effective but mostly inefficient in computational time because every instance in a case library must be compared during its reasoning, i.e., retrieval. To overcome this inefficiency, a clustering method of kernel k-means (KKM) was employed to compress the case library, where similar cases were clustered into a fixed number of representative cases. Using this set of representative cases, retrieval time can be further reduced significantly. In the following section, the employed techniques are briefly discussed. 2. Employed techniques
Following the above discussion, we employed WPT for feature extraction, and KPCA for dimension reduction for this work. These two techniques were used for preprocessing the raw captured patterns to form a case library. KKM was examined for clustering the case library into a set of representative cases, for which CBR works in diagnosis. The CBR design and reasoning steps were subsequently addressed. 2.1. Wavelet transform
In the last decade, Wavelet Transforms (WT) have been widely applied in diverse fields of random signal processing ( Akansu et al., 2010 ). The transformation of a signal to another form using
WT does not change the information content in the original signal. WT uses multi-resolution technique where different frequencies are analyzed with different resolutions in order to generate a time-frequency representation of the signal. Discrete wavelet transform (DWT) and WPT were derived from the WT family. Although DWT and WPT all decompose a signal into several bands representing low and high frequencies, they differ in the number of resultant bands. The details about DWT and
WPT are introduced in the following sections. 2.1.1. Discrete wavelet transform (DWT) 2008 ) has been proven very efficient in signal analysis for many engineering applications ( Scott et al., 2008 ; Bhowmik et al., 2009 ; Celik and Tjahjadi, 2009 ; Pringle et al., 2008 ; Uyar et al., 2008 ;
Nun  X  ez-Carrera et al., 2009 ; Chen and Wang, 2009 ). The general concept of DWT is depicted in Fig. 3 . Under DWT, a signal x is generally decomposed iteratively by two digital filters, followed by down-sampled by a factor of 2. The first digital filter g [ ]isa high-pass filter and the discrete mother wavelet is used to extract the high frequencies of a signal. The second digital filter h [ ]isa low-pass filter, which is a mirror of g [ ], aiming for the extraction of low frequencies. The down-sampled outputs of a signal with g [ ] and h [ ] are defined as detail D u and approximation A respectively, where u is the decomposition level. Then the same filtering process for the approximation of A u to produce D A u  X  1 continues until it reaches the termination condition. All DWTs can be specified in terms of a low-pass filter h , which satisfies the standard quadrature mirror filter condition:
H  X  z  X  H  X  z 1  X  X  H  X  z  X  H  X  z 1  X  X  1  X  1  X  where H ( z ) denotes the z -transform of the filter h . The comple-mentary high-pass filter of H ( z ), namely G ( z ), can be defined as G  X  z  X  X  zH  X  z 1  X  X  2  X 
A sequence of filters with increasing length (indexed by u ) can be obtained, from u  X  0to M 1, where M is the length (or number of points) of the signal x :
H  X  z  X  X  H  X  z 2 v  X  H u  X  z  X  X  3  X 
G  X  z  X  X  G  X  z 2 v  X  G u  X  z  X  X  4  X  with the initial condition H 0 ( z )  X  1 and v is the scale parameter. It is expressed as a two-scale relation in time domain: h  X  k  X  X  X  h m 2 v h v  X  k  X  X  5  X  g  X  k  X  X  X  g m 2 v h v  X  k  X  X  6  X  where the subscript [ ] m m indicates the up-sampling by a factor of m , and k is the discrete equally sampled time. The normalized wavelet and scale basis functions j v,l ( k )and c v,l ( k ) can be defined as  X  k  X  X  2 v = 2 h v  X  k 2 v l  X  X  7  X  c  X  k  X  X  2 v = 2 g v  X  k 2 v l  X  X  8  X  where the factor 2 v /2 istheinnerproductofthenormalization; v and l are the scale parameter and the translation parameter, respectively. The DWT decomposition of a signal x ( k ) can be described as
A  X  l  X  X  x  X  k  X  j v , l  X  k  X  X  9  X 
D  X  l  X  X  x  X  k  X  c v , l  X  k  X  X  10  X  2.1.2. Wavelet packet transform (WPT) WPT ( Firouzjah et al., 2009 ; Zheng et al., 2001 ; Yu and
Kamarthi, 2010 ; Morsi and El-Hawary, 2009 ) is a generalization of wavelet decomposition that offers a richer signal analysis. In the decomposition of a signal by DWT, only the lower frequency band is decomposed, giving a right recursive binary tree structure whose right lobe represents the lower frequency band, and its left lobe represents the higher frequency band. In the corresponding
WPT decomposition, the lower and the higher frequency bands are decomposed giving a balanced binary tree structure ( Mallat, 2008 ) as shown in Fig. 4 . 2.2. Kernel principal component analysis
Principal component analysis (PCA) is a usual method for feature extraction. PCA always performs well in dimensionality reduction when the input variables are linearly correlated. However, for nonlinear case, PCA cannot give good performance. Hence PCA is extended to nonlinear version under SVM formula-tion called Kernel PCA (KPCA) ( Teixeira et al., 2008 ; Cao et al., 2004 ; Kim and Oommen, 2004 ; Huang, 1999 ). KPCA involves solving a in the following sets of equations: O a  X  l a  X  11  X  where O kl  X  K ( x k , x l ) for k , l  X  1, y , N , and x dimension. The kernel function K is chosen as Radial Basis Function (RBF), i.e., K ( x , y )  X  exp( : x y : /2 s 2 ), with the user-predefined standard deviation s . The vector a  X  [ a 1 ; y eigenvector of O and l A R is the corresponding eigenvalue where N is the number of training data. The transformed variables (score variables) z i for vector x become z  X  x  X  X  largest eigenvalue, i  X  1, 2, y , p. p is the largest nonzero eigenva-lue l p of the eigenvector a p . Therefore, based on the p pairs ( a for j  X  1to p , the input vector x A R n can be transformed to a nonlinearly uncorrelated variable z  X  [ z 1 , y , z p ] where p o n .An important to note is that the eigenvectors a i should satisfy the normalization condition of unit length: a T where l 1 Z l 2 Z ? Z l p 4 0, i.e., l i are nonzero. A post-pruning procedure can be performed to produce a further reduced feature vector. After acquiring the p pairs ( a j , l j ), j  X  1to p , all l normalized eigenvalues l j 0 , deleted from backward l j 0 until S l 0 r 0.95, finally the index l is set to j . Meeting the condition where index l o p , the eigenvectors a j ( j  X  1to l ) are selected to produce a reduced feature vector, which retains 95% of the information content in the transformed features. Usually 5% information loss is a rule of thumb for dimensionality reduction. The general concept is depicted in Fig. 5 . 2.3. Clustering
One of the most popular clustering algorithms is K-means ,where homogeneous groups are identified by minimizing the clustering error. The clustering error is defined as the sum of the squared Euclidean distances between each data point and the corresponding cluster center. The Euclidean distance algorithm is based on the assumption that the data space consists of isolated elliptical regions.
However, such assumption is not always valid such in specific applications ( Tzortzis and Likas, 2009 ). This issue was tackled by the introduction of Kernel K-means (KKM) ( Scholkopf et al., 1998 ). The essence is an extension of the standard K-means algorithm that maps data points from input space to a h igher dimensional feature space through a nonlinear transformation and minimizes the clustering error in the feature space.

In this study, the original case library captured by the scope meter is first grouped into different subsets based on its corre-sponding class of fault. Clustering is then applied to every single subset to produce several clusters, which are considered as representative cases. These representative cases are used for
CBR retrieval and reuse, i.e., classification, instead of the original cases. The original case library (CB) is only kept for reference. The general overview of clustering is shown in Fig. 6 . 2.3.1. K-means
The most common form of the clustering algorithm uses an iterative refinement heuristic known as K-means analysis ( Han and Kamber, 2007 ). It starts by partitioning the input data points into K initial sets called clusters, either at random or using some heuristic. The mean points (centroids) m t of the K clusters are calculated from t  X  1 to K. For each available data point, it is assigned to the cluster associated with its closest centroid m .

When new data points are added to the clusters, the centroids m must be updated. These two steps are repeated until convergence is achieved, i.e., centroids m t become stable and no longer changed. Practically, the algorithm is to minimize the total intra-cluster variance V , or, the squared error function: V  X  and m t is the centroid of the t th cluster. Fig. 7 demonstrates the idea where the solid points represent the centroids of the corresponding cluster. 2.3.2. Kernel K-means solution and convergence of K-means depend on the initial centroids of clusters. Secondly clusters must be linearly separable so that distinct partitions can be formed among different clusters.
KKM is a generalization of K-means, which overcomes the previous drawbacks, where data points are mapped from input space to a higher dimensional feature space through a nonlinear transformation ^ and then K-means is applied in the feature space. This results in linear separators in feature space that corresponds to nonlinear separators in input space. As a result,
KKM avoids the problem of K-means that the clusters must be linearly separable in the input space. The objective function E that
KKM tries to minimize is equivalent to the clustering error in the feature space shown in E  X  m 1 , ... , m M  X  X  where x i is the training data for i  X  1to N , and N is the number of training data. C k is a cluster for k  X  1 X  M. m k is the corresponding centroid of C k and m k can be calculated by m k  X  X  P N i  X  1 the squared Euclidian distances in (15) can be computed without explicit knowledge of the transformation ^ using :
F  X  x 2.4. Case-based reasoning
CBR is a problem solving paradigm that, in many aspects, is fundamentally different from other major artificial intelligence approaches. A new problem is solved by finding a similar past case from a case library, and reusing it in a new situation. CBR is also an approach to incremental learning, since a new experience is retained each time a problem has been solved, making it immediately available for future problems. The CBR field has grown rapidly over the last two decades, as seen by its increased share of papers and successful applications in many practical problems ( Kolodner, 1993 ; Perner, 2002 ; Perner, 2009 ; Kowalski et al., 2005 ; Vong and Wong, 2010 ).

A typical CBR system is composed of four sequential steps ( Fig. 8 ): 1. Retrieve: retrieve the most relevant case(s). 2. Reuse: reuse the case(s) to attempt to solve the new problem. 3. Revise: revise the proposed solution, if necessary. 4. Retain: retain the new solution as a part of a new case.
In the retrieval stage of CBR, a simple similarity function is usually employed to locate the nearest neighbor for the current problem from the case library. For this specific application, the similarity between two cases can be simply computed by the
Euclidean distance between two cases c and c 0 , defined as sim  X  c , c 0  X  X  1 where c ( i ) and c 0 ( i ) are the i th feature of the cases c and c 0 , respectively. w i is the user-defined weight or importance of the i th feature in a case, and range ( c ( i )) is the normalized maximum value in the range of the i th feature in a case. For example, if c ( i ) lies on the range of [ 1, 3], the normalized range becomes [0,4] and range ( c ( i ))  X  4. 3. Domain features for engine ignition patterns
When an engine starts firing, its secondary coil produces rapid high voltage causing a spark plug to produce spark. This high voltage is called firing voltage . The spark voltage represents the voltage required to maintain spark for the duration of the spark line. The duration is called burn time . After the burn time, the energy in the ignition coil nearly exhausts, and the residual energy forms slight oscillations in the ignition coil. This entire procedure is shown in Fig. 9 . Using the spark ignition pattern to diagnose the engine fault is a common diagnostic method for automotive engineers. With reference to the handbooks ( Liu and Chu, 2005 ; Crouse and Anglin, 1993 ), some domain knowledge for a pattern can be observed for engine fault diagnosis ( Fig. 9 ): Firing voltage ( F 1 ); Burn time ( F 2 ); and Average spark voltage ( F 3 );
Concerning implementation issues in this study, the amplitude of all sampling points, y i , is firstly normalized to the values between zero and one by dividing the maximum firing voltage 15 kV. In this study, all patterns start from the firing voltage ( F which is at the first sampling point: F  X  y 1 = 15000  X  18  X  where y 1 is the actual value of the first sampling point. Ideally, burn time ( F 2 ) starts from the spark voltage and ends at the position where the spark voltage falls to zero, but practically the voltage could slightly oscillate after burn time, so that exact zero value may not be reached. In this study, when the voltage falls to within 0.1% of the firing voltage, it is considered as zero and the burn time ends. Therefore, F 2 is measured based on the following procedure: a  X  1 ;
Repeat a  X  a  X  1 ;
Return F 2  X  a 1  X  X  = N ; == Normalization and N is no : of data points Terminate ;
Until a  X  N ;  X  19  X  where a also indicates the end point of burn time. With the index a and time step q , the average spark voltage of the spark line ( F ) can be calculated as follows:
F 3  X  1 t 4. System workflow preprocessing ( Fig. 10 ) and CBR diagnosis. The ignition patterns are passed to the preprocessing module for normalization, feature extraction under WPT to form a feature vector and then dimen-sion reduction under KPCA. The preprocessed patterns are subsequently stored in WPT_KPCA_DATA ( Fig. 10 ). Meanwhile, the three domain features for every pattern are also extracted to form 3F_DATA ( Fig. 10 ). The features in 3F_DATA combine with that in WPT_KPCA_DATA to constitute a set of cases in a case library called CB ( Fig. 11 ). After preprocessing, the case library CB gets ready for reasoning, which contains not only the domain features of the ignition patterns but also the preprocessed features processed by WPT and KPCA. In order to tackle the uncontrolled growth of case library, the case library CB is compressed as WORK_SET using KKM. For example, there are nine classes of ignition system faults, and every class contains only the top five representative cases, the total number of representative cases in WORK_SET is 9 5  X  45. The whole con-cept about producing CB and WORK_SET is also illustrated in
Fig. 10 . The case library WORK_SET is normally used for reasoning in CBR diagnosis module while CB is used for case accumulation and future reproduction (update) of representative cases.
When a new pattern (case) arrives, the similarity value between this new case and all the cases in the case library is calculated. Based on the similarity values, the most similar three classes of past cases are retrieved and presented for selection. Revision would be necessary if the retrieved cases are incorrect.
Finally, the user-selected solution may be retained to the case library CB. If necessary, CB can be compressed again to update
WORK_SET by clustering (e.g., KKM) under user X  X  initiation, or designated at regular time intervals. Although KKM is already an incremental algorithm, it is still recommended to re-compress from scratch the new CB to become a new WORK_SET because the procedure leads to higher accuracy in experimental results. In addition, the time taken for re-compression by KKM usually takes less than 30 s for hundreds to a thousand of cases. After updating the CB by KKM, the number of the cases in the WORK_SET still remains unchanged. The details of these two modules are presented below. 4.1. Preprocessing module 4.1.1. Normalization
The number of sampling points of every captured pattern is not exactly the same due to engine speed fluctuation and various testing conditions. All patterns are usually normalized within the same range. Normalization of the ignition patterns are done in terms of amplitude and duration. For the amplitude, since all patterns already lie on the same range of 0 X 15 kV, they are simply mapped within 0 and 1 by dividing 15 kV. In the current study, the number of sampling points for every pattern is less than 17,000. For the sake of conservation, a standard duration for all patterns is set to 18,000, in order to avoid information loss. To standardize the duration of all patterns, steady-state values are appended to the rear part of the patterns if necessary. Normally, the steady-state value for the ignition pattern is equal to zero (0 V). For those patterns having data points fewer than 18,000, zeros are appended. This concept is illustrated in Fig. 12 . 4.1.2. Feature extraction by WPT
Ignition patterns are time-varying and non-stationary, i.e., every engine (same or different models) can produce ignition patterns of different length and amplitude even under the same fault. To build a classification system for these time-varying patterns, it is inappropriate to compare the ignition patterns directly. The captured features of ignition patterns can be considered for comparison. WPT provides a tool to decompose and extract the high and low frequency subbands of ignition patterns. The extracted features of a pattern are the combination of wavelet coefficients of the different frequency subbands of the original pattern. With these extracted features, comparison of ignition patterns can be done appropriately. 4.1.3. Dimension reduction by KPCA
Although the patterns can become comparable after WPT extrac-tion, the dimension of these extracted features is still very high (about 2250). This high dimension can degrade the diagnosis performance because each of the WPT feature has to be compared for diagnosis. To tackle this issue, KPCA is applied to obtain a smaller set of principal components of the W PT features. Nevertheless, the dimension can be further reduced using the method mentioned in Section 2.2. With the eigenvalues l j obtained from KPCA, the corresponding transformed feat ures can be deleted backwards until S l j r 0.95. Therefore, only a limited number of the principal components are necessary by which 95% of the information in the features can be retained. Finally, the obtained principal components are considered as the partial case representation for the ignition patterns, which are saved in WPT_KPCA_DATA ( Fig. 10 ). 4.1.4. Extraction of domain features
According to Section 3, the domain features F 1 , F 2 and F the ignition patterns can be calculated and saved in 3F_DATA as [ F 1 , F 2 , F 3 ]( Fig. 10 ). These three domain features along with the features preprocessed by WPT and KPCA constitute the full contents of the cases in CB ( Fig. 11 ), so that the cases in CB contain domain knowledge and the computationally extracted features of the ignition patterns. 4.2. CBR diagnosis module The concept of the CBR diagnosis module is illustrated in Fig. 13 , which includes clustering and a sequence of reasoning steps as described in the following sections. 4.2.1. Clustering by KKM
The quality of a CBR diagnostic system depends on its case library in which only representative cases should be kept. Although these representative cases can be carefully prepared at the time the CBR diagnostic system is designed, many cases are to be retained continually to CB and it gradually degrades the performance of the diagnostic system. Hence a periodic main-tenance of the case library is necessary through clustering (e.g., by KKM) ( Fig. 13 , Step 4). In the current study, another case library
WORK_SET was constructed for normal use, maintenance and update so that all original cases were kept unchanged in CB. In the case library, there are many cases belonging to different classes of faults. The cases belonging to the same class of fault can be grouped together to become a subset, from which five clusters are obtained to form five representative cases for the corresponding class of fault. For example, there are nine different classes of faults, i.e., totally 45 (5 9) cases (five representative cases for each of the nine subsets) are maintained in WORK_SET. These 45 representative cases are used for CBR retrieval and reuse, i.e., classification, instead of the original cases. The case library CB is only kept for reference or future update (re-clustering). This technique can significantly improve the CBR diagnosis accuracy and efficiency. As a comparison, traditional K-means and KKM will be separately applied to CB to generate WORK_SET. 4.2.2. Retrieval stage
The similarity function defined in (17) is modified to include the comparison of domain features as shown in (21), where F j j th domain feature for a pattern x , and PP i ( x )isthe i th feature of preprocessed pattern x by normalization, WPT and KPCA: sim  X  x , y  X  X  1
The important w j for domain features and w i for WPT_KPCA features are individually user-adjustable, where n is the number of features in PP( x ), i.e., the number of selected principal compo-nents after KPCA. Practically, the user just needs to specify the total amounts of importance S w j and S w i , respectively. Under assumption, S w j  X  S w i  X  0.5. Since all domain features are equally important among themselves, w j  X  0.5/3  X  0.1667 for j  X  1 X 3. Simi-larly, w i  X  0.5/ n for i  X  1to n . The range of domain features and all
WPT features in a case are normalized to [0 y 1], i.e., ran-ge( F j )  X  range ( x i )  X  1. In addition, S w j  X  S w i (21) can be converted to sim  X  x , y  X  X  1 Based on the defined similarity function, similar cases from
WORK_SET along with the corresponding similarity values against the new problem can be returned ( Fig. 13 , Step 1). In the study, three most similar (possible) classes of faults are returned for user selection, instead of three most possible cases. This can really provide decision support to the user. From this viewpoint, CBR paradigm is more suitable for constructing an expert system for diagnosis. 4.2.3. Reuse and revise stage sented to the user for the check-up of the corresponding ignition components ( Fig. 13 , Step 2). If no returned case can correctly diagnose the new problem, the user can provide a correct solution after his/her check-up of the ignition system. This can produce a useful problem X  X olution pair in the future. 4.2.4. Retain stage its solution should be checked for retention according to the similarity between this new problem and the cases in CB ( Fig. 13 ,
Step 3). This procedure aims to minimize the risk of performance degeneration. In the present study, this checking procedure can be done using (22). For a new problem having similarity over 90% against the cases in CB, it is not recommended to retain the new problem. Nevertheless, this situation will be presented to the user and it is up to the user X  X  final decision. This simple checking procedure does not totally prevent case redundancy in the case library. Update (re-compression) is therefore necessary under user X  X  initiation after a period of running the diagnostic system.
After clustering from CB to WORK_SET, the redundancy of cases can be eliminated to maintain the efficiency of CBR diagnosis module. 4.2.5. Update of case library library CB, clustering can be carried out to update WORK_SET, i.e., new (and the same number of) representative cases are produced from CB to provide an update and more accurate diagnosis ( Fig. 13 , Step 4). This step also overcomes the issues of incre-mental and decremental update of MLP and SVM models as discussed in Section 5.4.7. 5. Experiment and results proposed CBR methodology. Besides, typical classification approaches, SVM and MLP, were also used to respectively construct a classification system for making the comparison with
CBR approach. Details of the experiments and their results are discussed in the following sections. 5.1. Data sampling different models of engines were used to imitate the nine malfunctioning symptoms characterized by the ignition patterns as shown in Fig. 1 . The nine symptoms were selected as demon-strative examples; potentially more symptoms can be captured and trained. In this study, three well-known inline 4-cylinder 4-stroke engines, HONDA B18C, HONDA D15B and MITSUBISHI 4G15, were employed. Different models of engines were used for training that can enhance the generalization of the classification system. The raw ignition spark patterns were obtained using a computer-linked automotive scope meter with a pickup clamp ( Fig. 14 ). Ignition patterns were captured at a sampling frequency of 100 kHz (higher rate), i.e., 100,000 sampling points per second, or equivalently, 100 sampling points per millisecond (ms). Igni-tion patterns were recorded in a PC and converted into an excel format ( .csv format) for analysis.

For every symptom in each test engine, four ignition patterns (one pattern for each cylinder) were captured over two different engine speeds according to the standard testing conditions in ( Liu and Chu, 2005 ) (at idle speed and at 2000 rpm). Totally 216 ignition patterns were captured into a data set D t . D t divided into training data set TRAIN (3/4 of D t ) and test data set
TEST (1/4 of D t ) for training and verification, i.e., D
In the case study, the total number of ignition patterns in TRAIN has 162 cases (i.e. 9 symptoms 4 cylinders 2 testing con-ditions 3 engines 3/4), while TEST contains 54 cases (i.e. 9 symptoms 4 cylinders 2 testing conditions 3 engines 1/4). Preprocessing procedures presented in Section 5.3 were used on these two data before being passed to the diagnostic system. 5.2. Experimental environment and implementation All experiments were performed using a PC with a Core Duo
T1700 at 1.80 GHz and 1 GB RAM. The experimental environment for preprocessing module was Matlab R2008a in MS Windows XP platform. In addition, Matlab Wavelet Toolbox (for WPT), Neural
Networks Toolbox ( Demuth and Beale, 2001 ) (for MLP), LIBSVM ( Chang and Lin, 2001 ) (for SVM), LS-SVMLab ( Pelckmans and
Suykens, 2003 ) (for KPCA), and Kernel K-Means Toolbox ( Chen 2009 ) (for KKM) were used. Other simple procedures were implemented by the authors. The CBR diagnostic systems were implemented using Java under NetBean IDE. 5.3. Data preprocessing
The general procedure includes the following sequential steps: data normalization, WPT feature extraction, KPCA dimension reduction and combination of domain features. The training data set TRAIN was transformed to CB for MLP and SVM training through these steps. For CBR diagnostic system, one extra step using KKM clustering is required to transform CB into WORK_SET for CBR training. Similarly, the test set TEST was transformed to PRO_TEST. The details of preprocessing steps are shown in the following sections. 5.3.1. Data normalization and application of WPT for feature extraction
The patterns in TRAIN and TEST were normalized according to the methods described in Section 4.1.1. The corresponding domain features of the patterns were extracted and saved in 3F_DATA ( Fig. 10 ). In order to reduce the training and test time, all patterns in TRAIN and TEST were compressed into 1/8 times (2250 points) by WPT. This step can also eliminate white noise. WPT features were then extracted. Mother wavelet and the level of decomposition L are usually selected based on trial-and-error. Among the family of the wavelet transform, the Daubechies wavelet (db-N ) is the most popular one ( Daubechies, 1990 ), where N  X  1 X 10. Since the only way to estimate the best mother wavelet for an application is through trial-and-error, the authors attempted the following combinations of N  X  1 X 10 and L  X  4 X 6 for db-N in order to estimate the best setting for the current diagnostic system. The required time depends on the complexity of the application domain. In the current study involving several hundred cases, several seconds are necessary for an experiment of decomposition in one kind of mother wavelets. Experimental results in Table 1 show that the decomposition using db-6 with level L  X  5 worked the best for CBR diagnostic system. 5.3.2. Application of KPCA for dimension reduction
In practice, it is unnecessary to use all the WPT features but the most important ones should be selected. Hence, KPCA was used to reduce the number of WPT features. There are no systematic procedures to figure out the best kernel and its hyperparameter except through trial-and-error. Different kernel functions were therefore tested and the experimental result revealed that Polynomial kernel with d  X  4 performed the best for this application. Every experiment under a specific setting (kernel and hyperparameter) took less than 1 s. Based on the experimental setup, the first 42 principal components retained 95% of the information in the WPT features. After WPT and KPCA processes, the data already became classifiable using the first two extracted principal components as shown in Fig. 15 . 5.3.3. Combination of domain features After feature extraction by WPT and dimension reduction by KPCA, the preprocessed patterns were saved in WPT_KPCA_DATA.
Together with 3F_DATA and WPT_KPCA_DATA, the case library CB was constructed ( Fig. 10 ). The total number of features for a case was then equal to 3  X  42  X  45. Therefore, the training data set
TRAIN was transformed into CB for training a general diagnostic system using MLP or SVM. For CBR diagnostic system, the following extra  X  X  X lustering X  X  step is necessary. 5.3.4. Application of KKM for clustering result showed that the RBF kernel fitted better the current application. The number of clusters for KKM was adjusted to five because only the top five representative cases in a subset were necessary. For nine classes of faults, 45 clustered cases in total (i.e., 9 classes 5 representatives) in WORK_SET were processed under clustering from CB ( Fig. 10 ). One remark is that both of the case libraries CB and WORK_SET contained the same number of features (i.e., same dimension), but the number of cases in
WORK_SET was restricted, i.e., 45 representative cases were used in this case study. 5.4. Diagnosis and comparison by different methods
WORK_SET, and the preprocessed test set PRO_TEST, different methods such as CBR MLP, and SVM were applied to their corresponding preprocessed data set and test set. In order to illustrate the usefulness of the preprocessing steps in this study, the experiments of MLP and SVM with the raw training set TRAIN were also included. The results in Tables 2 and 3 include three different performance aspects, where training time is the con-struction time for classifier using the training cases in TRAIN, CB or WORK_SET, total running time is the classification time for all of the test cases in TEST or PRO_TEST; average running time is total running time divided by the number of test cases. 5.4.1. Organization of CBR diagnostic Systems
According to Section 5.3.2, the number of principal compo-nents of WPT features was 42. Therefore, n was assigned to 42 in the similarity function (22). The classification accuracy of CBR diagnostic system can be measured over data set PRO_TEST. In order to evaluate the effects of no clustering , K -means , and KKM respectively, six CBR classifiers were constructed for comparison, namely CBR with single solution returned over CB (CBR-1)
CBR with best three solutions with different classes returned over CB (CBR-3) CBR with single solution returned using K -means over WORK_ SET (CBR-K -1)
CBR with best three solutions with different classes returned using K -means over WORK_SET (CBR-K-3)
CBR with single solution returned using KKM over WORK_SET (CBR-KKM-1) and CBM with best three solutions with different classes returned using KKM over WORK_SET (CBR-KKM-3)
A remark about the outputs of CBR-3, CBR-K-3 or CBR-KKM-3 is that, under current similarity measure (17), the cases (not the classes) with the highest similarity values were returned. It is not informative for a decision maker if all the three returned cases are of the same class. Therefore, all cases in CB or WORK_SET were first matched against the new input case x new and then ranked under similarity values. The ranking resembled a table similar to Table 4 . When x new was similar to Case-49 (Class-3), it was also similar to other cases in Class-3. The most similar cases in each of the top three ranked classes were retrieved. For instance, Case-49 (Class-3), Case-12 (Class-1) and Case-87 (Class-5) were directly presented to the user. 5.4.2. K-means vs. kernel K-means
The accuracies and time performance of K-means and KKM are shown in Tables 1 and 2 , respectively. The accuracy of CBR-KKM-3 classifier was somewhat 1.5% higher than the CBR-K-3 classifier ( Table 1 , db-6, L5, 94% vs. 92.5%) but the training time of the CBR-KKM-3 classifier was around 57% longer than the CBR-K-3 classifier ( Table 2 , 13.84 s vs. 8.83 s). The accuracy of CBR-KKM-1 was also somewhat 4% higher than the CBR-K-1 classifier ( Table 1 , db-6, L5, 90% vs. 86%). The training time of CBR-KKM-1 classifier was also around 52% longer than the CBR-K-1 classifier ( Table 2 , 12.53 s vs. 8.25 s). In summary, the CBR classifiers with WORK_ SET by KKM perform better than that by K-means. 5.4.3. Comparison among CBR classifiers
The proposed multiple-solution returned CBR classifier performed better than the single solution returned CBR classifier. Fig. 16 shows a snapshot of the recommendation of three possible faults from the multiple solutions returned by the CBR classifier. Although the average accuracies of CBR-K-3 classifier and CBR-KKM-3 classifier were, respectively, 2.5% and 1% lower than the CBR-3 classifier ( Table 1 , db-6, L5), the average running time of CBR-K-3 and CBR-KKM-3 was about 23 times and 25 times faster than the CBR-3 classifier ( Table 2 , 0.048 s vs. 1.18 s and 0.046 s vs. 1.18 s), respectively. This reduction of time plays an important role when more and more cases are included.
Moreover, it is believed that the quality of the representative cases (clusters) by KKM could be increased with sufficient number of cases provided in the future, so that the average accuracy of CBR-KKM classifiers could be further improved. 5.4.4. MLP training
MLP with single hidden layer by back propagation (BP) algo-rithm ( Wong et al., 2010 ) was used to train classifiers with the training data TRAIN and CB, respectively. The MLP classifiers were subsequently tested over the test data PRO_TEST. The architecture of the MLP classifier using TRAIN in this study contained 2250 input neurons, 10 hidden neurons and 4 output neurons (i.e. 4 bits) to represent nine different classes. Similarly, the MLP classifier using CB contained (3  X  42  X  ) 45 input neurons, 10 hidden neurons and 4 output neurons. Tan-sigmoid function was used in both classifiers in the hidden layer, followed by a purely linear function in the output layer. 5.4.5. SVM training
SVM ( Cristianni and Shawe-Taylor, 2000 ), a well-known machine learning technique, was applied to deal with a wide range of engineering problems ( Vong et al., 2006 ; Tao et al., 2009 ;
Yang et al., 2005 ) due to its high generalization. According to ( Abe, 2005 ), KPCA followed by SVM with linear kernel works the best for some practical applications. Linear kernel was therefore selected as the kernel function. Moreover, two hyper parameters ( g , C ) for SVM classifier were determined using 10-fold cross validation. In this case study, the value of g was taken from the range of  X 10 to 10 (except 0) with an increment of 2, and the value of C was taken from the range from 1 to 1000 with an increase in magnitude of 10 (i.e., 1, 10, 100, 1000). The result of the choice of ranges resulted in a total of 40 (10 4) combinations for these two hyper parameters. By testing all combinations, the best combination of ( g , C ) (the one with the highest accuracy) was chosen as (2, 100). 5.4.6. Comparison of CBR with MLP and SVM
The results of MLP and SVM are shown in Table 3 . For the MLP classifier, the most attractive choice occurred when the average running time over CB (0.002 s) was only considered. However, its training time over CB of 36 s was much longer than CBR classifiers. For the SVM classifier, the situation was more or less the same as MLP. The excessive time consumption of SVM was not caused by the training time, but for relative long searching for hyper parameters g and C . The searching time over CB of 23 s was also longer than that of CBR classifiers. In addition, standard MLP and SVM can just return a most probable case, which may mismatch the true fault. Hence the average accuracy of MLP and SVM over CB were just 84% and 89%, respectively; their performance was lower than those of the CBR classifiers. 5.4.7. Issue of incremental and decremental data update training data may be identified after the classification system has been built, leading to the issue of incremental and decre-mental data update. The issue that degraded system general-ization and accuracy was always found in MLP and SVM. In case when an update of the classification model is necessary, it is suggested that MLP and SVM should retrain from scratch. On the contrary, a major advantage of CBR is its ease of classifier maintenance, i.e., the representative cases in the case library WORK_SET. When new cases arrive, they can be simply saved in
CB and then incrementally updated to WORK_SET by applying simple clustering technique. In the case where conflict cases exist, one of the conflicting cases can be manually removed. Moreover, only the subset containing this deleted case needs a fast re-clustering a second time. This is another advantage of CBR paradigm over other machine learning methods, and hence it can be considered for one of the future research directions. 6. Conclusions successfully applied to construct a reliable computer-aided diag-nostic system for the automotive engine ignition system. From the experimental results, the CBR classifier produced higher accuracy than the traditional MLP and SVM classifiers. The most important appeal of CBR over MLP and SVM is that multiple solutions can be recommended to the user for final decision. This is a more practical and reliable procedure for the automotive mechanics. In addition, several issues about ignition pattern diagnosis had been tackled. The first one is the normalization of ignition patterns. The second one is the feature extraction of ignition patterns for comparison. The third one is the time issue of traditional CBR retrieval procedure, which is improved by KPCA and KKM.
 enhanced with clustering technique were very satisfactory and promising. Under the same training set and test set, the accuracy of the proposed multiple-solution CBR diagnostic system (CBR-KKM-3) was about 11% and 6% better than the standard
MLP and SVM, respectively ( Table 3 ). Although the average running time for CBR was longer than MLP and SVM, the training time for CBR was also much shorter than the standard MLP and
SVM. In addition, the issue of knowledge update for CBR is another advantage by simply adding or removing cases into/from the case library, while knowledge update for standard MLP and
SVM requires much more sophisticated incremental and decre-mental algorithms, which may even affect the classification accuracy after the update of knowledge. Concerning this issue, further research is aimed to study the effect of incremental and decremental update under CBR and SVM, respectively. In sum-mary, Table 5 shows the characteristics of the classification systems built by CBR-3, CBR-KKM-3, MLP and SVM.

Finally, this work was the first attempt at combining domain knowledge and extracted pattern features for engine ignition system diagnosis. In addition, the integration of WPT, KPCA,
KMM and CBR for building an intelligent diagnostic system is a first trial. The comprehensive comparison of different combina-tions of the approaches mentioned for engine ignition system diagnosis is also an original work. From the perspective of auto-motive engineering, the diagnostic system can save the automotive mechanics from a lot of time and effort to interpret the ignition patterns for diagnosis. For future development, more symptoms from different models of engines can be captured to provide a wider range of training cases, so that the classifier can become more general. Moreover, the classifier may be embedded in the electronic control unit of the automobile or computer-aided diagnostic tool to perform real-time ignition pattern analysis. Acknowledgments The research is supported by the University of Macau Research Grant, Grant no. RG064/09-10S/VCM/FST.
 References
