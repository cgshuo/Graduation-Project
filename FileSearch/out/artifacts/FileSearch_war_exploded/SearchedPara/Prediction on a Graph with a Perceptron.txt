 We study the problem of robust online learning over a graph. Consider the following game for is adversarial, the learner will always mispredict; but if nature is regular or simple, there is hope that a learner may make only a few mispredictions. Thus, a methodological goal is to give learners whose total mispredictions can be bounded relative to the  X  X omplexity X  of nature X  X  labeling. In this paper, we consider the cut size as a measure of the complexity of a graph X  X  labeling, where the size of the cut is the number of edges between disagreeing labels. We will give bounds which depend on the cut size and the diameter of the graph and thus do not directly depend on the size of the graph. The problem of learning a labeling of a graph is a natural problem in the online learning setting, as well as a foundational technique for a variety of semi-supervised learning methods [2, 3, 4, 5, 6]. For example, in the online setting, consider a system which serves advertisements on web pages. The web pages may be identified with the vertices of a graph and the edges as links between pages. The online prediction problem is then that, at a given time t the system may receive a request to serve an advertisement on a particular web page. For simplicity, we assume that there are two alternatives to be served: either advertisement  X  X  X  or advertisement  X  X  X . The system then interprets the feedback as the label and then may use this information in responding to the next request to predict an advertisement for a requested web page. 1.1 Related work There is a well-developed literature regarding learning on the graph. The early work of Blum and Chawla [2] presented an algorithm which explicitly finds min-cuts of the label set. Bounds have been Figure 1: Perceptron on set V M . proven previously with smooth loss functions [6, 7] in a batch setting. Kernels on graph labelings were introduced in [3, 5]. This current work builds upon our work in [8]. There it was shown that, given a fixed labeling of a graph, the number of mistakes made by an algorithm similar to the kernel perceptron [9] with a kernel that was the pseudoinverse of the graph Laplacian, could be bounded by the quantity [8, Theorems 3.2, 4.1, and 4.2] Here u  X  { X  1 , 1 } n is a binary vector defining the labeling of the graph,  X  G ( u ) is the cut size 1 and negative labels, D G is the diameter of the graph and bal ( u ) := 1  X  1 n | P u i |  X  2 measures the label balance . This bound is interesting in that the mistakes of the algorithm could be bounded in terms of simple properties of a labeled graph. However, there are a variety of shortcomings in this result. First, we observe that the bound above assumed a fixed labeling of the graph. In practice, the online data sequence could contain multiple labels for a single vertex; this is the problem of label noise . Second, for an unbalanced set of labels the bound is vacuous, for example, if u = algorithm of two dense clusters connected by a few edges, for instance, two m -cliques connected by a single edge (a barbell graph, see Figure 2). If each clique is labeled with distinct labels then we the first clique contains one vertex which is labeled as the second clique (see Figure 3). Previously  X 
G ( u ) = 1 , but now  X  G ( u ) = m and the bound is vacuous. This is the problem of concept noise ; in this example, a  X (1) perturbation of labeling increases the bound multiplicatively by  X ( m ) . 1.2 Overview A first aim of this paper is to improve upon the bounds in [8], particularly, to address the three problems of label balance, label noise, and concept noise as discussed above. For this purpose, we apply the well-known kernel perceptron [9] to the problem of online learning on the graph. We discuss the background material for this problem in section 2, where we also show that the bounds of [1] can be specialized to relative mistake bounds when applied to, for example, prediction on the graph. A second important aim of this paper is to interpret the mistake bounds by an explanation in terms of high level graph properties. Hence, in section 3, we refine a diameter based bound of [8, Theorem 4.2] to a sharper bound based on the  X  X esistance distance X  [10] on a weighted graph; which we then closely match with a lower bound. In section 4, we introduce a kernel which is a simple augmentation of the pseudoinverse of the graph Laplacian and then prove a theorem on the performance of the perceptron with this kernel which solves the three problems above. We conclude in section 5, with a discussion comparing the mistake bounds for prediction on the graph with the halving algorithm [11] and the k -nearest neighbors algorithm. In this section, we describe our setup for Hilbert spaces on finite sets and its specification to the graph case. We then recall a result of Gentile [1] on prediction with the perceptron and discuss a special case in which relative 0 X 1 loss (mistake) bounds are obtainable. 2.1 Hilbert spaces of functions defined on a finite set We denote matrices by capital bold letters and vectors by small bold case letters. So M denotes the by I . We also let 0 and 1 be the n -dimensional vectors all of whose components equal to zero and one respectively, and e i the i -th coordinate vector of IR n . Let I N be the set of natural numbers and I N ` := { 1 , . . . , ` } . We denote a generic Hilbert space with H . We identify V := I N n as the indices of a set of n objects, e.g. the vertices of a graph. A vector w  X  IR n can alternatively be seen as a function f : V  X  IR such that f ( i ) = w i , i  X  V . However, for simplicity we will use the notation w to denote both a vector in IR n or the above function.
 A symmetric positive semidefinite matrix M induces a semi-inner product on IR n which is defined with the above semi-inner product is K = M + , where  X  +  X  denotes pseudoinverse. We also define the coordinate spanning set product on H ( M ) . The set V M acts as  X  X oordinates X  for H ( M ) , that is, if w  X  X  ( M ) we have if M is positive definite. We note that equation (3) is simply the reproducing kernel property [12] for kernel M + .
 When V indexes the vertices of an undirected graph G , a natural norm to use is that induced by the graph Laplacian. We explain this in detail now. Let A be the n  X  n symmetric weight matrix The distance matrix  X  associated with G is the per-element inverse of the weight matrix, that is,  X  ij = 1 A ij (  X  may have +  X  as a matrix element). The graph Laplacian G is the n  X  n matrix defined as G := D  X  A where D = diag ( d 1 , . . . , d n ) and d i is the weighted degree of vertex i , d = P n j =1 A ij . The Laplacian is positive semidefinite and induces the semi-norm When the graph is connected, it follows from equation (4) that the null space of G is spanned by the constant vector 1 only. In this paper, we always assume that the graph G is connected. Where it is not ambiguous, we use the notation G to denote both the graph G and the graph Laplacian. 2.2 Online prediction of functions on a finite set with the perceptron Gentile [1] bounded the performance of the perceptron algorithm on nonseparable data with the linear hinge loss. Here, we apply his result to study the problem of prediction on a finite set with the perceptron (see Figure 1). In this case, the inputs are the coordinates in the set V M  X  X  ( M ) defined above. We additionally assume that matrix M is positive definite (not just positive semidefinite as in the previous subsection). This assumption, along with the fact that the inputs are coordinates, enables us to upper bound the hinge loss and hence we may give a relative mistake bound in terms of the complete set of base classifiers { X  1 , 1 } n .
 a sequence of examples, M A denotes the set of trials in which the perceptron algorithm predicted incorrectly and X = max t  X  X  A k v i t k M , then the cumulative number of mistakes |M A | of the algorithm is bounded by for all u  X  { X  1 , 1 } n , where M u = { t  X  I N ` : u i t 6 = y t } . In particular, if |M u | = 0 then Proof. This bound follows directly from [1, Theorem 8] with p = 2 ,  X  = 1 , and w 1 = 0 . Since M is assumed to be symmetric positive definite, it follows that { X  1 , 1 } n  X  H ( M ) . Thus, the hinge of [1, Theorem 8] directly with mistakes.
 We emphasize that our hypothesis on M does not imply linear separability since multiple instances of an input vector in the training sequence may have distinct target labels. Moreover, we note that, for deterministic prediction the constant 2 in the first term of the right hand side of equation (5) is optimal for an online algorithm as a mistake may be forced on every trial. The bound for prediction on a finite set in equation (5) involves two quantities, namely the squared norm of a classifier u  X  { X  1 , 1 } n and the maximum of the squared norms of the coordinates v  X  V M . In the case of prediction on the graph, recall from equation (4) that k u k 2 G := u &gt; Gu = P of the labeling induced when u  X  { X  1 , 1 } n . In particular, with boolean weighted edges ( A ij  X  { 0 , 1 } ) the cut simply counts the number of edges spanning disagreeing labels.
 The norm k v  X  w k G is a metric distance for v , w  X  span ( V G ) however, surprisingly, the square of the norm k v p  X  v q k 2 G when restricted to graph coordinates v p , v q  X  V G is also a metric known as the resistance distance [10], It is interesting to note that the resistance distance between vertex p and vertex q is the effective resistance between vertices p and q , where the graph is the circuit and edge ( i, j ) is a resistor with the resistance  X  ij = A  X  1 ij .
 effective resistance between vertex p and another vertex  X  0  X . The vector 0 , informally however, is further characterize k v p k 2 G . First, we observe qualitatively that the more interconnected the graph the smaller the term k v p k 2 G (Corollary 3.1). Second, in Theorem 3.2 we quantitatively upper bound k v graph (including q = p ), which in turn may be upper bounded by the eccentricity of p . We proceed with the following useful lemma and theorem, as a basis for our later results.
 Lemma 3.1. Let x  X  X  then k x k  X  2 = min w  X  X  k w k 2 :  X  w , x  X  = 1 .
 The proof is straightforward and we do not elaborate on the details.
 Theorem 3.1. If M and M 0 are symmetric positive semidefinite matrices with span( V M ) = span( V M 0 ) and, for every w  X  span( V M ) , k w k 2 M  X k w k 2 M 0 then where v i  X  X  M , v 0 i  X  X  M 0 and a  X  IR n .
 where the first inequality follows since  X  x 0 k x 0 k 2 the minimization problem in the right hand side of Lemma 3.1. While the second one follows immediately from the assumption that k w k 2 M  X k w k 2 M 0 .
 As a corollary to the above theorem we have the following when M is a graph Laplacian. Corollary 3.1. Given connected graphs G and G 0 with distance matrices  X  and  X  0 such that  X  ij  X   X  0 ij then for all p, q  X  V , we have that k v p k is strictly more connected. The second inequality is the well-known Rayleigh X  X  monotonicity law which states that if any resistance in a circuit is decreased then the effective resistance between any two points cannot increase.
 We define the geodesic distance between vertices p, q  X  V to be d G ( p, q ) := min | P ( p, q ) | where the minimum is taken with respect to all paths P ( p, q ) from p to q , with the path length defined distance on the graph between p and the furthest vertex on the graph to p , that is, d G ( p ) = A graph G is connected when D G &lt;  X  . A tree is an n -vertex connected graph with n  X  1 edges. The following lemma, a well known result (see e.g. [10]), establishes that the resistance distance can be be equated with the geodesic distance when the graph is a tree.
 Lemma 3.2. If the graph T is a tree with graph Laplacian T then r T ( p, q ) = d T ( p, q ) . The next theorem provides a quantitative relationship between k v p k 2 G and two measures of the connectivity of vertex p , namely its eccentricity and the mean of the effective resistances between vertex p and each vertex on the graph.
 Theorem 3.2. If G is a connected graph then by Corollary 3.1, if T is the Laplacian of a tree T  X  G then r G ( p, q )  X  r T ( p, q ) for p, q  X  V . Therefore, from Lemma 3.2 we conclude that r G ( p, q )  X  d T ( p, q ) . Moreover, since T  X  G can be any tree, we have that r G ( p, q )  X  min T d T ( p, q ) where the minimum is over all trees T  X  G . Since the geodesic path from p to q is necessarily contained in some tree T  X  G it follows that d G ( p, q ) over q and the definition of d G ( p ) .
 We identify the resistance diameter of a graph G as R G := max p,q  X  V r G ( p, q ) ; thus, from the previous theorem, we may also conclude that We complete this section by showing that there exists a family of graphs for which the above in-equality is nearly tight. Specifically, we consider the  X  X lower graph X  (see Figure 4) obtained by connecting the first vertex of a chain with p  X  1 vertices to the root vertex of an m -ary tree of depth one. We index the vertices of this graph so that vertices 1 to p correspond to  X  X tem vertices X  and vertices p + 1 to p + m to  X  X etals X . Clearly, this graph has diameter equal to p , hence our upper bound above establishes that k v 1 k 2 G  X  p . We now argue that as m grows this bound is almost tight. From Lemma 3.1 we have that k v 1 k  X  2 G = min w  X  X  ( G ) k w k 2 G :  X  w , v 1  X  = 1 . We note that by symmetry, the solution  X w = (  X  w i : i  X  I N p + m ) to the problem above satisfies  X  w i = z if i  X  p + 1 since  X w must take the same value on the petal vertices. Consequently, it follows that k v bound this minimum by choosing w i = p  X  i p  X  1 for 1  X  i  X  p . Thus, w 1 = 1 as it is required, w p = 0 and we compute z by the constraint set of the above minimization problem as z =  X  p 2 m . A direct for the flower graph is matched by a lower bound with a gap of 1. We define the following symmetric positive definite graph kernel, we prove the needed properties of H ( G b c ) as a necessary step for the bound in Theorem 4.2. As we shall see, these properties moderate the consequences of label imbalance and concept noise. To prove Lemma 4.1, we use the following theorem which is a special case of [12, Thm I,  X  I.6]. Theorem 4.1. If M 1 and M 2 are n  X  n symmetric positive semidefinite matrices, and we set M := ( M + 1 + M + 2 ) + then k w k 2 M = inf {k w 1 k 2 M every w  X  X  ( M ) .
 Next, we define  X  u  X  [0 , 1] as a measure of the balance of a labeling u  X  { X  1 , 1 } n as  X  u := ( unbalanced one.
 Lemma 4.1. Given a vertex p with associated coordinates v p  X  X  G and v 0 p  X  X  G b Moreover, if u , u 0  X  X  X  1 , 1 } n and where k := |{ i : u 0 i 6 = u i }| we have that Proof. To prove equation (11) we recall equation (3) and note that k v 0 p k 2  X  v To prove inequality (12) we proceed in two steps. First, we show that Indeed, we can uniquely decompose u as the sum of a vector in H ( G ) and one in H ( 11 &gt; n 2 b ) as k u  X  Second, we show, for any symmetric positive definite matrix M , u , u 0  X  X  X  1 , 1 } n and c &gt; 0 , that two elements of H ( M ) and H ( 1 c I ) as u 0 = u + ( u 0  X  u ) and observe that k u 0  X  u k 2 1 Theorem 4.1 it then follows that k u 0 k 2 M follows by combining equations (13) and (14) with M = G b 0 .
 We can now state our relative mistake bound for online prediction on the graph.
 Theorem 4.2. Let G be a connected graph. If { ( v i t , y t ) } ` t =1  X  V G b examples and M A denotes the set of trials in which the perceptron algorithm predicted incorrectly, then the cumulative number of mistakes |M A | of the algorithm is bounded by and In particular, if b = 1 , c = 0 , k = 0 and |M u | = 0 then Proof. The proof follows by Theorem 2.1 with M = G b c , then bounding k u k 2 G b Lemma 4.1, and then using max t  X  X  A k v i t k 2 G  X  R G by equation (9).
 The upper bound of the theorem is more resilient to label imbalance, concept noise, and label noise than the bound in [8, Theorems 3.2, 4.1, and 4.2] (see equation (1)). For example, given the noisy barbell graph in Figure 3 but with k n noisy vertices the bound (1) is O ( kn ) while the bound (15) with b = 1 , c = 1 , and |M u | = 0 is O ( k ) . A similar argument may be given for label imbalance. In the bound above, for easy interpretability, one may upper bound the resistance diameter R G by the geodesic diameter D G . However, the resistance diameter makes for a sharper bound in a number of natural situations. For example now consider (a thick barbell) two m -cliques (one labeled  X +1 X , one  X -1 X ) with ` edges ( ` &lt; m ) between the cliques. We observe between any two vertices there are at least ` edge-disjoint paths of length no more than five, therefore the resistance diameter is at most 5 /` by the  X  X esistors-in-parallel X  rule while the geodesic diameter is 3 . Thus, for  X  X hick barbells X  if we use the geodesic diameter we have a mistake bound of 16 ` (substituting  X  u = 0 , and R G  X  3 into (16)) while surprisingly with the resistance diameter the bound (substituting b = 1 4 n , c = 0 , |M u | = 0 ,  X  u = 0 , and R G  X  5 /` into (15)) is independent of ` and is 20 . In this paper, we have provided a bound on the performance of the perceptron on the graph in terms of structural properties of the graph and its labeling which are only indirectly dependent on the number of vertices in the graph, in particular, they depend on the cut size and the diameter. In the following, we compare the perceptron with two other approaches. First, we compare the percep-tron with the graph kernel K 1 0 to the conceptually simpler k -nearest neighbors algorithm with either the graph geodesic distance or the resistance distance. In particular, we prove the impossibility of bounding performance of k -nearest neighbors only in terms of the diameter and the cut size. Specif-ically, we give a parameterized family of graphs for which the number of mistakes of the perceptron is upper bounded by a fixed constant independent of the graph size while k -nearest neighbors prov-ably incurs mistakes linearly in the graph size. Second, we compare the perceptron with the graph kernel K 1 0 with a simple application of the classical halving algorithm [11]. Here, we conclude that the upper bound for the perceptron is better for graphs with a small diameter while the halving al-gorithm X  X  upper bound is better for graphs with a large diameter. In the following, for simplicity we limit our discussion to binary-weighted graphs, noise-free data (see equation (16)) and upper bound the resistance diameter R G with the geodesic diameter D G (see equation (9)). 5.1 K -nearest neighbors on the graph We consider the k -nearest neighbors algorithms on the graph with both the resistance distance (see equation (7)) and the graph geodesic distance. The geodesic distance between two vertices is the length of the shortest path between the two vertices (recall the discussion in section 3). In the following, we use the emphasis distance to refer simultaneously to both distances. Now, consider the family of O `,m,p of octopus graphs. An octopus graph (see Figure 5) consists of a  X  X ead X  which the t i, 0 are all identified as one vertex r which acts as the root of the m tentacles. There is an edge (the body) connecting root r to the vertex c 1 on the head. Thus, this graph has diameter D = max( p + 2 , 2 p ) and there are ` + mp + 1 vertices in total; an octopus O m,p is balanced if ` = mp + 1 . Note that the distance of every vertex in the head to every other vertex in the graph is no more than p + 2 , and every tentacle  X  X ip X  t i,p is distance 2 p to other tips t j,p : j 6 = i . We now argue that k -nearest neighbors may incur mistakes linear in the number of tentacles. To this end, choose p  X  3 and suppose we have the following online data sequence Note that k -nearest neighbors will make a mistake on every instance ( t i,p ,  X  1) and so, even assuming the performance of the perceptron with the graph kernel K 1 0 (see equation (10)). By equation (16), the number of mistakes will be upper bounded by 10 p + 5 because there is a cut of size 1 and the diameter is 2 p . Thus, for balanced octopi O m,p with p  X  3 , as m grows the number of mistakes of the kernel perceptron will be bounded by a fixed constant. Whereas distance k -nearest neighbors will incur mistakes linearly in m . 5.2 Halving algorithm We now compare the performance of our algorithm to the classical halving algorithm [11]. The halving algorithm operates by predicting on each trial as the majority of the classifiers in the concept class which have been consistent over the trial sequence. Hence, the number of mistakes of the halving algorithm is upper bounded by the logarithm of the cardinality of the concept class. Let can be uniquely identified by a choice of k edges and 1 bit which determines the sign of the vertices in the same of partition (however we overcount as not every set of edges determines a classifier). The number of mistakes of the halving algorithm is upper bounded by O ( k log n k ) . For example, on a line graph with a cut size of 1 the halving algorithm has an upper bound of d log n e while the upper bound for the number of mistakes of the perceptron as given in equation (16) is 5 n + 5 . Although the halving algorithm has a sharper bound on such large diameter graphs as the line graph, it unfortunately has a logarithmic dependence on n . This contrasts to the bound of the perceptron which is essentially independent of n . Thus, the bound for the halving algorithm is roughly sharper on graphs with a diameter  X  (log n k ) , while the perceptron bound is roughly sharper on graphs with a diameter o (log n k ) . We emphasize that this analysis of upper bounds is quite rough and sharper bounds for both algorithms could be obtained for example, by including a term representing the minimal possible cut, that is, the minimum number of edges necessary to disconnect a graph. For the halving algorithm this would enable a better bound on the cardinality of K k G (see [13]). While, for the perceptron the larger the connectivity of the graph, the weaker the diameter upper bound in Theorem 3.2 (see for example the discussion of  X  X hick barbells X  at the end of section 4). Acknowledgments We wish to thank the anonymous reviewers for their useful comments. This work was supported by EPSRC Grant GR/T18707/01 and by the IST Programme of the European Community, under the PASCAL Network of Excellence IST-2002-506778.

