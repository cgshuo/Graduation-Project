 Retrieval with Logical Imaging is derived from belief revision and provides a novel mechanism for estimating the relevance of a document through logical implication (i.e. P ( q  X  d )). In this poster, we perform the first comprehensive evaluation of Logical Imaging (LI) in Information Retrieval (IR) across several TREC test Collections. When compared against standard baseline models, we show that LI fails to improve performance. This failure can be attributed to a nuance within the model that means non-relevant documents are promoted in the ranking, while relevant documents are de-moted. This is an important contribution because it not only contextualizes the effectiveness of LI, but crucially ex-plains why it fails . By addressing this nuance, future LI models could be significantly improved.
 Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval -Retrieval Models General Terms: Theory, Experimentation Keywords: Logical Imaging, Probability kinematics
Logical Imaging (LI) is a technique for belief revision which has been employed in a retrieval model introduced by Crestani et al [3]. They proposed to derive the probability of rele-vance R of a document d given a query q ,namely P ( R | d , q ), by computing the probability of a conditionalization, in par-ticular P ( d  X  q ) (Imaging on the document) or P ( q  X  d ) (Imaging on the query). While the original proposal of applying LI in IR was put forward by van Rijsbergen [4] in 1986, it took several years before Amati et al [1] and Crestani et al [3] provided the first retrieval methods us-ing LI. The first implemented the latter conditionalization, P ( q  X  d ), while the second used the conditionalization P ( d  X  q ).

In this poster, we focus on the latter method which was studied in somewhat more, but still limited, depth. Briefly, this technique assumes that a probability 1 is associated with each term in the collection. Successively, this LI method scores a document d summing not only the probabilities of terms appearing both in a query and in the document d , but
In [3], this probability is approximated by the IDF weight of the term, being the set of IDF weights of term in the collection monotonical to a probability distribution. also the probabilities of terms which are considered similar to the query terms belonging to d 2 . The intuition behind the model is that the probability mass of similar terms will be moved into the document and increase the document X  X  relevance if more similar to the query terms. In [3] a number of experiments were conducted on small test collections (i.e. CACM and CRAN) in an attempt to determine whether this intuition would improve retrieval performance. While these experiments reported improvements over an IDF baseline, the model was never compared to models such as TF.IDF or BM25. Nor was it ever tested on large scale test collections. Furthermore, no working i mplementation exists.

In this work, we revisit LI and perform a comprehensive evaluation of the model on several TREC test collections. The remainder of this poster is structured as follows: in the next section we provide an overview of LI applied to document Imaging [3]. Then in Section 3, we outline the set of experiments undertaken as part of this study along with the results of the experiments. The poster concludes in Section 4, stating the main contribution of this work and the possible avenues for future investigation.
A probability distribution P on the set T of terms in a collection D is initially defined such that the sum of all terms probabilities is one. Each document d is represented using terms belonging to T . A document can either be true or not true in the context of a term, i.e. the document either contains or not the term. In order to evaluate the probability of the conditionalization, namely P ( d  X  q ), LI on d is applied, leading to the following computation: where t d ( q ) is the truth function which returns 1 if and only if q is true at t d , 0 otherwise, and t d is the most similar term to t for which d is true. The term similarity is usually computed by means of the co X  X ccurrences of the two terms. In particular, the Expected Mutual Information Measure ( EMIM ) 3 has been used in [3] and is adopted in this work as well.
Note, this makes the method computationally expensive, i.e. every term outside the document must be every time compared to the terms inside the document [2].
The similarity between term t i and term t j is defined as EMIM ( t i ,t j )= P ( t i ,t j ) is the probability the two terms co X  X ccurs together in a windows of text, usually set to have length 10.
The following experimental comparison was performed on three TREC Collections: Associated Press (AP8889) and Wall Street Journal (WSJ8792) using TREC 1, 2, 3 Topics, and the Los Angeles Times (LA8990) using the TREC 6, 7, 9 Topics. Each collection was indexed using Lemur, where they were stemmed and stopped. The LI Model was also implemented in Lemur. Given the previous implementation problems regarding efficiency [2], this implementation re-ranks the IDF baseline, since the IDF baseline is essentially LI without Imaging. In our experiments the top 1,000 docu-ments were re-ranked. For each collection, we compared LI, against IDF, TF.IDF and BM25, where significance testing was performed using the t-test ( p&lt; 0 . 05). Table 1 presents a summary of results. While LI provides some improvement over IDF, it is significantly and substantially outperformed by TF.IDF and BM25 across MAP, p@10, p@20, and bpref for all collections. Clearly, LI is inferior to standard baseline models.
 Table 1: Values in percentage of MAP, P10, P20 and
Despite the theoretical soundness, the current formula-tion of LI is not effective in retrieval tasks. After examin-ing the scoring function, we identified a nuance within the model that contributes to its ineffectiveness. The follow-ing example highlights the problem. Consider documents d 1 = { pet, cat, dog, bird, shop } and d 2 = { fish, chip, shop where we have indicated in brackets the terms present in each document. Given a IDF or TF.IDF retrieval system, a user submitting the query q = { pet, shop } will receive a list of documents where d 1 is ranked higher than d 2 ,since P ( R | d 1 , q ) &gt;P ( R | d 2 , q )(seeEq.1andEq.2). How-ever, LI would revise the initial probability of Relevance given each document and q , namely Eq. 1 and Eq. 2. The initial probabilities are revised in accordance with a prob-ability kinematics policy that transfers probabilities from terms absent from the document to terms present in it. The selection of terms subjected to such kinematics is per-formed employing EMIM. In this example, for document d 1 there is a transfer of probabilities from terms fish , chip to shop ,since fish and chip have a higher similarity to shop than to any other term in d 1 . Thus, the revised proba-bility of shop in d 1 is P d 1 ( shop )= P ( shop )+ P ( fish ). Consequently, LI retrieves document d 1 in response to q with a score proportional to Eq. 3. Similarly, in docu-ment d 2 term shop is the attractor of probabilities trans-Figure 1: Equations representing the probabilities asso-fers having origins from terms pet, cat, dog, bird , leading to P d 2 ( shop )= P ( shop )+ P ( pet )+ P ( cat )+ P ( dog )+ P ( bird ). Thus, d 2 would be retrieved by LI with a score proportional to Eq. 4.
 Comparing the scores associated with the two documents, LI ranks d 2 higher than d 1 ,sinceEq.4 &gt; Eq. 3. 4 Clearly, this is not desirable as non relevant information is promoted above relevant.

Another problem stems from the fact that a document containing related terms does not benefit from the trans-fers, as only external terms are transfered into the docu-ment. This tends mean that shorter documents tend to be favored over longer. Since length normalization is impor-tant for retrieval functions it is likely that this problem also contributes to poorer retrieval performance.

In conclusion, in this poster we have revisited LI per-forming the first, thorough and comprehensive evaluation of the model proposed in [3] and provide a working imple-mentation 5 The major findings of this work are: (1) LI does not significantly improve the overall performance (map and bref), (2) it is significantly worse than standard retrieval models like TF.IDF and BM25, and (3) the LI Model is af-fected by a nuance in the scoring function which demotes relevant documents. Nonetheless, LI does provide some im-provement at early precision which suggests that it may have some potential if the problems in the ranking function can be addressed. Further work will explore these differences, along with re-considering how LI can be applied within a state of the art model, such as the Language Model, which appears to be naturally suited to the belief revision process. The authors would like to thank W. Vanderbauwhede, and IRF 6 . This work is partially funded by EPS RC EP/F014384/. [1] G. Amati and S. Kerpedjiev. An Information Retrieval [2] F. Crestani, I. Ruthven, M. Sanderson, and C. J. van [3] F. Crestani and C. J. van Rijsbergen. Probability [4] C. J. van Rijsbergen. A new Theoretical Framework for Note that since we assume P ( t )= idf ( t ), then P ( fish )= P ( pet )= P ( cat )= P ( dog )= P ( bird ).
Available from the first author by request. http://www.ir-facility.org/
