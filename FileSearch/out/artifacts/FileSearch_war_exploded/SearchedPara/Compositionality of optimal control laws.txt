 Stochastic optimal control is of interest in many fi elds of science and engineering, however it re-mains hard to solve. Dynamic programming [1] and reinforcement learning [2] work well in dis-crete state spaces of reasonable size, but cannot handle continuous high-dimensional state spaces characteristic of complex dynamical systems. A variety of function approximation methods are available [3, 4], yet the shortage of convincing results on challenging problems suggests that exist-ing approximation methods do not scale as well as one would like. Thus there is need for more ef fi cient methods. The idea we pursue in this paper is compositionality. With few exceptions [5, 6] this good-in-general idea is rarely used in optimal control, because it is unclear what/how can be composed in a way that guarantees optimality of the resulting control law.
 Our second motivation is understanding how the brain controls movement. Since the brain remains pretty much the only system capable of solving truly complex control problems, sensorimotor neu-roscience is a natural (albeit under-exploited) source of inspiration. To be sure, a satisfactory un-derstanding of the neural control of movement is nowhere in sight. Yet there exist theoretical ideas backed by experimental data which shed light on th e underlying computational principles. One such idea is that biological movements are near-optimal [7, 8]. This is not surprising given that motor behavior is shaped by the processes of evolution, development, learning and adaptation, all of which resemble iterative optimization. Precisely what a lgorithms enable the brain to approach optimal per-formance is not known, however a clue is provided by another prominent idea: compositionality. For about a century, researchers have been talking about motor synergies or primitives which somehow simplify control [9 X 11]. The implied reduction in dimensionality is now well documented [12 X 14]. However the structure and origin of the hypothetical primitives, the rules for combining them, and the ways in which they actually simplify the control problem remain unclear. We will be able to derive compositionality rules for fi rst-exit and fi nite-horizon stochastic optimal control problems which belong to a certain clas s. This class includes both discrete-time [15 X 17] and continuous-time [17 X 19] formulations, and is rather general, yet affords substantial simpli fi -which in turn is the solution to a linear equation obtained from the Bellman equation by exponen-tiation. Linearity implies compositionality as will be shown here. It also makes a number of other things possible: fi nding the most likely trajectories of optimally-controlled stochastic systems via deterministic methods; solving inverse optimal c ontrol problems via convex optimization; applying off-policy learning in the state space as opposed to the state-action space; establishing duality be-tween stochastic optimal control and Bayesian estimation. An overview can be found in [17]. Here we only provide the background needed for the present paper.
 The discrete-time problem is de fi nedbyastatecost q ( x )  X  0 describing how (un)desirable different states are, and passive dynamics x 0  X  p (  X | x ) characterizing the behavior of the system in the absence of controls. The controller can impose any dynamics x 0  X  u (  X | x ) it wishes, however it pays a price (control cost) which is the KL divergence between u and p . We further require that u ( x 0 | x )=0 whenever p ( x 0 | x )=0 so that KL divergence is well-de fi ned. Thus the discrete-time problem is Let I denote the set of interior states and B the set of boundary states, and let f ( x )  X  0 , x  X  B be a fi nal cost. Let v ( x ) denote the optimal cost-to-go, and de fi ne the desirability function Let G denote the linear operator which computes expectation under the passive dynamics: For x  X  I it can be shown that the optimal control law u  X  (  X | x ) and the desirability z ( x ) satisfy On the boundary x  X  B we have z ( x )=exp(  X  f ( x )) . The linear Bellman equation can be written more explicitly in vector-matrix notation as where M =diag(exp(  X  q I )) P II and N =diag(exp(  X  q I )) P IB . The matrix M is guaranteed to have spectral radius less than 1 , thus the simple iterative solver z I  X  M z I + N z B converges. The continuous-time problem is a control-af fi ne Ito diffusion with control-quadratic cost: The control u is now a (more traditional) vector and  X  is a Brownian motion process. Note that rescaling q . The optimal control law u  X  ( x ) and desirability z ( x ) satisfy where the 2nd-order linear differential operator L is de fi ned as shown that the continuous-time formulation is a spe cial case of the discrete-time formulation. This Then, in the limit h  X  0 , the integral equation exp formula for KL divergence between Gaussians, the KL control cost in the discrete-time formulation reduces to the quadratic control cost in the continuous-time formulation.
 The reason for working with both formulations and emphasizing the relationship between them is that most problems of practical interest are continuous in time and space, yet the discrete-time for-mulation is easier to work with. Furthermore it lead s to better numerical stability because integral equations are better behaved than differential equa tions. Note also that the discrete-time formula-tion can be used in both discrete and continuous state spaces, although the latter require function approximation in order to solve the linear Bellman equation [20]. The compositionality developed in this section follows from the linearity of equations (1, 3). We focus on fi rst-exit problems which are more general. An example involving a fi nite-horizon problem will be given later. Consider a collection of K optimal control problems in our class which all have the same dynamics  X  p (  X | x ) in discrete time or a ( x ) ,B ( x ) , X  in continuous time  X  the same state cost rate q ( x ) and the same sets I and B of interior and boundary states. These problems differ only in their fi nal costs f k ( x ) .Let z k ( x ) denote the desirability function for problem k ,and u  X  u k ( x ) optimal control laws for new problems in our class. We will call the K problems we started with component and the new problem composite .
 Suppose the fi nal cost for the composite problem is f ( x ) , and there exist weights w k such that Thus the functions f k ( x ) de fi ne a K -dimensional manifold of composite problems. The above condition ensures that for all boundary/terminal states x  X  B we have Since z is the solution to a linear equation, if (5) holds on the boundary then it must hold everywhere. Thus the desirability function for the composite problem is a linear combination of the desirability functions for the component problems. The weights in this linear combination can be interpreted as compatibilities between the control objectives in the component problems and the control objective in the composite problem. The optimal control law for the composite problem is given by (1, 3). The above construction implies that both z and z k are everywhere positive. Since z is de fi ned as an exponent, it must be positive. However this is not necessary for the components. Indeed if holds for all x  X  B , then (5) and z ( x ) &gt; 0 hold everywhere even if z k ( x )  X  0 for some k and x . In this case the z k  X  X  are no longer desirability functions for well-de fi ned optimal control problems. Nevertheless we can think of them as generalized desirability functions with similar meaning: the larger z k ( x ) is the more compatible state x is with the agenda of component k . 3.1 Compositionality of discrete-time control laws When z k ( x ) &gt; 0 the composite control law u  X  can be expressed as a state-dependent convex combination of the component control laws u  X  The second term above is u  X  m k ( x ) . The composition rule for optimal control laws is then the mixture weights can be simpli fi ed as Note that 3.2 Compositionality of continuous-time control laws Substituting (5) in (3) and assuming z k ( x ) &gt; 0 , the control law given by (3) can be written as The term in brackets is u  X  Then the composite optimal control law is Note the similarity between the discrete-time result (7) and the continuous-time result (9), as well as the fact that the mixing weights are computed in the same way. This is surprising given that in one case the control law shifts the mean of the distribution given by the passive dynamics. Here we specialize the above results to the case when the components are continuous-time linear quadratic Gaussian (LQG) problems of the form The component fi nal costs are quadratic: The optimal cost-to-go function for LQG problems is known to be quadratic [21] in the form At the prede fi ned fi nal time T we have V k ( T )= F k and  X  k ( T )=0 . The optimal control law is The quantities V k ( t ) and  X  k ( t ) can be computed by integrating backward in time the ODEs Now consider a composite problem with fi nal cost Figure 1: Illustration of compositionality in the LQG framework. ( A ) An LQG problem with quadratic cost-to-go and linear feedback control law. T =10 is the fi nal time. ( B, C ) Non-LQG problems solved analytically by mixing the solutions to multiple LQG problems.
 This composite problem is no longer LQG because it has non-quadratic fi nal cost (i.e. log of mix-ture of Gaussians), and yet we will be able to fi nd a closed-form solution by combining multiple LQG controllers. Note that, since mixtures of Gaussians are universal function approximators, we can represent any desired fi nal cost to within arbitrary accuracy given enough LQG components. Applying the results from the previous section, the desirability for the composite problem is The optimal control law can now be obtained directly from (3), or via composition from (9). Note that the constants  X  k ( t ) do not affect the component control laws (and indeed are rarely computed in the LQG framework) however they affect the composite control law through the mixing weights. We illustrate the above construction on a scalar example with integrator dynamics dx = udt +0 . 2 d X  . The state cost rate is q ( x )=0 .Weset w k =1 for all k .The fi nal time is T =10 . The component fi nal costs are of the form In order to center these quadratics at c k rather than 0 we augment the state: x =[ x ;1] .Thematrices de fi ning the problem are then The ODEs (10) are integrated using ode45 in Matlab. Fig 1 shows the optimal cost-to-go func-lem ( Fig 1A ) is just an LQG. As expected the cost-to-go is quadratic and the control law is linear with time-varying gain. The second problem ( Fig 1B ) has a multimodal cost-to-go. The control law is no longer linear but instead has an elaborate shape. The third problem ( Fig 1C ) resembles robust control in the sense that there is a f1at region w here all states are equally good. The corresponding control law uses feedback to push the state into this f1at region. Inside the region the controller does nothing, so as to save energy. As these examples illustrate, the methodology developed here signi fi cantly extends the LQG framework while preserving its tractability. We showed how composite problems can be solved once the solutions to the component problems are available. The choice of component boundary conditions de fi nes the manifold (6) of problems that can be solved exactly. One can use any available set of solutions as components, but is there a set which is in some sense minimal? Here we offer an answer based on singular value decomposition (SVD). We focus on discrete state spaces; conti nuous spaces can be discretized following [22]. Recall that the vector of desirability values z ( x ) at interior states x  X  I , which we denoted z I , satis fi es the linear equation (2). We can write the solution to that equation explicitly as where G =(diag(exp( q I ))  X  P II )  X  1 P IB . The matrix G maps values on the boundary to values on the interior, and thus resembles Green X  X  function for linear PDEs. A minimal set of primitives corresponds to the best low-rank approximation to G .Ifwede fi ne "best" in terms of least squares, aminimalsetof R primitives is obtained by approximating G using the top R singular values: S is an R -by-R diagonal matrix, U and V are |I| -by-R and |B| -by-R orthonormal matrices. If we now set z B = V  X  r ,whichisthe r -th column of V ,then Thus the right singular vectors (columns of V ) are the component boundary conditions, while the left singular vectors (columns of U ) are the component solutions.
 The above construction does not use knowledge of the family of composite problems we aim to solve/approximate. A slight modi fi cation makes it possible to incorporate such knowledge. Let the family in question have parametric fi nal costs f ( x,  X  ) . Choose a discrete set {  X  k } of the parameter  X  ,andformthe |B| -by-K matrix  X  with elements  X  ik =exp(  X  f ( x i , X  k )) , x i  X  B . As in (4), this choice restricts the boundary conditions that can be represented to z B =  X  w ,where w is a K -dimensional vector. Now apply SVD to obtain a rank-R approximation to the matrix G  X  instead of G . We can set R  X  K to achieve signi fi cant reduction in the number of components. Note that G  X  is smaller than G so the SVD here is faster to compute.
 We illustrate the above approach using a discretization of the following 2D problem: The vector fi eld in Fig 2A illustrates the function a ( x ) . To make the problem more interesting we introduce an L-shaped obstacle which can be h it without penalty but cannot be penetrated. The domain is a disk centered at (0 , 0) with radius time spent inside the disk. The discretization involves |I| = 24520 interior states and |B| = 4163 boundary states. The parametric family of fi nal costs is This is an inverted von Mises function specifying the desired location where the state should exit the disk. f ( x , 0) is plotted in red in Fig 2A .Theset {  X  k } includes 200 uniformly spaced values of  X  . The SVD components are constructed using the second method above (although the fi rst method gives very similar results). Fig 2B compares the solution obtained with a direct solver (i.e. using the exact G )for  X  =0 , and the solutions obtained using R =70 and R =40 components. The desirability function z is well approximated in both cases. In fact the approximation to z looks perfect with much fewer components (not shown). However v =  X  log ( z ) is more dif fi cult to approximate. The dif fi culty comes from the fact that the components are not always positive, and as a result the composite solution is not always positive. The regions where that happens are shown in white in Fig 2B . In those regions the approximation is unde fi ned. Note that this occurs only near the boundary. Fig 2C shows the fi rst 10 components. They resemble harmonic functions. It is notable that the higher-order components (corresponding to smaller singular values) are only modulated near the boundary  X  which explains why the approximation errors in Fig 2B are near the boundary. In summary, a small number of components are suf fi cient to construct composite control laws which are near-optimal in most of the state space. Accuracy at the boundary requires additional components. Alternatively one could use positive SVD and obtain not just positive but also more localized components (as we have done in preliminary work). Figure 2: Illustration of primitives obtained via SVD. ( A ) Passive dynamics and cost. ( B ) Solutions Figure 3: Preliminary model of arm movements. ( A ) Hand paths of different lengths. Red dots denote start points, black circles denote end points. ( B ) Speed pro fi les for the movements shown in (A). Note that the same controller gener ates movements of different duration. ( C ) Hand paths generated by a composite controller obtained by mix ing the optimal controllers for two targets. This controller "decides" online which target to go to. We are currently working on an optimal control model of arm movements based on compositionality. The dynamics correspond to a 2-link arm moving in the horizontal plane, and have the form  X  contains the shoulder and elbow joint angles,  X  is the applied torque, M is the con fi guration-dependent inertia, and n is the vector of Coriolis, centripetal and viscous forces. Model parameters are taken from the biomechanics literature. The fi nal cost f is a quadratic (in Cartesian space) cen-tered at the target. The running state cost is q = const encoding a penalty for duration. The above model has a 4-dimensional state space (  X  ,  X   X  ). In order to encode reaching movements, we introduce an additional state variable s which keeps track of how long the hand speed (in Cartesian space) has remained below a threshold. When s becomes suf fi ciently large the movement ends. This aug-mentation is needed in order to express reaching movements as a fi rst-exit problem. Without it the movement would stop whenever the instantaneous speed becomes zero  X  which can happen at rever-sal points as well as the starting point. Note that most models of reaching movements have assumed prede fi ned fi nal time. However this is unrealistic because we know that movement duration scales with distance, and furthermore such scaling takes place online (i.e. movement duration increases if the target is perturbed during the movement).
 The above second-order system is expressed in general fi rst-order form, and then the passive dy-namics corresponding to  X  =0 are discretized in space and time. The time step is h =0 . 02 sec . The space discretization uses a grid with 51 4 x3 points. The factor of 3 is needed to discretize the variable s . Thus we have around 20 million discrete states, and the matrix P characterizing the passive dynamics is 20 million -by -20 million. Fortunately it is very sparse because the noise (in torque space) cannot have a large effect within a single time step: there are about 50 non-zero entries in each row. Our simple iterative solver converges in about 30 iterations and takes less than 2 min of CPU time, using custom multi-threaded C++ code.
 Fig 3A shows hand paths from different starting points to the same target. The speed pro fi les for these movements are shown in Fig 3B . The scaling with amplitude looks quite realistic. In partic-ular, it is known that human reaching movements o f different amplitude have similar speed pro fi les around movement onset, and diverge later. Fig 3C shows results for a composite controller ob-tained by mixing the optimal control laws for two different targets. In this example the targets are troller instead of an interpolating controller. De pending on the starting point, this controller takes the hand to one or the other target, and can also switch online if the hand is perturbed. An inter-polating controller can be created by placing the targets closer or making the component fi nal costs less steep. While these res ults are preliminary we fi nd them encouraging. In future work we will explore this model in more detail and also build a more realistic model using 3rd-order dynamics (incorporating muscle time constants). We do not expect to be able to discretize the latter system, but we are in the process of making a transition from discretization to function approximation [20]. We developed a theory of compositionality applicable to a general class of stochastic optimal control problems. Although in this paper we used simple examples, the potential of such compositionality to tackle complex control problems seems clear.
 Our work is somewhat related to proto value functions (PVFs) which are eigenfunctions of the Laplacian [5], i.e. the matrix I  X  P II . While the motivation is similar, PVFs are based on intuitions (mostly from grid worlds divided into rooms) rather than mathematical results regarding optimality of the composite solution. In fact our work suggests that PVFs should perhaps be used to approx-imate the exponent of the value function instead of the value function itself. Another difference is that PVFs do not take into account the cost rate q and the boundary B . This sounds like a good thing but it may be too good, in the sense that such generality may be the reason why guarantees regarding PVF optimality are lacking. Nevertheless the ambitious agenda behind PVFs is certainly worth pursuing, and it will be interesting to compare the two approaches in more detail. their paper is restricted to combination of LQG controllers for fi nite-horizon problems, it contains very interesting examples from complex tasks such as walking, jumping and diving. A particularly important point made by [6] is that the primitives can be only approximately optimal (in this case obtained via local LQG approximations), and yet their combination still produces good results. [1] D. Bertsekas, Dynamic Programming and Optimal Control (2nd Ed) . Bellmont, MA: Athena [2] R. Sutton and A. Barto, Reinforcement Learning: An Introduction . MIT Press, Cambridge [3] D. Bertsekas and J. Tsitsiklis, Neuro-dynamic programming . Belmont, MA: Athena Scienti fi c, [4] J. Si, A. Barto, W. Powell, and D. Wunsch, Handbook of Learning and Approximate Dynamic [5] S. Mahadevan and M. Maggioni,  X  X roto-value functions: A Laplacian farmework for learn-[6] M. daSilva, F. Durand, and J. Popovic,  X  X inear bellman combination for control of character [7] E. Todorov,  X  X ptimality principles in sensorimotor control, X  Nature Neuroscience , vol. 7, no. 9, [8] C. Harris and D. Wolpert,  X  X ignal-dependent noise determines motor planning, X  Nature ,vol. [9] C. Sherrington, The integrative action of the nervous system . New Haven: Yale University [10] N. Bernstein, On the construction of movements . Moscow: Medgiz, 1947. [11] M. Latash,  X  X n the evolution of the notion of synergy, X  in Motor Control, Today and Tomorrow , [12] M. Tresch, P. Saltiel, and E. Bizzi,  X  X he construction of movement by the spinal cord, X  Nature [13] A. D X  X vella, P. Saltiel, and E. Bizzi,  X  X ombin ations of muscle synergies in the construction of [14] M. Santello, M. Flanders, and J. Soechting,  X  X ostural hand synergies for tool use, X  JNeurosci , [15] E. Todorov,  X  X inearly-solvable Markov decision problems, X  Advances in Neural Information [16]  X  X ,  X  X eneral duality between optimal control and estimation, X  IEEE Conference on Decision [17]  X  X ,  X  X f fi cient computation of optimal actions, X  PNAS, in press , 2009. [18] S. Mitter and N. Newton,  X  X  variational approach to nonlinear estimation, X  SIAM J Control [19] H. Kappen,  X  X inear theory for control of nonlinear stochastic systems, X  Physical Review Let-[20] E. Todorov,  X  X igen-function approximation methods for linearly-solvable optimal control [21] R. Stengel, Optimal Control and Estimation . New York: Dover, 1994. [22] H. Kushner and P. Dupuis, Numerical Methods for Stochastic Optimal Control Problems in
