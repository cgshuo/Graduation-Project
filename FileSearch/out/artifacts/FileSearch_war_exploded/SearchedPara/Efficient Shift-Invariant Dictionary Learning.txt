 Shift-invariant dictionary learning (SIDL) refers to the prob-lem of discovering a set of latent basis vectors (the dictio-nary) that captures informative local patterns at different locations of the input sequences, and a sparse coding for each sequence as a linear combination of the latent basis el-ements. It differs from conventional dictionary learning and sparse coding where the latent basis has the same dimension as the input vectors, where the focus is on global patterns instead of shift-invariant local patterns. Unsupervised dis-covery of shift-invariant dictionary and the corresponding sparse coding has been an open challenge as the number of candidate local patterns is extremely large, and the num-ber of possible linear combinations of such local patterns is even more so. In this paper we propose a new framework for unsupervised discovery of both the shift-invariant basis and the sparse coding of input data, with efficient algorithms for tractable optimization. Empirical evaluations on multi-ple time series data sets demonstrate the effectiveness and efficiency of the proposed method.
  X  Computing methodologies  X  Factor analysis; Learning latent representations; dictionary learning; sparse coding; time series
Sparse representation models, such as those in dictionary learning, have been proposed for obtaining both a succinct set of vectors as the new basis (also called the dictionary) from unlabeled input sequences, and for representing each sequence as a sparse linear combination of the basis ele-ments, i.e., the sparse coding of the data. Successful appli-cations include those in dimensionality reduction [6], image restoration [ 13 , 2, 18], signal compression [15 , 17 ] and com-pressed sensing [4 , 9].

Conventional dictionary learning models assume that the induced basis is of the same dimensionality of the input data, thus the basis elements (vectors) and the input se-quences (also vectors) are strictly aligned from coordinate to coordinate. This simplistic assumption enables efficient algorithms for finding the optimal basis from input data; however, it also significantly limits the scope of potential applications of those models. Consider the three-time-series example from [ 24 ] reproduced in Figure 1 for instance. Time series No. 1 and series No. 3 share the bumps (as plotted in red) which are similar in shape but appear at different locations. We would consider this pair (1 and 3) more sim-ilar than the other pairs but such similarity cannot be cap-tured, obviously, using a global non-shift-invariant pattern (corresponding to a basis element) with the full span over the time series. On the other hand, such similarity could be captured if we allow the basis elements to have a shorter span and to occur at different locations in the time series. Such a desirable kind of basis is called shift-invariant basis, and finding such a basis from data is referred as solving the shift-invariant dictionary learning (SIDL) problem. Figure 1: Importance of local patterns (plotted in red) versus that of the entire time series.

As a challenging problem with a potentially broad range of applications, SIDL has received increasing attention in re-cent machine learning research [8 , 23 , 14 , 24 ]. Existing work and potential strengths/limitations can be grouped and out-lined as follows: First, methods such as [ 24], which focus on exhaustive or heuristic search over all possible local pattern candidates, are either highly inefficient or sub-optimal, or both. Second, methods such as [ 7], which rely on the avail-ability of sufficiently labeled data in order to learn discrim-inant local patterns, are not applicable when labeled data are lacking or are extremely sparse. Third, methods such as [8], which use convolution to model invariant shift of local patterns, has the drawback of not offering any method for sparse coding of input data. In other words, those methods are capable of learning shift-invariant local patterns as a new basis, but lack of the ability to combine the shift-invariant patterns in a sparse coding of the original data. Recall that sparse coding is a highly desirable for scalable data analysis tasks (including classification and clustering of time series), as well as for the interpretability of the analysis and the robustness of system predictions.

In this paper, we propose a new approach to SIDL which combines the strengths and addresses the aforementioned limitations of the previous methods. Specifically, our model allows the basis elements to be much shorter than the length of input series, to occur at different locations in the input series, and to be optimally combined into a sparse cod-ing of observed data, thus representing each input series as as a sparse linear combination of the short basis ele-ments. More importantly, both the shift-invariant basis and the sparse coding are jointly optimized in an unsupervised learning framework with our new algorithms for highly effi-cient computation. Empirical results on multiple times se-ries demonstrate the effectiveness and efficiency of the pro-posed approach.
Suppose we have a set of n input data points with di-mension p , { x i  X  R p } n i =1 , we want to learn a set of K basis elements, i.e., a dictionary, D = [ d 1 ,..., d K ] in R q  X  K sparse coding  X  i  X  R K for each input data point x i . In clas-sic sparse encoding and dictionary learning, the dimension of the basis is the same as that of the data point, i.e. q = p .
Given a fixed dictionary D , for an input data point sparse encoding aims to find a sparse linear combination of the basis vectors as its representation. Formally, sparse coding with ` 1 regularization amounts to solving where  X  is controls the balance between restoration error and coding sparsity. It is well known that ` 1 penalty yields a sparse solution, and the above LASSO problem can be efficiently solved with coordinate descent [19 , 20 ]. Other sparsity penalties such as ` 0 regularization can be used well, however solving sparse coding with ` 0 penalty is often in-tractable. In this paper, we focus on the ` 1 regularized set-ting.
When we lack a pre-existing dictionary a priori or we wish the dictionary to reflect directly the properties of the data, dictionary learning can induce both the sparse coding for the input data points and the dictionary by solving the following optimization problem where c is a constant and the norm constraint on d j is nec-essary to avoid degenerate solutions. The above problem is not convex in ( D ,  X  ) jointly, but is convex if we fix either variable. Hence alternate optimization between both sets of variables is a common approach to address dictionary learn-ing.

When D is fixed, the above problem is essentially a sparse coding as described in Section 2.1 . When all  X  s are fixed, the above problem is quadratic programming with quadratic constraints (QCQP) and there exist many algorithms to solve it efficiently.
In this section, we present our shift-invariant dictionary learning (SIDL) to capture both the locality of representa-tive patterns as well as preserve a sparse representations for the data points.

Unlike classic dictionary learning which enforces the same dimensionality for the basis and the data point ( q = p ), shift-invariant dictionary relaxes this constraint by q  X  p , allowing the basis to slide along the support of the data point. For real problems, we may often set q to be much smaller than p ; however we emphasize that classic DL is a special case of SIDL with the setting q = p .
 For a data point x i  X  R p and shift-invariant basis d k  X  we introduce a variable t ik to denote the location 1 where d is matched to x i , with t ik = 0 indicating that d k is aligned to the beginning of x i and that t ik = p  X  q indicating the largest shift d k can be aligned to x i without running beyond the boundary of x i , as shown in Figure 3. Hence the possible values for t ik are all integers 2 in [0 ,p  X  q ].
Obviously, shifting a basis element d by an amount of t is
The idea of shift invariant basis can also be applied to data with more than one  X  X irections X , such as image. For exam-ple, for images, the basis now tunrs to be a rectangular area and the shift is represented by two variables on each direc-tion, the idea proposed in this paper can be easily extended to this case. For brevity, we focus on data with only one  X  X irection X  in this paper and leave the extensions as future work.
As we only consider  X  X iscretized X  data points, the shift can only take integer values. equivalent to defining a new vector as where
Given a set of input data points { x i } n i =1 , shift-invariant dictionary learning aims to learn the dictionary D = [ d 1 the sparse coefficients {  X  ik } and the corresponding shifts { t ik } entirely from the data in an unsupervised way: In this section, we present an efficient algorithm to solve SIDL. Similar to classic DL, the SIDL problem is non-convex; hence, we employ an anternative optimization scheme.
Given the dictionary D fixed, Problem ( 5) turns to solving for the basis matching location t ik and the the coefficients  X  . Also note that with D fixed, Problem ( 5) can be de-composed to learning the coefficients and the basis shift for every x i independently. Hence in this subsection we drop the subscript and simply use x to represent any single input data point from the data set for clarity.

To learn  X  and { t k } for input x , we adopt coordinate descent to estimate the coefficients  X  and a greedy approach to estimate { t k } . Minimizing over  X  k and t k with {  X  arg min For this one-dimensional optimization problem, its solution for  X  k (as a function of the optimal basis shift t  X  k ) is with all basis except for d k and S (  X  ) is the shrinkage operator defined as The above solution for  X  k depends on the shift t k k th basis element, and since it can only take integer values from [0 ,p  X  q ], a naive approach is to enumerate all pos-sible t k , compute the corresponding  X  k and pick the pair of ( t  X  k , X   X  k ) which yields the minimum objective defined in Algorithm 1 Shift-invariant sparse coding
Input: data point x  X  R p , dictionary D = [ d 1 ,..., d K Output: sparse coding  X   X  , matching offsets { t  X  k } K k =1
Initialize  X  randomly repeat until convergence (6). However, by plugging Eq. (7) back to Problem ( 6) and with simple math manipulation, we arrive at the following theorem which is much more efficient to compute.

Proposition 4.1. The optimal solution for Problem ( 6) is and Proof. See Appendix A.

Proposition 4.1 suggests that we only need to compute the dot product between the basis element and the segment from the input data point; all updates are exact which do not in-volve parameter tuning. Algorithm 1 outlines the suggested procedure for solving shift-invariant sparse coding.
When the coefficients  X  and basis shifts { t ik } are fixed, updating the dictionary requires solving the following opti-mization problem: Coordinate descent is used to solve for d as well, when min-imizing over d k with { d j } j 6 = k fixed: arg min This is a least square problem with quadratic constraints, which we can solve via its dual problem. The Lagrangian is:
L ( d k ,u ) = 1 where b x i , x i  X  P K j 6 = k  X  ij T ( d k ,t ij ) is the residue for x u  X  0 is the Lagrangian multiplier. Minimizing L ( d over d k , we get an analytic form for d  X  k as Algorithm 2 Shift-invariant dictionary update
Input: data point { x i } n i =1 , sparse coding  X  , matching offsets { t k } K k =1 Output: dictionary D = [ d 1 ,..., d K ]
Initialize D randomly repeat until convergence Algorithm 3 SIDL
Input: data points { x i } n i =1 , desired basis dimension q , desired number of basis K
Output: dictionary D = [ d 1 ,..., d K ], sparse coding {  X  i } n i =1 , basis shifting location { t ik }
Initialize D , {  X  i } n i =1 and { t ik } randomly repeat until convergence through q + t ik . Eq. ( 14 ) suggests that the optimum d a weighted average of the corresponding matching segments of all residues { b x i } . Plugging it back to the Lagrangian and maximizing the dual problem we finally get detailed proof of Eq. (15 ). Algorithm 2 outlines the learning procedure for shift-invariant dictionary updating.
Given unlabeled input data points, SIDL alternates be-tween optimization of the sparse coding and updating the dictionary. Algorithm 3 outlines the learning procedure for shift-invariant ditionary updating.
The time complexity of SIDL consists of two parts, one for shift-invariant sparse coding and the other for shift-invariant dictionary update. For shift-invariant sparse coding (Algo-rithm 1), the cost for one outer iteration, i.e., finding the op-timum shifting locations and basis coefficients for K basis, is O ( Kq ( p  X  q + 1)), where O ( q ) comes from the cost of com-puting inner product of two vectors of length q and p  X  q + 1 is the possible integer scope for solving t k by Proposition 4.1 , hence the total complexity for one call to Algorithm 1 is O ( M 1 Kq ( p  X  q )) where M 1 is the maximum number of iterations allowed in Algorithm 1.

The core part of shift-invariant dictionary update (Algo-rithm 2) is the computing for d k , which takes O ( nq ) oper-ations to compute the e x and scale it to get d k according to Eq. ( 15 ). Thus the total complexity for one call to Algo-rithm 2 is O ( M 2 Knq ) where M 2 is the maximum number of iterations allowed in Algorithm 2.

In fact, we do not necessarily require Algorithm 1 and 2 to converge in every call from 3, since as long as the objective function decreases in both shift-invariant sparse coding and shift-invariant dictionary update, the entire algorithm is still guaranteed to converge. Hence M 1 and M 2 can be set to be quite small, e.g., 10 to 20 iterations in both Algorithm 1 and 2 would suffice. Therefore, the total time complexity for SIDL is O ( MKq ( n + p  X  q )), where M is the total number of iterations allowed or needed for Algorithm 3 to converge.
As we mentioned, classical dictionary learning is a special case of the proposed framework of SIDL by setting q = p , hence the time complexity for classical dictionary learning is O ( MKpn ). By comparing the complexity of the proposed SIDL framework to classical DL, because holds for any q &lt; min( n,p ), hence in terms of asympotic complexity, the proposed SIDL is more efficient than stan-dard classical DL in terms of time complexity. If q is close to min( n,p ), then the proposed SIDL will be of about the same compelexity in computation. Empirical timing of SIDL can also be found in Section 5.3 . In this section, we evaluate the proposed Shift-Invariant Dictionary Learning (SIDL) from two aspects. One is to in-vestigate its performance as a data reconstruction algorithm in reconstructing unseen data with the dictionary learned from training data, and the other is to evaluate the quality of learned sparse representations as features for downstream tasks, specifically for classification.
Before presenting our experimental results, we first list the data sets on which SIDL is performed. We use several real-world time-series data sets in all evaluations from the UCR time series archives 3 [5], which are listed below with brief descriptions. http://www.cs.ucr.edu/  X eamonn/time series data/ Table 1 lists related statistics about the data sets mentioned above.
We first investigate how SIDL can be used as a data re-construction method by learning the shift-invariant basis on the training set and use the learned dictionary to encode the signals from the test set. We report the reconstruction error on all data sets, as well how its performance changes with different values of the sparsity regularization parameter  X  , basis vector length q and dictionary size K .

We train SIDL with different parameters on the training portion of each data set and use the learned dictionary to re-construct the testing data. Specifically, on the training data, we use SIDL the learn the shift-invariant dictionary D , and then given the testing data points, we solve for their sparse codings with D fixed. The Mean Squared Error (MAE) for reconstruction is measured as where  X   X  ik and t  X  ik are obtained by solving the shift invariant sparse coding problem as described in Section 4.1 with d k trained from the training data points.

Figure 3 presents the complete reconstruction performance of SIDL on all time series data sets with different parame-ter configurations. It X  X  worth noting that when q p equals 1, our proposed model reduces to classical dictionary learning. The dictionary sizes in Figure 3 are all set to be K = 50 as we have examined different values of K and observed similar patterns, so we omit the detailed graphs.

On five out of the six datasets, we can see that SIDL ac-tually achieves (i.e. when q p &lt; 1) significantly lower recon-struction error than classical dictionary learning, given the same number of basis elements and same sparsity regular-ization strength . This suggests that not only SIDL produces a better dictionary to generalize to unseen data but also it results in smaller dictionary (since our basis elements are shorter than those of classical dictionary learning). Results from Figure 3 demonstrate that SIDL can be an effective al-ternative to yield unsupervised sparse representations com-pared to classical dictionary learning.
 The above results show the reconstruction performance of SIDL doing a grid search of the parameters. When applied pratically, the optimal choice of q p and the degree of spar-sity regularization  X  can be obtained via cross-validation on splits from the training set, as we will show in the evaluation for using codings from SIDL as features for classification in Section 5.4.
We plot the running time of SIDL with different parameter setting in Figure 4. As disscussed in Section 4.3.2 , when the length of the basis is quite smaller than that of the input signal, SIDL will be much faster than classical DL, such as q = 0 . 1 p and p = 0 . 25 p in Figure 4. When q gets closer to p , as we can see from 4, the running time for SIDL is getting closer to that of DL, especially for larger K .

We also plot how the training objective of SIDL and DL decreases in terms of iterations. We set the convergence threshold, which is defined as the relative function objective change, to be 10  X  5 , i.e. convergence is achieved when where F ( i ) is the objective function for training after the i th iteration. It X  X  clear to see when  X  = 1, all the SIDL runs converge in fewer iterations than DL, except for q = 0 . 75 p which takes about 200 more iterations to converge. When  X  = 10, which encourages sparser solutions, all the SIDL runs converge in fewer iterations than DL. These imply that the proposed SIDL method can be at least as efficient as classical DL, with the flexibility to model shift-invariant basis present in the data.

It X  X  also worth emphasizing that, from Figure 4, although the training loss minimum DL achieved is slightly better than those of SIDLs, this does not necessarily mean that the basis found by DL is any better than those found by SIDL. In fact, as we have shown in the data restruction and we will show in the classification tasks, the basis found by SIDL actually are better than those found by DLs.
In this section, we evaluate the encodings output by SIDL as feature representations for time series classification tasks. For all data sets, we train the dictionary on the training set and apply it onto the test data points to get their en-codings. The dictionary size K , the length of basis element q and the sparsity regularization parameter  X  are chosen via 3-fold cross validation on the training set. We compare the classification results using sparse coding output by SIDL versus using raw input as features. Below is a list of classi-fication algorithms we used for the evaluation. dictionary learning setting.
 (a) Time taken for SIDL with different q and DL to converge (convergence thresh-old = 10  X  5 ) when trained on the Trace data set with sparsity regularization pa-rameter  X  = 1.
Table 2 presents the classification accuracies of various classification methods of SIDL on all data sets, with compar-ison to the baseline method. One interesting result to point out is that, without sparse encodings (such as DL and SIDL), classification with raw time series representations works al-ways worse than naive 1NN method with Euclidean distance except on the ECGFiveDays data set. This suggests for data with temporal dependencies, such as time series, using raw representation as features for SVM is not a good idea. Nevertheless, sparse codings of the sequential data does help improve classification, compared to SVM-Raw .

SVM-SIDL achieves either the best or second best results on five data sets out of six, demonstrating the benefits of modeling shift-invariant local patterns in the data and though classical dictionary learning alleviate the problem suffered by SVM-Raw , it still lacks the flexibility to model local pat-terns. This further validates our motivation that a method that models local patterns in sequential data will better rep-resent and explain the data.

As also validated by previous literature, 1NN-DTW is in-deed a strong baseline method for time series classification, achieving three times of best and once of second best ac-chosen through cross validation from { 0.001, 0.01, 0.1,1,10,100,1000 } ) curacies over all data sets, while the proposed SVM-SIDL twice of best and three times of second best. It X  X  worth mentioning that the proposed method is formulated to reduce reconstruction error, instead of classification error . Besides, it X  X  worth pointing out that nearest neighbor based methods suffer from expensive computational burden from pairwise DTW between the test instance and each of all training ex-amples when doing prediction.

Also it is worth mentioning that SVM-SIDL performs best on the  X  X oughest X  dataset in the collection, i.e., ECGFiveDays , with a large margin over the baseline methods, including 1NN-DTW . This again reinforces the benefits of modeling shift-invariant local patterns for sequential data mining.
In this section, we showcase the basis elements learned by SIDL without any human supervision. Figure 5 presents sample time series from each of the four classes and plots the learned basis elements together with the time series for the Trace data set. The two largest basis of each time series (in terms of the absolute value of the coefficient of that basis) are plotted with their shift locations in Figure 5. Also the degree of sparsity of the resulting codings (propotions of zero coefficients) for each time series are shown in the figure titles. It can be observed from the plots that the learned basis do capture local and discriminative patterns of the time series from different classes. This further validates the idea of representing time series with local basis elements and the proposed method can be used to efficiently learn them, even in the absence of true class labels. All the basis elements are learned with random initialization at the start of SIDL; it is possible that the algorithm might work even better given meaningful initializations.

Similarly, sample time series for Gun Point are plotted in Figure 6. Again, though the dictionary and the encod-ings are learned in an unsupervised fashion, it is clear that the learned representations for the time series do represent meaningful local patterns and more importantly, these pat-terns are relevant indicators for classification, as shown em-pirically by the example time series.

More sample time samples for ECGFiveDays are shown in Figure 7.
This paper presents a new framework for shift-invariant dictionary learning to capture local patterns from input sig-nals. This framework relies on the assumption that the basis vectors may be be shorter than the full input signal. In other words useful temporal patterns may be embedded in differ-ent locations of a longer time series. We also presenet an efficient learning algorithm to estimate the shift-invariant sparse coding as well as a set of effective basis vectors. In our experiments on benchmark time series datasets for clas-sification evaluations, the proposed method produces basis vectors that are more useful for signal reconstruction, ex-hibiting a lower reconstruction error. These same basis vec-tors also produce comparable or even more accurate classi-fications than several state-of-the-art baseline methods..
There are several promising directions in extending the work. First, the formulation of SIDL does not make any assumptions about the input signals, such as smoothness, sparsity or bounded norms [10 ]. If such assumptions are true for the input signals, we can exploit these characteris-tics to further improve the proposed method. Second, tem-poral patterns may compress or stretch in time, and hence incorporating dynamic time warping (DTW) [ 16 , 11 ] into the framework may further improve the process by normal-izing the basis vectors. Third, it is possible to extend the framework to take advantage of supervision such as partial labeling of the input signals [12 ], to help identify local pat-terns that are pertinent to the learning task. In addition, one may also want to investigate initialization strategies of the dictionaries and sparse codings based on priors (such domain knowledge) about the input signal to improve the resulting coding quality.
 We thank the anonymous reviewers for their helpful com-ments. [1] R. J. Alcock and Y. Manolopoulos. Time-series [2] C. Bao, J. Cai, and H. Ji. Fast sparsity-based [3] C. Chang and C. Lin. LIBSVM: A library for support [4] X. Chen, Z. Du, J. Li, X. Li, and H. Zhang.
 location with the time series in red and blue. matching location with the time series in red and blue. [5] Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, [6] I. Gkioulekas and T. E. Zickler. Dimensionality [7] J. Grabocka, N. Schilling, M. Wistuba, and [8] R. B. Grosse, R. Raina, H. Kwong, and A. Y. Ng. [9] Y. Huang, J. Paisley, Q. Lin, X. Ding, X. Fu, and [10] W. Jiang, F. Nie, and H. Huang. Robust dictionary their matching location with the time series in red and blue. [11] E. J. Keogh and M. J. Pazzani. Scaling up dynamic [12] J. Mairal, F. R. Bach, J. Ponce, G. Sapiro, and [13] J. Mairal, F. R. Bach, J. Ponce, G. Sapiro, and [14] A. Mueen, E. J. Keogh, and N. E. Young.
 [15] R. Rubinstein, M. Zibulevsky, and M. Elad. Double [16] H. Sakoe and S. Chiba. Dynamic programming [17] K. Skretting and K. Engan. Image compression using [18] C. Studer and R. G. Baraniuk. Dictionary learning [19] R. Tibshirani. Regression shrinkage and selection via [20] T. T. Wu and K. Lange. Coordinate descent [21] Y. Wu and E. Y. Chang. Distance-function design and [22] X. Xi, E. J. Keogh, C. R. Shelton, L. Wei, and C. A. [23] L. Ye and E. J. Keogh. Time series shapelets: a new [24] J. Zakaria, A. Mueen, and E. J. Keogh. Clustering
Proof. Plugging Eq. ( 7) back to Eq. ( 6), there are three cases:  X  If | T ( d k ,t k ) &gt;  X  If T ( d k ,t k ) &gt; )  X  If T ( d k ,t k ) &gt; )
Aggregating the above three cases, since no matter which per bounded by 1 2 k b x k 2 , we arrive at the statement in Propo-sition 4.1.
 Proof. Plugging Eq. ( 14 ) of d k back to the Lagrangian Eq. ( 13 ), we have Taking the derivative of L w.r.t u yields L ( u ) =  X  From the KKT conditions, we also have u  X  0 and Hence  X  if k e x k P n  X  If k e x k P n
