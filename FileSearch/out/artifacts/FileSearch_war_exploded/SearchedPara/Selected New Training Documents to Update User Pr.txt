 Relevance Feedback (RF) has been proven very effective for improving retrieval accuracy. Adaptive information filter-ing (AIF) technology has benefited from the improvements achieved in all the tasks involved over the last decades. A difficult problem in AIF has been how to update the sys-tem with new feedback efficiently and effectively. In cur-rent feedback methods, the updating processes focus on up-dating system parameters. In this paper, we developed a new approach, the Adaptive Relevance Features Discovery (ARFD). It automatically updates the system X  X  knowledge based on a sliding window over positive and negative feed-back to solve a nonmonotonic problem efficiently. Some of the new training documents will be selected using the knowl-edge that the system currently obtained. Then, specific fea-tures will be extracted from selected training documents. Different methods have been used to merge and revise the weights of features in a vector space. The new model is designed for Relevance Features Discovery (RFD), a pat-tern mining based approach, which uses negative relevance feedback to improve the quality of extracted features from positive feedback. Learning algorithms are also proposed to implement this approach on Reuters Corpus Volume 1 and TREC topics. Experiments show that the proposed ap-proach can work efficiently and achieves the encouragement performance.
 H.3.3 [ Information Search and Retrieval ]: Information filtering. Relevance feedback, Selection process Algorithms Adaptive, Information Filtering, Text Mining, Negative feed-back, Pattern Mining
Information Filtering (IF) is the process of filtering in-coming data streams based on a description (profile) of the user X  X  topic. It is used to re-rank a list of documents as soon as they arrive, with respect to each predefined topic of interest. Major studies in this area can be grouped into two main groups. First, the purpose of knowledge extrac-tion from user feedback is to build the user X  X  profile. The second deals with how effectively an efficiently a user profile can be updated with a new feedback in order to follow the user X  X  interest change and improve the quality of the filtering system.

The objective of knowledge extraction is to build the user X  X  profile in order to find a subset of features available in the feedback documents to describe what the user needs. This is a particularly challenging task in modern information analy-sis, from both an empirical and a theoretical perspective [10, 12]. This problem has received attention from researchers in Data Mining, Web Intelligence and Information Retrieval (IR) communities.

In order to deal with adaptive issues in an IF model, there are two main areas of focus. The first involves updates of the user X  X  profile to follow changes in the user X  X  interests and this is out of our scope in this paper. The second area involves updating the user profile to solve nonmonotonic problems. For instance, given the system X  X  limited number of training documents about the  X  X gent X , the IF systems may return information objects such as  X  X ntelligent Agent X ,  X  X roperty Agent X , or  X  X oftware Agent X . However, when more informa-tion about the user X  X  actual information needs is obtained later on, the system can determine that the user is only interested in  X  X oftware Agent X  [7].

Many models have been developed to deal with adaptive issues in IF approaches. Most of those models focus on up-dating the model parameters to calibration and optimizing thresholds and to following the changes in the user X  X  interest. Those problems have been studied in the adaptive filtering area and include topic profile adaptation using incremen-tal Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold op-timization strategies using probabilistic calibration or local fitting techniques [38, 39, 35, 24, 2, 36]. However, it seems that threshold optimization problems are not at issue any more. This is because the IF aim is to re-rank the incoming set of documents based on user profile rather than making Yes or No decisions for each document.

Our focus however, is on solving the nonmonotonic prob-lem by updating the user X  X  profile with new information. There are two challenging issues for solving a nonmonotonic problem. The first is how to select a document that contains new knowledge that a system does not have. The second is-sue is how to evaluate and update based-knowledge with the new one in an efficient way.

In order to make a breakthrough for the two challenging issues, this paper proposes an innovative approach based on relevance feature discovery (RFD) to efficiently update the user X  X  profile with new training documents. In RFD high-level features (patterns) and low level features will be ex-tracted from initial training documents. The low-level term weights are evaluated according to both their specificity and their distributions in the higher level features, where the higher level features include both positive and negative pat-terns [9, 1].

In the new approach, as soon as the system receives new set of training documents, the documents will be evaluated using the based-knowledge that system has. The documents that are correctly classified by the system will be discarded. Then new knowledge will be extracted from selected docu-ments in new training dataset. Both based-knowledge and new knowledge combine with each other X  X  and are tested us-ing the original new training dataset. The goal of all of this is to update the system with new knowledge to solve non-monotonic problems, keep the effectiveness of the system high and reduce the time required for updating tasks. Ex-perimental results have been formulated for the latest data collections, Reuters Corpus Volume 1 (RCV1) and TREC filtering assessor topics, to evaluate the proposed approach. The experimental results strongly support the aims of the proposed approach.

The remainder of this paper is organized as follows. Sec-tion 2 introduces a detailed overview of the related works. Section 3 reviews the concepts of patterns in text documents. It is also covers the use of negative feedback to update and revise discovered features in positive feedback. Section 4 introduces the proposed model including equations and al-gorithms. The empirical results and discussion reported in section 5 and 6 respectively. Then the concluding remarks follow in the last section.
Relevance feedback has been used widely in the area of information retrieval. It has been shown to be effective with different kinds of retrieval models [22, 8, 19, 23, 37]. The idea of relevance feedback is to involve the user in the re-trieval process so as to improve the final result set.
The popular term-based IR models include the Rocchio al-gorithm [22, 4], Probabilistic models and Okapi BM25 [18, 5] (more details about Rocchio algorithm and BM25 can be found in Section 5.2), and language models, including model-based methods and relevance models [17, 8, 37, 16, 31]. Generally, in the vector space model, terms have been extracted from feedback by using the Rocchio algorithm. Those terms are used to form a new query vector by maxi-mizing its similarity to relevant documents and minimizing its similarity to non-relevant documents [22]. In the lan-guage modelling approaches, the key elements are the prob-abilities of word sequences, which include both words and phrases (or sentences). They are often approximated by n-gram models [28], such as Unigram, Bigram or Trigram, for considering term dependencies.

Some kinds of retrieval models also used pseudo relevance feedback [15, 14] especially when there are no relevance judg-ments available. In pseudo it has assumed that a small num-ber of top-ranked documents in the initial retrieval results are relevant and then relevance feedback is applied. How-ever, this kind of feedback suffers from similarity (all eggs in one basket) and uncertainty [6].

IR models are the basis of the ranking algorithm that is used in search engines to produce the ranked list of docu-ments [34] according to their relevance to a give query [3]. Different to the IF ranking task, classification is the task of assigning documents to predefined categories. While in IR the answer to a query is simply a list of potentially rel-evant documents, in IF the relevant content of such docu-ments has to be located and extracted from the document. A comprehensive review of text classification methods can be found in [25]. To date, many classification methods, such as Naive Bayes, Rocchio, kNN and SVM have been devel-oped in IR [15].

A promising model Relevance Feature Discovery (RFD) was a model that has been proposed for IF within the data mining community and has shown encouraging improvements of effectiveness [9]. Closed sequential patterns were discov-ered in positive text documents, where a pattern was a set of terms that frequently appeared in a paragraph. The de-ployed method was applied to the extracted patterns to over-come the low frequency problem. Based on the positive fea-tures some negative documents were selected (or called of-fenders) that were closed to the extracted features in the pos-itive documents. The features were extracted from selected negative documents used for groups and low-level features (terms) were devised based on both their appearances in the higher-level features (patterns) and their categories [9, 1].
An adaptive information filtering system can be studied from two points of views. First, is the ability of IF system to extract different knowledge for different users in different interested topics. Second, is the ability of updating and re-viewing the weight of features in the hypothesis space model when is received anew feedback. Traditionally, information involved in knowledge-based systems has been manually ac-quired in collaboration with domain experts. However, both the high cost of such a process and the existence of textual sources containing the required information have led to the use of automatic acquisition approaches. In the early eight-ies, text based intelligent (TBI) systems began to manipu-late text so as to automatically obtain relevant information in a fast, effective, and helpful manner [30].

In order to improve the quality of IF and follow user changes a new feedback system needs to be provided over time for users. Updating the system with the new feedback can be done in a batch classifier fashion [36]. However, in reality the system starts with a limited number of feedback responses though it will receive more later on. To update the system with new feedback in a batch fashion when new feedback is received the old feedback and the user X  X  profile rebuild will be combined with the whole dataset of feed-back. This kind of update is time-consuming and needs more space to store all feedback. To solve this issue it is important to update the system with the knowledge that need only in efficiency manner. Updating system adaptively requires focusing on two main issues. First, how to recog-nize the knowledge that system has not learnt. Second, how to update the system knowledge efficiently without reducing effectiveness?
Many algorithms have been developed to deal with this issue. Those algorithms are mainly focused on extracted knowledge automatically, set system parameters, calibration and optimize threshold or for updating the users X  profiles to follow changes in interest [35, 14, 30, 36, 29]. Our study dif-fers from all the above work in that we used a pattern-based model to extract features from relevance feedback. Those features will be updated by adding new features or revis-ing the weight in the hypothesis spas when receives a new training dataset. The main target for all that is to reduce the updating time and keep the effectiveness high in order to avoid time-consuming problems. The updating process in this study aims to solve the nonmonotonic problem and improve the effectiveness of the filtering process through the use of a batch method is time-consuming and needs to store all the feedback that is received.
To extract knowledge from documents, each document d in the dataset is split into a set of transactions. In this paper, we assume that each document d in the dataset is split into a set of paragraphs PS ( d ). For a given topic, relevance feature discovery in text documents aims to find a set of features, including patterns, terms and their weights, in a training set D , which consists of a set of positive documents, D + , and a set of negative documents, D  X  . The following definitions can be found in [9] and [11].
The common hypothesis space used can be defined as T , a set of terms, which can be viewed as words or keywords in text documents. A sequence S =  X  s 1 ,s 2 ,...,s n  X  ( s is an ordered list of terms. Note that the duplication of terms is allowed in a sequence. This is different from the usual definition where a pattern consists of distinct terms. A sequence s 1 = &lt; x 1 ,...,x i &gt; is a sub-sequence of an-other sequence s 2 = &lt; y 1 ,...,y j &gt; , denoted by s iff  X  j 1 ,...,j i such that 1  X  j 1 &lt; j 2 ... &lt; j x 1 = y j 1 ,x 2 = y j 2 ,...,x i = y j i . Given s 1 v s ally say s 1 is a sub-pattern of s 2 , and s 2 is a super-pattern of s 1 . Covering set of a termset X , in document d , which includes all paragraphs dp  X  PS ( d ) is used to donate the absolute support and relative support as:
For example, Table 1 lists a set of paragraphs for a given document d , where PS ( d ) = { dp 1 ,dp 2 ,...,dp 6 } , and du-plicate terms are removed. Let min sup = 0 . 3, giving rise to ten frequent patterns which are illustrated in Table 2. For pattern { t 3 , t 4 , t 6 } in Table 2 the absolute and relative support can be calculating as:
Generally, the positive coverset ( X,d ) used to calculate the support of the termset X unless the termset appear in negative documents only. Based on the relative support, a sequential pattern X in document d is called frequent pat-tern if sup r ( X,d ) is greater than or equal to a predefined minimum support ( min sup )  X  .

A frequent sequential pattern X is called closed sequen-tial pattern if there exists no frequent sequential patterns X such that X &lt; X 1 and supp a ( X ) = supp a ( X 1 ). The relation &lt; represents the strict part of the subsequence relation v . For example, Table 2 contains ten frequent patterns; how-ever, it includes only three closed patterns: &lt; t 3 &lt; t 1 ,t 2 &gt; , and &lt; t 6 &gt; .
To overcome of the patterns low-frequency problem, a deploying method has been developed to deploying higher-level patterns over low-level terms. The evaluation of term supports (weights) term-based approaches is based on its appearances in documents. However, in this research the evaluating of term supports based on their appearances in patterns.

To improve the efficiency of the pattern taxonomy mining (PTM), an algorithm, SPMining ( D + ,min sup ) [33], was proposed (also used in [32, 11]) to find closed sequential patterns for all documents  X  D + , which used the well-known Apriori property in order to reduce the searching space. For all positive documents d i  X  D + , the SPMining algorithm can discover all closed sequential patterns, SP i , based on a given min sup . (We do not repeat this algorithm here because of the length limitation of the paper.)
Let SP 1 , SP 2 , ..., SP n be the sets of discovered closed sequential patterns for all documents d i  X  D + ( i = 1 ,  X  X  X  ,n ), where n = | D + | . For a given term t , its weight in discovered patterns can be described as follows: where | p | is the number of terms in p .

After the weight of terms have been computed from the training set, the following rank will be assigned to every incoming document d to decide its relevance: where w ( t ) = w ( t,D + ); and  X  ( t,d ) = 1 if t  X  d ; otherwise  X  ( t,d ) = 0.
It is clear that use all negative feedback would lead to the increase of the boundary of noises knowledge and the error rate. Then some of the negative documents that are very close to the positive model (call offender documents) se-lected. An offender document is a negative document that most likely classified as a positive document and forces the system make a mistakes. In order to select determine offend-ers documents we assume that both DP + and T features have been extracted from D + use the SPMining algorithm. The negative documents rank use the knowledge that has been extracted from positive documents as follows: where, d is the documents and D  X  is the descendent rank-ing order for all negative documents.

Relevance features should be mainly discovered from the positive documents. Therefore, the offenders are normally defined as the top-K negative documents in a ranked list of negative documents D  X  and K less than | D + | . The offender documents can be define as: where, D  X  o is the list of the offender documents that has been selected from negative documents D  X  , and n is the number of positive documents in the training set.

The terms are grouped into three groups (specific positive terms, general terms and noise terms) based on their speci-ficity to the user X  X  interested topic. Given a term t  X  T , its coverage + is the set of positive documents that contain t , and its coverage  X  is the set of negative documents that con-tain t . Then, the specificity of a given term t in the training set D = D +  X  D  X  define as follows: where coverage + ( t ) = { d  X  D + | t  X  d } , coverage  X  { d  X  D  X  o | t  X  d } , and n = | D + | .

Then, the following classification rules for determining the general terms G , the positive specific terms T + , and the noise terms T  X  : where  X  2 is an experimental coefficient, the maximum bound of the specificity for the general terms, and  X  1 is also an ex-perimental coefficient, the minimum bound of the specificity for the general terms. It is easy to verify that G  X  T +  X  . Therefore, { G,T + ,T  X  } is a partition of all terms.
Formally, let DP + be the union of all discovered patterns of the pattern taxonomies of D + , and let DP  X  be the union of all discovered patterns of the pattern taxonomies of D where a closed sequential pattern of D + (or D  X  ) is called a positive pattern (or negative pattern ).

It is obviously that  X  d  X  D + such that t  X  d for all t  X  T since spe ( t ) &gt;  X  2  X  0 for all t  X  T + . Therefore, for each t  X  T + , it can obtain an initial weight by the deploying method on D + (using the higher level features).

For the term in ( T  X   X  G ), there are two cases. If  X  d  X  D such that t  X  d , t will get its initial weight by using the deploying method on D + ; otherwise it will get a negative weight by using the deploying methods on D  X  o as shown in the following function:. where  X  is coefficient parameter set to  X  1 in this paper.
The initial weights of terms finally are revised according to the following principles: increment the weights of the positive specific terms, decline the weights of the negative specific terms, and do not update the weights of the general terms. The details are described as follows: where w is the initial weight from D + or D  X  o .
The ideas of update the system with new training docu-ments are to provide the system with a new knowledge to follow user need changes over time or to improve the effec-tiveness of filtering process. Generally, the system started with small sample of training document to solve the cold start problem. Then the user provides more feedback about what they want as long as they go [36]. The new feedback is a list of documents judged (relevant 1, irrelevant 0) by users. There are two ways to update the system with the new training documents. The easy one is to combine new training documents with the old one and train the system again from begging. The other way is to extract knowledge from the new training documents, then efficiently and ef-fectively feed it to the system. Our focus is to update the system in efficient manner by extract knowledge from some of the new training documents that contains new knowledge to the system. Then merge the new knowledge with the old one that system has effectively to solve nonmonotonic problem efficiently.

To easily implement that in this paper we group the train-ing documents into two groups: First, the initial group that system start with (called basic training documents D b ). Sec-ond, is the new training dataset D n . The sliding window has been used to select new training documents. The size of the window set to be 25 documents consist of positive and neg-ative training documents, D n = D + n  X  D  X  n . The size of the new training window was set based on the size of the dataset (RCV1).
The new feedback can be categorized into two main cate-gories: first, is a document that contains more explanation about what user need on the same topic. Second category is documents that contain new area or topic which are in-dicate that the user changed his interest topic and that is out of our scope. In this paper we focus in the first category whereas, the user provide more information about interest topic that begin with. The new feedback will overlap with the currant knowledge that system has. Based on that the new training documents group into two groups. First, group contains knowledge that system is clearly learnt (redundant documents). The second group contains new knowledge that is not clear to the system. To avoid overloaded knowledge and reduce the time complexity the new approach use only new training documents that contains new knowledge to the system. To select those document the new training docu-ments evaluated and rank based on the exiting knowledge that system has used the following ranking function. where w ( t ) = w ( t,D ); and  X  ( t,d ) = 1 if t  X  d ; otherwise  X  ( t,d ) = 0.

After ranking the documents the most relevant documents are in the top of the list. We view those documents as redun-dant documents because they contain the same knowledge as the system. Then the precision is set a weight value (e.g 95%)to compare the ranking result with the original result that was provide by the users. Starting from the top of the ranking list calculate the precision for each document and stop when the precision get less than 95%. The same will be done for the negative documents starting from the end of the list where we expected the negative documents are. The documents that set between 95% precision of positive and 95% of negative will be selected as a set D s to update the system because they contains some new knowledge. Figure 1 shows the process of select new training documents.
Algorithm SNTSelect describes the details of select some documents D s from a new training documents D n . We as-sume that the initial features, &lt; T,DP + ,DP  X  &gt; , have been extracted from the initial training set D b . The algorithm takes the new training set D n and the extracted features from D b . It firstly ranks the documents D n in step 2 to step 4. After that, it removes the redundant documents and selects the target documents D s that will be used to update the initial knowledge. Algorithm SNTSelect takes times for SNTSelect ( D n ,T ) Input: A new training set, D n = D + n  X  D  X  n , Output: extracted features from D s , Method: 1: let m = | D n | , TP = 0, TN = 0, s = 1, e = m ; 2: foreach d  X  D n do 3: rank ( d ) =  X  t  X  d  X  T w ( t ); 4: let D n = { d 1 ,d 2 ,...,d m } in descendent ranking order, 5: for i = 1 to m ; 6: TP = 0, TN = 0; 7: for i = 1 to m ; 8: D s = { d i | d i  X  D n ,s  X  i &lt; e } ; rank and select some of the training documents, that takes O ( mlog m ).
Several approaches have been suggested for updating the system with new training datasets. In this paper, we also use the same techniques as used for mining the initial training set to extract knowledge from D s (see Section 3, or [9]). The result of this step is a list of terms with their weight and attributes.

Generally the system starts with limited number of train-ing documents, or called the initial training set D b = D D b . As mentioned before, all documents d  X  D b are split into paragraphs. Therefore, a given document d yields a set of paragraphs PS ( d ). All closed sequential patterns DP in D + b extracted used SPMining algorithm [9]. The result of SPMining is a list of patterns with initial weight. To get term weights from extracted patterns the deploying function (section 3.2) applied for each pattern. The initial weight for each term calculated based on the weight of closed patterns in D + b .

Let T b = { t b 1 ,t b 2 ,...,t bm } be a set of terms which are ex-tracted from DP + b . For given term t b , we have coverset { p | p  X  DP + ,t  X  p } , and the initial weight w i ( t b is calculated using Eq. (1). Some of the negative documents are also selected (call offender) [9] to update and revise the terms in T b ; and we can have closed patterns DP  X  b extracted from the selected negative documents. Then the deploying function applied to DP  X  b to evaluate the initial term weights; and we have coverset  X  ( t b ) = { p | p  X  DP  X  ,t  X  p } .
If the term t b appear only in D + then coverset  X  ( t b ) =  X  and also the term appear only in D  X  then coverset + ( t b Term weights are also revised in Algorithm NRevision() (see [9]) based on their specificity, spe function (see Eq. (3)).
The revised term weight is written as w u ( t b ) that will be used for the adaptive process. Based on the above analysis, we describes the features T b and the knowledge (functions) in the initial training set D b as follows:
As we assumed, the system starts with the initial training set D b and then selects a new training set D s from the in-coming feedback D n , such that D s  X  D n . D s . We also use the same method that has been applied to extract features and knowledge from D b . The features T n and the knowledge (functions) in the selected training set D s can be described as follows:
Both features and knowledge in the initial training set and the new training set will be merged as the final result of the adaptive process. The final features T 0 = T | D are updated as follows: The functions t 0 of term t that appears only in one group T n or T b will be the same as for the single training set. On the other hand, the terms t 0 that appear in both T n and T the term weight has been changed as the above.

The specificity scour then can be calculated using Eq. (3); and each term t 0 can be allocated in one of the three groups T
The initial weights of terms finally are revised according to the following principles: increment the weights of the pos-itive specific terms T 0 + , decline the weights of the negative specific terms T 0 X  , and keep the weight of general terms G as it is. The details are described as follows: where w i is the initial weight from D b and D s .
The time complexity of knowledge extraction and merging technique are mainly decided by the number of selected new training documents. Therefore, The time complexity of this algorithm is O ( | T | X | d | X  n ). In merging process for each spe ( t ), where | d | is the average size of the documents, | D the number of offenders and | D s | X  n . Therefore, The time complexity of this algorithm is O ( | T | X | d | X  n ).
In summary, the process of Adaptive RFD model (ARFD) consists of the following steps: (1) Mining features and func-tions from the initial (or the base) training set D b ; (2) Se-lecting target documents D s from a new training set D n in order to remove some redundant documents; (3) Mining fea-tures and functions from the target documents D s ; and (4) merging these features and the functions discovered from the both initial training set and the selected target documents. The objective of this process is to learn new knowledge in the new training set in order to refine the discovered knowledge in the initial training set.
The proposed model aims to efficiently approximate the performance of the merging to the performance of the origi-nal system, which just simply adds the new training set into the initial one and then finds the features and functions. Our model starts with an initial training set then updates the discovered knowledge using a new training set. The orig-inal system is designed as a baseline model that combines both the initial training set with the new one. To test the efficiency of the proposed model, the updating time includ-ing the document selection, relevance feature discovery and merging will be calculated and compared with the baseline model. In this paper, we conduct routing filtering to avoid the need of threshold tuning, which is beyond our research scope. This section discusses the testing environment includ-ing the data collection, baseline models, evaluation methods and the result.
Reuters Corpus Volume 1 (RCV1) was used to test the effectiveness of the proposed model. RCV1 corpus consists of all and only English language stories produced by Reuter X  X  journalists between August 20, 1996, and August 19, 1997 with total 806,791 documents. The document collection is divided into training sets and test sets.

TREC (2002) has developed and provided 100 topics for the filtering track aiming at building a robust filtering sys-tem. The topics are of two types: 1) A first set of 50 topics are developed by the assessors of the National Institute of Standards and Technology (NIST)(i.e., assessor topics); the relevance judgements have been made by assessor of NIST. 2) A second set of 50 topics have been constructed artificially from intersections of pairs of Reuters categories (i.e., inter-section topics) [27]. For that reason we use the 50 assessor topics in this paper where the result is more reliable.
RCV1 collection is marked in XML. To avoid bias in ex-periments, all of the meta-data information in the collection has been ignored. The documents are treated as plain text documents by preprocessing the documents. The tasks of re-moving stop-words according to a given stop-words list and stemming term by applying the Porter Stemming algorithm are conducted [13].
Both term-based and pattern-based approaches have been used as a baseline model to compare with the proposed model. The well-known term-based methods Rocha and BM25 are use to extract features from training documents to build the user X  X  profile. For the pattern-based approach the up-to-date pattern mining based methods sequential closed patterns implemented in RFD model. The model, called RFD, discovers sequential closed patterns from positive and negative documents, deploys discovered patterns on their terms. Then used negative feedback to group and revised the extracted features from positive documents as shown in section 3.4 as well [9].

To conduct the experiments, all models implemented and trained and tested used the same training dataset in the same machine. However, the training documents in all base-line models are combined of the based training dataset and the new one. Then all the system trained used the combined training set at the beginning in batch fashion.

Our approach is called Adaptive Relevance Features Dis-Table 3: Adaptive Relevance Features Discovery re-sults in all assessor topics.
 covery (ARFD). This approach is based on RFD model and adding the ability to update the extracted knowledge adap-tively. To get the experimental result the system trained at the beginning used an initial training dataset. In RFD and ARFD the minimum support set min sup = 0 . 2 (relative support),  X  1 = 0 . 2 and  X  2 = 0 . 3. The size of the window in new training documents in ARFD is set to be 25 documents.
The Rocchio algorithm [21] has been widely adopted in the areas of text categorization and information filtering. It can be used to build the profile for representing the concept of a topic that consists of a set of relevant (positive) and irrelevant (negative) documents. The Centroid ~c of a topic can be generated as follows: where we set  X  =  X  = 1 . 0 in this paper. The set of the parameters  X  and  X  were suggest by [26]. However, different setting suggested by others was tested. We use the setting that gave the best result.

BM25 [5, 20] is the one of state-of-the-art retrieval func-tions used in document retrieval. The term weights are es-timated using the following BM25 based equation:
W ( t ) = tf  X  ( k 1 + 1) where N is the total number of documents in the training set; R is the number of positive documents in the training set; n is the number of documents which contain term t ; r is the number of positive documents which contain term t ; tf is the term frequency; DL and AVDL are the document length and average document length, respectively; and k and b are the experimental parameters (the values of k 1 and b are set as 1.2 and 0.75, respectively, in this paper).
For each topic, we also choose 150 terms in the positive documents based on tf*idf values for all term-based baseline models. The data are pre-processed include the tasks of removing stop-words and stemming term by applying the Porter Stemming algorithm [13]. The effectiveness was measured by four different means: The F-beta ( F  X  ) measure, Mean Average Precision (MAP), the break-even point ( b/p ), and Interpolated Average Preci-sion (IAP) on 11-points . Precision ( p ), Recall( r ) and F calculated by the following functions: where TP (true positives) is the number of documents the system correctly identifies as positives; FP (false positives) is the number of documents the system falsely identifies as positives; FN (false negatives) is the number of relevant documents the system fails to identify.
 The parameter  X  = 1 is used in our study, which means that recall and precision is weighed equally. Mean Average pre-cision is calculated by measuring precision at each relevant document first, and averaging precision over all topics. The b/p break-even point indicates the value at which precision equals recall. The larger a b/p , MAP, IAP or F  X  -measure score is, the better the system performs. 11-points measure is also used to compare the performance of different systems by averaging precisions at 11 standard recall levels (i.e., re-call=0.0, 0.1, ..., 1.0).

Statistical method is also used to analyse the experimental results. The t-test assesses whether the means of two groups are statistically different from each other. The paired two-tailed t-test is used in this paper. If DIF represents the difference between observations, the hypotheses are: Ho : DIF = 0 (the difference between the two observations is 0). Ha : DIF 6 = 0 (the difference is not 0). N is the sample size of group. The test statistic is t with N  X  1 degrees of freedom ( df ). If the p -value associated with t is low ( &lt; 0.05), there is evidence to reject the null hypothesis. Thus, there is evidence that the difference in means across the paired observations is significant. The ARFD model is compared with RFD, Rocchio, BM25, and SVM models for each variable b/p , MAP , IAP , F  X  =1 over all assessor topics, respectively.
The result of the proposed model ARFD in all five mea-sures including time are presented in Table 3. Each row presents the average result for all assessor topics in RCV1 dataset. In each topic the system start from the initial train-ing documents then add a window of new training docu-ments. The size of the window set to be 25 documents. Each window of training documents selected randomly. To test the robustness of the proposed model, we conduct the process of adaptive 6 times for the same initial training set. Table 3 indicates the result of the 6 times updating.
Table 4 shows the results of the main baseline model that is used in this paper. Each loop presents the average re-sult for all assessor topics in RCV1 dataset. For each topic the initial training dataset is combined with the same new training dataset that use in the same topic and the same loop in ARFD system. The combination of the two dataset is used to train RFD system from beginning. Each topic in each loop is run separately. The average time for each run calculated and presented in the Table 4. The P-valu of comparison between the two models (RFD and ARFD) are shown in Table 5.

More result used the state of art are shown in Table 7 and Table 6. The method that used to combine the initial training dataset with a new window of training documents is the same that used in RFD model. Each topic in each loop in each model used the same training documents but Table 4: Relevance Features Discovery results used batch training documents in all assessor topics.
 Table 5: P-value result comparing RFD and ARFD model.
 in different manner. As mentioned before the testing done in 6 loops, each loop uses the same initial training dataset with different new window of training dataset. The number after the model name in Table 3, 4, 5, 6 and 7 is indicate the loop number. For the model with  X * X  is the model that used only the initial training dataset. Each loop in each model used the same training dataset.
The main process of ARFD is aiming to update and revise features weight in vector space efficiently. In this section, we discuss the issues of using more training documents and the use of proposed model to speed up the updating time.
Generally, using more training documents are lead to the improvement of system effectiveness. This has been sup-ported by the experiment result conducting by the proposed and baseline models as shown in Table 4, 3 6 and 7. How-ever, the percentage of improvement is affected by several reasons.

From result providing in Table 4, we can compare the result of used only initial training dataset with the use of initial dataset and 25 more documents. It is clearly that the pattern-based model RFD mode use more 25 documents im-average precision compare with RFD* that used only the ini-tial training dataset. All the 6 loops use the same number of training documents, however, the percentage of improve-ment is affected by the quality of the training documents. The quality of the training documents can be measured by how those documents clearly present the user X  X  need. Also, the number of positive and negative documents that appear in the training dataset also affects system effectiveness. Comparing also with the traditional term-Based model Rocchio and BM25 in Table 6 and Table 7. The table shows that use more 25 training documents improve the over Table 6: Rocchio model used batch training docu-ments in all assessor topics.
 Table 7: BM25 model used batch training docu-ments in all assessor topics.

From the above analysis, it seems that percentage of im-provement in term-based approaches by using more training documents is much more batter than pattern-base model. However, it is difficult to say that, term-based approaches are more reliable to use in adaptive system more than pattern-based approach. It can be because the patterns-based model (RFD) is already gave better result. More research needs to be done to investigate this issue.
Generally, result proposed in Table 4, Table 3 are shows that using more training documents would lead to improve the effectiveness of the system. The percentages of improve-ment are affected by the quality of the training documents. In order to see the improvement in the proposed model, we compare it with the same model used batch training docu-ments. Both models use the same training dataset but in different methods. As shown in Table 4 and Table 3 that using batch training documents would lead better improve-ment than our approach. However, is that improvement significant or not deserve the time that does take, Table 5 clarify this point. Comparing RFD result with ARFD result, the average of p-valu result in all five measures are grater than 0 . 05 as shown in Table 5. It indicates that the different between RFD that use batch training documents and ARFD that used the proposed model are not significant.

Our approach aims to update the system with new train-ing dataset efficiently to improve the effectiveness and solve the nonmonotonic problem. As we see before that the effec-tiveness is almost the same for RFD and ARFD. To mea-sure the efficiency compare the time that take to update the system in RFD and ARFD. The time for each loop pro-posed in Table 4 and Table 3. In average ARF take about in Table 5 shows that the different between time strongly significant. Comparison result between times in each loop for RFD and ARFD proposed in Figure 2. The huge dif-ferent in time between RFD and ARFD can be see clearly Figure 2: Time comparison result between RFD and ARFD. in the Figure. In this paper we did not compare BM25 and Rocchio with the proposed because BM25 and Rocchio are terms-based approaches, which is not fare to compare with pattern-based model in time.

Finally, the result shows that the proposed model achieves their objective design. It is improve the updating process time and keep almost the same performance of filtering tasks.
In this paper we proposed an adaptive information filter-ing system called adaptive relevance features discovery. The adaptive model is built on the top of pattern-based method using data mining technique to extract patterns from rele-vance feedback. The main aim of this method is the efficient revision and updating of extracted features weight in vector space using new training documents to solve the nonmono-tonic problem. A method of selecting useful training docu-ments among the new training dataset was developed. Then new knowledge will be extracted from selected training doc-uments. Different methods have been used to merge the new knowledge with the based-knowledge. The combination of the knowledge will be tested to ensure that it helps to solve the nonmonotonic problem. Compared with the baseline models that use batch training documents, the experiments on RCV1 and TREC topics demonstrate that the efficiency of updating the system using the proposed model are sig-nificantly improved and maintain almost the same level of effectiveness. Experiments show that the proposed approach can work efficiently to achieve encouraging performance on the time required to update the system. However, it is still challenging to effectively and efficiently update the system knowledge with a new one. [1] A. Algarni, Y. Li, Y. Xu, and R. Y. Lau. An effective [2] J. Callan. Learning while filtering documents. In [3] X. Geng, T.-Y. Liu, T. Qin, A. Arnold, H. Li, and [4] T. Joachims. A probabilistic analysis of the rocchio [5] K. S. Jones, S. Walker, and S. E. Robertson. A [6] K. Y. Lanbo Zhang Yi Zhang, Jadiel de Arma. UCSC [7] R. Lau, A. H. M. ter Hofstede, and P. D. Bruza. [8] V. Lavrenko and W. B. Croft. Relevance based [9] Y. Li, A. Algarni, and N. Zhong. Mining positive and [10] Y. Li and N. Zhong. Mining ontology for [11] Y. Li, X. Zhou, P. Bruza, Y. Xu, and R. Y. Lau. A [12] X. Ling, Q. Mei, C. Zhai, and B. Schatz. Mining [13] B. Liu. Web Data Mining: Exploring Hyperlinks, [14] Y. Lv and C. Zhai. Adaptive relevance feedback in [15] C. D. Manning, P. Raghavan, and H. Schtze.
 [16] D. Metzler and W. B. Croft. Latent concept expansion [17] J. M. Ponte. A language modeling approach to [18] C. J. V. Rijsbergen. Information Retrieval . [19] S. E. Robertson and K. Sparck Jones. Relevance [20] S. E. Robertson, S. Walker, and M. Hancock-Beaulieu. [21] J. Rocchio. Relevance feedback in information [22] J. J. Rocchio. Relevance feedback in information [23] G. Salton and C. Buckley. Improving retrieval [24] R. E. Schapire, Y. Singer, and A. Singhal. Boosting [25] F. Sebastiani. Machine learning in automated text [26] A. Singhal, M. Mitra, and C. Buckley. Learning [27] I. Soboroff and S. Robertson. Building a filtering test [28] F. Song and W. B. Croft. A general language model [29] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive [30] J. Turmo, A. Ageno, and N. Catal`a. Adaptive [31] X. Wang, H. Fang, and C. Zhai. A study of methods [32] S.-T. Wu, Y. Li, and Y. Xu. Deploying approaches for [33] S.-T. Wu, Y. Li, Y. Xu, B. Pham, and P. Chen. [34] C. C. Yang. Search engines information retrieval in [35] Y. Yang and B. Kisiel. Margin-based local regression [36] Y. Yang, S. Yoo, J. Zhang, and B. Kisiel. Robustness [37] C. Zhai and J. Lafferty. Model-based feedback in the [38] Y. Zhang. Using bayesian priors to combine classifiers [39] Y. Zhang and J. Callan. Maximum likelihood
