 Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) has become a well-established and highly im-portant NLP component which directly benefits var-ious downstream applications, such as text summa-rization (Trandab  X  at  X , 2011), recognizing textual en-tailment (Sammons et al., 2012) or QA systems (Shen and Lapata, 2007; Moreda et al., 2011). Its goal is to detect verbal or nominal predicates, to-gether with their associated arguments and semantic roles, either by PropBank/Nombank (Palmer et al., 2005; Meyers et al., 2004) or FrameNet (Baker et al., 1998) analysis. In its traditional form, however, SRL is restricted to the local syntactic context of the predicate as in the following example from Ruppen-hofer et al. (2010): In a FrameNet-style analysis of the sentence, the predicate place evokes the P LACING frame, with two frame elements (roles) overtly expressed (T
HEME and T IME ) but with one role  X  G OAL  X  be-yond the embedded relative clause and thus beyond the scope of the SRL parser. Such implicit roles, or null instantiations (NIs) (Fillmore, 1986; Ruppen-hofer, 2005) are much harder to detect automatically, as they require to broaden the analysis to the sur-rounding discourse, commonly also to preceding (or following) sentences.

State-of-the-art approaches to implicit SRL (iSRL) are supervised and need a groundwork of hand-annotated training data  X  which is costly, ex-tremely sparse, limited to only a handful of predi-cates, and requires careful feature engineering (Ger-ber and Chai, 2012; Silberer and Frank, 2012; Li et al., 2015). A first attempt has been made to combine the scarce resources available (Feizabadi and Pad  X  o, 2015), but given the great diversity of predicate-specific roles and enormous complexity of the task, the main issues remain (Chen et al., 2010).
 A promising exploratory effort recently made by Gorinski et al. (2013) aims to overcome the anno-tation bottleneck by using distributional methods to infer evidence for elements filling null instantiated roles. The authors do not rely on gold annotations but instead learn distributional properties of fillers induced from a large corpus.
 Our Contribution : We propose an extension of the distributional idea for unsupervised iSRL to loosen the need for annotated training data. Specifically, we propose to induce predicate and role-specific proto-typical fillers from large amounts of SRL annotated texts in order to resolve null instantiations as (se-mantically and syntactically) similar elements found in the context. Parts of our approach have been suc-cessfully applied in traditional SRL (Hermann et al., 2014), but not yet to implicit roles. Our work dif-fers from Gorinski et al. (2013) in that we extend discrete context vectors to SRL-guided embeddings and experiment with a variety of different configu-rations. We intend not to set a new benchmark beat-ing the current state-of-the-art for supervised iSRL, but rather provide a simple and alternative strategy which does not rely on manually annotated gold data. Still, we demonstrate that our method is highly competitive with supervised methods on one out of two standard evaluation sets and that it can easily be extended to other predicates for which no implicit gold annotations are available. 2.1 Prototypical Fillers We use large amounts of explicit SRL annotations to compute predicate-specific protofillers (prototypical fillers) for each frame element (role) individually: where N is the total number of tokens filling a par-ticular role and E (  X  ) is an embedding function which maps a word w i to its distributed representation, i.e., a precomputed vector of d dimensions. Note that only those words contribute to the protofiller of a frame element which occur in this role. 2.2 Identifying Null Instantiations Our approach generalizes over labeled filler in-stances of the frame (P LACING in the example) as found in corpus data, e.g., placed on the middle pic-ture , planted on the top of the church , hung over the river , laid on the table , etc. We exploit their syntactic (here: prepositional) and semantic prop-erties (inanimate, spacial NPs) in order to capture a composed meaning and thus to approximate the correct implicit role in the centre of this room . We measure similarity between a trained protofiller # X  v p and a candidate constituent # X  v c by cosine similar-null instantiation which maximizes the inner prod-uct with the protofiller. As candidate constituents for an implicit argument we initially consider all ter-minal and non-terminal nodes in a context window of the predicate, ruling out those categories which never occur as implicit arguments, which do contain the target predicate and/or which are already overt arguments. The result set comprises mainly nouns, verbs and PPs. Candidate constituents in our evalua-tion data are available from their respective (manual) syntax annotation, but could easily be extracted us-ing automated phrase-structure parsers. The candi-date vectors for arbitrary length n-grams are derived in the same way (by means of Equation 1). 2.3 Training Resources &amp; Tools In accordance with domain-specific evaluation data, we chose to learn protofillers on two distinct cor-pora: The Corpus of Late Modern English Texts, CLMET (Smet, 2005) (  X  35M tokens, 18th X 20th century novels) and a subset of the English Giga-word corpus (Graff and Cieri, 2003) (  X  500M to-kens of Newswire texts). We label the first one with mantic parser. We employ MATE 2 (Bj  X  orkelund et al., 2009) to obtain a PropBank/NomBank analysis for each sentence in Gigaword.
 Table 1 highlights general statistics on the number of predicates collected from both corpora. Two ob-servations are worth noting: While on average the number of explicitly realized roles/frame elements per predicate/frame in both data sets is similar, we find more predicate instances in CLMET than in Gi-gaword. This is due to the FrameNet lexicon and its more fine-grained modeling of lexical units, as opposed to PropBank. Also note that FrameNet currently specifies 9.7 frame elements per lexical comprises non-core arguments  X  is much larger than what can explicitly be labeled by the SRL systems.
Regarding the distributional component, we ex-perimented with a variety of distributed word repre-sentations: We chose out of the box vectors; Col-lobert et al. (2011), dependency-based word embed-dings (Levy and Goldberg, 2014) and the pre-trained Google News vectors from word2vec 4 (Mikolov et al., 2013). Using the same tool, we also trained cus-tom embeddings (bag-of-words and skip-gram) with 50 dimensions on our two corpora. In order to assess the usefulness of our approach, a quantitative evaluation has been conducted on two iSRL test sets which have become a de facto stan-dard in this domain: a collection of fiction novels from the SemEval 2010 Shared Task with manual annotations of null instantiations (Ruppenhofer et al., 2010), and Gerber and Chai (2010) X  X  augmented NomBank data set. Table 2 shows some general statistics on the number of implicit roles and can-didate phrases involved in our experiments. As to have a comparison with the supervised approaches referred to in this study, we also provide the size of the training data.
 3.1 SemEval Data In Table 4, we report the classification scores for the ( NI-only ) null instantiation linking task on the SemEval data, given the parsed candidate phrases and the gold information about the missing frame sults of our best-performing configuration, obtained from protofillers trained on the late modern English texts and Collobert et al. (2011) embeddings (C&amp;W) with the search space for candidate NIs limited to the current and previous sentence. As a reference, we compare our results to the two best models (M 1 and M 1 0 ) by Silberer and Frank (2012), the vector-based resolver ( VEC ) by Gorinski et al. (2013)  X  which is most similar to ours  X  and, finally, their ensemble combination of four semantically informed resolvers by majority vote (4 X ).
 The figures in Table 4 suggest that our approach clearly outperforms the vector-based method by Gorinski et al. (2013) and is best in terms of over-all recognition rate (recall) among all systems. One potential reason for that might be that, in contrast to the VEC resolver, we do not compute mere con-text vectors but do rely on the valuable annotations obtained from explicit SRL structures. Also, we do not restrict our analysis to head words only, as we have seen that syntactic information from function words is crucial for the resolution of null instantiated roles, too. Moreover, our distributional protofiller method is highly competitive with state-of-the-art performance by Silberer and Frank (2012), yet does not yield better results in terms of F 1 score. Note however that, in contrast to their approach, ours is largely unsupervised and does neither rely on gold coreference chains, nor do we need to train on im-plicit semantic roles in a supervised setting. An error analysis of our method reveals that it is par-ticularly effective for NIs encountered in the same sentence as the target predicate (44.4% accuracy), which seems plausible given the contextual setup in which protofillers are derived. 3.2 NomBank Data Compared to the SemEval data, Gerber and Chai (2010) X  X  augmented NomBank resource covers only ten nominal predicates, which allows us to nicely vi-sualize the distributional profile based on their pro-totypical fillers. For each predicate, we simply con-catenate all per role computed protofillers and apply multidimensional scaling to project the so obtained vectors onto two dimensions (cf. Figure 1).

We observe that the predicate grouping is now based on the prototypical fillers that they co-occur with: In the Wall Street Journal texts, loss , loan and investment are similar because their proto-agents (A0 fillers) who lose, lend and invest resp. are se-mantically shared (i.e. companies, banks). Simi-larly, bid , cost and fund are related in that the tar-gets or commodities (A2) are all money-financed. Finally, the predicates sale and plan are to be ex-pected as outliers as they are less homogeneous in their prototypical argument structure.

We have empirically evaluated our protofiller method also on this data set: Table 3 reports the classification scores for implicit argument resolution compared to the state-of-the-art (Laparra and Rigau, 2013). We restrict the search for implicit arguments to certain predicate-specific parts-of-speech, since some syntactic constituents (e.g., SBAR) never oc-cur as implicit arguments. For choosing the final implicit arguments for each individual predicate in-stance, we follow the same deterministic strategy as described in Gerber and Chai (2010), which in-formally states that, if a certain role is not overtly expressed (within a chain of mentions of the same predicate in previous sentences), it is an implicit candidate. POS lists and cosine similarity thresholds which trigger an actual prediction have been opti-mized on the development set. The context window for candidate NIs is optimal for the current and pre-vious two sentences in our setting, which explains why the the number of candidate constituents is ap-proximately twice as large for the NomBank predi-cates (cf. Table 2).

Our best-performing protofillers are again ob-tained by Collobert et al. (2011) embeddings sub-stituting explicit SRL annotations in the Gigaword corpus, and with custom-trained embeddings using the continuous bag-of-words model. Overall, our re-sults significantly exceed the highly informed base-line but cannot beat the state-of-the art on this test set. For some predicates, the protofillers seem to generalize better (higher recall), and in particular for the low-frequency predicates ( fund ), precision can be increased. Also, we found that the dependency-based word embeddings do perform slightly worse (not shown), compared to our optimal two con-figurations. This might be due to the fact that the inherent properties of dependency-based con-texts mostly focus on relations between semanti-cally valuable nouns, ignoring ( X  X kipping X ) func-to the pre-computed Google News vectors which come with a frequency cutoff excluding stop words, again a constraint which is harmful for the correct identification of implicit roles. Furthermore, skip-gram embeddings perform significantly worse than the embeddings derived by the continuous bag-of-words implementation (relative decrease in F 1 by more than 30%). Finally, we observed that infer-ring implicit roles for nominal predicates is much more challenging because our collected fillers ex-hibit a much greater variation. For example, the protoagents of loan can roughly be divided into two categories, institutions and countries. This in turn introduces noise and has a negative effect on the quality of the singleton protofillers which by vector average capture neither of the two groups perfectly. Promising alternatives could operate on (topic-like) protofiller clusters which we leave for future work. We have described a lightweight approach for the resolution of implicit semantic roles which does not rely on manual gold annotations. For each predicate-specific role, our method generalizes over explicit SRL-guided annotations incorporating pre-trained word embeddings. This allows us to capture their idiosyncratic properties and use the so-inferred protofillers to find null instantiated roles by means of distributional similarity.

Our method has proven to be generally useful, in particular on the SemEval data, where it is compet-itive with supervised systems. Its greatest benefit stems from its simplicity and from that fact that it allows to induce null-instantiated roles for arbitrary predicates. As it is applicable even if no iSRL train-ing data is available, it represents a promising tech-nique to address iSRL data scarcity issues.
 In our experiment, we employed PropBank/Nom-Bank-style (i)SRL annotations, and our general de-sign clearly benefits from using small-scale inven-tories of semantic roles. It should be noted though, that our approach is not restricted to any particular SRL tagset, but can be equally applied to other role inventories with similar degrees of consistence and size. Beyond SRL annotations in a strict sense, this might even extend to syntactic dependency annota-tions that are occasionally taken as a substitute for semantic roles proper. In particular, we see poten-tial in combining our experiments with on-going ef-forts to cross-lingual projection, adaptation and har-monization of syntax annotations along the lines of Sukhareva and Chiarcos (2014, 2016) and related approaches based on frameworks such as the Uni-ful, an adaptation using grammatical relations rather than semantic roles represents a promising possibil-ity to create iSRL annotation and iSRL annotation tools for other languages, as Universal Dependen-cies are becoming increasingly available for major and low-resourced languages and can be projected to others.

The protofillers involved in this study are available at: http://acoli.cs. uni-frankfurt.de/resources . The authors would like to thank Joyce Chai and Hendrik De Smet for providing us access to their resources and corpora. We are grateful to Egoitz Laparra for sending us their evaluation script and also thank the anonymous reviewers for their valuable feedback and insightful comments.

