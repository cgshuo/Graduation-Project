
Wang-Chien Lee  X  John McPherson  X  Graph has been a ubiquitous and essential data representation to model real world objects and their relationships. Today, large amounts of graph data have been generated by various applications. Graph summarization techniques are crucial in uncovering useful insights about the patterns hidden in the underlying data. However, all ex-isting works in graph summarization are single-process solutions, and as a result cannot scale to large graphs. In this paper, we intro-duce three distributed graph summarization algorithms to address this problem. Experimental results show that the proposed algo-rithms can produce good quality summaries and scale well with increasing data sizes. To the best of our knowledge, this is the first work to study distributed graph summarization methods.
Graph has been a ubiquitous and essential data representation to model real world objects and their relationships. Today, large amounts of graph data have been generated by various applications, including social networks, biological networks, WWW, etc. With the overwhelming wealth of information encoded in these graphs, there is a crucial need for tools to summarize large graphs into con-cise forms that can be easily understood.

Graph summarization has attracted a lot of research interests re-cently. Various graph summarization techniques [12, 9, 13] have been proposed to help users extract and understand the information encoded in large graphs. The goal of graph summarization methods is to produce a compact and informative summary graph that un-covers the underlying topology characteristics of the original graph. For example, Figure 1(b) and Figure 1(c) show two summaries of the original graph G in Figure 1(a). The summaries themselves are also graphs. Every node in a summary, called a super-node , contains a set of nodes from the original graph. Every edge in a summary, called a super-edge , represents restrictively an all-to-all relationship between the nodes in the corresponding super-nodes. For example, in Figure 1(b), the super-edge between V means that every node in V 1 ( v 1 ) is connected to every node in V  X 
This work was done while this author was in IBM Almaden Re-search Center.
 ( v 2 and v 3 ). Notice that not all summaries are lossless. The sum-mary graph in Figure 1(b) exactly recreates the original graph, but the summary graph in Figure 1(c) reconstructs into a graph differ-ent from the original. In particular, the edge  X  v 2 , v the original graph but is missing in the summary, whereas the edge  X  v 2 , v 3  X  is spurious in the summary as it does not exist in the orig-inal graph. The total number of missing and spurious edges is the error associated a summary graph. Intuitively, a smaller summary, which is easier to visualize and understand, also tends to introduce more errors. Therefore, the challenge for graph summarization is to produce relatively small summaries while minimizing the errors.
With the skyrocketing expansion of graph data size in recent years, mining and visualizing large graphs become more and more difficult. Graph summarization can be used to help compress large graphs into more manageable size to visualize and study. Since the summaries are also graphs themselves, investigation of various graph properties can be conducted on the summary graphs instead. Although there are several existing graph summarization methods that are quite effective and efficient in producing summaries [9, 12], all of them are single-process in-memory solutions. Process-ing millions to billions of nodes and edges (with intermediate data structures) easily requires more memory than provided in a single machine and parallelization of computation for efficiency. This pa-per exactly addresses this need. To the best of our knowledge, this is the first work to study distributed graph summarization methods.
There are a number of challenges for implementing distributed graph summarization algorithms. First of all, as nodes and edges are distributed in different machines, a seemingly simple operation in the centralized graph summarization algorithm requires message passing and careful coordination across multiple node in the dis-tributed environment. Secondly, the centralized algorithm doesn X  X  need to worry about how computation is distributed, but a good dis-tributed graph summarization method should fully distribute com-putation across different machines for efficient parallelization. Last but not the least, as computation and communication costs will be the dominating factors in the distributed graph summarization al-gorithm, smart techniques are needed to avoid unnecessary com-munication and computation as much as possible.

In this paper, we proposed three distributed algorithms for large scale graph summarization, implemented on top of Apache Giraph ( giraph.apache.org ), an open source distributed graph pro-cessing platform. The first algorithm, DistGreedy, is a non-trivial adaptation of a centralized greedy algorithm. However, the greedi-ness of this algorithm requires examining all pairs of nodes with 2-hop distance, thus causes a large amount of computation and communication cost. The second algorithm, called Dist-Random, reduces the number of examined node pairs using random selec-tion. But randomness negatively affects the effectiveness of the al-gorithm. Our last algorithm, Dist-LSH, addresses the limitations of the previous two algorithms. It employs a novel technique called, Striped-MinHash, to directly pinpoint the good candidates for ex-amination, thus completely eliminate the computation and the net-work cost associated with unnecessary examinations. Through ex-periments we demonstrate the effectiveness and efficiency of the proposed algorithms. Although the proposed algorithms are evalu-ated under Apache Giraph, they are generic for other parallel pro-cessing frameworks because data and computations are partitioned into each graph summary nodes.
Graph summarization was first proposed independently by [12] and [9] in 2008. In both works, a summary graph is defined as a compact graph representing the original graph topology (see Fig-ure 1), and graph summarization is the process to construct a sum-mary graph from a given graph. The two works differ mainly in whether the nodes in the original graph are of the same type in the problem definition. In this work, we adopt the same setting as in [9], i.e., all nodes are of the same type. For ease of presentation, we consider only undirected graphs in this paper. Adapting meth-ods discussed in this paper for directed graphs is fairly straightfor-ward, hence omitted due to space limit.

Given a graph G = ( V, E ) , a summary graph for G is de-noted as S ( G ) = ( V S , E S ) . The summary S ( G ) is an aggre-gated graph, in which V S = {V 1 , V 2 ,  X  X  X  , V k } is a partition of the nodes in V ( S k i =1 V i = V and  X  i 6 = j , V i T V call each V i a super-node , representing an aggregation of a subset of the original nodes. For simplicity, we use V ( v ) to denote the super-node that an original node v belongs to. In addition, Each  X  X  i , V j  X   X  E S is called a super-edge , representing all-to-all con-nections between nodes in V i and nodes in V j . In other words,  X  v m  X  V i , v n  X  V j ,  X  v m , v n  X   X   X  X  i , V j  X  . Due to the all-to-all connection representation of super-edges, a graph summarization process may introduce information loss. Let  X  i,j denote the all-to-all connections between the two corresponding node sets V V , and A i,j represent the set of actual edges between them in the original graph. If the super-edge  X  X  i , V j  X  exists in the summary graph, then |  X  i,j |  X  | A i,j | spurious edges are introduced. Oth-erwise, | A i,j | edges are missing from the summary graph. More formally, we define the error associated with a pair of super-nodes V and V j in a summary graph as follows:
Accordingly, the total error for a summary graph S ( G ) can be
Once a user selects a summary resolution, i.e. the number of super-nodes, naturally our goal is to generate a summary graph that minimizes the total error.
 Graph Summarization Problem: Given a graph G and a desired number of super-nodes k , compute a summary graph S ( G ) with k super-nodes, such that the summary error is minimized.
 It has been proved that graph summarization is NP-hard [12]. The difficult part is determining the super-nodes V S , Once the super-nodes are decided, constructing the super-edges with minimum sum-mary error can be achieved in polynomial time.

Recall that the error between a pair of super-nodes V i and V comes from either |  X  i,j | X  X  A i,j | spurious edges, or | A edges. Accordingly, the optimal edge assignment strategy is simply adding an super-edge  X  X  i , V j  X  when | A i,j | &lt; 1 2 V and V j unconnected otherwise.

Under this super-edge assignment strategy, the connection error among each pair of super-nodes V i and V j is:
Thus, the graph summarization problem is essentially the prob-lem of determining the k super-nodes.

Centralized Algorithms. In [9], two centralized heuristic-based algorithms for graph summarization, Greedy and Random, are pro-posed. Although our graph summarization definition is slightly dif-ferent, these two algorithms are still applicable after minor changes. Below, we will briefly describe the two centralized algorithms, as they serve as the baseline for our distributed algorithms.
Both the greedy and the randomized algorithms start with a sum-mary graph initialized as the original graph, and iteratively merge a pair of super-nodes into one super-node to form a summary graph with a lower resolution, until k super-nodes remain in the final sum-mary graph. In each iteration, both algorithms choose a super-node pair that introduces low error increase. The error increase of merg-ing two super-nodes V i and V j to form one super-node V m super-node V i . In the above equation, e  X  m,  X  denotes the total con-nection error associated with the merged super-node V m , and e e j,  X   X  e  X  i,j is the total errors associated with V i and V merge. As e  X  i,j is counted twice in e  X  i,  X  + e  X  j,  X  e
The two algorithms differ in the policy of choosing which pair of nodes to merge in each iteration. The Greedy algorithm always examines all node pairs within 2-hop aways and chooses the pair with the minimum error (smallest  X  i,j ), while the Random algo-rithm first randomly picks a node and merges it with the best node in its 2-hop neighborhood.
Giraph is an open source implementation of Pregel [8] proposed by Google. It supports both iterative algorithms and vertex-to-vertex communication in a distributed graph, thus is a natural fit for us to implement distributed graph summarization algorithms. Note that although this paper only demonstrates how distributed graph summarization algorithms are implemented in Giraph, the same algorithms can be easily implemented in other similar graph processing systems, such as GraphLab [7] and Trinity [11], with minor adaption.

A typical Giraph program consists of an input step, where the graph is initialized (e.g., loading and distributing vertices to worker machines), followed by a sequence of iterations, called supersteps , separated by global synchronization barriers, and finally an output step to write down the results.
Giraph employs a vertex-centric model. Each vertex is consid-ered an independent computing unit that inherits from the prede-fined Vertex class. Each vertex has a unique id, a set of outgo-ing edges and application-dependent attributes of the vertex and its edges which are necessary for the computation. A vertex instance carries two states ( active and inactive ). In superstep i , each active vertex can receive messages sent to it by other vertices in super-step i  X  1 , query and update the information of itself and its edges, initiate graph topology mutation, communicate with global aggre-gation variables, and send messages to other vertices for the next superstep i +1 . All these computation logics are executed in a user-overridable function named C OMPUTE (). After all active vertices finish their local C OMPUTE () functions in a superstep, a global syn-chronization phase allows global data to be aggregated from each vertex X  X  submitted values, and messages created by each vertex to be delivered to their destinations. At the beginning, all vertices are active . A vertex can voluntarily deactivate itself by calling V OTE -T
H ALT () or be passively activated by some incoming messages from other vertices. The overall program terminates when every vertex votes to halt and there is no message to any vertex.
In Giraph, messaging is the major mechanism for communica-tion. By calling S END M ESSAGE ( dest, msg ) , a vertex can send a message to the destination vertex dest with the message body msg . Note that a vertex can send message(s) to any vertex (not only to its neighbors). Aggregator is a mechanism for global communication and synchronization. By calling A GGREGATE ( aggr, val ) , the ver-tex provides a value val for the global aggregator variable aggr . After each superstep, Giraph aggregates the provided values from all vertices for each aggregator variable. The aggregator variables will be available to all vertices in the next superstep.
The Giraph computing framework consists of one master and a number of workers. The master is responsible for coordinating and controlling the computation process, and each worker works on a subset of vertices and executes their computations. At start, a customizable P RE A PPLICATION () call is executed in the master to initialize application-specific global data structure (e.g. global ag-gregators). Then, each superstep starts with calling the overridable P
RE S UPERSTEP () function and ends with calling another overrid-able P OST S UPERSTEP () function in the master for updating global data structures in each superstep. When the whole job finishes, a customizable P OST A PPLICATION () function is executed in the master for finalizing the global data structure.
For distributed graph summarization, we follow the same itera-tive merging mechanism in the centralized algorithm in Section 2: starting from the original graph as the summary (each node is a super-node) and iteratively merging super-nodes until k super-nodes left. However, there are two major challenges to achieve this itera-tive merging mechanism in the Giraph distributed environment: 1. It is very easy to decide which pairs of super-nodes are good candidates for merge and perform these merge operations in a cen-tralized algorithm, since everything runs in a single process with shared memory. But in the Giraph distributed environment, all the decisions and operations have to be done in a distributed way through message passing and synchronization. 2. The centralized algorithm only merges the optimal pair of super-nodes in each iteration. And it requires N  X  k iterations to produce a summary of size k , where N is the number of nodes in the original graph. However, to fully utilize the parallelization in a distributed environment, we need to find multiple pairs of nodes to merge, and simultaneously merge them in each iteration.

The above two challenges define two crucial tasks that a dis-tributed graph summarization algorithm needs to perform in each iteration: Candidates-Find task and Merge task. The Candidates-Find task decides on the pairs of super-nodes to be merged, whereas the Merge task executes these merges.

In this paper, we propose three distributed graph summarization algorithms: DistGreedy, DistRandom and DistLSH. The three al-gorithms share the same operations in the Merge task, but differ in how merge candidates are selected. DistGreedy and DistRandom are modeled after the centralized Greedy and Random algorithms introduced in Section 2, respectively. Both suffer from significant drawbacks. To address their limitation, we propose a novel dis-tributed graph summarization algorithm, called DistLSH.
For all three algorithms, we can naturally map each super-node in the summary graph as a Giraph vertex and the neighbors of the super-node as the adjacent vertices for the Giraph vertex. The data structure for the Giraph vertex is shown in Figure 2. Each Gi-raph vertex has three attributes associated with vertices and two attributes associated with edges: Since the introduced data structure is associated to edges, the space complexity is still linear to the number of edges O ( | E | ) . Based on this data structure, we rewrite Eq. 2 as
Algorithm 1 shows an overview of the three distributed graph summarization algorithms. Each iteration of the distributed algo-rithms is processed in multiple supersteps. The first few supersteps perform the Candidates-Find task (details will be provided in Sec-tion 5), and the remaining supersteps perform the Merge task (de-tails will be provided in Section 6). We use an aggregator, called ExecutionPhase , as the global coordinator to indicate which phase of an iteration the C OMPUTE () function is currently executing in. Based on the previous value of ExecutionPhase , we can set the right value to this aggregator in the P RE S UPERSTEP function be-fore each superstep starts. Another aggregator, called ActiveNodes , is used to keep track of the number of super-nodes in the current summary. When the summary size is less or equal to the required size k , the value of the ExecutionPhase will be set to DONE. In
Algorithm 1: Distributed Graph Summarization Overview this case, in the C OMPUTE () function, every vertex will vote to halt. Then the whole program will finish.

At the end of the program, the resulting graph will be written down. Using this graph, we can derive the summary graph. For each vertex, the owner-id will tell us whether it is a super-node in the resulting summary, using the adjacent vertices with their size and conn information, we can infer whether a super-edge should exist or not (see Section 2). If the actual nodes in each super-node are also desired, again the owner-id can help us derive the member-ship information. Essentially, the id of each vertex and its owner-id form an edge in a ownership forest . The leaf nodes are the nodes from the original graph and the roots are the super-nodes in the summary. A simple connected component algorithm on this graph will compute all the memberships. The label propagation based connected component algorithm [5] can be implemented straight forwardly in Giraph. If the resulting summary has size less than k (this could happen because multiple merges happen in each it-eration), we randomly choose a number of roots in the forest and reverse the merges to get the summary of size k . However, in most practical cases, users are content enough with a summary of a size roughly equal to k . Since these post processing steps are relatively trivial, we omit the details in the interest of space.
We now explain in detail how to find pairs of super-nodes as candidates to merge in DistGreedy, DistRandom and DistLSH.
As DistGreedy is based on the centralized Greedy algorithm, it looks at super-nodes that are 2-hops away to each other and thrives to find the pairs with minimum error increase. To control the num-ber of super-node pairs to be merged in each iteration, we use a threshold called ErrorThreshold as the cutoff for which pairs qual-ify as merge candidates. More precisely, every pairs with error increase less than ErrorThreshold will become merge candidates. Initially, ErrorThreshold is 0, which means we start with merging pairs with no error increase at all. Whenever the number of merge candidates fall below 5% of the current summary size, the algo-rithm increases ErrorThreshold by a controllable parameter, called ThresholdIncrease , for the subsequent iterations.
 Figure 3: Node merge: from before-merge to after-merge.

The major task in the DistGreedy algorithm is to compute the actual error increase for each pair of 2-hop-away super-nodes. This is fairly simple in the centralized Greedy algorithm, but becomes much more complex in the distributed environment, as the informa-tion to compute the error increase is distributed in different places. As shown in Figure 3, the error increase for merging a pair of super-nodes V i and V j can be decomposed into 3 parts:
In Figure 3, we use scope to define all the information each Gi-raph vertex knows: this super-node X  X  own size , selfconn , conn to all its neighbors and the neighbors X  sizes . It is clear that the bits and pieces used to compute the total error increase for merging two super-nodes are distributed in the scopes of different Giraph vertex. In the following, we will describe in detail how to compute the dif-ferent parts of the error increase using V i and V j in Figure 3 as an example.

Computing  X  com i,j requires the error increase associated with the connections of V i and V j to all their common neighbors. For a common neighbor, say V p , the error before the merge is e (ref. Eq. 4). The error after the merge e  X  m,p = min { ( size size j )  X  size p  X  ( conn i,p + conn j,p ) , conn i,p + conn
Thus, the error increase of merging V i and V j w.r.t. common mation needed to compute  X  p i,j can be found in the scope of the common neighbor V p . As a result, the total  X  com i,j can be collec-tively computed by all the common neighbors,  X  com i,j = P
For  X  uni i,j , the computation requires only unique neighbors of each super-node. As a result, V i and V j can independently compute this part of error increase. As an example, for the unique neighbor V q in Figure 3,  X  q i,j = e  X  m,q  X  e  X  i,q . The error increase associated with V i  X  X  unique neighbors thus is  X  uni-i i,j = P V  X  node has to know who are the unique neighbors for each candidate
Algorithm 2: F IND C ANDIDATES () for DistGreedy merge partner . This requires the common neighbors to register with the two super-nodes before hand.

Computing  X  self i,j requires collaboration between V i and V tween the two super-nodes, the one with a larger id, say V its selfconn to V i . Then at V i ,  X  self i,j can be computed as  X  e size i + size j .

Lastly, all the three parts of error increase will be aggregated at the super-node with the smaller id, V i in our example. This requires messages from common neighbors for  X  com i,j and messages from V for  X  uni-j i,j and for V j .selfconn . Then V i can simply test whether the total error increase is below ErrorThreshold or not to decide on whether the two super-nodes should be merged.

Algorithm 2 shows the pseudo code for DistGreedy X  X  FindCandi-dates function. There are three phases for this function. These three phases correspond to the computation of the three different parts of error increase. In different phases, the Giraph vertex plays different roles in the computation. Again, here aggregator ExecutionPhase is to indicate which phase the current superstep is in. In the first phase, the Giraph vertex plays the role of a common neighbor, V to a potential merge candidate V i and V j . Since the neighbors of V are all two hops away from each other, all neighbor pairs are po-tential candidates to merge. As a result, V p will compute  X  all pairs of neighbors V i and V j where i 6 = j , and send  X  super-node in the pair with the smaller id, V i . It also sends a mes-sage to V i and V j to register itself as a common neighbor. In the second phase, the current Giraph vertex plays the role as one super-node in a potential merge candidate ( V i , V j ) . If it is the super-node with the smaller id, V i , then it will receive a message  X  each common neighbor V p of V i and V j . It will aggregate these values to compute  X  com i,j . At the same time, it also registers V as a common neighbor of V i and V j . Since now it knows all the common neighbors, it can infer who are the unique neighbors and compute  X  uni-i i,j . On the other hand, if the current Giraph vertex is the super-node with the larger id, V j , then it will receive a message from each common neighbor V p of V i and V j , indicating that V a common neighbor of V i and V j . As a result, the unique neighbors are known, and  X  uni-j i,j can be computed and sent to V it also send its selfconn j to V i . In the third phase, the current Gi-raph vertex plays the role of the super-node with the smaller id, V , in a potential merge candidate ( V i , V j ) . Now, V i computed  X  com i,j and  X  uni-i i,j in the second phase, it will also receive  X  i,j and selfconn j . Using selfconn j and its own information, V can compute  X  self i,j . At the end, the total error increase  X  computed and a decision on whether to merge V i and V j will be made based on ErrorThreshold .

To analyze the time complexity of this algorithm, we use d to denote the average number of neighbors of a vertex. Therefore, the average number of 2-hop away neighbors for a vertex is d In DistGreedy, the computation of all the different  X  com vertex V i is essentially a loop over all its 2-hop away neighbors. So, its time complexity is O ( d 2  X  N ) , where N is the total number of vertices. Same complexity analysis applies to the  X  self i,j phase. The  X  uni i,j computation phase iterates through each 1-hop neighbor V q to compute  X  q i,j for every 2-hop neighbor V thus has a time complexity of O ( d 3  X  N ) . Overall, DistGreedy has a time complexity of O ( d 3  X  N ) for each Candidates-Find step.
The DistGreedy algorithm described in the previous section blindly examines all super-node pairs of 2 hops away to each other to see whether they should be merged. This process incurs a large amount of computation and network messages. In order to reduce the num-ber of super-node pairs to be examined, DistRandom randomly se-lects some super-node pairs to examine. In the selection process, we want every super-node to have a chance to be merged with an-other super-node. Similar to DistGreedy, DistRandom also has the following three supersteps. 1. Every super-node randomly selects one neighbor and sends 2. The neighbor receives the message and forwards it to a ran-3. The 2-hop away neighbor receives this message and use it
On average each super-node will receive a message from one of its 2-hop neighbors. Calculating the error increase takes O ( d ) time. So, DistRandom has a time complexity of O ( d  X  N ) for one Candidates-Find step.
For a large network with millions of nodes, evaluating all possi-ble super-node pairs in 2-hop distance can be prohibitively expen-sive. As many such super-node pairs do not qualify the merge cri-teria, a lot of work is wasted. Although DistRandom can reduce the number of super-node pairs examined, by random sampling it also misses some really good merge candidates. In this section, we in-troduce another distributed graph summarization algorithm, called DistLSH. DistLSH leverages a technique called Locality Sensitive Hashing (LSH) [4] to quickly find out pairs of super-nodes which are likely to be good merges. However, as we will show later in this section, LSH cannot directly applied to our problem. Instead, we invent a novel approach, called Striped-MinHash.

LSH Background: We first provide some brief background in-formation on LSH. LSH is a method to probabilistically reduce di-mensions of high-dimension data. Using LSH, similar data items in the high-dimension space are hashed into the same buckets with high probabilities. In this study, we adopt one type of LSH, called MinHash [2]. MinHash is used to quickly estimate the similarity between two sets. For a set A and a hash function h (  X  ) that maps each element a i in A to an integer number, the MinHash of A is defined as the element a i of A with the minimum hash value of h (  X  ) . More precisely, h min ( A ) = a i s.t.  X  a j  X  A, h ( a A very nice property of MinHash is that the probability of hash col-lision of two sets A and B is exactly the Jaccard similarity [2] of the two sets: Pr( h min ( A ) = h min ( B )) = | A  X  B | / | A  X  B | .
Before understanding the intuition behind our Striped-LSH ap-proach, we first introduce a concept, called connection weight . For-mally, the connection weight between two super-nodes V i and V is defined as w i,j = | A i,j | / |  X  i,j | . With w i,j definition of e  X  i,j (ref. Eq. 2) as
The DistLSH algorithm only examines super-node pairs that are highly likely to be good merges that bring in minimal error in-creases. We observe that a good merge should satisfy the following two criteria: 1) the two super-nodes share a lot of common neigh-bors, 2) they have similar weights to each neighbor. The first cri-terion is easy to understand, in the following we explain why the second is important. Let the neighbor sets of V i and V j N , respectively. For every neighbor V p  X  N i  X  N j , we look at w i,p and w j,p . In the case when V p is only a unique neighbor to one of the super-nodes, say V i , then w j,p = 0 . When the weights of the two connections are very similar, w i,p  X  w j,p , then after merging V i and V j into V m , w m,p = and w j,p are both less than 0.5, e  X  i,p = | A i,p | , e e m,p = | A i,p | + | A j,p | . As a result, the error increase w.r.t neigh-when w i,p and w j,p are both greater than 0.5, the error increase is also 0. On the other hand, if w i,p and w j,p are very different, e.g. w i,p 0 . 5 and w j,p 0 . 5 , then w m,p will be somewhere between w i,p and w j,p . If w m,p &lt; 0 . 5 ,  X  p i,j = 2 | A otherwise,  X  p i,j = |  X  i,p | X  2 | A i,p | . In both cases,  X  fore, if for every neighbor in N i  X  N j , the connection weights of V and V j are similar, the total error increase for merging the two super-nodes will be minimal, making them a perfect merge candi-date.

We have found out what good merges look like, the next question is how to efficiently pinpoint such good merges. If we apply the MinHash approach to the neighbor set of each super-node, we can quickly find out the super-node pairs with similar neighbor sets. But this is not enough, as the important connection weights are not considered at all in MinHash. In order to address this limitation of MinHash, we propose a Striped-MinHash approach. As shown in Figure 4, Striped-MinHash takes a weighted set A = { a 1 : w 1 , ..., a i : w i , ... } as input, divides the weight range [0 , 1] into R stripes, and applies a different MinHash function to each stripe. In stripe r , the r -th MinHash function h r to a derived set A r = { a i | w i &gt; r  X  1 R } . Given two weighted sets A and B , if at strip r , they are hashed to the same bucket, then we say that A and B are a hash hit at strip r , denoted as hit r Otherwise, hit r ( A, B ) = 0 . The total number of hits for A and B is defined as hit ( A, B ) = P R r =1 hit r ( A, B ) . It is clearly to see that the more similar these two weighted sets are, the more hits they are likely to get using Striped-MinHash.
 We directly apply the Striped-MinHash approach in our distributed Dist-LSH algorithm to find out super-node pairs that have similar weighted neighbor sets (each neighbor is associated with the con-nection weight), a.k.a good merge candidates. We use a parame-ter HitsTheshold to control the merge candidates: only super-node pairs with number of hits more than HitsTheshold will be merged. Initially HitsTheshold = R , whenever the number of merge candi-dates falls below 5% of the current summary size (so that more than 95% of the current graph can not be compressed), the algorithm de-creases HitsTheshold by 1 for the subsequent iterations. Figure 5: Not only is V h the result of MinHash, but also it is the local coordinator of hash collisions in Giraph.

Applying Striped-MinHash and deciding on the merge candi-dates is carried out in a fully distributed fashion in three supersteps as shown in Algorithm 3. Again, we use the aggregator Execution-Phase to indicate what operations each superstep should perform. In the first phase (Hash Phase), each Giraph vertex performs Striped-MinHash on its super-node V i  X  X  weighted neighbor set. For each strip r , the result of the MinHash h r min is a super-node V the neighbor set. Note that MinHash is used here in a very smart way. As shown in Figure 5, MinHash doesn X  X  only tell us the hash value, but also points us to a valid super-node in the current sum-mary graph where the collection of hash collisions can be executed. V will then send a message to notify V h that the r -th MinHash is hashed to V h .

In the second phase (Collision Phase), a Giraph vertex V h receive a number of messages indicating who is hashed to V which strip. For each strip r , V h collects all the super-nodes hashed to itself. Note that every pair of super-nodes in this collection is
Algorithm 3: F IND C ANDIDATES () for Dist-LSH actually a hash collision. Suppose, in both strip r and r V j are hashed to V h . Then, V h can perform a local aggregation to count the partial number of hits between V i and V j . After that, V sends this partial count to the super-node with a smaller id among the pair, V i .

In the third phase (Aggregation Phase), super-node V i receives partial counts of hits for itself with other super-nodes, and compute the total numbers of hits. Then for each potential partner, it tests whether the number of hits is above the HitsT hreshold . We now analyze the complexity of the Candidates-Find step of DistLSH. In Hash Phase, there are R striped hashes and each re-quires iterating through the neighbor set, resulting in O ( R  X  d ) time. In Collision Phase, since each super-node sends R messages in pre-vious phase, each super-node also receives R messages on average and puts them into R buckets. In the following, we estimate that the expected number of collision pairs is ( R  X  1) N/ 2 . T HEOREM 5.1. The expected pairs of merge candidates in each FindCandidates step of DistLSH is ( R  X  1) N/ 2 .

P ROOF . We assume that super-nodes fall into R buckets with uniform distribution. Therefore, the number of nodes in one bucket (denoted as t ) follows the binomial distribution Pr( t )  X  B ( n = R, p = 1 /R ) . A bucket having t nodes will generate t  X  ( t  X  1) / 2 pairs of merge candidates, the expected number of total pairs for E ( t )  X  1) &lt; (1  X  1 /R ) / 2 = ( R  X  1) / 2 R and we have R buckets in total, so there are ( R  X  1) / 2 total pairs of merge candidate. For all vertices, there are ( R  X  1) N/ 2 pairs of merge candidates.
Based on Theorem 5.1, the time complexity of Collision Phase is O ( R/ 2  X  N ) . Finally, in the Aggregate Phase each node will receive R/ 2 hit messages on average. The overall time complexity of DistLSH is O (( dR + R )  X  N ) in each Candidates-Find step.
After the Candidates-Find task, we obtain a set of super-node pairs to be merged. In this section, we provide details on how to merge these super-nodes distributedly. For every vertex merge, in-stead of creating a new merged super-node, we always reuse the super-node with the smaller id as the merged super-node. Specif-ically, the super-node with larger id shall set its owner-id to the merged super-node, and call V OTE T O H ALT () to turn itself to inac-tive.

As discussed in the previous section, we know that the decision on whether to merge super-nodes V i and V j is always made at the super-node with the smaller id, V i . Therefore, at the beginning of the Merge task, V i already knows which other super-node(s) will be merged into it. However, the tricky issue is that there could be another merge decision that requires V i merged into V g . In this case, V j should be eventually merged into V g .To efficiently merge multiple super-node pairs distributedly, we introduce a repeatable merge decision propagation phase to ensure all the super-nodes know whom they eventually should be merge into. This design decision is essential to save overall supersteps and messages, since vertex id is much cheaper to propagate than real vertex data.
In Decision Propagation Phase , we add a Logical And aggrega-tor called PropagationDone to detect whether there are un-propagated merge decisions. Before each superstep, this aggregator is initial-ized to TRUE. During the execution of the superstep, if any vertex thinks propagation should be continued, it will aggregate FALSE to PropagationDone , which will result in FALSE at the end of the superstep. If PropagationDone is FALSE, the next superstep will continue the merge decision propagation until all the super-nodes know their eventual merge destination.

Taking the above merge case as an example. In the first super-step, the super-node with the smaller id in a merge candidate will send the merge decision to the other super-node. In our case, V will notify V j and V g will notify V i . In the second superstep, V receives a message from V i and changes its owner-id to V ilarly, super-node V i receives a message from V g and changes its owner-id to V g . However, V i remembers that it has sent a merge decision to V j , so it needs to propagate the new owner V Because of this, V i aggregates FALSE to PropagationDone and the propagation continues in the next superstep. In the third super-step, V j finally receives the new merge destination V g and updates its owner-id to V g . At the end of this superstep, every super-node agrees on the fact that all the merge decisions are propagated and we are ready to actually merge the super-nodes.

Recall that in the vertex data structure, each super-node keeps its neighbor X  X  basic information ( nbr.size , nbr.conn ). In Connec-tion Switch Phase , each super-nodes to be merged shall notify its neighbors to update this neighbor information. For example, V sends out a connection switch message to every neighbor to no-tify them to alter connection destination from V i to V g ensures that when V i turns itself to inactive, its neighbor X  X  connec-tions are not corrupted.
Algorithm 4: M ERGE ()
In Connection Merge Phase , receivers of the connection switch messages shall update their neighbor list with the new neighbor ids. In addition, each super-node that needs to be merged will also send its self-data to merge owner, including self.size , self.conn , all neighbor X  X  nbr.size s and nbr.conn s.

Finally, in the State Update Phase , each merge owner receives the information from all the super-nodes that needs to be merged into it, and performs the actual merge by updating all the attributes in the local data structure. At the end of this phase, it also sends out a message to every neighbor with its new nbr.size information.
Optimization for Hub Nodes. We found out that many real graph data sets exist a small number hub nodes. These hub nodes often connect to a large number of low degree nodes. In many cases, these nodes only connect to the hub nodes and have degree of one. These hub nodes create computation imbalance problems for both DistGreedy and DistLSH algorithms. In DistGreedy, a hub node results in a huge number of neighbor pairs to be examined for merge candidacy; whereas in DistLSH, the large number of one-degree neighbors of a hub node will create a large amount of hash collisions gathered at the hub node. To solve the problem, it is fairly easy to see that all the 1-degree nodes of the hub node can be safely merged into one super-node without introducing any error increase. In fact, for all of our distributed algorithms, we first in-troduce 3 additional supersteps to merge all the 1-degree neighbors of hub nodes, then execute the corresponding algorithms. This op-timization for hub nodes significantly reduces the computation and network overhead. In all of our experiments in Section 7, the algo-rithms tested include this special optimization.
We now conduct experiments with various distributed graph sum-marization implementations on a cluster of 16-node IBM SystemX iDataPlex dx340. Each server consisted of two quad-core Intel Xeon E5540 64-bit 2.8GHz processors, 32GB RAM, and intercon-nected using 1GB Ethernet. Each server ran Ubuntu Linux (kernel version 2.6.32-24), Java 1.6, and Giraph trunk version downloaded in June 2012. Each server was configured to run up to 6 workers concurrently. The following configuration parameters were set in order to boost performance: Giraph check-pointing is turned off, -Dgiraph.useNetty=true is turned on in Giraph to use Netty for mes-sage passing, and a maximum of 3GB JVM heap space was used per worker. All experiments were repeated 3 or more times. We report the average of those measurements.

We designed the experiments with the following objectives: First, we want to find out whether our distributed algorithms strike a good balance between effectiveness and efficiency. Second, we want to study the effect of various parameter settings in our algorithms. Third, we want to evaluate the scalability of our algorithms, with in-creasing graph sizes, decreasing summary sizes, and different num-ber of workers.

We used both real datasets and synthetic datasets in our experi-ments. The three real datasets used are: the Enron email network, the As-Skitter Internet trace graph (both from snap.stanford. edu/data ), and the Gowalla location based social network [6]. Table 1 summarizes their basic statistics. We also use the R-MAT model [3] in the GTgraph suites [1] to generate synthetic graphs with power-law degree distributions and small-world characteris-tics. We set the average node degree in each synthetic graph to 5, and used the default values for the other parameters in the genera-tor. With synthetic graphs, we can freely examine the scalability of our distributed techniques across controlled graph sizes.
In all our experiments, we measured the quality of a summary graph using the errors introduced by summarization (see Section 2), and the efficiency of a graph summarization method using execu-tion time. The execution time includes the time for special handling of hub nodes, but excludes the time for the post processing step de-scribed in Section 4.
We first compare the summary qualities produced by the three distributed algorithms as well as their performance. As there is no prior art on distributed graph summarization, we use the summary qualities of the centralized Greedy algorithm as a standard to evalu-ate the effectiveness of the distributed algorithms. However, as the centralize algorithm cannot handle large scaled graphs, we are only able to apply it to the smallest Enron dataset.

Figure 6 and 7 collectively demonstrate the graph summary qual-ities and running time of the three distributed algorithm on the three real datasets, with different summary sizes. For DistLSH, the num-ber of stripes ( R ) in Striped-MinHash is set to be 50.
First of all, as shown in Figure 6(a), DistGreedy consistently pro-duces summaries with very comparable qualities (less than 9% dif-ference) to the centralized greedy algorithm. As a result, for large datasets, we can use DistGreedy as the standard for quality com-parison. In addition, DistLSH only introduces less than 20% more errors than Greedy (ref. Figure 6(a)). Compared to the centralized Greedy algorithm, the additional errors incurred by DistGreedy are due to multiple merges in each iteration, and the extra errors in DistLSH are caused by both the multiple merges as well as the candidates finding heuristic.

As Figure 6 shows, with the largest summary sizes ( 70%  X  80% of original graph size), all graphs can be summarized with small number of errors as it is easy to find many vertices with similar adjacent neighbors at the beginning. As summaries become more compact, qualities of summaries degrade. One may wonder why DistLSH performs slightly worse than other algorithms on the En-ron dataset when the summary size is 27 K , around 73% of the orig-inal graph. This is because the graph is quite small. The error gen-erated by the centralized Greedy algorithm at this stage is only 10 . Therefore, in this extreme case, several sub-optimal node merges in DistLSH algorithm may deteriorate summary quality. But when further summarizing the graph into more compact summaries, the DistLSH X  X  accidental sub-optimal merges won X  X  affect the overall summary quality much.

Figure 6 indicates that DistLSH consistently produces only slightly larger (no more than 30%) summary errors than DistGreedy, and Figure 7 shows that DistLSH is 2  X  11 times faster than Dist-Greedy under all settings. DistLSH strikes a good balance be-tween effectiveness and efficiency in graph summarization: 1) it consistently produces summaries with comparable qualities to Dist-Greedy or even centralized Greedy; 2) DistLSH is much faster than the other distributed algorithms.

Among the distributed algorithms, DistRandom is neither effec-tive nor efficient in most experimental settings. DistRandom only appears to be more efficient than the other two in the case of large summary sizes ( 70%  X  80% of original graph size). This is be-cause at the beginning of summarization, there exist a large number of good potential merges. Even with random selection, the chance of getting a good merge is still high. However, as the summary be-comes more and more compact, DistRandom results in a lot of bad choices not satisfying the merge criteria. As a result, DistRandom usually requires a lot more iterations to produce a summary with desired size.
In DistLSH, the number of stripes R in Striped-MinHash is a pa-rameter directly affecting the effectiveness and efficiency. When R is 1, DistLSH degrades to an algorithm that finds the merge candi-dates simply based on MinHash, at the price of losing all connec-tion weight information. However, although a larger R can capture the weight information better, it requires more computation. Fig-ure 8 illustrates how a larger R reduces the graph summarization errors. It seems that 50 is a good knee point that can reduce the error significantly without much sacrifice in performance. In the interest of space, we do not display the execution time of DistLSH with different R , as the running time follows a pattern of linear increase with R .
We test the scalability of our distributed algorithms on a num-ber of synthetic graphs. Figure 9 shows the execution time of dis-tributed algorithms with increasing graph sizes from 1 million to 100 million nodes, using a fixed number of 80 workers. The tar-get graph summary size is set as 0 . 1% of the original graph. Here the increase from around 600 seconds to roughly 6,000 seconds in DistLSH demonstrates its sub-linear runtime increase with graph sizes. Unfortunately, the largest graph sizes that DistGreedy and DistRandom can process in a reasonable amount of time (less than 2 hours) are 4 millions and 1 millions, respectively. With these rel-atively small graph datasets, we can still see that DistLSH is much faster than DistGreedy and DistRandom.

To show how our distributed algorithms scale with the number of workers, Figure 10 presents the runtime for a synthetic graph with 1 million vertices, when the number of Giraph workers varies from 20 to 80. Here the drop from 2 , 174 seconds to 636 seconds using 4 times as many as workers represents a speedup of about 3 . 4 . Due to the synchronization cost between supersteps, the ideal speedup 4 is impossible to achieve. 3 . 4 is already a very decent speedup ratio.
Graph summarization is a relatively new concept in graph analy-sis. It was proposed independently by [12] and [9] in 2008. Despite the differences in these two works, both employ a same fundamen-tal summary model, in which a summary itself is a graph, more compact yet informative. The nodes in the summary graph repre-10K and 50K respectively.
Figure 9: Scale with input size. Figure 10: Scale with cluster size. sent groups of nodes from the original graph, and edges in the sum-mary describe the relationships between groups of nodes. In [9], besides the summary graph, a set of edge corrections is also main-tained, so that the original graph can be reconstructed from sum-mary graph by applying the edge corrections. Based on Rissanen X  X  Minimum Description Length (MDL) principle [10], the authors in [9] formulated the graph compression problem into an optimiza-tion problem, which minimizes the sum of the size of the summary graph and the size of the edge correction set. The graph summa-rization model in [12] incorporates attributes associated with nodes and different types of edges besides the normal graph structure. As a result, the graph summarization method in [12] is able to let users select node attributes and edge types of interests and produce sum-maries with desired resolutions. A subsequent work [13] addressed two limitations of [12], namely the issues of a large number of at-tributes being selected and attributes with large value ranges. How-ever, none of the existing works addresses the issue of large scale graph summarization in a distributed environment.
This paper introduces three distributed algorithms for large-scale graph summarization on the Giraph distributed computing frame-work. Among them, the DistLSH algorithm applies the novel Striped-MinHash technique to linearly pinpoint the set of possible merge candidates, and achieves a nice balance between effectiveness and efficiency. Scalability testing on large synthetic graphs indicates that DistLSH scales nicely with graph sizes and cluster sizes. As DistLSH has been shown a practical algorithm for large-scale graph summarization on real email/Internet/social networks data, in the future work, we plan to mine the graph patterns from the sum-maries produced by DistLSH, and study the correlation between graph patterns and summary sizes.
