 Story segmentation is an interesting task aiming at partitioning a text, audio or video stream into a sequence of topically coherent segments known as stories. The increasing availability of multimedia data is fostering a new wave of seman-tic access to the media content. Story segmentation is an important prerequisite since various tasks, e.g., topic tracking, summarization, information extraction, indexing and retrieval, usually assume the presence of individual topical  X  X oc-uments X . Specifically, for a broadcast ne ws (BN) retrieval system, users expect short clips of relevant news stories rath er than an entire news stream in response to their specific queries. However, manual segmentation requires annotators to go through the whole stream, which costs tremendous labor. As the exponential proliferation of multimedia content on the Internet, automatic story segmenta-tion techniques are highly in demand and this need will continue to rise.
Previous efforts on automatic story segmentation focus on three categories of cues: (1) video cues such as anchor face and frame similarity [1], (2) acous-tic/prosodic cues such as significant pauses and pitch resets from audio [1,2], and (3) lexical cues from text transcrip ts. Compared to other two types of cues, lexical cues are more popular since it works on texts and speech recognition transcripts of multimedia sources. Main lexical approaches include word cohe-siveness, e.g. TextTiling [3], the use of cue phrases [4] and statistical modeling. TextTiling is based on a straightforward observation that different topics usually employ different sets of words. As a result, pairwise similarity measure between consecutive sentences can be used across t he text, and a local similarity minimum implies a possible topic shift.

These pairwise-similarity-based approaches achieve satisfying performance when the documents have sharp variations in lexical distribution, such as syn-thetic collections by concatenation of random texts [5]. However, in real-world collections, e.g. broadca st news and spoken lectures, transitions between top-ics are usually smooth and dis tributional varia tions are subtle. Recently, graph-based algorithms have drawn much attention for modeling real-world discourses 1 . In many natural language processing applications, entities can be naturally rep-resented as nodes in a graph and relations between them can be represented as edges. Research has shown that graph-based representations of linguistic units, as diverse as words, sentences and documents, give rise to novel and efficient solutions in a variety of tasks. Malioutov et al. [6] proposed a minimum cut (Min-cut) approach for spoken lecture s egmentation. They abstracted a text into a weighted undirected graph, where the nodes correspond to sentences and edges represent the pairwise senten ce similarities. The segmentation task thus shifts to a graph partitioning problem that optimizes the normalized cuts (N-cut) criterion [7], where the similarity within each partition and the dis-similarity across different partitions are both considered. This approach takes into account long-range changes in lexical distribution and outperforms the state-of-the-art similarity-based segmentation, such as the one developed by Choi et al. [8].

One of the major challenges of story segmentation on multimedia documents (e.g. BN) is that story boundaries have to be detected on inaccurate texts tran-scribed from audio via a large vocabulary continuous speech recognizer (LVCSR). The inevitable speech recognition errors pose significant difficulties in lexical-based story segmentation since noisy texts break lexical cohesion. Speech recog-nition errors result from adverse acoustic conditions, diverse speaking styles, and absence of in-domain vocabulary. The existence of out-of-vocabulary (OOV) words (i.e., words outside the vocabulary of the speech recognizer) is more com-mon for Chinese than other languages such as English. Chinese OOV words are largely named entities (e.g. Chinese person names and transliterated foreign names) that are keys to topic discrimi nation. Recently, the partial matching merit of subword lexical units [9], has been successfully applied to TextTiling-based automatic story segmentation of Chinese BN [10,11]. At the subword level, the incorrectly recognized words may in clude several subword units correctly rec-ognized and thus recover lexical cohesion in noisy transcripts.

In this paper, we propose a subword N-cut approach to automatic story seg-mentation of Chinese broadcast news. Graph-based segmentation is conducted on character and syllable sequences of Chinese LVCSR transcripts. We find that employing N-cut framework at the subword levels is more effective than the word level due to the partial matching merit of subwords in Chinese BN. The proposed approach demonstrates the potential of graph cut approach in story segmentation. Moreover, the effectiven ess of the graph cut approach deserves further investigation since broadcast news has a different genre with lectures. For instance, story segmentation in BN punctuates a news program into distinct topic units while spoken lecture segmentation probes sub-topic changes within a lecture that has a unique central topic. We perform a data-oriented study with the standard TDT2 Mandarin BN corpus that contains about 53 hours of VOA Mandarin Chinese BN audio 2 . The 177 audio recordings are accompanied with manually annotated meta-data (includ-ing story boundaries) and word-level speech recognition transcripts. The TDT2 audio was transcribed by the Dragon LVCSR with word, character and base syllable error rates of 37%, 20% and 15%, respectively. We adopt a home-grown Pinyin lexicon to get the syllable sequences of words. We separate the corpus into two non-overlapping parts: a development set of 90 recordings for parameter tuning and a set of 87 for story segmentation testing. According to TDT2, a de-tected story boundary is considered correct if it lies within a 15-second tolerant window on each side of a manually-annotated reference boundary. Lexical-based story segmentation holds the view that the inter-sentence simi-larity tends to be high within the same story and a inter-story boundary tends to exhibit a low similarity. Fig. 1 illustrates the text Dotplotting [6] of a broad-cast news transcript from the TDT2 corpus introduced in Section 2. This figure plots the cosine similarity scores betw een every pair of sentences in the text. A  X  X entence X  is defined as a fixed number of consecutive non-overlapping terms (e.g. words) in the transcript. This is because: 1) real sentence boundaries are not readily available in the speech recogn ition transcripts and sentence segmen-tation is another challenging task that is out of the focus of this paper; 2) the number of shared terms between two long sentences and between a long and a short sentence would probably yield incomparable similarity scores [10]. The op-timal length of the sentence is tuned on the training set. The intensity of a pixel ( i, j ) reflects the degree to which the i th sentence in the text is similar to the j th sentence. As shown in Fig. 1, lower intens ity means higher similarity. The verti-cal red lines denotes the manually annotated story boundaries. From this figure, we can clearly observe that the difference in similarity between intra-story re-gions and inter-story boundaries. Lexical approaches, such as TextTiling [3] and lexical chaining [12] are all based on this observation.

In the next, we present a lexical approach that regards broadcast news seg-mentation as a graph-partitioning task aiming at minimizing the normalized-cut criterion. Norm alized-cut has been successfully used in image segmentation [7]. Recently, Malioutov et al. [6] have introduced it to spoken lecture segmentation and have achieved superior performance over conventional lexical approaches. In this paper, we show that it can be used at the subword level to push forward the state-of-the-art of automatic story segmentation of Chinese BN. 3.1 Graph Representation of Text We depict a text into a weighted undirected graph G =( V , E ), where the set of nodes V corresponds to sentences and E is the set of weighted edges between each pair of nodes, as shown in Fig. 2 (a). The weight of a edge, w ( i, j ), define a similarity measure between sentences s i and s j , where higher scores indicate higher similarities. Fig. 2 (a) considers long-term similarities between nodes by constructing a fully connected graph. However, considering all pairwise relations may be problematic and a cutoff should be performed, i.e., edges exceeding a certain threshold distance is discarded [6]. Fig. 2 (b) shows a graph representation which discards edges between sentences whose distance exceed two. As shown later in Fig. 5, we empirically observe that an appropriate cutoff is especially important for story segmentation in BN. In a BN program, stories related to the same topic (i.e., a breaking news) may often re-occur for several times. Fig. 3 shows a sentence similarity dotplot of a BN transcript from the TDT2 corpus. The stories focusing on the same topic are reported at the beginning, middle and the end of the program, leading to high sent ence similarities between these stories (i.e., dark points in the rectangles in Fig. 3). Using fully connected graph, these re-occurring topics may implicitly decrea se the inter-class distance between the re-occurred topics and inbetweening topic s. Therefore, a fully connected graph may be useful to a topic tracking task, while an appropriate cutoff is essential and more applicable to a news story segmentation task. 3.2 Similarity Measure The weight of edges w ( i, j ) in the graph denotes a similarity measure between sentences s i and s j . Cosine similarity is usually used in story segmentation, which is defined by the cosine of the angle between two sentences 3 : where v i and v j are the term frequency vectors for sentences s i and s j , respec-tively. v t,i is the t th element of v i , i.e., the term frequency of word t registered in the vocabulary (with size of T ).

We further use similarity score smoothing to avoid possible story boundary false alarms caused by temporal low similarity points within the same story. Although sentence similarities incline to remain high within the same news story and low at inter-story borders, the individual similarity scores can be highly variable due to rigid matching on term repetitions in similarity computation. For example, a sudden very low score in the neighborhood of a sentence (may caused by very infrequent lexical terms, u se of synonyms instead of repetitions, etc.) within a story may induce a story tra nsition false alarm. Therefore, we use smoothed term frequency vectors achieved by exponentially weighted moving average. We add counts of words that occur in m adjacent sentences to the current sentence term frequency vector, i.e., where  X  is used to control the degree of smoothing. 3.3 Normalized Cut Criterion and Dynamic Programming Solution The graph partitioning task can be simply defined as dividing the graph G =( V , E ) into two disjoint partitions, P 1 and P 2 ,where P 1  X  X  2 = V and P dissimilarity between the two partitions is defined as the cut , which is the sum of the weights of the crossing (remove d) edges between the two partitions: The optimal partitioning of a graph is the one that minimizes the cut value, which is called Min-cut . Regarding our story segmentation work, Min-cut is to split the sentences of a text into two maximally dissimilar classes, i.e., by choosing  X  P 1 and  X  P 2 to minimize: The Min-cut criterion only can make sure that similarity between the two par-titions is minimized. It thus favors cutting small sets of isolated nodes in the graph [7]. To avoid partitioning out small sets of nodes, Shi et al. [7] have pro-posed normalized cut (N-cut). N-cut ensures the two partitions are themselves homogeneous by accounting for intra-partition similarity. It computes the nor-malized cut cost as a fraction of the total edge connections to all the nodes in the graph: where assoc ( P 1 , V )= u  X  X  shown that, by minimizing Eq. (5), we can simultaneously minimize the similarity across different partitions and maximize the similarity within each partition [7].
N-cut based story segmentation is to p artition  X  X he bag of sentences X  into K disjoint sets A 1 ,A 2 , ... ,A K , so that the similarity among the vertices in a set A i is maximized and across different sets A i ,A j the similarity is minimized. The K -way N-cut criterion [7] is adopted:
Ncut K ( V )= where { A 1 ... A K } forms a partition of the graph, and V X  A K is the rest set of the entire graph except partition K .

The problem of minimizing normali zed cuts on a graph is a NP-complete task [7]. However, finding story boundaries is a linear problem, i.e., all of the nodes (sentences) between the leftmos t and the rightmost nodes of a partic-ular partition should belong to the that partition (story). With this linearity constraint, a dynamic programming (DP) solution [6] can be used to exactly minimize the N-cut cost in polynomial time:
B [ i, n ] = argmin where C [ i, n ] is the N-cut segmentation of the first n sentences into i segments. The i th segment, A j,n ,beginsatnode s j and ends at node s k . B [ i, n ]isthe back-pointer table that is used to recover the optimal sequence of the segment boundaries. Eq. (9) and (10) indicate the initial condition, i.e. the N-cut value of the trivial segmentation of a text into one segment and the first segment starts with the first node. The time complexity of the DP solution is O ( KN 2 ), where K is the number of partitions and N is the number of nodes in the graph. 4.1 Merits of Use of Subwords in Chinese Lexical-based story segmentation approaches usually involve word matching, e.g., word frequency count in similarity measure [3], [6] and linking word repetitions in lexical chaining [12]. However, the inevitable speech recognition errors may induce severe word matching failures, resulting in incorrect lexical similarity measures. However, the recognition error rates at subword levels are much lower than the word level, as can be seen from Section 2. At subword levels, we can conduct partial matching and this will partially recover the relations among words. This partial matching merit is especially important for Chinese LVCSR transcripts.
Chinese is highly different from western languages such as English both in written and spoken forms. A Chinese word is formed by one to several compo-nent characters, and there is no space be tween words serving as word delimiters in a Chinese text. In fact,  X  X ord X  is not defined explicitly in Chinese and word segmentation is definitely not unique. As a result, the same string of characters may be segmented into different word sequences in different places in a LVCSR transcript. In some cases, even more than one word sequences are both syntac-tically valid and semantically meaningful. For example, the word (North Korea) is segmented to (North) and (Korea) and they both occur in a news story in the TDT2 Mandarin corpus. In this case, it is impossible to relate them by rigid word matching. The high flexibility of Chinese word segmenta-tion is easy to cause word matching failures. However, the above problem can be solved by character matching because d ifferent segmentations still share the same component characters.

A Chinese character is pronounced as a tonal syllable. In Mandarin, about 1200 phonologically allowed tonal syllables correspond to over 6500 commonly used simplified Chinese characters. When tones are disregarded, the number is reduced to only about 400, known as base syllables. This indicates that there arealargenumberof homophones sharing the same base syllable. Tones are often mis-recognized by the speech rec ognizer, which contributes a lot to the recognition errors. In Chinese LVCSR transcripts, it is common that a word is substituted by another character sequence with the same or similar pronunci-ations, in which homophone characters are the probable substitutions. Table 1 shows some word matching failures due to speech recognition errors. Rigid word matching cannot link the original word and their substitutions together. How-ever, matching at subword levels can recover their connections.

Flexible word-building in Chinese makes the limited Chinese characters to produce unlimited words. Hence, ther e does not exist a commonly accepted Chi-nese lexicon. Consequently, the OOV problem is more pronounced in Chinese LVCSR transcripts, especially in the BN program that focuses on timely events. Many OOV words in BN are named entities (NE). An OOV word appeared in different places of a spoken document may share part of the characters or be substituted by several totally different character strings with the same (or par-tially same) syllable sequence. For exa mple, foreign proper names are common OOV words in Chinese spoken documents as they are transliterated to Chinese character sequences based on the pronunciations (i.e. phonetic transliteration). As a result, speech recognizer may return d ifferent character sequences with the same or similar pronunciations, probably their homophones. Matching at syllable level can recover these highly-topic-related OOV words due to partial matching. Some examples are shown in Table 2. 4.2 Subword N-Cut Motivated by the merits of subwords in lexical matching in Chinese BN tran-scripts, we propose a N-cut story segmentation approach on different Chinese subword representations, i.e., character and syllable n-gram units. Given a sen-tence composed of a sequence of words { w 1 w 2 w 3 ... w Q } and the sequence of their component characters or syllables { c 1 c 2 c 3 ... c L } , the overlapping subword n-gram is defined in Table 3. Higher order subword overlapping n-grams ( n  X  4) can be formed accordingly. Overlap bet ween subwords is used to reduce the pos-sibility of missing any useful information embedded in the subword sequence.
For the same LVCSR transcript of a broadcast news program, we observe that the syllable-bigram-based dotplot is clearer than the word-based dotplot, as illustrated in Fig. 4. It means syllable-bigram-based inter-sentence similar-ity decreases across differen t partitions and increase within partitions, leading to clearer story boundaries. Based on this observation, we propose to perform subword N-cut on the subword n-gram representation of sentences (Table 1) according to the N-cut pro cedure described in Sectio n 3. In the similarity com-putation defined in Eq. (1), v t,i denotes the term frequency of the corresponding overlapping n-gram unit t in sentence s i . We carried out story segmentation experiments to evaluate the proposed sub-word N-cut approach with several state-of-the-art lexical-based approaches, i.e., (1) word-based TextTiling [3], (2) subwor d-based TextTiling [10], (3) word-LSA-based TextTiling [8], (4) subword-LSA-ba sed TextTiling [11] and (5) word-based N-cut [6]. Recall, precision and their harmonic mean, i.e. F1-measure, are used as evaluation criteria. Empirical paramete r tuning was first performed on the devel-opment set of TDT2, which selects parameters achieving the best F1-measure of story segmentation. Empirical parameter s include the sentence length (i.e. word block length), the cutoff value,  X  and m for similarity smoothing in Eq. (2).
Fig. 5 shows the F1-measure curve of syllable-bigram-based N-cut with differ-ent graph cutoff values. This plot reveals the difference between broadcast news and spoken lectures. Malioutov et al. [6] pointed out that taking into account long-distance lexical dependencies yield s substantial gains in segmentation per-formance. In their spoken lecture segme ntation work, they have achieved the best performance with edge cutoff threshold at 100 and 200 sentences. In our broadcast news segmentation, the optimal cutoff values for all testing word and subword N-cuts are around 15 X 35 (15 X 20 for syllable-bigram, as show in Fig. 5). Small cutoff values accord with our explanation in Section 3.1: stories with the same topic often re-occur in a news program and considering long-distance sim-ilarity (i.e., a large cutoff value) can be harmful to the news segmentation task. From Fig. 5, we also observe an interesting bi-peak phenomenon. This is in ac-cordance with the short and long stories. A broadcast news program (e.g. VOA, the source of TDT2) is usually composed of brief news reporting headlines and detailed news that focus on intensive reports on news events. The first peak with small cutoff (around 15 X 20) fits the short brief news and the second peak with big cutoff (around 120 X 140) fits the long detailed news. The highest F1-measure is achieved by a small cutoff is due to the large number of brief news in TDT2 (over 3/5 of the corpus).
 The experimental results on the test s et are summarized in Fig. 6 and Fig. 7. Results indicate that the syllable-bigram-based N-cut achieves the best F1-measure of 0.6911 with relative improvement of 11.5% over Malioutov X  X  word-based N-cut(F1=0.6197) [6] and 4.74% over the character-bigram-LSA-based TextTiling (F1=0.6598) [11]. In general, s yllable/character unigram and bigram N-cuts show superior performance, wh ile trigram and 4-gram N-cuts present inferior performance. This is mainly due to the fact that (1) the probability of long sequences with correctly recognized characters/syllables is smaller than short character/syllable units; (2) the most frequently used words in Chinese are bi-character. Trigram and 4-gram LSA based TextTiling achieves better F1-measure as compared with other trigram and 4-gram approaches because of LSA X  X  noise-removal feature [11].
 This paper has modeled Chinese BN segmentation as a graph partitioning task under N-cut criterion that aims to simultaneously optimize the sentence simi-larity within each story and dissimilarity across different stories. Motivated by the robustness of subword units in partial matching of Chinese words, we have proposed to perform N-cut on character/syllable overlapping n-grams of noisy broadcast news transcripts with speech recognition errors. We conclude from the experiments that the proposed method can effectively improve the automatic story segmentation performance on Chinese BN. Syllable-bigram-based N-cut achieves the best F1-measure of 0.6911 with relative improvement of 11.5% over Malioutov X  X  word-based N-cut that has an F1-measure of 0.6197.

Current K-way N-cut approach cannot automatically determine the granular-ity of a resultant segmentation. The number of stories ( K )hastobesetapriori. We plan to introduce the graduated graph cuts (GGC) approach [13] to BN seg-mentation, which can automatically determine the optimal number of partitions and keep the best granularity of the whole segmentation. Since latent semantic analysis (LSA) has the merit of concept matching, we also plan to introduce LSA to subword-based N-cut to achieve more robustness in story segmentation of noisy speech recognition transcripts.
 This work was supported by the National Natural Science Foundation of China (60802085 and 60872145), the Research Fund for the Doctoral Program of Higher Education in China (20070699015), the Cultivation Fund of the Key Scientific and Technical Innovation Project, Ministry of Education of China (No.708085) and the NPU Foundation for F undamental Research (W018103).

