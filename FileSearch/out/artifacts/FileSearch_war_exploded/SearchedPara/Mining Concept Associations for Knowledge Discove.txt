 automatic discovery of knowledge. The main theme of our paper is based on the parts X , which means a document collection often, reveals interesting information other facts, propositions or hypotheses can be formed by using some techniques to discover previously unknown logic connections among the existing information we have. concepts across documents. A traditional search involving, for example, two person this, the search results in documents containing one of the names. This research across documents that connect these two individuals? For example, both may be information is gleaned from multiple documents. A generalization of this task involves query terms representing general concepts (e.g. airplane crash, criminal prosecution). We refer to this type of query as a concept chain query, a special case of Abdelgani and Mohammed Saleh in the corpus. The connection is through the concept textual distance between the two concepts in question. 
The remainder of the paper is organized as follows. Section 2 discusses related introduces semantic approach using graph-ba sed retrieval model. Section 5 describes the experiments based on processing the 9-11 corpus and is followed by conclusions. Much of the work in hypotheses generation makes use of an idea originated by Swanson in the 1980s called the  X  X omplementary structures in disjointed literatures X  (CSD). Swanson realized that large databases of scientific literature could allow discoveries to be made by connecting concepts using logical inference. He proposed a simple  X  X  influences B, and B influences C, therefore A may influence C X  model for detecting instances of CSD that is commonly referred as Swanson X  X  ABC model [6]. Using this technique, he found a connection implying patient benefit between fish oil was real. Since their pioneering contributions this kind of knowledge discovery work has attracted the attention of other researchers. Gordon and Lindsay were among the Srinivasan has used this approach to demonstrate the feasibility of her approach based on MeSH terms and UMLS semantic types and presented open and closed text mining 
Mohammed Saleh , who provided fuel from his Yonkers gas station to make bombs, obtained legal permanent residency by marrying an American. Ibrahim Ilgabrowny passed messages between conspirators and obtained five fraudulent 
Nicaraguan passports for his cousin, El Sayyid Nosair, and his family. Nosair, convicted of conspiracy, married an American in 1982 and became a citizen in 1989. He was also convicted of a gun charge in the killing of Rabbi Meir Kahane in 1990. Amir Abdelgani picked up fuel and helped determine targets; he, too, was married to an American. algorithm that are built within the discovery framework established by Swanson and algorithm and we extended this technique [2, 3, 4]. 
There has been work on discovering connections between concepts across documents using social network graphs, where nodes represent documents and links represent connections between documents. However much of the work on social network analysis has focused on special problems, such as detecting communities [8]. [9] is the work which is close to the problem being presented here. The authors model the problem of detecting associations between people as finding a connection subgraph and present a solution based on electricity analogues. However there are several differences which should be noted. The most notable difference is the reliance on URL links to establish connections between documents. Our approach extracts associations based on content (textual) analysis. Second, the connection subgraph approach does not. Finally, the connection subgraph solution only addresses named entities whereas this approach extends to general concepts. Concept extraction involves running an information extraction engine, Semantex [1] on the corpus. Semantex tags named entities, common relationships associated with person and organization, as well as providing subject-verb-object (SVO) relationships. We extract as concepts all named entities, as well as any noun phrases participating in SVO relationships. All named entity instances are retained as instances of their Action . In the following sections, we use semantic type to denote named entity concept category. 3.1 Concept Profiles topic . We build topic profiles by first identifying a relevant subset of documents from the text collection, then identify characteristic concepts (single words and/or phrases) with the topic in the sentence level. The pr ofiles are weighted vectors of concepts as shown below for a topic T j : concept dictionary. 3.2 Employing Semantic Types in Profiles Up to now our profiles are simply vectors of weighted concepts. Now we describe how to further differentiate between the concepts using semantic types. Basically concepts are separated by semantic type an d concept weights are computed within the context of a semantic type. This result is a vector of concept vectors, one for each of semantic types. Thus, computed weight for m x,y . To calculate weight, we use a variation of TF*IDF weighting scheme and then normalize the weights: Where d=1,  X  X  X  , l and u j,x,y = n j,x,y  X  log(N/n x,y ).

Here N is the number of documents in the collection, n x,y is the number of documents in which m x,y occurs and n j,x,y is the number of retrieved documents for T j ( u concepts in the domain for semantic type x ). 
Table 1 illustrates a portion of the concept profile that is constructed for Bush ; the best concepts are shown. 3.3 Generating Pa ths Between Concepts This stage finds potential conceptual connections in different levels, creates the concept. The basic algorithm is based on the method proposed in [5] but we adapted it topics/concepts of interest designated, A and C . Output: Levels of potential concepts ranked by their weights within specified semantic types. A potential conceptual connection between A and C is a path starting reaching the ending topic C . The Concept Graphs, which are an extension of concept vectors, are made automatically by calculating the significance of concept-concept associations. The formal definition is as follows: 4.1 Graph Construction weighted label graph where N is a set of n odes; E is a collection of weighted edges. 
Weight  X  A,B can be calculated as the co-occurrence frequency of concept A and B within the window, in our experiment, we use the following formula analogous to Dice Coefficient to measure this relationship: document collection. N A, B is the co-occurrence frequenc y of concept A and B within the window. Based on this model, each document can be represented as follows: and C j (1  X  i &lt; j  X  n). To normalize A , we get a matrix W = [  X  i,j ] . 4.2 Generating and Ranking Concept Chains them according to the weight of th e corresponding selected chains. 
Firstly the graph undergoes cleaning phase. The user may adjust the size of the constructed graph using parameter Edge Support . This parameter is to filter the edges whose weights are below such threshold designated by the user. Next, the graph chain of length 1 means there is a direct link between these two concepts within the window; The chains of length more than 1 indicate there exists an unapparent ranked by their estimated potential. Due to computational consideration, the algorithm combines the depth-first search and width-first search together. The users may specify setting the appropriate parameters. The ranking of concept chains takes a total order defined as follows: has a higher precedence than r j ) if 1. the length of r i is less than that of r j or 2. their lengths are the same, but the total weight of chain r i (sum of the weights For the experiments we used the 9/11 commission report as the corpus. This involves processing a large open source document collection pertaining the 9/11 attack, including the publicly available 9/11 co mmission report. The whole collection was including 9131 concepts is created and each concept is assigned to one or more ontology category. 5.1 Evaluation set The objectives of the evaluation were to measure precision and recall of the concept chains of lengths ranging from 1 to 4. The chains were selected as follows: 5.2 Experiment Result query pairs in the evaluation set and generates concept chains of lengths ranging from 1 to 4 for each query pair. We make the comparison by calculating the average precision and recall of the chains each technique created for all the query pairs. Table 2 summarizes the results we obtain on executing concept chain queries from the evaluation set. The comparison will be used to emphasis the strength and weakness of each technique compared to each other. 
As a post analysis, the concept-profile model handles a majority of the queries with model presented here. Through combining concept profiles and ontology information, we got good coverage of the links we were looking for. However, the precision in the semantic links represented by the graph model was better than that of concept-profile model, which performs 83.77% comparing with 76.19% achieved by concept-profile model. Our main conclusion is that if we need much focused information then the best When we look for greater coverage of information we should use the concept-profile model. This paper focuses on detecting links between two concepts across text documents (e.g. two persons). We interpret such a query as finding the most meaningful evidence trail across documents that connect these two concepts. We proposed to use link-analysis techniques over the extracted f eatures provided by Information Extraction Engine for finding new knowledge and compared two approaches to perform this other is the graph-based approach which combines text mining, graph mining and link analysis techniques. Counterterrorism corpus is used to evaluate the performance of each model and demonstrates that the semantic links represented by the graph model is preferable for finding focused information. For greater coverage of information we should use the concept-profile based approach. 
Future directions include the development of more sophisticated retrieval models that can combine various evidence sources (concept order, occurrence, context, which are specific to a corpus. These matched instances can then be used to look for other, similar scenarios. 
