 Lian Yan lian yan@csgsystems.com Robert Dodier robert dodier@csgsystems.com CSG Systems, Inc., 11080 Circle Point Rd., Westminster, CO 80020 Michael C. Mozer mozer@cs.colorado.edu Richard Wolniewicz richard wolniewicz@csgsystems.com CSG Systems, Inc., 11080 Circle Point Rd., Westminster, CO 80020 For many real-world classification problems, the typi-cal classification accuracy (correct classification rate) is not a sufficient metric for classifier performance (Provost et al., 1998). Consider the problem of pre-dicting churn, that is, which customers will switch from one service provider to another. In the telecom-munications industry, typical monthly churn rates are in the neighborhood of 2% (Mozer et al., 2000). The trivial solution of labeling all customers as nonchurn-ers yields a 98% correct classification rate. In general, classification accuracy can be misleading for a two-class problem when the class distribution is highly im-balanced. To elaborate, consider the simple example of a data set consisting of 10 positive and 90 nega-tive samples. Suppose the classifier outputs are binary, and that there are 8 errors. Regardless of which sam-ples are misclassified, the classification accuracy will be 92%. However, consider two different cases lead-ing to 8 total errors. In the case where all 8 errors are among the negative samples, the false positive rate (rate of negative samples being labeled as positive) is 9% and the false negative rate (rate of positive samples being labeled as negative) is 0%. In the case where all 8 errors are among the positive samples, the false pos-itive rate will be 0% but the false negative rate will be 80%. The two cases yield quite different errors yet would be indistinguishable using the measure of clas-sification accuracy.
 An alternative means of evaluating classifier perfor-mance that does not confound the false-positive and false-negative error rates is the Receiver Operating Characteristic or ROC curve (Green &amp; Swets, 1966). The ROC curve depicts the performance of a classifier by plotting the true positive rate against the false posi-tive rate. Assuming that a classifier produces a contin-uous output (e.g., class posterior probabilities), then the output must be thresholded to label each sample as positive or negative. Thus, for each setting of the deci-sion threshold, a true-positive and a false-positive rate is obtained. By varying the decision threshold over a range from 0 to 1, the ROC curve is produced. The upper graph in Figure 4 includes three ROC curves with the true and false positive rates on the y -and x -axes, respectively. The more bowed the curve is to-ward the upper left corner, the better is the classifier X  X  ability to discriminate between the two classes. Thus, area under the ROC curve ( AUC ) is a general, ro-bust measure of classifier discrimination performance, regardless of the decision threshold, which may be un-known, changeable over time, or might vary depending on how the classifier will be used.
 It seems that the AUC is not easy to compute. How-ever, note that the ROC curve of a finite set of samples is based on a step function, and it can be shown that the AUC is exactly equal to the normalized Wilcoxon-Mann-Whitney (WMW) statistic (Mann &amp; Whitney, 1947) in the form: where is based on pairwise comparisons between a sample x i , i = 0 , . . . , m  X  1, of random variable X and the sample y , j = 0 , . . . , n  X  1, of random variable Y . If we iden-sifier outputs for n negative examples, we obtain the AUC for our classifier via Eq. 1. 1 The expression of U can be further simplified if the { x 0 , x 1 , . . . , x m  X  1 { y 0 , y 1 , . . . , y n  X  1 } are merged and sorted in ascending order, yielding U = 1 mn ( r of x i , and tic (Wilcoxon, 1945). The statistic U is an estimator of P [ X &gt; Y ], but we only emphasize the equivalence between the WMW statistic and the AUC. The inde-pendence and other distribution requirements to use U as a test statistic are irrelevant here. The ROC curve or the AUC has been utilized ex-tensively to assess classifier performance (e.g., Hand, 2001; Provost &amp; Fawcett, 1998; Weiss &amp; Provost, 2001), but how to optimize the ROC curve or the AUC has been rarely addressed. If the goal is to achieve the best correct classification rate, classifiers such as neural networks are typically trained by minimizing a mean squared error (MSE) or the cross-entropy (CE) objective function, or by maximizing the likelihood mizing classification accuracy by minimizing the MSE or CE cannot guarantee maximization of the WMW statistic or the AUC. Thus, if the AUC is the quan-tity of concern, it would be better to train a classifier to directly optimize the AUC. Unfortunately, the AUC itself, even in the equivalent form of WMW statistic, is nondifferentiable and cannot be optimized by gradient-based methods. In Verrelst et al. (1998), the authors recognized the limitation of using the MSE to optimize the ROC curve, and proposed to numerically calculate the AUC with the trapezoidal integration rule, and to maximize the AUC by simulated annealing. However, the paper reported that the simulated annealing ap-proach does not improve the results over the MSE cri-terion. In Mozer et al. (2001), several approaches were proposed to try to improve a specific point on the ROC curve. However, these approaches assume that the de-cision threshold and/or the misclassification costs are known. Here, the focus of our work is to improve the overall ROC curve, measured by the AUC. Caruana et al. (1996) recognized that MSE might overfit the model when only relative ranking rather than explicit discrimination is required. They proposed an algo-rithm called Rankprop that tried to learn the rank-ings iteratively. However, the convergence properties of this algorithm were questionable, and we have ex-perimentally found that it is difficult to obtain conver-gence. In the information retrieval field, some related work exists on optimizing ranks, (e.g., Bartell et al., 1994; Vogt &amp; Cottrell, 1998). Bartell et al. use a differentiable function based on the Guttman X  X  Point Alienation statistic to measure the correlation between the target and estimated ranking. Vogt and Cottrell showed that this measure can be seen as a scaled ver-sion of d 0 = (  X  x  X   X  y ) / X  y , where  X  x is the mean of the x i and  X  y and  X  y are the mean and standard devia-tion of the y j . We have explored using d 0 and its sev-eral variations for training neural network classifiers, and have found that they did not improve the AUC in our real-world data sets (Rattenbury &amp; Yan, 2001). Finally, one would simply weight positive and nega-tive samples differently during training, especially for imbalanced data sets, to improve the AUC. Weiss &amp; Provost (2001) report that the best weight (class dis-tribution) needs to be experimentally determined for each data set. Later we will compare the best ROC curve (in terms of the AUC) obtained by trying several weights for the MSE training with the proposed new algorithm X  X  results. The AUC and classification accuracy criteria can be dissociated from one another, i.e., the AUC is sensi-tive to quite different aspects of a classifier than the classification accuracy. We argue this point with a two-alternative classifier that is used to assign a  X  X os-itive X  or  X  X egative X  label to each sample, in the follow-ing manner. We assume that the classifier produces a continuous output, and is run on a data set consisting of m positive and n negative samples. The set of sam-ples can be ranked by their classifier output, where the rank varies from 0, the rank of the lowest output, to m + n  X  1. Samples with identical output are assigned the average rank. Given some threshold  X  , such that all samples with rank  X   X  are labeled  X  X ositive X  and other samples are labeled  X  X egative, X  one can deter-mine which samples are labeled correctly and which are errors. If k is the total number of errors in the data set, the classification accuracy is 1  X  k/ ( m + n ). In contrast, the WMW statistic depends not only on k but also on the ranks of the samples which are clas-sified incorrectly. Assuming k &lt; n , for each  X  , there are sible values of the WMW statistic depending on the ranks of the k misclassified samples. To simplify, as-sign 1 to all samples with rank  X   X  and 0 to others. Then, from Eq. 1, the mean and variance of this set of WMW statistic values, which associate with the same classification accuracy 1  X  k/ ( m + n ), can be obtained:  X   X  . Figure 1 (upper) shows an example of the relation-ship between  X  u and the positive-example proportion m/ ( m + n ) when classification accuracy is fixed. We can see that the WMW statistic becomes increas-ingly dissociated from classification accuracy when the class distribution becomes more imbalanced. Gener-ally, minimizing the MSE or CE for optimizing clas-sification accuracy cannot guarantee maximization of the WMW statistic or the AUC, especially for imbal-anced data sets. Figure 1 (lower) demonstrates that the WMW statistic does not necessarily increase with MSE X  X  decrease when the MSE is minimized during training. Here we propose an alternative objective function for training classifiers to directly optimize the ROC curve. The proposed objective function can be applied to any parametric classifier. For any such classifier, one can optimize the objective function with respect to the pa-rameters using gradient based methods. In the results below, we apply the proposed objective to a typical multilayer perceptron (MLP) with softmax outputs, with a single hidden layer and direct connection be-tween the input and output layers. 3.1. New objective functions To directly optimize the AUC, we can try to maximize the WMW statistic in Eq. 1. However, the function I ( x i , y j ) in Eq. 2 is nondifferentiable. A differentiable approximation to Eq. 2 is the sigmoid function where  X  &gt; 0. Then, one can try to maximize However, as shown in Figure 2, S ( x i , y j ) with a small  X  , e.g.,  X  = 2, softens I ( x i , y j ) too much when 0  X  x  X  1 and 0  X  y j  X  1. A large  X  would make S ( x i , y j ) close to I ( x i , y j ), but this brings in numerical problems during optimization because of steep gradients. For the large data sets in our experiments, we have seen that the optimization could not proceed successfully with  X  &gt; 2. Alternatively, we use the differentiable function
R 1 ( x i , y j ) = where 0 &lt;  X   X  1 and p &gt; 1, and train a classifier by minimizing the objective R 1 ( x i , y j ) is shown in Figure 2 with S ( x i , y j ) and as well another function R 2 ( x i , y j ) = as an approximation to I ( x i , y j ), while R 1 ( x i , y proximates I (  X  x i ,  X  y j ). We have found that maximiz-ing the WMW statistic, because it makes the optimiza-tion focus on maximizing the difference between x i and y j rather than on moving more pairs of x i and y j to satisfy x i  X  y j &gt;  X  , which is the key notion in the WMW statistic using I ( x i , y j ). Instead, during the process of minimizing U R , when a positive sample has a higher output than a negative sample by a margin  X  , this pair of samples will not contribute to the objec-tive function. Essentially, the influence of the training samples is adaptively adjusted according to the pair-wise comparisons during training. Note that the posi-tive margin  X  in U R is needed for better generalization performance. 3.2. Discussion of U R and U S First, we look at the training process of the different objective functions over the data set SPECTF heart in the UC Irvine machine learning repository, which has predefined training and test sets. Figure 3 shows the variation of the WMW statistic U in Eq. 1 as a function of the number of epochs of training, for both the training and test sets, for optimizing four objec-tive functions, where  X  = 0 . 1 and p = 2 in R 1 ( x i , y and  X  = 2 in S ( x i , y j ). We can see that minimiz-ing the objective function U R based on R 1 ( x i , y j ) ob-tains the largest WMW statistic for both training and test. Although minimizing CE achieves the same WMW statistic for the training set, it obtains the low-est WMW statistic for the test data. Overfitting is observed for both MSE and CE based training, but R 1 ( x i , y j ) based training does not incur obvious over-fitting. Moreover, even if early stopping can be applied so that the MSE or CE training can stop when the value of the WMW statistic over the test set reaches the maximum, this maximum value is still smaller than the final WMW statistic obtained by minimizing U R . We will see that this is true in our other experiments as well. It is also obvious in Figure 3 that maximizing U S based on S ( x i , y j ) is inferior to minimizing U R . The upper panel of Figure 4 presents the ROC curves for the UC Irvine Credit Approval data set, based on 10-fold cross validation. The figure shows that the objective function U R generates a significantly better ROC curve than CE-and MSE-based training. Here, we chose  X  = 0 . 7 and p = 2 for R 1 ( x i , y j ). A natural question is then how to choose  X  and p . As shown in Figure 4 (lower), the WMW statistic U , i.e., the AUC, for the test data is quite insensitive to  X  over a large range. In general, we can choose a value between 0.1 and 0.7 for  X  . Also, we have found that p = 2 or 3 achieves similar, and generally the best results. 4.1. Churn prediction In the wireless telecommunications industry, it costs around five times as much to sign on a new subscriber as to retain an existing one. It is thus crucial to predict customer behavior and proactively build last-ing customer relationships. We build neural network models to predict churn, using both static customer data such as demographics and application informa-tion, and time series data of historical usage, billing, and customer service. Up to one year X  X  worth of past usage, billing, and customer service data can be used to predict which customers will likely churn within the next one or two months. For example, a model is trained over input data from June to September, with the class label (churn or nonchurn) given for October. The trained model is then used over data from July to October to make predictions for November. The training and test data extracted from different time windows make the problem even more difficult because of the data nonstationarity issue (Yan et al, 2001). The results in this study are based on data sets from a major US wireless service provider. The number of customers in the training set is close to 70,000, and the test set from a forward-shifted time window has over 70,000 customers. Six months past usage, billing, and customer service data were available for both train-ing and test sets. The task is to predict churn over a two month window, for which the average churn rate is roughly 6%. 134 raw features were extracted from a variety of data sources. After preprocessing of the raw data, a 55-dimensional feature vector was used to represent each customer. In Figure 5 (upper), we compare the ROC curves over the test data by differ-ent training methods. The model trained by minimiz-ing the new objective function U R in Eq. 8 achieves a better ROC curve than the models trained by MSE or CE. Moreover, by trying several weights for posi-tive samples during the MSE-based training, we have found weighting all the positive samples by 2 (versus 1 for all negative samples) achieved the best ROC curve over the test set. Still, we can see in Figure 5 that this ROC curve, obtained by extensive search of weights, is overall worse than the one achieved by minimizing U
R . We also show the training process in Figure 5 (lower), which compares the variation of the WMW statistic U during training over both the training and test sets for the new objective function U R and MSE. Similar to previous results, minimizing the new objec-tive function obtains larger WMW statistic for both training and test data, and, unlike MSE-based train-ing, it does not overfit the training data. Note again that even the largest value of the test set WMW statis-tic for MSE minimization (at epoch 13) is smaller than the final WMW statistic obtained by minimizing U R . 4.2. Cross-sell acceptance prediction In this application, we build models for an US cable service provider to predict which customers will likely accept a cross-sell offer of a particular new service. Ac-curate prediction of the acceptance can significantly save the service provider X  X  campaign costs and dra-matically improve the offer acceptance rate by target-ing only those customers who will likely accept the new service. From the similar data sources to those of churn prediction, 135 raw data fields are extracted, and 41 transformed features are used as the neural network inputs. In this study, five months worth of past usage, billing, and customer service data are used to predict which customers will likely accept the new service within the next month. Training and test sets were extracted from different time windows, with the test data window forward-shifted by one month. For training, we have over 130,000 customers with a posi-tive sample rate of only 0 . 5%. The test set consists of a similar number of customers with a positive sample rate of 0 . 7%. Figure 6 (upper) demonstrates again that the new objective function U R achieves substantial im-provement of the ROC curve over both the MSE and CE training. For this data set, weighting the positive samples by 10 for the MSE training obtained the best ROC curve over the test set, but it is still worse than the one achieved by minimizing R U . Figure 6 (lower) shows the difference between the training processes of the new objective function U R and the CE. 5.1. Multi-membership classification In some applications, samples are associated with multiple classes, e.g., people may have multiple dis-eases, and these are called multi-membership sam-ples. We extend our work here to the following multi-membership classification problem. In addition to churn prediction, service providers would also like to know in advance why a subscriber wants to churn so that the customer service representative (CSR) can make appropriate offers and try to save the subscriber. Some subscribers may have multiple reasons, e.g., competition and billing problem. Also, the CSR can generally try more than one reason during the con-versation with a subscriber. Thus, the conventional accuracy, just measuring if the predicted top reason is correct, is insufficient. Instead, this application actu-ally requires ranking the reasons for each subscriber. Assume there are L predefined reason categories, and subscriber k , k = 0 , . . . , K  X  1, is associated with m reasons, 0 &lt; m k  X  L . An MLP structure with L outputs is trained over these K samples. Denote x ki , i = 0 ,  X  X  X  , m k  X  1, as the classifier output for subscriber k  X  X  associated reason i , and y kj , j = 0 ,  X  X  X  , L  X  m as the output for reason j which is not a stated reason of subscriber k . We then train the MLP model by minimizing the objective We apply this objective function to churn reason pre-diction for an US cable service provider. Churn rea-sons are predicted for potential churners in the next month based on the similar data sources mentioned in Section 4. The customer base includes more than 330,000 subscribers, but only about 3,100 subscribers with churn reasons can be used to train the model. More than 20% of the 3,100 subscribers are associ-ated with more than one reason. There are 15 pre-defined reason categories, with the smallest category consisting of only 0.03% of the churners and the largest category consisting of 40.8% of the churners. Typi-cally, reason prediction is a much more difficult prob-lem than churn prediction because of available training set size, very noise data, and imbalanced, overlapping categories. Table 1 shows the 10-fold cross validation results for the new objective function U m , the MSE, and just using the prior distribution, i.e., ordering the reasons for each subscriber according to the prior class distribution. P 1 is defined as the rate of the number of churners, whose top predicted reason is among the true reasons, to the total number of churners, and P 2 and P 3 are the rates of the number of churners, who have at least one of the true reasons among the top two or three predicted reasons, respectively, to the total num-ber of churners. We can see that the new objective U m outperforms the MSE for all the accuracy measures. 5.2. Focusing on the lower-left part of the If the classifier produces high outputs for positive clas-sifications and low outputs for negative classifications, the lower-left part of the ROC curve measures the per-formance over those samples with higher classifier out-puts. This portion of the ROC curve is important be-cause many applications restrict the actions, e.g., con-tacting customers, among the classifier outputs above a certain threshold or within a certain percentage in the top. One way to extend our algorithm to focusing on the lower-left part of the ROC curve is to map the classifier output s by the function with  X  s = classifier outputs. Here,  X  &gt; 1 but is close to 1, e.g.,  X  = 1 . 1, and  X   X  1 and is generally chosen as 1. Thus, this function maps the classifier outputs below  X   X   X  s to zero. Then, when we train a classifier by minimizing the objective the optimization is focused on those samples with out-puts larger than  X   X   X  s . Figure 7 demonstrates the im-proved lower-left part of the ROC curve over Credit Approval achieved by minimizing U f . Two 10-fold cross validation ROC curves are shown in Figure 7, with the curve obtained by minimizing U R replicated from Figure 4. We have demonstrated that mean-squared error and cross entropy are not the most appropriate objective functions for training a classifier when the goal is to maximize the discriminative ability of the classifier across a range of decision thresholds. We proposed a new objective function that is a differentiable approxi-mation to the WMW statistic, or to the area under the ROC curve. Results both over two UC Irvine data sets and several real-world customer behavior prediction problems demonstrate reliable improvements achieved by our new algorithm. In some cases the magnitude of the improvement may appear to be small, but the resulting economic gain is substantial in a large-scale real world application (Mozer et al., 2001).
 Our work can still be extended in several directions. Hand and Till (2001) proposed a generalization of the AUC for multi-class classification problems based on the WMW statistic. The algorithm proposed here can be directly applied to this generalization, and may be advantageous for problems with imbalanced class dis-tribution. Secondly, our work is related to Bayesian similarity (Breuel, 2001). In (Breuel, 2001), the opti-mization requires mn training patterns and a subse-quent stage of simulated annealing. For large m and n , this process is intractable. We can optimize an ob-jective associated with Bayesian similarity in a simi-lar way to the algorithm here for ROC optimization. Finally, it would also be interesting to extend the al-gorithm to focus on any specified range of the ROC curve.
 Acknowledgments References
