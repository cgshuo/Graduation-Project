 Large-scale web search engines need to crawl the Web con-tinuously to discover and download newly created web con-tent. The speed at which the new content is discovered and the quality of the discovered content can have a big impact on the coverage and quality of the results provided by the search engine. In this paper, we propose a search-centric solution to the problem of prioritizing the pages in the fron-tier of a crawler for download. Our approach essentially orders the web pages in the frontier through a random walk model that takes into account the pages X  potential impact on user-perceived search quality. In addition, we propose a link graph enrichment technique that extends this solution. Finally, we explore a machine learning approach that com-bines different frontier prioritization approaches. We con-duct experiments using two very large, real-life web datasets to observe various search quality metrics. Comparisons with several baseline techniques indicate that the proposed ap-proaches have the potential to improve the user-perceived quality of web search results considerably.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Web search engine; web crawling; URL prioritization; web frontier; discovery; frontier ranking; result relevance; ran-dom walks. This work is conducted during the author X  X  internship at Yahoo Labs Barcelona.
 c  X 
Large-scale search engines need to crawl the web content continuously to be able to cope with the constant growth and the content-wise dynamic nature of the Web [25]. In com-mercial search engines, web pages are downloaded through two independent crawling processes: refreshing and discov-ery [5]. The objective of the first process is to selectively refetch pages whose content was already downloaded and stored in the web repository of the search engine. This way, potential modifications in the online content can be detected and reflected to the index of the search engine, reducing the staleness of search results presented to the users. The ob-jective of the second process is to discover previously unseen pages and fetch their content for the first time. This lets the search engine grow its web frontier 1 and increase its cover-age of the Web. While discovery may be seen primarily as a mechanism to increase the recall of the search engine (with respect to the content available on the Web), it can also affect the quality of search results presented to the users.
In this work, we are mainly interested in the discovery process, which essentially requires maintaining a priority queue for the web pages in the frontier. The download de-cisions of the crawler are guided by this queue such that highly ranked (important) web pages in the queue are tried to be fetched earlier than the rest. Besides the scalability and efficiency challenges in maintaining a web-scale prior-ity queue [17, 21], the main issue here is to decide how the pages should be ordered in the queue for download. This is because, in practice, delaying the download of important pages may have direct implications on the overall quality of the search engine X  X  web repository and, consequently, on the user experience with the retrieved search results.

The discovery process requires a metric to quantify web page importance and prioritizing pages accordingly. The early work on this issue relied on page importance metrics that exploit the connectivity of a page in the web graph. A commonly adopted approach was to attribute higher im-portance to pages that have larger PageRank values (com-puted over a partially complete web graph). Although the connectivity-based metrics performed fairly well in model-
The web frontier refers to the set of web pages that were dis-covered by the crawler but whose content is not yet fetched. ing page importance, they were indirect in the way they captured the impact of crawling a web page on the search engine X  X  result quality. Based on this observation, the latter work adopted a more direct approach where the discovery process is guided by the actual impact on search quality. For example, the technique proposed in [29] prioritized pages in decreasing order of their likelihood of being retrieved in search result pages, increasing the utility of web crawling for many search queries and users. The main difficulty in search-centric approaches is the need to estimate the impact of a web page that is not yet exposed to any user in search results. That is, estimating the impact of a page requires using only auxiliary signals without exploiting any explicit user feedback (e.g., clicks or dwell time) about the page.
The main contribution of this work is a hybrid frontier prioritization approach that combines the two lines of work mentioned above. In particular, we propose a novel random walk model that incorporates the inferred search impact of pages into the standard connectivity-based page importance computation. We measure the actual search impact of a web page in terms of two different metrics: the number of times the page is clicked and the number of times it is viewed in search results. To infer these two measures for a newly discovered web page, we exploit the observed click and view counts of its referring pages which were previously shown in search results. Moreover, we enhance our random walk model by a novel teleportation approach which lets us go beyond the original web graph by connecting pages that have a good chance of being influential for each other in terms of their search impact. Finally, we combine the proposed technique and various baselines under a machine learning model to show further improvements.

We evaluate the performance of the proposed frontier pri-oritization approach over two large-scale, real-life web col-lections. As the ground truth for search quality, we use a web search query log obtained from Yahoo. Comparisons with several frontier prioritization approaches taken from the literature indicate that the proposed approach can lead to considerable improvement in search quality. In particu-lar, we observe that our approach obtains up to 60% X 70% of the search quality attainable by an ideal web crawler while outperforming the best baseline by a margin of 5% X 10%.
The rest of the paper is organized as follows. We present some preliminary information about our problem in Sec-tion 2. In Section 3, we describe the data and baselines used in our work. The proposed frontier prioritization ap-proach is presented in Section 4. We report the results of our experiments in Section 5. In Section 6, we present a machine learning solution for the problem. In Section 7, we survey the related work. Section 8 concludes the paper.
Search engines continuously crawl the Web to discover and download newly created content. It is practically impossible to crawl all the pages in the Web within a time frame that will enable meaningful use of the crawled collection. Thus, crawlers aim to download and maintain a reasonably repre-sentative and high-quality snapshot of the Web by (i) keep-ing important pages in the collection fresh, (ii) expanding the collection with new important pages, and (iii) remov-ing unimportant pages from the collection. Herein, we focus on (ii), which requires prioritizing the pages in the crawler X  X  frontier according to a page importance metric.
As discussed in Section 1, previously proposed page im-portance metrics can be grouped under two headings as connectivity-based [1, 23, 27] and search-centric [28, 29] page importance metrics. Herein, we are only interested in search-centric page importance metrics as they provide a more direct measure of the crawled web collection X  X  utility for the search engine. For a given web page p , we measure the search impact in two different ways: Without loss of generality, these two impact measures can be extended to a set of pages. For a set S of web pages, the click impact is denoted by I C ( S,t ) and is equal to the sum of the click impacts of all pages in S . The view impact of S is denoted by I V ( S,t ) and is computed similarly.
In the rest of the paper, we use the above-mentioned two measures although the proposed techniques are not tied to a particular measure. In our analyses, we focus on the short-term search impact of the crawled collection (e.g., within a few days) rather than the long-term impact, which largely depends on the recrawling policy as addressed in [7, 26, 28].
We formally define the frontier prioritization problem in the expected case as follows.
 Definition 1 (Frontier Prioritization Problem).
 Given a set D of already crawled URLs, a set F of URLs in the frontier, a time period t , and a limit ` indicating the number of new URLs to be added to the collection, find a subset S  X  F , such that | S |  X  ` , and depending on the impact definition, either E C ( S ) = P p  X  S I ( S ) = P p  X  S I V ( p,t ) is maximized, where E C and E V two potential objective functions in the expected case.
We note that the described frontier prioritization prob-lem is NP-hard in the worst case and cannot admit the fully polynomial-time approximation scheme [29]. Hence, we opt for an expected-case optimization, where the time complex-ity of the algorithm depends on the time complexity of the computation of rank distribution.
In this section, we first introduce the datasets we used to evaluate the proposed techniques. We then provide a brief description of the techniques we used as baselines. Table 1 presents some statistics about the experimental datasets.
We assume a system crawling the web pages in batches.
Wikipedia is an Internet encyclopedia with 18 billion page views and nearly 500 million unique visitors each month [9]. With 34 million pages, 3 dense linking among its articles, constant revisions on existing articles, and few article re-movals, Wikipedia stands as a small-scale representative of the informative portion of the Web. Wikipedia regularly provides database dumps of all of its available content. 4 Our Wikipedia dataset contains the pages in the English Wikipedia dumps provided for the January 1, 2013  X  April 30, 2014 period. 5 In the dumps, each Wikipedia page can have multiple revisions that are available in the Web in dif-ferent time periods. Fortunately, the dumps also provide the revision time of each page. Hence, we can clearly identify the time period that each revision was online.
 In our experiments, we assumed that the revisions of Wikipedia pages that were online from January 1, 2013 to December 31, 2013 were already downloaded by the crawling system, i.e., these revisions constituted set D . We placed in the frontier (set F ) the revisions that were (i) linked by the revisions in set D , (ii) online from January 1, 2014 to April 30, 2014, and (iii) not present in D . For each revision in D and F , we counted the number of times it was shown in a search result page and the number of times it was clicked, during a short time period after the revision was created.
The WebCrawl dataset is obtained by sampling pages from a large-scale web crawl performed by Yahoo during June 2014. In this case, set D contained pages that were crawled before June 15, 2014, while set F contained pages linked by the pages in set D and crawled before the end of June 2014. We filtered both sets D and F to ensure that D  X  F =  X  . As in the case of the previous dataset, for each page in D and F , we counted the number of times it was shown in a search result page and the number of times it was clicked during a limited time period after the page was crawled.
As the baseline frontier prioritization techniques, we eval-uate the following five approaches:
In this section, we describe the proposed web frontier-prioritization approach. The heart of our approach lies in the propositions that (i) high impact pages in F are more likely to be discovered by following the outbound links of high impact pages in D , and (ii) the current web brows-ing trend is such that a user is more likely to  X  X eleport X  to a related page by going through a search engine instead of jumping to a random page within a browsing session. Based on these two propositions, we propose a random walk model that subsumes the impact of pages into the tradi-tional link-based computation (proposition (i)). We then enhance the model with a novel teleportation mechanism (proposition (ii)) by virtually connecting similar pages based on their link structure. Finally, we elaborate on the conver-gence property of our model.
We argue that high impact pages in D are more likely to link other high impact pages in F . We assume that, for each page p  X  D , we already have the I C ( p,  X  t ) and/or I ( p,  X  t ) scores. Let D C , D V , D NC , and D NV denote the set of clicked, viewed, not clicked, and not viewed pages in D , respectively. Similarly, let F C , F V , F NC , and F NV denote the set of clicked, viewed, not clicked, and not viewed pages in F , respectively. We note that D = D C  X  D NC = D V  X  D NV and F = F C  X  F NC = F V  X  F NV . As an example, based on the click impact metric, Figs. 1(b) and 1(c) illustrate the partition of a sample D set into D C and D NC sets and the partition of a sample F set into F C and F NC sets, respectively.
Given a set X  X  D , let the set of pages in F linked from the pages in X be denoted as L ( X  X  F ). Then, we define the average frontier click-impact ratio , r c ( X  X  F ), of pages in X as follows: r c ( X  X  F ) = Similarly, we define the average frontier view-impact ratio , r ( X  X  F ), of pages in X as follows: r v ( X  X  F ) =
We use the implementation in the scikit-learn toolkit [30]. D Table 2: Analysis of how pages in the downloaded set are linked with search impacting pages in the frontier set Intuitively, r c and r v provide a measure of how important X is in suggesting good candidate pages for crawling. Ta-ble 2 shows the frontier impact ratio values for the pages in D C , D V , D NC , and D NV . We observe that pages in D and D V have significantly higher impact ratio values than the pages in D NC and D NV . In other words, despite the fact that clicked and viewed pages in the downloaded page set constitute a small fraction of this set, these pages are promising sources for discovering new pages (frontier) with high search impact.
We have observed in Table 2 that the relatively small set of clicked or viewed pages in D are more likely to directly link clicked or viewed pages in F . Generally speaking, the high-impact pages in the frontier are likely to be discov-ered through the outbound links of the high-impact pages in the downloaded set. Furthermore, in our analyses, we also observed that there is a significant number of high-impact pages in D that do not directly link pages in F , but are at two-or three-hop distance to the high-impact pages in F . These observations pointed us in the direction of a random walk model where the impact of pages can trickle down to the pages that are reachable via following outbound links.
A random walk on a given graph is a Markov process, where each node represents a state and a walk transiting from one state to another state based on a transition prob-ability matrix. One of the well-known random walk algo-rithms is PageRank [27], and variants of it are used for frontier prioritization [1] in state-of-the-art web crawlers. PageRank determines the importance of pages by modeling the web surfing behavior of users. The random surfer model of PageRank assumes that a user visiting a given page can either follow one of the outbound links of that page or arbi-trarily jumps to another page. Formally, the random walk process of PageRank can be defined as where M ij is the transition probability from page p i to p x ( j ) is the importance score of page p j at step t ,  X  is a damping factor that controls how often the walker jumps to an arbitrary node, v j is the initial probabilistic importance score (generally set to 1 /n , where n is the number of nodes in the graph), and L  X  i is the set of inbound links of page p , respectively. When t is iterated enough, the importance scores reach a stationary distribution that can be used as a source of information for ranking pages.

The main drawback of the traditional PageRank process in Eq. (3) is that it captures only the observed linking char-acteristics of pages and ignores other sources of information which can also be indicators of their importance. To over-come this drawback, we introduce an impact factor for every page and use this to augment the traditional PageRank pro-cess to make use of other sources of information. The pro-posed random walk model ( RW ) allows the random walker to take into account multiple sources of information and per-form the voting adaptively and more effectively.

To define our model formally, let G = {V , E} denote the link graph, where vertices in V = D  X  F with |V| = n represent pages and edges in E represent the links among pages. We compute the transition probability for p i  X  p M ij =1 / | L + i | , where L + i denotes the set of outbound links of page p i (obviously, p j  X  L + i ). We define the impact factor F i of a page p i used in embedding the search-centric impact of a page as Here, the term I i / max j I j represents the normalized search-centric impact of p i . 8 d + i  X  [0 , 1] is the normalized out-degree We note that, in the implementation, we use I i = I ( i,  X  t ) + or I i = I V ( i,  X  t ) + , depending on the opti-mization objective. I i is smoothed by adding a small value of p i , i.e., d + i = | L + i | /max k | L + k | . The ( d added to enable a fine-tuning capability for favoring pages with high out-degrees. As  X  gets larger than zero, the model increases the effect of impactful pages with high out-degree. Our observation is that the set of pages with non-zero impact in D are rather small, and among such pages, the ones that have a high out-degree can be considered as hubs linking authoritative pages [19]. We set  X  = 1 for the Wikipedia dataset and  X  = 0 for the WebCrawl dataset because of this characteristic.

The impact factor is embedded into Eq. (3) to iteratively compute the importance score x t ( j ) of a page p j as
Intuitively, our random walk process can be explained as follows. When following the outbound links of a node, the random walker is likely to contribute the voting impact of the node to its neighbors with a high rate when this node has a good impact (i.e., more clicks or views in our case). Other-wise, its neighbors will receive its voting impact with a low rate. Here, we should note that, the importance score com-putations of the pages in the frontier according to Eq. (5) can be done in a separate iteration as the outbound links of those pages are not known. Hence, their contributions to other pages X  importance scores are practically zero. In other words, it is possible to compute the accumulated score con-tribution of the pages in D to pages in F first and reflect that onto pages in F in one iteration later. For further com-putational efficiency, it is possible to compute the scores of pages in D offline and update them incrementally adopting techniques similar to those defined for incremental PageR-ank computation [11].

It is worth mentioning that RW converges to a stationary distribution. Let  X  be the diagonal matrix with diagonal elements ( F 1 , F 2 ,.., F n ), I be the n  X  n identity matrix, v be the transpose of 1  X  n uniform stochastic vector, and M denote the transition matrix for G .
 Proposition 1. ( I  X   X  M T  X ) is invertible for all M ,  X  , X  .
Proof. Let P = M T  X , we need to prove that I  X   X  P is invertible. Equivalently, we prove its transpose I  X   X  P T vertible, which can be proved by showing that ( I  X   X  P T 0 only has the trivial solution y = 0.
Let u = argmax j y j . Eq. (6) infers (empirically set to 0.001) to its absolute count to avoid ran-dom ranking in the case all inbound links to a page have zero impact value.
 Since  X  &lt; 1 and F u  X  1, we have (1  X   X  F u ) &gt; 0 and hence y u  X  0. Similarly, let v = argmin j y j . We have y v  X  0. As y  X  y u , this implies y u = y v = 0 to satisfy all inequalities. Consequently, y i = 0 for all i , or y = 0. Thus, I  X   X  P invertible. Equivalently, ( I  X   X  M T  X ) is invertible.
Proposition 2. The iteration in Eq. (5) converges to (1  X   X  )( I  X   X  M T  X )  X  1 v .

Proof. Let P = M T  X . We can rewrite Eq. (5) in matrix form as
We will show that lim Because  X  &lt; 1, this column sum converges to zero when t  X   X  . We then derive lim given Proposition 1, by applying Neumann series, Eq. (8) becomes: Hence, Convergence proved.
In the previous section, we have sketched our random walk model, which presumes that when a random walker is at some page p , it can either visit the outbound links of p or teleport to a random page with a certain probability and these visit probabilities provide some voting impact to the rank of the candidate pages. In this model, the teleportation probability to other pages is fixed.

We hypothesize that the user browsing experiences gen-erally start with a query issued to a search engine and by clicking on a search result, possibly followed by a number of page visits performed by tracking the outbound links of the encountered pages. Then, if the search task did not end, it is followed by another (possibly related/refined) query to the search engine. In this scenario, teleportation is also gen-erally performed via visits to a search engine and a user is more likely to  X  X eleport X  to a related or similar page instead of a random page in a search session. Following this hypoth-esis, in our random walk model, when a page p is visited, Table 3: Analysis on the search impact of pages in F linked via enriched edges from D C and D V even if the outbound links of p are not followed, pages that are similar to p should have a higher chance of being visited next.

The implication of the above discussion for prioritizing frontier pages is that pages in F that are similar to high impact pages in D are more likely to be teleported to. There are a number of ways for performing this special form of teleportation. Herein, we opted to enhance the link graph via virtual edges that link similar pages. Since one of the best information we have about the pages in the frontier is their in-link structure, we define a similarity metric based on the in-link structure of pages. We refer to the set of virtual edges as E 0 . Extending the original graph G via virtual edges in E 0 forms the enriched graph EG = {V , E X  X  0 } . Applying our random walk model on EG yields a random walk model with a graph enrichment solution ( RW-EG ) for the frontier prioritization problem.

In this enhanced model, we consider pages linked by the same pages as similar. We create a virtual edge between a page p i and another page p j if and only if they have at least one page p x that has links to both of them. The weight w ij of the virtual edge between p i and p j is computed as w
Fig. 2 provides an example for our link graph enrichment technique. The regular link graph G is shown on the left while the enriched graph EG is shown on the right. Shaded vertices in the figure indicate already crawled pages, and white vertices indicate URLs in the frontier. The darker a crawled vertex is, the higher the search-impact of the cor-responding page. As seen in the figure, two virtual edges are added to G after the enrichment process. The edge be-tween v 5 and v 7 is added as both of them have a common inbound link from v 3 . The weight of the ( v 5 ,v 7 ) edge is 1 since the number of common link providers of these two ver-tices is one. The edge ( v 5 ,v 6 ) is added as both of them have common inbound links from v 2 , v 3 , and v 4 . The weight of ( v 5 ,v 6 ) edge is 3 since the number of common link providers is three.

Table 3 shows the frontier impact ratio values for the pages in D C and D V after enrichment. We observe that, via en-richment, the impact of the set of pages in the frontier that are linked by D C and D V has increased. Fig. 1(d) demon-strates how the link graph changes after enrichment.
To formally define our random walk model with graph enrichment, let M 0 be an n  X  n matrix, such that M 0 ij in-dicates the virtual transition probability among pages p and p j based on virtual edges in E 0 . M 0 ij is computed as M links based on virtual edges and F 0 j as the influence impact of a page p j based on virtual edges. We propose to compute the impact-based importance score x t ( j ) of a page p j the enriched graph EG as x t ( j ) =  X  X  X Here, 0  X   X   X  1 is the hyper-parameter to control the pro-portion of the impact a page distributes to its neighbors via its original links. When  X  = 1, no virtual link is taken into account in the importance calculation, and when  X  = 0 only the virtual links are used. The proposed random walk process can be interpreted as follows. At every node, the random walker can either follow the original outbound links with a probability  X  X  or teleport to a similar node with a probability  X  (1  X   X  ). Otherwise, it teleports to an arbitrary node with probability 1  X   X  .

Similar to the RW model, the RW-EG model converges to a stationary distribution. Let  X  and  X  0 be diagonal matrices with diagonals ( F 1 , F 2 ,.., F n ) and ( F 0 1 , F tively.

Proposition 3 (Invertible). ( I  X   X  ( w M T  X  + (1  X   X  ) M 0 T  X  0 )) is invertible for all M , M 0 ,  X  ,  X  0 , X , X  . Proposition 4 (Convergence). The iteration in Eq. (10) converges to (1  X   X  )( I  X   X  (  X  M T  X  + (1  X 
Proof. Let P = w M T  X  + (1  X   X  ) M 0 T  X  0 . The proof is similar to those of Propositions 1 and 2.
We randomly split the frontier of each dataset into train-ing and test sets. The training set is used to train and tune the parameters of the algorithms, while the test set is used for evaluation. In both datasets, we use roughly 65% of the pages for training.

RW-EG is parametrized by  X  , which linearly combines the votes based on original links and the votes based on vir-tual links, and  X  , which determines the probability that the surfer chooses to follow one of the outbound of the current page. We set,  X  = 0 . 85 as suggested by [27] and tune  X  on the training data before applying the model on the test data. Our tuning strategy is straightforward: We vary  X  to identify the  X  value maximizing the objective function. For higher confidence and to avoid data randomness, we apply the 0.632+ bootstrapping technique [12] to generate multi-ple samples from the tuning data and measure the average performance on these samples while tuning.
Figs. 3a and 3b show the performance of RW-EG on the tuning data as we increase the value of  X  from 0 to 1 in steps of 0.1. For each  X  value on the x axis, we present the percentage of search impact of RW-EG with respect to that of an ideal ranking, which orders the pages in the frontier ac-cording to their actual search impacts. We select the highest ranked 5% and 10% frontier pages according to RW-EG for the Wikipedia dataset. Similarly, we select the highest ranked 1% and 2% frontier pages according to RW-EG for the We-bCrawl dataset. The summed impact of pages selected by RW-EG are divided against that of the ideal ranking to obtain the provided figures. We observe that the best combination for Wikipedia is at  X  = 0 . 1 and  X  = 0 . 2 for the click impact and view impact metrics, respectively. For WebCrawl , the best combination is found at  X  = 0 . 9 for the click impact metric and  X  = 0 . 8 for the view impact metric.
Figs. 4a, 4b, 5a, and 5b present the performance of all ap-proaches on the two datasets and in terms of the two impact metrics. The performance values are provided as a percent-age of the click impact and view impact measures attained by the ideal ranking (observed within the first week follow-ing the crawling). We note that similar results are observed when the observation period is limited up to a day. For Wikipedia , similar to [29], we aim to maximize the search impact while selecting 10% of the frontier. This is a rea-sonable assumption considering the size of the Wikipedia frontier. For WebCrawl , we aim to maximize the impact while selecting up to 2% (approximately 160K pages) of the frontier. In practice, it may be more effective to crawl a few hundred thousand pages in a crawling iteration and rerun the ranking algorithm with additional discovered pages.
Figs. 4a and 4b present the performance of the investi-gated approaches on the Wikipedia dataset. As seen in the figures, when the fraction of the downloaded pages exceeds 6% of the frontier, RW-EG provides the best results. How-ever, when the fraction of downloaded pages is below 5%, Q-Overlap and Q-Hybrid outperforms RW and RW-EG . This is due to the fact that the query overlap score used in Q-Overlap and Q-Hybrid is very effective in informative pages such as those in Wikipedia .

RW and RW-EG provide considerably better performance compared to PageRank , showing the positive influence of in-corporating click and view impact in predicting high-impact frontier pages. The difference in the performance of RW and RW-EG indicates the effectiveness of the proposed link graph enrichment technique. We note that, even though Random provides the worst ordering among all approaches, it still obtains approximately 20% of the impact attained by the ideal ranking, both in click and view impact metrics. This shows that the Wikipedia dataset, by its nature, contains high-impact pages.
Figs. 5a and 5b show the performance of all approaches on the WebCrawl dataset. As mentioned before, we evaluate the search impact up to top 2% of the frontier. Unlike the Wikipedia dataset, WebCrawl has a highly skewed distribu-tion in terms of impact, with a tiny fraction (0.4% X 0.8%) of pages receiving the majority of clicks and views.

In the figures, we first observe that Random provides nearly zero impact frontier selection. This is another indicator for the difference between Wikipedia and WebCrawl . Second, we observe that Q-Overlap and Q-Hybrid both perform better than PageRank and InDegree in identifying the top fraction (0.2%) of pages with high click impact. However, as they download a larger fraction of frontier pages, they perform rather poorly. As suggested in [29], a very rapid  X  X rying up X  trend is observed for Q-Overlap . WebCrawl possesses a very low fraction of high-impact pages, and selecting the set of pages that match previously observed queries in and itself is not enough in tackling the complex problem of identify-ing these small number of high-impact frontier pages and may even lead to the selection of low-impact pages. Fi-nally, Figs. 5a and 5b show that RW and RW-EG are superior when compared to the other algorithms. This proves the effectiveness of the proposed approaches that incorporate the search impact into a random walk model. Once again, we observe that RW-EG improves over RW by selecting higher impact pages in the early stages.
We investigate the quality of highly ranked frontier pages in terms of two other quality metrics: click-through rate (CTR) and PageRank score. Fig. 6 presents the normalized CTR values per page for the investigated algorithms. As seen in the figure, for the Wikipedia dataset, Q-Overlap and Q-Hybrid lead to higher average CTR while, for the WebCrawl dataset, RW and RW-EG lead to higher average CTR.
Fig. 7 presents the average PageRank scores for each ap-proach. As expected, PageRank performs the best in this metric. As RW and RW-EG employ random walk models, they also tend to select pages with relatively high PageR-ank scores. Similarly, as Q-Hybrid incorporates PageRank, it performs relatively well in this metric. We observe that pages selected by Q-Overlap tend to have low PageRank scores.
 Table 4: Features used in the machine learning model The analyses in the previous section show that RW and RW-EG both perform better than the baseline frontier pri-oritization approaches. However, the information provided by the baseline approaches may still be useful. Hence, in this section, we describe a machine learning approach ( ML ) that exploits the ranking scores of all available approaches. Our aim is to show that by combining approaches that make use of different information wells (e.g., links, search impact, query neediness), it is possible to perform better than the individual approaches.

In ML , for every page in the frontier, we compute the fea-tures given in Table 4. Then, a model is learned using the set of training pages, where the (ground-truth) search im-pact scores obtained by using the query logs are used as prediction targets. In practice, there exists a wide range of machine learning algorithms that can be used to train our model. In our experiments, we use the decision tree re-gression technique 9 and the online gradient-descent-based regression technique 10 with the Wikipedia and WebCrawl datasets, respectively.
We use the implementation in the scikit-learn toolkit [30].
We use the implementation in the Vowpal Wabbit toolkit.
Figs. 8a and 8b compare the performance of ML with those of RW and RW-EG in terms of the click impact metric, using the Wikipedia and WebCrawl datasets, respectively. We do not report the results for the view impact metric as the findings are similar. In Fig. 8a, we observe that ML outper-forms RW and RW-EG for the Wikipedia dataset. This finding encourages the idea that information coming from different prioritization approaches, when combined in the right way, can provide additional benefits. In Fig. 8b, however, we ob-serve that ML does not show the same relative performance for WebCrawl . We hypothesize that, as WebCrawl is highly skewed in terms of impact distribution, the task of combin-ing the features in the right way becomes much more diffi-cult. There are techniques to cope with this issue, such as supervising a random walk model [3] or stratified sampling via weighted random walks [20].
Some recent work investigated passive content discovery techniques [4, 18], such as those that rely on bookmark-ing or toolbar data, as complementary techniques to web crawling. We limit our attention here only to conventional crawling strategies where the Web is actively crawled by fol-lowing hyperlinks [25]. We also do not cover the scalability and efficiency issues encountered in crawling [21]. Interested reader may refer to [5] for a more detailed coverage of prac-tical issues involved in web crawling. Below we cover the two main lines of research on crawling: recrawling [7, 26, 28] and frontier prioritization [8, 13, 15, 16, 22, 23, 25, 29].
Recrawling. Evolution of the Web has been investigated in several research work [2, 24]. These work pointed out the rapid change in both content and link structure of the Web, demonstrating the importance of recrawling. An early study of the recrawling problem is available in [31]. The work in [7] investigated different page refresh policies. The authors of [7] defined two metrics to estimate the freshness of a web repository, and the refresh policies proposed therein were shown to improve the freshness over simpler refresh policies. The work in [28] proposed guiding the recrawling process based on the potential impact on the search expe-rience of users. The authors empirically demonstrated that the same user experience can be attained by allocating much fewer resources to the recrawling process, enabling the use of freed resources in the discovery process. In [26], the authors argued that ephemeral and persistent web content should be distinguished when deciding which pages should be re-freshed. Based on this observation, they designed a recrawl-ing policy that takes the longevity of web pages into account. This approach was shown to lead to better freshness in the web repository also reducing the crawling cost.

Frontier prioritization. The discoverability of the con-tent in the Web was investigated in [10]. The early work in [8] defined different page importance metrics and pro-posed crawling the Web according to page importance. The authors showed that importance-based prioritization of pages can significantly improve the quality of a web repos-itory. In [23], the relatively simple breadth-first crawling strategy was shown to discover high-quality pages early. In [13], the authors tried to prioritize the pages in the crawler X  X  frontier by exploiting the hierarchical structure of the Web and showed that this is more efficient than the PageRank-based prioritization approach proposed in [8]. The work in [16] focused on the discovery of good pages, specifically focusing on filtering out spam content during the crawling process. The authors of [29] proposed ordering the pages in the frontier according to their estimated im-pact on search results. They showed that their technique enables crawling of tail web content that is of interest to some users, not only popular content. In two concurrent work [14] and [6], the authors investigated the impact of well-known crawling strategies on web search effectiveness.
We formalized the frontier prioritization problem as a search-centric optimization problem, where the objective is to maximize the impact of the crawled collection on the result quality of the search engine. To tackle the prob-lem, we presented a novel random walk model that incorpo-rates the inferred search impact of pages into the standard connectivity-based page importance computation. We also proposed a link graph enrichment technique that approxi-mates user X  X  searching and browsing behavior and improves the voting process in the proposed random walk model. Fi-nally, we combined different solutions for the frontier prior-itization problem under a common machine learning model. To evaluate the proposed approaches, we conducted exper-iments over two large-scale web collections. The results of our experiments indicated that the proposed approaches per-form considerably better than the state-of-the-art frontier prioritization approaches in improving search quality. This work was supported by the ERC Advanced Grant ALEXANDRIA (339233) and the LEADS project (ICT-318809), funded by the European Community.
