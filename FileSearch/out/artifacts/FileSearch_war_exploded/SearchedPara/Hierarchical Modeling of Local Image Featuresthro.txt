 Finding a precise statistical characterization of natural images is an endeavor that has concerned research for more than fifty years now and is still an open problem. A thorough understanding of natural image statistics is desirable from an engineering as well as a biological point of view. It forms the basis not only for the design of more advanced image processing algorithms and compres-sion schemes, but also for a better comprehension of the operations performed by the early visual system and how they relate to the properties of the natural stimuli that are driving it. From both perspectives, redundancy reducing algorithms such as Principal Component Analysis (PCA), Inde-pendent Component Analysis (ICA), Independent Subspace Analysis (ISA) and Radial Factorization [11; 21] have received considerable interest since they yield image representations that are favorable for compression and image processing and at the same time resemble properties of the early visual system. In particular, ICA and ISA yield localized, oriented bandpass filters which are reminiscent of receptive fields of simple and complex cells in primary visual cortex [4; 16; 10]. Together with the Redundancy Reduction Hypothesis by Barlow and Attneave [3; 1], those observations have given rise to the idea that these filters represent an important aspect of natural images which is exploited by the early visual system.
 Several result, however, show that the density model of ICA is too restricted to provide a good model for natural images patches. Firstly, several authors have demonstrated that filter responses of ICA filters on natural images are not statistically independent [20; 23; 6]. Secondly, after whitening, the optimum of ICA in terms of statistical independence is very shallow or, in other words, all whitening filters yield almost the same redundancy reduction [5; 2]. A possible explanation for that finding is that, after whitening, densities of local image features are approximately spherical [24; 23; 12; 6]. This implies that those densities cannot be made independent by ICA because (i) all whitening filters differ only by an orthogonal transformation, (ii) spherical densities are invariant under orthogonal transformations, and (iii) the only spherical and factorial distribution is the Gaussian. Once local image features become more distant from each other, the contour lines of the density deviates from spherical and become more star-shaped. In order to capture this star-shaped contour lines one can use the more general L p -spherically symmetric distributions which are characterized by densities of the form  X  ( y ) = g ( k y k p ) with k y k p = ( P | y i | p ) 1 /p and p &gt; 0 [9; 10; 21]. Figure 1: Scatter plots and marginal histograms of neighboring ( left ) and distant ( right ) symmetric whitening As illustrated in Figure 1, the relationship between local bandpass filter responses undergoes a grad-ual transition from L 2 -spherical for nearby to star-shaped ( L p -spherical with p &lt; 2 ) for more distant features [12; 21]. Ultimately, we would expect extremely distant features to become independent, having a factorial density with p  X  0 . 8 . When using a single L p -spherically symmetric model for the joint distribution of nearby and more distant features, a single value of p can only represent a compromise for the whole variety of iso-probability contours. This raises the question whether a combination of local spherical models, as opposed to a single L p -spherical model, yields a better characterization of the statistics of natural image patches. Possible ways to join several local models are Independent Subspace Analysis (ISA) [10], which uses a factorial combination of locally L p -spherical densities, or Markov Random Fields (MRFs) [18; 13]. Since MRFs have the drawback of being implicit density models and computationally very expensive for inference, we will focus on ISA and our model. In principle, ISA could choose its subspaces such that nearby features are grouped into a joint subspace which can then be well described by a spherical symmetric model ( p = 2 ) while more distant pixels, living in different subspaces, are assumed to be independent. In fact, previous studies have found ISA to perform better than ICA for image patches as small as 8  X  8 and to yield an optimal p  X  2 for the local density models [10]. On the other hand, the ISA model assumes a binary partition into either a L p -spherical or a factorial distribution which does not seem to be fully justified considering the gradual transition described above. Here, we propose a new family of hierarchical models by replacing the L p -norms in the L p -spherical models by L p -nested functions, which consist of a cascade of nested L p -norms and therefore allow for different values of p for different groups of filters. While this family includes the L p -spherical family and ISA models, it also includes densities that avoid the hard partition into either factorial or L p -spherical. At the same time, parameter estimation for these models can still be similarly efficient and robust as for L p -spherically symmetric models. We find that this family (i) fits the data significantly better than ISA and (ii) generates interesting filters which are grouped in a sensible way within the hierarchy. We also find that, although the difference in performance between L p -spherical and L p -nested models is significant, it is small on 8  X  8 patches, suggesting that within this limited spatial range, the iso-probability contours of the joint density can still be reasonably approximated by a single L p -norm. Preliminary results on 16  X  16 patches exhibit a more pronounced difference between the L p -nested and the L p -spherically symmetric distribution, suggesting that the change in p becomes more important for modelling densities over a larger spatial range. L -Nested Symmetric Distributions Consider the function We call this type of functions L p -nested and the resulting class of distributions L p -nested symmetric . L -nested symmetric distributions are a special case of the  X  -spherical distributions which have a density characterized by the form  X  ( y ) = g (  X  ( y )) where  X  : R n  X  R is a positively homogeneous function of degree one, i.e. it fulfills  X  ( a y ) = a X  ( y ) for any a  X  R + and y  X  R n [7]. L p -nested functions are obviously positively homogeneous. Of course, L p -nested functions of L p -nested functions are again L p -nested. Therefore, an L p -nested function f in its general form can be visualized by a tree in which each inner node corresponds to an L p -norm while the leaves stand for the coefficients of the vector y .
 Because of the positive homogeneity it is possible to normalize a vector y with respect to  X  and obtain a coordinate respresentation x = r  X  u where r =  X  ( y ) and u = y / X  ( y ) . This implies that the random variable Y has the stochastic representation Y . = RU with independent U and R [7] which makes it a generalization of the Gaussian Scale Mixture model [23]. It can be shown that for a given  X  , U always has the same distribution while the distribution % ( r ) of R determines the specific  X  ( y ) [7]. For a general  X  , it is difficult to determine the distribution of U since the partition function involves the surface area of the  X  -unit sphere which is not analytically tractable in most cases. Here, we show that L p -nested functions allow for an analytical expression of the partition function. Therefore, the corresponding distributions constitute a flexible yet tractable subclass of  X  -spherical distributions.
 In the remaining paper we adopt the following notational convention: We use multi-indices to index single nodes of the tree. This means that I =  X  denotes the root node, I = (  X  ,i ) = i denotes its i th child, I = ( i,j ) the j th child of i and so on. The function values at individual inner nodes I are denoted by f I , the vector of function values of the children of an inner node I by f I, 1: ` I = ( f children of a particular node I is denoted by ` I .
 L -nested symmetric distributions are a very general class of densities. For instance, since every L p -norm k X k p is an L p -nested function, L p -nested distributions includes the family of L p -spherically symmetric distributions including (for p = 2 ) the family of spherically symmetric distributions. When e.g. setting f = k X k 2 or f = ( k X k p 2 ) 1 /p , and choosing the radial distribution % appropriately, one can recover the Gaussian  X  ( y ) = Z  X  1 exp  X  X  y k 2 2 or the generalized spherical Gaussian  X  ( y ) = Z  X  1 exp (  X  X  y k p 2 ) , respectively. On the other hand, when choosing the L p -nested function f as in equation (1) and % to be the radial distribution of a p -generalized Normal distribution % ( r ) = Z ISA model. Note, however, that not all ISA models are also L p -nested since L p -nested symmetry requires the radial distribution to be that of a p -generalized Normal.
 In general, for a given radial distribution % on the L p -nested radius f ( y ) , an L p -nested symmetric distribution has the form This means that the partition function of a general L p -nested symmetric distribution is the partition function of the radial distribution normalized by the surface area of the L p -nested sphere with radius f ( y ) . For a given f and a radius f  X  = f ( y ) this surface area is given by the equation where I denotes the set of all multi-indices of inner nodes, n I the number of leaves of the subtree under I and B [ a,b ] the beta function. Therefore, if the partition function of the radial distribution can be computed easily, so can the partition function of the multivariate L p -nested distribution. Since the only part of equation (2) that includes free parameters is the radial distribution % , maximum likelihood estimation of those parameters  X  can be carried out on the univariate distribution % only, because This means that parameter estimation can be done efficiently and robustly on the values of the L p -nested function.
 Since, for a given f , an L p -nested distribution is fully specified by a radial distribution, changing the radial distribution also changes the L p -nested distribution. This suggests an image decomposi-tion constructed from a cascade of nonlinear, gain-control-like mappings reducing the dependence between the filter coefficients. Similar to Radial Gaussianization or L p -Radial Factorization algo-rithms [12; 21], the radial distribution %  X  of the root node is mapped into the radial distribution of a p -generalized Normal via histogram equalization, thereby making its children exponential power distributed and statistically independent [22]. This procedure is then repeated recursively for each of the children until the leaves of the tree are reached.
 Below, we estimate the multi-information (MI) between the filters or subtrees at different levels of the hierarchy. In order to do that robustly, we need to know the joint distribution of their values. In particular, we are interested in the joint distribution of the children f I, 1: ` I of a node I (e.g. layer 2 in Figure 2). Just from the form of an L p -nested function one might guess that those children are L -spherically symmetric distributed. However, this is not the case. For example, the children f 1: `  X  of the root node (assuming that none of them is a leaf) follow the distribution This implies that f 1: `  X  can be represented as a product of two independent random variables Dir n 1 /p  X  ,...,n `  X  /p  X  following a Dirichlet distribution (see Additional Material). We call this distribution a Dirichlet Scale Mixture (DSM) . A similar form can be shown for the joint distribution of leaves and inner nodes (summarizing the whole subtree below them). Unfortunately, only the children f 1: `  X  of the root node are really DSM distributed. We were not able to analytically cal-culate the marginal distribution of an arbitrary node X  X  children f I, 1: ` I , but we suspect it to have a similar form. For that reason we fit DSMs to those children f I, 1: `  X  in the experiments below and use the estimated model to assess the dependencies between them. We also use it for measuring the dependencies between the subspaces of ISA. Fitting DSMs via maximum likelihood can be carried out similarly to estimating L p -nested distri-butions: Since the radial variables u and r are independent, the Dirichlet and the radial distribution can be estimated on the normalized data points { u i } m i =1 and their respective norms { r i } m i =1 inde-pendently.
 L -Spherically Symmetric Distributions and Independent Subspace Analysis The family of L -spherically symmetric distributions are a special case of L p -nested distributions for which f ( y ) = k y k p [9]. We use the ISA model by [10] where the filter responses y are modelled by a factorial combination of L p -spherically symmetric distributions sitting on each subspace Given an image patch x , all models used in this paper define densities over filter responses y = W x of linear filters. This means, that all models have the form  X  ( y ) = | det W | X   X  ( W x ) . The ( n  X  1)  X  n matrix W has the form W = QSP where P  X  R ( n  X  1)  X  n has mutually orthogonal rows and projects onto the orthogonal complement of the DC-filter (filter with equal coefficients), S  X  R ( n  X  1)  X  ( n  X  1) is a whitening matrix and Q  X  SO n  X  1 is an orthogonal matrix determining the final filter shapes of W . When we speak of optimizing the filters according to a model, we mean optimizing Q over SO n  X  1 . The reason for projecting out the DC component is, that it can behave quite differently depending on the dataset. Therefore, it is usually removed and modelled separately. Since the DC component is the same for all models and would only add a constant offset to the measures we use in our experiments, we ignore it in the experiments below.
 Data We use ten pairs of independently sampled training and test sets of 8  X  8 ( 16  X  16 ) patches from the van Hateren dataset, each containing 100 , 000 ( 500 , 000 ) examples. Hyv  X  arinen and K  X  oster [10] report that ISA already finds several subspaces for 8  X  8 image patches. We perform all exper-iments with two different types of preprocessing: either we only whiten the data (WO-data), or we whiten it and apply an additional contrast gain control step (CGC-data), for which we use the radial factorization method described in [12; 21] with p = 2 in the symmetric whitening basis.
 We use the same whitening procedure as in [21; 6]: Each dataset is centered on the mean over examples and dimensions and rescaled such that whitening becomes volume conserving. Similarly, we use the same orthogonal matrix to project out the DC-component of each patch (matrix P above). On the remaining n  X  1 dimensions, we perform symmetric whitening (SYM) with S = C  X  1 2 where C denotes the covariance matrix of the DC-corrected data C = cov [ PX ] .
 Evaluation Measures We use the Average Log Loss per component (ALL) for assessing the qual-ity of the different models, which we estimate by taking the empirical average over a large ensemble if the model distribution equals the true distribution and is larger otherwise. For the CGC-data, we adjust the ALL by the log-determinant of the CGC transformation [11]. In contrast to [10] this al-lows us to quantitively compare models across the two different types of preprocessing (WO and CGC), which was not possible in [10].
 In order to measure the dependence between different random variables, we use the multi-information per component (MI) 1 n  X  1 P d i =1 H [ Y i ]  X  H [ Y ] which is the difference between the sum of the marginal entropies and the joint entropy. The MI is a positive quantity which is zero if and only if the joint distribution is factorial. We estimate the marginal entropies by a jackknifed MLE entropy estimator [17] (corrected for the log of the bin width in order to estimate the differen-tial entropy) where we adjust the bin width of the histograms suggested by Scott [19]. Instead of the joint entropy, we use the ALL of an appropriate model distribution. Since the ALL is theoretically always larger than the true joint entropy (ignoring estimation errors) using the ALL instead of the joint entropy should underestimate the true MI, which is still sufficient for our purpose. Parameter Estimation For all models (ISA, DSM, L p -spherical and L p -nested), we estimate the parameters  X  for the radial distribution as described above in Section 2. For a given filter matrix W the values of the exponents p are estimated by minimizing the ALL at the ML estimates  X   X  over p = ( p 1 ,...,p q ) &gt; . For the L p -nested distributions, we use the Nelder-Mead [15] method for the optimization over p = ( p 1 ,...,p q ) &gt; and for the L p -spherically symmetric distributions we use Golden Search over the single p . For the ISA model, we carry out a Golden Search over p for each subspace independently. For the L p -spherical and the single models on the ISA subspaces, we use a search range of p  X  [0 . 1 , 2 . 1] on p . For estimating the Dirichlet Scale Mixtures, we use the fastfit package by Tom Minka to estimate the parameters of the Dirichlet distribution. The radial distribution is estimated independently as described above.
 When fitting the filters W to the different models (ISA, L p -spherical and L p -nested), we use a gradient ascent on the log-likelihood over the orthogonal group by alternating between optimizing the parameters p and  X  and optimizing for W . For the gradient ascent, we compute the standard Euclidean gradient with respect to W  X  R ( n  X  1)  X  ( n  X  1) and project it back onto the tangent space of SO n  X  1 . Using the gradient  X  W obtained in that manner, we perform a line search with respect to t using the backprojections of W + t  X  X  W onto SO n  X  1 . This method is a simplified version of the one proposed by [14].
 Experiments with Independent Subspace Analysis and L p -Spherically Symmetric Distribu-tions We optimized filters for ISA models with K = 2 , 4 , 8 , 16 subspaces embracing 32 , 16 , 8 , 4 components (one subspace always had one dimension less due to the removal of the DC component), and for an L p -spherically symmetric model. When optimizing for W we use a radial  X  -distribution for the L p -spherically symmetric models and a radial  X  p distribution ( k y I k k p k p the models on the single single subspaces of ISA, which is closer to the one used by [10]. After optimization, we make a final optimization for p and  X  using a mixture of log normal distributions ( log N ) with K = 6 mixture components on the radial distribution(s).
 L -Nested Symmetric Distributions As for the L p -spherically symmetric models, we use a radial  X  -distribution for the optimization of W and a mixture of log N distributions for the final fit. We use two different kind of tree structures for our experiments with L p -nested symmetric distributions. In the deep tree (DT) structure we first group 2  X  2 blocks of four neighboring SYM filters. Afterwards, we group those blocks again in a quadtree manner until we reached the root node (see Figure 2 A ). The second tree structure (PND k ) was motivated by ISA. Here, we simply group the filter within each subspace and joined them at the root node afterwards (see Figure 2 B ). In order to speed up parameter estimation, each layer of the tree shared the same value of p .
 Multi-Information Measurements For the ISA models, we estimated the MI between the filter responses within each subspace and between the L p -radii k y I k k p k , 1  X  k  X  K . In the former case we used the ALL of an L p -spherically symmetric distribution with especially optimized p and  X  , in the latter a DSM with optimized radial and Dirichlet distribution as a surrogate for the joint entropy. For the L p -nested distribution, we estimate the MI between the children f I, 1: ` I of all inner nodes I . In case the children are leaves, we use the ALL of an L p -spherically symmetric distribution as surrogate for the joint entropy, in case the children are inner nodes themselves, we use the ALL of an DSM. The red arrows in Figure 2 A exemplarily depict the entities between which the MI was estimated. Figure (2) shows the optimized filters for the DT and the PND 16 tree structure (we included the filters optimized on the first of ten datasets for all tree structures in the Additional Material). For both tree structures, the filters on the lowest level are grouped according to spatial frequency and orientation, whereas the variation in orientation is larger for the PND 16 tree structure and some filters are unoriented. The next layer of inner nodes, which is only present in the DT tree structure, roughly joins spatial location, although each of those inner nodes has one child whose leaves are global filters.
 When looking at the various values of p at the inner nodes, we can observe that nodes which are higher up in the tree usually exhibit a smaller value of p . Surprisingly, as can be seen in Figure 3 B and C , a smaller value of p does not correspond to a larger independence between the subtrees, which are even more correlated because almost every subtree contains global filters. The small value of p is caused by the fact that the DSM (the distribution of the subtree values) has to account for this correlation which it can only do by decreasing the value of p (see Figure 3 and the DSM in
