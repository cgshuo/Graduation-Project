 Investment decisions in the financial markets require careful analysis of information available from multiple data sources. In this paper, we present Atlas, a novel entity-based infor-mation analysis and content aggregation platform that uses heterogeneous data sources to construct and maintain the  X  X cosystem X  around tangible and logical entities such as or-ganizations, products, industries, geographies, commodities and macroeconomic indicators. Entities are represented as vertices in a directed graph, and edges are generated using entity co-occurrences in unstructured documents and super-vised information from structured data sources. Significance scores for the edges are computed using a method that com-bines supervised, unsupervised and temporal factors into a single score.

Important entity attributes from the structured content and the entity neighborhood in the graph are automatically summarized as the entity  X  X ingerprint X . A highly interactive user interface provides exploratory access to the graph and supports common business use cases.

We present results of experiments performed on five years of news and broker research data, and show that Atlas is able to accurately identify important and interesting con-nections in real-world entities. We also demonstrate that Atlas entity fingerprints are particularly useful in entity sim-ilarity queries, with a quality that rivals existing human-maintained databases.
 E.1 [ DATA STRUCTURES ]: Graphs and networks; H.4.2 [ Information Systems Applications ]: Decision support Algorithms, Design, Experimentation, Performance
Financial analysts and investors often rely on news stories, press releases and legal documents to make critical invest-ment decisions. However, facts or events reported in a sin-gle document are rarely sufficient for an investor to make an informed decision. Therefore, investors must conduct addi-tional research that typically involves browsing and search-ing multiple structured and unstructured databases. Since the windows of opportunity in financial markets are typi-cally short, investors often face a difficult tradeoff between conducting thorough research and making timely decisions.
We observe that an interesting property of new documents is that they sometimes substantially increase the significance of existing documents. This is because significant new facts or events that are related to an entity tend to revive interest in older information about that entity. Often, this  X  X oost X  in information significance propagates to entities that may not even be mentioned in the new document. Due to this phe-nomenon, seemingly unrelated news stories may have sub-stantial financial implications on real-life assets.
For example, the initial news alerts reporting the recent political unrest in Egypt merely reported the events. Since Egypt is one of the largest importers of wheat in the world, the unrest soon negatively impacted wheat prices in the in-ternational markets, which in turn may have impacted the financial standing of some of the commodity-based invest-ment vehicles and private and publicly traded companies with significant exposure to wheat, in addition to compa-nies with exposure to the country or the region in general. Again, existing databases (and news archives) are likely to contain all information needed to analyze possible implica-tions of the unrest, but an average investor may not even be able to find all relevant information before the markets start responding to the events.

In this paper, we present Atlas, a novel entity-based infor-mation analysis and content aggregation platform that uses heterogeneous structured and unstructured data sources to construct and maintain the  X  X cosystem X  around tangible and logical entities such as organizations, products, industries, geographies, commodities and macroeconomic indicators. At-las uses a directed graph as the main data structure where en tities are represented as vertices, and edges are generated using entity co-occurrences in unstructured documents and supervised information from structured data sources. To fa-cilitate efficient access to underlying content and to allow for temporal analysis, vertices and edges store pointers to rele-vant unstructured and structured content and metadata.
Since each entity may be connected to a large number of other entities, Atlas computes significance scores for edges using a novel method that combines supervised, unsuper-vised and temporal factors into a single score. In addition, Atlas automatically summarizes important entity attributes from the structured content and the entity neighborhoods in the graph into entity  X  X ingerprints X . As we show in Sec-tion 6.2.1, these fingerprints are particularly useful in entity similarity queries. Finally, a highly interactive user inter-face provides exploratory access to the graph and supports common business use cases.

The rest of this paper is organized as follows. We begin with a discussion of related work (Section 2) and provide an overview of Atlas system architecture (Section 3). We then describe the graph construction in detail (Section 4) provid-ing information on the data sources used (Section 4.1), issues and challenges encountered (Section 4.2.1), the computation of significance scores for edges (Section 4.3), the generation of entity fingerprints (Section 4.4), and the characteristics of the generated graph (Section 4.5). Next, we provide an overview of the user interfaces (Section 5) and empirically evaluate the graph X  X  performance (Section 6). We finally conclude and discuss ideas for future work (Section 7).
Our research relates to existing work in entity extrac-tion and disambiguation, entity-relation graphs, news event tracking and news document chaining, standardized analy-sis and management of unstructured information, and cross-database visualization, along with other research areas in in-formation retrieval, database management, knowledge man-agement and machine learning. We discuss a few represen-tative methods and systems here.

Entity extraction and disambiguation serves as the ba-sic building block in our system. Earlier entity extraction systems were mostly rule-based [11, 24, 15] whereas statis-tical methods [28, 12, 10] gained more popularity in recent years. These methods convert the entity extraction task to a problem of decomposing the unstructured text, and then labeling various parts of the decomposition [25]. Common methods of decomposing unstructured text include splitting the unstructured text along a predefined set of delimiters or into word chunks using NLP-based methods. Statistical methods such as Hidden Markov Models [1, 3], Maximum-Entropy-based methods [4], and Conditional Random Fields [18] are popular for labeling the decomposed text. Since multiple extracted entities may represent the same physi-cal entity (e.g., International Business Machines and IBM both represent the same company,) extracted entities must be disambiguated to avoid unnecessary duplication.
Bunescu and Pasca [6] used Wikipedia to train a disam-biguation SVM kernel whereas Cucerzan [9] proposed a dis-ambiguation process that focuses on maximizing the agree-ment between the document context, Wikipedia context, and the category tags associated with the candidate enti-ties. In contrast, Hassell et al. [14] used an Ontology as background knowledge for entity disambiguation.

Entity-Relation (ER) graphs have been proposed to model entity relationships. Chakrabarti et al. [8] used ER graphs to represent personal information networks, where nodes represented entities such as organizations, people, places, events, projects etc and edges represented explicit or proba-bilistic relationships obtained by parsing unstructured text. A proximity-based query language supported queries on the graph. In follow-up work [7], they used a PageRank-like method to improve the query execution performance. Minkov and Cohen [22] also used ER graphs to model personal in-formation, and used finite graph walks to induce a measure of entity similarity and to facilitate searching the graph.
News event tracking and finding connections between news stories is another active research area that is related to our work. Nallapati et al. [23] studied the problem of recog-nizing events and their dependencies in news stories. They generated a graph structure by discovering sub-clusters in news events and organizing them by their dependencies. In a similar approach, Mei and ChengXiang [21] discovered la-tent themes from text, constructed an evolution graph of themes and used HMMs to analyze the life cycle of these themes. Recently, Shahaf and Guestrin [27] studied the problem of finding coherent chains that connect a pair of news stories. They formalized the notion of story coherence as a linear program, and used a bipartite graph to measure the influence of a document on other documents. They also proposed methods to find and evaluate coherent chains.
Managing, analyzing and visualizing data from many struc-tured and unstructured data sources is often challenging. Ferrucci and Lally [13] proposed a middleware that provides standardized interfaces to acquire, analyze and access un-structured data. Their framework supports document-level and collection-level analysis and enables semantic searches and standardized access to resulting metadata. Lieberman et al. [19] proposed a query-based approach to visualize and explore heterogenous biomedical databases. They modeled database records as nodes in an ER graph, and used edges to link related records. Keyword-based queries return an initial set of nodes, and the user is then able to explore the links to records from multiple databases in a unified way.
Existing entity centric content aggregation systems are either community maintained or use proprietary methods. Freebase 1 is a community maintained entity graph that con-tains information on about 20 million 2 unique entities. En-tities are associated with one or more types, and may have additional properties: they are stored as nodes in a graph database and links represent relationships between entities. DBpedia [2] is another community effort that aims to ex-tract structured information about entities from Wikipedia and make it accessible on the web. Various facts about enti-ties, relationships to other sources, classifications in multiple concept hierarchies and data-level links to other web data sources are also maintained. The OKKAM [5] project aims to create a web-scalable entity name system to enable entity-centric information integration, and Quid 3 uses proprietary methods to map the world X  X  technologies with a goal of help-ing businesses identify their next strategic opportunities.
Similarly to the community maintained systems, Atlas uti-lizes supervised information about entities and their rela-tionships, where available. However, our work differs from h ttp://www.freebase.com
At the time of writing http://www.quid.com th ese systems in that we do not wholly rely on supervised information to add entities to the graph, or to establish con-nections. Instead, we use any available entity extraction and resolution system to find entities in unstructured documents, establish entity connections based on co-occurrences in these documents, and apply a novel method to compute the signifi-cance of these connections. In addition, we may then use any available supervised information, entity-to-document map-pings, and the entity X  X  neighborhood in the graph to auto-matically summarize important attributes as an entity  X  X in-gerprint, X  resulting in a significantly more scalable system.
The underlying data structure of the Atlas system is a complex directed graph, where each vertex represents an entity and each edge a connection between entities. The overall Atlas system, however, also includes the creation of this graph (Section 4) and a set of query interfaces that support a number of real-world use cases (Section 5). Here we introduce the terminology used in the rest of this paper and present the architecture of our system, along with tools and technologies used in various system components, and describe in more detail the fundamentals of the graph itself.
The key component in the Atlas system is that of an en-tity. An entity in this model may be a tangible reality, such as a person or a commodity; or an intangible concept, such as inflation, or a war. Both vertices and edges have prop-erties and references to relevant documents, and a single edge may represent multiple types of relationships between a source and a target vertex. The entity types used in the ver-sion of Atlas presented here are organizations, geographies, products, industries, commodities and macroeconomic indi-cators. Further entity types available in alternate versions of Atlas include people, technologies, facilities and media.
Entities may either be validated or discovered . Vali-dated entities are mapped to a known entity from a human-maintained structured data source whereas discovered enti-ties are solely obtained from unstructured data sources.
The fundamental principle behind Atlas is that if two en-tities co-occur in a document, then a relationship between them is present. Two key caveats to this assumption must be fully understood in order to faithfully implement it: firstly, it must be understood that the simple appearance of a text string matching an entity in a document does not mean that the document is about that entity; secondly, we must realize that whilst, on a per-document basis, this is likely to result in false-positive relationships, that over the aggregate of mil-lions of documents, these false positives will have a negligi-ble impact on the usefulness of the Atlas graph, as they are likely to receive low significance scores (Section 4.3).
Thus, Atlas takes in a wide range of unstructured docu-ments (Section 4.1) and the graph is built using the occur-rences of entities in these documents as raw data, combined with any structured information that may also be available. The core of the resulting system therefore loosely relates to the semi-supervised learning paradigm.
The majority of queries to the Atlas graph specify one or more entities as a parameter along with additional infor-mation to fine tune the returned results. The queries are answered by analyzing (a) the query entities X  intrinsic prop-erties (such as a person X  X  birth-date or an organization X  X  country of incorporation) (b) the query entities X  ngerprint , an abstraction of the entity X  X  properties (Section 4.4) and (c) up to degree-2 neighbors of the query entities in the graph.
Atlas makes use of as many types of evidence as are avail-able, but is able to provide meaningful answers when noth-ing more than the name of an entity and a set of documents in which the name appears are available. The more com-plete the data available, the more precise the answers re-turned are, but much may be inferred about an entity from its neighbors in the graph.
The main programming language used in the development of the Atlas system is Java. The graph is loaded in memory as a runtime Java object, and persisted to the file system as a serialized Java object. All information needed to construct the graph from scratch is stored in a relational database. Due to the scale and complexity of the application, a number of open-source libraries were used in developing the system. Amongst there were: Apache Lucene and XML Beans, Jetty and VTD-XML.

The Atlas system is deployed as a set of services, accessed by Flash or HTML-based client applications (Section 5), or programmatically by a number of products and inter-nal tools that master and update content for our organiza-tion. Requests and responses are sent using either standard HTTP, XML over HTTP, or batched using Google X  X  Pro-tocol Buffers 4 . Data partitioning is used to allow vertical and horizontal scalability, i.e., the graph is distributed be-tween multiple nodes in a de-centralized peer network, each of which owns a subset of documents or entities. Proto-col Buffers are used for inter-node communication to service user requests in the distributed graph (Figure 1).
Atlas graph construction (Figure 2) begins by selecting suitable documents from all data sources (Section 4.1) and h ttp://code.google.com/p/protobuf p reprocessing these documents (Section 4.1.1). The selected documents are then processed to extract entities and to re-solve these against structured data sources (Section 4.2).
At this stage, links  X  which are in essence, the edges of the graph  X  between these resolved entities are created, and the significance of these links is established (Section 4.3). As a final step, an intrinsic representation of each entity that we call its ngerprint is then created (Section 4.4) using avail-able supervised information, entity to document mappings, and the entity neighborhood in the newly established graph. 3.8 million news stories from the Reuters News Service published between December 2005 and October 2010, and 110,000 broker research documents from a number of leading financial institutions from the same time period were used to build the graph evaluated in this paper. Multiple human-maintained, structured databases available within our orga-nization or from on web were also used. The  X  X roduct Mas-ter X  database was constructed from Freebase, and contains about 8,000 product names. The TRCS 5 database contained structured information relating to the news documents. For organizations,  X  X rganization Authority X  contains corporate, industrial and geographical information for public and pri-vate companies;  X  X usiness Sectors X  maps companies to pre-defined industries; and X  X ompetitors, X  X ontains a list of com-petitors for a small fraction of public companies.
News is written in takes (incremental updates), each of which is published as soon as it is ready. To avoid duplica-tion, and to allow the users to correlate all takes of a story, journalists use the same unique identifier for all takes of a story. However, there are cases where the same story is filed under multiple identifiers, causing duplications. Since these duplications may inflate the statistics, we use simple heuris-tics such as headline matches and cosine similarity within a time window to drop these duplicates.

In a separate problem, news and broker research docu-ments are heavily sprinkled with boilerplate data  X  stan-dard terms and phrases which often mention company names
TR CS (Thomson Reuters Classification System) is a man-ually applied journalistic classification scheme, which was available to us for news documents, with tags such as OILG for a news story about Oil and Gas.
 and can cause misleading connections to be formed. In our removal algorithm, we hash each period and/or newline-delimited sentence that occurs in the entire dataset, and eliminate those which occur more than a given number of standard deviations from the mean. One of the most com-mon pieces of Boilerplate seen by Atlas is provided below:  X  X his information is provided by RNS The company news service from the London Stock Exchange X 
After these steps, any documents which are mostly empty were removed. Further, any documents which mention more than a given number of entities n were also not considered when building our graphs, as for high values of n no exam-ples were found where n! / 2! (n-2)! meaningful connections were actually present in the document. The value of n used in our experiments was empirically selected as 20.
Once pre-processing is complete, we aim to find the en-tities mentioned in each document, and where possible, re-solve these mentions to a structured dictionary of known entities, forming the basis of the Atlas graph. As we discuss in the next section, attempting to deal with every instance of every entity type supported by Atlas that was mentioned in five years of news turned out to be a significant challenge. Entity extraction was mostly dealt with through OneCalais, the enterprise version of OpenCalais 6 provided by ClearForest, a Thomson Reuters company. OneCalais uses NLP and machine learning techniques and has been commonly used by the research community [26, 17, 16]. Whilst Atlas contains additional features to extract entities that Calais currently has little support for, these are quite trivial in nature and focused mainly on basic string match-ing. The Calais platform also provides some degree of entity resolution against structured data, but in cases where it was unable to do so, the job of entity resolution fell to Atlas. At-las X  entity resolution mechanisms included string similarity, aliases from user feedback and expert-supplied rules. In addition to extracting entities from documents, OneCalais also provided a list of social tags associated with each document. Social tags are a new feature within OneCalais that provide automated classification based on an organic taxonomy derived from Wikipedia.
It was discovered that in our structured datasets, enti-ties are often listed by unintuitive names, such as the full legal names of organizations. Large corporations may have many subsidiary companies, and structured data on the re-lationship between these subsidiaries was not always avail-able. For example, Microsoft has at least 70 separate legally-recognized organizations worldwide. Where a parent-child chain was available in structured data, Atlas made use of it by counting each mention of a child as a mention of itself, and of every parent in the chain. A configurable parameter allows Atlas users to select the degree of desired granularity, i.e., from considering every legal entity to only considering top-level parent organizations. In cases where this infor-mation was not available in structured data, a number of heuristics were applied in an attempt to identify such rela-tionships. Of particular utility was a list of legal suffixes for companies that was made available to us by domain experts. h ttp://www.opencalais.com/
En tity extraction is not an exact science and a number of erroneous matches were returned by the OneCalais en-gine. Since our significance computation method tends to assign low scores to connections involving erroneous enti-ties, these had little impact on aggregate results, but they became prominent when we developed user interfaces to dis-play emerging or unusual relationships. Some common pat-terns that required special handling involves the journalis-tic style of listing companies in news, e.g.  X  X ntel Samsung Toshiba join hands to halve chip size  X  Nikkei X  was origi-nally extracted as a company called Intel Samsung Toshiba. Certain companies with short names containing common English words proved very difficult to accurately extract and resolve to. Examples include Business (a French pub-lication) and General Corporation (an American realtor). Where non-English words were present in the dataset, these often provided entity resolution problems, a common issue being person names extracted as organizations.

A final common issue in entity resolution was that of dis-tinguishing between mentions of a location, and mentions of that location X  X  governing body:  X  X ew York State, X  for example, is interchangeably used to refer to either the geo-graphical area or the governing body of that area.
It is an unfortunate fact that those who report the news are often incidentally mentioned within it, despite having no connection to the story itself. Whilst our boilerplate removal algorithm was able to strip out the vast majority of inappropriate mentions of media companies, a significant number remained in free text. An example from our dataset is  X ...from Morgan Stanley told Reuters in an interview on Friday that. X  Whilst entity extraction and resolution was perfect in this example, it skewed Atlas X  results as no use-ful connection exists between Morgan Stanley and Reuters. Ratings agencies were also affected by this problem.
The current version of Atlas does not handle these situa-tions well. In the future, we plan to incorporate NLP-based rules to handle common patterns that exhibit this problem, and also to utilize relevance scores returned by OneCalais.
It can be seen from a cursory examination of our data that large financial institutions truly have a sphere of influ-ence that touches nearly every point of the corporate world. Whether providing funding, services or advice, or buying or selling products, genuine relationships seem to exist between any given large financial organization and tens of thousands of other companies. Whilst aggregate results again solve the issue of ranking a large financial X  X  relationships, two prob-lems emerge from this situation.

The first is that, for small companies with few mentions, a large financial may be seen not only as a relatively strong connection, but also as a similar entity (given that the two appear in the same documents with relatively high frequency and therefore acquire the same neighbor entities). The sec-ond is that it can become very difficult to statistically guess or use attributes about the large financial: whilst they may be based in a given geography for example, it may be that the companies they are advising are doing business in a com-pletely different geography. The chained relationship which is therefore formed is not always true or meaningful, and can cause issues with a number of metrics.
A final issue in dealing with large datasets was that of optimization. As an example, there were over 0.75 billion pairs of organizations for which the similarity scores needed to be calculated in the graph evaluated in this paper. Dis-tributing the workload achieved only so much, and the use of vertical bitmaps to compare entities (e.g. based on doc-ument co-occurrence or possession of structured attributes) was a key factor in improving the performance of some of the algorithms used to generate the final graph.
As we have discussed in Section 3.2, Atlas uses entity co-occurrences in unstructured documents as primary means to establish connections between entities. This approach indeed maximizes the recall, but results in a lot of noisy connections. We address this problem by assigning a sig-nificance score to each edge, where higher values indicate stronger connections. Since we use a directed graph, there are always two edges between each pair of connected entities, each of which is assigned a different significance score. This allows Atlas to model a common real-life situation where a given entity E 1 may be very significant for a connected entity E 2 , but E 2 may not be equally significant for E Consider, for example, two companies that compete in an area that represents the core business for one company but only a small fraction of the other company X  X  business. Figure 3 presents actual Atlas significance scores between Facebook and some of its neighbors. The scores indicate that Twitter is more significant for Facebook than Microsoft, and both are more significant than Apple. In contrast, Face-book is important for Microsoft but not equally significant: Microsoft is a major shareholder and investor in Facebook, but is also involved in a variety of other business areas. The relationship between Facebook and Google exhibits a sim-ilar behavior. By contrast, Facebook is the major threat to MySpace X  X  core business, making the most significant connection in Figure 3 the MySpace  X  &gt; Facebook edge; whereas MySpace X  X  actions are now much less important to Facebook, which is clear from the fact that the connection in the opposite direction is of nearly 1 3 th e strength.
To compute the significance scores, we have considered a variety of strategies, and finally selected a few that capture different aspects of the relationship and produce superior results when combined (Section 6.1). Therefore, the Atlas significance scores are computed as a weighted average of multiple factors (where weights were empirically selected). The factors we have evaluated (Section 6.1) include: a ) Interestingness: Computed by considering the source and target entities as two variables, populating a 2x2 contingency-table with their frequencies from all avail-able documents (as in Section 1 of [29]), and applying an interestingness measure on the contingency table. Orig-inally proposed for finding interesting association rules, interestingness measures have been successfully used in many applications [20]. We evaluated all measures in Ta-ble 5 of [29], and selected  X  X utual Information X  based on its superior performance on our data (another measure  X  X dded Value X  was very close in terms of performance).
We have omitted the interestingness measure comparison for the reason of space. b ) Recent Interestingness: Same as interestingness, but co mputed only using documents in a user-definable (fixed to 6 months in our experiments) recent period. This fac-tor aims to boost emerging relationships. c ) Validation: A value of 1 if the relationship between source and target entities was validated by a human ex-pert (in available structured data), 0 otherwise. d ) Common Neighbors: A percentage of the degree-1 neighbors of the source entity that also occur in the degree-1 neighborhood of the target entity. e ) Industry Overlap: A percentage of the industries in source entity X  X  neighborhood that also occur in the degree-1 neighborhood of the target entity. f ) Geography Overlap: A percentage of the geographies in source entity X  X  neighborhood that also occur in the degree-1 neighborhood of the target entity. g ) Temporal Signi cance: A comparison of the recent interestingness value, with an interestingness value com-puted from historic (non-recent) documents giving a value of 1 to the factor if the former increased by more than , a value of 0 if the the recent interestingness de-creased by more than or a value of 0.5 otherwise, where that had gained strength in the recent time period, and penalizes relationships that had lost strength. h ) Element of Surprise: Using the same definitions of recent and historic documents as in the previous factor, this factor is assigned a value of 1 if the source entity X  X  neighborhood contains any new industries or geographies in the recent period that did not occur in the historic period, and the target entity shares at-least one such in-dustry or geography, 0 otherwise.
A primary business use-case from our organization for At-las was the need to find companies that were similar to a given company, overlooking any geographical differences. Should such functionality be made available, a party with an interest in a successful catering company in Denmark would, for example, be able to find similar opportunities for invest-ment in the emerging Chinese markets. Once geographical boundaries were crossed, it soon became obvious that two similar companies  X  in our example, companies that may be affected by the same commodities and laws and share sim-ilar types of neighbors  X  may never co-occur in documents, and would therefore never have an edge between them in the Fi gure 3: Atlas signi cance scores between Face-book and some of its neighbors Atlas graph. However, the aggregate properties of each com-pany X  X  neighbors, and the other information we were able to infer about them from Atlas, allowed us to construct a n-gerprint , an abstract representation of the company which could be compared to any other fingerprint in order to cal-culate a similarity score and support this use-case.
An entity X  X  fingerprint is a multi-dimensional abstraction of the entity based on a number of its attributes. Once a fin-gerprint is obtained for an entity, it may then be compared to fingerprints of other entities to understand the similari-ties and differences that exist. Fingerprints may therefore also be used to generate feature vectors in classification and clustering tasks. In the version of the Atlas graph presented in this paper, fingerprints were calculated and used only for organizations, to support the business case described above.
The factors used to generate fingerprints were, in the ma-jority, related to the neighborhood of a given organization: the entities of a given type to which it was related and tem-poral aspects of these (e.g. one attribute was  X  X merging neighbors X , thus fingerprints can represent a point-in-time view of an entity). Structured information about the or-ganizations, such as the country they are incorporated in and the industries they are known to operate in, were also incorporated into the fingerprints. Finally, we have used entity to document relationships to add the top-k classifi-cation codes (i.e., TRCS codes) and top-k social tags (i.e., Wikipedia article titles related to a document, as determined by OneCalais) for each company to the fingerprint. This was achieved by sorting each TRCS code or Social Tag with re-spect to the number of documents that contained the target company and were also assigned with the TRCS code or Social Tag, and selecting k most frequent results.
Each company X  X  fingerprint therefore includes the follow-ing attribute groups: industry hierarchy; geography hierar-chy; related industries; related geographies; related macroe-conomic indicators; related commodities; related TRCS codes; related social tags; and related entities (in two group-ings  X  those that are emerging and those that are stable).
A feature vector is created for each attribute group, and a similarity score between two fingerprints is computed as a weighted linear combination of the cosine similarity scores of group feature vectors, where weights were empirically se-lected by domain experts. This score represents how alike two companies are, rather than simply how connected they are in the graph.
Presented in Table 1 are the number of distinct entities (nodes)  X  a total of 85,163  X  found in the graph. Between these, 13.3 million connections (edges) were found, with each node having an average of 156 connections (std. dev. 886). On a single server with 32 CPU cores and 128GB of RAM, the graph took a little over 9 hours to build, and included the creation of data-stores such that the vast majority of queries were comfortably sub-second.

The concept of validation is important in a number of cal-culations in the Atlas system, and in displaying information back to end users. Furthermore, in the case of organizations, non-validated nodes were almost always entity extraction or entity resolution errors. In use-cases that reward surprise, such as those looking for emerging trends or uncommon con-nections, these caused a significant issue.

An organization is said to be validated if it is present in a database containing all public, and a good number of pri-vate, companies. This database is maintained and updated by a large team of analysts and researchers and powers some of our company X  X  largest commercial products. It took the Atlas team hundreds of man-hours to reach 95% validation level for organizations, and this work led to improvements to both the internal database and the OneCalais core engine.
Geographies were also validated against an internal database, but in this case the 3% validation figure isn X  X  quite as damning. Whilst a significant number (nearly 60%) of ge-ographies in the graph are invalid, these occur infrequently and do not affect many of the primary use-cases of Atlas. The remaining number are simply small or hyper-localized geographies not meriting a place in the database used, but extracted nonetheless by the OneCalais engine.

Products were validated against a database compiled specifically for this project from the open-source repository Freebase. Those that are not validated were returned from the OneCalais entity extraction module and appear to be valid, in the majority, although no formal evaluation of this has yet been performed.
In order to effectively explore a complex graph with tens of thousands of nodes and millions of edges, a set of novel user interfaces were designed and developed. The focus was to deliver specific business use-cases which would directly benefit the customers of Atlas. A select few rich user inter-faces were developed in Adobe Flex &amp; Flash with a larger number of HTML  X  X elper pages X  made available for those who wished to drill deeper into the raw data. Examples of these include the ability to look up a filtered list of any entity X  X  connections, together with supporting documents; the ability to view a side-by-side overlap of the characteris-tics of two similar entities; or the ability to provide manual feedback for future resolution (e.g. that  X  X SFT X  should be resolved to  X  X icrosoft X  where no other evidence is available).
The Find Similar view in Atlas (Figure 4) makes use of the fingerprint (see Section 4.4) of an organization. For any given company, the Find Similar view shows a list of the most similar companies along with their fingerprints in a given time range, and highlights the reasons for the match.
Other useful information returned includes an overview of the fingerprint for all companies returned, and a graph showing how each resulting company has trended with the query company over a recent period, with document links.
The output of machine learning and information retrieval systems is typically evaluated against human-labeled gold data, using standard objective metrics such as precision and recall. There are an unknown number of useful  X  X uestions X  that could be asked of a system like Atlas, and many of these have very subjective and constantly-changing answers, mak-ing it extremely difficult to obtain and maintain labeled data for evaluation. Even for well-defined tasks (e.g., retrieving top-K neighbors for a given entity), a comprehensive eval-uation was resource-prohibitive because of the number of entities available in the system.

Therefore, we have evaluated Atlas by selecting represen-tative samples of entities; obtaining Atlas output for two common tasks; and having domain experts review and grade the output. The next two sections present the results of these experiments. We also present a subjective comparison of Atlas output against existing systems and analyze the fingerprint overlap between two entities.
Fifty companies were randomly selected for evaluation from the Justmeans Global 1000 Sustainable Performance Leaders list 7 , twenty-five from the top 500 and twenty-five from companies ranked between positions 500 and 1000. For each of these companies, the most significant fifty connec-tions in Atlas were manually evaluated, with human experts identifying each result as either a genuine real-world connec-tion; as a connection with no meaningful foundation in the real-world; or as a duplicate connection (including a dupli-cate of the source company).
A number of significance factors were discussed in Section 4.3, but each of these was found to incorrectly skew the re-sults when presented with certain classes of entity, or types of source information. A weighted approach, was therefore taken, and its precision evaluated relative to each individ-ual factor. A precision of 100% would be achieved where a significance factor X  X  top-K connections were all valid, non-duplicate connections, where K is the total number of valid, non-duplicate connections that exist for the given source company. The graphs in Figure 5 present the results of this experiment, with the y-axis representing precision and the x-axis enumerating each of the fifty source companies con-sidered, sorted in decreasing order of weighted significance precision. h ttp://www.justmeans.com/top-global-1000-companies T able 2: Average Performance of Signi cance Fac-tors
Whilst individual factors occasionally achieve higher pre-cision than the final weighted significance, the average per-formance of the latter was found to consistently outperform any individual factor (see Table 2).
In the previous section, we have evaluated various signif-icance factors and concluded that our weighted significance score outperforms individual factors. We now evaluate its usefulness in a real scenario. A major business use-case for our organization was that of being able to identify the im-portant commodities for any given geography  X  currently a manual and error-prone task. Twenty countries of varying size were selected for evaluation, and their top-ten commod-ity neighbors in Atlas were evaluated by a domain expert.
In Figure 6 we present the results of this evaluation, sorted in the increasing order of the number of documents that con-tained each country in our dataset. We note that without any thresholding on the Atlas results, the system was still able to achieve 76% precision, and also that the majority of false-positives found were located towards the lower ends of Atlas ranking lists. A significant exception here was Israel X  X  most significant commodity, Nuclear Power: an unfortunate example of entity resolution gone awry: the supporting doc-uments were referring to nuclear weapons.
It is difficult to find ground truth against which to eval-uate the lists of most similar companies produced by Atlas for a given entity. Similarity is a vague and subjective term, and often closely coupled with the context in which a user is searching. Here, the Atlas system is compared against two well-known and well-used datasets, Reuters Knowledge Fi gure 6: Top Commodity Neighbors for Countries
T able 3: Similar Companies for Flowers Foods Inc and Google Finance X  X  Related Companies. Reuters Knowl-edge provides a manually-maintained list of similar compa-nies generated by analysts from company publications and releases. Google, like Atlas, uses an algorithmic approach. The  X  X ost recent quarter X  option was selected in Google Finance, and a three-month time period selected in Atlas. Reuters Knowledge offers no time-sensitive options, and may list between 0 and 30 similar companies. Google offers only the top-ten most similar companies (and in the majority of cases seen, returns 10 companies), whereas we have access to the entire list for Atlas, which, without thresholding, often contains in excess of 30 results. The company we selected for detailed evaluation, based on the belief that it represents some of the key positive and negative features of Atlas in evaluating similar companies compared to Google Finance and Reuters Knowledge, is Flowers Foods Inc, a company based in the USA that markets a range of top-selling con-sumer brand packaged foods. Table 3 shows the top similar results for Flowers Foods in each dataset.

The first point to note is that entity resolution is ever an issue when dealing with organizations. Whilst Keebler Foods and Keebler Holding are indeed separate legal entities, for the vast majority of use-cases, the distinction is not a useful one. Further, it is not rare for companies to merge or be acquired, and the multiple processes by which this may occur may also lead to confusion in a system.

Whilst both Atlas and Google were able to nearly flaw-lessly 8 extract companies with very similar business offerings as Flowers Foods Inc, their ranking differed. Only three of the Atlas top 10 were found in the Google top 10, all the Google top 10 except Vitafort were found within Atlas X  top 20. The only clear pattern was that Atlas seemed to fa-vor similarity based on product offerings (core business), whereas Google favored companies based in the same coun-try  X  the USA  X  with the exception of Grupo Bimbo S.A.B., based in Mexico (with a significant American presence).
Over a sample of 25 companies from various industries and geographies, 87% of Google top 10 similar companies appeared in Atlas X  top 20, and 34% of Atlas X  top 10 similar companies appear in Google X  X  top 10.

Reuters Knowledge returned a mere three similar compa-nies for Flowers Foods, all of which were also identified by
V itafort being the exception, it appears to be an erroneous match from Google Finance: Vitafort is wholly involved in entertainment intellectual property. b oth Atlas and Google. There was a strong correlation here between the top companies in Google and the top companies in Reuters Knowledge, whereas Atlas ranked the three com-panies from Reuters in positions 12, 20 and 7 respectively  X  suggesting that Atlas X  ranking methodology may differ to that used in current products. Of the 25 sample companies, Flowers Foods was found to be one of the worst examples of Atlas X  rankings corresponding with Reuters Knowledge. When querying on Toll Brothers, Inc, for example, Reuters Knowledge provided five results which Atlas ranked in po-sitions 3,9,5,1 and 2 respectively. For a larger company, Google, Reuters Knowledge provided seven results, which Atlas ranked in positions 1,2,10,19,14,4 and 6 respectively.
We stress once again that Atlas is not intended to re-produce the results of either Google Finance or Reuters Knowledge, but find that correlation with these two well-used tools, together with expert opinions provided to us, show the utility of Atlas-generated similar companies.
In addition to using entity fingerprints to compute similar-ity, it may often be useful to understand in detail the over-lap between two entities. Take, for example, YouTube (now owned by Google) and Facebook. These are both prominent Web companies, which shows in the overlapping attributes they have. However, their differences may also be easily seen at a glance using the companies X  fingerprints (Table 4), in which a score of 1 implies an attribute owned only by YouTube, and a score of 1 an attribute owned only by Facebook. A score of 0 implies a perfect overlap.
In this example, we can see that the Atlas graph correctly provides a strong bias towards YouTube in considering the geography attribute  X  X an Mateo , X  YouTube X  X  home county. Despite having no presence in San Mateo, Facebook is some-what linked to it by documents mentioning both YouTube (with San Mateo as a corollary) and Facebook, a common problem in evaluating similarity for two entities that are connected in the graph. An extraction error resulting from the suffix -ville has led to the popular Facebook social game  X  X armville X  X eing identified as a geography, and this is there-fore understandably considered related only to Facebook; it will provide negative evidence in computing a similarity score between these two companies.

In the Social Tags attribute type,  X  X orld Wide Web X  is correctly a strongly shared connection, providing posi-T able 4: Fingerprint overlap for YouTube and Face-book tive evidence towards similarity, whereas  X  X ideo Hosting X  for YouTube and  X  X ocial Network Service X  for Facebook ex-emplify the core differences between these two companies X  activities on the Web.

In the Recent Organizations attribute type, Viacom Inc was an attribute most related to YouTube, and Twitter an attribute biased somewhat towards Facebook. Viacom of-fers many video services, and has a natural relationship with YouTube, but also made offers to buy Facebook on multiple occasions. Twitter competes with Facebook for the real-time microblogging market: although it is mentioned in many news stories alongside YouTube, the relationship is often in-cidental. The final bi-directional similarity score between YouTube and Facebook given was 56%, and each of these companies was top of the others X  most similar list. Face-book X  X  second most similar company (51%) was LinkedIn, which offers professional social networking; YouTube X  X  sec-ond most similar company (49%) was its parent, Google.
In this paper, we presented Atlas, an entity-centric in-formation analysis, content aggregation, and visualization platform that uses all available structured and unstructured content to automatically model the ecosystem around enti-ties. Atlas uses a novel method to compute the significance of entity connections, and automatically summarizes key en-tity attributes into a fingerprint. The results of our objective and subjective evaluation indicate that our significance com-putation method is able to reward important and interesting connections, and the fingerprints are useful in finding similar companies, an important use case in our domain.

In the future, we plan to work on automatically identifying relationship types for entity connections, incorporating Web and markets fundamental data in graph generation, and im-proving entity resolution for geographies and products. [1] E. Agichtein and V. Ganti. Mining reference tables for [2] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, [3] V. R. Borkar, K. Deshmukh, and S. Sarawagi.
 [4] A. Borthwick, J. Sterling, E. Agichtein, and [5] P. Bouquet, H. Stoermer, C. Niederee, and A. Ma  X na. [6] R. C. Bunescu and M. Pasca. Using encyclopedic [7] S. Chakrabarti. Dynamic personalized pagerank in [8] S. Chakrabarti, J. Mirchandani, and A. Nandi. Spin: [9] S. Cucerzan. Large-scale named entity disambiguation [10] A. Culotta, T. Kristjansson, A. Mccallum, and [11] H. Cunningham, D. Maynard, K. Bontcheva, and [12] T. G. Dietterich. Machine learning for sequential data: [13] D. Ferrucci and A. Lally. Uima: an architectural [14] J. Hassell, B. Aleman-Meza, and I. Arpinar.
 [15] J. R. Hobbs, J. Bear, D. Israel, and M. Tyson. Fastus: [16] F. Hopfgartner and J. Jose. Semantic user modelling [17] F. Iacobelli, L. Birnbaum, and K. J. Hammond. Tell [18] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. [19] M. D. Lieberman, S. Taheri, w. Guo, F. Mirrashed, [20] H. H. Malik, J. R. Kender, D. Fradkin, and [21] Q. Mei and C. Zhai. Discovering evolutionary theme [22] E. Minkov and W. W. Cohen. Improving [23] R. Nallapati, A. Feng, F. Peng, and J. Allan. Event [24] E. Riloff. Automatically constructing a dictionary for [25] S. Sarawagi. Information extraction. Found. Trends [26] S. Sen, N. Stojanovic, and R. Lin. A graphical editor [27] D. Shahaf and C. Guestrin. Connecting the dots [28] K. Takeuchi and N. Collier. Use of support vector [29] P.-N. Tan, V. Kumar, and J. Srivastava. Selecting the
