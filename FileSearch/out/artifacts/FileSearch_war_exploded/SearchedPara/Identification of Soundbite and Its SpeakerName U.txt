 FEIFAN LIU and YANG LIU The University of Texas at Dallas 1. INTRODUCTION The increasing amount of broadcast news (BN) speech data requires auto-matic processing techniques to effectiv ely manage and access information in it. Various techniques and applications have been developed for this purpose, such as speech recognition, information extraction and indexing, spoken doc-ument retrieval, visual browsing, automatic summarization, and question an-swering. Broadcast news speech contains many soundbite segments [Maskey and Hirschberg 2006] that are speech clips or interview quotations from some specific speakers other than reporters and anchors. Identifying soundbite seg-ments as well as the associated speakers is important for many speech under-standing and opinion question answering applications. For example, a user may want to know a person X  X  opinion o r statement about something among a large collection of broadcast news speech. It is likely that this information is contained in soundbite segments. Therefore for this kind of information need, an automatic system needs to identify the soundbites and associate speaker names with them. This task is also an important component for rich transcription of speech, where speech recognition output is enriched with speaker names and punctuation marks (soundbite speech corresponds to quotation marks).
 Section 4.1 for more information about the data). Speaker turns are anno-tated with type information ( anchor , reporter ,and other that corresponds to soundbites), and speaker names (if available from the transcripts) in Chinese characters (name-ch) and Chinese pinyin (name-py). The soundbite segment in this example is shown in the dashed box (the third speaker segment), cor-responding to the speech from Chen Shuibian . Compared to the unstructured ASR output, this well-formatted human transcript has better readability and is easier to use by downstream language processing modules.
 gated soundbite detection [Liu 2006; Maskey and Hirschberg 2006; Liu and Liu 2007b] and soundbite name recognition [Liu and Liu 2007a] separately, using either the reference transcripts or automatic speech recognition (ASR) output. In this article, we present a pipeline architecture for soundbite distil-lation that includes soundbite detection and its speaker name recognition. We use the term  X  X oundbite distillation X  hereafter to refer to the entire pipeline framework. Both of the two components in the framework use supervised clas-sification framework based on multiple linguistic features. Performance for each module as well as the entire system is thoroughly evaluated and sys-tematically analyzed. In addition, we investigate the effect of using ASR out-put on the system performance, from speech recognition errors and automatic sentence boundary detection errors respectively.
 some related work. Section 3 presents the details of our pipeline framework for soundbite distillation. We describe our experimental setup and results in Section 4 and 5 respectively. Conclusions and future work appear in Section 6. 2. RELATED WORK Soundbite distillation shares some similarities with speaker diarization [Tranter and Reynolds 2006] in speech processing. Recently speaker diariza-tion has seen significant progress partly because of the benchmark tests such as the NIST Rich Transcription evaluation [NIST 2004] and the Technolangue ESTER evaluation [Galliano et al. 2004]. The aim of speaker diarization is to partition speech data into homogeneous segments (i.e., speaker turns) and then group them according to the speaker X  X  identity. There have been a lot of studies on speaker diarization in broadcast news domain [Gauvain et al. 1998; Ben et al. 2004; Barras et al. 2006], as well as meeting domain [Pardo et al. 2007; Huang et al. 2007]. The focus of many previous studies has been mainly on detecting speaker changes and clustering speaker turns according to the same speakers. Different approaches have been proposed for speaker segmentation. Reynolds and Torres-Carrasquillo [2004] used Bayesian In-formation Criterion (BIC) technique [Chen and Gopalakrishnam 1998] to de-tect change points within a window using a penalized likelihood ratio test. Gauvain et al. [1998] and Sinha et al. [2005] represented each fixed length win-dow with a Gaussian distribution and measured the distance between those windows using divergence. The clustering step is achieved by a bottom-up fashion, that is, agglomerative clustering [Gauvain et al. 1998; Ajmera and Wooters 2003; Reynolds and Torres-Carrasquillo 2004; Ben et al. 2004], or a top-down manner, that is, divisive clustering (e.g., evolutive hidden Markov models in Fredouille and Senay [2006]).
 for speaker segmentation and clustering, and produce locally consistent relative labels (such as speaker1, speaker2) rather than true speaker iden-tities (such as Bill Clinton ). Moraru et al. [2004] built acoustic models for some known speakers for whom training data exists, and then used acoustic similarity to identify their reoccurrence in the test data. This method is use-ful for recognizing known or reoccurring speakers (identifying their speech segments); however, we may not have this kind of prior knowledge for soundbite speakers. Other systems have also been developed to explore lin-guistic patterns such as word n -grams using the audio transcripts to iden-tify the speaker names [Canseco et al. 2005; Tranter 2006; Est  X  eve et al. 2007]. Ma et al. [2007] presented a conditional maximum entropy framework for identifying speaker names in audio content. They combined lexical and acoustic information in the classifiers, and demonstrated better performance than the state-of-the-art n -gramsystemwhenusingthesamesetoflexical features.
 attention recently. Research in this line has concentrated on two tasks, namely soundbite detection and soundbite speaker name recognition. Maskey and Hirschberg [2006] used conditional random fields for soundbite detection on English BN corpus and reported an accuracy of 67.4%, compared to the base-line of 46.5%. However, the results were only shown for the ASR transcripts and the effect of ASR was not investigated. Similar work has been conducted that assigns one of the three types of speaker roles (anchor, reporter/journalist, other/guest speaker) to each speech turn, instead of detecting only soundbites (which generally correspond to the  X  X ther /guest speaker X  category). Barzilay et al. [2000] applied BoosTexter, a boosting algorithm, and a maximum en-tropy model in an English BN corpus. Liu [2006] combined generative HMM approach with a conditional maximum entropy method for speaker role iden-tification task in a Mandarin BN corpus. Both reported significantly better performance over the baseline. In our previous work [Liu and Liu 2007b], we compared the binary vs. three-way classification and different feature weight-ing approaches. There is limited prior work for soundbite speaker identity. In Liu and Liu [2007a], we employed a statistical classification framework based on a variety of linguistic features for soundbite speaker name recognition. Our experiments on a Mandarin BN speech c orpus showed a recognition accuracy of 68.9%, 10% better than the baseline of choosing the person name closest to the soundbite. However, those experiments were conducted using only human transcriptions and reference soundbite segments.
 distillation task, including soundbite segment detection and soundbite speaker name recognition. In this article, we use transcriptions of BN speech for sound-bite distillation, and evaluated the performance and effect of each module on the entire system. In addition, we investigate the effect of ASR output as well as automatic sentence segmentation on the system performance.
 3. STATISTICAL CLASSIFICATION FRAMEWORK FOR SOUNDBITE Figure 2 illustrates the components needed for soundbite distillation: ASR, speaker turn segmentation, soundbite segment detection, and soundbite speaker name recognition. This article focuses on the last two modules, both using transcripts (manual or automatic transcription) as their inputs. In this study, we use human annotated speaker turn segments in order to avoid the confounding effect from segmentation errors. The following sections ex-plain the details of the soundbite detection and speaker name recognition components. 3.1 Classification-based Soundbite Detection For soundbite detection, the input is transcripts of BN speech with speaker turn change information, and thus the task is to determine whether a seg-ment is soundbite speech. Generally there are three types of speakers in BN shows: anchor, reporter, and soundbite speakers [Barzilay et al. 2000; Liu 2006]; therefore, we can formulate the soundbite detection problem as either a binary classification task (soundbite versus not), or a three-way classifica-tion task (identify whether a speech segment is from an anchor, reporter, or soundbite speaker). The former may reduce some inter-class noise and the lat-ter may benefit from the more discriminative features among different classes. We will compare which of the two schemes is more effective for soundbite detec-tion. Note that we are assuming we have the additional annotation of anchor and reporter roles in the training set. If this is not available, the three-way classification setup does not apply. We choose to use an SVM classifier for the soundbite detection task because of its superior performance for many classifi-cation tasks. as those in Liu and Liu [2007b], including lexical features and structural information based on length.
  X  X F-current. Unigram and bigram features in the first and the last sentence of the current speech turn.
  X  X F-context. Unigram and bigram features from the last sentence of the previous turn and from the first sentence of the following turn. We expect this feature to reflect some functional transition among different speakers and thus be able to model the interdependency relations among neighboring speech turns.
 et al. [2000]. Typically, the soundbite turn consists of fewer sentences or words than the turns of anchors and reporters. In addition, we also hypothesize that professional speakers, such as anchors and reporters, tend to use longer and more complex sentences, whereas speech in soundbites may be more sponta-neous and more likely to contain shorter sentences. Therefore, we used the following features.  X  X umber of words in the current speech turn.  X  X umber of sentences in the current speech turn.  X  X verage number of words in each sentence in the current speech turn. speech turn is represented as a feature vector for classification. In text cat-egorization and information retrieval, term weighting has been shown to im-pact system performance [Salton and Buckley 1988; Yang and Pedersen 1997]. Since we are treating soundbite detection as a classification task and are using features (in particular lexical features) similar to those widely used in other text categorization tasks, it is important to evaluate what is the best way to represent the features for our task.

We compare four different feature weighting methods for soundbite detec-tion in this article. The notations we use for the description of feature weight-ing are as follows. N is the number of speech turns in the training collection, M is the total number of features, f ik is the frequency of feature  X  i in the k th speech turn, n i denotes the number of speech turns containing feature  X  i , F (  X  assigned to feature  X  i in the k th turn using different approaches.  X  X F (term frequency) weighting
This is simply the frequency of the feature:  X  X F  X  IDF (inverse document frequency) weighting
This weighting method was originally proposed and applied to document re-trieval task [Jones 1972; Salton and Buckley 1988]. The id f value for a term, log( N / n i ), represents its specificity X  X hether it is an indicative feature for a particular segment or it occurs in many segments. The tf  X  id f weighting for afeatureis:  X  X F  X  IWF (inverse word frequency) weighting Similar to the idea of id f , Basili et al. [1999] used inverse word frequency.
Both idf and i w f can penalize high-frequency terms. The tf  X  i w f weighting for a feature is:  X  X ntropy weighting
Entropy-based weighting has been shown to be the most effective weighting approach in comparison with others [Dumais 1991]. This method assigns different weights to features via the following equation: where is the average entropy of feature  X  i . An even distribution of feature  X  i across all the speech turns results in a high entropy, which means that the discrim-inating ability of the feature is low; therefore feature  X  i will be given a small weight based on Equation 4. 3.2 Classification-Based Soundbite Speaker Identification This task is to determine the speaker X  X  name for a soundbite segment using the contextual information from transcripts. We first use a named entity (NE) tagger to obtain all of the person X  X  name hypotheses from the local context of the given soundbite. By local context, we mean the current soundbite segment, and the previous and the following segments until reaching another soundbite. Each name hypothesis is considered to be a candidate speaker name for the soundbite. Thus we can formulate a binary classification problem for every person name candidate and the corresponding soundbite, called an instance in this classification task. A positive tag of an instance means that the name is the soundbite speaker. Note that if a name occurs more than once in the surrounding turns of the soundbite segment, we only create one instance for it. An SVM classifier is used for the classification of these instances. and lexical information for classification of the instances [Liu and Liu 2007a].  X  X F-1. The position of the candidate name relative to the soundbite. We hy-pothesize that names closer to a soundbite are more likely to be the sound-bite speaker. This feature has the attribute value of  X  X ast X ,  X  X irst X ,  X  X nique X , or  X  X id X .  X  X irst X  means the candidate name appears the first among all the candidates.  X  X id X  means it is in the middle of multiple names.  X  X ast X  indicates that it is the closest name among all the hypotheses before the soundbite.  X  X nique X  is used when the candidate is the only person name in the region before or after the soundbite. Note that if a candidate name occurs more than once, the PF-1 feature represents the closest name to the soundbite.  X  X F-2. The position of a name in its sentence. Typically a name appearing earlier in a sentence (e.g., a subject) is more likely to be quoted later.  X  X F-3. An indicator feature to show where the name has occurred, before, inside, or after the soundbite. We added this because it is rare that a name inside a soundbite is the speaker of that soundbite.  X  X F-4. An indicator to denote if a candidate is in the last sentence just be-fore the soundbite turn, or is in the first sentence just after the soundbite turn.
 subject and thus more likely to be the speaker of the soundbite, therefore we include the frequency of a candidate name in the feature set. We set a threshold (five in our experiments) and use an accumulative representation for this frequency feature. There are five indicator functions, that is, freq  X  1, freq  X  2, freq  X  3, freq  X  4, and freq  X  5. We expect this to be a more robust representation than splitting the frequency values into exclusive regions and using binary features for each of those values. Taking the following scenario as an example: in the test set there is a name appearing four times, but in the training set, there are no instances with this feature, instead, there are some examples that have a name occurring three times. Using the cumulative thresholding way, the classifier would still be able to assign some weight to the test instance based on this feature (three indicator features are true and will contribute to the system X  X  decision).
 in the transcripts, we included unigram features. For example,  X  X re word+1= said X  denotes that the candidate name is followed by the word  X  X aid X , and that  X  X re X  means this happens in the region before the candidate soundbite speaker name. To reduce the noise, we removed the lexical features which occur fewer than three times across the whole training set. recognition system that is worth pointing out is conflict resolution. Since our approach treats each candidate name (together with its corresponding sound-bite) separately, we need to post-process the cases where there are multiple or no positive hypotheses for a soundbite during testing. To resolve this situation, we choose the instance with the highest confidence value from the classifier among all the candidate instances for a soundbite.
 4. EXPERIMENTAL SETUP 4.1 Data We use the TDT4 Mandarin broadcast news data in our experiment, which consists of 335 shows of news from different sources. Speech turn segmenta-tion, speaker role (anchor, reporter, and others) and name information were annotated manually in the transcripts (see the example in Figure 1). All the speech turns labeled as  X  X ther X  role are considered as our reference soundbite segments. The punctuation marks in the LDC transcripts were used to obtain the reference sentence information for feature extraction. We split the data set and used 31 shows as the development set, 24 shows as the test set, and the rest of the 280 shows as our training set.
 recognizer [Hwang et al. 2006], with an error rate of about 20%. We aligned the ASR output with the reference transcripts to obtain the reference sentence boundaries, as well as the reference speaker turn, role, and name information for the ASR words. In this procedure, we first used the  X  X iff X  Unix system func-tion to align the ASR words and reference human transcripts (a step similar to word error rate calculation), result ing in three mapping categories: aligned (including the matched words and substitutions), insertion, and deletion. For the inserted words that only appear in ASR output, we used the automatic sen-tence boundary hypotheses in ASR output, and obtained speaker information from the previous aligned reference word. If a reference sentence boundary is associated with a deleted word (a word that only occurred in reference tran-scripts), we mapped the boundary information to the ASR word before this deleted word.
 small amount of speech turns which were labeled by the annotator with an  X  X nknown X  role, and those soundbites that do not have the speaker names in the transcripts (labeled with  X  X nknown X  in name annotation in our corpus). For this current study, our focus is on the speech segments that the human annotators can determine their roles and the soundbites that the annotators can identify their speaker names based on the transcripts.
 were automatically tagged with named entities using the NYU tagger [Ji and Grishman 2005]. We used the libSVM toolkit [Chang and Lin 2001] and the RBF kernel for classification in our experiments. LibSVM supports multi-class classification, which we used for our three-way classification (anchor, reporter, and soundbite). All the parameters for SVMs were optimized using five-fold cross validation and grid search on the training set.
 4.2 Performance Measure  X  X oundbite detection
For soundbite detection, we use precision/recall/F-measure, as well as clas-sification accuracy, defined below.
When using the three-way classification setup, the hypotheses from the clas-sifier are mapped into binary tags for evaluation by combining the other two classes (anchors and reporters) into non-soundbites. The baseline per-formance for soundbite detection is obtained by determining all the speech turns as the majority class, that is, non-soundbite. We will report the base-line results using the classification accuracy metric, not F -measure (recall is 0).  X  X oundbite speaker name recognition
We use classification accuracy ( CA ) for instance classification and speaker name recognition accuracy ( RA ) as follows:
A reasonable baseline for soundbite speaker name recognition is to choose the closest name candidate before a soundbite as its speaker.  X  X ntire soundbite distillation system
For the entire system, we use precision/recall/ F -measure metrics, similar to the soundbite detection task, except th at here we need to correctly determine the speaker X  X  name for the detected soundbite segments. 5. EXPERIMENTAL RESULTS We first investigate the performance of the two components on the devel-opment set using human transcripts, with a focus on the effect of different features for the two individual modules. Then we use the test set to evaluate the impact of using speech recognition output on each component as well as the integrated pipeline soundbite distillation system. 5.1 Soundbite Detection on the Dev Set soundbite detection results (precision, recall, F -measure, and accuracy) on the dev set using five different feature sets for the binary and three-way classifica-tion setup.  X  X F X ,  X  X F-current X , and  X  X F X  stand for those features described in Section 3.1.1;  X  X utoff 1 X  indicates that only those features in  X  X F+SF X  occur-ring more than once in the training set are used;  X  X nly Unigram X  means that only the unigram features from  X  X F+SF X  are used. TF-based weighting (see Section 3.1.2) is used for all of the experiments.
 mance for both binary and three-way classification (comparing LF-current and LF in Table II), suggesting that those features capture some dependency information between neighboring speech turns. On the other hand, the length-based structural features (SF) are not useful in the binary classification con-figuration but do help three-way for all the performance measures (precision, recall, and accuracy). We performed a nalysis using the dev set to verify whether our hypothesis about the turn and sentence length for different speaker roles is correct. Table III shows the distribution of various mea-surements that we used as features. We can see that indeed anchor and re-porter speech turns and the sentences in their turns are longer than those of the soundbite segments. Similar patterns were also observed in the training set. The difference about the effectiveness of these structural features for the binary and three-way setup is probably because other features can distinguish soundbite vs. others, and adding these structural features does not yield fur-ther improvement. Figure 3 shows the histogram for the number of words in the segments for the three different roles: anchor, reporter, and soundbites. Again, soundbites segments are relatively shorter than the other types. In addition, we can see the long tail in the distributions.
 and bigram features. Removing low-frequency features (i.e., Cutoff-1) helps in binary classification, but not for three-way classification; removing bigram fea-tures (Only Unigram) improves performance in three-way classification, but not for the binary setup. From the results in Table II, it seems that using these feature selection always increas es precision and decreases recall rate, resulting in mixed results in F -measure or accuracy.
 segment to  X  X F X  for the setting of  X  X F+SF X , not just using the first and last sentence, but found significant degradation from the five-fold cross-validation on the training set. This might be because the training data is small when using more lexical features, and also suggests that indicative lexical features for soundbite detection often occur in the first sentence and the last sentence, and that including more lexical features might introduce more noise. a classifier using conditional random fields (CRF) [Phan et al. 2005]. Our goal is to utilize the contextual tag information to help better detect soundbite seg-ments. For example, we know an anchor speaker is often followed by a reporter, and a reporter proceeds a soundbite speaker. This sequence modeling approach was used in Liu [2006], where an HMM is used to model the transition among different speaker roles. In our experiments, we found that CRF X  X  performance is comparable to SVMs (better F -measure on the training set, but worse on the dev set), therefore we will continue to use SVM as the classifier for soundbite detection. We also observe that the SVM and CRF classifiers have different error patterns (i.e., SVMs have higher recall rate, whereas CRFs have better precision performance), suggesting that we may be able to combine the two classifiers for better system performance in our future work. soundbite detection results on the dev set using different weighting methods described in Section 3.1.2. Both the F -measure and accuracy results are pre-sented in the table. We used all the features listed in Section 3.1.1 for this experiment.
 perform much better than simply using local information of the term frequency. Interestingly, different problem formulations (binary versus three-way) seem to prefer different weighting methods.  X  X F*IDF X  works best for binary clas-sification while  X  X ntropy X  is better for three-way classification. We observe that entropy-based weighting always leads to higher precision and relatively lower recall in comparison with other weighting methods. Consistent with the findings on the information retrieval task in Dumais [1991], entropy-based weighting is more theoretic and seems to be a promising weighting choice for the soundbite detection task, compared to the two empirical solutions using IDF and IWF.
 fication strategy generally outperforms the binary setup. This suggests that the multiple-way formulation for soundbite detection can make use of more discriminating features among different speaker roles. Of course this requires additional annotation of speaker roles in the training data, which may not be available. 5.2 Soundbite Speaker Name Recognition on the Dev Set Table V shows soundbite speaker name recognition results on the dev set using different feature sets.  X  X F, Freq, LF X  are the features described in Section 3.2.1. We notice that the system performance generally improves with incrementally expanded feature sets, and using all the features yields an accuracy of 89% and 67.4%, an improvement of 4.5% and 7.3% compared to the baseline in terms of CA and RA respectively. We also varied the order of the features incrementally added, and found that adding frequency features (Freq) on top of lexical features (LF) only yielded marginal gain, suggesting frequency features are not useful when lexical cues are available. In addition, we evaluated some compound features using our current feature definition, but adding those did not improve system performance. The results in Table V also show that the two metrics, the name recognition accuracy and instance classification accuracy are not always correlated.
 measure the oracle RA , defined as the percent of the soundbites for which the correct speaker name (based on NER) appears in the region surrounding the soundbite. The oracle RA on the dev set is 79.1%, sufficiently higher than the current system performance. This suggests that there is still room for further improvement for soundbite speaker name recognition. Our analysis showed that about 8.3% of the soundbites do not have the correct name hy-pothesis due to an NER boundary error, and 12.5% due to the true name not occurring in the local context. 5.3 Performance of Soundbite Distillation System on the Test Set tection results on the dev set (Table II and IV), we choose to use the  X  X F+SF X  feature set and entropy-based weighting approach in this experiment. The results of soundbite detection on the test set are presented in Table VI.  X  X EF X  means using the human transcripts and human annotated sentences.  X  X SR ASB X  means using ASR output and automatic sentence segmentation results (based on the system from [Zimmermann et al. 2006]).  X  X SR RSB X  is obtained by aligning reference sentence boundaries in the human transcripts to the ASR words.
 (comparing REF and ASR conditions). Using automatic sentence boundary detection degrades performance even more (comparing RSB and ASB). In par-ticular, there is a significant decrease o f the recall rate when using automatic sentence boundary hypotheses. This might be because that the wrong sentence segmentation leads to misses of important cue words for soundbite detection. The results also show that because of the imbalance of the corpus (soundbite is the minority class), the precision/recall measure does not always correlate well with the classification accuracy.
 shows seem to be very different from other Mandarin BN sources in terms of structure and style, posing more problems to speech recognition and sentence boundary detection. That partly explains the relatively worse performance on the test set, even though it has a highe r baseline accuracy compared to the dev set. to use all the features to investigate the impact of ASR on soundbite speaker name recognition on the test set. Table VII shows the results. The notations used for the test conditions are the same as in Section 5.3.1.
 recognition performance compared to us ing reference transcripts. However, different from the soundbite detection module, we can see that using auto-matic detected sentence boundaries has negligible effect on soundbite speaker name recognition accuracy. This is n ot very surprising. As described in Section 3.2.1, feature extraction for this module relies less on sentence bound-aries than for soundbite detection. the evaluation of the pipeline soundbite distillation system, we employ the three-way classification setup in the soundbite detection module. Other con-figurations are the same as those used in the above two sections. The pre-cision/recall results are presented in Table VIII. As expected, we observe that the pipeline system performance degrades significantly, compared to the individual components (Table VI and VII), and also suffers from the errors from speech recognition and automatic sentence boundary detection. integrated system, we summarize all the results in Table IX. We show results for using ASR outputs vs. reference transcripts (ASR and REF); automatic sentence segmentation vs. reference sentence boundaries (ASB and RSB); and reference vs. automatic system output for soundbite segment detection and soundbite speaker recognition respectively (REF and Auto). We can easily notice that soundbite detection is more sensitive to the sentence boundaries errors (comparing the corresponding rows for ASR ASB and ASR RSB condi-tions). Table IX also shows that using automatic soundbite detection with ref-erence speaker name information yields better performance than using refer-ence soundbite segments with automatic speaker name recognition (the second and third rows for the three blocks, corre sponding to different test conditions), mainly because of the poor performance of speaker name recognition. 6. CONCLUSIONS AND FUTURE WORK This article presents a pipeline soundbite distillation system based on a sta-tistical classification framework for the two modules: soundbite segment de-tection and soundbite speaker name recognition in Mandarin broadcast news domain. We have investigated a variety of linguistic features and different strategies for each individual module. Our experimental results show that using ASR output hurts the soundbite speaker distillation performance signif-icantly. We also find that using automatic sentence segmentation degrades the performance of soundbite detection, but has negligible effect on the speaker name recognition module. In addition, i n the pipeline system framework, both of the two components (soundbite segment detection and soundbite speaker name recognition) impact the final system performance, but there is a greater impact from speaker name recognition than soundbite segment detection on the entire soundbite distillation system performance.
 modules and make better global decision based on hypotheses from both sub-tasks. For example, correctly associating a speaker name with a speech turn will help determine the speaker role for that segment. We will also investigate additional features (acoustic and syntactic features) for the two components. Finally, this study uses reference speaker turn segments and only addresses the problem of identifying which segment is soundbite and then determining its speaker name. We will evaluate the effect of using automatic speaker seg-mentation in our future work.
 We thank Sameer Maskey, Julia Hirschberg, and Mari Ostendorf for useful discussions, and Heng Ji for sharing the Mandarin named entity tagger.
