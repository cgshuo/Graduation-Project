 We consider a general cla ssification problem with C (  X  2) classes and n training instances. Each training insta nce consists of measurements x =( x 1 ,...,x d ) T on d features and a known class label y = { 1 , 2 ,...,C } . The training data set can be represented in the form of  X  = { ( x i ,y i ) ,i =1 ,...,n } . The goal of a classification task is to correctly predict the class label of a query q  X  d .
The K -nearest neighbor (KNN) method is a simple and appealing approach to this problem [3]. The KNN algorithm is also known as nearest neighbor (NN) algorithm for K = 1. It is well known that the asymptotic error rate of the NN rule is never more than twice the Bayes rate [1]. However, the major outstand-ing drawback of the KNN algorithm is that the whole training data set must be stored in memory to be used in the test phase. To identify the K nearest neighbors in the test phase, the distances between q and all training instances x i ( i =1 ,...,n ) must be computed. This can result in a prohibitively large storage requirement and slow testing sp eed. (Note that the term  X  X esting speed X  and  X  X nline speed X  can be used interchangeably.) Also, the presence of noisy in-stances (i.e., those with errors in the feature vector or class label, or those not representative of typical cases) can also degrade the generalization performance of KNN.

Prototype reduction techniques are concerned with reducing the number of training vectors (prototypes) to be used, which can reduce the storage require-ment and increase the testing speed simultaneously. Some prototype reduction methods identify the optimal subset of the representative instances from the original data, while the other approache s generate an entirely new set of objects as the artificial prototypes. Suppose the size of the stored training set is n ,then the testing time for KNN to classify one query is O ( dn 2 ). Hence, the reduction in storage requirement can subsequently enhance the testing speed of a KNN classifier. A comprehensive survey of the prototype reduction methods for KNN can be found in [12].
 A recent and very promising approach for pr ototype reduction, called Weighted Distance Nearest Neighbor (WDNN) [2], is based on retaining the informative in-stances and learning their weights for classification. The WDNN algorithm assigns ing instances and the corresponding weights with w i &gt; 0 will be retained (as the prototypes) in the test phase. Although only a fraction of the training set is re-tained, the generalization performance of WDNN can be equal to or even better than NN. To achieve this goal, the weights w i are determined by maximizing the leave-one-out cross-validation (LOOCV) accuracy. At each iteration of a LOOCV procedure, each training instance x i is regarded as the query and its class label is predicted by using all the other training instances. This procedure is repeated for i =1 ,...,n . The WDNN algorithm is a hill climbing optimization technique where each w i is updated by assuming all the other weights w j ( j = i )aregiven and fixed, and the optimal w i is determined by considering the threshold value for been shown that the WDNN algorithm can reduce, on average, more than 80% of the training set while retaining or improving the generalization accuracy of a NN classifier over several real data sets. In the same paper, the WDNN algorithm has also been shown to outperform several benchmarking prototype methods includ-ing A-NN [11], PW [7] and LPD [6].

Although the WDNN algorithm is well formulated and shows encouraging performance in practice, it can only work with K = 1. (Similarly, PW [7] and LPD [6] also work with K = 1 only). However, it has been shown that the best value of K is neither too large nor too small, and setting K&gt; 1 can reduce the sensitivity to noise and smooth the decision boundaries (see [1] and [3]). In thecaseof K = 1, being a nearest neighbor of an instance and classifying this because the decision on the classific ation of a query is determined by K nearest neighbors. This fact along with the possible tied votes make the weight learning for K&gt; 1 complicated and difficult.

In this paper, we extend the WDNN algorithm to a general weight learning al-gorithm for KNN with K  X  1. Naturally, the developed algorithm is dubbed the Weighted Distance K Nearest Neighbor (WDKNN) algorithm. In fact, WDNN is a special case of WDKNN as NN is a special case of KNN. As with WDNN, the WDKNN algorithm iteratively updates the instance weights by maximizing the LOOCV accuracy. However, the cr ucial difference between WDKNN and WDNN is that the weights returned by WDKNN are derived from an explicit objective function and model of the decision function. In addition, it has been shown that the optimal weights can be determined by using a subset of the training set only and the difficulties caused by K&gt; 1 have been successfully resolved by considering two threshold values in WDKNN.

The rest of this paper is organized as follows. In Section 2, the KNN classi-fication rule combined with instance weights is introduced. Section 3 presents our proposed WDKNN algorithm. The experiment results are given in Section 4. Some concluding remarks are given in Section 5. In this section, we introduce the KNN classification rule with the specified in-stance weights w i ( i =1 ,...,n ). Here, we assume that these weights have already been learned by the WDKNN algorithm an d we will present how they are learned in the following section.

The K nearest neighbors of q are found by using the weighted similarity between q and x i : where Here, D max is the maximum possible distance between two training instances in That is, D max = d j =1  X  2 j and  X  j represents the difference between maximum and minimum values of feature j . If we denote the neighborhood around q by can be represented as N ( q )= { x l |  X  w ( q , x l )  X   X  ( q ) } .
In the test phase, instead of using the traditional majority voting scheme on the K neighbors, we use the following decision function to classify q : and it can be determined by and 0 otherwise. The WDKNN algorithm presented in thi s section is an extension of the WDNN algorithm for K  X  1. The WDKNN algorithm assigns a weight w i (  X  0) to each training instance x i ( i =1 ,...,n ) by maximizing the LOOCV accuracy. This procedure is done in the training phase. Only the training instances and the associated weights with w i &gt; 0 are retained in the test phase.

The WDKNN algorithm is a hill climbing optimization technique for solving w , where the optimal weight w i is determined by assuming all the other weights w j ( j = i ) are given and fixed. We set w i =1 i ( i =1 ,...,n ), the optimal weight w i can be found by maximizing the objective function related to the LOOCV accuracy: where F ( x m | w i ) is the decision function of instance x m given that the weight for x i is w i . Here, x m is treated as a query in the LOOCV procedure. Obviously, the obtained w i is only suboptimal as the other weights change during the optimization process. Thus, the algorithm will be restarted after a fixed number of runs over the training data set. We follow [2] to restart the optimization process after three runs over the entire data in all experiments conducted in Section 4.

To optimize the weight w i for the training instance x i , we assume that x i is consistent on all other training instances x m ( m = i ) in the LOOCV test. Below is the precise definition.

Definition : A training instance is consistent on a query if it can either make this query be classified into its class or it is irrelevant of its prediction.
In our proposal, the decision function F ( x m | w i ) of instance x m is modeled as follows: where F ( x m | w i = 0) is the decision function with w i =0,  X  m is a threshold for w i . This model essentially means that x m will be classified without using ( x i ,w i ) unless they can make x m be classified into its class y i .Itiseasyto 1, where a three-class classification problem in two-dimensional space has been plotted. The class memberships of the training instances are distinguished by the various shapes of the data points. The query q is denoted by a black square and the instance x i is the data point with a green  X + X  label. If we set w i =0 ,w j = 1 ,  X  j = i , then the K (= 7) nearest neighbors of q are drawn with connection lines and q will be classified into the  X  X riangle X  class. On the other hand, if w i &gt; 1 . 18 ,w j =1 , will be replaced by x i and thus WDKNN will assign q into the  X  X iamond X  class. Hence, we have  X  m =1 . 18 in this example.

The value of threshold  X  m can be specified for each x m by considering three cases regarding the relationship between x m and ( x i ,w i ).
 Case 1: If we have F ( x m | w i =0)= y i ,then w i will not depend on ( x m ,y m ). (4), we have 1 But, according to equation (5), we also have which leads to and thus Now, (Note that this proof is valid regardless the appropriateness of the model given in equation (7).) w i will not depend on ( x m ,y m ). Proof. From the above given conditions, it is easy to arrive at leading to Hence, it can be seen that x m will be misclassified for all values of w i in this case.
  X  | m = i } that do not belong to Case 1 and 2. Then, the threshold value  X  can be determined for all ( x m ,y m )  X  M i as follows: nearest neighbors of x m ; (b) the class vote for class y i is the largest among all classes.

In order to satisfy condition (a), we must have leading to (b) After condition (a) is satisfied, the previous K th neighbor of x m will be replaced by ( x i ,y i ). By equation (4) -(5), which leads to According to the above t hree cases, the optimal w i can be found based on all x Assume there are L threshold values  X  m ranked in ascending order  X  1 &lt; X  2 &lt; ... &lt;  X  L . Then, L +1 values of w i are examined and the best one can be found by using equation (23). The first and last values examined are 0 and  X  L +  X  , respectively (  X  is a very small positive number). The rest values are chosen in the middle of two successive  X  m .

The compression rate ( CR ) is used to compute the rate at which the proto-types are reduced for a prototype reduction method: where r and n represent the reduced and original number of instances, respec-tively. The KNN (and hence NN) classifier r etains all training instances and thus it has CR =0. To validate the proposed WDKNN algorithm, we compared it with the tradi-tional KNN (including NN) algorithm and several other state-of-the-art prototype reduction methods in literature: learning vector quantization [4], learn-ing prototypes and distances (LPD) [6], adaptive nearest neighbor (A-NN) [11], prototype-dependent weighting (PW) [7], WDNN [2] and MWDNN [2]. Twenty real world benchmark data sets taken from the UCI Machine Learning Repos-itory 2 are used throughout the experiments (see Table 1). We performed the implementation using MATLAB R2007a on Windows XP with 2Duo CPU run-ning on 3.16 GHz PC with 3.25 GB RAM. 4.1 Experiments on UCI Data Sets We follow [7] [6] and [2] by using five-fold cross-validation (5-CV) to test the performance of various methods. The average accuracy and compression rate for various methods are compared. Both the KNN and WDKNN algorithms have a tuning parameter: K (neighborhood size). They are determined by selecting the integer ranging from 1 to 41 that corresponds to the maximal 5-CV accuracy on each data set. The selected values of K for both algorithms on each data set are reported in Table 1.

Figure 2 shows the LOOCV accuracies of WDNN and WDKNN during the weight learning progress at the first iteration. This figure was plotted based on one fold of a 5-CV test on the ionosphere data set and K =4wasusedfor WDKNN. The LOOCV accuracy was computed after each one of the weights had been updated. It can be seen that the LOOCV accuracy for WDKNN never increases during the learning process. Also, WDKNN has shown superior per-formance to WDNN during the whole optimization process.
 The mean classification accuracies an d compression rates of the NN, KNN, WDNN and WDKNN methods are summarized in Table 1. We noted that the accuracy of WDKNN is higher than that o f WDNN over all data sets. However, this higher accuracy is achieved at the e xpense of lower compression rates. This is because setting a larger K usually requires more training instances to be involved in the classification stage. It can be seen that WDKNN obtained the overall highest accuracy and also ach ieved the best accuracy in 17 out of 20 cases (with 1 equal best one). Based on th e accuracy values obtained by various methods over twenty data sets, we used a one-tailed paired t -test to compare WDKNN X  X  accuracies to those of NN, KNN and WDNN. We have statistical evidence that the average accuracy of WDKNN is significantly higher than the average accuracies of all of NN, KNN and WDNN for each significance level of 0 . 5% , 1% and 5%. Moreover, WDKNN achiev es higher accura cy than KNN with no more than 30% of the training data set on average.

Figure 3 shows the average online time required for KNN and WDKNN over the twenty real data sets. In the figure , each data set was denoted by the order in which they appeared in Table 1. We noted that the online time required for WDKNN is less than that of KNN over al l 20 data sets. The online speed of WDKNN is 7 . 5 times faster than KNN for the breast cancer (original) data set, and the average online speed of WDKNN over the twenty data sets is 2 . 6 times faster than that of KNN. Although the reduction in online time of KNN is trivial for these small to medium sized data sets, the reduction can be dramatic either the training set or the test set in creases, the advantage of WDKNN over KNN in terms of online execution speed will become more obvious.

Table 2 gives the average 5-CV accuracies of WDKNN in comparison with the published results of several state-of-the-art prototype reduction methods. The data sets used by these methods are taken from the Statlog project, so we only display WDKNN X  X  results on thes e data sets. The experimental proce-dures for the competing methods are t he same as those of WDKNN except that the feature weighting methods have only been used in PW, LPD, MDNN and MWDNN. In this paper, we focus on instance weighting, and thus we only em-ploy the basic Euclidean distance, which implicitly assumes that all features have equal weight. Hence, the accuracies of WDKNN would be consistently higher if the feature weighting scheme had also been used in WDKNN. Despite this un-fairness for WDKNN, it still achieves the b est average accuracy on all data sets. Using the one-tailed paired t -test (with a significance level of 5%), we have sta-tistical evidence that WD KNN outperforms all the competing methods. It must be noted that the A-NN and PW methods do not reduce the training size. In addition, these results obtained by WDKNN are comparable to or better than those obtained by other state-of-the-art methods recently published on the same tasks [9,8,5]. 4.2 Effect of Noise Since WDKNN is designed to be more robust in the presence of noise than WDNN, the experiments conducted in Section 4.1 were repeated with 20% uni-form class noise artificially added to each data set. This was done by randomly changing the class label of 20% of the training instances to an incorrect value (with an equal probability for each of the incorrect classes). The class labels of the test instances are not noisy. Note that the experimental settings here are the same as those in [2]. The performance of NN, KNN, WDNN and WDKNN were tested to see their robustness of noise.

Table 3 reports the average accuracies and compression rates of each method over twenty data sets. As can be seen, the accuracy of WDKNN is much better than that of WDNN. This is consistent with our motivation that using K&gt; 1 can reduce WDNN X  X  sensitivity of noise. The average values of K for both KNN and WDKNN on the noisy data become la rger compared to the real data sets. This also suggests that a larger K is more suitable for the noisy data sets. WD-KNN achieves the highest accuracy value a veraged over the entire data set. Also, the accuracy given by WDKNN is the best in 10 out of 20 cases. Using the one-tailed paired t -test (for both the 1% and 5% significance level), we have statis-tical evidence that WDKNN outperfo rms NN and WDNN in terms of accuracy, and we have no statistical evidence that the average accuracy of WDKN is larger than that of KNN. On average, WDKNN achieves the same level of accuracy as KNN by using no more than 35% of the training data set. In addition, we also found that the online time required for WDKNN is less than that of KNN over all noisy data sets. Specifically, the average online speed of WDKNN over the twenty noisy data sets is 2 . 2 times faster than that of KNN. In this paper, a novel prototype reduction method for KNN has been proposed. This method removes the instances that are more of a computational burden but do not contribute to better classification accuracy and it also assigns a weight to each retained training instance. Thes e weights are learned by maximizing the leave-one-out cross-validation classification accuracy, which is the true estimate of the generalization ability of a classifier. Empirical results have shown that WDKNN considerably reduces the size of the training set while retaining or improving the classification accuracy of KNN. In fact, the proposed method may also be useful for other lazy learning methods such as ALH [10] in terms of storage requirement and online speed.
