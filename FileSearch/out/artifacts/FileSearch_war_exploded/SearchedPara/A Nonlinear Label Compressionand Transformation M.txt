 Multi-label classification, the classification of objects into many, possibly inter-dependent, but non-disjoint binary classes, has received a lot of attention in recent years. Binary relevance (BR), the most basic method, predicts each label independently and thus does not make use of label correlations. Further methods can be divided into the families of (i) method adaptation schemes and (ii) prob-lem transformation schemes. Method adaptation schemes develop multi-label versions of standard machine learning schemes, like decision trees [ 2 ], k-Nearest Neighbors [ 25 ], or neural networks [ 12 ]. The nerual network based approach equips a previously known method with a different loss function. Using neural networks as multi-label classifiers is an obvious and simple extension to neural networks. Problem transformation schemes, by contrast, transform the multi-label problems into multiple single-label problems and then apply standard single-label methods to the transformed problems  X  the single-label learners and classifiers act as plug-ins. The results of the single-label classifiers are then transformed back to calculate the multi-label classifications. Examples for this approach are Ensembles of Classifier Chains (ECCs) [ 15 ] matrix factorizations to obtain a smaller set of pseudo-labels or latent labels as targets for standard single-label classifiers [ 17 , 24 ]. algorithms have some advantages over method adaptation methods: First, any type of single-label classifier can be used as plug-in. (a) If a classifier has a suit-able bias for a data set at hand, it can readily be used. (b) It can be chosen depending on the practical requirements of a project (e.g., a preference for lower errors over shorter running times, or vice versa). (c) New single-label learning schemes can be immediately used and tested, if needed, without the need for a potentially non-trivial adaptation of the method and time to achieve this. Fur-ther, transformation methods do not depend on any specific type of data, for instance, real-valued or graph data, as long as the base (single-label) classifier can handle them as input. This affects mainly highly specialized algorithms, like for instance method adaptation schemes based on specific neural networks architectures, that are mostly restricted to real-valued data [ 8 ]. formation), so-called label compression algorithms were suggested [ 17 , 19 , 24 ]. These algorithms compress the labels into a typically smaller label space and make the predictions on the compressed labels. The prediction is done by decom-pressing the predicted labels again. MLC-BMaD [ 24 ], for instance, uses Boolean matrix decomposition to generate a compression of the labels, which is used as input for a BR learner. The decompression is achieved by multiplying with a matrix that contains carefully chosen basis vectors. Hence, the dependencies among the labels are encoded in this basis matrix, which is part of the learned model. Another approach similar to MLC-BMaD is multi-label classification using the principal label space transformation [ 17 ]. This method uses singular value decomposition instead of Boolean matrix factorization in the compres-sion step. Label compression methods can also belong to the family of method adaptation schemes, for instance, in neural network variants [ 8 ] (see below). labels to improve the prediction. Nevertheless, so far, only few of the classifiers were able to cope with non-linear dependencies among the labels. One of the approaches targeting non-linear dependencies among the labels was proposed by Li and Guo [ 8 ]. Nevertheless, the method aims for a compression of both the input (the features) and the output (the labels) together. This differs from the one presented here, as we will solely focus on finding a compressed repre-sentation of the labels that encodes their dependencies. This allows us to make use of base learners that can cope with features that are not readily embedded into
R n (e.g. text, nominal). Li et al. [ 9 ] presented an algorithm that uses con-ditional Restricted Boltzmann Machines (RBMs) to train models on data sets with partially missing labels. All these methods belong to the method adaptation schemes, with the above mentioned disadvantages. Read et al. ensembles of classifiers can capture non-linear dependencies among the labels by functioning as a layered network. This is achieved by using RBMs as a base classifier in ECCs.
 This paper makes three contributions to the field: (i) We introduce the first method from the intersection of problem transformation approaches and label compression approaches that is able to capture nonlinear label dependencies. The algorithm, called Maniac 3 (m ulti-la bel classification usi ng a utoenc oders), compresses the labels using autoencoders [ 6 ], and then learns a multi-label model on the compressed label set. After the prediction, the same structure (autoencoder) is used to decompress the labels and obtain the final predictions. (ii) We introduce autoencoders to the set of available multi-label compressors and decompressors in problem transformation approaches. So far, autoencoders have surprisingly only been used on the whole data set, despite using them on the labels only seems to be an obvious approach. (iii) We state exactly where the algorithm performs well, namely on the task of  X  X enuine X  multi-label classifica-tion problems with many labels and dependencies among the labels and when the prediction of the correct bipartition of the labels is important. The remainder of the paper is organized as follows: In the next section, we describe in detail Maniac , the newly proposed algorithm. Subsequently, we explain the evaluation approach. Finally, we discuss the experimental results and give a conclusion. The algorithm works similarly to other label compression based algorithms like MLC-BMaD [ 24 ] or the method proposed by Tai and Lin [ 17 ]  X  Principal Label Space Transformation (PLST). In the first step, the labels are compressed using a compression algorithm. Then, base learners are trained on the compressed labels (see Fig. 1 ). In the case of Maniac , we use autoencoders for compression, in the case of MLC-BMaD Boolean matrix decomposition is used, Tai and Lin use singular value decomposition. The difference in using the autoencoders in this step is that unlike other approaches, this captures non-linear dependencies among the labels. Boolean matrix decomposition and SVD cannot capture them, hence the performance the prediction performance can be improved on data sets with non-linear dependencies. decompressor part, connected by a small central layer. (see Fig. 2 ). In the fol-lowing, we will explain details of the autoencoder training.
 data is clamped to the equally sized input and output layers of the autoencoder. The training data is compressed using the compressor part of the network, and then reconstructed again using the decompressor part. The result is compared to the original data, the reconstruction errors are used to tune the parameters of the compressor and the decompressor. More specifically, we used a Conjugate Gradi-ent (CG) algorithm, and computed the necessary gradients from reconstruction errors using classical backpropagation. If the training of an autoencoder is suc-cessful, the output of the network is similar to the input data. Since the output is reconstructed by the decompressor from the activations of neurons in the inner-most layer, the small innermost layer of the autoencoder can be thought of as an informational bottleneck. The activations of the neurons in this bottleneck layer constitute an efficient low-dimensional representation of the input data. In particular, this representation captures non-linear dependencies between the activations of the neurons in the input layer.
 of the autoencoder as Restricted Boltzmann Machines (RBMs), and train them using the Contrastive Divergence (CD) algorithm. Connection weights and neu-ron activation biases produced by contrastive divergence were then used as input to the backpropagation-based optimization algorithm.
 autoencoders. The training process of autoencoders is as follows. We start with a trivial autoencoder that has a single neuron layer and no connections. Suppose that we already have an autoencoder with 2 n +1 layers. We apply the compressor part to the data, and train a new RBM on the compressed data. The size of the new layer of hidden units (see Fig. 3 ) is determined by the factor hyperparameter (see also appendix). Then we unfold the RBM and merge it into the center of the original autoencoder, obtaining an autoencoder with 2 n + 3 layers (see Fig. 2 ). Hinton and Salakhutdinov originally proposed to keep unfolding all RBMs until the desired depth is reached, and fine-tune the final autoencoder with the backpropagation algorithm in the very end. However, we have found it beneficial to treat the unfolded RBMs as small autoencoders of depth 1 and also tune them with backpropagation. After gluing the new small autoencoder into the center of the previously obtained (2 we also tune the resulting (2 n + 3)-layer autoencoder with backpropagation. We evaluated multiple training strategies for the autoencoders, and the results showed that in contrast to the strategy originally suggested by Hinton and Salakhutdinov [ 6 ], when using a stream of autoencoders, better results can be achieved. Additionally, there seems to be no big difference from using the parameters suggested by Hinton compared to using an optimization of these parameters.
 The next step is then to train a base classifier (a multi-target model) using the compressed labels as new target variables. In the previous step, we extracted the dependencies from the labels into the autoencoders, hence, in the best case, there are no dependencies left among the latent labels. Therefore, it should not be beneficial to use a sophisticated multi-target learner over a simple BR model. Nevertheless, if the autoencoder does not manage to extract all dependencies, it might be beneficial to use a more advanced multi-target learner. The training phase of the algorithm is composed of these two steps, and the model consists of the autoencoders and the multi-target (BR) model.
 With an autoencoder, a threshold for binarization, and the trained base learn-ers, one can easily predict labels for new instances. First, the multi-target model is applied and the latent labels are predicted. Next, the autoencoders are used to decompress the latent labels. The final labels are obtained by thresholding, so that the output is binary.
 autoencoder similar to other multi-label classifiers using a threshold. It should be noted that the calculated confidences are mostly close to 0 and 1, and rarely somewhere in between. We optimized two parameters of the autoencoders, the compression factor and the number of layers using an internal holdout evaluation. We have also attempted to optimize the hyperparameters of Contrastive Divergence (used for RBM training), but the optimization turned out to be expensive, while not hav-ing much impact on the quality of the final model. In general, using RBMs for pre-training seemed to speed up the computation, but the effect of fine-tuning the small unfolded autoencoders and all the intermediate autoencoders seemed to outweigh the effects of Contrastive Divergence on the quality of the model. Therefore, all the parameters of Contrastive Divergence are essentially the same as proposed originally [ 6 ]. The optimization of the parameter for the final thresh-olding is very cheap, it can be accomplished by simple grid-search that tests thresholds in the interval [0 , 1] using 1000 steps for each column individually, and only models need to be applied, not trained.
 choice for a baseline as it uses no dependencies among labels, hence, a good multi-label classifier should perform better than BR. ECC is a fast and well performing method and currently considered as the benchmark method for multi-label classification [ 10 ]. We evaluated the algorithm using a repeated holdout evaluation with one third of the data set as test set, two thirds of the data set as training set and ran all evaluations 15 times, as suggested by Nadeau and Bengio [ 11 ], and calculated the corrected re-sampled t-test statistics implemented in WEKA [ 11 ]. For ECC, we set the number of chains to 10, BR did not require any parameters to be set, the parameters of MLC-BMaD we optimized using a greedy search, as suggested by Wicker et al. [ 24 ]. As Wicker their approach outperforms PLST [ 17 ], we did not compare to PLST seperately. the autoencoder is capable of extracting all dependencies and returning a set of latent labels that are completely independent. If this is the case, the learners can be trained independently, and the multi-target version of BR can be used. If there are dependencies left in the data, a more sophisticated multi-target learning method should be chosen. In our experiments, we used an adaptation of ECC for multi-target problems, as it has been proven to be a fast and well-performing learner. Both base learners were used and compared to the other models. 3.1 Implementation We implemented the algorithm in Mulan [ 21 ] using our own implementation of autoencoders 7 . The implementation provides a way to train a stream of autoen-coders, adding one layer at a time, given a compression factor. This is used to speed up the optimization process of choosing the right number of layers. Using an internal training set, autoencoders with one layer are trained. A model is trained on this autoencoder and the performance is evaluated on a test set. The autoencoders are extended to have more layers, and are evaluated again, and so on. Hence, we do not need to repeat the process of training an autoencoder with n layers, we can reuse the previous trained autoencoder and extend it. As base learners, we used an adaptation of ECC and BR for multi-target problems. These learners used random forests as a base learners due to their speed and typ-ically good performance. We used no parameter optimization except an internal holdout validation to optimize the number of layers and compression factor of the autoencoders. We used nine standard multi-label data sets from the data set repository of Mulan (see http://mulan.sourceforge.net/datasets.html ). The results of the experiments can be split into two parts: First, the performance of regarding the split into positive and negative labels (bipartition-based measures are given in Table 1 ), and second, the predicted ranking (confidence-based mea-sures are given in Table 2 ) 8 . The predicted bipartitions give an overall good performance. Yet the ranking or estimated confidences are rather bad. The lat-ter is caused by the autoencoder decompression algorithm, which assigns each label with high confidence in any case, yet the assignment of the confidence is not reliable. Therefore, if a label is incorrectly assigned to be positive or negative, it is not incorrectly assigned with a confidence a bit below the threshold, the con-fidence is set to a value close to 1 or 0. In the overall ranking then, it can be in a position in the ranking completely wrong and impact the quality of the ranking strongly. Thus, a low number of false positives or negatives has a huge impact on the ranking, much stronger than in the case of other classifiers. Hence all rank-ing based measures as area under ROC curve , one error ,or bad for Maniac . Nevertheless, Maniac performs well regarding other measures like accuracy or FMeasure , which simply take into account the bipartition. In some cases they strongly improve the performance compared to ECC or BR, even in the range of 20 % (e.g. the example-based accuracy of the medical data set). Hence, in the following, we will focus on discussing the bipartition-based measures. number of labels, the better the algorithm performs. In all three cases of [ 23 ], enron [ 7 ], and medical [ 13 ], Maniac outperforms ECC without exception, with in all cases a strong improvement in the range of up to 25 %. The higher the number of labels, the easier it is for the autoencoders to generate sensible latent labels, and capture the dependencies among the labels. On the other hand, data sets with only few labels, like flags [ 5 ], scene [ 1 ], and or 7 labels, are hard to compress any further. Hence the dependencies cannot be extracted by the autoencoders. When using data sets of that size, a compression based algorithm is rather useless. Hence the performance of data sets is in almost all cases worse than BR or ECC. In the medium range of number of labels, the trend is not that clear, in the case of the small improvements were possible, on the other hand, on genbase does not seem to work. Here, other aspects like label cardinality seem to be more important. Especially in the mid range of number of labels, it becomes evident that cardi-nality and density are important measures for Maniac . Despite lar in the number of labels to genbase and birds , it is easier for with a higher performance. The biggest difference between these data sets is the cardinality and density. Cardinality in yeast is more than four times higher than the cardinality in birds . While the number of instances is also higher, this does not seem to have an effect on the other data sets. For instance est of the top three data sets in terms of instances, yet there is no difference in performance. Other measures like the number of numeric or nominal features do not have an influence on the performance of Maniac . They are simply an input to the random forests and do not change anything for the autoencoders. If we compare the performance of Maniac using the multi-target versions of BR with Maniac using the multi-target version of ECC as base classifier, we can see that in most cases there is no difference. If there is a difference, BR outperforms ECC. This is a good indication that it is possible to extract all dependencies from the labels using autoencoders. The resulting compression consists of no dependencies, otherwise ECC would outperform BR. It was shown that ECC does not work well using regression models, due to the stronger error propagation in the chains [ 16 ]. Nevertheless, in this setting, this does not seem a problem: The performance is more or less the same, inde-pendent of using BR or ECC as base learners. Hence the dependencies must be extracted almost completely by the autoencoders, and the random forest most likely only uses the features for the prediction of latent labels, ignoring other latent labels in the case of ECC. Hence it is recommended to use BR as a base learner for Maniac , as this is beneficial in terms of runtime and resources (the data set for each base learner is smaller in terms of features for base learners in BR compared to ECC).
 autoencoders. This can give an indication to what extent non-linear dependencies are exploited by Maniac . If the autoencoders only have a single layer, they are a simple mapping function, not capable of handling non-linear dependencies. The results show that in this case Maniac does not perform too well. Hence, Maniac seems to use the non-linear dependencies to improve its classification. the number of layers. We evaluated several settings on the medical data set. The behavior was similar on all other data sets, with exception of the less than small data sets with 10 labels, where the compression was difficult due to the anyway small size of the data set. Clearly, the smaller the compressed data sets due to the compression factor, the faster the number of layers become too high to come up with a meaningful representation, and the overall accuracy tends towards a value of approximately 0.15. Nevertheless, with a compression factor of 0.8, the accuracy is much more stable and a higher number of layers seems to become beneficial. On the other extreme, with a compression factor of 0.2, after the first layer, the number of layers is reduced to 20 % of the original size: One layer appears to be the optimum. This is simply because after one layer the number of latent labels would become too small, if another layer would be added. is certainly higher than that of other algorithms, as it adds a rather expensive compression step, we were able to train autoencoders with one layer on a desk-top 9 for the labels of the biggest data set ( CAL500 ) in 58 s, two layers could be trained in only 133 s, using a compression factor of 0.85, which seems to be a typical setting the optimization process ends with. In our experiments, most optimal autoencoders used at maximum 4 layers in total. The most time con-suming step is the optimization of the parameters, which is reduced by using streams of autoencoders. In this paper, we presented a new approach to multi-label classification based on label compression, called Maniac . Unlike previously presented transformation-based methods, Maniac exploits non-linear dependencies among the labels. This is achieved by compressing the label space with autoencoders. The results showed that
Maniac strongly outperforms standard baseline methods for multi-label classification in the case of a high number of labels and seems to benefit from a high cardinality. While Maniac produces a good bipartition, the confidence is only a rough indication if a label is positive or negative and should not be used for ranking.
 While the method works already well for standard multi-label classification, in particular for bipartition-based measures and genuine multi-label data sets, it also has a high potential for online multi-label classification. Using the autoen-coder compression in an online scenario is trivial, and hence, ily adapted for online learning. The base models can be trained using an online learner, combined using BR, and the compression could be updated with every instance. Nevertheless, this would lead to a large runtime in each step which might not be practical for online learning. Hence, training the autoencoders in batches could be more convenient. On the other hand, the online learning could be com-pletely left to the autoencoders and the base models could be updated only from time to time or vice-versa. Another extension of this method would be to transfer it to multi-target learning, which has been recently done for many multi-label meth-ods (e.g. by Spyromitros-Xioufis et al. [ 16 ]). This step would be quite straightfor-ward, as autoencoders by default can compress numerical values.

