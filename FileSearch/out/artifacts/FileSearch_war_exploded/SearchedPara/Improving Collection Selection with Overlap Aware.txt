 Collection selection has been a research issue for years. Typ-ically, in related work, precomputed statistics are employed in order to estimate the expected result quality of each col-lection, and subsequently the collections are ranked accord-ingly. Our thesis is that this simple approach is insuffi-cient for several applications in which the collections typ-ically overlap. This is the case, for example, for the col-lections built by autonomous peers crawling the web. We argue for the extension of existing quality measures using estimators of mutual overlap among collections and present experiments in which this combination outperforms CORI, a popular approach based on quality estimation. We outline our prototype implementation of a P2P web search engine, coined MINERVA 1 , that allows handling large amounts of data in a distributed and self-organizing manner. We con-duct experiments which show that taking overlap into ac-count during collection selection can drastically decrease the number of collections that have to be contacted in order to reach a satisfactory level of recall, which is a great step to-ward the feasibility of distributed web search.
 H.4 [ Information Systems Applications ]: Miscellaneous; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  selection process, information filter-ing ; H.3.4 [ Information Storage and Retrieval ]: Sys-tems and Software X  Distributed Systems Partially supported by the EU within the 6th Framework Programme under contract 001907  X  X ynamically Evolving, Large Scale Information Systems X  (DELIS). http://www.minerva-project.org Algorithms, Design, Experimentation Peer-to-Peer information systems, distributed IR, query rout-ing, overlap estimation
Collection selection has been a popular research area in distributed information retrieval for many years. Most of the related work on this issue use pre-computed statistical information to estimate the expected result quality of dif-ferent collections en route to identifying the most promising sources for an information need. A key goal from a per-formance viewpoint is to minimize the number of individual collections that have to be gathered in order to achieve good result quality (usually measured in terms of recall in this dis-tributed setting).

Also in recent years, the Peer-to-Peer (P2P) paradigm has received increasing attention. While becoming popular mainly in the context of filesharing systems such as Gnutella or KaZaA, P2P has found its way into distributed informa-tion retrieval due to its ability to handle huge amounts of data in a distributed and self-organizing way. In a typical P2P system, all peers are equal and all of the functionality is shared among all peers so that there is no single point of failure and the load is evenly balanced across a large number of peers. These characteristics offer enormous potential ben-efits for search capabilities powerful in terms of scalability, efficiency, and resilience to failures and dynamics. Addi-tionally, such a search engine can potentially benefit from the intellectual input (e.g., bookmarks, query logs, etc.) of a large user community. One of the key difficulties, how-ever, is to efficiently select promising peers for a particular information need.

As such, research in P2P searching enjoys a large over-lap with research on distributed information retrieval and can highly benefit from existing work. However, the pecu-liarities of such an architecture require a different view on some key aspects. For example, the absence of a central-ized indexing facility together with the difficulties to calcu-late global metrics in this large and highly dynamic network hamper the use of traditional methods for collection selec-tion. Also, the absence of a central control instance causes the peers to learn about portions of the web in a largely un-coordinated manner. Given typical popularity distributions on today X  X  web, for example, it becomes obvious that these autonomous peers will not form disjoint partitions of their combined document space, but rather a highly overlapping set of collections.

Existing collection selection approaches taking into ac-count only the expected result quality of a collection will in-evitably lose some of their power in a P2P setting. For exam-ple, consider a scenario where two peers with high interest in current affairs have (independently of each other) crawled large fractions of a popular news site, such as cnn.com. A traditional approach to collection selection is likely to rank both peers high given a related query, even though their re-sults may highly overlap. Because the second peer is likely to not add many new documents to the query result, the performance of the query can be improved if another, com-plementary peer is chosen instead.
Our main contribution lies in showcasing the dramatic performance improvement possible by employing estimators of mutual overlap of collections. We will also present a novel technique for estimating a query-specific collection overlap and a novel way to combine a popular quality estimation metric with overlap estimators. Our overall goal is to make large-scale distributed search feasible. With this paper, we hope to make a decisive step toward this goal. We have im-plemented our new technique within our prototype P2P web search engine and we quantify the performance improvement of our contribution.

Section 2 gives an overview of related research in the dif-ferent fields that we touch with our work. Section 3 presents the architecture of a distributed P2P search engine that was used for our experiments. Section 4 briefly introduces CORI, a popular approach for collection selection, that is used as a baseline reference point for our work. Section 5 discusses one approach to quantify overlap and novelty into the col-lection selection process and introduces the necessary set of tools, in particular Bloom filters. Section 6 describes how we combine the measures for quality and novelty in a two-step approach. Section 7 presents a number of experiments to show the benefits of our approach. Section 8 concludes and briefly discusses future research directions.
In recent years, many approaches have been proposed for collection selection in distributed IR, among the most promi-nent the decision-theoretic framework by [11], the GlOSS method presented in [14], and approaches based on statisti-cal language models [22, 27]. [5] gives an overview of algo-rithms for distributed IR style result merging and database content discovery. [11] presents a formal decision model for database selection in networked IR. [19] investigates differ-ent quality measures for database selection. [13, 16] study scalability issues for a distributed term index. None of the presented techniques incorporates overlap detection into the selection process.

Estimating overlap of sets has been receiving increasing attention recently for modern emerging applications, such as data streams, internet content delivery, etc. [4] describes a permutation-based technique for efficiently estimating set similarities for informed content delivery. [12] proposes a hash-based synopsis data structure and algorithms to sup-port low-error and high-confident estimates for general set expressions. Bloom [2] describes a data structure for suc-cinctly representing a set in order to support membership queries. [17] proposes compressed Bloom filters that im-prove performance in a distributed environment where net-work bandwidth is an issue. [10] describes the use of statistics in ranking data sources with respect to a query. They use probabilistic measures to model overlap and coverage of the mediated data sources, but do not mention how to acquire these statistics. In con-trast, we assume these statistics being generated by the par-ticipating peers (based on their local collections) and present a DHT based infrastructure to make these statistics globally available. [28] considers novelty and redundancy detection in a cen-tralized, document-stream based information filtering sys-tem. Although the technique presented seems to be applica-ble in a distributed environment for filtering the documents at the querying peer, it is not obvious where to get these documents from. In a large-scale system, it seems impossi-ble to query all peers and to process the documents. [18, 15] have also worked on overlap statistics in the con-text of collection selection. They present a technique to esti-mate coverage and overlap statistics by query classification and data mining and use a probing technique to extract fea-tures from the collections. Expecting that data mining tech-niques will be very heavy for the envisioned, highly-dynamic application environment, we adopt a different philosophy. We adapt statistical methods and use them to estimate the overlap between data collections and we develop novel al-gorithms that utilize these for efficiently selecting the best collections during query processing.

Recent research on P2P systems, such as Chord [23], CAN [20], Pastry [21], P2P-Net [3], or P-Grid [1] is based on var-ious forms of distributed hash tables (DHTs) and supports mappings from keys, e.g., titles or authors, to locations in a decentralized manner such that routing scales well with n , the number of peers in the system. Typically, an exact-match key lookup can be routed to the proper peer(s) in at most O ( log n ) hops, and no peer needs to maintain more than O ( log n ) routing information. These architectures in-corporate algorithms to cope with failures and the dynamics of a P2P system as peers join or leave the system in an un-predictable manner. However, the approaches are limited to the exact matching of keys, making them suitable for single keyword queries on keys. This is however highly inappro-priate when queries should return a ranked result list of the most relevant approximate matches [7].

In the following we briefly discuss some prior and ongoing projects toward P2P Web search.

Galanx [26] is a P2P search engine implemented using the Apache HTTP server and BerkeleyDB. The Web site servers are the peers of this architecture; pages are stored only where they originate from. In contrast, our approach leaves it to the peers to what extent they want to crawl interesting fractions of the Web and build their own local indexes.

PlanetP [8] is a publish-subscribe service for P2P com-munities, supporting content ranking search. PlanetP dis-tinguishes local indexes and a global index to describe all peers and their shared information. The global index is replicated using a gossiping algorithm. The system appears to be limited to a few thousand peers.

Odissea [24] assumes a two-layered search engine architec-ture with a global index structure distributed over the nodes in the system. A single node holds the complete, Web-scale, index for a given text term (i.e., keyword or word stem). Query execution uses a distributed version of Fagin X  X  thresh-old algorithm [9]. The system appears to cause high network traffic when posting document metadata into the network, and the presented query execution method seems limited to queries with at most two keywords. The paper actually ad-vocates using a limited number of nodes, in the spirit of a server farm.
We assume a P2P collaboration in which every peer is autonomous and has a local index that can be built from the peer X  X  own crawls or imported from external sources and tailored to the user X  X  thematic interest profile. The index contains inverted lists with URLs for Web pages that contain specific keywords (a.k.a. terms).

A conceptually global but physically distributed directory, which is layered on top of a Chord-style Dynamic Hash Ta-ble (DHT), holds compact, aggregated information about the peers X  local indexes and only to the extent that the in-dividual peers are willing to disclose. We use the DHT to partition the term space, such that every peer is responsible for a randomized subset of terms within the global direc-tory. For failure resilience, availability, and load balancing, the directory entry for a term may be replicated across mul-tiple peers. Thus, peers in our system can play two roles: as directory peers and as peers storing index lists.
Directory maintenance, peer selection, and query process-ing work as follows. First, every peer with a local index pub-lishes a summary ( Post ) about every term in its local index to the directory. A hash function is applied to the term in order to determine the directory peer currently responsible for this term. This directory peer maintains a PeerList of all postings for this term from peers across the network. Posts contain contact information about the peer who posted this summary together with statistics to calculate IR-style mea-sures for a term (e.g., the size of the inverted list for the term, the maximum average score among the term X  X  inverted list entries, or some other statistical measure). These statis-tics are used to support the peer selection process, i.e., de-termining the most promising peers for a particular query.
The querying process for a multi-term query proceeds as follows: first, the query is executed locally using the peer X  X  local index. If the result is considered unsatisfactory by the user, the querying peer retrieves a list of potentially useful peers by issuing a PeerList request for each query term to the underlying overlay-network directory. Using database selec-tion methods from distributed IR and metasearch [5], a num-ber of promising peers for the complete query is computed from these PeerLists. This step is referred to as peer selec-tion . Subsequently, the query is forwarded to these peers and executed based on their local indexes. Note that this communication is done in a pairwise point-to-point manner between the peers, allowing for efficient communication and limiting the load on the global directory. Finally, the re-sults from the various peers are combined at the querying peer into a single result list; this step is referred to as result merging .

The goal of finding high-quality search results with re-spect to precision and recall cannot be easily reconciled with the design goal of unlimited scalability, as the best informa-tion retrieval techniques for query execution rely on large amounts of document metadata. Posting only compact, ag-gregated information about local indexes and using appro-priate peer selection methods to limit the number of peers involved in a query keeps the size of the global directory manageable and reduces network traffic, while at the same time allowing the query execution itself to rely on compre-hensive local index data. We expect this approach to scale very well as more and more peers jointly maintain the mod-erately growing global directory.

The approach can easily be extended in a way that mul-tiple distributed directories are created to store information beyond local index summaries, such as information about local bookmarks, information about relevance assessments (e.g., derived from peer-specific query logs or click streams), or explicit user feedback. This information could be lever-aged when executing a query to further enhance result qual-ity.
The following section briefly introduces CORI [5], one of the best and most popular benefit estimators for collection selection strategies that tries to estimate the expected result quality of a collection using aggregated statistics about the collections.

Following the terminology of the existing literature [6, 5], we refer to various statistical measures as per collection mea-sures; in our P2P context a collection is the local index content of a peer. We consider only queries with equally weighted terms; so a query is simply a set of terms.
CORI computes the collection score s i of the i -th peer with regard to a query Q = { t 1 , t 2 , ..., t n } in the following manner:
The computations of T i,t and I i,t use the number of peers in the system, denoted np , the document frequency ( cdf ) of term t in collection i , and the maximum document frequency ( cdf max ) for any term t in collection i : where the collection frequency cf t is the number of peers that contain the term t . We approximate this value by the number of peers that have published Posts for term t , i.e., the length of the PeerList for t . The values  X  and  X  are chosen as  X  =  X  = 0 . 4 [6].

CORI considers the size | V i | of the term space of a peer (i.e., the total number of distinct terms that the peer holds in its local index) and the average term space size | V avg over all peers that contain term t :
In practice, it is difficult to compute the average term space size over all peers in the system (regardless of whether they contain query term t or not). We approximate this value by the average over all collections found in the Peer-Lists .
As we have motivated in the introduction, considering overlap is a crucial task in order to make large-scale dis-tributed IR feasible. While existing approaches to collection selection typically try to estimate the relevance of a collec-tion to a given query, they do not consider the overlap with other, previously contacted collections. However, it is ob-viously inefficient to choose peers based on their relevance only, as relevant documents are completely worthless if they have been delivered by other collections before.

In our context, when estimating overlap, what we are ac-tually interested in is the novelty of a peer with respect to a given reference collection C ref : specifically, given a rep-resentation of the document space that has already been covered (e.g., by a local index or by other peers previously queried), we are interested in the contribution that the col-lection C i of peer P i can add to this space. More formally, we define novelty as | C p | X  X  C p  X  C ref | .

In this section, we discuss one approach of estimating the novelty of collections. Doing so in a distributed environment is a non-trivial task that becomes even more complicated as we are not interested in the overall novelty between collec-tions, but rather the novelty that the collections show in their results for a particular query.

In related literature one can find numerous techniques to represent sets in a compact way. For our experiments, we use Bloom filters, one of the most popular technique for this purpose and we develop our novelty estimator using them. We want to emphasize, however, that our design is fundamentally independent of the specific technique used and allows for any other approach as well, as long as it produces numeric scores for collection novelty.
We want to piggy-back all information necessary for nov-elty calculation onto the Post messages described in Section 3. This choice avoids having to retrieve this information at query time from a carefully selected subset of all peers which would add an extra network round-trip communica-tion and, thus, increase the latency encountered when exe-cuting a query.

In order to be able to estimate the overlap between col-lections, each collection publishes an appropriate summary about the documents it contains. Note that these summaries are fundamentally different from the summaries introduced before that are used to estimate the result quality of peers. Obviously, there are at least two conceptually different ways to create and publish these summaries: a) Create a summary for the complete collection and pub-b) Create term-specific summaries for (potentially a sub-While the first strategy might seem to be more efficient in terms of storage and bandwidth usage, we argue that strat-egy b) is preferable for two reasons: First, for query per-formance reasons, since it does not require an additional directory lookup for each peer and, second, since it facili-tates quality and novelty estimation of the peers X  collections specifically for the queried terms. It is instructing to note that a combination of these strategies in which each peer would include a summary in the style of strategy a) with every per-term post as illustrated in strategy b) would in-troduce an enormous degree of redundancy. This would go against the idea of efficiently using storage and bandwidth. At the same time, it would suffer from the lack of query-specificity illustrated before.

We assume that each peer holds one Bloom filter summa-rizing each of its index lists; these can easily be precomputed and stored in a database. We expect index lists (and, thus, their Bloom filters) to be rather static, so that summaries need not be reconstructed frequently. During the regular Post process, these per-term Bloom filters are added to the posts. Note that Bloom filters like all sparse bit vectors can be compressed very well using standard compression techniques like gzip in order to save bandwidth and storage resources. Also, there are enhancements that produce com-pressed Bloom filters [17] in order to save bandwidth and storage resources.
A Bloom filter (BF) [2] is a simple data structure that represents a set as a bit vector in order to efficiently (in time and space) support membership queries. With bit vectors being a very compact representation of a set, Bloom filters are an ideal representation in our environment where storage and bandwidth consumption is an issue.

For a particular set, a Bloom filter is a bit map of length m and is created by applying k hash functions on each member document, each yielding a bit location in the vector. Exactly (and only) these positions of the Bloom filter will be set to 1. To check if a given element is in the set, the element is hashed using the same hash function and the corresponding k bits of the Bloom filter are examined. If there is at least one of these bits that is not set to 1, the element is definitely not in the set; otherwise it is conjectured that it is in the set: There is a non-zero probability that the examined k bit positions were set by other documents, thus, creating a false positive . The probability of a false positive can be calculated by pf p  X  (1  X  e  X  kn/m ) k where n is the number of items in the original set.
Given the Bloom filter summaries for individual index lists, we need a way to combine these in order to estimate the novelty of collections with respect to multi-keyword queries. The querying peer receives for each query term a number of posts that contain summaries of the peers X  content plus a Bloom filter that represents the index list for a particular term at a particular peer. Now we consider a Peer P i and the set of its posted Bloom filters { bf i,t 1 , bf i,t 2 , bf mary that represents the set of documents of peer P i that contain all r terms (i.e., that are likely to be in the result list that this peer would return), we combine the peer X  X  r Bloom filters as follows:
Given two Bloom filters with the same hash function and of equal length, it is possible to calculate the Bloom filter that represents the intersection of the two sets by simply combining both filters (i.e. bit vectors) using a bitwise AND operation (cf. Figure 1). We denote this operation by &amp;. For a set { bf 1 , bf 2 , bf 3 , ..., bf r } of Bloom filters, the combined Bloom filter is given by bf i := bf 1 &amp; bf 2 &amp; bf approximates the coverage of each Peer P i with regard to the query. Figure 1: Combining Bloom Filters for Multi-Keyword Queries
This calculation iterates for every query term.
Given Bloom filter representations of the already seen document space and of the peers in question, we need to estimate the expected contribution ( novelty ) of each peer P i to the query result. For this purpose, we compare P i  X  X  Bloom filter bf i (that was obtained by combining its indi-vidual Bloom filters as described in Section 5.3) to the union of the Bloom filters bf comb := S j  X  S bf j , where S is the set of peers already selected for being queried, as the represen-tation for the document space that has already been covered before. We define the novelty of Peer P i (summarized in its Bloom filter bf i ) as i.e., as the number bits that are set in P i  X  X  Bloom filter that are not yet set in bf comb .

Analogously, we define the overlap between bf comb and the Bloom filter of a peer P i as i.e., the number of bits that are set in both Bloom filters. Note that this does not really yield the exact number of overlapping documents, but only a relative approximation that allows us to differentiate between several peers.
The accuracy and, thus, the potential of our approach is directly related to the false positive probability introduced before, which in turn highly depends on the length of the Bloom filters. We currently assume that all Bloom filters across the system are of equal length, simplifying the bit op-erations necessary in our approach. The approach based on Bloom filters for estimating novelty was selected primarily due to its simplicity for implementation. We are currently in parallel developing alternative techniques based on min-wise independent permutations [4] for this orthogonal but important issue. With this length being a trade-off between accuracy on the one hand and bandwidth and storage con-sumption on the other hand, we have chosen the size of the Bloom filters so that they are adequately able to represent even the largest index lists in our experimental environment and leave determining the optimal filter sizes for future work, as it is outside the scope of this paper.

This issue becomes even more important bearing in mind that Bloom filters are sent along with each Post, i.e., one Bloom filter per ( P eer  X  T erm )-pair. Similar to [28] we model relevance and overlap separately. To do so, we use a two-step approach that first ranks the collections according to an arbitrary quality estimator that yields numerical collection scores (like CORI) and subse-quently re-ranks the collections incorporating their pair-wise overlap.
 Algorithm 1 Algorithm to combine quality and overlap 1: input: s [ i ] and bf [ i ] for each peer i  X  P = { 1 ...n } 2: output: ranking rank [ r ] with all peers 3: S := P 4: j := argmax i  X  S { s [ i ] } 5: bf comb := bf [ j ] 6: rank [1] := j 7: S := S \{ j } 8: for r = 2 to n do 9: quality [ i ] := computeQuality () 10: novelty [ i ] := computeN ovelty () 11: j := argmax i  X  S {  X   X  quality [ i ] + (1  X   X  )  X  novelty [ i ] } 12: rank [ r ] := j 13: S := S \{ j } 14: bf comb := bf comb  X  bf [ j ] 15: end for
Algorithm 1 describes a simple combination of these two measures for quality and overlap. The computation works as follows: the algorithm has as input a set P of peers with scores s [ i ] and Bloom filters bf [ i ] for each of these peers (based on the quality estimation for a query). In the first step the top-ranked peer (with the highest score) is consid-ered as a starting point promising high quality results and gets the highest rank. Note that we do not yet consider overlap, as the first collection will by definition contribute only novel documents, since no document has been seen be-fore 2 . The further steps include picking the best peer of the remaining set of peers S in a loop. The computation of the best peer comprises the combination of quality and overlap of a peer i using a combination factor  X  : Algorithm 2 Algorithm to compute the quality measure 1: input: s [ i ] for each peer i  X  S = { 1 ...m } 2: output: quality [ i ] for all peers i 3: s max := max i  X  S { s [ i ] } 4: for i = 1 to m do 5: quality [ i ] := s [ i ] /s max 6: end for
Algorithm 2 describes the computation of the quality mea-sure for all peers i in a set of peers. Note that in our main algorithm 1 the set S of peers decreases. The division by
Yet this technique is easily applicable if we for example query the local collection first. In that case, the local peer is considered the top-ranked peer and the algorithm proceeds as usual the maximum score s max is used to normalize the original scores s [ i ] of all remaining peers in S . This way quality [ i ] of all peers is returned.
 Algorithm 3 Algorithm to compute the contribution of a peer 1: input: bf comb , bf [ i ] for each peer i  X  S = { 1 ...m } 2: output: overlap [ i ] for all peers i 3: for i = 1 to m do 4: newDocs [ i ] := newDocs ( bf [ i ] , bf comb ) 5: oldDocs [ i ] := oldDocs ( bf [ i ] , bf comb ) 6: o [ i ] := newDocs [ i ] /log ( oldDocs [ i ] + 2) 7: end for 8: o max := max i  X  S { o [ i ] } 9: for i = 1 to m do 10: novelty [ i ] := o [ i ] /o max 11: end for
The computation of the overlap measure for all peers i is described in Algorithm 3. First, the algorithm computes a not normalized measure o [ i ] by using the two functions newDocs () and oldDocs (). Both of them combine Bloom filters to estimate the number of new or old documents in bf [ i ] in comparison to bf comb . These two values are used to calculate o [ i ] which increases with a higher number of new documents and with a lower number of old documents. In the second step, the algorithm takes the maximum value of o [ i ] for normalization and returns overlap [ i ] of all remaining peers.

The two functions newDocs () and oldDocs () pick up the idea of novelty in Section 5. The newDocs () function returns the number of set bits in the first Bloom filter which are not set in the second one:
The function oldDocs estimates the number of already seen documents and returns the number of bits that are set in both Bloom filters:
Using this algorithm we get a refined ranking of all peers by considering quality and overlap. The combination factor  X  is variable between 0 and 1 to emphasize one of the two measures. In the case  X  = 1, the algorithm just takes the original quality scores and delivers the same ranking of peers we would get considering only the quality measure.
One pivotal issue when designing our experiments was the absence of a standard benchmark. While there exist a num-ber of benchmark collections for (centralized) Web search, it is not clear how to apply these collections to our scenario. While other studies partition these collections into smaller, disjoint pieces, we do not think this is an adequate approach. In contrast, we expect a certain degree of overlap among the collections, with popular documents being indexed by a sub-stantial fraction of all peers, but, at the same time, with a large number of documents only indexed by a tiny fraction of all peers. For this work, we have created a random sample of the GOV document collection [25] and partitioned it into dis-joint fragments. Statistical details about this sample (sub-sequently also referred to as reference collection ) are shown in Table 1. All recall measurements are relative to this ref-erence collection. Collections were created by using various strategies to combine these fragments. For a crude estimate of the potential of our proposals, we have split our sample into six fragments and created collections by choosing all subsets with three fragments, thus, ending up with 6 3 =20 collections. For a more sophisticated analysis, we have split the sample into 100 fragments and used a variety of strate-gies to assign those to the collections:
For the query workload we took all 50 queries from the topic-distillation track of the TREC 2003 Web Track bench-mark [25].

All experiments were conducted on a prototype imple-mentation of the MINERVA peer-to-peer web search engine described in Section 3. We order all collections according to their expected benefit to the query using CORI as a baseline and our proposed strategy. Preliminary experiments have shown that our algorithm produces the most robust results for values of alpha around 0.8; the exact choice of alpha is not critical, as the results for values of alpha between 0.6 and 0.9 showed only small differences. We therefore limit our experiments to this specific value.

In the experiments we compare the improvements in recall as we increase the number of peers that execute the query. Please note that, in all experiments, the recall when query-ing only one remote peer is equal for all strategies, as we currently don X  X  take overlap into account when we have an empty result set representation, i.e. the first peer is chosen using the standard CORI method. The performance met-ric (consistent with our long-term goal to make distributed web search feasible) is the number of peers needed to be con-tacted in order to achieve a combined result of acceptable recall.
We first conducted experiments on the 20 collections de-rived from subsets of three fragments each as described above. As shown in Figure 2, our approach substantially outper-forms CORI, that tends to select similar and, thus, highly overlapping peers. In order to reach a recall of 80%, for ex-ample, CORI needs to contact six peers, whereas our strat-egy can do the same with two peers. We would like to point out that considering overlap only in this environment would allow us to reach a recall of 100% with only 2 peers (because there are pairs of 2 peers each that cover the whole reference collection).
Next, we additionally duplicated each peer, i.e., ending up with 40 peers. Theses results are presented in Figure 3. As expected, CORI now shows improvement at only every second peer, as each collection exists twice and is assigned adjacent ranks when estimating their expected quality. Our approach, however, shows continuous recall improvements, as the mirror collections are pushed down in the peer rank-ing. Even though this setting might seem fitted to our strat-egy, we argue that mirrored collections appear frequently in real-life, e.g., collections are duplicated in order to achieve fault-tolerance. This is particularly true in a P2P setting were replication is a mean to cope with the high dynam-ics of the system, as peers unpredictably enter and leave the system. In this setting, our approach again outperforms CORI in terms of both recall and number of peers required to achieve recall thresholds.(cf. Figure 3).
On 20 peers created by randomly assigning each fragment to 4 peers, the improvements seen by our approach were smaller, because the pairwise overlap between the collections is almost negligible. Thus, this is a worst-case scenario for our algorithm. As can be seen in Figure 4, the improvement is typically less than 5%.
Next, we created peers using the alpha/beta -approach il-lustrated above. We chose r = 4, alpha = 0 . 2, and beta = 0 . 8 and created 40 distinct peers. The results closely resem-ble those obtained from the random collections, so we do not show a separate figure due to space constraints.
Finally, we created peers using the notion of a sliding win-dow as described above. We chose r = 10 and offset =2 to end up with 50 peers. Our approach again clearly outper-forms CORI in this setting; in order to reach a recall of 80%, for example, CORI on average needs to contact 20 peers, whereas our strategy achieves this goal by contacting only 7 peers (Figure 5). It clearly shows substantial improvements already and especially for a relatively small number of peers. This underlines the importance of a powerful collection se-lection strategy in a distributed search environment, where efficiency in terms of bandwidth consumption and latency mainly depends on the number of contacted peers.

We have introduced a query-specific estimator of mutual overlap between collections and presented a novel way to combine it with existing quality estimation metrics such as CORI. The experiments have proved the high potential of overlap-aware collection selection. It can drastically de-crease the number of collections that have to be queried in order to achieve good recall. Depending on the actual degree of overlap between the collections, we have seen remarkable improvements especially at low numbers of queried peers. This fits exactly with our scenario of distributed web search where we want to put low limits in the number of peers in-volved in a query. We believe this is a large step toward making distributed, large-scale web search feasible.
We will extend our experiments to larger-scale data sets, not only in the field of text retrieval but also in the field of (semi-)structured data. We also plan to investigate the influence of different quality selection estimators other than CORI. Future work also includes further investigation on choosing the most efficient size of Bloom Filters, considering the trade-off between accuracy on the one hand and band-width and storage consumption on the other hand. Also, other estimators of mutual overlap will be studied for their performance and estimation characteristics. A more thor-ough analysis of the scalability and the actual resource con-sumption of our approach is also on our agenda. [1] K. Aberer, M. Punceva, M. Hauswirth, and [2] B. H. Bloom. Space/time trade-offs in hash coding [3] E. Buchmann and K. B  X ohm. How to Run Experiments [4] J. Byers, J. Considine, M. Mitzenmacher, and S. Rost. [5] J. Callan. Distributed information retrieval. Advances [6] J. P. Callan, Z. Lu, and W. B. Croft. Searching [7] S. Chakrabarti. Mining the Web: Discovering [8] F. M. Cuenca-Acuna, C. Peery, R. P. Martin, and [9] R. Fagin. Combining fuzzy information from multiple [10] D. Florescu, D. Koller, and A. Y. Levy. Using [11] N. Fuhr. A decision-theoretic approach to database [12] S. Ganguly, M. Garofalakis, and R. Rastogi.
 [13] T. Grabs, K. B  X ohm, and H.-J. Schek. Powerdb-ir: [14] L. Gravano, H. Garcia-Molina, and A. Tomasic. Gloss: [15] T. Hernandez and S. Kambhampati. Improving text [16] S. Melnik, S. Raghavan, B. Yang, and [17] M. Mitzenmacher. Compressed bloom filters.
 [18] Z. Nie, S. Kambhampati, and T. Hernandez.
 [19] H. Nottelmann and N. Fuhr. Evaluating different [20] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and [21] A. Rowstron and P. Druschel. Pastry: Scalable, [22] L. Si, R. Jin, J. Callan, and P. Ogilvie. A language [23] I. Stoica, R. Morris, D. Karger, M. F. Kaashoek, and [24] T. Suel, C. Mathur, J. Wu, J. Zhang, A. Delis, [25] Text REtrieval Conference (TREC). [26] Y. Wang, L. Galanis, and D. J. de Witt. Galanx: An [27] J. Xu and W. B. Croft. Cluster-based language [28] Y. Zhang, J. Callan, and T. Minka. Novelty and
