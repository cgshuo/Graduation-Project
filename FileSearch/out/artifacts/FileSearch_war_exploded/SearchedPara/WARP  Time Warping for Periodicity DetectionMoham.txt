
Periodicity mining is used for predicting trends in time series data. Periodicity detection is an essential process in periodicity mining to discover potential periodicity rates. Existing period-icity detection algorithms do not take into account the presence of noise, which is inevitable in almost every real-world time se-ries data. In this paper, we tackle the problem of periodicity de-tection in the presence of noise. We propose a new periodicity detection algorithm that deals efficiently with all types of noise. Based on time warping, the proposed algorithm warps (extends or shrinks) the time axis at various locations to optimally remove the noise. Experimental results show that the proposed algorithm out-performs the existing periodicity detection algorithms in terms of noise resiliency.
Time series data captures the evolution of a data value over time. Examples of time series data are meteorological data con-taining several measurements, e.g., temperature and humidity; stock prices depicted in financial market; power consumption data reported in energy companies; and event logs monitored in com-puter networks. Periodicity mining is a tool that helps in predict-ing the behavior of time series data [15]. For example, periodicity mining allows an energy company to analyze power consumption patterns and predict periods of high and low usage so that proper planning may take place.

Noisy data is inevitable in reality, either due to unreliable data sources or during data transmission. In addition, in streaming ap-plications, data elements are subject to be dropped for processing reasons. Therefore, data mining techniques should be aware of such noisy data environments.

Discovering the periodicity rat e of time series data, hereafter referred to as periodicity detection , has drawn the attention of the data mining research community very recently. Indyk et al. [8] have addressed this problem under the name relaxed period ,and have developed an O ( n log 2 n ) time algorithm, where n is the length of the time series. We refer to the algorithm of [8] as RELAX. Elfeky et al. [7] have proposed an O ( n log n ) time al-gorithm for the same problem under the name segment periodic-ity . As the algorithm of [7] is convolution-based, we refer to it as CONV. In the data streaming context, Papadimitriou et al. [14] have proposed AWSOM that is a linear periodicity detection algo-rithm using Wavelets. AWSOM discovers only periodicity rates of powers of two due to the use of Wavelets to approximate the data stream. All three algorithms [8, 14, 7] suffer from the poor resilience to noise. To support this claim, we conduct an initial experiment using synthetic data, where different types of noise are considered, mainly replacement, insertion and deletion noise. This experiment studies the behavior of these algorithms: RE-LAX [8], CONV [7], and AWSOM [14] towards these types of noise as well as different mixtures of them. Results are given in Figure 1, in which we use the symbols  X  X  X ,  X  X  X , and  X  X  X  to denote the three types of noise, respectively. Two or more types of noise can be mixed, e.g.,  X  X  I D X  means that the noise ratio is distrib-uted equally among replacement, insertion, and deletion, while  X  X  D X  means that the noise ratio is distributed equally among inser-tion and deletion only. Figure 1 shows how the three algorithms treat the replacement noise different from the other types of noise, and how their noise resiliency deteriorates to low accuracy levels when the amount of introduced noise increases.

In this paper, we address the problem of periodicity detection in the presence of noise. We propose a new algorithm for periodicity detection that deals efficiently with all types of noise. In Section 2, we introduce the notation used throughout the paper. Then, we spot where the previous periodicity detection algorithms [8, 14, 7] fail to capture the insertion and deletion noise. In Section 3, we present our proposed time warping based algorithm for periodic-ity detection in the presence of noise, along with its online ver-sion that fits the data stream model. In Section 4, we evaluate the performance of the proposed algorithm, and compare it to the pre-vious periodicity detection algorithms. Finally, we briefly discuss the related work in Section 5, and summarize our conclusions in Section 6.
Assume that a sequence of n timestamped feature values is col-lected in a time series. For a given feature e ,let e i be the value of the feature at timestamp i . The time series of feature e is rep-resented as T = e 0 ,e 1 ,...,e n  X  1 . For example, the feature in a time series for power consumption might be the hourly power consumption rate of a certain customer, while the feature in a time series for stock prices might be the final daily stock price of a spe-cific company. If we discretize 1 the time series feature values into nominal discrete levels 2 and denote each level (e.g., high, medium, low, etc.) by a symbol (e.g., a , b , c , etc.), then the set of collected feature values can be denoted as  X = { a , b , c ,  X  X  X } . Hence, we can view T as a sequence of n symbols drawn from a finite alphabet  X  .

A time series may also be a sequence of n timestamped events drawn from a finite set of nominal event types. An example is the event log in a computer network that monitors the various events. Each event type can be denoted by a symbol (e.g., a , b , c ,etc.), and hence we can use the same notation above.
Segment periodicity [7], and similarly the relaxed period [8], are defined as follows.

A time series T is said to be periodic with a period p if it can be divided into equal length segments, each of length p , that are For example, the time series T = abcabcabc is clearly periodic with a period 3. Likewise, the time series T = abcabdabc is pe-riodic with a period 3 despite the fact that its second segment is not identical to the other segments. Since the symbols are con-sidered nominal, i.e., no inherent order is assumed, Hamming dis-tance is used as the similarity measure to compare any two seg-ments. Hamming dist ance compares the symbol s position-wise. In other words, the symbol at position i in a segment is compared to the symbol at position i in the other segment. Two segments are considered similar only if there are enough symbols that match position-wise. Hamming distance, however, fails to capture the similarity of two segments if they are out of phase, e.g., when few symbols are added to or deleted from one of the segments due to some sort of noise. For example, although the two segments abcdef and abgcde look similar (only an extra symbol in the third position in the second segment), the Hamming distance between these two segments is 4, which makes them less similar than they should be.

Therefore, we need another similarity metric that takes into ac-count insertion and deletion noise. In string matching problems, this metric is called the edit dis tance [4]. In time series similarity search, this metric is called time warping [2]. Time warping al-lows an elastic shifting of the time axis to accommodate similar, yet out-of-phase, segments.

Similar to the dynamic computation of edit distance [4], time warping is computed dynamically and hence is commonly referred to as DTW (Dynamic Time Warping). DTW is defined as follows. Let X =[ x 0 ,x 1 ,...,x n  X  1 ] and Y =[ y 0 ,y 1 ,...,y finite length sequences of symbols 3 , each of length n .Let X be the sequence X after removing the first element x 0 ,thatis X = [ x ,x 2 ,...,x n  X  1 ] . Then, the standard definition of DTW is where d ( x i ,y j ) is the distance between the two symbols x y . Since we deal with nominal symbols, the distance between two symbols is either 0 or 1 according to whether they match or not.
The DTW distance is computed as follows. An n  X  n matrix is constructed where the cell ( i, j ) contains the value d ( x In other words, the cell ( i, j ) corresponds to the alignment of the symbols x i and y j . A warping path is a contiguous path M = m 0 ,m 1 ,...,m K  X  1 from cell (0 , 0) to cell ( n  X  where m k corresponds to cell ( i k ,j k ) , i.e., m k = d ( x There are exponentially many warping paths, each corresponds to a particular alignment between the two sequences. The warp-ing cost of a specific warping path is the total distances to walk through this path: K  X  1 k =0 m k . In other words, the warping cost is the distance between the two sequences according to a spe-cific alignment. The DTW distance is the minimum warping cost among all possible warping paths.

Therefore, this minimum warping path can be found using dy-namic programming to evaluate the cumulative distance  X  ( i, j ) as the distance d ( x i ,y j ) found in the current cell and the minimum of the cumulative distances of the adjacent cells.  X  ( i, j )= d ( x i ,y j )+min {  X  ( i  X  1 ,j  X  1) , X  ( i The cumulative distance at the last cell ( n  X  1 ,n  X  1) corresponds to the sought DTW distance.
 Clearly, the time and space complexity of computing the DTW distance is O ( n 2 ) . Figure 2 gives an example for the DTW matrix constructed for the two segments abcdef and abgcde ,wherethe minimum warping path is circled.
Note that the Hamming distance can be seen as a special case of time warping where the warping path is constrained to be the diagonal, i.e., i k = j k = k .
The main idea of periodicity detection is that when we shift the time series p positions and compare the original time series to the shifted version, we find both time series very similar if p is a candidate period value. Let T ( p ) denote the time series T af-ter being shifted p positions. For example, if T = abcabdabc , then shifting T three positions results in T (3) =  X  X  X  X  abcabd , where the symbol  X  denotes the  X  X on X  X  care X  symbol. Compar-ing T to T (3) results in four matches out of six possible matches. We argue that such an occurrence of a large number of matches corresponds to a candidate period for the time series in hand. To ascertain this argument, assume that comparing T with T ( p ) sults in n  X  p matches for a certain index p and a time series T of length n . Then, e p = e 0 ,e p +1 = e 1 ,...,e n  X  1 = e and also, e 2 p = e p ,e 2 p +1 = e p +1 ,... , etc., which means that the segment of length p is periodic and p is a perfect period for T .If for another index q , comparing T with T ( q ) results in a number of matches slightly less than n  X  q due to a few mismatches, then q can be considered a candidate period for T .

In the next sections, we introduce our proposed algorithm that employs time warping for periodicity detection in the presence of noise. Our proposed algorithm is called WARP (WArping foR Pe-riodicity). First, the algorithm is presented for traditional time se-ries, where the whole time series is available (stored) for process-ing (Section 3.1). Then, the online version of WARP, which fits the data stream model, is presented (Section 3.2).
The main idea of our proposed WARP algorithm is to make use of the DTW matrix described earlier. Figure 3 shows the DTW matrix constructed by comparing a time series T = e ,e 1 ,...,e n  X  1 to itself. Clearly, the matrix is symmetric and all the diagonal elements are equal to 0. The minimum warping path is the diagonal, i.e., the DTW distance between a time series and itself is 0, which is trivial. However, this DTW distance is not our concern here. Our concern is rather the DTW distances between T and all its shifted versions. The key observation is that the DTW matrix contains all the comparisons between all the symbols of the time series. Each subdiagonal in the DTW matrix contains the elements that correspond to comparing the original time series T with one of its shifted versions. For example, the first subdiagonal, which starts from cell (0 , 1) and ends at cell ( n  X  2 ,n tains d ( e 0 ,e 1 ) ,d ( e 1 ,e 2 ) ,...,d ( e n  X  2 ,e n  X  1 comparing T with T (1) . Similarly, the second subdiagonal, which starts from cell (0 , 2) and ends at cell ( n  X  3 ,n  X  1) , corresponds to comparing T to T (2) . Consequently, the i th subdiagonal, which starts from cell (0 ,i ) and ends at cell ( n  X  i  X  1 ,n  X  to comparing T to T ( i ) . Therefore, the DTW distance between T and T ( i ) can be computed by finding the minimum warping path starting from cell (0 ,i ) .

Formally, consider the DTW matrix shown in Figure 3 of a specific time series T of length n . For each possible period value p =1 , 2 ,...,n/ 2 , we compute the minimum warping path M starting from cell (0 ,p ) . The warping cost that corresponds to the warping path M p represents the DTW distance between T and T ( p ) , and can be computed dynamically as the cumulative distance  X  ( n  X  p  X  1 ,n  X  1) .
 A low value of DTW T,T ( p ) indicates a high similarity be-tween T and T ( p ) , and consequently p can be considered a candidate period value for the time series T . The maximum value of DTW T,T ( p ) is n  X  p , which corresponds to align-ing the symbols without any shifts with different symbols at each position. Therefore, the confidence of a period value p is whose confidence is larger than or equal to the periodicity thresh-old  X  .
We can spot two problems in this technique. First, the zero val-ues in the diagonal will drag the minimum warping paths towards the diagonal. If a minimum warping path of a specific period value p coincides with the diagonal, this means that the p -positions shift in T ( p ) has been ignored in aligning T with T ( p ) . For example, Figure 4(a) shows the DTW matrix for T = abacde ,wherethe minimum warping path for p =2 is circled. To solve this prob-lem, our WARP algorithm replaces the zero values in the diagonal by large values (  X  ). These large values push the minimum warp-ing path away from the diagonal. For example, Figure 4(b) shows the minimum warping path for p =2 of the same DTW matrix of Figure 4(a) after altering the diagonal values to  X  .

The second problem is that if p turns out to be a candidate pe-riod value, the p th subdiagonal would contain so many zeros that it might drag the adjacent minimum warping paths, which is sim-ilar to the first problem when the diagonal drags the subsequent minimum warping paths. For example, since p =4 is a perfect period for T = abcdabcd , the DTW matrix would suggest that p =3 is also a candidate period. The warping costs of a candidate period value and the adjacent period values take the shape shown in Figure 5. Therefore, the obvious solution would be to consider only the minimum warping cost to indicate a better candidate pe-riod value. Consequently, our WARP algorithm considers all local minima warping costs to indicate candidate period values.
The key issue to perform the WARP algorithm online is to maintain the DTW matrix over the continuous arrival of data. We use the notion of a single sliding window that slides over the data stream. Let S = e 0 ,e 1 ,... be an infinite length data stream. Consider a window of length w ,andlet S i,w denote the portion of the data stream S that is of length w and starts at position i . Performing the WARP algorithm over the very first window S computes the DTW matrix and discovers all potential periods of values ranging from 1 to w/ 2 . Let the window slide z  X  w posi-tions such that the current portion of the data stream is S WARP algorithm is performed over the current portion, and a new DTW matrix is computed. The two matrices formed from the pre-vious and the current portions should be combined to compute the whole DTW matrix of the entire S of length z + w . Figure 6 shows that DTW matrix, where the shaded upper left region reflects the DTW matrix of S 0 ,w , and the shaded lower right region reflects the DTW matrix of S z,w . The unshaded regions are those whose values cannot be determined directly since sliding the window z positions means losing the first z symbols of the data stream, and hence they cannot be compared to the new arrived symbols.
Since all the values are important in searching for the minimum warping path, our online WARP algorithm tries to estimate the values at those unshaded regions of the DTW matrix of Figure 6. Recall that the distance between two symbols is either 0 or 1 ac-cordingtowhethertheymatchornot,i.e., u = v  X  d ( u, v )=0 for any two symbols u and v . Assume that we would like to es-timate the value d ( e i ,e j ) at cell ( i, j ) where 0  X  w  X  1 &lt;j  X  z + w  X  1 . In other words, the symbol e i is one of the lost symbols according to sliding the window, and the sym-bol e j is one of the new arrived symbols. Let e k be one of the symbols that overlap between the previous and current portions of the data stream, i.e., z  X  k  X  w  X  1 . Clearly, as in Figure 6, the cells ( i, k ) and ( k, j ) are in the shaded regions, and hence the values d ( e i ,e k ) and d ( e k ,e j ) are known. Therefore, we can de-rive d ( e i ,e j ) according to the truth table and justifications given in Table 1. In order to resolve the undetermined value of d ( e when d ( e i ,e k )= d ( e k ,e j )=1 , we can iterate over all possible values of k ( z  X  k  X  w  X  1 ) until ( e i ,e k )=0  X  d ( e where the value d ( e i ,e j ) can be determined. If for all values of k , d ( e i ,e k )= d ( e k ,e j )=1 , then we can conservatively estimate d ( e i ,e j ) to have the value 1.

The value of z is required to be less than or equal to the window length w so that we get an overlap region between the two shaded regions of the DTW matrix. This overlap region is essential to de-rive the values in the unshaded regions. Moreover, we can observe that the higher the value of z , the more values to be estimated, and so the DTW matrix becomes less accurate. However, the lower the value of z is, the slower the process of advancing the data stream gets. As a compromise, we choose z to be equal to w/ 2 .Inother words, we slide a window of length w a number of positions equal to w/ 2 .
 The selection of the window length w has another tradeoff. Whereas a small window length is required for early and real-time output, it slows down the process of advancing the data stream. Moreover, a large window length reduces the number of times the sliding occurs, which consequently improves the accuracy since the estimation occurs less frequently. However, we are bounded by the maximum memory size allocated for the DTW matrix. Clearly, the DTW matrix needs O ( n 2 ) memory, where n is the so-far length of the data stream. Theref ore, whenever the memory allo-cated for keeping the DTW matrix is consumed entirely, the online WARP algorithm throws away the first w rows and w columns of the DTW matrix, leaving only the bottom right ( n  X  w ) 2 This behavior conforms to the fact that the recent portion of the data stream is of more interest than the past ones.
This section contains the results of an experimental study that examines various aspects of both WARP and Online WARP. Ex-periments are conducted using synthetic data in order to examine the accuracy, resilience to noise and the time performance of the proposed algorithms. The three peri odicity detection algorithms: RELAX [8], CONV [7], and AWSOM [14] are compared with our proposed algorithms throughout the experiments.

We generate controlled synthetic time series data by tuning some parameters, namely, data distribution, period, alphabet size, type, and amount of noise. Both uniform and normal data distri-butions are considered. Types of noise include replacement, in-sertion, deletion, or any combination of them. Inerrant data is generated by repeating a pattern, of length equal to the period, which is randomly generated from the specified data distribution. The pattern is repeated till it spans the specified time series length. Noise is introduced randomly and uniformly over the whole time series. Replacement noise is introduced by altering the symbol at a randomly selected position in the time series by another symbol. Insertion or deletion noise is introduced by inserting a new symbol or deleting the current symbol at a randomly selected position in the time series. Unless otherwise stated, all the experiments are conducted using noisy data with 10% mixture of noise (replace-ment, insertion and deletion noises).
The first set of experiments inspects the accuracy of WARP with respect to the discovered potential periods. The accuracy measure that we use is the ability of the algorithm to detect the pe-riodicities that are artificially embedded into the synthetic data. To discover a period accurately, it is not enough to discover it at any periodicity threshold value. In other words, the periods discovered with a high periodicity threshold value are better candidates than those discovered with a lower periodicity threshold value. There-fore, we define the confidence of a discovered period to be the minimum periodicity threshold value required to detect this pe-riod. The accuracy is measured by the average confidence of all the periods that are embedded artificially into the synthetic data. Figure 7(a) gives the accuracy results of WARP for different com-binations of the embedded period and the data distribution. Fig-ure 8 compares the accuracy of WARP to that of RELAX, CONV, and AWSOM.

Figure 7(a) shows an unbiased behavior of WARP with respect to the period value, and shows that WARP is able to discover all the embedded periods at 85% confidence level. Figures 8(b) and 8(c) show that WARP is much more accurate than CONV when inser-tion and deletion noise are considered. In general, Figure 8 shows that WARP has a uniform accuracy with respect to all the period values, rather than the bias of RELAX towards the longer periods and the bias of AWSOM towards the shorter periods.

Figures 7(b) and 9 give the results of inspecting WARP X  X  re-silience to noise and comparing it to RELAX, CONV, and AW-SOM. Comparing Figure 7(b) to Figure 1 shows that WARP is much more resilient to all types of noise. Even when insertion and deletion noise are mixed with replacement noise, the accuracy does not deteriorate badly as is the case with the previous peri-odicity detection algorithms. Th ese results are further supported by the results given in Figure 9, where different mixtures of noise are considered. Figure 9(a) shows that WARP is as resilient to replacement noise as RELAX and CONV, and is more resilient to replacement noise than AWSOM. With respect to insertion and deletion noise, Figures 9(b) and 9(c) show that WARP is as re-silient as AWSOM, and is more resilient than RELAX and CONV. Although WARP is as resilient as AWSOM to insertion and dele-tion noise, Figure 9(c) shows that AWSOM accuracy fluctuates ir-regularly unlike WARP whose accuracy decreases monotonically with the increase in the noise ratio.

WARP X  X  better accuracy and resilience to noise does not come for free. Figure 10(a) shows that WARP takes more process-ing time than CONV. This behavior is due to the fact that the time complexity of WARP is O ( n 2 ) while the time complexity of CONV is O ( n log n ) . Note that the time complexity of RE-LAX is O ( n log 2 n ) , which is slower than CONV, and hence we selected CONV for the time performance comparison. Similarly, Figure 10(b) shows that Online WARP takes much more process-ing time than AWSOM. AWSOM is a Wavelet-based algorithm that takes O ( n ) time to compute. In conclusion, WARP trades processing time for more accurate periods and more resilience to noise.
The next set of experiments studies Online WARP in terms of its accuracy and time performance. The two variables that control the behavior of Online WARP are the window length w and the sliding size z .

Figure 11(a) gives the accuracy results while varying the win-dow length for three different sliding sizes. Figure 11(a) shows that increasing the window length improves the accuracy up to a certain level when the window length is enough to capture all em-bedded periods.

Figure 11(b) gives the accuracy results while varying the slid-ing size for three different window lengths. Figure 11(b) shows that decreasing the sliding size improves the accuracy due to the increase in the overlap region of Figure 6, which helps in better es-timation of the values in the unshaded regions. If the sliding size z is made equal to the window length w , there would be no overlap region. Consequently, all the values to be estimated are set to 1, which deteriorates the accuracy.

To evaluate the time performance of Online WARP, Figure 12 shows the time behavior of Online WARP with respect to varying the window length and the sliding size. Figure 12(a) shows that increasing the window length has a trend of reducing the process-ing time since the time needed to cover the whole data stream is reduced. Moreover, Figure 12(b) shows that increasing the slid-ing size reduces the processing time because of two reasons: (i) the number of values to be estimated decreases and therefore less time is needed, (ii) less time is needed to cover the whole data stream. This final result shows that there is a tradeoff for selecting the value of the sliding size and that z = w/ 2 can be considered a fair selection.

The last experiment studies the accuracy of Online WARP un-der different allocated memory sizes with three different window lengths. Figure 11(c) gives the results of this experiment. Fig-ure 11(c) shows that varying the memory size does not affect the accuracy as long as the window length is enough to capture the em-bedded periods. When the window length is so small that it does not capture the embedded periods, the accuracy decreases with the decrease in the allocated memory size. This behavior shows that although the space complexity of Online WARP is high, Online WARP achieves high accuracy levels under small allocated mem-ory as long as the sliding window length is selected to cover the embedded periods.
In Section 1, we have discussed the significant periodicity de-tection work in time series data and in data streams. In this section, we cover more work related to periodicity detection, and we cover the use of dynamic time warping in data mining applications.
Ma and Hellerstein [13] have developed a linear distance-based algorithm for discovering the potential periods with respect to the symbols of the time series. In other words, the algorithm of [13] discovers the periodicities of the symbols of the time series rather than the periodicity of the entire time series. Elfeky et al. [6] have given a formal definition for this type of periodicity, termed sym-bol periodicity , and have developed a one-pass convolution-based algorithm for symbol periodicity detection.

Berndt and Clifford [2] have introduced the concept of DTW to the data mining community. They have showed how to apply DTW as a time series similarity measure. Agrawal et al. [1] have acknowledged the benefit of DTW over the Euclidean distance as a similarity measure in the presence of noise for time series data. However, they have avoided using DTW for its resistance to in-dexing and its intensive computation. Since then, much work has focused on indexing DTW [12, 9], and some work has focused on reducing its processing time [11, 3].
In this paper, we have proposed a time warping algorithm, named WARP, for periodicity detection in the presence of noise. To handle efficiently all types of noise, WARP extends or shrinks the time axis at various locations to optimally remove the noise. Furthermore, we have proposed an online version of WARP that fits the data stream model. An empirical study using synthetic data shows that there is a tradeoff between noise resiliency and time performance. WARP is more noise resilient, yet requires more processing time, than the previous periodicity detection al-gorithms. Moreover, Online WARP is shown empirically to be reasonably accurate, even under low memory resources.

