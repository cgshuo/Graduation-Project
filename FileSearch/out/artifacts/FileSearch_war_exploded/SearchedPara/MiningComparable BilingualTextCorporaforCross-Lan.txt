 Integrating information in multiple natural languages is a challenging task that often requires man ually created lin-guistic resources suc h as a bilingual dictionary or examples of direct translations of text. In this pap er, we prop ose a general cross-lingual text mining metho d that does not rely on any of these resources, but can exploit comparable bilingual text corp ora to disco ver mappings between words and documen ts in di eren t languages. Comparable text cor-pora are collections of text documen ts in di eren t languages that are about similar topics; suc h text corp ora are often naturally available (e.g., news articles in di eren t languages published in the same time perio d). The main idea of our metho d is to exploit frequency correlations of words in di er-ent languages in the comparable corp ora and disco ver map-pings between words in di eren t languages. Suc h mappings can then be used to further disco ver mappings between doc-umen ts in di eren t languages, achieving cross-lingual infor-mation integration. Evaluation of the prop osed metho d on a 120MB Chinese-English comparable news collection sho ws that the prop osed metho d is e ectiv e for mapping words and documen ts in English and Chinese. Since our metho d only relies on naturally available comparable corp ora, it is gen-erally applicable to any language pairs as long as we have comparable corp ora.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Searc h and Retriev al]: Text Mining General Terms: Algorithms Keyw ords: Cross-lingual text mining, comparable corp ora, frequency correlation, documen t alignmen t
As more information becomes available online, we have also seen more and more information in di eren t natural languages suc h as English, Spanish, and Chinese. The web today consists of documen ts in man y di eren t languages. For a user who is interested in nding information from all the documen ts in di eren t languages, it would be very use-ful if we could integrate related information in multiple lan-guages [1].

Curren tly, cross-lingual information integration is often achiev ed through performing cross-lingual information re-triev al(CLIR) [17], whic h allo ws a user to retriev e documen ts in language A with a query in language B. Most CLIR tech-niques rely on man ually created linguistic resources suc h as a bilingual dictionary or examples of direct translations of words and documen ts [16]. Suc h resources may not al-ways be available, esp ecially for minorit y languages; in suc h a case, how to perform cross-lingual information integration would be a signi can t challenge. In this pap er, we prop ose a cross-lingual text mining metho d that can exploit compara-ble bilingual text corp ora to perform cross-lingual informa-tion integration without requiring any additional linguistic resources.

Comparable text corp ora are collections of text documen ts in di eren t languages that are about the same or similar topics. For example, news articles published in the same time perio d tend to rep ort the same imp ortan t international events in various topics suc h as politics, business, science and sports. Suc h data are naturally available to us, so it would be very interesting to study how to exploit them to perform cross-lingual information integration. Even when we have man ually created clean bilingual resources suc h as a bilin-gual dictionary , it may still be desirable to exploit suc h com-parable corp ora for two reasons: (1) New words and phrases are constan tly introduced and it would be hard to keep up-dating a dictionary to include them all. Approac hes suc h as what we prop ose in this pap er can poten tially be used to ac-quire translation kno wledge about suc h new words/phrases from comparable corp ora and help lexicographers update dictionaries. (2) Since comparable corp ora are additional resources, we may exp ect to achiev e better performance by com bining the exploitation of comparable corp ora with that of a bilingual dictionary .

We frame the problem of cross-lingual information in-tegration as one involving mapping or linking words and documen ts in di eren t languages. While comparable cor-pora have been studied extensiv ely in the existing literature (e.g.,[6, 10, 15, 5, 2, 8, 13]), almost all existing work assumes some kind of bilingual dictionary or translation examples to start with. We study how to map words and documen ts from comparable bilingual corp ora without requiring any ad-ditional linguistic resources suc h as a bilingual dictionary .
Our basic idea is to exploit the fact that the frequency dis-tributions of topically related words in di eren t languages are often correlated due to the correlated coverage of the same events. For example, the earthquak e and sea surge disaster that happ ened recen tly in Asia has been covered in the news articles in man y di eren t languages. We can thus exp ect to see a recen t peak of words suc h as \earth-quak e", \India", and \Indonesia" in news articles published in multiple languages. In general, we can exp ect that top-ically related words in di eren t languages tend to co-o ccur together over time. Thus if we have available comparable news articles over a sucien tly long time perio d, it is in-tuitiv ely possible to exploit suc h correlations to learn the asso ciations of words in di eren t languages.

The general idea of exploiting frequency correlations to acquire word translations from comparable corp ora has al-ready been explored in sev eral previous studies (e.g., [6, 10, 15]). However, none of them has adopted a direct compari-son of frequency distributions of candidate words as we do; rather they tend to compute the asso ciations between the words in the same language and then compare asso ciation patterns in two di eren t languages. Our idea and the over-all approac h app ear to be more similar to the metho d used in [7], but there the task is aligning sen tences in parallel corp ora.

With the word mappings, we can then try to matc h doc-umen ts in two di eren t languages based on how well the words in eac h documen t are correlated. We prop ose four dif-feren t metho ds for computing cross-lingual documen t simi-larit y, including a baseline exp ected correlation metho d, an IDF-w eigh ted correlation metho d, a TF-IDF metho d, and a translation mo del metho d. These metho ds allo w us to per-form cross-lingual documen t retriev al as well as linking the most strongly correlated documen ts together.

We evaluated our metho ds on a 120MB Chinese-English news rep ort corpus for both word asso ciation mining and documen t asso ciation mining. The results sho w that our metho d can disco ver meaningful word mappings and can generate meaningful documen t alignmen ts for information integration. The top rank ed word pairs sho w various kinds of interesting asso ciations between words in the two di eren t languages. The documen t alignmen t results have a high precision of 0.8 at a cuto of 100, meaning that 80% of the documen t pairs among the top 100 matc hing results are correctly matc hed.

The rest of the pap er is organized as follo ws. In Sec-tion 2, we discuss our data set. In Section 3 and Section 4, we presen t our metho ds for word asso ciation mining and documen t asso ciation mining resp ectiv ely. The exp erimen t results are rep orted in Section 5, and we summarize our work in Section 6.
The comparable corp ora we exp erimen t with are 6 mon ths' of news articles of Xingh ua English and Chinese newswires dated from June 8, 2001 through November 7, 2001. There are altogether 43488 documen ts in Chinese and 34751 doc-umen ts in English. The average documen t length is 204.6 words. In this data set, there are man y articles in English that have some comparable Chinese articles.

An example of comparable news articles is given in Fig-ure 1 the same international swimming championship in English and Chinese, resp ectiv ely. While these two arti-cles are from the same newswire source, and they cover the same event, they are not translations of eac h other. However, some words in the two articles are clearly trans-lations of eac h other. For example, the Chinese transla-tions of \swimming", \W orld Swimming Championships", and \Men's 400M Freest yle Heats" all occur in the Chinese documen t. (They are underlined.) Figure 1: A fragmen t of an English article (top) and a comparable Chinese fragmen t (bottom) about an international swimming championship
In this section, we presen t our metho d for mining cross-lingual word asso ciations. Our main idea is based on the observ ation that words that are translations of eac h other or about the same topic, tend to co-o ccur in the comparable corp ora at the same/similar time perio ds. Thus if we have some large comparable corp ora available, it is intuitiv ely possible to exploit suc h correlations to learn the asso ciations of words in di eren t languages.
 To see if our intuition can be supp orted empirically , in Figure 2, we compare the frequency distribution (over time) of Megawati with that of its Chinese tranlation \ " (left) and with that of another randomly chosen Chinese name (\Arafat") (righ t). We see that the distributions of the English \Mega wati" and its Chinese translation indeed look very similar with a high correlation of 0.855, while the distributions of \Mega wati" and the Chinese translation of \Arafat" are quite di eren t with a correlation of only 0.0324. These plots sho w that we can indeed exp ect to nd seman ti-cally relev ant mappings between words in di eren t languages by exploiting frequency correlations. We can thus represen t eac h word with a frequency vector and score eac h candidate pair of words (in di eren t languages) by the similarit y of the two frequency vectors.
 Formally , supp ose we have available comparable corp ora C = f ( s 1 ; t 1 ) ; :::; ( s n ; t n ) g , where s i and t umen ts asso ciated with an anc hor point of \ i " in language A and language B, resp ectiv ely. Let x (or y ) be a source (target) word in language A and language B, resp ectiv ely. We use c ( x; s i ) (or c ( y; t i )) to denote the coun ts of x (or y ) in s i (or t i ). The raw frequency vectors for x and y are thus ( c ( x; s 1 ) ; :::; c ( x; s n )) and ( c ( y; t 1 ) ; :::; c ( y; t
In order to mak e the frequency vectors more comparable across di eren t languages, it is desirable to normalize a raw frequency vector so that it becomes a frequency distribution over all the time points. That is, we divide all the coun ts by the sum of all the coun ts over the entire time perio d. Suc h a normalized frequency distribution would allo w us to focus on the relative frequency on di eren t days, whic h is presumably more comparable across di eren t languages than the original non-normalized coun ts.

Let ~x = ( x 1 ; :::; x n ) and ~y = ( y 1 ; :::; y n ) be the normalized frequency vectors for x and y , resp ectiv ely, where In order to compute the similarit y between ~x and ~ y (or word x and word y ), we use the Pearson's correlation coecien t, whic h is a commonly used statistic measure de ned as Using this correlation similarit y measure, we can score every word in language A against every word in language B to obtain a matrix of correlations. Suc h word-lev el mappings can supp ort a user to retriev e words in language A with a word in language B, pro viding some limited supp ort of navigation across languages. Moreo ver, the mappings can also be used to disco ver matc hed documen ts between the two languages, whic h we discuss below.
We can score how well a documen t d 1 in language A matc hes a documen t d 2 in language B by computing a sim-ilarit y score s ( d 1 ; d 2 ) based on how strongly correlated the words in d 1 and those in d 2 are. Technically , man y di eren t metho ds are possible. A natural baseline metho d is to com-pute the exp ected correlation between any word in d 1 and any word in d 2 , i.e., j d j and j d 2 j are the lengths of d 1 and d 2 , resp ectiv ely. We call this metho d Expected Correlation ( ExpCorr ). Clearly , ExpCorr assigns a weigh t to every matc hing word pair based on the corresp onding correlation and penalizes long documen ts due to the naturally high chances of matc hing, whic h are both reasonable.

One de ciency of ExpCorr , however, is that it does not distinguish a common word (e.g., \sp ort") from a more dis-criminativ e word (e.g., \swimming"). Intuitiv ely, matc hing a common word is a weak er evidence for con ten t matc hing between the two documen ts than matc hing a more discrim-inativ e word. A commonly used heuristic in information retriev al is to assign an Inverse Documen t Frequency (IDF) weigh t to eac h word, whic h penalizes popular (thus non-informativ e) words [14]. In our case, we asso ciate an IDF weigh t for a matc hing pair ( x; y ), whic h is de ned as where IDF ( w ) = log n +1 df ( w ) , and df ( w ) is the num ber of doc-umen ts in a language that con tains word w , often called the documen t frequency of a word.

Adding IDF weigh ting to ExpCorr , we obtain the follo w-ing IDF-weighte d Correlation metho d ( IDFCorr ): In both ExpCorr and IDFCorr , the similarit y score gro ws linearly to the coun t of a word in the documen t. However, intuitiv ely, having one extra matc h after matc hing a word 100 times does not add so much extra evidence as matc hing the word the rst time. We thus would like to have the similarit y score to gro w sub-linearly according to the coun t of a matc hing word. Again, in information retriev al, man y form ulas have been prop osed to heuristically normalize the coun t of words to achiev e this e ect. A popular and e ectiv e metho d is the BM25 term frequency normalization metho d [11, 12]. According to this form ula, the normalized coun t of word w in documen t d is given by where k 1 and b are parameters and AvgDocLen is the aver-age documen t length. In our exp erimen ts, we set k 1 = 1 : 2 and b = 0 : 75, whic h are the recommended default settings.
The BM25 weigh ting form ula pro vides an alternativ e way of normalizing the coun t of a word, so BM 25( x; d 1 ) and BM 25( y; d 2 ) can e ectiv ely play the same role as p ( x j d and p ( y j d 2 ), possibly with more reasonably normalization of the coun ts. Thus we use BM 25( x; d 1 ) and BM 25( y; d to replace p ( x j d 1 ) and p ( y j d 2 ), resp ectiv ely, in the IDFCorr approac h to obtain the follo wing form ula, whic h we refer to as BM25 Correlation ( BM25Corr ).
Finally , motiv ated by the language mo deling approac h to information retriev al [9, 17], we can also use the correlation between words to estimate a word translation mo del t ( x j y ) can de ne the similarit y between d 1 and d 2 as the likeliho od of \generating" d 1 with a mo del based on d 2 . That is, where is a smo othing parameter to introduce a bac k-ground language mo del p ( x jC ) for mo deling the noise (com-mon words) in d 1 and p ( y j d 2 ) is the relativ e frequency of word y in d 2 , i.e., p ( y j d 2 ) = c ( y;d 2 ) j d CorrT rans .
We use the data set describ ed in Section 2 to evaluate the prop osed word and documen t mapping metho ds. The documen ts published on the same day are aligned together; there are altogether 148 days. In order to supp ort ecien t computation of word correlations, we use the Lem ur toolkit to index all the documen ts in the comparable corp ora.
Ideally , we can compute the correlation between every word in English and every word in Chinese. However, this involves a huge num ber of com binations. Since not all words are interesting to matc h (e.g., common functional words are not interesting for the purp ose of information integration), we use the follo wing heuristics to signi can tly reduce the space. 1. We rst compute the entrop y [4] of a word in eac h language using the form ula H ( w ) = P t p ( t j w ) log p ( t j w ), where p ( t j w ) is the normalized frequency of word w at time point t (i.e., a day). 2. We then lter out the high entrop y words. These are usu-ally frequen t words as they tend to occur in everyda y's news articles. The highest entrop y is log(148), whic h is about 5, and we used a cuto of 4 : 8. 3. We further lter out those words with an overall low frequency with a cuto of 10. These words are rare, so their correlations may not be reliable.

In Table 1, we sho w the top 38 pairs with the highest correlations in the whole corp ora along with their correla-tions, most of whic h are extremely high qualit y matc hings. Man y num bers are correctly aligned based on the dates in the news articles in the two di eren t languages. There are also some direct translations of mon ths, suc h as \august" vs. 8. They are clearly learned from the mon th information in the news articles; in Chinese news articles, August is written with the num ber 8. The character matc hing \afghan" and \afghanistan" is one of the three characters in the Chinese translation of \Afghanistan" (i.e. matc hing \swimming" is also precisely its Chinese transla-tion, and the top two characters returned for \terror" are the exact translation of this word in Chinese (i.e. other interesting example is the matc hing of \APEC" and \ap ec". Interestingly , from this list, we can also infer that the two ma jor common themes in this corp ora app ear to be sports and terrorism since the best matc hing words seem to fall into these two categories. This is an additional bene t of our word asso ciation mining algorithm.
 Table 2: The 10 Chinese characters most correlated with \swimming" in English
In Table 2, we sho w the top 10 Chinese characters asso-ciated with the English word \swimming" along with their meanings in English. We see that almost all the top 10 characters are all closely related to the swimming activities. Naturally , as we go down the ranking list, the qualit y of matc hing gradually decreases.
From the obtained word correlations, we select the pairs with a correlation larger than 0.6, and use these relativ ely more reliable correlations to matc h documen ts between the two languages. To evaluate the results quan titativ ely, we select a sample of represen tativ e topics from English by per-forming word clustering using the simple mixture mo del presen ted in [20]. We generated 30 clusters in this way. We then tak e the top 5 English documen ts from the three randomly chosen clusters, to generate 15 seed English doc-umen ts. For eac h English documen t, we use the baseline metho d ExpCorr to score all the Chinese documen ts of the same day, the previous day, and the day after, and retriev e top 20 Chinese documen ts for eac h English documen t. We read these documen ts and judge whether they are about the same topic/theme as the English seed documen t. The pairs judged as covering the same topic are assumed to be correct mappings. The seed English documen ts and the retriev ed Chinese documen ts are com bined together to form a working set of documen ts in both languages, whic h con tains 15 En-glish documen ts and 239 Chinese documen ts . We then use all the four metho ds to compute the matc hing scores of all the seed English documen ts and all the Chinese documen ts in the working set, and tak e the 20 top-rank ed Chinese doc-umen ts for eac h English seed documen t for evaluation. This way, we can compare the performances of these four meth-ods with a con trolled sample of the top-rank ed pairs from the whole corp ora. Figure 3: Alignmen t results on the working set.
 Table 3: Precision at ranks on the working set.

In Figure 3 and Table 3, we sho w the precisions at di eren t ranks on the working set for the four metho ds. We see that the baseline metho d ExpCorr performs the worst, while BM25Corr performs the best. A comparison between Ex-pCorr and IDFCorr sho ws that the incorp oration of IDF weigh ting signi can tly impro ves the performance. A com-parison between IDFCorr and BM25Corr indicates that the sublinear normalization of TF using BM25 helps fur-ther impro ve the fron t end precision. The translation mo del metho d CorrT rans performs better than the baseline but substan tially worse than both IDFCorr and BM25Corr . We note that both IDFCorr and BM25Corr sho w roughly a monotonically decreasing curv e, suggesting that the mea-sures capture the seman tic correlation between documen ts and a high score generally indicates a more accurate matc h-ing. However, ExpCorr and CorrT rans both sho w sligh tly impro ved precision at the tail, suggesting that the measures are not quite accurate. Indeed, for ExpCorr , quite a few top rank ed pairs are non-relev ant. The precisions of both IDFCorr and BM25Corr are as high as 0.8 even at a cut-o of 100, meaning that among the top 100 pairs, 80% of them are correct matc hings. Figure 4: Alignmen t results on the augmen ted set. Table 4: Precisions at ranks on the augmen ted set
In order to understand how much bias the working set migh t introduce, for eac h English seed documen t, we fur-ther rank all the Chinese documen ts from the same day as the English seed documen t, the day before, and the day after. This time, we tak e the top 50 Chinese documen ts for eac h English seed documen t and pool them together for evaluation. The results are sho wn in Figure 4 and Table 4. Note that because we have not judged all these Chinese doc-umen ts, we assumed any matc hing with an unjudged docu-men t to be incorrect. This means that the performance we see actually represen ts a lower bound; the real performance can only be better.
 Comparing Table 3 and Table 4 indicates that the baseline ExpCorr performs similarly , indicating that the additional 30 Chinese documen ts retriev ed mostly have not made to the top pairs. The sligh t decrease in the precision at rank 50 and rank 100 suggests that there may be a couple un-judged Chinese documen ts sho wing up in the top 100 list. Note that, for the baseline metho d, these 30 Chinese doc-umen ts are unjudged, thus can only decrease performance. The BM25Corr metho d also performs similarly; actually its performance on the larger set is even sligh tly better at rank 50 and 100. This suggests that the additional 30 Chi-nese documen ts retriev ed may actually con tain some correct matc hings. Note that, in this case, the additional 30 Chinese documen ts may con tain judged correct matc hings, whic h are those documen ts that are among the top 20 documen ts re-turned using the baseline metho d, but failed to mak e to the top 20 documen ts by the BM25Corr metho d. Thus giving the BM25Corr metho d an opp ortunit y to retriev e more results has help ed it to impro ve the performance sligh tly.
Both IDFCorr and CorrT rans perform worse on the larger set, indicating that they are not very robust. Indeed, the precision of CorrT rans is all zeros for all the ranks. This suggests that the metho d cannot normalize the scores for di eren t English seed documen t well; as a result, some in-correct results in the additional 30 Chinese documen ts may have turned out to dominate the top pairs.

Comparing the four metho ds on ranking the augmen ted working set, we see that both IDFCorr and BM25Corr again perform better than the other two metho ds, and BM25Corr is clearly the best metho d among the four.
In this pap er, we prop ose and explore a completely un-sup ervised cross-lingual text mining metho d that can ex-ploit comparable bilingual corp ora to perform cross-lingual information integration. Our basic idea is to exploit the fre-quency correlations of words about the same topic to rst mine word asso ciations and then mine documen t asso cia-tions. These asso ciations can be used to integrate multilin-gual text information and supp ort cross-lingual information retriev al and navigation, whic h has been becoming more and more imp ortan t due to the rapid gro wth of multilingual doc-umen ts available on the Web. Evaluation of the prop osed metho d on a 120MB Chinese-English comparable news col-lection sho ws that the prop osed metho d is e ectiv e for map-ping words and documen ts in English and Chinese.
The most imp ortan t con tribution of our work is that we have demonstrated the feasibilit y of mining word and docu-men t asso ciations from comparable corp ora without relying on any additional (man ually created) linguistic resources. To the best of our kno wledge, all previous attempts on cross-lingual information integration rely on some man ually crafted linguistics resources suc h as a bilingual dictionary or translation examples. Since our approac h does not dep end on suc h resources, it is more general and robust than the existing metho ds.

Although we have sho wn promising results with our meth-ods, our metho ds can be further impro ved in sev eral ways. First, we could use the documen t matc hing results to in-duce new alignmen ts for the whole corp ora, whic h can then be used to impro ve our computation of word correlations. The new results of word correlations can be fed bac k to help generate impro ved documen t alignmen t. This way, we have an iterativ e algorithm for mining both word asso ciations and documen t asso ciations. Second, we have treated the whole documen t as an information unit. To impro ve information integration accuracy , it may be bene cial to alignmen t doc-umen t segmen ts. For example, we can use a sliding windo w to searc h for the best matc hing segmen ts when matc hing two documen ts. Finally , it would be very interesting to ex-plore how to design a mixture mo del that can mine word asso ciations and documen t asso ciations sim ultaneously .
We thank Ric hard Sproat and Dan Roth for helpful dis-cussions. This work is supp orted in part by DOI under the con tract num ber NBCHC040176. [1] J. Allan et al. Challenges in information retriev al and [2] L. Ballesteros and W. B. Croft. Resolving ambiguit y [3] A. Berger and J. La ert y. Information retriev al as [4] T. M. Cover and J. Thomas. Elements of Information [5] M. Franz, J. S. McCarley , and S. Rouk os. Ad hoc and [6] P. Fung. A pattern matc hing metho d for nding noun [7] M. Kay and M. Rosc heisen. Text translation [8] H. Masuic hi, R. Flourno y, S. Kaufmann, and [9] J. Ponte and W. B. Croft. A language mo deling [10] R. Rapp. Iden tifying word translations in non-parallel [11] S. Rob ertson and S. Walker. Some simple e ectiv e [12] S. E. Rob ertson, S. Walker, S. Jones, [13] F. Sadat, M. Yoshik awa, and S. Uem ura. Bilingual [14] G. Salton and M. McGill. Introduction to Modern [15] K. Tanak a and H. Iwasaki. Extraction of lexical [16] J. Veronis. Parallel text pro cessing: Alignmen t and [17] J. Xu, R. Weisc hedel, and C. Nguy en. Evaluating a [18] C. Zhai and J. La ert y. A study of smo othing [19] C. Zhai and J. La ert y. Tw o-stage language mo dels [20] C. Zhai, A. Veliv elli, and B. Yu. A cross-collection
