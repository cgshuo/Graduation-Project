 The query-performance prediction task is estimating the ef-fectiveness of a search performed in response to a query when no relevance judgments are available. Although there ex-ist many effective prediction methods, these differ substan-tially in their basic principles, and rely on diverse hypotheses about the characteristics of effective retrieval. We present a novel fundamental probabilistic prediction framework. Us-ing the framework, we derive and explain various previously proposed prediction methods that might seem completely different, but turn out to share the same formal basis. The derivations provide new perspectives on several predictors (e.g., Clarity). The framework is also used to devise new pre-diction approaches that outperform the state-of-the-art.
The query-performance prediction task has attracted a lot of research attention [4]. The goal of this task is to esti-mate the effectiveness of a search performed in response to a query when there is a lack of relevance judgments. The prediction can be performed before retrieval using the query and corpus-based information [19, 16]. Post-retrieval predic-tion, on the other hand, also uses information induced from the result list of the most highly ranked documents [4].
Although there exists abundance of effective prediction methods, these often rely on substantially different prin-ciples. More specifically, various predictors are based on completely different hypotheses with respect to what results in effective retrieval. For example, some post-retrieval pre-diction methods rely on the premise that a result list that exhibits high clarity with respect to the corpus indicates effective retrieval [10, 1, 11, 5, 18]. Other prediction meth-ods assume that effective retrieval should be manifested in a result list that is robust with respect to query perturba-tions [37, 42], document perturbations [36, 41], and retrieval method perturbations [2]. Another class of prediction meth-ods is based on various analyses of retrieval scores in the result list [35, 14, 42, 30, 12, 13].

Given the large variety of prediction approaches, and the underlying hypotheses on which they are based, a few ques-tions arise. The most fundamental one is whether there is a unified formal basis (framework) that can help explain var-ious prediction methods and techniques. A related, albeit more specific, question is  X  X hat formal aspects of prediction are shared by seemingly different prediction approaches? X . The operational question that naturally emerges is whether a formal analysis of the prediction task can give rise to new (effective) prediction approaches.

To address the questions stated here, we present a novel query-performance predictio n framework that is based on fundamental probabilistic IR p rinciples. We establish the framework by starting with a basic question that has not been explicitly addressed in previous work on prediction:  X  X hat is the probability that this result list is relevant to this query? X . This question is a generalization to the document-list case of the core question of probabilistic retrieval [33, 22, 27]:  X  X hat is the probability that this document is relevant to this query? X .

The framework we present sets the first formal grounds for integrating pre-retrieval and post-retrieval prediction meth-ods. We show that these two paradigms target different, yet complementary, formal aspects of prediction. Empirical evaluation demonstrates the merits of their integration.
We use the framework to explain and derive various post-retrieval predictors that might seem at first glance to rely on completely different hypotheses and use different principles. For example, we derive (explain) the Clarity predictor [10]. This derivation provides a novel perspective about the actual property of the result list that Clarity quantifies. Further-more, we use the framework to show that some predictors that are based on the result-list robustness notion [37, 36, 41, 42] and on analysis of the retrieval-scores distribution [14], implicitly share the same formal basis for prediction.
Our framework also provides a formal ground to using, for prediction, measures of query-independent properties of the result list (e.g., cohesion and dispersion); and, to integrating these with query-dependent measures that are the focus of most previously proposed predictors.

The proposed framework not only provides new insight into existing predictors, it also gives rise to novel predic-tion approaches. Some of these outperform state-of-the-art methods, as shown by experiments conducted with a large variety of TREC corpora (including ClueWeb).

To summarize, our main contributions are twofold. The first, and perhaps most important, is on the formal side. The prediction framework that we present sets unifying formal grounds for a variety of prediction methods that might seem to be completely different. The formal analysis also results in novel insights about existing, and commonly used, pre-diction methods and the connections between them. The second contribution is using the framework to devise new prediction approaches that outperform state-of-the-art.
Pre-retrieval query-performance predictors [19, 29, 25, 16, 39, 28] analyze the query expression, often using corpus-based information. Post-retrieval predictors also use in-formation induced from the result list of the most highly ranked documents [10, 1, 35, 37, 11, 5, 36, 2, 14, 42, 18, 30, 31, 7, 12, 13]. As noted above, the framework we present sets formal probabilistic grounds to the integration of pre-retrieval and post-retrieval prediction. We empirically show that the integration yields prediction quality that transcends the state-of-the-art. Furthermore, we show that the frame-work provides a unified formal basis that can be used to explain (derive), and provide new perspectives for, many previously proposed post-retrieval predictors.

A recently proposed framework [21] explains several post-retrieval predictors. The idea is that an effective result list is that which is similar to some pseudo effective list and dis-similar to a pseudo ineffective list. Our framework is shown to provide a formal probabilistic basis for this framework. Furthermore, our framework a ccounts for prediction aspects not accounted for by this framework [21]. (See Section 3.1.3 for a discussion.) Our framework also provides novel views of predictors such as Clarity [10] and WIG [42], which sub-stantially depart from those previously proposed [21].
A conceptual framework for predicting query difficulty [5] is based on measuring similarities between the result list, the query, and the corpus. Our framework provides formal grounds to several aspects of this framework; and, helps ex-plain several prediction methods and techniques that do not naturally fit in this framework; e.g, those using reference document lists [37, 36, 41, 14, 42].

A prediction framework [31], based on statistical deci-sion theory, uses multiple re-rankings of the result list and their estimated effectiveness. We show that our framework provides probabilistic grounds for the underlying prediction principle of this framework. Furthermore, we use our frame-work to explain predictors that cannot be explained in terms of this approach [31]; e.g., Clarity [10] and WIG [42]. In addition, the integration of pre-retrieval and post-retrieval prediction that emerges in our framework was not addressed.
Notions of result list cohesion (dispersion) [37, 36] were ar-gued to be correlated to some extent with retrieval effective-ness. We show that the formal prediction aspect targeted by measures of this query-independent result list property is complementary to that targeted by measures of query-dependent properties; the latter are the focus of most post-retrieval predictors. We also demonstrate the empirical mer-its of the integration of these two types of measures.
Integrating predictors using a linear interpolation of pre-diction values was employed with either pre-retrieval pre-dictors [15] or post-retrieval predictors [36, 14, 42, 31]. In contrast, our framework provides formal grounds to the in-tegration of different types of predictors which target differ-ent formal aspects of prediction; namely, pre-retrieval and post-retrieval prediction, and prediction based on query-independent and query-dependent measures of result list properties. Prediction integration as that proposed in previ-ous work can be used in our framework (e.g., for improving pre-retrieval prediction quality) to potentially further im-prove overall prediction quality. We leave the exploration of this direction for future work.
The query-performance prediction task is stated as esti-mating the effectiveness of a ranking induced by retrieval method M over a corpus of documents D in response to query q in lack of relevance judgments [4].

Our first key observation is that the goal of prediction can be stated, in probabilistic terms, as answering the question: What is the probability that this result list ( D res ), of the most highly ranked documents, is relevant to this query ( q )?
We focus on the result list and its ranking, rather than address the entire corpus ranking, as this is also the focus of the most commonly used evaluation measures (e.g., average precision, precision at top ranks, and NDCG). Furthermore, the question just stated is a generalization to the document list case of the question posed by Sparck Jones et al. [33] with respect to a single document:  X  X hat is the probability that this document is relevant to this query X ? This question directly connects to the probability ranking principle [26] that states that maximal retrieval effectiveness is attained if documents are ranked by their relevance probabilities. The question serves as the basis for the probabilistic retrieval approach [33] and was used for a parallel derivation of the language modeling approach [22, 27].
In what follows we use the question stated above as the basis for developing our prediction framework. Let r be the event of relevance. The query-performance prediction task is, then, estimating p ( r | q, D res ), which can be written as
The probability p ( D res | q ) does not depend on relevance, but rather on the likelihood of retrieving D res in response to q by the given retrieval method M . As a case in point, if document-query surface-level similarities are used for re-trieval, it is likely that D res is composed of documents con-taining many occurrences of query terms. Furthermore, an estimate for p ( D res | q ) can serve as a normalization factor to ensure the compatibility of prediction values across retrieval methods. However, as the evaluation of prediction quality in prior work was based on fixing the retrieval method (and varying the queries) [4], such normalization was not called for. Accordingly, the expression from Equation 1 reflects the core prediction task. It turns out, as we show below, that P base ( D res ; q ) is the basis for numerous query-performance prediction methods that might seem at first glance to be completely different and/or based on different hypotheses with regard to the characteristics of effective retrieval. We further show how new prediction approaches can be devised based on P base ( D res ; q ). Finally, we note that if D res contains a single document, then the prediction task modeled in Equation 2 is based on estimating the likelihood of the document given the query. This is the basis of the probabilistic approach to retrieval [33].
Estimating the prior probability of relevance to q in Equa-tion 2, p ( r | q ), is (in spirit) the goal of pre-retrieval prediction methods that operate prior to retrieval time [19, 16]. Indeed, pre-retrieval predictors quantify this query difficulty notion using information induced only from q and the corpus.
The probability p ( D res | q, r ) is the likelihood of the result list D res given that a relevance event happens for q ; i.e., the probability that D res is the list that provides information pertaining to q . This is the list-based generalization of the document likelihood principle used in probabilistic retrieval [33]. Estimating the list likelihood is the implicit goal of post-retrieval predictors.

Hence, our second key observation is that while by design, post-retrieval predictors use result-list-based information in addition to the information used by pre-retrieval predictors (the query and the corpus), the two types of predictors tar-get different formal aspects of prediction; that is, the prior probability of relevance to a query (pre-retrieval) and the likelihood of a result list given relevance to the query (post-retrieval). To the best of our knowledge, Equation 1, and consequently Equation 2, set the first formal grounds to integrating pre-retrieval and post-retrieval prediction. We demonstrate the empirical merits of the integration in Sec-tion 4. In what follows we focus on p ( D res | q, r ), i.e., post-retrieval prediction.
Our third key observation is that p ( D res | q, r )canbees-timated by using documents in D res as D res  X  X  proxies. We use  X  p (  X  ) to denote an estimate for p (  X  ). Let p ( d probability that d is the document relevant to q ; i.e., d  X  X  likelihood [33]. If we set  X  p ( d | q, r ) def =0for d a document not in the result list is not considered relevant; and, probability distribution over D res , and more generally, over the corpus D , then we can define the following: 1
Assuming that  X  p ( r | d i ) (the prior probability for d evance),  X  p ( d i ) (the prior probability for d i ), and  X  p (
As is standard in work on mixture models, Equation 3 is based on an independence assumption; specifically, D res is independent of q given d i and r ; i.e., we  X  X ack off X  from to its proxies ( d i ). (the prior probability for the result list) are uniformly dis-tributed, and since  X  p ( D res | d i ,r )=  X  p ( d i |D res Equation 3 then yields a generic post-retrieval predictor: P
Devising an estimate for the probability that D res is rele-vant regardless of a specific query,  X  p ( r |D res ), is the implicit goal of several post-retrieval prediction methods. For exam-ple, some predictors are based on the premise that result list cohesion indicates effective retrieval, wherein cohesion can be quantified by the list radius [5] and its clustering tendency [36] 2 . Conversely, a diversified result list might be considered as more likely to cover different aspects of the underlying information need, and thereby assumed to be relevant [5]. Hence, in Section 4 we empirically contrast the two hypotheses of list cohesion versus list dispersion as indicators for list relevance.

The summation in Equation 4 is of the weighted document the strength of its  X  X ssociation X  with D res given that the latter is relevant ( X  p ( d i |D res ,r )). For example,  X  p ( d can be based on the probability that d i is generated from a language model induced from D res .Wenextshowhow Equation 4 can be used to explain additional predictors. Explaining WIG. Let the document-list association strength in Equation 4,  X  p ( d i |D res ,r ), be a uniform distribution over D res ; i.e., each document in D res is considered as its equi-important representative. Then, the average document like-lihood score in D res , 1 k number of documents in D res , is used for prediction. This is perhaps the most direct manifestation of the probability ranking principle; that is, a result list with documents for which the probability for relevance is high is considered ef-fective (relevant). 3 Furthermore, this prediction principle is the basis of the weighted information gain (WIG) predic-tor [42]. WIG measures the difference between the average retrieval score in the list and that of the corpus. The di-vergence from the corpus retr ieval score serves to ensure inter-query compatibility of prediction values.
 Explaining Clarity. As mentioned above, list-cohesion mea-sures (e.g., radius) were suggested in prior work as perfor-mance predictors [5, 36]. These implicitly serve for the esti-mate  X  p ( r |D res ), of the result list relevance prior, in Equation 4. An alternative approach to measuring cohesion is using the corpus. That is, if a model of the result list is distant from that of the corpus, which can be viewed as a pseudo non-relevant document, then the list is considered focused; hence, the retrieval is presumed to be effective. This is the hypothesis underlying the Clarity prediction method [10].
However, the method used to compute Clarity explicitly depends on the query [10]. Thus, Clarity, in implementation , cannot be considered as an estimate for p ( r |D res ). Rather, Clarity turns out to be an estimate for the summation in Equation 4 as we show below.
In implementation, the query is used when computing query-based inter-document similarities [36]. We come back to this point later on.
In the probabilistic retrieval framework [33], for example, ranking is based on document likelihood values.
In Appendix A we show that the standard approach of computing Clarity can be (re-)written as: Sim ( d i ,X ), where X is either D res or D , is the similarity (based on the cross entropy measure) between a language model induced from d i and that induced from X .

Hence, Clarity is a special case of the generic predic-tor from Equation 4 with a uniform  X  p ( r |D res ). That is, Clarity is a weighted sum of document likelihood values, which in the language modeling case represent normalized query-likelihood values. (See Appendix A for details.) The document-list association strength ( X  p ( d i |D res ,r )) is estimated in the Clarity case, as is shown in Equation 5, using the corpus-regularized similarity between the document and the result list; the corpus-based regularization downplays the effect of general non-query-related aspects when computing similarity to the list.
 Thus, we attained the following novel perspective about Clarity. Suppose that the retrieval at hand, whose effec-tiveness we want to predict, uses the language-model-based surface-level similarity between documents and the query. This is the case for the query likelihood [32] and KL retrieval [23] methods that are commonly used in work on using Clar-ity for prediction. Hence, the induced ranking is essentially based on the document likelihood scores defined in Appendix A (i.e., normalized query-likelihood scores). Then, by Equa-tion 5, a result list for which the highest ranked documents are similar to the entire list  X  wherein similarity is corpus regularized  X  is presumed to be effective (relevant).
Equation 3 served as the basis for deriving the generic post-retrieval prediction approach presented in Equation 4. The underlying idea was to use documents in D res as its proxies for estimating the result list likelihood, p ( D res An alternative type of a proxy for D res is a reference docu-ment list [37, 41, 14, 42, 31]. Formally, a similar formulation to that in Equation 3 that uses a set S ref of reference doc-ument lists (denoted D ref )is:
P Thus, the relevance of D res is estimated based on its associ-ation with the reference lists,  X  p ( D res |D ref ,r ), where weighted by its presumed relevance,  X  p ( D ref | q, r ). As we dis-cuss below, Equation 6 sets the probabilistic formal grounds to quite a few predictors. These predictors differ by the choice of reference lists and inter-list association measure. mate  X  p ( D ref | q, r ) is a prediction problem in its own right, which is not addressed by most predictors that utilize refer-ence lists [37, 41, 2, 14, 42]. For example, the query feedback (QF) predictor [42] uses the overlap at top ranks between D res and a (single) list D ref for  X  p ( D res |D ref ,r ); trieved from the corpus using a (relevance) language model induced from D res . However, the presumed relevance of D ref , X  p ( D ref | q, r ), is not accounted for. Thus, it is not sur-prising that QF was found to be a highly effective predictor for the effectiveness of the reference result list ( D ref not only to that of the original result list ( D res ) [31]. In-deed, this finding can be formally explained using Equation 6. That is, the effectiveness (relevance) of the reference list D ref can be estimated using D res as its proxy, by switching their roles in Equation 6. (The inter-list association esti-mate, overlap at top ranks, is symmetric.)
Autocorrelation [14] is another example of a predictor that does not account for the presumed relevance of a single ref-erence list used for prediction. The reference list, is obtained by re-ranking D res using score regularization; and, Pearson correlation between retrieval scores serves for  X  p (
D res |D ref ,r ). In Section 4 we show that the autocorre-lation predictor does not only predict the effectiveness of D res , as originally reported, but also the effectiveness of D ref . This finding provides further support to the predic-tion principle presented in Equation 6.

Other predictors that use reference lists without account-ing for their presumed relevance include those using query perturbations [37], document perturbations [41], and retrieval method perturbations [2, 14], to induce reference lists. predictor [31] does estimate the presumed relevance of the reference lists used. Specifically, each reference list is created by re-ranking D res using a relevance language model. The presumed relevance of the reference list is estimated using previously proposed predictors. Thus, while UEF is based on statistical decision theory, it can be directly explained by Equation 6.

A prediction framework recently proposed [21] relies on the premise that the result list D res is relevant to the extent that it is similar to a pseudo relevant result list and dissimi-lar to a pseudo non-relevant result list. The framework was used to explain several post-retrieval predictors. In contrast to the framework we present in this work, this framework does not arise from a probabilistic analysis of the predic-tion task. Moreover, it does not account for several formal aspects of prediction that emerged in the development of our framework. These include integrating pre-retrieval and post-retrieval prediction and integrating query dependent and independent measures of result list properties. Yet, in Appendix B we show that Equation 1, which served as the basis for deriving our framework, can be used to provide formal probabilistic grounds to this framework [21].
We present an empirical exploration of three formal as-pects that emerged in the development of our prediction framework. These give rise to new prediction methods and shed new light on existing ones.

We study the merits of the integration of pre-retrieval and post-retrieval prediction in Section 4.2.1. In Section 4.2.2 we explore the use of measures of query-independent properties  X  specifically, cohesion and dispersion  X  of the result list for prediction. Finally, in Section 4.2.3 we focus on using reference lists for prediction. We begin by describing the experimental setup we used for evaluation in Section 4.1.
Table 1 presents the eight TREC-based experimental set-tings used for evaluation. TREC5 and ROBUST are com-posed primarily of newswire documents. WT10G is a small Web collection that contains some pages of low quality (e.g., spam). GOV2 is a much larger Web collection that is a crawl of the .GOV domain and hence contains mainly well edited pages. We also use the ClueWeb corpus (category B) [6], which is a large scale noisy Web collection, with the queries used in TREC 2009 (Clue09) and TREC 2010 (Clue10).
Retrieval effectiveness for ClueWeb can be significantly influenced by spam documents [8]. Thus, in what follows we explore the potential effects of spam on query-performance prediction. Specifically, we also perform evaluation where documents  X  X uspected X  X s spam that are initially highly ranked are removed from the result list. To that end, we scan the initial ranking from top to bottom and remove documents that are assigned by Waterloo X  X  spam classifier [8] a score below 50 [8, 3], until 1000 documents are accumulated. The score reflects the presumed percentage of documents in the ClueWeb English collection (category A) that are  X  X pam-mier X  than the document at hand. Clue09+SpamRm and Clue10+SpamRm denote the resulting (ranked) corpora for Clue09 and Clue10, respectively.
 The titles of TREC topics serve as queries. We applied Porter stemming and stopword removal, using the INQUERY list, to documents and queries using the Lemur/Indri toolkit
To measure prediction quality, we follow the standard practice in work on query-performance prediction [4]. Specif-ically, we report the Pearson correlation between the predic-tion values assigned by a predictor to the set of queries per setting, and the gro und-truth average pr ecision (AP@1000) values for these queries; the ground truth is determined based on TREC X  X  relevance judgments. 5 Statistically sig-nificant differences of prediction quality (between Pearson correlations) are determined at the 95% confidence level [34].
As in many previous reports of work on predicting query performance [10, 11, 41, 14, 16, 30, 17, 31], we use the query likelihood (QL) model [32] for the retrieval method. The goal of the predictors we study is to estimate the retrieval effectiveness of this standard language-model-based retrieval approach. The QL retrieval score assigned to document d in response to query q is Score QL ( q ; d ) def =log where p ( w | d ) is the probability assigned to term w by a lan-guage model induced from d ; q i is a query term. Unless oth-erwise stated, we use Dirichlet-smoothed unigram language models with the smoothing parameter set to 1000 [38]. feedback (QF) [42], and weighted information gain (WIG) www.lemurproject.org
Prediction-quality patterns similar to those we report are observed if Kendall X  X - X  is used for evaluation [4]. Actual numbers are omitted to avoid cluttering the presentation. [42] predictors in the following. These methods represent the three classes of post-retrieval prediction approaches men-tioned in Section 1; these include, measuring the clarity of the result list with respect to the corpus (Clarity), quanti-fying the robustness of the result list (QF), and analyzing properties of retrieval scores in the result list (WIG). WIG and QF were shown to yield state-of-the-art prediction qual-ity [42, 40, 31]. In Section 4.2.3 we address the autocorrela-tion predictor [14], which uses regularized retrieval scores.
The predictors just mentioned, as other previously pro-posed post-retrieval predictors, do not provide direct esti-mates for the likelihood of the result list. (Refer back to Equation 2 and the accompanying discussion. 6 )Rather,the assigned prediction values are correlated with this likelihood as implied by the attained prediction quality. We hasten to point out, however, that the purpose of using these predic-tors in the evaluation to follow is exploring aspects and prin-ciples that emerged in the development of our framework; specifically, the integration of different types of prediction approaches, namely, pre-retrieval and post-retrieval meth-ods and query-independent and query-dependent measures of properties of the result list; and, the use of reference lists for prediction. Devising novel predictors that provide direct estimates for the result list likelihood, and using them in our framework, is left to future work.

The post-retrieval predictors analyze the result list D res of the k documents that are the highest ranked by the QL re-trieval method. The Clarity predictor [10] measures the KL divergence between a relevance language model R D res in-duced from D res and a language model induced from the cor-pus. (See Appendix A for details.) The QF prediction value [42] is the number of documents that are both among the  X 
QF highest ranked by the QL method and among the  X  QF highest ranked by retrieval performed over the corpus using the relevance model R D res ;  X  QF is a free parameter. The prediction value of WIG is 1  X  | Score QL ( q ; D )), where | q | is the number of terms in q ; i.e., the (query-length normalized) difference between the aver-age retrieval score in D res and that of the corpus 7 .
One of the goals of the exploration we present in Section 4.2 is to study whether formal aspects of prediction that emerged in the framework described above can be used to devise prediction methods that outperform state-of-the-art predictors. Hence, we use highly optimized versions of Clar-ity, QF, and WIG as baselines. Specifically, we set the free parameters of each of these predictors to values maximizing prediction quality per setting. All three predictors rely on k , the number of documents in the result list; QF also depends on  X  QF . For all settings, except for those for ClueWeb, the value of k is selected from { 5 , 10 , 50 , 100 , 150 , 200 , 300 , 500 , 700 , 1000 , 2000 , 3000 , 4000 , 5000 } .ForClueWebweusea slightly more moderate range of values to alleviate the com-putational effort: k  X  X  5 , 10 , 50 , 100 , 150 , 200 , 300 , 500 , 700 ,
There is work on estimating average precision directly from score distributions [12]. The proposed estimates are still surrogates for estimates for the result list likelihood.
The corpus D is represented by the unsmoothed language model induced from the concatenation of the documents it contains; concatenation order has no effect, as we use uni-gram language models. While WIG X  X  original implementa-tion was with the Markov Random Field model [42], it was noted [42, 40] and shown [31] that WIG is highly effective for predicting the effectiveness of the query likelihood model. 1000 } . For all settings,  X  QF , the additional free parameter of 1000 } . Following previous recommendations [31], the num-ber of terms used by the relevance model, R D res ,whichis utilized by Clarity and QF, is set to 100; and, language models of documents from which the relevance model is con-structed are not smoothed (i.e., a maximum likelihood esti-mate is used).
Using Equation 2 we showed that pre-retrieval and post-retrieval prediction methods target different, yet comple-mentary, formal aspects of prediction. Hence, we now turn to study whether their integration can yield prediction qual-ity that transcends that of using each alone. To the best of our knowledge, this is the first such empirical study. For post-retrieval predictors we use the optimized Clarity, QF and WIG methods described above. For pre-retrieval prediction we use two sets of methods, each based on a dif-ferent type of statistics computed for a query term. The first set, referred to as IDF [10, 19, 16], uses the inverse document frequency (IDF) value of a query term. The second set of pre-retrieval predictors, denoted VarTF.IDF [39], uses the variance of the TF.IDF value of a query term in documents across the corpus in which it appears. Predictors based on VarTF.IDF were shown to substantially outperform other pre-retrieval predictors [16]; and, to outperform highly ef-fective post-retrieval predictors for Clue09 [17].
To aggregate the statistics for the query terms, we use the sum, average, and maximu m operators. Accordingly, we study the SumIDF, AvgIDF, MaxIDF, SumVarTF.IDF, AvgVarTF.IDF, and MaxVarTF.IDF predictors. If X is a pre-retrieval predictor and Y is a post-retrieval predictor, we use X  X  Y to denote their integration, attained by the multiplication of prediction values following Equation 2.
In what follows we focus on the question of whether the prediction quality of a highly optimized post-retrieval pre-dictor can be improved if integrated, as is , with pre-retrieval predictors. Our experiments show that the prediction qual-ity of the integration of pre-retrieval and post-retrieval pre-diction can be further improved, as expected, by optimizing the free-parameter values of the post-retrieval predictors to that end. (The pre-retrieval predictors do not incorporate free parameters.) Actual numbers are omitted due to space considerations and as they convey no additional insight.
Our first observation based on Table 2, which is in line with previous reports [4], is that all post-retrieval predictors, when used alone, almost always outperform all pre-retrieval predictors, when used alone, for TREC5, ROBUST, WT10G and GOV2. For the ClueWeb settings, the pre-retrieval pre-dictors can in some cases outperform the post-retrieval pre-dictors. This finding generalizes that presented in a recent report for (only) the Clue09 setting [17].

Evidently, spam removal for ClueWeb can have consider-able (positive or negative) effect on prediction quality. We see in Table 2 that in most cases the prediction quality of the predictors for Clue09+SpamRm is lower than that for Clue09, while the prediction quality for Clue10+SpamRm is superior to that for Clue10; the prediction quality differences can be quite substantial.
 Perhaps the most important observation with regard to Table 2 is the following. In quite a few cases, the integra-tion of pre-retrieval and post-retrieval prediction can yield prediction quality that much transcends that of each, even in cases where the former is far less effective than the latter; the improvements over using post-retrieval prediction alone are often statistically significant for the ClueWeb settings. For example, QF is the most effective among the three post-retrieval predictors, when these are used alone, for 6 out of the 8 experimental settings. Yet, the best prediction qual-ity reported in the QF block in the table (boldfaced num-bers), and more generally, for an entire setting (underlined numbers), is always attained when integrating QF with a pre-retrieval predictor. Furthermore, the prediction qual-ity of the integration in these cases is often much better than that of QF, although QF X  X  prediction quality is often substantially better than that of the pre-retrieval predictor with which it is integrated. Specifically, MaxIDF  X  QF and SumVarTF.IDF  X  QF turn out to be highly effective predic-tors. For Clarity and WIG, the best prediction quality is at-tained when integrating them with a pre-retrieval predictor in 7 and 4 out of the 8 experimental settings, respectively.
All in all, these findings attest to the complementary na-ture of pre-retrieval and post-retrieval prediction that for-mally emerged in Equation 2.
The generic post-retrieval predictor in Equation 4 uses a query-independent estimate for the effectiveness (relevance) of the result list ( X  p ( r |D res )). This estimate plays a comple-mentary formal role to that addressed by the query-dependent estimates used in most post-retrieval prediction methods. Thus, following Equation 4, we turn to study the potential merits of integrating the measures of query-independent and query-dependent properties of the result list.

We contrast two hypotheses. The first is that high co-hesion of the result list attests to reduced query drift [36], and hence, to improved retrieval. The second is that list dispersion might indicate increased cover of query aspects, and thereby imply effective retrieval [5]. We employ two measures that quantify list cohesion and dispersion: the di-ameter of the list, adapted in spirit from [5], and the list entropy, a variant of a measure proposed in [20].
 List diameter. We define the result list ( D res )diameteras Sim ( d i ,d j ) def =  X  KL divergence between the (unsmoothed) language model in-duced from d i and the (smoothed 8 ) language model induced from d j . Increased KL divergence corresponds to a larger difference between language models (i.e., weaker inter-document similarity); accordingly, D res is of greater diameter and con-sequently assumed to be less coherent (more dispersed). We use d-cohesion and d-dispersion to refer to 1 dim ( D dim ( D res ), respectively, that reflect the presumed extent of cohesion and dispersion, respectively.
 List entropy. Let Cent ( D res ) be the (arithmetic) centroid of language models of documents in D res : p ( w | Cent (
P to term w by an unsmoothed language model induced from d . We  X  X lip X  Cent ( D res ) by using the 100 terms w to which it assigns the highest probability, and re-normalize to yield a probability distribution. (As noted in Section 4.1, the same clipping practice was employed to the relevance model used by the Clarity and QF predictors.)
We define the entropy of the result list D res as the entropy of its centroid: Low entropy amounts to the term distribution in D res be-ing focused around a few terms, which potentially implies increased list cohesion; conversely, high entropy might in-dicate increased list dispersion. We use e-cohesion and e-dispersion to refer to 1 H ( D
The document language model is Jelinek-Mercer smoothed [38] with a 0 . 1 weight assigned to corpus-based term counts. which are measures of D res  X  X  presumed cohesion and disper-sion, respectively.

To integrate the query-independent list cohesion (disper-sion) measures with the optimized query-dependent post-retrieval predictors (Clarity, QF and WIG), we multiply their assigned values following Equation 4. The cohesion (dispersion) measures are computed for the k = 100 high-est ranked documents. (Using the top-50 documents yields similar prediction quality.) Table 3 presents the prediction quality numbers. (  X  indicates that a list cohesion (disper-sion) measure is integrated with a predictor.)
We can see in Table 3 that in most cases, the query-independent result-list cohesion (dispersion) measures yield very low prediction quality when used alone. The main ex-ception is the e-dispersion measure, which yields somewhat higher prediction quality for the ClueWeb settings. This finding echoes those from previous work [20] about list dis-persion being a potential signal for effective retrieval in the noisy Web setting.

We also see in Table 3 that for 7 out of the 8 experi-mental settings, the best prediction quality (underlined) is attained by integrating a dispersion measure (d-dispersion or e-dispersion) with a post-retrieval predictor. For all (op-timized) post-retrieval predictors, Clarity, QF and WIG, the best prediction quality (boldfaced) is almost always ob-tained when integrated with a dispersion measure rather than when used alone. The improvements over using the post-retrieval predictor alone can be substantial (e.g., for the ClueWeb settings) and statistically significant (e.g., for e-dispersion  X  Clarity and e-dispersion  X  QF over the ClueWeb settings). In most cases, integrating a post-retrieval predic-tor with the entropy-based dispersion measure (e-dispersion) outperforms using the post-retrieval predictor alone and in-tegrating the predictor with the diameter-based dispersion measure (d-dispersion).

The findings presented above support the merits of the integration of measures of query-independent and query-dependent result list properties that formally emerged in Equation 4. Specifically, quantifying list dispersion by mea-suring its term-distribution entropy, and integrating this measure with optimized post-retrieval predictors, yields pre-diction quality that can substantially transcend that of using the predictors alone.

It is important to note that our findings do not contra-dict those in previous work [36] with regard to increased list cohesion being an indicator for effective retrieval. Specifi-cally, cohesion was shown to be an effective indicator when measured in a query-dependent manner utilizing document-query similarities [36]. In contrast, we measure cohesion using query-independent estimates.
We presented the fundamental approach of using reference lists for prediction in Equation 6: The relevance of D res to q is estimated based on its associa-tion with reference lists D ref ( X  p ( D res |D ref ,r )), where each D ref is weighted by its presumed relevance to q ( X  p ( D ref As discussed in Section 3, this prediction paradigm under-lies several predictors [37, 41, 14, 42, 31]. However, many of Table 4: Using reference lists for prediction.  X  X  X  and  X  X  X  mark statistically significant differences with Clarity and RefList(Uni), respectively. Best result in a row is boldfaced. these predictors do not estimate, and consequently utilize, the presumed relevance of D ref [37, 41, 14, 42].

To further explore the merits, or lack thereof, of the reference-lists-based prediction paradigm, when using both the asso-ciation between D res and D ref and the estimated relevance of
D ref to q , we study the following novel simple predictor, henceforth referred to as RefList .
 In the empirical evaluation presented thus far, we used the QL retrieval method to induce the corpus ranking used to create the result list D res . The document language model smoothing parameter value was set to 1000. (See Section 4.1.) To create reference lists, D ref ,we re-rank D res the QL method with the smoothing parameter set to values in { 100 , 200 , 500 , 800 , 1500 , 2000 , 3000 , 4000 , 5000 , 10000 Thus, we get that S ref contains 10 reference lists that are created by varying the document (language) models.
We set the estimate for the association strength of D res and D ref ( X  p ( D res |D ref ,r )) to Pearson X  X  correlation between the retrieval scores of the documents. The estimate for the presumed relevance of D ref to q ( X  p ( D ref | q, r )) is set to ei-ther the Clarity value of D ref ( RefList(Clarity) )orto 1 ( RefList(Uni) ). We employ the predictors upon a result list, D res ,of k = 150 documents. The relevance model used by Clarity is constructed as described in Section 4.1. As a reference comparison, we use the optimized Clarity predictor used above and for which the value of k was tuned. Thus, the RefList predictors are underoptimized with respect to the Clarity baseline. We hasten to point out that RefList can also be optimized with respect to k and predictors other than Clarity can be used by RefList to estimate the rele-vance of D ref . Nevertheless, our goal here is to focus on the underlying principles of using reference lists for predic-tion rather than devise the most effective predictor. The prediction quality numbers are presented in Table 4.
We see in Table 4 that RefList(Clarity) improves  X  of-ten substantially, although statistically significantly in a sin-gle case  X  over RefList(Uni) for all non-ClueWeb settings (TREC5, ROBUST, WT10G, and GOV2). This finding pro-vides some support to the importance of using an estimate for the presumed relevance of the reference list. We also see that RefList(Clarity) substantially outperforms Clarity for 3 out of these 4 settings, and statistically significantly so for ROBUST, although RefList(Clarity) is underoptimized with respect to Clarity. These results support the merits of using reference lists for prediction, even if the reference lists are created using a very simple approach as we employ here.
Table 4 shows that for the ClueWeb settings, the RefList predictors and the optimized Clarity predictor yield very low prediction quality. This could be attributed to the noisy na-predicting the effectiveness of the result list at hand (
D , as originally proposed) and that of the reference ture of the corpus-based term counts (e.g., due to spam) that affects Clarity computation and the smoothing of document language models which is very important in the RefList pre-dictors. (See [20] for an additional explanation about Clar-ity X  X  low prediction quality for ClueWeb.) We found that using Clarity (alone) with  X  X erm clipping X  [18] yields a some-what improved prediction for Clue09 and Clue09+SpamRm but not for Clue10 and Clue10+SpamRm.
 Section 3.1.3, the QF [42] and autocorrelation [14] predic-tors use a single reference lis t. However, both predictors do not use an estimate for the presumed relevance of the ref-erence list. Hence, these predictors are based solely on the association between D res and D ref ( X  p ( D res |D ref ,r )). Thus, if we swap the roles of D res and D ref in Equation 6, and given that the inter-list association measure for both predic-tors is symmetric, we should get that the predictors do not only estimate the effectiveness of D res , but also that of This hypothesis was shown to hold for QF in some previous work [31], but the ClueWeb settings were not used. Hence, we (re-)examine the hypothesis for QF; and, we present a novel study of the hypothesis for autocorrelation.
We use the optimized QF predictor described in Section 4.1. The optimization was with respect to the prediction quality for the effectiveness of D res , which is computed (as isthecasefor D ref ) as at the above; that is, by Pearson correlation with the true AP@1000. We also use an opti-mized autocorrelation predictor. Specifically, the number of nearest neighbors considered when constructing the graph [14] is selected from { 5 , 10 , 50 } (a document is included in the set of its own nearest neighbors); and, k ,thenumberof documents in D res (and D ref ) is selected from { 50 , 100 optimize prediction quality for D res with respect to AP@ k . Accordingly, we report the pre diction quality of autocorre-lation for both D res and D ref with respect to AP@ k .We do not use AP@1000 since autocorrelation is based on re-ranking D res rather than ranking t he entire corpus.
We see in Table 5 that in most settings QF and autocor-relation post high prediction quality for the effectiveness of the reference list ( D ref ); often, the prediction quality tran-scends that for the original result list ( D res ). Both QF [42] and autocorrelation [14] were originally stated as targeting the effectiveness of D res and they were optimized to this end as described above. Hence, these findings  X  which are novel for autocorrelation  X  support the hypothesis stated above with regard to predictors using a single reference list with-out utilizing an estimate for its presumed relevance. That is, prediction is essentially performed for the effectiveness of
We used a language-model-based inter-document similarity measure rather than a vector-space-based measure as in the original proposal [14]. both the result list and the reference list. These findings further support the underpinning of the reference-list-based prediction approach from Equation 6.
We presented a novel probabilistic query-performance pre-diction framework. The importance of the framework is threefold. First, setting common formal grounds to various previously proposed prediction methods that might seem to rely on completely different principles and hypotheses. Sec-ond, providing new insights about commonly used predic-tion methods such as Clarity and the connections between them. Third, giving rise, based on formal arguments, to new prediction approaches that were empirically shown to im-prove over the state-of-the-art. Integrating various predic-tion types that emerged in our framework using additional approaches  X  e.g., machine-learning-based techniques  X  is a future venue we intend to explore.
 Acknowledgments We thank the anonymous reviewers, and Yuval Nardi, for their comments. This paper is based upon work supported in part by the Israel Science Foun-dation under grant no. 557/09, by IBM X  X  P h.D. fellowship and SUR award, by Google X  X  and Yahoo! X  X  faculty research awards, and by Miriam and Aaron Gutwirth Memorial Fel-lowship. Any opinions, findings and conclusions or recom-mendations expressed here are the authors X  and do not nec-essarily reflect those of the sponsors.
Clarity is defined as the KL divergence between a rele-vance language model R D res constructed from D res and a language model p (  X |D ) induced from the corpus [10]: w is a term in the vocabulary. R D res is a linear mixture of language models of documents in D res [24]: p ( w | d i ) is the probability assigned to w by a language model induced from d i ; and, p ( d i | q, r ) def = p ( q | d i normalized query-likelihood when using uniform distribu-tions for p ( r | d )and p ( d ), where d  X  X  res . 10
Using R D res  X  X  definition from Equation 8 in Equation 7, and applying arithmetic manipulation, we get that: P We define the similarity between d i and X ( D res or D )asthe minus cross entropy between their induced language models: Sim ( d i ,X ) def = p ( w | R D res ). Thus, Clarity is:
Applying log odds, which is a monotonic transformation, upon Equation 1 yields: log O ( r | q, D res ) def =log p ( r  X  r is the non-relevance event.
 Suppose we estimate p ( D res | r, q )and p ( D res |  X  r, q ) based on D res  X  X  similarity with a (pseudo) relevant and non-relevant document lists, respectively; and, use a uniform relevance prior, p ( r | q )= p (  X  r | q )= 1 2 . Then, we obtain the prediction principle underlying the framework proposed in [21].
While the common notation used in the language model-ing framework does not include an explicit relevance event indicator, here we follow Lafferty and Zhai [22] and use it.
