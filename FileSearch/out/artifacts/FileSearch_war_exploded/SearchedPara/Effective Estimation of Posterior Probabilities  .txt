
There has been increasing number of independently pro-posed randomization methods in different stages of decision tree construction to build multiple trees. Randomized de-cision tree methods have been reported to be significantly more accurate than widely-accepted single decision trees, although the training procedure of some methods incorpo-rates a surprisingly random factor and therefore opposes the generally accepted idea of employing gain functions to choose optimum features at each node and compute a sin-gle tree that fits the data. One important question that is not well understood yet is the reason behind the high ac-curacy. We provide an insight based on posterior probabil-ity estimations. We first establish the relationship between effective posterior probability estimation and effective loss reduction. We argue that randomized decision tree meth-ods effectively approximate the true probability distribution using the decision tree hypothesis space. We conduct exper-iments using both synthetic and real-world datasets under both 0-1 and cost-sensitive loss functions.
Quite different from widely accepted single decision tree algorithms, the family of randomized decision tree meth-ods introduces different methods of  X  X andomization X  into the decision tree construction process, and computes mul-tiple decision trees instead of a single decision tree. A rather complete survey and comparison of these approaches can be found in [13]. Randomization has been explored in both data selection and model induction. Data random-ization techniques include bootstrapping [4], feature subset randomization [1], data partitioning, and output perturba-tion [5]. In model induction, randomization has been ex-plored in both total random feature selection as employed in Random Decision Trees (RD) [11], and partial random feature selection in Random Forest [6]. They are many publications and independent studies to support the finding that randomized decision tree methods are highly accurate. However, the missing piece is to understand the reason why randomized decision tree methods produce highly accurate models. Our work in this paper explores the reason behind the results by studying the relative effectiveness of  X  X oste-rior probability estimation X  by traditional decision tree and randomized decision tree ensembles. We believe our find-ings not only help us understand the behavior and limitation of randomized decision tree methods but also provide some insights into how to design more accurate algorithms.
The basic procedure of decision trees and many related significant works can be found in [14]. We interpret de-cision trees as probability estimators and establish connec-tions between probability estimation and loss minimization. A feature vector x has probability P ( y | x ) to be a member of class y . By definition, P ( y | x ) is the percentage of times that x has class label y when x is sampled exhaustively; P ( y | x ) is determined by a typically unknown target func-tion of the dataset (or the function that generates the true label for each example) and is independent of any modeling techniques and training examples. A decision tree (denoted by  X  ) can be regarded as an estimator to the true probabil-ity. The estimated probability depends on both the feature vector x and the decision tree  X  , denoted by P ( y | x , X  mally, P ( y | x , X  ) = P ( y | x ) unless the true target model is the decision tree  X  itself or verified exhaustively for every x . Assume that n is the number of examples at a leaf node classifying x and n y is the number of examples among n with class label y , the estimated probability for x to be a member of class y is
Given a loss function L ( t, y ) where t is the true label and y is the predicted label, an optimal decision tree is one that minimizes the average loss L ( t, y ) for all examples, weighted by their probability. The optimal decision y  X  for x is the label that minimizes the expected loss E t ( L ( t, y for a given example x when x is sampled repeatedly and different t  X  X  may be given. If the true probability distribu-tion P ( y | x ) is given, we can use it to choose y  X  . It implies that if the estimated probability is equal to the true proba-bility for every example, i.e.,  X  x ,P ( y | x , X  )= P ( y prediction by  X  will always be the optimal decision for ev-ery example under any loss function, and no other models would have lower expected loss.

In order to evaluate  X   X  X  performance to minimize loss functions, we shall compare the difference between P ( y | x ) use their difference. However, in reality, class labels are provided in inductive learning, but true probability P ( y is not given for most problems, and it is strongly biased to assume P ( y | x )=1 from a single observation that x has class label y . Nonetheless, for a fixed loss function, ex-act values of P ( y | x ) may not be necessary to predict the optimal decision y  X  . As long as the estimated probability P ( y  X  | x , X  ) is above  X  X ome threshold X , the optimal decision y  X  will be predicted. For two-cl ass problems evaluated un-der 0-1 loss, y  X  is predicted as long as P ( y  X  | x , X  ) Similarly, for a cost-sensitive credit card fraud detection problem, assume that the amount of transaction of x is $1000, and the cost to challenge a fraud is $90. The deci-sion threshold to predict fraud is P ( fraud | x , X  )  X  1000 or P ( fraud | x , X  ) &gt; 0 . 09 . Obviously, the threshold is depen-dent on both the loss function and example.
 Definition 1 Given an example x , the decision threshold v ( y ) is the minimal probability to predict class label y ,i.e., P ( y | x , X  )  X  v ( y ) .
 Since we are only interested in predicting the optimal de-cision y  X  to minimize loss functions, we focus on the relative difference between P ( y  X  | x , X  ) and v ( y  X  ) P ( y  X  | x , X  ) &lt;v ( y  X  ) , the difference | P ( y  X  | x measures how far off one is in making the optimal predic-tion. On the other hand, when P ( y  X  | x , X  )  X  v ( y  X  ) difference is insignificant in the sense of reducing the given loss function.

One practical difficulty is that y  X  is dependent on P ( y and is not known either. However, we can assume y  X  to be thetruelabel t of x since predicting the correct class la-bel is expected to reduce any reasonable loss function. It is important to understand that assuming y  X  = t is different from, and less biased than P ( t | x )=1 . In fact, y  X  = plies that the true probability P ( t | x ) is within [ v encompasses P ( t | x )=1 . For example, assuming y  X  = t under 0-1 loss for a two-class problem is the same as as-suming P ( t | x )  X  [0 . 5 , 1] . Another example showing that P ( t | x )=1 is a strong assumption can be found in Sec-tion 5 paragraph 2.
In this paper, we consider four different approaches of randomized decision trees and their variations. The spir-its of many other approaches (reviewed in Section 7) are incorporated into these methods. One of the most  X  X an-dom X  methods is Random Decision Tree [11] or RD .RD computes an ensemble of typically 10 to 30 decision trees. During tree construction, the splitting feature at each node is chosen randomly from any  X  X emaining X  features without calling any information gain or other gain function. A cho-sen discrete feature on a particular decision path (starting from the root of the tree to the current node) cannot be cho-sen again since it is useless to test the same discrete value more than once, i.e., each split path will have the same dis-crete feature value. However, continuous features can be chosen multiple times, each time with a different randomly chosen threshold value. In order to prevent the tree from growing unnecessarily large, the tree stops growing when either a node becomes empty or the depth exceeds a prede-fined limit (e.g., the total number of features). One example to show the advantage of expanding the tree without test-ing gain functions can be found in [11]. It is an XOR-type problem such that each feature has no distinctive value by itself, but they only exhibit information gain when exam-ined collectively. When classifying an example, each tree outputs posterior probability as defined in Eq 1, then the probabilities from each tree in the ensemble are averaged as the final posterior probability estimate. The simplest imple-mentation of random decision tree is to remove the infor-mation gain check and choose an available feature (none-chosen discrete feature or continuous feature with random split value) randomly. Its name may superficially suggest total randomness, but the randomness of RD is only on the structure of each tree, and each tree is still consistent with the training data.
 When bagging is applied to decision tree (we call this BT ), each tree is computed from a bootstrap sample of the training set. In the original proposal [4], each tree X  X  classi-fication counts towards a vote o n the predicted class label. The one with the most number of votes will be the final prediction. Although the votes into different classes can be converted into probability by normalization, we use a prob-abilistic variation similar to RD, that is, each tree outputs a probability of each class and then the multiple probability outputs are averaged. We call the probabilistic variation of bagging BT+ .

Random Forest or RF [6] introduces randomness into decision tree in both data randomization and feature selec-tion. RF computes multiple trees, and each tree is con-structed from a bootstrap sample and by selecting at each node the feature with highest information gain among k randomly chosen features at the given node from the total feature set. The parameter k is provided by the user. RF performs either simple or weighted voting on the final pre-diction. As we did with bagging, we also use a probablistic variation of Random Forest called RF+ that sums posterior probabilities instead of adding votes.
 The last chosen algorithm is Disjoint Sample Trees or DST . The training set is randomly partitioned into several disjoint subsets of equal size. A tree is constructed from each partition. As for the other three methods, averaged probability is output as classfication. Different but signfi-cantly similar forms of DST have been proposed and used in many situations.

Bayesian Interpretation As compared with a single tree, Randomized decision tree methods estimate the true probability using a subset  X  of decision trees consistent with the training data. Formally, randomized decision tree approaches compute the following
P ( y | x ,  X ) =  X  is the ensemble or subset of decision trees chosen by a given randomized decision tree algorithm. P (  X  | D ) is the probability of decision tree  X  after observing training data D . Since no preference is given to any tree, each of them receives uniform posterior, 1 / |  X  | . Eq 2 is similar to deci-sion tree averaging [7], and is a specific form of Bayesian optimal classifier (BOC) [14] or Bayesian model averag-ing [12] applied to decision trees with a  X  X maller X  hypoth-esis space. Though widely used by statisticians, BOC re-ceives much less interests among data mining researchers, since it is deemed computationally prohibitive to enumerate every model. This is particularly true for decision trees. As-sume a trivial dataset with only two features and each fea-ture is bi-valued. The total number of unique trees that are consistent with the training set is as many as 8. When any feature is continuous, the number of trees consistent with a training set can be infinite. The four randomized decision tree algorithms obviously chooses a much smaller hypoth-esis space to avoid the computationally prohibitive task to enumerate every decision tree.

The discussion of the optimality of BOC (relative to sin-gle model) to estimate posterior probability can be found in both [14] and [12]. Even if the complete hypothesis space can be enumerated by an exhaustive BOC, there is still no guarantee that it will exactly produce P ( y | x ) for every For some problems, the true model that generates the labels may not be contained in the hypothesis space of decision trees. In this paper, we are interested in justifying that the specific choices of hypothesis space of randomized deci-sion tree approachesappr oximate the true probability better than single decision trees. If it is indeed true, the findings provide insights to find a possibly better subset of trees than existing approaches to approximate the true probability.
Reliability plot has been used previously to measure the performance of probability estimation [16]. As a sum-mary, we use the x -axis for the estimated probability and y -axis for the true probability. The probability estimate by a learner is good if all points are close to the y = x diagonal line. Since true probability is not given for many real-world datasets, examples with similar estimated probabilities are grouped or binned together and empirical probability (that replaces true probability) is calculated by dividing the num-ber of examples with given class label y by the number of examples in the bin. Details can be found in [16].
Due to limited number of examples, the exact shape of reliability plot depends on the chosen bin size (defined in [16]). Each curve carries no information on how many examples are in each bin. Reliability plot has no direct cor-relation to expected loss. MSE has been used previously to solve this problem [16]. Citations to earlier use of MSE can also be found in [16]. In [16], Squared error is defined as SE = y ( T ( y | x )  X  P ( y | x , X  )) 2 ,where T ( y | x ) has class y and 0 otherwise. This is equivalent to assume P ( t | x )=1 and t is the true label of x . Under this defini-tion, a model with 0 error rate could still have a high MSE. For example, we have a two-class problem { X  , + } and two examples, x 1 is + and x 2 is -. Assume that model  X  1 pre-dicts that p (+ | x 1 , X  1 )=1 . 0 and p (  X  X  x 2 , X  1 )=1 model  X  2 predict that p (+ | x 1 , X  2 )=0 . 6 and p (+ | x 0 . 6 . Obviously, if we predict the label with highest es-timated probability, both models will have 0% error rate. However, under the original definition of MSE,  X  1 will have MSE at 0, and  X  2 will have MSE of 0.32. The prob-lem comes from the strong assumption of P ( t | x )=1 . This assumption is appropriate for perfectly deterministic and noise-free problems, but is inappropriate for stochastic problems where P ( t | x ) =1 .
 In our experiments, whenever P ( y | x ) is given, we set T ( y | x ) to the true probability and MSE measures the ex-act accuracy in probability estimation. For many real-world applications, true probability is usually not known. Instead, we use MSE to measure how far the predicted probability estimate is from making the optimal decisions, as discussed in Section 3. To be specific, we use MSE to measure the difference between the estimated probability and the deci-sion threshold to predict optimal decision y  X  = t (as de-fined in Definition 1). Assume that the decision threshold for the true class label is v ( t ) . When predicted correctly, the exact value of P ( t | x , X  ) is not important. However, when |
P ( t | x , X  )  X  v ( t ) | quantifies how far the predicted probabil-ity is from making the correct decision. Intuitively, a model that is 0.1 off from v ( t ) is better than one that is 0.9 off. Based on the original definition of MSE in [16], we propose an improvement that takes all these factors into account. The predicate [[ ]] is defined as [[ a ]] = min (1 . 0 ,a the improved definition, a model that makes the correct pre-diction for x will always have MSE value of 0 regardless of the probability estimate. For a dataset of many exam-ples, unless they both make no mistakes for any examples, Figure 1. Synthetic Dataset: MSE and Error Rate Table 1. Synthetic Data Bias-Variance Decomp two models with the same error rate may not necessarily have the same MSE. For example, suppose that two mod-els X  probability estimate for every example except for one is the same, and they both make mistakes on this x .Obvi-ously, they have the same error rate. Assume that the true label of x is +. One model predict p (+ | x , X  1 )=0 . 4 and the other model predict p (+ | x , X  2 )=0 . 1 . The MSE for the first model is lower than the MSE for the second one. This is because the probability estimate is closer to mak-ing the correct classification, and it exactly quantifies how one model is closer to making the correct prediction than another one.

Log-loss or cross-entropy has been proposed previ-ously to measure probability estimate [16], defined as  X  it is well known that cross-entropy is undefined when ei-ther P ( y | x , X  )=0 or T ( y | x )=0 . Importantly, the ratio ( y | x ) has no direct relationship to the expected loss even if we choose T ( y  X  | x )= v ( y  X  ) .

Bias and Variance Decomposition For a given example x , different models will normally generate different prob-ability estimates, shorthanded as p i . When there is no class label noise, squared error can be decomposed into bias and variance using the standard bias-variance decomposi-tion method [3]. Bias quantifies the systematic error of a method, i.e., its average performance when the same algo-rithm is applied on multiple datasets, and variance quan-tifies the error due to variations from multiple training set. For squared error, the major prediction p m is the mean value of p i  X  X . Assume that t is the true probability, then bias is ( t  X  p K is the total number of training sets and models.
We use both synthetic and real-world datasets, the num-ber of class labels is either binary or multi-class, and the loss function is either 0-1 loss or cost-sensitive loss.
The advantage of synthetic datasets is that the true prob-ability distribution P ( y | x ) is formulated from the program script, and can be used to compute the  X  X xact X  MSE to com-pare results. We compute a sum from each feature multi-plied by a weight, s = a 1  X  x 1 + a 2  X  x 2 + ... + a d  X  Each feature is within [0 , + m ] , so the maximum value of s is S = m  X  ( a 1 + a 2 + ... + a d ) . The weight adjusts the impor-tance of a feature to calculate the class label. We compute a ratio from each feature vector and S by  X  = s S . The class la-bel is generated from the followin g probability distribution, P (0 | x )=  X   X  (1  X   X  ) ,P (1 | x )=  X   X   X , P (2 | x )=(1  X  and P (3 | x )=(1  X   X  )  X  (1  X   X  ) . For this dataset, it is impossible to have 0% classification error even if we know the true probability distribution. Assuming that for a par-ticular example,  X  is 0.3, and the probability distribution is P (0 | x )=0 . 21 ,P (1 | x )=0 . 09 ,P (2 | x )=0 . 21 P (3 | x )=0 . 49 . Obviously even if we know the true prob-ability distribution, the best guess is to predict class label  X 3 X . However we are still mistaken 51% of the time when x is sampled repeatedly.

In our experiments, the range of each feature value is between 0 and 5, each feature value is randomly chosen between 0 and 5, and there is no dependency among the different feature values. Three dimensions d =5 , 15 ,and 20 are explored. For each dimension, a fixed test dataset with 10000 examples is used, but four significantly differ-ent training data set sizes are tested, 100, 10000, 50000, and 100000. The weight coefficient is set as a i = i .The exact MSE and classification errors are in Figure 1, where the x -axis is the number of training examples and the y -axis is either MSE or classification error. The probability estimation by the two randomized decision tree approaches is not only consistently (with no exceptions among 24 tests) lower than single decision trees, but the MSE X  X  are also sig-nificantly lower. While the MSE X  X  of the two randomized decision trees are approximately between 0.1 and 0.3, the MSE X  X  of the two single decision tree methods are at least 0.55. The classification errors of the two randomized de-cision tree methods are also consistently and significantly lower than single decision trees. The theoretically lowest classification error even if we know the true probability dis-tribution is approximately 44%. The lowest classification error obtained by single decision trees is around 70% while the lowest error obtained by randomized methods is 65%.
Bias and Variance Decomposition In the bias and vari-ance decomposition experiment, the fixed test set has 2000 examples, and there are 100 training sets with 2000 exam-ples each. The results are summarized in Table 1. The error of the traditional single decision trees comes from high val-ues in both bias and variance. The improvements by RD and RF+ are due to reduction in both bias and variance, but the reduction in variance is more significant than in bias.
We have chosen three datasets donation (KDDCUP X 99), credit card fraud and adult datasets that were used in previ-ous studies [11, 10]. Detailed description about these three datasets and their loss functions (both 0-1 loss and cost-sensitive loss) can be found in [11]. The results on single decision tree and RD on these three datasets were reported earlier in [10]. However the other results are entirely new.
Accuracy The accuracy on the three chosen datasets is more straightforward than  X  X oss X , and is summarized in Ta-ble 2. Please note that donation and credit card datasets are cost-sensitive problems. The accuracy on these two datasets is total profits marked as ($). For the adult dataset, we use 0-1 loss function and the accuracy (marked as 0-1) is the number of correctly predicted examples in the test dataset. Clearly, all randomized approaches have either significantly higher or equivalent accuracy on all three data sets. The ad-vantage of randomized approaches are particularly obvious for (cost-sensitive) donation and credit card fraud dataset. Random approaches that choose features randomly (ran-dom forest with probabilistic output and random decision tree) have significantly higher accuracy than other random approaches that choose data subset randomly. For both the donation and credit card fraud datasets, the two methods with highest profit returns are random forest with probabil-ity output and random decision tree. Please notice that for the donation dataset, any methods that directly output class label instead of probability have a zero profit since they pre-dict everyone as non-donors.

Reliability Plots The reliability plots for single best tree and randomized decision tree methods for the dona-tion datasets are shown in Figures 2. There are 1000 bins of equal size. The top two plots are for single best unpruned decision tree. (A pruned tree has only one node predict-ing everyone is a non-donor.) Since most of the points are within the range between 0 and 0.35, we  X  X nlarged X  that area on a separate plot on the immediate right. The sin-gle best unpruned tree X  X  score scatters around the perfect matching line. However, random forest, random decision tree and disjoint sampling tree match the true probability very well. To summarize these results, we use MSE to mea-sure how closely the score matches the empirical probabil-ity. In other words, v used in Eq 3 is the empirical probabil-ity measured from the testing data. The MSE for single best unpruned tree is 0.01611 while the MSE for randomized de-cision tree methods (except for bagged decision tree) is at most 0.0124. The bagged decision tree has higher MSE than the single unpruned tree. We look at the predictions and find that this is due to the fact that the percentage of positives is around 5% and every tree trained from each bootstrap is highly correlated on the 95% negatives.

Bias and Variance Decomposition The bias and vari-ance decomposition on the donation dataset is summarized in Table 3. The original test dataset is chosen, and 100 training sets with 10000 examples each are sampled from the original training set. The true probability is the empiri-cal probability. Similar to the synthetic datasets, the reduc-tions by RD and RF+ come from both bias and variance, but mainly in variance.
Data Sets The artificial character dataset from UCI has been artificially generated by using a first order theory which describes the structure of ten capitol letters of the En-glish alphabet and a random choice theorem prover which accounts for heterogeneity in the instances. The capitol let-ters represented are the following: A, C, D, E, F, G, H, L, P, R. There are 1000 instances (100 per letter) in the training set, and 5000 instances (500 per class) in the test set.
Reliability Plots The reliability plots are per class, as shown in Figure 3. Since the dataset is relatively small considering there are 10 class labels, we have chosen to divide the range into 10 bins. A single decision tree may not predict a posterior probability in every bin since it only has a limited number of leaf nodes, and each node can output just one probability number. When we draw the curve for each method, we only use the line to connect immediately adjacent bins. If a model predicts in in bin 1, 2, 5 and 6, we connect bin 1 and 2, break between 3 and 4, and connect 5 and 6 again. In general, RD appears to be very systematic in every single class of all 10 classes. For all 10 plots, RD X  X  reliability plot is always continuous, monotonically increases and it mostly stops at scores between 0.3 and 0.4. Except for  X  X etter L X  of the artificial character dataset, RD can detect all positives with no false positives at the last bin. In contrast, the reliability plot of the single unpruned tree is obviously very un-systematic and varies significantly from class to class for both datasets. Unlike RD, the plots for most classes are not continuous. In many cases (letters A, C, E, F, G), the reliability plots are not monotonically increasing, i.e., higher predicted probability actually have a lower empirical probability measured from the data. RF+ X  X  reliability plots have characteristics from both RD and single unpruned tree. They are more systematic than the single decision tree, and there is clearly a monotonically increasing trend for all plots except for letters E and F. Between RF+ and unpruned best tree, RF+ reliability plot matches the unpruned tree in a number of cases (letter D, F, and L) Each tree of RF+ is trained from the same training set and uses the same  X  X nformation gain X  selection criteria as the single unpruned tree. There is some extent of correlation between RF+ and single unpruned tree. Comparing with RD, RF+ predicted probability ranges from 0 to 1, while RD X  X  predicted probability ranges from 0 to 0.4. This is due to the bias of the chosen hypothesis space  X  . The hypothesis space of RD are the trees whose feature at each node can be any remaining feature and the depth is limited to the number of features. Under this inductive bias, most leaf nodes are not pure, i.e., they contain a mixture of examples belonging to many different classes. However, RF+ chooses features with  X  X nformation gain X  and no depth limitation on pruning. Consequently, the leaf nodes of RF+ trees are much purer. To verify this, we include the prediction of an example from the dataset. The true class label for the example is letter R and all three models have the correct prediction. Interestingly however, all three models think that there is also some probability that this example could be letter P. Indeed, P and R do look similar. Clearly, RD X  X  prediction has none-zero probability for each of the 10 classes and the one with the highest probability is the true label. The top three classes that RD predicts to be closest to the true label are R, P and F. On the other hand, RF+ has none-zero probability for only the three letters R, P and F, which are exactly the top three possibilities predicted by RD. However, the unpruned single tree has a clean cut prediction that the choice is either P with .872 probability or R with .128 probability. Error Rate and MSE Error rate and MSE results are in Table 4. We measured predictio n error rate with three dif-ferent criteria. Top 1 means that if the true label of x is the one with the highest estimated probability, the prediction is considered correct. Top 2 means if the true class label of x is among the two class labels with the highest predicted probability, we consider x is predicted correctly. Top 3 is similar to top 2. For example, if the true class label of R and the three class labels with the highest predicted prob-abilities are F,R and P (and th eir predicted probabilities are .0606, .302 and .349 respectively), the prediction is consid-ered correct under both top 2 and top 3 criteria, but regarded as a mistake under top 1 criterion. For Table 4, RD has an error rate of 14.58% under the top 1 criterion, while RF+ X  X  error rate is 21.82%. The error rate of the two single de-cision trees are around 27.7%, which are much higher than both RD and RF+. When the criterion becomes less restric-tive, i.e., top 2 and top 3, the error rates of RD and RF+ decrease significantly and drop down to 0. However, for the two single decision trees, even after the criteria are less re-stricted, the error rate still remains a bove 7%. In particular, the error rate at top 3 is exactly the same as top 2. The sin-gle decision trees X  leaf nodes have examples of at most 2 classes and can only predict none-zero probability for two classes.

We have computed four different measurements of MSE according to Eq 3 and the results are in the bottom table of Table 4. The first row with leading column of  X 1.0 X  uses 1.0 as the decision threshold. This is the same measure used in Table 4. Error rate and MSE for the Artificial Char Table 5. Aritifical Char Bias-Variance Decomp [16]. The other rows in the MSE tables, i.e., top 1 to top 3, are the MSE measures with different decision thresholds. The decision threshold used by top 1 is the highest predicted probability. The decision threshold for top 2 and top 3 are the 2nd and 3rd highest predicted probability respectively. It is important to understand that the actual decision thresh-old chosen for each example is different, and it is different for different methods. Each example is predicted with dif-ferent probabilities and the prediction for the same example is different for different methods. The MSE measure is only meaningful to understand each method X  X  probability estima-tion towards achieving 0 error rate under the given criterion. A higher MSE means that on average the predicted proba-bilities for true labels are mor e off from the decision thresh-old for correct classification.

Bias and Variance Decomposition We used the original test set of 5000 examples and randomly sampled 100 train-ing sets with 250 examples each. Top 1 decision threshold is used as the true probability. The bias and variance de-composition results are summarized in Table 5. Similar to the bias and variance decomposition result of the synthetic dataset in Table 1, the reduction in MSE comes from both bias and variance. The reduction in variance is significant for both RD and RF+. The reduction in bias by RD is sig-nificantly more than by RF+.
Two important works in randomized decision trees that are  X  X eemingly X  not experimented within our study are Amit and Geman X  X   X  X andomized trees X  [1] as well as Di-etterich X  X   X  X andomly choose one of the top k attributes X  [9]. In their randomized trees, Amit and Geman randomly gen-erates feature subset from the complete feature set; from each feature subset, they run a conventional decision tree learner to compute the single best tree. Dietterich X  X   X  X an-domly choose one of the top k attributes X  method randomly selects one of the feature among the top k with highest in-formation gain. The spirit of both approaches is incorpo-rated in Breiman X  X  random forest [6], which is tested in our experiments. In an earlier work [2], Bauer and Ko-havi evaluated a few voting classification algorithms includ-ing bagging, boosting and arcing. The major differences in our study from this early work are as follows. Our base level decision trees output probabilities while they evalu-ated methods that output class labels. The combination method in this paper is to  X  X verage multiple probability out-puts X  rather than  X  X hoose a class label with the highest (ei-ther simple or weighted) vote. X  Probabilistic output is more flexible and easier to use under a variety of loss functions. Regarding algorithmic differences, our paper evaluated ran-dom decision tree, random forest and its probabilistic vari-ation, probabilistic variation of bagging, and disjoint sam-pling tree, which were not covered by their work. Addition-ally, our work concentrates on the ability to match the pos-terior probability, while their earlier work concentrates on minimizing 0-1 loss. In [15], the decision tree algorithm has been extended to provide reliable probability-based rank-ings of multiple classes. In [10], RD itself has been evalu-ated against the three binary datasets. In our paper, we con-sider a whole family of randomized decision tree methods not only RD, but also random forest plus, bagged probabil-ity tree and disjoint sampling tree. In addition, their various mechanisms are explained using Bayesian optimal classi-fier to justify our claim that their actual mechanism is to estimate the true posterior probability of the target function using the decision tree space. A problem with previously proposed MSE has been corrected. A significantly more ex-tensive empirical study including deterministic and stochas-tic datasets, and large multi-class problems was conducted. Our paper makes much wider and stronger claims than [10]. In [8], Chipman et al. proposed several approaches, particu-larly Monto-Carlo Markov Chain approach, to set the prior and posterior probabilities to more sophisticated random-ized approaches to construct random decision trees, i.e., randomly choosing one of the following four procedures, Grow, Prune, Change, and Swap. The major distinction is that the approaches explored in this paper employ rather simple uniform prior and posterior probability assigment as well as simple approaches to grow random trees.
We discussed both traditional decision tree and four ran-domized decision tree algorithms as estimators to true pos-terior probability. These four algorithms cover many ran-domization techniques and ideas independently proposed by several researchers, ranging from randomly-structuredto carefully-structured trees, and fro m original training set to randomized training set. We estab lished the relationship be-tween accurate posterior probability estimation and reduc-tion in expected losses. We proposed an improved definition of MSE to measure the effectiveness of probability estima-tion to minimize a given loss function. Experimental studies have found that the estimated probabilities by the four ran-domized decision tree methods appear to be very systematic and monotonically increasing, as shown in reliability plots. However, the reliability plots of single decision trees ap-pear to be sporadic and have no clear increasing trends. The estimated probability by randomized decision methods are well correlated with true probability. The improved MSE definition effectively measures a model X  X  ability to mini-mize expected loss. The MSE measurements for the same datasets are significantly lower for the randomized decision tree approaches than for single decision trees. The bias and variance decomposition shows that the reduction of MSE by randomized decision tree methods is in both bias and vari-ance, although the reduction in variance is more significant. We also find that the probability estimates of different ran-domized decision trees have different behaviors, such as the difference in predicted value range between RD and RF+.
Future Work The results in this paper have shown that minimizing error in proba bility estimation can effectively reduce both 0-1 and cost-sensitive losses. Though estimat-ing posterior probabilities directly is traditionally regarded as a more difficult problem than directly predicting class labels, our work shows that predicting reliable probabili-ties with the family of randomized decision trees appears straightforward and accurate. In the future, we are inter-ested in looking for a new method that explores a different hypothesis space than any of the existing randomized de-cision approaches, and ideally hope to estimate probability better. From the analysis of RD and RF+ in bias-variance decomposition and range of predicted values, it appears that some kind of their combination may be a good avenue to explore.

