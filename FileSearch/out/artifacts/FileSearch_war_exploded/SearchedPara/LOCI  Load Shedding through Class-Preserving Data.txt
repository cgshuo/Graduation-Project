
An avalanche of data available in the stream form is overstretching our data analyzing ability. In this paper, we propose a novel load shedding method that enables fast and accurate stream data classication. We transform input data so that its class information concentrates on a few fea-tures, and we introduce a progressive classier that makes prediction with partial input. We take advantage of stream data's temporal locality  X  for example, readings from a tem-perature sensor usually do not change dramatically over a short period of time  X  for load shedding. We rst show that temporal locality of the original data is preserved by our transform, then we utilize positive and negative knowledge about the data (which is of much smaller size than the data itself) for classication. We employ both analytical and em-pirical analysis to demonstrate the advantage of our ap-proach.
An avalanche of information is triggered by applications in a wide range of areas, including ubiquitous computing, e-commerce, sensor networks, astronomy and space explo-ration, etc. It pushes humanity into an era where the growth of data outpaces the advances in data analysis. In this pa-per, we focus on one of the most important and challenging tasks in large scale data analysis: searching for actionable insights from the data.

Albeit the efciency of data mining algorithms has been greatly improved in the recent years, we can only afford to mine a small percentage of the available data, and this per-centage keeps dropping. To make full use of the available data, we need advanced techniques that can distill useful in-formation from the data in a more efcient manner. In this paper, we address the challenge in the setting of stream data classication.
 Stream Data Classication Classication is arguably the most important tool for l-tering information and detecting events of interest. It is a well studied topic in the data mining and machine learning community. Recently, a lot of work has been devoted to classifying stream data [4, 8, 12, 2, 14].

A classication task consists of two phases. First, a model is learned from labelled (training) data. Second, the learned model is used to classify unlabelled (testing) data. Because model training usually incurs super-linear cost, a lot of work focuses on developing scalable approaches in the training phase [4, 8, 14]. Yet model training incurs a one-time cost only. Even for data with evolving class dis-tribution, the cost can be controlled by incremental learn-ing [8] or intelligent reusing of historical models [16, 15].
On the other hand, the process of applying learned mod-els on large volume, fast speed stream data for ltering and event detection becomes a major cost in stream data analy-sis. In a typical case, a large number of sensors simultane-ously send data to a central server for real time analysis, and each individual piece of raw data requires expensive pre-processing (e.g., feature extraction, data fusion, etc.) before it can be analyzed by the classication algorithm.

To handle this challenge, we introduce a novel load shed-ding method capable of dramatically reducing computation cost in classication without compromising the quality of classication. We show that our method provides good data ltering and event detection functions for the entire incom-ing data by inspecting only a small fraction of it. Load Shedding In a typical setting, a number of source nodes (sensors) S ,  X  X  X  , S m send data to a central node for real time classi-cation. Classication throughput is restricted by the com-munication bandwidth of the network and the computation power of the central node.

For each data item, we want to be able to make highly ac-curate classication by inspecting as little of its content as possible, or even without inspecting its content at all. This allows us to save much of the bandwidth in data transmis-sion, as well as most of the CPU cycles in feature extraction and other processes.

There is much work on feature extraction. For our task, we perform feature extraction with special requirements. First, the feature extraction method must preserve class sep-arability regardless of the distribution of the stream data. Second, it must support dynamic feature selection schemes so that we can allocate resource among all source nodes in order to maximize the overall accuracy. Third, the fea-ture extraction method must be able to take advantage of the temporal locality of streaming data to further reduce the input data size.

In summary, our approach explores the following oppor-tunities for load shedding on the central classier.  X  We perform classication in the transformed domain  X  We exploit active classication. In a multi-task envi-Paper Organization The rest of the paper is organized as follows. Section 2 highlights the challenges of the problem, and motivates our work with examples and intuitions. Section 3 gives an overview of our solution framework, LOCI. Section 4, 5, and 6 describe LOCI's major components: a class-preserving transform, a data acquisition scheme, and a pro-gressive classier. We show experiment results in Section 7, related work in Section 8, and we conclude in Section 9.
In order to reduce the load of classication, we must re-duce the information exchange between the source and the central node.
In many cases, we can classify an object x accurately based on incomplete information known about x . Then, we can use the saved resources to process other data. For this purpose, we introduce the concept of progressive classier .
A progressive classier is composed of a set of classi-ers, { C I for x based on partial information I 1 of x . Classier C I makes prediction based on partial information I 1 and and so on.

With a progressive classier, for a record x , the central node may take the following steps to classify x : 1. make prediction of x based on the partial information it 2. decide what additional partial information I k can best 3. based on how much condence I k can improve, decide
An important question is: what constitutes partial infor-mation of x ? Partial information of x can have many forms. For example, a feature value of x certainly represents some partial information of x . Assume x is a point in a multi-dimensional space dened by features { X 1 ,  X  X  X  , X d } . We can let I k = { x k } , that is, the k -th partial information of is the value of x on the k -th dimension.

However, a progressive classier based on such partial information may not be optimal at all. For classication, we are only interested in the  X class information X  of x , but such information may spread evenly among the features. This is demonstrated by the progressive classier in Figure 1(a), which shows knowing one more feature value adds limited amount of condence to the prediction.
An ideal progressive classier performs in the way as shown in Figure 1(b), where the rst few features contain most of the  X class information X  so that the condence of the classier reaches a high level after a small number of features are inspected. In order to have such classiers, the feature space must have the following properties: 1. The  X class information X  concentrates on the rst few 2. The features are independent of each other. It prevents
In many stream applications, data exhibits temporal lo-cality. For example, temperatures and humidity reported by a sensor usually do not change dramatically over a short pe-riod of time. This allows us to focus on changes in the data, rather than the data itself.

More specically, if a small change in a certain feature between time t and t + 1 is unlikely to result in a change of prediction, then the new data need not to be sent to the central server. Even if the change is signicant, sometimes, knowing there is such a change offers us enough informa-tion for decision making in a multi-task, load-shedding en-vironment, which means the changed data may not need to be sent either. We call it load-shedding through  X negative knowledge X .

In our approach, data is transformed before it is sent to the server. In order to take advantage of temporal locality and data transform together, we must ensure that the trans-form preserves temporal locality, thus enabling more load shedding options.

A previous work that aimed at reducing classication cost is Loadstar [3]. Loadstar also takes advantage of tem-poral locality of the data. However, Loadstar does not uti-lize feature extraction for better data representation. Be-cause of this, temporal locality and partial information can-not be exploited to its full advantage.
We introduce LOCI, a framework that enables us to clas-sify a data item based on its partial information. The rst step of classication is to learn a model from labelled data. In our framework, during the training phase, we learn not only models, but also a transform matrix, as well as a data acquisition scheme.

As illustrated by LOCI's system architecture in Figure 2, in the training phase, we carry out the following tasks:  X  We learn a data transform matrix U from the labelled  X  We learn a data acquisition scheme. The scheme takes  X  We learn a set of classiers that can progressively clas-
In the testing phase, we carry out the following tasks:  X  We apply the learned transform matrix U on the testing  X  Using the learned data acquisition scheme, the central  X  The classiers classify the data using the partial infor-
Our framework LOCI is illustrated in Figure 2. In Sec-tion 4 and 5, we describe in detail its two major compo-nents, the class-preserving transform and the data acquisi-tion scheme.
In this section, we introduce KL-3 transform and show that it satises all the desiderata for the purpose of load shedding in classication.
KL-3 transform is similar with principal component analysis (PCA). The different is that PCA aims at minimiz-ing the mean square error (MSE) while KL-3 aims at com-pressing the class separability of the original data space into a smaller number of features.

Formally, consider a set of vectors x in a d -dimensional space X , and let  X  i be the mean of x in class c i . The covariance matrix for data in class c i is We then calculate S w , the expectation covariance matrix over all classes: where K is the number of classes and p i is the probability of class c i .

Let  X  1 ,  X  X  X  ,  X  d be the eigenvalues of S w and u i is the corresponding eigenvector of the i -th eigenvalue  X  i . u sorted in a certain order which will be described later. The KL-3 transform is embodied by the transform matrix The transform is orthogonal, as it can be veried that:
Applying the transform, x in the original space X is changed into in space Y , and the covariance of the transformed dataset is
 X  y = E [( y  X   X  y )( y  X   X  y ) 0 ] = U 0 S w U = diag (  X  which means Y i and Y j ( i 6 = j ) are uncorrelated.
PCA ranks features by the value of  X  to minimize infor-mation loss. We need a different criterion, as our goal is to nd features with the most  X class information X . We rank feature Y j by: where S b is the interclass distribution matrix and  X  is the expectation of all examples from all classes. Intuitively, J ( Y j ) evaluates feature Y j 's class information, which we describe in detail in Section 4.3.
In this section, we show that the transform, together with the feature ranking criterion, satises all the desiderata we have mentioned.
 Independence By the same reasoning of Eq. 6, we can show that the features obtained by KL-3 are independent of each other.
 Class information preserving After x is transformed into y = ( y 1 ,  X  X  X  , y d ) in a new space ( Y 1 ,  X  X  X  , Y to know which feature Y j contains the most useful infor-mation for classication. We prefer a feature that can best separate data of different classes. In other words, we seek a feature that maximizes data's intra-class similarity , and minimizes its inter-class similarity .

The new criteria J ( Y j ) in Eq. 7 helps us to achieve this purpose. It has two parts: u 0 tion of  X  j , we can obtain that where y j is the value of feature Y j in data of class i , and  X  sures the intra-class similarity , that is, the more similar the data within each class, the smaller the  X  j , and the larger the J ( Y j ) .

For the part of u 0 where  X  j is Y j 's mean value of data across all classes. Thus,
S b u j measures the distance between the center of each class and the center of all data on dimension Y j . Intuitively, it measures the inter-class similarity , that is, the more dis-similar the data between different classes, the larger the S b u j , and the larger the J ( Y j ) .

Combining u 0 at nding features that maximize intra-class similarity, and minimize inter-class similarity, so the feature with largest will have the best class separability.
 Temporal locality preserving Let y = ( y 1 ,  X  X  X  , y d ) be the vector transformed from x = ( x 1 ,  X  X  X  , x d ) . According to Parseval's theorem, the KL-3 transform ensures that: Since KL-3 transform is an orthogonal transform, let y 0 be the vector transformed from x 0 , we have Eq. 12 indicates that the Euclidean distance between two points in the original feature space remains the same after they are transformed into the new space. Thus, if a record is close to x t  X  1 , then the transformed vector y t will also be close to y t  X  1 . That is, the transform maintains the temporal locality property of the original data.
We describe our data acquisition scheme built on top of the transform. We take advantage of data's temporal local-ity to reduce the amount of information exchange between source nodes and the central classier. Our scheme makes full use of partial information, including negative knowl-edge, to maximize load shedding.
Let S i be a source node, and assume at time t , it produces a value y t . We consider two situations: temporal class ag information bandwidth locality information sent obtained used y y y
In Table 1, the second column compares the  X class infor-mation X  in the boolean ag and the real y t . We use  X  y t  X  1 to represent the negative knowledge that y t 6 = y t  X  1 s the size of a real value. We can see that in the rst two cases, the server obtains almost all the  X class information X , yet uses only 1 bit of transmission, while the third case uses 1 + s bits. Our goal is to design a data acquisition scheme that mostly operates in the rst two cases.
In order to facilitate the communication between the source nodes and the central server using boolean ags , we use reference vectors and boolean vectors for our data ac-quisition scheme.
 At each source node S i , we maintain a reference vector, V i = ( v i 1 , v i 2 , ..., v id ) , where v ij (1  X  j  X  d ) KL-3 feature value of the data. Each V i has a mirror vector A boolean vector consists of a number of boolean ag s. Instead of sending one boolean ag a time, the source nodes send a vector of L boolean ags, where L is the size of a feature value. For example, if each feature value is a 8-bit integer, L is equal to 8. Thus, sending a boolean vector ( boolean ags) has the same cost as sending a value y t . Each source node maintains d d/L e boolean vectors to cover the d feature values.
We now introduce our data acquisition scheme. It op-erates as follows. First, each source node sends the rst boolean vector. After that, they wait for queries for sending more data. The central node requests a specic source node to send specic data (either a boolean vector, or a changed value) based on current classication quality and available bandwidth.

To facilitate our discussion, we rst give some deni-tions. Let ( y 1 , y 2 ,  X  X  X  , y d ) be the new KL-3 vector at source of source node S i .
 Denition 1 Next boolean vector If source node S i has already sent k boolean vec-tors, the next boolean vector, denoted as bv n ( s v Denition 2 Changed value If y k 6 = v ik , y k is called a changed value. The set of changed value is denoted as cv ( i ) . A changed value is called a sent changed value, if it has already been sent to the central node. The set of sent changed value is denoted as cv s ( i ) . Otherwise it is called un-sent changed value, and the set of un-sent changed value is denoted as cv u ( i )
After receiving some values from source node S i , central node will have some partial knowledge about the new K-L vector of S i . Formally, we dene current knowledge of S as Denition 3 Current partial knowledge Assume source node S i has already sent the rst k boolean vectors and some changed values to the central node. The server's current partial knowledge of data at S i is rep-resented by a vector of length kL , denoted as d c ( d 1 ,  X  X  X  , d kL ) , where 5.3.1 Selecting Source Nodes The central node decides to query a source node for more detailed information of a data item if i) its current classi-cation is of low quality, and ii) more information from the source node can improve the quality.

Let us rst dene the quality of classication. The qual-ity, denoted as Q ( i ) , is dened as the difference between the conditional probability of the best class and the second best class, given our current knowledge about the data item. More specically, we have where c is the class whose posterior probability is highest and  X  c is the second best class. If Q ( i ) is small, then the condence of the classier is low, and one way to improve the condence is to send more information to the classier. The central node queries the source node i whose Q ( i ) is the smallest. 5.3.2 Selecting Values After a source node S i is selected, the central node also needs to decide what information it needs. First of all, be-cause sending boolean values is cheaper, a boolean ag is always sent before its corresponding value is sent (if neces-sary).

There are | cv u ( i ) | unsent changed values and ( d d/L e X  k ) unsent boolean vectors, where k is the number of boolean vectors already sent. The central node will decide whether a new boolean vector or a certain changed value should be sent.
 The most benecial boolean vector It is easy to de-cide which boolean vector to send. Since features { Y 1 , Y 2 , ..., Y d } are sorted in descending order of class sep-arability, we select the next unsent boolean vector, which has the best class separability among the remaining boolean vectors.

We estimate the benet associated with the selected boolean vector with an assumption: the selected boolean vector is { 0 ,  X  X  X  , 0 } , that is, all corresponding features keep their current value. Thus, the benet of knowing the boolean vector can be estimated as: The most benecial changed value Intuitively, we want to choose a feature that has the highest class separability given the negative knowledge. We use conditional entropy to make the selection.

Assume Y j is the most benecial feature, its benet can be estimated as  X  c ( i ) = where c k and  X  c k are the classes whose posterior probability are the highest and the second highest when the selected feature takes value k , that is Y j = k , and p ( c k | d the posterior probability of c k , conditioned on d c Y j = k .

In conclusion, for source node S i , if  X  b ( i ) &gt;  X  c central node queries it for the next boolean vector, other-wise, it queries for the changed value.
In this section, we describe our progressive classier, which consists of a set of classiers each operates on some partial information of the data: { C I What is I j ? In Section 2, we hope I 1 contains most class information, I 2 contains the second most class in-formation, and so on. The KL-3 features can satisfy this requirement. So we dene I 1 = { v 1 , v 2 ,  X  X  X  , v L I 2 = { v L +1 , v L +2 ,  X  X  X  , v 2 L } , and so on, where ther positive information, which is the exact value of y j the negative knowledge  X  v ij .

Each component classier makes prediction based on different partial information. C I classify, C I on. When the rst boolean vector is sent, the central clas-sier uses C I uses C I time C I ways chooses the component classier based on the length of current knowledge.
 How is negative knowledge used in classication? In our system, each sub-classier is a naive Bayesian classi-er. It assigns y to class c i if: According to Bayes theorem, where For negative knowledge, e.g.,  X  v , we have p (  X  v | c i p ( v | c i ) and p (  X  v ) = 1  X  p ( v ) . Thus, negative knowledge integrates seamlessly with the Bayesian classier.
We used both synthetic and real-life data sets to study the performance of LOCI. All algorithms are implemented in C++. To simulate different levels of load shedding, we x the load (the number of source nodes) and adjust the per-centage of bandwidth resource that the system can provide at each time unit. We use both synthetic and real life data streams.
 Synthetic Data. We generate data for 100 source nodes. The data of each source node are generated with a hyper-plane. A hyperplane in d -dimensional space is denoted by equation: We label examples satisfying P d and examples satisfying P d number of dimension, d , is set to 50. Weights a i (1  X  i  X  d ) are set to random values in the range of [0,1]. Value a 0 as follows: where x t is the value at time t , s  X  X  X  1 , 1 } species the di-rection of value change,  X   X  [0 , 1] species the magnitude of change between time t and t  X  1 . Each time unit, s is randomly generated. In addition, we bound x t in the range between 0 and 1, that is, if x t &gt; 1 or x t &lt; 0 , we forcedly switch the direction by changing s from 1 to  X  1 , or vice versa.

To observe system's behavior in resource (bandwidth) allocation, we group the 100 source nodes into two cate-gories. The rst category has 20 source nodes, which gen-erate data with  X  in the range of [0 , 0 . 9] . The second cat-egory contains the other 80 source nodes, which generate data with  X  in the range of [0 , 0 . 1] . The rst category is volatile source nodes and the second non-volatile source nodes. With this partition, we can test whether central node can allocate more resources to appropriate source nodes.
We generate 10,000 labelled examples as training dataset. And for each source node we generate 50,000 un-labelled examples to test.
 Real-life Data We use the  X waveform X  dataset from the UCI repository as our real life dataset. It contains 21 at-tributes with continuous values and 3 classes. The train-ing dataset contains 5,000 examples. We create 10 source nodes, each of which containing 3,000 unlabelled exam-ples. Also, the 10 source nodes are partitioned into two cat-egories: 2 volatile source nodes and 8 non-volatile source nodes. In non-volatile ones, we generate testing data so that Euclidean distance of any two adjacent records is smaller than 0 . 1  X  MD where MD is the maximal distance between any two test records. While in volatile ones, we cancel this constraint. Preservation of Temporal Locality We study the preser-vation of temporal locality by the KL-3 transform. Both raw and KL-3 values, are discretized into 4 bins. We compare the percentage of  X change X  in the discretized raw values and KL-3 values with varying  X  in Eq 17. The result is shown in Figure 3. We can see that the percentage of  X change X  in the KL-3 values is similar to that in the raw values under different levels of uctuation. It veries our argument that KL-3 preserves temporal locality in the original data. Error rates of different transforms In this set of exper-iments, we compare the error rate of classication on raw data, KL-3 transformed data, and PCA transformed data. For PCA transformed data and raw data, we order features by their class separability, which is measured by informa-tion entropy. The error rate is the average error rate of all 100 source nodes. The result is shown in Figure 4. It can be seen that the error rate with the KL-3 transform is much lower than the other two cases. For raw data, since every attribute can inuence an example's class label, when load shedding happens, the accuracy will suffer from lacking of class information in shed attributes. For PCA transform, al-though it guarantees the minimal mean square error, it does not take class information into consideration. For the same reason with raw data, PCA features do not guarantee good classication accuracy. The accuracy of KL-3 is more sta-ble, and even if load shedding is at 80-90% level, we see no dramatic drop in the accuracy. This is so because KL-3 transform concentrates most class information into the rst few features, and the system makes its best effort to send their values to the central classier in load shedding. Effect of Negative Knowledge In this experiment, we demonstrate the benets of using negative knowledge. We compare the error rates when negative knowledge is and is not used in prediction under different levels of load shed-ding. When negative knowledge is not used, a queried source node sends attribute values one by one in xed order { y 1 , y 2 ,  X  X  X  , y d } . Central node combines Eq 12 and 13 to measure the classication quality and queries source node with poorest quality at each step. The result is shown in Fig-ure 5. As can be seen, negative knowledge can signicantly improve the classication accuracy.
 Resource Allocation In this experiment, we compare LOCI and a naive method in their allocation of bandwidth resources to source nodes. With the naive method, the band-width resource is evenly divided among all source nodes. From Figure 6, we can see that the naive algorithm always assigns 20% bandwidth to the volatile source nodes because there are 20 out of 100 source nodes that are volatile. In contrast, for LOCI, when the bandwidth provided by the central node shrinks, a larger part of it goes to the volatile source nodes. The reason is that for volatile source node, records contain more  X changing X  values. In order to ob-tain high overall accuracy, the central node needs to allocate more resource to them.

We also compare error rates. Figure 7 shows the error rate ratio between the volatile and the non-volatile source nodes under different levels of load shedding. As we can see, for the naive algorithm, because it sheds loads equally likely from all source nodes, as the percentage of load shed-ding increases, the error rate of the volatile nodes suffers more comparing to that of the non-volatile category. In contrast, LOCI allocates resources depending on the clas-sication condence. When the boolean vectors for volatile source nodes contain more 1's, which mean previous clas-sication is likely to change, it allocates more bandwidth to them, so volatile source nodes can still have accuracy simi-lar to non-volatile ones.
 Error Rate on Real-life Dataset In the next experiment, we test the performance of LOCI on real-life datasets. First, we compare the error introduced by load shedding in three cases: classifying with KL-3 transform, with PCA transform, and without any transform. For PCA and no-transform, the features are sorted by their separability, which is measured by information entropy. From Fig-ure 8(a), we can see that KL-3 transform has higher accu-racy and is more stable than PCA and no-transform. Resource Allocation on Real-life Dataset In Figure 8(b) and 8(c), we show the results of resource allocation on real-life dataset under different levels of load shedding. Figure 8(b) shows the percentage of resource allocated to volatile source nodes and Figure 8(c) shows the ratio of er-ror rate on volatile and non-volatile source nodes. The re-sults are similar with those on synthetic data: LOCI can automatically allocate more resources to volatile source nodes. And because of equally allocation strategy in naive algorithm, its accuracy will suffer from lacking of class in-formation. On the contrary, LOCI can allocate more re-source to volatile source nodes so that accuracy on volatile and non-volatile source nodes are approximately equal when load shedding is about 50-70%. Even if load shedding percentage reaches 90%, the accuracy of volatile source nodes is only slightly higher than that of non-volatile source nodes.
Jain et al. [9] proposed using Kalman lters to adap-tively manage resources in data stream management sys-tems, where the main goal is to minimize bandwidth usage under a given precision requirement. Olston et al. [11] pro-posed an adaptive-lter scheme for continuous queries over distributed data sources, where the main concern is also the tradeoff between the precision of the answers to queries and the communication cost. These studies are similar to ours in that they also consider the relation between precision and communication cost. The difference is that: 1) they both deal with aggregate queries, in which the quality of result is much easier to compute. 2) they assume source nodes can estimate the precision of the result, which is impossible in classifying streaming data.

Babcock et al. [1] studied the load shedding problem in systems that process continuous monitoring queries over data streams. The main idea of the study is that when over-load happens, inserting load shedders in various locations of the query plan can minimize maximum relative error among all queries (with high probability). However, this study was restricted to sliding window aggregate queries and did not consider queries among multiple streams. In [13], the load shedding schemas in DSMS Aurora is described. In the study, the load shedding is based on the QoS specications on latency, values, and loss-tolerance. However, it assumed static QoS curves (e.g., concave or piece linear curves) are available to guide load shedding.

A work related to ours is Loadstar [3], which aims at solving the problem of load shedding in classifying stream-ing data. Loadstar estimates the next values using markov-chain and sheds load depending on a dened Qos equation. However, Loadstar consider each record as a whole. It does not consider the class information distribution in records. When load shedding happens, Loadstar drops the whole records.

We consider the class information distribution within record when performing load shedding for classifying streaming data. A similar but simpler problem was studied in the eld of mining text streams [6]. It assigns a weight to each feature (keyword) based on its inter-class and cross-class importance. We use KL-3 transform to aggregate most class information into a few values, thus enabling the central node to make highly accurate classication by inspecting only a small part of the record. LOCI not only can allocate resource dynamically among source nodes, but also can al-locate to values containing more class information. So even when the number of attributes are very large, LOCI still can guarantee the overall accuracy.

Another eld related to our work is research on data transform methods. There are two groups of transforms: those used to extract features for classication and those used to represent original data [5, 7, 17, 10]. The differ-ence between them is the metric used. The latter (such as PCA, DWT and DFT ) usually uses one of the L norms. The most frequently used metric is the L 2 norm, better known as mean square error. While minimizing MSE, the new fea-tures often have poor class separability, just as shown in our example. The transforms for classication use metrics which measures class separability. The best class separabil-ity is Bayes error. But because of its computation complex, some measures have been developed to replace it. The pop-ular measures contain three groups. The rst group con-tains the Chernoff bound and Bhattacharyya bound. They have two substantial shortcomings: 1) they need a probabil-ity distribution function; 2) its computation cost is still high even the probability distribution function is known. The second group contains k-NN and Parzen estimation. Al-though they do not need a probability distribution function, but they are computational inefcient. The third is the scat-ter matrices, which is based on the idea that class separabil-ity increases as class means separate and class covariances become tighter. Because of its simplicity and easy compu-tation, it is popular in many applications. The transform we use belongs to the third group. Its properties make it an excellent transform for the load shedding scenario.
We have embarked on an era when data grows faster than our analyzing abilities. This prompts us to study load shed-ding techniques that do not compromise the quality of data analysis. In this paper, we explore opportunities of load shedding for stream classication. Our goal is to be able to make highly accurate classication by inspecting as lit-tle data as possible. We combined two strategies to achieve our goal. First, we transform the data so that information essential to our task concentrates on a small number of fea-tures. Second, we focus on changes in the data instead of data itself. The two strategies are incorporated in our intelli-gent data acquisition framework, which allocates resources in a way that maximizes the likelihood of nding results of interest.

