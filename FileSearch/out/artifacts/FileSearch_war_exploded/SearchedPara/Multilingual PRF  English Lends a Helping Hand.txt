 In this paper, we present a novel approach to Pseudo-Relevance Feedback (PRF) called Multilingual PRF (MultiPRF) . The key idea is to harness multilinguality. Given a query in a language, we take the help of another language to amelio-rate the well known problems of PRF, viz. (a) The expan-sion terms from PRF are primarily based on co-occurrence relationships with query terms, and thus other terms which are lexically and semantically related, such as morphological variants and synonyms, are not explicitly captured, and (b) PRF is quite sensitive to the quality of the initially retrieved top k documents and is thus not robust. In MultiPRF, given a query in language L 1 , it is translated into language L PRF is performed on a collection in language L 2 and the re-sultant feedback model is translated from L 2 back into L The final feedback model is obtained by combining the trans-lated model with the original feedback model of the query in L 1 .

Experiments were performed on standard CLEF collec-tions in languages with widely differing characteristics, viz., French, German, Finnish and Hungarian with English as the assisting language. We observe that MultiPRF outper-forms PRF and is more robust with consistent and signifi-cant improvements in the above widely differing languages. A thorough analysis of the results reveal that the second language helps in obtaining both co-occurrence based con-ceptual terms as well as lexically and semantically related terms. Additionally, the use of the second language col-lection reduces the sensitivity to performance of initial re-trieval, thereby making it more robust.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval, Retrieval Models, Search Process Algorithms, Performance, Design, Experimentation Multilingual, Pseudo-Relevance Feedback, Language Mod-els, Query Expansion
The central problem of Information Retrieval (IR) is to satisfy the user X  X  information need, which is typically ex-pressed through a short (approximately 2-3 words) and of-ten ambiguous query. The problem of matching the user X  X  query with the documents is rendered difficult by natural language phenomena like morphological variants , polysemy and synonymy . Relevance Feedback (RF) tries to overcome these problems by eliciting user feedback on the relevance of documents obtained from the initial ranking and then us-ing it to automatically refine the query. Since user input is hard to obtain, Pseudo-Relevance Feedback (PRF) [4, 30, 19] is used as an alternative, where the RF is performed by assuming the top k documents from initial retrieval as being relevant to the query. Based on the above assump-tion, the terms in the feedback document set are analyzed to choose the most distinguishing set of terms that charac-terize the feedback documents and as a result the relevance of a document. The query refinement is done by adding the terms obtained through PRF, along with their weights, to the actual query.

Although PRF has been shown to improve retrieval effec-tiveness, it suffers from the following drawbacks: (a) due to the assumption inherent in the PRF process, i.e. , relevance of top k documents, it is sensitive to the performance of the initial retrieval algorithm and as a result is not robust, and (b) the type of term associations obtained for query expan-sion is restricted to co-occurrence based relationships in the feedback documents, and thus other types of term associa-tions such as lexical and semantic relations (morphological variants, synonymy), which are relevant in the context of the query, are not explicitly captured.

In this paper, we propose a novel approach called Mul-tilingual Pseudo-Relevance Feedback (MultiPRF) to overcome both of the above limitations of PRF. We take help of a different language called herein the assisting language .
In MultiPRF, given a query in a source language L 1 , the query is automatically translated into the assisting language L 2 and PRF performed in the assisting language. The resul-tant terms are translated back into L 1 using a probabilistic bi-lingual dictionary. At the same time, a feedback model is also computed in L 1 and finally combined with the feed-back model obtained through the assisting language. The resultant model is finally used to re-rank the corpus and fetch a new ranked list of documents. Experiments on stan-dard CLEF [3] collections in languages with widely divergent characteristics such as French, German, Finnish and Hun-garian with English as the assisting language show that Mul-tiPRF achieves significant performance improvement over monolingual PRF. A point about why English is used as the assisting language is in order here. English shares about 72% of the web content. Larger coverage typically ensures higher proportion of relevant documents in the top k re-trieval [12]. This in turn ensures better PRF. Assisting the fact is the other fact that query processing in English is a simpler proposition than in most other languages due to English X  X  simpler morphology and wider availability of NLP tools for English.
 A thorough qualitative analysis of the results reveal that MultiPRF indeed overcomes the fundamental limitations of PRF. Firstly, since it relies on the PRF in two collections of different languages, it is more robust. Secondly, the assisting language helps in obtaining both co-occurrence based con-ceptual terms as well as lexically and semantically related terms. The proposed approach is especially attractive in the case of languages where the original retrieval is bad due to poor coverage of the collection and/or inherent complexity of query processing (for example term conflation ) in those languages. For example, Hungarian has only 0.2% share of web content 1 with a rich morphology. Experiments also show that MultiPRF improves over monolingual PRF even when the query translation accuracy is sub-optimal.
The organization of the paper is as follows: In section 2, we discuss the related work in the area. Section 3 explains the Language Modeling (LM) based PRF approach which is used for performing monolingual PRF and which forms our baseline. We present the MultiPRF approach: our proposed model in Section 4. Section 5 presents the experimental set up and results followed by a discussion of these results in section 6. Finally, section 7 concludes the paper by sum-marizing observations and outlining possible directions for future work.
PRF has been effectively applied in various IR frameworks like vector space models, probabilistic IR and language mod-eling [4, 15, 17, 33]. Several approaches have been proposed to improve the performance and robustness of PRF. Some of the representative techniques are (i) to refine the feed-back document set [19, 24], (ii) refining the terms obtained through PRF by selecting good expansion terms [5] and (iii) using selective query expansion [1, 7] and varying the impor-tance of documents in the feedback set [25]. Another direc-tion of work, often reported in the TREC Robust Track, is to use a large external collection like Wikipedia or the Web as a source of expansion terms [32, 27]. The intuition behind the above approach is that if the query does not have many relevant documents in the collection then any improvements in the modeling of PRF is bound to perform poorly due to query drift.

Several approaches have been proposed for including dif-ferent types of lexically and semantically related terms dur-ing query expansion. Voorhees et al. [28] use Wordnet for query expansion and report negative results. Recently, ran-http://www.netz-tipp.de/languages.html dom walk models [16, 6] have been used to learn a rich set of term level associations by combining evidence from various kinds of information sources mentioned so far like WordNet, co-occurrence relationships, web, morphological variants etc., . Metzler et al. [18] propose a feature based approach called latent concept expansion to model term de-pendencies.

All the above mentioned approaches use the resources available within the language to improve the performance of PRF. However, we make use of a second language (En-glish) to improve the performance of PRF. As mentioned earlier, this is an attractive proposition for languages where the original retrieval is bad due to poor coverage and inher-ent complexity of query processing due to rich morphology, word compounding etc .

The idea of using one language to improve the accuracy of another language in a specific task has been successfully tried for the problem of Word Sense Disambiguation (WSD) [8].

A recent work by Gao et al. [11] uses English to improve the performance over a subset of Chinese queries whose translations in English are unambiguous. They use inter-document similarities across languages to improve the rank-ing performance. The computation of cross language doc-ument level similarities between English and Chinese docu-ments is done using a bi-lingual dictionary. However, cross language document similarity measurement is in itself known to be an equally hard problem especially without using par-allel or comparable corpora [10]. Moreover, the scale of their experimentation is quite small and they demonstrate their approach only on a small class of queries in a single language.
The Language Modeling (LM) Framework for IR offers a principled approach to model PRF. In the LM approach, the document and query are modeled using multinomial distri-bution over words called document language model P ( w | D ) and query language model P ( w |  X  Q ) respectively. For a given query, the document language models are ranked based on their proximity to the query language model, measured us-ing KL-Divergence.
 Since the query length is short, it is difficult to estimate the query language model accurately using the query alone. In PRF, the top k documents obtained through the initial ranking algorithm are assumed to be relevant and used as feedback for improving the estimation of  X  Q . The feedback documents contain a mix of both relevant and noisy terms. The actual relevant terms modeled using the feedback lan-guage model  X  F is inferred from D F based on a Generative Mixture Model [33] formulation.
Let D F = { d 1 ,d 2 ,...,d k } be the top k documents re-trieved using the initial ranking algorithm. Zhai and Laf-Figure 1: Schematic of the Multilingual Pseudo-Relevance Feedback Approach ferty [33] model the feedback document set D F as a mixture of two distributions: (a) the feedback language model and (b) the collection model P ( w | C ). Assuming a fixed mixture proportion  X  in the feedback document set, the feedback lan-guage model is inferred using the EM Algorithm [9]. In the EM algorithm, the feedback model is iteratively refined by accumulating probability mass on most distinguishing terms which are more frequent in the feedback document set and less frequent across the entire collection. Let  X  F be the final converged feedback model. Later, in order to keep the query focus,  X  F is interpolated with the initial query model  X  to obtain the final query model  X  F inal .
  X 
F inal is used to re-rank the corpus using the KL-Divergence ranking function to obtain the final ranked list of documents. Henceforth, we refer to the above PRF technique by as Model Based Feedback (MBF) . In this section, we describe our main contribution -the Multilingual PRF approach. The schematic of the approach is shown in Figure 1.

Given a query Q in the source language L 1 , we automat-ically translate the query using a query translation system into the assisting language L 2 . We then rank the docu-ments in the L 2 collection using the query likelihood rank-ing function [14]. Using the top k documents, we estimate the feedback model using MBF described in the previous section. Similarly, we also estimate a feedback model using the original query and the top k documents retrieved from Table 1: Glossary of Mathematical Symbols used in explaining MultiPRF Table 2: Top Translation Alternatives for some sam-ple words in Probabilistic Bi-Lingual Dictionary the initial ranking in L 1 . Let the resultant feedback models be  X  F L 2 and  X  F L 1 respectively.
 The feedback model estimated in the assisting language  X 
L 2 is translated back into language L 1 using a probabilistic bi-lingual dictionary P L 2  X  L 1 ( f | e ) from L 2  X  L 1 The probabilistic bi-lingual dictionary P L 2  X  L 1 ( f | e ) is learned from a parallel sentence-aligned corpora in L 1  X  L 2 based on word level alignments. Tiedemann [26] has shown that the translation alternatives found using word alignments could be used to infer various morphological and semantic rela-tions between terms. For example, in Table 2, we show the top translation alternatives for some sample words. For example, the French word am  X ericain (american) brings dif-ferent variants of the translation like american, america, us, united, state, america which are lexically and semantically related. Hence, the probabilistic bi-lingual dictionary acts as a rich source of morphologically and semantically related feedback terms. During the step for translating the feedback model given in Equation 2, the translation model adds re-lated terms in L 1 which have their source as the term from feedback model  X  F L 2 .

The final MultiPRF model is obtained by interpolating the above translated feedback model with the original query model and the feedback model of language L 1 as given below:
Since we want to retain the query focus during back trans-lation the feedback model in L 2 is interpolated with the translated query before translation. The parameters  X  and  X  control the relative importance of the original query model, feedback model of L 1 and the translated feedback model obtained from L 1 and are tuned based on the choice of col-lection in L 1 and L 2 .
We evaluate the performance of our system using the stan-dard CLEF evaluation data [3] in four widely differing lan-guages -French, German, Finnish and Hungarian using more than 600 topics. We use English as the assisting language. The details of the collections, their corresponding topics and the assisting collections used for MultiPRF are given in Ta-ble 3. Note that we choose the English assisting collection such that the coverage of topics is similar to that of the orig-inal corpus so as to get meaningful feedback terms. In all the topics, we only use the title field. We ignore the topics which have no relevant documents as the true performance on those topics cannot be evaluated.

We use the Terrier IR platform [21] for indexing the doc-uments. We perform standard tokenization, stop word re-moval and stemming. We use the Porter Stemmer for En-glish and the stemmers available through the Snowball 2 pack-age for French, German, Finnish and Hungarian. Other than these, we do not perform any other processing on German, Finnish and Hungarian. However, in French, since some function words like l X  , d X  etc., occur as prefixes to a word, we strip them off during indexing and query processing, since that caused the baseline performance to decrease. We use standard evaluation measures like MAP , P@5 and P@10 for evaluation. Additionally, for assessing robustness, we use the Geometric Mean Average Precision (GMAP) metric [23] which is also used in the TREC Robust Track [27].
The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++ -a word align-ment tool [20] on a parallel sentence aligned corpora. For French-English, German-English and Finnish-English lan-guage pairs, we used the Europarl Corpus [22] and in case of Hungarian-English, we used the Hunglish Corpus 3 .
We make use of off-the-shelf translation systems available in the above language pairs. We use Google Translate 4 as http://snowball.tartarus.org/index.php http://mokk.bme.hu/resources/hunglishcorpus http://translate.google.com the query translation system as it has been shown to per-form well for query translation [29]. Later, we show that our approach is not dependent on Google Translate, and report results using a basic SMT system for query translation. For this, we evaluate the quality of the above Query Translation systems and analyze their impact on the quality of our re-sults. In the pathological case of term not being found in English after query translation, we only perform MBF on the source language L 1 .

We use the MBF approach explained in Section 3.1 as a baseline for all our comparisons. We use two-stage Dirich-let smoothing with the optimal parameters tuned based on the collection [34]. We tune the parameters of MBF, specifi-cally  X  and  X  , and choose the values which give the optimal performance on a given collection. We uniformly set the number of feedback documents, i.e., k as 10 i.e. top ten documents. The overall results are shown in Table 4. We observe that the optimal values of interpolation coefficients  X , X  in MultiPRF are almost uniform across collections and vary in the range 0.4-0.48.
As in Table 4, the results show that the MultiPRF ap-proach with English as the assisting language significantly outperforms the MBF approach across all datasets of all the chosen languages. We consistently observe significant im-provements in MAP (between 4% to 8%), P@5 (between 4% to 39%) and P@10 (around 4% to 22%). The MultiPRF approach is also more robust than plain MBF as reflected in the improvements obtained in GMAP scores (between 15% to 730%). This could be attributed in part to the re-duced sensitivity of our approach to the number of relevant documents in the feedback set of the source language. An analysis of the overall results reveal that MultiPRF lever-ages the performance in English language and adds relevant terms like morphological variants and synonyms in addition to co-occurrence based term relations. Besides this, it also improves the performance of some queries where the PRF performance was poor to start with, by bringing in related terms through PRF in L 2 and back translation.
To illustrate the qualitative improvement in feedback terms, a detailed analysis of a few representative queries is pre-sented in Table 5. Based on the above analysis, the improve-ments obtained by MultiPRF approach could be mainly at-tributed to one of the following three reasons:-(a) Retrieval Performance in L 2 is good and the resultant feedback model contains a lot of relevant terms, which when brought back to L 1 via back-translation leads to improvement. (b) During the back-translation process, important synonyms and pop-ular morphological variants (inflectional forms) of key terms are found, which otherwise were missing from the Model-Based feedback model. and (c) A combination of both the above factors.

For example, consider the French Query  X  X scar honori-fique pour des r  X ealisateurs italiens X  , meaning  X  X onorary Os-car for Italian Filmmakers X . Model Based Feedback on French expands the query using the top retrieved documents of the initial retrieval. However, here it introduces signifi-cant topic drift towards Oscar Scalfaro (a former Italian President) and Italian politics thus causing words such as { scalfaro, spadolin, gouvern } . However, feedback in En-glish produces relevant terms, which on translation back into French, introduces terms such as { cinem, cin  X east, r  X ealis } . This wrenches back the focus of the query from the political domain to the intended film domain, thus leading to per-formance increase. Another example of this phenomenon is the query  X  X es Drogues Anti-Cancer X  (Anti-Cancer Drugs). Here too MBF causes drift away from the intended mean-ing and instead to Drug-Trafficking, by introducing terms such as { traffic, entre, afghanistan } , which causes very poor performance on the query. MultiPRF however utilizes the good feedback performance of English on this query, to gen-erate a set of very relevant French terms such as { recerch, taxol, glaxo } . Hence the drift from the intended meaning towards drug-trafficking is corrected, by the introduction of the above mentioned terms, which help in bringing up the performance on this query. These examples demonstrate the robustness of the MultiPRF approach and the reduced sensitivity to the relevance of the top documents from the initial retrieval. Google Translate and another SMT system trained using Europarl corpus. Table 6: Comparison of Query Translation Quality using Google Translate and SMT system trained on Europarl Corpus on a scale of 0-1.

Apart from this we also see improvements on queries due to introduction of synonyms and other semantically related terms. For example, on the German query  X   X  Olunf  X  alle und V  X  ogel X  meaning  X  X irds and Oil Spills X , MBF performs poorly with many irrelevant terms introduced in the feedback model. However English finds some relevant terms, and additionally adds many terms to the feedback model, which are syn-onyms/semantically related to oil spills and birds, such as { olverschmutz, ol, olp, vogelart } . This helps in bringing up more relevant documents while reducing drift. Accurate Query Translation is fundamental to MultiPRF. As explained earlier, we chose Google Translate mainly due to its ease of availability. In this section, we study the im-pact of varying translation quality on the performance of our approach. We train a Statistical Machine Translation (SMT) system, on the French-English and German-English language pairs, by running an off-the-shelf publicly avail-able tools like Moses [13] on Europarl corpora. The above SMT system is quite simple because we do not perform any language-specific processing or any parameter tuning to im-prove the performance of the system and also it is limited by the domain of the parallel corpora which is parliamentary proceedings. To correlate the translation quality with the performance of MultiPRF, we evaluated the query transla-tions produced by Google Translate and SMT system on a three-point scale between 0 and 1 (0 -Completely Wrong Translation, 0.5 -Translation not optimal but query intent partially conveyed and 1 -Query intent completely con-veyed). The results are shown in Table 6. We compare the performance of MultiPRF using Google Translate, Ba-sic SMT system, and ideal query translations. The ideal translations were obtained by manually fixing some of the errors in the above two systems. The performance on ideal Table 8: Comparison of MultiPRF performance with MBF using an assisting collection in same lan-guage. Coverage of source and assisting collections given for comparison. query translations gives an idea of the upper bound on the performance of MultiPRF. The results of our evaluation are shown in Table 7.

As expected, the performance of MultiPRF on ideal trans-lations is the best followed by Google Translate and the Basic SMT system. The results demonstrate that trans-lation using the basic SMT system improves over monolin-gual MBF, especially P@5 and P@10. This shows that the performance of MultiPRF improves performance with any reasonably good query translation system.
One of the prime reasons for improvement in MultiPRF performance is good monolingual performance of assisting collection. The natural question which may then arise is whether the assisting collection needs to be in a different language. In this section, we study the performance of Mul-tiPRF when the assisting collection is in the same language. Given a query, we use MBF on both source and assisting collections and interpolate the resultant feedback models. The final interpolated model is used to rerank the corpus and produce the final results. For the experiments, we use the French and German collections (FR-01+02, DE-01+02) since they have additional collections (FR-06, DE-03) with larger coverage in their own language. The results of com-parison are shown in Table 8.

From the results, we notice that although the coverage of assisting collections in the source language is more than that of English, MBF still performs poorly when compared to MultiPRF. This can be attributed to the following reasons a) the MBF performance of a query, which is ambiguous or hard in the source language collection, will be bad due to the poor quality of top k documents retrieved during initial re-trieval. The quality of the top k documents will not change if the same ambiguous query is given to assisting collection in the source language. However, if source and assisting lan-guages differ, the ambiguity may get resolved during trans-lation causing an improvement in MBF performance. The above intuition is confirmed by the decrease in robustness, as reflected in the GMAP scores, when the source and target languages are same. b) it still suffers from the fundamental limitation of monolingual PRF i.e. the expansion terms in-cluded are only based on co-occurrence relations and does not include lexically and semantically related terms.
As discussed earlier, another major source of improvement in MultiPRF is due to the inclusion of lexically and seman-tically related terms. However, this alone does not justify the use of an assisting collection in a different language since the same effect could be achieved by using thesaurus based expansion in the source language. In this section, we show that augmenting MBF with both thesaurus based expansion and assisting collection in the same language is not effective when compared to MultiPRF.

Since there is no publicly available thesauri for the above mentioned European languages, as proposed in Xu et al. [31], we learn a probabilistic thesaurus P L  X  L , in source lan-guage L , from the probabilistic bi-lingual dictionaries in L-English P L  X  E and English-L P E  X  L . Given two words s and s 2 in source language L and e is a word in English ( E ), P L  X  L is given by: P Lexically and semantically related words like morphologi-cal variants and synonyms have a high probability score in P
L  X  L since they usually map to the same word in the tar-get language. Given a query, we initially run MBF in the source language and let  X  F L be the resultant feedback model. Later, we use the probabilistic thesauri to expand the feed-back model as follows: The above step includes morphological variants and syn-onyms for the terms in the feedback model. The final model is obtained by interpolating the  X  T hesaurus L with the MBF model  X  F L as shown in Equation 3.
 For the above experiments, we use the FR-01+02 and DE-01+02 French and German collections. The results of comparison is shown in Figure 2. It shows that MBF with both thesaurus based expansion and assisting collection in the source language does not perform as well as MultiPRF. MultiPRF automatically combines the advantage of PRF in two different collections and thesaurus based expansion. This addresses the fundamental limitations of MBF and re-0.1 0.2 0.3 0.4 0.5 Figure 2: MAP score comparison of MultiPRF and MBF with assisting collection in same language and Thesaurus Based Expansion. In MBF experiments, FR-06 and DE-03 were used as assisting collections for French and German respectively. sults in an improvement of both retrieval performance and robustness.
We presented a novel approach to PRF called Multilin-gual PRF in which the performance of PRF in a language is improved by taking the help of another language collection. We also showed that MultiPRF addresses the fundamen-tal limitations of monolingual PRF, viz. , (i) the inability to include term associations based on lexical and semantic relationships and (ii) sensitivity to the performance of the initial retrieval algorithm. Experiments on standard CLEF collections across a wide range of language pairs with varied degree of familial relationships show that MultiPRF consis-tently and significantly outperforms monolingual PRF both in terms of robustness and retrieval accuracy. Our error analysis pointed to the following contributing factors: (i) inaccuracies in query translation including the presence of out-of-vocabulary terms, (ii) poor retrieval on English query, and in a few rare cases, (iii) inaccuracy in the back trans-lation. We feel we have taken only the first step towards a direction of work with rich potential, viz. how a language can help another with respect to pseudo-relevance feedback .
As part of future work, we plan to vary the assisting lan-guage and study its effect on MultiPRF performance. Also, we would like to remove the dependence of MultiPRF ap-proach on availability of parallel corpora in the assisting language.
 We would like to thank ACM SIGIR and Amit Singhal (Don-ald B. Crouch Travel Grant) for the student travel grant to the first and second authors. The first author was also sup-ported by by a fellowship award from Infosys Technologies Ltd., India. [1] G. Amati, C. Carpineto, and G. Romano. Query [2] A. Berger and J. D. Lafferty. Information Retrieval as [3] M. Braschler and C. Peters. Cross-Language [4] C. Buckley, G. Salton, J. Allan, and A. Singhal. [5] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting [6] K. Collins-Thompson and J. Callan. Query Expansion [7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A [8] I. Dagan, A. Itai, and U. Schwall. Two Languages Are [9] A. Dempster, N. Laird, and D. Rubin. Maximum [10] T. S. Dumais, A. T. Letsche, L. M. Littman, and [11] W. Gao, J. Blitzer, and M. Zhou. Using English [12] D. Hawking, P. Thistlewaite, and D. Harman. Scaling [13] H. Hoang, A. Birch, C. Callison-Burch, R. Zens, [14] John Lafferty and Chengxiang Zhai. Probabilistic [15] K. S. Jones, S. Walker, and S. E. Robertson. A [16] J. Lafferty and C. Zhai. Document Language Models, [17] V. Lavrenko and W. B. Croft. Relevance Based [18] D. Metzler and W. B. Croft. Latent Concept [19] M. Mitra, A. Singhal, and C. Buckley. Improving [20] F. J. Och and H. Ney. A Systematic Comparison of [21] I. Ounis, G. Amati, P. V., B. He, C. Macdonald, and [22] K. Philipp. Europarl: A Parallel Corpus for Statistical [23] S. Robertson. On GMAP: and Other Transformations. [24] T. Sakai, T. Manabe, and M. Koyama. Flexible [25] T. Tao and C. Zhai. Regularized Estimation of [26] J. Tiedemann. The Use of Parallel Corpora in [27] E. Voorhees. Overview of The TREC 2005 Robust [28] E. M. Voorhees. Query Expansion Using [29] D. Wu, D. He, H. Ji, and R. Grishman. A Study of [30] J. Xu and W. B. Croft. Improving the Effectiveness of [31] J. Xu, A. Fraser, and R. Weischedel. Empirical [32] Y. Xu, G. J. Jones, and B. Wang. Query Dependent [33] C. Zhai and J. Lafferty. Model-based Feedback in the [34] C. Zhai and J. Lafferty. A Study of Smoothing
