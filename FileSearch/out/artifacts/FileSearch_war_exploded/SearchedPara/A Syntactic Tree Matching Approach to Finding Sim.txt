 While traditional question answering (QA) systems tailored to the TREC QA task work relatively well for simple questions, they do not suffice to answer real world questions. The community-based QA systems offer this service well, as they contain large archives of such questions where manually crafted answers are directly available. However, finding simila r questions in the QA archive is not trivial. In this paper, we propose a new retrieval framework based on syntactic tree structure to tackle the similar question matching problem. We build a ground-truth set from Yahoo! Answers, and experimental re sults show that our method outperforms traditional bag-of-word or tree kernel based methods by 8.3% in mean average precision. It further achieves up to 50% improvement by incorporating semantic features as well as matching of potential answers. Our model does not rely on training, and it is demonstrated to be robust against grammatical errors as well. H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval  X  Retrieval Models; I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  Text Analysis Algorithms, Measurement, Experimentation Question Answering, Syntactic Structure, Question Matching, Yahoo! Answers 
Traditional TREC QA task has ma de significant progress since it was first introduced in 1990s [1]. However, research on TREC QA has largely targeted on short, factoid-based, questions, for which concise answers are expected. For example, TREC QA simply expects the year  X  1960  X  for the simple question  X  In what year did Sir Edmund Hillary search for Yeti?  X . It was earlier claimed that while QA systems tailored to the TREC QA task worked relatively well for factoid-type questions, they might not be necessarily effective in question answering applications outside TREC [7]. In real world, more complex questions are usually asked, and users are more willing to obtain a longer and more comprehensive answer which contains sufficient context information. Traditional QA systems are now facing problems of being deployed into real world. 
With the blooming of Web 2.0, social collaborative applications such as Wikipedia, YouTube, Facebook etc. begin to flourish, and there have been an increasing number of Web information services that bring t ogether a network of self-declared  X  X xperts X  to answer questions pos ted by other people. This is referred to as the community-based question answering services (cQA). In these communities, anyone can ask and answer questions on any topic, and people seeking information are connected to those who know the answer. As answers are usually explicitly provided by human and are of high quality, they can be helpful in answering real world questions. 
Yahoo! Answers, launched on December 13, 2005, is now becoming the largest knowledge -sharing online community among several popular cQA servi ces. Over times , a tremendous number of previous QA pairs have b een stored in its database, and in most circumstances, users may directly get the answers from Yahoo! Answers by searching from this QA archive, rather than looking through a list of potentially relevant documents from the Web. As such, instead of extracting answers from a certain document corpus, the retrieval task in cQA becomes the task of finding relevant similar que stions with new queries. 
The similar question matching task is, however, not trivial. One of the major reasons is that instead of inputting just keywords or so, users form questions using natural language, where questions are encoded with various lexical, syntactic and semantic features. For example,  X  how can I lose weight in a few month?  X  and  X  are there any ways of losing pound in a short period?  X  are two similar questions asking for methods of losing weight, but they neither share many common words nor follow identical syntactic structure. This gap makes the similar question matching task difficult. Similarity measure t echniques based purely on the bag-of-word (BoW) approach may perform poorly and become ineffective in these circumstances. 
Syntactic or semantic features hence become vital for such task. The tree kernel function [5] is one of the most effective ways to represent the syntactic structure of a sentence. In general, it divides the parsing tree into seve ral sub-trees and computes the inner product between two vector s of sub-trees. Although there have been some successful app lications using it, like Question Classification [3,13,19], the tree kernel-like function has not been directly applied to finding similar questions in the QA archive. Moreover, its matching scheme is too strict to be directly employed to our question matching problem. In this paper, we re-formulate the tree kernel framework, and introduce a new retrieval model to find similar questions. We extensively study the structural representations of questions to encode not only lexical but also syntactic and sema ntic features into the matching model. Our model does not rely on training, and it is shown to be robust against grammatical errors as well. 
The rest of the paper is organized as follows: Section 2 gives a background introduction on the well-known tree kernel concept. Section 3 presents the architecture of our syntactical tree matching model. Section 4 desc ribes an improved model with semantic features incorporated. Section 5 presents our experimental results. Section 6 reviews some related works and Section 7 concludes our paper with directions for future works. 
Traditional information retrieval tasks adopt the BoW or language model etc. to perform re trieval. However, these purely lexical based approaches are ofte n inadequate to perform fine-level textual analysis if the task involves the use of more varying syntactic structures or complex semantic meanings. 
In order to utilize more structural or syntactical information and capture higher order depende ncies between grammar rules, Collins tried to consider all tree fra gments that occur in a parsing tree [5]. He defined the tree fragment to be any sub-tree that includes more than one node, with the restriction that the entire rule productions must be included. Zhang &amp; Lee [19] inherited it by proposing a slightly different de finition, in which all terminal symbols are included into sub-trees, arguing that the tree kernel can back off to the word linear kernel. 
Figure 1. (a) The Syntactic Tree of the Question  X  X ow to lose weight? X . (b) Tree Fragments of the Sub-tree covering "lose 
Figure 1 gives an illustration on how the tree decomposition works according to Zhang &amp; Lee X  X  definition. Figure 1(a) shows the entire syntactic parsing tree of the question  X  How to lose weight?  X , and Figure 1(b) shows all the sub-trees under the node of VP covering the phrase  X  lose weight  X . All the tree fragments produced contain the entire produc tion rule, i.e., any sub-trees containing a part of the production rule such as  X  VP  X  VB X  for  X  VP  X  VB X NP  X  are considered invalid. 
The tree kernel was designed based on the idea of counting the number of tree fragments that ar e common to both parsing trees, and it could be defined as: where N 1 and N 2 are sets of nodes in two syntactic trees T and C(n 1 ,n 2 ) equals to the number of common fragments rooted in nodes n 1 and n 2 . However, to enumerate all possible tree fragments is an intractable problem. The tree fragments are thus implicitly represented, and with dynamic programming, the value of C(n 1 ,n 2 ) can be efficiently computed as follows: where nc(n) is the total number of children of node n and ch(n,j) is the j-th child of node n in the tree. n 1 =n 2 denotes that the labels and production rules of node n 1 and n 2 are the same, and n denotes the opposite. The parameter  X  , a weighing factor, is used to resolve the kernel peaking problem. 
Although the tree kernel function has been successfully applied in some areas like question classification, there is no precedent work of using it to help find si milar questions. The tree kernel metric measures the distance between two sentences, but there are two major limitations that prevent it from being employed directly in our question matching problem: (a) the tree kernel function merely relies on the intuition of counting the common number of sub-trees, whereas the number might not be a good indicator of the similarity between two ques tions; and (b) the two evaluated sub-trees have to be identical to allow further parent matching, for which semantic representations ca nnot fit in well. To remedy the second issue, the Shallow Semantic Tree Kernel (SSTK) was proposed in [14], where Predicat e Argument Structures (PAS) are exploited to take dependencie s into account. However, it was noted to be computational expens ive for real world applications. 
In the remainder of this Section, we introduce a new retrieval model, named Syntactic Tree Matching (STM) , by reformulating the original tree kernel definition. We present a new weighting scheme for tree fragments to make the final distance metrics not only faithful to the similarity measure but robust enough against some grammatical errors. This gives rise in Section 4 to a fuzzy matching scheme, which incorpor ates semantic features and elegantly tackles the second limitation. 
We directly employ the definition of the tree fragment from [19], where terminal nodes were included as a part of tree fragments. Before introducing the weighting scheme of the tree fragment, we first give defin ition to the node weighing factor: 
Preliminary 1: The weighting factor  X  i denotes the importance of node i in the parsing tree. Its va lue differs for different types of nodes:  X   X  i =1.2, where node i is eith er the POS tag VB or NN 1  X   X  i =1.1, where node i is either VP or NP  X   X  i =1 for all other types of nodes 
We believe that different parts of the sentence have different importance, and the nouns and verbs are considered to be more important than other types of terms such as article, adjective or adverb. We also boost up the nodes of verb and noun phrases, to show their higher priority over other ordinary ones. 
With node weighing factor, we de fine the weighing coefficient (  X  ) of the tree fragment as follows: 
Preliminary 2: The weighting coefficient  X  k for tree fragment k conveys the importance of the tree fragment, whose value is the production of all weighing factors of node i that belongs to the tree fragment k, i.e.,
Intuitively, if a tree fragment contains lots of important nodes, its importance would be higher, and vice versa. The weighing coefficient can be reformulated into a recursive function factor of the tree fragment root, and  X  j is the weighting coefficient brought from its sub-trees that di rectly connect to the root. 
We further define the size of the sub-tree ( S i ) and its weighing factor (  X  ), together with the depth of the sub-tree ( D weighing factor (  X  ) as follows: 
Preliminary 3: The size of the tree fragment S i is defined by the number of nodes that it contains. The size of weighing factor  X  is a tuning parameter indicating the im portance of the size factor. 
Preliminary 4: The depth of the tree fragment D i is defined as the level of the tree fragment root in the entire syntactic parsing tree, with D root =1. The depth weighing factor  X  is a tuning parameter indicating the impor tance of the depth factor. 
The introduction of the size and depth factors of the tree fragment is to account for the fact that sub-trees with different sizes and at different levels ha ve different impact on the whole parsing tree. This impact could be interpreted in two aspects. First, a larger tree fragment contains more variety of senses. If a large portion of two parsing trees are of the same, their similarity would be higher. Second, the tree fragments at the bottom levels carry more significant semantic information than those at the upper level. This is because nodes at the upper layer usually determine the surface structure of a whole sentence, whereas nodes at the bottom layer contain information like word sense, inner phrase structures, and chunk relations etc., which are a lot more crucial. 
The two tuning parameters  X  and  X  denote the preference between size and depth. Higher  X  but lower  X  means the size factor is more favorable than the depth factor, and vice versa. 
Given the parameters listed a bove, we introduce the weighting scheme for the tree fragments: 
Definition 1: The weight of a tree fragment w i is defined as  X   X 
Si  X  Di , where  X  i is its weighting coefficient, S Due to stemming, we normalize all POS tags in the way that all plural noun POS tags are replaced by their single forms (e.g. 
NNS  X  NN ) and all verb POS tags are replaced by their base forms (e.g. VBN  X  VB ). tree and  X  is the depth weighting factor. 
Different from the tree weighting in [5], which penalized larger trees, our weighting scheme favors larger trees. Unlike the weighting proposed in [19], which simply considers the size and depth of the tree, our weighting scheme additionally takes into account the importance of the wo rds or phrases that a tree fragment covers. 
After introducing the weighting scheme of tree fragments, we need to match tree fragments and compute weights of the matched trees: Preliminary 5: If two tree fragments TF 1 and TF the weight of their resulting matching tree fragment TF is defined to be ) ( ) ( ) ( 2 1 TF w TF w TF w = . (4)
Recall that the weighting scheme of each tree fragment is determined by the formula  X  i  X  Si  X  Di , we may thus write the weight of the matched tree fragment as  X  1  X  2  X  S1+S2  X  D1+D2 . 
In view of the above, we introduce a new scoring function, named node matching score , between two nodes r 1 and r
Preliminary 6: The node matching score between two nodes r fragments under the roots of r 1 and r 2 . We use the following formula to describe it: where r 1  X  r 2 denotes the fact that either labels or production rules fragment under r 1 and r 2 , and  X  is the total number of tree fragments. 
We can reformulate the node matching score into following recursive version: is the j-th child of node n in the tree, and  X  is the total number of matched tree fragments (See Appendix A for proof of correctness of the recursive function 6). 
According to the comprehensive definition of weighting scheme for the tree fragments, two nodes with many tree fragments of higher weights are likely to produce higher node matching scores. This indicates that these node pairs may have covered very similar phrases. Ther efore, we argue that the node matching score provides a good measur e of the similarity between the sub-trees rooted under nodes r 1 and r 2 . 
In order to find the similarity score between two syntactic parsing trees T 1 and T 2 , we traverse them in post-order, and calculate the pair-wise node matching scores between the nodes in these two trees. Th is results in a | T 1 |x| T 2 | matrix of M(r use the summation of all scores in the matrix to represent the similarity score between two parsing trees: 
Definition 2: The similarity score or the distance metrics between two parsing trees is defined as 
However, as the score is very sensitive to the size of trees T and T 2 , we normalize it into the following: 
By making use of the recursive definition of the node matching score, one can calculate the final similarity score between two parsing trees in polynomial time with dynamic programming.
In real world, however, grammatical and spelling errors made by people are not uncommon, and these errors may have various influences on the resulting parsing tree. We observe that some of affect a small portion of the parsing tree at the deep level. For instance, the parsing trees for the sentences  X  I want doctor.  X  and  X  I want a doctor. X  differ only by one leaf node and one POS tag ( DT ). We name these errors with only minor effects on the parsing result at the deep level as interior errors. Our matching model is obviously safe for them. 
However, there are some other grammatical errors which may greatly alter the appearance of the parsing tree. We name them as exterior errors, due to the fact that they may change the shallow structure of the parsing tree. The preposition error, for example, is a kind of exterior errors. Figure 2 shows two parsing trees for two questions, in which 2(b) uses the preposition  X  to  X  instead of  X  for  X . 
As has been spotted by the dashed rectangles in Figure 2, the surface structures of these two parsing trees appear to be very different due to the preposition misuse. However, it is also observed that, the structures of th e chunks or phrases at the lower level are well preserved (as highlighted by the solid rectangles). This is common, as our large numbe rs of investigations show that the tree fragments at the lower level can be immune from exterior errors. In respect that tree fragments at the lower level are not affected by the exterior errors and the weight of the tree fragments at the lower level is relatively higher than those at the upper level, the matching score between two parsing trees will not be degraded much in case of errors. We therefore claim that our weighting scheme is robust to exterior grammatical errors. 
In the STM model above, if two parsing trees employ different leaf wordings or slightly transformed production rules, the tree fragments can hardly be matched. This becomes an evident drawback from the semantic point of view, and it motivates a modification to our original matching model. In order to capture more semantic meanings, we: (a) allow partial contribution from terminal words if they are shown to be closely related; (b) relax the production rules to allow for partial matching; and (c) use answer matching to bring in more semantically related questions. 
Firstly, we use WordNet, a freely available semantic network, to help measure the semantic similarity between two words. We employ Leacock X  X  measure [12], which uses the distance of the shortest path between two synsets to represent the semantic distance between two words, where the value is scaled by the overall depth of the taxonomy. In order to fit our matching model, in which the semantic score needs to be scaled between 0 and 1, we modify the Leacock X  X  measure into the following: 
Sem(w 1 ,w 2 ) = 1-distance(w 1 ,w 2 )/2D (9) where distance(w 1 ,w 2 ) is the length of the shortest path between two synsets of w 1 and w 2 , and D is the maximum depth of the taxonomy. In particular, we define the path length between two identical words to be 0, i.e., distance(w,w)= 0, or Sem(w,w) =1. 
Secondly, we allow partial matc hing of production rules in the way that two nodes with sufficiently similar production rules can be matched. This sufficiency in cludes omission or reversion of the modifiers, preposition phrases , conjunctions and so on. For instance,  X  NP  X  DT X JJ X NN  X  is considered to be similar to  X  NP  X  DT X NN  X , and can be matched. The complete matching rules are not listed here due to space. 
With the two relaxations defined above, we perform fuzzy matching between tree fragments. This could be achieved by modifying the matching scheme as proposed in Preliminary 5:
Preliminary 5 X : The weight of the matching tree fragment TF resulted from matching TF 1 and TF 2 is defined as: 
The new definition is in line with Preliminary 5 , except for the handling of terminal words and production rules. Two different terminal words can now be matched into a fuzzy word, and nodes with similar production rules can be aligned as well. Sem(w is the semantic similarity score as calculated from WordNet. 
In order to avoid generating too many improbable tree fragments and make the matching more accurate, we impose two restrictions in our design: 1. A confidence level of 0.75 is set on semantic distances 2. Only terminal words with the same POS tag 2 could be 
It is noticed that question matching, even at the semantic level as described above, do not suffice to capture all similar pairs in some circumstances. For example, two similar questions  X  Proper way to lose weight?  X  and  X  I X  X  too fat, help?  X  hardly share any common points. To further overcom e this kind of semantic gap, we introduce additional matching of questions via their answers, named answer matching . This matching is based on the intuition that if the answers to two questi ons are similar, the questions are considered to be semantically si milar even if they are lexically very different. Therefore, given a query, the answers to top ranked matching questions could be utilized to fetch more similar questions via answer matching, where the newly retrieved questions could have great varia tions in both lexicon and syntax. 
In this section, we present empirical evaluation results to assess the effectiveness of our STM t echnique for the similar question matching problem. In particular, we conduct experiments on the Yahoo! Answers QA archive and show that our STM is more effective than the original Tree Kernel function. We further show that the semantic-smoothed ve rsion gives additional boosting on matching precision. We issue getByCategory query provided in Yahoo! Answers API 3 calls to download QA threads from the Yahoo! site. We collected a total of around 0.5 million QA pairs from the Healthcare domain, over a 10-month period from 15/02/08 to 20/12/08 . It covers areas including diet, fitness, dental, diseases, men X  X  and women X  X  health, etc. We only focus on all resolved QA pairs, meaning questions that alr eady have been given their best answers. Based on the hypothesis that the best answer represents the most accurate information responding to the question, we can use it to directly answer a query should we find a similar question to the query. 
As there can be multiple questions asked in a single question thread, we segment each question thread into pieces of single-sentence questions by using que stion mark and 5W1H words heuristic. The reason is two-fold: (a) Different questions may ask about different aspects; to separate them is helpful to better match questions with user X  X  query. (b) The parser handles short sentences better than longer ones, for which ambiguous syntactic structures are likely to occur. 
In order to evaluate our retrieva l system, we divide our dataset into two parts. The first part (0.3M), covering the initial period of 3.5 months dated from 15/02/08 to 05/06/08, is used as the ground-truth setup; the rest is used as test-bed for evaluation. For ground-truth, we asked four annot ators to tag similar questions from the first part of the dataset. As the number of question Currently we focus on NN and VB matching, as WordNet only provides the hypernymy hierarch ical relationship for nouns and verbs. For adjectives and adve rbs, we may look into their synonyms, but it is difficult to give a quantitative similarity score between them. http://developer.yahoo.com/answers/ threads is huge, it becomes infeasible for annotators to go through all to check their similarities. To ease this, we employ the K-means text clustering method to first group similar answers. The rationale behind this is base d on the assumption that two questions are considered to be similar if their answers are similar. The answer groups thus help to find corresponding similar questions 4 . Among these clusters, we diversely choose 20 representative groups for each sub-category, in order to ensure well coverage on topics in each domain. A series of simple BoW-based retrievals are then performed on each group to get in more potentially similar questions. We believe the resulted question groups, which have potentially cove red both lexical and semantic similarity, are the good starting point for the tagging task. 
The tagging results, together with the dataset statistics, are shown in Table 1. There is a total of 301,923 question threads from 6 sub-categories and on aver age 1.96 questions were asked per question thread (referred to as  X  Q Ratio X  ). Among all, annotators have tagged 120 (20x6) groups of similar questions, with a total of 10255 questions serving as the ground-truth for later evaluation. Table 1. Statistics of Dataset Collected for the Ground-Truth 
Each annotator was also asked to indicate the topic of each group of similar questions. We use these topics as a guidance to choose the testing questions for our evaluation. A total of 120 questions, which are considered to be close enough to its groups in the ground-truth, are carefully chosen from the testing set for testing. These questions are of various lengths and in various forms. Table 2 shows some exampl e queries from this testing set. We first index all the collected questions and answers from Yahoo! Answers. By given a user query, an initial BoW retrieval is carried out on question inde x, where different retrieval techniques such as term weighi ng and relevance feedback are applied. Top 100 of the initial retr ieval results (R_STM) are then selected, each of which is matched against the user query via the STM module. A re-ranked matching result is then produced. We In order to generate clusters w ith higher inner similarity, we set a very high threshold to filter out irrelevant ones. further perform the answer matching to bring in more similar questions (R_AM). Two sets of questions are fused with linear interpolation ( AM R STM R _ ) 1 ( _  X   X  +  X   X   X  ) to make up the final similar question searching result. Figure 3 presents an overview of our retrieval system. 
To evaluate the performance of our retrieval model, we use five different system combinations for comparison: 1) BoW (baseline1): A Bag-of-Word approach that simply 2) BoW+TK (baseline2): BoW integr ated with the original tree 3) BoW+STM: BoW approach combined with the Syntactic 4) BoW+STM+SEM: Matching m odel 3) with semantic 5) BoW+STM+SEM+AM: Matching model 4) with answer 
We employ two performance metrics: mean average precision (MAP 10 5 ) and precision at the top one retrieval result. The evaluation results are illustrated in Table 3. From the Table, we draw the following observations: 1) BoW model itself achieves very high precision (79.08), and MAP 10: The MAP calculated on the returned top 10 questions. 2) Applying syntactic tree matc hing over simple lexical 3) Semantic-smoothed matching performs better than pure 4) Answer matching (AM) brings in significant improvements 
To support our statement in Sec tion 3.4 that the STM is robust to grammatical errors, we conduct experiments in this Section to examine the effect of various grammatical errors on MAP. 
For meaningful comparison, we mimic a noisy environment by manually injecting various common errors that the human users are likely to make into all testing questions. 50% of the testing questions are randomly inserted with interior errors like article, tense, plurality errors, while the other 50% are modified to bear random exterior errors, such as preposition errors, misplaced modifier etc. Along with grammatical errors in our simulation, we also consider modifying some te rms or phrases into non-standard short forms that have been freque ntly used on the Web, such as  X  4  X  for  X  for  X ,  X  your  X  for  X  you are  X , and  X  Im  X  for  X  I am  X  etc. 
We re-run the five systems on th e modified testing dataset and plot the results of performance vari ations in Figure 4. The top of bars indicates the original performance without grammatical errors and the bottom show s the resulting performance. 
We can see from Figure 4(a) that the systems with STM integrated still outperform the BoW and TK systems by a large margin in terms of MAP even in noisy environment. The top one precision of STM-embedded systems does not degrade as much in general as compared to TK as s hown in Figure 4(b). In fact, the BoW+STM system even outperforms the BoW and BoW+TK systems in noisy environment. This is evidence that the syntactic tree matching model is sufficiently robust to various forms of grammatical errors. We expect th at in real world situation, a STM-based system would give ve ry satisfying matching results. 
Interestingly, the MAP for BoW-only system does not drop much as compared to the others. We believe this is owing to the fact that purely lexical based approaches do not take word relations into consideration, and thus it is less influenced by the grammatical errors. Figure 4. Illustration of Vari ations on a) MAP b) Top one Although we have shown that ST M, together with SEM and AM improves question matching, there is still plenty of room for improvement. To further characterize the types of questions that have no impact or are adversely affected by STM, we perform micro-level error analysis on the te sting question set. We find that STM fails to match the syntactic structures of questions mainly due to the following three reasons: 1) Mismatch of question topics: In some cases, two questions 2) Flexibility of question representations: In real world, many 3) Extremely long queries: There ar e also some cases that the 
The idea of finding similar questions in cQA is to some extent related to passage retrieval in traditional QA, with the exception that question-to-question matching is much stricter than question-to-passage matching. Many t echniques have been developed towards passage retrieval, from the simple BoW-based model, language model [16], to some st ate-of-the-art techniques like dependency relations [6] etc. Mo st of these techniques can be employed to match questions, but their precision is not high because of the high recall requirement. Training is needed in some works as well. 
Likewise, the FAQ retrieval task is also closely related to the question matching problem. Early work such as the FAQ finder [4] combined statistical similarity measure with semantic measure using WordNet to rank FAQs. Some recent works [15,17] used more advanced translation-base d approaches to retrieve FAQ data. [11] and [10] mined the FAQ data from the Web and implemented their own retrieval systems. However, the community-based QA archive is different from FAQ collections in the sense that the scope of manually created FAQs is quite limited. Recent res earch begins to focus on large scale QA services from the Web. Some works have been conducted on the characteristic analys is on this type of services such as [2], but limited effort has been devoted to the question matching direction. Works proposed in [9] and [18] applied the translation-based model to find se mantically similar questions in cQA. 
In this paper, we have presented a novel syntactic tree matching method for the similar questions finding problem. We assessed the system based on the ground-truth built from Yahoo! Answers, and the evaluation results showed that our system produced competitive improvements in matching performance as compared to the traditional BoW or plain tree kernel function: a 5~12% improvement in MAP, and up to 8% in top one precision. We introduced a comprehensive tree weighting scheme to not only give a faithful measure on question similarity, but also handle grammatical errors gracefully. We further improved the system performance by incorporating semantic features and the answer matching module. Unlike other systems, our model does not rely on training, making it easily portable to other similar retrieval systems. 
Our empirical evaluation results and qualitative error analysis revealed that the syntactic tree matching model could be improved by integrating question analysis module and domain ontology. Moreover, most off-the-s helf parsers, including the one we used in our experiments, are not well-trained to parse questions. We believe that a more targeted parser which is trained on question sets may give better accuracy. 
The retrieval system in this work only focuses on the single-sentence question matching problem, and uses the best answers as it is. In future research, multiple-sentence questions with different purposes are to be analyzed, and a tailored answer summarization technique is to be developed as well to produce high quality answers from different sources. [1] Trec proceedings. [3] S. Bloehdorn and A. Mosch itti. Structure and semantics for [4] R. D. Burke, K. J. Hammond, V. A. Kulyukin, S. L. Lytinen, [5] M. Collins and N. Duffy. Convolution kernels for natural [6] H. Cui, R. Sun, K. Li, M. -Y. Kan, and T.-S. Chua. Question [7] A. Diekema, X. Liu, J. Chen, H.Wang, N. Mccracken, O. [8] J. Gao, J.-Y. Ni e, G. Wu, and G. Ca o. Dependence language [9] J. Jeon, W. B. Cr oft, and J. H. Lee. Finding similar questions [10] V. Jijkoun and M. de R ijke. Retrieving answers from [11] Y.S. Lai, K.A. Fung, and C.-H. Wu. Faq mining via list [12] C. Leacock and M. Chodrow. Combining local context and [13] A. Moschitti. Efficient c onvolution kernels for dependency [14] A. Moschitti, S. Quarteroni , R. Basili, and S. Manandhar. [15] S. Riezler, A. Vasserman, I. Tsochantaridis, V. O. Mittal, [16] F. Song and W. B. Croft. A general language model for [17] R. Soricut and E. Brill. Automatic question answering: [18] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for [19] D. Zhang and W. S. Lee. Question classification using By the definition of the node matching score , we have: = As the weighting coefficient  X  has the recursive definition of relations of tree fragment is just one level a bove its children X  X , which gives D = D k -1 , we may finally get the following: 
