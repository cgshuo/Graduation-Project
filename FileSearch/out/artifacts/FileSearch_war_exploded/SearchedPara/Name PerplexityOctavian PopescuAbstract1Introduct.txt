 The Person Cross Document Corefer ence (PCDC) task which requires that all and only the textual mentions o f an entity of type Person be individ u-ated in a large collection of text doc u ment s , is a challenging tasks for natural language pro c essing system s (Grishman 1994 ) . A PCDC sy s tem must be able to use the information existing in the co r-pus in order to assign to each pers on name me n tion (PNM) a piece of context relevant for corefe r ence. In many cases , the contextual info r mation relevant for coreference is very scarce or embedded in s e-mantic and ontological deep infe r ences, which are difficult to progra m , anyway . 
Unlike in other d isambiguation tasks, like word sense disambiguation for instance, where the di s-tribution of relevant contexts is mainly regulated by strong syntactic rules, in PCDC the relevance of contexts is a matter of interdependency. To exe m-plify, consider the name  X  John Smith X  and an o r-ganization, say  X  X .N. X . The context  X  X orks for U.N. X  is a relevant coreference context for  X  X ohn Smith X  if there is just one person named John Smith working for U.N.; if there are two or more John Smiths working for U.N., then  X  X orks for U.N. X  is no longer a relevant context for corefe r-ence. For the PCDC task, the relevance of the co n-text depends to a great extent on the diversity of the corpus itself, rather than on the specific rel a-tionship that exists between  X  X ohn Smith X  and  X  X orks for U.N. X . 
Valid coreference can be realized when a large amount of information is available. However, the requirement that only contextually provable coreferences be realized is too strong; the required relevant context is not actually explicitly found in the text in at least 60% of the times (Popescu 2007 ). 
This paper presents a statistical technique deve l-oped to give a PCDC system more information regarding the probability of a correct coreference, without performing deep semantic and ontological ana lyses. If a PCDC system knows that the prior probability for two PNMs to corefer is high, then the amount of contextual evidence required can be lowered and vice -versa. Our goal is to precisely define a statistical model in which the prior coreference prob abilities can be computed and, consequently, to design a PCDC system that d y-namically revises the context relevance accor d-ingly.

We review the PCDC literature relevant for our purposes , present the statistical model and show the preliminary results. The pa per ends with the Conclusion and Further Research section. In a classical paper (Bagga 1998 ), a PCDC system based on the vector space model (VSM) is pr o-posed. While there are many advantages in repr e-senting the context as vectors on which a s imilarity function is applied, it has been shown that there are inherent limitations associat ed with the vectorial model (Popescu 2008 ). These problems, related to the density in the vectorial space (superposition) and to the discriminative power of the si milarity power (masking), become visible as more cases are con sidered. (Gooi, 2004 ), testing the system on many names, empirically observes the variance in the results obtained by the same PCDC system. Indeed, considering just the sentence level context, w hich is a strong requirement for establishing coreference, a PCDC system obtains a good score for  X  X ohn Smith X . This is because the probability of coreference of any two  X  X ohn Smith X  mentions is low. But , as the relevant context is often outside the sentenc e containing the mention, for other types of names the same system is not accurate. If it considers, for instance,  X  X arack Obama X , the same system obtains a very low recall, as the pro b-ability of any two  X  X arack Obama X  mentions to corefer is very high. Wit hout further adjustments, a vectorial model cannot resolve the problem of co n-sidering too much or too little contextual evidence in order to obtain a good precision for  X  X ohn Smith X  and simultaneously a good recall for  X  X  a-rack Obama X . 
The relationship be tween the prior probabilities and the accuracy of a system is also em pirically noted in (Pederson 2005 ). In their experiment, the authors note that having in the input of the system the correct number of person s carrying the same name is likely to hurt the results of a system based on bi grams . This happens because the amount of context is statically considered. The variance in the results obtained by a PCDC sys tem has been noted also in (Lefever 2007, Popescu 2007 ) .

In order to improve the performances of PCDC systems based on VSM, some authors have f o-cused on methods that allow a better analysis of the context (Ng 2007 ) combined with a cascade cluster ing technique (Wei 2006 ), or have relied on advanced clustering tech niques (Chen 2006 ).

The technique we p resent in the next section is complementary to these approaches. We propose a statistical model designed to offer to the PCDC systems information regarding the dis tribution of PNMs in the corpus. This information is used to reduce the contextual data varia tion and to attain a good balance between precision and recall. The amount of contextual information required for the coreference of two or more PNMs depends on several factors. Our working hypothesis is that we can compute a prior probability of coreference for each name and use this probability to control the amount of contextual evidence required. Let us recall the  X  X ohn Smith X  and  X  X arack Obama X  e x-ample from the previous section. Both  X  X ohn X  and  X  X mith X  are American common first and last names. The chance that many different persons carry this name is high. On the other hand, as both  X  X arack X  and  X  X bama X  are rare American first and last names respectively, almost surely many me n-tions of this name refer only to one person. The arg ument above does not depend on the context, but just on the prior estimation of the usage of those names. Computing an estimation of a name X  X  f requency class, we may decrease or increase the amount of contextual evidence needed accordingly . 
To each one -token name we assoc iate the nu m-ber of different tokens with which it forms a PNM in the corpus. For example , for  X  X ohn X  we can have the set  X  X mith X ,  X  X . Kennedy X ,  X  X ravolta X  etc. We call this number the perplexity of a one -token name. The perplexity gives a direct estimation of the ambiguity of a name in the corpus. In Table 1 we present the relationship between the number of occurrences (in intervals, in the first column) and the average perplexity (second column). The fi g-ures reported here, as well as those in the next Se c-ti on, come from the investigation of the Adige500k, an Ital ian news corpus (Magnini 2006 ). occurrences (interval) average perplexity 1 -5 4.13 6 -20 8.34 21 -100 17.44 101 -1,000 68.54 1,000 -5,000 683.95 5,000 -31,091 478.23
We divide the class of one -token names in 5 categories according to their perplexity: very low, low, medium, high and very high. It is useful to keep separate the first and the last names. It has been shown that the average perplexity is thre e times lower for last names than for first names (Popescu 2007 ). Therefore, the first and last names perplexities play different roles in establishing the prior probability of coreference. The perplexity class of two -token names is computed using the foll owing heuristi cs: the perplexity class of two -token names is the average class of the perplexity of the one -token name s composing it . If the pe r-plexity class es of the one -token name s are the same, then the perplexity of the whole name is one class less (if possible).

The p erplexity classes represent a pa rti ti on of the name population; each name belongs to one and only one class. In establishing the border b e-tween two consecutive perplexity classes , we want to maximize the confidence that inside each str a-tum the prior coreference probability has a low variance. 
The relationship between the perplexity classes and the prior coreference probability is straigh t-forward. The lower the perplexity, the greater the coreference probability, and, therefore, the lower t he amount of relevant context required for coreference. 
In order to decide the percentage of the name population that goes into each of the perplexity class es , we use a distributional free statistics method. In this way we can compute the conf i-dence of th e prior conference probability estimates.
We introduce two random variables: X , a ra n-dom variable defined over the name population and Y, which represents the number of different persons carrying the same name. Let X 1 ,...,X n be a random sample of names from one perplexity class, and let Y 1 ,...,Y n be the corresponding values denoting the number of person s that carry the names X 1 ,...,X n . The indices have been chosen such that Y 1 ...,Y n is an ordered statistics: Y  X  Y 2  X  ...  X  Y n . Let F be the distribution function of Y. And let p be a given probability. If F(Y j ) -F(Y  X  p , then at least 100p percent of the probability distribution is between Y i and Y j ; it means that is the probability that the interval (Y i, Y j ) contains 100p percent of the Y values. In our case ,  X  is the confidence of the estimation that 100p percent of names from a certain perple x-ity class have the expected prior coreference pro b-ability in a given interval.
 The  X  probability is computed with the formula: where  X  is the extension of the factorial function ,  X 
In practice , we start with an interval that repr e-sents the prior coreference probability desired for that perplexity class. For exam ple we want to be  X  = 80% sure that p = 90% of the two -token names in the  X  X ery low X  perplexity class are names ca r-ried by a maximum of 2 persons. We choose a ra n-dom sample of two -token names from that perplexity class, the size of the random sample b e-ing determined by  X  and p  X  see equation (2). If the random sample satisfies (1) then we have the d e-sired perplexity class. If not, the one -token names that have the highest perplexity and were consi d-ered  X  X ery low X  are excluded  X  th ey are assigned to the next perplexity class -and the computation is re made .

In a preliminary experiment, using a sample of 25 two -token names from a part of the Adige500k corpus spanning two years , we have obtained the perplexity classes listed in Tabl e s 2 and 3. In Adige 500k there are 106, 192 different one -token names, which combine in to 429, 251 different two -token name s and 36, 773 three -token names. perplexity class percentage very high 5.3% H igh 8.7% M edium 20.9%
L ow 2 7.6 % very low 37.5 % perplexity class percentage very high 1.8 % H igh 3. 3 6 % M edium 17.51 %
L ow 20.31 % very low 57.02 %
The perplexity class of two -token names is computed as specified in the first pa ragraph of this page. In approximately 60% of the cases, a two -token name has a  X  X ow X , or  X  very low X  perplexity class. If a PCDC system computes the context similarity based on words with special properties or on named entities, in general at least four si m i-larities must be detected between two contexts in order to have a safe coreference. Our prelim inary results show that coreferr ing on the basis of just one special word and one named entity for those names in  X  X ow X  or  X  X ery low X  does not lose more than 1 , 5% in preci sion, while it gains up to 40 % in r e call for these cases. On the other hand , for  X  X ery high X  perplexity two -token names we were able to increase precision by requiring a strong er simila r-ity between context s . 
The gain of using prior coreference probabilities determined by the perplexity classes is important, especi ally for those names that are situated at the extreme:  X  X ery low X  perplexity with a big number of occurrences and  X  X ery high X  with a small nu m-ber o f occurrences. These cases establish t he inte r-val for the amount of contextual simil arity required for coreference.

However, the problematic cases remain when the perplexity class is  X  X ery high X  and the number of occurrences is very big. We have presented a di stributional free statistical method to design a name perplexity system, such that each perplexity class maximizes the number of names for which the prior coreference belongs to the same interval. This information helps the PCDC systems to lower/increase a dequately the amount of contextual evidence required for coreference.

In our preliminary experiment we have observed that we can adequately reduce the amount of co n-textual evidence required for the coreference of  X  X ow X  and  X  X ery low X  perplexity class. For the top perplexity class names the requirement for extra contextual evidence has increased the precision. 
The approach presented here is effective in dea l-ing with the problems raised by using a similarity metrics on contextual vectors. It gives a direct w ay of identify ing the most problematic cases for coreference. Solving these cases represent s our first objective for the future.

We plan to increase the number of cases consi d-ered in the sample required to delimit the perple x-ity classes. The equation (2) may be deve l oped further in order to obtain exactly the number of required cases for each perplexity class .
 A. Bagga, B. Baldwin.1998. Entity -based Cross -Document Co -referencing using the Vector Space Model , In Proceedings ACL.
 J. Chen, D. Ji, C. Tan, Z. Niu .2006. Unsupervised Relation Disambiguation Using Spectral Cluste r X  ing , In Proceedings of COLING C. Gooi, J. Allan.2004. Cross -Document Corefe r-ence on a Large Scale Corpus , in Proceeding ACL. R. Grishman.1994. Whither Written Language Evaluation? In proceedings H uman Language Technology Workshop, 120 -125. San Mateor.
 E. Lefever , V. Hoste , F. Timur .2007. AUG: A Combined Classification and Clustering Approach for Web People Disambiguation , In Proceed ings of SemEval B. Magnini, M. Speranza, M. Negri, L. Romano, R. Sp rugnoli. 2006.I -CAB  X  the Italian Content Annotation Bank. LREC 2006 V., Ng.2007. Shallow Semantics for Coreference Resolution , In Proceedings of IJCAI T. Pedersen, A. Purandare , A. Kulkarni . 2005. Name Discrimination by Clustering Similar Co n-texts , in Pro ceeding of CICLING O. Popescu, C. Girardi, 2008, Improving Cross Document Coreference , in Proceedings of JADT O. Popescu, B. Magnini.2007, Inferring Corefe r-ence among Person Names in a Large Corpus of News Collection , in Proceedings of AIIA O. Popescu, B. Magnini.2007. Irst -bp: WePS using Named Entities , In Proceedings of SEMEVAL O. Popescu, M. Magnini, L. Serafini, A. Tamilin, M. Speranza.2006. From Mention to Ontology: a Pilot Study , in Proceedings of SWAP Y. Wei, M. Lin , H. Chen.2006. Name Disambigu a X  tion in Person Information Mining , In Procee d X  ings of IEEE
