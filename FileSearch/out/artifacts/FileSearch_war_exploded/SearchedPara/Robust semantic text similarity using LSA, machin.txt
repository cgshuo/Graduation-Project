 Abhay Kashyap 1  X  Lushan Han 1  X  Roberto Yus 2  X  Jennifer Sleeman 1  X  Taneeya Satyapanich 1  X  Sunil Gandhi 1  X  Tim Finin 1 Abstract Semantic textual similarity is a measure of the degree of semantic equivalence between two pieces of text. We describe the SemSim system and its performance in the *SEM 2013 and SemEval-2014 tasks on semantic textual similarity. At the core of our system lies a robust distributional word similarity component that combines latent semantic analysis and machine learning augmented with data from several linguistic resources. We used a simple term alignment algorithm to handle longer pieces of text. Additional wrappers and resources were used to handle task specific challenges that include processing Spanish text, com-paring text sequences of different lengths, handling informal words and phrases, and matching words with sense definitions. In the *SEM 2013 task on Semantic Textual Similarity , our best performing system ranked first among the 89 submitted runs. In the SemEval-2014 task on Multilingual Semantic Textual Similarity , we ranked a close second in both the English and Spanish subtasks. In the SemEval-2014 task on Cross-Level Semantic Similarity , we ranked first in Sentence X  X hrase, Phrase X  X ord, and Word X  X ense subtasks and second in the Paragraph X  X entence subtask.
 Keywords Latent semantic analysis WordNet Term alignment Semantic similarity Semantic textual similarity (STS) is a measure of how close the meanings of two text sequences are Agirre et al. ( 2012 ). Computing STS has been a research subject in natural language processing, information retrieval, and artificial intelligence for many years. Previous efforts have focused on comparing two long texts (e.g., for document classification) or a short text with a long one (e.g., a search query and a document), but there are a growing number of tasks that require computing the semantic similarity between two sentences or other short text sequences. For example, paraphrase recognition (Dolan et al. 2004 ), tweets search (Sriram et al. 2010 ), image retrieval by captions (Coelho et al. 2004 ), query reformulation (Metzler et al. 2007 ), automatic machine translation evaluation (Kauchak and Barzilay 2006 ), and schema matching (Han et al. 2012 , 2015 ; Han 2014 ), can benefit from STS techniques.

There are three predominant approaches to computing short text similarity. The first uses information retrieval X  X  vector space model (Meadow 1992 ) in which each piece of text is modeled as a  X  X  X ag of words X  X  and represented as a sparse vector of word counts. The similarity between two texts is then computed as the cosine similarity of their vectors. A variation on this approach leverages web search results (e.g., snippets) to provide context for the short texts and enrich their vectors using the words in the snippets (Sahami and Heilman 2006 ). The second approach is based on the assumption that if two sentences or other short text sequences are semantically equivalent, we should be able to align their words or expressions by meaning. The alignment quality can serve as a similarity measure. This technique typically pairs words from the two texts by maximizing the summation of the semantic similarity of the resulting pairs (Mihalcea et al. 2006 ). The third approach combines different measures and features using machine learning models. Lexical, semantic, and syntactic features are computed for the texts using a variety of resources and supplied to a classifier, which assigns weights to the features by fitting the model to training data (Saric et al. 2012 ).

In this paper we describe SemSim , our semantic textual similarity system. Our approach uses a powerful semantic word similarity model based on a combination of latent semantic analysis (LSA; Deerwester et al. 1990 ; Landauer and Dumais 1997 ) and knowledge from WordNet (Miller 1995 ). For a given pair of text sequences, we align terms based on our word similarity model to compute its overall similarity score. Besides this completely unsupervised model, it also includes supervised models from the given SemEval training data that combine this score with additional features using support vector regression. To handle text in other languages, e.g., Spanish sentence pairs, we use Google Translate API 1 to translate the sentences into English as a preprocessing step. When dealing with uncommon words and informal words and phrases, we use the Wordnik API 2 and the Urban Dictionary to retrieve their definitions as additional context.

The SemEval tasks for semantic textual similarity measure how well automatic systems compute sentence similarity for a set of text sequences according to a scale definition ranging from 0 to 5, with 0 meaning unrelated and 5 meaning semantically equivalent (Agirre et al. 2012 , 2013 )). For the SemEval-2014 workshop, the basic task was expanded to include multilingual text in the form of Spanish sentence pairs (Agirre et al. 2014 ) and additional tasks were added to compare text snippets of dissimilar lengths ranging from paragraphs to word senses (Jurgens et al. 2014 ). We used SemSim in both *SEM 2013 and SemEval-2014 competitions. In the *SEM 2013 Semantic Textual Similarity task , our best performing system ranked first among the 89 submitted runs. In the SemEval-2014 task on Multilingual Semantic Textual Similarity , we ranked a close second in both the English and Spanish subtasks. In the SemEval-2014 task on Cross-Level Semantic Similarity , we ranked first in Sentence X  X hrase, Phrase X  X ord and Word X  Sense subtasks and second in the Paragraph X  X entence subtask.

The remainder of the paper proceeds as follows. Section 2 gives a brief overview of SemSim explaining the SemEval tasks and the system architecture. Section 3 presents our hybrid word similarity model. Section 4 describes the systems we used for the SemEval tasks. Section 5 discusses the task results and is followed by some conclusions and future work in Sect. 6 . In this section we present the tasks in the *SEM and SemEval workshops that motivated the development of several modules of the SemSim system. Also, we present the high-level architecture of the system introducing the modules developed to compute the similarity between texts, in different languages and with different lengths, which will be explained in the following sections. 2.1 SemEval tasks description Our participation in SemEval workshops included the *SEM 2013 shared task on Semantic Textual Similarity and the SemEval-2014 tasks on Multilingual Semantic Textual Similarity and Cross-Level Semantic Similarity . This section provides a brief description of the tasks and associated datasets.

Semantic textual similarity The semantic textual similarity task was introduced in the SemEval-2012 Workshop (Agirre et al. 2012 ). Its goal was to evaluate how well automated systems could compute the degree of semantic similarity between a pair of sentences. The similarity score ranges over a continuous scale [0, 5], where 5 represents semantically equivalent sentences and 0 represents unrelated sentences. For example, the sentence pair  X  X  X he bird is bathing in the sink. X  X  and  X  X  X irdie is washing itself in the water basin. X  X  is given a score of 5 since they are semantically equivalent even though they exhibit both lexical and syntactic differences. However the sentence pair  X  X  X ohn went horseback riding at dawn with a whole group of friends. X  X  and  X  X  X unrise at dawn is a magnificent view to take in if you wake up early enough. X  X  is scored as 0 since their meanings are completely unrelated. Note a 0 score implies only semantic unrelatedness and not opposition of meaning, so  X  X  X ohn loves beer X  X  and  X  X  X ohn hates beer X  X  should receive a higher similarity score, probably a score of 2.

The task dataset comprises human annotated sentence pairs from a wide range of domains (Table 1 shows example sentence pairs). Annotated sentence pairs are used as training data for subsequent years. The system-generated scores on the test datasets were evaluated based on their Pearson X  X  correlation with the human-annotated gold standard scores. The overall performance is measured as the weighted mean of correlation scores across all datasets.

Multilingual textual similarity The SemEval-2014 workshop introduced a subtask that includes Spanish sentences to address the challenges associated with multilingual text (Agirre et al. 2014 ). The task was similar to the English task but the scale was modified to the range [0, 4]. 3 The dataset comprises 324 sentence pairs from Spanish Wikipedia selected from a December 2013 dump of Spanish Wikipedia. In addition, 480 sentence pairs were extracted from 2014 newspaper articles from Spanish publications around the world, including both Peninsular and American Spanish dialects. Table 2 shows some examples of sentence pairs in the datasets (we included a possible translation to English in brackets). No training data was provided.

Cross-Level Semantic Similarity The Cross-Level Sentence Similarity task was introduced in the SemEval-2014 workshop to address text of dissimilar length, namely paragraphs, sentences, words and word senses (Jurgens et al. 2014 ). The task had four subtasks: Paragraph X  X entence (compare a paragraph with a sentence), Sentence X  X hrase (compare a sentence with a phrase), Phrase X  X ord (compare a phrase with a word) and Word X  X ense (compare a word with a WordNet sense). The dataset for the task was derived from a wide variety of genres, including newswire, travel, scientific, review, idiomatic, slang, and search. Table 3 shows a few example pairs for the task. Each subtask used a training and testing dataset of 500 text pairs each. 2.2 Architecture of SemSim The SemSim system is composed of several modules designed to handle the computation of a similarity score among pieces of text in different languages and of different lengths. Figure 1 shows its high-level architecture, which has two main modules, one for computing the similarity of two words and another for two text sequences. The latter includes submodules for English, Spanish, and text sequences of differing length.

At the core of our system is the Semantic Word Similarity Model , which is based on a combination of LSA and knowledge from WordNet (see Sect. 3 ). The model was created using a very large and balanced text corpus augmented with external dictionaries such as Wordnik and Urban Dictionary to improve handling of out-of-vocabulary tokens.

The Semantic Text Similarity module manages the different inputs of the system, texts in English and Spanish and with varying length, and uses the semantic word similarity model to compute the similarity between the given pieces of text (see Sect. 4 ). It is supported by subsystems to handle the different STS tasks:  X  The English STS module is in charge of computing the similarity between  X  The Spanish STS module computes the similarity between Spanish sentences  X  The Cross-Level STS module is used to produce the similarity between text
In the following sections we detail the previous modules and in Sect. 5 we show the results obtained by SemSim at the different SemEval competitions.
 Our word similarity model was originally developed for the Graph of Relations project (UMBC 2013 ) which maps informal queries with English words and phrases for an RDF linked data collection into a SPARQL query. For this, we wanted a similarity metric in which only the semantics of a word is considered and not its lexical category. For example, the verb  X  X  X arry X  X  should be semantically similar to the noun  X  X  X ife X  X . Another desiderata was that the metric should give highest scores and lowest scores in its range to similar and non-similar words, respectively. In this section, we describe how we constructed the model by combining LSA and WordNet knowledge and how we handled out-of-vocabulary words.
 3.1 LSA word similarity LSA word similarity relies on the distributional hypothesis that the words occurring in similar contexts tend to have similar meanings (Harris 1968 ). Thus, evidence for word similarity can be computed from a statistical analysis of a large text corpus. A good overview of the techniques for building distributional semantic models is given in Lapesa and Evert ( 2014 ), which also discusses their parameters and evaluation.
Corpus selection and processing A very large and balanced text corpus is required to produce reliable word co-occurrence statistics. After experimenting with several corpus choices including Wikipedia, Project Gutenberg e-Books (Hart 1997 ), ukWaC (Baroni et al. 2009 ), Reuters News stories (Rose et al. 2002 ), and LDC Gigawords, we selected the Web corpus from the Stanford WebBase project (Stanford ( 2001 )). We used the February 2007 crawl, which is one of the largest collections and contains 100 million web pages from more than 50,000 websites. The WebBase project did an excellent job in extracting textual content from HTML tags but still offers abundant text duplications, truncated text, non-English text and strange characters. We processed the collection to remove undesired sections and produce high quality English paragraphs. Paragraph boundaries were detected using heuristic rules and only paragraphs with at least two hundred characters were retained. We eliminated non-English text by checking the first twenty words of a paragraph to see if they were valid English words. We used the percentage of punctuation characters in a paragraph as a simple check for typical text. Duplicate paragraphs were recognized using a hash table and eliminated. This process produced a three billion word corpus of good quality English, which is available at Han and Finin ( 2013 ).

Word co-occurrence generation We performed part of speech (POS) tagging and lemmatization on the WebBase corpus using the Stanford POS tagger (Toutanova et al. 2000 ). Word/term co-occurrences were counted in a moving window of a fixed size that scanned the entire corpus. 4 We generated two co-occurrence models using window sizes  X  1 and  X  4 5 because we observed different natures of the models.  X  1 window produces a context similar to the dependency context used in Lin ( 1998 ). It provides a more precise context, but only works for comparing words within the same POS category. In contrast, a context window of  X  4 words allows us to compute semantic similarity between words with different POS tags.

Our experience has led us to conclusion that the  X  1 window does an effective job of capturing relations, given a good-sized corpus. A  X  1 window in our context often represents a syntax relation between open-class words. Although long distance relations can not be captured by this small window, this same relation can also appear as  X  1 relation. For example, consider the sentence  X  X  X ho did the daughter of President Clinton marry? X  X . The long distance relation  X  X  X aughter marry X  X  can appear in another sentence  X  X  X he married daughter has never been able to see her parent again X  X . Therefore, statistically, a  X  1 window can capture many of the relations the longer window can capture. While the relations captured by  X  1 window can be wrong, the state-of-the-art statistical dependency parsers can produce errors, too.
Our word co-occurrence models were based on a predefined vocabulary of about 22,000 common English words and noun phrases. The 22,000 common English words and noun phrases are based on the online English Dictionary 3ESL, which is part of the Project 12 Dicts . 6 We manullay exclude proper nouns from the 3ESL because there are not many of them and they are all ranked at the top places since proper nouns start with an uppercase letter. WordNet is used to assign part of speech tags to the words in the vocabulary because statistical POS parsers can generate incorrect POS tags to words. We also added more than 2,000 verb phrases extracted from WordNet. The final dimensions of our word co-occurrence matrices are 29,000 29,000 when words are POS tagged. Our vocabulary includes only open-class words (i.e., nouns, verbs, adjectives and adverbs). There are no proper nouns (as identified by Toutanova et al. 2000 ) in the vocabulary with the only exception of an exploit list of country names.

We use a small example sentence,  X  X  X  passenger plane has crashed shortly after taking off from Kyrgyzstan X  X  capital, Bishkek, killing a large number of those on board. X  X , to illustrate how we generate the word co-occurrence models. The corresponding POS tagging result from the Stanford POS tagger is shown in Fig. 2 . Since we only consider open-class words, our vocabulary for this small example will only contains 10 POS-tagged words in alphabetical order: board_NN, capital_NN, crash_VB, kill_VB, large_JJ, number_NN, passenger_NN, plane_NN, shortly_RB and take_VB. The resulting co-occurrence counts for the context windows of size  X  1 and  X  4 are shown in Tables 4 and 5 , respectively.
Stop words are ignored and do not occupy a place in the context window. For example, in Table 4 there is one co-occurrence count between  X  X  X ill_VB X  X  and  X  X  X arge_JJ X  X  in the  X  1 context window although there is an article  X  X  X _DT X  X  between them. Other close-class words still occupy places in the context window although we do not need to count co-occurrences when they are involved since they are not in our vocabulary. For example, the co-occurrences count is zero between  X  X  X hort-ly_RB X  X  and  X  X  X ake_VB X  X  in Table 4 since they are separated by  X  X  X fter_IN X  X  that is not a stop word. The reasons we only choose three articles as stop words are (1) they have high frequency in text; (2) they have few meanings; (3) we want to keep a small number of stop words for  X  1 window since close-class words can often isolate unrelated open-class words; and (4) we want to capture the relation in the pattern  X  X  X erb ? the (a, an) ? noun X  X  that frequently appear in text for  X  1 window, e.g.,  X  X  X now the person X  X .

SVD transformation Singular value decomposition (SVD) has been found to be effective in improving word similarity measures (Landauer and Dumais 1997 ). SVD is typically applied to a word by document matrix, yielding the familiar LSA technique. In our case we apply it to our word by context_word matrix. In literature, this variation of LSA is sometimes called HAL (Hyperspace Analog to Language; Burgess et al. 1998 ).
Before performing SVD, we transform the raw word co-occurrence count f ij to its log frequency log  X  f ij  X  1  X  . We select the 300 largest singular values and reduce the 29 K word vectors to 300 dimensions. The LSA similarity between two words is defined as the cosine similarity of their corresponding word vectors after the SVD transformation.

LSA similarity examples Table 6 shows ten examples obtained using LSA similarity. Examples 1 to 6 show that the metric is good at differentiating similar from non-similar word pairs. Examples 7 and 8 show that the  X  4 model can detect semantically similar words even with different POS while the  X  1 model yields much worse performance. Examples 9 and 10 show that highly related but not substitutable words can also have a strong similarity, but the  X  1 model has a better performance in discriminating them. We call the  X  1 model concept similarity and the  X  4 model relation similarity , respectively, since the  X  1 model performs best on nouns and the  X  4 model on relational phrases, regardless of their structure (e.g.,  X  X  X arried to X  X  and  X  X  X s the wife of X  X ). 3.2 Evaluation TOEFL synonym evaluation We evaluated the  X  1 and  X  4 models on the well-known 80 TOEFL synonym test questions. The  X  1 and  X  4 models correctly answered 73 and 76 questions, respectively. One question was not answerable because the question word,  X  X  X alfheartedly X  X , is not in our vocabulary. If we exclude this question,  X  1 achieves an accuracy of 92.4 % and  X  4 achieves 96.2 %. It is worth mentioning that this outstanding performance is obtained without testing on the TOEFL synonym questions beforehand.
 According to ACL web page on state-of-the-art performance to TOEFL Synonym Questions, 7 our result is second-to-best among all corpus-based approaches. The best performance was achieved by Bullinaria and Levy with a perfect accuracy but it was the result of directly tuning their model on the 80 questions. It is interesting that our models can perform so well, especially when our approach is much simpler than Bullinaria and Levy X  X  approach ( 2012 ) or Rapp X  X  approach ( 2003 ). There are several differences between our system and existing approaches. First, we used a 3-billion word corpus with high quality, which is larger than corpora used by Bullinaria and Levy (2-billion) or Rapp (100 million). Second, we performed POS tagging and lemmatization on the corpus. Third, our vocabulary includes only open-class or content words.

WordSimilarity-353 evaluation WordSim353 is another dataset that is often used to evaluate word similarity measures (Finkelstein et al. 2002 ). It contains 353 word pairs, each of which is scored by 13 to 16 human subjects on a scale from 0 to 10. However, due to the limited size of the vocabulary that we used, some words in the WordSim353 dataset (e.g.,  X  X  X rafat X  X ) are outside of our vocabulary. Therefore, we chose to use a subset of WordSimilarity353 that had been used to evaluate a state-of-the-art word similarity measure PMI max (Han et al. 2013 ) because both share the same 22,000 common English words as their vocabulary. This subset contains 289 word pairs. The evaluation result, including both the Pearson correlation and Spearman rank correlation coefficients, is shown in Table 7 . Since WordSim353 is a dataset more suitable for evaluating semantic relatedness than semantic similarity, we only evaluate the  X  4 model on it. The LSA  X  4 model performed better than PMI max in both Pearson and Spearman correlation.

If the 289 pairs could be thought as a representative sample of the 353 pairs and if we could thereby compare our result with the state-of-the-art evaluations on WordSimilarity-353 (ACLwiki 2015 ), the Pearson correlation of 0.642 produced by our model is ranked as the second to best among all state-of-the-art approaches.
Comparison with Word2Vec For the final evaluation, we also compared our LSA models to the Word2Vec (Mikolov et al. ( 2013 )) word embedding model on the TOEFL synonym questions. We used the pre-trained model also published by Mikolov et al. ( 2013 ), which was trained on a collection of more than 100 billion words from Google News. The model contains 300-dimensional vectors, which has the same dimension as our models. One question word  X  X  X ranquillity X  X  is not in the pre-trained model X  X  vocabulary. 8 Therefore, both sides have one question not answerable. The results of both systems are shown in Table 8 Our LSA models X  performance is significantly better than the Word2Vec model, at a 95 % confidence level using the Binomial Exact Test.
 3.3 Combining with WordNet knowledge Statistical word similarity measures have limitations. Related words can have similarity scores only as high as their context overlap, as illustrated by  X  X  X octor X  X  and  X  X  X ospital X  X  in Table 6 . Also, word similarity is typically low for synonyms having many word senses since information about different senses are mixed together (Han et al. 2013 ). We can reduce the above issues by using additional information from WordNet.

Boosting LSA similarity using WordNet. We increase the similarity between two words if any relation in the following eight categories hold: 1. They are in the same WordNet synset. 2. One word is the direct hypernym of the other. 3. One word is the two-link indirect hypernym of the other. 4. One adjective has a direct similar to relation with the other. 5. One adjective has a two-link indirect similar to relation with the other. 6. One word is a derivationally related form of the other. 7. One word is the head of the gloss of the other or its direct hypernym or one of 8. One word appears frequently in the glosses of the other and its direct hypernym. Notice that categories 1 X 6 are based on linguistic properties whereas categories 7 X 8 are more heuristic. We use the algorithm described in Collins ( 1999 ) to find a word gloss header. 9
We define a word X  X   X  X  X ignificant senses X  X  to deal with the problem of WordNet trivial senses. The word  X  X  X ear X  X , for example, has a sense  X  X  X  body of students who graduate together X  X  which makes it a synonym of the word  X  X  X lass X  X . This causes problems because this is not the most significant sense of the word  X  X  X ear X  X . A sense WordNet frequency count is at least five; or (3) its word form appears first in its synset X  X  word form list and it has a WordNet sense number less than eight.
We require a minimum LSA similarity of 0.1 between the two words to filter out noisy data when extracting relations in the eight WordNet categories because some relations in the categories are not semantically similar. One is because our heuristic definition of  X  X  X ignificant sense X  X  sometimes can include trivial sense of a word, therefore, noise. The second reason is that the derivational form of a word is not necessarily semantically similar to the word. The third reason is that the category 7 and 8 can produce incorrect results sometimes.

We adopt the concept of path distance from Li et al. ( 2003 ) and assign path distance of zero to the category 1, path distance of one to the categories 2, 4, and 6, and path distance of two to categories 3, 5, 7, and 8. Categories 1 X 6 follow the definition in Li et al. but 7 and 8 are actually not related to path distance. However, in order to have a uniform equation to compute the score, we assume that these two categories can be boosted by a score equivalent to path length of 2, which is longest path distance in our eight categories. The new similarity between words x and y is computed by combining the LSA similarity and WordNet relations as shown in the following equation, where D ( x , y ) is the minimal path distance between x and y . Using e a D  X  x ; y  X  to transform simple shortest path length has been demonstrated to be very effective by Li et al. ( 2003 ). The parameter a is set to 0.25, following their experimental results.
Dealing with words of many senses For a word w with many WordNet senses (currently ten or more), we use its synonyms with fewer senses (at most one third of that of w ) as its substitutions in computing similarity with another word. Let S x and S y be the set of all such substitutions of the words x and y respectively. The new similarity is obtained using Eq. 2 .
Finally, notice that some of the design choices in this section were made by intuition, observations, and a small number of sampled and manually checked experiments. Therefore some of the choices/parameters have a heuristic nature and may be suboptimal. The reason why we do so is because many parameters or choices in defining eight categories,  X  X  X ignificant senses X  X , and similarity functions are intertwined. Due to the number of them, justifying every design choice requires many detailed experiments, which we cannot offer in this paper. The LSA ?-WordNet model was originally developed for Graph of Relation project or a schema-agnostic graph query system, as we pointed earlier in the paper. The advantage of LSA ? WordNet model with these intuitive and pragmatic design choices over the pure LSA model has been demonstrated in the experiments on the graph query system (Han 2014 ). An online demonstration of a similar model developed for the GOR project is available (UMBC 2013 ). 3.4 Out-of-vocabulary words Our word similarity model is restricted by the size of its predefined vocabulary [set of ( token, part-of-speech ) tuples]. Any word absent from this set is called an out-of-vocabulary (OOV) word. For these, we can only rely on their lexical features like character unigrams, bigrams, etc. While the vocabulary is large enough to handle most of the commonly used words, for some domains OOV words can become a significant set. Table 9 shows the fraction of OOV words in a random sample of 500 sentences from the STS datasets when Stanford POS tagger was used. Table 10 gives some example OOV words.

The most common type of OOV words are names (proper nouns) and their adjective forms. For example, the noun  X  X  X alestine X  X  and its adjective form  X  X  X alestinian X  X . This is apparent from Table 9 where some datasets have high OOV nouns and adjectives due to the nature of their domain. For instance, MSRpar and headlines are derived from newswire and hence contain a large number of names of people, places, and organizations. Other OOV words include lexical variants ( X  X  X ummarise X  X  vs.  X  X  X ummarize X  X ), incorrectly spelled words ( X  X  X terprise X  X ), hyphen-ated words (multi-words), rare morphological forms, numbers, and time expressions. Since our vocabulary is actually a set of ( token, part-of-speech ) tuples, incorrect POS tagging also contributes to OOV words.

Since OOV words do not have a representation in our word similarity model, we cannot compare them with any other word. One approach to alleviate this is to include popular classes of OOV words like names into our vocabulary. For highly focused domains like newswire or cybersecurity, this would be a reasonable and useful approach. For instance, it would be useful to know that  X  X  X ozilla Firefox X  X  is more similar to  X  X  X oogle Chrome X  X  than  X  X  X dobe Photoshop X  X . However, for generic models, this would vastly increase the vocabulary size and would require large amounts of high quality training data. Also, in many cases, disambiguating named entities would be better than computing distributional similarity scores. For instance, while  X  X  X arack Obama X  X  is somewhat similar to  X  X  X oe Biden X  X  in that they both are politicians and democrats, in many cases it would be unacceptable to have them as substitutable words. Another approach is to handle these OOV terms separately by using a different representation and similarity function.
Lexical OOV word similarity We built a simple wrapper to handle OOV words like proper nouns, numbers and pronouns. We handle pronouns heuristically and check for string equivalence for numbers. For any other open class OOV word, we use simple character bigram representations. For example, the name  X  X  X arack X  X  would be represented by the set of bigrams { X  X a X , X  X r X , X  X a X , X  X c X , X  X k X  X . To compare a pair of words, we compute the Jaccard coefficient of their bigrams and then threshold the value to disambiguate the word pair. 10
Semantic OOV word similarity. Lexical features work only for word disambigua-tion and make little sense when computing semantic similarity. While many OOV words come from proper nouns, for which disambiguation works well, languages continue to grow and add new words into their vocabulary. To simply use lexical features for OOV words would restrict the model X  X  ability to adapt to a growing language. For example, the word  X  X  X oogle X  X  has become synonymous with  X  X  X earch X  X  and is often used as a verb. Also, as usage of social media and micro-blogging grows, new slang, informal words, abbreviations come into existence and become a part of common discourse.

As these words are relatively new, including them in our distributional word similarity model might yield poor results, since they lack sufficient training data. To address this, we instead represent them by their dictionary features like definitions, usage examples, or synonyms retrieved from external dictionaries. Given this representation, we can then compute the similarity as the sentence similarity of their definitions (Han et al. 2013 ; Kashyap et al. 2014 ). To retrieve reliable definitions, we need good sources of dictionaries with vast vocabulary and accurate definitions. We describe two dictionary resources that we used for our system.

Wordnik Davidson ( 2013 ) is an online dictionary and thesaurus resource that includes several dictionaries like the American Heritage dictionary, WordNet, and Wikitionary. It exposes a REST API to query their dictionary, although the daily usage limits for the API adds a bottleneck to our system. Wordnik has a large set of unique words and their corresponding definitions for different senses, examples, synonyms, and related words. For our system, we use the top definition retrieved from the American Heritage dictionary and Wikitionary as context for an OOV word. We take the average of the sentence similarity scores between these two dictionaries and use it as our OOV word similarity score. Usually the most popular sense for a word is Wordnik X  X  first definition. In some cases, the popular sense was different between the American Heritage Dictionary and Wikitionary which added noise. Even with its vast vocabulary, several slang words and phrases were found to be absent in Wordnik. For this, we used content from the Urban Dictionary.
Urban Dictionary Urban ( 2014 ) is a crowd-sourced dictionary source that has a large collection of words and definitions added by users. This makes it a very valuable resource for contemporary slang words and phrases. Existing definitions are also subject to voting by the users. While this helps accurate definitions achieve a higher rank, popularity does not necessarily imply accuracy. We experimented with Urban Dictionary to retrieve word definitions for the Word2Sense subtask as described in Sect. 4.3.2 . Due to the crowd-sourced nature of the resource, it is inherently noisy with sometimes verbose or inaccurate definitions. For instance, the top definition of the word  X  X  X rogrammer X  X  is  X  X  X n organism capable of converting caffeine into code X  X  whereas the more useful definition  X  X  X n individual skilled at writing software code X  X  is ranked fourth. In this section, we describe the various algorithms developed for the *SEM 2013 and SemEval-2014 tasks on semantic textual similarity. 4.1 English STS We developed two systems for the English STS task, which are based on the word similarity model explained in Sect. 3 . The first system, PairingWords , uses a simple term alignment algorithm. The second system combines these scores with additional features for the given training data to train supervised models. We apply two different approaches to these supervised systems: an approach that uses generic models trained on all features and data ( Galactus/Hulk ) and an approach that uses specialized models trained on domain specific features and data ( Saiyan/SuperSaiyan ).

Preprocessing We initially process the text by performing tokenization, lemmatization and part of speech tagging, as required by our word similarity model. We use open-source tools from Stanford coreNLP (Finkel et al. 2005 ) and NLTK (Bird 2006 ) for these tasks. Abbreviations are expanded by using a complied list of commonly used abbreviations for countries, North American states, measurement units, etc.

Term alignment Given two text sequences, we first represent them as two sets of relevant keywords, S 1 and S 2 . Then, for a given term t 2 S 1 , we find its counterpart in the set S 2 using the function g 1 as defined by where sim  X  t ; t 0  X  is the semantic word similarity between terms t and t 0 .
Similarly, we define the alignment function g 2 for the other direction as: g 1  X  t  X  and g 2  X  t  X  are not injective functions so their mappings can be many-to-one. This property is useful in measuring STS si milarity because two sentences are often not exact paraphrases of one anoth er. Moreover, it is often nece ssary to align multiple terms in one sentence to a single term in the o ther sentence, such as when dealing with repetitions and anaphora, for example, mappi ng  X  X  X eople writing books X  X  to  X  X  X riters X  X .
Term Weights Words are not created equal and vary in terms of the information they convey. For example, the term  X  X  X ardiologist X  X  carries more information than the term  X  X  X octor X  X , even though they are similar. We can model the information content of a word as a function of how often it is used. For example, the word  X  X  X octor X  X  is likely to have a larger frequency of usage when compared to  X  X  X ardiologist X  X  owing to its generic nature. The information content of a word w is defined as where C is the set of words in the corpus and freq ( w ) is the frequency of the word w in the corpus. For words absent in the corpus, we use average weight instead of maximum weight. It is a safer choice given that OOV words also include misspelled words or lexical variants.

Term alignment score We compute a term alignment score between any two text sequences as where S 1 and S 2 are the sets of relevant key words extracted from their corresponding text, t represents a term, ic ( t ) represents its information content and g 1  X  t  X  or g 2  X  t  X  represents its aligned counterpart. The mean function can be the arithmetic or the harmonic mean. 4.1.1 PairingWords: align and penalize We hypothesize that STS similarity between two text sequences can be computed using where T is the term alignment score, P 0 is the penalty for bad alignments, and P 00 is the penalty for syntactic contradictions led by the alignments. However P 00 had not been fully implemented and is not used in our STS systems. We show it here just for completeness.

We compute T from Eq. 6 where the mean function represents the arithmetic mean. The similarity function sim 0  X  t ; t 0  X  is a simple lexical OOV word similarity back-off (see Sect. 3.4 ) over sim ( x , y ) in Eq. 2 that uses the relation similarity .
We currently treat two kinds of alignments as  X  X  X ad X  X , as described in Eq. 8 . The similarity threshold h in defining A i should be set to a low score, such as 0.05. For the set B i , we have an additional restriction that neither of the sentences has the form of a negation. In defining B i , we used a collection of antonyms extracted from WordNet (Mohammad et al. 2008 ). Antonym pairs are a special case of disjoint sets. The terms  X  X  X iano X  X  and  X  X  X iolin X  X  are also disjoint but they are not antonyms. In order to broaden the set B i we will need to develop a model that can determine when two terms belong to disjoint sets.
 We show how we compute P 0 in the following: w  X  t  X  and w p  X  t  X  terms are two weighting functions on the term t . w f  X  t  X  inversely weights the log frequency of term t and w p  X  t  X  weights t by its part of speech tag, assigning 1.0 to verbs, nouns, pronouns and numbers, and 0.5 to terms with other POS tags. 4.1.2 Galactus/Hulk and Saiyan/Supersaiyan: supervised systems The PairingWords system is completely unsupervised. To leverage the availability of training data, we combine the scores from PairingWords with additional features and learn supervised models using support vector regression (SVR). We used LIBSVM (Chang and Lin 2011 ) to learn an epsilon SVR with a radial basis kernel and ran a grid search provided by Saric et al. ( 2012 ) to find the optimal values for the parameters cost, gamma and epsilon. We developed two systems for the *SEM 2013 STS task, Galactus and Saiyan , which we used as a basis for the two systems developed for the SemEval-2014 STS task, Hulk and SuperSaiyan .
 Galactus and Saiyan: 2013 STS task We used NLTK X  X  (Bird 2006 ) default Penn Treebank based tagger to tokenize and POS tag the given text. We also used regular expressions to detect, extract, and normalize numbers, date, and time, and removed about 80 stopwords. Instead of representing the given text as a set of relevant keywords, we expanded this representation to include bigrams, trigrams, and skip-bigrams, a special form of bigrams which allow for arbitrary distance between two tokens. Similarities between bigrams and trigrams were computed as the average of the similarities of its constituent terms. For example, for the bigrams  X  X  X ouse ate X  X  and  X  X  X at gobbled X  X , the similarity score is the average of the similarity scores between the words  X  X  X ouse X  X  and  X  X  X at X  X  and the words  X  X  X te X  X  and  X  X  X obbled X  X . Term weights for bigrams and trigrams were computed similarly.

The aligning function is similar to Eq. 3 but is an exclusive alignment that implies a term can only be paired with one term in the other sentence. This makes the alignment direction independent and a one X  X ne mapping. We use refined versions of Google ngram frequencies (Michel et al. 2011 ), from Google ( 2008 ) and Saric et al. ( 2012 ), to get the information content of the words. The similarity score is computed by Eq. 6 where the mean function represents the Harmonic mean. We used several word similarity functions in addition to our word similarity model. Our baseline similarity function was an exact string match which assigned a score of 1 if two tokens contained the same sequence of characters and 0 otherwise. We also used NLTK X  X  library to compute WordNet based similarity measures such as Path Distance Similarity, Wu X  X almer similarity ( 1994 ), and Lin similarity ( 1998 ). For Lin similarity, the Semcor corpus was used for the information content of words. Our word similarity was used in concept, relation, and mixed mode (concept for nouns, relation otherwise). To handle OOV words, we used the lexical OOV word similarity as described in Sect. 3.4 .

We computed contrast features using three different lists of antonym pairs (Mohammad et al. 2008 ). We used a large list containing 3.5 million antonym pairs, a list of about 22,000 antonym pairs from WordNet and a list of 50,000 pairs of words with their degree of contrast. The contrast score is computed using Eq. 6 but the sim  X  t ; t 0  X  contains negative values to indicate contrast scores.
We constructed 52 features from different combinations of similarity metrics, their parameters, ngram types (unigram, bigram, trigram and skip-bigram), and ngram weights (equal weight vs. information content) for all sentence pairs in the training data:  X  We used scores from the align-and-penalize approach directly as a feature.  X  Using exact string match over different ngram types and weights, we extracted  X  We used the LSA-boosted similarity metric in three modes: concept similarity  X  For WordNet-based similarity measures, we used uniform weights for Path and  X  Contrast scores used three different lists of antonym pairs. A total of six features
The Galactus system was trained on all of the STS 2012 data and used the full set of 52 features. The Saiyan system employed data-specific training and features. More specifically, the model for headlines was trained on 3000 sentence pairs from MSRvid and MSRpar, SMT used 1500 sentence pairs from SMT europarl and SMT news, while OnWN used only the 750 OnWN sentence pairs from last year. The FnWN scores were directly used from the Align-and-Penalize approach. None of the models for Saiyan used contrast features and the model for SMT also ignored similarity scores from exact string match metric.

Hulk and SuperSaiyan: 2014 STS task We made a number of changes for the 2014 tasks in an effort to improve performance, including the following:  X  We looked at the percentage of OOV words generated by NLTK (Table 11 ) and  X  While one X  X ne alignment can reduce the number of bad alignments, it is not  X  To address the high fraction of OOV words, the semantic OOV word similarity  X  The large number of features used in 2013 gave marginally better results in
In addition to these changes Galactus was renamed Hulk and trained on a total of 3750 sentence pairs (1500 from MSRvid, 1500 from MSRpar and 750 from headlines). Datasets like SMT were excluded due to poor quality. Also, Saiyan was renamed SuperSaiyan . For OnWN, we used 1361 sentence pairs from previous OnWN dataset. For Images, we used 1500 sentence pairs from MSRvid dataset. The others lacked any domain specific training data so we used a generic training dataset comprising 5111 sentence pairs from MSRvid, MSRpar, headlines and OnWN datasets. 4.2 Spanish STS Adapting our English STS system to handle Spanish similarity would require to select a large and balanced Spanish text corpus and the use of a Spanish dictionary (such as the Multilingual Central Repository Gonzalez-Agirre et al. 2012 ). As a baseline for the SemEval-2014 Spanish subtask, we first considered translating the Spanish sentences to English and running the same systems explained for the English subtask (i.e., PairingWords and Hulk ). The results obtained applying this approach to the provided training data were promising (with a correlation of 0.777). So, instead of adapting the systems to Spanish, we performed a preprocessing phase based on the translation of the Spanish sentences (with some improvements) for the competition (see Fig. 3 ).
 Translating the sentences To automatically translate sentences from Spanish to English we used the Google Translate API, 11 a free, multilingual machine translation product by Google. It produces very accurate translations for European languages by using statistical machine translation (Brown et al. 1990 ), where the translations are generated on the basis of statistical models derived from bilingual text corpora. Google used as part of this corpora 200 billion words from United Nations documents that are typically published in all six official UN languages, including English and Spanish.

In the experiments performed with the trial data, we manually evaluated the quality of the translations (one of the authors is a native Spanish speaker) and found the overall translations to be very accurate. Some statistical anomalies were noted that were due to incorrect translations because of the abundance of a specific word sense in the training set. On one hand, some homonym and polysemous words are wrongly translated which is a common problem of machine translation systems. For example, the Spanish sentence  X  X  X as costas o costa de un mar [ ... ] X  X  was translated to  X  X  Costs or the cost of a sea [ ... ] X  X . The Spanish word costa has two different senses:  X  X  X oast X  X  (the shore of a sea or ocean) and  X  X  X  X ost X  X  (the property of having material worth). On the other hand, some words are translated preserving their semantics but with a slightly different meaning. For example, the Spanish sentence  X  X  U n coj X   X n es una funda de tela [ ... ] X  X  was correctly translated to  X  X  A cushion is a fabric cover [ ... ] X  X . However, the Spanish sentence  X  X  X na almohada es un coj X   X n en forma rectangular [ ... ] X  X  was translated to  X  X  X  pillow is a rectangular pad [ ... ] X  X . 12
Dealing with statistical anomalies The aforementioned problem of statistical machine translation caused a slightly adverse effect when computing the similarity of two English (translated from Spanish) sentences with the systems explained in Sect. 4.1 . Therefore, we improved the direct translation approach by taking into account the different possible translations for each word in a Spanish sentence. For that, we used Google Translate API to access all possible translations for every word of the sentence along with a popularity value. For each Spanish sentence the system generates all its possible translations by combining the different possible for sentence SPASent1 ). For example, Fig. 4 shows three of the English sentences generated for a given Spanish sentence from the trial data.

To control the combinatorial explosion of this step, we limited the maximum number of generated sentences for each Spanish sentence to 20 and only selected words with a popularity &gt;65. We arrived at the popularity threshold through experimentation on every sentence in the trial data set. After this filtering, our input for the  X  X  X ews X  X  and  X  X  X ikipedia X  X  tests (see Sect. 2.1 ) went from 480 and 324 pairs of sentences to 5756 and 1776 pairs, respectively.

Computing the similarity score The next step was to apply the English STS system to the set of alternative translation sentence pairs to obtain similarity scores final similarity score for the original Spanish sentences. An intuitive way to combine the similarity scores would be to select the highest, as it would represent the possible translations with maximal similarity. However, it could happen that the pair with highest score have one or even two bad translations. We used this approach in our experiments with the trial data and increased the already high correlation from 0.777 to 0.785. We also tried another approach, computing the average score. Thus, given a pair of Spanish sentences, SPASent1 and SPASent2 , and the set of possible translations generated by our system for each sentence, TrSent 1  X f TS 1 1 ; ... ; TS 1 n g and TrSent 2  X f TS 2 1 ; ... ; TS 2 m g , we compute the similarity between them by using the following formula: where SimENG ( x , y ) computes the similarity of two English sentences using our English STS systems. Using this approach and the trial data we increased the correlation up to 0.8. Thus, computing the average instead of selecting the best similarity score obtained a higher correlation for the trial data. Using a set of possible translations and averaging the similarity scores can help to reduce the effect of wrong translations. We also speculate that the average behaved slightly better because it emulates the process followed to generate the gold standard, which was created by averaging the similarity score assigned to each pair of Spanish sentences by human assessors. 4.3 Cross-Level STS The SemEval-2014 task on Cross-Level Semantic Similarity presented a new set of challenges. For the Paragraph X  X entence and Sentence X  X hrase subtask, we used the systems developed for the English STS with no modifications. However, the Phrase X  X ord and Word X  X ense subtasks are specific to very short text sequences. If the constituent words are relatively rare in our model, there may not be sufficient discriminating features for accurate similarity computation. In the extreme case, if these words are not present at all in our vocabulary, the system would not be able to compute similarity scores. Table 12 shows the OOV fraction of the 1000 words from training and test datasets. While rare and OOV words can be tolerated in longer text sequences where the surrounding context would capture the relevant semantics, in tasks like these, performance is severely affected by lack of context. 4.3.1 Phrase to word As a baseline, we used the PairingWords system on the training set which yielded a very low correlation of 0.239. To improve performance, we used external resources to retrieve more contextual features for these words. Overall, there are seven features: the baseline score from the PairingWords system, three dictionary based features, and three web search based features.

Dictionary features In Sect. 3.4 we described the use of dictionary features as representation for OOV words. For this task, we extend it to all the words. To compare word and phrase, we retrieve a word X  X  definitions and examples from Wordnik and compare these with the phrase using our PairingWords system. The maximum score when definitions and examples are compared with the phrase constitutes two features in the algorithm. The intuition behind taking the maximum is that the word sense which is most similar to the phrase is selected.
Following the 2014 task results, it was found that representing each phrase as a bag of words fared poorly for certain datasets, for example slang words and idioms. To address this, we added a third dictionary based feature based on the similarity of all Wordnik definitions of the phrase with all of the word X  X  definitions using our PairingWords system. This yielded a significant improvement over our results submitted in SemEval task as shown in Sect. 5 .

Web search features Dictionary features come with their own set of issues, as described in Sect. 3.4 . To overcome these shortcomings, we supplemented the system with Web search features. Since search engines provide a list of documents ranked by relevancy, an overlap between searches for the word, the phrase and a combination of the word and phrase, is evidence of similarity. This approach supplements our dictionary features by providing another way to recognize polysemous words and phrases, e.g., that  X  X ava X  can refer to coffee, an island, a programming language, a Twitter handle, and many other things. It also addresses words and phrases that are at different levels of specificity. For example,  X  X bject oriented programming language X  is a general concept and  X  X ava object oriented programming language X  is more specific. A word or phrase alone lacks any context that could discriminate between meanings (Miller 1995 ; Ravin and Leacock 2000 ). Including additional words or phrases in a search query provides context that supports comparing words and phrases with different levels of specificity.
Given the set A and a word or phrase p A where p A 2 A , a search on A will result in a set of relevant documents D A . Given the set B and a word or phrase p B where p
B 2 B , if we search on B , we will receive a set of relevant documents for B given by D B . Both searches will return what the engine calculates as relevant given A or B . So if we search on a new set C which includes A and B , such that p A 2 C and p B 2 C then our new search result will be what the engine deems relevant, D C .If p A is similar to p B and neither are polysemous, then D A , D B and D C will overlap, shown in Fig. 5 a. If either p A or p B is polysemous but not both, then there may only be overlap between either A and C or B and C , shown in Fig. 5 b. If both p A and p B are polysemous, then the three document sets may not overlap, shown in Fig. 5 c. In the case of a polysemous word or phrase, the overlapping likelihood is based on which meaning is considered most relevant by the search engine. For example, if p A is  X  X bject oriented programming X  and p B is  X  X ava X , there is a higher likelihood that  X  X bject oriented programming X  ?  X  X ava X , p C , will overlap with either p A or p B if  X  X ava X  as it relates to  X  X he programming language X  is most relevant. However, if  X  X ava X  as it relates to  X  X offee X  is most relevant then the likelihood of overlap is low.
Specificity also affects the likelihood of overlap. For example, even if the search engine returns a document set that is relevant to the intended meaning of  X  X ava X  or p , the term is more specific than  X  X bject oriented programming X  or p A and therefore less likely to overlap. However since the combination of  X  X bject oriented programming X  ?  X  X ava X  or p C acts as a link between p A and p B , the overlap between A , C and B , C could allow us to infer similarity between A and B or similarity specifically between  X  X bject oriented programming X  and  X  X ava X .
We implemented this idea by comparing results of three search queries: the word, phrase, and the word and phrase together. We retrieved the top five results for each search using the Bing Search API 13 and indexed the resulting document sets in Lucene ( 2004 ) to obtain term frequencies for each search. For example, we created an index for the phrase  X  X pill the beans X , an index for the word  X  X onfess X , and an index for  X  X pill the beans confess X . Using term frequency vectors for each we calculated the cosine similarity of the document sets. In addition, we also calculated the mean and minimum similarity among document sets. The similarity scores, the mean similarity, and minimum similarity were used as features, in addition to the dictionary features, for the SVM regression model. We evaluated how our performance improved given our different sets of features. Table 19 shows the cumulative effect of adding these features. 4.3.2 Word to sense For the Word to Sense evaluation, our approach is similar to that described in the previous section. However, we restrict ourselves to only dictionary features since using Web search as a feature is not easily applicable when dealing with word senses. We will use an example word author#n and a WordNet sense Dante#n#1 to explain the features extracted. We start by getting the word X  X  synonym set, W i , from WordNet. For the word author#n , W i comprises writer.n.01 , generator.n.03 and author.v.01 . We then retrieve their corresponding set of definitions, D i . The WordNet definition of writer.n.01 ,  X  X  X rites (books or stories or articles or the like) professionally (for pay) X  X  2 D i . Finally, we access the WordNet definition of the sense, SD . In our example, for Dante#n#1 , SD is  X  X  X n Italian poet famous for writing the Divine Comedy that describes a journey through Hell and purgatory and paradise guided by Virgil and his idealized Beatrice (1265 X 1321) X  X .

We use the PairingWords system to compute similarities and generate the following four features, where n is the number of word X  X  synonym set. Since approximately 10 % of the input words fall out of WordNet X  X  vocabulary, we supplement this by using Wordnik and reduce the OOV words to about 2 %. In the process, we create two additional features: where DK  X  D n  X  its top five Wordnik definitions.

Following the SemEval-2014 task results, we experimented with additional definitions and dictionary resources like Urban Dictionary , generating two more features: where the DU i and DW i are the word X  X  definitions from Urban Dictionary and Wordnik, respectively.

Table 13 shows the Pearson X  X  correlation between the individual features and the corresponding gold standard scores for the given training set of 500 Word X  X ense pairs. We used features F 1  X  F 6 to train SVM regressions models to compute the similarity scores for the evaluation dataset. In the following we present the results of our system in the *SEM 2013 and SemEval-2014 competitions. First, we show the results for the English STS task in *SEM 2013 and SemEval-2014. Then, we show the results for the Spanish STS task in SemEval-2014. Finally, we explain the results for the 2014 Cross-Level STS task.
English STS The *SEM 2013 task attracted a total of 89 runs from 35 different teams. We submitted results from three systems to the competition: PairingWords , Galactus , and Saiyan . Our best performing system, PairingWords , ranked first overall (see Table 14 ) while the supervised systems, Galactus and Saiyan , ranked second and fourth, respectively. On the one hand, the unsupervised system, PairingWords , is robust and performs well on all domains. On the other hand, the supervised and domain specific system, Saiyan , gives mixed results. While it ranks first on the headlines dataset, it drops to 36 on OnWN (model trained on 2012 OnWN data, see Table 14 ) where it achieved a correlation of 0.56. Galactus  X  model for headlines (trained on MSRpar, MSRvid) was used on OnWN and the correlation improved significantly from 0.56 to 0.71. This shows the fragile nature of these trained models and the difficulty in feature engineering and training selection since in some cases they improved performance and added noise in other cases.
The English STS task at SemEval-2014 included 38 runs from 15 teams. We presented three systems to the competition: PairingWords , Hulk , and SuperSaiyan . Our best performing system ranked a close second overall, 14 behind first place by only 0.0005. Table 15 shows the official results for the task. The supervised systems, Hulk and SuperSaiyan , fared slightly better in some domains. This can be attributed to the introduction of named entity recognition and semantic OOV word similarity. deft-news and headlines are primarily newswire content and contain a significant number of names. Also, deft-news lacked proper casing. An interesting dataset was tweet-news which had meta information in hashtags. These hashtags often contain multiple words that are in camel case. As a preprocessing step, we only stripped out the  X # X  symbol and did not tokenize camel cased hashtags.
While there are slight improvements from supervised models on some specific domains, the gains are small when compared to the increase in complexity. In contrast, the simple unsupervised PairingWords system is robust and consistently performs well across all the domains.

Spanish STS The SemEval-2014 Spanish STS task included 22 runs from nine teams. The results from our three submitted runs are summarized in Table 16 . The first used the PairingWords system with the direct translation of the Spanish sentences to English. The second used the extraction of the multiple translations of each Spanish sentence and the PairingWords system. The third used the Hulk system with the direct translation. Our best run achieved a weighted correlation of 0.804, behind first place by only 0.003. Notice that the approach selected based on the automatic translation of the Spanish sentences obtained good results for both datasets.
 The  X  X  X ews X  X  dataset contained sentences in both Peninsular and American Spanish dialects (the American Spanish dialect contains more than 20 subdialects). These dialects are similar but some of the words included in each are only used in some regions and the meaning of other words differ. The use of Google Translate, which handles Spanish dialects, and the computing of the average of the similarity of the possible translations helped us to increase the correlation in both datasets.
The results for the  X  X  X ikipedia X  X  dataset are slightly worse due to the large number of named entities in the sentences, which PairingWords cannot handle. However, we note that the correlation obtained by the two Spanish native speakers used to validate the dataset was 0.824 and 0.742, respectively (Agirre et al. 2014 ). Our best run obtained a correlation of 0.743 for this dataset which means that our system behaved as well as a native speaker for this test. Finally, the Hulk system, which was similar to the Pairing run and used only the direct translation per sentence, achieved better results as it is able to handle the named entities in both datasets. After the competition, we applied the Hulk system with the multiple translations of each sentence generated by our approach, obtaining a correlation score of 0.8136, which would make the system first in the real competition.
Cross-Level STS The 2014 Cross-Level STS task included 38 runs from 19 teams. The results from our submitted runs are summarized in Table 17 . For the Paragraph X  X entence and Sentence X  X hrase tasks we submitted three runs each which utilized the PairingWords , Hulk , and SuperSaiyan systems. For the Phrase X  X ord and Word X  X ense tasks we submitted a single run each based on the features explained in Sects. 4.3.1 and 4.3.2 , respectively. It is interesting to note the drop in correlation scores with the size of the text sequences. The domain-specific system SuperSaiyan ranked first in the Sentence X  X hrase task and second in the Paragraph X  Sentence task. This could be attributed to its specially trained model on the given 500 training pairs and the presence of a significant number of names.
 The Phrase X  X ord run achieved a correlation of 0.457, the highest for the subtask. Table 18 shows some examples where our system performed poorly. Our performance was slightly worse for slang and idiomatic categories when compared to others because the semantics of idioms is not compositional, reducing the effectiveness of a distributional similarity measure. After the competition we explored the inclusion of a third dictionary based feature to solve this problem (as explained in Sect. 4.3.1 ). The overall correlation for the subtask increased from 0.457 to 0.581 with the addition of this feature. This increase in performance can be attributed to the increase in correlation for idiom and slang categories as seen in Fig. 6 . We also conducted ablation experiments to identify critical features and understand the relationship between dictionary and search features. Table 19 shows the improvement in correlation scores with the addition of features. Figure 7 compares the correlation with gold standard across different categories between search and dictionary features.

We limited the documents we processed to the top five search results while computing web search features. As a separate task, we conducted another experiment which focused on comparing search result sizes. We trained a model for each search result size (1, 3, 5, 7, 10, 12) using the given training dataset. We then tested and measured our performance using the test data set as conveyed in Table 20 . Future experiments will extend this test further to determine the number of documents at which performance is negatively affected. As can be seen in Fig. 8 , the general trend indicates performance gain with increase in the number of search results.
 The Word X  X ense run ranked first in the subtask with a correlation score of 0.389. Table 21 gives some examples where the system performed poorly. We noticed that the top definition of Wordnik is not always reliable. For example, the first definition of cheese#n is  X  X  X  solid food prepared from the pressed curd of milk X  X  but there is a latter, less prominent one, which is  X  X  X oney X  X . Another issue is the absence of some words like wasteoid#n in Wordnik.

Following the task results, we included Urban Dictionary as a dictionary source and included additional features (as explained in Sect. 4.3.2 ). Instead of using trained models, we also experimented with simple average of highly correlated features. Table 22 shows the correlation scores of different combinations of features. We noticed a slight increase in our score after including Urban Dictionary . Also we observed better scores when we used a simple average of highly correlated features. We described the resources and algorithms that we developed for our SemSim system and its performance in the *SEM 2013 and SemEval-2014 evaluation tasks for semantic textual similarity. In the 2013 task, we ranked first out of 89 submitted runs. In the 2014 task on Multilingual Semantic Similarity , we ranked second in both English and Spanish subtasks. In the 2014 task on Cross-Level Semantic Similarity , we ranked first in Sentence X  X hrase, Phrase X  X ord and Word X  X ense subtasks while ranking second in the Paragraph X  X entence subtask. Our strong performances can be attributed to a powerful distributional word similarity model based on LSA and WordNet knowledge.

Our unsupervised PairingWords system employed a simple term alignment algorithm and achieved robust performance across all datasets. There was interest in including syntactic features initially, but given the nature of the task, adding them would likely give minimal gains. While we achieved good scores for the Phrase X  Word and Word X  X ense subtasks, there is considerable room for improvement. Future improvements include better use of dictionary resources, especially when dealing with informal language. Also, the treatment of multiword expressions (MWE) could be improved as they are currently included in the dictionary and treated as single terms. Another area of application is to augment our word similarity model with domain specific similarity models, such as cybersecurity or healthcare.
 References
