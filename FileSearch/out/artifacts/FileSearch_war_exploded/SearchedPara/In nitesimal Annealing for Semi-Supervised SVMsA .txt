 First, the KKT optimality conditions of a condition-ally optimal solution is written as follows: Lemma 6 For a given  X  y 2 f 1 ; 1 g jUj , the necessary and sufficient conditions for a f to be the optimal so-lution of the convex problem (5) is (8) and We omit the proof of this lemma because they are straightforwardly derived by using Lagrange multiplier theory (Boyd &amp; Vandenberghe, 2004). Here, we just note that the derivation is almost same as the standard SVM case because the predicted labels  X  y are fixed here. Based on Lemma 6, we first prove Theorem 5.
 Proof of Theorem 5 Let f ^ y and f ^ y 0 be two condi-tionally optimal solutions defined in pol( X  y ) and pol( X  y respectively, and consider a situation that the former f ^ y is at a boundary of pol( X  y ). To prove the theorem, we suppose for the moment that it is also conditionally optimal in the next polytope pol( X  y 0 ), i.e., f ^ y = f Since f ^ y and f ^ y 0 are conditionally optimal, they satisfy the optimality condition (13): and respectively. From our current assumption that f ^ y = f 0 and the fact that (14) is rewritten as Now, it is clear that the two conditions (15) and (17) cannot be satisfied at the same time, and it disprove our assumption that f ^ y = f ^ y 0 .
 Noting that f ^ y 2 pol( X  y 0 ) and that it is not the con-ditionally optimal solution in pol( X  y 0 ), we immediately arrive at the conclusion that f ^ y 0 is a better S 3 VM so-lutions than f ^ y . Q.E.D.
 Next, we prove Lemma 3, which is immediately ob-tained from Lemma 6 and Theorem 5.
 Proof of Lemma 3 First, if the conditionally opti-mal solution f ^ y is in the strict interior of the convex polytope pol( X  y ), it is clear that there is no better solu-tion in the arbitrary neighborhood of f ^ y . It suggests that f ^ y is a local optimal solution of S 3 VM is it is in the strict interior of pol( X  y ). On the other hand, from Theorem 5, f ^ y is not a local optimal solution of S 3 VM because there exists a strictly better solution in the adjacent convex polytope pol( X  y 0 ). Combining the fact that f is conditionally optimal if and only if (8) and (13) are satisfied, it is clear that (8) and (9) are the necessary and sufficient conditions of a local optimal solution. Q.E.D.
 The computational cost of the entire algorithm (from C = 0 to C ) depends on the number of so-called breakpoints in the CP-step and the number of move-ments to adjacent polytopes in the DJ-step. It has been reported in many empirical studies (Efron &amp; Tib-shirani, 2004; Hastie et al., 2004) that the number of breakpoints is O ( n ), where n is the training set size. We also observed in our experiments that the total number of breakpoints in all CP steps scales almost linearly with respect to O ( jLj + jUj ).
 The main computational cost in each breakpoint is the same as that in the SVM regularization path (Hastie et al., 2004). That is, at each breakpoint, we need to solve a rank-one update problem of a linear system of equations of size O ( jMj ), which costs O ( jMj 2 ). On the other hand, the number of movements between two polytopes depends on the number of unlabeled instances. In our experience, this number also scales linearly with respect to O ( jUj ). Note that, if we use a warm-start strategy from the previous conditionally optimal solution, the computational cost of the DJ-
