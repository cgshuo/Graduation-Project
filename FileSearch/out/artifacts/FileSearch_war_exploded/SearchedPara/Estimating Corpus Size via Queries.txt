 We consider the problem of estimating the size of a collec-tion of documents using only a standard query interface. Our main idea is to construct an unbiased and low-variance estimator that can closely approximate the size of any set of documents defined by certain conditions, including that each document in the set must match at least one query from a uniformly sampleable query pool of known size, fixed in advance.

Using this basic estimator, we propose two approaches to estimating corpus size. The first approach requires a uni-form random sample of documents from the corpus. The second approach avoids this notoriously difficult sample gen-eration problem, and instead uses two fairly uncorrelated sets of terms as query pools; the accuracy of the second ap-proach depends on the degree of correlation among the two sets of terms.

Experiments on a large TREC collection and on three major search engines demonstrates the effectiveness of our algorithms.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Measurements Corpus Size, Random Sampling, Estimator Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00.
The overall quality of a web search engine is determined not only by the prowess of its ranking algorithm, but also by the caliber of its corpus, both in terms of comprehen-siveness (e.g. coverage of topics, language, etc) and refine-ment (e.g. freshness, avoidance of spam, etc). Obviously, comprehensiveness is not the same as size  X  for instance as suggested in [8] one can easily build a public server for 100 billion pages each representing a random combination of Nostradamus prophecies and a dedicated search engine can as easily index them, in a feat of utter insignificance. See also http://searchenginewatch.com/searchday/article. php/3527636 for a more serious discussion of comprehensive-ness versus size.

Nevertheless, the ability to produce accurate measure-ments of the size of a web search engine X  X  corpus, and of the size of slices of the corpus such as  X  X ll pages in Chinese indexed in Yahoo! from US-registered servers, X  is an impor-tant part of understanding the overall quality of a corpus. In the last ten years both the scientific literature and the popular press dealt at length with methodologies and esti-mates for the size of the various public web search engines and indirectly, the  X  X ize of the web X . (See Section 2.)
Perhaps surprisingly, the problem of estimating the size of a corpus slice is also important for the owners of search en-gines themselves. For example, even the simple problem of efficiently counting the size of a result set raises non-trivial problems. In fact, Anagnostopoulos, Broder, and Carmel [1] show a Google example using the queries george and washington where the identity | A  X  B | = | A | + | B | X  X  A  X  B | is off by 25%. (They present an algorithm that enables effi-cient sampling of search results, but to our knowledge, the required data structures have not been implemented in any web search engine.)
Further, the problems worsen when the goal is to esti-mate the effective size of a corpus slice; that is, the size of the slice discounted for pages that no longer exist, have been redirected, do not actually match the query, etc. In this case, even for a one term query, counting the number of documents in the term posting list gives only an upper bound of the effective size of the matching set.

An approach initiated by Bharat and Broder [6] is to pro-duce only relative sizes, that is, to estimate the ratio be-tween the sizes of several engines. Their approach is based on pseudo-uniform sampling from each engine index, first by sampling a query pool of English words, and then by sam-pling the results of the chosen query, followed by a capture-recapture estimates (that is, sample one engine, test contain-ment in the other); however their method is heavily biased in favor of  X  X ontent rich X  documents and thus the ultimate results are problematic. This has been recently corrected by Bar-Yossef and Gurevich [4], who present a truly uniform sampling method that at least in principle should yield ac-curate ratios, but their method still needs to test contain-ment, a non trivial procedure that introduces biases of its own. Note also that both the Broder and Bharat algorithm, as well as the Bar-Yossef and Gurevich algorithm yield only documents that match at least one query from their query pool, so in fact they only estimate the relative sizes of a certain subset of the corpus.

Estimating absolute corpus sizes appears to be a more difficult task. Previous work [13, 6] has had to fall back on sources other than public query interfaces, such as http:// searchenginewatch.com/reports/article.php/2156481 and http://searchengineshowdown.com/stats , or has had to trust the corpus size reported by search engines themselves. In this paper we propose a technique for estimating absolute corpus sizes without such additional sources.
As we stated earlier, the ability to estimate the size of the corpus and of its various slices is important in understand-ing different characteristics and the overall quality of the corpus. A method for estimating absolute corpus size using a standard query interface can lead to methods for estimat-ing the corpus freshness (i.e., fraction of up-to-date pages in the corpus), identifying over/under-represented topics in the corpus, measuring the prevalence of spam/trojans/viruses in the corpus, studying the comprehensiveness of the crawler that generated the corpus, and measuring the ability of the corpus to ably answer narrow-topic and rare queries [4]. Fi-nally, relative corpus size estimates offers competitive mar-keting advantage and bragging rights in the context of the web search engines.
Our method has two parts. First, we give a technique to count the size of specific broad subsets of the entire docu-ment corpus. Second, we show how to use this technique to generate an estimate of the size of the entire corpus. We begin our discussion of the method with an example. Assume that we wish to count the number of pages that con-tain an eight-digit number. A natural approach would be to produce a random sample of, say, 1 out of every 100,000 such numbers. One could then submit a query to a search engine for each number in the sample, count up the number of resulting documents, and multiply by 100,000. Unfor-tunately, some documents could contain many eight-digit numbers, and might therefore be counted multiple times by this procedure.

Let us modify the scheme as follows. Any document con-taining at least one eight-digit number should contribute 1 to the total count of documents in the set. But if a cer-tain document d contains k different eight-digit numbers, we will allocate its total count of 1 by distributing 1 /k to each of the eight-digit numbers it contains. In the previous scheme, when d is returned in response to a certain query, it contributes 1 to the overall count; in the new scheme it will contribute 1 /k , which is the reciprocal of the number of eight-digit numbers on the page.

Under the new scheme, we again take a sample of eight-digit numbers and submit each as a query. We then add up for each result document the reciprocal of the number of eight-digit numbers in that result document, and again multiply the final sum by 100,000. This new value is easily seen to be an unbiased estimator of the total number of documents; we provide a formal proof in Section 3.
More generally, our basic estimator allows us to count many different subsets D A of the entire corpus D . The scheme will apply whenever D A has two properties: first, a query pool A can be defined such that D A is the union of the results of all queries in the pool A . And second, for any document, it is possible to determine efficiently how many queries from the query pool would have produced the docu-ment as a result.

We have seen eight-digit numbers as an example query pool. One could also employ queries chosen carefully from a query log. We will also consider approaches to modifying the query pool on the fly in order to reduce the variance of the estimator. Finally, the techniques also allow us to filter the results of each query on the fly, for example, by entirely removing documents that contain more than max-imum threshold number of query terms from the pool, or that do not conform to a certain target length, specified as a range of bytes. The flexibility of the estimator is key to its power, as many issues that arise in the real world may be addressed by modifying the definition of D A .
We have now defined a basic estimator to count the size of a subset D A of the total corpus. This estimator may be used in many ways to perform counts of corpus sizes. The first method involves a random sample of the documents in a corpus, and an efficient function to determine whether a particular document belongs to D A . Given this, we may simply estimate from the random sample the probability p
A that a document from the corpus belongs to D A , and estimate the overall corpus size as | D A | /p A . We show in our experiments that this approach can provide very accurate measurements with quite reasonable sample sizes.

Nonetheless, it is quite difficult to generate a random sam-ple of pages on the web, despite the efforts of many authors. Worse yet, when a technique is chosen and applied, there are no clean approaches to estimating the quality of the result-ing sample, and so the sample bias cannot be understood. Even worse, because the problem of evaluating the quality of a random sample is so difficult, there has been very little academic work even to understand the extent to which tech-niques for generating random samples deviate from uniform.
Our basic estimator may also be applied in a second way to estimate the size of a corpus, without the presence of a random sample. We must resort to another assumption about the corpus, but one that is quite different from the assumption that a particular technique generates a uniform random sample. We assume that there are two query pools A and B that produce independent subsets D A and D B of the corpus. Note that independence here is different from disjointness. D A and D B may share documents, but the fraction of documents that belong to D A should be the same whether we consider the entire corpus, or just D B . If this is true, we may estimate the size of the corpus as | D A |  X  | D B | / | D A  X  D B | .

The accuracy of this method depends heavily on the inde-pendence assumption, a crucial question in all probabilistic IR models. If the terms are correlated then we can only produce bounds based on their correlation. The following example might help build the reader X  X  intuition for this is-sue. Assume that we apply our method using two sets of perfectly independent English terms and get a very accu-rate estimate of corpus size. Now the engine owners double its size by adding a large number of Chinese pages. If we repeat our experiments we will report the same number as before (since we will never or seldom see a Chinese page), even though the engine size has doubled. What happened? Well, our term sets used to be independent in the old corpus but now they are correlated: if we choose a page from D A it is now more likely to belong also to D B just by dint of being in English.

This might seem as a limitation of our method, but in fact all query based estimation methods proposed so far suffer from this query vocabulary bias, and the contribution of our paper is to give a methodology where this bias can be correctly quantified by relating it to a fundamental concept in probabilistic information retrieval [16].

In some ways, the meaning of uncorrelated sets is a philo-sophical one. The estimator may be viewed as returning an estimate of that part of the corpus in which A and B are uncorrelated. While we do not advocate this perspective, we observe that if the sets are chosen appropriately, this may be the part of the engine of most interest from a measure-ment standpoint or it can be used to infer the relative sizes of search engines.
We first apply our methods to a large TREC collection consisting of over 1.2 million documents. Since we know the exact corpus size, these experiments are designed to demon-strate the effectiveness of our methods. We choose the query pool to be the set of all five-digit numbers. For the approach using random document samples, we obtain fairly high accu-racies even with small number of samples. The error is sig-nificantly reduced when we modify the query pool to discard the most frequent terms in the query pool. We also estimate the corpus size using two uncorrelated query pools: set of five-digit numbers and a set of medium frequency words.
We then apply our methods to the Web by estimating what we call the  X  X isible X  corpus size of three major search engines. We choose the query pool to be the set of all eight-digit numbers and use this to estimate the visible corpus size. We also apply the two uncorrelated query pools approach: set of eight-digit numbers and a set of medium frequency words.
The paper is organized as follows. Section 2 discusses the related work on corpus size estimation and sampling random documents from a corpus. Section 3 presents the basic esti-mator and a variance reduction method. Section 4 presents our two approaches for corpus size estimation using the ba-sic estimator. Section 5 contains the experimental results for a large TREC collection and Section 6 contains the ex-perimental results for three major search engines. Section 7 concludes the paper.
Bharat and Broder [6] sampled the content of search en-gines using conjunctive and disjunctive queries composed of terms extracted from pages of the Yahoo! Directory. From a lexicon of 400K words, terms were combined to form around 35K queries. The sample consisted of one random URL cho-sen from the first 100 results returned by each queries. The fraction of the sampled pages from one search engine that are also present in the index of another search engine gives an estimate of the overlap between the two. The paper also estimates a search engine X  X  coverage of the whole web from overlaps of pairs of search engines. A known problem with this technique is that it is biased toward content-rich pages with high rank.

The same year, Lawrence and Giles [13] reported size esti-mates using random queries based on a log of queries submit-ted to a search engine. To avoid bias toward highly ranked pages they use only queries for which the search engine re-trieves all the results. However such query sampling does not provide a uniform random sample of the pages in the corpus. Later in [12], the authors extend the results and provide some other interesting statistics about search engine corpora and the accessible web. A big problem associated with directly comparing query results is that the approach is highly dependent on the underlying IR techniques used by the search engine to answer queries.

Guli and Signorini [10] repeated Bharat and Broder X  X  ex-periments using Open Directory ( www.dmoz.org ) as source of keywords. They also used a modified technique to com-bine more than 2M terms into queries.

A core primitive in search engine size estimation is a way to obtain a uniform sample from its corpus using the query interface. Several papers report techniques to uniformly sample a search engine corpus using only the public query in-terface [3, 11, 13, 15, 4]. Many of them are based on random walks. A good survey of the methodologies and techniques used for the Web size and search engine size estimation is in [2].

Recently, Bar-Yossef and Gurevich [4] propose two new methods to obtain an unbiased sample of a search engine cor-pus. The first method is based on sampling, where queries are generated as word phrases from a corpus of documents and queries that return too many results are rejected. At a very high level, the analytic ideas in this method are simi-lar to ours. The second method is based on random walks on documents and terms, but suitably unbiased using sta-tistical techniques in order to produce a provably uniform document sample.

An interesting related issue is the study of mirrored hosts or duplicate pages and there has been much work done in this area; see, for instance, [5, 7]. Duplication is an impor-tant issue affecting the search quality of search engines, but the focus of this paper will be on size estimates.
Liu, Yu, and Meng [14] propose a method for estimating the corpus size based on the idea of two independent sets. Let D 1 and D 2 be two independent random sample of doc-uments and let D 3 = D 1  X  D 2 . The search engine size is then estimated to be | D 1 || D 2 | / | D 3 | . This idea is somewhat related to our method of uncorrelated query pools.
Callan and Connell [9] proposed query-based sampling in the context of distributed information retrieval for acquiring  X  X esource descriptions X  that accurately represent the con-tents of each database without relying on their internals. This work is related to ours since we try to estimate cor-pus size based only on the search engine X  X  public API. Wu, Gibb, and Crestani [17] propose methods for estimating and maintaining archive size information.
In this section we discuss the basic estimator that will be the backbone of all our approaches. The mean of this unbi-ased estimator can be related to the corpus size. We assume that the index supports the basic query interface: given a query, return all the documents that match the query.
The first naive idea would be to use random queries and construct an estimator based on the number of documents returned for such queries. Unfortunately, this does not work, the difficulty being that there is no way of knowing the uni-verse of all queries. Without this knowledge, it may not be possible to obtain an unbiased estimator.

We circumvent this difficulty by working with a known and fixed set of queries, called a query pool . For simplicity of exposition, we assume that each query is just one term. However, our methods will apply to any query pool that satisfies two conditions: first, the size of the pool should be known, and second, it should be possible to determine for any document how many queries in the pool match this doc-ument. We show (Lemma 1) how to construct an estimator whose mean is the number of documents in the corpus that match at least one query from this query pool.
 Notation. Let D be the set of documents. We treat doc-uments as a set of terms and use the notation  X  a  X  d  X  to indicate that a term of query a occurs in the document d . A query pool is a set of terms. For a query pool A , let D
A  X  D be the set of documents such that every document in D A contains at least one term in A . Define the weight of a document d  X  D A with respect to A to be the inverse of the number of terms in A that occur in the document, i.e., The definition of D A guarantees that all weights are finite. The weight of a query a with respect to A is simply the weight of all documents containing the query, defined as follows:
Intuitively, if we encounter a document d which contains many query terms, each term should be given only partial credit for the document. Thus, we define our basic estimator as follows: i.e., the average weight of a query with respect to A .
We now show that this quantity times | A | is an unbiased estimator of the number of documents containing at least one term in A .
 Lemma 1.

Proof. Thus, Lemma 1 guarantees an unbiased estimator whose mean is | D A | / | A | . By sampling the query pool uniformly at random, | D A | can be estimated. We now discuss the issues in sampling.
All the estimators such as W A,D can be estimated by the usual sampling techniques. We will illustrate the method to estimate W A,D . Let X be the random variable given by P random from the query pool A . Clearly E [ X ] = W A,D . We pick k terms a 1 , . . . , a k independently and uniformly at ran-dom from A and estimate the quantity X i = P d  X  D for each of the a i  X  X . We then compute an averaged esti-mator X by averaging X 1 , . . . , X k . It is easy to see that E [ X ] = E [ X ] = W A,D . Using Chebyshev X  X  inequality, it follows that Using this expression, if k  X  (10 / 2 )var[ X ] /E 2 [ X ] for in-stance, then with probability at least 0.1, the averaged es-timator X approximates W A,D to within factor (1  X  ). To boost the probability of success from 0 . 1 to 1  X   X  for an arbi-trary  X  , we can compute O (log 1 / X  ) such averaged estimators and take their median value.

Ideally, we wish to make k as small as possible. To be able to do this, we need to make sure that E [ X ] is not too small. For instance, this means that we cannot pick the query pool A to be terms that occur very rarely, since this will make the estimation of p A harder. The second point to note is that if the variance var[ X ] is large, then it implies that k has to be large. We address the second issue in greater detail in the next section. The variance of the random variable X can be very large. As an example, consider the following extreme scenario. The query pool A decomposes into A 1 and A 2 so that | A 1 || A but each a 1  X  A 1 occurs in a large number of documents in D A and each such document contains only terms in A 1 . Consequently, the contribution to W A,D by a 1  X  A 1 is large. However, few random samples from A will hit A 1 , owing to its small cardinality. Therefore, the number of samples need to be high. In other words, the distribution corresponding to X can have a heavy tail and we need a lot of samples to hit the tail (we give some evidence of this in Figure 1).
We now illustrate a generic method to ameliorate the problem: identify the tail and truncate it. Let A 0  X  A be those queries whose weights contribute to the tail of X of mass  X  ; random sampling of documents in D , once again, can be used to identify candidate queries in A 0 . We then re-define a new query pool  X  A such that  X  A = A \ A 0 . The hope is that the random variable W  X  A,D has lower variance than W
A,D . We now proceed to formalize this and analyze con-ditions under which truncation of a random variable causes its variance to reduce.
 Notation. Let f be a probability distribution on an ordered domain U and let the random variable X  X  f . Consider the conditional random variable Y = X | X &lt;  X  , i.e., its distribution f | &lt; X  is given by truncating f at  X  and rescaling by the conditional mass Pr X  X  f [ X &lt;  X  ]. We would like to study var[ Y ] vs. var[ X ].

If f can be arbitrary, then there can be no relationship between var[ X ] and var[ Y ]. It is straightforward to con-struct an f such that var[ Y ] &lt; var[ X ]. With little effort, we can also construct an f such that var[ Y ] &gt; var[ X ]. Let f be a distribution with support of size three given by f (  X  ) = f ( ) =  X / 2 and f (1) = 1  X   X  , for parameters 0 &lt; ,  X  &lt; 1 to be specified later. Let  X  = 1 be the thresh-old. Let X  X  f and let Y = X | X &lt;  X  . Now, it is easy to see that E [ X ] = 1  X   X  and E [ X 2 ] = (1  X   X  ) +  X  2 and so On the other hand, E [ Y ] = 0 and var[ Y ] = E [ Y 2 ] = Hence, if &gt;
However, if f is monotonically non-increasing, then we show that var[ Y ]  X  var[ X ], i.e., truncation helps to reduce variance. In fact, in the extreme case, truncation can turn infinite variance into finite variance. When the distribution is a power law, we show a quantitative bound of the reduc-tion in variance.
For simplicity, let us assume f is a discrete monotonically non-increasing distribution. Without loss of generality, let the support of f be [ n ] with f (1)  X  X  X  X  X  X  f ( n ) and without loss of generality, let  X  = n . Let g = f | &lt; X  , and X  X  f, Y  X  Let  X  = E [ f ].

Lemma 2. f ( n )( n  X   X  ) 2  X  P n  X  1 i =1 ( g ( i )  X  f ( i ))( i  X   X  )
Proof. First, we show that for 1  X  i  X  n , or equivalently,  X   X  (1 + n ) / 2. Without loss of generality, assume n is odd and let n 0 = n/ 2. Let a 1 = min i&lt;n 0 f ( b n 0 c ) and let a 2 = max i&gt;n 0 f ( i ) = f ( d n Observe that  X  = n 0  X  P i ( n 0  X  i ) f ( i ) and
X and thus,  X   X  n 0 = n/ 2, establishing (4). Finally, Lemma 3. var[ Y ]  X  var[ X ] .

Proof. var[ X ] = where (*) follows since the convex function P i g ( i )( i  X  y ) is minimized at y = E [ g ].
We consider the important case when f is given by a power law. In this case we obtain a quantitative bound on the reduction of the variance. For simplicity, we assume that f is a continuous distribution defined on [1 ,  X  ). Let f ( x ) = Pr[ X = x ] =  X x  X   X   X  1 for some  X  &gt; 1; the cumula-tive distribution function of X is then Pr[ X  X  x ] = 1  X  x
Suppose we discard the  X  fraction of the mass in the tail of of X , i.e., let x 0 be such that
Since  X  = Pr[ X &gt; x 0 ] = x  X   X  0 by definition, we get
Let Y be the random variable truncated at x 0 and rescaled by 1 / (1  X   X  ). We first show the following useful inequality.
Proof.
Lemma 5. If  X   X  3 , then var[ X ] =  X  var[ Y ] . If  X  &gt; 3 , then var[ Y ]  X  ((1  X   X  1  X  1 / X  ) / (1  X   X  )) 2 var[ X ]  X  var[ X ] .
Proof. The easy case is when  X   X  (2 , 3], in which case var[ X ] =  X  var[ Y ]. For the rest, we will assume  X  &gt; 3.
E [ X ] =  X / (  X   X  1) and E [ X 2 ] =  X / (  X   X  2) and so,
By integrating the pdf of Y from 0 to x 0 and using the value of x 0 from (5), we obtain and from which,
Finally, using (6) and (7),
Notice that since 0 &lt;  X  &lt; 1 and  X  &gt; 0, we have 0 &lt;  X   X   X 
Recall that our goal is to devise a method to estimate the corpus size, i.e., the number of documents in a search engine corpus. In this section we present two algorithms that achieve this goal. The two algorithms are based on different assumptions about the capabilities of the index. In the first algorithm, we assume that a uniform random document can be obtained from the corpus. In the second algorithm, we do away with the uniform document sampleability assumption, but instead use a different assumption, which will be evident from the description below.
Suppose we can obtain a uniform random sample of doc-uments in D . Then, using such a sample, we can estimate the fraction of documents in D that are also in D A . Then, using D A and Lemma 1, we can estimate the corpus size.
Corollary 6. If p A = | D A | / | D | , then Thus, by estimating p A via the random sample of docu-ments, we can estimate the corpus size.
Second, suppose uniform random sampling of documents in the corpus is not possible. We show that even under this constraint, the corpus size can be estimated provided we make assumptions about the query pool. The core idea is to use an additional query pool B with the property that B is reasonably uncorrelated with respect to A in terms of occurrence in the corpus (Corollary 8). In other words, we use the independence of the query pools A and B .
Let A, B be two query pools. Let D AB  X  D be the set of documents that contain at least one term in A and one term in B . Then, from Lemma 1, we have
Corollary 7. Here, notice that the summation is over all documents d that contain a and at least one term in B , i.e., the documents are  X  X iltered X  by the query pool B . Thus, Corollary 7 can be used to estimate | D AB | .

The significance of Corollary 7 is that it lets us estimate | D | without using any random access to documents in D , modulo appropriate assumptions on A and B . Suppose A and B are uncorrelated, i.e., Pr[ d  X  D A | d  X  D B ] = Pr[ d  X  D A ] = p A , then it is easy to see
Corollary 8. If A and B are uncorrelated set of terms, then Thus, Corollary 8 can be used to estimate the corpus size without resorting to sampling documents in the corpus uni-formly at random.

While Corollary 8 is very attractive if the query pool A and B are perfectly uncorrelated, in practice it may be hard to construct or obtain such sets. However, we observe even if the set of terms A and B are correlated, the measure of correlation directly translates to the quality of approxi-mation of the corpus size. More precisely, let p A | B denote Pr non-zero constants c 1  X  c 2 , then it follows along the lines of Corollary 8 that
In a straightforward way, our algorithm in Section 4.1 can be modified to estimate the size of various interesting subsets of the corpus. Subsets may be, for instance, the set of all Chinese documents, the set of documents that have no hyperlinks, or the set of documents that are at least 10K in size. Also note that even more generally, our basic estimator can be used to estimate any weighted sum of the documents, where every document has a weight that can be computed by looking at the document.
Given a subset of the corpus defined by a query pool, our basic estimator in Section 3 can be adapted to produce a uniform random sample from this subset of the corpus. This is done by applying a version of rejection sampling that takes into account the weight of a document. Thus, this method can be used to generate a uniform random document that, for instance, contains at least one US zipcode. Being able to generate a uniform random sample of a subset of the corpus is a powerful primitive and has numerous applications. We leave the details to the full version of the paper.
In this section we present our experiments on the TREC collection.
The document set D consists of 1,246,390 HTML files from the TREC .gov test collection; this crawl is from early 2002 ( ir.dcs.gla.ac.uk/test_collections/govinfo.html ). Our methodology is to first define a query pool, pick suffi-ciently many sample terms from this query pool, query the index for these sample terms, and then compute the weight of the sample query terms according to (3). All our sam-pling is done with replacement. For all the experiments, we compute 11 averaged estimators as discussed in Section 3.2 and the final estimate is the median of these 11 averaged estimators. We preprocess the entire data by applying lynx on each of the files to process the HTML page and output a detagged textual version. This is so that the meta-data information in the HTML files is not used in the indexing phase. Moreover, this also serves as a data cleaning phase. We tokenize the documents using whitespace as the separa-tor.

An important point to keep in mind is a consistent in-terpretation of a term  X  X ccurring X  in a document. This plays a role in two different cases. First, in the answers re-turned by the index  X  the index should return all and only documents in which the given query term occurs. Second, in the computation of weight of a term in (1)  X  we need to know how many terms from A occur in the document. The first of these cases can be handled easily by having a very  X  X trict X  definition of  X  X ccurring X  and checking to see if each document returned by the index for a given query term actually contains the term according to the definition of  X  X ccurring X . The second case is trickier, unless we have a reasonable understanding of the way the indexer operates. For sake of this experiment, we adopt the safe approach by hand-constructing a straight-forward indexer.
We illustrate the performance of our methods using two different query pools and random documents from the cor-pus. For both the query pools A and B , we estimate p A and p B by examining random TREC documents. The ex-periment in this case has been constructed to remove any systematic bias from the estimate of p A and p B in order to evaluate how well the techniques perform when the random samples are good.
We choose the first query pool A to be the set of all five-digit numbers, including numbers with leading zeros. Thus, | A | = 10 5 . Our notion of a term occurring in a document is governed by the regular expression /^\d{5}$/ . This will, for instance, ensure that 12 , 345 or 12345 . 67 are not valid matches for the term 12345  X  A . Under these definitions, we have | D A | = 234 , 014 and so p A = 0 . 1877 and 94,918 terms in | A | actually occur in some document in D .
The following table shows the error of results of our ex-periments with set A . Here, error is measured relative to | D | , which we know ( | D | = 1 , 246 , 390), as a function of the number of samples used for each averaged estimator. As we see, the error of the method is quite low.
 Variance reduction. We next investigate whether the per-formance of this method can be improved by applying the variance reduction techniques presented in Section 3.3. To understand this further, we compute the weights of all terms in A to see if it indeed has a heavy tail. Figure 1 shows the distribution of weights of terms in the set | A | . From the figure, it is easy to see that the distribution conforms to a power law (the exponent of the pdf is  X  -1.05). So, there
Figure 1: Distribution of weights of terms in | A | . is hope of improving the performance of the method by the variance reduction method outlined in Section 3.3. To iden-tify the candidate elements in A 0  X  the terms with highest weights  X  we resort to sampling once again. We randomly sample documents from the corpus, and for each term a  X  A that occur in a document d , we maintain a histogram of its weight according to w A d as in (1); note that these weights are in fact approximations to their actual weights w A a as in (2). We finally sort the terms in decreasing order of their accumulated weights and declare the terms that contribute to the top 75% of the weights to be present in A 0 .
This operation defines a new query pool A \ A 0 , upon which our methods apply as before. Thus, the correctness of this approach is not dependent on the exact determination of the most frequent terms, or even upon uniform sampling. The method is always correct, but the variance will only benefit from a more accurate determination. The results are shown below.

We can see that overall this method obtains estimates with significantly lower error, even with few samples.
We repeat the same experiments with a different set B of terms. This time we want to choose B with two properties: none of the terms in B matches too many documents and B is reasonably large. The former property will reduce the variance of the sampling steps if the occurrence of terms is correlated positively with the weights. We provide some ev-idence towards this. See Figure 2. We first extract all terms Figure 2: Correlation of weights and number of oc-currences of terms in A . in the document set D using whitespace separated tokeniza-tion and then we sort the terms according to their frequency of occurrence. We then pick B to be the terms (that are not purely numbers) that occur from position 100,000 to posi-tion 200,000 in this list. Thus, | B | = 100,000. Under these definitions, we obtain | D B | = 396 , 423 and so p B = 0 . 3180.
We see that the method performs extremely well and this can be attributed to our careful choice of the set B . In fact, our experiments showed no appreciable improvement when we applied the variance reduction method to the set B . This is to be expected, as B is specifically constructed so that no term occurs significantly more often than any other term, and so no term will introduce substantial skew into the measurement.
We need to construct query pools A and B that are rea-sonably uncorrelated. To do this, we first try A and B as defined before. Since we have the corpus available, we can actually measure the amount of dependence between the sets A and B . We explicitly calculate p A | B and p A . The values are p A = 0 . 1877 whereas p A | B = 0 . 2628 indicating some cor-relation between the term sets. To reduce the correlation, we modify the sets D A and D B to be slightly different.
We set D 0 A to be the set of documents that contain exactly one term from A ; D 0 B is defined analogously. We do this in order to reduce the potential correlation caused by large documents. Using this, we calculate p 0 A = 0 . 1219 , p 0 . 1437, and p 0 A | B = 0 . 1455, indicating a significant reduction in correlation.

Modifying Lemma 1, we can estimate | D 0 A | and | D 0 B | . We use Corollary 7 to estimate | D AB | . We proceed as before, except that in computing the weight of a sample term from B , we discard documents that do not contain any term from A . The following table shows the error of the method.
As we see from the results, the method obtain a reasonable estimate of the corpus size, without using any random sam-ple of documents. Obviously, the accuracy of this method can be improved if we work with even less correlated query pools. In this section we present our experiments on the Web. We used the public interface of three of the most prominent search engines, which we refer to as SE 1 , SE 2 , and SE We present three series of results for the Web. First we use the basic estimator defined in Section 3.1 to compute the relative sizes of the three engines. We then use the random document approach and the uncorrelated query pool approach defined in Section 4 to compute absolute sizes of the  X  X isible X  portion of the engines. (See below.)
Our methodology for the Web is similar to the one we used for TREC. We first define a query pool, sample suffi-ciently many terms from the query pool, and compute the weights as defined in (3). We postprocess the results us-ing lynx to remove the HTML markup of on each of result pages. Then we compute the weights based on this cleaned version of the result pages, using whitespace tokenization. We use the same definition of a term  X  X ccurring X  in a doc-ument as in the TREC experiment  X  the cleaned version of the document must contain the queried term. This elim-inates documents in which the query term does not appear in the document, including documents in which the query term appears only in anchor text, dead pages (404), and pages that match stemmed versions of the query term. Fur-thermore, since the capabilities of lynx as an HTML parser are limited, many pages are  X  X nvisible X  to our method, in particular most pages containing Java script, frames, Flash, etc. Thus the absolute sizes that we are reporting, are only estimates for the sets  X  X isible X  to our methodology. Assum-ing that each search engine carries the same proportion of  X  X isible X  pages, we can obtain relative sizes of the search engines, but absolute estimates are still an elusive goal.
We first compute the relative sizes of engines SE 1 , SE 2 and SE 3 , using the our basic estimator. We define A to be the set of all eight-digit numbers, including numbers with leading zeros, thus, | A | = 10 8 , and follow the approach de-tailed for the TREC experiments. The following table shows the results of our experiments with query pool A .
If we assume that p A is the same for all the three search engines, i.e., that the three engines index the same propor-tion of pages with eight-digit numbers, the above values pro-vide the relative sizes of the engines corpus. However, p varies from engine to engine. We used a random sample of pages provided to us by Bar-Yossef and Gurevich produced according to the random walk approach described in their work [4]. The following table shows p A for the three engines and the sample sizes.

Using these values, we can easily compute the absolute sizes of the  X  X isible X  portion of the three search engines (in billions) as below.

Next, we used the uncorrelated query pool approach to estimate the absolute corpus sizes. The query pool A is again the set of all eight-digit numbers, and as for TREC, we chose the query pool B to be the medium frequency words, which was determined from a histogram of term counts from the index of one of the search engines. We assume that these words are also the medium frequency words on the other two engines. Furthermore, we also verified that when queried, all three engines always returned less than 1000 results for all our samples from pool B . However, for the Web there is no straightforward way to verify that pool A and pool B are indeed independent or to estimate their correlation.
The following table shows the resulting estimates for the  X  X isible X  portion of the three search engines (in billions) using uncorrelated query pools.

As can be readily seen, the estimates are now larger al-though still much less than published values for the entire corpus of these engines while the relative sizes are fairly con-sistent. The next section explains why this happens.
Even though our methods are intended to produce abso-lute estimates for the corpus size, they are still affected by many idiosyncracies that exist in web documents. Here we list some of the caveats while drawing conclusions from the results above. 1. Even though our methods are fairly generic, they end 2. Even though we chose our query pools carefully, for 3. Even though the search engine may return fewer than 4. The number of samples used to estimate p A is fairly 5. In the uncorrelated query pool approach, our choice
We addressed the problem of estimating the size of a cor-pus using only black-box access. We constructed a basic es-timator that can estimate the number of documents in the corpus that contain at least one term from a given query pool. Using this estimator, we compute the corpus size us-ing two different algorithms, depending on whether or not it is possible to obtain random documents from the corpus. While the ability to randomly sample the corpus makes the problem easier, we show that by using two query pools that are reasonably uncorrelated, it is possible to obviate the need for random document samples. En route, we also obtain a novel way to provably reduce the variance of a random vari-able where the distribution of the random variable is mono-tonically decreasing; this technique may be of independent interest.

We applied our algorithms on the TREC collection to measure their performance. The algorithms that uses ran-dom document samples perform quite well as expected. More surprisingly, by carefully constructing query pools that are reasonably uncorrelated, we show that it possible to esti-mate the corpus size to modest accuracies. We also apply our algorithms to estimate the  X  X isible X  corpus size of major web search engines.
 We thank Ziv Bar-Yossef and Maxim Gurevich for their in-valuable help. We thank Anna Patterson for helpful com-ments, suggestions, and criticisms. [1] A. Anagnostopoulos, A. Z. Broder, and D. Carmel. [2] J. Bar-Ilan. Size of the web, search engine coverage [3] Z. Bar-Yossef, A. Berg, S. Chien, J. Fakcharoenphol, [4] Z. Bar-Yossef and M. Gurevich. Random sampling [5] K. Bharat and A. Broder. Mirror and mirror and on [6] K. Bharat and A. Z. Broder. A technique for [7] K. Bharat, A. Z. Broder, J. Dean, and M. R.
 [8] A. Z. Broder. Web measurements via random queries. [9] J. Callan and M. Connell. Query-based sampling of [10] A. Gulli and A. Signorini. The indexable web is more [11] M. R. Henzinger, A. Heydon, M. Mitzenmacher, and [12] S. Lawrence and C. Giles. Accessibility of information [13] S. Lawrence and C. L. Giles. Searching the world wide [14] K.-L. Liu, C. Yu, and W. Meng. Discovering the [15] P. Rusmevichientong, D. M. Pennock, S. Lawrence, [16] C. J. van Rijsbergen. Information Retrieval . [17] S. Wu, F. Gibb, and F. Crestani. Experiments with
