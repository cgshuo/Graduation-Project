 Consider the problem of repeatedly choosing advertisements to display in sponsored search to maximize our revenue. In this problem, there is a small set of positions on the page, and each time a query arrives we would like to assign, to each position, one out of a large number of possible ads. In this and related problems that we call online assignment learning problems, there is a set of positions, a set of items, and a sequence of rounds, and on each round we must assign an item to each position. After each round, we obtain some reward depending on the selected assignment, and we observe the value of the reward. When there is only one position, this problem becomes the well-studied multiarmed bandit problem [ 2 ]. When the positions have a linear ordering the assignment can be construed as a ranked list of elements, and the problem becomes one of selecting lists online. Online assignment learning thus models a central challenge in web search, sponsored search, news aggregators, and recommendation systems, among other applications.
 A common assumption made in previous work on these problems is that the quality of an assignment is the sum of a function on the (item, position) pairs in the assignment. For example, online advertising models with click-through-rates [ 6 ] make an assumption of this form. More recently, there have been attempts to incorporate the value of diversity in the reward function [ 16 ]. Intuitively, even though the best K results for the query  X  X urkey X  might happen to be about the country, the best list of K results is likely to contain some recipes for the bird as well. This will be the case if there are diminishing returns on the number of relevant links presented to a user; for example, if it is better to present each user with at least one relevant result than to present half of the users with no relevant results and half with two relevant results. We incorporate these considerations in a flexible way by providing an algorithm that performs well whenever the reward for an assignment is a monotone submodular function of its set of (item, position) pairs.
 Our key contributions are: ( i ) an efficient algorithm, T ABULAR G REEDY , that provides a (1  X  1 /e ) approximation ratio for the problem of optimizing assignments under submodular utility functions, ( ii ) an algorithm for online learning of assignments, TG BANDIT , that has strong performance guarantees in the no-regret model, and ( iii ) an empirical evaluation on two problems of information gathering on the web. We consider problems, where we have K positions (e.g., slots for displaying ads), and need to assign to each position an item (e.g., an ad) in order to maximize a utility function (e.g., the revenue from clicks on the ads). We address both the offline problem, where the utility function is specified in advance, and the online problem, where a sequence of utility functions arrives over time, and we need to repeatedly select a new assignment.
 The Offline Problem. In the offline problem we are given sets P 1 , P 2 , . . . , P K , where P k is the set of items that may be placed in position k . We assume without loss of generality that these sets are disjoint. 1 An assignment is a subset S  X  V , where V = P 1  X  P 2  X  X  X  X  X  P K is the set of all items. We call an assignment feasible , if at most one item is assigned to each position (i.e., for all k , | S  X  P k | X  1 ). We use P to refer to the set of feasible assignments.
 Our goal is to find a feasible assignment maximizing a utility function f : 2 V  X  R  X  0 . As we discuss later, many important assignment problems satisfy submodularity , a natural diminishing returns property: Assigning a new item to a position k increases the utility more if few elements have been assigned yet, and less if many items have already been assigned. Formally, a utility function f is We will also assume f is monotone (i.e., for all S  X  S 0 , we have f ( S )  X  f ( S 0 ) ). Our goal is thus, for a given non-negative, monotone and submodular utility function f , to find a feasible assignment S  X  of maximum utility, S  X  = arg max This optimization problem is NP-hard. In fact, a stronger negative result holds: Theorem 1 ([ 14 ]) . For any &gt; 0 , any algorithm guaranteed to obtain a solution within a factor of (1  X  1 /e + ) of max S  X  X  { f ( S ) } requires exponentially many evaluations of f in the worst case. In light of this negative result, we can only hope to efficiently obtain a solution that achieves a fraction of (1  X  1 /e ) of the optimal value. In  X  3.2 we develop such an algorithm.
 The Online Problem. The offline problem is inappropriate to model dynamic settings, where the utility function may change over time, and we need to repeatedly select new assignments, trading off exploration (experimenting with ad display to gain information about the utility function), and exploitation (displaying ads which we believe will maximize utility). More formally, we face a sequential decision problem, where, on each round (which, e.g., corresponds to a user query for a particular term), we want to select an assignment S t (ads to display). We assume that the sets P 1 , P 2 , . . . , P K are fixed in advance for all rounds. After we select the assignment we obtain reward f t ( S t ) for some non-negative monotone submodular utility function f t . We call the setting where we do not get any information about f t beyond the reward the bandit feedback model. In contrast, in the full-information feedback model we obtain oracle access to f t (i.e., we can evaluate f t on arbitrary feasible assignments). Both models arise in real applications, as we show in  X  5.
 The goal is to maximize the total reward we obtain, namely P t f t ( S t ) . Following the multiarmed bandit literature, we evaluate our performance after T rounds by comparing our total reward against that obtained by a clairvoyant algorithm with knowledge of the sequence of functions  X  f 1 ,...,f T  X  , but with the restriction that it must select the same assignment on each round. The difference between the clairvoyant algorithm X  X  total reward and ours is called our regret . The goal is then to develop an algorithm whose expected regret grows sublinearly in the number of rounds; such an algorithm is said to have (or be) no-regret . However, since sums of submodular functions remain submodular, the clairvoyant algorithm has to solve an offline assignment problem with f ( S ) = P t f t ( S ) . Considering Theorem 1, no polynomial-time algorithm can possibly hope to achieve a no-regret guarantee. To accommodate this fact, we discount the reward of the clairvoyant algorithm by a factor of (1  X  1 /e ) : We define the (1  X  1 /e ) -regret of a random sequence  X  S 1 ,...,S T  X  as Our goal is then to develop efficient algorithms whose (1  X  1 /e ) -regret grows sublinearly in T . Subsumed Models. Our model generalizes several common models for sponsored search ad selection, and web search results. These include models with click-through-rates , in which it is assumed that each (ad, position) pair has some probability p ( a,k ) of being clicked on, and there is some monetary reward b ( a ) that is obtained whenever ad a is clicked on. Often, the click-through-rates are assumed to be separable , meaning p ( a,k ) has the functional form  X  ( a )  X   X  ( k ) for some functions  X  and  X  . See [7, 12] for more details on sponsored search ad allocation. Note that in both of these cases, the (expected) reward of a set S of (ad, position) pairs is P ( a,k )  X  S g ( a,k ) for some nonnegative function g . It is easy to verify that such a reward function is monotone submodular. Thus, we can capture this model in our framework by setting P k = A X { k } , where A is the set of ads. Another subsumed model, for web search, appears in [ 16 ]; it assumes that each user is interested in a particular set of results, and any list of results that intersects this set generates a unit of value; all other lists generate no value, and the ordering of results is irrelevant. Again, the reward function is monotone submodular. In this setting, it is desirable to display a diverse set of results in order to maximize the likelihood that at least one of them will interest the user.
 Our model is flexible in that we can handle position-dependent effects and diversity considerations simultaneously. For example, we can handle the case that each user u is interested in a particular set A u of ads and looks at a set I u of positions, and the reward of an assignment S is any monotone-the case where the quality is the number of relevant result that appear in the first k positions. If I u equals all positions and g ( x ) = min { x, 1 } we recover the model of [16]. 3.1 The locally greedy algorithm A simple approach to the assignment problem is the following greedy procedure: the algorithm steps through all K positions (according to some fixed, arbitrary ordering). For position k , it simply chooses the item that increases the total value as much as possible, i.e., it chooses where, for a set S and element e , we write S + e for S  X  X  e } . Perhaps surprisingly, no matter which ordering over the positions is chosen, this so-called locally greedy algorithm produces an assignment that obtains at least half the optimal value [ 8 ]. In fact, the following more general result holds. We will use this lemma in the analysis of our improved offline algorithm, which uses the locally greedy algorithm as a subroutine.
 Lemma 2. Suppose f : 2 V  X  R  X  0 is of the form f ( S ) = f 0 ( S ) + P K k =1 f k ( S  X  P k ) where f 0 : 2 V  X  R  X  0 is monotone submodular, and f k : 2 P k  X  R  X  0 is arbitrary for k  X  1 . Let L be the solution returned by the locally greedy algorithm. Then f ( L ) + f 0 ( L )  X  max S  X  X  { f ( S ) } . The proof is given in an extended version of this paper [ 9 ]. Observe that in the special case where f k  X  0 for all k  X  1 , Lemma 2 says that f ( L )  X  1 2 max S  X  X  f ( S ) . In [ 9 ] we provide a simple example showing that this 1 / 2 approximation ratio is tight. 3.2 An algorithm with optimal approximation ratio We now present an algorithm that achieves the optimal approximation ratio of 1  X  1 /e , improving on the 1 2 approximation for the locally greedy algorithm. Our algorithm associates with each partition P k a color c k from a palette [ C ] of C colors, where we use the notation [ n ] = { 1 , 2 ,...,n } . For any Given a set S of (item, color) pairs, which we may think of as labeling each item with one or more colors, sample ~c ( S ) returns a set containing each item x that is labeled with whatever color ~c assigns to the partition that contains x . Let F ( S ) denote the expected value of f ( sample ~c ( S )) when each color c k is selected uniformly at random from [ C ] . Our T ABULAR G REEDY algorithm greedily optimizes F , as shown in the following pseudocode.
 Observe that when C = 1 , there is only one possible choice for ~c , and T ABULAR G REEDY is simply the locally greedy algorithm from  X  3.1. In the limit as C  X   X  , T ABULAR G REEDY can intuitively be viewed as an algorithm for a continuous extension of the problem followed by a rounding procedure, in the same spirit as Vondr  X  ak X  X  continuous-greedy algorithm [ 4 ]. In our case, the continuous extension is to compute a probability distribution D k for each position k with support in P k (plus a special  X  X elect nothing X  outcome), such that if we independently sample an element x k from D k , E [ f ( { x 1 ,...,x K } )] is maximized. It turns out that if the positions individually, greedily, and in round-robin fashion, add infinitesimal units of probability mass to their distributions so as to maximize this objective function, they achieve the same objective function value as if, rather than making decisions in a round-robin fashion, they had cooperated and added the combination of K infinitesimal probability mass units (one per position) that greedily maximizes the objective function. The latter process, in turn, can be shown to be equivalent to a greedy algorithm for maximizing a (different) submodular function subject to a cardinality constraint, which implies that it achieves a 1  X  1 /e approximation ratio [ 15 ]. T ABULAR G REEDY represents a tradeoff between these two extremes; its performance is summarized by the following theorem. For now, we assume that the arg max in the inner loop is computed exactly. In the extended version [ 9 ], we bound the performance loss that results from approximating the arg max (e.g., by estimating F by repeated sampling). Theorem 3. Suppose f is monotone submodular. Then F ( G )  X   X  ( K,C )  X  max S  X  X  { f ( S ) } , where  X  ( K,C ) is defined as 1  X  (1  X  1 C ) C  X  K 2 C  X  1 .
 It follows that, for any  X  &gt; 0 , T ABULAR G REEDY achieves a (1  X  1 /e  X   X  ) approximation factor using a number of colors that is polynomial in K and 1 / X  . The theorem will follow immediately from the combination of two key lemmas, which we now prove. Informally, Lemma 4 analyzes the approximation error due to the outer greedy loop of the algorithm, while Lemma 5 analyzes the approximation error due to the inner loop.
 Lemma 4. Let G c = { g 1 ,c ,g 2 ,c ,...,g K,c } , and let G  X  c = G 1  X  G 2  X  ...  X  G c  X  1 . For each color c , choose E c  X  R such that F ( G  X  c  X  G c )  X  max x  X  X  c { F ( G  X  c  X  x ) } X  E c where R c := { R :  X  k  X  [ K ] , | R  X  ( P k  X { c } ) | = 1 } is the set of all possible choices for G c . Then where  X  ( C ) = 1  X  1  X  1 C C .
 Proof (Sketch). We will refer to an element R of R c as a row , and to c as the color of the row. Let R H ( R ) = F S R  X  X  R . We will prove the lemma in three steps: ( i ) H is monotone submodular, ( ii ) T ABULAR G REEDY is simply the locally greedy algorithm for finding a set of C rows that maximizes H , where the c th greedy step is performed with additive error E c , and ( iii ) T ABULAR G REEDY obtains the guarantee (3.1) for maximizing H , and this implies the same ratio for maximizing F . To show that H is monotone submodular, it suffices to show that F is monotone submodular. Because F ( S ) = E ~c [ f ( sample ~c ( S ))] , and because a convex combination of monotone submodular functions is monotone submodular, it suffices to show that for any particular coloring ~c , the function f ( sample ~c ( S )) is monotone submodular. This follows from the definition of sample and the fact that f is monotone submodular.
 The second claim is true by inspection. To prove the third claim, we note that the row colors for a set of rows R can be interchanged with no effect on H ( R ) . For problems with this special property, it is known that the locally greedy algorithm obtains an approximation ratio of  X  ( C ) = 1  X  (1  X  1 C ) C [ 15 ]. Theorem 6 of [17] extends this result to handle additive error, and yields To complete the proof, it suffices to show that max R X  X  follows from the fact that for any assignment S  X  P , we can find a set R ( S ) of C rows such that sample ~c ( S R  X  X  ( S ) R ) = S with probability 1, and therefore H ( R ( S )) = f ( S ) . We now bound the performance of the the inner loop of T ABULAR G REEDY .
 Lemma 5. Let f  X  = max S  X  X  { f ( S ) } , and let G c , G  X  c , and R c be defined as in the statement of Lemma 4. Then, for any c  X  [ C ] , F ( G  X  c  X  G c )  X  max R  X  X  c { F ( G  X  c  X  R ) } X  K 2 C  X  2 f  X  . Proof (Sketch). Let N denote the number of partitions whose color (assigned by ~c ) is c . For R  X  X  c , where we have used the fact that  X  ~c ( R ) = 0 when N = 0 . The idea of the proof is that the first of these terms dominates as C  X   X  , and that E ~c [ X  ~c ( R ) | N = 1] can be optimized exactly simply by optimizing each element of P k  X  { c } independently. Specifically, it can f ( R ) = P [ N  X  2] E ~c [ X  ~c ( R ) | N  X  2] is a monotone submodular function of a set of (item, color) pairs, for the same reasons F is. Applying Lemma 2 with these { f k : k  X  0 } yields F ( G c ) + P [ N  X  2] E ~c [ X  ~c ( G c ) | N  X  2]  X  max R  X  X  c { F c ( R ) } . To complete the proof, it suf-fices to show P [ N  X  2]  X  K 2 C  X  2 and E ~c [ X  ~c ( G c ) | N  X  2]  X  f  X  . The first inequality holds because, if we let M be the number of pairs of partitions that are both assigned color c , we have P [ N  X  2] = P [ M  X  1]  X  E [ M ] = K 2 C  X  2 . The second inequality follows from the fact that for any ~c we have  X  ~c ( G c )  X  f ( sample ~c ( G  X  c  X  G c ))  X  f  X  . We now transform the offline algorithm of  X  3.2 into an online algorithm. The high-level idea behind this transformation is to replace each greedy decision made by the offline algorithm with a no-regret online algorithm. A similar approach was used in [ 16 ] and [ 18 ] to obtain an online algorithm for different (simpler) online problems.
 The following theorem summarizes the performance of TG BANDIT .
 Theorem 6. Let r k,c be the regret of E k,c , and let  X  ( K,C ) = 1  X  1  X  1 C C  X  K 2 C  X  1 . Then Observe that Theorem 6 is similar to Theorem 3, with the addition of the E [ r k,c ] terms. The idea of the proof is to view TG BANDIT as a version of T ABULAR G REEDY that, instead of greedily selecting single (element,color) pairs g k,c  X  P k  X { c } , greedily selects (element vector, color) pairs ~g k,c  X  P T k  X { c } (here, P T k is the T th power of the set P k ). We allow for the case that the greedy decision is made imperfectly, with additive error r k,c ; this is the source of the extra terms. Once this correspondence is established, the theorem follows along the lines of Theorem 3. For a proof, see the extended version [9].
 Corollary 7. If TG BANDIT is run with randomized weighted majority [5] as the subroutine, then where  X  ( K,C ) = 1  X  1  X  1 C C  X  K 2 C  X  1 .
 Optimizing for C in Corollary 7 yields (1  X  1 e ) -regret  X   X ( K 3 / 2 T 1 / 4 factors, where OPT := max S  X  X  n P T t =1 f t ( S ) o is the value of the static optimum. Dealing with bandit feedback. TG BANDIT can be modified to work in the bandit feedback model. The idea behind this modification is that on each round we  X  X xplore X  with some small probability, in x ) for each k  X  [ K ] , c  X  [ C ] , and x  X  P k . This technique can be used to achieve a bound similar to the one stated in Corollary 7, but with an additive regret term of O ( T |V| CK ) 2 3 (log |V| ) 1 3 . Stronger notions of regret. By substituting in different algorithms for the subroutines E k,c , we can obtain additional guarantees. For example, Blum and Mansour [ 3 ] consider online problems in which we are given time-selection functions I 1 ,I 2 ,...,I M . Each time-selection function I : [ T ]  X  [0 , 1] associates a weight with each round, and defines a corresponding weighted notion of regret in the natural way. Blum and Mansour X  X  algorithm guarantees low weighted regret with respect to all M time selection functions simultaneously. This can be used to obtain low regret with respect to different (possibly overlapping) windows of time simultaneously, or to obtain low regret with respect to subsets of rounds that have particular features. By using their algorithm as a subroutine within TG BANDIT , we get similar guarantees, both in the full information and bandit feedback models. We evaluate TG BANDIT experimentally on two applications: Learning to rank blogs that are effective in detecting cascades of information, and allocating advertisements to maximize revenue. 5.1 Online learning of diverse blog rankings We consider the problem of ranking a set of blogs and news sources on the web. Our approach is based on the following idea: A blogger writes a posting, and, after some time, other postings link to it, forming cascades of information propagating through the network of blogs.
 More formally, an information cascade is a directed acyclic graph of vertices (each vertex corresponds to a posting at some blog), where edges are annotated by the time difference between the postings. Based on this notion of an information cascade, we would like to select blogs that detect big cascades (containing many nodes) as early as possible (i.e., we want to learn about an important event before most other readers). In [ 13 ] it is shown how one can formalize this notion of utility using a monotone submodular function that measures the informativeness of a subset of blogs. Optimizing the submodular function yields a small set of blogs that  X  X overs X  most cascades. This utility function prefers diverse sets of blogs, minimizing the overlap of the detected cascades, and therefore minimizing redundancy.
 The work by [ 13 ] leaves two major shortcomings: Firstly, they select a set of blogs rather than a ranking , which is of practical importance for the presentation on a web service. Secondly, they do not address the problem of sequential prediction, where the set of blogs must be updated dynamically over time. In this paper, we address these shortcomings. Results on offline blog ranking. In order to model the blog ranking problem, we adopt the assumption that different users have different attention spans: Each user will only consider blogs appearing in a particular subset of positions. In our experiments, we assume that the probability that a user is willing to look at position k is proportional to  X  k , for some discount factor 0 &lt;  X  &lt; 1 . More formally, let g be the monotone submodular function measuring the informativeness of any set of blogs, defined as in [ 13 ]. Let P k = B X { k } , where B is the set of blogs. Given an assignment S  X  P , let S [ k ] = S  X  X  P 1  X  P 2  X  ...  X  P k } be the assignment of blogs to positions 1 through k . can be seen that f : 2 V  X  R  X  0 is monotone submodular.
 For our experiments, we use the data set of [ 13 ], consisting of 45,192 blogs, 16,551 cascades, and 2 million postings collected during 12 months of 2006. We use the population affected objective of [ 13 ], and use a discount factor of  X  = 0 . 8 . Based on this data, we run our T ABULAR G REEDY algorithm with varying numbers of colors C on the blog data set. Fig. 1(a) presents the results of this experiment. For each value of C , we generate 200 rankings, and report both the average performance and the maximum performance over the 200 trials. Increasing C leads to an improved performance over the locally greedy algorithm ( C = 1 ).
 Results on online learning of blog rankings. We now consider the online problem where on each round t we want to output a ranking S t . After we select the ranking, a new set of cascades occurs, modeled using a separate submodular function f t , and we obtain a reward of f t ( S t ) . In our experiments, we choose one assignment per day, and define f t as the utility associated with the cascades occurring on that day. Note that f t allows us to evaluate the performance of any possible ranking S t , hence we can apply TG BANDIT in the full-information feedback model.
 We compare the performance of our online algorithm using C = 1 and C = 4 . Fig. 1(b) presents the average cumulative reward gained over time by both algorithms. We normalize the average reward by the utility achieved by the T ABULAR G REEDY algorithm (with C = 1 ) applied to the entire data set. Fig. 1(b) shows that the performance of both algorithms rapidly (within the first 47 rounds) converges to the performance of the offline algorithm. The TG BANDIT algorithm with C = 4 levels out at an approximately 4% higher reward than the algorithm with C = 1 . 5.2 Online ad display We evaluate TG BANDIT for the sponsored search ad selection problem in a simple Markovian model incorporating the value of diverse results and complex position-dependence among clicks. In this for each position k  X  [ K ] . When presented an assignment of ads { a 1 ,a 2 ,...,a K } , where a k occupies position k , the user scans the positions in increasing order. For each position k , the user clicks on a k with probability p click ( a k ) , leaving the results page forever. Otherwise, with probability (1  X  p click ( a k ))  X  p abandon ( k ) , the user loses interest and abandons the results without clicking on position k + 1 . The reward function f t is the number of clicks, which is either zero or one. We only receive information about f t ( S t ) (i.e., bandit feedback ). In our evaluation, there are 5 positions, 20 available ads, and two (equally frequent) types of users: type 1 users interested in all positions ( p abandon  X  0 ), and type 2 users that quickly lose interest ( p abandon  X  0 . 5 ). There are also two types of ads, half of type 1 and half of type 2 , and users are probabilistically more interested in ads of their own type than those of the opposite type. Specifically, otherwise. In Fig. 1(c) we compare the performance of TG BANDIT with C = 4 to the online algorithm of [ 16 , 18 ], based on the average of 100 experiments. The latter algorithm is equivalent to running TG BANDIT with C = 1 . They perform similarly in the first 10 4 rounds; thereafter the former algorithm dominates.
 It can be shown that with several different types of users with distinct p click (  X  ) functions the offline problem of finding an assignment within 1  X  1 e +  X  of optimal is NP -hard. This is in contrast to the case in which p click and p abandon are the same for all users; in this case the offline problem simply requires finding an optimal policy for a Markov decision process, which can be done efficiently using well-known algorithms. A slightly different Markov model of user behavior which is efficiently is a function of the ad in the slot currently being scanned rather than its index. For a general introduction to the literature on submodular function maximization, see [ 19 ]. For applications of submodularity to machine learning and AI see [11].
 Our offline problem is known as maximizing a monotone submodular function subject to a (simple) partition matroid constraint in the operations research and theoretical computer science communities. The study of this problem culminated in the elegant (1  X  1 /e ) approximation algorithm of Vondr  X  ak [ 20 ] and a matching unconditional lower bound of Mirrokni et al. [ 14 ]. Vondr  X  ak X  X  algorithm, called the continuous-greedy algorithm , has also been extended to handle arbitrary matroid constraints [ 4 ]. The continuous-greedy algorithm, however, cannot be applied to our problem directly, because it requires the ability to sample f (  X  ) on infeasible sets S /  X  X  . In our context, this means it must have the ability to ask (for example) what the revenue will be if ads a 1 and a 2 are placed in position #1 simultaneously . We do not know how to answer such questions in a way that leads to meaningful performance guarantees.
 In the online setting, the most closely related work is that of Streeter and Golovin [ 18 ]. Like us, they consider sequences of monotone submodular reward functions that arrive online, and develop an online algorithm that uses multi-armed bandit algorithms as subroutines. The key difference from our work is that, as in [ 16 ], they are concerned with selecting a set of K items rather than the more general problem of selecting an assignment of items to positions addressed in this paper. Kakade et al. [ 10 ] considered the general problem of using  X  -approximation algorithms to construct no  X  -regret online algorithms, and essentially proved it could be done for the class of linear optimization problems in which the cost function has the form c ( S,w ) for a solution S and weight vector w , and c ( S,w ) is linear in w . However, their result is orthogonal to ours, because our objective function is submodular and not linear 2 . In this paper, we showed that important problems, such as ad display in sponsored search and computing diverse rankings of information sources on the web, require optimizing assignments under submodular utility functions. We developed an efficient algorithm, T ABULAR G REEDY , which obtains the optimal approximation ratio of (1  X  1 /e ) for this NP-hard optimization problem. We also developed an online algorithm, TG BANDIT , that asymptotically achieves no (1  X  1 /e ) -regret for the problem of repeatedly selecting informative assignments, under the full-information and bandit-feedback settings. Finally, we demonstrated that our algorithm outperforms previous work on two real world problems, namely online ranking of informative blogs and ad allocation.
 Acknowledgments. This work was supported in part by Microsoft Corporation through a gift as well as
