 Recommender systems leverage user demographic informa-tion, such as age, gender, etc., to personalize recommenda-tions and better place their targeted ads. Oftentimes, users do not volunteer this information due to privacy concerns, or due to a lack of initiative in filling out their online profiles. We illustrate a new threat in which a recommender learns private attributes of users who do not voluntarily disclose them. We design both passive and active attacks that so-licit ratings for strategically selected items, and could thus be used by a recommender system to pursue this hidden agenda. Our methods are based on a novel usage of Bayesian matrix factorization in an active learning setting. Evalua-tions on multiple datasets illustrate that such attacks are indeed feasible and use significantly fewer rated items than static inference methods. Importantly, they succeed without sacrificing the quality of recommendations to users. Categories and Subject Descriptors: H.2.8 Database Applications: Data Mining Keywords: Recommendations, Privacy, Active Learning.
Recommender systems rely on knowing their users  X  not just their preferences (i.e., ratings on items), but also their social and demographic information, e.g., age, gender, po-litical affiliation, and ethnicity [14, 19]. A rich user profile allows a recommender system to better personalize its ser-vices, and at the same time enables additional monetization opportunities, such as targeted advertising.

Users of a recommender system know they are disclos-ing their preferences (or ratings) for movies, books, or other items (we use movies as our running example). A recom-mender may obtain additional social and demographic in-formation about its users by explicitly soliciting it [14, 19]. While some users may willingly disclose such information, others may be more privacy-sensitive and elect to only re-veal their ratings. Privacy research has shown that some users are uncomfortable revealing their demographic data to personalization systems [1, 15]. Even when such services provide transparency about their data collection and use practices [1], some users are unwilling to disclose personal data despite the allure of personalized services. In [14] the authors conduct a small scale user study on Amazon Turk that examines how to motivate users to disclose their demo-graphic data.

For users who wish to withhold some demographic infor-mation, a recommender can still undermine their attempt at privacy. In previous work [30], we show that users X  movie ratings can be used to predict their gender with 80% accu-racy. Other studies also show the potential to infer demo-graphics from a range of online user activities [2, 4, 18, 20]. In this work, we consider a recommender system that offers a legitimate service, yet is simultaneously malicious: it pur-posefully attempts to extract certain attributes from users who choose to withhold them. Unlike previous work that studies static attacks on the complete data, we consider an active learning setting, in which the recommender system aims to efficiently (quickly and accurately) infer a user X  X  private attribute via interactive questioning. Recommender systems routinely ask users to rate a few items, as a means to address a  X  X old start X  setting, or to improve the quality of recommendations. We leverage these instances of inter-actions with the user, alongside with the observation that item selection is at the recommender X  X  discretion, to propose a new attack. We hypothesize that if the sequence of ques-tions (items to rate) is carefully selected, the recommender system can quickly (so as not to be detected by the user) determine a user X  X  private attribute with high confidence, thereby violating her privacy. A key idea in the design of this attack is to leverage matrix factorization (MF) as the basis for inference. Most recommender systems use matrix factorization (MF) models as a building block for provid-ing recommendations [17]. While MF is well understood for rating prediction, it has generally not been applied for in-ference. To the best of our knowledge, this paper is the first to leverage MF as the basis for building both (a) an infer-ence method of private attributes using item ratings and (b) an active learning method that selects items in a way that maximizes inference confidence in the smallest number of questions.

Our contributions are as follows:  X  First, we propose a novel classification method for deter- X  Second, we demonstrate that the resulting classification  X  Third, we show that our active learning method is very  X  Fourth, we extensively evaluate the above classifier and
A number of studies have shown that user demographics can be inferred from various types of online user activity. For example, Bhagat et al. [2] show that it is possible to learn age and gender information from blogs. Mislove et al. [20] study data from online social networks and illustrate that even when only a fraction of users provide profile attributes (such as location, interests, etc.), it is possible to infer these attributes among users who do not disclose them. Bi et al. [4] show how demographics can be inferred from search queries, and Kosinski et al . [18] show that several personality traits, including political views, sexual orientation, and drug use can be accurately predicted from Facebook  X  X ikes X .
Recommender systems were shown to be exploitable by several works utilizing off-line attacks [5, 22, 30]. Closest to our setting, Weinsberg et al. [30] empirically studied how to infer a user X  X  gender from her movie ratings using a va-riety of different classifiers, showing that logistic regression and SVMs succeed with an accuracy close to 80%. We de-part from [30] in multiple ways. First, we introduce a novel factor-based classifier, that relies on the Bayesian assump-tions behind MF. Second, we study a recommender system in an adversarial setting that actively adapts item selection to quickly learn the private attributes. Finally, we establish that our classifier is very well suited for this task. The Bayesian model underlying MF (discussed in detail in Section 3.2) was recently employed by Silva and Carin [28] to actively learn the actual factors ( i.e. , the user and item profiles) in MF. More specifically, the authors consider a rec-ommender system that adaptively selects which items to ask its users to rate in order to diminish the entropy of its user and item profiles as quickly as possible. The entropy estima-tion is based on the Gaussian noise and prior assumptions underlying MF, which we also employ in our work. A vari-ety of active learing objective were also studied by Suther-land et al. [29], including minimizing the prediction error on unrated items, reducing the profile uncertainty, and identi-fying highly rated items. We depart from the above works as the goal of our learning task is to discover a user X  X  demo-graphic information, captured by a categorical type, rather the above objectives motivated by rating prediction.
In the classic active learning setting [8,9], a learner wishes to disambiguate among a set of several possible hypotheses, each represented as a function over a set of inputs. Only one of the hypotheses is valid; to discover it, the learner has access to an oracle that returns the evaluation of the valid hypothesis on a given input. In the case of a noiseless oracle, that always returns the correct evaluation on a query, Gen-eralized Binary Search (GBS) discovers the valid hypothesis in a number of queries within a polylogarithmic factor from the optimal [8, 9]. Our setup can be cast into the above framework in the context of a noisy oracle, whose evalua-tions may not necessarily be exact. GBS is known to yield arbitrarily suboptimal results in the presence of noise [10]. Though algorithms for restricted noise models exist (see, e.g., [23] and [10]), no algorithm with provable performance guarantees is known in the presence of an oracle with ar-bitrary noise. Unfortunately, none of the existing models apply to the noisy setting we encounter here.
We consider a recommender system that provides a legiti-mate item recommendation service, but at the same time maliciously seeks to infer a private user attribute. The system has access to a dataset, provided by non-privacy-sensitive users, that contains item ratings as well as a cat-egorical variable, which we refer to as the user type . The type is a private attribute such as gender, age, political af-filiation, etc. A new user, who is privacy sensitive (i.e., her type is unknown) interacts with the system. The recom-mender system actively presents items for the user to rate, masquerading it as a way to improve recommendations in the cold-start setting. In this context, our goal is twofold: 1. We wish to design a type classifier that discovers the type 2. We wish to address the problem of actively learning a For the attack to be considered successful, the recommender system needs to obtain high confidence in the value of the inferred type, with a minimum number of questions posed to the user. As our classifier and item selection methods rely heavily on matrix factorization, we review this as well as the latent factor model that underlies it below.
We use the following notation to describe the training dataset of the recommender. The dataset comprises of rat-ings to m items in set M  X  { 1 ,...,m } given by n users in set N  X  { 1 ,...,n } . We denote by r ij the rating of user i  X  X  to item j  X  X  , and by E  X  X  X M the set of user-item pairs ( i,j ), for which a rating r ij is present in the dataset. Each user is characterized by a categorical type, which cap-tures demographic information such as gender, occupation, income category, etc. Focusing on binary types, we denote by t i  X  X   X { +1 ,  X  1 } the type of user i  X  X  .

We assume that the ratings are generated from the stan-dard generative model used in matrix factorization, aug-mented with type-dependent biases. More specifically, there exist latent factors u i  X  R d , i  X  N , and v j  X  R d , j  X  M (the user and item profiles , resp.) such that ratings are: where ij  X  N (0 , X  2 0 ) are independent Gaussian noise vari-ables and z jt is a type bias , capturing the effect of a type on the item rating. Our model is thus parametrized by [ z and item profiles: for all i  X  X  , j  X  X  , i.e. , profiles are sampled from independent zero-mean mul-tivariate Gaussian priors.

The Gaussian priors (2) are used in many works on so-called Bayesian matrix factorization (see, e.g. , [21, 25, 28]). Under (1) and (2), the maximum likelihood estimation of the model parameters reduces to the standard [16, 17] mini-mization of the (non-convex) regularized error: 1 min ( i,j )  X  X  and types t i , i  X  X  , the parameters U,V,Z can be computed as local minima to (3) through standard methods [17], such as gradient descent or alternating minimization, while  X  and  X  are computed through cross-validation.
We now turn our attention to the following classification problem. Suppose that the recommender system, with ac-cess to the dataset of ratings and types, has computed a set of item profiles V as well as a set of biases Z , e.g. , by minimizing (3) through gradient descent. A new user ar-rives in the system and submits ratings for items in some set A  X  M , but does not submit her type. In order to by-pass the user X  X  attempt at privacy, we need to construct a classifier to infer the type of this new user.

In this section, we present a classifier that uses the item profiles and biases ( i.e. , the latent factors obtained through matrix factorization) to accomplish this task. We refer to this classifier as a Factor-Based Classifier ( FBC ). Crucially, FBC is consistent with the Bayesian model of matrix fac-torization presented in the previous section. In particular, it amounts to the maximum a-posteriori estimation of the type under the bi-linear noise model (1) and the priors (2).
For A  X  M the set of items for which the user submits ratings, we introduce the following notation. We denote by Note that, as is common practice, to ensure that the profiles U,V obtained by (3) are invariant to a translation (shift) of the ratings, we do not regularize the category biases (or, equivalently, we assume no prior on Z ). r
A  X  [ r j ] j  X  A  X  R | A | the vector of ratings provided by the user, by V A  X  [ v T j ] j  X  A  X  R | A | X  d the matrix of profiles for items rated, and by z At  X  [ z jt ] j  X  A  X  R | A | the vector of type-t biases for items rated.

As in the previous section, we assume the new user has an unknown profile u  X  R d and a type t  X  { X  1 , +1 } , such that the ratings she submits follow (1), i.e. , where j  X  N (0 , X  2 0 ). That is, conditioned on u and t , the ratings r A = [ r j ] j  X  A  X  R | A | given to items in A  X  [ M ] are distributed as follows: where  X  2 0 is the noise variance.

Moreover, we assume as in the previous section that profile u follows a zero-mean Gaussian prior with covariance  X  2 u and that the type follows a uniform prior ( i.e. , each of the two types is equally likely), i.e.:
Under the above assumptions, it is natural to classify the incoming user using maximum a posteriori estimation of the type t . In particular, FBC amounts to Under this notation, FBC can be determined as follows: Theorem 1. Under noise model (5) and prior (6) , the FBC classifier is given by V The proof can be found in the extended version of our pa-per [3]. There are several important observations to be made regarding FBC , as defined by Theorem 1.
 Set of Classifiers. We first note that FBC in fact defines a set of classifiers, each parametrized by set A  X  M : each such classifier  X  t : R | A |  X  X  X  1 , +1 } takes as input any possi-ble set of ratings r A  X  R | A | as input. Note however that all classifiers are trained jointly from the ratings dataset: this  X  X raining X  amounts to determining the item profiles V and the item biases Z through matrix factorization. With V and Z learned, when presented with ratings r A , the classifier can compute the vectors  X  r A ,  X  A and the matrix M A needed to determine the type. Indeed, the fact that training the clas-sifier amounts to computing the latent factors/item profiles is consistent with the observation that FBC shares the same underlying Bayesian assumptions as matrix factorization.
The second task in designing this threat is to find a user X  X  type quickly . In what follows, we present two strategies for addressing this problem. The first is a passive strategy: the recommender presents items to the user in a predetermined order, computed off-line. The second is an active strategy: the recommender selects which item to present to the user next based on the answers she has given so far. Both strate-gies are extensively evaluated in Section 6. Algorithm 1 FBC -Selection
A simple, passive method for presenting items to the user is to (a) sort items j  X  X  with respect to |  X  j | , the absolute value of the gap between the type biases, and (b) present items to the user in decreasing order. We call this strategy MaxGap : intuitively, this method identifies the most dis-criminative items in the dataset, and solicits responses to these items first. Clearly, MaxGap does not take into ac-count (or adapt to) how the user rates the items presented so far. Despite this limitation, as we will see in Section 6, this simple strategy actually performs surprisingly well in many cases, especially when there exist many highly discrimina-tive items. When this is not the case, however, an active strategy is needed, motivating our second method. Our active method, FBC -Selection , is summarized in Algorithm 1. Let  X  t be the FBC classifier defined by (8). Given observed ratings r A  X  [ r j ] j  X  A  X  R | A | , for some A  X  M , we define the risk L (  X  t ( r A )) of the classifier to be 0 if the prediction is correct, and 1 otherwise. Conditioned on r the expected risk is E [ L (  X  t ( r A )) | r A ] = 1  X  Pr( i.e. , it equals the 1 minus the confidence of the classifier, the posterior probability of the predicted type, conditioned on the observed ratings. Since, by (7), FBC selects the type that has the maximum posterior probability, the expected risk is at most (and the confidence at least) 0.5.

FBC -Selection proceeds greedily, showing the item that minimizes the classifier X  X  expected risk at each step. More specifically, let A be the set of items whose ratings have been observed so far. To select the next item to present to the user, the algorithm computes for each item j  X  M\ A , the expected risk E [ L (  X  t ( r A  X  r j )) | r A ] if rating r
R This expected risk depends on the distribution of the unseen rating r j conditioned on the ratings observed so far.
Under noise model (5) and prior (6), the expected risk for each item j can be computed in a closed form. In particu-lar, let M A ,  X  r A ,  X  A be as defined in Theorem 1. Then, the expected risk when revealing the rating of item j is propor-tional to the following quantity, derived in [3]: where A j = A  X  { j } . The integration above is w.r.t. r i.e. , the predicted rating for item j . The outcome of the above integration can be computed in closed form in terms of the error function erf (i.e., no numerical integration is necessary ). The formula can be found in [3]. Each iteration amounts to computing the  X  X cores X  L j for each item j not selected so far, and picking the item with the lowest score (corresponding to minimum expected risk). Once the item is presented to the user, the user rates it, adding one more rating to the set of observed ratings. The process is repeated until the confidence of the classifier (or, equivalently, the expected risk) reaches a satisfactory level. FBC-Selection requires the computation of the scores L j after each interaction with the user. Each such calcula-tion involves computing the determinant det( X  A j ), as well as the matrix M Aj = ( I  X  V A j  X   X  1 A pear in (9). Though having a closed form formula for (9) avoids the need for integration, computing each of these matrices directly from their definition involves a consider-able computational cost. In particular, the cost of com-puting  X  A =  X I + V T A V A is O ( d 2 | A | ). Computing  X  and det( X  A j ) have a cost O ( d 3 ) multiplications using, e.g. , LU-decomposition, which can be dropped to O ( d 2 . 807 ing Strassen X  X  algorithm for multiplication [7]. Finally, the computation of M A requires O ( | A | X  d 2 + | A | 2  X  d ) multipli-cations. As a result, the overall complexity of computing L mance of these computations can be significantly reduced by constructing these matrices incrementally: M A j ,  X   X  1 det( X  A j ) can be computed efficiently from M A ,  X  det( X  A ), exploiting the fact that  X  A j =  X  A + v j v T results from  X  i through a rank-one update. We discuss this below.
 Incremental computation of det( X  A j ) . The determinant can be computed incrementally using only O ( d 2 ) multiplica-tions through the Matrix Determinant Lemma [11], namely: Incremental computation of  X   X  1 A one update of a matrix can be computed through the Sherman-Morisson formula [27], which gives: and again reduces the number of multiplications to O ( d 2 Incremental computation of M A j . Finally, using (11), we can also reduce the cost of computing M A j , as: where  X  = V A ( X   X  1 A the computation cost to O ( | A | 2 + d 2 ) multiplications.
In conclusion, using the above adaptive operations re-duces the cost of computing L j by one order of magnitude to O ( | A | 2 + d 2 ) , which is optimal (as M A is an | A | X | A | matrix, and  X  A is d  X  d ). The rank-one adaptations yield such performance without sophisticated matrix inversion or multiplication algorithms, such as Strassen X  X  algorithm. The we refer to resulting algorithm as IncFBC ; we empirically compare the two implementations in Section 6. Algorithm 2 PointEst Active Learning
An alternative method for selection can be constructed by replacing the exact estimation of the expected risk with a  X  X oint estimate X  (see also [28]). In fact, such a selection method can be easily combined with an arbitrary classifier that operates on user-provided ratings as input. This makes such an approach especially useful when the expected risk is hard to estimate in a closed form. We therefore outline this method below, noting however that several problems arise when the risk is computed through such a point estimation.
We describe the method for a general classifier C , also summarized in Algorithm 2. Given a set of ratings r A over a set A  X  X  , the classifier C returns a probability Pr C ( t | r for each type t  X  T . This is the probability that the user X  X  type is t , conditioned on the observed ratings r A a set of observed ratings r A , we can estimate the type of the user using the classifier C though maximum likelihood a-posteriori estimation, as  X  t ( r A ) = arg max t  X  X  Pr Using this estimate, we can further estimate the most likely profile  X  u  X  R d through ridge regression [12] over the ob-served ratings r A and the corresponding profiles V A (see Al-gorithm 2 for details). Using the estimated profile  X  u and the estimated type  X  t , we can predict the rating of every item j  X  M\ A as  X  r j =  X  u T v j + z j  X  t , and subsequently es-timate the expected risk if the rating for item j is revealed as min t  X  X  Pr C ( t | r A  X   X  r j ) . We refer to this as a  X  X oint esti-mate X , as it replaces the integration that the expected risk corresponds to with the value at a single point, namely, the predicted rating  X  r j .
 Using such estimates, selection can proceed as follows. Given the set of observed ratings A , we can estimate the risk of the classifier C for every item j in M\ A through the above estimation process, and pick the item with the mini-mum estimated risk. The rating of this item is subsequently revealed, and new estimates  X  t and  X  u can thusly be obtained, repeating the process until a desired confidence is reached.
Clearly, point estimation avoids computing the expected risk exactly, which can be advantageous when the corre-sponding expectation under a given classifier C can only be computed by numerical integration. This is not the case for FBC , as we have seen, but this can be the only tractable option for an arbitrary classifier. Unfortunately, this es-timation can be quite inaccurate in practice, consequently leading to poor performance in selections; we observe such a performance degradation in our evaluations (Section 6). Put differently, a point estimate of the risk takes into ac-count what the predicted rating of an item j is in expec-tation , and how this rating can potentially affect the risk; however, it does not account for how variable this prediction is. A highly variable prediction might have a very different expected risk; the exact computation of the expectation does take this into account whereas point estimation does not.
In this section we evaluate the performance of our methods using real datasets. We begin by describing the datasets and experiments, then perform a comparative analysis of both passive and active methods. Datasets. We evaluate our method using three datasets: Movielens, Flixster [13], and Politics-and-TV (PTV) [26]. The Movielens dataset includes users X  ratings for movies alongside with the users X  gender and age. For simplicity, we categorize the age group of users as young adults (ages 18 X 35), or adults (ages 36 X 65). Flixster is a similar movie ratings dataset, and contains user gender information. PTV includes ratings by US users on 50 different TV-shows, along with each user X  X  gender and political affiliation (Democrat or Republican). We preprocessed Movielens and Flixster to consider only users with at least 20 ratings, and items that were rated by at least 20 users. Since PTV includes only 50 TV-shows, we preprocessed the data to ensure that each user has at least 10 ratings. Table 1 summarizes the datasets used for evaluation. For each user type, the table shows the ratio between the number of users of one type versus the other type (as labeled in the table). Further details on the datasets can be found in [3].
 Evaluation Method. In our setting, the recommender sys-tem infers user attributes from a set of strategically selected items. To understand the effectiveness of FBC compared to other classification methods in an adversarial setting, we perform the following evaluation. We first split the dataset into a training set (e.g., users that are willing to disclose the private attribute) and evaluation set (e.g., users that are privacy-sensitive), and train different classifiers on the training set  X  e.g., in the case of FBC we learn the item pro-files and biases. Then, for each user in the evaluation set, we incrementally select items for the user to rate. In the passive methods, the selection of next item does not depend on the previous ratings provided by the user, whereas in the active methods it does.

After the user rates an item, we use the classifier to infer the private type. For any user, since we only have the rating information for the set of movies that she has rated, we limit the selection process to this set. Users may have rated different number of movies, for instance, roughly 50% of the users of Movielens have rated less than 100 movies out of 3000 (see [3]). Therefore, we limit the number of questions asked to 100 for Movielens and Flixster and all 50 for PTV. Unless specified, all evaluations of FBC were done using the efficient incremental implementation IncFBC . Figure 1: Cumulative distributions of the type bias ( | z the different datasets, zoomed to the top 20% of items. Evaluation Metrics. We evaluate classification perfor-mance through the area under the curve (AUC) metric, and prediction performance through the root mean squared er-ror (RMSE). If a recommender system uses our method to maliciously learn user features, it is important that such a mechanism for strategic solicitation of ratings has a mini-mal impact on the quality of recommendations, otherwise the user may detect its hidden agenda. We measure the quality of recommendations by holding out an evaluation set of 10 items for each user. After every 10 questions (so-licited ratings) we predict the ratings on the evaluation set by applying ridge regression using the provided ratings and item profiles to learn a user profile. We predict the ratings on the evaluation set and compute the RMSE over all users. Parameter settings. We split each dataset into training and testing and perform MF with 10-fold cross validation. We learn the item latent factors required by FBC using the training set, with type biases for age, gender and political affiliation as applicable to the three datasets. For MF, we use 20 iterations of stochastic gradient descent [17] to mini-mize (3), using the same regularization parameter for users and movies. Through 10-fold cross validation we determined the optimal dimension to be d = 20, and the optimal reg-ularization parameter to be 0 . 1, for each of the biases. We also compute the optimal  X  used in the classifier (8) through 10-fold cross validation to maximize the AUC, resulting in  X  = 100 for gender and  X  = 200 for age for the Movielens dataset,  X  = 10 for gender and political views for the PTV dataset, and  X  = 200 for gender for the Flixster dataset. Figure 2 shows the AUC and RMSE obtained using Max-Gap , and two other passive methods  X  Random and En-tropy. For reference, we also show the performance of our active method IncFBC . Random selection is a natural base-line as users may rate items in any arbitrary order. The second method, Entropy, presents items to the user in de-scending order of their rating entropy, i.e., start with items that have polarized ratings. This method was shown to be efficient in a cold-start setting in [24] as it can quickly build user profiles in a matrix factorization based recommender system.
 AUC. Figure 2a shows that MaxGap performs significantly better than the other passive methods on both Movielens and PTV datasets. For the first 10 questions, which are critical when considering the need for quick inference, it is the best passive method on all datasets. Interestingly, on Movielens-Gender and PTV-Politics, MaxGap performs very similar to the adaptive, more complex IncFBC . As a result of the greedy nature of MaxGap , we expect it to per-form well on datasets that have items with large biases. To better understand MaxGap  X  X  performance, Figure 1 shows the top 20% of the cumulative distributions of the type bi-ases over the set of items in the different datasets. The plot clearly shows that the items in PTV-Gender have the low-est biases, resulting in the poorest performance of MaxGap Conversely, in Movielens-Gender, the biases are the largest, thus MaxGap performs well, in par with the adaptive In-cFBC . In PTV-Politics the biases are not overly high, but since most users rate all items, a few discriminating items are sufficient to enable MaxGap to perform well. This ob-servation is supported by the findings of [6] that identified 5 TV-shows that immediately reveal a user X  X  political views. RMSE. Figure 2b plots the RMSE over increasing number of rated items, for MaxGap , Random, and Entropy, along with IncFBC . Even though IncFBC and MaxGap are de-signed to explore a specific attribute of the user X  X  profile, they perform very well. Their RMSE is very close to that of Random and Entropy, with the MaxGap visibly worse only in one case, the PTV-Gender dataset. Since IncFBC and MaxBias focus on quickly learning a singe attribute of the user X  X  profile, it is expected that they perform worse than the other methods, that aim to explore attributes more broadly. However, the figures show that IncFBC and MaxGap are almost identical to the other methods, and MaxGap only perform worse in the PTV-Gender dataset. Moreover, in all datasets, IncFBC performs close to a random selection, in-dicating that IncFBC does not incur significant impact on the RMSE relative to an arbitrary order in which a user may rate items. Finally, IncFBC has an RMSE similar to the entropy method, which is designed to improve the RMSE in a cold-start setting.

These results show that a malicious recommender system that uses IncFBC to infer a private attribute of its users can also use the solicited ratings to provide recommendations, making it difficult for users to detect the hidden agenda.
We compare our selection method to the logistic and multi-nomial classifiers by adapting them to an active learning setting. These classifiers were the top performing among those studied in previous work [30] for gender prediction. Following [30], we train both of these classifiers over rating vectors padded with zeros: an item not rated by a user is marked with a rating value of 0. In order to use logistic and multinomial classifiers in an active learning setting we use the point-estimate ( PointEst ) method as described in Section 5.4 (see Algorithm 2). For the remainder of this sec-tion we refer to PointEst with a logistic and multinomial classifiers as Logistic and Multinomial , respectively. AUC. Figure 3a plots the AUC of classification for a given number of question using PointEst and IncFBC selection, for all datasets. PointEst selector enables us to compare FBC with the other classifiers for which we do not have a closed-form solution for selection. In all datasets, the plots show that IncFBC significantly outperforms both logistic and multinomial within a few questions, and reaches an im-provement in AUC of 10 X 30% in the Movielens and Flixster datasets. PointEst with the other classifiers is unable to achieve a good classification accuracy.

To put this in perspective, Table 2 shows the performance of these classifiers, and that of a non-linear classifier SVM with RBF kernel, using all user ratings. Note that this table Flixster-Gender. PTV-Political Views, Flixster-Gender. considers all ratings performed by all users in each dataset, whereas the plots in Figure 3a show the average AUC com-puted over the users that have rated the indicated number of questions. Logistic and in some cases multinomial classifiers perform significantly better than FBC , when classifying over the entire dataset. This shows that although any of these classifiers could be used for a static attack [30], FBC is bet-ter suited to adaptive attacks with fewer available ratings. For instance, using IncFBC with just 20 movies per user we obtain a classification accuracy that is reasonably close to that obtained by static inference techniques which use the complete dataset.
 RMSE. For completeness, Figure 3b provides the RMSE using the different active methods, showing that IncFBC has a lower RMSE on almost all datasets. Figure 4: Running time improvement of IncFBC over FBC . Running Time. Finally, we seek to quantify the improve-ment in execution time obtained by the incremental compu-tations of IncFBC . We ran both FBC and IncFBC on a commodity server with a RAM size of 128GB and a CPU speed of 2.6GHz. Figure 4 shows the average time per movie selection for both FBC and IncFBC for increasing number of questions (movies presented to the user). The error bars depict the 95% confidence interval surrounding the mean. The plot shows that when the number of questions is small the time per question is relatively constant, and increases with the number of questions. As discussed in Section 5.3, when the number of questions is smaller than the dimension of the factor vectors (in our case d = 20), the complexity of the efficient algorithm is dominated by d . In the first few questions FBC is slightly faster than IncFBC as a result of the efficient implementation of inversion for small matri-ces. However, as the matrix becomes larger, the size of the matrix dominates the complexity and the incremental com-putations performed in IncFBC are significantly faster than FBC , reaching a speedup of 30%.
We presented a new attack that a recommender system could use to pursue a hidden agenda of inferring private at-tributes for users that do not voluntarily disclose them. Our solution, that includes a mechanism to select which ques-tion to ask a user, as well as a classifier, is efficient both in terms of the number of questions asked, and the runtime to generate each question. Moving beyond binary attributes to multi-category attributes is an interesting open question. Exploring the attack from the user X  X  perspective, to better advise users on ways to identify and potentially mitigate such attacks is also an important future direction.
