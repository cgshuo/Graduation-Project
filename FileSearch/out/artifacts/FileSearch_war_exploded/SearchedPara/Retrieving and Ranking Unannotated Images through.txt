 We present a new image search and ranking algorithm for retrieving unannotated images by collaboratively mining online search results which consist of online image and text search results. The online image search results are leveraged as reference examples to per-form content-based image search over unannotated images. The online text search results are utilized to estimate the reference im-ages X  relevance to the search query. The key feature of our method is its capability to deal with unreliable online image search results through jointly mining visual and textual aspects of online search results. Through such collaborative mining, our algorithm infer-s the relevance of an online search result image to a text query. Once we obtain the estimate of query relevance score for each on-line image search result, we can selectively use query specific on-line search result images as reference examples for retrieving and ranking unannotated images. We tested our algorithm both on the standard public image datasets and several modestly sized personal photo collections. We also compared our method with two well-known peer methods. The results indicate that our algorithm is superior to existing content-based image search algorithms for re-trieving and ranking unannotated images.
 H.5.1 [ Information Interfaces and Presentation ]: Multimedia Information Systems; I.4.9 [ Image Processing and Computer Vi-sion ]: Applications; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Query Formulation, Retrieval Models, Search Process  X  C orresponding author: S. Xu; contact him at xus1 " at " orn-l " dot " gov.  X  This manuscript has been authored by UT-Battelle, LLC, un-der Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publish-er, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irre-vocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes.
 Algorithms, Experimentation, Performance Multimedia information retrieval, web information retrieval, online reference image collection, query processing, retrieving unannotat-ed images, web search mining
Today, many people need to manage their photos, which by no means is an easy task especially when the photo collection becomes very large. One of the frequent management tasks is to search for a photo. Using the mainstream image search tools, which mostly rely on traditional text based retrieval methods, can be cumbersome and frustrating because ordinary photos are rarely annotated. If there are annotations, they tend to be terse, subjective, and biased.
To enable searching and ranking unannotated images, we intro-duce a new content-based image retrieval algorithm by leveraging online image search results X  X sing them as reference examples to inform and guide the unannotated image search and ranking pro-cess. A key feature of our algorithm is its capability of automati-cally inferring the relevance of an online image search result to its corresponding textual query through collaboratively mining both visual contents of and text clues associated with the online image search result. After having derived the estimate of image-to-query relevance for each reference image obtained from the online image search step, we can further compute the relevance of an arbitrary unannotated image to a textual query according to the unannotated image X  X  collective visual similarity with the entire reference im-age set. We calculate the collective visual similarity between an unannotated image and the reference image set by accumulating the individual pairwise image visual similarities between the unan-notated image and each example image in the reference image set. During this accumulation process, we also take into account the query relevance score of each reference image example. All in all, an unannotated image that has higher visual similarities with more query relevant example images tends to obtain a higher collective visual similarity score. Finally, based on the derived collective vi-sual similarity score for each candidate unannotated image with respect to the reference image set, we can search and rank all the unannotated images accordingly.
Fergus et al. [9] proposed a method that dynamically derives a visual object model through learning from Google image search results. Their method however does not leverage the annotati on or description text of an online image. Popescu et al. [1] pro-posed a content-based image retrieval method using an ontology driven approach for improving image retrieval precision. Kidambi and Narayanan [17] introduced a human computer integrated ap-proach for content-based image retrieval, taking advantage of user feedbacks. Hsiao et al. [12] represented an image as a visual bag-of-words followed by a relevance feedback scheme for ranking im-ages. Jing and Baluja [15] adapted Google X  X  PageRank algorithm for product image search. Their approach is based on image sim-ilarities estimated from low-level visual features proposed in [23]. Compared with our algorithm, their method does not look into the text clues associated with an image. Liu et al. [22] gave a high-level overview of content-based image retrieval research from the perspective of semantics understanding. For more comprehensive reviews on recent progress in content-based image retrieval and we-b image search, readers are referred to [27, 16, 5, 22, 28, 24]. These are also many surveys on domain specific content-based image re-trieval methods and systems, e.g. Muller et al. X  X  survey [25] on content-based image retrieval for medical applications.
Lin et al. contributed a probabilistic model based algorithm [19] for re-ranking image search results according to images X  surround-ing texts. In comparison, our algorithm does more X  X t also explores the visual content of an image. Cai et al. [3] proposed a hierarchic image clustering method through jointly analyzing visual, textual, and link information of an image. However, their algorithm is pro-posed for clustering images, which cannot be directly applied to retrieving or re-ranking images. Zhou and Huang X  X  image retrieval algorithm [31] jointly uses the keywords and the visual content of an image for image retrieval. But their method uses reference im-ages in isolation during the image retrieval process whereas our algorithm considers the collective image similarity between a giv-en candidate unannotated image and an entire reference image set. Jia et al. [14] proposed a personal album annotation algorithm. Their algorithm and Feng et al. X  X  [7] algorithm, similar to ours, use clues from the visual content and HTML text of an image for image content understanding. Jia et al. X  X  method is a supervised learning approach, which needs a set of pre-labelled images as training sam-ples, whereas our method does not need any human labels. Feng et al. X  X  method independently trains two separate classifiers, one us-ing text features and the other using visual features of an image. In contrast, our method collaboratively analyzes the visual and tex-t aspects of online image search results in one unified procedure. Through our integral analysis based on image and text data fusion, our method can adaptively prioritize relevant online reference im-ages. As a result, our algorithm can deal with noisy reference im-age samples more superbly than Feng et al. X  X  algorithm, which will be proved through our experiments. Recently, Liu et al. [20] [21] developed a textual query based personal photo retrieval system, which also uses web image search results to facilitate querying unannotated personal photos. However, their method focuses on leveraging the relevance feedback strategy to deal with the noisy unreliable online image search results. In contrast, our method col-laboratively mines the visual contents and text clues accompany-ing each online image search result to discriminate reliable online image search results from irrelevant ones. Compared with their relevance feedback based strategy, which requires end user partic-ipation, our method can autonomously apply online image search results in a selected manner to guide the unannotated image search and ranking process. The hybrid mining feature of our algorithm allows our method to automatically cope with the noisy and unre-liable nature of web image search results. It is possible to further strengthen the image search capability of our algorithm by adopt-ing the relevance feedback strategy proposed in Liu et al. X  X  work, which is a meaningful topic for future exploration. Fergus et al. [8] proposed an algorithm that learns object categories from Google X  X  image search results. The key part of their algorithm is a model that considers spatial and scalar information of images for learning the potential categories and tags of Google search result images. They also incorporatd an incremental learning algorithm [18] in their method to support relevance feedback by end users. People have also explored the idea of mining web text search results for establishing tagged image databases. For example, Schroff et al. [26] proposed to build image databases by collecting images with-in webpages of web text search results. In their method, they utilize the textual contents of web search results for image tag identifica-tion and selection.
Given a textual query Q , we first use a third party commercial image search engine (Google Image Search in our current imple-mentation) to perform an online image search. The purpose is to acquire a set of reference images to perform content-based image search over the candidate unannotated image set. Assume there are  X  ( Q ) reference images obtained for the input image query Q , which are organized as a reference image set G ( Q ) , { G G ( Q ) , , G  X  ( Q ) ( Q ) } , where each G i ( Q ) is an online search result image.

It is common knowledge that not all the online image search re-sults are closely related to the query; some are completely irrele-vant. Treating these noisy online search result images indiscrimi-nately will introduce great noise and instability to our content based image search process. To overcome this problem, in our method, we estimate each reference image X  X  relevance to the image search query. The resultant image-to-query relevance scores can help our algorithm differentiate between good reference images and noisy or irrelevant ones, which improves the overall accuracy and reliability of our method. In Sec. 3.2 X  X ec. 3.4 below, we will explain how we estimate reference images X  query relevance through collaboratively mining online search results, in both the text and image domains.
To estimate each reference image X  X  relevance to a text query Q , we first compare the similarity between the text context associated with the reference image and the text context of Q . Our assumption is: the more aligned the two contexts is, the more likely the refer-ence image reflects the image search intent as expressed by the text query Q . In the following, we explain in detail our text analysis based initial image-to-query relevance estimation procedure.
Given a text query Q , we first submit the query to Google (tex-t) search and retrieve the top N text search results. In our ex-periments, the typical value assignment for N falls in [100 , 500] , which shall never exceed the total number of search result docu-ments returned by Google for the query. Let the j -th text search result document be D j ( j  X  [1 , N ]) . All these search result doc-uments form an online text search result set for Q , denoted as D ( Q ) , { D 1 , D 2 , , D N } . Next, we derive non-stop word-s X  frequency distribution in D ( Q ) to characterize these words X  re-spective importance. To balance between text search result docu-ments with different lengths, we work with relative word frequency rather than absolute word frequency. The derivation process for a word X  X  relative frequency is as follows: assume a non-stop word wd i occurs y i,j times in the document D j and the total number of occurrence of all the non-stop words in D j is m j . We then calculate the relative word frequency of wd i in D j as y cumulate wd i  X  X  relative word frequencies for all the documents in D ( Q ) and treat the sum as wd i  X  X  accumulated relative word fre-quency , t i ( Q ) , i.e. t i ( Q ) , an inverse word frequency term for each non-stop word in D ( Q ) . Assume n i is the total number of documents containing the word wd i in D ( Q ) . Then, the word wd i  X  X  inverse word frequency term with respect to D ( Q ) is formulated as ln(1 + N n accumulated relative word frequency t i ( Q ) and its inverse word frequency ln(1+ N n uct of the two terms, t i ( Q ) ln(1 + N n of wd i . Finally, we organize the TF-IDF terms of all the non-stop words occurring in D ( Q ) as a vector  X  text ( Q ) , i.e. where V ( Q ) is the number of distinct non-stop words in D ( Q ) .
It should be noted that each reference image in our method is obtained through an online image search process. A property of images obtained in this way is that every image is linked back to its source webpage where we can obtain surrounding text for the image. Denote a reference image G j ( Q )  X  X  source webpage as P j ( Q ) . The source documents of all the webpages associated with the online image search results G ( Q ) form a document set D  X  ( Q ) , { P 1 ( Q ) , , P  X  ( Q ) ( Q ) } . For each non-stop word wd that occurs in the source document of the webpage P j ( Q ) , we also calculate its relative word frequency among all the non-stop words occurring in the source document and denote the result as f In a similar procedure as described in Sec. 3.2.1, we also derive a non-stop word wd i  X  X  inverse word frequency with respect to the webpage source document set D  X  ( Q ) as ln(1 +  X  ( Q ) n  X  is the total number of webpage source documents containing the non-stop word wd i . We organize the product of the above calcu-lated relative word frequencies and inverse word frequencies for all the non-stop words occurring in P j ( Q ) as a V  X  j ( Q ) dimensional vector  X  img ( Q ) , assuming there are V  X  j ( Q ) distinct non-stop words appearing in P j ( Q ) . That is,
Previously in Sec. 3.2.1 and Sec. 3.2.2 we derived two types of text contexts, one from the online text search results and the oth-er from the webpage source document associated with an online image search result. We have also looked at how to characterize the two types of text contexts via their respective word frequency distribution vectors  X  text ( Q ) and  X  img ,j ( Q ) . Next, we derive an online reference image G i ( Q )  X  X  relevance to its query Q , which is denoted as r i ( Q ) , by measuring the semantic alignment between the two types of text contexts through computing the inner product of their respective word frequency distribution vectors. That is, where  X  i is a V ( Q )  X  V  X  i ( Q ) dimensional matrix whose element on the u -th row and v -th column is the semantic relatedness be-tween the u -th non-stop word in the online text search results and the v -th non-stop word in P i ( Q ) , measured by the algorithm pro-posed in [10]. We then treat the resulting r i ( Q ) as our initially estimated image-to-query relevance for the online reference image G ( Q ) . We organize all the estimated initial reference image to query relevance as a  X  ( Q ) dimensional vector, i.e. R init r 1 ( Q ) , r 2 ( Q ) , , r  X  ( Q ) ( Q ) . The reason why we call this the initial relevance estimation is because not all the reference images acquired through online image search are accompanied by quality annotation texts. Such unreliability in the surrounding text of on-line reference images will surely affect the quality of our text anal-ysis based image to query relevance estimation, a problem com-monly suffered by nearly all annotation text based image search methods. Hence we call the above estimation on online reference images X  relevance to the textual query the initial estimation . In the following, we will discuss how to refine this initial estimation.
Given the initial image-to-query relevance estimation for all the reference images, we then refine the estimated image-to-query rele-vance through conducting an image content similarity based propa-gation procedure, which will be presented in Sec. 3.4. A key step of the procedure is to measure the pairwise image similarity between reference images. In our method, we introduce a visual patch set based pairwise image similarity estimation method, which collab-oratively mines the online text and image search results.
In our method, we calculate pairwise reference image similarity through analyzing image content similarity at the granularity of vi-sual patch sets rather than at the whole image level. Such finer gran-ularity offers us a more elaborate analysis capability, which we will make clear shortly. To extract visual patch sets from the reference image set, we first apply the feature extraction algorithm proposed in [2] to obtain a collection of visual patches for every reference image in the reference image set G ( Q ) . These visual patches are defined as Speeded-Up Robust Features (SURF), each of which in-cludes a circular effective feature region and a feature descriptor. More concretely, we represent a visual patch v as v = ( C, Des ) , where C represents v  X  X  circular effective feature region, and Des is v  X  X  feature description vector. We further define the distance be-tween two visual patches as the L 2 distance between the description vectors of the two visual patches. Two visual patches are consid-ered identical if their L 2 distance is below 0.01.

Based on the concept of visual patches just introduced, we can define a visual patch set which is a set of visual patches detected from an image whose circular regions are overlapping. Formally, a visual patch set of size l is defined as: where v i = ( C i , Des i ) and  X  i, j = 1 , 2 , , l, v v . Here the relationship connects with is defined as follows: v i connects with v j  X  X  X  In the above,  X  represents an empty set. In the following, we use the notation |  X  | to denote the size of the visual patch set  X  .
Finding all the visual patch sets contained in a reference im-age set incurs an exponential amount of computational overhead. Fortunately, we only need to derive some most frequently occur-ring visual patch sets in our method as they provide the most es-sential evidence for estimating pairwise image similarity. Calcu-lating frequently occurring visual patch set is a much more af-fordable computational task. In our method, we extracted visual patch sets from all the reference images in G ( Q ) whose occur-rence numbers exceed a certain threshold using the method pro-posed in [30]. A typical threshold value used in our experiments is 5. Two visual patch sets are identical if they contain the same set of constituent visual patches. Let  X  i ( Q ) denote a frequent vi-sual patch set detected from the reference image set G ( Q ) . All the frequently occurring visual patch sets detected from the reference image set G ( Q ) are organized as a collection, which is denoted z ( Q ) frequent visual patch sets contained in the reference image set G ( Q ) .

It is noted that in [30], the authors also proposed to use visu-al patch sets to estimate pairwise image similarity. Unlike our method, their algorithm only detects visual patch sets containing no more than two patches; our algorithm explores visual patch sets of all sizes. The reason why their algorithm tightly limits the size of visual patch sets considered is because their algorithm relies on extracting visual patch sets from a large number of sample images to perform content-based image search, an image collection that aims to work universally for all the queries. Such an algorithm de-sign feature translates to a high degree of computational overhead in visual patch set extraction. In contrast, our algorithm is designed to use a much smaller set of sample images acquired from online image searches in a query specific manner. The query-specific na-ture of our algorithm X  X  reference image acquisition process allows us to work with a very limited number of sample images each time for performing content-based image retrieval. Such careful control of reference image set size successfully restricts the time spent on extracting visual patch sets from the reference image set. This algo-rithm feature enables us to explore visual patch sets of larger sizes in our method, leading to a richer set of image features for image content understanding.

Once we have detected the visual patch sets from all the refer-ence images, we can then analyze those patch sets commonly oc-curring in a pair of reference images for estimating pairwise image similarity. This practice is inspired by the classic T F  X  IDF based document similarity measurement method [29]. Borrowing their idea for document similarity estimation in the text domain, in our method, we attempt to estimate pairwise image similarity through counting the common visual patch sets existing among two images and in a weighted way, prioritizing co-occurrence of salient visual patch sets. The assumption is, the larger the number of salient vi-sual patch sets commonly existing among two reference images is, the more similar the two images would be.
Based on the pairwise reference image similarity estimation method introduced in the previous subsection, we now introduce a multi-step propagation procedure to refine the initial image-to-query rel-evance estimation for reference images. We call the resultant esti-mation the refined image-to-query relevance . Our work is inspired by the recent work on product image ranking by Jing and Baluja, who modified the traditional Pagerank algorithm for ranking im-ages through an iterative process [15].

First, we construct a pairwise image similarity matrix S  X  ( Q )  X   X  ( Q ) where the element on the i -th row and the j -th column of S is the image similarity between the i -th and the j -th reference im-ages, G i ( Q ) and G j ( Q ) . The pairwise image similarity is estimat-ed using the method introduced in Sec. 3.3. Noticing that the more steps we take in propagating a belief, the less reliable the propagat-ed belief tends to be, we penalize the results obtained with more steps of propagation using an attenuation factor  X  . To formally define this multi-step propagation process, we employ the matrix exponent notation [11], as follows: where S  X  ( Q )  X   X  ( Q ) is the pairwise image content similarity matrix; R init and R refined are respectively the initial and refined estimated image-to-query relevance for all the reference images;  X  is the propagation attenuation constant. A typical value assignment for  X  , as used in all our experiments reported in this paper, is 0.3.
The propagation procedure described in (6) can be concisely represented as R refined , e  X  S R init , where the matrix exponent can be efficiently computed by observing the fact that S is symmetric. Therefore, we can decompose S as S , QDQ  X  1 , where Q is an or-thogonal  X  ( Q )  X   X  ( Q ) matrix containing the eigenvectors of S , and D is an  X  ( Q )  X   X  ( Q ) diagonal matrix containing the eigenvalues of S . We can then rewrite e  X  S as follows: The truncation decision of stopping at the fifth term is emprically made in our experimentation. We plan to explore more system-atically the optimal assignment of this truncation threshold in our future work. According to this new representation, the matrix expo-nentiation can be efficiently computed by an eigenvalue decompo-sition process followed by a few matrix multiplication and addition operations. As mentioned earlier, the constant  X  is the propagation attenuation parameter that penalizes the results obtained through multiple steps of propagation.
 Combining (6) and (7), we have: Each time when a query Q is submitted, the initial image-to-query relevance score R init is first estimated and then the corresponding R refined can be derived through matrix multiplications and addition-s according to (8). The i -th component of R refined is denoted as r j ( Q ) , which represents the j -th reference image G j ( Q )  X  X  rel-evance to the query Q .
Given an arbitrary unannotated image I x , to estimate I x vance to the textual query Q , we refer to a set of reference images highly similar to I x and use these reference images X  query rele-vance for estimating I x  X  X  relevance to the query. Our idea is to leverage the  X  X ntelligence of the crowd, X  i.e., if multiple reference images that are highly relevant to the query exhibit a high visual similarity with I x , I x is then more likely to be closely related to the query.

More concretely, given a textual query Q , we perform an online image search to obtain a collection of reference images G ( Q ) = { G 1 ( Q ) , G 2 ( Q ) , , G  X  ( Q ) ( Q ) } (Sec. 3.1). In Sec. 3.2 X  X ec. 3.4, we have explained our method for estimating an arbitrary refer-ence image G j ( Q )  X  X  relatedness to the query Q , which is denoted as r refined j ( Q ) . We have also explained our method to extract fre-quent visual patch sets from G ( Q ) , which are denoted as  X  ( Q ) = {  X  k } , where each  X  k is an extracted frequent visual patch set (see Sec. 3.3).

For each visual patch set  X  k , we first estimate its relevance to the query Q . Since we have estimated each reference image G relevance to the query Q , our method estimates each visual patch set X  X  relevance to the query by distributing the image to query rele-vance to the individual visual patch sets X  relevance to the query. To realize the above idea, we first estimate the salience of each visual patch set contained in a reference image G i . Again, we borrow the traditional TF-IDF scheme in the text processing domain for char-acterizing a visual patch set X  X  salience. Let m j (  X  k ) be the number of times the visual patch set  X  k occurs in the j -th reference image G ( Q ) . Then we can mimic the TF term for the visual patch set  X  as (1+ln m j (  X  k )) . Let m (  X  k ) be the number of images in the ref-erence image collection G ( Q ) that contain the visual patch set  X  Then we can emulate the IDF term for  X  k as ln(1 +  X  ( Q ) call that  X  ( Q ) is the total number of images in the reference image collection. Overall, we define the TF-IDF term for the visual patch the relative salience of the visual patch set  X  k among all the visual patch set contained in the reference image G i as follows: According to the relative salience of each visual patch set contained in G i , we then proportionally attribute G i  X  X  relevance score to the query Q to individual visual patch sets present in the image. That is, the fraction of query relevance score allocated for the visual patch ence image G j ( Q )  X  X  refined estimated relevance score to the query Q . Finally, we derive  X  k  X  X  overall relevance score to the query, de-noted as  X  (  X  k , Q ) , by summing up the fractional relevance scores  X  k acquired from all the reference images, i.e.:
With every visual patch set X  X  query relevance estimated through (10), we can now estimate an unannotated image I x  X  X  image-to-query relevance, denoted as b  X  ( I x , Q ) , based on the query relevance of all the visual patch sets carried in I x , as follows: Overall, via (11), we can estimate an unannotated image X  X  relative-ness to a textual query through weighted sum of the query relevance of all the visual patch sets present in the image.

Finally, given an input query Q , each search candidate image I will be assigned a query relevance score b  X  ( I x , Q ) via (11). Those candidate images whose estimated query relevance scores are above a certain threshold will be returned as image search results for the query. All the search result images are also ranked in a descendent order according to each image X  X  estimated query relevance score.
We invited eight volunteers to participate in our experiments con-ducted on two standard image datasets: the Caltech101 image col-lection [6] and the MIRFLICKR collection [13]. We compared the measured performance of our algorithm with that of two peer content-based image retrieval algorithms. All our experiments were performed on a desktop computer with 2.66GHz Intel Core 2 Duo CPU and 2GB main memory, running Windows XP.
We evaluate the quality of an image search and ranking result by adopting the normalized discounted cumulative gain (NDCG) metric. The participating users would assign a numerical label to each of the top N top search result images to indicate their opinions on these images X  relevance to the corresponding queries. The nu-meric label, in the range of [0 , 1] , represents the image X  X  relevance with the query ( 1 :  X  X ully relevant X ; 0 :  X  X ully irrelevant X ). For those image search experiments performed on the standard image col-lections, since all the images in the collection have been carefully classified and labeled, we automatically assign a binary value of either 0 or 1 to indicate whether a search result image is relevant to the query or not. In all the experiments reported in this paper, we restrict N top to be no more than 50 .
Much of the execution time of our algorithm is spent on down-loading online reference images through Google image search. For a typical query performed in our experiments for the Caltech101 data set, we use the top 50 images retrieved from Google image search as the reference image collection, i.e.  X  ( Q ) = 50 . Table 1 shows the run-time breakdown statistics for each part of our algo-rithm. The exact duration of downloading time needed varies from query to query, which also heavily depends on the quality of a par-ticular user X  X  Internet connection. For real world deployment, our algorithm can run on the server side in order to avoid the reference image downloading time. Therefore, in our analysis, we exclude the online reference image downloading step.
The Caltech101 image collection contains 8677 images, pre-labelled with 101 textual tags, which has been widely used as a benchmark for image retrieval and object recognition. We use each textual tag as the image query keyword and take all the 8677 images in the entire Caltech101 image collection as the candidate image search result set. Therefore, all together, we would perform 101 sessions of image retrieval experiments over the collection. At the end of each image querying experiment, we evaluate the performance of our algorithm by calculating its NDCG score for the image query-ing session. If the search result image comes from the then chosen image class, it is considered a correct image retrieval result; other-wise, it is a false retrieval result. We also perform the same image query sessions on the MIRFLICKR collection, which consists of Step 1: Download reference images ISP-dependent
Step 2: Estimate initial image-to-query Step 3: Detecting visual patch sets 10  X  4 0 sec.

Step 4: Refine image-to-query
Step 5: Retrieve and rank unannotated Table 1: Run-time breakdown of different components of our a lgorithm for performing a typical image search query in our experiments. We report the time consumed by every step of our algorithm individually. 25000 pre-tagged images that were used in recent years for Image-CLEF cross language image retrieval tasks [4].

For comparison purposes, we also implemented two recent im-age retrieval algorithms respectively proposed in [21] and [7], which we abbreviate as  X  X XTJ X  and  X  X SC X  in our following discussions. We chose these two algorithms to compare with because they share the same idea of mining online information for image search as our method; the two peer algorithms also achieve today X  X  leading performance among all existing methods for image search. During our comparison, we apply the three algorithms respectively over the Caltech101 and MIRFLICKR collections for image retrieval exper-iments. Note that the  X  X SC X  algorithm requires a labelled training set. We employed the 10-fold cross validation as our evaluation strategy.
In Figure 1(a) and Figure 2(a), we report the NDCG scores ob-tained by three algorithms respectively over the image querying sessions. All these results are generated fully automatically, i.e. no user feedback is provided for the algorithms. From these results, we can clearly see that our algorithm consistently outperforms the other two algorithms.
In practice, a frequent strategy for improving the performance of an image retrieval system is to incorporate relevance feedback. For example,  X  X XTJ X  proposed a relevance feedback based approach to enhance their image classifiers X  performance interactively via cross-domain learning. To explore our algorithm X  X  image search capability and potential more thoroughly, we also conduct exper-iments with relevance feedback. In our comparative study, we perform three rounds of relevance feedback when executing each query. In each feedback round, a user randomly selected from our participation group is asked to mark one relevant image and one ir-relevant image among the top N top (usually 50 ) images in the query result. For the algorithm of  X  X XTJ, X  their method is designed with a relevance feedback module and hence can directly use the pro-vided relevance feedback data to improve its image search result. Our algorithm however is not equipped with a relevance module because we focus on providing a fully automatic mechanism for image search, aiming at the most convenient user experience. For a fair comparison, when comparing search results of our algorithm with those of LXTJ under the relevance feedback setting, we intro-duce a trivial relevance feedback step for our algorithm X  X f a user indicates a negative search result item, i.e. the item is not related (a) without relevance feedback (b) with relevance feedback F igure 1: NDCG scores of image search experiments conducted over the MIRFLICKR image set using  X  X XTJ, X   X  X SC, X  and our algorithm respectively. We illustrate the key statistics of all the NDCG scores using boxplots. to the query, we simply remove the item from the returned search image list. If a user indicates a positive search result item, our al-gorithm simply ignores the user input. Figure 1(b) and Figure 2(b) report the respective NDCG scores of image search experiments performed over two public image datasets by our algorithm and LXTJ with a three-round user feedback. The results show that our algorithm still outperforms the LXTJ algorithm with three rounds of user feedback.
We aim to more realistically emulate users X  image search be-haviors and scenarios in reality. This time, when doing an image retrieval, instead of using the original query keyword, we use a syn-onym of the keyword as the query text. For example, for the query keyword of  X  X otus, X  we use  X  X ily X  or  X  X ily lotus X  or  X  X otus flower X  or  X  X ater-lily X  as the query text in our image retrieval experiments. For this new set of experiments, we randomly select 10 image tags from the Caltech101 collection as our retrieval target objects to per-form ten image retrieval experiments, one experiment for each tag. In each experiment, we invited five volunteers. We asked each vol-unteer to give a few words that could best describe the common content of a few randomly selected images from the same image tag in the Caltech101 collection. In this process, we hide the orig-inal image tag to these volunteers; and the answer given by each volunteer is kept hidden from other volunteers. Therefore, each volunteer needs to come up with a name for the image tag inde-pendently. For each image tag, if there is a word that appears in at least three volunteers X  description texts, we will accept the word as a commonly agreed user nominated search query for the image query. If there are more than one word commonly named by more than three volunteers during our image querying experiments, we will randomly choose one word as the keyword to compose the im-age query. We designed this set of experiments to emulate image search scenarios in reality when the query text submitted by the end user does not use exactly the same word as the labels of an online image. For the  X  X SC X  algorithm, again half of images in each class of Caltech101 were used as the training set and the other half were used as the testing set. Figure 3 shows key statistics on the results of our image search experiments achieved by all three algorithms. F igure 2: NDCG scores of 101 image search experiments con-ducted over the Caltech101 image set using  X  X XTJ, X   X  X SC, X  and our algorithm respectively. We report the NDCG scores for all 101 image search sessions (a), (b) and also illustrate their key statistics using boxplots (c), (d). For easy reading, all the NDCG scores are sorted in the ascending order according to the NDCG score of our method. In (a) &amp; (c) we compare our algorithm X  X  performance with that of LXTJ and FSC, all with-out using any relevance feedback measurement; in (b) &amp; (d) we compare the performance of our algorithm and that of LXTJ, both using three rounds of relevance feedback.
 F igure 3: Statistics of the NDCG scores attained by  X  X XTJ, X   X  X SC, X  and our method respectively, in our experiments simu-lating real-world user search scenarios. (a) lists user nominated query keywords. In (b), we illustrate NDCG scores using box-plot diagrams, where the box-plots  X  X asic X  and  X  X xtended X  re-spectively indicate search scores achieved by our method when we use the exact image class names (basic) and the user nom-inated keywords (extended) as image search queries to perfor-m image searches. We also provide search scores achieved by  X  X XTJ X  and  X  X SC X  for image searches using user nominated search keywords for comparison.
 These results again demonstrate the superiority of our algorithm to the other two peer methods.
Eight volunteers were invited to use the prototype image search system that implements our algorithm to retrieve images from their respective personal photo albums. The number of photos in a vol-unteer X  X  personal photo album is between 500 and 4000. The num-ber of photos in the eight volunteers X  personal photo albums are 1172 , 729 , 2305 , 671 , 1380 , 2768 , 3884 , 3374 respectively.
We ask every volunteer in our study to perform 30 image search-es. For each volunteer, his or her 30 image search queries are for-mulated by the volunteer himself or herself by freely choosing 30 search keywords. We then rank all the image search query words suggested by the eight volunteers according to the frequency that a query word was commonly suggested by multiple users. Once all the query words are ranked, we select the top 20 most frequent-ly suggested query keywords. Figure 4(a) lists these twenty image query keywords used in our experiments. Figure 5 shows some ex-ample image search results by our algoirthm in our personal photo retrieval experiments. Image searches are performed using these query words over each user X  X  personal photo collection. Afte r that, we evaluate the NDCG score for each query performed over a us-er X  X  personal photo collection, following the procedure introduced earlier in Sec. 4.1. For each user, we perform twenty image search sessions, where each time we use one of the twenty image search keywords as the image search query and performed the search ses-sion over the user X  X  personal photo collection. Hence for each user, we obtain twenty NDCG scores, one for each image query session performed. Figure 4(b) reports the key statistics on the distributions of these twenty NDCG scores attained by our method for personal photo search experiments performed for every user respectively.
For comparison, we also report the NDCG scores of personal image retrieval experiments, performed using  X  X XTJ X  and  X  X SC X  respectively, under the same experiment setup. Concerning the  X  X SC X  algorithm, we notice that the algorithm cannot be run in a straight-forward manner on a personal photo album because it needs a labelled training set. To enable the  X  X SC X  algorithm to run in our comparison, we first retrieve the top 500 images for each query keyword using Google image search as its training set. In Figure 4, we use yellow, red, and blue boxplots to respectively illus-trate the statistic distributions of search scores obtained using  X  X X-TJ, X   X  X SC, X  and our algorithm when performing twenty personal image retrieval sessions for every user in our study. According to results shown in the figure, for all the subjects in our user study, the image search scores obtained using our algorithm are noticeably higher than the image search scores of the other two peer methods, which demonstrates the general superiority of our algorithm to the two existing methods for retrieving unannotated personal photos.
We have introduced a new content-based image search and rank-ing algorithm for unannotated images. Our algorithm infers the relevance of an unannotated image to a text query through a set of reference images from online image search. To deal with noisy ref-erence image samples, we introduced an elaborate joint text anal-ysis and image visual content analysis procedure to set apart the good and the bad (i.e., irrelevant) reference images. Then through a weighted visual phrase based scheme, we transfer the image-to-query relevance information on the reference images to an arbi-trary unannotated image. The transferred relevance is then used to retrieve and rank unannotated images. The experimental result-s convincingly show the effectiveness and advantages of our new algorithm for searching unannotated images given an input textual query.

In the future, we plan to explore advanced machine learning al-gorithms that can more reliably and comprehensively understand the semantics of an unannotated images for content-based image search. We also plan to design an intelligent and friendly user in-terface that can leverage user feedback or utilizes social tags to im-prove the retrieval quality through online learning. We may consid-er extending our algorithm to automatically and efficiently annotate unlabelled images on the Internet. This problem is related to the task tackled by our algorithm in this paper, which is to find relevant images for a given text query, but in the automatic image labeling task, the goal is to look for text term(s) that can maximize their relevance to a given image. We expect many steps of our current algorithm can be readily used to address that new problem. Songhua Xu and Hao Jiang contribute equally to this paper. Songhua Xu has performed this research as a Eugene P. Wigner Fellow and staff member at the Oak Ridge National Laboratory, managed by UT-Battelle, LLC, for the U.S. Department of Energy under Con-tract DE-AC05-00OR22725. This research is also partially spon-sored by Centers for Medicare &amp; Medicaid Services of United S-tates Department of Health and Human Services. [1] P. Adrian, M. Christophe, and M. Pierre-Alain. Ontology [2] H. Bay, A. Ess, T. Tuytelaars, and L. V. Gool. Speeded-up [3] D. Cai, X. He, Z. Li, W.-Y. Ma, and J.-R. Wen. Hierarchical [4] P. Clough, M. S, and H. M  X  l  X zller. H.: A proposal for the clef [5] R. Datta, J. Li, and J. Z. Wang. Content-based image [6] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative [7] H. Feng, R. Shi, and T.-S. Chua. A bootstrapping framework [8] R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman. Learning [9] R. Fergus, P. Perona, and A. Zisserman. A visual category [10] E. Gabrilovich and S. Markovitch. Computing semantic [11] R. A. Horn and C. R. Johnson. Topics in Matrix Analysis . [12] J.-H. Hsiao, C.-S. Chen, and M.-S. Chen. A novel our algorithm respectively. [13] M. J. Huiskes and M. S. Lew. The mir flickr retrieval [14] J. Jia, N. Yu, and X.-S. Hua. Annotating personal albums via [15] Y. Jing and S. Baluja. Pagerank for product image search. In [16] M. L. Kherfi, D. Ziou, and A. Bernardi. Image retrieval from [17] P. Kidambi and S. Narayanan. A human computer integrated [18] L.-J. Li and L. Fei-Fei. Optimol: Automatic online picture [19] W.-H. Lin, R. Jin, and A. Hauptmann. Web image retrieval [20] Y. Liu, D. Xu, I. W. Tsang, and J. Luo. Using large-scale web [21] Y. Liu, D. Xu, I. W. Tsang, and J. Luo. Textual query of [22] Y. Liu, D. Zhang, G. Lu, and W.-Y. Ma. A survey of [23] D. G. Lowe. Distinctive image features from scale-invariant [24] R. Mar X e, P. Denis, L. Wehenkel, and P. Geurts. Incremental [25] H. Muller, N. Michoux, D. Bandon, and A. Geissbuhler. A [26] F. Schroff, A. Criminisi, and A. Zisserman. Harvesting [27] A. W. M. Smeulders, M. Worring, S. Santini, A. Gupta, and [28] L. Torresani, M. Szummer, and A. Fitzgibbon. Learning [29] I. H. Witten, A. Moffat, and T. C. Bell. Managing gigabytes: [30] Q.-F. Zheng and W. Gao. Constructing visual phrases for [31] X. S. Zhou and T. Huang. Unifying keywords and visual
