 One of the most important assumptions made by many classifica-tion algorithms is that the training and test sets are drawn from the same distribution, i.e., the so-called  X  X tationary distribution as-sumption X  that the future and the past data sets are identical from a probabilistic standpoint. In many domains of real-world applica-tions, such as marketing solicitation, fraud detection, drug testing, loan approval, sub-population surveys, school enrollment among others, this is rarely the case. This is because the only labeled sample available for training is biased in different ways due to a variety of practical reasons and limitations. In these circumstances, traditional methods to evaluate the expected generalization error of classification algorithms, such as structural risk minimization, ten-fold cross-validation, and leave-one-out validation, usually re-turn poor estimates of which classification algorithm, when trained on biased dataset, will be the most accurate for future unbiased dataset, among a number of competing candidates. Sometimes, the estimated order of the learning algorithms X  accuracy could be so poor that it is not even better than random guessing. There-fore, a method to determine the most accurate learner is needed for data mining under sample selection bias for many real-world ap-plications. We present such an approach that can determine which learner will perform the best on an unbiased test set, given a pos-sibly biased training set, in a fraction of the computational cost to use cross-validation based approaches.
 H.2.8 [ Database Management ]: Database Applications -Data Min-ing Algorithms classification, sample selection bia s, stationary distribution, cross-validation Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00.
Consider the following typical situation a data mining practi-tioner faces. He or she has been given a training set and is asked to build a highly accurate predictive model that will be applied to make a prediction on some future set of testing instances. The prac-titioner has at his or her disposal a variety of algorithms to learn classifiers, such as decision trees, naive Bayes and support vector machines, and wishes to determine the best performing algorithm. The standard approach to determine the most accurate algorithm is to perform cross-validation or leave-one out validation on the training set or if the Vapnik-Chervonenkis dimension of the model space is known, to perform structural risk minimization [Shawe-Taylor et X  al 1996]. These standard approaches have served the data mining and machine learning community well.

However, as data mining algorithms are applied to more chal-lenging domains, the assumptions made by traditional algorithms are violated. Perhaps one of the strongest assumptions made by classification algorithms is known as the  X  X tationary distribution assumption X  in the machine learni ng literature [Vapnik 1995] and  X  X on-biased distributi on X  in the data mining literature [Zadrozny, 2004].

D EFINITION 1.1. Stationary or Non-Biased Distribution As-sumption [Vapnik 1995] Each and every training set instance and test set instance is identically and independently drawn from a common distribution Q ( x ,y ) .

However, consider the example, where we are asked to build a model to predict if a particular drug is effective for the entire pop-ulation of individuals, that is, instances in the future test set will be an unbiased sample. However, the available training data is typi-cally a sample from previous hospital trials where individuals self select to participate and are representative of the patients at that hospital but not of the entire popular [Zadrozny, 2004]. In the ap-plication of data mining to direct marketing, it is common practice to build models of the response of customers to a particular offer using only the customers that have received the offer in the past as the training set, and then to apply the model to the entire customer database. Because these offers are usually not given at random, the training set is not drawn from the same population as the test set. Therefore, a model constructed using this training set may not perform well for the entire population of customers.

In this paper we relax the stationary distribution assumption and instead allow the training set and test set to be drawn from differing distributions but within some weak limitations.

D EFINITION 1.2. Biased Training Inst ance Distribution As-sumption Each and every training instance is drawn from distribu-tion P ( x ,y ) , and each and every test set instance is identically and independently drawn from distribution Q ( x ,y ) . Q ( x ,y unbiased distribution of instances, P ( x ,y ) is a biased distribution, and Q ( x ,y ) = P ( x ,y ) .

The above definition only states that P ( x ,y ) = Q ( x the two distributions may still differ in many different ways. The framework presented by Zadrozny [Zadrozny, 2004] discusses vari-ous types of bias. For example, one of the prevalent bias is  X  X eature bias X , which is best understood via the standard decompositions P ( x ,y )= P ( x )  X P ( y | x ) and Q ( x ,y )= Q ( x )  X Q ( bias occurs when P ( x ) = Q ( x ) , but either P ( y | x )= Q ( P ( y | x ) = Q ( y | x ) could be true (details in Section 2). An exam-ple of feature bias is the drug modeling example given earlier. The chance of encountering a feature vector (representative of a person in this situation) is different in the chosen hospital from the general population. When the training data is significantly over-sampled or under-sampled for some feature vectors, the represented concept of drug effect P ( y | x ) can also be different from the intended true concept Q ( y | x ) of when the drug will be effective.

When the assumption of stationary distribution is lifted, it raises problems for answering the question:  X  X hich classification algo-rithm finds the best performing model? X  As we shall see the tra-ditional approaches which are used extensively by the data mining community, such as cross-validation and leave-one-out validation, perform hopelessly when sample bias occurs. In some circum-stances, the order of expected accuracy of competing models is not even better than random guessing.

Previous work on this problem by Zadrozny [Zadrozny, 2004] noted that some learners, such as logistic regression and hard-margin support vector machines, are invariant to feature bias and describes how to correct this type of sample bias for those learners that are sensitive to feature bias, such as decision trees and naive Bayes. However, this work is limited to situations where one could build a model that is asymptotically close to the true unbiased model Q ( y | x ) . Recently however, Fan, Davidson, Zadrozny and Yu [Fan X  et al 2005] illustrated that this is not always possible, and all types of learner may be effected by feature sample bias. It is difficult to know which algorithm is not affected by bias without knowing the true model Q ( y | x ) . Importantly however, the true model is generally never known for real-world problems. That is, we can-not apply some types of learners and assume that they will be able to overcome sample bias. Another restriction of both [Zadrozny, 2004] and [Fan X  et al 2005] is that feature bias is assumed to only affect the probability of feature vector P ( x ) but not the condi-tional probability P ( y | x ) .Inotherwords, P ( x ) = Q ( x ) P ( y | x )= Q ( y | x ) . We will discuss in Section 2 that this is not always true. Given this earlier result, the problem associated with learning with a biased sample is: P ROBLEM 1.1. The Learning From Biased Sample Problem Given a labeled training set D , an unlabeled test set T , such that D and T may or may not be drawn from the same distribution, and a series of learning algorithms ( L 1 ...L k ): Which learner when applied to D generates the model that is most accurate on T ?
We begin by discussing various types of sample selection bias as well as the notations used throughout the paper. In Section 3, we empirically show that traditional approaches, cross-fold validation and leave-one-out validation on the training set, can give mislead-ing, sometimes pessimistic, solutions to Problem 1.1. In particular, we provide an explanation of their poor performance in Section 3.4. In Section 4, we describe and explain the mechanism of the proposed algorithm to solve Problem 1.1. Section 5 empirically il-lustrates that our algorithm outperforms the traditional approaches. To be exact, with both biased and unbiased datasets, we show that our algorithm is capable of choosing the best learner 84% of the time while cross-validation and leave one-out validation achieve rates approximately from 40% to 68%. Importantly, in Section 6, we have applied the proposed approach to two application prob-lems, charity donation solicitation and credit card card fraud detec-tion, where sample selection bias is a common problem, and both are significant in size and require cost-sensitive loss functions. The proposed method correctly ordered performance of all competing classifiers, while cross-validation was right 58% of the time.
Assume that the event s =1 denotes that a labeled example ( x ,y ) is selected from the unbiased joint distribution Q ( x examples into the training set D ,andthat s =0 denotes that is not chosen. Using the dependency on s =1 , the training set is sampled from the distribution  X  P ( x ,y,s =1) .Since s fixed, we can simplify this notation by removing s =1 , and it be-comes P ( x ,y ) . In other words, we define the training distribution as, P ( x ,y )=  X  P ( x ,y,s =1) .

With these notations, the training distribution P ( x ,y ) ing distribution Q ( x ,y ) are thus related by P ( x ,y )= 1) = Q ( x ,y )  X   X  P ( s =1 | x ,y ) . This relationship is straightfor-ward by applying the product rule, such that  X  P ( x  X  P ( x ,y )  X   X  P ( s =1 | x ,y ) . As sample selection bias is already de-noted through s =1 ,  X  P ( x ,y ) is the same as the true unbiased distribution or  X  P ( x ,y )= Q ( x ,y ) . In particular, the notation  X  P ( s =1 | x ,y ) is equivalent to  X  P ( s =1 | x ,y )  X , as introduced by Zadrozny in [Zadrozny, 2004]. To be exact, P ( denotes the probability to sample an example ( x ,y ) from into the training set, and it is described as a conditional probability on the feature vector x and class label y .
In [Zadrozny, 2004], four different types of sample selection bias are clearly discussed according t o the conditional dependency of s =1 on x and y . It is important to emphasize that that in all cases the test set examples are assumed to be unbiased, since the model will be used on the entire population. To clarify discussion for the rest of this paper, a complete summary of all notations, definitions and assumptions made in this paper can be found in Figure 2.
In the complete independent case s is independent from both x and y , i.e., P ( s =1 | x ,y )= P ( s =1) . That is, the sample selection bias depends on some other event completely independent of the feature vector x and the true class label y .

In the feature dependent case or feature bias case , the selec-tion bias s is explicitly dependent on the feature vector conditionally independent of the true class label y given P ( s =1 | x ,y )= P ( s =1 | x ) . This type of sample selection is extensive in many mining applications. For example, in the direct marketing case mentioned earlier, the customers are selected into the training sample based on whether or not they have received the offer in the past. Because the decision to send an offer is based on the known characteristics of the customers (that is, seeing the response (that is, y ) then the bias will be of this type. This type of bias also occurs in medical data where a treatment is not given at random, but the patients self-select to receive the treat-ment. Therefore, the population that received the treatment in the past is usually not a random sample of the population that could potentially receive the treatment in the future.
 Figure 1: Some possible effects of a) feature bias and b) class bias on training set. There are two classes, only areas with pos-itive class  X + X  are shaded. The darkness or intensity shows fre-quency of examples in the highlighted region.
In the class dependent case , the selection bias is explicitly de-pendent on y ,andis conditionally independent from the feature vector x given y , i.e., P ( s | x ,y )= P ( s | y ) . This occurs when there is a correlation between the label value and the chance of ap-pearance in the database. For example, people with higher income may be less inclined to answer a survey about income. Thus, if we are trying to learn to predict an individual X  X  income category using survey data, class dependent bias is likely to occur. For a mod-eling technique that predict scores or posterior probabilities, class bias could be resolved by experimenting with decision threshold in ROC plot. As an example, for a two class problem, where the pos-itive class label is significantly under-sampled in the training data, one could choose a decision threshold much lower than 0.5 to pre-dict the positive class. Issue on how to resolve bias for a particular learner is an orthogonal problem from the main focus of this paper.
In the complete dependent case , there is no assumption or ex-plicit knowledge about any restriction of the independence of s given x and y , and both the example description and its label influ-ence whether the example will be chosen into the training set.
Figure 1 illustrates some possible effects of feature bias as well as class bias on the training and test set distributions. In situation a), we illustrate one simple case of  X  X eature bias X  where it explicitly changes Q ( x ) but not the class boundaries or Q ( y | x ) in the training set, the feature vector is under-estimated in region 1, over-estimated in region 2, and, for region 3, there is no effect. In situation b), class bias changes the class boundaries. The positive class or Q ( y =+) is under estimated in the training set, and hence the positive regions shrink in size and  X  X ntensity X .
It is important to understand that for feature bias, where P 1 | x ,y )= P ( s =1 | x ) , the bias s is independent of y given In other words, the sample selection bias can be completely de-scribed by a conditional probability distribution on x alone. How-ever, this does not imply that s =1 is completely independent of y or it is not necessarily true that P ( s =1 | y )= P under feature sampling bias. In effect, the prior class probabil-ity P ( y )= P ( y | s =1) of the training data may or may not be equal to the prior class probability Q ( y ) of the unbiased data. The reason is that P ( y | s =1) is the cumulative probability over examples with the same class label value y . And the number of these examples being sampled is dependent on P ( s =1 | x ) sider some feature values that are over-sampled due to feature bias. If the conditional probability for one class label for these over-sampled feature values is significantly higher than other feature values, the prior class probability in the sampled data P is likely to be different from Q ( y ) . For example, assume that Q ( y =+ | x =2)=1 and feature bias over-samples examples with x =2 ,then P ( y =+)= P ( y =+ | s =1) gets closer to 1 as more and more x =2 examples are sampled due to feature bias.
In addition, another important fact about  X  X eature bias X  is that the conditional probability P ( y | x )= P ( y | x ,s =1) in the training set may or may not be equal to the true unbiased conditional probability or either P ( y | x )= Q ( y | x ) Q ( y | x ) could be true. As an extreme case, if some feature vec-tors x are not sampled at all in the training data, strictly speaking P ( y | x ,s =1) for these examples are undefined. One example that clearly makes P ( y | x ,s =1)= Q ( y | x ) true is a determinis-tic problem (or Q ( y | x )=1 for one and only one class label) that some feature vectors are over sampled, but all possible feature vec-tors are included. Indeed, in both [Zadrozny, 2004] and [Fan X  et al 2005], to show P ( y | x ,s =1)= P ( y | x ) , one implicit assumption is that P ( s =1 | x ) &gt; 0 or P ( s =1 , x ) &gt; 0 for all valid fea-ture vectors. However, this may not be practical since it effectively means that training data ought to include all valid feature vectors.
A similar argument can be made for class bias where P 1 | x ,y )= P ( s =1 | y ) . Under class bias, s is independent of given y . It means that the bias can be completely formulated as a function of y alone without direct use of feature vector x not necessarily imply P ( s =1 | x )= P ( s =1) . In essence, the de-pendency of s on x under class bias is indir ectly via the conditional probability Q ( x | y ) .

In summary, for both feature bias and class bias, the explicit de-pendency in the definition on either feature vector x or class label y is due to practical knowledge on how the training set is sampled. However since feature vector x and class label y are related and not independent (otherwise there is not much utility to learn a classifi-cation model that estimates Q ( y | x ) . It is because Q ( when y and x are independent), both feature bias and class bias are related to x as well as y either explicitly or inexplicitly. The notion that feature bias is completely independent from class label, and that class bias is completely independent from feature vector, are more restrictive and may not be practical. By definition, the depen-dency of  X  X omplete dependence case X  on both y and x is trivially true. In every case of sample selection, the conditional probability P ( y | x ) of the sample could be the same as or different from the unbiased conditi onal probability Q ( y | x ) .

Now considering all three cases of sample selection bias, their actual distinction is on how much explicit knowledge is known about how the training data is sampled or how the bias is intro-duced. In many practical situations, the sample bias can be either explicitly or implicitly related to both feature vector x label y . Due to these direct and indirect relationships, without any other information, when one observes discrepancy in the probabil-ity of feature vector x between training and testing sets, the sample selection selection bias can actually be any one of these three types.
In a testing environment where the true label for testing data is given, we can check if P ( y | s =1)= P ( y ) is true. If so, then the only possible bias is either feature bias or complete dependence case, but cannot be class bias. This is trivially true since P 1) = P ( y ) implies y and s are independent. Additionally, if one could also assume that the sample bias does not change the decision boundary in the training data or P ( y | x ,s =1)= P ( y | x ) possible bias is feature bias following the analysis below:
We begin this section by introducing the eleven data sets used throughout the paper.
Since in our learning problem definition (see Problem 1.1) we do not explicitly know if the training sample is biased, we include data sets with little or no sample bias such as several Newsgroup data sets, in addition to the UCI data sets that will be purposefully biased.

We perform experiments on articles drawn from six pairs of News-groups [Rennie 2003]. In half of these problems (Mac-Hardware, Baseball-Hoc, and Auto-space), the articles in the newsgroups are very similar, and in the other half (Christ-Sales, MidEast-Elec, and MidEast-Guns), are quite different. The training and test datasets are created using the standard bydate division into training (60%) and test (40%) based on the posting date. This division poten-tially creates a sample bias. For example, in the MidEast-Guns newsgroup, the word  X  X aco X  occurs extensively in articles in the Therefore, instances of articles c ontaining the word  X  X aco X  in the training set are much more populous than in the test set. Since the proportion of each class label is the same in the training and test data sets, there is no class label bias. We used Rainbow [McCal-lum 1998] to extract features from these news articles. The feature vector for a document consists of the frequencies of the top ten words by selecting words with highest average mutual information with the class variable.

The UCI data sets are purposefully biased by sorting the train-ing set on the first attribute and removing the first 25% of records therefore creating a selection bias based on the first feature. The test sets are unaltered, thus there is at least an explicit feature bias bias from the training to the test sets. If the original dataset also has some explicit bias on class label (which we did not check), the bias can also be complete dependence bias.
For each data set we attempt to estimate the generalization er-ror from the training set for a variety of learners to determine the best performing learner. There are two common areas/approaches to achieve this. The structural risk minimization approach bounds the generalization error as a function of the training set error and Vapnik Chervonenkis (VC) dimension. Formally, GE  X  TE + r ror, TE is the training set error, VC ( L ) is the VC dimension of the learner L , n is the size of the training set and  X  is the chance of the bound failing. However, this bound derivation explicitly makes the stationary distribution assumption and makes no claim to formally hold when it is violated as in our case [Vapnik 1995].

Two empirical alternatives for generalization error estimation commonly used in data mining is ten-fold cross-validation and leave-one-out validation. In the ten-fold cross-validation approach, the training data set is divided into ten equally sized, randomly cho-sen folds. Each fold is used to evaluate the accuracy of a model built from the remaining nine folds, the average accuracy on the hold-out fold is then used as an estimate of generalization error. Typically, as in our experiments, the entire process is repeated one hundred times with different randomly generated folds. With the leave-one-out validation approach, each instance is used as a test set and a model built from all remaining instances. Though other techniques motivated from the probability and statistics literature can be used to find the best performing model, they in fact return Table 1: Accuracy of Four Classifiers on the Test Set for Vari-ous Newsgroup Datasets Table 2: Accuracy Order of Four Classifiers on Test Set of Var-ious Newsgroup Datasets similar results to cross-validation. It is well known that asymptot-ically leave-one-out validation is identical to Aikake X  X  information criterion (AIC) and that for reasonable (small) values of k that the Bayesian information criterion (BIC) returns similar results to k -fold cross-validation [Moore 2001].
For the Newsgroup data, the actual testing accuracy and their order among four algorithms on each of the six datasets are sum-marized in Table 1 and 2. As a comparison, the testing accuracy and their order estimated by ten-fold cross-validation are shown in Table 3 and 4, and the corresponding results by leave-one-out are in Table 5 and 6. In each table, DT=decision tree, LR= linear re-gression, NB = naive Bayes, and SVM=support vector machine.
We find that ten-fold cross-validation can be used to accurately predict the order of learner most of the time in all but 1 of the 6 data sets (Tables 2 vs 4). As in all our results, an asterisk indicates an incorrect result when compared to the true test set error. However, for leave-one-out validation, in 5 out of 6 data sets, the learner ac-curacy order is incorrectly predicted (Tables 2 vs 6). On the other hand, both ten-fold and leave-one-out appear to sometimes provide poor estimates of the learner accuracy (Table 1 vs Tables 3 and 5) with the average difference between the actual error and error esti-mated by ten-fold (leave-one-out) being 1.6 (3.6) with a minimum of 0 (0) and maximum of 7.5 (14.1).
 With purposefully biased UCI datasets (the bias described in Section 3.1), we find that both ten-fold and leave-one-out valida-tion do not indicate well which learner performs the best. The ac-tual testing accuracy is summarized in Table 7, and the estimated accuracy by ten-fold cross-validation and leave-one-out are in Ta-bles 8 and 9. If we summarize the results in complete accuracy order, the results would appear pessimistic. Instead, we have cho-sen a pairwise comparison. For each data set, the four classifiers X  accuracy are compared against each other giving rise to C combinations (DT vs NB, DT vs LR, DT vs SVM, NB vs LR, NB vs SVM and LR vs SVM). Therefore, for our five UCI datasets, there are 30 classifier comparisons (6 per dataset). Table 10 shows the correct pairwise comparison obtained from the test set. Table 11 shows that the results of using ten-fold cross-validation repeated Table 3: Accuracy for Ten-Fold Cross-Validation of Four Clas-sifiers on Training Set of Various Newsgroup Datasets. Aver-aged Accuracy over 100 Trials. c.f. Table 1 Table 4: Accuracy Order for Ten-Fold Cross-Validation of Four Classifiers on Training Set of Various Newsgroup Datasets. Averaged Accuracy over 100 Trials. An  X * X  indicates a different ordering than Table 2. 100 times (at great computation cost) are correct only 17 out of the 30 times. The ten-fold cross-validation is a woeful method to indi-cate learner accuracy with the average difference being 6.2 (mini-mum of 0.6 and maximum 20.9) (Tables 7 and 8). The results for leave-one-out validation results (Tables 12) are even worse. For the 30 pairwise comparisons, only 15 have been correctly predicted. For predicted values, the average difference in accuracy is 6.4 with the minimum being 0.4 and the maximum 21.2 (Tables 7 and 9).
The training accuracy results (not shown) are almost identical to the results for leave-one-out validation, and hence is also a poor indicator of the classifiers X  true accuracy on the test sets for both the Newsgroup datasets and biased UCI datasets. This is to be accepted as the biased training data set is not representative of the unbiased test set.
Consider the distributions Q ( x ,y )= Q ( y | x )  X Q ( x ) the test set is drawn and the biased distribution P ( x ,y P ( x ) from which the training set is drawn. For simplicity of dis-cussion, let us assume that P ( y | x )= Q ( y | x ) , and we know from the dataset generation procedure that P ( x ) = Q ( x ) . Even if our learner perfectly estimates the true conditional probability the estimated generalization error will still most likely be incor-rect. Let P ( y  X  X  x ) be the probability for the most likely label for a particular instance, then the learner X  X  lowest generalization error possible is GE = est generalization error that can be estimated from the training set is  X  GE = For example in Figure 1:a) the error for Region 1 will be under estimated compared to the region X  X  true error. This is because  X  x  X  Region 1 , P ( x ) &lt; Q ( x ) . An over and under occurrence of instances in the training set compared to the test set will lead to systematically under or over stating the generalization error. This is also indicated by our experimental results (Tables 7 and 8). Each and every technique under-estimates the true error for the Breast and Vote data sets, while every technique over-estimates the true Table 5: Accuracy for Leave-One-Out Validation of Four Clas-sifiers on Training Set of Various Newsgroup Data Sets. c.f. Ta b l e 1 .
 Table 6: Accuracy Order for Leave-One-Out Validation of Four Classifiers on Training Set of Various Newsgroup Data Sets. An  X * X  indicates a different ordering than Table 2. error for Iris and Wine. For Pima, three out of the four classifi-cation techniques over estimate the true error. Similar results to cross-validation are observed for leave-one-out validation.
The previous experimental results illustrate that traditional cross-validation based approaches cannot be used effectively to deter-mine which learner will outperform the others when the training set is biased. In this section, we propose one that can. Assume that  X  a and  X  b are two classifiers trained by algorithms L a and L b from the training set D . We are interested to order  X  and  X  b  X  X  accuracy on unlabeled test set T . The intuition is to make use of the testing data X  X  feature vectors but the training data X  X  labels. The conceptual steps of ReverseTesting are 1. Classify test data T with  X  a and  X  b . As a result, T 2. Train  X  X ome new classifiers X  from T a and T b . 3. Evaluate  X  X hese new classifiers X  on labeled training data D . 4. Based on the accuracy of  X  X hese new classifiers X  on D ,use
The name  X  X everseTesting X  comes from the procedure to  X  X ome back X  to the training data. In the above basic framework of Rever-seTesting, it does not specify either the exact ways to train  X  X ew classifiers X  or the exact  X  X ome rules X . We next instantiate these basic procedures with a preferred implementation.
The two classifiers,  X  a ,  X  b , are constructed by applying learning algorithms L a and L b on the training data set D . To determine which one of two classifiers is more accurate on T , the first step is to use both classifiers,  X  a and  X  b , to classify the unlabeled test set to obtain two  X  X abeled X  data sets T a and T b . In the second step, Table 7: Performance of Four Classifiers on Test Set of Pur-posefully Biased UCI Data Sets Table 8: Accuracy for Ten-Fold Cross-Validation of Four Clas-sifiers on Training Set of Purposefully Biased UCI Data Sets. Averaged Accuracy over 100 Trials Table 9: Accuracy for Leave-One-Out Validation of Four Clas-sifiers on Training Set of Purposefully Biased UCI Data Sets. we construct four new classifiers by applying L a and L two labeled test sets, T a and T b , respectively, and these four new classifiers are named as  X  a a ,  X  b a ,  X  a b ,and  X  b b new classifier built using algorithm L b on T a or the test set labeled by  X  a . Since the original training set D is labeled, we can use D to evaluate the accuracy of these four new classifiers. Assume that their accuracy on D is AccT rain a a , AccT rain b a , AccT rain AccT rain b b respectively, i.e., AccT rain b a is the accuracy of  X  D . It is important to understand that AccT rain b a is not the typical training set accuracy, rather it is the accuracy on the training set of a classifier trained by L a from labeled original test data T
Next, we use two simple rules based on these four accuracy mea-surements to determine the better performing classifier between  X  and  X  b on the unlabeled test set T .

C ONDITION 4.1. If ( AccT rain b a &gt; AccT rain a a )  X  (
AccT rain b b &gt; AccT rain a b ) ,then  X  b is expected to be more accu-rate than  X  a on unlabeled test set T .

C ONDITION 4.2. If ( AccT rainb a a &gt; AccT rain b a )  X  (
AccT rain a b &gt;AccT rain b b ) ,then  X  a is expected to be more accu-rate than  X  b on unlabelled test set T .

C ONDITION 4.3. Otherwise,  X  a and  X  b are tied and hard to dis-tinguish.
 Assume that  X  b is more accurate than  X  a on the testing data T . Under this assumption, there are more examples with correct labels in T b (or T labeled by  X  b )than T a . By means of its predicted labels, T b describes a  X  X oncept X  that is expected to be closer to the true model than T a . For a reasonable learning algorithm, the classifier Table 10: Pairwise Competitive Performance of Four Classi-fiers on Test Set of Various Purposefully Biased UCI Data Sets. Each entry indicates which of the classifiers outperformed the other. built from T b is expected to be more accurate than a classifier built from T a by the same algorithm. Conditions 4.1 and 4.2 capture this reasoning and also rules out that the converse situation since either T a or T b is typically a better labeling of the test set.

In summary, if  X  a and  X  b don X  X  have the same accuracy, either i) ( AccT rain a a &gt; AccT rain b a )  X  ( AccT rain a when  X  a is more accurate than  X  b , or ii) ( AccT rain b (
AccT rain b b &gt; AccT rain a b ) when  X  b is more accurate than  X  expected to be true. In other words, if ( AccT rain a a &gt; AccT rain (
AccT rain a b &gt; AccT rain b b ) ,  X  a is more accurate than  X  (
AccT rain b a &gt; AccT rain a a )  X  ( AccT rain b b &gt; AccT rain is more accurate than  X  a .

When  X  a is more accurate than  X  b , could other orders of accu-racy, for example, ( AccT rain a a &gt; AccT rain b a )  X  ( AccT rain b b ) be true? In some rare situations, it could happen that a more correctly labeled T may not induce a more accurate classi-fier. These rare situations include learning algorithms that do not behave reasonably, and those stochastic problems where the true label of some examples have probabilities significantly less than 1 or formally  X  ( x ,y ) , Q ( y | x ) 1 . When neither Condition 4.1 nor Condition 4.2 is true,  X  a and  X  b are either tied or hard to separate. The complete algorithm is summarized in Figure 3.
The proposed algorithm is significantly less time consuming than the traditional approaches. With ten-fold cross-validation to com-pare two learners, we need to build 2  X  10  X  100 models (2 learners and 10 folds repeated 100 times), and with leave one-out validation, 2  X  n models where n is the number of instances in the training data set. However, for ReverseTesting, we instead need only to build 6 models (two models from the training set, then four models from the labeled test set). For models comparison, ten-fold cross-validation and leave-one-out construct  X  10  X  100 and  X  n models Table 11: Pairwise Competitive Performance for Ten-Fold Cross-Validation of Four Classifiers on Training Set of Various Purposefully Biased UCI Data Sets. Averaged Accuracy over 100 Trials. Each entry indicates which of the classifiers outper-formed the other. An asterisk means a difference to the correct value in Table 10 respectively, and ReverseTesting construct +4  X  ( +1)  X  / 2 2 +3 models. Approximately, only when there were more than 500 algorithms to compare or &gt; 500 , ReverseTesting could be be less efficient than cross-validation.
We begin by evaluating our algorithm on the Newsgroup data sets where ten-fold cross-validation but not leave-one-out valida-tion performed well at choosing the best performing learner. The results are summarized in Table 13. Importantly, we see that for the Newsgroup data sets, which may or may not be biased, that Rever-seTesting performs exactly the same as ten-fold cross-validation (Table 4 vs 13) ) and significantly better than leave-one-out val-idation (Table 6). These results are important since Newsgroup datasets have small or no sample selection bias. This illustrates that the proposed algorithm works well when the stationary or non-biased distribution assumption holds.
 For the purposefully biased UCI d atasets (the bias described in Section 3.1), the pairwise comparison results of ReverseTesting are shown in Table 14. We see that, out of the 30 comparisons, there are only 5 errors as opposed to 13 errors when using ten-fold cross-validation and 15 errors when using leave-one-out validation. In 3 of the 5 errors, Condition 4.3 occurred and hence no decision on which classifier performed best could be made.

Considering both Newsgroup and UCI datasets, counting the number of * X  X  or losses in all tables and the total number of all entries (20 for Newsgroup and 30 for biased UCI), the summary is Table 12: Pairwise Competitive Performance for Leave-One-Out Validation of Four Classifiers on Training Set of Various Purposefully Biased UCI Data Sets. Each entry indicates which of the classifiers outperformed the other. An asterisk means a difference to the correct value in Table 10 Table 13: Accuracy Order for ReverseTesting of Four Classi-fiers on Training Set of Various Newsgroup Datasets. An  X * X  indicates a different ordering than Table 2.
Newsgroup 20 3153 biased UCI 30 13 15 5(3)
Sum 50 16 30 8 % Choose Best Learner 68% 40% 84% It clearly shows that the proposed algorithm can choose the cor-rect learner most of the time, while ten-fold cross-validation and leave-one-out validation cannot.
We have applied ReverseTesting to two important applications where sample selection bias is known to exist. The first application is charity donation dataset from KDDCUP X 98 and the second is a month-by-month data of credit card fraud detection. These prob-lems are particularly interesting since both employ cost-sensitive loss function as opposed to 0-1 loss, and both problems contain rather large datasets. Figure 3: A preferred implementation of ReverseTesting to de-termine the best Learner
For the donation dataset (Donate), suppose that the cost of re-questing a charitable donation from an individual x is $0.68, and the best estimate of the amount that x will donate is Y ( x ) efit matrix (converse of loss function) is: The accuracy is the total amount of received charity minus the cost of mailing. Assuming that p ( donate | x ) is the estimated probabil-ity that x is a donor, we will solicit to x iff p ( donate 0 . 68 . The data has already been divided into a training set and a test set. The training set consists of 95412 records for which it is known whether or not the person made a donation and how much the do-nation was. The test set contains 96367 records for which simi-lar donation information was not published until after the KDD X 98 competition. We used the standard training/test set splits since it is believed that these are sampled from different individuals thus incuring feature bias [Zadrozny, 2004]. The feature subsets (7 fea-tures in total) were based on the KDD X 98 winning submission. To estimate the donation amount, we e mployed the multiple linear re-gression method. As suggested in [Zadrozny and Elkan, 2001], to avoid over estimation, we only used those contributions between $0 and $50.

The second data set is a credit card fraud detection (CCF) prob-lem. Assuming that there is an overhead v = $90 to dispute and investigate a fraud and y ( x ) is the transaction amount, the follow-ing is the benefit matrix: The accuracy is the sum of recovered frauds minus investigation costs. If p ( fraud | x ) is the probability that x is a fraud, fraud Table 14: Pairwise Competitive Performance of Four Classi-fiers of Various Purposefully Biased UCI Data Sets using Re-verseTesting on Training Set. Each entry indicates which of the classifiers outperformed the other. An entry of  X ?? X  indi-cates that Condition 4.3 occurred and hence no decision could be made. An asterisk means a difference to the correct value in Ta b l e 1 0 Table 15: Cost-senstive  X  X ccuracy X  of Four Classifiers on the Test Set for Donation and CCF Data Sets is the optimal decision iff p ( fraud | x )  X  y ( x ) &gt;v . The dataset was sampled from a one year period and contains a total of .5M transaction records. The features (20 in total) record the time of the transaction, merchant type, merchant location, and past payment and transaction history summary. We use data of the last month as test data (40038 examples) and data of previous months as training data (406009 examples), thus obviously creating feature bias since no transactions will be repeated.

For cost-sensitive problems, the most suitable methods are those that can output calibrated reliable posterior probabilities [Zadrozny and Elkan, 2001]. For this reason, we use unpruned single deci-sion tree (uDT), single decision tree with curtainlment (cDT), naive Bayes using binning with 10 bins (bNB), and random decision tree with 10 random trees (RDT). Since both datasets are significant in size and leave-one-out would have taken a long time to complete, we only tested 10 cross-validation, repeated for 10 times for each dataset, to compare with ReverseTesting. The detailed accuracy re-sults are summarized in Table 15 and 16, and pairwise orders are summarized in Table 17 and Table 18. ReverseTesting predicted exactly the order as the actual pairwise order on the test datasets for both donation and credit card fraud detection, so its results are the same as Table 17. On the other hand, for pairwise order, 10-fold cross validation was correct in 7 out 12 times.
 Table 16: Cost-sensitive  X  X ccuracy X  for Ten-Fold Cross-Validation of Four Classifiers on Donation and CCF Datasets Table 17: Pairwise Competitive Performance of Four Classi-fiers on Testing Data of Donate and CCF. ReverseTesting pre-dicted exactly the same order
The sample selection bias problem has received a great deal of attention in econometrics. There it appears mostly because data are collected through surveys. Very often people that respond to a sur-vey are self-selected, so they d o not constitute a r andom sample of the general population. In Nobel-prize winning work, [Heckman, 1979] has developed a two-step procedure for correcting sample se-lection bias in linear regression models, which are commonly used in econometrics. The key insight in Heckman X  X  work is that if we can estimate the probability that an observation is selected into the sample, we can use this probability estimate to correct the model. The drawback of his procedure is that it is only applicable to linear regression models. In the statistics literature, the related problem of missing data has been considered extensively [Little and Ru-bin, 2002]. However, they are generally concerned with cases in which some of the features of an example are missing, and not with cases in which whole examples are missing. The literature in this area distinguishes between different types of missing data mecha-nisms: missing completely at random (MCAR), missing at random (MAR) and not missing at random (NMAR). Different imputation and weighting methods appropriate for each type of mechanism have been developed.

More recently, the sample selection bias problem has begun to receive attention from the machine learning and data mining com-munities. Fan, Davi dson, Zadrozny and Yu [Fan X  et al 2005] use the categorization in [Zadrozny, 2004] to present an improved cat-egorization of the behavior of learning algorithms under sample selection bias (global learners vs. local learners) and analyzes how a number of well-known classifier learning methods are affected by sample selection bias. The improvement over [Zadrozny, 2004] is that the new categorization considers the effects of incorrect mod-eling assumptions on the behavior of the classifier learner under sample selection bias. In other words, the work relaxes the as-sumption that the data is drawn from a distribution that could be perfectly fit by the model. The most important conclusion is that most classification learning algorithms could or could not be af-fected by feature bias. This all depends on if the true model is con-tained in the model space of the learner or not, which is generally unknown. Smith and Elkan [Smith and Elkan, 2004] provide a sys-tematic characterization of the different types of sample selection Table 18: Pairwise Competitive Performance Predicted by Ten-Fold Cross-Validation on Donate and CCF Data Sets. A * indi-cates a difference in order from the test data bias and examples of real-world situation where they arise. For the characterization, they use a Bayesian network representation that describes the dependence of the selection mechanism on ob-servable and non-observable features and on the class label. They also present an overview of existing learning algorithms from the statistics and econometrics litera ture that are appropriate for each situation. Finally, Rosset et al. [Rosset et al., 2005] consider the situation where the sample selection bias depends on the true label and present an algorithm based on the method of moments to learn in the presence of this type of bias.
Addressing sample selection bias is necessary for data mining in the real world for applications such as merchandise promotion, clinical trial, charity donation, etc. One very important problem is to study the effect of sample selection bias on inductive learn-ers and choose the most accurate classifier under sample selection bias. Some recent works formally and experimentally show that most classifier learners X  accuracy could be sensitive to one very common form of sample selection bias, where the chance to select an example into the training set depends on feature vector directly on class label y . Importantly, this sensitivity depends on whether or not the unknown true model is contained in the model space of the learner, which is generally not known either before or after data mining for real-world applications. This fact makes the problem to choose the most accurate classifier under sample selec-tion bias a critical problem. Our paper provides such a solution.
We first discuss three methods for classifier selection under sam-ple selection bias, ten-fold cross-validation, leave-one-out-validation and structural risk minimization, and empirically evaluate the first two. Our experiments have shown that both the predicted order and value of the learners X  accuracy are far from their actual perfor-mance on the unbiased test set. In the worst cases, the predicted order is not even better than random guessing. This re-confirms the need to design a new algorithm to select classifiers under sample selection bias. We propose a new algorithm that is significantly different from those three methods to evaluate a classifier X  X  accu-racy. In our problem formulation, we do not assume that the train-ing and testing data are drawn from the same distribution. In other words, they could be drawn from either the same or different distri-bution. Quite different from the ways of solely relying on labelled training data, we make use of unlabelled test data during model selection. The basic idea and process is to use the competing clas-sifiers trained from the training set to label the test data set, i.e, one labeled test set for each competing classifier, and then re-construct a set of new classifiers from these labeled test sets. We then or-der the original competing classifiers accuracy based on the new classifiers X  accuracy on the training set.

Experimental studies have f ound that when th ere is little or no sample selection bias, our proposed algorithm predicts the order of performance as good as ten-fold cross-validation and significantly better than leave-one-out validation. Importantly, when there is sample selection bias, our proposed algorithm is significantly bet-ter (5 errors including 3 undetermined) than cross-validation (13 errors) and leave-one-out (15 errors) among 30 pairwise compar-isons. For two high volume cost-sensitive applications, charity do-nation solicitation and credit card fraud detection, where sample selection bias is a common problem, ReverseTesting is correct in predicting all 12 pairwise orders, while ten-fold cross-validation is correct in only 7 of these cases.

Future Work The proposed algorithm correctly orders the ac-curacy of competing learners. However, it does not estimate the actual accuracy on the test data itself. This is another challenging problem since the label of the test set is not given in our problem setting. A possible solution is to combine cross-validation and Re-verseTesting in some ways. We choose a preferred implementation of ReverseTesting. It is interesting to evaluate other possibilities on how to train the new classifiers from labeled test data and the rules to induce performance order. Th is paper shows the utility to use feature vector from testing data to evaluate models. Another idea could be to improve the model, similar to semi-supervised learn-ing, by looking at these feature vectors in unbiased testing data, to overcome sample selection bias.
 We thank the anonymous reviewers for their valuable suggestions and comments.
