 In this paper, for the first time, we study the problem of mapping keyword queries to questions on community-based question answering (CQA) sites. Mapping general web queries to questions enables search engines not only to discover explicit and specific information needs ( questions ) behind keywords queries, but also to find high quality information ( answers ) for answering keyword queries. In order to map queries to questions, we propose a ranking algorithm con-taining three steps: Candidate Question Selection , Candi-date Question Ranking , and Candidate Question Group-ing . Preliminary experimental results using 60 queries from search logs of a commercial engine show that the presented approach can efficiently find the questions which capture user X  X  information needs explicitly.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Performance Question Search, Community-based QA, Data Integration
With the development of Web 2.0, users can share their knowledge by asking and answering questions in the com-munity based question answering (CQA) sites, such as Ya-hoo! Answers and Live QnA . As more and more persons seek information on them, CQA services have became an-other popular information seeking mechanism except search engines, e.g., according to Yahoo! X  X  statement, Yahoo! An-swers has a million users worldwide, and has compiled 400 million questions. Compared with keywords queries used in search engines, questions in CQA sites express users X  infor-mation needs explicitly. Moreover, each question has several answers contributed by other users, and one of them is la-beled as the best answer. Thus, CQA data is a type of high quality data, which contains users X  information needs expressed in questions as well as answers for these informa-tion needs. An interesting problem will be  X  Can general web queries be mapped to questions?  X . If yes, not only can we serve question answers to users but also, by understanding user information needs as explicit questions, we can better serve Web search results.

Motivated by this, we study the problem of mapping web queries to questions (MQ2Q) on CQA sites, which can sig-nificantly improve the performance of search engines in the following two aspects:
The first aspect is integrating CQA data , a type of high quality information source, into general search engines. CQA data, contributed by millions of users, contain much human knowledge and experience. This kind of knowledge is rare in general web pages. Through MQ2Q, users can directly get related questions as well as the answers. For example, given a query  X  clean PSP UMD  X , the related question  X  What X  X  the best way to clean a PSP UMD?  X , and its answers can satisfy the information needs directly. Hence, MQ2Q can integrate high quality CQA data into search engines.
 The second aspect is deepening interpretations of queries . Using MQ2Q, queries can be interpreted into popular and relevant questions, which express users X  information needs explicitly. For instance, given a query q 1 :  X  iPod nano vs iPod shuffle  X , related questions like Q 1 :  X  Difference between the iPod nano and iPod shuffle?  X  or Q 2 :  X  X  ow can I transfer songs from an iPod nano to an iPod shuffle, while they are both plugged into my Laptop?  X , and Q 3 :  X  Which is better size and cost wise, an iPod nano or an iPod shuffle?  X  show users X  intentions behind the query clearly. With deepened interpretations through query mapping, MQ2Q can boost search engines X  performance significantly.

As our target, we define MQ2Q as the problem of mapping a query to a set of questions that represent various popu-lar information needs behind the query. Therefore, MQ2Q contains the following challenges:
Challenge 1: How to find related questions, which may not contain all the keywords for a given query? A related qu estion is a question that captures a clear information need behind the given query. As an example, in the above exam-ples, Q 1 , Q 2 , and Q 3 are relevant questions for the query q , although they do not match the keyword  X  X s X . However,  X  iPod nano vs iPod shuffle?  X  is not a relevant question, be-cause it does not express user X  X  information needs explicitly.
Challenge 2: How to find popular questions among a set of related questions? A popular question is a question that denotes an information need which may be asked by most users. Continuing the aforementioned example, given a query q 1 , Q 1 , Q 2 , and Q 3 are all relevant questions; whereas obviously, Q 1 and Q 3 , which are about comparing two items, are more likely to meet the user X  X  information needs than Q Consequently, Q 1 and Q 3 should be ranked higher than Q 2
Challenge 3: How to find various kinds of information needs and group popular questions accordingly? Given a set of ranked questions Q 1 , Q 3 and Q 2 , some of them like Q and Q 3 , representing the same kind of information needs for the query, should be grouped together. The Q 2 , which is different from Q 1 and Q 3 should be in another group.
To the best of our knowledge, the most related work is question retrieval  X  X inding similar questions for a specified quesiton [1, 2, 5], but they do not study the MQ2Q prob-lem specifically dealing with all the above challenges. First, most question retrieval requires either large amount of train-ing data [2, 5] or linguistic strcutrues [1] that are difficult or impossible to obtain in keyword queries. Second, all the above methods do not take the popularity of questions into consideration when ranking is performed. Intuitively, among a set of relevant questions, the question that has been asked frequently should be the most likely to capture the informa-tion needs behind the query.

Another possible solution is the traditional information retrieval method. However, the ranking function based on the similarity between bags of words are biased to short questions. For instance, given a query  X  iPod  X , questions re-turned by either the language model or the vector space model are  X  iPod..?  X  or  X  iPod?  X . Nevertheless, these ques-tions are meaningless for answering the query. Yahoo! An-swers also provides a search function for searching questions. We do not know the details of its algorithm, but in some cases, it suffers from the same problem with the traditional IR model. Furthermore, the ranking functions of both tra-ditional IR method and Yahoo! Answers do not consider the question popularity neither.

To tackle the MQ2Q problem, we propose a three stage algorithm. First, it extracts  X  topics  X  from questions and queries, and relevant questions are selected based on match-ing  X  X opics X  of queries. Second, for each relevant question, a score is calculated by exploring a relevance feature and a popularity feature. The popularity feature is based on the assumption that if a question is asked frequently in CQA sites, it is much more likely to be queried again. Third, the popular questions are grouped into various kinds of infor-mation needs, using a clustering algorithm. For evaluation, we compare the algorithm with the traditional language model. Experimental results demonstrate that our proposed approach significantly outperformed the traditional language model, and 47% improvement is obtained in terms of top 5 precision.
 In what follows: Section 2 presents our solution to handle MQ2Q; Section 3 reports experimental results; and Section 4 concludes the paper.
In this section, we describe our three stage algorithm, in-cluding Candidate Selection, Candidate Ranking, and Can-didate Grouping. Before the discussion, we formulate a question representation as a data record containing a T itle , a Description , Answers , a Category , and other meta data.
Given a query, the algorithm first selects a set of relevant questions. The selection method is based on the following assumption: Given an information need, a user can directly post a question according to the need, or select some words from the question in mind as parts of a query. These words shared by the question, and the query are often noun phrases confining the query into certain topics. We call these words topic terms. Thus, if a question represents an information need behind the query, the question must have topic words of the query.

We define topic words of a question as noun phrases ap-pearing in question T itle and terms appeared in question Category . Noun phrases in Question Title can be extracted by a standard POS tagger. Hence, a question Q i can be rep-resented as a topic set, denoted by QT S i = ( T i 1 ; T i 2 For example, given a question  X  How do you transfer iPod songs to your library that thinks it already exists on iTunes?  X , which belongs to the category  X  Music \ Music Players  X , the topic set are  X  iPod , Songs , Library , iTunes , Music , Players  X .
We also define topic words of a query as noun phrases in the query. However, the POS tagger can not be directly applied to simple keywords without a sentence structure. In order to extract topics from a query, we use topic terms appeared in questions to estimate whether a term in a query is a topic, which can be calculated as: where T opicF req ( t i ) t i 2 QS is the frequency of t i tions as a topic term, F req ( t i ) t i 2 QS is the times of term t appeared in all questions. Then, whether a term t i is a topic term can be determined by whether T opic ( t i ) exceeds a threshold . All topic terms in the query form a query topic set , denoted as qT S .

After extracting topics from queries and questions, a can-didate question Q i is obtained by determining whether top-ics of the question cover topics of the query q . The candidate question set Can q can be constructed by following equation.
In order to determine which question is most likely to represent the user X  X  information needs for a given query, our algorithm ranks candidate questions based on the relevance feature and the popularity feature.

The relevance feature aims to measure the similarity between the queries and questions. Evidently, the more sim-ilar to the query, the more possible the question represents the information needs behind the query. There are many similarity measurements. We utilize the score P ( Q | q ) esti-mated by the language model as the similarity metric for a question Q and a query q .
 wh ere w i is a term in query q , n is the length of q , and the p ( w i | D ) is calculated as: where f req ( w i ) is the frequency of w i appeared in Q , N is the frequency of total terms in the data set, and Count ( w is the frequency of w i appeared in the data set.
The popularity feature aims to measure the popular-ity of questions. Obviously, the more frequently be asked, the more possible the question represents a popular infor-mation need behind the current query. In order to compute the popularity of a question, we follow the idea of Question Utility [4], which uses a centrality score C ( Q ) to estimate P ( Q ) based on the lexRank [3] over a graph of connected questions. However, this algorithm can not get good results for questions distributed in different categories, and it is also biased to short questions containing general words such as a question  X  iPod help?  X . The reason may be that original LexRank algorithm is developed for ranking a small number of sentences appeared in one document. To overcome defi-ciencies of the previous approach, we propose a topic-based LexRank. First, the algorithm builds a directed graph with the consideration the topics and category information. In the graph, each node represents a question. If a asymmetric similarity called topic coverage between Q i and Q j exceeds a threshold value, there will be an edge from a node Q i to a node Q j . Then, for a question Q , the centrality C ( Q ) is calculated based on the Random Walk Algorithm with the following weighting scheme: where N is the total number of nodes in the graph, d is a dampening factor, adj ( Q ) is the set of nodes adjacent to Q , and deg ( v ) denotes the degree of node v . Finally, we use C ( Q i ) as our popularity score P op ( Q i ).

The topic coverage is defined as follows: if Q i and Q j belong to the same category, the T CS ( Q i ; Q j ) is defined to be 0; otherwise it is defined as: T CS ( Q i ; Q j ) =  X  where QT S i is the topic set of question Q i .

Given the relevance feature score P ( Q i | q ) and the popu-larity feature P ( Q i ), we employ a single formula to combine them as a final score, denoted by Score ( Q i ), for each ques-tion. Then, questions are ranked by Score ( Q i ). One simple way is taking the popularity score P op ( Q i ) as the probabil-ity of Q in the language model. Therefore, the final score can be calculated as:
In CQA sites, many questions are very similar, which makes the top 10 returned questions for a query nearly the same. Thus, in order to get popular questions focusing on various topics of the specified query, the third step of the al-gorithm use a simple grouping algorithm to re-organize the ranked questions. The pseudo-code of candidate question grouping algorithm is depicted in Algorithm 1. The selected question of each cluster is outputted as results. Al gorithm 1 Candidate Question Grouping Algorithm
Inp ut: a ranked candidate question list RQL , Output: selected question list SQL
Procedure: 1: add the first question Q of RQL to SQL 2: remove the question Q from RQL 3: for each question Q r in RQL do 4: for each question Q s in SQL do 5: sim = CosineSimilarity ( Q r ; Q s ) 6: record the maximum of sim as maxsim 7: end for 8: if maxsim &lt; SimilarityT hreshold then 9: insert Q r into SQL 10: end if 11: add Score ( Q r ) to Score ( Q s ) 12: if size( SQL ) &gt;N then 13: return 14: end if 15: end for 16: score ( Q s ) = = N umberof SimilarQuesitons 17: rank SQL 18: return SQL
This section experimentally evaluates the efficiency of our proposed approach.
 Experimental Setup: We collected question data from Yahoo! Answers. To reduce the overhead of crawling, while maintaining a meaningful scope of experiments, we crawled questions related to the  X  iPod  X  topic. To crawl questions in the  X  X Pod X  topic, we extracted 2,366 queries containing  X  X Pod X  from search logs of a commercial engine, and sub-mitted them to Yahoo! Answers X  Search API, resulting in 155,003 questions. We randomly sampled 60 queries from  X  X Pod X  X ueries for evaluation. We only use words in question titles, and all words are stemmed with the Porter stemmer.
To verify our model, we use the Query Likelihood Lan-guage Model (LM) as the baseline method, since it is a state-of-the-art retrieval model. We compare it with our MQ2Q without question grouping model (ranking according to Score ( Q i )) and MQ2QC with question grouping model. For each model, we got 10 results for each query. Given results for each query, an assessor is asked to label it with  X  relevant  X  or  X  irrelevant  X . As mentioned in Section 1, a rele-vant question must contain a clear information need. Hence, a question  X  iPod battery help!  X  is not relevant for the query  X  iPod batteries  X . Besides, the accessor is asked to select pop-ular questions from results for each question. If the accessor does not find any popular/interesting question, there will be no popular question for these results.

Based on the labeled data, we make use of two metrics for evaluating each model. They are TOP N precision ( P @ N ) and Mean Reciprocal Rank ( M RR ). The P @ N is used to measure the percentage of relevant questions among top N returned results, defined as dard measurement used by TREC QA. For a query, the Re-ciprocal Rank ( RR ) of a model is defined as 1 r , where r is the highest rank of a document retrieved by the model that is judged relevant. In contrast, we evaluate r as the highest rank of a question retrieved by the model judged popular.
Quantitative Results: Table 1 lists results of the Top 5 precision ( P @5), Top 10 precision ( P @10), and M RR of three models. We can observe that either MQ2Q model or iP od downloaded video...? iP od video downloads? Do wnloading video on video-iPod? where can I download free music video X  X  for my iPod? Ho w do you download music videos on an iPod? Where can I download music videos for my iPod nano? Ho w do you download music videos on an iPod? Ho w do I get a YouTube video onto my iPod? Ho w can I download dvds on my video iPod? M Q2QC model outperform the baseline method significantly in all three measurements. Furthermore, the MQ2QC model performs nearly the same as the MQ2Q model in terms of Top N precision, since they are based on the same rank-ing method. The MQ2QC model performs better than the MQ2Q model in terms of M RR , since top 10 questions re-turned by MQ2QC are more diverse than that returned by MQ2Q, which are more likely to contain a popular informa-tion need. For the LM result, we can find that P @5 is lower than P @10. This is because, its ranking method can not rank relevant questions appropriately. On the other hand, P @5 of both the MQ2Q and MQ2QC models are higher than P @10. It shows that relevant questions are ranked in the top places. Consequently, based on the aforementioned re-sults and analysis, we can conclude that our ranking method can find popular and relevant questions efficiently.
Case Study: Table 2 presents top 3 results of three mod-els for the query  X  iPod downloaded videos  X , and Figure 1 illustrates the top 3 results returned by Google. The in-formation needs behind the query may be where to down-load videos for iPod. For the LM results, questions seems to be matched with the query, but they do not contain a clear information need. Users will not be interested in these questions. Google also returns some relevant pages like a re-(Q 1:)Why does my ipod touch battery go down when its off? (Q2:) iPod Nano...my battery never fully charges? (Q 1:) What video format do I need so I can play a video on my iPod? (Q2:) My iPod is not playing my music videos and my movies what do I do? (Q 1:) Where can i get free iPod game X  X  movie X  X  and music video X  X ? (Q2:) Free iPod games for the ipod mini? (Q 1:) What devices do I need in order to connect iPod with car CD player? (Q2:) Anyone know what you con-nect to a CD player to use it with your iPod? la ted software pages, apple X  home pages, and a how-to page. However, these pages do not answer user information needs directly, which require users to make efforts in exploring these sites. Questions returned by the MQ2Q and MQ2QC models do capture the popular information needs behind the query. Moreover, results in MQ2QC cover 3 different pop-ular information needs, while questions returned by MQ2Q are essentially the same. We also show some query and ques-tion pairs returned by our algorithm in Table 3. Note that, popular information needs behind each query can be clearly captured by mapping the query to questions.
The key contributions of this paper can be summarized as follows: (1) We first investigate the problem of mapping web queries to questions, and present some potential appli-cations of this problem. (2) We propose a MQ2QC model for solving the challenges in the problem. (3) Experimental results demonstrate that it can efficiently find popular ques-tions that capture various users X  information needs behind a general keyword query. This work was supported in part by NSFC 61003049, the Fundamental Research Funds for the Central Universities under Grants 2012QNA5018 and 2013QNA5020, and the Key Project of ZJU Excellent Young Teacher Fund. [1] Y. Cao, H. Duan, C.-Y. Lin, Y. Yu, and H.-W. Hon. [2] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar [3] D. R. Radev. Lexrank: graph-based centrality as [4] Y.-I. Song, C.-Y. Lin, Y. Cao, and H.-C. Rim. Question [5] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for
