 Dept. of Computer Science and Engineering must be separated by a low density valle y.
 comparable to the state-of-the-art on several common UCI datasets. section 5. 2.1 Outline of the Idea density and (ii) the label assignment conforms to these clusters.
 approximated from the eigen vectors of a kernel matrix obtained from the unlabeled data. constructing a classication function.
 This suggests the follo wing learning strate gy: simply using the KPCA basis does not seem to benet from unlabeled data and, in fact, cannot basis subselection procedure sho ws results comparable to the state of the art. resulting in poor performance due to overtting.
 We now proceed with the detailed discussion of our algorithm and its analysis. 2.2 Algorithm labeled examples f ( x unlabeled examples f x sian kernel k ( x ; z ) = exp k x z k 2 where K cluster and is nearly zero everywhere else (no sign change).
 Specically , we say that a vector e = ( e either 8 i e vectors. Our goal is to learn a linear combination of the eigen vectors P eigen vectors are zeros. Thus, the task is more of model selection or sparse approximation. examples and apply a L 1 penalty (on studied in [ZY06 ]. Thus we would seek a solution of the form is the rst l elements of v regularization parameter . Note that solving the abo ve problem is equi valent to solving because for any given 2 [0 ; 1 ) , there exists a t 0 such that the two problems have To obtain a classication function which is dened everywhere, we use the Nystrom extension of the i th eigen vector dened as tains indices of all nonzero ^ ment is given by and can be computed while training.
 Algorithm for Semi-supervised Learning Input: f ( x Parameters: !;t; model consisting of linear combination of representati ve eigen vectors. is a con volution operator L The eigenfunctions of the symmetric positi ve denite operator L which will be required for our analysis. Let ^ that Lasso is sign consistent if, as l tends to innity , signs of ^ sparse model, matching zeros of ^ of generality assume = ( q let us write the rst q and j N j q columns of as predictors to be less than 1, which can be written as = max 1 the follo wing result, of irr epr esentable condition holds.
 which in turn describes a good classication function.
 Theor em 3.2. Let q be the minimum number of columns of the design matrix 2 R l j N j , con-the number of unlabeled examples u satisfies u &gt; 2048 q 2 log ( 2 ) where literature [CC96 , RV95 , SB07 , SNZ08 ]. 3.1 Brief Ov erview of the Analysis point of vie w, which is a well studied eld [W ai06 , ZH06 , CP07 ]. the eigenfunctions of the con volution operator L representati ve eigenfunctions increases exponentially fast. 3.2 Separation Requir ement p is beha vior of the eigenfunction of L Theor em 3.3. [SBY08a ] The top eig enfunction L value on the support of the underlying density , (4) satisfies j L property), wher e p is the underlying probability density function. but is satised by all eigenfunctions of L when the underlying probability distrib utions are p corresponding eigenfunctions in the abo ve three cases are 1 the corresponding operators as L p 1 Then we can write, L p T T values T operator is applied to the mixture. Clearly , we can not expect T there is suf cient overlap between p that its density function p satisfies 1) R dist ( z ; R ) t , the volume S = f x 2 ( X n R ) \ B ( z ; 3 t= R
S p ( x ) d x C 1 exp where the distance between a point x and set D is dened as dist ( x ; D ) = inf follo wing lemma.
 Lemma 3.1. Let p be . If = ! p d then T The estimate of in the abo ve lemma, where we hide the log factor by , is by no means tight, L depending on the mixing weights about the eigenfunction corresponding to the lar gest eigen value. Lemma 3.2. For any e ing to the lar gest eig envalue 1) For all x 2X nR ; j L 2) For all z 2R and x 2X nR ; j L exponentially fast. 3.3 Finite Sample Results We start with the follo wing assumption.
 Assumption 2. The N simple and bounded away from zer o.
 Note that Nystrom extension eigen values of L to the bounded away from zer o part, which ensures that if we restrict to lar gest N is to some other function in L 2 ( X ; P when eigen values of L Lemma 3.3. Suppose Assumption 2 holds and the top N k probability at least (1 ) the following holds, 1) h 2) 1 3.4 Concentration Results Ha ving established that f next, we need to consider what happens when we restrict each of the Note that the design matrix 2 R l j N j is constructed by restricting the f points f x No w consider the j N jj N j matrix C = 1 First, applying Hoef fding' s inequality we establish, Lemma 3.4. For all i;j 2 N and Ne xt, consider the j N j j N j normalized matrix C 0 where C 0 sample results yields Theorem 3.2. 4.1 Toy Dataset Here we present a synthetic example in 2-D. Consider a binary classication problem where the eigenfunctions corresponding to the lar gest eigen values. path associated with representati ve eigenfunctions. 4.2 UCI Datasets (manifold regularization) method Laplacian SVM (LapSVM) [BNS06 ], fully supervised SVM and and the average is reported.
 of non-zero coef cients.
 variability than LapSVM. average sparsity A and number of eigen vectors ( j N j or 20). 4.3 Hand written Digit Recognition in pairwise classication of handwritten digits and compare its performance with LapSVM. For terms and the kernel as reported by [BNS06 ] for a similar set of experiments, namely we set As can be seen our method is comparable to LapSVM. of our method. KPCA + Lasso. [BPV03] Y. Bengio, J-F . Paiement, and P. Vincent. Out-of-sample Extensions for LLE, Isomap, [CP07] E. J. Candes and Y. Plan. Near Ideal Model Selection by ` [Das99] S. Dasgupta. Learning Mixture of Gaussians. In 40th Annual Symposium on Foundations [HTF03] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning Data [RBV08] L. Rosasco, M. Belkin, and E. De Vito. Perturbation Results for Learning Empirical [RV95] J. Ratsaby and S. Venkatesh. Learning From a Mixture of Labeled and Unlabeled Ex-[SB07] K. Sinha and M. Belkin. The Value of Labeled and Unlabeled Examples when the Model [SBY08b] T. Shi, M. Belkin, and B. Yu. Data Spectroscop y: Learning Mixture Models using [SSM98] Bernhard Scholk opf, A. Smola, and Klaus-Robert Muller . Nonlinear Component Anal-[Tro04] J. A. Tropp. Greed is Good: Algorithmic Result for Sparse Approximation. IEEE Trans. [W ai06] M. Wainwright. Sharp Thresholds for Noisy and High-dimensional Reco very of Spar -[ZH06] C. Zhang and J. Huang. Model Selection Consistenc y of Lasso in High Dimensional [ZY06] P. Zhao and B. Yu. On Model Selection Consistenc y of Lasso. Journal of Mac hine
