 Multi-view semi-supervised learni ng methods try to e xploit the com-bination of multiple views along with large amounts of unlabeled data in order to learn better predictive functions when limited la-beled data is available. However, lack of complete view data lim-its the applicability of multi-view semi-supervised learning to real world data. Commonly, one data view is readily and cheaply avail-able, but additionally views may be costly or only available in some cases. This work aims to make multi-view semi-supervised learn-ing approaches more applicable to real world data specifically by addressing the issue of missing views. We introduce CoNet, a fea-ture generation method that learns a mapping from one view to an-other that is specifically designed to produce features that are useful for multi-view semi-supervised learning algorithms. The mapping is then used to fill in views as pre-processing. Our comprehensive experimental study demonstrates the utility of our method as com-pared to the state-of-the-art multi-view semi-supervised learning methods for this scenario of partially observed views.
 H.2.8 [ Database Applications ]: Data Mining; I.5.2 [ Design Method-ology ]: Feature evaluation and selection semi-supervised learning, multi-view learning, missing data
With the fast development of cost-effective data collection meth-ods in imaging, the health care industry, the web, social networks, and sensor networks, data from multi-sensory devices, i.e., multi-view data, become ubiquitous. In the multi-view data setting, in-formation collected from each sensory device is a  X  X iew X . Often individual views are sufficient for prediction tasks given enough la-beled data. Multi-view semi-supe rvised learning methods aim to take advantage of large amounts of unlabeled data by enforcing view-specific predictor consensus on the unlabeled data. Multi-view semi-supervised learning (MVSSL) has been shown to be ef-fective in a variety of applications including text mining [5, 45, 46], image annotation [14, 39], and chemical classification [11, 12]. A key limitation that restricts the wide application of existing MVSSL approaches to a wide range of real-world data sets is that those approaches require the completeness of the data set. Com-plete multi-view data, however, are rare and a much more common scenario is incomplete multi-view data where views may only be available for a subset of samples. For example, for prediction tasks involving chemicals, molecular structure features based on chemi-cal graphs (view 1) can be readily obtained, but obtaining the chem-ical bioactivity data (e.g., chemical-protein interaction profiles) for a set of proteins (view 2) can be costly and time-consuming. As an-other example in medical diagnos tics [45] where additional views correspond to expensive tests like MRI imaging, information from such views are subject to opportunity. Yet another example of in-complete views comes from webpage classification where incom-ing link text features provide a convenient second view [5]. Such information may not be always available for new webpages since it requires time and resources to collect.

This case of MVSSL with various amounts of incomplete view data, which we call multi-view semi-supervised learning with par-tially observed views , is commonly encountered in many real-world applications but has barely been addressed in the data mining and machine learning literature. The first method to claim credit for considering missing views in the MVSSL setting is the Gaussian process co-regularization (GPCR) approach [45]. Under this ap-proach missing views are handled in a Bayesian framework by integrating out the missing view function values. Though it has achieved promising preliminary results, GPCR has several limita-tions. First, GPCR is built on a particular MVSSL framework, co-regularization, which is not always the best or most appropriate for a given application. Second, GPCR essentially ignores those unla-beled data points without a second view, limiting its applicability to cases with little-to-no second view data. A closely related direction to handling partially observed views is the study of MVSSL meth-ods when there is no second view data [6, 7, 16, 27, 31, 41, 48, 49]. The most recent, state-of-the-art method in this category is pseudo multi-view co-training (PMC) [7], which is also the first in this cat-egory to explicitly consider conditions for the success of MVSSL algorithms. This method works b y choosing a feature partition at each iteration in order to artificially derive two views. However all of the methods in this category com pletely ignore additional view data and hence cannot take advantage of such data when available. Furthermore, whereas appropriate real data inherently satisfies the desired conditions, with artificially constructed view data the satis-faction of such conditions can only be approximately estimated. In addition feature-splitting approaches like PMC will fail when all or most of the features in a view are needed for a predictor to achieve high-performance. Furthermore the transformation needed to re-sult in two sufficient views may be more complex than a simple partition. Additionally these met hods are also often tied to a par-ticular MVSSL algorithm, e.g., PMC is closely integrated with the co-training algorithm and it is not clear if it could even be applied to a co-regularization algorithm, for example.

We aim to extend MVSSL to handle cases with partially ob-served views. In our study, we assume there is one view that is present in all data. The rest of the views may only be partially observed. Although this assumption may seem restrictive at first glance, it is quite generic in real-world examples. For example, in the chemical activity prediction example that we cited previously, features computed from chemical structures are always available (since those features are computed). As another example, in the webpage classification example, for every webpage, features com-puted from the content of the page itself (e.g., the bag-of-word rep-resentation of the page) are always available but the incoming link information may be missing.
 To solve the problem, we have designed a unified approach, Co-Net, which uses a feature-generation network for learning a map-ping to fill in missing views. A motivating observation is that feature generation approaches ar ewidelyusedtoimproveperfor-mance for standard supervised learning tasks, therefore we might expect a feature generation approach to also be helpful in the MVSSL setting. However, a key difference is that the goal for the gener-ated data is different -in this case the generated view data should have properties making it useful for MVSSL, that is in conjunc-tion with the original data. We start with the idea of using random nonlinear feature generation functions to generate new view data. Random nonlinear features allow var iability in the generated view: the data points are  X  X cattered X  to some extent so that labeled data points may be closest to different unlabeled data points in the gen-erated view. This helps ensure that conditions sufficient for the success of MVSSL algorithms are met, in particular the  X  X xpan-sion X  condition [3] requiring that there is some chance that some unlabeled data instances can be labeled with  X  X onfidence X  in one view but not the other. By incorporating these features together in a network structure, we can then fine-tune the collective set of feature generation functions to further ensure that the conditions for MVSSL algorithms are met, namely label consistency and view variability, and additionally that the generated features are consis-tent with any partial view data available. This results in a very nat-ural approach to generating features for MVSSL. Our approach has the key advantages of operating as a pre-processing step which al-lows the subsequent application of the most application-appropriate MVSSL algorithm to the completed data, efficient out-of-sample extension, and the ability to make use of additional view data when available. Our comprehensive experimental study demonstrates the utility of the CoNet method as compared to the state-of-the-art MVSSL methods GPCR and PMC.
Multi-view semi-supervised learning has attracted significant re-search interest in recent years [9, 42, 43]. Methods for multi-view semi-supervised learning generally exploit in some way the idea of predictive agreement on unlabeled data for ideal functions from each view, whether explicitly or implicitly. MVSSL approaches can be roughly divided into three major categories: pseudo-labeling approaches, which iteratively label unlabeled instances [5]; co-reg-ularization approaches, which incorporate the agreement idea into an optimization problem via constraints or regularization terms [14, 37, 47]; and active learning approaches, which use the agreement idea to select unlabeled instances for labeling by a human [25].
View Generating Functions. Theoretical results were estab-lished and verified in experiments showing that improved general-ization error could be achieved by using pre-defined view-generating functions mapping one view to another to fill in missing views and effectively increasing the training set size for each view [1]. The limitation of this work is that the existence of  X  X atural X  view map-ping functions (e.g., translators for cross language text categoriza-tion) is assumed. Such natural view mapping functions do not exist for many applications.

View Splitting for MVSSL. One extreme case of partially ob-served views is the case of having only a single view. There are several approaches that aim to extend the ideas of multi-view semi-supervised learning to single view learning, following a general idea of splitting the features of one view into multiple sets [6, 27]. Recently, one such approach was proposed in which features are split into two views according to criteria that included satisfying the expansion condition for co-training [3], by finding a split such that some unlabeled instances are labeled with confidence in one view but not the other given the current view models [7]. However fea-ture splitting approaches rely on the assumption that the split sets of features will be sufficient for learning. This means they cannot be applied to data where most of the features are needed for learning a good predictor, for example, s ee Figure 3; splittin g the features in this case would result in overlapping classes in each new view. Secondly, even if useful redundancy is present in a single view, this redundancy may be in the form of arbitrary linear combinations of the features or more complex functions of the features, as opposed to the more restricted mapping of feature partitioning.

Additionally for the single view case, several approaches based on using diverse predictors have been proposed [16, 41, 48, 49]. However, in addition to restricting the choice of algorithms, these approaches do not have a clear way for choosing which predictors to use. For instance in one approach co-training was performed using  X  -nearest-neighbor regressors with different distance metrics and/or values of  X  in place of different views, but mixed results were obtained depending on the arbitrary choices [49], and further this limits what methods can be used and diversity may come at the cost of worse performance for the individual predictors used.
It is also worth mentioning that many latent model, multi-modal fusion methods [8, 22, 26] might also be used to estimate missing views, but these approaches have the goal of combining different views into one as opposed to exploiting the variability in distinct views, and as such they do not consider the subsequent application of MVSSL algorithms.

When we say that one view is  X  X issing X  in MVSSL for a data instance, we mean that all the feature values in that view are not recorded. In this sense we are discussing structured missing val-ues, which is dramatically different from handling random missing values [24]. We use the following notations throughout the rest of the paper. We use lowercase letters to represent scalar values, lower-case let-ters with an arrow to represent vectors (e.g.,  X  X  X  ), uppercase letters to represent matrices, and uppercase calligraphic letters to represent sets. We use  X  X  X   X  X  X   X  X  X   X  =( a  X  -dimensional vector  X  X  X  . Unless stated otherwise, all vectors are column vectors.

In MVSSL with partially observed views, we have two sets of data. One set is a set of  X  labeled samples, e.g., { (  X  X  X  1 ..., (  X  X  X  1  X  , X   X  2  X  ,..., X   X   X   X  , X   X  ) } X  X  X  1  X  X  2  X  X  . Additionally we have asetof  X  unlabeled data points from the same spaces,  X  X  X  1  X  X  2 .  X  is the number of views.

For simplicity we will restrict further discussion to the case of  X  =2 views, though all the proposed methods can be extended to more than two views. We take  X  1 to be  X   X  1 and  X  2 to be some positive integers  X  1 and  X  2 , i.e., view 1 has  X  1 view 2  X  2 features. We also restrict the label space to  X  = { X  1 , 1 } since all of the applications discussed and tested in the experiments deal with binary classification. Additionally we assume that one view is always present but the other is potentially missing in some samples, for two reasons. First, this is the scenario encountered in all data sets used in the proposed experiments, and is the most commonly encountered one. Second, solving this case immediately provides a solution to the case of additional views that may also have missing view cases, simply by computing pair-wise feature generation functions for filling in each view.
There has been much research on the conditions for which MVSSL may lead to improved predictive performance. There are at least four directions. First origina lly the condition of conditional inde-pendence of views given the class label was proposed as the re-quired condition for the success of co-training [5]. Second for the co-regularization method [46] showed how the co-regularization approach was equivalent to using a special data-dependent kernel for the support vector machine. [38] simplified the theoretical anal-ysis and established similar bounds as [34] and further proposed a co-regularized alternative to manifold regularization [4] that of-fered significant empirical improvement in their experiments. Fol-lowing this direction [45] designed a Bayesian MVSSL algorithm that handles missing views.

We follow a different direction of view expansion. It has been shown that an  X  X xpansion X  condition, weaker than conditional in-dependence, is sufficient for MVSSL to improve over single view learning [3]. This condition requires that there exist some instances whose labels are not confidently 1 known in one view but are confi-dently known in the other view, so that labels could be propagated iteratively between views. One illustrative way of thinking about this is with the following example with two data views. Suppose an unlabeled instance  X  X  X  1 in view 1 is in a region in which a given predictive model is confident corresponds to label  X  , e.g., due to being close to many  X  -labeled instances in that view. It may be reasonable to assume with confidence that the label of  X  X  X  1  X  . Then the expansion condition would require that the same un-labeled instance, (  X  X  X  1 , X   X  2 ) not be in such a confident region when restricted to the second view,  X  X  X  2 in view 2 , at least for some such (  X  X  X  1 , X   X  2 ) in the unlabeled data. For example,  X  X  X  2 may only be near other unlabeled instances in view 2 . If this condition always holds as confident labels are propagated between views, than all of the instances can be labeled. This example is illustrated in Figure 1, where the solid rectangle corresponds to the positive class and the dotted box shows a possible  X  X xpanded X  region for the location of the corresponding view 2 point. This potential shuffling means that labeled points can end up near different unlabeled points in the sec-
In the theoretical results of the cited paper  X  X onfident X  means  X  X ith probability one X  i.e., absolute certainty. The authors con-sider particular scenarios where certain regions of the input space can be labeled with absolute certainty. In practice this is relaxed to mean  X  X elative confidence X  for the specific model being used, for example, if a linear model is used the unlabeled instances whose labels are considered to be the most confidently known are usually taken as those farthest from the hyperplane defined by the linear model. ond view and therefore label confidence (based on proximity) can be transferred to the unlabeled points.

This condition motivates the idea proposed here of using the distances between the profiles of the data in each view for deter-mining if pairs of views provide sufficiently complementary in-formation, when evaluating candidate values for filling in missing views. Here  X  X rofile X  refers to a vector capturing the relationship between a data instance  X  X  X   X  in view  X  and all of the unlabeled data in that view,  X  X  X   X   X  +1 ,..., X   X   X   X  +  X  . Specifically here the profile vec-tor  X  X  X   X  in view  X  of distances between  X  X  X   X  and each  X  X  X   X   X  X  X   X  =  X  (  X  X  X   X  , X   X  An additional motivation for this idea comes from theoretical anal-ysis for co-regularization [38]. In providing a generalization er-ror bound, the authors also found that the key factor that reduced the bound was a sum of distances between the profiles of the la-beled data in each view, with the profiles calculated using a kernel function [38]. The greater these differences in profiles between the views are, the greater the bound on generalization error is reduced.
This motivating difference in profiles idea is incorporated into the proposed approach through a term in the objective function for a feature generation mapping that encourages the sum of squared profile differences file in the second view which may be generated and  X   X  is a dis-tance function, potentially different from  X  . We call this  X  X on-trasting view regularization X  and this term is described in Section 4.4. The intuition is that, by finding a feature generation function that causes labeled data to be nearby different unlabeled data in the generated view than in the original view, this creates the poten-tial for MVSSL methods to effectively utilize the multiple views, for example, through confident label propagation as previously de-scribed.
The main idea behind our approach is to use random nonlin-ear feature functions to introduce v ariability in generated views, and to fine-tune these functions to match sufficient conditions for the success of multi-view semi-supe rvised learning methods and to be consistent with available view 2 data. Matching the available view 2 data also helps to ensure the generated second view is use-ful for classifying the data. To generate random nonlinear feature functions, we generate random projection directions by iteratively sampling a vector  X  X  X  from a  X  1 -dimensional spherical Gaussian and then normalizing  X  X  X  to have length 1 . We than choose an initial offset uniformly at random in the range of the values taken by the projected data (both labeled and unlabeled). A sigmoid transfer function,  X  (  X  )=1 / (1+exp(  X   X  )) is then applied to introduce nonlinearity.

In order to allow easy fine-tuning of the feature functions, we group functions together into a multi-layered network, i.e., our ap-proach fits naturally into a neural network framework. The final layer is the feature output layer o f the network, and each feature function shares all lower layers to allow easier fine-tuning. Each layer is initially generated using the random projection procedure as described above. In our experiments we take the approach of using a single hidden layer followed by the feature output layer, as using a large enough number of hidden nodes can allow sufficient expressivity [10].

In addition we consider the recent advancement from the side of neural networks and explore the initialization strategy of deep belief networks -pre-training the network as a generative model using contrastive divergence [20]. This alternative for initializing the feature generation network potentially provides better perfor-mance and stability as it may capture the data manifold and pre-vent overfitting -identifying an accurate lower-dimensional feature representation for the data could facilitate the feature generation network learning.

Subsequently the first conditio n to ensure thr ough fine-tuning is consistency with available labeled data, which we achieve by adding an additional output node to the network and using a typical loss function for this output node in an overall objective function for the network. Another term is added to the objective function penalizing the distance between generated view 2 instances and ac-tual view 2 instances when available. Finally, although using ran-dom nonlinear features can already help to shuffle the distances between labeled and unlabeled points, we add a  X  X ontrasting view regularization X  (Section 3.2) term to the objective to help ensure this characteristic. Details are given in the following sub-sections.
A neural-network model is proposed for the feature generation network, mapping one view to another. The general model is de-picted in Figure 2, which shows a particular network with three input features in view 1 , three output features in view 2 hidden layer of three units.
 Figure 2: Example feature generation network model, where inputs are entered at the bottom and computations propagate through to the top.

An input  X  X  X  1 from view 1 is presented to the network, each set of values is transformed by a linear function at each node and passed through a nonlinear transformation  X  () to get the output of the node, here we use the sigmoid transformation  X  (  X  )=1 / (1+ exp(  X   X  )) . Thus the vector of outputs for a layer  X  is given by  X   X   X   X   X  (  X   X   X   X   X   X  1 +  X   X   X  ) where  X   X  and  X   X   X  corresponds to the weight matrix and bias vector for the  X   X  X  X  layer of the network, respec-tively,  X   X  0  X   X  X  X  1 for  X  =1 ,..., X  where  X  is the number of layers in the network. The generated feature view, which corresponds to the second view and also must have the same number of features as the second view if available, here corresponds to the output of the second-to-last set of nodes in the network, counting from the bot-tom. In order to also incorporate good performance on the labeled training data, the network X  X  final output is the predicted label.
The weights and biases are then learned from the available data by attempting to find a local minimum of an objective function. In its most basic form, corresponding to a basic feature generation, or neural, network, the objective function is just the sum of a loss term approximating misclassification error. The basic objective function is given by Equation 1, where  X   X   X , X  is the output of the an input to the network of  X  X  X  1  X  .
Since the objective function and all transfer functions are differ-entiable, gradients are straight-forward to compute using the chain rule which results in backpropagation with the network structure. A gradient descent approach is then used to find a local solution.
Once the weights and biases are learned from the data, the model can be applied to each instance missing another view, to generate the missing view for that instance. To ensure generated view data is on the same scale as the available view 2 data, we first gener-ate all view 2 data instances, normalize the data, and then (option-ally) fill in the available real view 2 data. Afterwards, any desired multi-view semi-supervised learning algorithm can be applied to the completed data.
When another sufficient and contrasting view is known to exist, and is present in some cases, ideally the training for the feature gen-eration model should take advantage of this available second view data, to help find a better feature generation function and ensure classification sufficiency of the generated view 2 data. The feature generation model should be biased toward a model that generates values close to the true second view values. This is easily accom-plished in the proposed feature generation network model by incor-porating an additional penalty term in the objective function. The penalty term is the sum of the square differences between the gen-erated view 2 feature output and the true view 2 feature vector for an instance. Let  X  denote the index set of instances for which the second view is present, and  X  =  X  X  X  X  . Then the basic objective func-tion including available second view data is given by Equation 2, where  X   X   X , X  is the output of the  X   X  X  X  layer on an input to the network of  X  X  X  1  X  for  X  in a given index set and  X  =1 ,..., X  ,where  X  trols a trade-off between fitting the labeled data well and fitting the available second view data well.
The new term is differentiable so standard gradient descent ap-proaches are still applicable, and gradient computations are accom-plished succinctly with basic matrix operations.
In order to incorporate the aforementioned differing profile idea in estimating the neural network model, an additional term is added to the objective function of Equation 2, given in Equation 3. This term biases the learning, forcing the generated view to differ more in its labeled data instances X  distances to unlabeled data for larger values of the regularization parameter  X  2 . The idea is that this in turn can help the subsequent application of MVSSL methods to be effective by providing the oppor tunity to utilize differing con-fidence in label predictions for unlabeled data for different views, e.g., by propagating confident label predictions between views, as explained in Section 3.2.
Again this term fits within the backpropagation framework and allows computation with basic matrix operations.

Additionally, for huge amounts of unlabeled data a stochastic gradient approach can be used in estimating the unlabeled data pro-file distances -a sample of the unlabeled data in such cases could be used to estimate the difference in profiles, and thus a random sample could be taken at each gradient update.

The basic training and testing procedures for multi-view semi-supervised learning approaches combined with the proposed fea-ture generation approach are given by Algorithms 1 and 2, respec-tively.
 Algorithm 1 Training with the Feature Generation Network Input: A set of data  X  containing (view 1, view 2, label) triplets, in which view 2 and labels may be missing for a given instance, ini-tial weights and offsets  X   X  ,  X   X   X  ,  X   X  , a multi-view semi-supervised learning algorithm  X  which outputs a predictive function  X   X  1  X  X  2  X  X  X  given complete training data. Additional parameters for the feature generation network,  X  1 ,  X  2 , number of backpropa-gation iterations  X  , and whether or not to use only the generated view 2 data.
 Output: Final weights and biases for the network  X   X  ,  X   X  the trained predictor  X   X  . -Use  X  iterations of gradient descent to find an approximate local solution to Equation 2 with Equation 3 added to the objective. -Use the learned network (  X   X  ,  X   X   X  ,  X   X  ) from the previous step to generate view 3 for all instances in  X  . Normalize the generated view 3 data. -Fill in any missing view 2 instances of  X  with those from the previous step, the generated view 3 ; optionally replace non-missing view 2 instances with the generated ones as well. Denote the completed data  X   X  . -Apply algorithm  X  to the completed multi-view semi-supervised data  X   X  to obtain  X   X  .
 Algorithm 2 Testing using the Feature Generation Network Input: A set of data  X  containing (view 1, view 2) pairs, in which view 2 and may be missing for a given instance, a trained feature generation network (  X   X  ,  X   X   X  ,  X   X  ), and a trained predictive function  X  :  X  1  X  X  2  X  X  X  , and whether or not to use only the generated view 2 data.
 Output: Predictions  X   X  X  X  for each instance of  X  . -Use the trained network (  X   X  ,  X   X   X  ,  X   X  ) to fill in any missing view instances of  X  and optionally replace the available second view data; denote the completed data  X   X  . -Apply  X  to each instance in  X   X  to obtain the predicted  X  instance.
The recent resurgence in interest in neural networks in the ma-chine learning and data mining communities is the result of differ-ent interpretations of / assumptions about the networks; the models along with these new interpretations/assumptions are often referred to as  X  X eep belief networks X  due to a different generative proba-bilistic (i.e., belief) perspective being assigned to the multi-layer networks [13, 15, 19, 20, 28, 30, 35]. In general most modern approaches keep the same layered structures, and in terms of pre-dictions and network outputs, in general the same feed-forward ap-proach is used to generate layer and label outputs. Additionally backpropagation is commonly still used to fit the net to the data af-ter pre-training. The key difference of the modern approaches are the assumptions of the underlying probabilistic models which can result in different pre-training strategies [13], for example, using layer-wise contrastive divergence [19] to pre-train networks layer-by-layer with unlabeled data. A key practical difference between past neural network methods and modern ones is in how the net-works are pre-trained or initialized. Also, even standard neural net-work methods that do not use pre-training and just use the back-propagation have still been used recently to achieve state of the art performance [44]. Although our approach is for generating an additional, complementary set of features as opposed to replacing an existing one, this view generation problem could offer a new direction for work on deep network architectures, and our regular-ization terms could be viewed as additional ways to prevent overfit-ting with such architectures. An important component of our work is testing the combination of the deep belief network approach with our method, through pre-training the feature generation network.
We test our method with synthetic and real data. For each exper-iment we report results in terms of test error if the data is balanced, and also Matthews Correlation Coefficient (MCC) and F1 Score if the data is unbalanced. Let  X  X  X  denote the number of true positive predictions,  X  X  X  the number of false positives,  X  X  X  false negatives, and  X  X  X  true negatives. Note that MCC and F1 score attain their best values at 1, and test error at 0, and MCC takes into account both false positive and false negative rates whereas F1 score does not take into account the false negative rate.

We compare our method CoNet with two state-of-the-art meth-ods. The first method has the claim of being the first approach to handle missing view data in the MVSSL setting, Gaussian pro-cess co-regularization (GPCR) [45]. The second is the most re-cent approach to applying MVSSL to the single view case (com-pletely missing second view -i.e., whatever second view data is available is ignored) and reported state-of-the-art results -pseudo multi-view co-training (PMC) [7]. We obtained the code for PMC from the authors, and used the  X  X aussian Processes for Machine Learning Toolbox X  version 3 . 1 [32] to implement GPCR. Note that for our experiments in general we cannot apply basic multi-view semi-supervised learning methods not designed to handle missing view data, such as co-training, as baselines. This is because view 2 is missing at random and may not be present even in the labeled data, or if it is it may only be present for one class due to the of-ten highly imbalanced nature of the data. Additionally we com-pare with the baseline of only using the single omnipresent (first) view, using a Gaussian process classifier with this view (View 1 GP) [33]. For all methods, we use the same logistic loss model for fair comparison. PMC uses logistic regression models for the base classifiers, and we use logistic likelihood models in GPCR and in a Gaussian process classifier for the view 1 only baseline (View 1 GP). For the MVSSL algorithm used by CoNet we use either GPCR with logistic likelihood or co-training with  X  1 regularized logistic regression classifiers as the base models. To simplify the experiments we choose either co-training or GPCR as the MVSSL algorithm used by CoNet based on which gave the best MCC when no second view data is available.

Additionally to allow straight-forward comparison with the GPCR method, all of our experiments are carried out in a transductive set-ting, i.e., the unlabeled data (or some portion of it) for a given trial also corresponds to the test data. Note that CoNet itself is not re-stricted to a transductive setting. For the real data experiments, we perform experiments for CoNe t with both random initialization and the contrastive divergence pre-training and also both filling in ( X  X ill X ) and not filling in ( X  X o fill X ) the second view with the ob-served second view for instances when it is available (observed). For the CoNet methods we fix the number of backpropagation gra-dient descent iterations to 100. For all methods we report the re-sults for the parameters giving the best average performance, where averages are taken across 100 or more random splits of the data, which essentially corresponds to reporting results of model selec-tion if labels were available for some or all of the unlabeled data. Thus we avoid the model selection issue which is common prac-tice in this type of scenario (e.g., [2, 5, 23, 37, 38]), and essentially shows the results achievable given an ideal model selection method for the scenario. Since there is usually a very limited amount of labeled training data in the MVSSL setting, standard model selec-tion approaches like cross-validation often fail [36], so the common procedure of reporting subsequent performance after model selec-tion would not be at all representative of the underlying methods X  performances but rather of the (poor) performance of the model se-lection approach used. Model selection in this scenario is still an open problem [17]. We discuss the model selection issue in more detail and alternative model selection approaches in a technical re-port [29].
We present results for an illustrative 2D data experiment, for the task of learning a function to separate two overlapping sets of Gaussian-distributed data. Data for two views was generated inde-pendently from the same Gaussian distribution for each class. In this way the two views come from the same distribution, but are conditionally independent given the class label -an ideal scenario for multi-view semi-supervised learning algorithms. We vary the mean fraction of second view data available from 0% to the ideal case of 100% , by removing each data instance from the second view completely at random with fixed probability corresponding to each fraction. For each trial, 2 labeled training points and beled points, were generated for each class using the two Gaussian distributions. Figure 3 shows a sample of the generated data in each view.

This data set demonstrates a simple case where existing single-view approaches are generally not well-suited. In this case, feature-splitting cannot be effective since both f eatures are needed for suf-ficiency; splitting the features would result in different data classes largely overlapping in both views. Additionally there are no clear clusters -the marginal distributions look similar to unimodal group-ings of points.

We choose the state-of-the-art Gaussian process co-regularization algorithm [45] as the base algorithm to be applied after filling in the missing views with our CoNet method. In addition we use the ver-Figure 3: Sample of two views of data generated for an ideal 2D test case sion of this algorithm that can handle missing views to compare our method with, as it is the state-of-the-art approach [45]. In addition we report results for comparing with a view-mapping approach -an approach that only directly tries to learn a mapping from view 1 to view 2 using the available data. This corresponds to using our same feature generation network approach to generate the second view, without using the proposed bias, corresponding to Equation 2.

First we varied the mean fraction of second view data available from 0 . 0 to 1 . 0 in increments of 0 . 05 . The experiment was repeated for 200 random samples of the data, and average test error and stan-dard deviation is reported in Table 1 and Figures 4 and 4(b). (a) From 0 (no view 2) to 1 (all) Figure 4: Test error vs. mean fraction of view 2 present for the 2 -Gaussian data set Ta b l e 1 : M e a n  X  std. dev. of test error from 200 trials for each method on the 2 -Gaussian data, for 0 % second view data available.
 Figure 5: Performance criteria vs. contrasting view regulariza-tion parameter and vs. number of hidden units in hidden layer 1for 0 % second view data for the 2 -Gaussian data set
The proposed feature generation approach was found to perform significantly better than using the same base classifier with a single view of the data, or using the state-of-the-art GPCR method, espe-cially in two extreme ranges of having very little view 2 data, and having close to the amount of view 2 data needed to achieve the best performance. Additionally without the contrasting view regu-larization (CVR) term, and with the exact same network structure and approach to initialization and training, the feature generation approach ( X  X oNet CVR X ) took much more view 2 data to come close to the same level of performance as CoNet. We also show the results of repeating the experiment zoomed in more closely on the beginning region, this time varying the mean fraction of view data present from 0 . 0 to 0 . 1 in increments of 0 . 01 . The results are shown in Figure 4.

Furthermore, the results for the single view case -i.e., no view 2 data available are shown in Table 1, here also compared with the state-of-the-art single view met hod, pseudo-multi-view co-training (PMC). In this case PMC fails because the features cannot be par-titioned in such a way to form sufficient views -in this case both features are needed to separate the classes well. This highlights the need for a more complex mechanis m to generate the new view from the existing ones, which CoNet provides.
The WebKB Course data set is a collection of 1051 websites from four universities, belonging to two categories: course web-sites or non-course websites. There are 230 websites in the course category, and 821 in the non-course category, making the data set unbalanced. The first view consists of text on the webpage it-self, the second view consists of the link text of links from other webpages linking to the webpage. We use co-training as the base MVSSL algorithm to be used after filling in the missing views with CoNet for this data set.

We obtained the webpage and link text data 2 then applied stan-dard text pre-processing using Weka [18] to obtain 2,168 features in the text view and 338 features in the link view. As in [5], for each experiment iteration we randomly sample 3 course and course instances for labeled training. The remaining instances were used for the unlabeled data and also testing -a transductive setting so that we could compare with GPCR. We then varied the mean fraction of second view data available from 0 . 0 to 1 . 0 of 0 . 1 . Here the second view is missing completely at random -that is for a given fraction, each view 2 instance is present with proba-bility given by that fraction. We repeated the experiment for each fraction value and report the mean results. For the base classifier for co-training we used  X  1 regularized logistic regression, with the regularization parameters set to 0 . 001 for view for view 2 throughout since these worked well for basic co-training when view 2 was completely available -though as long as these values were not too large (less than 1) the performance stayed basi-cally the same. For the comparison state-of-the-art methods GPCR and PMC we varied all of the parameters by powers of 10 and re-port the results for the best set of parameters in each case. The overall results for the Course data are shown in Figure 6. This plot shows CoNet with pre-tr aining (denoted as  X  X oNet X ) and without pre-training (denoted as  X  X oNet NoP X ) compared with the other methods for varying amounts of expected fraction of view 2 data present (observed), from no view 2 data ( 0 . 0 )toallview2 data ( 1 . 0 ). The results for F1 score follow the exact same trend as for MCC so we do not show them here. Again the other methods are the Gaussian process classifier with the single view ( X  X iew 1
Available here: http://www.cs.cmu.edu/afs/cs.cmu. edu/project/theo-51/www/co-training/data/ GP X ) [33], the state-of-the-art Gaussian process co-regularization (GPCR) [45], and the state-of-the-art single view method, pseudo-multi-view co-training (PMC) [7]. GPCR required significantly more view 2 data to perform better than single view learning for this data. However CoNet was able to take advantage of the avail-able second view data, obtaining the best performance. Also, in this case using pre-training resulted in a significant improvement for CoNet when limited view 2 data was available. (a) MCC vs. fraction present Figure 6: MCC and Test error vs. mean fraction of view 2 present for the WebKB Course data set
In Table 2 we show the effect each component of CoNet has, and also the difference between filling in cases with available view 2 data (denoted  X  X ill X ) and using only the generated view 2 data (denoted  X  X o fill X ). That is we correspondingly fix one or both of  X  and  X  2 to 0 , i.e.,  X  X o Reg X  corresponds to both fixed to Only X  to  X  2 =0 , and  X  X VR Only X  to  X  1 =0 . We show results for the version of CoNet with pre-training and only for MCC, but the other performance criteria have similar trends, and the trends for no pre-training are also similar except that using the available view 2 data becomes the better strategy sooner, at the fraction of that for fraction present equal to 0 . 0 , the  X  X ill X  and  X  X o-fill X  results are the same since there are no available view 2 instances to fill in, and for 1 . 0 since view 2 is present for all instances all  X  X ill X  results are the same.

From these results we observe a general trend -at first, with less view 2 data available (observed), using the generated view 2 as op-posed to filling in the real view is more effective, and further the contrasting view component is more important. As more view 2 data becomes available, so that a better mapping to view 2 can be learned, then filling in the available view 2 data becomes the better strategy, and the view-matching component becomes more impor-tant. Usually both components are needed for CoNet to achieve its best performance, and in most cases one or both components have a significant effect on performance. For the case of limited view 2 data one reason that filling in the available view 2 data does not help might be that the generated view 2 data is very different from the available view 2 data since there is not yet enough to learn a very accurate view mapping function. Another reason using the real view 2 where available becomes a better strategy as more view 2 data is observed is because the real view 2 data has built-in the desirable properties for MVSSL methods, e.g., of sufficiency for classification, whereas for the generated view we can only estimate these properties.
We next evaluated these methods on a chemical toxicity pre-diction task using a data set from the Environmental Protection Agency (EPA) TOXCAST program [21] ( http://www.epa. gov/ncct/toxcast/ ) which includes experimental results con-ducted on 309 unique chemical pesticides. In vitro tests were per-formed with 624 different assays -we take the results of these tests as the feature set for the second view. Since both the animal toxic-ity endpoints and the in vitro second view data are time consuming and expensive to obtain (e.g. the study cost millions of dollars and took more than a year), this data set fits the MVSSL with partially observed views scenario well. Afte r basic pre-processing, e.g., re-moving duplicates and compounds with missing or inconclusive endpoint results, the data set consists of 225 chemical compounds with 597 view 2 features. For the class label we took the toxicity endpoint of  X  X umors on mous e liver X , resulting in 68 positive and 157 negative instances so this data set is also imbalanced. To ob-tain a large set of related unlabeled data, we searched the PubChem database ( http://pubchem.ncbi.nlm.nih.gov/ )forall compounds with the keywo rd  X  X esticide X  or  X  X er bicide, X  resulting in an additional 1262 compounds added to the data set. To ob-tain the common, readily-available view 1, we extracted numeri-cal chemical descriptors from the full set of compounds using the DRAGON software (version 5) [40] for the atom-centered frag-ment descriptors, resulting in a total of 103 features in view 1. For each trial, we randomly sampled half of the labeled data to be used as training data, and the other half to be included with all of the un-labeled data and for testing. Since only those data instances from the original TOXCAST collection have the second view available, the maximum obtainable fraction of view 2 data present is only ap-proximately 0 . 15 . Therefore for this data set we only tested two cases: no view 2 data (labeled fraction present of 0 . 0 ) and all avail-able view 2 data (labeled fraction present of 0 . 15 ). For this data set we use GPCR as the MVSSL algorithm used by CoNet.
The results for the chemical toxicity data are summarized in Ta-ble 3. For this data set, unlike the text data set, using pre-training for the network (denoted as  X  X oNet X ) was somewhat detrimental to performance compar ed to the randomly initialized net (CoNet NoP). Aside from the type of data (e.g., chemical descriptors as opposed to images or text), this may also be due to overfitting of the generative model since there are many more features in view 2 than view 1 in this case. Further improvement may be possible by more thorough experimentation with the pre-training approach used.

Although PMC achieves slightly lower test error than the CoNet methods, it has significantly worse scores under the balanced per-formance criteria (MCC and F1 score) which are more indicative of efficacy for this data. The results indicate that essentially the method cannot detect the positive cases well but still has low test error due to the highly imbalanced nature of the data. On the other hand CoNet scores highly under the more balanced performance criteria, and still manages to reach nearly the same test error in the case of the small amount of partial view data available. This is similar when CoNet (NoP -the no pre-training version -in partic-ular) is compared with the other methods. With respect to MCC, arguably the most balanced criterion, CoNet obtains significantly better performance compared to all other methods. With respect to F1 score, the single view GP classifier has a slightly better score for the expected fraction of 0 . 0 view 2 data present and GPCR has a slightly better score for the fraction of 0 . 15 . However these are not significantly different from the CoNet NoP scores. To give an idea of how the methods compare under the different criteria, we show the results of ANOVA with multi-comparison tests in Table 4. An entry of  X 1 X  indicates a significant difference in the means of the given performance criterion for the two methods at the five percent level.
 Table 4: ANOVA multi-comparison test results for each of MCC, F1 score, and test error criteria on the Chemical Toxicity data, for 0 . 15 fraction of view 2 data present. A  X 1 X  indicates significant difference in mean between the two methods at the 5 percent level.

Table 5 shows the comparison between CoNet with no pre-training (NoP) with both view matching regularization (VMR) and con-trasting view regularization (CVR) and with one or neither, cor-responding to setting the appropriate parameter/s to 0 . For this data  X  X o fill X  to using only the generated view 2 data.
 including both components was necessary to achieve the best per-formance.
An obstacle for multi-view semi-supervised learning approaches when applied to real world data is the lack of complete multiple view data. For example, a common scenario is that one data view is readily and cheaply available, but additional views may only be available in some cases and may be costly to obtain. Current work to address such scenarios is limited and also each previous ap-proach has some limitations. In summary, existing approaches ei-ther are not able to incorporate partial view information when avail-able or are not applicable or effective with limited amounts of addi-tional view data. Additionally, the previous works either make re-strictive assumptions, are method-dependent, or fail to incorporate a way of enforcing the approach to be useful for subsequent appli-cation of multi-view semi-supervised learning algorithms. To ad-dress these limitations, we introduced a unified approach for multi-view semi-supervised learning with missing views that can be ap-plied to the full range of problems with incomplete view informa-tion. We propose a feature-generation learning approach, based on fine-tuning random nonlinear feature functions, for learning a mapping to fill in missing views, with a particular bias incorpo-rated that is motivated by theoretical results on multi-view semi-supervised learning. This is carried out using additional terms in the objective function of a feature generation network model that encourages the data instances in distinct views to be nearby dif-ferent unlabeled instances. We demonstrated the efficacy of our method with synthetic and real data experiments and for these ex-periments our method achieved superior performance to two recent state-of-the-art approaches designed for the case of MVSSL with missing views.
This work has been supported by the National Science Founda-tion under Grant No. 0845951 and a Graduate Research Fellowship award for B.Q.
