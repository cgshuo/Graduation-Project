 Data-intensive software are increasingly prominent in driving global processes such as scientific/medical research, E-go vernance, and social networking. Large amounts of data is collected, processed, and stored by these systems in databases . For example, the Norwegian Customs and Excise department uses the TVINN system to processes about 25000 to 30,000 declarations on weekdays and around 3000 declarations on weekends. TVINN stores validated transactional informa-tion such as declarations in a central database. It processes incoming declarations to verify their conformance to well-formedness rules, customs laws and regula-tions before accepting a declaration in the database. This scen ario is prevalent in many data-intensive software systems dealing with transaction data which comprises of semi-structur ed/structured data in medium/high volume. Testing the data-intensive software systems in the industrial context is the subject of this paper. At Certus Centre, our objective is to identify and provide innovative solutions to thriving industrial problems. Certus is founded as a consortium with several industry and public administration partners and is primarily funded by the Research Council of Norway. We interact with industry partners in weekly meetings to understand and help address their challenges using state-of-the-art research findings and tools. We go by the motto-Industry is our lab! , owing to the fact that our scientific challenges emerge from industry. This paper narrates the story of our collaboration with the Norwegian Customs and Excise department who bring to us the problem of testing the TVINN system.

The TVINN system at the Norwegian Customs and Excise department has been in operation since 1988. It is a massive database application that processes declarations sent as standard EDIFACT messages from companies concerned with import/export in Norway. A batch application called EMIL, written in the legacy language Sysdul, processes about 30,000 declarations/day to notify cus-toms officers about the correctness of the d eclarations. The correctness is verified based on a large set of well-formedness rules and customs laws and regulations. For instance, when a declared amount de viates largely fro m a pre-computed statistical value a declaration is sent back for further evaluation by a compa-ny/customs officer. The customs laws are updated periodically with changes in governmental policies. Customs laws also can be updated for a short period of time by customs officers at borders. This is called mask control . Therefore, the dynamic change of rules in the TVINN system makes testing it constant effort. Moreover, a recent major development i n the TVINN system is its migration from Sysdul to a system developed in Java. Will the new system function equiv-alent to the one currently under operation? Will the system regress? This is the challenge that a public service such has to face. Current practice in testing in-volves domain experts manually creating test databases that aim to reveal faults and differences between different versions of TVINN. Talking to testing experts we understand that manual test creation is cumbersome and most importantly its hard to know where the tester X  X  journey ends. There is an exact lack of the notion of test coverage . In this paper, we outline Certus center X  X  efforts to address this particular challenge.

We present a methodology to automatically generate test data that cover all valid T-wise interactions between database features. These features correspond to selected field values in the specification of a database X  X  input domain which is the database schema .Weusethe feature modelling formalism to help a test engineer select variation points in a da tabase schema as a feature model. It also allows the tester to specify forbidden f eature combinations and feature inter-dependencies. Using a feature model as input we apply our previously developed approach in [1] to generate a set of configurations that cover all valid T-wise inter-actions between database fields of choice. This configuration generation module is scalable to large feature models and satisfies all feature interdependencies in the configuration. The approach is based on transforming a feature model and T-wise combinations to a set of constrai nt systems in Alloy. These systems are solved concurrently and based on a divide-and-combine strategy. The solutions are concurrently generated as sets of configurations that are combined into a final set. We transform these configurations to insertion queries to populate a test database. However, certain fields are still placeholders for values in their ap-propriate domains. The fields that have little significance to a tester are updated using generated SQL update queries. The update queries either update values to maintain data integrity (domain and referential integrity) or random values for fields with less significance to the tester. The result is a complete set of queries to populate a test database covering all valid T-wise interactions between database features of interest. We implemented the methodology in a prototype tool called Fa k t u m (fact in Norwegian).

We performed experiments to validate our methodology. We used a common database schema developed in collaboration with the Norwegian Customs and Excise as the input domain and specify a feature model to select data features concerning imports from Brazil, India, China, and the USA. We generated a test database using these inputs that covers all 2-wise/pairwise interactions between a total of 75 features such as currencies, country codes, country groups, tax fee codes, and declaration categories to name a few. We generated about 935 con-figurations covering all valid pairwise interactions (of 10804 all possible pairwise interaction) between the chosen feature s. These 935 configurations were trans-formed to a set of Structured Query Language (SQL) queries to populate a test database. The time complexity to detect the validity of a single pair (/tuple) is O (1). The average time to detect validity is about 12 ms. We generated 187 sets of configurations covering divided subsets of pairs. The generation of configura-tions is time and scope bounded. Every call to Alloy X  X  SAT solver is bounded to an average of 400 ms and 6803 calls to the solver were necessary to obtain the final set. The large number of calls to Alloy X  X  SAT solver is a trade-off to address scalability. We combined a total of 935 configurations from 187 sets that were solved concurrently in 48 minutes.
 We summarise our contributions as follows: Contribution 1: We propose a scalable methodology and a prototype tool Fa k t u m to automatically generate databases that cover all T-wise interactions between data fields of testing interest.
 Contribution 2: We demonstrate through experiments on an industrial case study that our methodology is scalable and can be used surgically generate tests. The paper is organised as follows. In Section 2, we present a detailed overview of the case study at the Norwegian Customs and Excise and the challenges associated with the complexity of the testing task. In Section 3, we present a methodology based on automatic generation of test databases satisfying T-wise interactions to address the testing problem at the Norwegian Customs and Excise. We present results from a conc rete experiment in Section 4. Section 5 discusses of related work, while we conclude with a summary of our experience in Section 6. We describe our industrial case study from the Norwegian Customs and Ex-cise as illustrated in Figure 1(a). The system under study is the Tollvesenets startside for internettfortolling (TVINN). An overview of TVINN process flow is presented in Figure 1(a) and official description is available at https://fortolling.toll.no/Tvinn-Internett . Customs officers and indus-tries associated with import/export create declarations at Norwegian ports of entry. These declarations are encapsulated in the EDIFACT standard for busi-ness communication. A declaration is encapsulated as an EDIFACT CUSDEC message. These messages are sent to TVINN X  X  central server where they are pro-cessed by a batch application called EMIL. EMIL parses EDIFACT messages and verifies them against well-formedness rules. It then verifies if the declared amount is accurately computed based on a statistical value for an item. These rules depend on numerous factors such as (a) 260 countries of origin divided in 88 country groups (b) over 160 currencies (c) taxes are computed based on about 900 tax code groups (d) an list of more than 10,000 items. A declaration can be categorized into 6 different categor ies. The simplest categories being complete and reject . The response from TVINN is sent back as an EDIFACT CUSRES message to the declaring company. Rules in TVINN evolve on a regular basis depending on new governmental policies, sanctions, and change in political par-ties. TVINN also may contain time-bounded rules created by customs officers. These time-bounded rules are ephemeral and exist for a short period of time. For instance, a customs officer decides to thoroughly check 20 trucks coming from a nation X in civil war. He/she will possibly create a rule to check all trucks from X for the next 3 hours. This rule is called mask control will disappear after 3 hours. These rules can change on an everyday basis without anticipation making TVINN a highly dynamic system.

TVINN is a complex and dynamic database application that processes up-to 30,000 declarations per day. Testing TVINN is a challenge since it evolves rapidly. Moreover, in 2012-2013 TVINN w ill undergo a migration from its native implementation Sysdul to Java. Will the new implementation in Java regress with respect to the old one? This is the question that intrigues the Norwegian Customs and Excise at present and in the years to come.

Testing TVINN has been intuitively achieved by a small testing staff executing a subset of a large number of readily available records of real-world declarations . However, using these real-world declarations or records present four important problems: No Coverage Guarantee: Records obtained from real-world transactions such as customs declarations cover a realistic subset of the database X  X  domain (set of all possible combination of values in fields and tables of a database). However, they often do not cover combinations of values that are very rare or exceptional. Very Large Set of Test Records: Accumulating information from real-world transactions can easily give rise to an ever-growing set of data records. Many of these records share similarities and hence are redundant for the purpose of testing. Cost-effective testing will requ ire a selection of a minimal set of records that precisely captures testing intentions. A minimal set will also have modest time and space requirements for tes ting efforts such as nightly tests. Confidentiality: Governments/enterprises involving financial transactions or military data for instance have stringent confidentiality agreements with their clients. Therefore, its often impossible for them to outsource their testing efforts to external agencies who would use real-world records.
 Constantly Changing Rules: Records for testing often have a lifetime and need to be discarded. For instance, in the Norwegian Customs and Excise system changes when sanctions are imposed on countries or significant changes happen in currency exchange rates. Legacy transaction records may not be used anymore to test the evolved system.
 Our objective at Certus Software V&amp;V center, Simula Research Laboratory, is to present a solution to address these problems. In this section, we present a methodology to generate test databases that cover all T-wise interactions between data fields o f interest. The overview of our method-ology is shown in Figure 3 (a). In Section 3.1, we present foundational notions that will be used to describe our methodology. We present the different steps in our methodology in Section 3.2. In Sectio n 3.3, we describe the implementation of the methodology in our tool Fa k t u m . 3.1 Foundations Database Schema. The first input artifact in our methodology is a database schema . It specifies the input domain of a database. We briefly describe the well-known concept of a database schema. More information on them can be found in a standard database textbook such as in [2]. A database schema typically contains one or more tables . A table contains fields with a domain for each field. Typical examples for field types/domains are integer, float, double, string, and date. The value of each field must be in its domain hence maintaining domain integrity in a database. A table contains zero or more records which is a set of values for all its fields within their domain. A table may contain one or more fields that are referred to as primary keys . This means that each record is identified by its primary key. Table may refer to primary keys in other tables via foreign keys . The value of foreign keys must match the value of a primary key in another table. This is known as a referential integrity constraint. We refer to the combined concepts of referential integrity and domain integrity as data integrity . Records in a database must satisfy data integrity as specified by its a database schema. Databases can be queried using Structured Query Language (SQL) queries. These queries are both used to create and populate a database and query it for information presented as a table or a view. In this paper, we generate several hundred SQL INSERT and UPDATE queries to populate a test data base (see Section 3.2 and Section 4 for more information on g enerated queries).

As a running example, we present a schema developed along with our indus-try partner, the Norwegian Customs and Excise, in Figure 1(b). The database schema consists of four tables and is created on a MySQL server. We describe the tables and some of the fields in them. The Customers table is used to store records of customers. A customer is identified by a CustomerID which is a primary key (indicated PK). A customer can make one or more declarations. These declarations are stored in the Declarations table that refer to acustomer using a foreign key (indicated FK). A declaration can have one or more items that is stored in the Items table. Every item has an item code and a statisti-cal value of its cost. There can be different types of taxes on an item which is stored in the Taxes table. The most common form of tax is the value added tax or VAT. Taxation rules are often expressed on the country group, tax fee code group (from the Taxes table) of import and the item code (from the Items table). The 10,000 items codes, 88 country groups, and 934 tax fee codes can potentially give rise 12.9 trillion 3-wise possible taxation laws. However, only 195,000 taxation laws are used in practice.
 Feature Model of Database Variability. Populating the database schema requires selection amongst a set of choi ces for its field values. The second in-put artifact to our methodology is a model of variation points or choices in the database schema. We use the feature modelling formalism to specify the vari-ability in a database schema. The feature modelling formalism is described in detail in [3]. Typically, a feature model is used to specify the different features in a software product line and their inter-dependencies. Inter-dependencies are constraints on the choice of features. Some features are mandatory, some are optional, some features require other features while some features are mutually exclusive (XOR) with respect to other features. Features can be abstract or con-crete . Abstract feature help in classification and hierarchy while concrete features go into a final software configuration. A configuration of a feature model is a finite set of concrete features that satisfy feature model constraints. The notion of feature models is very popular and a general formalism to specify variability in software product lines and software artifacts in general. We use the feature modelling formalism in this paper to specify variability in a database schema.
In Figure 2, we present the feature model of database variability in the Nor-wegian Customs and Excise schema. The mandatory root feature specifies the database identifier which is TollCustoms . All child features of root in the second level specify identifiers for the different tables available in the database. The third level contains features for database fields in each of the tables. In the fourth level of model the features specify the different possible values for each field. The database, table, and field features in the first, second, and third level respectively are manda-tory but abstract features. The values for fields are mutually exclusive which means only one value can be associated to a field. For instance, Declarations is table fea-ture that has a Category feature. There are six differ ent declaration categories. Only one of the values such as FU can be associated to the category field. We present the field values in a concise manner in Figure 2. We show the first pos-sible variation in a field value while we only show the number of other possible possible values due to space limitations. A selection of field value features for all fields across all tables is what we call a record configuration . A record configuration specifies the exact values that will go into each field for a record in each and every table of the database. We use the term configuration interchangeably with record configuration. Multiple record configurations represents multiple records that go into the different tables of the database. For instance, [ FU,USD,BR,Import,... ] is part of a record configuration that shows values that will go in as a record in the Declarations table.
 Combinatorial Interaction Testing. Combinatorial interaction testing (CIT) is a recognized software testing technique introduced by Cohen [4], that tests all interactions between features, parameter values or in our case database field vales. An interaction can be seen as a tuple of software features .Awidely cited NIST study of the fault databases of several real-life systems reports that all the faults in these databases are caused by no more than six factors [5]. If test features are modelled effectively, a T -wise testing can expose all the faults that involve no more than T features. However, pairwise or 2-way testing has beenshowntobebothtimeefficientan d effective for most real case studies http://www.pairwise.org/results.asp . This motivated us to focus on CIT of all pairwise interactions of database field values of interest (see Section 4). T-wise testing requires that every T-wise interaction of between all features/-database field values is present at least once in a set of reco rd configurations. Generating record configurations that cover all T-wise interactions is a challeng-ing task for very large feature models. There is a combinatorial explosion in the number of possible interactions with the increase in the number of features and the value of T. For instance, 10,000 items codes, 88 country groups, and 934 tax fee codes give rise to 192.5 million pairwise/2-wise and 12.9 trillion 3-wise interactions. In [1], we present a scalable approach and tool to generate a set of configurations that cover all T-wise interactions between features. We use the approach in [1] to generate a set of record configurations that cover all T-wise interactions between database field values. Although, our approach makes the problem tractable it still depends on the computing resources (running in par-allel) for a very large number of interac tions. Therefore, in our experiments we go one step further and surgically select only a relevant subset of field values. 3.2 Methodology Description We describe the methodology in four phases where each phase subsumes several sub-steps. An illustrative overview of the methodology is given in Figure 3 (a). Phase 1. Tester interaction: A tester provides three inputs (a) A database schema specification as described in Section 3.1. (b) Variability in a database as a feature model. The variations in database field values for TollCustoms is shown in Figure 2 (c) The value of T which represents the strength of interactions needed to be tested. For instance, when T = 2 we intend to test all 2-wise or pairwise interactions between database field values. If we consider all pairwise interactions between two fea tures declaration category FU and currency code USD we have 2 2 different interactions as shown in Figure 3(b). Similarly, we have 2 3 possible 3-wise interactions between a set of 3 features. In general, with n features we have 2 T  X  n T possible interactions minus those interactions that are forbidden by feature model inter-dependencies.
 Phase 2. Generation of database configurations covering all T-wise field interactions: A record configuration specifies the field values for each record (across multiple tables) in a dat abase. Therefore, we first generate a set of configurations that cover all T-wise interactions between concrete features (in grey) specified in a feature model such as in Figure 2. This is achieved via steps enumerated 1-4 in Figure 3(a). These steps are derived from our previous work in [1]. We briefly describe the approach from [1] here. In Step 1, we automatically transform a feature model to a constraint satisfaction problem A in the formal language Alloy [6][7]. In Step 2, we generate Alloy predicates that encode tuples of features representing T-wise interactions between concrete field value features (shown in grey in Figure 2). We insert these predicates into the Alloy model A . In Step 3, we detect all T-wise Alloy predicates consistent with A and reject the others which are not accep ted by the feature model specification. In Step 4, we use a divide-and-combine strategy to generate sets of configurations that satisfy interaction tuples divided in subsets. We combine the sets to obtain a set of configurations that cover all T-wise interactions between features. Phase 3. Transformation to SQL queries: The record configurations gener-ated in the previous step need to be transformed into SQL queries to populate a test database. There are three steps 5-7 from Figure 3(a) involved in this phase. In Step 5, we generate SQL INSERT queries to populate a database with record configurations that cover T-wise interac tions. The field values in a record config-uration are transformed to an INSERT query in a straightforward manner. For instance, an INSERT query for the Declarations table is shown in Listing 1.1.
However, we notice that the INSERT query only contains values for a subset of all fields in record. The remaining fields have a NULL value. Therefore, in Step 6, we generate UPDATE SQL queries to fill in values for the remaining fields. The values of the remaining fields can be pseudo-randomly generated in their respective domains while satisfying data integrity constraints. For instance, a value for CustomerID in the declaration table must be an numeric string with at least 8 digits (domain integrity) and every customer id must be present in the Customers table (referential integrity). In our methodology, coverage of T-wise interactions has priority. Therefore, we generate unique random values for all remaining fields. All foreign key field values are identical to their primary key values to ensure referential integrity. In Listing 1.2, we illustrate an UPDATE SQL query to complete the partial record created using INSERT query in Listing 1.1.
 Phase 4. Population of Test Database: The last phase of our methodology is a straightforward population of a MySQL test database. The database is pop-ulated using INSERT queries followed by UPDATE queries (as shown in Step 8, Figure 3 (a)) generated in Phase 3 of our methodology. The test database generated in for the common database schema can be exported to more sophis-ticated industrial data-intensive systems in Step 9. This separation of database specifications allow us to work on a testing-specific subset of the entire database at Norwegian Customs and Excise. The full TVINN database contains several entities and fields that do not interest t esters. The collaborative effort with the Norwegian Customs and Excise departm ent helped us extract a testing specific subset with bi-directional exportability of records. 3.3 Implementation in Faktum We implement the database synthesis tool Fa k t u m for T-wise interaction testing in Java. The implementation is standalone and uses libraries for constraint solving such as Alloy . A prototypical implementation of the tool to generate configura-tions is available online https://sites.google.com/a/simula.no/dbtwise/ . We perform an experiment to synthesize a test database for the Norwegian Cus-toms and Excise case study and discuss the database synthesis results, scalability, and threats to validity of our approach. 4.1 Experimental Setup We develop a test scenario to automatically synthesize a database that will test rules applying to Norwegian imports/exports from/to Brazil, India, China and USA. The feature model of database variability is specified in Figure 4. This is a subset of the full feature model shown earlier in Figure 2. This fea-ture model specifies the variations of fields of interest in the database schema shown in Figure 1(b). This selection of features surgically pin-points a specific testing zone in the vast input domain. We use the feature model in Figure 4 as input to generate a set of configurations that cover all 2-wise/pairwise in-teractions between field values. The total number of interactions is the num-ber of interactions between 37 concrete features minus the invalid interactions due to the XOR relation between field values in the same field. There are 4 interactions that need to be covere d by a set of record configurations. 4.2 Results of Discussion We generate 935 record configurations covering all valid 2582 pairwise inter-actions of features in Figure 4. In the worst case, a naive non-optimal set of con-figurations will have the size 2582 with one interaction covered per configuration. These record configurations are transformed to SQL queries to populate a test database. The set of SQL queries are available in https://sites.google.com/ a/simula.no/dbtwise/ . In this paper, we focus on understanding the scalability of our approach for the industrial case study.

An important step in our methodology verifies if an interaction between database field values is indeed consistent with respect to its feature model. Ev-ery interaction is a tuple of field values that must be present in a configuration. However, not all tuples are valid with respect to the feature model in Figure 4. For instance, a tuple cannot contain two different categories for a declaration since all categories are mutually exclusive (XOR). Therefore, these forbidden tuples must be weeded out. Each tuple is transformed to an Alloy predicate and concurrently solved with the Alloy model the complete feature model. If the predicate conflicts with the Alloy model and its facts then its discarded. Checking tuple validity has a time complexity of O (1) for a feature model of finite size. This is experimentally validated in Figure 5. We measure the time spent in the block of code for checking tuple validity. We observe that the aver-age time in this block is about 12 ms when measured every 10 seconds. We use the non-intrusive perf4J library to perform the measurements.

Another important step is the generation of a set of configurations satisfying all valid tuples of feature interactions. We use the divide-and-combine approach to concurrently generate configurations satisfying all interaction tuples divided in exclusive subsets. Our methodology creates 187 concurrent sets of configurations satisfying the 2582 interaction tuples. The total number of configurations in the 187 sets is 935. We measure the number of calls to the constraint solver as shown in Figure 6. Creating the 935 configuration required about 6804 calls to Alloy  X  X  SAT solver. However, the average time spent in the solver was within about 450 ms due to the finite scope of the solver. The large number of calls to a constraint solver is a trade-off to achieve high scalability. Theoretically, we can imagine just one call to a constraint solver attempting to generate a set of configurations satisfying all 2582 interactions. However, in practice this is intractable and our divide-and-combine strategy address this exact issue. 4.3 Threats to Validity Our experiments do not consider a certain number of factors that could affect our generalized outcomes on scalability. The scalability of our methodology may most likely be affected by the increase in the number and complexity of constraints in the feature model. However, we suggest surgically creating test models covering interactions between only relevant features for a tester. This will most often keep the satisfaction pro blem tractable. Another, factor we did not consider is presence of numerical constraints between database fields that are generated randomly. For instance, the value of computed tax for an item is a function of the statistical value of an item. In this paper, these database fields were associated to random values without conforming to numerical constraints since in the general intention for testing was the interaction and not he numerical correctness of fields storing real numbered values. An extension to our tool Fa k t u m will be to ensure that numerical constraints are satisfied between real-valued fields. In this paper, we present an approach to generate databases from test config-urations covering T-wise interactions between field values. Hence, we position our contribution with respect to work done in two areas: (a) generation of test configurations covering T-wise interactions in a feature model (b) generation of synthetic data for databases.

Researchers have proposed several approaches for generating test configura-tions satisfying T-wise and notably the 2-wise/pairwise coverage criteria. The pioneering work in this area is the AETG approach [8] and its implementation to address highly-configurable software systems [9]. Based on a greedy algorithm, AETG generates N-wise covering arrays for a set of parameters and converts these arrays into a set of test cases. Ho wever, it cannot deal with constraints among the parameters, thus limiting its adoption to feature models with con-straints. Oster et. al. [10] uses a greedy and ad-hoc algorithm based on the maximum number of valid pairs within each configuration while satisfying con-straints. Recently, Johans en [11] introduced the SPLCATool (Software Product Line Covering Array Tool) to generate test configurations from feature models. Similar to the above mentioned approaches, the tool uses greedy algorithm to enforce all pairs in a set of configurations. The tool is quite efficient and has been used on large feature models. The tool PACOGEN developed by Hervieu et. al. [12] goes a step further and generates a minimal set of configurations that covers all pairwise using a time-aware constraint solving procedure. However, for the problem of generating database records covering all T-wise interactions it was necessary to populate a test database with the generated configurations. There-fore, we used our previously developed approach described in Perrouin et.al. [1] to generate a set of configurations covering T-wise and satisfying feature model constraints. The approach in [1] is scalable as it based on a divide-and-combine strategy. A popular commercial tool CTE-XL can used to generate pairwise and 3-wise interactions between fields. Howe ver, it does not generate complete con-figurations containing interactions and satisfying dependencies between fields. This step to solve constraint satisfaction problem for complete configurations is required to completely populate a database.
 Important work to generate synthetic databases include [13], [14], [15], [16]. Database generation tools such as in [14], [17], [13] allow users to specify the data distributions over attributes and intra-attribute correlations. Houkjaer et al. [14] use a graph model containing primary-foreign keys. This model is used to guide the data generation process. In all these wo rks, the question of g enerating general T-wise interactions-covering data is not addressed and considering constraints among the fields is irrelevant. In that respect, the methodology proposed in this paper innovates also w.r.t. to database generation tools. In this paper, we address the problem of t esting TVINN, a data-intensive system in the Norwegian Customs and Excise department. TVINN processes all custom declaration coming in and out of Norway. We introduce a methodology imple-mented in a tool called Fa k t u m , to populate a test database with configurations that cover all T-wise interactions between selected custom declaration database fields. In an experiment, we develop a variability model (feature model) and a database schema both developed in collaboration with the Norwegian Customs and Excise department. We use our tool to automatically generate 935 config-urations covering all valid 2582 2-wise/pairwise interactions of features in less than 48 minutes. The scalability of our approach and the possibility to generate test data covering all T-wise interactions has given way to food for thought in our academia-industry partnership. For instance, we realize that random ele-ments, in the test database, have a low degree of comprehensibility for testers at TVINN. Hence, we propose the use of Fa k t u m for partial population of test databases. The remaining elements ar e manually completed by testers at Nor-wegian Customs and Excise. The partial test data gives testers the confidence of covering all valid T-wise interactions. Our future work with our partner will in-volve a large-scale empirical evaluation of test generation while carefully taking into account the clauses of n on-disclosure agreements.
 Acknowledgement. We thank the Norwegian Customs and Excise department for their trustful interactions with us. In particular, we would like to thank Atle Sander, Astrid Grime and Katrine Langset for their valuable inputs. We thank the Research Council of Norway for their generous support ; it would not have been impossible to setup such a close industry-academia collaboration for high
