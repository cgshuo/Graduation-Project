 A fundamental aspect of rating-based recommender systems is the observation process, the process by which users choose the items they rate. Nearly all research on collaborative filtering and recommender systems is founded on the as-sumption that missing ratings are missing at random. The statistical theory of missing data shows that incorrect as-sumptions about missing data can lead to biased parameter estimation and prediction. In a recent study, we demon-strated strong evidence for violations of the missing at ran-dom condition in a real recommender system. In this paper we present the first study of the effect of non-random miss-ing data on collaborative ranking, and extend our previous results regarding the impact of non-random missing data on collaborative prediction.
 H.3.3 [ Information Search and Retrieval ]: Information filtering; G.3 [ Probability and Statistics ]: Multivariate statistics; I.2.6 [ Learning ]: Parameter learning Algorithms, Performance recommender systems, collaborative filtering, ranking, prob-abilistic models, non-random missing data
Collaborative filtering and recommender systems are prin-cipally concerned with two related problems: rating predic-tion and ranking. The goal of the rating prediction task is to accurately predict the rating a user would assign to an individual item. The goal of the ranking task is to provide the user with a personalized list of top ranked items. Col-laborative filtering methods attempt to solve both problems by leveraging rating data collected from a large community of users.

Research on recommender systems has focused almost ex-clusively on properties of rating prediction and ranking meth-ods including prediction accuracy [1], ranking accuracy [15], novelty and diversity [17, 16], and explainability [5]. In this paper, we focus on properties of the underlying rating obser-vation process. We define the rating observation process as the process through which users select the items they choose to rate. In particular, we assess the impact of non-random observation processes on rating prediction and ranking.
Most prior research on collaborative filtering and recom-mender systems is founded on the assumption that missing ratings are missing at random. However, the statistical the-ory of missing data developed by Little and Rubin [6] shows that incorrect assumptions about missing data can lead to biased parameter estimation and prediction in a wide range of models and methods including clustering [1], matrix fac-torization [2] and other probabilistic models [11].
The key concept in Little and Rubin X  X  theory is the miss-ing at random condition . This condition is quite intuitive in the collaborative filtering setting. It essentially states that the probability that a rating is missing does not depend on the value of that rating, or the value of any other missing rating. The condition is easily violated in recommender sys-tems if, for example, users are more likely to supply ratings for items that they do like, and less likely to supply rat-ings for items that they do not like. We recently reported substantial evidence for violations of the missing at random condition in recommender systems based on an online study conducted at Yahoo! Research involving over 35 , 000 partic-ipants [8].

The main contribution of this paper is the first empiri-cal analysis of ranking in the presence of non-random miss-ing data. We also extend our previous investigation into the effect of non-random missing data on rating prediction by testing two additional baseline methods: nearest neigh-bour regression and matrix factorization. Our results show that two methods that incorporate a non-random missing data model, MM/CPT-v and MM/Logit-vd, outperform the baseline methods when evaluating rating prediction and rank-ing on items selected at random . We believe this is a more accurate measure of performance than testing on items se-lected by the user since it better reflects an important goal of recommender systems: to make predictions and recommend items that the user has not yet seen or rated .
A collaborative filtering data set can be represented as a rectangular matrix x where each row in the matrix repre-sents a user, and each column in the matrix represents an item. x nd denotes the rating of user n for item d .Let N be the number of users in the data set, D be the number of items, and V be the number of rating values. To reason about the observation process, we require a representation for missing and observed rating values. We introduce a com-panion matrix of response indicators r where r nd =1if x nd is observed, and r nd =0if x nd is not observed. Hierarchical collaborative filtering models such as clustering often con-tain latent values that are never observed. We denote latent values associated with data case n by z n .Wedenotethe corresponding random variables with capital letters.
Following Little and Rubin, we introduce a parametric joint probability distribution on the data x , response indi-cators r , and latent values z [6]. We adopt the factorization of the joint distribution of the data random variables X ,re-sponse indicator random variables R , and latent variables Z shown in Equation 1.  X  and  X  are the parameters of the distribution.

Little and Rubin refer to P ( R | X , Z , X  ) as the missing data model and P ( X , Z |  X  ) as the data model. P ( R | X , Z , X  )is what we have been referring to as the observation process. The intuition behind this factorization, under the additional assumption that data cases are independently and identi-cally distributed, is that all of a user X  X  ratings are first gen-erated according to the data model P ( X n , Z n |  X  ), and the missing data model P ( R n | X n , Z n , X  ) is then used to decide which ratings will be observed and which will be missing.
Little and Rubin classify missing data into several types including missing completely at random (MCAR), missing at random (MAR), and not missing at random (NMAR) [6, p. 14]. The MCAR condition is defined in Equation 2, and the MAR condition is defined in Equation 3. Under MCAR the response probability for an item or set of items cannot depend on the data values in any way. Under the MAR con-dition, the data vector is divided into a missing part x mis and an observed part x obs according to the value of r in ques-tion: x =[ x mis , x obs ]. The intuition is that the probability of observing a particular response pattern can only depend on the elements of the data vector that are observed under that pattern. Both MCAR and MAR require the additional technical condition that the parameters  X  and  X  be distinct, and that they have independent prior distributions.
Missing data is NMAR when the MAR condition fails to hold. The simplest reason for MAR to fail is that the proba-bility of not observing a particular element of the data vector depends on the value of that element. In the collaborative filtering case this corresponds to the idea that the probabil-ity of observing the rating for a particular item depends on the user X  X  rating for that item, which is quite natural.
When missing data is missing at random, maximum likeli-hood inference based only on the observed data x obs will be unbiased. We demonstrate this result in Equation 7. The key property of the MAR condition is that the response probabilities are independent of the missing data, allowing the complete data likelihood to be marginalized indepen-dently of the missing data model. However, when missing data is not missing at random, this important property fails to hold, and it is not possible to simplify the likelihood be-yond Equation 4 [6, p. 219]. Ignoring the missing data mechanism will clearly lead to biased parameter estimates and biased predictions since an incorrect likelihood function is being used. For non-identifiable models such as mixtures, we will use the terms  X  X iased X  and  X  X nbiased X  in a more gen-eral sense to indicate whether the parameters are estimated under the correct likelihood function.
 While this analysis of the impact of non-random missing data is based on maximum likelihood estimation, it imme-diately extends to the case of Bayesian inference. It also extends to learning in models like regularized matrix factor-ization that can be cast as probabilistic models [10]. In the case of non-parametric methods such as nearest neighbour regression, it may be possible to correctly identify relevant neighbours for a user or item in the presence of non-random missing data using common similarity measures like Pearson correlation [4]. However, it is clear that if missing data is not missing at random, the resulting rating predictions will be biased. Consider, for example, a case where low-valued ratings are more likely to be missing, this would create an over-abundance of high rating values in the observed data and the resulting rating predictions would be biased upward.
Unfortunately, the theory does not easily extend to the analysis of methods for learning to rank [14], or to the analy-sis of the ranking task itself. What we do know is that learn-ing rating prediction models will be subject to bias in the presence of non-random missing data. Using these models for ranking in a standard score-and-sort framework is likely to pass some of this bias along to the inferred rankings, re-sulting in a degradation of ranking performance. The main question we are interested in is can we obtain better rating prediction and ranking performance using simple models of the non-random missing data process?
The framework we consider for learning and prediction with non-random missing data follows the basic outline sug-gested by Little and Rubin [6]: We combine a probabilistic model for complete data, in this case a multinomial mixture (a) Mixture Model v model, and the multinomial mixture/Logit-vd model. clustering model, with a probabilistic model of the missing data process. The missing data models we consider capture some properties of a non-random missing data process, but are necessarily simplistic since our aim is to simultaneously estimate the parameters of both the complete data model and the missing data model. We begin by describing three models based on multinomial mixture clustering that incor-porate different missing data assumptions. We also briefly describe the baseline matrix factorization and nearest neigh-bour methods.
The finite multinomial mixture model pictured in Figure 1(a) is a basic clustering model for discrete data. The ran-dom variables Z n are cluster or mixture component indicator variables. They indicate which mixture component is asso-ciated with each data case and take values from the discrete set { 1 , ..., K } . The random variables Z n are not observed and are referred to as latent variables. The mixing propor-tions  X  k give the prior probability of observing a data case from each of the K clusters. The parameters of the mixture component distributions are denoted by  X  k .Thecompo-nent distributions are a product of independent multinomi-als where  X  vdk = P ( x nd = v | Z n = k ). We denote the prior distribution on the mixture component distributions  X  k with hyper-parameters  X  by P (  X  k |  X  ). We denote the prior distri-bution on the mixing proportions  X  with hyper-parameters  X  by P (  X  |  X  ). We use independent Dirichlet distributions for both P (  X  k |  X  )and P (  X  |  X  ). We give the probabilistic model for the multinomial mixture model in Equations 8 to 11. The square bracket notation [ s ] represents an indicator function that takes the value 1 if the statement s is true, and 0 if the statement s is false.
 Clustering models have a very natural interpretation in the collaborative filtering domain: the latent variable z n indi-cates the group or cluster that user n belongs to, and the parameters  X  k specify the preferences of a prototypical user that belongs to group k .

The default when dealing with missing data in a mixture model is to invoke the missing at random assumption. Under the missing at random assumption, the missing data model can be ignored, and inference, learning, and prediction can be based on the observed data only. In the multinomial mixture model, missing data can be analytically summed out of the observed data posterior.
The multinomial mixture model is a natural baseline model for collaborative filtering. It does not include an explicit missing data model, and hence relies on the MAR assump-tion to deal with missing data. We now review an extension of the multinomial mixture model that includes an explicit non-random missing data mechanism that we call CPT-v [8]. CPT-v is a simple missing data model where the probability that a rating is observed depends only on that underlying rating value. This model can capture the idea that a user X  X  preferences for a particular item can influence whether the user rates that item, but the effect is the same for all items.
The CPT-v missing data model is parameterized using a conditional probability table consisting of V Bernoulli pa-rameters  X  v (hence the name CPT-v). The parameters  X  v give the probability that an item will be rated if its true rating value is v : P ( r nd =1 | x nd = v )=  X  v .ABayesian network representation of the combined finite multinomial mixture model/CPT-v model is given in Figure 1(b). The response indicators are assumed to be independently sam-pled for each item, leading to the missing model given in Equation 12. The prior distribution on the  X  v parameters is a Beta distribution as seen in Equation 13.
 P ( r n | x n , X  )= The specification of the multinomial mixture complete data model is given by Equations 8-11.
The CPT-v missing data model is restrictive in that it asserts the same conditional missing data rates for all items. We now present a more flexible missing data model that we call Logit-vd . The Logit-vd model allows the probability that a rating is missing to depend on both the value of the underlying rating and the identity of the item. The Logit-vd model specifies a logistic form for this relationship (hence the name Logit-vd) as seen in Equation 14.

P ( r nd =1 | x nd = v )=  X  vd = 1 The  X  v factor models a non-random missing data effect that depends on the underlying rating value. This effect is con-strained to be the same across all items. The  X  d factor mod-els a per-item missing data effect. This effect can be useful if all items do not have the same exposure in a recommender system. This situation can arise, for example, when some items are more heavily promoted than others. A Bayesian network representation of the combined finite multinomial mixture model/Logit-vd model is given in Figure 1(c).
In the Logit-vd model, the response indicators are as-sumed to be independently sampled for each item, yielding the missing data model given in Equation 15. The model pa-rameters  X  v and  X  d are given Gaussian prior distributions as seen in Equation 16. The specification of the underlying multinomial mixture model for complete data is again given by Equations 8 to 11.
 P ( r n | x n , X  )=
We present a generalized Expectation Maximization (EM) algorithm to simultaneously estimate the parameters of the Logit-vd missing data model and the multinomial mixture complete data model [3]. To help simplify the notation, we introduce the auxiliary variables  X  dkn in Equation 17. The EM updates for the combined multinomial mixture/Logit-vd model are given in Algorithm 1. The  X  v and  X  d parame-ters are updated using a gradient step with the step length  X  set using a line search on each iteration to ensure conver-gence.

The EM algorithm for the multinomial mixture/CPT-v model parameters follows directly from the Logit-vd case if we set  X  vd =  X  v for all d in Equation 17, and replace the M-Step updates for  X  v and  X  d given in Algorithm 1 with the closed-form M-Step update given below.
 Algorithm 1 Generalized EM for MM/Logit-vd
E-Step: q n ( k )  X  q n ( k, v, d )  X  q n ( k ) q n ( v, d )  X 
M-Step:  X  We note that learning the parameters of the multinomial mixture model under the missing at random assumption is accomplished using the standard EM algorithm for discrete mixtures. Further details for all models can be found in [7].
CPT-v and Logit-vd employ simple conditional Bernoulli selection models for the response variables. The advan-tage of this form of missing data model is computational tractability. Pre-computing and caching intermediate fac-tors gives highly efficient EM algorithms for both CPT-v and Logit-vd with approximately the same computational cost per iteration as learning the multinomial mixture un-der the missing at random assumption. The computational complexity in both cases depends on the number of observa-tions, not the data matrix size. Both models of course ignore important information that might influence whether or not particular items will be observed, and do not incorporate feature-based information about users and items that could be very helpful in overcoming the effects of non-random missing data. Neither type of information is available for the data set we consider, but an advantage of a probabilistic approach is that the basic models can easily be extended to deal with additional features and side information.
We consider a probabilistic matrix factorization model with global mean offset as seen in Equations 21 to 23 [10]. U n denotes a length K user factor vector while Y d denotes alength K item factor vector.  X  g is the global average rat-ing. The item parameter vector Y d can be thought of as (a) Yahoo! Random and Netflix. representing the strength of a set of features describing each item. The user parameter vector U d can be thought of as representing the user X  X  affinity for items described by each feature. Training for the matrix factorization model is ac-complished by numerical optimization of the log likelihood function. Without loss of generality we can assume  X  2 =1, at which point learning the model is equivalent to standard regularized matrix factorization with penalty  X  =1 / X  2 0
The final method we consider is item-based nearest neigh-bour regression. We use an adjusted cosine similarity met-ric as shown below [12], combined with the standard nearest neighbour regression prediction rule. We found that restrict-ing the prediction rule to positive similarities only resulted in better prediction performance as noted previously by Tak  X  acs et al. [13].  X  x n denotes the average of user n  X  X  observed rat-ings while  X  x nd denote the prediction for user n and item d .

Our empirical analysis is based on the Yahoo! Music rat-ings for User-Selected and Randomly Selected Songs, version 1.0 data set, which is available through the Yahoo! Web-scope data sharing program. 1 This data set is essentially identical to the data set used in our previous study [8]. It presents a unique opportunity to test collaborative filtering methods that incorporate missing data models. It contains ratings for items selected at random, in addition to ratings for items selected by the user. It permits the evaluation of rating prediction and ranking methods using a novel pro-
Contact academicrelations@yahoo-inc.com or visit http://research.yahoo.com/Academic Relations for details on obtaining Yahoo! Webscope data sets. tocol that considers ratings for randomly selected items as described in Section 5.

The data set consists of ratings collected during normal user interaction with Yahoo X  X  LaunchCast internet radio ser-vice, as well as ratings for items collected using an online survey. We will refer to the set of ratings collected through normal interaction with the recommender system as ratings for user-selected items and denote this set by X u . We will refer to the set of ratings collected through the survey as ratings for randomly selected items and denote this set by X
The data set contains 15 , 400 users, all with at least 10 ratings for user-selected items in X u .5 , 400 of these users also have exactly 10 ratings for randomly selected items in the set X r . The data set is based on 1 , 000 songs selected at random from the LaunchCast catalog. There are a total of approximately 250 , 000 ratings for user-selected items in X u and exactly 54 , 000 ratings for randomly selected items in X s .

Figure 2 shows the marginal distribution of ratings for randomly selected items in X r compared to the distribution of ratings for user-selected items in X u ,andseveralother popular collaborative filtering data sets. The two sets of ratings X u and X r in the Yahoo! data set exhibit markedly different marginal statistics. The most pronounced feature of the ratings for randomly selected items in the Yahoo! data set is that they contain many fewer high ratings compared to the ratings for user-selected items. This points strongly to a possible violation of the missing at random condition in the Yahoo! ratings for user-selected items. Marlin et al. present further properties of the data set and additional arguments supporting a possible violation of the missing at random assumption in this data set [8].
The experimental protocol we employ is significantly more involved than typical collaborative filtering evaluation pro-cedures based on historical rating data as we deal with two rating sources (user-selected and randomly selected items). In this section we describe in detail the data set manipu-lation needed to enable this experimental protocol, but the basic idea is very simple: we train models on ratings for user-selected items and test both on held-out ratings for user-selected items and held-out ratings for randomly se-lected items. We argue that testing on randomly selected items is a more accurate measure of performance than test-ing on items selected by the user since it better reflects an important goal of recommender systems: to make predic-tions and recommend items that the user has not yet seen or rated.
We begin by filtering the data so that each user has at least 11 ratings in the set X u instead of the original 10. We choose 10 user-selected items from X u to form a set of held-out user-selected test items X hu . The remaining ratings in X u form the set of user-selected observed ratings X ou .The additional filtering insures at least one rating for each user in X ou . All of the ratings for randomly selected items X are held-out for testing.

To enable a cross-validation assessment of weak/strong generalization error, we split the users with ratings for ran-domly selected items (the users who participated in the on-line survey and data collection experiment) into five equal sized groups. For each cross-validation fold, we select one of the five groups of users to form a set of test users. All of the remaining users form the set of training users. We obtain a total of six sets of ratings for each cross-validation fold: X
We train models on the observed ratings for user-selected items contained in the training user set X ou tr . Conditioning on each training user X  X  observed ratings in X ou tr ,weevalu-ate weak generalization performance on held-out ratings for user-selected items contained in the training user set X hu We separately evaluate weak generalization performance on ratings for randomly selected items contained in the train-ing user set X r tr . Next, conditioning on each test user X  X  observed ratings in X ou te , we evaluate strong generalization performance on held-out ratings for user-selected items con-tained in the test user set X hu te . We separately evaluate strong generalization performance on ratings for randomly selected items contained in the test user set X r te .
In the experiments that follow, we train each mixture model using 1 , 5 , 10 and 20 mixture components. The Logit-vd Normal prior parameters  X  2 and  X  2 weresetto10topro-vide a broad prior around zero. The mixture model hyper-parameters  X  and  X  and the CPT-v prior parameters  X  were all fixed to 2 to provide minimal smoothing. No attempt was made to update the hyper-parameters in this work. For the matrix factorization model, we trained for 10 , 000 iter-ations using limited memory BFGS [9, p. 224] or until the change in the objective function was less than 10  X  5 .We considered ranks K =1 , 5 , 10 , 20, and regularization param-eters 0 . 1 , 1 , 5 , 10. Each of the mixture models was trained for 10 , 000 EM iterations or until the change in the average log posterior probability was less than 10  X  8 .
We evaluate rating prediction performance in terms of normalized mean absolute error (NMAE), computed as seen below, assuming there are T test items per user with indices i (1 ,n )to i ( T,n ). The normalizing constant (equal to 1.6 here) is the expected MAE assuming uniformly distributed predictions and true ratings. When evaluating rating pre-diction performance, we set the predicted rating for item d and user n to the median of the posterior predictive distri-bution for that user/item combination since this minimizes the MAE in expectation. We truncate predictions to [1 , 5] when predicting ratings if the prediction method does not guarantee this property automatically.
 We evaluate ranking performance using a standard metric in information retrieval ranking applications, the normal-ized discounted cumulative gain (NDCG@L). The NDCG@L score is computed as seen above where  X  ( l, n ) is the index of the item with rank l when test items are sorted in descend-ing order by true rating x nd , X   X  ( l, n ) is the index of the item with rank l when items are sorted in descending order ac-cording to their predicted ratings  X  x nd . When sorting by true and predicted ratings, ties can be broken arbitrarily without affecting the NDCG@L score. L denotes the length of the ranked list of items we return to the user. When evaluating ranking performance, we set the predicted rating for item d and user n to the mean of the posterior predictive dis-tribution for that user/item combination. Note that lower NAME indicates better prediction performance while higher NDCG indicates better ranking performance.
Figures 3(a) and 3(b) present the weak generalization rating prediction performance on randomly selected items and user-selected items for item-based nearest neighbour regression (iKNN), the matrix factorization model (MF), the multinomial mixture model (MM), the multinomial mix-ture/ CPT-v model combination (MM/CPT-v), and the multi-nomial mixture/Logit-vd model combination (MM/Logit-vd) as a function of the number of latent dimensions K . For MF the best values with respect to the regularization parameter  X  are shown for each K . Note that we use all neighbours in the iKN N method, so its performance is con-stant across K .

Figures 3(c) and 3(d) show the strong generalization per-formance on randomly selected items and user-selected items for each of the five models. Strong generalization results are reported for the complexity K giving the best weak gen-eralization performance. The optimal model complexities were chosen independently for user-selected and randomly selected items. Note that the standard errors are repre-sented on the plots using error bars.

The results on randomly selected test items clearly show that the MM/Logit-vd model achieves slightly better rat-ing prediction performance than MM/CPT-v, while both achieve significantly better performance than the iKNN, MM and MF models that operate under the missing at random assumption. On user-selected items, the basic MM model significantly out-performs both MM/CPT-v and MM/Logit-vd. The best results we obtained with the MF and iKNN models are worse than for MM. This is likely due to the fact that the data is highly sparse and the MM model has the fewest parameters to learn (4 , 000 K for MM versus 16 , 000 K for MF). More advanced forms of regularization may yield better performance for MF on user-selected items. The lim-ited effect with respect to K for all methods is again likely also due to the relatively small data set size. (c) Strong Rand NMAE
We note again that computing the prediction error on ran-domly selected test items is more relevant than prediction error on user-selected test items when the models are sub-sequently used in a score-and-sort ranking framework. Es-timating prediction error on user-selected items only gives an estimate of prediction performance on items that were previously selected by the user. This is clearly an unreliable estimate of prediction performance over the whole rating matrix in the case of the Yahoo! data set, or the results on the two test sets would be equal.
The collaborative ranking task is to produce an ordered top-L list of highest rated items for each user. The NDCG@L score provides a measure of quality for top-L lists. Actu-ally evaluating NDCG@L in the collaborative ranking case is complicated by the fact that we do not have access to the true ratings for all of the items each user has not rated. The standard procedure for estimating ranking performance is to use held-out lists of user-selected items. A unique feature of the Yahoo! data set is that we have access to an addi-tional set of randomly selected test items. Due to the small overlap between randomly selected items and user-selected items when the Yahoo! data set was collected, the randomly selected items are a good approximation to a random sample of items not rated by each user.

We perform ranking experiments on the Yahoo! data set based on held-out lists of 10 randomly selected items and 10 user-selected items. Weak generalization NDCG@L es-timates were computed for models with 1, 5, 10, and 20 latent dimensions. The optimal model complexity was de-termined independently for each value of the list length L based on weak generalization performance, and the corre-sponding strong generalization NDCG@L value is displayed. Figure 4(a) shows the strong generalization NDCG@L per-formance for each model estimated on lists of 10 randomly selected items, while Figure 4(b) shows the same compari-son based on lists of 10 user-selected items. The results show that on the user-selected items, the MM model performs as well as MM/Logit-vd and MM/CPT-v, and all three out-perform MF and iKNN. On lists of randomly selected items, MM/Logit-vd and MM/CPT-v both perform significantly better than the basic MM and MF models.
We have presented new empirical results comparing the ranking and rating prediction performance of methods that assume the MAR condition and methods that include a model of the missing data mechanism. Results show that methods that include a non-random missing data model out-perform methods that assume the MAR condition on both the prediction and ranking tasks when the evaluation is based on randomly selected test items. We have argued that the use of randomly selected test items more accurately re-flects the tasks of interest: prediction and ranking for items not previously rated by the user. A very interesting direction for future research is to consider combining methods that optimize ranking performance, as in the work of [14], while simultaneously accounting for the presence of non-random missing data.
We would like to thank Sam Roweis and Malcolm Slaney for their important contributions to earlier stages of this research. This research was supported by Natural Sciences and Engineering Research Council of Canada and the Killam Trusts. [1] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [2] D. Decoste. Collaborative prediction using ensembles [3] A. Dempster, N. Laird, and D. Rubin. Maximum [4] J. L. Herlocker, J. A. Konstan, A. Borchers, and vd on the Yahoo! data set. [5] J. L. Herlocker, J. A. Konstan, and J. Riedl. [6] R. J. A. Little and D. B. Rubin. Statistical Analysis [7] B. Marlin. Missing Data Problems in Machine [8] B.Marlin,R.Zemel,S.Roweis,andM.Slaney.
 [9] J.NocedalandS.J.Wright. Numerical Optimization . [10] R. Salakhutdinov and A. Mnih. Probabilistic matrix [11] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted [12] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. [13] G. Tak  X  acs, I. Pil  X  aszy, B. N  X  emeth, and D. Tikk. Matrix [14] M. Weimer, A. Karatzoglou, Q. Le, and A. Smola. [15] M. Weimer, A. Karatzoglou, and A. Smola. Adaptive [16] M. Zhang and N. Hurley. Avoiding monotony: [17] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and
