 In this paper, we recast static index pruning as a model induction problem under the framework of Kullback X  X  prin-ciple of minimum cross-entropy. We show that static index pruning has an approximate analytical solution in the form of convex integer program. Further analysis on computa-tion feasibility suggests that one of its surrogate model can be solved efficiently. This result has led to the rediscovery of uniform pruning , a simple yet powerful pruning method proposed in 2001 and later easily ignored by many of us. To empirically verify this result, we conducted experiments un-der a new design in which prune ratio is strictly controlled. Our result on standard ad-hoc retrieval benchmarks has con-firmed that uniform pruning is robust to high prune ratio and its performance is currently state of the art. H.1.1 [ Systems and Information Theory ]: Information theory; H.3.1 [ Content Analysis and Indexing ]: Index-ing methods; H.3.4 [ Systems and Software ]: Performance evaluation (efficiency and effectiveness) Static index pruning; principle of minimum cross-entropy; model induction; uniform pruning
Kullback discussed one famous problem in his seminal work [14] about inducing a probability measure based on some previous measurement. When one has some initial hy-pothesis about a system and seeks to update this measure-ment incrementally, she needs to choose a new hypothesis from a set of feasible measures that best approximates her current belief. Here, the difficulty lies in defining the notion of closeness in the probability space. While at the time this was an important issue in everyday probabilistic modeling, a genuine solution had yet to come.

To answer the question he had raised, Kullback introduced a method called  X  X inimum discrimination information, X  or in the recent literature known as the principle of minimum cross-entropy . This approach has later become one of the most influential inductive principles in statistics, and also has benefited numerous fields, including some subareas in information retrieval [15,20]. Kullback X  X  solution was simple and elegant: One shall choose a measure that most closely resembles the previous measurement in terms of Kullback-Leibler divergence. Specifically, this is equivalent to solving the following optimization problem, given some prior mea-sure p and a set of feasible measures F :
In this paper, we apply this induction framework to a classic problem in information retrieval, called static index pruning . Static index pruning is a task that reduces the in-dex size for improving disk usage and query throughput [1]. Size reduction is done by removing index entries. Generally, the aim in static index pruning is to find a subset of index entries that best approximates the full index in terms of re-trieval performance. This aspect, as we will show later, is closely related to model induction.

One key assumption in this paper is that an inverted index is a nonparametric, conditional distribution of document D given term T , i.e., p ( D | T ). This follows directly from Chen et al. X  X  definition [10], which allows us to measure the resem-blance between two versions of inverted indexes the way we do probability distributions. Here, the following definitions put static index pruning into the framework of Equation (1):
This conception marks the very beginning of our quest for developing an efficient solution of static index pruning. Through analysis, we first show that static index pruning is essentially a combinatorial optimization problem. Never-theless, in Section 3, we manage to obtain a weaker analyt-ical solution that is practically operable by trading off some mathematical rigor. We found that, under appropriate as-sumptions, static index pruning reduces to a convex integer program. But this is not a good solution in general, since the number of variables in the convex program is linear to the number of postings in the inverted index, which may easily e xceed a few millions on any medium-sized text collection. That means this solution does not scale at all, even with the latest super-efficient convex solver.

We further attacked this problem using an alternative ap-proach, called surrogate modeling. We created a surrogate problem that is easier to solve. As we will show in later sections, this analytical solution has pointed us to a general version of a simple pruning method called uniform prun-ing . Sharp-eyed readers might notice that uniform pruning is by no means a new invention. Uniform pruning was orig-inally introduced to static index pruning in Carmel et al. X  X  paper as a baseline approach [9]. In a preliminary experi-ment, Carmel et al. compared this method with their term-based pruning method. Using TF-IDF as the score func-tion, they found that, even though term-based method per-formed slightly better, in general the performance for both approaches was roughly comparable. While this was indeed a very interesting finding, the exploration was discontinued as they went ahead to study other important issues.
To the best of our knowledge, since then uniform pruning has not been studied in any follow-up work. It is easy to see why this has been the case. The lack of control on one experiment variable, prune ratio, has made the performance result difficult to interpret. When we make comparisons be-tween methods, this variable needs to be strictly controlled so that the comparisons make sense. Nevertheless, very few in the previous work adopted this design. As a result, there was no obvious way to conduct any form of significance test-ing to static index pruning. Without serious scrutiny X  X y which we mean significance assessment X  X t is only reason-able to dismiss uniform pruning, for that it seemed like an ad-hoc and maybe inferior approach.

In our study, the rediscovery of uniform pruning has gained us a second chance to rethink this issue. Our answer was a redesigned empirical study, in which prune ratio for each experimental method is strictly controlled to minimize the experimental error, and the performance is analyzed using multi-way repeated-measure analysis of variance. As we will shortly cover, the experiment result suggests that uniform pruning with Dirichlet smoothing significantly outperformed the other term-based methods under diverse settings.
The rest of the paper is structured as follows. Section 2 covers an overview to static index pruning and the relevant research. In Section 3, we motivate static index pruning in the minimum cross-entropy framework and show that the analytical solution leads to the uniform pruning method. An empirical study is given in Section 4. We put the theoretical and empirical evidence together and discuss the implication in Section 5. Section 6 delivers the concluding remarks.
In the coming subsections, we briefly review the literature and discuss the recent development of static index pruning. Following an overview, some notable pruning methods will be treated in slightly more details. Note that this is only aimed at providing enough background knowledge for the reader. A complete coverage is not attempted here.
The idea of static index pruning first appeared in the groundbreaking work of Carmel et al. [9] and has since gar-nered much attention for its implication to Web-scale re-trieval [8,11]. Static index pruning is all about reducing in-dex size X  X y removing index entries from the inverted index. This technique was proposed to mitigate the efficiency issue caused by operating a large index, for that a smaller index loads faster, occupies less disk space, and has better query throughput. But since only partial term-document mapping is preserved, a loss in retrieval performance is inevitable.
Much effort has been driven towards developing impor-tance measures of individual index entries, so that one can easily prioritize index entries on their way out of the index. Many such measures have been proposed and tested in var-ious retrieval settings. One simple example is impact , the contribution of a term-document pair to the final retrieval score [8, 9]. Other approaches in this line include probabil-ity ranking principle (PRP) [7], two-sample two proportion (2P2N) [19], and information preservation (IP) [10]. Some measures assess only term importance [6], so the correspond-ing pruning algorithms can only choose between keeping the entire term posting list or not at all. Some others assess only documents importance [21].
Term-based pruning (or term-centric pruning ) is proposed by Carmel et al. [9]. It was so named because it attempts to reduce the posting list for each term in the index. The basic idea is to compute a cutting threshold for each term, and throw away those entries with smaller impact values. Since the cutting threshold depends on some order statistics (i.e., the k -th largest impact value) about the posting list, term-based pruning is less efficient than the other methods.
In contrast to the aforementioned term-centric approach, document-centric pruning seeks to reduce the posting list for each document. B  X  uttcher and Clarke [8] considered the contribution for term t to the Kullback-Leibler divergence D( d || C ) between document d and the collection model C . This quantity is used to measure the importance of a post-ing. Analogously, for each document, a cutting threshold has to be determined based on some order statistics.
There are also other pruning strategies that focus on re-moving an entire term posting list (whole-term) or an en-tire document (whole-document) all at once. Blanco and Barreiro [6] presented four term-importance measures, in-cluding inverse document frequency (idf), residual inverse document frequency (ridf), and two others based on term discriminative value (TDM). They adopted a whole-term pruning strategy. Analogously, Zheng and Cox [21] proposed an entropy-based measure in a whole-document pruning set-ting. Both parties have reported comparable performance to term-based pruning on some standard benchmark.

Blanco and Barreiro [7] developed a decision criterion based on the probability ranking principle [18]. The idea is to take every term in the index as a single-word query and calculate is used in prioritizing all the term-document pair. Since there is only one cutting threshold determined globally, the implementation is relatively easy and efficient.

Thota and Carterette [19] used a statistical procedure, called two-sample two-proportion (2P2N), to determine if the occurrence of term t within document d is significantly different from its occurrence within the whole collection. Chen et al. [10] developed a method called information preser-vation . They suggest using the innermost summand of the conditional entropy H ( D | T ) to measure predictive power contributed by individual term-document pairs to the index m odel. This quantity is claimed easier to compute than the probability ranking principle.

Altingovde et al. [2] proposed an interesting query-view technique that works orthogonally with the aforementioned measure-based approaches. The general idea is to count the number of time a document falls within the top-k window of any given query collected from the query log. The count collected from a query is then evenly distributed to individ-ual query terms. Thus the larger this number, the greater importance the posting is. The query view algorithm would later use this information to prune the entries.

Our work in this paper departs from the previous effort in three major ways. First, our approach is model-based, meaning that we infer a pruned model as a whole rather than partially. This is a novel approach in contrast to all the previous methods. Second, other information-theoretic approaches, such as Zheng and Cox [21] and Chen et al. [10], focused on minimizing the loss of information, while ours focused on minimizing the divergence from the full index. These are entirely different concepts in information theory. Three, our result on the uniform pruning method is more general than Carmel et al. X  X  description because we consid-ered the query model p ( t ). Our take of uniform pruning is a weighted version, which may be useful when such a query model (e.g., session logs) is available.
Let us first develop some notation for describing an in-verted index. Let T denote the set of terms and D denote the set of documents. We define an index entry ( posting ) as a 3-tuple of the form ( t, d, n ), where t  X  T , d  X  D , and n  X  N + (i.e., n is a positive integer.) This means that  X  X erm t appears n times in document d . X  We further consider an inverted index as a probabilistic model p ( D | T ;  X  ) that takes a set of index entries  X  as parameters. This model is there-fore nonparametric because the number of its parameters is not fixed. For brevity, in this paper we sometimes abuse the notation and use one symbol, e.g.,  X  , to represent both a distribution and its parameters.

In static index pruning, one seeks to induce a pruned model  X  from a full model  X  0 such that the following two constraints are satisfied: (i)  X  is a subset of the full model  X  , and (ii) the size of  X  is 1  X   X  times the size of  X  0 . Here, 0 &lt;  X  &lt; 1 denotes the prune ratio . Note that these con-straints only specify what we need as the output from static index pruning, not how pruning shall be done. As there are exponentially many ways to prune an index down to a given ratio, it is natural to ask how does one engineer this decision to avoid excessive performance loss.

Now, to illustrate this point, let us assume the existence of a function g (  X  ) that measures the retrieval performance of model  X  . With this hypothetical construct, we formally define static index pruning as follows.
It is not difficult to envision static index pruning being formulated this way, as a constrained optimization problem. For now, we shall focus on estimation of this hypothetical function. The conventional approach, as discussed in Sec-tion 2.2, is to devise an importance measure to take the role of g (  X  ), which is expected to capture certain properties of an index relevant to retrieval performance. Yet one caveat is that sometimes we risk being arbitrary: The importance measure may only be empirically tested and does not neces-sarily come with any theoretical guarantee.

One simple idea that we had failed to see casted away all these doubts. We noticed the similarity between this formulation and Kullback X  X  famous induction framework. As we replace g (  X  ) in Equation (2) with the negative Kullback-Leibler divergence (KL divergence), the static index pruning problem reduces to a model induction problem, written in a minimization form:
In the following subsections, we shall develop a procedure to practically solve this optimization problem. For brevity, we write p (  X  ) and p 0 (  X  ), respectively, to denote the models parametrized by inverted indexes  X  and  X  0 . The probability measures that we consider here are conditional distributions of D given T . To make this explicit, we define:
Before diving into the full analysis, we need to make ex-plicit two important assumptions.

Assumption 1 (Query and Index Models). We can separate a joint distribution of D and T into a product of two models: (1) a distribution of T , called the query model, and (2) a conditional distribution of D given T , called the index model. We assume there is only one query model q ( t ) and it is independent of the index model in use. In other words, we have: Sometimes, we simply write p ( t ) or p 0 ( t ) to denote the query model when the meaning is clear in the context.

Assumption 1 simply states that the query model q ( t ) (or p ( t )) has to be estimated from somewhere else. It makes little sense to infer a query model from the index.
Assumption 2 (Normalization Factor). Let p ( t | d ) and p 0 ( t | d ) be the conditional distributions of T given D for the induced and the original models, respectively. Let I be a binary variable that indicates whether an index entry ( t, d, n ) (for some n  X  N + ) in the original model is retained in the induced model. We have p ( t | d )  X  I t,d p 0 ( t | d ) /Z where Z d is the normalization factor for document d .
In Assumption 2, we introduce a normalization factor Z d for each document d . As we shall address later, setting an appropriate value for Z d is the key step in the subsequent analysis. To correctly normalize p ( t | d ), we need to set: But this would make the resulting formula intractable, since t he value of I t,d depends on other variables in the same doc-ument, i.e., I  X  ,d . To deal with this issue, we suggest setting Z d = k for all d  X  D , where k &gt; 0 is some constant. Us-ing this normalization trick results in weak inference and inevitably sacrifices mathematical rigors. We want to em-phasize that this is a necessary compromise, without which the following analysis would not have been possible. Now, we shall go ahead and analyze the objective function. First of all, let us write out the objective in full:
We use Assumption 1 to dissect the joint distribution p ( d, t ) into the product of the query model p ( t ) and the index model p ( d | t ). Applying Bayes Theorem to p ( d | t ) and p ( d | t ) and assuming uniform p ( d ) and p 0 ( d ), we have the objective organized as follows:
Observe that, since in this optimization framework we look for a subset of  X  0 , we are essentially dealing with a combinatorial problem ( X  X ssignment problem X ). Each index entry ( t, d, n )  X   X  0 either stays within the induced model  X  or gets removed. This combinatorial nature is best charac-terized via the indicator variables I  X  ,  X  in Assumption 2.
Let us now replace all the occurrences of p ( t | d ). Note that, under the setting Z d = k (suggested), all the normalization factors cancel out. We have:
X
As we separate the support of the inner summation over d into two subsets according to whether I t,d is switched on, i.e., one over { d | I t,d = 1 } and the other over { d | I the latter sub-summation disappears since 0 log 0 = 0. The resulting equation becomes:
X
Notice that the innermost logarithm does not depend on d anymore. We can therefore move that entire term out of the inner summation. From there, we have the inner summation over d canceled out. The equation is now written as:
We can get rid of the numerator, i.e., P d  X  p 0 ( t | d  X  ), in the logarithm when minimizing this equation, because the nu-merator does not depend any combinatorial choice we make. Once again, we rewrite it as a maximization problem by tak-ing the negation. The final form of static index pruning is expressed as the following:
Input : a global threshold  X  begin end Algorithm 1: The weighted uniform pruning algorithm.
Equation (10) is in general ill-posed even though it can be solved with a convex integer program solver. This is because the number of index entries can easily exceed a few millions in any production retrieval system. Solving this exactly is only possible for very small test collections. To tackle this issue, we resort to a technique called surrogate modeling (or optimization transfer), which approximates the original ob-jective using a majorization/minorization function that is analytically or numerically efficient to compute. See Lange et al. [16] for a comprehensive treatment.

To the best of our knowledge, there are two major ap-proaches for inducing such surrogate models: taking the first-order Taylor approximation, or using the Jensen X  X  in-Recall that Jensen X  X  inequality states that the following prop-erties hold for any convex (or concave) function f :
Let f be the logarithmic function. The original objective in our problem (Equation 10) now corresponds to the left-hand side E f ( X ). Since the logarithmic function is concave, we have the surrogate model f ( E X ) an upper bound of the original objective: or equivalently: This surrogate model has a simple analytical solution: Sort the index entries according to weighted query likeli-It can be shown that a simple maneuver such as Algorithm 1, called weighted uniform pruning , guarantees to maximize the objective. This corresponds to a weighted version of Carmel et al. X  X  uniform pruning method. This algorithm would fall back to the unweighted form when we supply a uniform p ( t ) and a plug-in estimate of the query likelihood. Note that the plug-in approach is valid only when the score function is proportional to the true likelihood.

For simplicity, we take a very loose definition of query likelihood in this paper so as to cover the well-known BM25 function. As we shall present shortly, the empirical result even more sophisticated objective. shows that the performance of BM25 is no worse than that o f a rigorously defined language model (with Jelinek-Mercer smoothing). What is left unsettled is how to estimate  X  given a target prune ratio  X  . This issue is treated in Section 4.2.
Thus far, we have established the theoretical ground for uniform pruning. Our next quest is to find empirical evi-dence that supports this result. In the coming subsections, we shall briefly describe the experiment settings and present the experimental result in greater detail.
We used three test collections in this experiment: TREC disks 4 &amp; 5, WT2G, and WT10G. The first two collections are tested against topics 401-450 and the latter against top-ics 451-550. For each topic, we tested both short (title) and long (title + description) queries. Details about the bench-mark are summarized in Table 1. All three collections were ments, we applied the porter stemmer and used the stan-dard 411 InQuery stoplist. No additional text processing is done to the test collections.

According to how index traversal is preferred, a prun-ing method can be either term-centric or document-centric. Since different traversal strategies rely on different index creation procedures, it is difficult to have both sets imple-mented in one place. For simplicity, in this experiment we fo-cused only on term-centric methods. Specifically, we tested the following methods: 1. Uniform pruning (UP) [9]: This method is the subject 2. Top-k term-centric pruning (TCP) [9]: We set k = 10 3. Probability ranking principle (PRP) [7]: h ttp://www.lemurproject.org/indri.php Table 1: Test collections and the corresponding query topics.
 4. Information preservation, with uniform document prior
We are aware that document-length update may improve the TCP and PRP retrieval performance [5,7]. Nevertheless, in this study we did not implement this feature. This shall be addressed in the future work.
In this experiment, we settled on 9 fixed prune levels at  X  = 0 . 1 , 0 . 2 , . . . , 0 . 9. To control the prune ratio, compari-son is only allowed between experimental runs at the same prune level. In each reference method, the true prune ratio depends on some parameter (e.g.,  X  in TCP and PRP), which we called the threshold parameter . To reduce the index down to the right size, we employed two different approaches to determine this cutting threshold: 1. Sample percentile: Collect the prune scores on top of 2. Bisection: Take an interval of feasible parameter values
Bisection requires several test-prune runs into the entire index and is therefore more time-consuming. Sample per-centile needs only one pass through the index, but the re-sulting prune ratio can be less precise than that with the values learned using bisection. In this paper, we applied bi-section to TCP to learn the parameter  X  , and applied sam-ple percentile to the rest of methods. Specifically, we used a sample size of 10% of the entire index. For either case, the prune ratio error is controlled to within  X  0 . 2%. MAP (t) .1 .2 .3 .4 .5 .6 .7 .8 .9 TCP 160 157 152 145 139 134 127 118 095 U P-bm25 159 158 154 148 144 140 135 127 102 U P-dir 160 157 153 149 145 143 142 137 120 UP-jm 160 158 153 146 140 133 126 110 085 P RP 156 152 148 142 133 123 110 088 045 IP-u 156 153 148 140 135 123 107 092 046 MAP (td) .1 .2 .3 .4 .5 .6 .7 .8 .9 TCP 203 197 187 177 163 147 142 130 091 U P-bm25 204 189 177 170 160 158 141 141 116 U P-dir 204 199 191 183 177 175 174 164 136 UP-jm 204 199 185 179 164 150 137 103 074 P RP 186 174 160 150 136 123 112 090 050 IP-u 189 171 165 149 143 124 116 095 051 P10 (t) .1 .2 .3 .4 .5 .6 .7 .8 .9 TCP 260 256 252 246 245 236 209 201 174 U P-bm25 259 258 254 239 226 225 204 192 168 U P-dir 261 257 254 248 249 242 241 236 222 UP-jm 261 259 253 249 243 237 225 190 168 P RP 256 250 239 224 201 182 157 125 107 IP-u 257 248 243 214 202 193 164 133 106 P10 (td) .1 .2 .3 .4 .5 .6 .7 .8 .9 TCP 347 347 339 332 315 290 279 267 231 U P-bm25 351 339 336 309 295 287 264 248 209 U P-dir 347 350 341 338 327 317 316 311 291 UP-jm 347 350 341 335 319 299 267 229 191 P RP 344 324 300 281 249 222 181 169 127
IP-u 340 329 310 280 253 233 186 175 122
We followed Blanco and Barreiro [7] for using BM25 as the post-pruning retrieval method. Retrieval performance is measured in mean average precision (MAP) and precision-at-10 (P@10). The result on the largest set WT10G is sum-marized in Figure 1 (see Figures 2 and 3 at the very end of this paper for results on the smaller sets). Each figure has four sets of measure-to-query-type combinations, and the result for each combination is given both as a table on the left and a plot on the right. These combinations from top to bottom, respectively, are MAP-t, MAP-td, P@10-t, and P@10-td. Table columns and x-axes in the plots indi-cate prune levels, from 0.1 to 0.9 (10% to 90%). Rows and curves indicate pruning methods.

Our result shows that, at small prune levels (  X  0 . 5), all these methods differ little in performance, and the difference at larger prune levels seems more evident. Both PRP and IP-u, whose performance was nearly identical, have consis-tently achieved the bottom performance in all settings. In general, the performance for the UP family and TCP is com-parable, though UP-dir performed slightly better than the other. We noticed that the performance of UP-dir is also robust to high prune ratio. On WT10G, when tested under an extreme setting with 90% prune ratio, UP-dir still man-aged to retain 75% of the baseline MAP for short queries, and 66.7% for long queries. Of the baseline P@10, UP-dir retained 85.1% for short queries and 83.9% for long queries. Under a less aggressive setting such as 50% prune ratio, UP-dir have done even better by retaining 90.6% and 86.8% of baseline MAP, and 95.4% and 94.2% of baseline P@10, re-spectively for short and long queries.
We further conducted an analysis of variance (ANOVA) to check if the performance difference is significant. Due to the unbalanced size of measurement, we tested each corpus independently. Here, we assume a fixed-effect, 4-way no interaction, repeated measure design, expressed as: where Y i,j,k,l is the measured performance, a i is the query-type effect, b j the prune-level effect, c k the method effect, and d l the topic effect, and  X  i,j,k,l denotes the error.
The result is covered in Table 2. Each row indicates a measure-effect combination and each column a test corpus. Test statistics, such as degrees of freedom (DF) and F-values (F), are given for every test case. We used partial eta-square p ) to measure the effect size [17]. We first ran an omnibus test to see if any main effect is significant. Of all three collections, all the main effects were tested significant for Methods that differ significantly do not share the same group label. p &lt; 0 . 001. Further analysis shows that query type and prune method have relatively small effect sizes, suggesting that query topic and prune ratio have much greater influence on the retrieval performance than the others do.

Post-hoc tests are then called for to examine the difference caused by different factor values. Since our experimental setting involves multiple comparison, we employed Tukey X  X  honest significance difference (HSD) to control the overall Type I error [12]. Note that since Tukey X  X  HSD is a one-way test, only one effect is tested in each run. In the following paragraphs, we summarize the HSD results for all the main effects. Here, since our focus is on the method effect, we shall briefly cover the other three for the sake of completeness. method effect. For each measure-corpus combination, we as-signed group labels, e.g.,  X  X  X  to  X  X  X , to individual methods based on the pairwise differences in their means. The differ-ence between two methods is significant if and only if they share no common group label.

The result is briefly summarized as follows. First, the UP family and TCP consistently achieved top performance in both MAP and P@10 across different test settings. In the leading group, UP-dir delivers slightly better performance than the others. This is even more pronounced under the Web settings, in which UP-dir significantly outperformed the rest of methods in MAP (on both corpora) and in P@10 (on WT10G only). Second, the performance for the rest of UP family and TCP is in general comparable. Take UP-bm25 and TCP. The performance difference between the two is subtle: UP-bm25 was shown superior in MAP on Disks 4 &amp; 5 but inferior in P@10 on WT2G. Third, PRP and IP-u are inferior to all the other methods. This result is consistent with our analysis on the raw performance measurements. formance than short queries (t), which is expected because short queries are less precise than longer ones. This differ-ence is confirmed on all three test collections, and appears more evident in the largest set WT10G.
 ones in both metrics, which is also expected since more ag-gressive pruning results in less information in the index. Ac-cording to the pairwise comparison made within the HSD test, this result is generally true except for a few small pairs such as 0.1-against-0.2. Specifically, WT10G has many such insignificant small pairs, suggesting that retrieval on larger Web collections is less sensitive to information loss. size of topic pairs, e.g., topics 451-551 on WT10G has pro-duced 4950 such pairwise comparisons. In general, only a small number of queries have significantly deviated from the average performance, meaning that most queries are de-signed to be about equally difficult.
T he experiment result for uniform pruning is generally in line with our understanding to impact, much of this was con-tributed by the previous work in index compression and dy-namic pruning. Since many ideas come from the same out-let in the indexing pruning community, it is no surprise that uniform pruning is related to many existing impact-based methods. For example, Anh et al. [3] concluded that impact-sorted indexes combined with early termination heuristics can best optimize retrieval system performance. This tech-nique is conceptually equivalent to uniform pruning. Fur-ther work in this line investigated impact-based pruning, an application of impact-sorting to dynamic query pruning [4]. And again, this is a dynamic version of uniform pruning. Adding to these results, our analysis shows that impact-based methods are good approximate solutions to the pro-posed model induction problem.

One further question that invites curious eyes is why Dirich-let smoothing worked so well with uniform pruning that it significantly outperformed all the other variations on our Web benchmark WT2G and WT10G. So far the answer is still unclear to us. Here, let us discuss a few possibilities:
With the argument given in Section 3 about the convex integer program, one may argue that it is important to pre-vent depleting any term posting since doing so would take the objective in Equation 10 to minus infinity. In other words, an additional constraint, called  X  X o depletion X , shall be added into the index pruning guideline. This is because, even though we do not attempt to solve the convex program, the constraint still needs to be enforced to guarantee that in-formation loss is bounded. In this respect, it is necessary to adopt a top-k preserving strategy (i.e., skip any term post-ing that has less than k entries), such as the one in TCP, to avoid depleting term postings.
In this paper, we review the problem of static index prun-ing from a brand new perspective. Given the appropriate assumptions, we show that this problem can essentially be tackled within a model induction framework, using the prin-ciple of minimum cross-entropy. The theory guarantees that the induced model best approximates the full model in terms of probability divergence. We show that static index prun-ing can be written as a convex integer program. Yet exact inference, though possible as it might be, is generally com-putationally infeasible for large collections. So we further propose a surrogate model to address the computation is-sue, and show that uniform pruning is indeed an optimal solution to the formalism. To verify the correctness of our result, we conducted an extensive empirical study. The ex-periment was redesigned to take two factors, variable control and significance testing, into consideration. This setup has helped us reduce possible experimental bias or error.
Our result confirms that, when paired with the Dirichlet smoother, the performance of uniform pruning is state of the art. Significant improvement over the other methods were observed across diverse retrieval settings. Uniform prun-ing also exhibits an advantage in robustness with respect to large prune ratio. Specifically, our result on WT10G for short queries suggests that uniform pruning with the Dirich-let smoother retains at least 90% of the baseline performance at 50% prune ratio and 85% at 80% prune ratio. To the best of our knowledge, this is by far the best performance ever re-ported for static index pruning on the standard benchmark.
This research work has given rise to many technical issues, some have been addressed in Section 5 and some remain un-settled. It shall be interesting to see how uniform pruning responds to other test environments, such as different re-trieval engines, corpora, or tasks. Document-length update and pseudo relevance feedback have been two landmark is-sues that we are ready to explore. Since we did not fine-tune the baseline performance, testing pruning methods against optimized, strong baseline shall provide more insight about this art. Besides all these possibilities, one promising direc-tion is to extend the model induction idea to other type of structured data, such as lexicons or language models. Fur-ther investigation into the theory may shed us some light in the role that impact plays in different IR tasks.
We would like to thank Wei-Yen Day, Ting-Chu Lin, and the anonymous reviewers for their useful comments. [1] I. S. Altingovde, R. Ozcan, and O. Ulusoy. A [2] I. S. Altingovde, R. Ozcan, and O. Ulusoy. Static [3] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space [4] V. N. Anh and A. Moffat. Pruned query evaluation MAP (t) .1 .2 .3 .4 .5 .6 .7 .8 .9 TCP 225 221 213 206 198 186 172 149 109 U P-bm25 227 226 223 218 208 194 168 166 143 UP-dir 225 222 213 206 197 190 178 157 117 U P-jm 224 221 211 202 192 180 163 140 103 PRP 227 224 219 209 194 168 148 149 108 I P-u 227 225 220 211 195 173 147 147 108 MAP (td) .1 .2 .3 .4 .5 .6 .7 .8 .9 TCP 251 246 239 232 219 201 184 162 117 U P-bm25 250 246 235 229 222 202 179 176 158 UP-dir 251 247 239 232 220 210 196 173 133 U P-jm 250 246 238 228 216 194 173 148 110 PRP 253 246 228 212 195 168 148 146 129 I P-u 252 240 230 215 195 173 145 138 125 P10 (t) .1 .2 .3 .4 .5 .6 .7 .8 .9 TCP 438 434 434 430 428 426 424 384 326 U P-bm25 438 440 436 438 438 412 370 358 294 U P-dir 438 434 432 428 426 434 416 398 344 UP-jm 438 434 432 428 424 416 410 370 318 P RP 436 442 444 452 428 400 340 284 204 I P-u 438 440 440 450 430 406 338 278 190 P 10 (td) .1 .2 .3 .4 .5 .6 .7 .8 .9 TCP 476 468 478 474 468 460 462 432 358 U P-bm25 470 486 482 470 444 420 400 390 324 U P-dir 474 468 472 462 458 450 440 442 386 UP-jm 476 470 470 468 466 442 430 412 336 P RP 486 486 468 470 428 388 350 316 236
I P-u 474 478 474 464 436 400 340 292 226 [5] R. Blanco and A. Barreiro. Boosting static pruning of [6] R. Blanco and A. Barreiro. Static pruning of terms in [7] R. Blanco and A. Barreiro. Probabilistic static [8] S. B  X  uttcher and C. L. A. Clarke. A document-centric [9] D. Carmel, D. Cohen, R. Fagin, E. Farchi, [10] R.-C. Chen, C.-J. Lee, C.-M. Tsai, and J. Hsiang. [11] E. S. de Moura, C. F. dos Santos, D. R. Fernandes, [12] D. Hull. Using statistical testing in the evaluation of MAP (t) .1 .2 .3 .4 .5 .6 .7 .8 .9 TCP 248 242 233 222 208 195 176 143 090 U P-bm25 249 250 249 236 225 213 188 133 108 U P-dir 247 240 234 226 207 195 198 181 139 UP-jm 247 239 229 220 199 173 143 107 080 P RP 253 249 237 226 196 166 114 092 063 I P-u 250 247 240 225 205 164 126 081 074 MAP (td) .1 .2 .3 .4 .5 .6 .7 .8 .9 TCP 288 278 258 241 222 206 183 147 096 U P-bm25 288 276 260 237 226 209 193 151 122 UP-dir 290 279 268 258 238 225 224 209 161 UP-jm 289 281 262 245 217 187 149 113 078 P RP 284 262 233 214 189 166 119 093 067 IP-u 284 263 236 216 190 161 124 087 079 P10 (t) .1 .2 .3 .4 .5 .6 .7 .8 .9 TCP 416 410 408 398 400 380 374 338 268 U P-bm25 414 420 418 386 380 384 340 238 210 U P-dir 414 402 402 404 380 386 392 374 332 UP-jm 414 404 402 402 378 348 336 306 248 P RP 418 418 408 394 362 342 250 126 144 I P-u 410 408 400 390 380 338 268 138 126 P10 (td) .1 .2 .3 .4 .5 .6 .7 .8 .9 TCP 464 446 428 410 410 388 356 328 302 U P-bm25 464 446 412 370 378 384 326 268 218 U P-dir 460 446 432 436 422 428 422 408 346 UP-jm 460 446 430 420 388 354 340 276 260 P RP 442 434 392 382 340 334 260 142 146
IP-u 450 430 402 392 348 324 258 158 170 [13] R. J. Hyndman and Y. Fan. Sample quantiles in [14] S. Kullback. Information Theory and Statistics . Wiley, [15] J. D. Lafferty and C. Zhai. Document language [16] K. Lange, D. R. Hunter, and I. Yang. Optimization [17] D. C. Montgomery. Design and analysis of [18] S. Robertson. The probability ranking principle in IR. [19] S. Thota and B. Carterette. Within-Document [20] C. Zhai and J. Lafferty. Model-based feedback in the [21] L. Zheng and I. J. Cox. Entropy-Based static index
