 Privacy and security has become the focus of many data mining researches [1~4] . It is while maintaining a high level of accessibility for data miners. 
A novel privacy-preserving classification mining algorithm is proposed in this pa-perturbing probability. Thirdly, a data perturbing method is described to perturb origi-tree classification algorithm-PPCART is given. R. Agrawal proposed a privacy-preserving classification mining algorithm in which construct the probability distribution of the original data, so the reconstruction accu-tioned data sets by using security multi-party computation [2] . D. Agrawal recovered original data probability distribution by EM [3] , and said the algorithm outperformed ciencies of R. Agrawal X  X . W.L. Du proposed a kind of privacy-preserving classifica-rithm only fitted boolean type attributes. 
As is mentioned in above section, our algorithm is superior to all the others in that it rectifies their deficiencies and combines their merits. 3.1 Support Count of Attributes Value Definition 1. Support count of attributes value values in Y are equal to y i . 3.2 Attribute Transition Probability Matrix Definition 2. Single attribute transition probability matrix fine A X  X  attribute transition probability matrix P A as  X   X  X  X   X  X  X  denotes the probability of attribute value a  X   X  ; Definition 3. Single split attribute transition probability matrix trix P A = r  X  X  X  is the distribution probability of attribute value a k in original data. transition probability matrix in the same way. Definition 4. Double split attributes joint transition probability matrix  X  X  X  , (1 , 1 ) P tribute B  X  X  value b i becoming b j and attribute A  X  X  value a k becoming a l . Definition 5. Multiple split attributes joint transition probability matrix 3.3 Data Perturbing Method ity matrix P (Aj) = perturbed data set D= { D 1 , D 2 ,..., D n }. 3.4 Measure of Privacy-Preserving Level Definition 6. Measure of privacy-preserving level Privacy-preserving level of an attribute =  X  X quare root of the number of non-zero ele-ments of an attribute transition probability matrix X /the domain size of the attribute in original data. 4.1 Method to Recover the Original Support Count of Attributes Value from 4.2 How to Choose Split Attribute and Split Point by the Original Support (1) Calculating gini { c 1, c 2, ..., c m }, related to m different classes-C i ( i =1,..., m ). Then (2) Choose split attribute and split point by calculating gini split and Gain where S 1 = gini( S 1 )= 1-2 The largest Gain corresponds to the best split attribute and split point. 
Sequentially, a decision tree is created in the same way. 4.3 Privacy-Preserving Decision T ree Classification Algorithm-PPCART Partition( S , split_attr_list&amp;value&amp;flag) Input: Perturbed samples set S, split attributes, split point values and flag; Output: a decision tree; (1) Create node N ; (3) For each attribute A do adopted to test the adaptability of PPCART algorithm in this experiment (Please refer performance in different conditions, we adopt five classification functions introduced in document [5] to assign values to the label attribute  X  X roup X . 5 ( )
Other experimental results show that classification accuracy of PPCART has little 
