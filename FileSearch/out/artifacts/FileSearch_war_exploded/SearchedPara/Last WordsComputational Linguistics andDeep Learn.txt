
Stanford University 1. The Deep Learning Tsunami
Deep Learning waves have lapped at the shores of computational linguistics for several major Natural Language Processing (NLP) conferences. However, some pundits are predicting that the final damage will be even worse. Accompanying ICML 2015 in Lille, France, there was another, almost as big, event: the 2015 Deep Learning Workshop.
The workshop ended with a panel discussion, and at it, Neil Lawrence said,  X  X LP is flattened. X  Now that is a remark that the computational linguistics community has to take seriously! Is it the end of the road for us? Where are these predictions of steam-rollering coming from?
Yann LeCun said:  X  X he next big step for Deep Learning is natural language under-standing, which aims to give machines the power to understand not just individual words but entire sentences and paragraphs. X  1 In a November 2014 Reddit AMA (Ask
Me Anything), Geoff Hinton said,  X  X  think that the most exciting areas over the next five years X  time we do not have something that can watch a YouTube video and tell a story about what happened. In a few years time we will put [Deep Learning] on a chip that fits into someone X  X  ear and have an English-decoding chip that X  X  just like a real Babel fish. X  2 And Yoshua Bengio, the third giant of modern Deep Learning, has also increasingly oriented his group X  X  research toward language, including recent excit-ing new developments in neural machine translation systems. It X  X  not just Deep Learn-ing researchers. When leading machine learning researcher Michael Jordan was asked at a September 2014 AMA,  X  X f you got a billion dollars to spend on a huge research project that you get to lead, what would you like to do? X , he answered:  X  X  X  X  use the billion dollars to build a NASA-size program focusing on natural language processing, in all of its glory (semantics, pragmatics, etc.). X  He went on:  X  X ntellectually I think that NLP is fascinating, allowing us to focus on highly structured inference problems, on issues that go to the core of  X  X hat is thought X  but remain eminently practical, and on a technology that surely would make the world a better place. X  Well, that sounds very nice! So, should computational linguistics researchers be afraid? I X  X  argue, no. To return to the
Hitchhiker X  X  Guide to the Galaxy theme that Geoff Hinton introduced, we need to turn the book over and look at the back cover, which says in large, friendly letters:  X  X on X  X  panic. X  2. The Success of Deep Learning
There is no doubt that Deep Learning has ushered in amazing technological advances in the last few years. I won X  X  give an extensive rundown of successes, but here is one example. A recent Google blog post told about Neon, the new transcription system for
Google Voice. 3 After admitting that in the past Google Voice voicemail transcriptions often weren X  X  fully intelligible, the post explained the development of Neon, an im-proved voicemail system that delivers more accurate transcriptions, like this:  X  X sing a (deep breath) long short-term memory deep recurrent neural network (whew!), we cut our transcription errors by 49%. X  Do we not all dream of developing a new approach to a problem which halves the error rate of the previously state-of-the-art system? 3. Why Computational Linguists Need Not Worry Michael Jordan, in his AMA, gave two reasons why he wasn X  X  convinced that Deep
Learning would solve NLP:  X  X lthough current deep learning research tends to claim to encompass NLP, I X  X  (1) much less convinced about the strength of the results, compared to the results in, say, vision; (2) much less convinced in the case of NLP than, say, vision, the way to go is to couple huge amounts of data with black-box learning architectures. X 
Jordan is certainly right about his first point: So far, problems in higher-level language processing have not seen the dramatic error rate reductions from deep learning that have been seen in speech recognition and in object recognition in vision. Although there have been gains from deep learning approaches, they have been more modest than sudden 25% or 50% error reductions. It could easily turn out that this remains the case. The really dramatic gains may only have been possible on true signal processing tasks. On the other hand, I X  X  much less convinced by his second argument. However,
I do have my own two reasons why NLP need not worry about deep learning: (1) It just has to be wonderful for our field for the smartest and most influential people in machine learning to be saying that NLP is the problem area to focus on; and (2) Our field is the domain science of language technology; it X  X  not about the best method of machine learning X  X he central issue remains the domain problems. The domain problems will not go away. Joseph Reisinger wrote on his blog:  X  X  get pitched regularly by startups doing  X  X eneric machine learning X  which is, in all honesty, a pretty ridiculous idea.
Machine learning is not undifferentiated heavy lifting, it X  X  not commoditizable like EC2, and closer to design than coding. X  5 From this perspective, it is people in linguistics, people in NLP, who are the designers. Recently at ACL conferences, there has been an over-focus on numbers, on beating the state of the art. Call it playing the Kaggle game. More of the field X  X  effort should go into problems, approaches, and architectures.
Recently, one thing that I X  X e been devoting a lot of time to X  X ogether with many other 702 collaborators X  X s the development of Universal Dependencies. a common syntactic dependency representation and POS and feature label sets that can be used with reasonable linguistic fidelity and human usability across all human languages. That X  X  just one example; there are many other design efforts underway in our field. One other current example is the idea of Abstract Meaning Representation. 4. Deep Learning of Language
Where has Deep Learning helped NLP? The gains so far have not so much been from true Deep Learning (use of a hierarchy of more abstract representations to promote generalization) as from the use of distributed word representations X  X hrough the use of real-valued vector representations of words and concepts. Having a dense, multi-dimensional representation of similarity between all words is incredibly useful in NLP, but not only in NLP. Indeed, the importance of distributed representations evokes the  X  X arallel Distributed Processing X  mantra of the earlier surge of neural network methods, which had a much more cognitive-science directed focus (Rumelhart and
McClelland 1986). It can better explain human-like generalization, but also, from an engineering perspective, the use of small dimensionality and dense vectors for words allows us to model large contexts, leading to greatly improved language models. Espe-cially seen from this new perspective, the exponentially greater sparsity that comes from increasing the order of traditional word n -gram models seems conceptually bankrupt. curs within deep representations can theoretically give an exponential representational advantage, and, in practice, offers improved learning systems. The general approach to building Deep Learning systems is compelling and powerful: The researcher defines a model architecture and a top-level loss function and then both the parameters and the representations of the model self-organize so as to minimize this loss, in an end-to-end learning framework. We are starting to see the power of such deep systems in recent work in neural machine translation (Sutskever, Vinyals, and Le 2014; Luong et al. 2015). for language in particular, and for artificial intelligence in general. Intelligence requires being able to understand bigger things from knowing about smaller parts. In particular for language, understanding novel and complex sentences crucially depends on being able to construct their meaning compositionally from smaller parts X  X ords and multi-word expressions X  X f which they are constituted. Recently, there have been many, many papers showing how systems can be improved by using distributed word represen-tations from  X  X eep learning X  approaches, such as word2vec (Mikolov et al. 2013) or GloVe (Pennington, Socher, and Manning 2014). However, this is not actually building
Deep Learning models, and I hope in the future that more people focus on the strongly linguistic question of whether we can build meaning composition functions in Deep
Learning systems. 5. Scientific Questions That Connect Computational Linguistics and Deep Learning
I encourage people to not get into the rut of doing no more than using word vectors to make performance go up a couple of percent. Even more strongly, I would like to suggest that we might return instead to some of the interesting linguistic and cognitive issues that motivated noncategorical representations and neural network approaches. gerund V-ing form, such as driving . This form is classically described as ambiguous between a verbal form and a nominal gerund. In fact, however, the situation is more complex, as V-ing forms can appear in any of the four core categories of Chomsky (1970): ambiguity but mixed noun X  X erb status. For example, a classic linguistic text for being a noun is appearing with a determiner, while a classic linguistic test for being a verb is taking a direct object. However, it is well known that the gerund nominalization can do both of these things at once:
This is oftentimes analyzed by some sort of category-change operation within the levels noncategorical behavior in language.
 (1972). Diachronically, the V-ing form shows a history of increasing verbalization, but in many periods it shows a notably non-discrete status. For example, we find clearly graded judgments in this domain:
Various combinations of determiner and verb object do not sound so good, but still much better than trying to put a direct object after a nominalization via a derivational morpheme such as -ation . Houston (1985, page 320) shows that assignment of V-ing forms to a discrete part-of-speech classification is less successful (in a predictive sense) than a continuum in explaining the spoken alternation between -ing vs. -in X  , suggesting that  X  X rammatical categories exist along a continuum which does not exhibit sharp boundaries between the categories. X  704 mates, Whitney Tabor. Tabor (1994) looked at the use of kind of and sort of , an example that I then used in the introductory chapter of my 1999 textbook (Manning and Sch  X  utze 1999). The nouns kind or sort can head an NP or be used as a hedging adverbial modifier: such as the following pair, which suggests how one form emerged from the other.
Beginning in Middle English, ambiguous contexts, which provide a breeding ground for the reanalysis, start to appear (the 1570 example in Example (13)), and then, later, examples that are unambiguously the hedging modifier appear (the 1830 example in
Example (14)):
This is history not synchrony. Presumably kids today learn the softener use of kind/sort of first. Did the reader notice an example of it in the quote in my first paragraph?
Whitney Tabor modeled this evolution with a small, but already deep, recurrent neural network X  X ne with two hidden layers. He did that in 1994, taking advantage of the opportunity to work with Dave Rumelhart at Stanford.
 tributed representations for modeling and explaining linguistic variation and change.
Sagi, Kaufmann, and Clark (2011) X  X ctually using the more traditional method of La-tent Semantic Analysis to generate distributed word representations X  X how how dis-tributed representations can capture a semantic change: the broadening and narrowing of reference over time. They look at examples such as how in Old English deer was any animal, whereas in Middle and Modern English it applies to one clear animal family.
The words dog and hound have swapped: In Middle English, hound was used for any kind of canine, while now it is used for a particular sub-kind, whereas the reverse is true for dog .
 of words such as gay over the last century (exploiting the online Google Books Ngrams corpus). At a recent ACL workshop, Kim et al. (2014) use a similar approach X  X sing word2vec X  X o look at recent changes in the meaning of words. For example, in Figure 1, they show how around 2000, the meaning of the word cell changed rapidly from being close in meaning to closet and dungeon to being close in meaning to phone and cordless .
The meaning of a word in this context is the average over the meanings of all senses of a word, weighted by their frequency of use.
 modeling phenomena characterize the previous boom in neural networks. There has been a bit of a kerfuffle online lately about citing and crediting work in Deep Learning, and from that perspective, it seems to me that the two people who scarcely get men-tioned any more are Dave Rumelhart and Jay McClelland. Starting from the Parallel
Distributed Processing Research Group in San Diego, their research program was aimed at a clearly more scientific and cognitive study of neural networks.
 approaches for rule-governed linguistic behavior. Old timers in our community should remember that arguing against the adequacy of neural networks for rule-governed linguistic behavior was the foundation for the rise to fame of Steve Pinker X  X nd the foundation of the career of about six of his graduate students. It would take too much space to go through the issues here, but in the end, I think it was a productive debate. It led to a vast amount of work by Paul Smolensky on how basically categorical systems can emerge and be represented in a neural substrate (Smolensky and Legendre 2006).
Indeed, Paul Smolensky arguably went too far down the rabbit hole, devoting a large part of his career to developing a new categorical model of phonology, Optimality
Theory (Prince and Smolensky 2004). There is a rich body of earlier scientific work that has been neglected. It would be good to return some emphasis within NLP to cognitive and scientific investigation of language rather than almost exclusively using an engineering model of research.

Language Processing is seen as so central to both the further development of machine learning and industry application problems. The future is bright. However, I would encourage everyone to think about problems, architectures, cognitive science, and the details of human language, how it is learned, processed, and how it changes, rather than just chasing state-of-the-art numbers on a benchmark task. 706 Acknowledgments References
