 Nicolas Loeff loeff@uiuc.edu David Forsyth daf@uiuc.edu Deepak Ramachandran dramacha@uiuc.edu Manifold Learning algorithms exploit geometric (or correlation) properties of datasets in high-dimensional spaces. The literature is too large to review in detail here (163 references in a recent review (Zhu, 2006)). Many different approaches have been pursued that uti-lize manifold structure such as constructing an explicit parametrization (e.g. (Tenenbaum et al., 2000; Roweis &amp; Saul, 2000; Donoho &amp; Grimes, 2003)), introducing a penalty term that imposes smoothness conditions on functions restricted to the manifold (e.g. (Sindhwani et al., 2006)), adjusting kernel smoothing bandwidths to account for manifold properties (e.g. (Bickel &amp; Li, 2007)), and infering labels for unlabeled data using a harmonic smoother (e.g. (Zhu et al., 2003)).
 There is a rough distinction in semi-supervised learn-ing between manifold based algorithms that expect data to lie embedded in a space of lower intrisic di-mensionality, and cluster-based algorithms that ex-pect data to lie in clumps (the distinction seems to explain some differences in performance on different datasets (Chapelle et al., 2006)). There is some dis-agreement about the benefits of using unlabeled data, which may not always improve the asymptotic error rate of a regression estimator (Lafferty &amp; Wasserman, 2007). On the other hand, (Niyogi, 2008) argues that manifold learning is useful insofar as the marginal of the data P x can be linked with the conditional P y | x via the manifold.
 Computational Complexity is a common problem for most semi-supervised approaches. Write l for the number of labeled data items and u for the number of unlabeled data items. Many algorithms scale as badly as O (( l + u ) 3 ) (Zhu, 2006). Transductive support vector machines must solve a quadratic programming problem in ( l + u ) variables (Joachims, 1999). Manifold smoothing of an SVM solves a quadratic programming problem in l variables, followed by a linear problem in l + u variables; the situation is better for a linear SVM if feature vectors are sufficiently sparse (Sind-hwani et al., 2006). Harmonic smoothing solves a rel-atively sparse linear system in l variables. This prob-lem is relatively tractable, because the linear system involves the Laplacian of the smoothing kernel and so should be diagonally dominant (see (Dyn et al., 1986) for relevant observations in the context of radial basis functions). Each method must pay the cost of forming the Laplacian. For functional approximation schemes other than kernel smoothing, the complexity of current manifold learning methods in the number of training examples appears to be high. This is a problem  X  it is natural to want to use a manifold regularization term with such methods as tree-structured classifiers, and with very large datasets.
 Gradient Boosting poses function approximation as a variational problem, then uses a form of coordi-nate ascent on that problem ((Friedman, 1999); sec-tion 2). In this paper, we describe a variation on gradient boosting that can exploit a manifold regu-larization term, is fast and efficient for many forms of functional approximation (section 2.1), provides out of sample extensions (section 4), offers performance at the state of the art on standard datasets, and is capa-ble of handling very large datasets (section 6). In the extreme case, when there is no supervision, the gen-eralized method gracefully degrades into a clustering method (section 4). Finally, we show that our frame-work also easily extends to multi-class problems by choosing suitable loss functions (section 5). We follow convention by minimizing the sum of an expected loss and a regularization term. We must pre-dict labels y  X  Y for patterns x  X  X . We assume a probability distribution P x,y over X  X Y .
 We will further assume the support of the marginal P x lies on a domain M X  X  . Typically, this domain is of lower intrinsic dimension than X ; the term manifold is widely used to refer to such domains, though we require no manifold properties.
 Write the predictor as F ( x ), and the cost function as  X  ( y,F ( x )). We would like to find the function mini-mizer F  X  = arg min F  X  X  V [ F ], of the cost functional restricted to some function family H . Our regulariza-tion term is of the same form as that of (Sindhwani et al., 2006), and encourages smoothness of the solu-tion in regions of high probability density. We control the complexity of the solution by choosing H and using the shrinkage approach of (Friedman, 1999).
 This expression is very general. There are many pos-sible choices for  X  [ y,F ]. Expressions such as | y  X  F | and ( y  X  F ) 2 are typically used for regression. Expres-sions such as exp(  X  yF ) and the binomial log likelihood log(1 + exp(  X  2 yF )) penalize the margin yF , and are typically used for classification. 2.1. ManifoldBoost Framework 2.1.1. Stagewise Functional Minimization ( P x Following the work of Friedman (Friedman, 1999), we will find a additive solution of the form We will proceed in a greedy fashion. Assume we have a solution for M = m ; we will then minimize V [ F m + f m +1 ] with respect to f m +1 . After (Friedman, 1999), we obtain a descent direction from the first variation of V where Write  X  u,v  X  for the usual inner product in L 2 . Now  X V [ F m ,f ] is a linear functional of f , so there is some G
V ( F m )  X  which we regard as the  X  X radient X  of the cost  X  such that  X V [ F m ,f ] =  X  G V ( F m ) ,f  X  . Now we have that  X  G V ( F m ) ,f  X  is equal to
Z ( f ( x ) h R Assuming sufficient regularity, recalling that P x = 0 on the boundary of the support of P x , and using the first Green identity, we have that  X  G V ( F m ) ,f  X  is equal to
Z ( f ( x ) h R where  X  2 M =  X  X  X  X  X  M is the Laplace-Beltrami op-erator. The optimal descent direction is a function f that maximizes  X  X  G V ( F m ) ,f  X  (subject, if necessary to a norm constraint on f ). The term f m +1 =  X f is obtained using line search, minimizing the true cost V [ F m +  X f ] with respect to  X  . 2.1.2. Finite Data Generally, neither P x nor P x,y are known. Instead, we have sample of labelled data { x i ,y i } l i =1 , and of un-labelled data { x i } u i = l +1 . Now integrals become sums over data points. Generally, { f m ( x ) } will belong to a parametric family of functions (e. g. Radial Basis functions, decision trees, etc  X  ).
 The Laplacian operator in equation 5 must be dis-cretized. In high dimensions, we cannot triangulate the data set. A smoothed Laplacian is equivalent to the difference between a short-scale average of the data and a long-scale average (e.g. the use of unsharp mask-ing in photography, or the difference of Gaussians in computer vision). The graph Laplacian is a linear operator that takes a function on the graph to the weighted difference between the function value and the average of the K nearest neighbours. This means it is usual to approximate the Laplacian operator with the graph Laplacian L (e.g. see (Sindhwani et al., 2006)). Write the graph Laplacian as L M . The cost function becomes Again, assume we know F m , and seek f m +1 . We will find a function f that maximizes  X  X  G V ( F m ) ,f  X  then we will weight this function using line search. The inner product is  X  a,b  X  = 1 N P N i =1 a ( x i )  X  b ( x have that  X  G V ( F m ) ,f  X  = Now  X  X  G V ,f  X  is linear in f , and so we should maxi-mize subject to a norm constraint on f . If the norm is fixed, then maximizing this expression is equivalent to This means any squared loss regression algorithm can be used to find the optimal parameters. Our varia-tional formulation explains why Friedman X  X  choosing to make f parallel to the gradient G V and posing the problem as squared error minimization is natu-ral. Once the descent direction f is found, the final f m +1 =  X f is obtained using line search, minimizing the true cost V [ F m +  X f ]. We offer two example algorithms with calculations to illustrate our extremely general formalism. For each example, we consider the binary case ( y  X  { X  1 , 1 } , y = 0 for unlabeled data), and use the negative binomial log likelihood as the loss function (Fried-man, 1999):  X  ( y,F ) = log(1 + exp(  X  2 yF )) For this case, whatever classifier we use represents F ( x ) = m , the inner product with the  X  X radient X  becomes,  X  G V ( F m ) ,f  X  = The cases now differ by the procedures used to choose the optimal f Tree-ManifoldBoost : As in L 2 TreeBoost (Fried-man, 1999), we use regression trees as base learners. A tree has the form f m +1 ( x ) = P S s =1  X  m +1 ,s I [ x  X  R where I [  X  ] = 1 if the expression inside is true, and I [  X  ] = 0 otherwise.
 To minimize || G V  X  f || 2 , we must search for the pa-rameters R s (which determine the geometry of the tree) and  X  s (which determine weights within region). Once a tree has been found, we fix R s and min-imize V ( F m ( x ) + P S s =1  X  m,s [ x  X  R s ]) with respect to {  X  s } , using a standard continuous optimization method (BFGS; see (Bertsekas, 1996)). In each round, we use a small number of descent steps to prevent over-fitting.
 Algorithm 1 Tree ManifoldBoost Algorithm 1: F 0 ( x ) = 1 / 2[log(1 + y )  X  log(1  X  y )] 2: for m = 1 to M do 3: Compute G V as in (8) 4: Obtain regression tree { R s,m } by minimizing 5: Find {  X  m,s } using BFGS and  X  X   X  X  7: end for The algorithm converges when M rounds have been run, or the relative change in the cost function in a round is below a threshold. Probability estimates for each x can then be estimated by inverting the loss function: p ( y = 1 | x ) = 1 / (1 + exp(  X  2 F M ( x ))). This in turn can be used for classification:  X  y i = where cost k a,b is the penalty for choosing label a when b is the correct label.
 Figure 1 shows a toy example for semisupervised clas-sification taken from (Sindhwani et al., 2006) (two moons dataset). The unlabeled datapoints are de-picted in green and the diamonds represent the labeled examples (one for each class). The algorithm also can provide likelihood estimates, as seen in the right fig-ures.
 RBF-ManifoldBoost : Tree functions are not the only possible approximation to the  X  X radient X . Step 4 in algorithm 1 can be modified so that R radial ba-sis function of width  X  , each with a weight w r and centered in a datapoint are chosen as approximation. Again, a BFGS step can be performed to improve the loss by fitting the weights w r . Algorithm 2 describes this.
 Algorithm 2 RBF ManifoldBoost Algorithm 1: F 0 ( x ) = 1 / 2[log(1 + y )  X  log(1  X  y )] 2: for m = 1 to M do 3: Compute G V as in (8) 4: Choose R RBFs greedily to minimize 5: Find { w r } using BFGS and  X  X   X  X  6: F m ( x ) = F m  X  1 ( x ) +  X  P r w r RBF r, X  ( x ) 7: end for Complexity: The procedure itself is linear in n = l + u , in the Laplacian neighborhood K , the dimension-ality of x and the number of rounds. The complexity of the algorithm depends then on the base regressor, and the computation of the Laplacian matrix. Influ-ence trimming can also be used to get tenfold speedups (Friedman, 1999), although the algorithm is still linear in the number of datapoints. The essential step in semi-supervised learning is the observation that similar data items should tend to have similar labels, which means that semi-supervised learning method should be capable of clustering. Our framework can naturally be extended to unsupervised learning, where one wishes to cluster data and the choice of label for a cluster is arbitrary. As there are no labeled data, the first term in equation 6 becomes zero and the problem is, under the constraints P i F ( x i ) = 0, P i F ( x i ) 2 = N (this is a form of spectral clustering problem, see (Sind-hwani et al., 2006); without the constraints, the prob-lem is ill-posed). Our formalism yields a greedy method for this problem, rather than the usual gen-eralized eigenvalue problem. To manage constraints, we use the Augmented Lagrangian method (Bertsekas, 1996), which adds a penalty in each round for con-straint violations in the unconstrained problem. We choose F  X  to be for non-decreasing sequences { c m 1 ,c m 2 } M m =1 . Af-ter each round, the values of the Lagrange mul-tipliers are increased by the constraint violation (Bertsekas, 1996)  X  m +1 1  X   X  m 1 + c m 1 ( P i F ( x i  X  is applied in each round. Algorithm 3 describes the tree-based version.
 The algorithm converges to a local minimum of the constrained problem. This formulation, unlike ISOMAP, naturally takes care of out-of-sample evalu-ation. Compared to (Sindhwani et al., 2006), the com-putational complexity is greatly reduced. On the other hand the solution is greedy, and there is no straightfor-ward term for controlling the complexity of the func-tion in the ambient space; this is achieved through the depth of the trees used in the algorithm.
 Algorithm 3 Unsupervised Tree ManifoldBoost Al-gorithm 1: Initialize F 0 ( x ) randomly, with zero mean and low 2: for m = 1 to M do 3: Compute G V of the penalized, uncontrained 4: Obtain regression tree { R m,s } by minimizing 5: Find {  X  m,s } using BFGS and fixing { R m,s } 7: Update Lagrange multipliers using the constrain 8: end for Algorithm 1 can be extended to K -class problems by introducing a multinomial cost in equation 1, where p ( c ) ( x ) represents the belief example x belongs to class c , and y ( c ) is a binary variable which is one if example x belongs to class c . As in (Friedman, 1999) we use the symmetric multiple logistic transform Smoothness of F ( c ) is enforced by defining the cost V ( { F ( c ) } ) to be The inner product of f ( c ) with gradient of V becomes, for class c , Now one regression tree is fitted per class at each round to approximate each descent direction. As in the two terminal nodes are fixed, and the parameters  X  ( c ) m,s for regions in each tree are learned in order to minimize the total cost V . We use a couple of BFGS iterations per round to find these parameters. In order to do this, the derivatives of the cost with respect to  X  ( c ) have to be computed.
 Once the final { F ( c ) M ( x ) } are computed, the proba-bility for a given example of each label can be esti-mated and thus the label can be classified as  X  c ( x i ) = is assigned when label c 0 is correct. The complexity of this algorithm is also linear in the number of classes, but it scales highly sub-linearly with the number of rounds M when inluence trimming is used (Friedman, 1999). 6.1. Comparison to Other Regularized Kegl et. al. (K  X egl &amp; Wang, 2005) introduce Reg-Boost , an extension to AdaBoost which incorpo-rates a weight decay that depends on a Laplacian reg-ularizer. Our approach is different several senses: first, ours is based on the GradientBoost framework while theirs is based on AdaBoost, second, in the sense that ManifoldBoost does not require manifold-regularized base learners. This makes their approach limited in Algorithm 4 K-Tree ManifoldBoost Algorithm 1: Let p ( c ) 0 be the frequencies of each class c . 3: for m = 1 to C do 4: Compute p ( c ) m ( x ) as in eq. 13 for all c . 5: for c = 1 to C do 6: Compute G ( c ) V as in (14) 7: Obtain regression tree { R ( c ) m,s } by minimizing 8: end for 11: end for the types of learners to be used (they use stumps only). Also, the ensemble classifier should be smooth on the manifold, but regularizing each of the base learners may result in over-smoothing of the overall solution. We compare our results with (K  X egl &amp; Wang, 2005) on standard UCI benchmark datasets. Whenever possi-ble we tried to use the same configuration as (K  X egl &amp; Wang, 2005) 1 . We set number of nearest neighbors K = 8 and used binary weights to compute the graph Laplacian. We used regression trees of fixed depth 3 as learners. The datasets were normalized to zero mean and unit variance. The learning rate was set to  X  = 0 . 1 after (Friedman, 1999). Only  X  was explored for dif-ferent values. We used 5-fold cross validation for de-termining parameters and 10-fold cross validation for error estimation. Table 1 compares our performance with that of AdaBoost , RegBoost , and (M. Belkin &amp; Niyogi, 2004) as reported in (K  X egl &amp; Wang, 2005). In the fully supervised problems, there is a difference in performance for the Sonar dataset, an impovement in the Ionosphere dataset, and a very slight decrease in performance in the Pima Indians dataset with respect to RegBoost , well within a standard deviation. It should be noted that the variance in the performance of the algorithm is consistently smaller for our algo-rithm. (K  X egl &amp; Wang, 2005) also tests the algorithm under semi-supervision, using 100 labeled and 251 un-labeled examples. We ran our algorithm under the same conditions, using the stumps to prevent overfit-ting. In this case our algorithm outperforms (K  X egl &amp; Wang, 2005) and (M. Belkin &amp; Niyogi, 2004), as our mean performance over 10 runs is more than a stan-dard deviation above theirs. No variance of results is reported in (K  X egl &amp; Wang, 2005). (Chen &amp; Wang, 2008) proposes an interesting alter-native approach to regularized boosting based on the more traditional framework of boosting  X  X eak X  learn-ers outlined in (Mason et al., 2000). As a consequence, they need to assign pseudo-class labels to unlabeled data (labels assigned with the current F t ( x )) while learning the ensemble. In contrast, ManifoldBoost uses base regressors to measure the confidence of the prediction and does not commit to { X  1 , +1 } classifi-cation at each step. Smoothing this seems more natu-ral in a formulation that penalizes second derivatives (Laplacian cost). 6.2. Comparison to Other Semi-Supervised We measured the performance of our two-class RBF-ManifoldBoost algorithm on the SSL data sets, a stan-dard benchmark for semi-supervised learning problems introduced in (Chapelle et al., 2006), and compared with the 14 other state-of-the-art semi-supervised learning algorithms discussed there. In table 2 we present results for five data sets, 2 of which are cluster-like and 3 manifold-like. On the manifold-like data sets, we are at the state of the art and no single algo-rithm does uniformly better than us. On the cluster-like data sets, our performance is good compared to most other regularization-based and manifold learn-ers but is not as good as the specialized clustering algorithms Cluster-Kernel and SGT (Spectral Graph Transducer).
 Parameter search was performed following section 21.2.5 of (Chapelle et al., 2006) when possible, us-ing the same ranges for  X  , the RBF width  X  , dis-tance metric, K , etc. For the base regressors, we used R  X  { 15 , 30 } as the numbers of RBFs, and M = 500 rounds. The learning rate was again chosen as  X  = 0 . 1. These parameters were obtained in small-scale experi-ments and then fixed. Results reported are the means over the different splits.
 The running times for a MATLAB implementation on a 2 GHz machine was in the order of minutes. Unfor-tunately, running times for the other algorithms were not reported in (Chapelle et al., 2006). 6.3. SecStr Data Set We also ran experiments on the SecStr data set (Chapelle et al., 2006), which is a problem of predict-ing the secondary structure of protein sequences from their amino acid chains. This is a large-scale and chal-lenging data set with 83,000 labeled and 1.2 million unlabeled examples. Semi-supervised algorithms have made little improvement to this benchmark so far (Ta-ble 3), and the best result is the manifold-regularized learning algorithm (Sindhwani et al., 2006), which yields a 29% error rate on a subset of the data with 10,000 labeled and 73,000 unlabeled examples. Tree-ManifoldBoost with  X   X  { 0 , 10  X  5 , 10  X  3 , 0 . 1 , 1 } achieved similar performance on the same subset in approximately 45 minutes of training time (after com-puting the Laplacian matrix). We used stumps, K = 6 and  X  = 0 . 05. No model selection was performed. We used as similarity measure the Hamming distance be-tween the best alignment of sequences. The results reported are the mean over the 10 splits.
 When we used the whole dataset (1.3 million se-quences) with  X   X  { 0 , 10  X  3 , 1 } , there is virtually no performance improvement. This may be due to the smaller parameter search space, or to peculiarities of the dataset. When we analysed the structure of the manifold on the labeled subset, we observed that al-most 20% of sequences at distance 1 (that is, shifted by one position to the left or right) had a different la-bel. Therefore the manifold assumption is not strong on this set.
 As far as we know, this is the first time results are reported on the complete SecStr dataset. Our algo-rithm is efficient and therefore can handle datasets of this size. Learning time is in the order of three hours for 1.3 million samples (leaving aside the computa-tion of the graph Laplacian, which took significantly longer) l 100 1000 10000 SVM 44.59 33.71 Cluster Kernel 42.95 34.03 QC randsub (CMN) 42.32 40.84 QC smartonly (CMN) 42.14 40.71 QC smartsub (CMN) 42.26 40.84 Boosting (assemble) 32.21 LapRLS 42.59 34.17 28.55 LapSVM 43.42 33.96 28.53 Tree-ManifoldBoost (83K) 42.70 33.43 28.96
Tree-ManifoldBoost (1.3M) 43.28 33.42 29.07 We have presented a new boosting framework for reg-ularized learning in a greedy, stage-wise procedure. It is flexible enough to handle the whole range of super-vision, from fully supervised classification to unsuper-vised clustering. The framework is general, accepts many different function approximation techniques, is 1-NN 49.00 13.65 16.66 48.67 3.89 5.81 47.88 46.72 43.93 42.45
SVM 49.85 30.60 20.03 34.31 5.53 9.75 47.32 46.66 23.11 24.64 efficient and fast at each round of boosting, handles multi-class and wholly unsupervised problems, and produces results at the state of the art. We are work-ing on understanding important aspects of the algo-rithm, in particular, generalization, error bounds, con-vergence and local minima.

