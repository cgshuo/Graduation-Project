 Faculty of Mathematics, Informatics, and Mechanics, The University of Warsaw, Banacha 2, 02-097 Warszawa, Poland E-mail: andrzejanusz@gmail.com 1. Introduction
Many practical applications of data mining and machine learning techniques involve construction of predictive models. Abundance of available data and decreasing cost of computation power contributed to the growing a demand for highly accurate classi fi cation and decision support systems. Often, for tasks such as client X  X  risk/credit rating, recommendation systems or spam classi fi cation, transparency and interpretability of a model can be sacri fi ced in favor for its precision. One way to construct an accurate prediction system is to combine weaker models into an ensemble .

The ensemble can be de fi ned as a set of separately trained classi fi ers whose predictions for previously unseen objects are combined in order to achieve better accuracy [1]. Many researchers have investigated the problem of constructing successful committees of predictive models [1,3,14].

Numerous experiments on real-life datasets con fi rmed that the most accurate ensembles are character-ized by diversity and high performance of individual classi fi ers. The most commonly used methods of constructing such sets of predictive models are bagging 1 and boosting [4,5,14]. In both of these meth-ods a single learning algorithm is used to create multiple classi fi ers. In the classical bagging algorithm (e.g. [6]), the t raining dataset is re sampled mu ltiple times and the models are trained on each of the bootstrap samples. Tested instances are then assessed by each of the models and the fi nal prediction is made by voting or by averaging the output of individual predictors. One example of a practical applica-tion of the bagging approach is Breiman X  X  Random Forest [7]. In this algorithm, multiple decision trees are grown for different subsets of objects and attributes. The importance of individual trees is estimated based on their prediction performance for off-the-bag training samples and the fi nal prediction is decided by a weighted voting.

In the boosting approach, an ensemble is built incrementally. In each step of the algorithm, a new clas-si fi er is constructed and new weights are assigned to training instances so that the examples misclassi fi ed by previous classi fi er become more important in the next step. The resulting model aggregates individ-ual models based on their predictive power. A typical example of the boosting algorithm is AdaBoost proposed by Freund and Schapire [8].

The desired diversi fi cation of models in the ensemble can be also achieved by including different learning algorithms, using different parameter settings or features selection techniques [9 X 11]. This ap-proach can be used in combination with bagging or boosting and it was proved to be successful in practice [3,10,12].

Another factor that in fl uences the effectiveness of classi fi er committees is a voting strategy or an aggregation function, which may be seen as methods of combining decisions of individual models in the ensemble. Many voting and aggregation methods ha ve been investigated in th e literature [2,13]. The most popular methods include majority voting and weighted voting, in which the weights are usually dependent on accuracies of the base predictors. A major drawback of such approaches is that they do not take into account correlation between decisions of aggregated models. As a consequence, in a committee constructed using performance-based weighting, two highly correlated but slightly more accurate models would be preferred over those which are more independent, even though a combination of the later is likely to outperform a combination of the former.

To overcome this issue, a framework has been proposed in which importance of individual models depend not only on their performance but also on relations between their predictions and the predictions of other models [13,15]. This approach is analogical to the feature selection problem and many feature selection algorithms may be directly employed in order to search for an optimal set of predictors. To gen-eralize this idea even further, researchers came up with a notion of meta-learning [16,17]. Figure 1 shows a schema for construction of an ensemble using the meta-learning technique. A wide array of machine learning algorithms can be used to perform meta-learning, such as the classical linear/logistic regres-sion (this method was used, for example, in The Net fl ix Competition a nd KDD Cup X 2010), Arti fi cial Neural Networks or the Naive Bayes . For example, in [18], authors use Normalizing Neural Networks to combine votes of rule-based classi fi ers computed from approximate decision reducts and in [19] the same method is used to optimize the Naive Bayes classi fi ers. In [16] the Naive Bayes classi fi er has been successfully used to improve classi fi cation accuracy of differently constructed decision trees on several biomedical datasets.

The genetic algorithm [20,21] may also be used as an ensemble meta-learning method. In [22], the genetic algorithm has been used to optimize a committee of neural network classi fi ers, by searching for heterogeneous sets of attributes on which the neural network were trained. A similar approach is presented in [23], where the genetic algorithm is utilized as a wrapper for combining SVM models.
The Genetic Meta-Blender (GMB) presented in this paper is also a meta-learning algorithm for con-structing an ensemble from a heterogeneous set of predictors. It combines the base models treating their predictions for available data as new features. It does not take into account the performance of individu-als but instead, it uses the genetic algorithm to search for a global optimum of a given scoring function (which could be, for example, an area under the ROC c urve). Utilization of the genetic approach makes the GMB fl exible and robust. It can be used as an optimization frame for a wide array of problems (loss functions). Additionally, the GA provides tools that can help to avoid over fi tting to training data and, as a result, construct a more accurate ensemble.

This approach was used during the Australasian Data Mining 2009 Analytic Challenge competition achieving the best overall result.

This paper is arranged as follows: This section brie fl y discuss the problem of combining predictive models. Section 2 describes the Australasian Data Mining 2009 Analytic Challenge and gives some insights of a nature of data used in that competition. Section 3 presents a general idea behind Genetic Meta-Blender and the following Section 4 shows implementation details of the model for the purpose of AusDM X 2009 Analytic Challenge. The results achie ved in that competition are given in Section 5 where additionally, a detailed evaluation of GMB, on a dataset from SIAM SDM X 11 Contest, is presented. Finally, Section 6 summarizes this research and draws some possible directions for the future work. 2. AusDM 2009 analytic challenge
Australasian Data Mining 2009 Analytic Challenge was a special event of the AusDM 2009 Confer-ence that took place on 1 X 4 December 2009 in Melbourne, Australia. The challenge was related to the problem of ensembling 3 and it was divided into two tasks. Three datasets were made available for both tasks, each consisting of a different number of expert models that made predictions of movie ratings from the Net fl ix database. The models were provided by The Ensemble and BellKor X  X  Pragmatic Chaos  X  X he two teams that placed fi rst and second in the Net fl ix competition [24].

For the fi rst set of tables the task was to minimize the root mean squared error of made predictions and for the second set the task was to maximize the area under the ROC curve (AUC score). The quality measure used in this task was a Gini coef fi cient which can be computed as Gini =2  X  X  AU C  X  0 . 5 | . The datasets from the fi rst task (later on called RMSE) were labeled with the actual movie ratings  X  integers from the set { 1000 , 2000 , 3000 , 4000 , 5000 } . In the second task (later on called AUC), the data tables were labeled with binary decision attributes whose meaning was slightly different for each table. In both tasks the available datasets had three sizes. The small datasets contained 30000 movie ratings described by 200 expert models. Results achieved on those datasets were posted on a leaderboard which was publicly available durin g the compe tition and they were not ta ken into account in the fi nal ranking. The medium and the large datasets had 40000 and 100000 movie ratings respectively and were described by 250 and 1151 predictors. All the datasets were divided into a training and a test sets in proportion of 50% / 50% ratings each. The true label values were available only for the ratings from the training sets. Figure 2 shows distributions of scores achieved by individual models from the large data tables of the RMSE and AUC tasks. They were computed on the training sets. It is interesting to notice that only 5 models from the RMSE task (  X  0 . 4% )and 10 models from the AUC task (  X  0 . 9% ) performed better than the ensemble made by simply averaging all the available models (the blue lines on the plots). Addi-tionally, on average, 4 only 17 models from the RMSE task (  X  1 . 5% )and 36 models from the AUC task (  X  3 . 1% ) were superior to a random ensemble which was constructed by averaging 10 randomly chosen predictors. In comparison to the scores achieved by the ensembles made of the best 10 models from each task (the green lines), the median scores of the random ensembles (the light blue lines) were lower only by  X  1 . 05% and  X  1 . 64% , respectively. The question arises: is there a better way of combining models in the ensemble than simple averaging? Australasian Data Mining 2009 Analytic Challenge was meant to fi nd an answer to that question. 3. Genetic Meta-Blender  X  A general idea
The main idea of the Genetic Meta-Blender is simple: instead of averaging or assigning weights based on performances of individual predi ctors, the GMB utilizes the genetic algorithm to optimize proportions between models and materialize the fi nal blend. This optimization is done by searching for an appropriate set of weights of the models, which approximately maximizes a prede fi ned scoring function. Other parameters of the ensemble (for example, parameters of the voting scheme) can be tuned along the weights. The selection of the pr oper scoring function should be dic tated by the quality measure which will be used to evaluate the performance of the ensemble. For example, if the task is to minimize the root mean squared error of the predictions then the following scoring function should be maximized: predicted target value of the i -th training example made by the j -th model in the ensemble. Of course, if effectiveness of the model is measured using different loss function, the scoring function of the GA should be changed accordingly. A proper de fi nition of the scoring function is crucial for performance of the resulting ensemble. It is worth noting that, by adjusting the scoring function and appropriately de fi ning the chromosomes, the GA can also be used to perform non-linear optimization.

Other techniques could also have been used to optimize parameters of the ensemble for particular tasks (loss functions). There are plenty robust algorithms minimizing the squared-error loss (for example [25]), different methods can be used to maximize the accuracy or the AU C score (e.g. the RankBoost algorithm described in [26]). Nevertheless, a main advantage of the genetic approach is that it makes GMB very fl exible, as it provides a single frame that can be utilized regardless of a task and a quality measure used for evaluation of the model. For example, in AusDM X 2009 Analytic Challenge, GMB was used for the classi fi cation and the regression task. Alt hough in each of the tas ks the quality of fi nal solutions was evaluated based on different criteria (RMSE and AUC), the necessary modi fi cation of the blending algorithm was narrowed to the scoring function. The GMB can be employed to construct an ensemble, which is close to optimal for a given task, simply by changing the scoring function. Moreover, this fl exibility of the GA makes it possible to de fi ne a scoring function and chromosomes in such a way that the GA would, in fact, tune parameters of a different optimization algorithm.

Another advantage of using the GA for optimization is that it provides an insight on a stability of the outputted solution. Usually (that depends on a stopping criterion), the last population contains several different chromosomes with very little differences in scores. In real-life applications, the fi nal model can be constructed using a chromosome selected by an expert. Alternatively, several suboptimal models may be computed using top N chromosomes and their predictions can be combined by averaging or majority voting. As a result, in practice (as it shown in the next section), the GA-based optimization is often able to yield better-generalized mode ls and lead to more accura te predictions on test data. Additionally, the GMB is easy to implement as independent parallel tasks, which makes it computationally feasible even for large datasets.

In comparison to a different approach that utilizes the GA to combine predictive models, described in [22,23], the genetic optimization in GMB is independent from generation of the weaker predictive models. The input data for GA needs to be prepared prior to learning of the parameters and may consist of predictions made by a diverse set of classi fi cation or regression models.

To learn the optimal set of parameters of the model it is necessary to prepare a suf fi cient number of training data. In order to avoid over fi tting, it is recommended to compute predictions of all learning algorithms which are being utilized for the whole training data using the cross-validation technique. Additionally, models which were constructed for each cross-validation fold can also make predictions for the test data. Those predictions may be combined in the fi nal ensemble to make it even less vulnerable to over fi t. An exemplary scheme of the GMB method is given in Appendix. It corresponds to the way the GMB was used in the experiments described in Section 5. 4. GMB in practice
In this section the approach which were used in the AusDM 2009 Analytic Challenge is discussed. As it was described in Section 2, the data in this competition consisted of predictions made by numerous models. This fact made a good opportunity to verify ho w well a multi-level ensemble combined with the GMB optimization method will perform.

In the challenge, the assessment of samples from the score data sets was conducted in two steps. First, for each training set, a wide range of predictive models was constructed. The GMB scheme showed in Appendix was implemented in the RSystem [27]. For the AUC task, popular classi fi cation models available in standard R libraries were tried: linear and logistic regression models (library stats ), neural networks (library nnet ), recursive partitioning trees (library rpart ), k -NN (library class ), the random forest (library randomForest ) and the generalized boosting models (library gbm ). The linear, logistic and neural network models were additionally averaged over multiple runs on different attribute subsets. The neural networks had one hidden layer which contained 1 to 5 neurons. The recursive partitioning trees were bagged and a few values of the complexity parameter were tried. The k -NN algorithm was used as a scoring model, several k values between 50 and 150 were used. The generalized boosting models were fi tted with the bernoulli , gaussian and the adaboost (exponential) loss functions.

For the RMSE task, due to lack of time for experiments with the parameters settings, only linear regression models and simple neural networks were used.

Each model X  X  prediction values for samples from the training sets were acquired by the cross-validation test and used in the second step as an input for the blending algorithm. The genetic algorithm used different scoring functions for each of the tasks. It tried to directly maximize the AUC or minimize the RMSE by assigning importance levels (weights) to models in the ensemble. The population of chro-mosomes was coded as a list of vectors of weights. In the experiments, the population size was set to 500 . The weight of each model was treated as a gene. The total number of models included for the GA optimization (a chromosome size) in the fi nal submission was dependent on the size of the datasets and the task. It was limited to 20 for the small and medium AUC data, to 25 for the large AUC data and to 10 for all sizes RMSE data. The restriction on number of models was introduced to avoid over-fi tting. Some other precautions, such as a restriction on the granularity of the weights of the models were also taken. The fi nal set of weights was computed as an average of 10 best chromosomes from the last population.
Figure 3 presents a schema of the genetic optimization process in GMB (a genetic cycle). The proba-bilities of the replication, mutation and crossover operations for a particular chromosome were computed using the roulette wheel selection technique, based on a distribution of scores ( fi tness values) in the pop-ulation. Exact copies of the chromosomes chosen for replication were taken to the next generation. The chromosomes which were chosen for mutation were randomly modi fi ed on a small number of genes (the genes were also chosen at random) and added to the new generation. Next, the chromosomes chosen to crossover were randomly matched in pairs to produce two offspring. The new chromosomes were com-puted as a weighted averages of the parent chromosomes with a ratio of 2:1 and 1:2 , respectively. Finally, scores of the new generation members were computed and the chromosomes with lower scores were eliminated so that the size of the population do not exceed the starting value (which was set to 500 in all experiments). In this way the selection of a chromosome was not directly dependent on its fi tness but instead, it was conditioned on its ranking in the population. It is also worth noting that this approach is more computationally ef fi cient than the traditional one, in which all chromosomes undergo the genetic operations and only their selection depends on the score, since it requires evaluation of a lower number of chromosomes in each iteration of the genetic cycle.

The GA algorithm was stopped when the averaged quality of the population members did not change signi fi cantly in 5 consecutive generations. In practice, this stopping criterion led to generation of popula-tions containing several different chromosomes with similar scores. In all experiments the algorithm con-verged relatively fast, the total number of generations never exceeded 50 and usually oscillated around 20 . The exact computation complexity of the GA is dif fi cult to assess, since it strongly depends on the utilized scoring function and the st opping criterion. For example, when optimizing the squared error loss by simply adjusting weights of base predictors, the computation cost of a single genetic cycle is linear in terms of number of objects and population size. In practice, the execution times of the GA were comparable to other meta-learning algorithms used in experiments. 5. Experimental evaluation
This section presents results of experiments in which GMB was applied to combine predictive mod-els created for the purpose of two data mining comp etitions: Australasian Data Mining 2009 Analytic Challenge and SIAM SDM X 11 Contest: Prediction of Biological Properties of Molecules from Chemical Structure.

Figure 4 shows the fi nal results of the presented method in the AusDM Challenge. They were com-puted by the organizers of the competition on the test parts of datasets (true values of the target attributes from test sets were not revealed to the contestants). The scores achieved by the GMB on the medium and large datasets are compared to several baseline results provided by the organizers after completion of the challenge. Beside the average of all experts, the best expert and the average of top 10 experts, the per-formance of 3 other meta-models are given. The light blue and dark blue bars indicate the scores of two approaches which were particularly popular (e.g. during the Net fl ix competition). The fi rst one employs a linear regression model (or logistic regression for the AUC task) as a meta-learning algorithm which combines decisions of experts in the ensemble. The second one is similar, in a sense that it also utilizes the linear or logistic regression models but in thi s method, those predictors are additionally bagged and constructed for different subsets of attributes to create a second-level ensemble. The results of those two models have been provided by the organizers of the challenge. The third of the compared baseline approaches, denoted by the green bars in the fi gure, is a straight average of the meta-models which went into the optimization by the GMB. Those scores were included to verify how much prediction accuracy was gained by the usage of the genetic optimization method.
 The GMB achieved the best overall score among all participants and was awarded with the Grand Champion Prize. More details about the AusDM 2009 Analytic Challenge, tables with the scores of all competitors and brief descriptions of the lead ing methods can be found at the http://www.tiberius.
The second series of experiments was conducted on data from SIAM SDM X 11 Contest, which was associated with 2011 SIAM International Conference on Data Mining. It was organized by Simula-tions Plus, Inc. and hosted by the TunedIT web platform [28]. Data in this competition consisted of descriptions of a chemical compounds structure, which were derived using the ADMET Predictor(TM) program. The task in this challenge was to predict whether a molecule has a desired biological property (a binary classi fi cation). A kind of the property in question and the exact meaning of the conditional attributes was not revealed during the contest.

The dataset contained 1046 objects in total but only the training part of 674 molecules with known decision values was used in the experiments. The objects were represented by real-valued vectors of length 242 . There were no missing values in the data. A distribution of the binary decision class was considerably uneven (a 4:1 ratio) and this fact was considered to be one of the main dif fi culties in the challenge.

Because of the imbalanced deci sion values, in the SIAM SDM X  11 Contest a qua lity of submitted solutions was evaluated using a modi fi ed version of Youden Index: where TP is a number of true positive predictions  X  molecules that were rightfully assigned to the posi-tive decision class, FN (false negatives) corresponds to a number of molecules that were falsely labeled with the negative decision value, TN (true negatives) is a number of compounds correctly classi fi ed to the negative decision class and FP (false positives) is a number of objects mislabeled with the positive decision value. In other words, Balanced Youden Index is the minimum from accuracies within differ-ent decision classes. This measure was used to evaluate results in the second series of experiments for consistency. The data from this particular data mining competition was chosen for experiments for a few reasons. Due to its small size it was possible to accurately evaluate performance of several methods for combining predictive models using the cross-validation test. The second reason was the performance measure used in this competition to assess the quality of competing models  X  it was interesting to observe how well different algorithms can deal w ith such an unorthodox criterion. Fina lly, the competitive setting of the challenge gives one more opportunity to compare the GMB with the state-of-art.

In the fi rst part of the experiment 9 classi fi cation models were constructed. They were implemented in the R System using popular libraries. RandomForest1 , NNEnsemble , LogEnsemble and GBM (with the bernoulli loss function) models were designed just as the corresponding models from the original experiment on AusDM X 2009 Challenge data. Additionally, several other classi fi cation methods were implemented:  X  SVM : the Support Vector Machine scoring model.  X  RandomForest2 :the Random Forest scoring model for which the set of available features was ex- X  k -NN_FS: the k -NN scoring model with attribute selection based on the correlation fi lter.  X  k -NN_DW: the double weighted k -NN model. In this model, local and global weights were assigned  X  DRBS :the Dynamic Rule-based Similarity model. It is a CBR model in which a similarity measure
Classi fi cation was derived from the scores of the models by setting a cut-off value below which objects were assigned to the second decision class. This value was experimentally set to all predictors in order to maximize their performance on training data.

Performances of those models, in terms of Balanced Youden Index ( BY I ), were estimated using the 10 -fold cross-validation schema which was repeated 10 times using different data splits. Table 1 shows a comparison of scores achieved by the individual predictors.

The genetic optimization of the blend was performed with exactly the same parameter settings as for the AusDM X 2009 data (see Section 4). However, unlike in the previous experiment, the genetic algorithm searched not only for the best set of predictor weights but also tried to fi nd a cut-off value which would balance accuracies for different decision classes. The values of weights assigned to the individual predictors are presented as the last column in Table 1. It is interesting to notice that there is little correlation between performance of the models and their weights in the ensemble (the Spearman X  X  should not be based on the performance of individual models.

In the second part of the experiment the GMB was compared to several other algorithms for combining predictive models. The majority voting classi fi cation was treated as a baseline. In the second voting method, the predictions of individuals were weighted by their correlation with the true classi fi cation measured on the training set. Among other meta-learning algorithms, the logistic regression and the arti fi cial neural network ensembles were chosen for the tests. Those particular models were selected because they had been tested as meta-learning algorithms in studied of other researchers ([18,22], the Net fl ix Competition and KDD C up X 2010 reports). They were constructed within the same framework as the GMB using the same base predictors. The logistic regression ensemble consisted of 500 logistic models trained on different subsets of objects and base classi fi ers. Their predictions ware averaged and the fi nal classi fi cation was assign using manually tuned cut-off value. In the neural network ensemble 600 models were computed on 200 different subsets of objects and base predictors (for each subset 3 neural networks were trained with randomly initialized connection weights). Their fi nal classi fi cation was decided analogically to the logistic ensemble. The fi nal scores achieved by the compared ensembles are presented in Table 2.

The GMB ranked fi rst among the tested models. Its BY I score was signi fi cantly higher than score of any other ensemble. In fact, the GMB was the only algorithm which signi fi cantly 6 outperformed the majority voting ensemble. Another interesting observation is that performances of other meta-learning algorithms are rather disappointing in comparison to the baseline. For example, the neural network ensemble is, on average, only slightly better than the best individual predictor ( RandomForest2 )andit is signi fi cantly worse than the majority voting. This may be due to complexity of the evaluation criteria used in this challenge, which required non-linear optimization of the ensemble. 6. Conclusions
The main scope of this paper was the Genetic Meta-Blender  X  a method of optimizing an ensemble of multiple predictive models using the genetic algorithm. The general idea of this meta-model, as well as its practical application to data from two international data mining competitions, was presented. The datasets and the tasks of those challenges were also described.
 The results showed in Fig. 4 and Table 2 con fi rm usefulness of the GMB. For the AusDM X 2009 Challenge data, the genetic optimization led to a better score of the fi nal solution than the straight average in 3 out of 4 datasets. The differences between the optimized and the averaged models were generally less signi fi cant for the RMSE task, which was perhaps due to lower diversi fi cation of the utilized predictors. For those datasets, it was also noticeable that the mu lti-level ensembles outperfo rmed the single-level ones. For example, the bagged version of the logistic regression ensemble (the dark blue bars on the charts from the AUC tasks in Fig. 4) achieved  X  3 . 0% better score on the large AUC data than the model without bagging. Finally, the standard statistical or machine-learning models proved that they can be very effective as meta-learning algorithms for combining predictions in the ensemble.

Detailed evaluation of the GMB on the SIAM SDM X 11 Contest data additionally highlights the large potential of this method, especially in situations when the quality of a model is measured using some complex criteria. In such a case, the genetic algorithm may search not only for an optimal set of weights of models but it may also try to fi nd reasonable settings for other parameters of the ensemble (such as the cut-off values).

There is a lot of possibilities for further investigation of applications of the genetic algorithms to the problem of constructing successful committees of predictive models. For example, some classi fi cation algorithms can be more accurate in predicting whether an object belongs to a speci fi c decision class. Intelligent decomposition of data into areas in which particular classi fi ers could have different in fl uence on the fi nal decision value would greatly improve quality of the predictions. If such a decomposition method had some parameters, their values could have been tuned by the GMB in the same optimization process as the weights of the predictors. This approach, along with additional experiments on real-life data, will be the main scope of research on the GMB method in the nearest future.
 Acknowledgements
This research was partially supported by the grants N N516 368334 and N N516 077837 from Ministry of Science and Higher Education of the Republic of Poland and also by the National Centre for Research and Development (NCBiR) under Grant No. SP/I/1/77065/10 by the Strategic scienti fi c research and experimental development program: Interdisciplinary System for Interactive Scienti fi c and Scienti fi c-Technical Information.
 References Appendix
