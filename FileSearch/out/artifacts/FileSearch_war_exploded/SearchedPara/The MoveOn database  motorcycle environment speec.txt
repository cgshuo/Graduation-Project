 Joachim Ko  X  hler Abstract The MoveOn speech and noise database was purposely designed and implemented in support of research on spoken dialogue interaction in a motorcycle environment. The distinctiveness of the MoveOn database results from the requirements of the application domain X  X n information support and operational command and control system for the two-wheel police force X  X nd also from the specifics of the adverse open-air acoustic environment. In this article, we first outline the target application, motivating the database design and purpose, and then report on the implementation details. The main challenges related to the choice of equipment, the organization of recording sessions, and some difficulties that were experienced during this effort, are discussed. We offer a detailed account of the database statistics, the suggested data splits in subsets, and discuss results from automatic speech recognition experiments which illustrate the degree of complexity of the operational environment.
 Keywords Speech database Noise database Spoken dialogue interaction Open-air noise environment 1 Introduction One of the most challenging tasks in the design of dialogue systems operating in outdoor environment is the development of a noise-robust interaction interface. In human X  X achine interaction interfaces, which operate in noisy environments and where speech is the dominant modality (i.e. either speech-driven or multimodal), the performance of the speech recognition engine is crucial for the overall success of the interaction. As it is well-known, any acoustic mismatch between training and operational conditions reflects in a decreasing speech recognition performance (Gong 1995 ). The negative consequences of the acoustic mismatch, usually caused by factors such as background noise, transmission-channel induced effects, etc., require that special attention is paid for proper estimation and compensation of the variability of the operational environment. Nowadays, state-of-the-art techniques for dealing with time-varying noisy environments rely on statistical modelling of data collected from the operational environment. These data are utilized for modelling and compensating for the variability of the factors that have the worst impact on the robustness or for adaptation of the acoustic models accordingly. Thus, the robust operation of the speech interface for a certain application of interest often depends on the availability of domain-specific speech data, i.e. audio recordings from prospective users of the system, collected in the operational (or close to operational) conditions. This dependence motivated the development of various speech databases which address the needs of different application domains. In the following, we briefly mention some indicative previous work on speech databases in a vehicle environment:  X  Kawaguchi et al. ( 2000 ) described in detail the design and development of a  X  Lee et al. ( 2004b ) described the development of Korean corpora in a car  X  Lee et al. ( 2004a ) detailed the procedures followed towards building a speaker  X  SpeechDat-Car comprised the largest set of databases collected in a car
To this end, little work has been conducted towards the creation of speech corpora for the needs of speech recognition in the motorcycle environment. One such effort was made within the SmartWeb project. The German SmartWeb Motorbike Corpus (Kaiser et al. 2006 ) was recorded under the special circumstances of a motorbike ride. The motorcyclist would use the SmartWeb system to retrieve information related and dependent on his current activity. The experimental setup followed corresponded to a X  X s close as possible X  X eal human X  X achine interaction with the SmartWeb system. The hardware used to capture the speech signal was chosen with respect to the limited available space and with care to ensure a correct operation.
The development of the MoveOn motorcycle speech and noise database was motivated by the lack of suited speech databases in the British English language, offering recordings in a motorcycle on-the-move environment and meeting the objectives of the MoveOn project (Kalapanidas et al. 2008 ). Our requirements led to several differences compared to the similar SmartWeb Motorbike Corpus. The language of the MoveOn database is British English. While the SmartWeb Motorbike Corpus focuses on natural spoken queries for spoken dialogue systems assisting in different situations on the motorbike, the MoveOn database has its focus closer to command and control applications for police forces. The MoveOn requirements allow a local processing of speech. Thus, we avoided wireless speech transmission and used a cabled setup instead of Bluetooth and UMTS, which caused several drop outs in the SmartWeb setup. In an earlier work (Winkler et al. 2008 )we briefly outlined the main design solutions of the database under development and offered a preliminary account for the collected speech and noise recordings. In the present work, we offer a comprehensive description of the completed database with detailed account for the statistics, annotation procedures and results, data splits, etc., and with reference speech recognition results, indicating the complexity of the problem. The remainder of this article is organized as follows: Sect. 2 is devoted to the MoveOn application and the objectives set to the speech and noise database. In Sect. 3 the design characteristics and the contents of the corpus are detailed. It covers the design of the linguistic contents of the database, the audio prompts and the equipment setup. In Sect. 4 we describe the implementation of the database. This includes a description of the actual data collection equipment, the recording procedure followed, the results of the noise and the speech annotation procedures, and the database organization and distribution of its content. In Sect. 5 we offer results from automatic speech recognition experiments, which serve for illustrating the degree of complexity of the environmental conditions. Finally, Sect. 6 concludes this work with comments about the applicability and availability of the MoveOn database. 2 The MoveOn setup In this section we briefly introduce the MoveOn concept and outline the MoveOn application, as the objectives set to the MoveOn spoken interaction conditioned the database design (Sect. 3 ) and defined the context for the database implementation (Sect. 4 ). 2.1 The MoveOn application Among the main objectives of the MoveOn 1 project was the development of a robust multi-modal and multi-sensor low-distraction dialogue interaction system that supports information access and operational command-and-control protocols for the two-wheel police force in the UK (Kalapanidas et al. 2008 ). The information support is obtained either remotely from the control centre in the police station, or locally through the functionality provided by the wearable computing environment developed within the project. This environment offers several functionalities such as navigation support, accessing the local user-specific repository, storing video and audio streams for reporting and evidence collection purposes, automated logging and diary capabilities, information recall and storage on request, visualization and alert mechanisms, communication with colleagues on the road or in flying vehicles, etc. The remote information access guarantees command, control, and guidance support as well as access to forensic and other police databases located at the central police station. The modular design also offers opportunities for integration of extensions such as automated plate number recognition, etc.

In brief, the MoveOn multimodal interaction system is a wearable solution initially designed to consist of a helmet, jacket and handlebar device. The helmet and the jacket are connected through a flexible connector located just below the scruff of the neck. A scroller-based interface was designed for the handlebar of the motorcycle for haptic control. The helmet incorporates microphones, headphones, visual feedback, a miniature camera and some supporting local-processing electronics. It has a flexible connection to the jacket that provides the power supply and the data and control interfaces. The jacket incorporates the main processing power, storage repository, communication equipment and power capacity of the wearable system, but also a number of sensors, an LCD display, and some vibration feedback actuators. Auxiliary microphone and headphone are integrated in the upper part of the waist, at the front side near the collar, for guaranteeing the spoken interaction and communication capabilities when the helmet is off.

The multimodal user interface developed for the MoveOn application consists of audio and haptic inputs, and audio, visual and vibration feedbacks to the user. The concept for non-obtrusiveness of interaction is implemented through a user-driven interaction strategy, situational awareness and accounting for the cognitive load of the user at each specific instance. Information is channelled back when the user has the capacity to attend and is ready to receive using the channels which are the most appropriate in the specific situation. Due to the specifics of the MoveOn application, involving hands-busy and eyes-busy motorcyclists, speech is the dominating interaction modality, especially when the user is on the move.

The spoken interface consists of multi-sensor speech acquisition equipment, speech pre-processing, speech enhancement, speech recognition, and text-to-speech synthesis components, which are integrated into the multimodal dialogue interaction framework based on the Olympus/RavenClaw framework (Bohus et al. 2007 ; Bohus and Rudnicky 2003 ), but extended for the needs of multimodal interaction. Specifically, the Olympus/RavenClaw dialogue interaction framework, successor of the CMU Communicator, relies on a centralized architecture, where the central unit X  X eferred to as hub X  X rovides synchronization for the rest of the components. Each component in the system is a server by itself, i.e. ASR, TTS, speech pre-processing, speech enhancement, etc. are servers, which communicate either directly with each other or through the central hub. 2.2 Objectives of the MoveOn spoken interaction Speech input in the MoveOn system is the main modality for command and control dialogue interaction (human X  X achine interaction, HMI) as well as human X  X uman communication (HHC). Despite the development of a noise-robust helmet, when driving in high speed, i.e. when patrolling the area on a motorcycle, chasing other vehicles, etc., especially engine and wind noise can severely affect the interpretation of the spoken commands. In these occasions the MoveOn system must be capable of delivering the required information in a non-obtrusive manner or X  X f this is not possible X  X witching from HMI back to simple HHC as currently used by motorcycle police forces.
These objectives can be fulfilled only through a careful design of the spoken interface and accounting for the noise conditions of the real-world environment. The latter required the development of a corresponding speech and noise database. These data served for creating the acoustic models of the speech recognizer, and for modelling and compensating effects of the additive interferences from the operational environment. Summing up, the MoveOn Database has the purpose of providing representative speech and noise data, typical for the domain and environment, enabling a successful development and testing of acoustic modelling and noise reduction approaches for a reliable HMI on a motorcycle. Target language is British English. 3 Characteristics and contents of the database Both speech and noise data were collected simultaneously. The database covers a variety of different driving and environmental conditions from a realistic acoustic environment, recorded while professional police officers were performing simulated patrolling activities. Our design of the linguistic content targeted at the coverage of application-specific commands, along with phonetic balance. The language of the database is British English. The recording equipment was chosen with respect to the application design and the particularities of the two-wheel vehicle driving environment. 3.1 Linguistic content The linguistic content of the database was designed according to the requirements of the MoveOn application. Terminology and expressions related to the specific communication protocol used during a routine operation of the police force were covered: command words and phrases, application words and phrases, and most of their synonyms. For guaranteeing sufficient representation of all phonemes, a number of phonetically rich sentences, taken from the British English Speech-Dat(II)-FDB4000 database (Van den Heuvel et al. 2001 ), were included in the prompt sequences.

Table 1 presents the structure of an exemplary MoveOn prompt sheet, where the items  X  X W001-AW065 X  are application-specific words and phrases and correspond to a list of command and control functionalities. In order to increase the frequency of appearance of the specific items, they appear twice in each prompt sheet. Next, the items  X  X D001-BD005 X  are randomized sequences of five isolated digits in one utterance. Application words-phrases correspond to a list of command and control functionalities. Mandatory words and their synonyms correspond to the words-phrases used during an operation for defining current status/taking an action, such as  X  X  X erson in custody X  X ,  X  X  X stimated time of arrival X  X . Optional words-phrases are phrases structured in the format of clause entry or statement and are used during an operation such as  X  X  X aking an area search X  X ,  X  X  X ody check please X  X ). Ten spontaneous answers to questions stated in the prompt sheet are expected from the speaker X  items  X  X P001-SP010 X . All the items described in Table 1 , except the SR ones (phonetically rich sentences), are identical for all audio prompt sheets. The SR items are different for each audio prompt sheet for guaranteeing sufficient frequency of occurrence of every phoneme. Further, all items are randomly distributed within each prompt sheet, ensuring a fair distribution of the prompts regarding the expected noise conditions along the static predefined route. 3.2 Design of the audio prompts The nature of the application setup with limited availability of manual and visual senses of the speaker while driving a motorcycle, brought up several challenges when coming to the implementation of the prompt-sheet. A previous attempt for creating databases in the vehicle environment (Kawaguchi et al. 2000 ) guided the subject by prompting phonetically rich sentences through a headset, while the whole procedure was controlled by the operator. A similar technique was followed in (Kaiser et al. 2006 ), where the person was instructed to imagine a certain situation connected with a task to solve. However, in the present case situational prompting was not feasible, as it is hard to force the user utter commands within spontaneous speech, thus the motorcyclist had to repeat the prompted word or phrase heard from the earphone attached in the helmet. For that purpose all the prompt items were recorded in studio environment by a native speaker of British English. In total, twenty-three prompt-sheets were created. Each prompt sheet starts with a short introduction, informing the speaker about the procedure he has to follow. Then there follows an initial silence, which lasts twice the duration of the introduction. Each prompt starts with a short phrase which introduces the motorcyclist if he has to repeat a word, a phrase, or answer spontaneously to a question. Every prompt ends with a DTMF tone, after which the speaker is expected to speak. The silence after each prompt lasts twice the total duration of the prompt, ensuring that the speaker will have sufficient time to pronounce the utterance at a convenient moment during the driving task.

Each prompt sheet consists of 302 prompts, obtained by one repetition of the AW items (65 prompts) ? 10 repetitions of 4 SP items (40 prompts) ? the items listed in Table 1 (197 prompts). The 4 out of 10 SP items which are selected to be repeated ten more times are those which are related to the current speed, the current location, the traffic conditions and the number plate of a nearby car. The resulting length of a typical prompt sheet is approximately 85 min. 3.3 Equipment Kaiser et al. ( 2006 ) reported various problems and drawbacks with the equipment while recording a similar database for German speech on a motorcycle. Considering both the reported issues and the objectives of the MoveOn project we defined rather hard requirements for the database and the recording equipment to realize high quality speech and noise recordings. We decided to use three microphones: two close-talk microphones attached inside the motorcycle helmet and a throat microphone placed around the neck of the speaker. That way we achieved a trade-off between limitations (due to the adverse environment with its limited space) and versatility of the database. While the close-talk microphones provide standard speech recordings with good frequency responses, the throat microphone enables additional approaches for speech processing providing a noise robust speech signal with less environmental noise but different acoustic characteristics.

In order to cope with the limited space in the helmet, the close-talk microphones needed to be small and lightweight. A good and almost linear frequency response in the relevant spectrum of speech was considered desirable. Directional microphones and specific frequency responses, which are often used for microphones in adverse environments, were not considered in order to avoid effects on the natural speech and noise spectrum. The close-talk microphones must further provide low distortion for high acoustic pressure levels in order to achieve speech signals of sufficient quality even under the extreme noise conditions anticipated on a motorcycle. Hardly any throat microphone with detailed specifications was available during the database design specification phase, so we based our decision on a preliminary test of two available models.

Although a sampling rate of 16 kHz is sufficient for automatic speech recognition, we recorded the entire database at a sampling rate of 44.1 kHz with 16 bits resolution for better capturing the properties of the noise environment. The higher sampling rate and resolution used allows utilization of the MoveOn database within research efforts on modelling the effects from the adverse acoustic environment, other than the ones considered for the MoveOn project. 3.4 Data collection campaigns We prepared and accomplished audio recordings in two different environments. In a first campaign, speech was recorded on a motorcycle in a realistic environment. In a second campaign additional sessions were recorded in a silent office environment using the identical hardware setup including the motorcycle helmet. The purpose of the second campaign was a reference collection of clean speech data recorded with the same hardware. Thus, effects caused by the helmet acoustics can also be analysed independently from interferences caused by the motorcycle environment. Several types of motorcycles and helmets were used during the data recording campaign, most of them typical for the British police forces. The list of motorcycles includes amongst others BMW RS1200, Honda Pan European, BMW K1100, and Honda GoldWing GL1800. Furthermore, we experimented in different sessions with a variety of helmets (e.g. Shoei XR1000, Schuberth C2, Shoei Multitec, etc.), which cover the typical helmets used in the daily routine.

We defined the protocol of the first recording campaign with special care to capture well the operational environment and the domain of the MoveOn application. For that purpose we chose a controlled environment in terms of a fixed route through the city and suburbs of Birmingham, UK. The route contains major environmental conditions, e.g. major and minor city roads, motorways, tunnels and country roads. This fixed route enabled a more convenient assessment and interpretation of the various environmental noise types and ensured a sufficient coverage of the major noise conditions. The route was used in both directions and the sequence of the prompt items within the prompt sheets was randomized in order to guarantee, that in different sessions a specific utterance is recorded in different environmental conditions. A video of the route was recorded in support of the database development and as a review of the characteristics of the chosen route.

For the needs of the first data collection campaign, professional police motorcyclists from West Midlands Police, UK, were recruited. All speakers were native speakers X  X ith and without area-specific pronunciation accent. Recruiting experienced motorcyclists was considered important for guaranteeing safety of the speakers, since their routine makes them less susceptible for mistakes caused by additional distraction and workload. Further, selecting experienced police officers enabled a broader understanding of police procedures and protocols, as well as common terms of communication between police officers, on which the database is mainly based. Both qualifications contributed to the quality of the database. However, a disadvantage which came in consequence of our decision was the fact that hardly any female police motorcyclist can be found, so that only male speakers were available for the outdoor recording campaign. A total of 40 recording sessions with 29 different speakers were accomplished on the motorcycle. Typically, up to two sessions were recorded by each speaker. For two very committed speakers more than two sessions are available due to hardware problems in some of their sessions. Compared to other databases (Kaiser et al. 2006 ) (36 speakers X  difficulties in recruiting female speakers) the number and characteristics of recruited speakers are satisfactory, considering the available resources within the whole project.

As the speakers from the motorcycle recordings were not available for further recordings, we recruited additional British English speakers for the needs of the second collection campaign. We selected six male and four female native speakers X  X artly with an area specific pronunciation accent X  X o increase the coverage of different speaker characteristics in the database. Except for the speakers and the environmental setup, i.e. choosing an office environment instead of the realistic motorcycle environment, we kept all parameters of the second campaign identical to the parameters of the outdoor campaign. We recorded 10 additional sessions with 10 different speakers in an office environment. 4 Database implementation The database implementation effort is documented in the following subsections, starting with the equipment setup, the recording procedures, the data annotation and the database validation, as this applies for both the first and the second campaign, with respect to the different environmental setup. Afterwards we offer a description of the database organization and define the data splits into subsets.
 4.1 Equipment setup The recording device had to be light-weight and rugged and further had to fulfill strict technical requirements including a support for TRS and XLR connectors, 48 Volts bias support for the high quality close-talk microphones, a rugged construction and battery and memory for a minimum of 90 min of high-quality audio recordings Only recording devices with up to two recording channels were available that also fulfilled these requirements, so we decided to use two audio recorders in parallel to cover all input audio channels. After preliminary tests, the ZOOM H4 2 recorder was chosen, because of its appropriate casing, good technical specifications and a multi-plug concept for connecting microphones. The ZOOM H4 device supports recordings with a sampling rate of up to 96 kHz with precision of 16 or 24 bits per sample. In the MoveOn setup we recorded all channels with a sampling rate of 44.1 kHz and 16 bits per sample. Data storage and battery power was found out sufficient for more than 90 min of continuous recording in the selected format, which was the maximum time estimated to finish the route. In addition, the ZOOM H4 also supported the required phantom power for the AKG close-talk microphones.

The two close-talk microphones were fixed firmly in the helmet, to the left and to the right of the mouth of the speaker. Using two microphones enable noise reduction approaches based on multiple channels and increase the reliability of the system, if technical problems with one of the microphones occur. The miniature lavalier microphone AKG C417 3 was selected as it is small and lightweight and provides both an almost linear frequency response and low distortion for high acoustic pressure levels. It is an omnidirectional microphone, and thus, it does not need to be directed to the mouth. The last avoids problems with a wrong adjustment of the microphones but has the drawback that environmental noise is not reduced by the microphone. Even though this is considered to be disadvantageous for the application of robust automatic speech recognition (ASR), this solution provides the opportunity to collect acoustic information about the major noise sources in the motorcycle domain. From a perceptual perspective the lack of noise reduction seems not to be a major problem for understanding of the spoken utterance, but the ASR performance is affected to some extent as we will show in Sect. 5.3 . However, the closed acoustics of full-face helmets still guaranteed a relatively good signal-to-noise ratio (SNR). The two helmet microphones were connected to the first recording device.

In addition, a throat microphone was placed around the neck and positioned on the throat of the motorcyclist, so as to capture the vibrations from the larynx. The throat microphone enabled noise-robust capturing of speech at the cost of band-limitation of the speech signal. This is because of the positioning of the throat microphone, placed at the throat directly picking up vibrations produced by the larynx instead of capturing air-borne sound. Hence, major requirement for the throat microphone was robustness towards mechanical stress, but sensitivity for vibrations caused by the vocal cords. A lack of detailed technical specifications for available throat microphones at the start of the project impeded a systematic evaluation of devices. However, two throat microphones were tested during the preparation phase, the Tork Max Throat Mic 4 and the Alan AE 38 Throat Microphone. 5 Both microphones were only available with a proprietary connector and a proprietary bias, so that an additional adaptor was build to connect the microphones to the recording device. Due to some blackouts caused by problems with the adaptor of the Tork Max Throat Mic, the Alan AE 38 Throat Microphone was finally used. The Alan AE 38 provided ear phones which were used to play back the audio prompts. In brief, the Alan AE 38 is a single transducer throat microphone with a neck strap to fixate the transducer on the larynx. A wrong or loose adjustment of the transducer leads to a distorted throat microphone signal and must be avoided. This is a problem especially for very small necks, as the neck strap does not provide enough pressure to make sufficient contact between transducer and larynx. In our setup the throat microphone was connected to the second audio recording device. The throat microphone provides a low-pass band-limited speech signal, which is nearly free of any additive interference from the environment.
 As in previous related work (Kaiser et al. 2006 ) distortion caused by the Bluetooth connection between helmet microphones and recording device was reported, we used a wired setup of the equipment to avoid this source of distortion.
In Fig. 1 , we summarize the recording setup. The first device recorded signals from the two in-helmet microphones and the second device recorded the signal coming from the throat microphone. The ear phones played back the audio prompts and were connected to the output of the first device (audio prompt channel). In addition to the three microphone channels, a channel superimposing the audio from the pre-recorded prompts and the audio from the close-talk microphones was recorded for the needs of a precise synchronization and in support of the annotation process. For every session, every signal corresponding to each microphone was recorded in one file. The synchronization of the collected material was applied manually, utilizing one signal from recorder 1 and one from recorder 2. The items used for the realization of the MoveOn database are listed in Table 2 .

The hardware setup was preliminarily tested in a laboratory environment and on a small motorcycle in a realistic environment. During this process the interaction of all devices as well as the mechanical resistance of the single components and the entire setup were tested and improved. Cabling and setup were also evaluated in terms of safety and driver X  X  distraction to avoid additional endangerment of the motorcyclist. Potential pitfalls and problems during the preliminary tests were noted in a check list to avoid these problems during the recording campaign.

The recording level of both devices was adjusted carefully to avoid clipping of speech signals. We arranged the close-talk microphones about 4 cm left and right from the mouth of the speaker in the motorcycle helmet. The distance slightly varies from helmet to helmet. Both close-talk microphones are fixated with hook-and-loop tape on the cushion of the helmet. Hence, the equipment can easily and reliably be adapted to the different helmets used during the recording campaigns. The throat microphone was put around the neck of the motorcyclist with the provided neck strap. The recording devices, as well as supplementary equipment and cables, were stored in a bag pack to keep the setup independent of the motorcycle and for guaranteeing the safety of the motorcyclist. 4.2 Recording procedures We prepared several forms and questionnaires based on the recommendation of Schiel and Draxler ( 2003 ), including a speaker protocol and a session protocol. The speaker protocol covers all relevant information about each speaker, such as information related to pronunciation accent, age, gender etc., and the session protocol covers information about the particular recording session. An introduction describing the idea and background of the data collection campaign was offered to each speaker to improve the comprehension about the task and the way to act and speak during the recording session. A recording manual and check lists were prepared to introduce and support the supervisor of the recording sessions. All necessary forms were handed to the supervisor a week in advance in order to familiarize with the procedures.

Before each recording session the speakers were introduced to the recording procedure and the route by the session supervisor. During the motorcycle recordings the supervisor was not present as any interaction on the motorcycle would have been difficult. In the office recordings the supervisor also sat next to the speaker during the entire recording procedure. The motorcyclists went either clockwise or counter-clockwise along the defined route in Birmingham. After completing the route all required information about session and speaker was filled into the protocol forms assisted by the session supervisor. The collected information include: time and date, type of helmet and motorcycle, weather conditions, technical problems, traffic conditions, deviations from the route etc. Table 3 offers details about the nature of these information. All recordings took place in 2007 and 2008. 4.3 Annotation The annotation of the MoveOn noise and speech database was realized in two parallel procedures: annotation of speech and annotation of background noise. The annotation process was performed using Praat (version 4.6.09) (Boersma 2001 ). For each session, two different annotation file-templates were given to the annotators X  one for the noise annotation and another for the speech annotation. In Fig. 2 we show a schematic overview of the annotation structure with reference to the annotation tiers used as well as some details about the information expected to be filled by the annotators. Each session was processed by one annotator, native speaker. More details about each of the two annotation processes are described in the following sections. 4.3.1 Speech annotation Three tiers are used for speech annotation: a Speaker tier with automatically extracted prompt boundaries for visual support of the annotators as well as a Words and an Affect tier.

Latest research in speech recognition points out that speech recognition performance is affected by the underlying affect in speech (Athanaselis et al. 2005 ). Thus, the inclusion of the tier Affect was considered during the design of the speech annotation. Annotation of the affective states would allow successful modelling of a range of affective states related to a variety of emotional states such as happy, relaxed, stressed, bored, etc. The annotators were asked to define the area in the activation-evaluation space (Whissell 1989 ) where the affective state of the motorcyclist can be placed based on their human intuition: Positive-Active (posa), Positive-Pasive (posp), Negative-Active (nega), Negative-Passive (negp) and Neutral (neu).

The annotation of the tier Affect revealed only a small number of utterances with emotional data {posa (39), posp (52), nega (9), negp (52)}. All the remaining instances were marked as neutral (neu). This low amount of non-neutral utterances can be explained by the fact that the speakers were not asked to act, as our main objective was to collect naturally occurring emotional speech, as it occurs during the patrolling activities.

The tier Words contains the utterance boundaries defining the area where the speaker is expected to utter the prompt. These boundaries were estimated automatically in advance and match with those existing in the speaker tier. These boundaries provide valuable help to the annotators, who were asked to refine them, and then to transcribe the uttered word or phrase. During the annotation process we followed the SpeechDat conventions (Van den Heuvel et al. 2001 ) for denoting word truncations, non-understandable speech and non-speech acoustic events. The lexicon of the speech database was created inheriting British English SpeechDat conventions with SAMPA phoneme transcriptions (Van den Heuvel et al. 2001 ; Wells 1997 ) utilizing text-to-phoneme converter. 6 The lexicon has 1411 entries and the corpus a total of 39762 running words. In Table 4 we present the phoneme frequencies in the speech corpus. The following phonemes were rare: OI, U@ and Z. Overall, the database design specifications about minimum frequency of appearance of each phoneme ((number of sessions)/10 as in Van den Heuvel 1999 ) were achieved.

In Table 5 , the number of items per category and their respective duration in seconds are presented. Here, RU items correspond to the out-of-prompt sheet transcriptions or to transcriptions where it was not possible to identify the code (the item X  X  category was obtained from the prompt channel, thus in cases where this channel was not available due to technical problems the characterization of the item was not possible). The latter occurred in sessions where the prompt channel, which is used to synchronize the sequence of the audio prompts with one of the in-helmet channels, was not recorded properly. In total, the completed database consists of approximately 6 h of segments annotated as speech. 7 4.3.2 Noise annotation We used several distinct tiers for the noise annotations: Air Wind Noise and Engine Noise for the respective noise types, Sound Event for short, transient noise {(passing) vehicle, etc.}, and Other Noise for all other, general background noise {traffic, tunnel, etc.}. Further, the state of the visor {open, closed} X  X f it was possible to determine X  X as marked in the tier Visor, while the automatically generated tier Speaker (identical to the one in the speech annotations) provided visual support about the prompt borders to the annotators.

A preliminary inspection of the data revealed a range of common noise types and sound events. Thus, for the dominant and most frequent ones, air wind and engine noise, a distinct tier was assigned. The annotators were asked to define the boundaries for the segments which contain such events, and assign intensity levels. The intensity of these events was labelled with one, two or three  X  X  ?  X  X  symbols, according to their amplitude. Successful modelling of the environmental conditions imposed marking a separate tier for the state of the helmet X  X  visor {open, closed}, as the state usually changed the acoustics, which was often clearly audible to the annotators. The annotation was performed by different annotators but validated by a single person to achieve a consistent annotation for all sessions.

The dominant background interference were noise types  X  X  X ir wind noise X  X  and  X  X  X ngine noise X  X , which usually coincide. The intensity of both types of noise is correlated as the intensity of air wind noise usually increases with increasing the velocity of the motorcycle, i.e. with the engagement of the engine. All other types of noise (annotated either in the tier  X  X  X ther noise X  X  or  X  X  X ound event X  X ) occur less frequently in the database, and their intensity is usually lower, when compared to air wind and engine noise. Table 6 shows the percentage of occurrence of the different noise types in the MoveOn database at the different intensities. For instance, for about 83 % of the recording time no  X  X  X ther noise X  X  was reported as background and for about 94 % of the recording time no  X  X  X oise event X  X  was reported, whereas air wind noise is present for more than 50 % and engine noise for more than 90 % of the recording time. 4.4 Database validation The validation procedure illustrated in Fig. 3 , adapted to the needs of the MoveOn project, followed the existing standards (Van den Heuvel 2001 ; Van den Heuvel 2000 ) and is described in detail below.

A pre-validation procedure took place right after the first recordings and annotations. The objective of this stage was to uncover serious errors in the design and implementation of the recording and annotation procedures. The result of the pre-validation process imposed repetition of the annotation of the first three sessions, towards correcting a variety of deviations from the predefined annotation conventions. Corrections and recommendations were communicated to the anno-tators to avoid further deviations in the annotation process.

Once the database annotation completed and the annotations were checked by an expert, we performed database validation. The database validation was realized with respect to the database specifications and development standards defined within the MoveOn project.

In the validation procedure, noise annotation was validated manually including a check for consistent naming of the same noise types for all sessions and a check for correct annotation based on random samples of each recording session. In the noise annotation validation the size of the analyzed sample was 20 % of the data, similar to Van den Heuvel ( 1999 ). No mistakes were detected in the noise annotation. Regarding the speech annotation, initially, automatic error spotting of the transcribed items was performed utilizing the SpeechDat British English dictionary (Wheatley and Ascham 1998 ), enriched with the MoveOn-specific vocabulary. The significant number of mistakes found imposed a detailed inspection of approxi-mately 10 % of the collected data. This inspection revealed certain types of common transcription errors in all sessions, such as transcripts with numbers, spelling mistakes. Thus, the annotators where instructed to reprocess the speech annotations of the database on its whole accordingly.

Revalidation of the later outcome was performed automatically following the same procedure utilized in the validation process. Results indicated absence of any mistakes in the speech transcriptions, thus the database was declared ready for organization and distribution. 4.5 Database organization The MoveOn database is provided with defined training and test sessions after dividing the indoor and outdoor recordings in balanced subsets, utilizing informa-tion detailed in Table 7 . We used a ratio of 80 % training and 20 % testing data. In the training/testing splits defined within this work two speakers (107, 136) appear in the test set, as they have completed more than one session, to allow both speaker-dependent and speaker-independent experimentations. Specifically, given that for a limited number of sessions during the recording campaign, data loss (in one or more audio channels) took place, we consider the data completeness as an important criterion for performing a fair split of the database. Furthermore, since both the motorcycle and the helmet type affect the environmental conditions, they were considered to be important criteria for splitting the datasets. Finally, sex information was included as criterion, while age statistics were not considered due to the limited number of speakers in the database.

Based on the general database organization described above, we defined various training and test sets to enable the evaluation of different aspects of robust speech recognition. The results were two major training and test subsets per channel, first a complete set (full set) with all available data per channel and second a core set containing only utterances that were recorded on all three channels synchronously. Thus, the core set is the most general test and training set enabling both a direct comparison of the performance of all three channels and an evaluation of robust speech recognition approaches making use of more than one microphone channel. We further define several evaluation subsets. The office and the motorcycle subsets contain training and test sets with data only from the office respectively the motorcycle recording sessions. A command and control test subset reduces the general test set to a subset of utterances, which comply with the definition of command and control phrases (AW and CP items). The training set for command and control is identical to the full core training set.

Table 8 shows the number of utterances for each evaluation set and subset. The number of utterances for each channel is lower than the total number in Table 5 , as not all sessions provide all recording channels due to failures of the recording equipment. 5 Baseline experiments The MoveOn Speech and Noise database offers the possibility of evaluating different aspects of robust speech recognition. These aspects comprise robust automatic speech recognition in realistic noise conditions, in general, as well as the MoveOn specific task of a robust command and control system, in particular. 5.1 Evaluation datasets In our evaluation, the defined subset of command and control phrases only included application words-phrases (AW) and confirmation phrases (CP).

In the following subsections we first compare the speech recognition perfor-mance on the core evaluation set and the full evaluation set for each channel separately. Further evaluations are conducted based on the core set and core subsets to enable a direct comparison of all available channels. 5.2 ASR setup A statistical approach for automatic speech recognition on subword level with hidden Markov models (HMMs) was used based on the Hidden Markov Model Toolkit (HTK). 8 We trained acoustic models from 39-dimensional feature vectors containing the first 12 Mel frequency cepstral coefficients (i.e. without the 0-coefficient), plus energy and their first and second order derivatives. Zero mean subtraction and cepstral normalization were performed. Each state of the HMM was described by 16 Gaussian mixtures with diagonalized covariance matrices. A set of acoustic models contains a monophone model for each SAMPA phoneme (plus silence and short pause models). For each channel a separate set of acoustic models was trained using the channel specific dataset.

In Evaluation I the acoustic performance is evaluated on a phoneme basis without using any lexical knowledge. Each phoneme of the SAMPA phoneme set with 44 phonemes was considered to have the same probability of occurrence. Phoneme interdependencies and lexical knowledge were not considered to evaluate only the acoustic performance of the speech recognition system. In the reported results, average accuracies are mean accuracies for all users. The user X  X  phoneme accuracy is computed as 1 PER where PER is the phoneme error rate. PER equals to the number of all phoneme errors (insertions, deletion, substitutions) divided by the total number of phonemes.
In Evaluation II we evaluated the command and control performance of the system. Therefore, an additional finite grammar was introduced and the sentence recognition performance was determined. The grammar defines a set of potentially interesting commands for human X  X omputer interaction systems for police motor-cyclists and is based on the AW and CP items of the database. The grammar incorporates 133 words and 1016 nodes and has a perplexity of about 7.3. The command and control phrases have different lengths between 1 and 11 words with an average of 3 words. Most phrases are defined by the following structure:
Essential commands like  X  X  X onfirm X  X  or  X  X  X ancel X  X  contain one word only. Usually, commands specify the device to control and the command to trigger a desired action for this device. Furthermore, some commands require a parameter to define a new value, e.g.  X  X  X adio ? change channel to ? two X  X  ( h device i ? h command i ? h parameter i ). So every item in the structure above can contain more than one word. We chose this grammar to enable an intuitive system, which is still restricted enough to enable a reliable recognition performance in the difficult environment on the motorcycle. 5.3 Experimental results First the acoustic performance of the speech recognition system was determined for the full and the core evaluation set in Evaluation I. Then the more specific command and control task based on the requirements of the MoveOn project was exemplarily evaluated in Evaluation II. 5.3.1 Evaluation I: acoustic performance The core evaluation set has a reduced amount of data compared to the full evaluation set, as only sessions available for all three channels are considered. In a channel to investigate the differences in the recognition performance, before we compare all three channels based on the core evaluation set.

In Table 9 the phoneme recognition accuracies averaged over the results for each speaker are presented for each channel and three different training and test set combinations. The first column shows the performance for acoustic models trained on the full training set and tested on the full test set for each channel. The second column presents the results for the same training set but tested on the core test set of each channel. The results for the core test set tested on acoustic models trained on the core training set are shown in the last column.

The phoneme recognition accuracy for the right and the left microphone channels is nearly identical, but the recognition performance on the throat microphone channel is distinctively lower. The results for the different training and test set combinations for each channel are almost equal, especially comparing the last two columns with results based on the same test set. The higher amount of training data in the full training set compared to the core training set seems to have no major effect on the recognition performance. Hence, we will only use the core evaluation sets for the following evaluations.

We further split the core training and test sets into two subsets containing office recordings respectively motorcycle recordings only. This step enables an evaluation of the influence of the environmental conditions on the recognition performance. Both subsets are recorded with the same hardware setup, but the office subset contains no background noise at all while the motorcycle recordings are from a realistic environment with a variety of background noises and noise levels.
These subsets are rather small (please refer to Table 8 ) and might not be sufficient to train representative acoustic models. However, several effects based on acoustic mismatch can still be shown by this approach.

In Table 10 we present the results for each channel and subset on a speaker level. The acoustic models trained on the office subset perform best for the test speakers of the office environment (speakers in sessions 005 and 010) while the acoustic models trained on the motorcycle subset perform best on the test speakers from the motorcycle domain (all other speakers). This effect is not surprising as there is no environmental mismatch between training and test set for these setups. However, the acoustic models for the throat microphone channel trained on the office subset show an equal performance on both office and motorcycle test speakers as opposed to the results for the close-talk channels. This can be explained by a rather small acoustic mismatch between both subsets due to the throat microphone technology, which does not capture as much environmental noise as standard close talk microphones. On the other hand, acoustic models trained on the motorcycle subset of the throat microphone data show a much better performance for the speakers of the same environment but a lower performance for the speakers from the office environment. Thus, the environment still influences directly or indirectly the signal captured by the throat microphone. One of the influencing factors could be the style of speaking, which is usually influenced by the environment and environmental noise, e.g. introduced by the Lombard effect (Wakao et al. 1996 ; Junqua et al. 1999 ). Furthermore, the signal quality of the throat microphone is dependent on a proper adjustment of the microphone. A poor adjustment without sufficient constant pressure between sensor and larynx (especially for smaller necks of women such as in the case of the test speaker in session 010) reduces the signal quality considerably affecting acoustic model quality and recognition performance.
 5.3.2 Evaluation II: command and control The quality of a command and control system is defined by the sentence correctness, which is directly stating the percentage of correctly recognized commands. As only valid commands can be recognized by the system, the test set has to be reduced to a subset with valid command and control utterances only (see Table 8 ).

In Table 11 we present the sentence recognition accuracy for each channel and test speaker based on this core evaluation set for command and control. The test sessions recorded in the realistic environment X  X ncluding utterances in low SNR X  show a very good performance with an accuracy rate on sentence level in the range of 94 to 100 %. The recognition performance on the throat microphone channel is slightly lower, but compared to the phoneme accuracy rates presented in Table 10 , the difference to the close-talk microphone channel performance is less. The additional lexical knowledge provided by the command and control grammar was able to correct most of the phoneme errors of the acoustic recognition process.
Next, we compared the phoneme accuracy rates for the core test set and the core test subset with command and control phrases only (please refer to Table 12 ). The recognition accuracy was determined by the setup described in Sect. 5.3.1 . The performance on the command and control subset is approximately 6 % higher for all channels. This can be explained by the design of the MoveOn database, which focused on command and control applications (AW and CP items) and thus provides higher phoneme frequencies for words typically used in these items. Consequently, phonemes that appear in the command and control utterances are better represented, and hence their acoustic models are better trained than the ones of other phonemes like rare phonemes of phonetically rich sentences in the database.

The baseline experiments for robust automatic speech recognition and command and control setups illustrated the versatility of the MoveOn database for diverse aspects of research in automatic speech recognition. These experiments also show the capability of the MoveOn database to serve as an evaluation test-bed with realistic data from the adverse acoustic environment characteristic for motorcycles on the move. Furthermore, the results of the second part of the evaluation indicate that a reliable command and control application on the motorcycle is feasible, provided that the task is restricted enough (e.g. by a simple grammar) and sufficient representative data for training acoustic models is available. 6 Conclusion In this article we presented the design and implementation of the unique MoveOn motorcycle British English speech and noise database that was collected for the needs of research and development of spoken dialogue interaction systems operating on a motorcycle on the move. This database has the primary goal to support the development of an information support command and control interface, which is specific for the operations of the two-wheel police force. However, the design of the database was kept sufficiently general which allows its use for a wider range of applications in motorcycle on the move environments. The speech and noise statistics show good coverage in terms of phoneme distribution and provide information about predominant types of background noise. The usefulness of the MoveOn database for the needs of a command and control application operating in motorcycle environments was illustrated in two exemplary evaluations presented in Sect. 5 . While the first setup reveals the difficulties of the particular environment and the different microphone channels, the second one demonstrates good accuracy on sentence level, using a rather restrictive grammar. The database is in process of being released through ELRA 9 in 2012.
 References
