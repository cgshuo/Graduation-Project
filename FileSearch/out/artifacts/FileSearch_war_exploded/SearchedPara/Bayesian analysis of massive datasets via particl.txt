 Markov Chain Monte Carlo (MCMC) techniques revolution-ized statistical practice in the 1990s by providing an essen-tial toolkit for making the rigor and flexibility of Bayesian analysis computationally practical. At the same time the increasing prevalence of massive datasets and the expansion of the field of data mining has created the need to produce statistically sound methods that scale to these large prob-lems. Except for the most trivial examples, current MCMC methods require a complete scan of the dataset for each it-eration eliminating their candidacy as feasible data mining techniques. 
In this article we present a method for making Bayes-ian analysis of massive datasets computationally feasible. The algorithm simulates from a posterior distribution that conditions on a smaller, more manageable portion of the dataset. The remainder of the dataset may be incorporated by reweighting the initial draws using importance sampling. Computation of the importance weights requires a single scan of the remaining observations. While importance sam-pling increases efficiency in data access, it comes at the ex-pense of estimation efficiency. A simple modification, based on the "rejuvenation" step used in particle filters for dy-namic systems models, sidesteps the loss of efficiency with only a slight increase in the number of data accesses. 
To show proof-of-concept, we demonstrate the method on a mixture of transition models that has been used to model web traffic and robotics. For this example we show that estimation efficiency is not affected while offering a 95% re-duction in data accesses. 
The need for rigorous statistical analysis has not gone un-noticed in the data mining community. Statistical concepts such as latent variables, spurious correlation, and problems involving model search and selection have appeared in widely noted data mining literature [6, 12]. However, algorithms, model fitting methods that actually work on massive data-permission and/or a fee. '02 Edmonton, Alberta, Canada Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. sets, have been slow to appear. 
Bayesian analysis is a widely accepted paradigm for es-timating unknown parameters from data. In applications with small to medium sized datasets, Bayesian methods have found great success in statistical practice. In particular, ap-plied statistical work has seen a surge in the use of Bayes-ian hierarchical models for modeling multilevel or relational data [3] in a variety of fields including health and education. Spatial models in agriculture, image analysis, and remote sensing often utilize Bayesian methods and invariably re-quire heavy computation (see [1] for an overview). As shown in [7], kernel methods also have a convenient Bayesian for-mulation, producing posterior distributions over a general class of prediction models. The power of Bayesian analysis comes from the transparent inclusion of prior knowledge, a more natural probabilistic interpretation of parameter esti-mates, and greater flexibility in model specification. 
While Bayesian models and the computational tools be-hind them has revolutionized the field, they continue to rely on algorithms that perform thousands even millions of laps through the dataset in order to produce estimates of the posterior distribution of the model parameters. For massive datasets Bayesian methods still begin by a "load data into memory" step, make compromising assumptions, or resort to subsampling to skirt the issue. 
If the most severe penalty comes when requesting data, algorithms might exist that only use a small manageable portion of the dataset at any one time. This paper proposes such an algorithm. It performs a rigorous Bayesian compu-tation on a small, manageable portion of the dataset and adapts those calculations with the remaining observations. The adaptation attempts to minimize the number of times the algorithm loads each observation into memory. 
Except for the simplest of models and regardless of the style of inference, estimation algorithms almost always re-quire repeated scans of the dataset. We know that for well-behaved likelihoods and priors, the posterior distribu-tion converges to a multivariate normal [4, 15]. For large but finite samples this approximation Works rather well on marginal distributions and lower dimensional conditional distributions but does not always provide an accurate ap-proximation to the full joint distribution [8 l. The normal approximation also assumes that one has the maximum like-lihood estimate for the parameter and the observed or ex-
Importance sampling is a general Monte Carlo method for 
As with importance sampling, the goal is to generate a 
Figure 1 shows the Metropolis-Hastings algorithm [13, 17], 
MCMC methods have two main advantages that make second advantage is that there is no need to compute the normalization constant of f(/91x) since it cancels out in (6). The Gibbs sampler [9] is a special case of the Metropolis-Hastings algorithm and is especially popular. If/9 is a multi-dimensional parameter, the Gibbs sampler sequentially up-dates each of the components of/9 from the full conditional distribution of that component given fixed values of all the other components and the data. For many models used in common practice, even the ones that yield a complex pos-terior distribution, sampling from the posterior's full condi-tionals is often a relatively simple task. Conveniently, the acceptance probability (6) always equals 1 and yet the chains often converge relatively quickly. The example in section 4 utilizes a Gibbs sampler and goes into further detail of the example's full conditionals. 
MCMC as specified, however, is computationally infeasi-ble for massive datasets. Except for the most trivial ex-amples, computing the acceptance probability (6) requires a complete scan of the dataset. Although the Gibbs sam-pler avoids the acceptance probability calculation, precal-culations for simulating from the full conditionals of f(/91x) require a full scan of the dataset, sometimes a full scan for each component! Since MCMC algorithms produce depen-dent draws from the posterior, M usually has to be very large to reduce the amount of Monte Carlo variation in the posterior estimates. While MCMC makes fully Bayesian analysis practical it seems dead on arrival for massive data-set applications. Although this section has not given great detail about the MCMC methods, the important ideas for the purpose of this paper are that 1. MCMC methods make Bayesian analysis practical, 2. MCMC often requires an enormous number of laps 3. given a /9 drawn from f(/91x) we can use MCMC to The last point will be the key to implementing a particle filter solution that allows us to apply MCMC methods to massive datasets. We will use this technique to switch the inner and outer loops in figure 1. The scan of the dataset will become the outer loop and the scan of the draws from f(/91x) will become the inner loop. 
So far we have two tools, MCMC and importance sam-pling, to draw samples from an arbitrary posterior distribu-tion. In this section we discuss a particular form of impor-tance sampling that will help perform Bayesian analysis for massive datasets. 
Ideally we would like to sample efficiently and take ad-vantage of all the information available in the dataset. A factorization of the integrand of the right hand side of (3) shows that this is possible when the obseryations, xi, are independent given the parameters, 0. Such conditional in-dependence is often satisfied, like in the class of hierarchical models, even when the observations are marginally depen-dent. Let D1 and D2 be a partition of the dataset so that every observation is in either D1 or D2. As noted for (1) we would like to sample from the posterior conditioned on all of the data, f(OID1 , D2). Since sampling from f(OlD1, same computations take place, in practice physically scan-ning a massive dataset is far more expensive than scanning a parameter list. However, massive models as well as massive datasets exist so that in these cases scanning the dataset may be cheaper than scanning the sampled parameter vec-tors. We will continue to assume that scanning the dataset is the main impediment to the data analysis. 
We certainly can sample from f(0[D1) more efficiently than from f(OID1,D2 ) since simulating from f(0[D1) will require a scan of a much smaller portion of the dataset. We also assume that, for a given value of 0, the likelihood is read-ily computable up to a constant, which is almost always the case. When some data are missing, the processing of an ob-servation in D2 will require integrating out the missing infor-mation. Since the algorithm handles each observation case by case, computing the observed likelihood as an importance weight will be much more efficient than if it was embedded and repeatedly computed in a Metropolis-Hastings rejection probability computation. Placing observations with missing values in D2 greatly reduces the number of times this inte-gration step needs to occur, easing likelihood computations. 1. Load as much data into memory as possible to form 2. Draw M times from f(OID1 ) via Monte Carlo or 3. Purge the memory of D1 4. Create a vector of length M to store the logarithm of 5. Iterate through the remaining observations. For each 6. Rescale to compute the weights Figure 2: Importance sampling for massive datasets 
The algorithm shown in figure 2 does have some draw-backs. While it makes great gains in reducing the number of times the data need to be accessed the Monte Carlo vari-ance of the importance sampling estimates grows quickly. The problem is easily demonstrated graphically as shown in figure 3. The wide histogram represents the sampling dis-tribution f(O[D1) that generates the initial posterior draws. 
Figure 4 shows the decay of the effective sample size for a simulated example. The data come from a three-dimensional 
Gaussian with mean 0 and covariance equal to the identity matrix. The posterior therefore concerns the three mean and the six covariance parameters. We sampled M = 1000 times from the posterior conditioned on n = 100 observations. Af-ter 300 additional data points the ESS has dropped to 10, a 99% loss in estimation efficiency from the initial Monte 
Carlo sample of M = 1000. At this point 65 of the initial 1000 draws account for 99% of the total weight. Figure 4 also overlays the ESS curve assuming a known covariance and the expected ESS curve derived next. 
The following theorem concerning the variance of the im-portant sampling weights can help us gauge the effect of these problems in practice. The theorem assumes that we observe a finite set of multivariate normal data, xi. As before we will partition the xi's into two groups, D1 and 
D2. To get accurate estimates of the mean, #, we will be concerned about the variance of the importance sampling weights,  X (#[D1, D2, ~)/ X (#[D1, ~), where  X (.) is the nor-mal density function. The theorem gives the variance of these importance sampling weights averaged over all possi-ble datase.ts with a flat prior for p. 
THEOREM 1. If, for j = 1 .... , N, 1. Xj ~ Nd(#, ~) with known covariance ~, 2. D1 = {xl .... ,x,~} and D2 = {x,~+t .... ,XN}, and 3. p'~'Nd(po, Ao) then lim ED2ED,Var,iD,,E ( X (#ID1,D2,E)'~ 
Proof: The most straightforward proof of the theorem in-volves simply computing the big multivariate Gaussian in-tegral in (18). [] 
Theorem 1 basically says that in the multivariate normal case with a flat prior the variance of the importance sam-pling weights is on average (19). These results may hold approximately in the non-normal case if the posterior dis-tributions and the likelihood are approximately normal. As we should expect, when n --N the variance of the weights is 0. As N increases relative to n the variance increases quickly. This is unfortunate in our case since we would like to use this method for large values of N and high dimen-sional problems. Looking at this result from the effective sample size point of view we see that 
If we draw M times from the sampling distribution when the size of the second partition D2 is equal to the size of the first partition D1, the effective sample size is decreased by a factor of 2 d. Although things are looking grim for this method, recent advances in particle filters sidestep this problem by a simple "rejuvenation" step. E 
Figure 4: The reduction in effective sample size with the addition of 1,000 observations. The top jagged curve assumes a known covariance while the bottom jagged also estimates the covariance. The smooth curve is the expected ESS with a known covariance. 3. PARTICLE FILTERING FOR MASSIVE 
The efficiency of the importance sampling scheme descr-ibed in the previous section deteriorates when the impor-tance weights accumulate on a small fraction of the initial draws. These 0i with the largest weights are those parameter values that have the greatest posterior mass given the data absorbed so far. The remaining draws are simply wasting space. 
Sequential Monte Carlo methods [5] aim to adapt esti-mates of posterior distributions as new data arrive. Particle filtering is the often used term to describe methods that use importance sampling to filter out those "particles," the 01, that have the least posterior mass after incorporating the ad-ditional data. All of the methods struggle with maintaining a large effective Monte Carlo sample size while maintaining computational efficiency. 
The "resample-move" or "rejuvenation" step developed in [10] greatly increases the sampling efficiency of particle filters in a clever fashion. We can iterate step 5's outer loop shown in figure 2 until the ESS has deteriorated below some tolerance limit, perhaps 10% of M. Assume that this occurs after absorbing the next nl observations. At that point we have an importance sample from the posterior conditioned on the first n-t-nl data points. Then resample M times with replacement from the 0i where the probability that 8i is selected is proportional to wl. Note that these draws still represent a sample, albeit a dependent sample, from the posterior conditioned on the first n -t-nz data points. 
Several of the 91 will be represented multiple times in this new sample. For the most part this refreshed sample will be devoid of those 0~ not supported by the data. Remember that the basic idea behind MCMC was that given a draw from f(O]xl .... , x,~+,Q ) we can generate an-other observation from the same distribution by a single 
Metropolis-Hastings step. Although this new draw will still be dependent, it will have less dependence than leaving it 9 signed to cluster c. Lastly we update the cluster assign-ments conditional on the newly sampled values for the tran-sition matrices. The new cluster assignment for sequence the probability that transition matrix Pc generated the se-quence. With these new cluster assignments we return to (21) and so the Gibbs sampler iterates. 
As noted in section 2.2, each iteration of the MCMC al-gorithm requires a full scan of the dataset, in this case two 
Figure 6 shows the results for the number of times the 
For comparison, the line at 300 in figure 6 indicates the same for both the particle filter and the full Gibbs sampler. 
Figure 6 shows a 95% reduction in the total number of data accesses when using the particle filter. 
The tick marks along the bottom mark the points at which a rejuvenation step took place. Note that they are very fre-quent at first and decrease as the algorithm absorbs addi-tional observations. The marginal posterior standard de-viation approximately decreases like O(1/x/~ ) so that the target is shrinking at a slower rate as we add more data. 
From the ESS approximation in (20) we can estimate the frequency of rejuvenation. As before, let n be the size of the initial sample. Now let Nk be the total number of obser-
For the mixture example, the effective number of parame-
While efficiency as measured with the number of data ac-duce the same results as the unweighted analysis of the mas-sive dataset. It is possible that a posterior conditioned on the pseudo-dataset may offer a good importance sampling distribution so that some combination of data-squashing, importance sampling, and particle filtering could provide a coherent solution. While clearly the method needs to undergo more empiri-cal work to test the boundaries of its limitations, the deriva-tion and preliminary simulation work shows promise. If we can generally reduce the number of data accesses by 95% 
MCMC becomes viable for a large class of models useful in data mining. The sequential nature of algorithm also allows the analyst to stop when uncertainty in the parameters of interests has dropped below a required tolerance limit. Par-allelization of the algorithm is rather straightforward. Each processor manages a small set of the weighted draws from the posterior and is responsible for updating their weights and computing the refresh step. The last advantage that we discuss here involves convergence of the MCMC sam-pler. As noted in section 2.2, the key to MCMC begins with assuming that we have an initial draw from f(01x ). While in practice the analyst usually just starts the chain from some reasonably selected starting point, the particle filter approach 'allows us to sample directly from the prior to ini-tialize the algorithm. Sampling from the prior distributions often used in practice is usually simple. Then the particle filter can run its course starting with the first observation. 
Even though subsequent steps introduce dependence, the algorithm will always generate new draws from the correct distribution without approximation. Bayesian analysis coupled with Markov chain Monte Carlo methods continues to revitalize many areas of statistical analysis. Some variant of the algorithm we propose here may indeed make this pair viable for massive datasets. [1] J. Besag, P. Green, D. Higdon, and K. Mengersen. 
Bayesian computation and stochastic systems (with discussion). Statistical Science, 10:3-41, 1995. [2] I. Cadez, D. Heckerman, C. Meek, P. Smyth, and 
S. White. Visualization of navigation patterns on a web site using model-based clustering. Technical 
Report MSR-TR-00-18, Microsoft Research, March. [3] B. Carlin and T. Louis. Bayes and Empirical Bayes Methods for Data Analysis. Chapman and Hall, Boca 
Raton, FL, 2nd edition, 2000. [4] M. DeGroot. Optimal Statistical Decisions. 
McGraw-Hill, New York, 1970. [5] A. Doucet, N. de Freitas, and N. Gordon. Sequential 
Monte Carlo Methods in Practice. Springer-Verlag, 2001. [6] J. Elder and D. Pregibon. A statistical perspective on knowledge discovery in databases. In U. M. Fayyad, 
G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, Advances in Knowledge Discovery and Data 
Mining, chapter 4. AAAI/MIT Press, 1996. [7] M. Figueiredo. Adaptive sparseness using Jeffreys prior. In Neural Information Processing Systems -
NIPS ZOO1, 2001. [8] A. Gelman, J. Carlin, H. Stern, and D. Rubin. 
Bayesian Data Analysis. Chapman Hall, New York, 1995. [9] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine 
Intelligence, 6:721-741, 1984. [10] W. Gilks and C. Berzuini. Following a moving target -Monte Carlo inference for dynamic Bayesian models. 
Journal of the Royal Statistical Society B, 63(1):127-146, 2001. [11] W. Gilks, S. Richardson, and D. J. Spiegelhalter, editors. Markov Chain Monte Carlo in Practice. 
Chapman and Hall, 1996. [12] C. Glymour, D. Madigan, D. Pregibon, and P. Smyth. Statistical themes and lessons for data mining. Data 
Mining and Knowledge Discovery, 1(1):11-28, 1997. [13] W. K. Hastings. Monte Carlo sampling methods using 
Markov chains and their applications. Biometrika, 57:97-109, 1970. [14] A. Kong, J. Liu, and W. Wong. Sequential imputation and Bayesian missing data problems. Journal of the 
American Statistical Association, 89:278-288, 1994. [15] L. Le Cam and G. Yang. Asymptotics in Statistics: 
Some Basic Concepts. Springer-Verlag, New York, 1990. [16] D. Madigan, N. Raghavan, W. DuMouchel, M. Nason, 
C. Posse, and G. Ridgeway. Instance construction via likelihood-based data squashing. In H. Liu and H. Motoda, editors, Instance Selection and Construction -A data mining perspective, chapter 12. 
Kluwer Academic Publishers, 2001. [17] N. Metropolis, A. Rosenbluth, M. Rosenbluth, 
A. Teller, and E. Teller. Equations of state calculations by fast computing machine. Journal of 
Chemical Physics, 21:1087-1091, 1953. [18] M. Ramoni, P. Sebastiani, and P. Cohen. Bayesian clustering by dynamics. Machine Learning, 47(1):91-121, 2002. [19] G. Ridgeway. Finite discrete Markov process clustering. Technical Report MSR-TR-97-24, 
Microsoft Research, September. [20] G. Ridgeway and S. Altschuler. Clustering finite discrete Markov chains. In Proceedings of the Section on Physical and Engineering Sciences, pages 228-229, 1998. [21] S. M. Ross. Probability Models. Academic Press, 5th edition, 1993. [22] D. Spiegelhalter and R. Cowell. Learning in probabilistic expert systems. In J. Bernardo, J. Berger, A. Dawid, and A. Smith, editors, Bayesian Statistics, volume 4, pages 447-466. Clarendon Press, 
Oxford, 1992. 
