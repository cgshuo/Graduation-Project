 Selecting an informative subset of features has important applications in many data mining tasks especially for high-dimensional data. Recently, simultaneous selection of fea-tures and feature groups (a.k.a bi-level selection) becomes increasingly popular since it not only reduces the number of features but also unveils the underlying grouping effect in the data, which is a valuable functionality in many applications such as bioinformatics and web data mining. One major challenge of bi-level selection (or even feature selection only) is that computing a globally optimal solution requires a pro-hibitive computational cost. To overcome such a challenge, current research mainly falls into two categories. The rst one focuses on nding suitable continuous computational surrogates for the discrete functions and this leads to vari-ous convex and nonconvex optimization models. Although efficient, convex models usually deliver sub-optimal perfor-mance while nonconvex models on the other hand require signi cantly more computational effort. Another direction is to use greedy algorithms to solve the discrete optimiza-tion directly. However, existing algorithms are proposed to handle single-level selection only and it remains challenging to extend these methods to handle bi-level selection. In this paper, we ful ll this gap by introducing an efficient sparse group hard thresholding algorithm. Our main contributions are: (1) we propose a novel bi-level selection model and show that the key combinatorial problem admits a globally opti-mal solution using dynamic programming; (2) we provide an error bound between our solution and the globally opti-mal under the RIP (Restricted Isometry Property) theoret-ical framework. Our experiments on synthetic and real data demonstrate that the proposed algorithm produces encour-aging performance while keeping comparable computational efficiency to convex relaxation models.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms Feature selection, supervised learning, bi-level learning, com-binatorics, optimization, dynamic programming
Feature selection plays a critical role in many data mining applications that handle high-dimensional data and has been one of the most active research topics in machine learning. Over the past decades, with the development of compressive sensing techniques [24, 9, 11], joint modeling of prediction and feature selection gains its popularity and draws exten-sive studies [34, 19, 1, 33, 31, 30]. In the meantime, it is also believed that when the data possesses certain grouping structures, selecting feature groups together with individual features can be bene cial [32, 26, 7, 17, 28]. In the litera-ture, simultaneous selection of features and feature groups is also referred to as bi-level selection [17, 29] and we will use these two terms interchangeably throughout the paper.
Formally, we consider the following linear model in this paper: the observations y can be generated via y = A x +  X  , where each row of A 2 R n p contains a sample and  X  represents the noise. x 2 R p is the parameter in this linear regression setting. In addition, the elements of x are divided into j G j mutually exclusive feature groups with G i denoting the indices that belong to the i th group. The target is to nd an accurate estimator x for x based on our observations of A and y . In the meantime, we expect the solution to yield sparsity in both feature level and group level, i.e., only a small number of features and feature groups are selected and thus a large potion of the elements/groups of x admits the value of zero. Mathematically, we formulate our problem in Eq. (1) below, where we attempt to nd the best solution (in the sense of least squares) among all candidates containing no more than s 1 nonzero values and taking up to at most s feature groups:
Un fortunately, computing an optimal solution of the above problem requires enumerating all elements in the feasible set and thus incurs a prohibitive cost. A natural and popular approach is to replace the discrete constraints in Eq. (1) by their continuous computational surrogates. Sparse group lasso [14] applies the classical  X  1 -relaxation on both feature-level and group-level and the resulting convex optimization problem can be efficiently solved. To enhance the quality of approximation, nonconvex relaxations using DC program-ming [28] and nonconvex penalties such as group MCP [7] and group bridge [16] are introduced, with extra computa-tion effort. The connections between various convex and nonconvex bi-level learning models are investigated in the literature [29]. On the other hand, instead of nding suit-able continuous surrogates, computing a local solution of the discrete optimization problem directly also receives plenty of attention. The iterative hard thresholding (IHT) [5, 6], or-thogonal matching pursuit [25] and group orthogonal match-ing pursuit [20] all fall into this category. Although the opti-mization is by nature nonconvex, the efficiency of these algo-rithms is usually comparable (if not better) to that of convex relaxation models. However, to the best of our knowledge, these algorithms are proposed for feature selection only or group selection only. Whether they can be extended to han-dle bi-level selection properly and efficiently has not been much explored.

In this paper, we ful ll such a gap by introducing a hard thresholding model that is capable of bi-level selection. Our main contributions are: (1) we propose a novel bi-level se-lection model and show that the key combinatorial problem admits a globally optimal solution using dynamic program-ming; (2) we provide an error bound between our solution and the globally optimal under the RIP (Restricted Isometry Property) theoretical framework [9, 8]. We have evaluated the proposed algorithm on synthetic and real data. Results show that the proposed algorithm demonstrates encourag-ing performance while keeping comparable computational efficiency as convex relaxation models.

The remaining of the paper is organized as follows: We present our algorithm for Problem (1) and discuss different variants in Section 2. In Section 3, we investigate a key sub-problem in our method and propose a dynamic programming algorithm that nds an optimal solution. The convergence property of the overall optimization framework is discussed in Section 4 and we present extensive empirical evaluation in Section 5. Section 6 concludes the paper and lists our plan of future work. For notations, we mainly follow the symbols introduced in Eq. (1), i.e., A stands for the design (sample) matrix, y is the response, x G i represents the re-gression model restricted on the i th group and f denotes the objective function.
Motivated by the iterative hard thresholding algorithm for  X  -regularized problems [6] and the recent advances on non-convex iterative shrinkage algorithm [15], we adopt the Iter-ative Shrinkage and Thresholding Algorithm (ISTA) frame-work and propose the following algorithm for solving Prob-lem (1):
In the proposed algorithm above, f denotes the objec-tive function and the \SGHT" in Algorithm 1 stands for the following Sparse Group Hard Thresholding (SGHT) Al gorithm 1 ISTA with Sparse Group Hard Thresholding Inpu t: A , y , s 1 , s 2 , &gt; 1 Output: solution x to Problem (1) 1: Initialize x 0 . 2: for m 1 ; 2 ; do 3: Initialize L 4: repeat 6 : L L 7: until line search criterion is satis ed 8: if the objective stops decreasing then 9: return x m 10: end if 11: end for pro blem with v as the input:
Like most ISTA-based optimization algorithms, it is of critical importance that we can compute the projection step accurately and efficiently. In our case, the key part is exactly the SGHT problem. Although there are well established re-sults on hard thresholding algorithms for  X  0 -regularization, adding one more constraint on group cardinality greatly complicates the problem and requires deeper analysis. We will present detailed discussion on how to compute an opti-mal solution to this problem efficiently in the next section. Before that, we rst introduce several possible variants of the proposed method. Notice that the target of Algorithm 1 is a nonconvex optimization problem. Different strategies for initialization and step-size may not only provide different convergence behavior, but also lead to a completely different solution. We consider three aspects in this paper: step-size initialization, line search criterion and acceleration option. To provide an initial value of the step-size (Line 6. in Algorithm 1), we consider two strategies: a constant value and the Barzilai-Borwein (BB) method [2]. The BB method essentially nds the best multiple of identity matrix to ap-proximate the Hessian matrix such that the least squares error of the secant equation is minimized, i.e., L k is initial-ized to wit h a safeguard bound, where  X  g =  X  f ( x k )  X  f ( x k 1 and  X  x = x k x k 1 . In this paper, we set L k = max(1 ; 
We consider two line search termination criteria in this paper, which we name as Lipschiz criterion and sufficient decrease criterion. Speci cally the Lipschiz criterion nds th e smallest L that the following inequality is satis ed: f ( x k ) f ( x k 1 )+  X  X  X  f ( x k 1 ) ; x k x k 1  X  + L
On the other hand, the sufficient decrease criterion aims to nd the smallest L such that:
Inequality (3) is the standard way for  X  1 -regularized opti-mization [3] and is applied extensively in structured sparse learning [19]. Inequality (4) and its variants are favored by most of the recent investigations on nonconvex regularized problems [4, 27, 15].
The ISTA framework has been shown to possess a conver-gence rate of O (1 =k ) for a class of  X  1 -regularized/constrained optimization problems and can be further improved to O (1 =k via adding a carefully designed search point [21, 3]. How-ever, whether the same strategy still works or makes the optimization diverge in the regime of nonconvex optimiza-tion remains unknown. In this paper we consider both of them and retain the notation of FISTA [3] to denote the ISTA with the acceleration trick. See Algorithm 2 for more detail about our FISTA.
 A lgorithm 2 FISTA with Sparse Group Hard Thresholding Inp ut: A , y , s 1 , s 2 , &gt; 1 Output: solution x to Problem (1) 1: Initialize x 1 , x 0 , 1 0, 0 1 2: for m 1 ; 2 ; do 5: In itialize L 6: repeat 7: x m SGHT( u m 1 L  X  f ( u m )) 8: L L 9: until line search criterion is satis ed 10: if the objective stops decreasing then 11: return x m 12: end if 13: end for T able 1: Speci c settings for each variant consid-ered in the paper. The last two columns denote the Lipschiz and sufficient decrease line search criterion respectively.
 V ariants FISTA ISTA BB Const Lips Dec ISTA-L  X  X  X   X  FISTA  X   X   X  FISTA-C  X   X  X  X 
T able 1 summaries different variants we consider in this paper. All these variants will be examined in our experi-ments. We conclude this section by presenting several addi-tional features of the proposed algorithm.
 Remark 1. One signi cant advantage of adhering to the discrete model is that incorporating prior knowledge about the grouping structure is quite straight-forward. Remember that the two parameters in our model are just the upper-bound of features and feature groups respectively. In ad-dition, model selection procedures such as cross-validation can be greatly facilitated since we only need to consider integer values, which are often quite small in real-world ap-plications. On the contrary, the regularizers in most of the existing works are real-valued and may not provide much insights for parameter-tuning.
 Remark 2. Although we consider our bi-level learning model in a linear regression setting, the technique can be readily extended to more general problems by choosing ap-propriate loss functions. Particularly, in order to extend our model to classi cation tasks, the widely-used logistic loss function can be applied instead of the least squares function in Eq. (1) and the proposed Algorithm 1 can be applied by changing the procedure that computes the gradient. In gen-eral, the proposed model can be extended to any convex loss functions with a simple gradient computation.
In this section, we show how to solve the SGHT problem in Eq. (2) efficiently using dynamic programming. Before presenting our algorithm, we rst explore some key proper-ties of Problem (2). As highlighted previously, the major challenge comes from the two coupled constraints. There-fore, we rst consider the special case where only one of the two constraints is present. Some straight-forward analysis leads to the following results:
Lemma 1. If only the cardinality constraint is present, the optimal solution of Problem (2) can be obtained by set-ting the p s 1 smallest (in absolute value) elements of v to zero. Similarly for group cardinality constraint, it suffices to nd the j G j s 2 smallest groups (in  X  2 -norm) and set them to zero.

Based on Lemma 1, it is also easy to verify that for any optimal solution x of Problem (2), each element x i is ei-ther equal to v i or zero, where the subscript i denotes the i th element of the vector. Therefore we have the following proposition providing an equivalent but discrete characteri-zation of the original SGHT problem:
Proposition 1. Finding the optimal solution of problem (2) is equivalent to the following Sparse Group Subset Selec-tion (SGSS) problem:
Given a set S on which a nonnegative value function f is de ned. C = f C 1 ; C 2 ; ; C j G j g is a collection of disjoint subsets of S such that S = with the maximum value such that the cardinality of S is no more than s 1 and S  X  has nonempty intersections with at most s 2 elements from C . The value of a subset is de ned as the summation of all the values of its elements.
We claim that the SGHT has an optimal solution if and only if we can nd an optimal solution for the SGSS problem. We provide a one-way reduction (the \if" part) here. The other way is almost identical. The original SGHT problem can be reduced to SGSS by simply setting S = f 1 ; 2 ; ; p with the value function de ned as f ( i ) = v 2 i for all 1 i p and C i = G i for all 1 i j G j . Suppose S  X  is the optimal solution of SGSS. Then the optimal solution of SGHT can b e readily obtained via:
In the sequel, we will focus on the SGSS problem and pro-vide an efficient algorithm to compute its globally optimal solution. The term cardinality and group cardinality are used to characterize the size of S  X  and the number of ele-ments from C with which S  X  has a nonempty intersection, respectively.

Let T ( i; j; k ) denote the maximum value we can obtain by choosing a subset S  X  , whose cardinality is no more than k and group cardinality is at most j . In addition, S  X  is only allowed to have nonempty intersection with C 1 ; C 2 ; ; C Therefore T is in essence a three-dimensional table of size ( j
G j +1) ( s 2 +1) ( s 1 +1) (the table is zero-indexed). It is easy to verify that, if we are able to compute all the values in table T correctly, the maximum value one can get in the SGSS problem is given by T ( j G j ; s 2 ; s 1 ).
Next we propose a dynamic programming algorithm to compute the table T . The motivation behind our method is the existence of optimal substructure and overlapping sub-problems [18], two major ingredients for an efficient dynamic programming algorithm. More speci cally, when we try to compute T ( i; j; k ), the optimal solution must fall into one of the two situations: whether the C i is selected or not. If not, we can simply conclude that T ( i; j; k ) = T ( i 1 ; j; k ). On the other hand, if C i is selected, we need to determine how many elements from C i are included in the optimal solution. Suppose the optimal solution takes t elements from C i , then we must have T ( i; j; k ) = T ( i 1 ; j 1 ; k t ) + CH ( i; t ), where CH ( i; t ) denotes the maximum value one can get from choosing t elements out of C i . The optimal t can be com-puted via enumeration. To sum up, the computation of T ( i; j; k ) can be written in the following recursive form: T ( i; j; k ) = max
It is clear from above that T ( i; j; k ) can be computed using only the values in the table T with smaller indices. Therefore we can compute each element of the table T in increasing or-der for each index; see Figure 1 for more detail. In addition, to further reduce the complexity, function CH ( i; t ) can be precomputed before the dynamic programming process. We present the detailed description of the proposed method in Algorithm 3. From table T , we are able to calculate the min-imum objective value of the SGHT problem, which is exactly (  X  v  X  2 solution x , all we need to know is the indices of selected elements in S and the optimal solution can be constructed through Eq. (5). We compute such information by adding one table P (stands for path) in the proposed algorithm. Speci cally, P ( i; j; k ) = 0 means the C i is not selected in the computation of T ( i; j; k ). Otherwise we set P ( i; j; k ) = arg max which is just the number of selected features in the i th group ( C i ) in the optimal solution. To recover the indices of all the selected elements, we will start from P ( j G j ; s 2 a backtracking procedure and record the number of selected elements in each group. Algorithm 4 provides a formal de-scription of this process. It accepts the table P as input and returns the cnt table which contains the number of selected elements in each group. Finally computing the optimal x only amounts to keeping the top selected elements for each group and setting the remains to zero.  X   X  [ 0 ,  X  2 ] Fi gure 1: Illustration of the order of computation for each element in T . While computing T ( i; j; k ) , we only need values in those red squares, which are lo-cated in the previous rectangle (in terms of i -axis) and of equal or smaller coordinates on axes j and k . Therefore the computation can be naturally carried out in three nested loops, one for each axis respec-tively.

We analyze the time complexity of our proposed algorithm as follows. Notice that the time needed to precompute the table CH is give by: the dynamic programming part for computing both T and P takes and the backtracking needs clearly O ( j G j ) operations. There-fore the overall time complexity is When the number of features and feature groups selected is small, the SGHT problem can be solved efficiently.
In this section, knowing that the key SGHT sub-problem can be efficiently computed, we assess the quality of the so-lution produced by the overall optimization procedure (Al-gorithm 1). Speci cally, since the constraints of Eq. (1) are nonconvex and only a local minimum can be found through our proposed method, we are interested in studying how close (in terms of Euclidean distance) the obtained solution to the optimal solution of the optimization problem (1). Al-though we are not aware of the optimal solution, the bound A lgorithm 3 Dynamic programming algorithm for SGSS Inp ut: S , C = Output: T , P 1: T 0, CH 0, P 0 2: for i = 1 to j G j do 3: sort C i in decreasing order of magnitude 4: for t = 1 to j G i j do 5: CH ( i; t ) CH ( i; t 1) + C i ( t ) 6: end for 7: end for 8: for i = 1 to j G j do 9: for j = 1 to s 2 do 10: for k = 1 to s 1 do 11: T ( i; j; k ) T ( i 1 ; j; k ) 12: for t = 1 to G i do 13: w T ( i 1 ; j 1 ; k t ) + CH ( i; t ) 14: if w &gt; T ( i; j; k ) then 15: T ( i; j; k ) = w 16: P ( i; j; k ) = t 17: end if 18: end for 19: end for 20: end for 21: end for A lgorithm 4 Linear backtracking algorithm for nding the number of selected elements in each group Inp ut: P , s 1 , s 2 Output: cnt 1: j s 2 , k s 1 2: for i = j G j downto 1 do 3: cnt ( i ) P ( i; j; k ) 4: if cnt ( i ) &gt; 0 then 5: j j 1 6: k k cnt ( i ) 7: end if 8: end for b etween our solution and the optimal one can be analyzed under the theoretical framework of restricted isometry prop-erty (RIP) [9]. A matrix A 2 R n p is said to satisfy the RIP property with constant s if the following property holds for any s -sparse vector x , i.e.,  X  x  X  0 s :
The RIP constant essentially assesses the extent to which the given matrix resembles an orthogonal matrix and theo-retical analyses often require certain upperbound on the RIP constant. It is easy to see that s is non-decreasing w.r.t s and a smaller value of s indicates more rigid conditions we require from A . In order to apply the RIP based analysis for our method, a group-RIP constant is introduced to in-corporate the group structure. Matrix A has a group-RIP constant g if for any vector x that spans no more than g groups, i.e., are satis ed:
Our next result provides an error bound between an op-timal solution of Problem (1) and the solution produced by our proposed Algorithm 1 with L xed to 1.

Theorem 1. Let x be a globally optimal solution of Prob-lem (1) and x k be the solution we obtain after the k th iter-ation in Algorithm 1 with L = 1 . If c 1 &lt; 1 2 , the following result holds: wher e e = y Ax , c 1 = min f 3 s 1 ; 3 s 2 g , c 2 = min In addition, if c 2 &lt; 1 4 , it is also true that:
Th eorem 1 clearly shows that the parameter estimation error of the proposed algorithm decreases linearly (with co-efficient of 2 c 1 or 4 c 2 ) till a xed error term is met. In addition, such an error term is proportional to the predic-tion error of the optimal solution of Problem (1). The proof of Theorem 1 mainly utilizes the technique in [12] and the details are left in the Appendix. We provide an illustra-tive example of the convergence procedure in Figure 2: if the assumptions on the (group) RIP constant hold, the se-quence generated by running our algorithm is guaranteed to converge into a region centered at x with radius at most c  X  e  X  2 , where c is a constant. As we can observe from Fig-ure 2 and Theorem 1, the difference between the unknown globally optimal solution of Problem (1) and ours is upper-bounded by a multiple of the underlying error term  X  e  X  2 In addition, such a difference cannot be canceled unless we have e = 0, in which case Theorem 1 essentially states that our method admits a linear convergence rate [22]. sel ected groups and the number of selected features deter-mine the time complexity. We conduct the evaluation in four different scenarios, each of which demonstrates the relation-ship between the running time and some particular factors while keeping other factors unchanged. Speci c settings are listed in Table 2.
 Table 2: Experiment setup for evaluation of SGHT Fi xed variable # Group # Feature s 1 s 2
Figure 3 demonstrates the running time (in seconds) of our SGHT algorithm of all four scenarios. Speci cally, the nearly at curve in our third experiment corroborates with the theoretical result that the number of groups is not a major factor of the time complexity. In other cases, our al-gorithm exhibits its capability of handling large-scale appli-cations. Particularly, when only a small number of features and feature groups are wanted, as is the common situation in high-dimensional variable selection, our algorithm is ca-pable of computing a globally optimial solution for SGHT with a performance competitive to its convex computational surrogate such as the soft-thresholding [10].
We study the convergence behavior of different implemen-tations of our discrete optimization approach proposed in Section 1. The evaluation is carried out on a collection of randomly generated data sets ( A ; y ). Speci cally, we gen-erate A 2 R n p ; y 2 R n , where the values of n and p are chosen from the following set: f (100 ; 2000) ; (100 ; 5000) ; (1000 ; 20000) ; (1000 ; 50000) All of the p features are partitioned into groups of size 100. The value of s 2 is selected from f 0 : 1 j G j ; 0 : 2 j select 10% and 20% groups. s 1 is set to 5 s 2 , which leads to the effect of within-group sparsity.

For all of the variants, we terminate the programs when either the relative change of objective value in two consec-utive iterations or the gradient of the objective is less than a given threshold. The objective values of up to the rst 100 iterations as well as the running time for each variant are reported in Figure 4. The results demonstrate the effect of BB to initialize the step-size. Both ISTA with lipschiz line search criterion (blue in Figure 4) and FISTA (black in Figure 4) deliver superior performance, particularly for large data sets and large number of selected groups/features.
We examine the proposed bi-level method on synthetic data which consist of both group selection and bi-level vari-able selection. The data generation follows the procedures recommended in the literature [32, 29]: the data set is gen-erated via the linear model y = A x +  X  , where both of the design matrix A 2 R 100 200 and the noise term  X  follow a normal distribution. The ground truth x is partitioned into 20 groups of equal size. In addition, two kinds of grouping structure are considered in this experiment; see Figure 5 for more detail. The goal is to obtain an accurate (in terms of least squares) estimator of x that also preserves the grouping structure, given only A and y . Fi gure 5: Illustraion of the grouping effect in the ground truth model x . Both cases include redun-dant groups (group 7 to group 20). In addition, the rst case contains a bi-level sparsity. The val-ues within each group are identical, as shown in the color map.

State-of-the-art bi-level feature learning algorithms, in-cluding the convex sparse group lasso, two fractional mod-for group selection) and DC approximation approach [28], are included for comparison. It is worth mentioning that the DC approach deals with exactly the same formulation as ours but resort to using continuous computational surro-gate. In addition, we also include orthogonal matching pur-suit (OMP) and group orthogonal matching pursuit (gOMP) in the experiments as they provide baseline results for dis-crete optimization approach. For both fractional models, we choose 5 regularizers from the interval [10 8 ; 10 2 ]. For DC approach and our method, s 2 is selected from f 2 ; 4 ; 6 ; 8 ; 10 and s 1 is chosen from the set of f 2 s 2 ; 4 s 2 ; 6 s 2 Since the parameters of OMP and gOMP are just the num-ber of selected features and feature groups respectively, we set f 6 ; 12 ; 18 ; ; 60 g as the candidate parameter set for OMP and similarly f 2 ; 4 ; 6 ; ; 10 g for gOMP. Five-fold cross-validation is carried out to choose the best parameter for each method. The tuned models are then tested on an i.i.d testing set. Following the setups in previous work [7, 28], the number of selected groups/features, the number of false pos-itive selections and false negative selections and the running time (in seconds) are reported in Table 3. We can observe that the approaches with discrete parameters (OMP, gOMP, DC approach and our method) deliver more accurate esti-mation on the number of groups and features, compared to regularization-based approaches. Particularly, our method demonstrates the best performance in the bi-level selection tasks and is second only to gOMP in the scenario of group selection. The low false positive rate means that redundant groups are effectively screened. However, this could lead to a relatively high but still reasonable false negative rate. Such a phenomenon is also observed in existing work [7]. As of efficiency, it is expected that OMP and gOMP are the most efficient methods due to their cheap and small num-ber of iterations. Among others, our method requires the least amount of running-time. In addition, the DC approach, which needs to re ne the continous surrogate within each it-eration, requires the most computational effort (nearly twice of the time of our method). We conclude the experiment section with a study on the Boston Housing data set [13]. The original data set is used as a regression task which contains 506 samples with 13 fea-tures. Furthermore, to take into account the non-linear rela-tionship between variables and response, up to third-degree polynomial expansion is applied on each feature, as sug-gested in previous works [23]. Speci cally, for each variable x , we record x , x 2 and x 3 in the transformed data and gather them into one group. We randomly take 50% of the data as the training set and leave the rest for testing. The pa-rameter settings for each method follow the same spirit in our last experiment and are properly scaled to t this data set. We t a linear regression model on the training data and report the number of selected features, feature groups as well as the mean squared error (MSE) on the testing set in Table 4. Five-fold cross validation is adopted for parameter tuning and all the results are averaged over 10 replications. We can observe from the table that our method shows the best prediction results with the least amount of features and feature groups.
In this paper, we study the problem of simultaneous fea-ture and feature group selection. Unlike existing meth-ods which are based on continuous computational surrogate for the discrete selection problem, we focus on the discrete model directly. Systematic investigations are carried out on optimization algorithms, convergence property as well as empirical evaluations. The proposed model delivers supe-rior performance in both group selection and bi-level vari-(the histograms).
 Table 4: Comparison of performance on the Boston Housing data set. All the results are averaged over 10 replications.
 a ble selection settings and possesses signi cant advantage on efficiency, particularly when only a small number of fea-tures and feature groups are demanded. In addition, due to the discrete parameters, model selection procedures such as parameter tuning can be greatly facilitated. We plan to extend our method to more challenging biomedical applica-tions, particularly those with block-wise missing data.
This work was supported in part by NIH (R01 LM010730) and NSF (IIS-0953662, MCB-1026710, and CCF-1025177). [1] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. [2] J. Barzilai and J. M. Borwein. Two-point step size [3] A. Beck and M. Teboulle. A fast iterative [4] E. G. Birgin, J. M. Martnez, and M. Raydan.
 [5] T. Blumensath and M. E. Davies. Iterative [6] T. Blumensath and M. E. Davies. Iterative hard [7] P. Breheny and J. Huang. Penalized methods for M ethods [8 ] E. J. Candes. The restricted isometry property and its [9] E. J. Candes and T. Tao. Decoding by linear [10] D. Donoho. De-noising by soft-thresholding.
 [11] D. L. Donoho. Compressed sensing. Information [12] S. Foucart. Sparse recovery algorithms: sufficient [13] A. Frank and A. Asuncion. UCI machine learning [14] J. Friedman, T. Hastie, and R. Tibshirani. A note on [15] P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye. A [16] J. Huang, S. Ma, H. Xie, and C. Zhang. A group [17] J. Huang, T. Zhang, et al. The bene t of group [18] C. E. Leiserson, R. L. Rivest, C. Stein, and T. H. [19] J. Liu, S. Ji, and J. Ye. SLEP: Sparse Learning with [20] A. C. Lozano, G. Swirszcz, and N. Abe. Group [21] Y. Nesterov et al. Gradient methods for minimizing [22] J. Nocedal and S. J. Wright. Numerical Optimization . [23] G. Swirszcz, N. Abe, and A. C. Lozano. Grouped [24] R. Tibshirani. Regression shrinkage and selection via [25] J. A. Tropp and A. C. Gilbert. Signal recovery from [26] L. Wang, G. Chen, and H. Li. Group scad regression [27] S. J. Wright, R. D. Nowak, and M. A. Figueiredo. [28] S. Xiang, X. Shen, and J. Ye. Efficient Sparse Group [29] S. Xiang, L. Yuan, W. Fan, Y. Wang, P. M.
 [30] Y. Xu and D. Rockmore. Feature selection for link [31] J. Ye and J. Liu. Sparse methods for biomedical data. [32] M. Yuan and Y. Lin. Model selection and estimation [33] T. Zhang. Multi-stage convex relaxation for feature [34] H. Zou and R. Li. One-step sparse estimates in
Proof. Let w k denote x k  X  f ( x k ). It is clear that where the last inequality comes from the optimality of x k +1 After eliminating  X  x w k  X  2 2 from both sides we can obtain: =2  X  x k A T ( Ax k y ) x ; x =2  X  x k A T ( Ax k ( Ax + e )) x ; x =2  X  ( I A T A )( x k x ) A T e ; x =2  X  ( I A T U A U )( x k x ) A T e ; x 2(  X  I A T U A U  X  2  X  x k x  X  2 +  X  A x 2( c 1  X  x k x  X  2 + where the set U is the union of support of x , x k and x k +1 and the last inequality is from the fact that the spectral norm of I A T U A U is upperbounded by j U j [6]. The rst conclusion then follows from expanding the last term and compute the power series.

To prove the second result, a ner treatment of the set U above is needed. Speci cally, we consider the following four sets: and it is easy to veryfy that: Therefore we can conclude that: where the rst inequality is from our proof of the rst result and we apply the Cauchy inequality to obtain the last in-equality. The proof is completed by expanding the last term and computing the resulting power series.
