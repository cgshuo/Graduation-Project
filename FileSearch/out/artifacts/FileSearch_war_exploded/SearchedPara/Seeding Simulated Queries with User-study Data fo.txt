 In this paper we perform a lab-based user study (n=21) of email re-finding behaviour, examining how the charac-teristics of submitted queries change in different situations. A number of logistic regression models are developed on the query data to explore the relationship between user-and contextual-variables and query characteristics includ-ing length, field submitted to and use of named entities. We reveal several interesting trends and use the findings to seed a simulated evaluation of various retrieval models. Not only is this an enhancement of existing evaluation methods for Personal Search, but the results show that different models are more effective in different situations, which has implica-tions both for the design of email search tools and for the way algorithms for Personal Search are evaluated. H.3.3 [ Information Search and Retrieval ]: Measurement,Experimentation, Human Factors Personal Search, Email Re-finding, User Study, Evaluation
It is well documented in the literature that people reg-ularly need to re-access and re-use information that they have created or accessed in the past [11, 27, 29] and that existing desktop management and search tools are inade-quate to support this activity effectively, resulting in huge frustration and waste of resources [1, 4, 5, 28]. Personal Information Access (PIA) as a research area focuses on pro-viding search solutions to help people re-find information they have seen or accessed previously or information relat-ing to themselves [16]. The need for better tools to support PIA is clear. However, there are two main research issues that must be addressed to facilitate progress in this area:
First, little is known about the behaviour of users in re-sponse to information re-access needs, i.e. the behaviour that search tools should support. An understanding of what people need and how they act in order to achieve those aims is essential to know how to best support user behaviour, either through algorithmic or interface support.

Second, new tools and algorithms are difficult to evaluate scientifically given a lack of open and accepted test collec-tions and evaluation frameworks for Personal Search. Al-though progress in this area has been made [3, 20, 21], the current state of the art approach suffers from a number of limitations and is not widely used as a result.

In this paper we contribute to resolving these issues. First, we perform a user study to learn about the characteristics of user querying behaviour when re-finding email messages and the factors which can influence the types of queries peo-ple submit. We examine the relationship between user and contextual variables and query characteristics including the length of the query, the field submitted on and the use of named entities. Second, we utilize the findings of this study to improve the query simulation process as applied in the literature [20, 21]. We analyse the performance of several retrieval algorithms based on simulated query profiles gener-ated from the user study data. Taking this approach allows us to investigate and understand the relationship between user behaviour and algorithmic support for re-finding.
The remainder of the paper is structured as follows: Sec-tion 3 presents the user study, detailing the study design and data analyses; Section 4 describes the seeded evalua-tion process, explaining the choice of collections, the query generation method, and the retrieval models tested; Section 5 presents the experimental results; Section 6 discusses the scope and limitations of our work. Finally, Section 7 outlines our main conclusions and plans to extend the work in the future. First, to motivate the work and provide a platform to discuss our findings, we summarize appropriate related work.
Previous work has shown people to re-access and re-find information regularly. For example, 60-80% of web page visits are re-accesses [23] and roughly 40% of web searches are performed with the aim of re-finding something seen before [27]. Desktop search tool logs show that on average users submit over 4 queries per day and email is the type of media people re-find most often [11, 10].

Re-finding queries tend to be much shorter than typical web queries. Dumais and her colleagues [11] and Cutrell and his colleagues [10] both report average queries lengths of 1.6 terms compared to the well documented 2.3 for web search. Tyler and Teevan [29] found that web re-finding queries had on average 12.1 characters compared to 18.9 for queries to find new results. Desktop search logs show that re-finding queries rarely contain advanced query operators with as little as 7.5% of the queries containing features, such as boolean operators, phrases, or field restrictions specified [11]. Named entities play an important role in re-finding with queries often containing references to people. A quarter of queries in [11], for example, contained people X  X  names.
Context is another important aspect of re-finding. What people tend to remember [13] and how difficult they per-ceive re-finding tasks to be [15] both vary in different sit-uations. Given these relationships, it is possible that dif-ferent behaviour will be exhibited in these situations as has been shown previously for web search engine behaviour. In web search, when users are faced with a difficult task, they start to formulate more diverse queries, use advanced op-erators more, and spend longer examining result pages [2]. If similar behavioural changes occur when performing diffi-cult re-finding tasks then it would important consequences for the way behaviour is simulated in automated evaluation approaches for Personal Search. Here we seek to identify im-portant situational variations to inform the choice of tasks in simulated evaluations.
The difficulties in evaluating Personal Search behaviour are well-documented [17, 19]. In addition to major privacy and participation issues, there is a lack of shared resources, such as test collections and tasks. The main approaches to date have been naturalistic investigations with specific tools [11, 10, 9], case studies for particular scenarios [19] and lab-based user studies [7, 25, 17]. While all of these approaches offer advantages, none are suited to the controlled, repeat-able evaluation of search algorithms.

Methods for automated evaluation have also been pro-posed. For example, Chernov and colleagues [8] suggested that researchers volunteer their own personal data to cre-ate a shared test collection for research purposes. Kim and Croft use pseudo-desktop collections that share the proper-ties of personal collections to avoid privacy issues[20]. Three separate collections were created from the TREC Enterprise Track X  X  W3C dataset, by taking the email messages for three prominent people in the collection and augmenting them by gathering related documents of various types from the Web. A further collection was created by using public university documents relating to individuals. These datasets have since been publicly released.

Using their collections, Kim and Croft create known-item retrieval tasks based on simulated queries [3]. Simulated queries are a potentially powerful method of scientific eval-uation for Personal Search. However, there are problems with current implementations, which are over-simplified and make assumptions about user behaviour that are not neces-sarily true. For example, query terms are typically drawn in-dependently from the document and either do not make use of field information [3] or assume that all fields are equally likely to be queried on [20]. Further, current implementa-tions do not incorporate what we already know about user behaviour (e.g. that people often make use of named entities in queries).

It seems likely to us that the kinds of queries submitted will change in different scenarios. Re-finding behaviour is guided by user recollections [6] and people remember differ-ent things in different situations, with this being heavily in-fluenced by contextual factors [13]. We hypothesize that the types of queries submitted will change in different situations. If this is the case, not only should this be incorporated in simulations, but it may mean that the type of algorithmic support required will also vary situationally, with obvious implications for search tools.

To test this hypothesis we performed a controlled user study to examine re-finding behaviour for email messages. We analyse some of the data collected to learn about the characteristics of the queries submitted and how these can change in different situations.
We decided to focus on email search tasks because look-ing for different types of documents may lead to different behaviour and therefore require a larger-scale study to in-vestigate properly. As email is the type of object re-found most often and there are appropriate collections available, we felt this would be a good starting point for research of this kind.

Our study population included 21 participants from a well-known British university, consisting of a mix of aca-demic and research staff, undergraduate computer science students and a post-graduate class with a variety of under-graduate academic backgrounds, including former business, geography, modern-languages and philosophy students. The participants had been using their collections for varying time periods, with the post-grads having relatively new collec-tions (the average age of collection was less than 3 months) and the academic staff comparatively older collections (avg. age  X  3years). Reflecting this, some of the collections con-tained few messages (min = 95) and others several thou-sand (max =8954, median = 5132). The participants also reported using email for different purposes. While the stu-dents tended to use email mainly for class announcements and collaborative working, the academics used email for a wide range of purposes, including task and contact manage-ment, data storage, version control, collaborative authoring, as well as simple communication.

We went to great lengths to establish realistic re-finding tasks for participants that could be performed on their own personal collections without invading individual privacy. This was achieved following the methodology proposed by [17] and involved performing a number of preliminary studies with the participants and their peers, including interviews, collection tours and diary studies of the re-finding tasks peo-ple in these groups perform. This work allowed us to estab-lish a pool of experimental tasks suitable for each groups of users. These pools reflected the contents of their collec-tions and simulating the kinds of re-finding tasks they may perform in a naturalistic setting. The task pools contained example tasks of each of the three types identified in [17]: Lookup tasks involved finding specific pieces of information, such as passwords or phone numbers from an email; Item tasks involved finding complete emails perhaps to print out or forward to someone else; and multi-item tasks involved re-finding multiple email messages and sometimes process-ing to content of those mails to complete the task 1 .
Each participant was allocated 9 tasks from these pools to complete on 3 systems (3 tasks per system -1 of each type), 2 search-based interfaces and a third interface where the participants could only browse through their folders to find the information required to complete tasks. Here, we only study the queries submitted to the search systems because we were interested in understanding how the characteristics of submitted queries changed in different situations. Both search interfaces provided an interface widget to select the field that the query would be submitted to, but it is impor-tant to acknowledge that the search systems were not the same, differing in the way queries were submitted and in the way that results were presented. The main difference was in the way results were presented. The first system pre-sented results as a standard list, while in the second system results were clustered graphically by date received. Full de-tails of the systems can be found in [12]. We account for differences in the systems in our data analyses below.
The responses from pre-and post-task questionnaires com-bined with the demographic data and collection statistics for the experimental population provided a rich basis to in-vestigate the variables influencing the querying behaviour Further details of the experimental design and user popu-lation can be found in our previous publications [13, 14], which analysed different aspects of participant behaviour.
In total 347 queries were submitted. The mean length of the queries was 1.48 words (max = 7). A good mixture of fields were queried on. The most commonly queried on field was the sender field (39.48% of queries contained at least one clause on sender field), the least common field was  X  X o or cc X , which only featured in 7.5% of queries. Only 13.3% of queries were submitted against all fields. Named Entities (NEs) were heavily used with 60.5% of queries containing a reference to the name of a person, place, event or thing. Peoples names were most common, featuring in 40% of all queries and 24.21% of all queries contained a NE other than a person X  X  name.

To understand the influence various contextual factors had on the characteristics of submitted queries we developed a number of logistic regression models. Logistic regression is a useful way of describing the relationship between one or more independent variables (e.g., the number of emails in a collection or the user filing strategy) and a binary response variable that has only two possible values expressed as a probability, such as ( X  X ontains a NE X  or  X  X oes not contain a NE X ). We were interested in several query characteristics including, the field the query was applied to (e.g.  X  X ontains a clause for Sender field X ), types of named entities contained within the query and query length (i.e, whether the query is longer or shorter than the mean value). There were other factors of interest in the logs. For example, spelling mistakes were obviously present in some queries and expert query
The exact experimental tasks can be found in [12].
The full questionnaires can be found in [12]. syntax was sometimes used to exploit email etiquette, e.g.  X  X wd X  or  X  X e X  in the subject line. However, examples of these kinds were too rare to be considered in the analyses.
In total 7 models were generated. All available factors (24 in total) collated from the user study were analysed initially using a stepwise procedure in order to isolate any significant relationships. The stepwise procedure automat-ically enters and removes factors at each step assessing the overall goodness of fit of the linear regression model ([22] provide an overview on generalized linear models and the stepwise procedure). As an example, Table 1 presents the regression model associated with the length of the query in words. The remaining models can be found in the Appendix. These other models are associated with the following query characteristics: hasSender (whether or not the query con-tains a clause on the Sender field), hasSubj (whether or not the query contains a clause on the Subject field), hasBody (whether or not the query contains a clause on the Body field), NE (whether or not the query contains a NE), Per-son NE (whether or not the query contains a NE person), and Other NE (whether or not the query contains a NE -other than person-). Examining these tables shows a num-ber of contextual variables, some of which are highlighted in bold. While all of these variables contribute to the model X  X  predictive power, the significant factors (marked in bold), are those that exert the biggest influence. For instance, the query length model (Table 1) shows that, in our study, hav-ing an old collection significantly influenced the length of queries submitted, while other variables, such performing the task frequently did not.

The generated models indicate that several variables had an influence on the users X  querying behaviour. Here we fo-cus on variables that featured significantly in several of the models developed: Collection age (whether the collection was new ( &lt; 1 year old), medium (up to 2 years old) or old ( &gt; 2 years old); Task temperature [26](if the sought-after information was hot (had been accessed in the last week), warm (accessed in the last month), cold (had not been ac-cessed for over 1 month) or range, where multiple emails needed to be re-found and no temperature category fitted; Task difficulty (high vs medium vs low); User experience (high vs low); and User filing strategy (filers vs no filers vs spring-cleaners [30]) 3 .

Table 2 summarizes the important findings for all of the models generated, showing the contextual variables that were significant factors in many models and how these factors in-fluenced the characteristics of submitted queries.
Examining these variables reveals several interesting trends:
Task difficulty and user experience were derived from the participants X  responses in the questionnaire. value is used when generating queries).

The main conclusions from these analyses are that: 1) querying behaviour changes in different situations, and 2) clear relationships exist between certain variables and the query submitted in terms of length, field to which the terms were submitted and the use of NEs in the query.

In the next part of the paper we use these results as the basis of what we argue is an improved query simulation pro-cess to better understand how different retrieval algorithms perform in the various situations shown to be important in these analyses.
The simulated evaluation process requires the selection of an appropriate collection. Here we use four such collec-tions. The first collection is the set of emails contained in the CS collection described in [21]. This is a collection of 806 emails from a computer science department X  X  mailing list. Three further collections were generated from TREC Enterprise track dataset by following the methodology used in [20]. This involved filtering the W3C mailing list collec-tion where the name of each person was tagged, enabling us to identify prominent individuals. We chose three such indi-viduals and isolated the messages sent and received by them to create three unique collections (W3C U1: 3943 emails, W3C U2: 3152 emails, and W3C U3: 1892 emails). These collections share many of the properties of personal collec-tions and thus seem a reasonable way to simulate personal collections without compromising privacy.
Strategies for building simulated queries have been pro-posed for known-item web page search [3] and for desktop search [20]. Essentially, they are based on randomly select-ing a document (known-item) from the collection and algo-rithmically selecting query terms from the target document. This leads to the automatic generation of simulated queries and relevance judgments. These methods have been shown to be very effective and have been evaluated successfully un-der different dimensions (i.e predictive and replicative valid-ity [3, 20]). This simulation approach is appropriate in our case because we work with email re-finding queries, which are typically known-item queries [17] and are a sub-problem of desktop search [20].

We have adapted and improved these techniques in a num-ber of ways. Current simulations are over-simplified and make assumptions about user behaviour that are not neces-sarily true. For example, query terms are typically drawn in-dependently from the document and either do not make use of field information [3] or assume that all fields are equally likely to be queried on [20]. Our analysis in Section 3.2 demonstrates that the presence of fields in real queries is not uniform and the resulting statistical models allow the gener-ation of queries incorporating appropriate statistics for the presence of terms from different fields. A further problem with current implementations is that they do not incorpo-rate other important aspects of user behaviour (e.g. that people often make use of named entities in queries). Neither [3] nor [20] consider any kind of named entity information to guide the query production process. We claim that this is problematic because the literature shows that queries for known-item tasks contain a high number of named entities [11, 10], a fact that is further evidenced in our data. There-fore, biasing the query generation towards named entities is likely to produce more realistic queries.
The data analyses in Section 3 revealed that a number of variables influenced the characteristics of submitted queries [See Table 2]. For instance, different collection ages led to different patterns in variables, such as query length or pres-ence of named entities. In contrast, other query character-istics were unaffected by this contextual variable. The data show, for instance, that different collection ages did not in-fluence the probability that the query contained a clause submitted to the Sender or Body fields, nor whether the query was more likely to contain a person or another kind of NE. The simulation process has to account for this.
Another important point to note is that we aim to com-pare an assorted set of retrieval models. This means our simulation should generate flat queries, consistent with the approaches in the literature [3, 20]. The fact that the trends in Section 3 come from analyses of query logs where the queries are often structured is not a problem because these trends can be used to infer the fields at which particular terms were targeted and, therefore, help us to produce sim-ulated flat queries with query terms in a way that reflects real-life behaviour.

In order to replicate queries associated to the situations described in Table 2 our simulation proceeds as follows: a) Obtain a general model from the complete query log that reflects the general statistics found empirically for all the target variables (length, hasSender, hasSubj, hasBody, NE, Person NE, Other NE). For instance, if we randomly draw a one-item sample from this general model we could obtain (3 , 1 , 0 , 1 , 1 , 1 , 0), which essentially says that we need to produce a 3-term query the terms for which should come from multiple fields (Sender+Body fields). The query should also contain a named entity that refers to a person. b) For each situation (i.e. each potential value for each of the variables of interest), obtain a situation-dependent model that biases the query generation process towards the pattern determined by the situation. For instance, the value of the collection age variable strongly influences the follow-ing query variables: length, hasSubj, and NE. Therefore, for the three possible values of the collection age variable (old, medium, new) we obtain three situation-dependent models that give us proper statistics for the three query variables. Again, this is computed from the distribution obtained em-pirically from our user study query logs.

Next, we produce artificial queries for each situation of interest by repeating the following process: 1. randomly draw one item from the situation-dependent 2. obtain a random item from the general model whose 3. initialise an empty query. 4. randomly select a document d i from the collection as 5. if the target query demands a NE then randomly ex-6. complete the query (up to the target length) with (non 7. record the query-doc pair in the relevance judgments
For each situation (i.e for each possible value of the vari-ables of interest [See Table 2]) we generated and evaluated 10 sets of 100 simulated queries and we report the average performance obtained over the 10 query sets.
In our experiments we evaluated an assorted set of re-trieval algorithms: a) the well-known bm25 ,whichignores any document structure and represents the email as a flat bag of words; b) Language Models (LMs) based on Query likelihood [32]. Here, we considered both Jelinek-Mercer and Dirichlet smoothing and tested the following alternatives: lm email , where the document LM is constructed from all document fields (considering the doc as plain text), and lm body , lm from , lm to ,and lm subject where the mod-els are constructed from a single field of the document (and the remaining fields are ignored); c) a well-known mixture-based model ( lmmix ), as described in [24]. This defines the The collections were preprocessed using the Stanford Name Entity Recognizer, which automatically detects NEs and distinguishes between particular types of NEs (including person, location and organization) and is available at http://nlp.stanford.edu/software/CRF-NER.shtml document X  X  LM as a a combination LMs associated to every document representation: where P ( w | d i ) is the i-th representation X  X  LM, and  X  the weight on the i-th model, with i  X  i =1.Inourcase,we build one LM for every field plus one LM for the document as a whole. This leads to a document model based on com-bining five different representations. Again, these models are smoothed with Dirichlet or Jelinek-Mercer smoothing. We experimented with the four collections described in Section 4. For training purposes, for each collection, we generated 140 simulated queries and relevance judgments by taking account each of the variables of interest (10 queries for each of the 14 situations [see 2nd column in Table 2]). This provides a diverse training set and avoids over-fitting to any particular situation (the parameters are tuned from generic queries). Training was done by parameter sweep-ing 5 , where we optimized for Mean Reciprocal Rank (MRR), which is a standard measure to evaluate known-item search algorithms. For simplicity, the smoothing configuration was fixed for each situation after training (i.e. the performance reported for the LMs refers to the LMs with the smoothing configuration, Dirichlet or Jelinek-Mercer, that was optimal in the training for the given situation).

Once the parameters for each model were fixed, the testing stage was done with new simulated queries (100 queries for each situation). To account for the randomness within the query simulation process we repeated the testing process 10 times (with 10 different 100-query sets) and we report the average MRR obtained. The collections were indexed with Indri and we removed 733 common words from the emails. No stemming was applied.

The results are presented in Table 3. For each situation, the values for the best performing model and all models that did not perform statistically poorer than the best are highlighted in bold 6 .

Despite different query profiles being used, the results show that overall lm email and lmmix were the best per-forming models. In each of the investigated situations one of these models (and often both) was (were) in the the set of best performing models. For some situations the model bm25 was included in the set of best performing models. Overall, however, bm25 is not able to compete with lm email or lmmix. The models lm email and bm25 are similar in the sense that both of them represent the document as a whole. Still, the evolved term weighting incorporated within bm25 does not seem to offer added value wrt the simpler weighting schemes implemented by LM approaches [32]. This might be
For BM25, we fixed k 3 to 1000 (the effect of k 3 is negligible for short queries like ours) and varied k 1 (from 0 to 10 in steps of 0.2) and b (from 0 to 1 in steps of 0.1). For the LMs, we varied the smoothing parameters as follows:  X  (Dirichlet smoothing parameter) from 0 to 5000 in steps of 500, and  X  (Jelinek-Mercer smoothing parameter) from 0 to 1 in steps of 0.1. For lmmix we also varied the  X  i  X  X  between 0 and 1 in steps of 0.1
Statistical significance was estimated with a paired t-test (p-value=0.05) due to the characteristics of our retrieval task, where lengths of documents and term statistics deviate strongly from those found in more standard document retrieval scenarios.
Despite many of the queries consisting of single terms and many of these being NEs, the models that represent the emails with a single field (i.e. lm from, lm to, lm subject and lm body) perform poorly and are never as effective as the best models. Overall, the results clearly indicate that lm email or lmmix are optimal for re-finding email messages. Nevertheless, there are some trends in the results that, de-pending on the contextual situation, could help us decide which of these two models to use. For instance, if the collec-tion is old then we should select lm email because it is always among the best performing models whereas lmmix is not. In constrast, for new or medium collection ages we should go for lmmix, which performs stronger overall than lm email in these contexts. Similar analyses lead us to conclude that a) lm email is preferable when the user does not use fold-ers, while lmmix is better for spring cleaners; b) lm email performs more consistently for difficult tasks and lmmix is better choice for tasks with easy or medium difficulty, and c) lm email is a good choice for re-finding messages that have not been accessed for long time periods (cold temperature tasks) whereas lmmix is better for re-finding messages that have been accessed more recently (warm or hot temperature tasks).
 Summing up, our findings reveal a quite interesting trend. It seems that when conditions are somehow difficult (the sought-after email has not been accessed for a long time, the task is perceived as difficult, users who do not file mes-sages, or the collection is old) then we should go for a simple model that does not take into account structure and simply represents the document as a bag of words (lm email). In contrast, when conditions are somehow easier, then lmmix, which is a more evolved model that considers weights for the email fields, seems to be a more suitable choice.
We would also like to note that many of these scenarios can be automatically detected (e.g. the age of the collection or the user filing strategy) and, therefore, search applica-tions can directly adapt their behaviour depending on the context. Some situations, such as the temperature of the task or the task difficulty, are more difficult to infer. Our findings seem to endorse further research on methods of au-tomatically detecting such contextual variables. This could be achieved based on learning from the user interactions with the system including factors, such as the type and number of queries submitted, query sessions, etc.
The approach taken in this paper is extremely novel. We used user study data analyses to directly seed the query simulation process for evaluating retrieval models for email search. Ideally, however, before evaluating the performance of the models, we should try to establish the validity of the simulated queries. According to Zeigler [31], there are three kinds of validation that can be performed on a simulation; predictive, structural and replicative. A model has predic-tive validity if it can produce the same data output as the real system (i.e. comparing the query terms for a given known-item from the simulated model and a real system). A model has structural validity if the way it operates is a reflection of how the real system operates. Finally, replica-tive validity is achieved if the model produces output that lm from 0.054 0.071 0.047 0.069 0.076 0.067 0.052 0.072 0.069 0.078 0.043 0.048 0.061 0.061 lm to 0.024 0.023 0.013 0.024 0.028 0.020 0.014 0.022 0.027 0.010 0.020 0.020 0.025 0.028 Figure 1: Mean performance of the retrieval models for different values of the contextual variables is similar to the output of the real system (e.g. equivalent retrieval performance).

Previous work on query simulation for known-item tasks has established validity using both predictive [20] and replica-tive approaches [3, 20]. As our simulations build on the ap-proaches used in these studies and in the case of [20] we make use of the same datasets, it is likely that the queries generated as a whole will be similarly valid. Nevertheless, it would be nice to have been able to validate our approach at the level of situation. Unfortunately, we are restricted in terms of what we can do to validate our simulated queries. We cannot perform predictive or replicative validity because these approaches require real queries along with an appro-priate test collections.

In our case, we have real queries (from the user study) but do not have appropriate collection data (privacy con-cerns mean we cannot access user study participant collec-tions). We also have publicly available test collections, but unfortunately for these collections we do not have suitable real queries against which to validate our simulated queries. What we can do with respect to replicative validity is com-pare the average performance of the evaluated models with the performance in the user study. Figure 1 shows the trends for the systems evaluation and Figure 2 provides the user study performance in terms of rate of task completion.
The results for the User experience and Task Temperature variables follow similar trends in both the system and user evaluations. This suggests that our simulations are most accurate in these situations. Also, for the filing strategy variable, in both evaluations, the spring-cleaners were the best performing group. The final two variables, Collection Age and Task Difficulty have trends that do not at all match the user study performance. It is important to note, how-ever, that the user study results are user performance figures and do not necessarily reflect query performance. Although the quality of query will likely be a good indicator of overall performance, if the user is skilled at looking through lots of Figure 2: User Study Performance in terms of per-centage of task completion. messages or in recognising relevant results this will skew the results.

It is also possible that for the Collection Age variable, the size of collection may be a larger factor in overall perfor-mance than the query generated. Older collections will most likely be larger in size and our results in Section 5 demon-strate how collection size can influence performance. The CS dataset (806 emails) was associated with much higher perfor-mance than the other collections (1892-3943 emails). This underlines the need for other ways of validating queries. It also suggests that bigger test collections are needed for this kind of work. The collections we used are equivalent in size to typical email collections [30, 13]. However, collections can be much bigger [18]. It is important that automated experiments reflect this.

There are other limitations to our work that we should mention. Although we extend previous simulations to incor-porate fields and NEs, we do not consider how discriminative the selected terms or queries are. In the past, some effective simulations [3] have been based on selecting discriminative terms from the documents (tf/idf-like term selection). Nev-ertheless, our documents tend to be short, we remove com-mon words and NEs make up a significant number of our query terms. Therefore, it is unlikely that further extending our simulations to consider the discriminative power of the terms would improve the realism of our simulation.
Another limitation of our work is that our simulation (as in [3, 20, 21]) only draws terms, named entities etc. from within the target document so the simulation does not ac-count for user error. It would be relatively simple to include terms outside the document, for instance by interpolating terms with a collection language model, but we felt it would beyond the scope of this paper to do this. Similarly, we do not consider spelling mistakes or the fact that often multi-term queries will be phrases.
This paper has made two main contributions by first, in-vestigating email re-finding queries and then second, by us-ing the findings to seed a simulated evaluation of retrieval models for email search. The main findings can be sum-marised as follows:
Although we feel that the presented work is an impor-tant starting point for the fusion of user and systems IR approaches, it is important to acknowledge that it is only the starting point. Currently we are working on ways to im-prove the methodology in order to incorporate a means of validating simulated queries. We are also looking to extend the approach to look at other kinds of personal data includ-ing visited web pages, personal files, and calender entries. In other related work we are looking at ways of implicitly detecting contextual variables from live re-finding behaviour so that search applications can make use of different retrieval models appropriately depending on the contextual situation. Table 7: Regression Model for hasNamedEntity Table 9: Regression Model for hasOtherNamedEn-tity
