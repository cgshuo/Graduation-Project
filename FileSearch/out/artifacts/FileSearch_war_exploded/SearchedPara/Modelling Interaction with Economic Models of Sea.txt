 Understanding how people interact when searching is cen-tral to the study of Interactive Information Retrieval (IIR). Most of the prior work has either been conceptual, obser-vational or empirical. While this has led to numerous in-sights and findings regarding the interaction between users and systems, the theory has lagged behind. In this paper, we extend the recently proposed search economic theory to make the model more realistic. We then derive eight inter-action based hypotheses regarding search behaviour. To val-idate the model, we explore whether the search behaviour of thirty-six participants from a lab based study is consistent with the theory. Our analysis shows that observed search behaviours are in line with predicted search behaviours and that it is possible to provide credible explanations for such behaviours. This work describes a concise and compact rep-resentation of search behaviour providing a strong theoreti-cal basis for future IIR research.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval:Search Process; H.3.4 [ Information Storage and Retrieval ]: Systems and Software:Performance Evaluation Theory, Experimentation, Economics, Human Factors Retrieval Strategies, Search Behaviour, Evaluation
How information seekers behave and interact with Infor-mation Retrieval systems is a fundamental question in the area of Interactive Information Retrieval [10]. Consider-able empirical and observational research has been under-taken which has led to various findings about users, their behaviours, their perceptions of systems and system perfor-mance [21, 23, 33, 36, 37, 38, 43, 44]. For example with respect to querying behaviour, it has been observed that users worked harder on systems that were less effective by posing more queries [33, 38] and that they could achieve their goal by posing a series of short queries [23]. How-ever, users issued fewer queries when the cost of querying increased [6, 13], while more experienced users posed fewer queries than inexperienced users [43]. Such observations are valuable, helping to piece together how users behave and act under various circumstances. However, it is difficult to link such findings together as much of the empirical and observa-tional studies have been performed independently, and have not be guided by any underlying theory [22]. Consequently, developing formal models and theories that describe, predict and explain search behaviour has been hailed as one of the grand challenges of IIR [10, 18].

While numerous models of the information retrieval and seeking process have been developed (e.g. [8, 9, 11, 15, 17, 24, 46]), they have been largely conceptual in nature show-ing where researchers should focus their attention. However, such models do not make predictions or explain observed search behaviours, i.e. they are descriptive [20]. For ex-ample, a popular descriptive model is Bates X  Berry Picking metaphor which described users as foragers, who go from patch to patch, choosing the best berries from the bushes in each patch [8]. While this metaphor might be an apt de-scription of how people interact with search results, it does not provide any insight into why people behave like this, nor does it help predict how people will behave under differ-ent circumstances. In [20], such models are referred as pre-theoretical, in the sense that they point out the relationships between factors which can be used to develop more formal and predictive models of interaction from which hypotheses can be generated. While these lines of research have been useful, the focus has mainly been on answering the question: how people behave when searching? However, a fundamen-tal question persists: why do people behave in such ways?
A promising development in the late 1990s was the intro-duction of Information Foraging Theory (IFT) [25], which sought to predict and explain various information interac-tions. Under IFT, for example, the berry picker X  X  actions are quantified mathematically in order to make predictions about how they will behave, where it is assumed that the berry picker will seek to maximise the rate at which they acquire relevant information. The theory implies that the berry picker will stay longer in a patch when it takes them longer to get there (i.e. if the cost of a query increases, then the user will examine more results). While IFT received a lot of initial interest, most research was focused on brows-ing [26, 27] rather than applied specifically to ad-hoc topic retrieval. There has recently however been a renewed in-terest in developing formal models of interaction which are mathematical and computational in nature [2, 3, 7, 6, 16, 45], that specifically focus on topic retrieval. In these works the relationship between the user X  X  interactions, the asso-ciated costs, and the benefit obtained from the system is formalised. As a consequence, the use of such models can lead to directly testable hypotheses about search behaviour; either directly from the theory or via simulations. However, such models have been criticised because they often make numerous assumptions about users [4].

In this paper, we will be focusing on the model of search based on Economic theory [2, 5]. While the initial theory provided some interesting insights and explanations about how users behave [2], it makes a number of critical modelling assumptions which detracts from its realism and applicabil-ity. Furthermore, empirical research has shown that the current theory failed to convincingly explain the observed search behaviours when tested [5]. In this work, we pro-pose a new model of search, addressing some of the limita-tions of the previous attempt. This leads to a number of deeper insights regarding search behaviours and a number of novel contributions: (1) we provide a better explanation of observed search behaviours, (2) we show how the model leads to a number of specific hypotheses about search be-haviour, (3) we explain how the search behaviour of users will change as cost and performance change, and (4) we pro-vide a comprehensive empirical analysis contrasting actual search behaviour of thirty-six participants against the theory developed. Finally, we show that the predicted relationships between cost, performance and interaction hold and the ob-served search behaviour is consistent with the theory.
Economics provides an array of tools for modelling deci-sion making, dealing with risk and handling uncertainty [39]. Early Information Retrieval (IR) research exploited such tools to examine IR systems in a number of ways ranging from purchasing decisions [1, 29] to ranking [16, 28, 41] to user behaviour [2, 12, 14]. Initial attempts focused on the trade-off between the cost of an IR system and its effective-ness. In [1, 29], Axelrod and Rotheberg compare different mechanised IR systems available during the late 1960s and early 1970s by performing a cost benefit analysis in order to decide which system to purchase. In [14], Cooper took a more user-oriented perspective. He modelled the trade-off between the amount of time a user should spend searching versus how much time the system should spend searching. In the same period, Robertson [28] examined the problem of ranking in terms of the costs and benefits of ranking one document above another. This led to the formulation of the Probability Ranking Principle (PRP) which essentially applies decision theory to the ranking problem [28]. More recently, Fuhr revised and extended the PRP to consider a series of interactions in the interactive Probability Ranking Principle (iPRP) [16]. This generalised model accounted for the different costs and benefits associated with particular choices when ranking documents.

In [40], Varian outlined three directions in which eco-nomics could be useful for search: (1) to obtain better esti-mates of the probability of relevance, (2) to apply Stigler X  X  theory on Optimal Search Behaviour to IR [35], and (3) to examine the economic value of information using consumer theory,  X  X here a consumer is making a choice to maximise expected utility or minimise expected cost X  [40]. A number of different works have begun to examine these directions. For example, in [41], Wang and Zhu used Portfolio theory to obtain better estimates of relevance by accounting for the uncertainty associated with the probability estimates when ranking. While in [12], Birchler and Butler explain how Stigler X  X  theory can be applied to search in order to pre-dict when a user should stop examining results in a ranked list. However, they did not conduct any empirical study to verify whether the theory was consistent with users ac-tual behaviour. In a variation on Varian X  X  third suggestion, Azzopardi suggested that Production Theory could be used to model the search process instead [2]. This led to the development of Search Economic Theory (SET) which has been specifically developed to model ad-hoc topic retrieval. However, there are a number of limitations and problems with this approach. In the next section, we shall provide an overview of the model and review the criticisms of the ap-proach, before proposing a new economic model of search.
In [2], the search process was modelled using an analogy to Production Theory [39]. Production Theory, also known as the theory of firms, models the situation where a firm takes inputs (such as capital and labour) and converts them to output (such as products or services). Applied to search, a user with a search engine is considered the firm, where the inputs are the user interactions and the output is the gain received from the relevant documents found during the search process. The model proposed abstracts the search process down to two main interactions: (i) querying, and (ii) assessing, and consists of two functions that characterise gain and cost. The gain function proposed in [2] was: where it is assumed that the total amount of gain (i.e. ses-sion based Cumulative Gain [19]) is determined by the num-ber of queries issued ( Q ) and the number of documents as-sessed per query ( A ), k expresses the quality of the tech-nology and how well it is used, while  X  denotes the relative efficiency of querying versus assessing. This function is of-ten referred to as the Cobbs-Douglas production function in Economics [40]. In [2], it was shown that simulated usage data on several standard retrieval models closely fitted this function. In Figure 1, we have plotted the gain curve for BM25 ( k = 5 . 394,  X  = 0 . 576, ncg = 0 . 6 [2]). Each point on the curve represents the same amount of gain, but for different combinations of the inputs, Q and A .

The following cost function was then used to ascribe a cost to each combination of inputs: where c q is the cost of issuing a query, and c a is the cost of assessing a document 1 . To show how the cost relates to gain,
Figure 1: Plot of the Gain Function for BM25. we have plotted the cost curve in Figure 2 where c q = 4 and c a = 1 for the gain curve above. According to the theory, users will seek to minimise their cost for a given level of gain (or alternatively maximise their gain for a given cost). By inspecting the cost curve plot, it is clear that the minimum cost is when A = 10, which corresponds to when Q = 18. Any other combination of inputs would result in a higher cost. In [5], the cost of a query was varied to show that as c q increases, then A increases, while Q decreases. This led to the formulation of the query-cost interaction hypothesis. While this is an intuitive prediction of search behaviour, the model makes a number of assumptions and as a result has a number of limitations.
Figure 2: Plot of the Cost Function for the BM25.
Three main assumptions of the approach were pointed out in [5]: (1) rationality and optimality, (2) the fixed interac-tion, and (3) the abstraction of the search interface. The first assumption is a common modelling assumption often em-ployed ( c.f. [9, 25, 30]). While it assumes that users will act in a rational way, it is widely acknowledged [2, 25, 30] that users are not perfect and are susceptible to biases (e.g. [42]). However, this does not mean that users do not try to act in such a manner. In [33, 38], they showed that users adapted their behaviour to the system to obtain the same level of gain. While in [27], they showed that users did tend to min-imise effort when engaging with the scatter-gather system. These findings suggest that this is a reasonable assumption, but it is clear that there is scope to improve the model on this front. However, we leave this to further work and focus on addressing more pragmatic issues first.

Next there is the assumption that user interaction is fixed, i.e. that users will examine A documents per query. Clearly, users will not always examine the same number of docu-ments for each query that they issue. In [2], it is acknowl-edged that user behaviour is not static nor is it fixed, and so they argued that this value represents the average num-ber of documents examined per query. This approximation is similar to those made by numerous evaluation measures (i.e. p@k, NDCG@100, etc [32]), where the depth is fixed. However, it would be possible to generalise the model and use a distribution to represent the number of documents per query rather than the mean. Of course, this would signifi-cantly increase the complexity of the model which may not be necessary. In its current form, though, it is possible to overcome this limitation by comparing how the search be-haviour, characterised by A ? and Q ? , would change when moving from one situations to another. For example, when the cost of querying is low on one interface, but high on another, how would search behaviour change? This is of-ten referred to as comparative statics in Economics and in the next section we use this method to generate specific hy-potheses regarding search behaviour.

Finally, this model of the search interface is quite simplis-tic and reminiscent of the TREC abstraction of a search in-terface [34]. For instance, viewing the search result pages or inspecting snippets are not considered by the model. How-ever, such interactions come at a cost to the user, and also helps them to decide what action to perform next. In this respect, the model currently assumes that these aspects do not play a significant role in shaping the user X  X  behaviour. A similar problematic assumption has made by most eval-uation measures as they ignore the influence of the inter-face [32, 37, 38]. The cost function also ignores these ad-ditional costs which are likely to have an impact on the usefulness and accuracy of the economic model. We seek to address these limitations of the model by using a more accurate model of the search interface.
In addition to these assumptions, the study conducted in [5] showed mixed empirical evidence to support the the-ory. Specifically, they tested the query-cost interaction hy-potheses using a between-subjects experimental design. On the TREC Aquaint collection with three Robust 2005 ad-hoc topic tasks, participants used a BM25-based search system but with different search interfaces. The three interfaces were Condition 1 : a structured interface which slowed query entry (high query cost), Condition 2 : a standard search box (medium query cost), and Condition 3 : a stan-dard search box along with eight query suggestions (low query cost). It was found that participants on the high cost interface issued fewer queries ( Q 1 = 19 . 4) and examined more documents per query ( A 1 = 4 . 7) than the other two conditions, thus supporting the theory. However, partic-ipants on the low cost interface, contrary to expectation, issued slightly fewer queries ( Q 3 = 31 . 2) and examined more documents per query ( A 3 = 2 . 5) than the medium cost interface (where Q 2 = 35 . 0 and A 2 = 1 . 6 ). It was hypothesised that the reason behind this finding was that participants on the low cost interface spent more time on the search results page considering the query suggestions; something that was not part of the initial model, i.e. time on search page. Also participants on the low cost interface had access to good quality query suggestions and so experi-enced higher levels of precision, on average.

These findings strongly motivate a revision of the theory, and question whether it can provide credible explanations of search behaviours. In this work, we shall develop a new model of search which is more realistic to determine whether this can improve the ability of the model to describe, predict and explain search behaviours.
Given the aforementioned limitations, we shall revise the current model by modifying the gain and cost functions to be more intuitive and reflective of the actual search process. To make such refinements, we shall assume that the search interface is much like a standard web search interface con-sisting of a query box (or query area) and search button. When a query is issued to the IR system the search result page shows: (i) the number of search results, (ii) the current page number, (iii) a list of n results (usually n = 10 results per page) and (iv) a next and previous button. Each search result has a title (often shown as a blue link), a snippet from the document, along with the URL/domain. This style of interface is usually referred to as the ten blue links.
On this interface, the user can perform a number of inter-actions: (i) (re)query, (ii) examine the search results page, (iii) inspect individual result snippets, (iv) assess documents and (v) visit subsequent results pages. Each of these actions have an associated cost and so are likely to affect search behaviour. This model of the search interface is much like those assumed in [31, 34], where the evaluation measures de-veloped in those works include the processing time and effort associated with viewing and examining snippets. For exam-ple, with Time Biased Gain (TBG) [34], the interactions the user performs are dependant on the perceived relevance of a snippet, the reading speed of users, and the probabil-ities of viewing/judging documents (and similarly with the U-measure [31]).

During the course of interaction, a user will pose a number of queries ( Q ), examine a number of search result pages per query ( V ), inspect a number of snippets per query ( S ) and with some probability p a assess a number of documents per query ( A ). Each interaction has an associated cost where c q is the cost of a query, c v is the cost of viewing a page, c is the cost of inspecting a snippet, and c a is the cost of assessing a document. Note that the costs could be time as in [34] or effort as in [31]. With this model of the search interface we can construct a new cost function that includes these additional variables and costs, such that the total cost of interaction is: c ( Q , V , S , A ) = c q . Q + c v . V . Q + c s . S . Q + c This new cost function provides a richer representation of the costs incurred during the course of interaction. How-ever, with the introduction of these additional variables it significantly increases the complexity of the model. In order to simplify the cost function, we will make a number of as-sumptions. First, we assume that V represents the average number of search pages viewed per query much like the as-sumption regarding the number of documents assessed, but somewhat weaker. This is because most of the time only one page of results is viewed. Consequently we will treat V as a constant v , which represents the mean number of pages examined per query. However, it would be possible to encode the number of page views per query more precisely by using a step function based on the number of snippets viewed, representing the fixed cost incurred to load and view each page of results 2 . However, we leave this extension for further work. Second, we shall assume that the number of documents assessed will be proportional to the number of snippets viewed, and that users will inspect a snippet before examining a document, thus S  X  A . If we let the probabil-ity of assessing a document given the snippet be p a , then the expected number of assessments viewed per query would be A = S . p a . Substituting these values into the cost model, we obtained: c ( Q , V , S , A ) = c q . Q + c v . v . Q + c s . A We can now reduce the cost function to be dependent only on A and Q , such that: As we shall see later, this leads to some interesting insights into search behaviour given changes in cost. Now to make the model more intuitive, we propose a small change to the existing gain function, shown in Eq. 1, such that: In the original model  X  represented the relative efficiency of querying and assessing. Here we have decoupled this re-lationship and address an obvious problem: if the user is-sued m queries and examined n documents per query and all n documents per query were relevant, the gain would be m  X  n . With the original gain function shown in Equation 1, it would not be possible to set  X  such that g ( Q , A ) = m . n . However, this revised function can cater for this situation by setting  X  =  X  = 1. Furthermore, with this functional form it is possible to directly estimate the gain given a particular result list (or set of). Now that we have revised the model, we can now consider what this model says about search be-haviour.
Using this model it is possible to determine what the op-timal search behaviour (in terms of Q and A ) would be given the parameters of our model. To do this we assume that the objective of the user is to minimise the cost for a given level of gain (or alternatively, maximise their gain for a given cost). This occurs when the marginal gain equals the marginal cost. We can solve this optimisation problem with the following objective function (using a Lagrangian Multiplier  X  ):  X  = ( c q + c v . v ) . Q + c s where the goal is to minimise the cost subject to the con-straint that the amount of gain is g . This is the analytical analogy to the graphical solution presented in subsection 3. By taking the partial derivatives, we obtain: and:  X   X   X  Q
Setting these both to zero, and then solving, we obtain the following expressions for the optimal number of assessments per query A ? : and the optimal number of queries Q ? :
Using this analytical solution we can now generate a num-ber of testable hypotheses about search behaviour by consid-ering how interaction will change when specific parameters in the model increase or decrease. It should be noted that we do not believe that users will act optimally. However, like previous work [2, 9, 25], we do believe that users will try to act in such a manner. We therefore expect that actual search behaviour will be in line with the predicted search behaviour. With that in mind, we present eight hypothe-ses regarding search behaviour of which three relate to how performance affects behaviour, four relate to how the costs affect behaviour and the remaining hypothesis is based on the probability of assessing documents. These hypotheses are made assuming that the user wishes to obtain a fixed level of gain and that all other things are equal.
In this subsection we define the three performance-based hypotheses. The k-performance-interaction hypothe-sis is as follows: as k increases the number of queries issued will decrease, while the number of documents examined per query will remain constant for a fixed level of gain (as shown in Figure 3). From the equations above we can see that A ? independent of k , which is why A remains constant. While if we examine Equation 10, we can see that as k tends to infinity, then Q ? tends to zero. Of course, in practice at least one query needs to be submitted to the system (i.e. Q  X  1). In [2], k is said to represent the efficiency of the user or the system in identifying or returning relevant docu-ments. So k could represent: (i) the probability of issuing a query that returns relevant documents, (ii) the probability of a user selecting a relevant document from the ranking, (iii) the precision of the result list, or (iv) some combination of these factors. This is an open question.
The  X  -performance-interaction hypothesis can be formulated as follows: as  X  increases, the number of as-sessments per query will decrease, while the the number of queries will increase. The  X  -performance-interaction hypothesis is: as  X  increases, the number of assessments per query will increase, while the number of queries will de-crease (as shown in Figure 4). Here  X  represents the qual-ity of the ranked list, while  X  represents the quality of the queries issued. There is an interesting relationship between these parameters. Since A ? must be positive then the frac-tion  X   X   X   X  also needs to be positive (assuming the costs c and c a are also positive). This implies that  X  needs to be greater than  X  . Furthermore, as  X  tends to  X  , then the fraction  X   X   X   X  tends to infinity, suggesting that assessing is preferable to querying. Restated, it is preferable to continue assessing the current ranked list in order to find more rel-evant documents, rather than issuing another query. For example, if a user posed the perfect query which returned all the relevant documents, then subsequent queries would be redundant and only increase the overall costs. Another interesting observation is that if  X  equals one or tends to one, then the queries issued are likely to be independent of each other. That is, they would be returning different relevant documents (or sets of), whereas if  X  equals zero or tends to zero, then the queries issued are likely to be similar and so would be returning relevant documents that have already been observed (and thus not contributing to the overall gain).
Next, we define the series of cost-based hypotheses re-garding search behaviour. The Query-cost-interaction hypothesis , which has already been asserted in [5], is as follows: as the cost of a query c q increases, the number of documents assessed per query will increase, while the num-ber of queries issued will decrease (as shown in Figure 5). It should be clear from the Equation 9 that this is the case be-cause as c q approaches infinity, A ? also approaches infinity. In turn, the number of queries issued will decrease, because as A becomes larger, Q ? will tend to zero. As previously mentioned, Q must be equal to one or greater.

Similarly, we can formulate the page-cost-interaction hypothesis : as the cost of viewing a page increases, the number of documents assessed per query will increase, while the number of queries issued will decrease.

The Assessment-cost-interaction hypothesis is: as the cost of an assessment increases, the number of docu-ments assessed per query will decrease, while the number of queries issued will increase. Since the assessment cost c appears in the denominator in Equation 9 then any increase will reduce the number of assessments.

Similarly for the Snippet-cost-interaction hypothesis is: as the cost of processing snippets increases, the number of documents assessed per query will decrease, while the number of queries issued will increase. Figure 5: Plot of A ? and Q ? as query cost changes.
Finally, the Assessment-probability-interaction hy-pothesis can be stated as follows: as the probability of assessing increases given the result snippet, the number of documents assessed increases, while the number of queries issued decreases (as shown in Figure 6). Here, if a user ex-amines every document in the ranked list, then p a equal one meaning that they also examine every snippet. Figure 6: Plot of A ? and Q ? as the probability of assessment changes.
Now lets consider whether this new model can provide an explanation for the search behaviour observed in [5] and previously described in subsection 3.2. Recall that in their experiment, the page costs in the suggestion interface were higher than on the standard interface (while the query costs were similar). It was also the case that when compared to the standard interface, fewer queries were issued, and more documents per query were examined (which was incon-sistent with their expectations). However, given the page-cost-interaction hypothesis this observation is to be expected as participants on the suggestion interface spent longer per page. Alternatively, the change in behaviour may have been due to differences in performance as participants on the sug-gestion interface experienced higher levels of precision (and thus an increase in  X  ). According to the  X  -interaction hy-pothesis, such a change would also result in a similar ob-servation. Consequently, the refined model provides two credible and possible explanations of the observed search behaviour, which the previous model could not.
In this section, we will undertake an analysis of the search behaviours of users to see whether actual search behaviours are consistent with the hypotheses generated in the previous section. To perform this analysis, we used the search logs from the study conducted by Azzopardi et. al. [5], which was previously described in Subsection 3.2. The search logs con-tained the transactions for 36 participants on the three dif-ferent conditions: ( 1 ) structured search interface, ( 2 ) stan-dard interface and ( 3 ) suggestion interface, where there were 12 participants per condition and each participant performed three search tasks from the TREC Aquaint Collection (344: Abuses of E-mail, 347: Wildlife Extinction, and 435: Curb-ing Population Growth).

For this analysis, we used time (in seconds) to represent the cost of the various interactions as done in [5]. We also focused on analysing the search behaviour on a topic by topic basis (which was not done in [5]). There are two main reasons for this: (i) topics are a major source of variation where users are likely to face different difficulties depending on the topic (experiencing different costs and different levels of performance because of the topic), and (ii) the model is session based representing the interaction when searching for a specific topic. Furthermore, we performed the analysis on each condition, though aggregating all conditions together resulted in similar findings.

From the search logs it was possible to extract for each user on a given topic: the number of queries issued ( Q ), the number of pages examined ( V ), the number of snippets viewed ( S ) and the number of documents examined ( A ). It was also possible to extract the time spent issuing a query ( c q ) and the time spent examining a document ( c d ). The time per snippet and time per page was not possible to di-rectly extract from the log data. However, we could extract the time spent on the search engine result page ( t serp ) from which it was possible to estimate c s and c v . This was per-formed by assuming that t serp = c s . S + c v . V , where V was set to the average number of pages (and thus a constant) and we fitted the data to this function. Table 1 reports the estimated values and the goodness of fit r .

The next set of parameters that we estimated were k and  X  . This was performed on a query-by-query basis. For each query, we calculated the Cumulative Gain [19] for each rank from 1 to 50. We then estimated the parameters for k and  X  using a least squared approximation. Note that if the cumulative gain across all ranks was zero for a given query then both k and  X  were zero. Table 1 shows the estimates for k and  X  for each condition and topic given the condition. Further note that when reporting significance we used a one-way ANOVA test for comparing means.
Now we shall examine on each of the conditions, whether the change in querying and assessing is consistent with the predicted trends. For the this part of the analysis, we have plotted the change in querying and assessing for the dif-ferent model parameters. Note that we are averaging over sessions/topics for each condition -so we are essentially as-suming that all the other variables are equal. While this is not the case in practice, we hope that the predicted trend is strong enough to be observed. k-interaction hypotheses : Figure 7 shows the plots for each condition for each topic when k (top) and  X  (bottom) changes, and how A and Q respond. If we first consider the k -plots, then we can see for conditions 2 (mid) and 3 (right), that as k increases, Q decreases  X  which was im-plied by the k -interaction hypothesis. However, the number of documents assessed also tends to increase  X  which was not implied by the hypothesis. Furthermore, if we examine the Mid: Condition 2, Right: Condition 3.
 c plot for condition 1 (top, left plot), this trend does not ap-pear to hold. For topics 344 and 347, the k values are equal, yet the A and Q values are quite different. This may be be-cause not all other things are equal. If we look at the  X  -plot for condition 1 (bottom, left plot), the  X  values for these two topics are quite different:  X  344 = 0 . 25 and  X  347 = 0 . 4. This may explain the difference observed on the k -plot. The  X  -interaction hypothesis states that an increase in  X  , would result in more documents being examined (i.e. A  X  ) and less queries being issued (i.e. Q  X  ). When we compare the two topics on condition 1, we see that for topic 344, A is 1.5 and Q is 9, while for 347, A increases to 6.45 and Q decreases to 4.42; which falls in line with the  X  -interaction hypothesis.  X  -interaction hypotheses : In terms of the  X  -plots, we see that across each condition the trend lines follow the  X  -interaction hypothesis: as  X  increases, we see that A in-creases, while Q tends to decrease. This is despite the fact that we are averaging over all sessions/users, where many other factors are varying. Intuitively, this suggests that the performance of the query characterised by  X  plays a major role in shaping the interaction of the user and that  X  appears to dominate over k , suggesting that there is an ordering to the importance of these hypotheses.

In Table 2, we have performed a deeper analysis with re-spect to  X  to determine if the number of documents exam-ined was affected by  X  . For each condition, we grouped the queries issued into those where  X  was equal to zero (i.e. no TREC relevant results were returned) and another group where  X  was greater than zero (i.e. TREC relevant results were returned). For each group, we computed the number of documents examined and used an ANOVA test to deter-mine if the means were different. For most (all but one) conditions/topics the difference was significantly different. This further shows that  X  plays a significant role in shaping interaction.

Note we have excluded reporting on the  X  -interaction hy-pothesis as there were numerous ways in which  X  could have been estimated. However, each was open to interpretation, and so a subject of investigation in its own right. Thus, we leave this for further work.
Query-cost-interaction hypothesis : From the top plots in Figure 8, we can see that for conditions 1 and 2, as the cost of a query increases, the number of assessments in-creases, while the number of queries decreases. This is in line with the hypothesis. However, in condition 3, the trend was the opposite. It may be that for this condition the performance factors are overriding or hiding the influence of the query cost. For topic 344, the k and  X  parameters are very low compared to the other topics for this condi-tion. Furthermore, the cost of a query between topics on this condition were not significantly different (F(2,33)=0.97, p=0.38), suggesting that other factors could be responsible for the observed trend. Although not shown due to space
Topic Cond. A (  X  = 0 ) A (  X  &gt; 0 ) p ( b &gt; 0 ) Q
All 2  X  0.64  X  1.5 3.0  X  4.1 0.42 408 344 2  X  0.51  X  1.2 1.5  X  2.2 0.26 205 347 2  X  0.96  X  2.3 3.8  X  5.1 0.56 106 435 2  X  0.77  X  1.4 3.7  X  3.9 0.60 97 Table 2: The mean and standard deviation of the number of documents assessed per query for each topic and all topics for each condition, along with the probability of issuing a query where b &gt; 0 and the number of queries Q. ? (  X  ) indicates a significance difference at p &lt; 0 . 05 (p &lt; 0 . 01 ). constraints, a similar trend was also observed for the page-cost-interaction hypothesis .

Assessment-cost-interaction hypothesis : The bot-tom plots in Figure 8 show the trends given the document cost. For condition 1, the expected behaviour was observed, while for the other conditions the trend is rather mottled. This suggests that the document cost interaction hypothesis may not hold. To inspect this further, we compared the doc-ument costs between topics and found that there were no sig-nificant differences in any of the conditions (1: F(2,33)=1.88, p=0.16, 2: F(2,33)=2.07, p=0.13, and 3: F(2,33)=0.88, p=0.42). This suggests that the observed differences were due to other factors and not document cost (see below).
Assessment-Probability-Interaction hypothesis : The top plots in Figure 9 show how the interaction changed as the probability of assessment increased. The trend across all conditions was similar, and consistent with the hypothesis that as the probability of assessment increased, the number of documents assessed per query increased, while the num-ber of queries decreased. We also found although not shown due to space constraints, that the data was also consistent with the snippet-cost-interaction hypothesis.

Now, we turn our attention back to the document costs, and how the trend did not meet our initial expectations. In Figure 9, the bottom plots show the change in A and Q for the expression c s p is less than one, the fraction c s p ence than c a . Consequently, as c s p expect to see the number of documents assessed decrease, while the number of queries would increase. For each condi-tion, we see that this trend is observed. This suggests that the probability of assessment played a larger role in deter-mining search behaviour for the range of values of c a and c in this experimental data. This is also confirmed by follow-up significance tests which shows that the mean p a within each condition was significantly different (1: F(2,33)=13.1, p &lt; 0.001, 2: F(2,33)=26.7, p &lt; 0.001, and 3: F(2,33)=13.6, p &lt; 0.001). However, if c a was substantially larger than the document cost would dominate the expression and we would then expect c a to have a greater influence on search behaviour. An interesting direction for future work would be to examine whether this is the case or not when the cost of assessing a document is varied significantly between dif-ferent conditions.
In this paper, we have developed a new economic model of the search process. This new model has led to a num-ber of insights regarding search behaviour, which have been expressed as a series of eight interaction hypotheses. The model provides a better account of past empirical observa-tions, and when we analysed the user interactions of thirty-six users we found trends consistent with the theory. In the cases where the trends bucked against our expectations, it was revealed that certain variables had a greater influ-ence on search behaviour, thus explaining these deviations. From our analysis, it appears that the performance variable  X  plays the most important part in determining the search behaviour, and then the probability of assessing, p the cost of inspecting a snippet, c s . However, the ordering depends on the magnitude of the costs and probabilities. Furthermore, this exposition shows how the revised theory can provide credible explanations of observed behaviours; providing a compact representation of expected search be-haviour, which practitioners and researchers can draw upon when designing experiments and in explaining and under-standing observed search behaviour.

While this is a promising step forward, the model is far from perfect, and there are many ways in which it could be improved. Further work is needed to make the model even more realistic of the search process, removing approx-imations and including scope to include constraints and/or known biases [42]. The model also does not include the probability of issuing a good query or the probability of cor-rectly marking a document relevant, either. Such proba-bilities are included in the users models of TBG [34] and U-Measure [31], for instance. So it would be a natural step to include these within the economic model, or look to in-tegrate these measures into the model. Also, while we have generated a number of testable hypotheses, more work is required to empirically test and examine whether these hy-potheses hold in practice on and across more topics, on dif-ferent types of tasks, and under various conditions. How-ever, now that we have developed these hypotheses it has paved the way forward for future experimentation that is theoretically underpinned -something that was found lack-ing in many past works [22]. Another future direction is to develop IFT [25] and the iPRP [16] in order to generate a similar set of hypotheses about search behaviour in order to compare and contrast the different theoretical frameworks.
Acknowledgments I would like to thank Keith van Rijs-bergen, Guido Zuccon and David Elswelier for their helpful and insightful comments and discussions. Also, I would like to thank the participants at ADCS 2013 for their feedback on [3] and the reviewers of this paper for their supportive comments and suggestions. All of which has helped to im-prove this work and provide many directions for future work. [1] C. W. Axelrod. The economic evaluation of [2] L. Azzopardi. The economics in interactive [3] L. Azzopardi. Economic models of search. In [4] L. Azzopardi, K. J  X  arvelin, J. Kamps, and M. D. [5] L. Azzopardi, D. Kelly, and K. Brennan. How query [6] F. Baskaya, H. Keskustalo, and K. J  X  arvelin. Time [7] F. Baskaya, H. Keskustalo, and K. J  X  arvelin. Modeling [8] M. J. Bates. The design of browsing and berrypicking [9] M. J. Bates. Training and education for online. [10] N. J. Belkin. Some(what) grand challenges for [11] N. J. Belkin, R. N. Oddy, and H. M. Brooks. Ask for [12] U. Birchler and M. Butler. Information Economics . [13] J. Brutlag. Speed matters for google web search. In [14] M. D. Cooper. A cost model for evaluating [15] S. Erdelez. Information encountering: a conceptual [16] N. Fuhr. A probability ranking principle for [17] P. Ingwersen and K. J  X  arvelin. The Turn: Integration [18] K. J  X  arvelin. Ir research: systems, interaction, [19] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [20] K. J  X  arvelin and T. D. Wilson. On conceptual models [21] D. Kelly, V. D. Dollu, and X. Fu. The loquacious user: [22] D. Kelly and C. Sugimoto. A systematic review of [23] H. Keskustalo, K. J  X  arvelin, A. Pirkola, T. Sharma, and [24] C. C. Kuhlthau. Developing a model of the library [25] P. Pirolli and S. Card. Information foraging. [26] P. Pirolli and W. T. Fu. Snif-act: a model of [27] P. Pirolli, P. Schank, M. Hearst, and C. Diehl. [28] S. E. Robertson. The probability ranking principle in [29] D. H. Rothenberg. An efficiency model and a [30] D. M. Russell, M. J. Stefik, P. Pirolli, and S. K. Card. [31] T. Sakai and Z. Dou. Summaries, ranked retrieval and [32] M. Sanderson. Test collection based evaluation of [33] C. L. Smith and P. B. Kantor. User adaptation: good [34] M. D. Smucker and C. L. Clarke. Time-based [35] G. J. Stigler. The economics of information. The [36] S. Sushmita, H. Joho, M. Lalmas, and R. Villa. [37] A. Turpin and W. Hersh. User interface effects in past [38] A. H. Turpin and W. Hersh. Why batch and user [39] H. R. Varian. Intermediate microeconomics: A modern [40] H. R. Varian. Economics and search. SIGIR Forum , [41] J. Wang and J. Zhu. Portfolio theory of information [42] R. White. Beliefs and biases in web search. In [43] R. W. White, S. T. Dumais, and J. Teevan.
 [44] R. W. White and D. Morris. Investigating the [45] R. W. White, I. Ruthven, J. M. Jose, and C. J. V. [46] T. D. Wilson. Human information behavior. Informing
