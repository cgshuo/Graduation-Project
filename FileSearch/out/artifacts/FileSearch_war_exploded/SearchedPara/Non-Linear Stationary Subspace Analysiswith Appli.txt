 Mahsa Baktashmotlagh  X  X  mahsa.baktashmotlagh@nicta.com.au Mehrtash T. Harandi  X  mehrtash.harandi@nicta.com.au Abbas Bigdeli  X  abbas.bigdeli@nicta.com.au Brian C. Lovell  X  lovell@itee.uq.edu.au Mathieu Salzmann  X  mathieu.salzmann@nicta.com.au
University of Queensland, School of ITEE, QLD 4072, Australia NICTA  X  , Locked Bag 8001, Canberra, ACT 2601, Australia Video classification is a challenging computer vision task with many potential applications, such as action recognition, anomaly detection, and face recognition. A key ingredient to video classification is the design of effective video models. A popular approach to this problem is to learn a low-dimensional representation of the videos for each individual class using methods such as PCA (Ali &amp; Shah, 2010), ICA (Long et al., 2012), ISA (Le et al., 2011) or LPP (Tseng et al., 2012). Classification is then performed by projecting a query video to the low-dimensional representation of each class, and finding which projection is most similar to the training data.
 Despite their success for video classification, an inher-ent limitation of most dimensionality reduction meth-ods is their assumption that, within each class, the video signal of all the examples is drawn from a sin-gle, potentially multi-modal, distribution. While part of the signal is indeed common to all videos in the class, other parts are specific to each individual video. From a signal processing perspective, it would there-fore seem more reasonable to model each video as the superposition of a stationary part, shared across all videos of the class, and a non-stationary part, only present in this particular video. In most cases, the non-stationary part is irrelevant for video classifica-tion, since it only carries information about a single video. Modeling this information, as most dimension-ality reduction methods do, only introduces noise, and therefore might degrade the classification performance. In light of this observation, in this paper, we introduce the use of stationarity for video classification. Being shared by all the videos, a stationary signal makes a very good representative of the class. To illustrate this, in Fig. 1, we compare the components recovered with PCA and the stationary signal extracted from the Hand Waving class of the KTH dataset. Note that this stationary signal better captures the action than PCA. To extract the stationary part of the data, we intro-duce two methods: Kernel Stationary Subspace Analy-sis (KSSA) and Non-Linear Stationary Subspace Anal-ysis (NLSSA). These two methods are non-linear ex-tensions to Stationary Subspace Analysis (SSA) (Hara et al., 2012). Given a time-varying multivariate sig-nal, SSA searches for a subspace shared across mul-tiple portions of the signal (i.e., epochs), such that the projections of these epochs onto the subspace are stationary. Following a weak notion of stationarity, SSA minimizes the KL divergence between the dis-tributions of all pairs of projected epochs. Here, we show that SSA can be kernelized, thus allowing us to model a non-linear mapping between the original sig-nal and its stationary representation via the use of kernels. Furthermore, we introduce a modification of this KSSA formulation that remains non-linear, while significantly reducing computational complexity. To summarize, our contribution is twofold: We intro-duce the use of stationarity for video classification and propose two novel non-linear algorithms to extract the stationary part of an observed signal. We demonstrate the benefits of our approach over exiting techniques on dynamic texture recognition, scene classification, and action recognition. Video classification has attracted a lot of interest since it provides a solution to a wide class of problems, such as action, or dynamic scene recognition. For many existing approaches, constructing a compact discrimi-native representation of videos has been an important research focus. In particular, Principal Component Analysis (PCA), which computes a low-dimensional representation of the data so as to minimize its re-construction error, has been widely used to model videos (Ali &amp; Shah, 2010). More recently, several vari-ations of PCA, such as Generalized PCA (Vidal et al., 2005), Mixtures of Probabilistic PCA (Gu et al., 2001; Tipping et al., 1999) and Kernel PCA (Hotta, 2012; Chan &amp; Vasconcelos, 2007) have been proposed for video classification. Unfortunately, PCA-based ap-proaches suffer from the fact that the components yielding the best reconstruction of the data might not be the most relevant ones for classification purpose. As an alternative to PCA, Independent Component Analysis (ICA) has been utilized to find a subspace of the data. ICA minimizes the mutual information of the projections of the data along the different com-ponents. In (Long et al., 2012), it was employed to learn spatio-temporal filters from unlabeled video data. Similarly, Independent Subspace Analysis (ISA), a generalization of ICA, was used to learn invariant spatio-temporal features from video for action classi-fication (Le et al., 2011). As PCA-based approaches, these methods try to model the entire data instead of focusing on the part that is shared across all videos of the same class, and thus relevant for classification. Other subspace methods that more directly exploit the structure of the training data have been used for video classification. For instance, (Tseng et al., 2012) employed (Adaptive) Locality Preserving Projection (LPP) for silhouette-based human action recognition. However, while LPP attempts to preserve the neigh-borhood structure of the data, this structure may not necessarily reflect the underlying classes. More di-rectly focused on the classification problem, an ap-proach relying on Kernel Fisher Discriminant Anal-ysis (FDA) was proposed in (Campos et al., 2011). While this approach better accounts for the underly-ing classification problem, its results do not seem to be competitive with the state-of-the-art.
 Many other video classification methods that do not make use of subspace representations have also been proposed. For instance, Support Vector Machines (SVMs) have become popular to perform video clas-sification using various image features, such as His-togram of Oriented Gradients (HOG) (Thurau &amp; Hlav  X ac, 2008), or SIFT (Sivic &amp; Zisserman, 2003). Since SVMs do not directly take temporal information into account, spatio-temporal features had to be de-signed (Knopp et al., 2010; Niebles et al., 2008; Wang et al., 2012).Probabilistic generative models have also been proposed to represent a video as the output of a linear dynamical system (LDS) (Doretto et al., 2003; Chaudhry et al., 2009; Saisan et al., 2001) or of a Bag-of-Dynamical-systems (Ravichandran et al., 2009; Chan et al., 2010).
 Here, we introduce a non-linear approach to video modeling that focuses on extracting the information that is stationary across all videos of the same class. As a consequence, the resulting video representation is particularly well-suited for classification purpose. Fur-thermore, the notion of stationarity is intuitively well-adapted to model the temporal nature of the video signal and lets us make use of many image features. In this section, we briefly review Stationary Subspace Analysis (SSA) (B  X unau et al., 2009) that serves as a starting point for our approach. SSA is a blind source separation method that aims to factorize a mul-tivariate time series into stationary and non-stationary sources. Its underlying assumption is that the ob-served signal X is generated from a linear mixture of d stationary sources S s  X  R d  X  T and D  X  d non-stationary ones S n  X  R D  X  d  X  T . This can be written as posed of A s and A n , which span the stationary and non-stationary subspaces, respectively. The goal of SSA is to estimate a de-mixing transformation W from the data X such that stationary and non-stationary sources can be separated. The de-mixing matrix is related to A by SSA exploits the notion of weak, or wide-sense, sta-tionarity, which translates into considering d sources as stationary if they have the same mean and vari-ance over time. To this end, the data is divided into N time blocks, or epochs. The stationary part of the signal is then extracted by finding the projection W , such that the mean and covariance of the projected signal are the same for all epochs. Within this linear framework, this is equivalent to comparing the projec-tions of the mean  X  i and covariance  X  i of the origi-nal signal of each epoch to the projections of the av-erage mean  X   X  = (1 /N ) P N i =1  X  i and average covariance  X  = (1 /N ) P N i =1  X  i . As a distance measure, SSA uti-lizes the Kullback-Leibler divergence, which yields the minimization problem min Solving this problem for W is ambiguous. There-fore, additional orthogonality constraints of the form  X  W T = I are enforced to restrict the solution space. In practice, the solution is obtained by searching for a matrix B in the special orthogonal group SO ( D ), and by taking W = I d B , with I d containing the first d rows of the identity matrix. In (B  X unau et al., 2009), a steepest descent method on SO ( D ) was utilized to determine B . However, the non-convexity and flat-ness of the objective near the global solution made the optimization process slow (Hara et al., 2012). This was overcome in (Hara et al., 2012) by introduc-ing an approximate analytical solution to SSA. While this formulation has proved more robust than the origi-nal SSA, it still assumes a linear mapping between the original signal and its stationary part. In the next section, we show how stationarity can be exploited for video classification, and how Analytic SSA (Hara et al., 2012) can be kernelized to model non-linear mappings. In this section, we introduce the use of stationarity for video classification. We first discuss how to extract the stationary part of a set of videos belonging to the same class, and how to model a non-linear mapping between such stationary signal and the original video data. Finally, we show how to employ this process for video classification. 4.1. Stationarity for Video Modeling In this section, we present our approach to obtaining a compact representation of videos. Let us recall that, ultimately, our goal is video classification. Therefore, we are not necessarily interested in deriving a repre-sentation that allows for the best reconstruction of the signal, but we rather seek to find the invariants of the videos belonging to the same class. These invariants can be thought of as the part of the signal that is sta-tionary across all videos.
 containing the N training videos of one specific class, where V i = [ v i, 1 ,  X  X  X  , v i,m i ] represents one video in that class, with v i,j  X  R D the vector of image features for the j th frame of video i . Following SSA, we as-sume that V is generated by a linear combination of d stationary sources and D  X  d non-stationary sources, i.e., V = AS in Eq. 1. Taking each video in the class as one epoch, we then search for a de-mixing matrix W , related to A by Eq. 2, such that the projections of the mean and covariance of each video are as similar to each other as possible. This can be expressed in terms of the KL divergence, as shown in Eq. 3. Here, to avoid the limitations of SSA discussed in Section 3, we rely on Analytic SSA (Hara et al., 2012).
 More specifically, by expanding the KL divergence in Eq. 3, W can be obtained by solving the problem where  X  , This problem is non-convex and therefore hard to solve (Hara et al., 2012). However, its objective func-tion can be replaced with a convex upper bound. To this end, the logdet term is approximated with its second order Taylor expansion  X  f ( W , W  X  ) near the optimal solution W  X  . Although this optimal solu-tion is unknown, the constraints W  X   X   X  W  X  = I and W  X   X  i W  X  = I make the dependency on W  X  disap-pear. As shown in (Hara et al., 2012), this yields the bound  X  f ( W , W  X  )  X  .
 Since the other term in the objective is quadratic, this bound makes it possible to approximate the problem in Eq. 4 as where
C = A solution to this problem can be obtained by solving the generalized eigenvalue problem C X  =  X   X   X   X  , and taking W = [  X  1  X  X  X   X  d ] T as the d generalized eigen-vectors with smallest eigenvalues. Details of the full derivation can be found in (Hara et al., 2012). 4.2. Kernel Stationary Subspace Analysis Analytic SSA is a linear method in nature. As such, it cannot model nonlinear mappings between the video signal and its stationary parts. To overcome this limitation, we introduce a kernelized version of SSA, which, as all kernel methods, boils down to performing SSA in a high-dimensional feature space.
 Let  X  : R D  X  X  be the function mapping a frame v i,j to a high-dimensional feature space. We can write the means and covariances required to encode stationarity in H as  X   X  Following the standard approach to kernelizing an al-gorithm, we represent the projection W as a linear combination of the examples in H , which can be ex-pressed as W =  X   X ( V ) T , with  X  unknown. By mak-ing use of a kernel function k , such that k ( v i,j , v i 0 ,j 0 W  X   X  i W ,  X   X  K i  X  T , (9)
W  X   X   X  W =  X  As shown in supplementary material, we can then de-rive a similar bound as in Analytic SSA, which now takes the form  X  f (  X  ,  X   X  )  X  This, in conjunction with the definitions of Eqs. 7-10, lets us re-write the problem in Eq. 5 as The solution to this problem can be obtained by solv-ing the generalized eigenvalue problem C K  X  =  X   X  K X  . A drawback of KSSA is that it relies on the compu-tation of the inverse of  X  K , which is a rank-deficient matrix. Indeed, it can be shown that the individual  X  K i matrices have rank m i  X  1, which implies that the maximum rank of  X  K is P N i =1 m i  X  N . This problem can be alleviated by replacing  X  K with  X  K +  X  I with a small  X  . This, however, does not address the high computational complexity of KSSA due to the inver-sion and multiplications of large matrices. Next, we introduce a solution to overcome these weaknesses. 4.3. Non-Linear Stationary Subspace Analysis To address the limitations of KSSA, we observe that the presence of  X  K  X  1 in the final problem is related to the presence of  X   X   X  1 in the solution of Analytic SSA. We therefore propose to transform the data in such a way that the average covariance matrix  X   X  becomes identity. To keep the benefits of working in a high-dimensional feature space, we search for such a trans-formation in H . Note that this does not truly restrict the solutions of our approach, since W was originally only defined up to a linear transformation.
 More specifically, we search for a transformation P such that P  X   X   X  P T = I . Equivalently, this can be expressed as the eigenvalue problem U  X   X   X  U T =  X  . As before, U can be expressed as a linear combination of the examples in H , i.e., U =  X   X ( V ) T . By exploiting the definitions introduced in Section 4.2, we obtain the eigenvalue problem from which we can obtain  X  . This yields the transfor-mation P =  X   X  1 / 2  X   X ( V ) T , which lets us project the data as where K is the kernel matrix of all the training videos. In practice, we keep the eigenvectors of  X  K that account for 98% of its variance.
 Since the projected data is already expressed in terms of kernels (i.e., does not depend on the mapping  X ), we can directly apply SSA to it. By defining  X  P we can re-write the problem in Eq. 3 as where we further assumed that  X   X   X  , and thus  X   X  0, which can be achieved by a centering procedure de-scribed in supplementary material. Following Analytic SSA, non-linear SSA (NLSSA) can be formulated as corresponds to the eigenvalue problem C P  X  =  X   X  . Algorithm 1 shows the pseudo-code for NLSSA. 4.4. Computational Complexity The solutions of Analytic SSA, KSSA and NLSSA are obtained by solving (generalized) eigenvalue problems, whose computational complexity is O ( s 3 ) for s  X  s ma-trices. Computing C and  X   X  in Eq. 6 requires O ((2 N + 1) D 3 ) and O ( NFD 2 ) operations, respectively, where F = P N i =1 m i . This yields O ((2 N + 2) D 3 + NFD 2 ) flops for Analytic SSA. For KSSA, computing C K and  X  K requires O ((2 N + 1) F 3 ) and O ( F 3 ) operations, and hence O ((2 N + 2) F 3 ) flops overall. Finally for NLSSA, computing P and C P requires O ( pF 2 + F 3 ) Algorithm 1 : Non-Linear Stationary Subspace Analysis and O (2 Np 3 ), respectively, where p denotes the num-ber of eigenvectors of  X  K retained to generate P . As a result, the overall computational complexity of NLSSA is O ( pF 2 + F 3 + 2 Np 3 ). The difference in computa-tional cost between KSSA and NLSSA mostly comes from the factor (2 N + 2) in KSSA. 4.5. Video Classification with KSSA and We now describe how our algorithms can be used for video classification. Let V c = [ V c 1 ,  X  X  X  , V c N matrix of training videos for class c . For each class c , we use either KSSA to compute the matrix  X  c from Eq. 11, or NLSSA to obtain the projection W c from Eq. 15. Since stationarity is expressed in terms of sim-ilarities with respect to the average over the epochs of the projected means and covariances , we use these quantities to represent each class. For KSSA, we de-fine m c =  X  where  X  k c and  X  K c can be obtained from Eqs. 8 and 10. For NLSSA, because of the transformation of the data in feature space, we simply have m c = 0 and M c = I . Given a query video of m frames Q = [ q 1 ,  X  X  X  , q m ], we perform classification by projecting Q in the subspace of each class independently. We make use of the mean and covariance of the projected data. With KSSA, this yields where  X  K c Q is obtained similarly as  X  K i in Eq. 9. For NLSSA, the data is first centered in kernel space ac-cording to the training  X   X   X  ,c , and then transformed trix computed between the training data in class c and the query video. We then compute the corresponding mean and covariance of the projected data as where  X  c Q is the covariance of the transformed data. Classification is achieved by determining which class c gives the representation ( h c , H c ) most similar to ( m c , M c ). Since SSA employs the KL-divergence as a similarity measure, it comes as a natural choice for classification. We therefore search for the class that yields the lowest value KL ( N ( h c , H c ) ||N ( m c , M While our final representation may not be ideal for a classifier such as SVM, it is very well-suited for the simple classifier that we used. Furthermore, it would not prevent us from using other discriminative clas-sifiers, such as the Minimax Probability Machine of (Lanckriet et al., 2003). Note that, given W , we could also use other representations to exploit SVMs. We be-lieve that a full study of other possible classifiers goes beyond the scope of this paper. We evaluate our approach on the tasks of traffic scene classification, dynamic texture recognition and action recognition, and compare its performance against the state-of-the art methods in each task. To demon-strate the importance of modeling stationarity, we also compare our results with those obtained by replacing NLSSA with PCA and kernel PCA in our approach. Performance is measured as the average accuracy over all classes. In all the experiments, we used the linear kernel and the RBF kernel. We report the classifica-tion accuracies obtained with N  X  1 stationary com-ponents, where N is the number of videos in the class. According to (Hara et al., 2012), to avoid spurious stationary signals, the number of stationary directions should be less than the number of epochs (i.e., at most N  X  1). This gives us a systematic way of defining the number of stationary components, which proved very effective in practice. In our experiments we relied on 2D descriptors computed in individual frames. How-ever, our method is not restricted to this choice. In particular, we could easily take temporal information into account in the features by concatenating the fea-tures of consecutive frames, or by making use of 3D spatio-temporal descriptors. 5.1. Synthetic Data As a first experiment, we compare the performance of our NLSSA and KSSA algorithms with the linear ASSA method using synthetic data generated follow-ing a protocol similar to the one in (M  X uller et al., 2011): We first generated random stationary and non-stationary sources. We then mixed these sources, but used a non-linear mixing function based on an expo-nential mapping instead of the linear mixing matrix used in (M  X uller et al., 2011). Each epoch is thus a non-linear mixture of stationary sources (shared by all epochs) and non-stationary sources (specific to each epoch). We set the dimensionality of the observed sig-nal to D = 5, the number of stationary sources to d = 2 and the number of samples in each epoch to m = 5. We varied the number of epochs N from 5 to 50.
 We applied ASSA, KSSA and NLSSA to extract the stationary sources from the observed signals. To eval-uate the quality of the results, we measured the L2 distance between the estimated stationary sources and the ground-truth ones. Fig. 2 shows the mean, min and max distance error over 10 splits as a function of N . Note that the error is shown on a log-scale axis. Our non-linear approaches with an RBF kernel yield much lower distances between the estimated and the true stationary sources than ASSA. Note that NLSSA yields a slightly higher error than KSSA. However, as will be seen in the next experiments, this entails no loss of accuracy on the classification results. Furthermore, NLSSA yields much faster runtimes than KSSA. 5.2. Traffic Scene Classification As a first experiment on real data, we used the UCSD traffic video dataset (Chan &amp; Vasconcelos, 2005) to classify videos based on the density of traffic. The dataset is partitioned into 3 classes corresponding to light, medium and heavy highway traffic congestion. It contains a total of 254 video sequences: 165 se-quences of light traffic, 45 of medium traffic, and 44 of heavy traffic. Each video contains between 42 and 52 frames of size 320  X  240, which, following com-mon practice (Sankaranarayanan et al., 2010), are normalized and downsized to 48  X  48 greyscale im-ages. We compare the performance of our two algo-rithms (KSSA and NLSSA) against the Linear Dy-namical Systems model (LDS), compressive sensing LDS (CS-LDS) (Sankaranarayanan et al., 2010), and Probabilistic Kernels (KL-SVM) (Chan &amp; Vasconce-los, 2005), and against our PCA and kernel PCA base-lines. Note that the baselines learn an LDS model from raw pixel values and use the parameters of this model for classification. Since our formulation does not re-ally let us make use of such representation, we utilized HOG features with 28 orientation bins as image de-scriptors, which are still extracted from the same raw pixel values. We employed the four train/test splits of (Sankaranarayanan et al., 2010).
 Table 1 shows that our approach outperforms the base-lines, with the exception of KL-SVM which achieves similar accuracy. Note that KSSA and NLSSA yield the same performance, which shows that the small loss of accuracy in the estimated signal noticed in Section 5.1 leaves classification accuracy unaffected. However, training with KSSA took 8070 . 2 s , as op-posed to 135 . 1 s with NLSSA. Since here on average N = 60, this ratio reflects the analysis in Section 4.4, and shows the benefit of NLSSA over KSSA. Hence-forth, we therefore only present NLSSA results. 5.3. Dynamic Texture Recognition Dynamic textures (DT) are video sequences depicting phenomena such as the motion of water, fire, clouds and smoke, that are known to exhibit stationary pat-terns. Our method therefore seems a natural choice for DT recognition. For this task, we used the DynTex++ dataset (Ghanem &amp; Ahuja, 2010), which has been widely used for benchmarking DT classification meth-ods. It is composed of 36 dynamic texture classes, each of which contains 100 video sequences of 50 greyscale frames of size 50  X  50. We used histograms of Local Binary Pattern (LBP) codes (Ahonen et al., 2006) as image features. In particular, we computed a total of 128 different codes for each image based on the relative intensities of a pixel and its 7 neighbors. Our choice of LBP as features in this experiment was motivated by the fact that other baselines used these features, which have shown to perform well in texture classi-fication due to their robustness to monotonic illumi-nation changes. Following (Ghanem &amp; Ahuja, 2010), 50 sequences from each class were randomly selected as training data and the remaining sequences used for testing. We report the mean accuracy and standard deviation over 10 such splits.
 We compare our results with our PCA and kernel PCA baselines, as well as with the state-of-the-art methods DL-Pegasos (Ghanem &amp; Ahuja, 2010) and DFS (Xu et al., 2011). The classification accuracies are shown in Table 2. Our approach with an RBF kernel achieves the highest accuracy, which confirms the importance of having a non-linear mapping. Note that the results of DL-PEGASOS (Ghanem &amp; Ahuja, 2010) (63 . 7%) were also obtained with LBP features. This clearly shows that the superiority of our results is not due to the choice of image features, but to the model itself. 5.4. Action Recognition To evaluate the performance of our approach on action recognition, we used three publicly available datasets: KTH (Schuldt et al., 2004) , Ballet (Wang &amp; Mori, 2009) and UCF sports (Rodriguez et al., 2008). The KTH dataset consists of six different human ac-tions performed by 25 subjects in four different sce-narios: outdoor, outdoor with scale variation, outdoor with different clothes and indoor. The dataset con-tains 2391 sequences in total. We relied on the stan-dard training/testing splits of (Castrodad &amp; Sapiro, 2012), and used HOG features with 128 orientation bins as image descriptors. Recognition accuracies are reported in Table 3, where it can be seen that our ap-proach with a linear kernel performs best. In contrast with previous experiments, our PCA and kernel PCA baselines perform very poorly. This demonstrates the benefits of exploiting stationarity for this task. The Ballet dataset (Wang &amp; Mori, 2009) contains 44 real video sequences with significant intra-class varia-tions in terms of spatial and temporal scales, clothing and movements. It consists of 8 actions such as jump-ing, turning, leg swinging and standing still performed by 3 subjects. While some previous experiments with the Ballet dataset (Wang &amp; Mori, 2009) tackle the problem of action recognition in still images, we per-form recognition from full video sequences. For each action, the samples were randomly split into training and testing sets of similar sizes. We utilized HOG features with 28 orientation bins as image representa-tion and used epochs of 12 frames. In Table 4, we report the average classification accuracy and stan-dard deviation over 10 splits for the state-of-art ker-nel version of affine hull set matching (KAHM) (Ce-vikalp &amp; Triggs, 2010), Grassmann discriminant anal-ysis (GDA) (Hamm &amp; Lee, 2008) and our approach. Note that our approach with an RBF kernel outper-forms the baselines. We also compared our method against using the same HOG features with KAHM. Note that the classification accuracy of KAHM with HOG features decreased to 75 . 4%. This again shows that improvement over the baselines really comes from our model, and not just from the features we used. The UCF sports dataset (Rodriguez et al., 2008) con-tains 150 real videos with non-uniform backgrounds and moving camera/subjects. It consists of 10 cate-gories of human actions collected from various sports, such as kicking, lifting weights, running and skate-boarding. The number of videos for each action varies from 6 to 22. We used the region of interest provided with the dataset, and, to deal with the small num-ber of examples, duplicated each training video. We employed HOG features with 128 orientation bins for image representation. For testing, we followed a stan-dard leave-one-out (LOO) protocol 1 . Table 5 com-pares the recognition accuracies of our approach and several baselines. Note that, again, our PCA and kernel PCA baselines perform poorly, which indicates the importance of stationarity. While Sparse Model-ing (SM) (Castrodad &amp; Sapiro, 2012) yields a slightly higher accuracy than our approach, it relies on features learnt from data. Replacing our simple HOG features with such features might improve our accuracy and will be a topic of our future work. We have proposed an approach to video classification that relies on extracting the stationary parts of the signal of each class from training videos. To this end, we have introduced two methods, KSSA and NLSSA, that allow modeling non-linear mappings between the video data and their stationary parts. Our experi-ments have shown the importance of exploiting sta-tionarity for video classification, as well as the benefits of accounting for the non-linearity of the signal. A cur-rent limitation of our approach is its use of the notion of weak stationarity, which can be fooled by the pres-ence of mixtures of non-stationary signals that appear to be stationary. In the future, we therefore intend to study the use of more sophisticated measures of sta-tionarity. We also plan to investigate how subspace clustering ideas could be exploited to model each class with more than one subspace.
