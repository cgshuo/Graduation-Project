 Dependenc y parsing is an alternati v e to constituenc y analysis with a v enerable tradition going back at least tw o millenia. The last century has seen at-tempts to formalize dependenc y parsing, particu-larly in the Prague School approach to linguistics (T esni ` ere, 1959; Mel  X  cuk, 1988).

In a dependenc y analysis of syntax, w ords directly modify other w ords. Unlik e constituenc y analysis, there are no interv ening non-le xical nodes. W e use the terms child and parent to denote the dependent term and the go v erning term respecti v ely .
P arsing has man y potential applications, rang-ing from question answering and information re-trie v al to grammar checking. Our intended ap-plication is machine translation in the Microsoft Research T reelet T ranslation System (Quirk et al., 2005; Menezes and Quirk, 2005). This system e x-pects an analysis of the source language in which w ords are related by directed, unlabeled dependen-cies. F or the purposes of de v eloping machine trans-lation for se v eral language pairs, we are interested in dependenc y analyses for multiple languages.
The contrib utions of this paper are tw o-fold: First, we present a training algorithm called Bayes Point Machines (Herbrich et al., 2001; Harrington et al., 2003), which is as easy to implement as the per -ceptron, yet competiti v e with lar ge mar gin meth-ods. This algorithm has implications for an yone interested in implementing discriminati v e training methods for an y application. Second, we de v elop parsers for English, Chinese, Czech, and Arabic and probe some linguistic questions re g arding depen-denc y analyses in dif ferent languages. T o the best of our kno wledge, the Arabic and Chinese results are the first reported results to date for directed depen-dencies. In the follo wing, we first describe the data (Section 2) and the basic parser architecture (Section 3). Section 4 introduces the Bayes Point Machine while Section 5 describes the features for each lan-guage. W e conclude with e xperimental results and discussions in Sections 6 and 7. W e utilize publicly a v ailable resources in Arabic, Chinese, Czech, and English for training our depen-denc y parsers.

F or Czech we u sed the Prague Dependenc y T ree-bank v ersion 1.0 (LDC2001T10). This is a corpus of approximately 1.6 million w ords. W e di vided the data into the standard splits for training, de v el-opment test and blind test. The Prague Czech De-pendenc y T reebank is pro vided with human-edited and automatically-assigned morphological informa-tion, including part-of-speech labels. T raining and e v aluation w as performed using the automatically-assigned labels.

F or Arabic we used the Prague Arabic De-pendenc y T reebank v ersion 1.0 (LDC2004T23). Since there is no standard split of the data into training and test sections, we made an approxi-mate 70%/15%/15% split for training/de v elopment test/blind test by sampling whole files. The Ara-bic Dependenc y T reebank is considerably smaller than that used for the other languages, with approx-imately 117,000 tok ens annotated for morphologi-cal and syntactic relations. The relati v ely small size of this corpus, combined with the morphological comple xity of Arabic and the heterogeneity of the corpus (it is dra wn from fi v e dif ferent ne wspapers across a three-year time period) is reflected in the relati v ely lo w dependenc y accurac y reported belo w . As with the Czech data, we trained and e v aluated us-ing the automatically-assigned part-of-speech labels pro vided with the data.

Both the Czech and the Arabic corpora are anno-tated in terms of syntactic dependencies. F or En-glish and Chinese, ho we v er , no corpus is a v ailable that is annotated in terms of dependencies. W e there-fore applied head-finding rules to treebanks that were annotated in terms of constituenc y .

F or English, we used the Penn T reebank v ersion 3.0 (Marcus et al., 1993) and e xtracted dependenc y relations by applying the head-finding rules of (Y a-mada and Matsumoto, 2003). These rules are a simplification of the head-finding rules of (Collins, 1999). W e trained on sections 02-21, used section 24 for de v elopment test and e v aluated on section 23. The English Penn T reebank contains approxi-mately one million tok ens. T raining and e v aluation ag ainst the de v elopment test set w as performed us-ing human-annotated part-of-speech labels. Ev alu-ation ag ainst the blind test set w as performed us-ing part-of-speech labels assigned by the tagger de-scribed in (T outano v a et al., 2003).

F or Chinese, we used the Chinese T reebank v er -sion 5.0 (Xue et al., 2005). This corpus contains approximately 500,000 tok ens. W e made an approx-imate 70%/15%/15% split for training/de v elopment test/blind test by sampling whole files. As with the English T reebank, training and e v aluation ag ainst the de v elopment test set w as performed using human-annotated part-of-speech labels. F or e v alu-ation ag ainst the blind test section, we used an im-plementation of the tagger described in (T outano v a et al., 2003). T rained on the same training section as that used for training the parser and e v aluated on the de v elopment test set, this tagger achie v ed a to-k en accurac y of 92.2% and a sentence accurac y of 63.8%.

The corpora used v ary in homogeneity from the e xtreme case of the English Penn T reebank (a lar ge corpus dra wn from a single source, the W all Street Journal) to the case of Arabic (a relati v ely small corpus X  X pproximately 2,000 sentences X  X ra wn from multiple sources). Furthermore, each language presents unique problems for computational analy-sis. Direct comparison of the dependenc y parsing results for one language to the results for another language is therefore dif ficult, although we do at-tempt in the discussion belo w to pro vide some basis for a more direct comparison. A common question when considering the deplo yment of a ne w language for machine translation is whether the natural lan-guage components a v ailable are of suf ficient quality to w arrant the ef fort to inte grate them into the ma-chine translation system. It is not feasible in e v ery instance to do the inte gration w ork first and then to e v aluate the output.

T able 1 summarizes the data used to train the parsers, gi ving the number of tok ens (e xcluding traces and other empty elements) and counts of sen-tences. 1 W e tak e as our starting point a re-implementation of McDonald X  s state-of-the-art dependenc y parser (McDonald et al., 2005a). Gi v en a sentence x , the goal of the parser is to find the highest-scoring parse  X  y among all possible parses y  X  Y : F or a gi v en parse y , its score is the sum of the scores of all its dependenc y links ( i, j )  X  y : where the link ( i, j ) indicates a head-child depen-denc y between the tok en at position i and the tok en at position j . The score d ( i, j ) of each dependenc y link ( i, j ) is further decomposed as the weighted sum of its features f ( i, j ) .

This parser architecture naturally consists of three modules: (1) a decoder that enumerates all possi-ble parses y and computes the ar gmax; (2) a train-ing algorithm for adjusting the weights w gi v en the training data; and (3) a feature representation f ( i, j ) . T w o decoders will be discussed here; the training al-gorithm and feature representation are discussed in the follo wing sections.

A good decoder should satisfy se v eral proper -ties: ideally , it should be able to search through all v alid parses of a sentence and compute the parse scores ef ficiently . Ef ficienc y is a significant issue since there are usually an e xponential number of parses for an y gi v en sentence, and the discrimina-ti v e training methods we will describe later require repeated decoding at each training iteration. W e re-implemented Eisner X  s decoder (Eisner , 1996), which searches among all pr ojective parse trees, and the Chu-Liu-Edmonds X  decoder (Chu and Liu, 1965; Edmonds, 1967), which searches in the space of both projecti v e and non-projecti v e parses. (A pro-jecti v e tree is a parse with no crossing dependenc y links.) F or the English and Chinese data, the head-finding rules for con v erting from Penn T reebank analyses to dependenc y analyses creates trees that are guaranteed to be projecti v e, so Eisner X  s algo-rithm suf fices. F or the Czech and Arabic corpora, a non-projecti v e decoder is necessary . Both algo-rithms are O ( N 3 ) , where N is the number of w ords in a sentence. 2 Refer to (McDonald et al., 2005b) for a detailed treatment of both algorithms. In this section, we describe an online learning al-gorithm for training the weights w . First, we ar -gue wh y an online learner is more suitable than a batch learner lik e a Support V ector Machine (SVM) for this task. W e then re vie w some standard on-line learners (e.g. perceptron) before presenting the Bayes Point Machine (BPM) (Herbrich et al., 2001; Harrington et al., 2003). 4.1 Online Lear ning An online learner dif fers from a batch learner in that it adjusts w incrementally as each input sample is re v ealed. Although the training data for our pars-ing problem e xists as a batch (i.e. all input sam-ples are a v ailable during training), we can apply online learning by presenting the input samples in some sequential order . F or lar ge training set sizes, a batch learner may f ace computational dif ficulties since there already e xists an e xponential number of parses per input sentence. Online learning is more tractable since it w orks with one input at a time.
A popular online learner is the perceptron. It ad-justs w by updating it with the feature v ector when-e v er a misclassification on the current input sample occurs. It has been sho wn that such updates con-v er ge in a finite number of iterations if the data is lin-early separable. The a v eraged perceptron (Collins, 2002) is a v ariant which a v erages the w across all iterations; it has demonstrated good generalization especially with data that is not linearly separable, as in man y natural language processing problems. Recently , the good generalization properties of Sup-port V ector Machines ha v e prompted researchers to de v elop lar ge mar gin methods for the online set-ting. Examples include the mar gin perceptron (Duda et al., 2001), ALMA (Gentile, 2001), and MIRA (which is used to train the parser in (McDonald et al., 2005a)). Conceptually , all these methods attempt to achie v e a lar ge mar gin and approximate the maxi-mum mar gin s olution of SVMs. 4.2 Bay es P oint M achines The Bayes Point Machine (BPM) achie v es good generalization similar to that of lar ge mar gin meth-ods, b ut is moti v ated by a v ery dif ferent philoso-ph y of Bayesian learning or model a v eraging. In the Bayesian learning frame w ork, we assume a prior distrib ution o v er w . Observ ations of the training data re vise our belief of w and produce a poste-rior distrib ution. The posterior distrib ution is used to create the final w where p ( w | D ) is the posterior distrib ution of the weights gi v en the data D and E tation tak en with respect to this distrib ution. The term | V ( D ) | is the size of the ver sion space V ( D ) , which is the set of weights w i that is consistent with the training data (i.e. the set of w i that classifies the training data with zero error). This solution achie v es the so-called Bayes P oint , which is the best approx-imation to the Bayes optimal solution gi v en finite training data.

In practice, the v ersion space may be lar ge, so we approximate it with a finite sample of size I . Further , assuming a uniform prior o v er weights, we get the follo wing equation: Equation 4 can be computed by a v ery simple al-gorithm: (1) T rain separate perceptrons on dif ferent random shuf fles of the entire training data, obtaining a set of w i . (2) T ak e the a v erage (arithmetic mean) of the weights w i . It is well-kno wn that perceptron training results in dif ferent weight v ector solutions if the data samples are presented sequentially in dif-ferent orders. Therefore, random shuf fles of the data and training a perceptron on each shuf fle is ef fec-ti v ely equi v alent to sampling dif ferent models ( w i in the v ersion space. Note that this a v eraging op-eration should not be confused with ensemble tech-niques such as Bagging or Boosting X  X nsemble tech-niques a v erage the output h ypotheses, whereas BPM a v erages the weights (models).

The BPM pseudocode is gi v en in Figure 1. The inner loop is simply a perceptron algorithm, so the BPM is v ery simple and f ast to implement. The outer loop is easily parallelizable, allo wing speed-ups in training the BPM. In our specific implemen-tation for dependenc y parsing, the line of the pseu-docode corresponding to [  X  y t = w i  X  x t ] is replaced by Eq. 1 and updates are performed for each in-correct dependenc y link. Also, we chose to a v erage each indi vidual perceptron (Collins, 2002) prior to Bayesian a v eraging.

Finally , it is important to note that the definition of the v ersion space can be e xtended to include weights with non-zero training error , so the BPM can handle data that is not linearly separable. Also, although we only presented an algorithm for linear classifiers (pa-rameterized by the weights), arbitrary k ernels can be applied to BPM to allo w non-linear decision bound-aries. Refer to (Herbrich et al., 2001) for a compre-hensi v e treatment of BPMs. Dependenc y parsers for all four languages were trained using the same set of feature types. The feature types are essentially those described in (Mc-Donald et al., 2005a). F or a gi v en pair of tok ens, where one is h ypothesized to be the parent and the other to be the child, we e xtract the w ord of the par -ent tok en, the part of speech of the parent tok en, the w ord of the child tok en, the part of speech of the child tok en and the part of speech of certain adjacent and interv ening tok ens. Some of these atomic fea-tures are combined in feature conjunctions up to four long, with the result that the linear classifiers de-scribed belo w approximate polynomial k ernels. F or e xample, in addition to the atomic features e xtracted from the parent and child tok ens, the feature [P ar -entW ord, P arentPOS, ChildW ord, ChildPOS] is also added to the feature v ector representing the depen-denc y between the tw o tok ens. Additional features are created by conjoining each of these features with the direction of the dependenc y (i.e. is the parent to the left or right of the child) and a quantized measure of the distance between the tw o tok ens. Ev ery tok en has e xactly one parent. The root of the sentence has a special synthetic tok en as its parent.

Lik e McDonald et al, we add features that con-sider the first fi v e characters of w ords longer than fi v e characters. This truncated w ord crudely approx-imates stemming. F or Czech and English t he addi-tion of these features impro v es accurac y . F or Chi-nese and Arabic, ho we v er , it is clear that we need a dif f erent back of f strate gy .

F or Chinese, we truncate w ords longer than a sin-gle character to the first character . 3 Experimental results on the de v elopment test set suggested that an alternati v e strate gy , truncation of w ords longer than tw o characters to the first tw o characters, yielded slightly w orse results.

The Arabic data is annotated with gold-standard morphological information, including information about stems. It is also annotated with the output of an automatic morphological analyzer , so that re-searchers can e xperiment with Arabic without first needing to b uild these components. F or Arabic, we truncate w ords to the stem, using the v alue of the lemma attrib ute.

All tok ens are con v erted to lo wercase, and num-bers are normalized. In the case of English, Czech and Arabic, all numbers are normalized to a sin-gle tok en. In Chinese, months are normalized to a MONTH tok en, dates to a D A TE tok en, years to a YEAR tok en. All other numbers are normalized to a single NUMBER tok en.

The feature types were instantiated using all or -acle combinations of child and parent tok ens from the training data. It should be noted t hat when the feature types are instantiated, we ha v e considerably more features than McDonald et al. F or e xample, for English we ha v e 8,684,328 whereas the y report 6,998,447 features. W e suspect that this is mostly due to dif ferences in implementation of the features that back of f to stems.

The a v eraged perceptrons were trained on the one-best parse, updating the perceptron for e v ery edge and a v eraging the accumulated perceptrons af-ter e v ery sentence. Experiments in which we up-dated the perceptron based on k-best parses tended to produce w orse results. The Chu-Liu-Edmonds al-gorithm w as used for Czech. Experiments with the de v elopment test set suggested that the Eisner de-coder g a v e better results for Arabic than the Chu-Liu-Edmonds decoder . W e therefore used the Eisner decoder for Arabic, Chinese and English. T able 2 presents the accurac y of the dependenc y parsers. Dependenc y accurac y indicates for ho w man y tok ens we identified the correct head. Root ac-curac y , i.e. for ho w man y sentences did we identify the correct root or roots, is reported as F1 measure, since sentences in the Czech and Arabic corpora can ha v e multiple roots and since the parsing algorithms can identify multiple roots. Complete match indi-cates ho w man y sentences were a complete match with the oracle dependenc y parse.

A con v ention appears to ha v e arisen when report-ing dependenc y accurac y to gi v e results for English e xcluding punctuation (i.e., ignoring punctuation to-k ens in the output of the parser) and to report results for Czech including punctuation. In order to f acil-itate comparison of the present results with pre vi-ously published results, we present measures includ-ing and e xcluding punctuation for all four languages. W e hope that by presenting both sets of measure-ments, we also simplify one dimension along which published results of parse accurac y dif fer . A direct comparison of parse results across languages is still dif fi cult for reasons to do with the dif ferent nature of the languages, the corpora and the dif fering stan-dards of linguistic detail annotated, b ut a compar -ison of parsers for tw o dif ferent languages where both results include punctuation is at least preferable to a comparison of results including punctuation to results e xcluding punctuation.

The results reported here for English and Czech are comparable to the pre vious best published num-bers in (McDonald et al., 2005a), as T able 3 sho ws. This table compares McDonald et al.  X  s results for an a v eraged perceptron trained for ten iterations with no check for con v er gence (Ryan McDonald, pers. comm.), MIRA, a lar ge mar gin classifier , and the current Bayes Point Machine results. T o determine statistical significance we used confidence interv als for p=0.95. F or the comparison of English depen-denc y accurac y e xcluding punctuation, MIRA and BPM are both statistically significantly better than the a v eraged perceptron result reported in (McDon-ald et al., 2005a). MIRA is significantly better than BPM when measuring dependenc y accurac y and root accurac y , b ut BPM is significantly better when measuring sentences that match completely . From the f act that neither MIRA nor BPM clearly outperforms the other , we conclude that we ha v e successfully replicated the results reported in (Mc-Donald et al., 2005a) for English.

F or Czech we also determined significance using confidence interv als for p=0.95 and compared re-sults including punctuation. F or both dependenc y accurac y and root accurac y , MIRA is statisticallty significantly better than a v eraged perceptron, and BPM is statistically significantly better than MIRA. Measuring the number of sentences that match com-pletely , BPM is statistically significantly better than a v eraged perceptron, b ut MIRA is significantly bet-ter than BPM. Ag ain, since neither MIRA nor BPM outperforms the other on all measures, we conclude that the results constitute a v aliation of the results reported in (McDonald et al., 2005a).

F or e v ery language, the dependenc y accurac y of the Bayes Point Machine w as greater than the ac-curac y of the best indi vidual perceptron that con-trib uted to that Bayes Point Machine, as T able 4 sho ws. As pre viously noted, when measuring ag ainst the de v elopment test set, we used human-annotated part-of-speech labels for English and Chi-nese.

Although the Prague Czech Dependenc y T ree-bank is much lar ger than the English Penn T reebank, all measurements are lo wer than the corresponding measurements for English. This reflects the f act that Czech has considerably more inflectional morphol-ogy than English, leading to data sparsity for the le x-ical features.

The results reported here for Arabic are, to our kno wledge, the first published numbers for depen-denc y parsing of Arabic. Similarly , the results for Chinese are the first published results for the depen-denc y parsing of the Chinese T reebank 5.0. 4 Since the Arabic and Chinese numbers are well short of the numbers for Czech and English, we attempted to determine what impact the smaller corpora used for training the Arabic and Chinese parsers might ha v e. W e performed data reduction e xperiments, training the parsers on fi v e random samples at each size smaller than the entire training set. Figure 2 sho ws the dependenc y accurac y measured on the complete de v elopment test set when training with samples of the data. The graph sho ws the a v erage cluding punctuation. dependenc y accurac y for fi v e runs at each sample size up to 5,000 sentences. English and Chinese accuracies in this graph use oracle part-of-speech tags. At all sample sizes, the dependenc y accu-rac y for English e xceeds the dependenc y accurac y of the other languages. This dif ference is perhaps partly attrib utable to the use of oracle part-of-speech tags. Ho we v er , we suspect that the major contrib u-tor to this dif ference is the part-of-speech tag set. The tags used in the English Penn T reebank encode traditional le xical cate gories such as noun, prepo-sition, and v erb . The y also encode morphological information such as person (the VBZ tag for e xam-ple is used for v erbs that are third person, present tense X  X ypically with the suf fix -s), tense, number and de gree of comparison. The part-of-speech tag sets used for the other languages encode le xical cat-e gories, b ut do not encode morphological informa-tion. 5 W ith small amounts of data, the perceptrons do not encounter suf ficient instances of each le xical item to calculate reliable weights. The perceptrons are therefore forced to rely on the part-of-speech in-formation.

It is surprising that the results for Arabic and Chi-nese should be so close as we v ary the size of the training data (Figure 2) gi v en that Arabic has rich morphology and Chinese v ery little. One possible e xplanation for the similarity in accurac y is that the rather poor root accurac y in Chinese indicates parses that ha v e gone a wry . Anecdotal inspection of parses suggests that when the root is not correctly identi-fied, there are usually cascading related errors.
Czech, a morphologically comple x language in which root identification is f ar from straightfor -w ard, e xhibits the w orst performance at small sam-ple sizes. But (not sho wn) as the sample size in-creases, the accurac y of Czech and Chinese con-v er ge. W e ha v e successfully replicated the state-of-the-art results for dependenc y parsing (McDonald et al., 2005a) for both Czech and English, using Bayes Point Machines. Bayes Point Machines ha v e the ap-pealing property of simplicity , yet are competiti v e with online wide mar gin methods.

W e ha v e also presented first results for depen-denc y parsing of Arabic and Chinese, together with some analysis of the performance on those lan-guages.

In future w ork we intend to e xplore the discrim-inati v e reranking of n-best lists produced by these parsers and the incorporation of morphological fea-tures. Figure 2: Dependenc y accurac y at v arious sample sizes. Graph sho ws a v erage of fi v e samples at each size and measures accurac y ag ainst the de v elopment test set.
 W e w ould lik e to thank Ryan McDonald, Otakar Smr  X  z and Hiro yasu Y amada for help in v arious stages of the project.

