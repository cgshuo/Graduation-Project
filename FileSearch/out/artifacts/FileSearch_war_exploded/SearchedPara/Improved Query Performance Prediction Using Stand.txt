 Query performance prediction (QPP) is an important task in information retrieval (IR). In this paper, we (1) develop a new predictor based on the standard deviation of scores in a variable length ranked list, and (2) we show that this new predictor outperforms state-of-the-art approaches without the need for tuning.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval: Query formulation General Terms: Experimentation, Measurement, Perfor-mance Keywords: Information Retrieval, Query Performance Pre-diction
Query performance prediction (QPP) has been a vibrant area of IR research over the last decade [2, 4, 3]. The mo-tivation for QPP is that, if we can predict the performance of a query for a given system, we can automatically develop different strategies for dealing with these different queries. Predictors for this task are usually divided into two classes: pre-retrieval and post-retrieval . Pre-retrieval predictors are usually computationally less expensive but suffer from poor performance. Post-retrieval predictors are more computa-tionally expensive as they use the ranked output (and/or scores) of a system, but achieve a higher performance than their counterparts. In general, the effectiveness a predictor is usually measured by calculating the correlation between the output of the predictor and the actual performance (i.e. average precision) of the queries on a system. Pearson X  X  ( r ) and Spearman X  X  (  X  ) are two common correlation coefficient X  X  used.
The data used in this paper consists of a number of TREC collections and a considerably large number of topics avail-able for those collections. The title field was used as a short query for each of the collections, while the desc field was Table 3: Natural tendency for longer queries to re-turn increased  X  of scores without an increase in performance (MAP) are assigned a score greater than a certain percentage ( x ) of the top score. For example, if we choose x = 90%, all documents that have a score of at least 90% of the top score are included in the standard deviation calculation. Table 2 shows the performance of this approach on three of the col-lections for a BM 25 system. We can see that performance (i.e. correlation) is optimised at x = 50% (i.e. all document scores that are at least 50% of the top score for a given query are are included in the standard deviation calculation). Re-sults on all other collections used in this work (not included due to space limitations) report a similar trend. This simple method means that a varying number of documents are in-cluded in the standard deviation calculation, and that these documents are of a certain quality (as determined by the system itself).

Furthermore, we also determined that there is a natural tendency for longer queries to produce ranked lists with a higher deviation of document score, although these longer queries might not produce a higher performance. Table 3 outlines this phenomenon. Therefore, we normalised the standard deviation with respect to query length. Thus, our new normalised query performance predictor is n (  X  50% ) = sqrt ( ql ) where ql is the query length. The last column of Table 2 confirms that this new normalised predictor out-performs the unnormalised version on the collections. Fur-thermore, both new predictors (  X  50% and n (  X  50% )) are sig-nificantly correlated with average precision. Now that we have developed a new predictor we compare it against some state-of-the-art approaches.
In these experiments, we use a BM 25 system and com-pare the performance of a number of state-of-the-art pre-dictors against our newly developed predictor. The best pre-retrieval predictors from the literature are the simplified clarity score ( scs ), the average idf of query terms ( idf avg ), and the maximum idf of the query terms ( idf max ). The best post-retrieval predictors from the literature are query clarity ( clarity ) [1], ncq [5], standard deviation at 100 documents (  X  100 ), the maximum standard deviation in the ranked-list (  X  max ), and a variable cut-off point ( k ) approach [4] (  X  k ) which includes a tuning parameter  X  which we set to 5.
Table 4 shows the performance of the predictors aver-aged over the News collections for each query type ( title and desc ). Firstly, we can see that while pre-retrieval predictors are useful for short queries, they are poor on longer queries. The clarity score achieves steady performance across the col-lections and query types. However, the predictors based on standard deviation are generally more highly correlated with query performance. Table 5 shows the best predictors
