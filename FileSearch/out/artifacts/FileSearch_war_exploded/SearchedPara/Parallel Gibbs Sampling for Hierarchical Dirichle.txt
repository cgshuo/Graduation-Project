 The hierarchical Dirichlet process (HDP) is an intuitive and elegant technique to model data with latent groups. How-ever, it has not been widely used for practical applications due to the high computational costs associated with infer-ence. In this paper, we propose an effective parallel Gibbs sampling algorithm for HDP by exploring its connections with the gamma-gamma-Poisson process. Specifically, we develop a novel framework that combines bootstrap and Re-versible Jump MCMC algorithm to enable parallel variable updates. We also provide theoretical convergence analysis based on Gibbs sampling with asynchronous variable up-dates. Experiment results on both synthetic datasets and two large-scale text collections show that our algorithm can achieve considerable speedup as well as better inference ac-curacy for HDP compared with existing parallel sampling algorithms.
 G.3 [ Mathematics of Computing ]: Probability and Statis-tics Parallel Inference; Hierarchical Dirichlet Process; Topic Model
Modeling large, complex, real-world domains often de-mands powerful models which can handle rich relational structures. Mixture models, such as those to model groups of data with shared characteristics by enforcing a shared set of mixture components, are one of the most intuitive and also effective solutions. For instance, the latent Dirichlet allocation (LDA) model has been proven successful in mod-eling a collection of documents [4] and the nonparametric extensions with hierarchical Dirichlet processes (HDP) in-herit the advantage of LDA and allow the flexibility to learn the number of mixture components automatically from the data [14].

The HDP has achieved success in modeling many differ-ent types of data. However, the biggest challenge towards applications is their inability to scale to large datasets. Re-cently, some excellent work has been conducted to address this challenging problem, which can be summarized into two directions. One is the general-purpose implementation of parallel inference algorithms [7, 8, 9]. These algorithms are general enough to be applied to any type of graphical mod-els, but it is difficult for them to achieve the desired speedup in specific models. The other direction is the approximation or vanilla parallelism of specific models, such as [10, 2, 16] for parallel inference of LDA models. Furthermore, [17] pro-poses an exact parallel sampling algorithm for HDP, which is the first nontrivial distributed sampling algorithm for HDP that converges to the true distribution provably. However, there is still room for improvement since the algorithm suf-fers from unbalanced workload and increasing rejection rate of the Metropolis Hasting step with increasing number of processors.

In this paper, we propose a parallel Gibbs sampling algo-rithm for HDP by exploring its connection with the gamma-gamma-Poisson process, which has a rich potential for de-veloping various parallel inference algorithms. We propose a parallel sampling algorithm based on the augmented gamma-gamma-Poisson process model and reconstruct the dataset using a bootstrap technique which brings the independence across different mixture components. Therefore we can per-form independent variable update for each mixture compo-nent. We also provide theoretical convergence analysis for the parallel Gibbs sampling algorithm with asynchronous variable updates, i.e., each variable updates without explicit coordination, which is common in a distributed system. We demonstrate the accuracy of our proposed algorithm on the synthetic datasets and faster convergence rate on large-scale real world datasets.

The rest of the paper is organized as follows: we first review the basic ideas in Section 2. In Section 3, we describe our parallel sampling algorithm based on gamma-gamma-Poisson process and discuss theoretical convergence analysis. Finally, we show the experiment results in Section 4.
A Dirichlet process mixture model (DPMM) is a mix-ture model with an infinite number of mixture components, where the Dirichlet process (DP), with the base distribu-tion H and concentration parameter  X  , serves as the non-parametric prior of the mixing measure over all components. Given a dataset { x i } N i =1 of size N , DPMM assumes that the i th data point x ( i ) is generated from the mixture component  X 
As proved by Ferguson in [5], G = P  X  k =1  X  k  X   X  k with the discrete support {  X  k }  X  k =1  X  H where H is the space of all mixture components,  X   X  k is Dirac-delta function of com-ponent  X  k and  X  k is the associated mixture weight. Each  X   X  H represents a mixture component, and each mixture component has its distribution over the data space p ( x |  X 
In some applications, we may be interested in modeling groups of data with shared mixture components and prior over mixing measures. Therefore the hierarchical Dirichlet process (HDP) mixture model is proposed as a hierarchical structural extension of DPMM [14]. That is, given a dataset group index, and N d as the size of d th group, HDP assumes that they are generated as follows:
The upper level DP generates G 0 from the mixture com-ponent space H as the common base measure to ensure that each group shares mixture components with a positive prob-ability. Then each group is generated from DPMM based on the mixture component space G 0 , with independent con-centration parameter  X  . Topic model is one nice example for HDP, where each latent topic corresponds to the mix-ture component, the collection of documents is the groups of data, and each data point is a word in the document.
The HDP has achieved success in modeling observations with latent groups in hierarchical structures [14, 13, 12]. However, the computational cost of inference over HDP makes it infeasible for practical applications. Various inference techniques have been investigated to solve this problem, such as variational inference, sampling techniques and so on. Among them, sampling algorithms have become pop-ular due to their simplicity and high inference quality [14]. However, they are also known to suffer from the slow conver-gence rate. Therefore several parallel sampling algorithms have been proposed to speed up the convergence via parallel computing paradigm.

Slice sampler [15] is one example of parallel sampling al-gorithm for DPMM. It reduces the infinite mixture compo-nent space to a finite subset at each step, where the mixture component assignment for each data point is conditionally independent of the rest and thus can be updated simulta-neously. Parallelizing slice sampler is simple but requires multiple times of synchronization within each update, which could lead to significant communication overhead and makes it impractical for the distributed learning scenario.
In [2], an approximation of the HDP is introduced and a parallel sampling algorithm is developed for the approximate model. It divides the data into subsets and each processor synchronously updates the variable through a Gibbs sam-pler based on the current state of its local dataset and the previous state of the global dataset. The processors commu-nicate with each other asynchronously to ensure the global convergence. This algorithm is well suited for the parallel environment with distributed storage but the inference re-sults may not be as accurate as the original HDP.

The first real-sense parallel sampling algorithm for HDP has been proposed in [17]. It is based on examining an equivalent generative model of HDP with auxiliary variables. Conditioned on the auxiliary variables, each processor can update its local variables independently. Data points are assigned to processors based on the status of auxiliary vari-ables, and each processor learns the mixture component in-dependently. That is, the algorithm implicitly separates the mixture components space to each processor. One major issue with the algorithm is the potential imbalanced work-load across processors. In addition, the rejection rate of the Metropolis Hasting step for updating auxiliary variables could be high and thus resulting in slower convergence rate.
As we can see, most existing work on parallel sampling for HDP has limitations either in inference accuracy or con-vergence rate. Given the large demand in practical appli-cations, seeking an effective parallel sampling algorithm re-mains an important and challenging task.
In this section, we describe in detail our approach: we first review the gamma-gamma-Poisson process and its equiva-lence to HDP, and then introduce the parallel Gibbs sam-pling algorithms on the equivalent model.
A gamma-Poisson process is a two-level hierarchy of com-pletely random process defined on base measurable space H [19].It is known that a random process  X  0 drawn from gamma-Poisson process with parameter { m,H } is defined as follows: where PoisP refers to Poisson process and GaP refers to Gamma process. If H is discrete and H = P  X  k =1  X  k where  X  k is the associated atom weight (which becomes the mixture weight in the equivalent mixture model of HDP de-fined later), the generation process of  X  0 can also be de-scribed as: and
The gamma-gamma-Poisson process is defined by replac-ing the based measure H in gamma-Poisson process with another random measure G 0 drawn from a gamma process GaP(  X H ). The equivalence between HDP and gamma-gamma-Poisson process is well-studied in [19] section 3.1. The gen-erative process of both HDP and gamma-gamma-Poisson process are summarized in Table 1. Note that unlike G Table 1: Summary of the generative process of HDP and gamma-gamma-Poisson processes the sum of the weight of G 0 i is no longer 1, so it does not represent a distribution. But since G 0 i is proportional to G , we can obtain G i easily by normalization. Furthermore, instead of normalizing the weights explicitly, we can resort to the property of Poisson process that given the sum of several independent Poisson random variables, the Poisson random variables are conditionally distributed as multino-mial distributions with the normalized weights. Thus the normalization is achieved implicitly.

We take topic models as an example to illustrate the gen-erative process of gamma-gamma-Poisson process. The up-per level DP which generates the global topic space G 0 in HDP is replaced by the gamma process with same parame-ters G 0 0 = P  X  k =1  X  k  X   X  k  X  GaP(  X H ), where  X  k represents the weight for the k th topic. For each document, we first draw the topic mixing parameter G 0 d |{ G 0 0 } X  GaP( G 0 0 ). Similar to eq(1), we can also have G 0 d = P  X  k =1  X  0 dk  X   X  k , where  X  weight for the k th topic in the d th document. According to G , we use the Poisson process to generate the count mea-sure  X  0 d |{ m,G 0 d }  X  PoisP( mG 0 d ), which can be represented as  X  0 d = P  X  k =1 n dk  X   X  k . Each n dk has its definitive meaning: the number of words that belongs to topic k in document d . Finally, we generate the words for each document according to n dk and the conditional word distribution given the topic. Notice that [19] provides similar derivations except that the last two steps are combined to directly generate n dkx , i.e., the number of word x that belongs to topic k in document d .
Even though we establish the equivalence between HDP and gamma-gamma-Poisson process, these two models, how-ever, behave differently. For simplicity, we use the topic models to illustrate the difference. The advantage of gamma-gamma-Poisson process is that for each different topic, the generative process is completely independent with each other in the entire process, which is the merit of composing com-pletely random processes. Note that when we mention the k th topic, we are referring to all variables  X  k ,  X  k ,  X  n dk with the same topic index k . This key property lays the foundation of the parallel Gibbs sampling algorithm. Figure 1: Graphical model representation of topic models by (a) HDP and (b) gamma-gamma-Poisson process for one document. Figure 2: Dependency between n dk and N d . With N d observed, { n dk } K k =1 will form a clique. Otherwise, they are independent with each other.

Another major difference is how we treat the variables n dk and N d (see in Figure 1). In HDP, n dk does not directly appear in the model, but N d is given. This agrees better with the real world setting, where the size of the dataset is usually observed. In gamma-gamma-Poisson process, N d is implicitly determined after we generate n dk , and it cannot be acquired beforehand. This raises one major challenge to parallel inference algorithms because any explicit assump-tion on N d will breakdown the cross-topic independence. Therefore we develop a technique that combines bootstrap and Reversible Jump MCMC algorithm to address this par-ticular challenge.

The inference task of HDP is defined as follows: given the hyperparameters of the model and the observation, how can we infer the parameter  X  k which characterizes the con-ditional word distribution given the topic as well as the as-sociated topic distribution  X  0 k and  X  dk ,d  X  { 1 ,...,D } for each topic k ? The input is a collection of documents. We represent the d th document by X d and its length by N d . Applying the finite approximation on the number of topic K as in [19], we have a much simpler model representation as follows: where H  X  represents the generative model for  X  k , e.g., the Dirichlet distribution.

The joint distribution can be computed as
Our goal is to design a parallel sampling algorithm which can update each topic and its associated variables asyn-chronously in parallel. Thus, it is important to analyze vari-able dependence across topics. From the graphical model in Figure 1(b), we can see that different topics are only con-nected by their common child nodes x . All  X  k are dependent with each other through x dk , which is necessary for learn-ing topics jointly. But they are independent given the topic assignment for each word, so we can achieve independent up-date by grouping the word by topic. All n dk for any given d are connected by N d , as shown in Figure 2. This depen-dence among n dk is the  X  X ide effect X  of the new model, which is undesirable and impedes us from developing efficient par-allel sampling algorithms. Our solution is to  X  X nobserve X  the variable N d , by constructing a document with flexible length. Details are provided in the sampling step for updat-ing n dk .

We propose the updating rules for  X  k ,  X  k ,  X  0 dk and n follows: (See Algrithm 1 for pseudocode.)
Updating n dk . We update n dk by the Metropolis-Hasting step based on Reversible Jump MCMC with two equally weighted proposed jumps:  X  n dk  X  n dk +1 X ( n ++ dk ) or  X  n n dk  X  1 X ( n  X  X  X  dk ). In the likelihood function, the factors regard-ing n dk (given d and k ) are In addition, with the dataset X d given, the likelihood func-tion becomes where p x ( k )  X  p ( x |  X  k ) is the normalized likelihood of topic assignment. Such normalization is equivalent as condition-ing on observation X , which is necessary for deriving correct acceptance rate since the length of document is different be-fore and after the jump.

As we can see, the change of n dk also leads to the change of topic assignment in the d th document, which changes n d  X  other topics. Given X d , this operation cannot be conducted within the k th topic. Therefore, as discussed above, we need to construct a new document X 0 d of flexible length X 0 d Algorithm 1 Parallel Sampling Algorithm for Gamma-Gamma-Poisson Process 1: Inputs: Group of dataset X d , parameter  X ,m . Number 2: Outputs: Mixture component  X  k and corresponding 3: Construct stacks S d for each dataset d 4: Construct empty collection X 0 dk and empty buffer B dk 5: Initialize  X  k , X  k , X  0 dk randomly 6: for each mixture component k asynchronously in paral-7: repeat 8: for each dataset d do 9: for nIter from 1 to maxNIter do 10: Draw u  X  Uniform(0 , 1) 11: if u &lt; 0 . 5 then 12: Pop x  X  from S d and add it to X 0 dk with ac-13: else 14: Pop x  X  from X 0 dk randomly and add it to B dk 15: end if 16: end for 17: Push all element from B dk to S d if B dk exceeds 18: Update  X  0 dk by equation 8 19: end for 20: Update  X  k by equation 9 21: Update  X  k by equation 10, and update partition 22: until convergence 23: end for 24: Normalize {  X  k } X  X   X  0 k } and {  X  0 dk } X  X   X  dk } 25: Return mixture component  X  k and corresponding on the original X d . In this way, we can increase or decrease n dk without affecting other topics directly.

First, we build a stack S d to store X 0 d . Each element x  X   X  S d is randomly drawn from X d with replacement. This ensures that, for all n , the empirical distribution of the first n elements in S d is an approximation to the empirical dis-tribution of X d . We also pre-group the elements in X 0 X dk by the topic assignment.

When we propose an increase on n dk , we pop a new word x  X  from S d and accept the increase with acceptance rate A If the proposal is accepted, we will add x  X  to X 0 dk and assign it to the k th topic. Otherwise it returns to S d .
When we propose a decrease on n dk , we randomly choose one word x  X  from X 0 dk . The acceptance rate A n  X  X  X  If the proposal is accepted, we will delete x  X  from X 0 dk return it to the stack S d . Figure 3: The structure of reconstructed dataset and data flow.
 In our implementation, we also add a buffer B dk between X dk and S d . The word returning to S d will be first stored at B dk , and returned to S d shortly after. This is helpful to avoid consecutive rejections on outliers. m can be em-pirically set proportional to 1 /K , which increases the ac-ceptance rate. Note that a larger value of m will impede convergence at the initial stage, when n dk is small. The overall flow is illustrated in Figure 3. X 0 d serves as an ap-proximation to the original dataset X d . There might be bias in X 0 d due to unassigned but visited elements in stack S The accuracy of this approximation is tested with synthetic datasets in Section 4.1.

This approach appears to be similar to online algorithms, but they are fundamental different: in online settings, we only have one pass of the observations. In our algorithm, although we feed X 0 dk with the data in stream, the rejected data will return to the stack eventually and similarly for the deleted data from X 0 dk . This is crucial because it helps to maintain that the empirical distribution of X 0 d is close to X , otherwise X 0 d could be strongly affected by the selection bias during the add-and-delete process.

Updating  X  0 dk . In the likelihood function, the factors regarding  X  0 dk are which means that  X  0 dk follows a gamma distribution with n dk +  X  k and m + 1 as its scale and shape parameter respec-tively. Therefore, we update  X  0 dk based on n dk and  X  follows
Updating  X  k . In the likelihood function, the factors re-garding  X  k are Because  X  k is usually quite small, the first order Laurent expansion provides a simple and accurate approximation, which is  X ( z )  X  1 /z when | z | &lt; 1.  X  k is approximately distributed as Figure 4: The thread architecture for the proposed parallel sampling algorithm We can either use this approximation, or treat it as the proposed distribution in a Metropolis-Hasting step.
Updating  X  k . We update  X  k based on its posterior dis-tribution. The updating rules for all variables are summarized in Table 2.
As mentioned before, our algorithm updates each topic and its associated variables asynchronously in parallel. Each topic is assigned to a thread. The architecture is showed is Figure 4. Moreover, to minimize the possible conflicts when the same document is accessed by different topics at the same time, we separate the documents to several disjoint subsets at step 8 in Algorithm 1. In each iteration, we up-date the topic only based on a subset of documents and rotate through all subsets, so the conflict can be avoided completely. Similar techniques have been used by [18] in a parallel GPU sampling algorithm for LDA to reduce param-eter storage redundancy and avoid access conflicts.
The only connection across topics is through computing term p x  X  ( k ) when updating n dk . We store the partition p  X  ( k ) = p ( x  X  |  X  k ) /Z ( x  X  ). We update Z periodically to en-sure the accuracy.

Because the updates are made asynchronously for each topic, we store each topic update with its time stamp. So we can sort the updates according to its time stamp and reconstruct the parameters for the whole model.
For Gibbs sampling based parallel sampling algorithm, the variables are updated asynchronously in parallel, which is different from that in sequential Gibbs sampling algorithm, where the order of updates is fixed. The proof of conver-gence for Gibbs sampler does not generalize to all parallel Gibbs sampling algorithms. Fortunately, for some of them, the order of updates has been proven to be irrelevant to a certain extent. For instance, in chromatic Gibbs sampler [6], all possible orders of updates within the same color are indeed equivalent. In this section, we study the convergence for general parallel sampling algorithm based on Gibbs sam-pling. There are two differences: The term  X  X lobal iteration X  is not well defined in asynchronous settings. Here we use it loosely: we separate the clock time to disjoint intervals. If in each time interval, every random variable has been updated at least once, we call such interval as a global iteration. We also define the order of update as follows: Definition 3.1 (Order of update) . An order of update is a sequence of index from the index set of random variables, where each index must occur at least once.

For example, (1 , 2 , 1 , 3) and (3 , 2 , 1) are legitimate orders We model the order of update by a random distribution P O which depends on various factors including the algorithm itself, the computation hardware, the probabilistic model (i.e. the graph structure of the graphical model), and current state of the sample. We prove in the following theorem that if the distribution P O is independent with the current state of the sample, the convergence is guaranteed.
 Theorem 3.1. If a probabilistic model P (  X  ) has positive sup-port and the distribution P O on order of update is indepen-dent with the current state of the sample, the convergence of Gibbs sampler is guaranteed, and it converges to the true distribution P (  X  ) .
 Proof. First, we exam the stationary measure of the Markov chain defined by Gibbs sampler with an arbitrary order of updates O , we first prove that, if the probabilistic model P (  X  ) has positive support, the stationary measure is the true distribution P (  X  ). The proof is very similar to that for Gibbs sampler. Assume that we have x = ( x 1 ,x 2 ,...,x n )  X  P (  X  ), we update the variables according to the order of updates O = ( i 1 ,i 2 ,...,i n O ). For the first step, we have
X where x  X  i represents the set { x 1 ,...,x i  X  1 ,x i +1 after second step, the result x (2) also has the distribution P (  X  ). By simple induction, we prove that after updating x
O , we have x the statement. Note that it indicates that the stationary measure is invariant to different order of updates, and it is indeed the true distribution P (  X  ).

With this statement, we then prove our main theorem with help of the main result in [11], which indicates that for an inhomogeneous Markov chain, when there exists a probability measure P (  X  ) such that each of the different steps corresponds to a nice ergodic Markov kernel with stationary measure P (  X  ), it will converge to P (  X  ). We have already proved that P (  X  ) is the stationary measure of any order of updates, so the Markov kernel at each step has the same stationary measure P (  X  ), which proves the theorem.
The assumption of the independence with the current state of the sample is essential to the conclusion, and simi-lar issue has been discussed in [1] for adaptive MCMC . The validity of such assumption relies on the algorithm and its implementation. For example, in LDA, the speed of updat-ing the word distribution for a topic depends on the current number of words assigned to the topic. However, such de-pendence is not strong enough to affect the convergence of our algorithm, which is confirmed by later experimental re-sults.
In this section, we first study the accuracy of our parallel sampling algorithm (G2PP) on the synthetic dataset, then we test the interpretability of the G2PP on the Bitcoin Blog dataset, and finally we compare with with other baseline al-gorithms on large real world dataset in terms of convergence rate.

We implement the synchronous Gibbs sampler (Synch) [2] based on the posterior sampling with auxiliary variable al-gorithm described in [14], the AVparallel algorithm (AVP) based on the Chinese restaurant franchise (CRF) with global update scheme proposed in [17] and the slice sampler based on [15]. All algorithms are implemented in C++. We test all algorithms under the same hyperparameter setting. Note that the iteration for G2PP and other algorithms are not equivalent, so it is unreasonable to compare the performance based on number of iteration. Therefore, we present our re-sults based on actual runtime.

In our experiments, the convergence rate of AVP is signifi-cantly slower than the rest of candidates. The reported con-vergence time for CRF of a java implementation in the origi-nal paper is around 4000 minutes on the same NIPS dataset, which is far longer than the convergence time of other algo-rithms. The slice sampler also suffers from slow convergence in the preliminary experiments. Note that AVP and slice sampler are theoretically accurate. In contrast, G2PP and Synch are approximate sampling algorithms. Therefore, we only present the results of G2PP, Synch and Gibbs, which is Synch with one processor.
In order to demonstrate the inference accuracy of G2PP, we design two experiments on the synthetic datasets. The algorithm performance is measured by the prediction accu-racy and model perplexity. Table 3: F1 Scores on mixture of Gaussian dataset by G2PP and Synch Table 4: F1 Scores on LDA dataset by G2PP and Synch
We first generate a synthetic dataset with mixture of Gaus-sian, which consists of 50 bivariate Gaussian distributions, each with mean distributed according to Norm(0 , 10) and variance of 0 . 01. The dataset contains one million data points in total.

We evaluate the results according to the F1 score between clusters obtained by each algorithm and the ground truth. We adopt the definition of F1 score in [17], which is defined on the pairwise observations. We define the precision and same cluster under the ground truth, and P ( p ) is all pairs of data points of the same cluster under the prediction. The result is shown in Table 3. The F1 scores shown in the ta-ble represent the average prediction results on the Markov chain, excluding the burn-in period. We can see that G2PP yields comparable and even better performance in terms of prediction accuracy than Gibbs sampler.
We also test our algorithm on a second synthetic dataset generated by latent Dirichlet allocation (LDA). The syn-thetic corpus contains 1000 documents and 10 latent topics. Each document is of the same length of 1000. The vocabu-lary size is set to 1000. The concentration parameters of all Dirichlet distributions are set to 1. We compare the perfor-mance of G2PP with Gibbs sampler and synchronous Gibbs sampler (Synch) [2] on F1 score and model perplexity.
The F1 scores for each method are listed in Table 4 and the perplexity curves over time is shown in Figure 5. As we can see, G2PP performs better in both F1 score and per-plexity. In addition, Synch shows decreasing accuracy when the number of processors increases, because each processor owns smaller subset of data.

In summary, we empirically show that our parallel sam-pling algorithm not only does not compromise on accuracy, but also may achieve slight improvement in part due to the bias induced by X 0 d (which makes it more robust on outliers). Figure 5: The convergence plot for G2PP and Synch with 1, 2, 4, 8 processors on LDA dataset.
 Table 5: Latent topics Inferred by G2PP on Bitcoin dataset
We test our G2PP algorithm on a real world dataset for topic modeling. We collected the online blogs related to the Bitcoin posted between Nov. 19th, 2013 and Dec. 20th, 2013 based on Google News search results. Bitcoin is a peer-to-peer payment system and digital cryptocurrency introduced as open source software in 2009 by pseudonymous developer Satoshi Nakamoto, which becomes extremely popular in the last few months of 2013.

We preprocessed the dataset in a standard manner. We only consider the blog posts in English, which leads to a collection of 1899 documents with averaged length of 292 and 7379 unique words. The top 5 latent topics and the top 10 words associated with each topic by G2PP is listed in Table 5. We also present the results of Gibbs sampler in Table 6. Both results are intuitive and reasonable. For example, in Table 5, Topic 1 is the introduction of Bitcoin , Topic 3 is the events related to China, which is the main cause for both soar and drop in Bitcoin price. Moreover, the resemblance between the results from Gibbs and G2PP is remarkable, which indicates that the results of G2PP are as reliable as the Gibbs sampler. Moderate differences in the word and topic ranking are due to the randomness in the inference process. Table 6: Latent topics inferred by Gibbs on the Bit-coin dataset
With the results on the synthetic datasets and this real-world application dataset, we can safely confirm that the inference results by G2PP is accurate in terms of both statis-tical criteria and topic interpretation. Next, we investigate the performance of convergence rate.
We choose two benchmark datasets, i.e., NIPS1-17 (NIPS) dataset 1 and NYTimes news (NYT) dataset 2 for conver-gence analysis. The statistics of the two datasets are listed in Table 7.
We split the dataset into a training set with 2284 docu-ments and a test set with 200 documents. We evaluate the algorithm by the perplexity on the test set. The conver-gence time and final perplexity are listed in Table 8. The typical convergence curves are shown in Figure 6. The re-sult shows that G2PP performs well in terms of convergence speed, accuracy and scalability with respect to the number of processors.

Note that as the number of processors increases, the per-plexity value by Synch increases, which means that the ac-curacy decreases. Moreover, Synch becomes more likely to be trapped in local optimal. By  X  X ocal optimal X , we refer to the phenomenon that the perplexity stably stays at a higher level than the known optimal value before reaching it. For Synch on 16 processors, it is possible that all runs are trapped in local optimal.
NYTimes news dataset is a large dataset contains over 100 million words. The inference of HDP on such a dataset is extreme time-consuming and impractical. Efficient inference http://ai.stanford.edu/~gal/data.html http://archive.ics.uci.edu/ml/datasets/Bag+of+ Words [3] Figure 6: The convergence plot for G2PP and Synch with 1, 4, 16 processors on NIPS dataset.
 Table 8: The experiment results of different sam-pling algorithms on NIPS dataset. Note that the results marked by  X  represent those didn X  X  converge within limited time.
 algorithm on such scale will significantly widen the range of application for the powerful HDP.

We selected a test set of 3000 articles from the dataset and used the rest as the training set. We evaluate the algorithm by the perplexity on the test set. The convergence curves are illustrated in Figure 7. All the results are obtained on 16 processors.
 On NYTimes dataset, G2PP can converge in a short time. While there is no good way (except for running the exper-iment for infinite time) to tell whether Synch finally con-verged or trapped in  X  X ocal optimal X , it is safe to conclude that G2PP can provide better parameter estimation than Synch within a reasonable time limit.
In section 4.3, the G2PP algorithm performs consistently better than other baselines. But the underlying reason is unclear at this point. The major question is that because the G2PP performs bootstrap on the original dataset, is the speedup merely an outcome of subsampling on the original dataset? As the computation complexity is approximately proportional to the total length of documents, inference on the subsampled dataset should yield competitive speedup at the expenses of inference accuracy. Subsampling on the original dataset also places an unknown effect on the  X  X ocal Figure 7: The convergence plot on NYT dataset for G2PP and Synch with 16 processors. Figure 8: The convergence plot for Gibbs with sub-sampling on NIPS. # P denotes the number of pro-cessors, and C is the percentage of subsampling. optimal X  phenomenon. We design a series of experiments to answer this question.

In one experiment, we test the speedup and the accuracy of Gibbs algorithm with dataset subsampling. For each run, we first subsample the training set of NIPS dataset to the C % of its original length, then we conduct the inference only based on the subsampled dataset. The perplexity is tested on same test set as in section 4.3.1. We choose C 8. The results by G2PP on the original dataset are also included for comparison.

As shown in Figure 8, subsampling provides a flexible tradeoff between speed and accuracy. Note that the speedup is not proportional to 1 /C , because subsampling affects both the computational complexity of each iteration and the over-all convergence rate. By varying the subsampling rate, we can achieve comparable speedup similar to parallel algo-rithms with 4 processors on the NIPS dataset. But the ac-curacy decreases quickly when we further lower the subsam-pling rate. More importantly, we observe that the G2PP algorithm provides better trade off between accuracy and convergence rate comparing to Gibbs sampling with sub-sampling. This demonstrates that unique factors other than bootstrapping contribute to the gain in performance by G2PP.
In this paper, we proposed a parallel Gibbs sampling al-gorithm for HDP based on gamma-gamma-Poisson process. By actively bootstrapping the dataset, we constructed a new dataset with flexible size. Together with Reversible Jump MCMC, we proposed a parallel Gibbs sampling algo-rithm where each mixture component can update in paral-lel. The proposed algorithm is also suitable for distributed system. We showed its accuracy on synthetic datasets and remarkable speedup comparing with other HDP sampling algorithms on large scale real world dataset. We also ex-amined convergence for parallel Gibbs sampling algorithm with asynchronous variable updates, and we provided the necessary condition to ensure convergence.

For future work, we will further improve the proposed al-gorithm from the following perspectives: First, we will study the systematic approach for setting parameter m ; Second, we will investigate how to choose a dynamic upper bound on the number of mixture components; Lastly, we will exam-ine the generality of the gamma-gamma-Poisson process for parallel inference over other nonparametric mixture models.
We thank Shang-Hua Teng, Eric Xing, Jun Zhu for dis-cussions, and Xinran He for proof-reading. The research was sponsored by the NSF research grants IIS-1134990, NSF re-search grants IIS-1254206, and Okawa Foundation Research Award. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agency, or the U.S. Government. [1] C. Andrieu, N. De Freitas, A. Doucet, and M. I. [2] A. Asuncion, P. Smyth, and M. Welling.
 [3] K. Bache and M. Lichman. UCI machine learning [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] T. S. Ferguson. A bayesian analysis of some [6] J. Gonzalez, Y. Low, A. Gretton, C. Guestrin, and [7] J. Gonzalez, Y. Low, and C. Guestrin. Residual splash [8] J. Gonzalez, Y. Low, C. Guestrin, and D. O X  X allaron. [9] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, [10] D. Newman, A. Asuncion, P. Smyth, and M. Welling. [11] L. Saloff-Coste and J. Z  X u  X niga. Convergence of some [12] K.-A. Sohn and E. P. Xing. A hierarchical dirichlet [13] E. Sudderth, A. Torralba, W. Freeman, and [14] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. [15] S. G. Walker. Sampling the dirichlet mixture model [16] Y. Wang, H. Bai, M. Stanton, W.-Y. Chen, and E. Y. [17] S. Williamson, A. Dubey, and E. P. Xing. Parallel [18] F. Yan, N. Xu, and Y. Qi. Parallel inference for latent [19] M. Zhou and L. Carin. Augment-and-conquer negative
