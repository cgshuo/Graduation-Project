 As a crucial task for discourse analysis, discourse relation recognition (DRR) aims to automatically identify the internal structure and logical relation-ship of coherent text (e.g., T EMPORAL , C ONTIN -GENCY , E XPANSION , etc). It provides important information to many other natural language pro-cessing systems, such as question answering (Ver-berne et al., 2007), information extraction (Cimi-ano et al., 2005), machine translation (Guzm  X  an et al., 2014) and so on. Despite great progress in ex-plicit DRR where the discourse connectives (e.g.,  X  because  X ,  X  but  X  et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), im-plicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008).

Conventional methods for implicit DRR di-rectly rely on feature engineering, wherein re-searchers generally exploit various hand-crafted features, such as words, part-of-speech tags and production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and De-nis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these man-ual features are labor-intensive and weak to cap-ture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods.
Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specifically for implicit DRR. The neglect of this important domain may be due to the follow-ing two reasons: (1) discourse relation distribution is rather unbalanced, where the generalization of deep models is relatively insufficient despite their powerful studying ability; (2) training dataset in implicit DRR is relatively small, where overfitting problems become more prominent.

In this paper, we propose a Shallow Convolu-tional Neural Network (SCNN) for implicit DRR, with only one simple convolution layer on the top of word vectors. On one hand, the network structure is simple, thereby overfitting issue can be alleviated; on the other hand, the convolution operation and nonlinear transformation help pre-serve the recognition ability of SCNN. This makes our model able to generalize better on the test dataset. We performed evaluation for English im-plicit DRR on the PDTB-style corpus. Experi-mental results show that the proposed method can obtain comparable even better performance when compares against several baselines. In Penn Discourse Treebank (PDTB) (Prasad et al., 2008), implicit discourse relations are anno-tated with connective expressions that best convey implicit relations between two neighboring argu-ments, e.g.
 the connective  X  But  X , which is annotated manu-ally, is used to express the inferred COMPARISON relation.

We learn a classifier for implicit DRR based on convonlutional neural network. The overall model model, each word in vocabulary V corresponds to a d -dimensional dense, real-valued vector, and all words are stacked into a word embedding matrix
Given an ordered list of n words in an argument, we retrieve the i -th word representation x v from L with its corresponding vocabulary index v . All word vectors in the argument produce the following output matrix: Following previous work (Collobert et al., 2011; Socher et al., 2011a), for each row r in X , we explore three convolutional operations to obtain three convolution features average , min and max as follows: In this way, SCNN is able to capture almost all im-portant information inside X (one with the highest, lowest and average values). Besides, each convo-lution operation naturally deals with variable argu-ment lengths (Note that c  X  R d ). Back to Figure 1, we present c avg , c min and c max with red, purple and green color respectively.

After obtaining the features of both arguments, we concatenate all of them into one vector, and then apply tanh transformation and length nor-malization successively to generate the hidden lay-ers: a = where h  X  R 6 d is the hidden layer representa-tion. The normalization operation scales the com-ponents of a feature vector to unit length. This, to some extent, eliminates the manifold differences among different features.

Upon the hidden layer, we stack a Softmax layer for relation recognition, parameter matrix, b  X  R l is the bias term, and l is the relation number.

To assess how well the predicted relation y rep-resents the real relation, we supervise it with the gold relation g in the annotated training corpus us-ing the traditional cross-entropy error, Combined with the regularization error, the joint training objective function is where m is the number of training instances, y t is the t -th predicted distribution,  X  is the regulariza-tion coefficient and  X  is parameters, including L , W and b . 2 To train SCNN, we first employ the toolkit word embedding matrix L using a large-scale un-labeled data. Then, L-BFGS algorithm is applied to fine-tune the parameters  X  . We conducted a series of experiments on English implicit DRR task. After a brief description of the experimental setup and the baseline systems, we further investigated the effectiveness of our method with deep analysis. 3.1 Setup For comparison with other systems, we formu-lated the task as four separate one-against-all bi-nary classification problems: one for each top level sense of implicit discourse relations (Pitler et al., 2009).

We used the PDTB 2.0 corpus 4 (Prasad et al., 2008) for evaluation. The PDTB corpus contains discourse annotations over 2,312 Wall Street Jour-nal articles, and is organized in different sections. Following Pitler et al. (2009), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter op-timization. For each relation, we randomly ex-tracted the same number of positive and negative instances as training data, while all instances in sections 21 and 22 are used as our test set. The statistics of various data sets is listed in Table 1. We tokenized PDTB corpus using Stanford NLP Table 1: Statistics of positive and negative in-stances in training (Train), development (Dev) and test (Test) sets. C OMP .=C OMPARISON , C T d = 128 and  X  = 1 e  X  4 . Besides, the unlabeled data for word embedding initialization contains 1.02M sentences with 33.5M words. 3.2 Baselines We compared our model against the following baseline methods:  X  SVM: This method learns a support vector  X  TSVM: This method learns a transductive  X  RAE: This method learns a recursive autoen-
Rutherford and Xue (2014) show that Brown cluster pair feature is very impactful in implicit DRR (Rutherford and Xue, 2014). This feature is superior to one-hot representation for the in-teractions between two arguments, such as cross-argument word pair features in our baseline meth-ods. We therefore conducted two additional exper-iments for comparison:  X  Add-Bro: This method learns an SVM clas- X  No-Cro: This method learns an SVM clas-
In addition, to further verify the effectiveness of normalization, we also compared against SCNN model without normalization ( SCNN-No-Norm ).
Throughout our experiments, we used the toolkit SVM-light 6 (Joachims, 1999) in all the SVM-related experiments. Following previous work (Pitler et al., 2009; Lin et al., 2009), we adopted the following features for baseline meth-ods: Bag of Words: Three binary features that check whether a word occurs in A rg1, A rg2 and both ar-guments.
 Cross-Argument Word Pairs: We group all words from A rg1 and A rg2 into two sets W 1 , W 2 respectively, then extract any possible word pair ( w i ,w j )( w i  X  W 1 ,w j  X  W 2 ) as features. Polarity: The count of positive, negated positive, negative and neutral words in A rg1 and A rg2 ac-cording to the MPQA corpus (English). Their cross products are used as features.
 First-Last, First3: The first and last words of each argument, the pair of the first words in two arguments, the pair of the last words in two argu-ments, and the first three words of each argument are used as features.
 Production Rules: We extract all production rules from syntactic trees of arguments. We de-fined three binary features for each rule to check whether this rule appear in A rg1, A rg2 and both arguments.
 Dependency Rules: We also extracted all de-pendency rules from dependency trees of argu-ments. Similarly, we defined three binary features for each rule to check whether this rule appear in A rg1, A rg2 and both arguments.

In order to collect bag of words, production rules, dependency rules, and cross-argument word pairs, we used a frequency cutoff of 5 to remove rare features, following Lin et al. (2009). 3.3 Results and Analysis All models are evaluated by assessing the accuracy and F1 scores on account of the imbalance in test set. Besides, for better analysis, we also provided the precision and recall results.

Table 2 summarizes the performance of dif-ferent models. On the whole, the F1 scores for implicit DRR are relatively low on average: C
OMP ., C ONT ., E XP . and T EMP . about 32%, 50%, 65% and 28% respectively. This illustrates the difficulty in implicit DRR. Although we ex-pected unlabeled data could obtain improvement, we observed negative results appeared in TSVM : C
OMP . and C ONT . dropped 1.14% and 0.79% re-spectively 7 . The F1 scores of T EMP . and E XP . are improved (1.27% and 0.63% respectively). The main reason may be that our unlabeled data is not strictly from the discourse corpus.

Incorporating Brown cluster pair features en-hances the recognition of C OMP . and C ONT .. Par-ticllarly, No-Cro achieves the best result in C OMP . 34.22%. But we found no consistent improve-ment in E XP . and T EMP .: No-Cro loses 2.74% in T
EMP .; Add-Bro loses 0.88% and 2.12% in E XP . and T EMP . respectively. This result is inconsistent with the finding of Rutherford and Xue (2014). The reason may lie in the training strategy, where we used sampling to solve the problem of unbal-anced dataset while they reweighted training sam-ples.

Compared with SVM-based models, RAE per-forms poorly in three relations, except E XP . which has the largest training dataset. Maybe RAE needs more labeled training data for better re-sults. However, SCNN models perform remark-ably well, producing comparable and even bet-ter results. Without normalization, SCNN-No-Norm gains 0.57%, 2.98% and 3.1% F1 scores for C
ONT ., E XP . and T EMP . respectively, but loses 2.11% for C OMP .. We obtain further improvement using SCNN with normalization: 0.71%, 2.17%, 6.52% and 4.93% for C OMP ., C ONT ., E XP . and T
EMP . respectively. This suggests that normaliza-tion is useful for generalization of shallow models.
From Table 2, we found that our models do not achieve consistent improvements in precision, but benefit greatly from the gains of recall. Besides, our model works quite well for small dataset (Both accuracy and F1 score are improved in T EMP .). All of these demonstrate that our model is suitable for implicit DRR. In this paper, we have presented a convolutional neural network based approach to learn better DRR classifiers. The method is simple but effec-tive for relation recognition. Experiment results show that our approach achieves satisfactory per-formance against the baseline models.

In the future, we will verify our model on other languages, for example, Chinese and Arabic. Be-sides, since our model is general to classification problems, we would like to investigate its effec-tiveness on other similar tasks, such as sentiment classification and movie review classification, etc. The authors were supported by National Nat-ural Science Foundation of China (Grant Nos 61303082 and 61403269), Natural Science Foundation of Jiangsu Province (Grant No. BK20140355), Natural Science Foundation of Fu-jian Province of China (Grant No. 2013J01250), the Special and Major Subject Project of the Industrial Science and Technology in Fujian Province 2013 (Grant No. 2013HZ0004-1), and 2014 Key Project of Anhui Science and Technology Bureau (Grant No. 1301021018). We thank the anonymous reviewers for their insightful comments. We are also grateful to Kaixu Zhang for his valuable suggestions.

