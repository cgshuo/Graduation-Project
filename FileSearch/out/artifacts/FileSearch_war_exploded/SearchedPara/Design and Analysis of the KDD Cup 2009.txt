 We organized the KDD cup 2009 around a marketing prob-lem with the goal of identifying data mining techniques ca-pable of rapidly building predictive models and scoring new entries on a large database. Customer Relationship Manage-ment (CRM) is a key element of modern marketing strate-gies. The KDD Cup 2009 offered to participants an op-portunity to work on a large marketing database from the French Telecom company Orange. The tasks were to predict the propensity of customers to switch provider (churn), buy new products or services (appetency), or buy upgrades/add-ons proposed to them to make the sale more profitable (up-selling). The challenge, which lasted from March 10 to May 11, 2009, attracted over 450 participants from 46 coun-tries. We attribute its popularity to several factors: (1) A generic problem relevant to the Industry (a classifica-tion problem), but presenting a number of scientific and technical challenges, including many missing values (about 60%), a large number of features (15000) and a large num-ber of training examples (50000), unbalanced class propor-tions (fewer than 10% of the examples of the positive class), noisy data, and the presence of categorical variables with many different values. (2) Prizes (Orange offers 10000 Eu-ros in prizes). (3) A well designed protocol and web site (we benefitted from past experience). (4) An effective ad-vertising campaign using mailings and a teleconference to answer potential participants questions. The results of the challenge were discussed at the KDD conference (June 28, 2009). The principal conclusions are that ensemble meth-ods are very effective and that ensemble of decision trees offer off-the-shelf solutions to problems with large numbers of samples and attributes, mixed types of variables, and lots of missing values. The data and the platform of the chal-lenge remain available for research and educational purposes at http://www.kddcup-orange.com/ . Customer Relationship Management (CRM) is a key ele-ment of modern marketing strategies. The KDD Cup 2009 offered the opportunity to work on large marketing databases from the French Telecom company Orange to predict the propensity of customers to switch provider (churn), buy new  X 
Corresponding author isabelle@clopinet.com participants were provided with masked customer records and their goal was to predict whether a customer will switch provider (churn), buy the main service (appetency) and/or buy additional extras (up-selling), hence solving three bi-nary classification problems. Churn is the propensity of customers to switch between service providers, appetency is the propensity of customers to buy a service, and up-selling is the success in selling additional good or services to make a sale more profitable. Although the technical diffi-culty of scaling up existing algorithms is the main emphasis of the challenge, the dataset proposed offers a variety of other difficulties: heterogeneous data (numerical and cat-egorical variables), noisy data, unbalanced distributions of predictive variables, sparse target values (only 1 to 7 percent of the examples examples belong to the positive class) and many missing values. There is value in a CRM system to evaluate the propen-sity of customers to buy. Therefore, tools producing scores are more usable that tools producing binary classification results. The participants were asked to provide a score (a discriminant value or a posterior probability P ( Y = 1 | X )), and they were judged by the area under the ROC curve (AUC). The AUC is the area under the curve plotting sen-sitivity vs. (1  X  specificity) when the threshold  X  is varied (or equivalently the area under the curve plotting sensitivity vs. specificity). We call  X  X ensitivity X  the error rate of the positive class and  X  X pecificity X  the error rate of the negative class. The AUC is a standard metric in classification. There are several ways of estimating error bars for the AUC. We used a simple heuristic, which gives us approximate error bars, and is fast and easy to implement: we find on the AUC curve the point corresponding to the largest balanced accuracy BAC = 0.5 (sensitivity + specificity). We then estimate the standard deviation of the BAC as: where m + is the number of examples of the positive class, m  X  is the number of examples of the negative class, and p + and p  X  are the probabilities of error on examples of the positive and negative class, approximated by their empirical estimates, the sensitivity and the specificity [14]. The fraction of positive/negative examples posed a challenge to the participants, yet it was sufficient to ensure robust prediction performances (as verified in the beta tests). The database consisted of 100,000 instances, split randomly into equally sized train and test sets: On-line feed-back on AUC performance was provided to the participants who made correctly formatted submissions, us-ing only 10% of the test set. There was no limitation on the number of submissions, but only the last submission on the had to download the data), the same data sample was used for the three marketing tasks. In a second step, the fea-ture construction language was used to generate 20,000 fea-tures and obtain a tabular representation. After discarding constant features and removing customer identifiers, we nar-rowed down the feature set to 15,000 variables (including 260 categorical variables). In a third step, for privacy reasons, data was anonymized, discarding variables names, random-izing the order of the variables, multiplying each continuous variable by a random factor and recoding categorical vari-able with randomly generated category names. Finally, the data sample was split randomly into equally sized train and test sets. A random subset of 10% of the test set was desig-nated to provide immediate performance feed-back. The website of the challenge http://www.kddcup-orange. com/ was thoroughly tested by the KDD cup chairs and vol-unteers. The datasets were downloaded and checked. Base-line methods were tried to verify the feasibility of the task. A Matlab R  X  version of the data was made available and sample code were provided to format the results. A sample submis-sion of random results was given as example and submitted to the website. The results of the Na  X  X ve Bayes method were also uploaded to the website to provide baseline results. The Toy problem on the LARGE dataset consisted of one single predictive continuous variable (V5963) uniformly dis-tributed on the interval [0 , 2 . 0]. The target value was ob-tained by thresholding V5963 at 1 . 6 and adding 20% noise. Hence for 80% of the instances, lying in interval [0 , 1 . 6], the fraction of positive examples is 20%; for the remaining 20% lying in interval ]1 . 6 , 2 . 0], the fraction of positive examples is 80%. The expected value of the AUC (called  X  X rue AUC X ) can easily be computed 1 . Its value is approximately 0 . 7206. Because of the variance in the sampling process, the AUC effectively computed using the optimal decision rule (called  X  X ptimal AUC X ) is 0 . 7196 for the training set and a 0 . 7230 for the test set. Interestingly, as shown in Figure 1, the opti-mal solution was outperformed by many participants, up to 0.7263. This illustrates the problem of multiple testing and shows how the best test performance overestimates both the expected value of the AUC and the performance of the opti-mal decision rule, increasingly with the number of challenge submissions.
If we call T the total number of examples, the (expected value of) the total number of examples of the positive class P is the sum of the number of positive examples in the first and the second intervals, i.e., P = (0 . 2  X  0 . 8 + 0 . 8  X  0 . 2) T = 0 . 32 T . Similarly, the total number of negative examples is N = (0 . 8  X  0 . 8 + 0 . 2  X  0 . 2) T = 0 . 68 T . If we use the optimal decision rule (a threshold on V5963 at 1 . 6) the number of true positive examples is the sum of the number of true positive examples in the two intervals, i.e., TP = 0 + (0 . 2  X  0 . 8) T = 0 . 16 T . Similarly, the number of true negative examples is TN = (0 . 8  X  0 . 8) T = 0 . 64 T . Hence, the true positive rate is TPR = TP/P = 0 . 16 / 0 . 32 = 0 . 5 and the true negative rate is TNR = TN/N = 0 . 64 / 0 . 68 ' 0 . 9412. The balanced accuracy (or the AUC because BAC = AUC in this case) is therefore: BAC = 0 . 5 ( TPR + TNR ) = 0 . 5 (0 . 5 + 0 . 9412) = 0 . 7206. However, for the challenge, the participants were not placed under all these constraints for practical reasons: it would have been both too constraining for the participants and too difficult to enforce for the organizers. The challenge focused on maximizing accuracy under time constraints. For the LARGE dataset, the overall score of the Orange in-house classifier is 0.8311, with the following results on the test dataset: The challenge was to beat these results, but the minimum requirement to win prizes was only to outperform the basic Na  X  X ve Bayes classifier. The key elements of our design were: The competition rules were inspired from previous chal-lenges we organized [8]. The full rules are available from the website of the challenge http://www.kddcup-orange.com/ . They were designed to attract a large number of participants and were successful in that respect: Many participants did not participate in the fast challenge on the large dataset, Table 1: Prize winners. Top 3: fast track. Bottom 3: slow track.
 U. Melbourne, Australia 27 0.8250 2 0.8484 The overall winner is the IBM Research team [16] who ranked first in both tracks. Six prizes were donated by Orange to top ranking participants in the fast and the slow tracks (see Table 1). As per the rules of the challenge, the same team could not earn two prizes. If the ranking of a team entitled it to two prizes, it received the best of the two and the next best ranking team received the other prize.
 All the winning teams scored best on the large dataset (and most participants obtained better results on the large dataset then on the small dataset). IBM Research, ID Analytics, and National Taiwan University (NTU)  X  X nscrambled X  the small dataset. This may have provided an advantage only to NTU since  X  X nscrambling X  affected only the slow track and the two other teams won prizes in the fast track. We briefly comment on the results of the winners.
 This last point was important for Orange to assess the time taken for generating state-of-the-art models, since speed of model generation is a key requirement in such applications. Furthermore, very small improvements (from 0.8493 to 0.8521) were made after the 5 th day (SLOW challenge) 2 .
 One of the aims of the KDD cup 2009 competition was to find whether there are data-mining methods which are sig-nificantly better than others. To this end we performed a significance analysis on the final results (last submission be-fore the deadline, the one counting towards the final ranking and the selection of the prize winners) of both the SLOW and FAST track. Only final results reported on the large dataset were included in the analysis since we have realized that submissions based on the small dataset were consider-ably inferior.
 To test whether the differences between the teams are sta-tistically significant we followed a two step analysis that is specifically designed for multiple hypothesis testing when several independent task are involved [9]: First we used the Friedman test [13], to examine the null hypothesis H 0 , which states that the AUC values of the three tasks, (Churn, Appe-tency and Up-Selling) on a specific track (FAST or SLOW) are all drawn from a single distribution. The Friedman test is a non-parametric test, based on the average ranking of each team, where AUC values are ranked for each task sep-arately. A simple test-statistic of the average ranks is suf-ficient to extract a p-value for H 0 ; In the case when H 0 is rejected, we use a two tailed Nemenyi test [20] as a post-hoc analysis for identifying teams with significantly better or worse performances.
 Not surprisingly, if one takes all final submissions, one finds that H 0 is rejected with high certainty (p-value &lt; 10  X  12 ). Indeed, significant differences are observed even when one inspects the average final AUCs (see Figure 3), as some sub-missions were not substantially better than random guess, with an AUC near 0.5. Of course, Figure 3 is much less in-formative than the significance testing procedure we adopt, which combines the precise scores on the three tasks, and not each one separately or their averages.
 Trying to discriminate among the top performing teams is more subtle. When taking the best 20 submissions per track (ranked by the best average AUC) -the Friedman test still rejects H 0 with p-values 0 . 015 and 0 . 001 for the FAST and SLOW tracks respectively. However, the Nemenyi tests on these reduced data are not able to identify significant differ-
This improvement may be partly attributed to  X  X nscram-bling X ; unscrambling was not possible during the fast track of the challenge (first 5 days). employed. In the SLOW track, the participants were largely free of such constraints and many used abundant computer and human resources. Our analysis shows that significant improvements in performance are difficult to obtain, even at the expense of a huge deterioration of the other criterions. Figure 4.d gives a comparison between the submissions re-ceived and the best overall result over increasing periods of time: 12 hours, one day, 5 days, and 36 days. We compute the relative performance difference where TestAUC  X  X  X  is the best overall result. The values of  X  for the best performing classifier in each interval and for the reference results are found in Table 2. On each box, the central mark is the median, the edges of the box are the 25 th and 75 th percentiles, the whiskers extend to the most extreme data points not considered to be outliers; the outliers are plotted individually as crosses.
 The following observations can be made: Table 2 reveals that, at the end of the challenge, for the average score, the relative performance difference between the baseline model (basic Na  X  X ve Bayes) and the best model is over 20%, but only 2 . 46% for SNB. For the best rank-ing classifier, only 0 . 33% was gained between the fifth day (FAST challenge) and the last day of the challenge (SLOW challenge). After just one day, the best ranking classifier was only 1 . 60% away from the best result. The in-house system (selective Na  X  X ve Bayes) has a result less than 1% worse than the best model after one day.
 We conclude that the participants did very well in building models fast. Building competitive models is one day is defi-nitely doable and the Orange in-house system is competitive, although it was rapidly beaten by the participants. To assess the relative difficulty of the three tasks, we plot-ted the relative performance difference  X   X  (Equation 2) for increasing periods of time, see Figure4.[a-c].
 The churn task seems to be the most difficult one, if we The appetency task is of intermediate difficulty. Its day The up-selling task is the easiest one: the day one perfor-There is a good correlation between the results on the test set (100% of the test set), TestAUC , and the results on the validation set (10% of the test set used to give a feed back to the competitors), V alidAUC . We computed the Pear-son correlation coefficient after removing the result having a test AUC lower than 0.5 (probably corresponding to er-rors in submissions). We obtained 0.9960  X  0.0005 (95% confidence interval) for the first 5 days and of is of 0.9959  X  0.0003 for the 36 days. This seems to indicate that, (i) the validation set provided useful feedback to the partici-pants, without compromising the test set; (ii) the partici-pants did not overfit the validation set. 3 The analysis of correlation task by task gives the same information, on the entire challenge (36 days) the correlation coefficient is for the Churn task: 0.9860  X  0.001; for the Appetency task: 0.9875  X  0.0008 and for the Up-selling task: 0.9974  X  0.0002. We also asked the participants to return training set predic-tion results, hoping that we could do an analysis of overfit-ting by comparing training set and test set performances. Unfortunately, because the training set results did not af-fect the ranking score, some participants did not return real predictions made by their classifier, but rather ran-dom results or the target labels. However, if we exclude extreme performances (random or perfect), we can observe that (i) a fraction of the models performing well on test data have a good correlation between training and test per-formances; (ii) there is a group of models performing well on test data and having an AUC on training examples signifi-cantly larger. Large margin models like SVMs [2] or boost-ing models [11] behave in this way. Among the models per-forming poorly on test data, some clearly overfitted (had a large difference between training and test results). We analyzed the information provided by the participants in the fact sheets to determine which methods were employed to tackle the challenge:
Many participants took the precaution of using cross-validation on training data for model selection, rather than using the validation set performance, to avoid overfitting the validation set. The results of the KDD cup 2009 exceeded our expectations in several ways. First we reached a very high level of par-ticipation: over three times as much as the most popular KDD cups so far. Second, the participants turned in good results very fast: within 7 hours of the start of the FAST track challenge they exceeded the baseline provided by Or-ange. The performances were only marginally improved in the rest of the challenge, showing the maturity of data min-ing techniques. Ensemble of decision trees offer off-the-shelf solutions to problems with large numbers of samples and at-tributes, mixed types of variables, and lots of missing values. Ensemble methods proved to be effective for winning, but single models are still preferred by many customers. Further work include matching the performances of the top ranking participants with single classifiers. We are very grateful to the Orange company who donated the data, the computer servers, many hours of engineer time, and the challenge prizes. We would like to thank all the Or-ange team members, including Fabrice Cl  X erot and Raphael F  X eraud. We also gratefully acknowledge the ACM SIGKDD and the Pascal2 European network of excellence who sup-ported the work of the KDD Cup 2009 co-chairs Isabelle Guyon and David Vogel and the web site development. The support of Google and Health Discovery Corporation al-lowed students to attend the workshop. We are very grateful to the technical support of MisterP and Pascal Gouzien. [1] H. Akaike. Information theory and an extension of [2] B. E. Boser, I. Guyon, and V. Vapnik. A training al-[3] M. Boull  X e. Compression-based averaging of Selective
