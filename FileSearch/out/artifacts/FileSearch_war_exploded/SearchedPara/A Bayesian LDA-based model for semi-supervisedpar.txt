 over 97% on in-domain data [1, 2], the performance on unknown in-domain words is below 90% and the performance on unknown out-of-domain words can be be low 70% [3]. Additionally, few our model substantial advantage over standard HMM-based mo dels.
 sparse prior and employing a Bayesian approach contributes significantly to performance. [7], with an error reduction of up to 57% when the amount of sup ervision is small. speech tags (labels) T = { t model (on the right). part-of-speech tag in the corresponding context.
 context model (separated by a dotted line in the figure).
 For every word type w class s tokens of w The ambiguity class s the different values for s observed in the dictionary. The ambiguity class s different morphological features m of the orthographic form of w of distribution selected by its morphology features m are always observed (they are determined by w model. We discuss the ambiguity class model in detail in Sect ion 3.1. In the second step the word context model generates all insta nces w part-of-speech tags t multinomial distribution  X  eters  X  support in  X  w t according to tag-specific (depending on t ken c sentence He often walks to school , the context words of that instance of walks are c c = to , and c models for part-of-speech tagging in different ways [4, 8]. Each context word c by a multinomial with parameters  X  parameters  X  . The parameters  X  A sparse Dirichlet prior on  X  accurate on ambiguous words, indicating that the distribut ion is heavily skewed. in the corpus. Each context word c from a word (document) specific distribution  X  of multiple context features.
 in unsupervised parsing). given an (incomplete) tagging dictionary and a set of natura l language sentences. via the variables s 3.1 Ambiguity class model: details and parameter estimatio n consist of one or two elements.
 multinomial parameters  X  u which selects a tag form the ambiguity class with uniform dis tribution.  X  parameters using EM (with add-1 smoothing in the M-step). 3.2 Parameter estimation for the word context model and pred iction given complete nary, the ambiguity class s  X  , given  X  and the ambiguity classes s ambiguity classes for all word types. We use  X  to denote the vector of all multinomials  X  denote the vector of all  X  likelihood we would like to maximize is: L ( ing the most likely assignment of the part-of-speech tag var iables to instances of words. the LDA model [11]. It depends on variational parameters  X  independent. Each  X  ters  X  multinomial  X  below for simplicity): E fixed variational parameters  X   X  maximize with respect to  X  with respect to  X  using a simple grid search.
 For predicting the tags t Since according to Q all tags t Q 3.3 Parameter estimation for the word context model and pred iction with incomplete where for all words, the ambiguity classes s the ambiguity classes s ical features m the unknown word types.
 hidden variables corresponding to that word type: We now introduce a variational distribution including new h idden variables s That is, for each possible ambiguity class s rameters specific to that ambiguity class. Instead of single variational parameters  X  word with known s of eters. The derivation is slightly complicated by the fact th at s to w rithm as described in Section 3.2, for word types whose ambig uity classes s by the variational distribution Q . For each possible tag set s of tags given that ambiguity class t  X  ( s We compute P ( t  X  ( s  X  whose approximate posterior given the data is Dirichlet wit h parameters  X  with respect to  X  available, and in the other two settings the coverage of the d ictionary is greatly reduced. which is a subset of the training data and is the same test set u sed by [7, 9]. The models in the table are: model is irrelevant when a compete dictionary is available b ecause all s two settings for the LDA model we assume that s variational parameters  X  word types was detrimental to performance.
 LDA+AC is our full model including the model of ambiguity classes of words given their mor-tively.
 the word-specific distributions over tags  X  the topic-specific multinomials for context words  X  estimates for these parameters by applying an EM algorithm. We do add-1 smoothing for  X  the ambiguity class model for s assumed to have ambiguity classes containing all 17 tags. PLSA+AC extends the PLSA model by the inclusion of the ambiguity class model.
 the table are for a model which incorporates morphological f eatures. of speech tags and p ( t non-Bayesian HMM model, whose results we show as well.
 the HMM and CE models whose results are shown in the Table have been trained on less unsu-beled data are used with these models.
 outperforms ML HMM (and even the Bayesian HMM) models shows t hat predicting the tags of of PLSA. LDA achieves an error reduction of up to 36% over PLSA . Third, our ambiguity class also outperforms the oracle models.
 class model and our word context model.

