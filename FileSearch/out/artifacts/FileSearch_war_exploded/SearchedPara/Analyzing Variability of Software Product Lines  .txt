 Software Product Line Engineering (SPLE) is an approach to systematically reuse software-related artifacts among different, yet similar, software products [ 6], [ 19]. Reuse of artifacts, such as requirements specifications, design documents and code, often results in the creation of a myriad of variants. Managing such a variety of arti-facts X  variants is a significant challenge. Thus, SPLE promotes the definition and management of software product lines (SPLs), which are families of similar software systems, termed software products. An important aspect of SPLE is managing customized, or configured for us e in a particular context X  [ 10]. 
In SPLE, different artifacts need to be managed. Of those, requirements manage-expectations of different stakeholders from the requested system. These stakeholders include users and customers and not just developers. Second, requirements are the drivers of other development activities, including analysis, design, implementation, and testing. Finally, requirements are relevant to many development methods, includ-ing agile ones (through concepts such as user stories). 
Several studies have suggested using requirements specifications in order to identi-fy and analyze commonality and variability of software products. In these studies, requirements are operationalized or realized by features, and variability is usually represented as feature diagrams , the main aid for representing and managing variabili-ty in SPLE [ 5], [ 11]. The current studies commonly apply only semantic similarity metrics, that is, seek similarities of terminology, in order to identify common features, create feature diagrams, and analyze the variability of the resultant feature diagrams. As we will show via examples, using only semantic considerations might limit the perceived from an external point of view of a user or a customer. Such a view is im-portant for reaching different reuse decisi ons, e.g., when conducting feasibility stu-dies, estimating software development efforts, or adopting SPLE. In addition, current variability analysis methods take into account intermediate outcomes of the behavior that may not matter to external stakeholders , such as users and customers. For exam-ple, a system may intermediately keep information in case a transaction fails, but this would be of no interest when the behavior ends successfully. Hence, when analyzing variability of software products, we aim at minimizing the impact of intermediate outcomes which cannot be used for (and might confound) comparing the products from an external point of view. 
In this work we propose to overcome the shortcomings of pure semantic-based va-riability analysis by combining semantic similarity with similarity of software beha-vior as manifested in requirement statements. To compare software behavior we apply an ontological view of dynamic aspects of systems which we proposed in earlier work [ 20], [ 21]. For a given requirement, we consider the behavior it represents in the ap-plication ( X  X usiness X ) domain. Taking an external point of view, behavior is described occurs. We use semantic metrics to evaluate the similarity of related behavioral ele-ments and use this similarity to analyze variability. 
The rest of this paper is structured as follows. Section 2 reviews related work, ex-emplifying limitations of current approaches. Section 3 briefly provides the ontologi-cal background and our framework for classifying software variability. Section 4 introduces the ontological approach to variability analysis and demonstrates its appli-cability. Section 5 presents preliminary results and discusses advantages and limita-tions. Finally, Section 6 summaries the work and presents future research directions. As mentioned above, the main approach to va riability analysis in SPLE is semantic  X  based on text similarity measures. Semantic text similarity measures are commonly classified as knowledge-based or corpus-based [ 9], [ 16]. 
Corpus-based measures identify the degree of similarity based on information de-among words in a large corpus of text. Sent ence similarity is computed as the cosine of the angle between the vectors representing the sentences X  words. 
Knowledge-based measures use information drawn from semantic networks. Many of these methods use WordNet [ 26] for measuring word (or concept) similarity. This can be done in different ways, including measuring path length between terms on the semantic net or using information content, namely, the probability to find the concept in a given net. Several measures have been suggested to extend word similarity to sentence similarity. These measures consider sentences as vectors, sets, or lists of words and suggest ways to calculate sentence similarity using word similarities (e.g., [ 13], [ 14], and [ 16]). The MCS method [ 16], for example, calculates sentence similar-ity by finding the maximum word similarity score for each word in a sentence with words in the same part of speech class in another sentence. The derived word similari-ty scores are weighted with the inverse document frequency scores that belong to the corresponding word. 
In the context of analyzing software products variability, different studies have suggested ways to use textual requirements to generate variability models in general and feature diagrams in particular. Examples of such studies are [ 7], [ 17], and [ 25]. In [ 25], for instance, the semantic similarity of the requirements is measured using LSA. Then, similar requirements are grouped using a hierarchical agglomerative clustering algorithm. Finally, a Requirements Description Language enables the specification and composition of variant features. 
All the above methods employ only semantic considerations. Furthermore, the si-As mentioned before, such statements might include aspects (e.g., intermediate out-comes) that are less or not relevant for analyzing variability from an external perspec-using a series of examples. 
The first example refers to the following requirements: (1)  X  X he system should be able to report on any user update activities X ; (2)  X  X ny user should be able to report system activities X . 
Applying the well-known and commonly used semantic similarity method LSA 1 , the similarity of these sentences is 1. This would imply that their semantic meanings however, that these requirements are quite different: the first represents behavior that requirement represents a behavior triggered by an external user who intends to report his/her system activities. 
As a second example, consider the following two requirements: (3)  X  X he system will allow different functions based on predefined user profiles X ; (4)  X  X ifferent operations should be allowed for different user profiles X . situation accurately: the two requirements represent very similar domain behaviors. 
Finally, the following two requirements can be considered similar from an external point of view, although they differ in their levels of details of intermediate actions. plays the outputs of the activity. X  (6)  X  X f the user is authorized to perform an action, the system initializes the parame-ters needed by the action. The user performs the action and the system responds that system presents the action's outcomes. X  
However, the LSA-based value of the similarity of these two requirements is rela-tively low (0.57), failing to reflect their similarity from an external point of view. 
To overcome the above limitations, we propose to combine a semantic approach and considerations which reflect system behavior as manifested in requirements statements and modeled ontologically. We use concepts from Bunge's ontological model [ 2, 3] and its adaptations to software and information systems [ 23, 24] in order to define behaviors and use them for variability analysis. We have chosen this ontology because it formalizes concepts that are important for representing functionality and behaviors. Specifically, these concepts include things, states, events, and transformations. Furthermore, Bunge X  X  ontological model has already served us to define software variability classes [ 20, 21]. 
Bunge's ontological model [ 2], [ 3] describes the world as made of things that pos-sess properties . Properties are known via attributes , which are characteristics assigned to things by humans. A state variable is a function which assigns a value to an attribute of a thing at a given time. The state of a thing is the vector of state variables X  values at a particular point in time. For a state s, s.x denotes the value of the state variable x in s. the thing. Finally, a state can be stable or unstable: a stable state can be changed only by an external event. An unstable state will be changed by an internal event. 
We exemplify the above concepts using a library management domain. In this domain, book status can be considered a state variable, defining whether a book is book becomes past-due can be considered an internal event, which is initiated when a certain period has passed from borrowing and the book is not yet returned. e &gt; occurs. The full behavior includes intermediate states the thing traverses due to its own transformations in response to the external events. However, only (s 1 , E, s*) are  X  X isible X  from an external (user) point of view. 
In our example, borrowing can be considered a behavior, which starts in the state ready to lend (i.e., when the book status is  X  X n the shelf X  and the librarian is  X  X vaila-borrowed (i.e., the book status is  X  X orrowed X  and the librarian is  X  X vailable X  again). 
We further made two assumptions regarding the things whose behavior we model [ 21]: no interruption (external events can affect a thing only when it or at least one of its components is in a stable state) and stability assumption (all things we deal with in practice will eventually reach stable states). 
Finally, we defined similarity of behaviors in terms of similarity of their external events and states [ 20]: 
Event similarity: Two external events are considered similar if they appear to be the same in the application domain. 
State similarity: Two states s and t are considered similar with respect to a set of state variables X, iff  X  x  X  X s.x = t.x. X is termed the view of interest . 
Based on these definitions we identified eight classes of external variability, name-ly, variability that refers to software functionality as visible to users (see Table 1). 
In the current work, we use textual software requirements as the basis for automat-ic identification of domain behaviors and their elements (namely, the initial and final states and the external events). We use semantic measurements in order to refine event and state similarity definitions. Perceiving a software system as a set of intended changes in a given domain, we fo-cus on systems X  behaviors as specified by or represented in functional requirements. Functional requirements commonly refer to actions (what should be performed?) and objects (on what objects, also termed patients , should the action be performed?). They action is performed?), and the temporal constraints (when is the action preformed? in what conditions is it performed?). 
There are different ways to write and phrase functional requirements. For our purpose, we assume that they are specified as user stories or descriptions of use cases . We further assume that each use case or user story represents a single behavior of the requested system 2 . For example, consider the following requirement which describes a typical use case in a library management system: 
Our approach consists of four steps: (1) pre-processing which checks the quality of the individual requirements and identifies the need for corrections or improvements; (2) extraction of the main behavioral elements from a requirement, e.g., the require-ment X  X  agents (who?), actions (what?), and patients (on what objects?); (3) Classifica-tion of the extracted main behavioral elements according to the ontological definition of behavior (in terms of states and events); and (4) measuring requirements variabili-ty based on the framework presented in [ 20], [ 21]. 
Pre-processing is out of the scope of this paper. It may use existing quality models, such as that presented in [ 1]. In the following sub-sections we elaborate on steps 2-4. 4.1 Extraction of the Main Behavioral Elements In order to extract the main behavioral elements of software requirements we use their semantic roles in the phrase. Currently, we refer to five semantic roles which are of special importance to functional requirements. These roles, their labels, and the aspects they fulfill in functional requirements are listed in Table 2. Using SRL 3 , we specify for each requirement R a list of behavioral vectors BV R ={bv i } i=1..n . Two types of behavioral vectors are identified: action and non-action vectors. The following definitions formally specify the behavioral vectors for these two types. Examples are provided immediately afterwards. Definition 1. An action vector represents an activity (identified by a verb) in the be-havior: bv i ::= (Agent i , Action i , Patient i , Instrument i , Source i ), where: -Agent i , Patient i and Instrument i are as explained in Table 2. -Action i is the verb predicate of the phrase. -Source i  X  {AM-TMP, AM-ADV, None} indicates whether the vector originates 
An action vector is derived from a non-modifier phrase or a compound modifier actions). Definition 2. A non-action vector represents the temporal or adverbial pre-condition of the behavior (or part of it): bv i ::= (Modifier i , Source i ), where: -Modifier i includes the atomic modifier phrase -Source i  X  {AM-TMP, AM-ADV} indicates whether the vector originates from a 
A non-action vector is derived from an atomic modifier phrase which includes no verb (and thus is not further parsed by SRL). 
Table 3 lists the derived behavioral vectors for our previous requirement of the library management example. Vector #5 is a non-action vector. All other vectors represent ac-refer) using the algorithm in [ 18] (e.g. the agent  X  X he X  becomes  X  X  borrower X ). 
The next step in the analysis is to arrange the behavioral vectors of each require-ment in a temporal order. We do this by constructing temporal graphs: bv
The construction of edges in this graph is done in two steps. First, we use syntactic ordering , based on the order of the argument vectors in the requirement X  X  phrasing. Replacement of a pronoun by the relevant noun is indicated with pronoun [noun] . Second, we apply semantic ordering , using the machine learning algorithm suggested in [ 15], to update the syntactic edges based on six types of temporal relations derived from the text. These relations are listed in Table 4. Whenever a semantic relationship contradicts a syntactic one, we use the semantic relationship as shown in the table. and Z are temporal phrases or events,  X  indicates their order
Fig. 1 exhibits the temporal graph for our example (Table 3). The changes the se-mantic ordering causes to the syntactic orde r (the gray arrows) are depicted with the black arrows 7 . 4.2 Classification of the Behavioral Vectors We now turn to the classification of the dynamic aspects of the requirements to initial states, external events, and final states. To this end, we first classify each behavioral particular, we examine the Agent and Action components of action vectors: the agent can be internal , external , or missing (as in passive phrases) 8 ; independently, the action can have an active or a passive meaning 9 . All non-action vectors are considered inter-nal, as they do not represent an actual action, but a pre-condition for the behavior (or part of it). Accordingly, we identify six generic cases (see Table 5). class(bv 3 ) = class(bv 4 ) = EXTERNAL; class(bv 5 ) = class(bv 6 ) = INTERNAL. 
Behavioral vectors classified as EXTERNAL represent actions performed by ex-ternal agents and therefore are considered external events (E) . In contrast, behavioral vectors classified as INTERNAL represent actions performed or pre-conditions checked by the system. They are considered to reflect states: initial, final, or interme-behavioral vectors that precede (in the temporal graph) the sequence of external be-behavior. Of those, only vectors whose sources are modifiers (and thus represent pre-arguments, only internal behavioral vectors which follow the sequence of external behavior. Of those, only action vectors whose sources are not modifiers (and thus All other internal behavioral vectors, i.e., those interleaved with the external beha-vioral vectors, are considered to be manife sted by intermediate states. Such actions (and related states) are not currently taken into consideration in our analysis, which is based on an external view of behaviors. 
Behavioral vectors for which the agent is unknown are classified at this stage into multiple behavioral elements (e.g., both initial state and external events). The decision when calculating for each vector the most similar counterparts. 
We next formally define the behavior as sociated with a requirement and exemplify this definition on our requirement: its temporal graph TG R , the behavior associated with R is defined as a triplet B R =(s 1 , E, s*), where: -The initial state (s 1 ) includes all internal or unknown vectors originated from mod--The external events (E) include all potentially external behavioral vectors (name-
In our previous example, we obtain the classification of behavioral vectors as can be considered either an extern al event or an initial state. bv 5 = (the copy identifi-cation number and the borrower number are valid, AM-ADV) does not appear at all as it represents a pre-condition originated from an adverbial modifier and appearing after external events. Thus, bv 5 cannot be considered an ini tial neither final state (but rather an intermediate state). 4.3 Measuring Requirements Variability Having two requirements, their behavioral vectors, and the classification of the similarity . The definitions are followed by an example. Definition 5 (Behavioral Vectors Similarity). Given two behavioral vectors, the vectors similarity is calculated as follows: 1. If the two vectors are action vectors , the vectors similarity is the weighted average 3. If one vector is an action vector (say v 1 ) and the other is a non-action vector , the Definition 6 (Behaviora l Element Similarity). Given two requirements, R 1 and R 2 , average of the maximal pair-wise similarities. Formally expressed: BS (R 1 , R 2 | bh) = where: 
As an example consider the following requirements: 
For calculating component semantic similarities we used an MCS version that han-dles phrases rather than complete sentences. We set the component weights to 0.3, 0.4, 0.2, and 0.1 for agents, actions, patients, and instruments, respectively, perceiving agents and actions as the dominant components in behavioral vectors similarities. We obtain initial state similarity for the given re quirements of 1 (no special pre-conditions in both requirements), external events similarity of 0.78 (due to differences in the agents that initiate the events), and final state similarity of 1 (as the final state of the first requirement is included in the final state of the second requirement). Note that we chose an asymmetric metric for defining behavioral element similarity, meaning that behavior R 2 when behavior R 1 is required. The asymmetry in this measure reflects the the second for the first, as exemplified by the two requirements above. 
Based on the behavioral element similarity, we classify the outcome of comparing end, we define event similarity threshold (th e ) and state similarity threshold (th s ): 1. Initial states are considered similar if and only if BS (R 1 , R 2 | s 1 ) &gt; th s . 2. External events are considered similar if and only if BS (R 1 , R 2 | E) &gt; th e . 3. Final states are considered similar if and only if BS (R 1 , R 2 | s*) &gt; th s . 
Assuming an event similarity threshold greater than 0.5 (e.g., 0.8), the variability class to which requirement 1 belongs with respect to requirement 2 is # 2 (see Table 1: similar cases and responses, different interactions). This class accurately describes the requirements variability. To evaluate the proposed approach, we compared its outcomes to evaluations by ex-perts. We provided five experts, each having 10 to 25 years of experience in require-ments engineering and software development, with 10 requirements. For each re-quirement, four alternative systems to be considered were presented to the experts. Each alternative was describes as a requirement. The full set of requirements and for each requirement based on the similarity to the given requirement in terms of the amount of changes needed to adapt the alternatives to the requirement. Since experts X  ranking requires some subjective considerations, there was no full agreement between (Si  X  Sj). For each requirement there were four possible alternatives yielding six such relations. This provided a total of 60 relations for the 10 requirements. There were 55 relations on which most experts (at least four out of the five experts, 80%) agreed. We conducted the analysis described in this work for the same set of requirements. We used the behavioral element similarities to calculate overall similarity, which can serve as a basis for ranking alternatives. The weight of initial state similarity was set 0.5. This reflected an assumption that the final state of behaviors (usually specifying system output) is the dominant element in defining behavior similarity. We followed a similar procedure using the well-known semantic similarity method LSA, which as noted is based only on semantic considerations. Table 7 summarizes the results of the ontological approach and LSA with respect to experts 11 . 
As can be seen, our approach performed better than LSA in comparison to rankings by experts. We believe that our approach has an additional advantage to better requirements analysts, can see not only the overall calculated similarity, but also more their reuse decisions more evidence-based and feasibility studies more systematic. 
Analyzing the relations missed by the ontological approach, we observed the fol-lowing. First, some of the requirements included phrases that explain reasons, e.g.,  X  X o the librarian can make inter-library loans X . These phrases were interpreted by the approach as an integral part of the behavior (part of the external events in this case). Second, in a few cases, where the requirements statements included very complicated ments of the different phrases. Finally, we observed that in some cases our approach resulted with the conclusion that two alternatives are very similar to the given re-quirement and the experts subjectively preferred one alternative over the other. We proposed a method to analyze variability and similarity of software requirements based on combining semantic and behavioral aspects of requirement statements. To formalize the external (user-oriented) aspects of software behavior we used an ontological model where a specific functional requirement is modeled as a triplet: initial state, external events, and the final state of the system. We have shown how such a representation can be obtained automatically by: (1) applying semantic vectors in common terms; (3) ordering the vectors temporally based on modifiers identified in the semantic analysis; and (4) extracting the initial state, external events, and final state for each functional requirement. We then suggested a way to measure nary evaluation, the approach yielded results more similar to experts X  evaluations than those of a well-known semantic similarity measure  X  LSA. 
In the future, we intend to extend the approach in several ways. First, we intend to consider additional semantic roles, e.g., location modifiers. Second, we plan to refine the similarity measures to include a choice of specific state variables rather than com-plete behavioral vectors, thus having a way to reflect user views more faithfully. This will enable us to analyze variability of software requirements from different points of view that may reflect different purposes or stakeholders. Users may consider two software behaviors similar while developers may consider them different, or vice versa. Similarly, such differences might exist among users. The choice of state va-riables to represent different points of view can be included in the behavioral analysis the variability analysis different ordering of the occurrence of external events. 
