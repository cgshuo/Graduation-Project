 BAOLI LI, WENJIE LI, and QIN LU The Hong Kong Polytechnic University 1. INTRODUCTION
Topic detection and tracking (TDT) aims to automatically recognize new top-ics and find new stories on already known topics in temporally-ordered news streams [Allan 2002]. Since the establishment of Topic Detection and Track-ing research in 1997 [Allan et al. 1997], extensive studies have been carried out to improve TDT system performance with temporal information. These efforts are motivated by two facts: temporal information is an important at-stream.

Most past research depends on a story X  X  publication time [Allan et al. 1998a, 1998b; Yang et al. 1998]. The similarity value between two stories or an in-coming story and a topic, that is, the likelihood that two stories are relevant to each other or that a story is on-topic, is adjusted according to the differences between their publication times. This strategy does not take into account the fact that a story X  X  publication time is usually not identical to the topic time.
Obviously, a news story is not necessarily published on the same day that the reported events take place.

To deal with this problem, recent research projects have applied temporal information processing in order to detect and normalize temporal expressions in text [Kim and Myaeng 2003, 2004; Li 2003; Makkonen and Ahonen-Myka 2003, 2004]. However, they neglect the phenomenon of time granularity variance in expressions of a time in temporally-ordered news streams. People tend to use temporal expressions with different granularities to refer to an identical event as time lapses. For example, when an explosion has just happened, we may refer to its time by temporal expressions like  X  X oday X  or  X  X esterday X  in news stories, but, a few days later, we are likely to mention the event as  X  X hat explosion happened last week X .

Past research does not reach a consensus on whether temporal information is useful in TDT. Experiments on different datasets with the same methods may result in conflicting conclusions. For example, Allan et al. [1998a, 1998b] find that using temporal information with a simple strategy does not help to improve the performance of a TDT system on the TDT2 corpus, while it is effective on the
TDT-pilot corpus. Therefore, extensive research needs to be done to explore the effectiveness of using temporal information in TDT. In this research, we seek to determine whether there is a more reasonable method for using temporal information in TDT.

In the official TDT evaluations [Allan 2002], research on TDT is factored into five subtasks: story segmentation, new event detection, story link detec-tion, topic detection, and topic tracking. In this study, we focus on the subtask of topic tracking with an expectation that the proposed method may also be ap-plicable to other subtasks. A topic-tracking system is expected to detect all new stories on a given topic, which is defined by one or a few on-topic stories. After a careful investigation on temporal information in news streams, especially on time granularity variance when expressing a time in natural language, we pro-pose a topic-tracking algorithm with time granularity reasoning. Experiments on two different TDT corpora show that our proposed method could make good use of temporal information in news stories, and it consistently outperforms the baseline centroid algorithm and other algorithms which consider temporal relatedness.
 The rest of this article is organized as follows. Section 2 reviews related work. Section 3 summarizes characteristics of temporal information in news streams.
In Section 4, we present our new strategy with time granularity reasoning for using temporal information in topic tracking. Experiments and discussion are provided in Section 5. Finally, Section 6 concludes the article and gives our future plan. 2. RELATED WORK
Since the early days of TDT research, temporal characteristics of news streams have been noticed and utilized [Allan et al. 1998a, 1998b; Yang et al. 1998].
These initial and subsequent studies assume that the likelihood of two sto-ries dealing with the same topic is less and less as the time distance between them becomes larger and larger. The publication time or just the sequence number of each story is used to measure the time distance. Many researchers exploit temporal information in their TDT systems along this line, but their methods vary in some ways. Allan et al. [1998a, 1998b] increase the similar-ity threshold with a time penalty, according to the time distance between two stories. Most other researchers adopt methods that directly adjust the sim-temporal relatedness. Yang et al. [1998] use a linear model with a time window, while Franz et al. [2001] use an exponential time decay model [Brants et al. 2003]. Experiments by Umass and CMU researchers [Allan et al. 1998a] on the TDT-pilot corpus show that these simple algorithms are effective, but this claim is not valid on other TDT corpora [Carbonell et al. 1999; Brants et al. 2003].

One interesting and related study by Braun and Kaneshiro [2003] does not directly utilize the temporal information within a story. They regard temporal references (e.g., words like tomorrow and February) as good content cues and increase the weights of words around these temporal references. The effective-ness of their method is demonstrated on a selected part of TDT3 corpus.
The previous approaches are all information retrieval-based and do not re-sort to much natural language processing. They are simple and robust, but not the best choice: in most cases, the publication time of a story is not the exact time of the event described in the text. A story may refer to an already known event that occurred several months or years ago. Therefore, recent studies try to incorporate temporal information processing technology to improve TDT sys-tem performance.

Allan et al. [1999] and Kumaran et al. [2004] focus on exploring the effective-ness of using named entities in TDT and take temporal expression as one type of the investigated named entities. In their studies, temporal expression enti-ties have not been normalized. Kim and Myaeng [2003, 2004] investigate the impact of temporal information extracted from news articles, instead of the use of publication times alone, on a Korean corpus they developed. They make use of finite state automata and a lexicon containing time-revealing vocabulary to identify and normalize temporal expressions in Korean news stories. Their ex-periments show that the normalized temporal information does help to improve the performance of a topic-tracking system. It needs to be pointed out that their evaluation method is different from the TDT official process and more similar to that of a topic-detection subtask. Li [2003] uses the same method as Kim and
Myaeng [2003, 2004] and experiments with the TDT2 Mandarin corpus. The results show that this method does help topic detection but has little impact on topic tracking. All these approaches rely on an exact match between normalized temporal terms when measuring temporal relatedness. This exact match strat-egy brings a critical drawback to these methods. For example,  X  X uly 7, 2005 X  and  X  X uly 2005 X  are regarded as two distinct times without any relationship in these methods.
 To overcome the limitation of the exact match strategy just mentioned,
Makkonen and Ahonen-Myka [2003] propose a scheme to measure the simi-larity between two normalized temporal terms. In their method, a temporal expression is internally represented as an interval with a start and an end-point on the timeline. To determine the temporal similarity, the portion of overlap with respect to the lengths of the intervals is measured. This scheme seems to be too mechanical to be able to account for the psychological char-acteristics of expressing temporal information in news streams. For instance, conciseness requires us not to always refer to an event with its exact time in news stories. If an expression with coarser granularity (e.g., month) is enough for differentiating the event from others in a certain context, we usu-ally do not give more detailed time information with finer granularity (e.g., day) except for emphasis. Obviously, we cannot take it for granted that a coarser temporal expression is less likely to refer to an event than a finer one.

To sum up, we observe the following points from previous research. First, all of the proposed methods are based on the strategy that adjusts similarity val-ues according to temporal relatedness, either directly or indirectly, by changing similarity thresholds. Second, to employ temporal information obtained from text, it is mandatory to have a translator which recognizes temporal expres-sions and normalizes them into some computable format. Finally and most important, a mechanism for comparing two temporal expressions is especially required for a cross-document application like TDT. In Section 4, we propose a new method with time granularity reasoning for deciding whether two times are relevant to an identical topic.

Temporal granularity reasoning has been extensively studied for temporal databases, data mining, formal specifications, and problem solving [Bettini et al. 2000; Goralwalla et al. 2001; Combi et al. 2004]. However, this research mostly considers how to convert temporal data from one granularity to another.
In topic detection and tracking, we need a mechanism for inferring whether there may exist a coreference relationship between two times with different granularities, which appears in temporally-ordered news streams. Cognitive processes in natural language exhibit a big challenge in this situation. As far as we know, this problem has not been explored in the literature.
 3. TEMPORAL INFORMATION IN NEWS STREAMS
To effectively utilize temporal information for topic tracking, we first try to make a comprehensive investigation of temporal information in news streams.
A news stream is a collection of temporally-ordered stories. Each story may contain several time expressions. In this section, we investigate the character-istics of time information in news streams from two aspects, local and global.
Here, local means that we analyze time information occurring in each individ-ual story, and global means that we take the whole temporally-ordered collec-tion as an examining object. 3.1 Local View
From a local point of view, a news story may contain several time expressions with different granularities. In this section, we first explore the characteristics of time information associated with an individual story. 3.1.1 Types of Temporal Information. Temporal information in a news story may be either timestamped, explicit, or implicit.  X  Timestamp 1 . A news story is usually associated with a timestamp, which, as metadata, indicates when the story itself is released. The timestamp is often used as a reference time for normalizing temporal expressions in a story and is also regarded as the publication time of a story. There is a special kind of timestamp in news texts, which, if any, appears at the beginning of a story, for example BUENOS AIRES, Oct. 23.  X  Explicit time of a temporal expression 2 . It is the meaning of a time expression which is analogous to the sense of a word. A precise temporal expression is usually mapped to a specific point or duration on a timeline. As explicit time is an intrinsic attribute of an expression, we thus use a temporal expression and the explicit time of a temporal expression interchangeably whenever there is no ambiguity.  X  Implicit time underlying a temporal expression. It is the time that a tempo-ral expression refers to, and it may be different from the expression X  X  explicit time, especially when the granularity of the expression is coarser. For exam-ple, when we say  X  X ondon bombings in July 2005 X , the implicit time of the expression July 2005 may be July 7, 2005 and/or July 21, 2005.

The attached timestamp puts a story at a point on the timeline, while the explicit time of a temporal expression in a story positions an event on the time-line, and implicit time connects two time-expressions within a story or across two stories. If all temporal expressions denote times with the same granular-ity, there is no need to distinguish between explicit and implicit time. However, temporal expressions appearing in text or delivered in natural language are full of diversity, including time granularity variance, which exhibits the charm of language. The fact that a coarser time may refer to a finer one makes it difficult to judge whether there is a coreference relationship between two times. 3.1.2 Time Granularity in News Story. To define and interpret time-related concepts, a set of primitive temporal entities, which is called a time domain or a timeline, is introduced in the classical time granularity model [Bettini et al. 2000]. The set has a total order relationship on the primitive tem-poral entities and can be represented as a pair ( T ;  X  ), where T is a nonempty set of time instants and  X  is a total order on T .
 A time granularity can be formally defined as follows [Bettini et al. 2000].
Definition. A granularity is a mapping G from the integers (the index set) to subsets of the time domain such that: (2) if i &lt; k &lt; j and G ( i ) and G( j ) are nonempty, then G ( k ) is nonempty.
Each nonempty subset G ( i ) is called a granule of G . The first condition states that granules in a granularity do not overlap and that their index order is the same as their time domain order. The second condition states that the subset of the index set that map to granules is contiguous. As an example, G can be defined as a special mapping G , with G year since 2000 subset of the time domain corresponding to the year 2000, G the one corresponding to the year 2001, and so on.

Informally, a granularity is a unit of measurement for time durations. So it is meaningful to compare two granularities with each other. If a granularity G as a time span is larger than G B ,wesay G A is coarser than G say G A is finer than G B [Goralwalla et al. 2001]. When we say  X  X he granularity of a temporal expression X , we actually mean the granularity of the explicit time underlying a temporal expression.

A timeline T associated with a set of granularities GS and a set of con-version functions F between the granularities determines a calendar which is formally represented as a triplet &lt; T , GS , F &gt; the most widely-used one today, and the set of granularities in this calendar consists of year, month, week, day, hour, minute, and second.
 In our study, we only consider the following time granularities: G G
Month , and G Ye a r . The granularities finer than G Day G
Minute , are ignored as G Day is usually fine enough to describe an event in news articles and to distinguish it from other events. Time units in other calendars such as the Chinese lunar calendar are also ignored because they are seldom used in news text.

For a temporal expression in news text, we zoom it out to an internal ex-pression with the most specific granularity allowed. Finer information beyond the permitted granularities ( G Day , G Week , G Month , and G example, spring of 2005 will be truncated as  X 2005 (year) X , and  X 10:30AM, May 28, 2005 X  will be converted to the date of May 28, 2005.

Table I gives the distributions of temporal expressions with different gran-ularities in the TDT2 Mandarin corpus. The statistical data are obtained with our Chinese temporal expression detection and normalization module. They provide an overview of the temporal expressions X  usage in news stories, even though the tool is not perfect 3 . There are 18,711 Chinese news stories coming from three sources (XinHua, VOA, and ZaoBao) in the corpus. We will give more information about this corpus in Section 5.1. Most time expressions in
Chinese news stories are with granularities G Day and G Year expressions are delivered in granularity G Day , and 35.84% in G rest of the expressions, about 11.91%, are expressed in G of temporal expressions with G Week is relatively small. Moreover, news texts from different sources exhibit diversities in the distributions of time expres-sions with different granularities. For example, broadcast news stories from
VOA tend to use fewer expressions with G Year , compared to those from other two sources. 3.1.3 Number of Temporal Expressions. A news story may contain zero time expression when its content is tightly associated with its publication time.
On the contrary, some stories may include many temporal expressions when they aim at covering more time periods and events. Table II shows statistics on the number of time expressions used in news stories in the TDT2 Mandarin corpus. On average, a news story contains 4.45 temporal expressions. 57.07% stories include 2 to 5 time expressions. News stories from VOA have fewer temporal expressions than those from the other two sources. They have 1.8 temporal expressions on average, while those from XinHua and ZaoBao include 4.67 and 5.13 expressions, respectively. Moreover, 21.85% stories from VOA contain no time expressions in their contents.
Obviously, there may be more than one temporal expression in a given on-topic story, each of which may convey different time information and be asso-ciated with different events. An example of a news story is given in Figure 1, which is the first on-topic story of the topic 20020 (China Airlines Crash) an-notated in the TDT2 Mandarin corpus. There are several temporal expressions in this story, but only one of them ( X   X /Feb. 16th, 1998) is relevant to the target event. Previous research does not try to differentiate topic times from nontopic ones. They regard all times in a story as on-topic ones, which may lead to errors while detecting and tracking topics. 3.2 Global View
From a global point of view, individual news stories are connected in temporal order and form a news stream. In this section, we further investigate temporal relatedness between stories in news streams where stories arrive continuously over time. 3.2.1 Time Structures of Topics. A topic has its own lifecycle in a news stream; it appears, it is developed, and finally, it fades away. The time spans of topics, which are determined by the topics X  granularities (event or activity) and the degree of people X  X  attention, vary greatly. The quantities of on-topic stories exhibit the different phases of a topic. For example, the development phase is often accompanied with a burst of on-topic stories.

A few research projects have been done to automatically detect topics and determine the time structures of the topics. As a subtask of TDT, topic detection aims at grouping stories on different topics and building time structures is a by-product. On the contrary, researchers from the text mining community em-phasize how to automatically determine time structures of evolutionary topics [Kleinberg 2002; Mei and Zhai 2005].

We try to obtain some insights for utilizing temporal information in TDT by analyzing the time structures of topics in news streams. Kleinberg [2002] and Mei et al. [2005] propose automatic methods to discover the temporal theme structures from text streams. As TDT corpora contain manual anno-tation such an automatic annotation tool is not necessary for our study. In the following, we investigate the time structures of topics annotated in the TDT2 corpus.

Figure 2 illustrates the number of stories on each topic along the timeline in the TDT2 corpus. Due to the limitation of space, we just show the twenty topics that are annotated both in English and Mandarin text. X-coordinate stands for the date from January 1, 1998 to June 30, 1998. Y-coordinate corresponds to topic ID in the TDT2 corpus. The vertical amplitude at each point proportionally indicates the number of on-topic stories on a date for a specific topic which include both English and Mandarin news stories. In Figure 2, at least ten topics have a time span longer than three months.
Some topics exist in the entire observation window. A typical topic is Asian Eco-nomic Crisis with ID 20001. Almost every day, there are some stories relevant to this topic. In this case, when we track long-lived topics in a limited dura-tion, temporal information seems to provide no help for differentiating stories.
On the other hand, some topics X  lifecycles are relatively short, such as topic 20089 (Afghan Earthquake) and 20057 (World Figure Skating Champs). The assumption that stories beyond a predefined span are not on-topic seems un-reasonable. Mean while, it is really difficult to set an appropriate fixed time span for all topics beforehand.
 3.2.2 Time Granularity Variance along Timeline. When we refer to the time of a topic as time lapses, we may use different granularities at different time points or in different stories. We are more likely to use finer time expres-sions at the points near the topic time. As a topic fades away, however, we are inclined to use coarser temporal expressions for referring to the topic. There-fore, we need to explore how the granularities of a set of temporal expressions which refer to the same topic vary along the timeline.

As far as we know, there is no such a corpus with annotations of the re-ferred time for each temporal expression. So we have to find an indirect way to investigate the time granularity variance in news streams.

As we mentioned a temporal expression refers to an implicit time that may be different from its explicit time. It is the time distance between the referred implicit time of a temporal expression and the publication time of the story in which the expression appears that determines the granularity of the expression.
In other words, there is some relationship among the explicit time of a temporal expression, its implicit time, its granularity, and the timestamp of the story where the time expression appears. We try to discover this relationship using data mining technology. We first build a sample set, and then apply data mining technology to it.

For each temporal expression appearing in text, we can derive a sample that consists of its offset on day, week, month, and year from the current time (i.e., the story X  X  timestamp) and its granularity. Undetermined value is marked as  X ? X . Then, if an expression like last year appears in text, we get a corresponding sample in ARFF 4 format as  X ?,?,?,1,year X .

We extract and normalize temporal expressions in stories that span 5 years in People Daily 5 (2000 X 2004), and create a sample set containing 483,898 valid samples (the samples like  X 0,0,0,0,day X  that denote the same date as the pub-lication date are discarded). We mine the sample set with the data-mining tool WEKA [Witten and Frank 2000]. C4.5 decision tree learning algorithm (J48 in
WEKA) is applied, and the generated rules are then checked and revised by humans. Finally, we obtain the rules in Figure 3 which infer the possible gran-ularities that a journalist may use to refer to a time when writing a temporal expression in a news story. Intuitively, when we refer to a time, we tend to use coarser granularities if there is a larger distance between the referred time and the time when we write. On the contrary, finer granularities will be used when the distance is smaller.

Given a target topic time ( Topic Time ), all possible granularities G set that a temporal expression ( Test Time ) in an incoming story S can take for referring to
Topic Time are determined by 13 heuristic rules, according to the Topic Time  X  X  granularity and the difference between the publication time ( SRL Time )ofthe story S and the refereed topic time ( Topic Time ). The SRL Time  X  X  granularity is G Day , while the Topic Time  X  X  granularity may be G Day
To better understand the algorithm in Figure 3, we rephrase some decision rules here. When the Topic Time  X  X  granularity is G Day , we can use a time expression with granularity G Day to refer to Topic Time , whatever, SRL Time is. If the difference between Topic Time and SRL Time is larger than 30 days, we can also use granularities G Month and G Ye a r . If the difference is less than 2 days and
Topic Time and SRL Time are within the same week and the same month, we can only use granularity G Day . 4. TOPIC TRACKING WITH TIME GRANULARITY REASONING
The investigation in Section 3 discloses the phenomenon of time granularity variance when people express a time in temporally-ordered news streams.
Unfortunately, this phenomenon was ignored in the previous algorithms for topic tracking with temporal information. It is thus interesting for us to ex-plore whether there is a more effective way to utilize temporal information in topic tracking. We propose a new algorithm for deciding whether a coreference relationship exists between two times with different granularities. With this reasoning strategy, we take into account the phenomenon of time granular-ity variance when measuring temporal relatedness between an incoming story and a topic. Then, we enhance the centroid topic-tracking algorithm with time granularity reasoning.

In the following sections, we first describe the widely-used baseline centroid algorithm. Then, the time granularity reasoning, which infers whether there exists a coreference relationship between two times, is presented. Finally, we employ this inference mechanism in our proposed topic-tracking algorithm to help decide whether a time in a new story may refer to the topic time. 4.1 Centroid: A Baseline Algorithm Most topic tracking systems take a centroid method based on the Vector Space
Model [Salton et al. 1975]. It is simple yet effective [Larkey et al. 2004]. With this method, a central topic vector is constructed as the topic representative. Each incoming story is then evaluated against this centroid while tracking.
The similarity between an incoming story and the centroid is computed as a decision value. If the value exceeds a predefined threshold, the new story will be labeled as on-topic.

This centroid method is the baseline in our research. Candidate features in our implementation include named entities, for instance, location, person and organization X  X  name; nouns; verbs except modal verbs and copula; Chinese characters within nouns and verbs. The weight W f is calculated by Equation (1). TF f rence in story s j . DF f the feature f i occurs. N is the total number of stories in the training collection.
Many similarity measures [Salton 1989] have been proposed in the IR com-munity such as Euclid distance, Dice coefficient, Jaccard coefficient, weighted sum, and consine value of the angle between two vectors. We choose the fol-lowing widely-used Cosine similarity of two vectors x y = ( y
In Equation (2), O i is the contribution of the feature f between two vectors. 4.2 Time Granularity Reasoning
For utilizing temporal information in TDT, a reasoning mechanism for compar-ing two times is especially required. A straightforward strategy is exact match.
Two times are judged to be identical or have some relationship between them only when they are exactly matched. Kim et al. [2003, 2004] and Li [2003] take this simple method, which ignores the phenomenon of time granularity variance in text, and assume that July, 2005 and July 7, 2005 have no rela-tionship. Makkonen et al. [2003, 2004] extend exact match with mathematical time reasoning. The portion of two times X  overlap determines their relation-ship. For example, the similarity between July, 2005 and July 7, 2005 is 0.0323 (1/31), which means that they refer to an identical time with 3.23% possibility.
Apparently, this mechanical strategy neglects the psychological and cognitive characteristics of natural language, especially the phenomenon that human beings express a time with different granularities as time lapses. Therefore, we try to design a reasoning scheme for inferring whether there exists some relationship between two times with the same or different granularities in temporally-ordered stories.

Due to the arbitrariness of natural language, it seems impossible to estab-lish a rigorous reasoning mechanism that considers the phenomenon of time granularity variance in news streams. Our time granularity reasoning scheme consists of heuristic rules derived from quantitative and qualitative analyses.
For TDT application, we define five coreference levels between a topic time and a test time (from weak to strong): L Null , L Year , L Month dicates no relationship between two times. Given a topic time, the coreference level between it and a test time in an incoming story is determined by these two times, their granularities, and the distance between the topic time and the new story X  X  publication time.

In Figure 4, we give the algorithm for determining the coreference level between a test time in a story and a topic time, that is, whether the former may refer to the latter and to what degree. Given a target topic time Topic Time , we first determine all possible granularities G set that a temporal expression ( Test Time ) in the new story S can take for referring to it. This is done by the heuristic rules in Figure 3, which are based on the Topic Time  X  X  granularity, and the time distance between Topic Time , and the publication time ( SRL Time )of the story S . The granularities of Topic Time and Test Time vary from G G
Day . If two times X  granularities are identical, they will be compared with exact match strategy. Otherwise, the finer time (i.e., the one with finer granularity) will be zoomed out to the same granularity as the other one for comparison. The comparison is performed only when the zoomed-out granularity is a possible one, i.e. within G set . The coreference level between Topic Time and Test Time is finally assigned the granularity level on which they match. The rules X  order guarantees that we obtain only the strongest coreference relationship between Topic Time and Test Time .

For time granularities G Day , G Week , G Month , and G Year are trivial as the finer one in these pairs is covered by the other. The other two zooming out operations, that is, G Week = &gt; G Month and G need some special treatment as a week can span two adjacent months or years.
Here, we have two choices:  X  zooming-out a week to the month or year that covers the majority of the week. For example, when we zoom out the week  X 52464WK X  2005, Monday-July 3, 2005, Sunday 8 ) from G Week to G we get June 2005 and 2005, respectively (i.e., Zooming out ( X  52464WK  X , G Week = &gt; G Month ) =  X  June 2005  X , and Zooming out ( X  52464WK  X , G
G Ye a r ) =  X 2005 X ). With this strategy, we take the middle day of a week as its representative for zooming-out it to a coarser time.  X  zooming-out a week to a set of months or years that covers all days of the week. For the same example, when doing Zooming out ( X  52464WK  X , G
G Month ), we get a set { June 2005 , July 2005 } . With this strategy, the first and the last day of a week are regarded as the week X  X  representative for zooming-out. Each time in the zoomed-out set is equally effective for comparison with other times.

To simplify the reasoning procedure, we choose the first strategy for zooming 4.3 Tracking Topics with Time Granularity Reasoning
We incorporate time granularity reasoning to improve the centroid topic-tracking algorithm. Other important enhancements include determining a topic time or a set of topic times from the given on-topic stories, and raising the sim-ilarity value between a new story and the target topic only when they are temporally and semantically related. Figure 5 gives the sketchy overview of our proposed topic-tracking method.

In the preprocess stage, we adopt a rule-based method (finite state automata) to extract and normalize temporal expressions [Li 2003]. A uniform internal token like  X 2005YY07MM07DD X  or  X 52465WK X  is assigned for each temporal expression in a news story. Expressions with granularities other than G G
Week , G Month , and G Year , are ignored. This module achieves 86.8% recall and 89.7% precision on 100 randomly selected news stories according to our specifi-cation. Since it is not our focus in this article, we do not give more details about this module here.
 In the following sections, we provide more detailed information about steps T 2 and R 6 in Figure 5.
 4.3.1 Determining the Set of Topic Times. Ideally, only one time is associ-ated with one topic. It seems true for topics relating to a specific event. How-ever, for topics involving a series of events, their temporal attributes may be a range of time or include several discrete times. For example, in TDT2 Topic
Descriptions 9 , the time of topic 20005 (Upcoming Philippine Elections) is anno-tated as January 1998 through May 1998. Moreover, although it is expected that only one time among all times appearing in the given on-topic stories is most rel-evant to the target topic, locating such a time is not a trivial task. On the other hand, we can assume that all times in the given on-topic stories implicitly deter-mine the temporal attribute of a topic. Therefore, we decide to use a set of times to describe a topic X  X  temporal attribute. It may contain one or several times.
To determine the set of topic times from the given on-topic stories, we use several straightforward strategies as follows.  X  AllT . Use all times in the given on-topic stories.  X  AllVT . Use all times that are eligible to be a topic time. Due to the promptness of news stories, a time is a candidate topic time only when it is not far in the past (e.g., in the previous ten days) or too distant in the future;  X  FrqT . Use the most frequent candidate time. When there are several can-didates with the highest frequency, the time appearing first is chosen; if a given on-topic story does not contain any time, its publication time will be regarded as a topic time.

The first strategy (AllT) is widely used in previous research, and it does not attempt to judge whether a time in the given on-topic stories is an eligible topic time. As indicated in Section 3, a news story usually contains several times and some or most of them are not directly related to the kernel topic. There-fore, we propose and explore other strategies to sift through invalid topic times step-by-step. Among the three strategies, the last one (FrqT), which aims at au-tomatically obtaining one topic time based on frequency, is the most aggressive. 4.3.2 Increasing Similarity Value. As indicated in Equation (2), each di-mension contributes to the final similarity between an incoming story and a target topic. We suppose that time, as one important attribute of a topic, con-tributes to similarity value as much as the most important features do when a new story is temporally related to the target topic. On the other hand, the time dimension should be effective only when other important semantic dimen-sions match, which guarantees that we do not increase the similarity value for temporally-related but semantically irrelevant stories. Obviously, news stories released on the same day may not all deal with the same topic, although they are very likely to be temporally related. Therefore, it is not reasonable to in-crease the similarity between an incoming story and a target topic with a fixed value, for example, 0.1 or 0.2.

At step R 6 in Figure 5, the increment  X  is determined by the largest sum operand O in Equation (2) whose corresponding feature f should be among the most important ones ( IFS ) for describing the target topic. The formal description is given in Equation (3). Actually, the increment according to the time coreference level between the incoming story and the tar-get topic. At present, we only consider whether they are temporally related.
Therefore, we take a uniform weight 1 for all coreference levels.
To determine the most important features of a topic from the given on-topic stories, we adopt the method proposed by Li et al. [2005] in which a feature selection metric (e.g., Chi-Square) is used as the criteria of importance. To deal with the problem of extremely unbalanced data in TDT while calculating the metric, off-topic samples of about M times the on-topic ones are randomly cho-sen. The average of N pass randomizations is regarded as the final value for each feature to avoid unstable performance. In the meantime, a story is split into a set of clauses each of which is taken as an independent sample. By this kind of expansion, we can simply use Boolean indicators for each feature in calculating selection metric. We choose the top k features as important ones. 5. EXPERIMENTS AND DISCUSSION 5.1 Datasets
We test our proposed method on two datasets: the TDT2 Mandarin Corpus (v3.2) and the Mandarin part of the TDT3 corpus. The former contains data collected during the first six months of 1998, while the latter contains data collected during the last three months of 1998. These data are all from three sources:
XinHua News Agency (abbreviated as XIN), VOA Mandarin, and ZaoBao News (abbreviated as ZBN). The characteristics of these two datasets are summarized in Table III. 5.2 Evaluation
All TDT tasks are cast as detection ones. As to topic tracking, each new incoming story will be judged by whether it belongs to a particular topic. A yes/no decision and a confidence value for this decision will be output by the tracking system.
The performance of a TDT system is then measured by a linear combination of two kinds of errors, misses and false alarms. It is called detection cost, and computed as follows [NIST 2004]: where: P Miss = #( Missed Detections )/# Targets ,
In Equation (4) P target is the a priori probability of finding a target (i.e., on-topic story), and constantly set to 0.02 in past TDT evaluations. C the costs of a missed detection and a false alarm, respectively, and are usually specified with 10 and 1, which penalizes misses more than false alarms. To make the performance metric in a more meaningful range, C by Equation (5).
The final performance score for a TDT system is usually calculated by the topic-weighted method, which accumulates errors separately for each topic and then averages the error probabilities over topics.

Besides the normalized detection cost ( C Det ) Norm , the decision error trade-off (DET) curve is another tool to measure the performance of a TDT system. The
DET curves are visualizations of the trade-off between the missed detection rate ( P
Miss ) and the false alarm rate ( P Fa ). The curves are constructed by sweeping a threshold through the system X  X  space of decision scores. At each point in the score space, P Miss and P Fa are estimated and plotted as a connected line. An example can be seen in Figure 6. A better performance corresponds to a curve closer to the lower-left corner of the graph. The minimum value found on the curve is known as MIN(( C Det ) Norm ), which is the optimal value that a system could reach at the best possible threshold. In the following experiments, we use the topic-weighted minimal normalized detection cost as the metric for comparing different methods. 5.3 Experimental Settings
In topic tracking, a system is first provided with a small number of on-topic stories (typically from one to four) and is then expected to detect all the other stories on that topic in the streaming of news story. In our study, we use one on-topic story for training as it is required in recent TDT evaluations [NIST 2004].
To verify the effectiveness of our proposed topic-tracking algorithm, we ex-periment with the following methods on the two different datasets.  X  C NT NR . Use the baseline centroid method which does not extract and nor-malize temporal expressions in news stories.  X  C WT NR . Use the centroid method with temporal expression recognition and normalization but without time granularity reasoning; each temporal expression is converted to an internal term and exactly matching is used for comparison. This is the method adopted by Kim et al. [2003, 2004] and Li [2003].

The following four variants of our method extract and normalize temporal expressions and employ a time granularity reasoning mechanism, but they use different strategies for acquiring one topic time or a set of topic times.  X  C WT WR AllT . Take the AllT strategy as described in Section 4.3.1;  X  C WT WR AllVT . Use the AllVT strategy as described in Section 4.3.1;  X  C WT WR FrqT . Use the FrqT strategy as described in Section 4.3.1;  X  C WT WR ManT . For each topic, use a human-specified topic time; the manually specified topic times are derived from the WHEN attribute of the seminal event for each topic which is available on the LDC Web site (http://www.ldc.upenn.edu/Projects/TDT/). This method will show a perfor-mance upper bound of our algorithm.

To compare our method with other algorithms that utilize temporal infor-mation in TDT, we test two additional topic-tracking methods. They are based on the baseline centroid method without extracting and normalizing temporal expressions in news stories. The distance between the publication dates of an incoming story and the given on-topic one determines the temporal relatedness between the new story and the target topic. The final similarity value is decayed while the time distance becomes larger. These two methods use two different time decay models [Brants et al. 2003].
  X  C NT TS Exp . Use the IBM exponential time decay model [Franz et al. 2001]. The original cosine similarity value Sim is adjusted as follows: age is the time distance between two stories, while twin is a parameter for controlling the decay X  X  step.  X  C NT TS Linear . Use a linear time decay model such as Yang et al. [1998] as follows 11 :
As the mathematical algorithm proposed by Makkonen et al. [2003, 2004] demonstrates much poorer performance than the traditional centroid method, we do not duplicate this method in our experiments.

In addition, for C WT WR methods, we use Chi-Square as the feature selec-tion metric to determine the most important features, and parameters M and
N are set to 10. 5.4 Results and Discussion Table IV gives the topic-weighted minimal normalized detection costs for four C WT WR variants with different k 12 values on the TDT2 mandarin corpus.
As k decreases, the optimal cost becomes smaller. When k equals 5, the system achieves its best performance. Therefore, in the following experiments, we take 5 for parameter k when applicable.

The C WT WR AllT method, which does not distinguish topic times from all times in the given on-topic story, performs quite well on the TDT2 Mandarin corpus. Its improved variant, that is, the C WT WR AllVT method, achieves a little better performance by sifting out obviously invalid topic times. The per-formances of these two methods all exceed that of the C WT WR FrqT method.
However, the minimal costs achieved by these two methods are still much higher than those of the C WT WR ManT method, which shows the necessity of deter-mining a precise topic time or a more compact subset of topic times.
In the C WT WR FrqT method, the strategy for automatically determining topic time by frequency seems too simple and inaccurate. Actually, in our ex-periments, only 5 out of 20 topics are assigned exactly the same topic times as those annotated by humans. Therefore, we plan to further explore how to determine topic time with high precision in the future. Moreover, in our exper-iments, we use only one given on-topic story for training. In this situation, the
C WT WR FrqT method may not have the chance to exhibit its advantage. We expect this method to be more effective when more on-topic stories are available.
It deserves further study. The comparison of performances shows that if we can automatically determine topic time with high precision or manually assign a topic time for a target topic beforehand, a tracking system with time granular-ity reasoning will achieve higher performance; otherwise, the C WT WR AllVT method is a quite good alternative.

We compare our proposed topic-tracking method with other related methods that utilize temporal information. The results on the two datasets are presented in Tables V and VI respectively.

In Tables V and VI, the C WT NR method does not show an advantage over the baseline C NT NR method. On the TDT2 corpus, its optimal cost is a bit lower than that of the C NT NR method, whereas on the TDT3 corpus, the latter is marginally superior to the former. It seems that temporal information processing is not rewarded. The main reason lies in its simple and mechanical time reasoning strategy. In the C WT NR method, two times are considered to refer to the same time only when they exactly match. With detection and normalization, temporal expressions in either relative or absolute format are converted to an identical term if they denote the same time. However, time granularity variance, when expressing a time in human language, is ignored in the C WT NR method. For example, September 2005 and September 8th, 2005 are converted to distinct terms (2005YY09MM and 2005YY09MM08DD), and have no relation to each other. Moreover, errors occurring during extraction and normalization may also lower the system X  X  performance. As a whole, the gain and the loss may counteract in the C WT NR method.

All the variants of our proposed method, which incorporate time granular-ity reasoning, provide better performances over the baseline C NT NR method on both the TDT2 and TDT3 corpora. The C WT WR AllVT method obtains, respectively, 5.15% and 5% improvement over the baseline method on the two datasets. The highest performance is obtained with human-annotated topic times (the C WT WR ManT method). On the TDT2 corpus, the topic-weighted minimal normalized detection cost drops 10.26% from 0.1882 to 0.1689; on the
TDT3 corpus, it decreases 8.26% from 0.1259 to 0.1155. These upper perfor-mance boundaries on two different datasets exhibit the great potential of our proposed strategy with time granularity reasoning.

We test the C NT TS Exp and C NT TS Linear methods with different twin values. These two methods rely solely on the publication time. The
C NT TS Exp method achieves its best performance when twin is 160 and 75 on the TDT2 and TDT3 corpora, respectively. For the C NT TS Linear method, the optimal twin values are 200 and 90, respectively. In our experiments, the parameter twin tends to be larger to reduce the negative effects of the simple strategy for utilizing temporal information. With a suitable twin, these two methods behave similarly to the C WT NR method. On the TDT2 corpus, they obtain a bit gain where on the TDT3 corpus they are inferior to the baseline method. As it is difficult to determine a reasonable and optimal twin before-hand, these two methods are not appealing.

From the optimal thresholds in Tables V and VI, we can observe the change of similarity between two stories when doing some additional processing. The similarity between an incoming story and the given on-topic one decreases when incorporating the module of temporal expression extraction and normalization because some common words such as today and yesterday are converted to different internal tokens, which makes them less likely to match with each other.

With time granularity reasoning, two distinct terms like  X 2005YY09MM X  and  X 2005YY09MM08DD X  may be coreferred, and thus the similarity may increase.
As a result, the optimal threshold for the C WT WR ManT method is higher than that of the baseline. However, for the C WT WR AllVT method, irrelevant times in the set of supposed topic times may inappropriately raise the similarity values for some off-topic stories which finally causes the optimal threshold to be lower than that of the C WT NR method.
 In accord with the similarity adjustment strategy in Equation (6), the
C NT TS Exp method achieves its best performance when the threshold is close to about two times that of the baseline C NT NR method. The similarity de-creases as time lapses in the C NT TS Linear method which leads the optimal threshold close to that of the baseline when the decay factor halftime is large enough.

Figures 6 and 7 plot the topic-weighted DET curves of four methods on the two datasets. Although these curves are very close to each other, the methods with time granularity reasoning demonstrate better performance in most of the threshold space. In Figure 6, the C WT WR AllVT and C WT WR ManT methods perform markedly better in the high-recall area (lower-right end of the curves), while they perform better at the high-precision area (upper-left end of the curves) in Figure 7. The C WT WR ManT X  X  DET curves on the two different datasets distinctly demonstrate the effectiveness and potential of our topic-tracking method with time granularity reasoning. 6. CONCLUSIONS AND FUTURE WORK
In this article, we present a new strategy with time granularity reasoning for using temporal information in topic tracking. The proposed algorithm follows the common strategy that adjusts the similarity value between an incoming story and a target topic according to their temporal relatedness. Compared to the previous algorithms, our method has four distinguishing characteristics.
First, we try to determine a topic time or a set of topic times for a target topic from the given on-topic stories. It helps avoid the negative influence of other irrelevant times in the seminal stories. Second, we take into account the phe-nomenon of time granularity variance when deciding whether a coreference relationship exists between two times. Third, both publication time and times expressed in the text are considered in our method. Finally, as time is just one attribute of a topic, we can increase the similarity value between a story and a topic only when they are related not only temporally but also semantically.
Experiments on the TDT corpora show that the proposed algorithm could make good use of temporal information in text and improve the performance of a baseline centroid system. With a reasonable time-reasoning strategy, tem-poral information does help improve a TDT system X  X  performance. Although we focused on the subtask of topic tracking in this article, we believe that our proposed method is also applicable to other subtasks.

We are planning to test our method on more datasets, especially on English corpora. As precise topic times would be helpful for improving the system X  X  performance, we expect to explore more effective methods for determining topic times in the future. Moreover, we consider extending our method to be able to dynamically revise topic times when more (pseudo) on-topic stories become available.
 We gratefully acknowledge the valuable comments and suggestions from Dawei Song, Jian-Yun Nie, and four anonymous reviewers.

