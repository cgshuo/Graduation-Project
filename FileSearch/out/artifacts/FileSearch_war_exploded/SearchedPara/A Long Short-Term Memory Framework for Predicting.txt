 There has been many recent attempts to detect and understand humor, irony and sarcasm from sen-tences, usually taken from Twitter (Reyes et al., 2013; Barbieri and Saggion, 2014; Riloff et al., 2013; Joshi et al., 2015), customer reviews (Reyes and Rosso, 2012) or generic canned jokes (Yang et al., 2015). Bamman and Smith (2015) and Karoui et al. (2015) included the surrounding context. Our work has a different focus from the above. We analyze transcripts of funny dialogues, a genre somehow neglected but important for human-robot interaction. Laughter is the natural reaction of peo-ple to a verbal or textual humorous stimulus. We want to predict when the audience would laugh. The utterances before the punchline are the setup. Without them, the punchlines may not be perceived as humorous (the last utterance, out of context, may be a political complaint), only with proper setup a laughter would be triggered. The humorous intent is also strengthen by the fact the dialog takes place in a bar (evident from the previous and following utter-ances), where a request of 40 ml of  X  X thyl Alcohol X  is unusual and weird.

Our previous attempts on the same corpus (Bert-ero and Fung, 2016b; Bertero and Fung, 2016a) showed that using a bag-of-ngram representation over a sliding window or a simple RNN to cap-ture the contextual information of the setup was not ideal. For this reason we propose a method based on a Long Short-Term Memory network (Hochre-iter and Schmidhuber, 1997), where we encode each sentence through a Convolutional Neural Network (Collobert et al., 2011). LSTM is successfully used in context-dependent sequential classification tasks such as speech recognition (Graves et al., 2013), de-pendency parsing (Dyer et al., 2015) and conversa-tion modelling (Shang et al., 2015). This is also to our knowledge the first-ever attempt that a LSTM is applied to humor response prediction or general hu-mor detection tasks. We employ a supervised classification method to detect when punchlines occur. The bulk of our classifier is made of a concatenation of a Convo-lutional Neural Network (Collobert et al., 2011) to encode each utterance, followed by a Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) to model the sequential context of the dialog. Before the output softmax layer we add a vector of higher level syntactic, structural and sentiment features. A framework diagram is shown in Figure 1. 2.1 Convolutional Neural Network for each The first stage of our classifier is represented by a Convolutional Neural Network (Collobert et al., 2011). Low-level, high-dimensional input feature vectors are fed into a first embedding layer to obtain a low dimensional dense vector. A sliding window is The convolution and max-pooling operation is ap-plied individually to each feature, and the three vec-tors obtained are then concatenated together and fed to the final sentence encoding layer, which combines all the contributions. 2.2 Long/Short Term Memory for the The LSTM is an improvement over the Recurrent Neural Network aimed to improve its memory ca-pabilities. In a standard RNN the hidden memory layer is updated through a function of the input and the hidden layer at the previous time instant: where x is the network input and b the bias term. This kind of connection is not very effective to main-tain the information stored for long time instants, as well as it does not allow to forget unneeded informa-tion between two time steps. The LSTM enhances the RNN with a series of three multiplicative gates. The structure is the following: where is the element-wise product. Each gate fac-tor is able to let through or suppress a specific update contribution, thus allowing a selective information retaining. The input gate i is applied to the cell input s , the forget gate f to the cell value at the previous time step c t  X  1 , and the output gate o to the cell out-put for the current time instant h t . In this way a cell value can be retained for multiple time steps when i = 0 , ignored in the output when o = 0 , and for-gotten when f = 0 .

As dialog utterances are sequential, we feed all ut-terance vectors of a sitcom scene in sequence into a Long Short-Term Memory block to incorporate con-textual information. The memory unit of the LSTM followed by a laughter would be the setup for the punchline.

We obtained a total of 135 episodes, 1589 overall scenes, 42 . 8% of punchlines, and an average inter-val between two punchlines of 2 . 2 utterances. We built a training set of 80% of the overall episodes, and a development and test set of 10% each. The episodes were drawn from all the seasons with the same proportion. The total number of utterances is 35865 for the training set, 3904 for the development set and 3903 for the test set. 3.2 Experimental setup and baseline In the neural network we set the size to 100 for all the hidden layers of the CNN and the LSTM, and 5 to the convolutional window. We applied a dropout regularization layer (Srivastava et al., 2014) after the output of the LSTM, and L2 regularization on the softmax output layer. The network was trained with standard backpropagation, using each scene as a training unit. The development set was used to tune the hyperparameters, and to determine the early stopping condition. When the error on the devel-opment set began to increase for the first time we kept training only the final softmax layer, this im-proved the overall results. The neural network was implemented with THEANO toolkit (Bergstra et al., 2010). We ran experiments with and without the ex-tra high-level feature vector.

As a baseline for comparison we used an imple-mentation of the Conditional Random Field (Laf-ferty et al., 2001) from CRFSuite (Okazaki, 2007), with L2 regularization. We ran experiments using laughters is a particularly challenging task. In some cases canned laughters are inserted by the show pro-ducers with the purpose of solicit response to weak jokes, where otherwise people would not laugh. The audience must also be kept constantly amused, extra canned laughters may help in scenes where fewer jokes are used. We proposed a Long Short-Term Memory based framework to predict punchlines in a humorous di-alog. We showed that our neural network is partic-ularly effective in increasing the F-score to 62 . 9% over a Conditional Random Field baseline of 58 . 1% . We furthermore showed that the LSTM is more ef-fective in obtaining an higher recall with fewer false positives compared to simple n-gram shifting con-text window features.

As future work we plan to use a virtual agent sys-tem to collect a set of human-robot humorous inter-actions, and adapt our model to predict humor from them.
 This work was partially funded by the Hong Kong Phd Fellowship Scheme, and partially by grant #16214415 of the Hong Kong Research Grants Council.

