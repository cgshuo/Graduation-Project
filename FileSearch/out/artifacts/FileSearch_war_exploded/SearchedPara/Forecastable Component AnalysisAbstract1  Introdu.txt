 Georg M. Goerg gmg@stat.cmu.edu With the rise of high-dimensional datasets it has be-come important to perform dimension reduction (DR) to a lower dimensional representation of the data. For simplicity we consider linear transformations W  X  R k  X  n , which map an n -dimensional X to a k  X  n di-mensional S = WX . Typically, the transformed data should be somewhat  X  X nteresting X ; there is no point in transforming X to an arbitrary S that is less useful, meaningful, etc. Let  X  ( S ) measure  X  X nterestingness X  of S . DR can then be set up as an optimization problem where (2) is a common DR constraint, which makes S j = w &gt; j X orthogonal (uncorrelated) to previously obtained signals.
 For example, principal component analysis (PCA) keeps large variance signals (Jolliffe, 2002)  X   X  ( X ) = E ( X  X  E X ) 2 in (1); independent component anal-ysis (ICA) recovers statistically independent signals (Hyv  X arinen and Oja, 2000); slow feature analysis (SFA) (Wiskott and Sejnowski, 2002) finds  X  X low X  sig-nals and is equivalent to maximizing the lag 1 auto-correlation coefficient.
 DR techniques are often applied to multivariate time series X t , hoping that forecasting on the lower-dimensional space S t is more accurate, simpler, more efficient, etc. Standard DR techniques such as PCA or ICA, however, do not explicitly address forecastability of the sources. For example, just because a signal has high variance does not mean it is easy to forecast. Thus let X  X  define interesting as being predictable. Fore-casting is not only good for its own sake (finance, eco-nomics), but even when future values are not immedi-ately interesting, signals that do have predictive power exhibit non-trivial structure by definition  X  and are thus easier to interpret. For example, the time series in Fig. 1 are ordered from least (S&amp;P500 daily returns) to most forecastable (monthly temperature in Notting-ham) according to the ForeCA forecastability measure  X ( x t ) I propose in Definition 3.1 below. And indeed moving from left to right they exhibit more structure. The main contributions of this work are i) a model-free, comparable measure of forecastability for (sta-tionary) time series (Section 3), ii) a novel data-driven DR technique, ForeCA, that finds forecastable signals, iii) an iterative algorithm that provably converges to (local) optima using fast eigenvector solutions (Sec-tion 4), and iv) applications showing that ForeCA outperforms traditional DR techniques in finding low-dimensional, forecastable subspaces, and that it can also be used for time series classification (Section 5). Related work will be reviewed in Section 6.
 All computations and simulations were done in R (R Development Core Team, 2010). Let y t be a univariate, second-order stationary time series with mean E y t =  X  y &lt;  X  , variance V y t =  X  2  X  ^ = 14.99% and autocovariance function (ACVF) The ACVF for univariate processes is symmetric in k ,  X  ( k ) =  X  y (  X  k ). Let  X  ( k ) =  X  ( k ) / X  (0) be the auto-correlation function (ACF). A large  X  ( k ) means that the process k time steps ago is highly correlated with the present y t . The sample ACFs that, e.g., S&amp;P 500 daily returns are uncorrelated with their own past (stock market efficiency); yearly tree ring growth is highly correlated over time with sig-nificant lags even for k  X  100 years; and intuitively temperature in month t is highly correlated with the temperature k = 6 (cold  X  warm) and k = 12 (cold  X  cold; warm  X  warm) months ago (or in the future). The building block of time series models is white noise  X  , which has zero mean, finite variance, and is un-correlated over time:  X  t  X  WN (0 , X  2  X  ) iff 1 i) E  X  t if  X  t is a Gaussian process, then it is also independent. For multivariate second-order stationary X t with mean 2  X   X  R n and covariance matrix  X  X the ACVF is a matrix-valued function of k  X  Z . In particu-lar,  X  X (0) =  X  X . The diagonal of  X  X ( k ) contains the ACVF of each X i ( t ); the off-diagonal element  X 
X ( k ) ( i,j ) is the cross-covariance between the i th and j th series at lag k : Contrary to  X  y ( k ),  X  X ( k ) is not symmetric, but 2.1. Spectrum and Spectral Density The spectrum of a univariate stationary process can be defined as the Fourier transform of its ACVF, where i = symmetric, the spectrum is a real-valued, non-negative function, S y : [  X   X , X  ]  X  R + . For white noise  X  t  X  all  X   X  [  X   X , X  ]. When  X  ( k ) &gt; 0 for k 6 = 0 the spec-trum has peaks at the corresponding frequencies. For example, the spectral density of monthly temperature series (right in Fig. 1) has large peaks at  X   X   X / 6 and  X / 12, which represent the half-and one-year cycle. 3 Vice versa, the ACVF can be recovered from the spec-trum using the inverse Fourier transform In particular, R  X   X   X  S y (  X  ) d X  =  X  2 y for k = 0. Let be the spectral density of y t . As f y (  X  )  X  0 and R  X   X  f y (  X  ) d X  = 1, the spectral density can be inter-preted as a probability density function (pdf) of an (unobserved) random variable (RV)  X  that  X  X ives X  on the unit circle. For white noise f  X  (  X  ) = 1 2  X  represents the uniform distribution U (  X   X , X  ). Remark 2.1 (Spectrum and spectral density) . In the time series literature  X  X pectrum X  and  X  X pectral den-sity X  are often used interchangeably. Here I reserve  X  X pectral density  X  for f y (  X  ) in (9) , as it integrates to one such as standard probability density functions. Forecasting is inherently tied to the time domain. Yet, since Eqs. (7) &amp; (8) provide a one-to-one mapping be-tween the time and frequency domain, we can use fre-quency domain properties to measure forecastability. The intuition for the proposed measure of forecasta-bility is as follows. Consider One can show that S y (  X  ) = p y (  X  ) (Gibson, 1994). If we have to predict the future of y t , then uncertainty about y t + h , h &gt; 0, is only manifested in uncertainty about Y , since cos (2  X  Y t +  X  ) is a deterministic func-tion of t : less uncertainty about Y means less uncer-tainty about y t + h . We can measure this uncertainty using the Shannon entropy of p y ( y ) (Shannon, 1948). It is thus natural to measure uncertainty about the future as (differential) entropy of f y (  X  ), where a &gt; 0 is the logarithm base.
 On a finite support [ b,c ] the maximum entropy occurs for the uniform distribution U ( b,c ); thus a flat spec-trum should indicate the least predictable sequence. And indeed, a flat spectrum corresponds to white noise, which is unpredictable by definition (using lin-ear predictors). Consequently, for any stationary y t with equality iff y t is white noise.
 Definition 3.1 (Forecastability of a stationary pro-cess) . For a second-order stationary process y t , let be the forecastability of y t .
 Contrary to other measures in the signal processing and time series literature,  X ( y t ) does not require actual forecasts, but is a characteristic of the process y t . It is therefore not biased to a particular  X  perhaps sub-optimal  X  model, forecast horizon, or loss function; as used in e.g., Box and Tiao (1977); Stone (2001). Properties 3.2.  X ( y t ) satisfies: a)  X ( y t ) = 0 iff y t is white noise. b) invariant to scaling and shifting: c) max sub-additivity for uncorrelated processes: The three series in Fig. 1 are ordered (left to right) by increasing forecastability and indeed larger b  X  cor-respond to intuitively more predictable real-world events: stock returns are in general not predictable; average monthly temperature is.
 We can thus use (12) to guide the search for optimal w that make y t = w &gt; X t as forecastable as possible. 3.1. Plug-in Estimator for  X  To estimate  X ( y t ), we first estimate S y (  X  ), normalize it, and then plug it in (11).
 An unbiased estimator of S y (  X  ) is the periodogram where  X  j = j/T , j = 0 , 1 ,...,T  X  1 are the (scaled) Fourier frequencies, and y T 1 = { y 1 ,...,y T } is a sample of y t . It is well known that (14) is not a good estimate (e.g., periodograms are not consistent). In the numer-ical examples we therefore use weighted overlapping segment averaging (WOSA) (Nuttal and Carter, 1982) b S (  X  j ) from the R package sapa : SDF(y,  X  X  X osa X  X ) . The bottom row of Figure 1 shows the normalized b f Remark 3.3. Typically, to estimate E g ( X ) for X  X  p ( x ) (here: g ( X ) = log p ( X ) ) the sample aver-age is solely over g ( x j ) without multiplicative p ( x terms. This however assumes that each x j is sam-pled from p ( x ) (and thus 1 n P n i =1 g ( x i )  X  E p R g ( x ) p ( x ) dx by the strong law of large numbers). While this is true in a standard sampling framework, here the  X  X ata X  are the Fourier frequencies  X  j and the fast Fourier transform (FFT) samples them uniformly (and deterministically) from [  X   X , X  ] and not according to the  X  X rue X  spectral density f (  X  ) . 4 Eq. (15) can be improved by a better spectral den-sity (Fryzlewicz, Nason, and von Sachs, 2008; Lees and Park, 1995; Trobs and Heinzel, 2006) and entropy es-timation (Paninski, 2003). Future research can also address direct estimation of (11)  X  as is common for classic entropy estimates (Sricharan, Raich, and Hero, 2011; Stowell and Plumbley, 2009). However, since neither spectrum nor entropy estimation are the pri-mary focus of this work, we use standard estimators for S y (  X  ) and then the plug-in estimator of (15). It must be noted though that b  X ( y T 1 ) in (15) is based on discrete rather than differential entropy. It still has the intuitive property that white noise has zero estimated forecastability, but now b  X ( y T 1 )  X  [0 , 1]; b  X ( y T the sample is a perfect sinusoid. Applications show that (15) yields reasonable estimates and we do not expect the results to change qualitatively for other es-timators. We leave differential entropy estimates of  X  to future work.
 Notice that  X ( y t ) relies on Gaussianity as only then f (  X  ) captures all the temporal dependence structure of y t . While time series are often non-Gaussian,  X (  X  ) is a computationally and algebraically manageable fore-castability measure  X  similarly to the importance of variance in PCA for iid data, even though they are rarely Gaussian. Recall from Eq. (1) that we want to find a linear com-bination of a multivariate X t that makes y t = w &gt; X t as forecastable as possible. Based on the forecastabil-ity measure in Section 3, we can now formally define the ForeCA optimization problem: max where (17) must hold since (11) uses the spectral den-sity of y t , i.e. we need V y t = w &gt;  X  X w = 1. Property 3.2c seems to let (16) only have a trivial boundary solution. However, it is intuitively clear that combining uncorrelated series makes forecasting (in general) more difficult, e.g., signal + noise. But if E x t y s 6 = 0 for some s,t  X  Z then combining them can make it simpler: for some  X   X  (0 , 1) it holds  X (  X x t + To optimize the right hand side of (16) we need to efficiently. We now show how to obtain f y (  X  ) by simple matrix-vector multiplication from f X (  X  ). 4.1. Spectrum of Multivariate Time Series and For multivariate X t the spectrum equals
S X (  X  ) = Contrary to the univariate case, (18) is in general complex-valued. Yet, since  X  X ( k ) =  X  X (  X  k ) &gt; S X (  X  )  X  C n  X  n is Hermitian for every  X  , S X (  X  ) = S
X (  X  ) &gt; , where z = a  X  ib is the complex conjugate of z = a + ib  X  C (Brockwell and Davis, 1991, p. 436). For dimension reduction we consider linear combina-tions y t = w &gt; X t , w  X  R n . By assumption E y t w In particular,  X  y (0) =  X  2 y = w &gt;  X  X w . The spec-trum of w &gt; X t can be quickly computed via S y (  X  ) = w &gt; S X (  X  ) w and consequently Since f y (  X  )  X  0 for every y t , w &gt; S X (  X  ) w  X  0 for all w  X  R n ; thus S X (  X  ) is positive semi-definite. 4.2. Solving the Optimization Problem Since  X  is invariant to shift and scale (Property 3.2b), we shall not only assume zero mean, but also contem-poraneously uncorrelated observed signals with unit variance in each component. WLOG consider U t =  X 
X X t ; thus E U t U the transformation for X t becomes c W X = c W U b  X   X  1 / 2 Problem (16) is then equivalent to where is the spectral entropy (Eq. (11)) of w &gt; X t as a func-tion of w . We use ` ( w ;  X  ) := log w &gt; S U (  X  ) w = log f w &gt; U (  X  ) for better readability.
 In practice we approximate (21) with b S U (  X  j )  X  C n  X  n and thus obtain 5 Here is the discretized version of (20), where b ` ( w ;  X  log w &gt; b S U (  X  j ) w . Notice that b S U (  X  j )  X  with  X  j while w  X  R n is fixed over all frequencies, which makes it difficult to obtain an analytic, closed-form solution. However, (22) can be solved iteratively borrowing ideas from the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin, 1977). 4.2.1. A Convergent EM-like Algorithm For every w  X  R n , k w k 2 = 1, h ( w ) has the form of a mixture model with weights w &gt; b S U (  X  j ) w  X  0 and  X  X og-likelihood X  b ` ( w ;  X  R ability distribution over {  X  j | 0 = 1 ,...,T  X  1 } . Just as in an EM algorithm, the objective h ( w ) can be optimized iteratively by first fixing w  X  w ( i ) b ` ( w ;  X  j ), and then minimizing the quadratic form Proposition 4.1. b S ( i ) U is positive semi-definite. Thus (24) can be solved analytically by the last eigen-vector of b S ( i ) U  X  automatically guaranteeing k w k 2 The procedure iterates until k w i +1  X  w i k &lt; tol for some tolerance level tol . For initialization we sam-ple w 0 from an n -dimensional uniform hyper-cube, U n (  X  1 , 1), and normalize to w 0 = w 0 / Theorem 4.2 (Convergence) . The sequence { w i } i  X  0 obtained via (24) converges to a local minimum b h
T ( w  X  ) =  X  is the smallest eigenvalue of b S (  X  ) U .
 Corollary 4.3. The transformed data y T, (  X  ) 1 = w Proof of Theorem 4.2. The entropy of a RV taking values in a finite alphabet {  X  0 ,..., X  T  X  1 } is bounded: 0  X  b h T ( w )  X  log a T for all w  X  R n . For convergence it remains to be shown that b h T ( w i )  X  b h T ( w i +1 equality iff w i +1 = w i = w  X  . First, since w i +1 is the last eigenvector of b S ( i ) U . Second, w where (27) holds as E p  X  log q =  X  P n j =1 p j log q  X  P n j =1 p j log p j = E p  X  log p for any q 6 = p . To lower the chance of landing in local optima we re-peat (24) for several random starting positions w 0 and then select the best solution.
 4.3. Obtaining a K -dimensional Subspace To obtain all K loadings W 1 ,...,K = [ w 1 ,..., w K ] that give uncorrelated series y j,t , we iteratively (starting at k = 1) i) compute w k , ii) project U onto the null space the EM-type algorithm on U ( k ) to obtain  X  w k +1 , and finally iv) transform  X  w k +1 back to loadings w ( k ) of U . Doing this for k = 1 ,...,K gives K loadings c W U . Loadings for X t are given by c W X = c W U b  X   X  1 / 2 X Here we demonstrate the usefulness of ForeCA to find informative, forecastable signals, but also as a tool for time series classification. 5.1. Improving Portfolio Forecasts Figure 2a shows daily returns of eight equity funds from 2002 / 01 / 01 to 2007 / 05 / 31 ( T = 1413). In the financial context finding forecastable series is an im-portant goal by itself, not just for structure discovery. In particular, we can interpret a linear combination w as a portfolio of stocks. The w  X  with the highest  X  gives the most forecastable portfolio.
 Figure 2b shows a bi-plot for PCA and ForeCA for ( w 1 , w 2 ) and ( w 3 , w 4 ). As PC 1 weighs all funds al-most equally, it represents the average market move-ment; the second component contrasts Gold &amp; Mining with the rest and we can therefore label PC 2 as the  X  X ommodity X  index. The third and fourth PC indicate energy/infrastructure and geographic regions.
 However, even though PC 1 is also the most pre-dictable PC, it has only a slightly larger b  X  than the most forecastable fund, India (Fig. 2c). On the other hand, combining Water (weight w water, 1 = 0 . 72) with Energy (0 . 58) is almost twice as forecastable as India (weights are from ForeC 1 in Fig. 2b). ForeC 2 also has high forecastability by selling Energy &amp; Water (  X  0 . 53 &amp;  X  0 . 47) and buying Mining &amp; Eastern Europe (0 . 55 &amp; 0 . 38). The third and fourth ForeCs seem to be hedg-ing strategies (ForeC 3: Water vs. Energy; ForeC 4: Latin America &amp; Gold vs. China &amp; Mining). As financial data only has very small autocorrelation  X  and usually at lag 1, if any  X , SFA and ForeCA yield overall very similar results, except for a  X  X rong X  rank-ing by SFA (Fig. 2c): SF 8 is the fastest feature (large, but negative lag 1 autocorrelation), yet it is the sec-ond most forecastable component. While it is true that white noise is slower than an auto-regressive process of order 1 ( AR (1)) with negative autocorrelation, the lat-ter is still more forecastable. Since we want to reveal intertemporal structure, white noise must be ranked lowest; and ForeCA indeed does so (Fig. 2d).
 ForeC 5 and 8 detect the 20 day lag (one trading month), but correlations are too low to achieve much higher forecastability than  X  simpler and faster  X  SFA. In the next example I study quarterly income data, where ForeCA can leverage its nonparametric power and detect important dependencies at various frequen-cies automatically from the data. 5.2. Classification of US State Economies I consider quarterly per-capita income growth rates of the  X  X ower 48 X  from 1982 / 1 to 2011 / 4 (last 30 years) where r j,t is the annual growth rate of region j . 6 Inter-ested in finding similar state economies within the US, we subtract the US baseline. Clustering states with similar economic dynamics can help to decide where to provide support when facing difficult economic times. For example, if certain states do not show any impor-tant dynamics on a 7-8 year scale  X  also known as the  X  X usiness cycle X  (Hughes Hallett and Richter, 2008)  X  then it might be better to support states that are affected by these global economy swings.
 The first row of Fig. 3 displays basic summary statis-tics: sample average, standard deviation, and first and fourth order autocorrelation. The second row give statistics related to forecastability: Fig. 3e shows based on the spectra in Fig. 3f; Fig. 3g shows the ab-solute lag 1 correlation (analogously for lag 4 in Fig. 3h), since two AR (1)s with a  X   X  lag 1 coefficient are equivalent in terms of forecasting (compare to SFA ranking in the portfolio example).
 The spectral densities of Nevada and Nebraska illus-trate the intuitive derivation of  X ( x t ) from Eq. (10): for Nebraska all frequencies are equally important and it is thus difficult to forecast any better than the sam-ple mean; contrary, Nevada X  X  income growth rates are mainly driven by a yearly cycle (  X  j  X  0 . 25) and low frequencies, thus Nevada is much easier to forecast. A similar dataset (but annually and for different years) has been analyzed in Dhiral, Kalpakis, Gada, and Put-tagunta (2001), who fit AR (1) models to the non-adjusted growth rates r j,t for 25 pre-selected states, and then cluster them in the model space. Although they obtain interpretable results, it is unlikely that US state economies only differ in their lag 1 coefficient. In particular, simple AR (1) models cannot capture the business cycle, which is clearly visible in Fig. 3f (even for the adjusted rates).
 Similarly, as SFA maximizes lag 1 correlation, it misses the quarterly cycle. ForeCA does not face this model selection bias, but can find forecastability across all frequencies. In particular, only ForeC 4 detects in-teresting high frequency signals (Fig. 4b). The most forecastable PCs are PC 5, 4, and 1; interestingly PC 3 is least important for forecasting among all 48 PCs. Also note that ForeCs are more interpretable than SFs or PCs (Figs. 4b -4d). Particularly, ForeC 1 shows a clear  X  25 year period (generation cycle), whereas PC 1 looks somewhat arbitrary. Yet, the associated load-ings in Fig. 4a are quite similar. Using predictability to separate signals is not new. In the classic time series literature Box and Tiao (1977) introduced canonical analysis and measure pre-dictive power by the residual variance of fitting vec-tor auto-regression (VAR) models. Recently Matteson and Tsay (2011) propose another DR technique that blends PCA and ICA by separating signals to the ex-tent of fourth moments (but not higher).
 Stone (2001) use predictability as a contrast function for blind source separation (BSS). While their ap-proach is similar to ours, it relies on subjective mea-sures of  X  X hort X  and  X  X ong X  term moving averages, which are then used to produce actual forecasts. Much work in BSS (Gomez-Herrero, Rutanen, and Egiazarian, 2010; Li and Adali, 2010), especially ICA, focuses on minimizing entropy rate. The entropy rate sian process is related to the spectrum via (Cover and Thomas, 1991, p. 417) However, these approaches require VAR model fits and/or numerical optimization.
 On the contrary, the ForeCA measure  X ( y t ) is based on information-theoretic uncertainty and is an inherent property of the stochastic process y t . We believe that this makes  X ( y t ) a more principled measure of fore-castability than model-dependent measures. Further-more, it can be estimated quickly using data-driven, nonparametric techniques.
 It is important to point out that spectral entropy , i.e., differential entropy of (11), is neither equal nor pro-portional to the entropy rate in (28). For particular processes they coincide (e.g., for an AR (1); Gibson (1994)), but in general they don X  X . They measure dif-ferent properties of the signal. Thus ICA algorithms based on entropy rate minimization do not yield the same results as ForeCA. In fact, the ForeCA measure can be used to rank ICs by decreasing forecastability. Cardoso (2004) gives an excellent account of the inter-twined relations between Gaussianity, autocorrelation, and dependence in multivariate time series and their effect on objective functions for BSS. Exactly because of this tangle, we only consider frequency properties of the signal and not entropy rate  X  since for forecasting the distribution itself is of minor importance compared to the temporal dependence. I introduce Forecastable Component Analysis (ForeCA), a new dimension reduction technique for multivariate time series. Contrary to other popular methods  X  such as PCA or ICA  X  ForeCA takes tem-poral dependence into account and actively searches for the most forecastable subspace. ForeCA minimizes the entropy of the spectral density: lower entropy implies a more forecastable signal. The optimization problem has an iterative, yet fast analytic solution, and provably leads to a (local) optimum.
 While SFA is a good approximation (maximizing lag 1 correlation), real world signals often have more com-plex correlation structure. The here proposed ForeCA can automatically detect arbitrary autocorrelation structure using nonparametric estimators. Applica-tions to financial and macro-economic data demon-strate that ForeCA is better than PCA and SFA at finding the most predictable signals, and can also be used for time series classifications.
 Box, G. E. P. and G. C. Tiao (1977). A canonical analysis of multiple time series. Biometrika 64 (2), 355 X 365.
 Brockwell, P. J. and R. A. Davis (1991). Time Se-ries: Theory and Methods (2 ed.). New York, NY: Springer Series in Statistics.
 Cardoso, J.-F. (2004). Dependence, correlation and gaussianity in independent component analysis. J. Mach. Learn. Res. 4 (7-8), 1177 X 1203.
 Cover, T. M. and J. Thomas (1991). Elements of In-formation Theory . Wiley.
 Dempster, A. P., N. M. Laird, and D. B. Rubin (1977). Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-ciety Series B Methodological 39 (1), 1 X 38.
 Dhiral, K. K., K. Kalpakis, D. Gada, and V. Putta-gunta (2001). Distance Measures for Effective Clus-tering of ARIMA Time-Series. In Proceedings of the 2001 IEEE International Conference on Data Min-ing , pp. 273 X 280.
 Fessler, J. A. and B. P. Sutton (2003). Nonuniform fast fourier transforms using min-max interpolation. IEEE Trans. Signal Process 51 , 560 X 574.
 Fryzlewicz, P., G. P. Nason, and R. von Sachs (2008). A wavelet-Fisz approach to spectrum estimation. Journal of Time Series Analysis 29 (5), 868 X 880. Gibson, J. (1994). What is the interpretation of spec-tral entropy? In Proceedings of IEEE International Symposium on Information Theory, 1994 , pp. 440. Gibson, J., S. Stanners, and S. McClellan (1993).
Spectral entropy and coefficient rate for speech cod-ing. In Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on , pp. 925  X 929 vol.2.
 Gomez-Herrero, G., K. Rutanen, and K. Egiazarian (2010). Blind source separation by entropy rate min-imization. Signal Processing Letters, IEEE 17 (2), 153  X 156.
 Hughes Hallett, A. and C. Richter (2008). Have the
Eurozone economies converged on a common Euro-pean cycle? International Economics and Economic Policy 5 , 71 X 101.
 Hyv  X arinen, A. and E. Oja (2000). Independent Component Analysis: Algorithms and Applications. Neural Networks 13 , 411 X 430.
 Jacques, L. and P. Vandergheynst (2010). Compressed
Sensing:  X  X hen sparsity meets sampling X  , Chap-ter 23, pp. 507 X 528. Wiley-Blackwell.
 Jolliffe, I. T. (2002). Principal Component Analysis (2 ed.). New York, NY: Springer.
 Lees, J. M. and J. Park (1995). Multiple-Taper Spectral-Analysis -A Stand-Alone C-Subroutine. Computers &amp; Geosciences 21 (2), 199 X 236.
 Li, X.-L. and T. Adali (2010). Blind spatiotemporal separation of second and/or higher-order correlated sources by entropy rate minimization. In Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on , pp. 1934  X 1937.
 Matteson, D. S. and R. S. Tsay (2011). Dynamic orthogonal components for multivariate time se-ries. Journal of the American Statistical Associa-tion 106 (496), 1450 X 1463.
 Nuttal, A. H. and G. C. Carter (1982). Spectral Esti-mation and Lag Using Combined Time Weighting.
 In Proceedings of IEEE , Volume 70, pp. 1111 X 1125. Paninski, L. (2003). Estimation of entropy and mutual information. Neural Comput. 15 (6), 1191 X 1253. R Development Core Team (2010). R: A Language and Environment for Statistical Computing . Vienna, Austria: R Foundation for Statistical Computing. ISBN 3-900051-07-0.
 Shannon, C. E. (1948). A Mathematical Theory of
Communication. Bell System Technical Journal 27 , 379 X 23, 623 X 656.
 Sricharan, K., R. Raich, and A. Hero (2011). K-nearest neighbor estimation of entropies with confidence. In Information Theory Proceedings (ISIT), 2011 IEEE International Symposiumon , pp. 1205  X 1209.
 Stone, J. V. (2001). Blind source separation using tem-poral predictability. Neural Comput. 13 (7), 1559 X  1574.
 Stowell, D. and M. D. Plumbley (2009). Fast Multidi-mensional Entropy Estimation by k-d Partitioning. IEEE Signal Processing Letters 16 , 537 X 540.
 Trobs, M. and G. Heinzel (2006). Improved spectrum estimation from digitized time series on a logarith-mic frequency axis. Measurement 39 (2), 120 X 129. Wiskott, L. and T. J. Sejnowski (2002). Slow Fea-ture Analysis: Unsupervised Learning of Invari-
