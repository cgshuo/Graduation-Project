 Giv en an image (or video clip, or audio song), how do we automatically assign keyw ords to it? The general prob-lem is to nd correlations across the media in a collection of multimedia objects like video clips, with colors, and/or motion, and/or audio, and/or text scripts. We prop ose a novel, graph-based approac h, \MMG", to disco ver suc h cross-mo dal correlations.

Our \MMG" metho d requires no tuning, no clustering, no user-determined constan ts; it can be applied to any multi-media collection, as long as we have a similarit y function for eac h medium; and it scales linearly with the database size. We rep ort auto-captioning exp erimen ts on the \standard" Corel image database of 680 MB, where it outp erforms do-main speci c, ne-tuned metho ds by up to 10 percen tage points in captioning accuracy (50% relativ e impro vemen t). H.2.8 [ Database Managemen t ]: Database Applications| Data Mining Design, Exp erimen tation
This material is based upon work supp orted by the Na-tional Science Foundation under Gran ts No. IIS-0121641, IIS-0083148, IIS-0113089, IIS-0209107, IIS-0205224, INT-0318547, SENSOR-0329549, EF-0331657, IIS-0326322, and by the Pennsylv ania Infrastructure Technology Alliance (PIT A) Gran t No. 22-901-0001. Additional funding was pro vided by donations from Intel, and by a gift from Northrop-Grumman Corp oration. y
Supp orted by the Post-do ctoral Fello wship Program of Ko-rea Science and Engineering Foundation (KOSEF) z Pinar Duygulu's curren t address: Departmen t of Computer Engineering, Bilk ent Univ ersit y, Ank ara, Turk ey, 06800 Cross-mo dal correlation, automatic image captioning, graph-based mo del
Giv en a collection of multimedia objects, we want to nd correlations across media. The driving application is auto-captioning, where the problem is de ned as follo ws:
Problem 1 (Auto-captioning) . Given a set S of color images, each with caption wor ds; and given one mor e, un-captione d image I , nd the best t (say, t =5) caption wor ds to assign to it.

However, the metho d we prop ose is general, and can be applied to video clips (with text scripts, audio, motion); on audio songs, with text lyrics, and so on.

Problem 2 (Informal-General) . Giv en n multime-dia obje cts, each consisting of m attributes (traditional nu-meric al attributes, or multime dia ones such as text, vide o, audio, time-se quenc e, etc). Find correlations across the me-dia (eg., correlate d keywor ds with image blobs/r egions; vide o motion with audio featur es).

For example, we want to answ er questions of the form \which keywor ds show up, for images with blue top" or \which songs are usual ly in the backgr ound of fast-moving vide o clips" .

We assume that domain exp erts have pro vided us with similarit y functions for all the involved media. The similar-ity function does not need to be perfect and is sucien t to our needs if it could appro ximately iden tify the neigh boring objects of an object.

There are multiple researc h pap ers, attac king parts of the problem. For example, to asso ciate words with images for automatic image captioning , people have prop osed meth-ods based on classi ers [15] or information retriev al tech-niques (relev ance mo del [10] and user feedbac k [26]), as well as building asso ciation mo dels (translation mo del [7]; hi-erarc hical mo del [2, 3, 4]; multi-resolution mo del [13]; co-occurrence mo del [16]). Video databases [25] spark e orts to asso ciate script words with faces [20], and visual/auditory characteristics with video genres (news or commercial) [18]. Similarly , there are successful e orts [24]. to asso ciate songs with their genres (lik e jazz, classical, etc.). Previous cor-relation disco very attempts suc h as LSI [19] and SDD [12] mostly consider categorical attributes. In this work, we pro-posed a general metho ds whic h consider both categorical and numerical attributes, as well as set-v alued attributes.
We would like to nd an unifying metho d, with the fol-lowing speci cations:
In the follo wing, we de ne the problem and describ e our prop osed metho d in Section 2 and 3, resp ectiv ely. Section 4 gives exp erimen tal results on real data. We discuss our observ ations in Section 5. Section 6 gives the conclusions.
We prop osed a novel approac h for cross-media correla-tions, and we use image captioning as an illustration. Table 1 sho ws the terminology we used in the pap er. The problem is more formally de ned as follo ws:
Problem 3 (Formal). Giv en a set S of n multime-dia obje cts S = f O 1 ; O 2 ; : : : ; O n g , each with m multime dia attributes, nd patterns/c orrelations among the obje cts and attributes.
 Sym bol Description V ( O i ) the vertex of G MMG for object O i .
 Table 1: Summary of symbols used in the paper We need to elab orate on the attributes: In traditional RDBMSs, attributes must be atomic (i.e., taking single val-ues, like \ISBN", or \video duration"). However, in our case, they can be set-value d , like a set of caption words, or, even missing altogether. Take problem 1 (auto-captioning) for example, S is a collection of captioned images, and we want to guess the (missing) caption terms of a new, uncap-tioned image. One attribute of an image is the \caption", whic h is set-v alued (a set of words).

We prop ose to gear our metho d towards set-v alued at-tributes, because they include atomic attributes as a special case; and they also smo othly handle the case of missing val-ues (null set). Thus, we only talk about set-v alued attributes from now on.

Definition 1. The domain D i of (set-value d) attribute i is the collection of atomic values that attribute i can choose from. The values of domain D i will be referr ed to as the domain tok ens of D i .

Assumption 1. For each domain D i ( i = 1 ; : : : ; m ), we are given a similarity function s i ( ; ) which assigns a score to each pair of domain tokens.

A domain can consist of categorical values, numerical val-ues, or numerical vectors. For problem 1 (auto-captioning), we have objects of m =2 attributes. The rst, \caption", has as domain a set of categorical values (English terms); the second, \image regions", is a set of p -dimensional nu-merical feature vectors ( p =30, as we describ e next). The similarit y function among the caption tok ens could be 1 if the two tok ens are iden tical, and 0 otherwise; the similarit y function for \regions" could be, say a function of the Eu-clidean distance between feature vectors. Let's elab orate on image captioning, before we presen t the main idea.
In the driving example of auto-captioning, the objects of interest are images. Eac h image has a set of regions ex-tracted from the image con ten t, and some of them also have a caption. See Figure 1 for the 3 sample images, their cap-tions and their regions. Figure 1: Three sample images, two of them anno-tated ((a1),(b1)), and their regions ((a2),(b2),(c2)); and their \MMG"graph (d). (Figures look best in color.)
For fair comparison, we use the same data used in the previous work[2, 7]. Image regions are extracted by a stan-dard segmen tation algorithm[22] (see Figure 1(d,e,f )). Eac h region is then mapp ed into a 30-dim feature vector. The p =30 features extracted from eac h region are the mean and standard deviation of RGB values, average resp onses to var-ious texture lters, its position in the entire image layout, and shap e descriptors (e.g., ma jor orien tation, or bounding region to real region area ratio)[7, 8]. Note that the exact feature extraction details are ortho gonal to our approac h -all our \MMG"metho d needs is a blac k box that will map eac h color image into a set of zero or more feature vectors.
Thus, we turned speci c auto-captioning problem (Prob-lem 1) into the general form (Problem 2) as follo ws: Giv en n image objects, eac h with m = 2 set-v alued attributes: \regions", a set of numerical feature vectors; \captions", a set of categorical terms, nd correlations across mo dalities (regions and words). For example, which caption terms are mor e likely, when the image has a blue, sky-like blob .
The question is what to do next, to capture cross-media correlations. Should we use clustering on feature vectors, or should we use some classi cation metho d, as it has been suggested before? And, if yes, how man y cluster cen ters should we sho ot for? Or, if we choose classi cation, whic h classi er should we use? Next we sho w how to handle, and actually , bypass, all these issues, for any multimedia setting.
The main idea is to represen t all the objects, as well as their attributes (domain tok ens) as nodes in a graph . For multimedia objects with m attributes, we obtain an ( m +1)-layer graph G MMG . There are m types of nodes (one for eac h attribute) and one more type of nodes for the objects. Next we describ e (a) how to generate this G MMG graph and (b) how to estimate cross-mo dal correlations using G MMG Gr aph construction. See Figure 1 for an example. We will denote as V ( O ) the vertex of object O , and as V ( a the vertex of the attribute value A = a i . We put an edge between the node of an object and the nodes of its attributes.
There is only one subtle point: For numerical and vector attributes, we need a way to re ect the similarit y between two attribute tok en values. Our approac h is to add an edge if and only if the two tok en values are close enough. For example, the orange \tiger" region r 6 and the orange sky region r 1 have feature vectors that are closed in Euclidean distance, and therefore, V ( r 1 ) and V ( r 6 ) are connected by an edge.
 We need to decide on a threshold for the \closeness". There are man y ways, but we decided to mak e the thresh-old adaptiv e: for eac h feature-v ector, choose its k nearest neigh bors, and add the corresp onding edges. We discuss the choice of k later, as well as the sensitivit y of our results to k . Computing the nearest neigh bors is straigh t-forw ard, because we already have the similarit y function s i ( ; ) for any domain D i (Assumption 1).

In summary , we have two types of links in our \MMG"graph: the nearest neighb or links ( NN-links ), between the nodes of two similar domain tok ens; and the obje ct-attribute-value links ( OAV-links ), between an object node and an attribute value node.

Example 1. Consider the image set S = I = f I 1 ; I 2 ; I ure 1). The graph corresponds to this data set has three types of nodes: one for the image obje cts i j 's ( j = 1 ; 2 ; 3 ); one for the regions r j 's ( j = 1 ; : : : ; 11 ), and one for the tiger g . Figur e 1(g) shows the resulting \MMG"gr aph. Solid arcs indic ate the obje ct-attribute-value (OAV-links) relation-ships; dashe d arcs indic ate nearest-neighb or (NN-links) re-lationships.
 In this example, we consider only k =1 nearest neigh bor, to avoid cluttering the diagram. Note that nearest neigh bor relationship is not symmetric, and we treat the edges as un-directional, whic h mak e some nodes have degree greater than 1 (for example, node V ( r 1 ): r 2 's nearest neigh bor is r , but r 1 's nearest neigh bor is r 6 ).

To solv e the auto-captioning problem (Problem 1), we need to dev elop a metho d to nd good caption words for the uncaptioned image (e.g., image I 3 ). This means that we need to estimate the anit y of eac h term to the uncaptioned image (i.e., the anit y of nodes t 1 , : : : , t 8 to node i discuss this next.
 turn the multimedia problem into a graph problem. Thus, we can tap the sizable literature of graph algorithms, and use o -the-shelf metho ds for assigning imp ortance to ver-tices in a graph, as well as determining how related is an un-captioned image (represen ted by node, say \A" in the graph), to the term \tiger" (represen ted, say, by node \B" in the graph).

We have man y choices: electricit y based approac hes [17, 6], random walks (PageRank, topic-sensitiv e PageRank) [5, 9], hubs and authorities [11], and elastic springs [14]. In this work, we prop ose to use random walk with restart (\R WR") for estimating the anit y of node \B" with resp ect to node \A". But, again, the speci c choice of metho d is orthogonal to our framew ork.

The \random walk with restarts" operates as follo ws: to compute the anit y of node \B" for node \A", consider a random walker that starts from node \A". The random walker chooses randomly among the available edges every time, except that, before he mak es a choice, with probabilit y c , he goes bac k to node \A" (restart). Let u A ( B ) denote the steady-state probabilit y that our random walker will nd himself at node \B". Then, u A ( B ) is what we want, the anit y of \B" with resp ect to \A".

Definition 2 (Affinity) . The imp ortanc e of node B with respect to node A is the steady-state probability u A of random walk with restarts, as de ne d above.

For example, to solv e the auto-captioning problem for im-age I 3 of Figure 1, we can estimate the steady-state proba-bilities u i 3 ( ) for all nodes of the graph G MMG , we can keep only the nodes that corresp ond to terms, and we can rep ort the top few (say, 5), as caption words for I 3 .
 Algorithms. For the general problem, the algorithm is as follo ws: First, build the \MMG" graph G MMG . When the user asks for the anit y of node \B" to node \A", estimate the steady-state probabilit y u A ( B ) de ned above.
The computation of the steady-state probabilities is very interesting and imp ortan t. We use matrix notation, for com-pactness. Let O q be the query object (e.g., image I 3 of Fig-ure 1). Supp ose that we want to nd the most related terms to O q . We do an RWR from node q = V ( O q ), and compute the steady state probabilit y vector ! u q =( u q (1) ; : : : ; u where N is the num ber of nodes in the graph G MMG .
The estimation of vector ! u q can be implemen ted e-cien tly by matrix multiplication. Let A be the adjacency matrix of the graph G MMG , and let it be column-normalized. Let ! v q be a column vector with all its N elemen ts zero, ex-cept for the entry that corresp onds to node q ; set this entry to 1. We call ! v q the \restart vector". Now we can formalize the de nition of the \anit y" of a node (De nition 2).
Definition 3 (Stead y-state vector). Let c be the probability of restarting the random walk from node q Then, the N -by-1 steady state probability vector, ! u q , (or simply, steady-state vector) satis es the equation:
The pseudo code of nding cross-mo dal correlations is sho wn in Figure 2. Let E be the num ber of edges in the graph built from the data set. The computational cost per iteration (step 4.1) is O( E ), for there are 2 E non-zero el-emen ts in the matrix A whic h are involved in the matrix multiplication. If we set a constan t maxim um num ber of iterations to be executed before con vergence, the overall cost for Algorithm-CCD is O( E ), linear to the data set size. Building the graph G MMG is a one time cost, and can be done ecien tly using a good nearest-neigh bor index (e.g., R+-tree [21]) over the objects.

Giv en a G MMG graph and an object O q . 1. Let ! v q =0, for all its N entries, except a '1' for the q -th entry. 2. Normalize the adjacency matrix of G MMG , A , by column. That is, mak e eac h column sum to 1. 3. Initialize ! u q = ! v q . 4. while( ! u q has not con verged ) Figure 2: Algorithm-CCD: Cross-mo dal correlation disco very
In this section, we sho w exp erimen tal results to address the follo wing questions:
In our exp erimen t, we use 10 image data sets from Corel, whic h is also used in previous works [2, 7]. On the average, eac h set has 5200 captioned images (eac h with about 4 cap-tioned terms), 1740 test images, 160 terms for captioning. One \MMG" graph is constructed for eac h data set, eac h of whic h has about 55,500 nodes and 180,000 edges.
 Quality . For eac h test image, we compute the captioning accuracy as the percen tage of terms whic h are correctly pre-dicted. For a test image whic h has m correct caption terms, \MMG" will predict also m terms. If p terms are correctly predicted, then the captioning accuracy for this test image is de ned as p m . This metric is also used in previous works [2, 7].

Figure 3 sho ws the average captioning accuracy for the 10 data sets. We compare our results with the results re-ported in [7] whic h builds a statistical translation mo del using exp ectation-maximization (EM) to capture correla-tion. We refer to the metho d as the \EM" approac h. On the average, on the same data used in [7], \MMG" achiev es captioning accuracy impro vemen t of 12 : 9 percen tage points, whic h corresp onds to a relativ e impro vemen t of 58%. The parameters are set at k =3 and c =0.8, but as sho wn later, the performance is insensitiv e to speci c settings. Figure 3: (EM and \MMG") The parameters for \MMG" are c = 0 : 8 , and K = 3 .

We also compare the captioning accuracy with even more recen t mac hine vision metho ds [2]: the Hierarc hical Asp ect Mo dels metho d (\HAM"), and the Laten t Diric hlet Allo ca-tion mo del (\LD A"). Figure 4 compares the best average captioning accuracy rep orted by the two metho ds (HAM and LD A), with those of the prop osed \MMG" metho d. The same data sets are used for fair comparison. Although both HAM and LD A impro ve on the EM metho d, they both lose to our generic \MMG" approac h (35%, versus 29% and 25%). We do not compare per data set accuracy due to the lack of suc h measuremen ts from [2], but we note that \MMG" gives signi can tly lower performance variance over the 10 data sets, by roughly an order of magnitude: 0.002 versus 0.02 and 0.03.
 Parameter defaults. We exp erimen t to nd out how would di eren t values of the parameters c (restart probabilit y) and k (nearest neigh bor size) a ect the captioning accuracy . Fig-ure 5 sho ws how the captioning accuracy of \MMG" varies with di eren t parameter settings: (a) xed k =3, vary c ; (b) xed c =0.9, vary k . The plateaus sho wn in the both plots indicate that the accuracy of the prop osed \MMG" is insen-sitiv e to the speci c setting of k and c . We sho w only the Figure 4: Comparing \MMG" with LDA and HAM on the 10 Corel data sets. LDA: ( , 2 )=(0.24,0.002); HAM: ( , 2 )=(0.298,0.003); MMG: (mean, variance)=(0.3503, 0.0002). : ac-curacy mean. 2 : accuracy variance. Figure 5: Plateaus in the plots show that the cap-tioning accuracy is insensitiv e to values of c and k . Data set \006". c : restart probabilit y, k : nearest neigh bor size. (a) Fix k =3, vary c . (b) Fix c =0.9, vary k . result on one data set \006", the results on other data sets are similar.
 Gener ality. \MMG" works on objects of any types. We design a exp erimen t of nding similar caption terms using \MMG". We use the same graph G MMG constructed for automatic image captioning. To nd the similar terms of a caption term t , we do \RWR", restarting from the node V ( t ). Table 2 sho ws the similar terms found for some of the caption terms. In the table, eac h row sho ws the caption term in question at the rst column, follo wed by the top 5 similar terms found by \MMG" (sorted by similarit y degree).
Notice that the retriev ed terms mak e a lot of sense: for example, the string 'branc h' in the caption is strongly re-lated to forest-and bird-related concepts ("birds", "owl", "nigh t"), and so on. Notice again that we did nothing spe-cial: no tf/idf, no normalization, no other domain-sp eci c analysis -we just treated these terms as nodes in our \MMG", like everything else.

Table 2: Similar terms of selected caption terms
We are sho oting for a metho d that requires no parameter tuning. Thus, here we discuss how to choose defaults for both our parameters, the num ber of neigh bors k , and the restart probabilit y c .
 Number of neighbor s k . In hindsigh t, the results of Fig-ure 5 mak e sense: with only k =1 neigh bor per region, the collection of regions is disconnected, missing imp ortan t con-nections and thus leading to poor captioning performance. On the other extreme, with a high value of k , everyb ody is directly connected to everyb ody else. The region nodes form almost a clique, whic h does not distinguish clearly between really close neigh bors, and just neigh bors.

For a medium num ber of neigh bors k , our NN-links appar-ently capture the neigh bors they should. Small deviations from that value, mak e little di erence, probably because the extra neigh bors we add, are at least as good as the previous ones.
 Restart probability c . For web graphs, the recommended value for c is typically c =0.15 [23]. Surprisingly , our exp er-imen ts sho w that good qualit y is achiev ed for c =0.8 or 0.9. We conjecture that what determines a good value for the \restart probabilit y" is the diameter of the graph. For the web graph, the diameter is appro ximately d =19 [1] whic h implies that the probabilit y p per ipher y for the random walker to reac h a node in the periphery is roughly (1 0 : 15) 19 =0 : 045. If we demand the same p per ipher y for the three-la yer graph (diameter is roughly 3) for captioning, then we have whic h is much closer to our empirical observ ations. Of course, the problem requires more careful analysis -but we are the rst to sho w that c =0.15 is not alw ays optimal for random walks with restarts.
We started from the image auto-captioning problem, and we dev elop ed \MMG", a general metho d that can spot cor-relations across media. The prop osed graph-based mo del can be applied to div erse multimedia data to nd multi-mo dal correlations. Moreo ver, to the best of our kno wledge, this is the rst e ort in multimedia databases, that prop oses suc h a graph-based approac h to nd patterns and correla-tions across media. The metho d has the follo wing desirable characteristics:
Future work could further exploit the promising connec-tion between multimedia databases and graph algorithms, that we prop ose here: imputation of missing values, outlier detection and any other data mining task that require the disco very of correlations as its rst step. [1] A. Alb ert, H. Jeong, and A.-L. Barabasi. Diameter of [2] K. Barnard, P. Duygulu, N. de Freitas, D. A. Forsyth, [3] K. Barnard, P. Duygulu, and D. A. Forsyth.
 [4] K. Barnard and D. A. Forsyth. Learning the seman tics [5] S. Brin and L. Page. The anatom y of a large-scale [6] P. G. Doyle and J. L. Snell. Random Walks and [7] P. Duygulu, K. Barnard, N. Freitas, and D. A. [8] C. Faloutsos. Searching Multime dia Datab ases by [9] T. H. Haveliw ala. Topic-sensitiv e PageRank. In [10] J. Jeon, V. Lavrenk o, and R. Manmatha. Automatic [11] J. Klein berg. Authoritativ e sources in a hyperlink ed [12] T. G. Kolda and D. P. O'Leary . A semidiscrete matrix [13] J. Li and J. Z. Wang. Automatic linguistic indexing of [14] L. Lovasz. Random walks on graphs: A surv ey. [15] O. Maron and A. L. Ratan. Multiple-instance learning [16] Y. Mori, H. Takahashi, and R. Oka. Image-to-w ord [17] C. R. Palmer and C. Faloutsos. Electricit y based [18] J.-Y. Pan and C. Faloutsos. VideoCub e: a novel tool [19] C. H. Papadimitriou, P. Ragha van, H. Tamaki, and [20] S. Satoh, Y. Nak amura, and T. Kanade. Name-it: [21] T. Sellis, N. Roussop oulos, and C. Faloutsos. The [22] J. Shi and J. Malik. Normalized cuts and image [23] Taher Haveliw ala, S. Kam var and G. Jeh. An [24] G. Tzanetakis and P. Cook. MARSY AS: A framew ork [25] H. Wactlar, M. Christel, Y. Gong, and [26] L. Wenyin, S. Dumais, Y. Sun, H. Zhang,
