 We propose a method to dynamically estimate the utility of doc-uments in a search session by modeling users X  browsing behaviors and novelty. The method can be applied to evaluate query refor-mulations in the context of a search session.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Soft-ware  X  performance evaluation (efficiency and effectiveness). Measurement, Performance, Experimentation, Human Factors.
 Evaluation; interactive search ; query reformulation; query sugge s-tion; search session . formation. However, users may issue ineffective queries for many reasons, such as typo and using words that are over -generalized or over-specified . Besides , it is also a search strategy to issue multi-ple queries for search [1] , for which each query deals with a facet or sub -topic of the problem . In a word, users usually need to re-formulate queries several times in a search session . in a search session. One is query suggestion [2-3] , which suggest s users with useful queries , so that the user s can take the suggested queries for search rather than reformulating their own queries. The other one is to optimize the search results of query reformulation s based on search histories , as studied in the TREC session track [4]. A popular method is to use the users  X  previous search queries and click -through data as relevance feedback for current searches [5]. As examined in [ 6], the new terms in users  X  query reformulations are very likely to come from users X  search histories, e.g. previous search que ries and contents of clicked and judged documents. needs to be solved is the evaluation of query reformulations in the context of a search session. Current studies usually evaluate query suggestions by user experiments [3] or using users X  reformulations from search logs as ground truth [6]. But the former is expensive and difficult to be reused, and the latter requires search logs that are mostly in accessible to the public . O ther studies [7-8] adopt a system -oriented evaluation approach, in which TREC -style ad hoc search datasets and evaluation metrics (e.g. P@k and nDCG@k) are adopted without considering the contexts of search sessions. Such method is cheap and reusable, but a s will be discussed in this paper, it is difficult to be justified and cannot cope with us ers X  query reformulation behaviors . la tion  X  X  search performance in the context of a search session. We believe the novelty factor should be considered in search session, i.e. a relevant document may be useless after it has been viewed by the user. We model users  X  browsing patterns and novelty in a session , so that the usefulness of a relevant document to the user will be discounted by both the probability that it has been viewed in previous searc hes and the user  X  X  need s for novelty . Our method can be applied to ad hoc search datasets or static session datasets [4] for evaluation of query suggestion algorithms [2-3, 6 -8] and session search algorithms [4-5]. search session: a search session is a process that involves one or many rounds of searches dealing with the same information need, for which we assume a set of documents can be judged as relevant to the whole session and the information need. This setting has been adopted in many related studies [4, 7 -10]. In the following discussion, w e use { q (1) , ... , q (n) } for a session, or q q refers to the i th query in the session.
 one of a session. Our purpose is to evaluate a query reformulation q  X  X  results ( n  X  2) in the context that the user searched for n  X  1 be updat ed in a search session when the user browses and exa m-ines search results , so that a relevant result may practically be useless to the user after it has been viewed many times . the beginning of the search session. In a static dataset, rel ( d ) is the relevance score of d judged by the user. After e ach time the user view ed d , d  X  X  utility has the probability  X  to be discounted to 0 , and the probability 1  X   X  to be ke pt the same. are ranked by the system . The user will always view the first re-sult of a query . After viewing a result, the user has the probability p to continue viewing the next result , and the probability 1  X  p to reformulate the next query or to leave the session.
 sion. We refer to the parameter  X  as the user  X  X  browsing novelty: a greater  X  value indicates a higher degree of browsing novelty and it is less likely that the user needs to view a result twice . adopt the browsing model in rank-biased precision (RBP) [11]. A similar browsing model has been adopted in the path -based search session evaluation methods [9]. However, our model differs from [9] in that we do not consider the cases that users may leave the search session before q (n) (modeled by p reform consider a session q (1...n) in the dataset as an existing fact which is caused by the user  X  X  decisions for reformulating from q context q (1...n-1) , we can define interactive relevance of a document d in the context ( irel ) as the expected utility of the document after rel ( d ) is the relevance score of d for the topic ;  X  is user  X  X  browsing been viewed by the user in R (i) , the search results of q 1 ( q (n) is the first query in the session), irel is reduced to rel . irel d q rel d P d R  X  the two extreme cases (e.g. either completely discount or not ). But the browsing model in P view ( d | R (i) ) can lead to natural discounting of relevance in a search session . According to the browsing model in section 2.1 , the probability that the user has viewed a result d in P the document is retrieved by a great number of previous search queries; 2) the document is ranked higher in the results of prev i-ous queries ; 3) a larger value of p or  X  is assigned . Let { d d } be a result list of 10 documents , each of which has relevance score 1. Figure 1 shows the irel scores for the 10 documents after the user viewed the result list { d 1 , d 2 , ... , d Figure 1. Discounting effects of irel with different parameters. we can first calculate irel based on the context q calculate ad hoc search evaluation metrics using irel scores rather than rel . We put  X  i  X  in front of the name of a n ad hoc search eva l-uation metric if it is calculated using irel , e.g. inDCG @10 means to calculate nDCG@10 using irel . uation methods [9] and nsDCG [10] from two perspectives. First, the irel method evaluates individual query  X  X  search performance in the context of a search session , while [9] and [10] both evaluate a whole search session X  X  search performance. Second, we explici t-ly considered users  X  browsing novelty in a search session , but [9] and [10] either assumes no novelty effect exists or ignores dupl i-cate documents in evaluatio n, which are difficult to be explained. However, both parameters may further be modeled by use r factors in interactive search: sults while some others may carefully examine one by one. Users of different styles may have different browsing persistence ( p ) and also different chances of missing relevant information in a doc u-ment , which can be used to model the parameter  X  . When brow s-ing style and effort are being considered,  X  and p may be related to each other . user  X  X  background knowledge and familiarity with related topics may influence whether, after viewing a result, the user can unde r-stand the information , which can be used to model  X  . Sometimes a user may gradually get familiar with the problem during a search session . Thus,  X  may change in different stages of the search ses-sion. search interactiveness , which is the extent to which the intera c-tive search problem is influenced by user s X  interaction s and differs from an ad hoc search problem. In our study, a higher p or  X  val ue will cause a greater difference betwe en rel and irel , and a greater difference between the interactive search problem and an ad hoc search one. use TREC session track dataset in 2011, which includes 76 search sessions, 280 queries, and 204 query reformulation pairs from real users. We do not use the dataset in 2010 because it is not created using real user experiments . TREC session track used Clueweb09 dataset. We retrieve r esults for each query using Indri (b y query likelihood model) on Clueweb09B dataset. Documents with w a-terloo spam rank scores less than 70 are removed.
 and the results of queries on different search systems, we use the TREC8 query track dataset . In TREC 8 query track dataset [13], 6 groups built 9 different retrieval systems, and created 21 different sets of queries for the same 50 topics. For each set of the queries, results on each of the 9 systems are provided. plement ed using the formula and parameter settings in [9]. We do not remove duplicate results in calculating nsDCG. changes of relevance in a search session , without golden standards , one may still argue the validity of irel and the evaluation metrics using irel . In our study, although we cannot fully prove the validi-ty of irel , we did find out some proof agains t using ad hoc search metrics for evaluating query reformulations. users X  reformulations ( q n-1  X  q n ). Table 2 shows the changes of ad hoc search performance from q n-1 to q n and the similarities b e-tween the pairs of queries X  top ranked results. First, we find , in general, users X  query reformulations will not cause significant change on ad hoc search performance . On average, the absolute values of changes in P@10, P@20, nDCG@10, and nDCG@2 0 do not ex ceed over 0.0 3, with certain degree of variance (standard deviation ranging from 0.2 to 0.3 on average); the changes are not significant by neither a paired t -test nor a Wilcoxon test (the r e-ported p values are for the paired t -test) . formulating for the purpose of improving queries X  ad hoc search performance (if we assume the users can in general reformulate effectively) , which can be a proof against the use of ad hoc search evaluation metrics in interactive search sessions. ings of results and the set s of documents returned in top position. We find on average the users  X  reformulations tend to retrieve r e-sults that are very diff erent from those from previous queries, with average jaccard similarity of results in top 10 and top 20 positions about only 0.35 . In general, rankings of results by q not correlated. These results indicate users may reformulate qu e-ries in order to find novel results that are different from those of previous queries . 
Table 2. Improvements of ad hoc search performance and rics . A popular method of calculating error rate [14] aims at stud y-ing the following problem : if statistically significant differences between two systems have been observed on one topic set , will we observe conflicting result s on another topic set ? This type of error rate is enough to indicate the stability of our metrics for evalua t-ing session search algorithms , as studied in [4-5]. However , for query suggestions , we need to also consider the effects of search systems , because it relies on the spec ific search systems to gene r-ate results for the query suggestions . A superior query suggestion method may be ineffective if we switch to other search systems. ly use one query set (A) from the 21 sets as  X  original queries  X , and two other sets (B and C) as two types of reformulations . Thus, w e will have 21  X   X  2 20  X  = 3990 pairs of  X  X ystems  X  for comparison and draw 3990 conclusions on which one is more effective . For each pair of query reformulations for comparison, results can be calc u-lated on different topics and different systems. We can calculate two types of error rate: in a system, if we observe statistically sig nificant difference be-tween two algorithms from one set of topics, whether conflicting results will be observed from another set of topics. Random part i-tions of topics are generated, i.e. we split 50 topics into two part i-tions of size n and 50  X  n . n ranges from 5 to 25. For n value, we randomly generate 100 different partitions for our study. For each of the randomly generated size n partition (referred to as  X  decision partition  X  in following discus sions), we use a paired t -test with p &lt; 0.05 to draw conclusions on whether  X  X   X  B X  is better or worse than  X  X   X  C X  , and check whether conflicting results are observed on the size 50  X  n partition. E rror rate is calculated as the rate that a conflicting conclusion is observed. on a set of topics, if we observe statistically significant difference between two query suggestion algorithms in one retrieval system , whether we will observe conflicting results from another system on the same set of topics. We also randomly generate subsets of the 50 topic s from size 5 to 25 (100 random subsets for each n value). For each random partition of topics , we iteratively exa m-ine for each pair of the 9 systems on whether significant results are observed from one system , but conflicting results are observed from another. Error rate is calculated as the number of times a conflicting result is observed divided by the number of times a significant result is observed on the decision partition. studying IR evaluation metrics, e.g. whether are conclusions from experiments on a limited num ber of topics generalizable? In com-parison, cross -system error rate studies a unique problem for que-ry suggestions , e.g. whether a good query suggestion for one r e-trieval system is still effective when we switch to another system. nsDCG@10, and inDCG@10 with different values of p and  X  . T o compare with previous studies of error rates [14] , we also estimate a trend line for error rate s. However, instead of using an expone n-tial function as suggested in [14 ], we find power functions may better fit with the trends . In general, we find expo nential function has limited fitness with the observed value s, with R 2 0.4 to 0.6 , while R 2 for power functions are around 0.7 to 0.8. 
Figure 3. Within -system error rate of inDCG, nDCG, and ics in the decision partition. This obs ervation is similar to prev i-ous studies in evaluation metrics for IR systems. When a limited number of topics are examined, e.g. n = 5 or 10, we will easily come to wrong conclus ions , no matter by ad hoc search metrics such as nDCG@10 or by session search metrics . we discount relevance by contexts (the higher p and  X  values and the higher interactiveness), the less stable the inDCG metric s are . To keep the clarity of figure 3, we only show error rate s of inDCG for 3 different parameter settings. The general trends we observed error rate of inDCG will increase if either p or  X  increases. For example, inDCG@10 with p = 0.95 and  X  = 0.8 will consistently have about 0.02 higher error rate than inDCG@10 with p = 0. 8 and  X  = 0.5 . The trends of parameter s are also consistent with the lowest error rate observed for nsDCG. Because we do not remove duplicate documents, nsDCG do not consider novelty and will not discount relevance of articles by contexts , which is comparable to the setting of inDCG with  X  = 0 . Results indicate the error rates of inDCG are comparable to those of nDCG and nsDCG; only when the search problem is highly interactive (with high values of p and  X  ), inDCG will have an observable higher error rate than nDCG and nsDCG . correlated between query suggestions  X  ad hoc search performance (by nDCG) and the search performance in a search session (by inDCG) . We applied the query suggestion method in [8] to TREC robust 04 dataset using Clueweb09B anchor texts as alternatives of query logs for query reformulation. For each topic, a list of query suggestions (including both query term addition and query term substitution patterns ) are generated. We select up to 100 top ranked queries by nDCG@10 and inDCG@10. Figure 4 shows the correlation of queries X  ranking s by nDCG and inDCG, and the overlap between the two ranked lists of queries. Results in Figure 4 indicate the higher degree of search interactiveness (greater p or  X  ), the less si milarity between queries  X  ad hoc search performance and the search performance in session. When the problem is high-ly interactive ( e.g. p = 0.95 and  X  = 0.8 ), queries X  performance by nDCG and inDCG are very different (spear man  X  X   X  is only 0.130 and jaccard similarity is 0.42). 
Figure 4. Similarity and correlation between ranked lists of and nsDCG . In general, when evaluating with a higher cutoff k value, we will observe a slightly lower error rate and higher sim i-larity and correlation between queries X  ad hoc search and session search performance . evaluation of query suggestions . Figure 5 shows results of cross system error rate s for inDCG , which indicates the comparative performance of queries are surprisingly consistent cross different search systems. Although previous studies indicate different IR systems have strong bias to certain types of topics, and practically even  X  X he best system is normally above average for most of the topics, and best for maybe 5%-10% of the topics X  [13] , our results in Figure 5 indicate the superior performance of one query over others in a retrieval system is very stable when we switch to other retrieval systems. E ven when only 5 topics are randomly sampled, two quer y sets will perform very similarly cross different systems on the sampled 5 topics . we come to a wrong conclusion cross different systems. But in general only less than 3% error rate s are observed, which indicate, onc e we find a good way of generat ing queries, it can be applied effectively to most of other retrieval systems. This also suggests results reported for query suggestion algorithm s using one retrie v-al system are very likely to be generalized to other retrieval sy s-tem s. mulations in the context of a search session, which can be used as economic alternatives of user studies and query logs to evaluate query suggestion algorithms [2, 8] or session search system [4, 5] . We find users tend to reformulate queries that can retrieve search results very different from those of previous queries, but users do not reformulat e to enhance the ad hoc search performa nce, which indicates ad hoc search metrics should not be adopted to evaluate query reformulations in a search session. The proposed evaluation methods are stable compared with existing metrics, and have the advantages of simulating users  X  browsing behaviors and novelty . We find the higher the search interactiveness, the less stable the evaluation metrics are . Besides, queries  X  search performance are very stable over different retrieval systems, which suggests query suggestion can be widely adopted as a general technique mostly indepen dent of the differences of search systems. dation grant IIS -1052773 and III-COR 0704628. [1] M. J. Bates. 1989. The design of browsing and b errypicking [2] R. Baeza -Yates, C. Hurtado and M. Mendoza. 2005. Query [3] D. Kelly, K. Gyllstrom, and E. W. Bailey. 2009. A compari-[4] E. Kanoulas, P. Clough, B. Carterette, and M. Sanderson. [5] Z. Yue, J. Jiang, S. Han, D. He. 2012. Where Do the Query [6] J. Jiang, D. He, S. Han, J. Wu. 2011. Pi tt at TREC 2011 se s-[7] L. Bing, W. Lam, and T. Wong. 2011. Using query log and [8] X. Wang and C. Zhai. 2008. Mining term association pa t-[9] E. Kanoulas, B. Carterette, P. D. Clough, and M. Sanderson. [10] K. J X rvelin, S. L. Price, L. Delcambre and M. L. Nielsen. [11] A. Moffat and J. Zobel. 2008. Rank-Biased Precision for [12] C. Clarke, M. Kolla, G. Cormack et al. 2008. Novelty and [13] C. Buckley and J . Walz. 1999. The TREC -8 Query Track. In [14] M. Sanderson, J. Zobel . 2005. Information Retrieval System 
