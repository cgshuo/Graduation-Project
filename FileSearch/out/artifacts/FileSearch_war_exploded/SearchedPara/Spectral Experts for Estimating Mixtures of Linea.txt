 Arun Tejasvi Chaganty chaganty@cs.stanford.edu Percy Liang pliang@cs.stanford.edu Stanford University, Stanford, CA 94305 USA Discriminative latent-variable models, which combine the high accuracy of discriminative models with the compact expressiveness of latent-variable models, have been widely applied to many tasks, including ob-ject recognition (Quattoni et al., 2004), human ac-tion recognition (Wang &amp; Mori, 2009), syntactic pars-ing (Petrov &amp; Klein, 2008), and machine translation (Liang et al., 2006). However, parameter estimation in these models is difficult; past approaches rely on local optimization (EM, gradient descent) and are vulnera-ble to local optima.
 Our broad goal is to develop efficient provably consis-tent estimators for discriminative latent-variable mod-els. In this paper, we provide a first step in this direc-tion by proposing a new algorithm for a simple model, a mixture of linear regressions (Viele &amp; Tong, 2002). Recently, method of moments estimators have been developed for generative latent-variable models, in-cluding mixture models, HMMs (Anandkumar et al., 2012b), Latent Dirichlet Allocation (Anandkumar et al., 2012a), and parsing models (Hsu et al., 2012). The basic idea of these methods is to express the un-known model parameters as a tensor factorization of the third-order moments of the model distribution, a quantity which can be estimated from data. The mo-ments have a special symmetric structure which per-mits the factorization to be computed efficiently using the robust tensor power method (Anandkumar et al., 2012c).
 In a mixture of linear regressions, using third-order moments does not directly reveal the tensor structure of the problem, so we cannot simply apply the above tensor factorization techniques. Our approach is to employ low-rank linear regression (Negahban &amp; Wain-wright, 2009; Tomioka et al., 2011) to predict the sec-ond and third powers of the response. The solution to these regression problems provide the appropriate symmetric tensors, on which we can then apply the tensor power method to retrieve the final parameters. The result is a simple and efficient two-stage algo-rithm, which we call Spectral Experts. We prove that our algorithm yields consistent parameter estimates under certain identifiability conditions. We also con-duct an empirical evaluation of our technique to un-derstand its statistical properties (Section 5). While Spectral Experts generally does not outperform EM, presumably due to its weaker statistical efficiency, it serves as an effective initialization for EM, significantly outperforming EM with random initialization. 1.1. Notation Let [ n ] = { 1 ,...,n } denote the first n positive integers. We use O ( f ( n )) to denote a function g ( n ) such that lim n  X  X  X  g ( n ) /f ( n ) &lt;  X  .
 We use x  X  p to represent the p -th order tensor formed by taking the tensor product of x  X  R d ; i.e. x  X  p i x alized dot product between two p -th order tensors:  X  X,Y  X  = P i metric if for all i,j  X  [ d ] p which are permutations paper will be symmetric). For a p -th order tensor X  X  ( R d )  X  p , the mode-i unfolding of X is a matrix X ments of X whose i -th index is equal to j .
 For a vector X , let k X k op denote the 2-norm. For a matrix X , let k X k  X  denote the nuclear (trace) norm (sum of singular values), k X k F denote the Frobenius norm (square root of sum of squares of singular val-ues), k X k max denote the max norm (elementwise max-imum), k X k op denote the operator norm (largest sin-gular value), and  X  k ( X ) be the k -th largest singular value of X . For a p -th order tensor X , let k X k all p unfoldings, and let k X k op = 1 p P p i =1 k X ( i ) note the average operator norm over all p unfoldings. For a tensor X  X  ( R d )  X  p , let cvec( X )  X  R tion of X . For example, if X  X  R d  X  d , cvec( X ) = ( X component of cvec( X ) is indexed by a vector of counts ( c 1 ,...,c d ) with total sum P i c i = p . The value of of index vectors k whose count profile is c . For sym-metric tensors X and Y ,  X  X,Y  X  =  X  cvec( X ) , cvec( Y )  X  . Later, we X  X l see that vectorization allow us to per-form regression on tensors, and collapsing simplifies our identifiability condition. The mixture of linear regressions model (Viele &amp; Tong, 2002) defines a conditional distribution over a response y  X  R given covariates x  X  R d . Let k be the number of mixture components. The generation of y given x involves three steps: (i) draw a mixture component h  X  [ k ] according to mixture proportions  X  = (  X  1 ,..., X  k (ii) draw observation noise from a known zero-mean noise distribution E , and (iii) set y deterministically based on h and . More compactly: The parameters of the model are  X  = (  X ,B ), where  X   X  R d are the mixture proportions and B = [  X  1 |  X  X  X  |  X  k ]  X  R d  X  k are the regression coefficients. Note that the choice of mixture component h and the observation noise are independent. The learning problem is stated as follows: given n i.i.d. samples ( x (1) ,y (1) ) ,..., ( x ( n ) ,y ( n ) ) drawn from the model with some unknown parameters  X   X  , return an estimate of the parameters  X   X  .
 The mixture of linear regressions model has been ap-plied in the statistics literature for modelling music perception, where x is the actual tone and y is the tone perceived by a musician (Viele &amp; Tong, 2002). The model is an instance of the hierarchical mixture of experts (Jacobs et al., 1991), in which the mixture proportions are allowed to depend on x , known as a gating function. This dependence allow the experts to be localized in input space, providing more flexibility, but we do not consider this dependence in our model. The estimation problem for a mixture of linear regres-sions is difficult because the mixture components h are unobserved, resulting in a non-convex log marginal likelihood. The parameters are typically learned us-ing expectation maximization (EM) or Gibbs sampling (Viele &amp; Tong, 2002), which suffers from local optima. In the next section, we present a new algorithm that sidesteps the local optima problem entirely. In this section, we describe our Spectral Experts al-gorithm for estimating model parameters  X  = (  X ,B ). The algorithm consists of two steps: (i) low-rank re-gression to estimate certain symmetric tensors; and (ii) tensor factorization to recover the parameters. The two steps can be performed efficiently using convex optimization and tensor power method, respectively. To warm up, let us consider linear regression on the re-sponse y given x . From the model definition, we have y =  X  &gt; h x + . The challenge is that the regression coef-ficients  X  h depend on the random h . The first key step is to average over this randomness by defining average regression coefficients M 1 def = P k h =1  X  h  X  h . Now we can express y as a linear function of x with non-random coefficients M 1 plus a noise term  X  1 ( x ): The noise  X  1 ( x ) is the sum of two terms: (i) the mixing noise  X  M 1  X   X  h ,x  X  due to the random choice of the mix-ture component h , and (ii) the observation noise  X  X  . Although the noise depends on x , it still has zero mean conditioned on x . We will later show that we can per-form linear regression on the data { x ( i ) ,y ( i ) } n duce a consistent estimate of M 1 . But clearly, knowing M 1 is insufficient for identifying all the parameters  X  , as M 1 only contains d degrees of freedom whereas  X  contains O ( kd ).
 Intuitively, performing regression on y given x provides only first-order information. The second key insight is that we can perform regression on higher-order pow-ers to obtain more information about the parameters. Specifically, for an integer p  X  1, let us define the average p -th order tensor power of the parameters as follows: Now consider performing regression on y 2 given x  X  2 . Expanding y 2 = (  X   X  h ,x  X  + ) 2 , using the fact that  X   X  h ,x  X  p =  X   X   X  2 ( x ) =  X   X  Again, we have expressed y 2 has a linear function of x  X  2 with regression coefficients M 2 , plus a known bias E [ 2 ] and noise. 1 Importantly, the noise has mean zero; in fact each of the three terms has zero mean by defi-nition of M 2 and independence of and h .
 Performing regression yields a consistent estimate of M 2 , but still does not identify all the parameters  X  . In particular, B is only identified up to rotation: if B = [  X  1 |  X  X  X  |  X  k ] satisfies B diag(  X  ) B &gt; = M 2 is uniform, then ( BQ ) diag(  X  )( Q &gt; B &gt; ) = M 2 for any orthogonal matrix Q .
 Let us now look to the third moment for additional information. We can write y 3 as a linear function of x  X  3 with coefficients M 3 , a known bias 3 E [ 2 ]  X   X  M 1 E [ 3 ] and some noise  X  3 ( x ):  X  3 ( x ) =  X   X  The only wrinkle here is that  X  3 ( x ) does not quite have zero mean. It would if  X  M 1 were replaced with M 1 but M 1 is not available to us. Nonetheless, as  X  M concentrates around M 1 , the noise bias will go to zero. Performing this regression yields an estimate of M 3 We will see shortly that knowledge of M 2 and M 3 are sufficient to recover all the parameters.
 Now we are ready to state our full algorithm, which we call Spectral Experts (Algorithm 1). First, we perform Algorithm 1 Spectral Experts input Datasets D p = { ( x (1) ,y (1) ) ,  X  X  X  , ( x ( n ) output Parameters  X   X  = ( X   X , [  X   X  1 | X  X  X |  X   X  k ]). 1: Estimate compound parameters M 2 ,M 3 using 2: Estimate parameters  X  = (  X ,B ) using tensor fac-three regressions to recover the compound parameters M 1 (4), M 2 (6), and M 3 (7). Since M 2 and M 3 both only have rank k , we can use nuclear norm regulariza-tion (Tomioka et al., 2011; Negahban &amp; Wainwright, 2009) to exploit this low-rank structure and improve our compound parameter estimates. In the algorithm, the regularization strengths  X  (2) n and  X  (3) n are set to for some constant c .
 Having estimated the compound parameters M 1 , M 2 and M 3 , it remains to recover the original parame-ters  X  . Anandkumar et al. (2012c) showed that for M 2 and M 3 of the forms in (5), it is possible to ef-ficiently accomplish this. Specifically, we first com-pute a whitening matrix W based on the SVD of M 2 and use that to construct a tensor T = M 3 ( W,W,W ) whose factors are orthogonal. We can use the robust tensor power method to compute all the eigenvalues and eigenvectors of T , from which it is easy to recover the parameters  X  and {  X  h } .
 Related work In recent years, there has a been a surge of interest in  X  X pectral X  methods for learning latent-variable models. One line of work has focused on observable operator models (Hsu et al., 2009; Song et al., 2010; Parikh et al., 2012; Cohen et al., 2012; Balle et al., 2011; Balle &amp; Mohri, 2012) in which a re-parametrization of the true parameters are recov-ered, which suffices for prediction and density estima-tion. Another line of work is based on the method of moments and uses eigendecomposition of a certain tensor to recover the parameters (Anandkumar et al., 2012b;a; Hsu et al., 2012; Hsu &amp; Kakade, 2013). Our work extends this second line of work to models that require regression to obtain the desired tensor. In spirit, Spectral Experts bears some resemblance to the unmixing algorithm for estimation of restricted PCFGs (Hsu et al., 2012). In that work, the obser-vations (moments) provided a linear combination over the compound parameters.  X  X nmixing X  involves solv-ing for the compound parameters by inverting a mixing matrix. In this work, each data point (appropriately transformed) provides a different noisy projection of the compound parameters.
 Other work has focused on learning discriminative models, notably Balle et al. (2011) for finite state transducers (functions from strings to strings), and Balle &amp; Mohri (2012) for weighted finite state au-tomata (functions from strings to real numbers). Sim-ilar to Spectral Experts, Balle &amp; Mohri (2012) used a two-step approach, where convex optimization is first used to estimate moments (the Hankel matrix in their case), after which these moments are subjected to spectral decomposition. However, these methods are developed in the observable operator framework, whereas we consider parameter estimation.
 The idea of performing low-rank regression on y 2 has been explored in the context of signal recovery from magnitude measurements (Candes et al., 2011; Ohls-son et al., 2012). There, the actual observed response was y 2 , whereas in our case, we deliberately construct powers y,y 2 ,y 3 to identify the underlying parameters. In this section, we provide theoretical guarantees for the Spectral Experts algorithm. Our main result shows that the parameter estimates  X   X  converge to  X  at  X  n rate that depends polynomially on the bounds on the parameters, covariates, and noise, as well the k -th smallest singular values of the compound parameters and various covariance matrices.
 Theorem 1 (Convergence of Spectral Experts) . As-sume each dataset D p (for p = 1 , 2 , 3 ) consists of n i.i.d. points independently drawn from a mixture of lin-ear regressions model with parameter  X   X  . 2 Further, as-sume k x k 2  X  R , k  X   X  h k 2  X  L for all h  X  [ k ] , | | X  S and  X  p 0 for each p  X  { 1 , 2 , 3 } . Let &lt; 1 2 . Suppose the number of samples is n = max( n 1 ,n 2 ) where n 1 =  X  n If each regularization strength  X  ( p ) n is set to for p  X  2 , 3 , then the parameter estimates  X   X  = ( X   X , returned by Algorithm 1 (with the columns appropri-ately permuted) satisfies for all h  X  [ k ] .
 While the dependence on some of the norms ( L 6 ,S 6 ,R 12 ) looks formidable, it is in some sense unavoidable, since we need to perform regression on third-order moments. Classically, the number of sam-ples required is squared norm of the covariance matrix, which itself is bounded by the squared norm of the data, R 3 . This third-order dependence also shows up in the regularization strengths; the cubic terms bound each of 3 ,  X  3 h and k ( x  X  3 )  X  2 k F with high probability. The proof of the theorem has two parts. First, we bound the error in the compound parameters estimates  X  M we use results from Anandkumar et al. (2012c) to con-vert this error into a bound on the actual parameter estimates  X   X  = ( X   X ,  X  B ) derived from the robust tensor power method. But first, let us study a more basic property: identifiability. 4.1. Identifiability from moments In ordinary linear regression, the regression coefficients  X   X  R d are identifiable if and only if the data has full rank: E [ x  X  2 ] 0, and furthermore, identifying  X  requires only moments E [ xy ] and E [ x  X  2 ] (by observing the optimality conditions for (4)). However, in mixture of linear regressions, these two moments only allow us to recover M 1 . Theorem 1 shows that if we have the higher order analogues, E [ x  X  p y  X  p ] and E [ x  X  2 p p  X  X  1 , 2 , 3 } , we can then identify the parameters  X  = (  X ,B ), provided the following identifiability condition holds: E [cvec( x  X  p )  X  2 ] 0 for p  X  X  1 , 2 , 3 } . This identifiability condition warrants a little care, as we can run into trouble when components of x are de-pendent on each other in a particular algebraic way. For example, suppose x = (1 ,t,t 2 ), the common poly-nomial basis expansion, so that all the coordinates are deterministically related. While E [ x  X  2 ] 0 might be satisfied (sufficient for ordinary linear regression), E [cvec( x  X  2 )  X  2 ] is singular for any data distribution. t which are linearly dependent. Therefore, Spectral Ex-perts would not be able to identify the parameters of a mixture of linear regressions for this data distribution. We can show that some amount of unidentifiability is intrinsic to estimation from low-order moments, not just an artefact of our estimation procedure. Sup-pose x = ( t,...,t d ). Even if we observed all moments E resulting coordinates would be monomials of t up to only degree 2 dr , and thus the moments live in a 2 dr -dimensional subspace. On the other hand, the param-eters  X  live in a subspace of at least dimension dk . Therefore, at least r  X  k/ 2 moments are required for identifiability of any algorithm for this monomial ex-ample. 4.2. Analysis of low-rank regression In this section, we will bound the error of the com-pound parameter estimates k  X  2 k 2 F and k  X  3 k 2 F , where  X  sis is based on the low-rank regression framework of Tomioka et al. (2011) for tensors, which builds on Ne-gahban &amp; Wainwright (2009) for matrices. The main calculation involved is controlling the noise  X  p ( x ), which involves various polynomial combinations of the mixing noise and observation noise.
 Let us first establish some notation that unifies the three regressions ((8), (9), and (10)). Define the obser-vation operator X p ( M p ) : R d  X  p  X  R n mapping com-pound parameters M p : Let  X  ( X p ) be the restricted strong convexity constant, Lemma 1 (Tomioka et al. (2011), Theorem 1) . Sup-pose there exists a restricted strong convexity constant  X  ( X p ) such that 1 n k X p ( X ) k 2 2  X   X  ( X p ) k  X  k 2 F and  X  ( p ) n  X  Then the error of  X  M p is bounded as follows: Going forward, we need to lower bound the restricted strong convexity constant  X  ( X p ) and upper bound the operator norm of the adjoint operator k X  X  p (  X  p ) k The proofs of the following lemmas follow from stan-dard concentration inequalities and are detailed in the supplementary material.
 Lemma 2 (lower bound on restricted strong convexity constant) . If then with probability at least 1  X   X  : for each p  X  [3] .
 Lemma 3 (upper bound on adjoint operator) . If then with probability at least 1  X   X  : for each p  X  [3] . 4.3. Analysis of the tensor factorization Having bounded the error of the compound parameter estimates  X  M 2 and  X  M 3 , we will now study how this er-ror propagates through the tensor factorization step of Algorithm 1, which includes whitening, applying the robust tensor power method (Anandkumar et al., 2012c), and unwhitening.
 Lemma 4. Let M 3 = P k h =1  X  h  X   X  3 h . Let k  X  M 2  X  M and k  X  M 3  X  M 3 k op both be less than for some &lt; 1 2 . Then, there exists a permutation of indices such that the parameter estimates found in step 2 of Algorithm 1 satisfy the following with probability at least 1  X   X  : for all h  X  [ k ] .
 The proof follows by applying standard matrix per-turbation results for the whitening and unwhitening operators and has again been deferred to the supple-mentary material. 4.4. Synthesis Together, these lemmas allow us to control the com-pound parameter error and the recovery error. We now apply them in the proof of Theorem 1: Proof of Theorem 1 (sketch). By Lemma 1, Lemma 2 and Lemma 3, we can control the Frobenius norm of the error in the moments, which directly upper bounds the operator norm: If n  X  max { n 1 ,n 2 } , then We complete the proof by applying Lemma 4 with the above bound on k  X  M p  X  M p k op . In the previous section, we showed that Spectral Ex-perts provides a consistent estimator. In this section, we explore the empirical properties of our algorithm on simulated data. Our main finding is that Spectral Experts alone attains higher parameter error than EM, but this is not the complete story. If we initialize EM with the estimates returned by Spectral Experts, then we end up with much better estimates than EM from a random initialization. 5.1. Experimental setup Algorithms We experimented with three algo-rithms. The first algorithm (Spectral) is simply the Spectral Experts. We set the regularization strengths  X  not very sensitive to these choices. We solved the low-rank regression to estimate M 2 and M 3 using an off-the-shelf convex optimizer, CVX (Grant &amp; Boyd, 2012). The second algorithm (EM) is EM where the  X   X  X  are initialized from a standard normal and  X  was set to the uniform distribution plus some small pertur-bations. We ran EM for 1000 iterations. In the final algorithm (Spectral+EM), we initialized EM with the output of Spectral Experts.
 Data We generated synthetic data as follows: First, we generated a vector t sampled uniformly over the b -dimensional unit hypercube [  X  1 , 1] b . Then, to get the actual covariates x , we applied a non-linear func-tion of t that conformed to the identifiability criteria discussed in Section 3. The true regression coefficients {  X  h } were drawn from a standard normal and  X  is set to the uniform distribution. The observation noise is drawn from a normal with variance  X  2 . Results are presented below for  X  2 = 0 . 1, but we did not observe any qualitatively different behavior for choices of  X  2 in the range [0 . 01 , 0 . 4].
 As an example, one feature map we considered in the one-dimensional setting ( b = 1) was x = (1 ,t,t 4 ,t 7 ). The data and the curves fit using Spectral Experts, EM with random initialization and EM initialized with the parameters recovered using Spectral Experts are shown in Figure 2. We note that even on well-separated data such as this, EM converged to the cor-rect basin of attraction only 13% of the time. 5.2. Results Table 1 presents the Frobenius norm of the difference between true and estimated parameters for the model, averaged over 20 different random instances for each feature set and 10 attempts for each instance. The experiments were run using n = 500 , 000 samples. One of the main reasons for the high variance is the variation across random instances; some are easy for EM to find the global minima and others more diffi-cult. In general, while Spectral Experts did not recover parameters by itself extremely well, it provided a good initialization for EM.
 To study the stability of the solutions returned by Spectral Experts, consider the histogram in Figure 1, which shows the recovery errors of the algorithms over 170 attempts on a dataset with b = 1 ,d = 4 ,k = 3. Typically, Spectral Experts returned a stable solution. When these parameters were close enough to the true parameters, we found that EM almost always con-verged to the global optima. Randomly initialized EM only finds the true parameters a little over 10% of the time and shows considerably higher variance.
 Effect of number of data points In Figure 3, we show how the recovery error varies as we get more data. Each data point shows the mean error over 10 attempts, with error bars. We note that the recov-ery performance of EM does not particularly improve; this suggests that EM continues to get stuck in a local optima. The spectral algorithm X  X  error decays slowly, and as it gets closer to zero, EM initialized at the spec-tral parameters finds the true parameters more often as well. This behavior highlights the trade-off between statistical and computational error.
 Misspecified data To evaluate how robust the al-gorithm was to model mis-specification, we removed large contiguous sections from x  X  [  X  0 . 5 ,  X  0 . 25]  X  ports recovery errors in this scenario. The error in the estimates grows larger for higher d . In this paper, we developed a computationally efficient and statistically consistent estimator for mixture of linear regressions. Our algorithm, Spectral Experts, regresses on higher-order powers of the data with a reg-ularizer that encourages low rank structure, followed by tensor factorization to recover the actual parame-ters. Empirically, we found Spectral Experts to be an excellent initializer for EM.
 Acknowledgements We would like to thank Lester Mackey for his fruitful suggestions and the anonymous reviewers for their helpful comments.
 Anandkumar, A., Foster, D. P., Hsu, D., Kakade,
S. M., and Liu, Y. Two SVDs suffice: Spectral de-compositions for probabilistic topic modeling and la-tent Dirichlet allocation. In Advances in Neural In-formation Processing Systems (NIPS) , Cambridge, MA, 2012a. MIT Press.
 Anandkumar, A., Hsu, D., and Kakade, S. M. A method of moments for mixture models and hidden
Markov models. In Conference on Learning Theory (COLT) , 2012b.
 Anandkumar, Anima, Ge, Rong, Hsu, Daniel, Kakade,
Sham M., and Telgarsky, Matus. Tensor decompo-sitions for learning latent variable models. CoRR , abs/1210.7559, 2012c.
 Balle, B. and Mohri, M. Spectral learning of general weighted automata via constrained matrix comple-tion. In Advances in Neural Information Processing Systems (NIPS) , Cambridge, MA, 2012. MIT Press. Balle, B., Quattoni, A., and Carreras, X. A spec-tral learning algorithm for finite state transducers. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases , 2011.
 Candes, E. J., Strohmer, T., and Voroninski, V.
Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming. Technical report, ArXiv, 2011.
 Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., and Ungar, L. Spectral learning of latent-variable
PCFGs. In Association for Computational Linguis-tics (ACL) . Association for Computational Linguis-tics, 2012.
 Grant, Michael and Boyd, Stephen. CVX: Matlab soft-ware for disciplined convex programming, version 2.0 beta. http://cvxr.com/cvx , September 2012. Hsu, D. and Kakade, S. M. Learning mixtures of spher-ical gaussians: Moment methods and spectral de-compositions. In Innovations in Theoretical Com-puter Science (ITCS) , 2013.
 Hsu, D., Kakade, S. M., and Zhang, T. A spectral algorithm for learning hidden Markov models. In Conference on Learning Theory (COLT) , 2009.
 Hsu, D., Kakade, S. M., and Liang, P. Identifiability and unmixing of latent parse trees. In Advances in Neural Information Processing Systems (NIPS) , Cambridge, MA, 2012. MIT Press.
 Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hin-ton, G. E. Adaptive mixtures of local experts. Neu-ral Computation , 3:79 X 87, 1991.
 Liang, P., Bouchard-C X ot  X e, A., Klein, D., and Taskar,
B. An end-to-end discriminative approach to ma-chine translation. In International Conference on Computational Linguistics and Association for
Computational Linguistics (COLING/ACL) , Syd-ney, Australia, 2006. Association for Computational Linguistics.
 Negahban, S. and Wainwright, M. J. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. ArXiv e-prints , December 2009.
 Ohlsson, H., Yang, A., Dong, R., and Sastry, S. CPRL  X  an extension of compressive sensing to the phase retrieval problem. In Advances in Neural Informa-tion Processing Systems (NIPS) , Cambridge, MA, 2012. MIT Press.
 Parikh, A., Song, L., Ishteva, M., Teodoru, G., and
Xing, E. A spectral algorithm for latent junc-tion trees. In Uncertainty in Artificial Intelligence (UAI) , 2012.
 Petrov, S. and Klein, D. Discriminative log-linear grammars with latent variables. In Advances in Neu-ral Information Processing Systems (NIPS) , Cam-bridge, MA, 2008. MIT Press.
 Quattoni, A., Collins, M., and Darrell, T. Conditional random fields for object recognition. In Advances in Neural Information Processing Systems (NIPS) , Cambridge, MA, 2004. MIT Press.
 Song, L., Boots, B., Siddiqi, S., Gordon, G., and Smola, A. Hilbert space embeddings of hidden
Markov models. In International Conference on Ma-chine Learning (ICML) , Haifa, Israel, 2010. Omni-press.
 Tomioka, R., Suzuki, T., Hayashi, K., and Kashima,
H. Statistical performance of convex tensor decom-position. Advances in Neural Information Process-ing Systems (NIPS) , pp. 137, 2011.
 Viele, Kert and Tong, Barbara. Modeling with mix-tures of linear regressions. Statistics and Com-puting , 12:315 X 330, 2002. ISSN 0960-3174. doi: 10.1023/A:1020779827503. URL http://dx.doi. org/10.1023/A%3A1020779827503 .
 Wang, Y. and Mori, G. Max-margin hidden con-ditional random fields for human action recogni-tion. In Computer Vision and Pattern Recognition
