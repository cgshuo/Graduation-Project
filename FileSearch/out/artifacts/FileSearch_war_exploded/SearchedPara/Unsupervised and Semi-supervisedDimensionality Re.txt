
Zhiyang Xiang 1,2 , Zhu Xiao 1,2 , Yourong Huang 1 , Dong Wang Dimensionality reduction or referred as embedding is an important technique for data mining and pattern recognition applications. It is often employed in high dimensional data processing to enable data visualization and or as a metric learning method. In real world applications it is always involved in two types of tasks. First, it can construct and find out the most informative dimensions, thus to enable machine learning techniques that only perform well on low dimensional techniques. Second, it can function as a metric learning process to augment dis-tance based supervised learning and semi-supervised learning (SSL). Depending on the availability of labeled information, dimensionality reduction can be cate-gorized into supervised, unsupervised and semi-supervised ones.
 There are linear and nonlinear dimensionality reduction methods. Multi-Dimensional Scaling (MDS) and Fisher Discriminant Analysis (FDA) are exam-ples of the linear methods; and Isomap [ 17 ], semi-definite programming (SDP) based embeddings [ 20 ] are examples of nonlinear methods. The major difference between them is that nonlinear methods employs more computationally costly methods such as graph similarity and SDP to construct the kernel matrix. Lin-ear methods such as classical MDS are efficient but are ineffective to learn the nonlinear embedding of data from high dimensional spaces to lower ones. On the other hand, the nonlinear methods are often less efficient because the graph similarity calculation, SDP and other nonlinear optimizations are hard to solve, especially when applying to real world large scale problems.
 Semi-supervised dimensionality reduction are often constructed by manipu-lations of the kernel matrix from their supervised or unsupervised counterparts. Such examples can be seen in [ 2 , 3 , 21 ]. The manipulation can come either as a form of pairwise constraints or approximations like graph kernels. An optimiza-tion problem would be difficult if its number of variables are of the same magnitude of the number of samples in training datasets. This has limited their usage in real world applications. Graph construction methods are often slow, too [ 6 ]. There is a recent trend to solve transfer domain learning with dimensionality reduction, which has an impact on SSL research because SSL is a special form of transfer domain learning [ 11 , 12 ]. Semi-supervised dimensionality reduction is equivalent to an embedding task on two domains, one of which is the data feature domain and the other is the augmented domain with labels as extra features. Transfer domain embedding methods such as [ 10 , 19 ] relies on SDP to find the kernel matrix. Unfortunately, SDPs are often hard to solve. An SDP with several thousand variables may be difficult for a mainstream personal computer to solve, but real world applications will easily reach this magnitude.
 Clustering is frequently employed to reduce the problem size. Competitive learning neural networks such as Self-Organizing Maps [ 8 ], Growing Neural Gas (GNG) [ 5 ] and Self-Organizing Incremental Neural Network (SOINN) [ 15 ]are widely used as data reduction techniques, because of their incremental and topol-ogy learning abilities. However, their applicability as feature reduction techniques are not fully explored. Moreover, the SSL designs of SOINN are not as thorough as GNG. In [ 1 , 9 ] the consensus and heuristical online label inferring extensions to GNG are studied. However, the two extensions assumed that the labeled informa-tion is so little that it can not affect the data distribution learned by GNG. In this paper we propose a semi-supervised dimensionality reduction method based on semi-supervised SOINN and linear dimensionality reduction. Our main contributions are as follows.
 1. A nonlinear dimensionality reduction framework with graph similarity from 2. Semi-supervised extensions to SOINN in the form of positive and negative 3. With the semi-supervised SOINN, the kernel constructed for dimensionality and preliminaries used in later sections. Section 3 introduces the framework of our work and the semi-supervised extension to SOINN. Then in Sect. 4 are the experiments and in Sect. 5 are the conclusions. In this section, theories and algorithms our work is based upon are introduced. More specifically, the linear dimensionality reduction and the single layered SOINN are described. 2.1 Linear Dimensionality Reduction Suppose P = { p ij } is the dissimilarity matrix of a training dataset. According to [ 7 , 18 ], the linear dimensionality reduction can be described as follows. First, calculate a matrix H as [ 7 , 18 ] Where l is the desired dimension count after dimensionality reduction. Then calculate eigendecomposition as P = UDU T , where D is a diagonal matrix with eigenvalues of H along the diagonal and only the non-zero l eigenvalues are selected. If all the eigenvalues are non-negative, then the dimensionality reduction is achieved by X = D 0 . 5 U T , where the columns of X are the data items after dimensionality reduction. If there are negative eigenvalues, then X = ( MD ) 0 . 5 U T , where the n  X  n matrix M = diag ( I n + where the pair ( n + ,n  X  ) is called the signature of the pseudo-Euclidean space [ 7 , 18 ]. The distance metric is altered as [ 18 ] steps. First, from the distances of the new data item to existing data items P = { p i } calculate a vector H = { h i } [ 18 ] Then the dimensionality reduction is accomplished as where H =[ h 1 ,h 2 ,  X  X  X  ,h i ,  X  X  X  ] T . 2.2 The Single Layered SOINN The single layered SOINN [ 15 ] can be divided into three phases. First, on an arrival of a data item, it decides whether to insert a new vector into the set W . Second, update the weight vectors. Third, discover the topology structure by label propagation.
 In step 1, each weight vector is said to control a spherical area in the data space. The sphere is centered at the position of the vector itself, and the diameter is the vector to its farthest topological neighbor by metric of Euclidean distance or the smallest distance to its neighbors if it has no topological neighbor. When a new sample V arrives and it is not controlled by its nearest neighbor W second nearest neighbor W t in W , then this vector is added to W . In step 2, if there is no edge between W s and W t , add one. Then if in step 1 the new sample is not added to W , update W s and its topological neighbors towards the new sample as where c s is the winning times of W s .  X  is a constant and in [ 15 ] it is set to 0.01. Step 3 is only invoked one time every  X  times of sample arrival. It assumes the convexity of data and try to divide graph learned into trivial graphs in the original SOINN algorithms. Since the purpose of our method is not for clustering under convexity assumption, this step is not inherited in our work. The step 3 is not introduced and not used in the semi-supervised extension which is to be introduced in later sections. In this section, the details of the proposed method is given. First, the problem formation and algorithm framework are elaborated. Second, the semi-supervised extension to SOINN is explained. The key idea is a knowledge reuse framework, where the learning result from SOINN is employed in defining the similarity of samples in the same space and under the same distribution as the training dataset (including the labeled and unlabeled). In the SOINN learned data rep-resentation, weight vectors that are similar to each other is more likely to be linked by more paths in the graph [ 15 ].
 3.1 Problem Formation and Algorithm Framework We approach dimensionality reduction by three steps. First, train semi-supervised SOINN on the input data. Second, calculate the dimensionality reduc-tion of SOINN weight vectors. Third, calculate the dimensionality reduction of newly arrived data by their similarity to the SOINN weight vectors. the kernel matrix. We use the SOINN trained graph as a graph kernel for fur-ther construction of a kernel matrix. In SOINN the more similar the two weight vectors are, the higher the probability that the two vectors are linked by more paths on the graph. Similarity between graph vertices can be calculated by the Edmonds-Karp algorithm [ 4 ]. However, graph similarity defined by graph cuts are often used for classification purposes. Since the variation of the similarity values can be highly skewed and possibly can not be embedded to an Euclidean space. If the graph similarities defined by graph cut are directly used in dimen-sionality reduction, the eigendecomposition of graph similarity matrix may gen-erate many negative eigenvalues. As a result, in our work we try to preserve the distance matrix P calculated from the original data, and use the graph similar-ity G = { g ij } calculated by Edmonds-Karp algorithm on the SOINN graph as constraints. The graph similarity G can be calculated from edge weights of where g ( x ) is a decreasing and positive valued function. This means that when connected by a single edge, the closer the two vector is, the higher the graph similarity. The specific choice of g ( x ) is irrelevant as long as it is decreasing. Put the elements of G in the upper triangle (and excluding the diagonal) in a vector S = { s k } (standing for similarity) and sort in decreasing order; and construct two vectors D = { d k } (standing for distance), K = element s k = g ij , d k = p ij ,k k = p ij . Since the diagonal of P , P must be zero, the optimization described by Eq. ( 8 ) is equivalent to which is a quadratic programming problem This can be solved by a standard solver. This approach is similar to Structure Preserving Embedding (SPM) [ 14 ]whichisbasedonSDP.However,theproposed approach tries to preserve the dissimilarity in the Minkovsky space instead of the Euclidean space.
 After acquiring the kernel matrix P through K . The dimensionality reduc-tion of weight vectors W from SOINN can be accomplished by linear dimen-sionality reduction. However, to calculate the dimensionality reduction of data items other than W with Eq. ( 3 ), the distance from this new data item V to each weight vector in W must be acquired. The distances can not simply be Euclidean distances || V  X  W i || , since the distances between weight vectors in W is changed in the new data space. We design a distance that emphasizes the local similarity as where k ( x ) is a kernel function, and it should be increasing and positive valued. This design is to ensure two properties. First, if a data item is close to W its distance to other weight vectors will be similar to those in the kernel matrix calculated from Eq. ( 8 ). Second, the dimensionality reduction will try to preserve the smoothness of data distribution in the original space. Finally, the distance vector is constructed as where P is the kernel matrix calculated by quadratic programming and T = [ t ,t ,  X  X  X  ,t i ,  X  X  X  ] T . The dimensionality reduction of a data item V can then be calculated by Eqs. ( 3 ) and ( 4 ).
 Besides, it is beneficial to add another add-hoc element to handle the trivial groups which are separated from the main cluster of weight vectors. One common way is to connect each trivial graph to its nearest trivial graph. This is to preserve the order of similarities between the trivial graphs. This strategy can be altered to fit real world problems. 3.2 Semi-supervised Extension to the Single Layered SOINN Labeled information can augment the learning result of competitive learning neural network. In this paper we propose a positive negative competitive learning strategy that is different from previous extensions in two ways. 1. Robust to noise in labeled information. 2. Avoid label conflicts of weight vectors when learning with large labeled datasets.
 The basic idea of positive and negative competitive learning is that when the label of the winner weight vector is contradicting with the current sample, the winner weight vector and its topological neighbors are moved away from the current sample. The implementation details are as follows.
 Step 1 is the same as the unsupervised learning step while the major difference is in step 2. In step 2, the compatibility of labels between the new input V and the winner W s must be checked first. If the label of V is the same as W V is unlabeled, step 2 will proceed the same as in the unsupervised learning, which is the positive competitive learning. If they are conflicting, W pushed away from V . But the scale of the push can not simply be because the input V might be a noise so far way from W s that W pushed away too much. To solve this problem we assume that the i nput V is highly likely to be consistent to the second winner W t . Then with the help of W t ,wehave where  X  is a user defined parameter. This updating strategy is under the assump-tion that the closer V is to W s , the less credential it is. We call this the negative competitive learning. Finally, the complete algorithm of semi-supervised dimen-sionality reduction with SOINN is illustrated in Algorithm 1 . First, experiments are carried out on artificial datasets to demonstrate the unsu-pervised and semi-supervised learning abilities of the proposed method. Then the evaluations are moved to the NSL-KDD [ 16 ] dataset. The reason for choos-ing this dataset is that the test dataset is from a different distribution of the training dataset, and handling such concept drift is one of the primary goals of SSL algorithm designs. 4.1 Artificial Datasets The first dataset (illustrated in Fig. 1 ) is unlabeled, on which the dimensionality reduction abilities of the proposed method and PCA, Isomap, LLE [ 13 ]are compared. The second dataset is partially labeled as illustrated in Fig. 2 .The second dataset is to demonstrate the SSL property of the proposed method. Experiment results of the unsupervised dimensionality reduction is in Fig. 3 ; and results on the second dataset is in Fig. 4 .
 able to perform dimensionality reduction as PCA and Isomap, and the results are smooth as expected. The proposed method separates the two rings better than PCA and the topology of data is better preserved than Isomap and LLE. the proposed method, the data items with different labels are redistributed such that they are more easily to be separated linearly. Moreover, the in the proposed method the differently labeled data are separated by a low density area. Algorithm 1. Semi-Supervised SOINN for Dimensionality Reduction (SOINN-DR) 4.2 The Intrusion Detection Dataset In this experiment the 34 numerical values of NSL-KDD intrusion detection dataset are used. The SSL learning ability of the proposed method is evaluated qualitatively and compared with other SSL methods such as label propagation, TSVM. To compare with the unsupervised and supervised learning methods, support vector machine (SVM) and Isomap are included as well. The 1 % of the training dataset is drawn to form the labeled dataset; and the complete test dataset is selected as the unlabeled dataset. The training of SSL methods are on the mixture of the labeled and unlabeled datasets and the evaluation results are on the unlabeled datasets. In this experiment we link all the trivial graphs generated by semi-supervised SOINN to a weight vector with a confirmed intrusion label, because samples far from the main body of data are more likely to be intrusions. The parameters are selected by grid search combining with cross validation. The selected parameters of the proposed method are max age = 500,  X  = 350, d = 15,  X  =0 . 6. The experiment results are listed in Table 1 . SSL methods to outperform the supervised and unsupervised methods because of the concept drift. Besides, the proposed method is able to finish much faster than TSVM (which takes hours per training). This is because the transductive kernel construction is time consuming and the dataset is large in size (with over 20000 samples). In summery, the proposed method is an accurate and efficient transductive learner alternative for TSVM.
 information. Then, weight vectors from SOINN are embedded to a lower dimen-sional space by a quadratic programming optimization, where the distances between weight vectors are preserved as much as possible and the graph similar-ities defined by the graph kernel from SOINN are satisfied as constraints. Then a similarity approximation is designed to process the remaining samples. The experimental results show that the proposed method is capable of unsupervised and semi-supervised dimensionality reduction, and it is an accurate and efficient transductive learner alternative for TSVM.
 future works. First, the semi-supervised extension to SOINN can be employed in other competitive learning neural networks such as GNG, Growing Cell Struc-tures etc., with some modifications. Second, the proposed method still require add-hoc designs when dealing with real world applications. It is worth investigat-ing a substitute for semi-supervised SOINN. Third, the algorithm frameworks generalization to transfer learning is possible since SSL is a special case of trans-fer domain learning.

