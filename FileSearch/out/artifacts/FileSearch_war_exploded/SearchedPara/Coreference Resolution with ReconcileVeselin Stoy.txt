 Noun phrase coreference resolution (or simply coreference resolution) is the problem of identi-fying all noun phrases (NPs) that refer to the same entity in a text. The problem of coreference res-olution is fundamental in the field of natural lan-guage processing (NLP) because of its usefulness for other NLP tasks, as well as the theoretical in-terest in understanding the computational mech-anisms involved in government, binding and lin-guistic reference.

Several formal evaluations have been conducted for the coreference resolution task (e.g., MUC-6 (1995), ACE NIST (2004)), and the data sets cre-ated for these evaluations have become standard benchmarks in the field (e.g., MUC and ACE data sets). However, it is still frustratingly difficult to compare results across different coreference res-olution systems. Reported coreference resolu-tion scores vary wildly across data sets, evaluation metrics, and system configurations.

We believe that one root cause of these dispar-ities is the high cost of implementing an end-to-end coreference resolution system. Coreference resolution is a complex problem, and successful systems must tackle a variety of non-trivial sub-problems that are central to the coreference task  X  e.g., mention/markable detection, anaphor identi-fication  X  and that require substantial implemen-tation efforts. As a result, many researchers ex-ploit gold-standard annotations, when available, as a substitute for component technologies to solve these subproblems. For example, many published research results use gold standard annotations to identify NPs (substituting for mention/markable detection), to distinguish anaphoric NPs from non-anaphoric NPs (substituting for anaphoricity de-termination), to identify named entities (substitut-ing for named entity recognition), and to identify the semantic types of NPs (substituting for seman-tic class identification). Unfortunately, the use of gold standard annotations for key/critical compo-nent technologies leads to an unrealistic evalua-tion setting, and makes it impossible to directly compare results against coreference resolvers that solve all of these subproblems from scratch.
Comparison of coreference resolvers is further hindered by the use of several competing (and non-trivial) evaluation measures, and data sets that have substantially different task definitions and annotation formats. Additionally, coreference res-olution is a pervasive problem in NLP and many NLP applications could benefit from an effective coreference resolver that can be easily configured and customized.

To address these issues, we have created a plat-form for coreference resolution, called Reconcile, that can serve as a software infrastructure to sup-port the creation of, experimentation with, and evaluation of coreference resolvers. Reconcile was designed with the following seven desiderata in mind:  X  implement the basic underlying software ar- X  support experimentation on most of the stan- X  implement most popular coreference resolu- X  exhibit state-of-the-art coreference resolution  X  can be easily extended with new methods and  X  is relatively fast and easy to configure and  X  has a set of pre-built resolvers that can be
While several other coreference resolution sys-tems are publicly available (e.g., Poesio and Kabadjov (2004), Qiu et al. (2004) and Versley et al. (2008)), none meets all seven of these desider-ata (see Related Work). Reconcile is a modular software platform that abstracts the basic archi-tecture of most contemporary supervised learning-based coreference resolution systems (e.g., Soon et al. (2001), Ng and Cardie (2002), Bengtson and Roth (2008)) and achieves performance compara-ble to the state-of-the-art on several benchmark data sets. Additionally, Reconcile can be eas-ily reconfigured to use different algorithms, fea-tures, preprocessing elements, evaluation settings and metrics.

In the rest of this paper, we review related work (Section 2), describe Reconcile X  X  organization and components (Section 3) and show experimental re-sults for Reconcile on six data sets and two evalu-ation metrics (Section 4).
Several coreference resolution systems are cur-rently publicly available. JavaRap (Qiu et al., 2004) is an implementation of the Lappin and Leass X  (1994) Resolution of Anaphora Procedure (RAP). JavaRap resolves only pronouns and, thus, it is not directly comparable to Reconcile. GuiTaR (Poesio and Kabadjov, 2004) and BART (Versley et al., 2008) (which can be considered a succes-sor of GuiTaR) are both modular systems that tar-get the full coreference resolution task. As such, both systems come close to meeting the majority of the desiderata set forth in Section 1. BART, in particular, can be considered an alternative to Reconcile, although we believe that Reconcile X  X  approach is more flexible than BART X  X . In addi-tion, the architecture and system components of Reconcile (including a comprehensive set of fea-tures that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art.

Coreference resolution has received much re-search attention, resulting in an array of ap-proaches, algorithms and features. Reconcile is modeled after typical supervised learning ap-proaches to coreference resolution (e.g. the archi-tecture introduced by Soon et al. (2001)) because of the popularity and relatively good performance of these systems.

However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e.g. Haghighi and Klein (2007)), structured approaches (e.g. McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e.g. Yang et al. (2003)) and a bell-tree search approach (Luo et al. (2004)). Most of these approaches rely on some notion of pairwise feature-based similar-ity and can be directly implemented in Reconcile. Reconcile was designed to be a research testbed capable of implementing most current approaches to coreference resolution. Reconcile is written in Java, to be portable across platforms, and was de-signed to be easily reconfigurable with respect to subcomponents, feature sets, parameter settings, etc.

Reconcile X  X  architecture is illustrated in Figure 1. For simplicity, Figure 1 shows Reconcile X  X  op-eration during the classification phase (i.e., assum-ing that a trained classifier is present).

The basic architecture of the system includes five major steps. Starting with a corpus of docu-ments together with a manually annotated corefer-ence resolution answer key 1 , Reconcile performs the following steps, in order: 1. Preprocessing. All documents are passed 2. Feature generation. Using annotations pro-3. Classification. Reconcile learns a classifier Table 1: Preprocessing components available in Reconcile. 4. Clustering. A clustering algorithm consoli-5. Scoring. Finally, during testing Reconcile
Each of the five steps above can invoke differ-ent components. Reconcile X  X  modularity makes it Table 2: Available implementations for different modules available in Reconcile. easy for new components to be implemented and existing ones to be removed or replaced. Recon-cile X  X  standard distribution comes with a compre-hensive set of implemented components  X  those available for steps 2 X 5 are shown in Table 2. Rec-oncile contains over 38,000 lines of original Java code. Only about 15% of the code is concerned with running existing components in the prepro-cessing step, while the rest deals with NP extrac-tion, implementations of features, clustering algo-rithms and scorers. More details about Recon-cile X  X  architecture and available components and features can be found in Stoyanov et al. (2010). 4.1 Data Sets Reconcile incorporates the six most commonly used coreference resolution data sets, two from the MUC conferences (MUC-6, 1995; MUC-7, 1997) and four from the ACE Program (NIST, 2004). For ACE, we incorporate only the newswire por-tion. When available, Reconcile employs the stan-dard test/train split. Otherwise, we randomly split the data into a training and test set following a 70/30 ratio. Performance is evaluated according to the B 3 and MUC scoring metrics. 4.2 The Reconcile 2010 Configuration Reconcile can be easily configured with differ-ent algorithms for markable detection, anaphoric-ity determination, feature extraction, etc., and run against several scoring metrics. For the purpose of this sample evaluation, we create only one partic-ular instantiation of Reconcile, which we will call Reconcile 2010 to differentiate it from the general platform. Reconcile 2010 is configured using the following components: 4.3 Experimental Results The first two rows of Table 3 show the perfor-mance of Reconcile 2010 . For all data sets, B 3 scores are higher than MUC scores. The MUC score is highest for the MUC6 data set, while B 3 scores are higher for the ACE data sets as com-pared to the MUC data sets.

Due to the difficulties outlined in Section 1, results for Reconcile presented here are directly comparable only to a limited number of scores reported in the literature. The bottom three rows of Table 3 list these comparable scores, which show that Reconcile 2010 exhibits state-of-the-art performance for supervised learning-based coreference resolvers. A more detailed study of Reconcile-based coreference resolution systems in different evaluation scenarios can be found in Stoyanov et al. (2009). Reconcile is a general architecture for coreference resolution that can be used to easily create various coreference resolvers. Reconcile provides broad support for experimentation in coreference reso-lution, including implementation of the basic ar-chitecture of contemporary state-of-the-art coref-erence systems and a variety of individual mod-ules employed in these systems. Additionally, Reconcile handles all of the formatting and scor-ing peculiarities of the most widely used coref-erence resolution data sets (those created as part of the MUC and ACE conferences) and, thus, allows for easy implementation and evaluation across these data sets. We hope that Reconcile will support experimental research in coreference resolution and provide a state-of-the-art corefer-ence resolver for both researchers and application developers. We believe that in this way Recon-cile will facilitate meaningful and consistent com-parisons of coreference resolution systems. The full Reconcile release is available for download at This research was supported in part by the Na-tional Science Foundation under Grant # 0937060 to the Computing Research Association for the CIFellows Project, Lawrence Livermore National Laboratory subcontract B573245, Department of Homeland Security Grant N0014-07-1-0152, and Air Force Contract FA8750-09-C-0172 under the DARPA Machine Reading Program.

The authors would like to thank the anonymous reviewers for their useful comments.

