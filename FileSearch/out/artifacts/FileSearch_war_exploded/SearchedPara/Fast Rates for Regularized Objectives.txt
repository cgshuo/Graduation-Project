 We consider the problem of (approximately) minimizing a stochastic objective on problems where f ( w ;  X  ) has a generalized linear form: loss of predicting  X  w , X  ( x )  X  when the true target is y , and r ( w ) is a regularizer. It is well known that when the domain W and the mapping  X  (  X  ) are bounded, and the function ` ( z ;  X  ) is Lipschitz continuous in z , the empirical averages minimizer and we can then establish convergence of F (  X  w ) to the population optimum with a rate of p 1 /n .
 r ( w ) =  X  2 k w k 2 as in SVMs and other regularized learning settings. In this paper we present an analogous  X  X ast rate X  for empirical minimization of a strongly convex can be used to analyze the performance of approximate minimizers obtained through approximate a &gt; 0 and  X  &gt; 0 , with probability at least 1  X   X  , for all w (of arbitrary magnitude): We emphasize that here and throughout the paper the big-O notation hides only fixed numeric con-stants.
 weakly convex).
 convergence with a rate of 1 /n for the SVM objective. This 1 /n rate on the SVM objective is the 1 / establish is that although the loss might converge at a rate of 1 / loss) always converges at a rate of 1 /n .
 In fact, in Section 3 we see how a rate of 1 /n on the objective corresponds to a rate of 1 / linear learning), based on the existence of some (unknown) low-norm, low-error predictor w . k w k  X  , as the standard ` 2 -norm. We consider a generalized linear function f : W  X   X   X  R , that can be written as in (2), defined over a closed convex subset W of a Banach space equipped with norm k X k . Lipschitz continuity and boundedness We require that the mapping  X  (  X  ) is bounded by B , i.e. k  X  (  X  ) k  X   X  B , and that the function ` ( z ;  X  ) is L -Lipschitz in z  X  R for every  X  . w 1 , w 2  X  W and  X   X  [0 , 1] we have: Recalling that w ? = arg min w F ( w ) , this ensures (see for example [2, Lemma 13]): convex in z and that r ( w ) is  X  -strongly convex (w.r.t. the norm k w k ). We now provide a faster convergence rate using the above conditions. Theorem 1. Let W be a closed convex subset of a Banach space with norm k X k and dual norm k X k  X  of size n , we have that for all w  X  W : (where [ x ] + = max( x, 0) ) following bound: magnitude): linear learning. That is, training by minimizing the empirical average of: infinite-data optimum of the objective.
 rate roughly 1 /n , the expected loss L (  X  w ) converges at a slower rate of roughly p 1 /n . Studying the convergence rate of the SVM objective allows us to better understand and appreci-inequalities on the generalization loss of  X  w , as we do in the following Section. Before moving on, we briefly provide an example of applying Theorem 1 with respect to the ` 1 -` long as k w k 1  X  B w (see [2]), yielding: over a sample of size n , we have that for all w  X  W :  X  F ( w ) =  X  E [ f  X  ( w )] where sup x k x k (all norms in this Section are ` 2 norms).
 We assume, as an oracle assumption, that there exists a good predictor w o with low norm k w o k with probability at least 1  X   X  : Optimizing to within opt = O ( B 2  X n ) is then enough to ensure sition: where we used the bound (11) to bound the second term, the optimality of w ? to ensure the third term is non-positive, and we also dropped the last, non-positive, term. This might seem like a rate of 1 /n on the generalization error, but we need to choose  X  so as to balance the second and third terms. The optimal choice for  X  is tuting (13) into (12): where  X  ( n ) chosen as in (13) , the following holds: Corollary 4 is demonstrated empirically on the right plot of Figure 1. The way we set  X  ( n ) in Corollary 4 depends on k w o k . However, using we obtain: min  X  F  X  ( n ) ( w ) + O ( B 2  X n ) , the following holds: expected loss of any fixed predictor.
 It is interesting to repeat the analysis of this Section using the more standard result: the norm of both the empirical and population optimums, and using (15) instead of Corollary 2 in our analysis yields the oracle inequality: predictor w o , and we make no assumptions about the kernel or the noise. We note that a more 1 /  X  To prove Theorem 1 we use techniques of reweighing and peeling following Bartlett et al [5]. g w in terms of its empirical average. We denote by G = { g w | w  X  W } . class. For any r &gt; 0 define g w  X  X  and the scaling factor ensures that E [ g r w ]  X  r .
 dependent) regularization terms r ( w ) . Define: where  X  &gt; 0 , with probability at least 1  X   X  , We will now proceed to bounding the two terms on the right hand side: Lemma 6. sup bound k  X  (  X  ) k  X   X  B , we have for all w , X  : and k ( w ) , and finally note that 4 k ( w )  X  1 , to get: Substituting (22) in (21) yields the desired bound.
 Lemma 7. R ( H r )  X  2 LB q 2 r  X n Proof. We will use the following generic bound on the Rademacher complexity of linear function-R ( H ( a ))  X  LB q 2 a
R ( H r ) = R  X   X  j =0 4  X  j H ( r 4 j )  X  with probability at least 1  X   X  we have: where D = LB q 1  X n (4 6 and 7 into (20). We now consider two possible cases: k ( w ) = 0 and k ( w ) &gt; 0 . i.e. F ( w )  X  F ( w ? ) + r . In this case (24) becomes: have E [ g w ]  X   X  E [ g w ]  X  4 r E [ g w ] have: Setting r = (1 + 1 a ) 2 (4 D ) 2 yields the bound in Theorem 1. Rates faster than 1 / dimensionality assumption, which do not hold in SVM-like settings. Bousquet [9] provided similar ing. Tsybakov [10] introduced a margin condition under which rates faster than 1 / here we make no such assumption.
 get bounds that have a localized complexity term and additional terms of order faster than 1 / resulting rate is n  X  ( V +2) / (2 V +2) , which indeed is better than 1 / necessary in order to obtain  X  X ast X  rates.
 convex, since it is flat in directions orthogonal to x .
 low-dimensional spaces were, as discussed above, other results also apply. An interesting observation about our proof technique is that the only concentration inequality we invoked was McDiarmid X  X  Inequality (in [6, Theorem 5] to obtain (20) X  X  bound on the deviations in terms of the Rademacher complexity). This was possible because we could make a localization argument for the `  X  norm of the functions in our function class in terms of their expectation. other regularized objectives.
 regularization parameters.

