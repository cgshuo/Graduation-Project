 Traditional models of information retrieval assume docu-ments are independently relevant. But when the goal is retrieving diverse or novel information about a topic, re-trieval models need to capture dependencies between doc-uments. Such tasks require alternative evaluation and op-timization methods that operate on different types of rel-evance judgments. We define faceted topic retrieval as a particular novelty-driven task with the goal of finding a set of documents that cover the different facets of an informa-tion need. A faceted topic retrieval system must be able to cover as many facets as possible with the smallest number of documents. We introduce two novel models for faceted topic retrieval, one based on pruning a set of retrieved documents and one based on retrieving sets of documents through di-rect optimization of evaluation measures. We compare the performance of our models to MMR and the probabilistic model due to Zhai et al. on a set of 60 topics annotated with facets, showing that our models are competitive. Categories and Subject Descriptors : H.3.3 [ Information Storage and Retrieval ] General Terms: Algorithms, Experimentation Keywords: information retrieval, novelty, diversity, prob-abilistic models
The concept of relevance is probably the most impor-tant and most vociferously debated in the field of informa-tion retrieval. Many researchers have settled on a so-called  X  X ystem-based X  notion of relevance that is amenable to fast research and development cycles. In this conception, doc-uments may be relevant on binary, graded, or continuous scales, but all documents are judged relevant independently of one another. Two identical documents are both relevant as long as they contain information the user needs. Most evaluation measures, including precision, recall, average pre-cision, and discounted cumulative gain, assume such a defini-tion; the Probability Ranking Principle for optimizing doc-ument rankings is also based on independent relevance.
Modeling documents as independently relevant does not necessarily provide the optimal user experience. Certainly, five relevant documents that all contain the same single piece of information are not as useful to a user as one relevant doc-ument that contains five separate pieces of information X  X et traditional evaluation measures would reward a system that provides the former more than one that provides the latter. Novelty and diversity tasks attempt to remedy this with new definitions of relevance and new evaluation measures.
We view such tasks as falling on a continuum: at one end, there is diversity through retrieving different results for inde-pendent interpretations of a query, in the way that Michael Jordan the basketball player and Michael Jordan the statis-tician largely occur in documents independently of one an-other. At the other, diversity is achieved through retrieving documents that are all relevant to the same interpretation, but cover different facets of the topic. For the Michael Jor-dan example, this might entail assuming that the basketball player is the correct interpretation, then ranking documents that cover different aspects of his life and career (e.g. his time with the Chicago Bulls, his time with the Washington Wizards, his gambling problems, etc.) with diversity. In be-tween these two endpoints there is a wide variety of tasks, and the precise types of optimization and evaluation needed may vary significantly from task to task.

In this work we attack a task closer to the latter end of the continuum: faceted topic retrieval . Our definition of faceted topic retrieval assumes a single  X  X orrect X  interpretation of a query; within that interpretation there are multiple facets, all of which must be represented in the retrieved documents. These facets are highly correlated, often appearing together in groups in the same documents. The faceted topic re-trieval system must be able to find a small set of documents that covers all of the facets: three documents that cover 10 facets will always be preferable to five documents that cover the same 10. Because of the high correlations among facets, some redundancy in the retrieved results is unavoid-able. Evaluation and optimization must take care to not penalize redundancy too much.

We propose a novel set-based probabilistic model for facet-ed topic retrieval. Our model makes no explicit attempt to control redundancy, yet it performs as well as greedy rank-ing methods that attempt to minimize redundancy such as MMR and the probabilistic method of Zhai et al. Further-more, upper bound experiments suggest that our model has greater flexibility and room for improvement.
The need for diversity in the result sets was addressed by Goffman in 1964 [8]. He stressed that the relevance of a document is dependent on the previous documents retrieved. Several researchers have been working on ways to eliminate the redundancy in the result set and have proposed models for diverse document ranking.

In their work on subtopic retrieval, Zhai et al. claim that there is more than one meaningful interpretation for a given query [16]. They assume that these interpretations indi-cate the various subtopics for the query. They re-order the results such that some results from each subtopic are ac-commodated in the top results with some probability. Their methodology involves handling the novelty and redundancy in a result set separately, then combining them in a cost function. This paper also introduces evaluation measures for subtopic retrieval that we will use for our task as well. The work of Zhai et al. is based on the Maximum Marginal Relevance (MMR) ranking function of Carbonell and Gold-stein [4]. The MMR approach aims to reduce the redun-dancy and achieve diversity in the result set by ranking documents that are relevant to the query but dissimilar to documents ranked above them. Similar work by Chen and Karger aims directly to provide the user with the answer to their interpretation for the query [5]. Their greedy algo-rithm incorporates negative feedback in order to maximize diversity in the result set by penalizing redundancy.
Clarke et al. note that the evaluation measure acts as an objective function, and claim that it should reflect user re-quirements [6]. They introduce an evaluation measure based on normalized discounted cumulative gain (nDCG [10]) that rewards novelty and diversity and penalizes redundancy. Since it discounts by rank, it seems to demand a greedy strategy for optimization. We argue that greedy strategies are not necessarily optimal for maximizing diversity.

The objective of our paper is similar to those of the above mentioned work: to provide a diverse ranking mechanism and remove redundancy. But while the previous work con-centrated on maximizing diversity and penalizing redun-dancy in a single optimization step (usually by means of a greedy algorithm), ours will maximize diversity among a set of documents without regard for redundancy. We wish to retrieve a smallest set of documents that cover a given set of facets, and this goal can conflict with the goal of min-imizing the redundancy among a set of documents: consider cases in which a particular facet is rare and only occurs in documents along with several much more common facets.
Let us define the faceted topic retrieval task in terms of the type of information need the user has and how that need is best satisfied. A faceted topic retrieval information need is one that has a set of answers X  X acets X  X hat are clearly delineated. Each of those answers may appear in multiple documents, but each answer is fully contained within at least one document (i.e. a user would not have to read two or more documents to understand how some piece of one of them is related to his or her need).
 An example faceted topic retrieval information need is: The facets of this need include invest in next generation technologies, increase use of renewable energy sources, invest in renewable energy sources, double ethanol in gas supply, shift to biodiesel, shift to coal , and more. Note that facets are not limited to any particular part of speech or type of entity; they can be phrases, named entities, places, objects, or a mix of types.
 All of the relevant documents must be on the same topic. While there may be room for different interpretations of a short query, the task definition is that the interpretation in the statement of the information need is the  X  X orrect X  one. A document is relevant to the need if it contains any of the facets (and support for that facet being relevant).
Each document can contain one or more facets, and each facet can be contained in one or more documents. Figure 1 shows how documents and facets can be related in a bipartite graph. Only relevant documents are shown here; quite a few documents have been judged nonrelevant to this need and thus do not contain any facet.
Despite the similar name, our task is quite different from  X  X aceted search X . In faceted search, items are classified into one or more groups called facets, and the user may narrow or expand her search using those facets [7]. In faceted topic retrieval, the task is to retrieve the individual facets of a particular query. The difference may be best expressed by noting that facets in faceted search are defined globally and independent of any query, while the facets in faceted topic retrieval are defined entirely by the information need.
Faceted topic retrieval is also quite different from recent diversity tasks such as those studied by Clarke et al. [6], Agrawal et al. [1], and Radlinski et al. [12]. In those works, diversity is a matter of satisfying varying user needs in a single ranked list for a query. The assumption is that a user is interested in a subset of the relevant material, and different users may be interested in different subsets. In faceted topic retrieval we assume all users are interested in all of the facets, like the standard ad hoc assumption that all users are interested in all of the relevant material. Our task is more similar to the  X  X ist question X  task of the TREC Question Answering track, in which a system must find answers to natural language questions such as  X  X ist 8 oil producing states in the United States X  [15]. For exam-ple, one of our queries is oil producing countries ; the infor-mation need is to find countries that produce and export oil, and also to distinguish OPEC nations from non-OPEC nations. The difference is that instead of extracting the facets and presenting them to the user, we present the user with a ranked list of documents as in traditional retrieval. Clearly, then, the documents in that ranked list should con-tain as many unique facets as possible. Additionally, instead of a natural language question, the query is a short list of keywords, as in the standard ad hoc retrieval task. The query for the example above, for instance, is reducing depen-dency on oil ; it does not explicitly ask for a list of proposed strategies. Finally, relevant facets are not necessarily all of the same entity type; people and organizations, people and places, noun phrases and dates, etc. may be mixed together in the facets.

Faceted topic retrieval is most similar to the subtopic re-trieval task studied by Zhai et al. [16]. In our view, the difference is primarily of degree: we believe facets will occur together in documents slightly more often than subtopics will. The problems are nevertheless so similar that a human may have difficulty distinguishing between them, and there-fore the evaluation methods and models Zhai et al. propose are natural for our task. Zhai et al. evaluated subtopic retrieval with measures called S-recall and S-precision . We use the same measures for our faceted topic retrieval task.
The primary evaluation question for a faceted topic re-trieval engine is how many of the facets that are attested in the corpus were retrieved. Given a set of facets and docu-ments judged according to whether they are relevant to the information need and contain each facet, S-recall at rank k may be defined as: where m is the number of known facets, D i is the docu-ment retrieved at rank i , and I (  X  ) is the so-called indicator function, which in this case is 1 if F i occurs in any of the documents ranked 1  X  k and 0 otherwise. This is equivalent to the definition given by Zhai et al. [16], but reformulated in terms of sets of documents.

The maximum value of S-recall at a particular rank k de-pends on the maximum number of facets that can be found in k documents. For the example in Figure 1, S-rec @1 can be at most 5/14 and S-rec @2 can be at most 8/14; at least 6 documents are required to achieve S-rec = 1. In this ex-ample, 6 is the minimum rank at which perfect recall can be achieved, and we will denote S-recall at that rank simply S-rec . We argue that the best way to satisfy a faceted topic retrieval need is to retrieve the smallest set of documents that contains all of the facets, and thus that S-rec is the most natural measure to evaluate a faceted topic retrieval system.
 Finding the minimum rank is an instance of the Minimum Set Cover problem and is therefore NP-Hard. To see this, consider the universe F of facets for a query Q . Define a doc-ument D i as a subset of F , i.e. a document contains a subset of facets. The minimum rank is equivalent to the size of the smallest subset of documents such that their union contains all elements of F  X  X xactly the Minimum Set Cover prob-lem. However, the way facets are empirically distributed in documents allows for some heuristics. First, any document that contains a subset of the facets contained in another document can be eliminated from consideration. Second, if any facets always occur in documents separately from other facets, one of those documents must be part of the set. We can then find an upper bound on the minimum rank by tak-ing documents in a greedy fashion according to the number of unsatisfied facets they satisfy. The size of the resulting set of documents is the minimum rank. Comparing this algo-rithm to exhaustive search suggests that it produces a very tight approximation, with an error of less than 0 . 5 on any individual set of facets.

One might think that focusing on the smallest set of docu-ments would result in very long documents being preferred, perhaps because those are more likely to contain more facets. But because facets are so closely related to each other (by the definition of the task), we do not believe that this will hap-pen; in fact, it is likely that more facets would be retrieved in short, very focused documents than in longer ones.
S-recall measures the ability of the system to find facets, but we would also like to know whether it ranks them well. Zhai et al. define S-precision at rank k as the minimum rank required by a perfect system to achieve recall of at least rec @ k , divided by k : k 0 /k , where k 0 is the minimum rank at which S-rec @ k could possibly be achieved. This can be understood by analogy to traditional precision by thinking of the number of relevant documents retrieved (the numerator of precision) as the minimum rank required to reach the recall at the same rank. This variant of precision has the same properties as traditional precision: it ranges from 0 to 1; it is greater when more unique facets have been retrieved; it approaches zero as k  X  X  X  .

Like finding the minimum optimal rank, calculating S-precision is NP-Hard. Again, the greedy approximation al-gorithm works very well in practice.
When a facet F j occurs in the document at rank 2 af-ter having already occurred in the document at rank 1, its appearance in document 2 is redundant . There is often a tradeoff between eliminating redundancy and retrieving the smallest set of documents that contain all the facets: less redundancy may require more documents to cover all the facets. Therefore we evaluate redundancy at rank k sepa-rately from recall and precision (Zhang et al. also argued for evaluating redundancy separately [17]). Redundancy is the average number of times each facet is duplicated up to rank k (if there are no relevant documents ranked above k , redundancy is undefined). Between two systems with the same S-rec , the one with lower redundancy should generally be preferred, but lower redundancy is not by itself a reason to prefer a system.

In Figure 1, four of the 14 facets would be retrieved more than once in the smallest possible set. It is impossible to cover all 14 facets without redundancy.
The Probability Ranking Principle is a well-known guide-line for ranking documents in standard IR tasks such as ad hoc retrieval [13]. It says that optimal performance is achieved when documents are ranked in decreasing order of probability of relevance. It therefore provides guidance for building retrieval systems: systems that do a better job at predicting relevance will perform better by precision and re-call measures.

The PRP assumes that documents are independently rele-vant [9]. This is not the case in faceted topic retrieval, as Fig-ure 1 suggests. If the top two documents are LAT20040204-.0043 and APE20040401.0108, there is no additional benefit to retrieving LAT20040430.0068 at rank 3, even though it is relevant to the topic. S-recall captures this by rewarding a system for retrieving a different, non-redundant, relevant document at rank 3, while traditional recall does not.
In this section we describe two well-known models for nov-elty ranking, and propose two new models. The two well-known models are a heuristic approach and a probabilistic analogue; the new models follow the same pattern.
A natural approach to this problem is maximal marginal relevance , defined by Goldstein &amp; Carbonell [4]. As the name suggests, MMR is a greedy ranking method that chooses the i th document in a ranking according to a combination of its similarity to the query and its similarity to the docu-ments ranked at positions 1 to i  X  1: MMR ( D i ,Q ) =  X sim 1 ( D i ,Q )  X  (1  X   X  ) max where sim 1 is a standard query-document scoring function, sim 2 is a similarity function between documents, and  X  is a parameter. When  X  = 1, a ranking by MMR is equiva-lent to a ranking by the query-document similarity. MMR is a simple but effective approach to novelty ranking, and therefore an obvious approach to faceted topic retrieval. Zhai et al. proposed a probabilistic interpretation of MMR. Documents are scored on the basis of two probabilities: a probability of relevance P ( rel | D i ) and a probability of con-taining novel information P ( new | D i ). These two probabili-ties are combined together in a scoring function as: Zhai et al. argue that there is no cost to presenting a novel relevant document ( c 1 = 0) and that the cost of present-ing a nonrelevant document is unaffected by whether that document is novel or not ( c 3 = c 4 ), resulting in the final rank-equivalent scoring function: s ( D i | D 1 ,...,D i  X  1 ) = P ( rel | D i ) The ratio c 3 /c 2 can be replaced with a single parameter  X  . The problem thus reduces to estimating P ( rel | D i P ( new | D i ). P ( rel | D i ) is naturally estimated using a lan-guage model. Zhai et al. present several methods for es-timating P ( new | D i ), the most effective of which is called AvgMix . The AvgMix estimate is calculated by maximizing the log-likelihood of observing D i after sampling n words from a mixture of an  X  X ld X  model (i.e. of a previously-ranked document) and a background model with respect to mixing parameter  X  . Greater  X  means D i is less likely to model the previously-ranked documents, and therefore more likely to be novel. This mixing parameter is found for each document at ranks 1 through i  X  1, then averaged for a final estimate of P ( new | D i ).
Instead of greedily ranking documents using an estimate of novelty, we could rank documents by their similarity to the query, then prune that ranking of the documents that are most similar to other documents in the ranking. In this method, we simply step down the ranked list of documents (in order of relevance) and prune documents with similarity greater than some threshold  X  . I.e., at rank i , we remove any document D j , j &gt; i , with sim 2 ( D j ,D i ) &gt;  X  . This approach may result in different rankings than MMR, since it uses query similarity and novelty in two separate steps rather than combining them in one.
Our set-based formulation of S-recall suggests a set-based ranking principle for faceted topic retrieval: retrieve the set of documents that maximizes the likelihood of capturing all of the facets. This can be visualized by generalizing Figure 1 to a graph in which instead of 0-1 edges between documents and facets, each edge has a weight representing the proba-bility that each document contains every possible facet in the universe. The goal of the faceted topic retrieval sys-tem is to find the smallest set of documents that  X  X overs X  the facet space with highest probability. Figure 2 shows the probabilistic facet graph. In this example, instead of 14 known facets there is a (countably) infinite universe of pos-sible facets, of which the 17 shown have highest probability, and the  X  X rue X  14 are a subset of those. Every document has some probability of containing every facet; the thickness of the edge reflects the strength of the belief.

Suppose we have a particular hypothetical set of facets F and a set of documents D . Denote the probability that D contains F as P ( F  X  D ). This is the probability we wish to estimate, and ultimately maximize over sets D and F . As the equation for S-recall suggests, this is a probabilistic OR problem: F j can be in document D 1 , or it can be in document D 2 , or in document D 3 , and so on. We do not require that it be in all of them, only that it be in at least one. The well-known  X  X um rule X  of probabilities tells us that
P ( F j  X  X  D 1 ,D 2 } = P ( F j  X  D 1  X  F j  X  D 2 ) As the number of documents grows, the number of clauses in the OR statement grows, and the number of terms in the expanded probability grows exponentially. With even a small set of documents it is infeasible to calculate. However, if we assume that a facet occurs in documents independently (i.e. P ( F j  X  D 1 ,F j  X  D 2 ) = P ( F j  X  D 1 ) P ( F j can estimate the probability as follows: P ( F j  X  X  D 1 ,D 2 } ) = 1  X  (1  X  P ( F j  X  D 1 ))(1  X  P ( F In general, then, the probability that a facet F j occurs in at least one document in a set D is and the probability that all of the facets in a set F are captured by the documents D is P ( F  X  D ) = Note that this involves a second independence assumption: that facets occur in documents independently of one an-other. Because the retrieval is set-based, these independence assumptions will not controvert the statement of the prob-lem that facets are correlated across documents.

It is fairly easy to see that maximizing P ( F  X  D ) with a subset D of corpus C , | D | = k , directly results in maximizing S-rec @ k , much as maximizing P ( R | Q,D ) for relevant doc-uments directly results in maximizing precision and recall. S-precision will not necessarily be maximized, nor redun-dancy minimized, but there is a tradeoff involved: because the set of documents that maximize recall at rank 2 is not necessarily contained in the set of documents that maximize recall at rank 3. 1 , we must make some decision to try to op-timize recall at a particular rank or to optimize the number of new facets retrieved at each rank. This decision comes into play at the optimization phase.

A faceted topic retrieval engine using this model must do three things. First, it must hypothesize a set of facets. Second, for each facet F j , it must estimate the probability P ( F j  X  D i ) that it occurs in each document D i . Third, it must have a way to select the smallest set of documents that is most likely to contain all the facets. We will consider each of these in turn.
Given only a short query, the system must be able to pro-duce some hypothetical set of facets against which to score documents. There are various ways we could do this, includ-ing clustering, topic modeling, relevance modeling, phrase extraction, and so on. Some approaches may be supervised, others unsupervised. We have chosen to evaluate two un-supervised probabilistic methods: topic modeling with LDA and relevance modeling. Others will be left for future work.
In both cases we will assume that we have been given a set of documents from which to  X  X xtract X  some facets. In-stead of trying to extract any particular word or phrase to use as a facet, we will instead build a  X  X acet model X  P ( w | F ). Our hope is that two different facet models will
If this type of containment were necessarily true, the eval-uation problems would not be NP-Hard; they would in fact be solvable in polynomial time by the greedy algorithm. The fact that the greedy algorithm only gives an approximation provides evidence for this claim. capture something about the vocabulary associated with dif-ferent facets by assigning higher probabilities to different terms. In our example above, we may have a facet model that corresponds to  X  X iofuels X  by giving higher probabili-ties to words that co-occur with  X  X iofuel X  more often than they co-occur with other terms, and one that corresponds to  X  X as tax X  by giving higher probabilities to words that co-occur with  X  X as tax X  more often than other terms. Then P ( D i | F biofuel ) &gt; P ( D i | F gas tax ) suggests that document D is more likely to contain the  X  X iofuels X  facet than the  X  X as tax X  facet (where P ( D i | F j ) = Q w  X  D unigram language model).

A relevance model is a distribution of words P ( w | R ) es-timated from a set of relevant or retrieved documents [11]. Similarly, we will estimate m  X  X acet models X  P ( w | F j ) from a set of retrieved documents using the so-called RM2 approach described by Lavrenko and Croft [11]: P ( w | F j )  X  P ( w ) Y where D F j is the set of documents relevant to facet F j are the facet terms, P ( w ) = P D P ( w | D i ) is a smoothed estimate. Since we do not know the facet terms or the set of documents relevant to the facet, we will estimate them from the retrieved documents. We obtain m models from the top m retrieved documents by taking each document along with its k nearest neighbors as the basis for a facet model.

In the LDA approach, the  X  X acets X  are actually latent vari-ables that are priors for document term occurrences [3]. Probabilities P ( w | F j ) and P ( F j ) are found through expecta-tion maximization. Then we can find P ( D i | F j ) = Q P ( w | F and P ( F j | D i )  X  P ( D i | F j ) P ( F j ), generally assuming a uni-form prior on documents. Again, we limit the calculation to documents retrieved for a particular query; this may limit the ability of LDA to identify topics that represent facets.
Facet models can be built from the set of documents in an initial ad hoc-style retrieval, and thus both of these can be seen as query expansion/relevance feedback methods. But instead of a single expanded query, there are m , where m is the hypothesized number of facets. In this work we assume constant, manually-selected m from query to query; a full optimization would be over the number of facets as well as hypothesized facets and subsets of documents.
Both the facet relevance model and LDA model produce generation probabilities P ( D i | F j ), i.e. the probability that sampling terms from the facet model F j will produce docu-ment D i . This is not a probability that a document contains a facet, which is what our model requires. However, much as the so-called query-likelihood P ( Q | D ) is not a probabil-ity of relevance yet is useful for ranking documents, these probabilities may still be useful in a faceted topic retrieval system. We consider this an empirical question.

Since the probabilities are likely going to be very small, to avoid numerical errors we will rescale them to a range more suited to the binomial containment variable. We elected to linearly scale them to the range [0 . 25 , 0 . 75]. In a practical sense, this defines the containment probability using gener-ation probability as a feature. We could easily incorporate additional features in a supervised training phase; this is left for future work.
In Section 4 we presented the probability that a set of facets occurs in a set of documents. Above we discussed ways to choose facets; we also need a way to select a subset of documents. Let y i = 1 if document D i is selected and 0 otherwise. Then we can define the likelihood function: If y i = 0 then (1  X  P ( F j  X  D i )) y i = 1 and D i therefore has no effect on the likelihood. This expression is maximized with the constraint that P y i  X  k , i.e. the total number of documents taken is no more than a hypothesized minimum number required to cover the facets.

Note that maximizing L ( y ) is a 0-1 integer programming problem, which is NP-Hard in general. We can approximate the solution in various ways. Perhaps the most intuitive is analogous to our greedy algorithm for S-recall: greedily take the document that maximizes the likelihood conditional on the documents that have already been taken. This ensures that the first document taken produces the greatest expected number of facets, the second produces the greatest expected number of facets that are different from those provided by the first, and so on. It also provides a natural ranking of documents in order of their selection, and does not require any estimate of k . In execution it is very similar to MMR.
The greedy approach, while accounting for both diversity and redundancy, cannot necessarily maximize diversity. We therefore propose a simpler set-based approximation scheme: for each facet F j , take the document that maximizes its probability arg max i P ( F j  X  D i ). Note that this provides no ranking of the documents selected, so we rank them by their original ad hoc retrieval scores.

An alternative approach is to relax y to a vector of real numbers rather than 0-1 integers. This results in a likelihood function that is convex and differentiable, and thus can be solved with conjugate gradient descent methods. The con-straint P y i  X  k is no longer valid in this approach, since y is no longer an indicator for the presence or absence of a document. Without that constraint, Eq. 1 is actually max-imized by giving maximum score to every document; we introduce a penalization term  X  P y 2 i =  X  || y || to ensure that maximum scores are assigned to those documents most likely to contain facets: log L ( y ) = After maximization, the scores y i provide a natural ranking of documents.
In this section we describe experimental faceted topic re-trieval systems and apply them to data annotated with facets. There is no standard corpus for faceted topic retrieval. Allan et al. investigated the relationship between system performance and human performance on a faceted topic re-trieval task [2]; we obtained the queries and facet judgments used in that work. The data consists of 61 topics, each with a short (3-6 word) query, and judgments of relevance to doc-uments in a subset of the TDT5 corpus. A few of the queries are ambiguous and duplicated with different statements of information need ( a la Sanderson [14]). For example, the query  X  X ush visits X  appears twice, once in the context of foreign leaders that traveled to the U.S. to visit George W. Bush, and once in the context of places that Bush visited during his time as president.

There are three levels of judgment: a binary relevance judgment for the document; for each relevant document, a list of facets it contains; and for each facet, a passage in the document that supports its relevance to that facet. The documents judged are the top 130 retrieved by a query-likelihood language model for the short query. Since few documents were judged, it is very possible that facets exist in the corpus but do not appear in the judged documents. To ensure we have judgments on all ranked documents, we will only rerank these 130 documents for each query.
Sixty topics were annotated by two assessors (one was annotated by only one assessor; this was discarded). The statement of the information need contained guidelines on how to assess facets, but within those guidelines assessors were free to name the facets however they liked. On aver-age, there were 44.7 relevant documents per query; each of those contained 4.3 facets. There were 39.2 unique facets on average, for an average of just under one unique facet per relevant document. Agreement about relevance was quite high (72% of all relevant documents were judged relevant by both assessors), but there was substantial disagreement about the number of facets per query (a difference of 8 facets on average). Assessor agreed about the number of facets per relevant document within one facet.
We implemented all the models described in Section 4 us-ing the Lemur toolkit, as well as standard language modeling and language modeling plus pseudo-feedback with relevance models. Whenever possible, we have used the same similar-ity or scoring functions between models to ensure the fairest possible comparison. Specifically, our models are:
For the set-based model, we have two different ways to hypothesize facets and score documents. ) Finally, we performed a manual  X  X racle X  experiment using one assessor X  X  facet labels as queries to score documents against using query-likelihood. Like the FM-RM, these scores were rescaled to [0 . 25 , 0 . 75] and the optimization methods in Section 4.4.3 applied. This provides a loose upper bound on the performance of FM. 2
As discussed above, the likelihood maximization is a 0-1 integer programming problem. This is NP-Hard in general. Instead of trying to solve it directly, we tested several dif-ferent approximate solutions: Our results below use max-set; we compare that to the other approaches in Section 6.1.
We used five-fold cross-validation to train and test sys-tems, and to obtain results for all 60 queries for each model. We divided the 60 queries into five folds of 12 queries each. The 48 queries in four folds are used as a training set to se-lect model parameters such as  X , X , X , ( m,K,v ) (for MMR, AvgMix, pruning, and set models, respectively). These pa-rameters are used to obtain ranked results on the remaining
It is a very loose bound because, as stated above, assessors could name facets however they liked. Any unusual names would cause poor scores. For example, one assessor used ab-breviated labels such as  X  X Dakota X ,  X  X ashDC X ,  X  X Carolina X ; no document could score well against such queries. 12 queries. The query splits were chosen randomly in ad-vance so that all experiments used the same training and testing data.

For each method we report S-recall at the minimum op-timal rank S-rec (which ranges from 0 to 1, larger values indicating better performance), redundancy at the mini-mum optimal rank (which has a minimum of zero but no upper bound; smaller is better), and mean average precision (MAP) using the document-level relevance judgments. We also show 11-point interpolated S-precision/S-recall curves.
The average minimum optimal rank is 10, which is a good match to the first page of results in web search engines. There is substantial variance over queries, however, with some having a minimum rank of 56 and others having a minimum rank of 1 (there is a single document that con-tains all the facets). Comparing our greedy algorithm to ex-haustive search on a subset of topics shows that the greedy algorithm X  X  approximation is not perfect but very close.
Since there were two assessors for each topic, we test hy-potheses about differences between systems using a two-way within-subjects ANOVA on S-recall. A two-way ANOVA calculates the variance in a measurement of recall due to differences between systems and due to differences between assessors, as well as interactions between the two. We would like to see that the variance due to systems is significant and outweighs any other source of variance. If this is the case, the comparison is robust to differences in assessors. Ideally we would like to see that variance due to assessors is not sig-nificant, and in particular that the interaction is negligible.
Table 1 shows S-recall, redundancy ratio, and mean aver-age precision (MAP) for all systems described in Section 5.2. Among the seven automatic methods, result-set pruning gives the best overall results, though we note that there is no sig-nificant difference in the S-recalls of MMR, pruning, and FM-RM. All three retrieved about 44% of the facets, com-pared to roughly 40% by the two baselines and AvgMix, and only 15% by FM-LDA. All of the models (except FM-LDA) exhibited a fairly high degree of redundancy, dupli-cating each facet at least 0.5 times (on average) in the rel-evant documents retrieved by the minimum rank. MMR had the lowest redundancy, significantly lower than pruning and FM-RM. The very low redundancy of FM-LDA may be explained by the fact that it retrieved very few relevant documents; the recall-redundancy in Figure 3 gives a better sense of how redundancy varies with S-recall for each run.
The three best runs are significantly better than any of the others. Table 2 shows a summary of the results of an ANOVA to determine whether differences among the top five automatic runs are affected by assessor disagreement. Indeed, system differences are significant, while assessor dif-ferences are not, and there is negligible interaction between system and assessor (systems are not fitting to certain as-sessors).

The  X  X anual X  results in Table 1 provide some loose upper bounds. It retrieved 68% of the facets, but still retrieved each of them almost as many times as the facet model. This suggests that a fairly high degree of redundancy is inevitable. Many of the harder-to-find facets are only present in docu-ments that contain easy-to-find facets; it is simply not pos-sible to retrieve all of these without some redundancy. This run therefore suggests that lower redundancy is only super-Table 1: S-recall and redundancy at the minimum optimal rank and average increase in S-recall from rank 1 to the minimum optimal rank for four faceted topic retrieval systems. Numbers are averaged over 60 topics with two sets of assessments each. The best automatic result for each column is in bold. An asterisk indicates statistical significance. Table 2: Two-way ANOVA results on S-recall for the LM baseline, MMR, AvgMix, pruning, and FM-RM. Differences between systems are significant while differences between assessors do not signifi-cantly affect the results. There is insignificant in-teraction between assessor and system. ficially desirable; optimizing for redundancy may result in  X  X arder X  facets being missed. We explore this in Section 6.2 below.
 Because it uses the set-based framework we presented in Section 4.4.1, this manual run also suggests that the set-based model is easily improved simply by improving the facet models: if a user provides some information about the facets, we can easily incorporate it into the facet models. This stands in contrast to MMR or AvgMix, which cannot incorporate such information as easily.

Figure 3 shows the 11-point S-precision/recall curves for seven systems. Five of them coincide closely, though the MMR, pruning, and FM-RM curves are clearly above the LM baseline and AvgMix curves. The FM-RM curve is above the others at both the highest and lowest recall lev-els, but not in between. The FM-LDA system significantly underperforms compared to the others. Figure 3 also shows redundancy increasing with S-recall. The LM baseline has the highest redundancy, followed closely by AvgMix. The FM is  X  X iddle-of-the-road X , almost exactly in between all the other automatic runs. FM-LDA and pruning coincide closely over all S-recall values. MMR and the manual run coincide closely up to S-recall 0.5; after that the redundancy of MMR increases to match or exceed that of pruning.
We also computed basic mean average precision (MAP) using the document-level relevance judgments. Note that MAPs in Table 1 are high because every system was able to rank all judged documents. They should be considered up-per bounds, though their relative ordering would not change with more judgments. All of the non-baselines had lower MAPs than the baselines, demonstrating the inadequacy of MAP for this task. Table 3: Results with different optimization meth-ods. Bolded numbers are the best across the row.
 An asterisk indicates statistical significance by a within-subjects 2-way ANOVA ( p &lt; 0 . 05 ; assessor effects are not significant). The max-set approxima-tion tends to provide the best diversity results, while the relaxation approach provides the best MAP.
Table 3 compares different approximate optimization ap-proaches for FM. The baselines do not require any opti-mization, so they are not shown. The max-set method gives significantly higher S-recall and lower redundancy for the facet model. For the LDA model, it gives better (but not significantly so) S-recall but also greater redundancy.
The marginal-likelihood approach for FM-RM gives sim-ilar results to AvgMix. Both may be seen as probabilistic variations of MMR, so this is not surprising. It is notable that marginal-likelihood provides the best S-recall for the manual facet model. Though the redundancy is higher than the max-set approach, this is probably a consequence of re-trieving more relevant material. These results suggest that marginal-likelihood is successful when there is a high degree of confidence in the facets, but less so if not.

The improvement in MAP under the relaxed optimization approach was surprising to us. The relaxed approach tends to give similar weights to documents that are similar in the facet space; it is more optimal in this approach to give three identical documents each weights of 1 / 3 than to give one of them a weight of 1 and the other two 0. Thus we hypoth-esize that it is identifying low-ranked relevant documents that are  X  X imilar X  in the facet space to higher-ranked docu-ments and moving them up in the ranking. The fact that its redundancy tends to be higher supports this.
Because all of the subproblems of faceted topic retrieval are difficult, there are multiple points of failure: the hy-pothesized facets could be unrelated to actual facets; even if the hypothesized facets are good, the probability estimates P ( F j  X  D i ) could be bad; even if the probability estimates are good, the optimization techniques could be bad because of independence assumptions or because of bad approxima-tions. In this section we will consider some of these ques-tions.
 Are the hypothesized facets anything like the actual facets? We looked at term probabilities for FM and LDA facet mod-els to try to determine whether there was any relationship between hypothesized facets and actual facets. Though we can only make subjective observations, the FM facet models do seem to capture actual facets in some cases. For ex-ample, for query 1 oil-producing nations , nearly all of the facet models could clearly be labeled with a nation or region; the first five correspond to  X  X ub-Saharan African nations X ,  X  X had X ,  X  X raq X ,  X  X ndonesia X , and  X  X urma X  based on the term distributions. But there are also overlapping models and du-plicates, and some facets that do have representative mod-els occur after enough duplicates that they are not ranked highly enough to be counted in S-rec .

On the other hand, for query 52 disarm landmines land mines , the facet models seem to correspond more to activist protests and demonstrations related to landmines rather than to strategies taken to disarm them (which is the stated user information need).

The LDA model performed poorly on average, possibly because of a lack of data to estimate topics or too much cor-relation between facets. There were a few topics for which it outperformed the baselines. Query 15 Bush visits is looking for foreign leaders that visited Bush in the U.S. Interest-ingly, looking at the term distributions suggests that the facet models are strongly associated with countries that vis-iting foreign leaders call home. For example, one of the top-ics gave high probability to terms such as london, britain, blair , etc. Query 20 Bush visits is instead looking for coun-tries Bush visited; while LDA outperformed the baseline for both, it was by a much smaller degree for 20 than for 15.
For other queries such as query 51 allies Israel , it was dif-ficult to identify how the topic models were differentiated. It seems likely that this is a result of having so few documents from which to estimate topic models.

How similar are the models in terms of facets they re-trieve? We looked at the amount of overlap in facets re-trieved by MMR, FM-RM, and pruning. Of all unique facets retrieved in the top 10 by MMR and FM-RM, 77% (476 of 613) were retrieved by both systems. For MMR and prun-ing, 82% (498 of 606) were retrieved by both. FM-RM and pruning agreed on 80% (491 of 610). The systems do display some differences.

Does any model do a better job on the  X  X ard X  facets? To answer this, we looked at pairs of models and identified the facets that were found by one model but not by the other. We then calculate the average number of documents these facets occurred in. For example, if MMR found the facet United Arab Emirates for the query oil producing countries , but FM-RM did not, we would look at the judgments and see that UAE appears in nine different documents. If FM-RM found Kuwait and MMR did not, we would look at the judgments and see that Kuwait appears in eight different documents. We would conclude that FM-RM did a very slightly better job at finding slightly harder facets for this query.

Over all 60 queries, the average number of appearances of facets retrieved in the top 10 documents by FM-RM but not by MMR is 2.77, whereas the average number of appearances of facets retrieved in the top 10 documents by MMR but not FM-RM is 4.13 (there were 613 unique facets found by both systems, with 137 found by one but not the other). FM-RM therefore seems to do a better job of finding  X  X arder X  facets. On the other hand, MMR seems to do better than pruning (2.54 average appearances for MMR versus 3.53 for prun-ing). These relationships hold consistently as the number of retrieved documents increases, suggesting that FM-RM is better able to find harder facets than MMR, which in turn is better than pruning.

Does true optimization of FM give better results? Though the optimization problem is NP-Hard, for small n it is fea-sible to try all ` 130 n  X  document subsets to determine which is optimal. With n = 2 there are ` 130 2  X  = 8385 possible sub-sets for each query. The problem quickly becomes infeasible; there are two orders of magnitude more possibilities when n = 3.

We took the size-2 subset with the greatest log-likelihood and calculated S-recall at rank 2. We compared that to S-recall at rank 2 for our other optimization methods with each model. The result is that the true optimal set produces better results than any approximate set: a 16% improvement in the case of FM down to a 1% improvement in the manual model. This suggests that there is value in exploring other optimization methods.

Can independence assumptions in FM be relaxed? Does it make a difference? In Section 4 we made an independence assumption to keep the computation tractable, namely that facets occur in documents independently: We can relax this assumption slightly by using covariance to model dependence between two documents: P ( F i  X  X  D 1 ,D 2 } ) = P ( F i  X  D 1 ) P ( F i  X  D 2 ) + Cov ( D where Cov ( D 1 ,D 2 ) is calculated by summing over all facets, i.e. it is a measure of the similarity between the two docu-ments in the facet space.

We followed the same procedure as above, calculating the likelihood over all subsets of size 2 and taking the set that produced the maximum. We compared to the size-2 set above. Overall there is not a great deal of difference in S-recall: a 6% decrease for FM, a 4% increase for LDA, and a 3% increase for the manual run. There is a 10% decrease in redundancy for FM, however (and negligible changes in the other two). Modeling dependence does have some effect with  X  X rue X  optimization, then, though there is interaction with the hypothesized facets that is difficult to quantify.
We have defined a type of novelty retrieval task called faceted topic retrieval : retrieve the facets of an information need in a small set of documents to be presented to the user. We presented two novel models for it: one that prunes a standard retrieval ranking and one a formally-motivated probabilistic model. We demonstrated that both models are competitive with MMR, and outperform another prob-abilistic model X  X nd all models outperform the traditional IR baselines. Additionally, an upper bound experiment sug-gests that our probabilistic model could easily and naturally incorporate information about facets provided by users or extracted from other sources to improve results, whereas the other models could not incorporate this information without some reformulation.

We have only scratched the surface of what is possible within this framework; there is ample opportunity for work on hypothesizing or averaging over facet sets of different sizes (using hierarchical clusters, for instance), estimating the most likely number of facets, using additional features of documents and relevant passages to estimate P ( F j  X  D with supervised approaches, exploring other optimization functions, and so on. We believe our model will be appli-cable to other problems as well, including metasearch and multi-modal retrieval. [1] R. Agrawal, S. Gollapudi, H. Halverson, and S. Ieong. [2] J. Allan, B. Carterette, and J. Lewis. When will [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [4] J. Carbonell and J. Goldstein. The user of mmr, [5] H. Chen and D. R. Karger. Less is more: Probabilistic [6] C. L. A. Clarke, M. Kolla, G. V. Cormack, [7] W. Dakka and P. G. Ipeirotis. Automatic extraction of [8] W. Goffman. On relevance as a measure. Information [9] M. D. Gordan and P. Lenk. A utility theoretic [10] K. J  X  arvelin and J. Kek  X  al  X  ainen. Ir evaluation methods [11] V. Lavrenko and W. B. Croft. Relevance-based [12] F. Radlinski, R. Kleinberg, and T. Joachims. Learning [13] S. E. Robertson. The probability ranking principle in [14] M. Sanderson. Ambiguous queries: Test collections [15] E. M. Voorhees and D. K. Harman, editors. TREC: [16] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond [17] Y. Zhang, J. Callan, and T. Minka. Novelty and
