 Correlation clustering is a basic primitive in data miner X  X  toolkit with applications ranging from entity matching to social network analysis. The goal in correlation clustering is, given a graph with signed edges, partition the nodes into clusters to minimize the num-ber of disagreements. In this paper we obtain a new algorithm for correlation clustering. Our algorithm is easily implementable in computational models such as MapReduce and streaming, and runs in a small number of rounds. In addition, we show that our algo-rithm obtains an almost 3 -approximation to the optimal correlation clustering. Experiments on huge graphs demonstrate the scalability of our algorithm and its applicability to data mining problems. Categories and Subject Descriptors: H.2.8 [Database Manage-ment]: Database Applications -Data Mining Keywords: Scalable clustering; Signed networks.
Correlation clustering is a basic primitive underlying several data management and data mining tasks such as deduplication [5,19,24], identifying communities [15] and network link prediction [12].
The correlation clustering problem is the following: given an undirected signed graph where each edge is labeled either positive or negative, partition the nodes into clusters so that the total num-ber of disagreements is minimized. Here, a disagreement is said to happen if a positive edge becomes an inter-cluster edge or a neg-ative edge becomes an intra-cluster edge. Note that the problem definition does not contain the number of clusters; this is one of the main attractions of correlation clustering, since in many appli-cations it is not possible to specify the number of clusters a pri-ori. Correlation clustering was first introduced by Bansal, Blum, and Chawla [9]. They showed the problem is NP-hard in gen-eral and presented a constant-factor approximation for complete graphs. The factor was later improved to three by Ailon, Charikar, Part of this work was done while the author was visiting Google. This work was partially supported by a Google Focused Research Award, and by a Google Faculty Award. This work was done while the author was at Facebook.
 and Newman [3]; Gionis, Mannila, and Tsaparas [22] obtained a deterministic three-approximation algorithm for weighted graphs obeying the triangle inequality.

Even though correlation clustering is defined for signed graphs, such graphs arise in a variety of contexts, some obvious and some less obvious. A sign (positive or negative) on an edge in graph can connote different semantics depending on the underlying setting. In case of social networks and trust/opinion networks, a positive sign denotes endorsement whereas a negative sign denotes disapproval; signed networks in social media have been extensively studied re-cently [29, 30]. If the nodes of the graph are items that are possibly duplicated, a positive edge can signify that the items might be the same and negative edges could mean they are not the same. If the nodes of the graph are vectors, the sign could correspond to the an-gle between the vectors, denoting how aligned or different they are to each other.

Correlation clustering has been used in a wide range of data min-ing applications. We outline a few here; see Section 2 for more. (i) In the entity deduplication problem, the goal is to remove en-tities that are duplicates of one another. Such duplicates can arise if the entities were obtained through independent sources/feeds. A positive edge between two entities indicates that the entities could be the same and a negative edge indicates that they could be differ-ent. Correlation clustering has been used in this setting to cluster this graph and produce entities that are duplicates [5, 19, 24]. (ii) In signed social networks, where positive edge denote friend-ship and negative edges denote enmity, correlation clustering is a natural way to identify communities [15]. It has also been used as a tool for the link classification problem, i.e., to predict the sign of a missing edge [12], with formal interpretations to the predic-tion complexity of link classification in a supervised transductive learning framework. (iii) In machine learning and data mining, the problem of ag-gregating multiple clusterings and the problem of consensus clus-tering can be viewed as special cases of the correlation clustering problem [22]; here, the edge weights correspond to the fraction of clusters separating the two nodes.

In many of these applications, the signed graphs exhibit the fol-lowing two key properties: the positive degree of each node is typically bounded and the graph is complete , with the understand-ing that the missing edges are to be interpreted as negative. The randomized algorithm of Ailon et al. [3], which obtains a three-approximation for such graphs, will be the starting point of our work. Their algorithm is tantalizingly simple: pick a pivot node uniformly at random, remove it and all its positive neighbors as a cluster, repeat. Despite its simplicity, it is clearly inefficient on massive graphs since it is inherently sequential, possibly removing only a few nodes in each round of processing the graph. Our results. In this work we obtain an efficient scalable algorithm for the correlation clustering problem. In particular, we show how to make the approximation algorithm of [3] round-efficient by pick-ing multiple pivots in parallel and repeating. While this idea is nat-ural, we show that one has to be careful in how the pivots must be picked in parallel; a less judicious choice can lead to poor quality solutions. We show two key features of our parallel pivot algo-rithm. (i) It runs in a logarithmic number of rounds for graphs with a constant number of positive neighbors; for arbitrary graphs, the number of rounds gets further multiplied by the logarithm of the maximum positive degree. (ii) For complete graphs, it outputs a solution that is a close to three-approximation to the optimal corre-lation clustering solution. Our algorithm is extremely easy to im-plement in MapReduce, streaming, and message-passing models.
Our proof for the running time is a delicate charging argument that tracks the node with large positive degrees and shows that each of them either disappears or has its degree halved after a certain number of rounds. Our proof for the approximation guarantee pro-ceeds by getting a lower bound on the optimal by considering the dual of a linear program. In contrast, for general graphs, we show that the bounded positive degree (or, for that matter, a bounded de-gree) assumption does not buy much algorithmic benefit: the prob-lem is as hard as the unbounded degree case.
 We then implement our algorithm in a real MapReduce system. We run our algorithm on a graph corresponding to an entity dedu-plication instance; this graph consists of around 30M nodes and close to a billion edges. The ability to run our algorithm on such a large instance shows its scalability and applicability. We also show that our algorithm achieves performance very close to that of [3], despite being much efficient in terms of its round complexity. We also implement our algorithm in the streaming model and demon-strate its ability to handle graphs with more than 2.5B edges, but only using a very limited amount of main memory.
The related work falls into three categories: the extensive theo-retical work on correlation clustering, the applications of correla-tion clustering to web mining and machine learning problems, and the theme of efficient, scalable algorithms for web-scale mining.
As discussed earlier, there has been a lot of work on obtaining good approximation algorithms for correlation clustering on com-plete graphs [3, 9, 22]. The algorithm of Ailon et al. (known as Pivot ) will be the starting point for our work. For general graphs, Demaine et al. [18] presented an O (log n ) -approximation algo-rithm; they also showed a logarithmic integrality gap for a natural linear program for the problem. There have been some work on considering natural extensions of the vanilla correlation clustering problem; for example, bipartite correlation clustering was studied by Ailon et al. [2], overlapping correlation clustering was studied by Bonchi, Gionis, and Ukkonen [11], chromatic correlation clus-tering was studied by Bonchi et al. [10], and subspace correlation clustering was studied by G X nnemann et al. [23]. To the best of our knowledge, there has been no result on scalable algorithms with provable bounds for correlation clustering.

Correlation clustering is such a basic primitive that it finds a multitude of applications in web mining applications; in many of these situations, the freedom to not specify the number of clusters to a clustering algorithm is often desirable. Correlation clustering has been extensively used in the general problem of deduplication. For example, a modification of correlation clustering (by including hard constraints) was considered [5] to perform large-scale dedu-plication of entity references; for an extensive survey on this topic, see [19, 24]. Further applications include disambiguation in people search [25], co-reference resolution [34], and many NLP applica-tions (see the references in [20]). In web search context, correla-tion clustering has been used to cluster query refinements in web search [36], automatically label query-URL pairs with human-like judgments [1], and segment Web pages [13]. In social network setting, it has been used for link classification in signed networks such as trust and opinion networks [12] and for clustering sparse graphs that arise in social networks [15]. Correlation clustering has also been applied in learning settings such as support vector ma-chines [21] and cluster aggregation [11]. See the surveys [27, 38].
MapReduce algorithms have been developed for basic graph prob-lems such as minimum spanning trees [26], triangle counting [37], and matching [28, 35]. In the combinatorial optimization setting, MapReduce algorithms for the problems of maximum coverage [16], densest subgraph [7], and k -means clustering [8] were obtained re-cently. All these algorithms have the flavor that they show how to parallelize an inherently sequential algorithm, but still almost pre-serving the original guarantees. Our work is yet another instance of this theme, though the actual techniques we use are vastly differ-ent from that of the above. The scalability of correlation clustering algorithms has been addressed before [5, 6, 20] but none of these proposes provably scalable and provably good algorithms.
In this section we set up the basic notation and problem defini-tions. Let G = ( V,E ) be an undirected graph, with | V | = n . Each edge e  X  E is labeled either positive or negative ; let E be the set of all positive edges and let E  X  = E \ E set of all negative edges. Let  X ( v ) = { w | { v,w }  X  E } be the neighbors of v  X  V , let deg( v ) = |  X ( v ) | be its degree , and let  X  = max v  X  V deg( v ) be the maximum degree in G . Simi-lar concepts are defined with respect to the labels: for example, let  X  + ( v ) = { w | { v,w }  X  E + } , deg + ( v ) = |  X   X  + = max v  X  V deg + ( v ) . If G is weighted , let w + : E  X  [0 , 1] and w  X  : E  X  [0 , 1] be the edge weight functions; here by weighted we mean that each edge has a positive weight and a neg-ative weight and they sum to 1.
 beled graph G = ( V,E ) , find a partition of V that minimizes the sum of positive edges across two clusters and the sum of negative edges inside the clusters.
 In other words, the goal of correlation clustering is to minimize the total number of disagreements. Note that the number of clusters is not specified as part of the objective; this is one of the major attrac-tions of correlation clustering. Also, the above definition naturally extends to weighted graphs, where the weight of an edge is used in charging for the disagreements. As we mentioned in Section 1, we will be stating our results in terms of  X  + (i.e, the number of pos-itive edges incident on every node); we will be interested in very efficient algorithms when  X  + is bounded.

Correlation clustering is an NP-hard problem [9] and hence there has been work on developing approximation algorithms for it (see Section 2). A clustering is said to be an  X  -approximation if its cost is at most  X  times the cost of the optimal correlation clustering. (Note that the number of clusters produced by an approximation algorithm need not be the same as the number of optimal clusters.) We focus on an elegant algorithm of Ailon, Charikar, and New-man [3], henceforth referred to as the Pivot algorithm.

The basic idea in Pivot is simple: pick a node uniformly at ran-dom; designate all the nodes connected to it by positive edges as a cluster; remove this cluster; and repeat. For future convenience, we state this algorithm in the following manner. The subroutine Algorithm 1 CreateCluster ( V,E + ,v ) 1: C  X  X  v } X   X  + ( v ) // positive neighbors 2: V  X  V \ C 3: E +  X  E +  X  V 2 4: output C as a cluster 5: return ( V,E + ) CreateCluster implements the task of creating a cluster and up-dating the graph, once a pivot is chosen. Given this, the Pivot al-gorithm follows easily. An analysis of this algorithm shows that it Algorithm 2 Pivot ( G = ( V,E + )) 1: while V 6 =  X  do 2: v  X  uniform at random node in V 3: ( V,E + )  X  CreateCluster ( V,E + ,v ) obtains a three-approximation when G is a complete graph [3].
Given the simplicity of Pivot , it is somewhat tempting to use it in large-scale applications. A careful scrutiny however indicates that the algorithm makes expensive multiple passes over the input, i.e., every time a new cluster is output. In fact, this behavior can be pathological: for example, if the graph has only negative edges or if the graph is a line, then Pivot makes a linear number of passes. This clearly limits its applicability on massive real-world graphs.
In this section we present a fast, scalable algorithm for correla-tion clustering. This algorithm guarantees a constant-factor approx-imation when the graph is complete and runs in polylogarithmically many rounds. For general graphs, we show that the problem is as hard to approximate even in the bounded-degree case.

A natural way to obtain a scalable parallel algorithm for correla-tion clustering would be to parallelize Pivot by picking many pivots in parallel and grow clusters around them (using CreateCluster ) and hope that the graph shrinks significantly in each round. To proceed in this direction, we need to stipulate the distribution with which multiple pivots should be picked. One has to be careful in choosing a distribution: we illustrate two plausible approaches and indicate their pitfalls on complete graphs.

Suppose multiple pivots are chosen with probability proportional to their degrees (this is desirable since it will also shrink the graph faster). One can show that with high probablity this algorithm pro-duces a non-constant approximation on a graph composed of ln n stars of positive edges having n/ ln n nodes each, and such that each star center pair is joined by a negative edge.

On the other hand, suppose multiple pivots are chosen inversely proportional to their degrees (this approach was used by Luby [32] to obtain a parallel algorithm for the maximal independent set prob-lem). In our setting, however, one can verify the following: for a positive-edge clique of n/ 2 nodes, where each node in the clique is connected with a positive edge to a new node and where the rest of the edges are negative, this algorithm would produce a non-constant approximation with high probability. In general, even if a particular way of choosing multiple pivots looks appealing, it is unclear how to preserve the approximation properties of Pivot . We present our algorithm called ParallelPivot (see Algorithm 3). As stated before, the idea behind the algorithm is to choose pivots in parallel and grow clusters around them. The pivots are chosen in a delicate manner in a two-step process: in the first step, each node is sampled with probability inversely proportional to the current maximum positive degree; this results in a set of active nodes. In the second step, active nodes that are adjacent to each other (line 6) are deactivated; the remaining active nodes are the pivots (line 7). If a non-pivot node is adjacent to more than one pivot, then it is assigned to the smallest of them (lines 10 X 11) according to a uni-form random permutation of the nodes (  X  in the algorithm). Then, as in Pivot , each pivot grows its own cluster (in parallel) using its positive neighbors (line 13). In the next section, we show two prop-erties of this algorithm: (i) unlike Pivot , it runs in logarithmically many rounds and (ii) like Pivot , it still outputs a constant-factor approximation for complete graphs.
 Algorithm 3 ParallelPivot ( G = ( V,E + ) , ) 1:  X   X  a random permutation of V 2: while E + 6 =  X  do 3:  X  +  X  max v  X  V deg + ( v ) // current max +ve degree 4: for v  X  V do 5: A  X  A  X  X  v } with prob. p = /  X  + // active node 6: A 0  X  A  X  S v  X  A  X  + ( v ) // adjacent active nodes 7: P  X  A \ A 0 // pivots 8: B  X  X  v | v  X  V \ P  X |  X  + ( v )  X  P | X  2 } 9: for v  X  B do 11: E +  X  E + \{{ v,w 0 }| w 0  X   X  + ( v )  X  ( P \{ w } ) } 12: for p  X  P do 13: ( V,E + )  X  CreateCluster ( V,E + ,p ) 14: for v  X  V do 15: output { v } as a cluster Number of rounds. We first prove a crucial point about the progress made by the algorithm in each round. A natural approach to track the progress of the algorithm is by lower bounding the number of nodes or edges removed in each round. Unfortunately, such a lower bound would be too weak to show that ParallelPivot fin-ishes in a small number of rounds; we therefore resort to a subtler argument. We track the maximum positive degree of the graph, and show that after logarithmically many rounds of the algorithm, nodes with large positive degree either disappear, or their positive degree shrinks by a constant factor.

L EMMA 1. Suppose that &lt; 1 2 . Fix a constant c &gt; 0 and let  X  +  X  1 be the maximum positive degree before beginning a round of the loop at line 2. Then, for any node v , after 8 c ln n rounds, with probability at least 1  X  n  X  c , either v will have been removed from the graph or it will have degree at most  X  + / 2 .

P ROOF . Consider the graph at the beginning of an arbitrary round of the algorithm and let  X  + , deg + (  X  ) ,  X  + (  X  ) be defined with re-spect to this graph. The probability that a node is activated in this round is p = /  X  + . Let v be an arbitrary node and consider the event  X  v =  X  X n this round of the loop exactly one neighbor w of v gets activated and no neighbor of this neighbor w gets activated. X  Then, Now, observe that since  X  +  X  max deg + ( v ) , deg + ( w ) , we have p  X  max deg + ( v ) , deg + ( w )  X  . Thus, and By calculus, it holds that (1  X  x ) 1 x  X  1 4 , for each x  X  0 ,  X  1 2 , we have p  X  1 2 and therefore We thus obtain, If deg + ( v ) &gt;  X  + 2 , then we have and hence Pr [  X  v ] &gt; / 8 .

Now consider round i and let  X  + i be the maximum positive de-gree before this round. Consider any node v that had positive de-gree at least  X  + i / 2 in round i , and consider any subsequent round j &gt; i . If the positive degree of v in round j is at most  X  v will have shrunk its positive degree (with respect to round i ) by a factor of at least 2 . Otherwise, since the maximum positive degree is non-increasing during the execution of the algorithm, we will have that Pr [  X  v ] &gt; / 8 even at round j . Therefore, after rounds, either v  X  X  positive degree shrinks by a factor of at least 2 or the probability that v will end up in a cluster is at least From this, we can bound the number of passes the algorithm makes on the input graph.
 C OROLLARY 2. Algorithm 3 terminates after at most rounds of its loop, with probability at least 1  X  1 /n c , for any con-stant c &gt; 0 .
 Approximation factor. We now proceed to prove the approxima-tion guarantee when G is a complete graph. We start by recalling a definition from [3]. Consider the original graph G = ( V,E ) , and call a set T  X  V of three nodes a bad triangle if it induces two edges in E + and one edge in E  X  . Let T  X  V 3 be the set of bad triangles in the original graph G = ( V,E ) . Let S S { v,v 0 } X  T  X  X  T be the union of the sets of nodes of the bad trian-gles that contain both v and v 0 . We now show that the distribution with which nodes as picked as pivots by ParallelPivot is close to the distribution induced by Pivot .

L EMMA 3. Suppose that &lt; 1 7 . Let v 0 ,v 00  X  V be two nodes in the instance that are contained in at least one bad triangle and let P be the set of pivots chosen in a round of Algorithm 3 at the beginning of which no node in S v 0 ,v 00 was part of a cluster. Then,
Pr { v 0 ,v 00 } X  P 6 =  X  X  S v 0 ,v 00  X  P 6 =  X   X  2 | S
P ROOF . For simplicity, let S = S v 0 ,v 00 . Clearly, | S |  X  3 . Note that before a pivot is chosen in S , the maximum of the degree of v and the degree of v 00 in the graph induced by S will be at least 2 . Therefore, before a pivot is chosen in S , we have  X 
We compute the probability that an arbitrary node v  X  S be-comes a pivot, given that S  X  P 6 =  X  . By Bayes X  rule, Observe that since v  X  S , the event  X  v  X  P  X  S  X  P 6 =  X   X  is equal to the event  X  v  X  P  X . Moreover, the event  X  v  X  P  X  is a subset of the event  X  v has been activated X ; the latter event has probability p . Therefore,
Now, consider the probability of the event  X  S  X  P 6 =  X   X ; it is not smaller than the probability that exactly one node v  X  S gets activated and that none of its neighbors outside of S gets activated. Since  X  + is an upper bound on the maximum positive degree, Recall that (1  X  x ) y  X  1  X  xy for each 0 &lt; x &lt; 1 ,y  X  1 . Hence, Pr[ S  X  P 6 =  X  ]  X | S | X  p  X  1  X  7 X  + p = (1  X  7 )  X | S | X  p. Therefore for any node v  X  S , we have
The main claim can be then proved by a union bound on the two events given by v = v 0 and v = v 00 .
 We now prove our main algorithmic result. Our proof follows the one in [3], adapting its single-pivot choice charging argument to our parallel multiple-pivot process.

T HEOREM 4. Suppose that &lt; 1 7 . Then, Algorithm 3 returns a 1 + 2 1  X  7 -approximate correlation clustering.

P ROOF . Observe that the total cost of the clustering C produced by Algorithm 3 can be upper bounded by the number of bad trian-gles that will be removed from the graph because of the selection of one of their nodes as a pivot. For T  X  T , let p T be the proba-bility that at least one of its nodes is chosen as a pivot while T is a completely part of the graph. Then, the expected cost incurred by the algorithm can be upper bounded by For lower bounding the cost of the optimum, we consider the fol-lowing linear program (LP) relaxation of correlation clustering. Note that in the integral solution, x v,v 0 = 1 if the edge between v and v 0 is not satisfied, and 0 otherwise. The dual of the above program can be written as follows. The value of any feasible solution of this dual is a lower bound on the optimal value of the LP, which in turn is a lower bound on the value of the optimal correlation clustering. We will show that is a feasible solution to the dual. Then, (1) and (2) will prove that the algorithm returns a 1 + 2 1  X  7 -approximation in expectation.
To show (2), consider any edge { v,v 0 }  X  E and for simplicity, let S = S v,v 0 . Recall that a bad triangle will increment the total cost of the algorithm X  X  solution if and only if at least one of its nodes is chosen as a pivot before the triangle disappears. The expected total cost of the | S | X  2 bad triangles containing { v,v be upper bounded in the following manner. If any node v 00 { v,v 0 } is chosen before any of { v,v 0 } , then a cost of 1 (i.e., the cost of the bad triangle { v,v 0 ,v 00 } ) is incurred and all the bad triangles containing { v,v 0 } will be eliminated; the probability of this event is at most 1 . If, on the other hand, at least one of v and v as a pivot before any other node in S , then the cost will be | S | X  2 (i.e., a cost 1 for each of the | S | X  2 triangles). By Lemma 3, the probability of this event is at most 2 | S | (1  X  7 ) . Thus, the expected total cost of the | S | X  2 bad triangles containing { v,v since the expected cost of a bad triangle is p T . Therefore, if y p / (1 + 2 1  X  7 ) , then all the dual constraints will be satisfied. Suppose that we fix an upper bound of  X  &lt; 1 7 on . Then, the ap-proximation ratio of our algorithm, as long as  X   X  , can be upper bounded by 3 + 14 1  X  7  X  . As approaches 0 , this ratio approaches 3 + 14 + O ( 2 ) = 3 + O ( ) . Concretely, if  X   X  = 1 14 , then we have that the approximation ratio is at most 3 + 28 .

We observe that the total number of bits communicated in a round is at most O ( n log n ) , which is also the size of the output and is therefore, in a sense, inevitable. We also observe that the analysis can be extended to weighted graphs that satisfy the so-called probability constraints , as in [3] (the algorithm still remains the same); we omit the analysis details in this version of the paper. Tightness of the running time. We show that our running time analysis is near-tight. To this end, we construct the following com-plete instance G = ( V,E ) by only specifying the positive edges; the rest of the edges will be negative. Choose some integer 1  X  k &lt; (1  X  ) d log 2 n e , and for each i = 1 ,...,k , create n/ 2 disjoint stars with 2 i leaves each and add them to G . It is easy to see that | V | =  X ( n ) and  X  + = 2 k , and that the number of executions of the algorithm X  X  loop on this instance is  X (log n  X  log  X  MapReduce. Recall that in the MapReduce model of computa-tion [17], the input is partitioned across a set of machines and the computation is split into a map phase and a reduce phase. In the map phase, each machine consumes its portion of the input and outputs a set of key-value tuples. In the reduce phase, all the tuples with the same key are processed by a single machine and output, if any, is emitted. These two phases can be repeated, yielding a multi-round algorithm. Since each phase is expensive from both computational and data access points of view, it is desirable to have MapReduce algorithms with a small number of rounds.

We now illustrate how ParallelPivot can be implemented in a small number of rounds in MapReduce. Clearly, the maximum pos-itive degree of a graph can be computed in one round of MapRe-duce. Likewise, the task of (randomly) selecting active nodes and computing pivots by removing adjacent active nodes (lines 5 X 7) can also be computed in one round. Once the pivots are com-puted, assigning each non-pivot node to a unique pivot neighbor (lines 10 X 11) can be done with an additional round. Finally, it is easy to observe that CreateCluster itself, with the graph update, can be implemented in yet another round. In Section 5.1 we show the performance of a MapReduce realization of our algorithm. Streaming. In the streaming model of computation [4], the input is read sequentially by a machine with a limited amount of main memory (typically, sublinear in the size of the input). The goal is design an algorithm that makes a few passes over the input in order to compute the function. In the graph case, input is assumed to be available as a stream of edges. It is easy to check that each of the steps of ParallelPivot can be implemented in the streaming model, where the only amount of memory is to store the pivots and their neighbors in each pass; in Section 5.3 we show the performance of a realization of our algorithm in the streaming model.
 Pregel. Pregel is a large-scale graph computing framework that is based on the bulk synchronous message-passing model [33]. In this model, each node of the graph is a computational unit, the compu-tation proceeds in supersteps, and within each superstep, messages can be asynchronously exchanged between the nodes. It is easy to check that the two communication-intensive steps in our algorithm, namely, finding the current maximum degree and computing the pivot nodes from the active nodes can themselves be implemented in a constant number of Pregel supersteps.
In this section we consider the correlation clustering problem on general (i.e., not necessarily complete) graphs. For general graphs, the best known is an O (log n ) -approximation algorithm. This algo-rithm, however, is based on rounding a linear program [18]. Given this, we turn our attention to the case when  X  is bounded (which is stronger than assuming  X  + is bounded). Unfortunately, we show that correlation clustering on bounded-degree graphs is as hard as solving correlation clustering on arbitrary graphs, thereby dashing any hopes for an improved centralized algorithm.

T HEOREM 5. There exists a constant c such that if there is a polynomial time  X  -approximation algorithm for correlation clus-tering on graphs with  X   X  c , then there is a polynomial time  X  -approximation algorithm for correlation clustering on every graph.
P ROOF . We show a reduction from an arbitrary graph G to a bounded-degree graph G 0 . Let G = ( V,E + ,E  X  ) , where | V | = n is sufficiently large. From G , we will construct (in polynomial time) G 0 with the following properties: there exists a canonical form of the (correlation clustering) solutions of G 0 such that every solution of G 0 can be transformed in polynomial time into a canon-ical solution having equal or smaller cost and there is a polynomial-time computable bijective mapping between canonical solutions of G 0 and solutions of G that leaves the cost of the solutions un-changed. These properties will complete the proof.
The key gadget we use in our proof is a constant-degree regular expander graph H = ( W,F ) with the following properties: (i) | W | = 2 n , (ii)  X  v  X  W, deg( v ) = 2 | F | / | W | = O (1) , and (iii) for each set S  X  W such that | S |  X  | W | / 2 , it holds that the cut ( S,W \ S ) has at least 2 | S | edges. Such graphs exist and can be constructed in polynomial time [31]. The idea then is to  X  X ensor X  each node in G with H so that the resulting graph will have constant degree and if the tensoring is done carefully, then the correlation clustering solutions will be preserved as well.

The target graph G 0 = ( V 0 ,E 0 + ,E 0 X  ) will be made up of n copies of H , one for each node in V . Each edge in each of these copies will be labeled positive. These copies will be connected with each other through what we call instance edges. Fix v i  X  V and consider v i  X  X  copy of H denoted H i = ( W i ,F i ) . Since | W 2 n , it is possible to find an injective function from  X  to W i ; we use w i,j to denote the node in W i corresponding to the ( v  X  X  side of) the edge { v i ,v j } in E +  X  E  X  . If { v i (resp., negative) edge, then we add a positive (resp., negative) in-stance edge between w i,j and w j,i . This completes the description of the instance G 0 . Note that no node in V 0 will be hit by more than one instance edge and therefore  X  0 will still be a constant.
Next, we show correctness. We say that a clustering of G canonical if no W i gets partitioned among two or more clusters. Observe that a canonical clustering of G 0 will have cost equal to the number of instance edges that are incorrectly clustered. More precisely, if { v i ,v j }  X  E + , then a canonical clustering of G pay 1 for that edge if it places W i and W j in two different clusters. Analogously, if { v i ,v j }  X  E  X  , then a canonical clustering of G will pay 1 for that edge if it places W i and W j in the same cluster. Therefore, if we produce a clustering of G from a canonical clus-tering of G 0 by putting v i and v j together if and only if W are together in the canonical clustering, then the cost of the clus-tering of G will equal the cost of the canonical clustering of G Conversely, to go from a clustering of G to a canonical clustering of G 0 with the same cost, it suffices to cluster the sets W together in G 0 if and only if v i and v j are clustered together in G .
Therefore, we only need to show that at least one optimal clus-tering of G 0 is canonical. We show this by methodically transform-ing a non-canonical solution to a canonical solution of smaller or equal cost in polynomial time. Indeed, suppose in the current non-canonical solution, W i is partitioned between k  X  2 clusters into mutually disjoint parts P 1 ,...,P k , with | P 1 |  X   X  X  X   X  | P | P 1 | +  X  X  X  + | P k | = | W i | . Necessarily, we have | P and therefore the number of non-instance (positive) edges that are cut by the original clustering is at least 2  X | W i \ P k lows from the expansion property (iii) of H i . If t k was the number of instance edges hitting P k that were misplaced by the original clustering, we then have that the contribution 1 of W i to the cost function is at least 2  X | W i \ P k | / 2 + t k / 2 . On the other hand if for each i = 1 ,...,k  X  1 , we disconnect the nodes in P i from their current cluster and we put them in P k  X  X  cluster, then we will have that all the non-instance edges of W i will be correctly clustered. Therefore, the cost of the set W i \ P k will be at most the cost of its instance edges, i.e., at most | W i \ P k | . The cost of P be limited to the cost of its instance edges, i.e., t k / 2 . Therefore, the total cost will be at most | W i \ P k | + t k / 2 . Therefore, merging all the P i  X  X  together will not increase the cost of the clustering. By repeating this merging operation, we will end up with a canonical clustering of cost not larger than that of the original clustering.
The contribution of a set of nodes to the cost function is defined as follows: a misplaced edge fully contained in the set has cost 1 , whereas a misplaced edge half-contained in the set has cost In light of this negative result, to obtain a scalable algorithm in the general case, one has to resort to heuristics. A few possibilities are: (i) One could run Algorithm 3 on general graphs, hoping that it will produce a reasonably good solution. Note however that the algorithm is still efficient, i.e., runs in O (log n  X  log  X  (ii) Instead of minimizing the number of disagreements in the correlation clustering objective (Definition 3.1), one could instead maximize the number of agreements. One way to do that would be to find a maximal matching M of ( V,E + ) and take each matched edge in M as a cluster and the remaining nodes as singletons. Note that this heuristic can be implemented scalable using the methods in [28]. The heuristic does produce an O ( X ) approximation for the maximization version 2 . Indeed, since no negative edges are clustered, no mistakes are made on the edges in E  X  . Since M  X  E + is a maximal matching, | M | X  | E + | exist an edge in E + whose endpoints are not covered by M . Thus, the proposed clustering correctly joins at least an  X  1  X  of the edges in E + . (iii) One could try to adapt the LP rounding algorithm of [18]. To do so, though, one would need to (1) solve the linear program, and (2) round its fractional solution to an integral one. It is un-clear whether these two steps can be carried out efficiently in a MapReduce-like framework.
We implemented our techniques in MapReduce and streaming, and evaluated it on the following datasets. The goal our experi-ments will be to show that (i) the solution quality of ParallelPivot is very close to that of Pivot and (ii) ParallelPivot runs in a very small number of rounds.
 Datasets. For our first dataset, we use the structured dataset embed-ded in HTML pages that was extracted by the Web Data Commons project ( webdatacommons.org/ ). The project extracts all Mi-croformat, Microdata, and RDFa data from the Common Crawl web corpus ( commoncrawl.org/ ), the largest and most up-to-data web corpus that is currently available to the public, and pro-vide the extracted data for download in the form of RDF-quads. From this dataset, we constructed all entities that correspond to places , where a place is defined as any entity that has a name , either a street address or geo co-ordinates, and optionally phone and website . Appendix 6 describes the mapping that we used from the RDF types to our schema, which can be used to repro-duce our dataset. We extracted around 35M places from Web Data Commons. Our goal is to resolve duplicates in this dataset. Given a pair of entities, we use the TF-IDF similarity between their fields to determine if they are duplicates. Since we cannot compute the similarity between all pairs, we use an n -gram index [14] on the fields to generate candidate pairs, and run our similarity condition on all candidates to generate duplicate edges. The resulting dataset has around 900M edges. We call this dataset WDC .

The second dataset that we use is the Cora dataset ( people. cs.umass.edu/~mccallum/data.html ). The Cora dataset consists of 1,878 citations to real papers. This dataset has been hand-clustered into groups referring to the same paper. While this is a much smaller dataset, the availability of ground truth enables us to study the accuracy of our algorithm, as well as compare it with Pivot . On this dataset, we trained a similarity classifier, and ran it on all pairs of node. The classifier resulted in 68,882 positive edges. We denote this dataset by CORA .
We recall that there exists a trivial 2 -approximation for the max-imization version: return the best of the all-in-one-cluster, and the each-in-its-own-cluster, partitions.
The third dataset that we use is an undirected version of a Twitter follower graph ( an.kaist.ac.kr/traces/WWW2010.html ).
 This graph consists of more than 41M nodes and 2.5B edges, with the maximum degree of 2.9M. In addition to being larger, this dataset has a higher average degree, a very high maximum degree, and a skewed degree distribution. We denote this dataset by TER . As before, we treat the follower edges as positive and the missing edges as negative. Figure 1: Number of nodes and edges remaining after each round on the WDC dataset ( = 0 . 5 , 1 ).

We ran 50 rounds of ParallelPivot on the WDC dataset with a setting of = 0 . 5 , and 25 rounds with a setting of = 1 . After each round, we plotted the number of remaining nodes, the number of remaining edges, the maximum and the average degree, the number of active nodes and the number of pivots.

Figure 1 shows the plots of number of remaining nodes and edges respectively. We see that the nodes decay almost linearly with the number of rounds. The large drop in the first round is due to the fact that a large number of nodes had degree 0 (they did not generate any candidate), and hence got removed in the first round. The number of edges show a more dramatic decay. The edge roughly halve in size after every 6 rounds for = 0 . 5 and 3 rounds for = 1 . For = 0 . 5 , after around 30-40 rounds, the graph is small enough to fit in the memory of singe machines, at which point we can simply use the Pivot algorithm to cluster the remaining graph. For = 1 , we need around 15-20 rounds.

Figure 2 plots  X  + (the maximum positive degree) of the graph after each round. The initial graph has  X  + = 1 , 400 , but it falls rapidly with each round. It also plots the average degree of the graph, which itself falls rapidly. This indicates that the graph be-comes progressively sparser.

The number of active nodes chosen at each round is directly pro-portional to the number of total remaining nodes and inversely pro-Figure 2: Maximum and average degree after each round on the WDC dataset ( = 0 . 5 , 1 ). portional to  X  + , both of which decrease over time. But  X  faster, resulting is more active nodes chosen at each round. Fig-ure 3 shows this plot. It also plots the number of pivots (number of active nodes not adjacent to any other active node). We see that most active nodes end up being pivots. As Figure 3 shows, this ratio remains flat at around 98% throughout the entire run for = 0 . 5 . A ratio of 100% would mean that all the pivots are truly random, and the algorithm would be equivalent in accuracy to the Pivot al-gorithm. Thus, a high ratio of 98% indicates that we are close to Pivot accuracy. For = 1 , the ratio is still very high, between 95% and 98%. Note that the theoretical guarantee of ParallelPivot is a 3 + O ( ) , which could be quite larger (for constant ) than the 3 -approximation returned by Pivot . Still, Figure 3 shows that, in practice, the two algorithms seem to be on par. Since Pivot cannot be run on such a large dataset, we cannot provide a direct compari-son of accuracy on this dataset.
We use CORA to evaluate the accuracy of our techniques. Even for this small dataset, running the optimal clustering is infeasible. Thus, we cannot directly evaluate the accuracy of ParallelPivot with respect to the optimum. Instead, we evaluate its accuracy with respect to the golden truth, and compare it with Pivot . For each of the two algorithms, we define two notions of accuracy. The first is the number of disagreements with respect to the golden truth, which is the sum of the false positives and false negatives. The second is the F -measure of each algorithm, which is the harmonic mean of the precision and the recall with respect to the golden truth.
Figures 4 plot the two measures of accuracy for ParallelPivot , for ranging from 0 . 1 to 0 . 9 , and compare it with Pivot . For each , we ran ParallelPivot 100 times and took the average. The shaded regions in each plot represent the error bars consisting of one stan-Figure 3: Number and fraction of active nodes chosen as pivots at each round on the WDC dataset ( = 0 . 5 , 1 ). dard deviation. We see that for the entire range of , both the dis-agreements and F -measure of ParallelPivot are on par with Pivot .
Figure 5 shows the number of rounds that ParallelPivot takes, as a function of . Note that the ground truth has 190 clusters, so Pivot , that processes one cluster as a time, will require 190 rounds. ParallelPivot starts with 140 rounds for = 0 . 1 , but only requires around 25 rounds when is close to 1. Note that our algorithm is not defined for &gt; 1 , since the probability /  X  + of choosing active nodes can become greater than 1. While our guarantee of (3 + ) approximation for ParallelPivot only holds for &lt; 0 . 14 , Figures 4 and 5 together show that ParallelPivot is at par with Pivot over the entire range of , and the number of rounds significantly go down when is close to 1.
We use TWITTER to demonstrate ParallelPivot in the streaming model. Since the number of edges in this dataset is more than 2.5B, which makes it hard to store the graph entirely on a machine with a small amount of main memory. The algorithm ran for 140 rounds.
Figure 6 shows the number of nodes and edges after each round of ParallelPivot . Perhaps because of the skewed nature of the de-gree distribution, the behaviors are different: the number of nodes decrease slowly in the beginning and rapidly towards the end, whereas the number of edges has the opposite behavior. Figure 7 shows the average and maximum degrees in each round. The behavior is sim-ilar to the number of edges, though the maximum degree drops rapidly; this enables more pivots to be chosen in subsequent rounds of ParallelPivot , which in turn contributes to an increased drop in the number of nodes as seen in Figure 6.

Figure 8 shows the number and fraction of active nodes chosen as pivots in each round of the algorithm. The number of active nodes (and pivots) is unimodal in nature, peaking at around the 100th round. Note that this fully determines the memory required by the Figure 4: The number of clustering disagreements and the F -measure as a function of on CORA . streaming algorithm. Using the average degree from Figure 7, we see that the the memory used by the algorithm is upper bounded by the space require to store a graph of around 200K nodes of average degree around 2, which is insignificant (especially, when compared to the size of the original graph).
In this paper we considered the problem of how to efficiently perform correlation clustering on massive graphs. We obtained a simple algorithm that runs in a provably small number of rounds and obtains a constant-factor approximation to correlation cluster-ing. Since correlation clustering is a basic web mining problem, our algorithm opens up the possibility of it being applied to very large instance. We illustrate this by running our algorithm on a real MapReduce and streaming systems on huge real-world instance;
Figure 5: The number of rounds as a function of on CORA . Figure 6: Number of nodes and edges remaining after each round on the TWITTER dataset. this is one of the largest instances where a correlation clustering algorithm has been run. It will be interesting to consider variants of correlation clustering have been used in data mining applications to see if such variants also admit an efficient parallel algorithm. Figure 7: Maximum and average degree after each round on the TWITTER dataset. Figure 8: Number and fraction of active nodes chosen as pivots in each round on the TWITTER dataset. The schema for our places data consists of name , address , street , locality , region , country , zip , category , description , phone , geo , and url . The following table shows all the RDF types in Web Data Commons that we mapped to each of our at-tribute. Not all attributes are present in all entities.
