 In recent years, graph kernels have received considerable in-terest within the machine learning and data mining com-munity. Here, we introduce a novel approach enabling ker-nel methods to utilize additional information hidden in the structural neighborhood of the graphs under consideration. Our novel structural cluster kernel (SCK) incorporates sim-ilarities induced by a structural clustering algorithm to im-prove state-of-the-art graph kernels. The approach taken is based on the idea that graph similarity can not only be described by the similarity between the graphs themselves, but also by the similarity they possess with respect to their structural neighborhood. We applied our novel kernel in a supervised and a semi-supervised setting to regression and classification problems on a number of real-world datasets of molecular graphs. Our results show that the structural cluster similarity information can indeed leverage the predic-tion performance of the base kernel, particularly when the dataset is structurally sparse and consequently structurally diverse. By additionally taking into account a large num-ber of unlabeled instances the performance of the structural cluster kernel can further be improved.
 H.2.8 [ Database Applications ]: Data Mining Algorithms, Experimentation, Performance Cluster Kernels, Graph Kernels, Structural Graph Cluster-ing, Cheminformatics, QSAR
The topic of graph similarity and in particular kernel ap-proaches have attracted considerable interest in recent years [13, 18, 20, 25]. To determine the similarity of two graphs, most approaches decompose the graphs in different ways: ei-ther into a potentially very large set of smaller subgraphs or related graph features, or into one or more larger common subgraphs (connected or disconnected). In this paper, we in-vestigate the question whether the structural neighborhood of two graphs can also contribute to similarity searches and consequently to improve prediction performance. In our set-ting, the structural neighborhood of a graph is determined by a recently proposed structural graph clustering approach called PSCG [21, 22].

In the work presented here, we propose a novel kernel called structural cluster kernel (SCK) which, in addition to existing kernel approaches, measures the similarity between two graphs, by their assignment to structural clusters found with PSCG. Our approach first employs the structural clus-tering algorithm to determine small, structurally homoge-neous regions in the input space, and then uses the pair-wise similarities between these regions to define a similarity measure for graphs. The approach taken here is to extend two state-of-the-art graph kernels using this structural dis-tance measure: the weighted decomposition kernel (WDK) [18] and the neighborhood subgraph pairwise distance kernel (NSPDK) [10].

To study the effectiveness of the SCK, we measured the prediction performance in the regression and classification setting, by employing several real-world datasets of molecu-lar graphs within our experiments. To show the advantage of combining graph similarity and structural cluster similarity, we compare our approach with the base kernels using graph similarity alone. Furthermore, we compare the SCK to a dif-ferent approach also employing structural clustering during model construction. We also investigate the performance of the SCK approach in the semi-supervised setting, where the base kernel is deformed by a cluster kernel encoding similar-ities between both labeled and unlabeled examples.

This paper is organized as follows: After discussing related work in Section 2, we introduce our proposed structural clus-ter kernel in Section 3. Section 4 presents and discusses our experimental results, before we conclude in Section 5.
The idea of combining kernels to improve prediction per-formance has attracted attention recently. Several types of cluster kernels, relying on different clustering algorithms, have been proposed by Chapelle et al. [6]. The authors present a general framework for constructing cluster ker-nels which implements the cluster assumption, i.e., the in-duced distance depends on whether the points are in the same cluster or not. Weston et al. [26] investigated the use of cluster kernels for protein classification by developing two simple and scalable methods for modifying a base ker-nel. The neighborhood kernel uses averaging over a neigh-borhood of sequences defined by a local sequence similarity measure, and the bagged kernel uses bagged clustering of the full sequence dataset to modify the base kernel. In both the semi-supervised and transductive settings, these tech-niques greatly improve the classification performance when used with mismatch string kernels. In work by Bodo and Csato [3] a kernel construction algorithm for supervised and semi-supervised learning was proposed, which constitutes a general framework for semi-supervised kernel construction. The technique clusters the labeled and unlabeled data by an agglomerative clustering technique, and uses the linkage distances induced by the clustering hierarchy to construct the kernel. Bodo and Csato [4] proposed two cluster kernel methods for semi-supervised learning which can be used for different types of datasets: one using hierarchical clustering, and another kernel for reweighting an arbitrary base kernel taking into account the cluster structure of the data.
Similar to Weston et al. [26] and Bodo and Csato [4], the cluster kernel proposed in this paper leverages information of a clustering algorithm to modify a base kernel. However, our approach differs from existing work in several respects. First, our structural cluster kernel can be applied in the domain of graphs. Second, it builds on two state-of-the-art graph kernels and a recently proposed structural graph clustering algorithm to determine small, structurally homo-geneous neighborhoods of the input space. The pairwise similarities between these neighborhoods are used to define a similarity measure for graphs which in turn is used to im-prove a base kernel. Third, the proposed cluster kernel can be used for both graph classification and regression, whereas the above mentioned cluster kernels were only tested on clas-sification tasks.
Parallel Structural Clustering of Graphs (PSCG) [21, 22] investigates the problem of finding groups of graphs shar-ing some structural similarity. Graphs with similar struc-tures are expected to be in the same cluster provided that their common substructures match to a satisfactory extent. The common substructure of a cluster can be considered as a scaffold present in all cluster members. Only connected substructures are considered as common substructures. The sizes of these common substructures are used as a measure of similarity between the graphs. A graph is assigned to a cluster provided that there exists at least one common sub-structure whose size is equal or greater than a user-defined threshold. In this way, a graph can simultaneously belong to multiple clusters (overlapping clustering) if the size of at least one common substructure with these clusters is equal or greater than the defined threshold. If a graph does not meet the threshold to share a common substructure with any cluster, the graph is not included in any cluster (non-exhaustive clustering). For one graph after the other, it is decided whether it belongs to an existing cluster or whether a new cluster should be created. Formally, the problem of structural clustering is defined as follows: given a set of graphs, X = { x 1 ,...,x n } , we assign each graph x cluster C j , such that the similarity between graphs is based on their structural similarity, including multiple, or overlap-ping cluster assignments. In graph clustering, one objective considered is to maximize the average number of graphs con-tained in a cluster, such that for each cluster C j there exists at least one common substructure that makes up a specific proportion,  X  , of the size of each cluster member. Consider-ing the state of a cluster C = { x 1 ,...,x m } 1 at any point in time, the criterion can formally be defined as: where cs determines all common subgraphs of a set of graphs, and  X   X  [0 , 1] is a user-defined similarity coefficient. If a new graph x m +1 is to be tested for inclusion in cluster C , we can thus infer a minimum size threshold for the substructures shared by this graph and the graphs in the cluster: where  X   X  [0 , 1] and x max is the largest graph in the cluster. To obtain meaningful and interpretable results, the mini-mum size of a graph considered for cluster membership is further constrained by a minimum graph size threshold. It excludes graphs that are too small from clustering. Thus, the identification of the cluster scaffold will not be impeded by the presence of a few graph structures whose shared com-mon substructure is much smaller than the one the major-ity of the cluster members share. For computing common substructures in graphs, we modified the graph mining al-gorithm gSpan [16, 27] that mines frequent substructures in a database of graphs satisfying a given minimum frequency constraint. For details of these modifications, the structural clustering algorithm and its performance, we refer the inter-ested reader to the original publications [21, 22].
The basic idea of the Weighted Decomposition Kernel (WDK) [18] is to focus on relatively small parts of a struc-ture, called selectors , that are matched according to an equal-ity predicate. The importance of the match is then weighted by a factor that depends on the similarity of the context in which the matched selectors occur.

More formally, a weighted decomposition kernel is charac-terized by a decomposition R ( s,z,x ) where s is a subgraph of x called the selector and z is a subgraph of x called the context of occurrence of s in x . This setting results in the following general form of the kernel:
K ( x,x 0 ) = X where  X  is a kernel on contexts and  X  is the exact matching kernel applied to selectors.

In this paper, selectors are single atoms and the matching kernel  X  ( s,s 0 ) is defined by the coincidence between the type of s and s 0 . The context kernel k is based on a soft match between substructures, defined by the distributions of label
In slight abuse of notation, we use the same indices as above. contents after discarding topology. In this paper, we use the following attributes labeling vertices and edges: atom type, atom charge and bond type. Contexts are formed as follows: Given a vertex v  X  V and an integer r  X  0, called the context radius. We denote by x ( v,r ) the substructure of x composed of the vertices within distance r from vertex v , and the set of all edges that have at least one end in the vertex set of x ( v,r ). More formally, we define the decomposition relation depending on r as R r = { ( s,z,x ) : x  X  X,s = { v } ,z = x ( v,r ) ,v  X  V } , where s is the selector and z is the context for vertex v . In our case, the matching kernel  X  ( v,v 0 1 if the two vertices v and v 0 have the same label.
The Neighborhood Subgraph Pairwise Distance Kernel (NSPDK) [10] is based on exact matching between pairs of small subgraphs. Formally, let R r,d ( A v ,B u ; G ) denote the relation between two rooted graphs A v , B u and a graph G to be true iff both A v and B u are in { N v r : v  X  V ( G ) } , where A v ( B u ) is isomorphic to some N r and D ( u,v ) = d . In words: the relation R r,d selects all pairs of neighborhood graphs of radius r whose roots are at distance d in a given graph G .

We define  X  r,d over G  X  G as the decomposition kernel on the relation R r,d , i.e., where  X  is the exact matching kernel. In words:  X  r,d counts the number of identical pairs of neighboring graphs of radius r at distance d between two graphs.
 The NSPDK is finally defined as:
In this work we impose an upper bound on the radius and is, NSPDK is limited to the sum of the  X  r,d kernels for all increasing values of the radius (distance) parameter up to a maximum given value r  X  ( d  X  ).
In this section, we introduce a novel kernel, called struc-tural cluster kernel, that leverages information of a cluster-ing algorithm to improve a base kernel representation. The main idea is to change the similarity metric of a base kernel so that the relative similarity between two points is higher if the points are in the same cluster. Our kernel uses a com-bination of two similarity measures: (1) a base kernel that computes structural similarity between pairs of graphs and (2) a cluster based similarity measure that describes how close examples are to each other in terms of the similarities between the clusters they belong to. The similarity between two clusters is computed by taking the average of the simi-larities between the cluster instances. In our application to molecule regression and classification, we use the WDK and NSPDK (see Section 3.2 and 3.3) as the base kernel. For the cluster based kernel, we use the structural clustering al-gorithm introduced in section 3.1 that clusters a dataset of graphs based on structural similarity. The cluster similarity information is used to improve pointwise similarities, based on which we construct the final kernel.

In the following, we describe the steps which are neces-sary to build the structural cluster kernel. Let D { ( x 1 ,y 1 ) ,..., ( x t ,y t ) } denote the training data, where x X represent the data points and y i their labels, respectively. Further, let D T st = { x t +1 ,...,x n } denote the set of test points. We first cluster the training set with the structural clustering procedure PSCG presented in Section 3.1. The resulting clusters are used to build a kernel representing the pairwise similarities between all clusters. In this kernel rep-resentation, each of the pairwise sets of the structural clus-ters is seen as a single data point, and a higher level kernel is designed so as to compare the two clusters. The similar-ity between two clusters is computed by taking the average of the sum of the pairwise similarities between all graph in-stances in both clusters. The kernel K ( C i ,C j ) is defined as
K ( C i ,C j ) = where K b ( x k ,x l ) represents the base kernel and C i { C 1 ,...,C p } . As mentioned earlier, we use the WDK and NSPDK as base kernel to compute the pairwise similarities between graphs. In the next step, we build a kernel repre-sentation K Cl ( x i ,x j ) based on the averaged pairwise simi-larities between the clusters x i and x j belong to. K Cl is defined as K where n x i denotes the number of clusters containing x i n j denotes the number of clusters containing x j and C k ,C { C 1 ,...,C p } . Thus, we map the points to a feature space where the pointwise similarities are equal to the cluster simi-larities in the input space. The points belonging to the same cluster will result in matrix entries close to one, whereas for the points from different clusters, the entries will be close to zero. Figure 1 illustrates the cluster kernel concept.
The cluster similarity weights K Cl ( x i ,x j ) are combined with the values of the base kernel K b ( x i ,x j ), thus forming the final kernel matrix. To sum up, the new structural clus-ter kernel is
We are faced with two problems in the construction of the above structural cluster kernel: (i) the base kernel matrix has to be positive semi-definite and (ii) the structural cluster kernel must be positive semi-definite. The first requirement is obvious, since we use the WDK and NSPDK as base ker-nels, which are known to be valid kernels. In the following, we provide a proof sketch to show that the structural cluster kernel is a valid kernel.

K Cl is a valid kernel, since each kernel value K Cl ( x i contains the average sum of pairwise similarities between all Algorithm 1 Structural Cluster Kernel
Given: training points D T rg = { ( x 1 ,y 1 ) ,..., ( x a) Cluster training points using PSCG [22, 21] c) Build cluster matrix on the training set e) Compute cluster assignments for all test points clusters, which in turn encompass the average sum of all training instances x i  X  X  x 1 ,...,x t } .

For each pair of clusters, we define one kernel that returns the average similarity between the two clusters for the first instance in cluster one and the second in cluster two. For all other instances, it returns zero. As the sum of two valid kernels is again a valid kernel, the resulting function is a valid kernel as well. Applying the kernel to two instances, we only consider the clusters to which the two instances are assigned, consequently most of the summands are equal to zero:
K Cl ( x i ,x j ) = 1 | n where n x i denotes the number of clusters containing x and m x i denotes the number of clusters not containing x
In kernel methods, for predicting the label of a new test point we need to perform kernel function calculations only between the test points and the training points. For com-puting the kernel entries, we first need to assign each test point to one or more clusters using the structural cluster-ing procedure to compute K Cl ( x i ,x t ). Based on this clus-ter assignment the similarity K Cl ( x i ,x t ) between the test point x t and all training points x i is computed by averag-ing the pairwise similarities between all clusters x t and x are assigned to (Equation 7). The kernel matrix K SC is ex-tended by taking the inner product between K Cl ( x i ,x t K ( x i ,x t ) between each test point x t and all training points x , i = 1 ,...,t .

The steps needed for the calculation of the structural clus-ter kernel are shown in Algorithm 1.
In semi-supervised learning, one tries to improve a classi-fier trained on labeled data by exploiting a relatively large Figure 1: Illustration of the cluster kernel concept. The cluster based similarity K Cl ( x 1 ,x 2 ) between the highlighted structures x 1 and x 2 is computed based on the averaged pairwise similarities between the clusters they belong to. x 1 belongs to C 1 and C 2 , x 2 to C 2 and C 3 . Thus, we need to compute the pairwise similarities between the cluster instances of cluster C
C 2 , C 1 C 3 , C 2 C 2 (which equals 1) and C 2 C 3 . set of unlabeled data. If unlabeled data is added to the relatively small labeled dataset, we expect that the new similarity, obtained via structural clustering and the use of unlabeled data, induces a better representational space for classification and regression than using only the labeled data. Therefore, we extend on the kernel construction in Section 3.4 by involving a large number of unlabeled data. The structural cluster kernel is constructed as follows: We first cluster both the labeled and unlabeled training data with the structural clustering procedure to determine small, structurally homogeneous neighborhoods of the input space. The resulting clusters are then used to build a kernel repre-senting the pairwise similarities between all clusters. As in the supervised setting, the cluster similarity information is used to improve pointwise similarities between the labeled data samples, based on which the final structural cluster kernel is constructed.
In this section, we first study the performance of our proposed structural cluster kernel in a supervised setting. Next, we investigate the cluster kernel approach in a semi-supervised setting to test if the prediction performance can be improved by including a large amount of unlabeled data. For all experiments, we employed the chemical domain as our application area by using real datasets of molecular graphs. In Table 1 an overview of the datasets is provided. Table 1: Overview of the datasets used for assessing our structural cluster kernel. n denotes the number of molecular graphs in the respective dataset.

In this section, we empirically compare the performance of our structural cluster kernel approach against five methods. 1. WDK : The Weighted Decomposition Kernel is used 2. NSPDK : The Neighborhood Subgraph Pairwise Dis-3. LoMoGraph : LoMoGraph [5] combines clustering and 4. LoMoGraph WDK : The method combines LoMo-5. LoMoGraph NSPDK : The method combines Lo-
We investigated our structural cluster kernel approach us-ing both NSPDK and WDK as base kernel. For SCK with NSPDK and SCK with WDK, we investigated not only the approach with the diagonal elements in the kernel matrices K ( C i ,C j ) (Equation 6) and K Cl ( x i ,x j ) (Equation 7) set to one, but also a second approach, where the diagonal el-ements are computed in the same way as the non-diagonal elements. We refer to these four approaches as SCK NSPDK (d=1), SCK NSPDK (d 6 =1), SCK WDK (d=1) and SCK WDK (d 6 =1).

In the experiments, regression and classification were per-formed using the Support Vector Machine (SVM) algorithm. Several user parameters were optimized by internal cross-validation. For SCK WDK, SCK NSPDK, WDK, NSPDK, LoMoGraph WDK, and LoMoGraph NSPDK, the trade-off between training error and margin, C , was selected from radius r for WDK in { 1 , 2 , 3 , 4 } . The parameter combina-tion resulting in the lowest mean absolute error (the highest accuracy) was then used for building the final model. All other SVM parameters were left at their default values. For NSPDK, the maximum radius r  X  was set to 2, and the max-imum distance d  X  to 5. For the SCK approaches, we set the similarity coefficient  X  of PSCG to 0.5. For FDAMDD and NCI AIDS we made an exception and set  X  to 0.3 to take into account the size and structural heterogeneity of the datasets. As for LoMoGraph, the parameters that were used for clustering were defined based on a set of criteria: the similarity coefficient of PSCG was chosen such that the local models consist of minimally 5% and maximally 20% of the training data. The rationale behind this choice is that a too small value of  X  results in large, heterogeneous clusters whereas a too big value of  X  produces very few, small clusters or no clusters at all. In both cases the predictivity of LoMo-Graph would be negatively affected. For the experiments on SCK, we used the same values for  X  for all datasets. Another parameter called minimum cluster size controls how many graphs a cluster must have at least so that a local model can be learned. This parameter was chosen greater than or equal to 20 as a lower bound for the number of graphs that are needed to train meaningful models.

Performance estimates are obtained using 100 times hold-out validation with a training set fraction of 66%. This means that 2 / 3 of the data are used for training a model while the remaining 1 / 3 is reserved for testing. To quantify predictive accuracy, we choose the relative mean error (re-gression) and classification accuracy (classification), which are standard measures in regression and classification set-tings. The Wilcoxon signed-rank test and the corrected re-sampled t-test [19] are applied to test for significant differ-ences at a significance level of 5%.

Tables 2, 3, 4 and 5 show the detailed experimental re-sults in terms of relative mean absolute error (regression) and accuracy (classification) for the various methods on all datasets. The results for LoMoGraph are taken from the original publication [5]. Since not all datasets were used in this paper, the table contains missing values. In the same ta-bles the second column shows the performance of the respec-tive SCK method as baseline to compare against. For better illustration we highlight the reference method in italic. We indicate whether the respective SCK method is significantly better or worse than the comparison methods at p &lt; 0 . 05 us-ing both the Wilcoxon signed-ranked test and the corrected resampled t-test. In the following, we discuss the results based on the more conservative corrected resampled t-test. Overall, our experimental results show that the structural cluster kernel with NSPDK as base kernel performs always better than all comparison methods using WDK as base kernel. This demonstrates that NSPDK is a much more powerful base kernel compared to WDK. Moreover, we ob-serve that the choice of setting the diagonal entries in the kernel matrix has a different effect on both SCK methods. Whereas setting the diagonal entries of the kernel matrix un-equal to one leads to better predictive performance for SCK NSPDK, setting the diagonal entries equal to one results in better predictive performance for SCK WDK. In the follow-ing, we analyze the performance of the SCK approaches on the different datasets. On the COX2 datasets, we observe no performance improvement of SCK NSPDK and SCK WDK over the respective base kernel. The datasets contain ex-tremely similar molecules, often differing in only one atom. Hence, the base kernel cannot be improved by the similar-ities induced by the structural clustering procedure. For the CPD, ISS and Biodeg datasets, a comparison between the mean absolute errors shows a clear performance advan-tage of SCK using both WDK and NSPDK as base kernel. Primarily, we explain this positive effect as a result of the structurally heterogeneity of the datasets consisting of many small molecules (  X  up to 10 atoms). Hence, the NSPDK alone is not suited to determine similarity between graphs. As a consequence, for these datasets the pairwise similarities between the small, structurally homogeneous neighborhoods can contribute to similarity and consequently to predictive performance. On FDAMDD, the proposed structural cluster kernel with NSPDK as base kernel yields performance degra-dation compared to NSPDK. This shows that taking into ac-count the similarities induced by PSCG has an adverse effect on the predictive performance. Although for this dataset a significant performance gain of SCK over the base kernel can be achieved by using WDK as base kernel, SCK WDK still has a higher mean absolute error compared to NSPDK. This demonstrates that NSPDK is much more powerful com-pared to WDK. For NCI AIDS and both CYP datasets the results on classification are clearly in favor of SCK NSPDK. On these datasets the structural cluster kernel with NSPDK improves over all other compared methods. However, for the corrected resampled t-test only four of the nine wins are statistically significant. Using WDK as base kernel, SCK can only achieve strong performance improvements on NCI AIDS. On the remaining classification datasets taking into account similarities induced by PSCG has either no sig-nificant effect or an adverse effect on predictive accuracy compared to the baseline methods (except for LoMoGraph on the Fontaine dataset). In summary, our structural clus-ter kernel approach is comparative to other methods, yet shows a strong performance increase on structurally more sparse datasets, i.e., chemically and structurally more di-verse datasets. On these datasets the base kernel alone is not suited to determine similarities between graphs due to the high structural heterogeneity within the dataset. Hence, the structural neighborhood of two graphs can substantially contribute to graph similarity and therefore to predictive performance of the constructed models.
In this section, we investigate whether incorporating un-labeled data in the clustering process can positively con-tribute to predictive performance. Since semi-supervised methods potentially give the greatest benefit when a large amount of unlabeled data is used, we tested our structural cluster kernel approach in large-scale experiments, enriching the training data by a large number of molecules from the vast chemical space. For this, we employed the ChemDB database, which contains nearly 5 M commercially available small molecules [7, 8], as a source of unlabeled data, ran-domly sampling 100,000 structures from it. Since in the su-pervised setting, SCK NSPDK (d 6 =1) performs always bet-ter than or equal to all methods using WDK as base kernel as well as to SCK NSPDK (d=1), LoMoGraph NSPDK and LoMoGraph, we only compared SCK in the semi-supervised setting against SCK NSPDK (d 6 =1) and NSPDK. As in the supervised setting, the SVM complexity constant, C , was selected from { 10  X  3 , 10  X  2 , 10  X  1 , 10 0 , 10 1 , 10 used the same parameter setting for the NSPDK and the similarity coefficient  X  .
The experimental results are shown in Tables 6 and 7 and in the bar charts in Figure 2. For completeness, the bar charts also depict the results for LoMoGraph NSPDK and LoMoGraph. The following discussion is based on the cor-rected resampled t-test. The results show that in the semi-supervised setting, SCK NSPDK achieves a strong perfor-mance gain on all datasets over the supervised approach: 10 of the 18 wins are statistically significant. For regression, the best results can be achieved on the toxicity datasets consist-ing of structurally more heterogeneous graphs. The results indicate that incorporating a large set of unlabeled data into the structural clustering process has a definite positive ef-fect on the predictive performance. As opposed to the super-vised setting, SCK NSPDK can improve over the base kernel on the FDAMDD dataset. This dataset is the largest one, comprising structurally heterogeneous molecules. Hence, ex-ploiting a large set of unlabeled data in the clustering step can contribute to graph similarity. The strongest perfor-mance gains with respect to NSPDK can be achieved on the classification datasets. Whereas in the supervised setting SCK NSPDK was not able to gain significantly with respect to the base kernel on the classification datasets, the semi-supervised approach shows significant improvements over NSPDK in three out of five cases.
 Table 6: Mean absolute errors with standard devi-ations of SCK NSPDK in both the semi-supervised and supervised setting and NSPDK on the classifi-cation datasets. Statistically significant results are reported using the Wilcoxon signed-rank test and the corrected resampled t-test (separated by a  X  |  X ).
In the work presented here, we proposed a novel graph ker-nel approach that incorporates similarity information based on structural graph clustering [21, 22] to improve state-of-the-art graph kernels. The proposed kernel is based on the idea that graph similarity can not only be determined by the similarity of the graphs alone, i.e., their structure, but also by the similarity of the graphs X  structural neighborhood. We investigated the performance of the structural cluster ker-nels for regression and classification by using several real-Table 7: Classification accuracies with standard deviations of SCK NSPDK in both the semi-supervised and supervised setting and NSPDK on the classification datasets. Statistically significant results are reported using the Wilcoxon signed-rank test and the corrected resampled t-test (separated by a  X  |  X ).
 Figure 2: a) Mean absolute errors with 95% confi-dence intervals and b) classification accuracies with 95% confidence intervals on the different comparison methods for the data sets in Table 1. world datasets of molecular graphs. In our experiments we performed a comparison with the weighted decomposition kernel, the neighborhood subgraph pairwise distance ker-nel, and a learning method combining clustering with clas-sification or regression for the prediction task. The results demonstrate that the proposed kernel approach yields an in-crease in performance on a number of datasets, in particular on structurally more diverse datasets. We also investigated the performance of our approach in the semi-supervised set-ting, by enriching relatively small labeled datasets by a large set of unlabeled data instances from the vast chemical space. The results show that within the semi-supervised setting our approach achieves gains in performance when compared to the supervised version as well as to the pure base kernel, in particular for classification. We believe that the approach presented is general as such, and can also be employed in conjunction with a variety of different kernels and clustering approaches and is therefore not restricted to graph mining alone. [1] Environmental Toxicity Prediction Challenge, [2] R. Benigni, C. Bossa, and M. Vari. Chemical [3] Z. Bodo. Hierarchical cluster kernels for supervised [4] Z. Bodo and L. Csato. Hierarchical and reweighting [5] F. Buchwald, T. Girschick, M. Seeland, and [6] O. Chapelle, J. Weston, and B. Sch  X  olkopf. Cluster [7] J. Chen, S. J. Swamidass, Y. Dou, and P. Baldi. [8] J. H. Chen, E. Linstead, S. J. Swamidass, D. Wang, [9] J. M. Collins. The DTP AIDS Antiviral Screen [10] F. Costa and K. De Grave. Fast neighborhood [11] S. Dzeroski, H. Blockeel, B. Kompare, S. Kramer, [12] F. Fontaine, M. Pastor, I. Zamora, and F. Sanz. [13] T. G  X  artner. Kernels for Structured Data . PhD thesis, [14] L. Gold, T. Slone, B. Ames, N. Manley, G. Garfinkel, [15] C. Helma, T. Cramer, S. Kramer, and L. De Raedt. [16] K. Jahn and S. Kramer. Optimizing gSpan for [17] E. Matthews, N. Kruhlak, R. Benz, and J. Contrera. [18] S. Menchetti, F. Costa, and P. Frasconi. Weighted [19] C. Nadeau and Y. Bengio. Inference for the [20] U. R  X  uckert, T. Girschick, F. Buchwald, and [21] M. Seeland, S. A. Berger, A. Stamatakis, and [22] M. Seeland, T. Girschick, F. Buchwald, and [23] J. J. Sutherland, L. A. O X  X rien, and D. F. Weaver. [24] J. J. Sutherland, L. A. O X  X rien, and D. F. Weaver. A [25] S. V. N. Vishwanathan, N. N. Schraudolph, [26] J. Weston, C. Leslie, E. Ie, D. Zhou, A. Elisseeff, and [27] X. Yan and J. Han. gSpan: Graph-based substructure [28] C. W. Yap and Y. Z. Chen. Prediction of cytochrome
