 Tina Eliassi-Rad  X  Christos Faloutsos F We address the problem of classification in partially labeled net-works (a.k.a. within-network classification) where observed class labels are sparse. Techniques for statistical relational learning have been shown to perform well on network classification tasks by ex-ploiting dependencies between class labels of neighboring nodes. However, relational classifiers can fail when unlabeled nodes have too few labeled neighbors to support learning (during training phase) and/or inference (during testing phase). This situation arises in real-world problems when observed labels are sparse.

In this paper, we propose a novel approach to within-network classification that combines aspects of statistical relational learn-ing and semi-supervised learning to improve classification perfor-mance in sparse networks. Our approach works by adding  X  X host edges X  to a network, which enable the flow of information from labeled to unlabeled nodes. Through experiments on real-world data sets, we demonstrate that our approach performs well across a range of conditions where existing approaches, such as collective classification and semi-supervised learning, fail. On all tasks, our approach improves area under the ROC curve (AUC) by up to 15 points over existing approaches. Furthermore, we demonstrate that our approach runs in time proportional to L  X  E , where L is the number of labeled nodes and E is the number of edges.
 H.2.8 [ Database Management ]: Database Applications -Data Min-ing; I.2.6 [ Artificial Intelligence ]: Learning; I.5.1 [ Pattern Recog-nition ]: Models -Statistical Algorithms, Design, Performance, Experimentation.
 Statistical relational learning, semi-supervised learning, collective classification, random walk.
We address the problem of within-network classification in sparsely labeled networks. Given a network of both labeled and unlabeled nodes, our goal is to provide labels for the unlabeled nodes. Here, labeling simply means assigning each node a class from among a set of possible classes (see Figure 1). Figure 1: Problem definition and our solution. We address the problem of label sparsity in network classification by creating  X  X host edges X  using random walks with restarts. These ghost edges connect an unlabeled node to the most relevant labeled nodes.

Suppose we want to identify fraudulent users of a cell phone network. In this case, our set of possible classes for users is {fraud, legit}. If a user is known to be involved in fraud, we assign a label of  X  X raud." If a user is known to not be involved in fraud, we assign a label of  X  X egit." Otherwise, we leave the node unlabeled. Our goal is then to assign labels ( X  X raud" or  X  X egit") to the unlabeled users.
Cell phone fraud is an example where networks are often very sparsely labeled. We have a handful of known fraudsters and le-gitimate users, but the labels are unknown for the vast majority of users. For such applications, it is reasonable to expect that we have access to labels for fewer than 10% , 5% , or even 1% of the nodes.
In addition to being sparsely labeled, cell phone networks are generally anonymized. That is, the nodes often contain no attributes besides class labels. These sparsely labeled, anonymized networks are the focus of this study. Put another way, our work focuses on univariate within-network classification in sparsely labeled net-works .

Techniques for statistical relational learning (SRL) have been shown to perform well on network classification tasks because of their ability to exploit dependencies between labels of related nodes [18]. These techniques use labeled data to learn a model of the dependencies between labels of neighboring nodes. The labels of unlabeled nodes can then be inferred by propagating informa-tion from labeled nodes throughout the network, according to this learned dependency model (i.e., collective classification). Unfortu-nately, sparse labels cause a number of problems for relational clas-sifiers. First, collective classification techniques can fail without sufficiently many labels to seed the inference process [15]. Second, fewer labeled nodes means fewer training examples from which to learn dependencies. Finally, even when a node is labeled, many of its neighbors will not be. This is equivalent to having many miss-ing attribute values in a traditional supervised learning setting. The result is that it is very difficult to learn an accurate model of the dependencies present in the data.

Within-network classification can also be viewed as a graph-based semi-supervised learning problem since we have both la-beled and unlabeled data available at training time. Although semi-supervised learning (SSL) is generally applied to non-network data, graph-based SSL approaches can be applied to within-network clas-sification, as we show. SSL techniques have the advantage that they do not rely as heavily on labeled training data and can make use of unlabeled data as well. However, graph-based SSL approaches generally do not learn dependencies from data, but instead assume local label consistency (i.e., that nearby points tend to have the same label). If the label consistency assumption is not met, SSL techniques can perform extremely poorly. Figure 2 provides a pre-view of our results. It plots classifier performance (measured by area under the ROC curve, a.k.a. AUC) versus data set ID. The data sets are ordered according to their local consistency (a.k.a. ho-mophily score). 1 Notice that the two proposed ghostEdge methods are consistently at the top, while the competing methods may per-form very poorly depending on the data set X  X  local consistency.
In this paper, we explore a novel approach to within-network classification that capitalizes on the strengths of both statistical re-lational learning (SRL) and semi-supervised learning (SSL), to pro-duce a classifier that is robust in the face of both sparse labeling and low label consistency. Our contributions are as follows:
The rest of the paper is organized as follows. In Section 2, we review related work. We present our proposed methods in Sec-tion 3. Sections 4 and 5 describe our experimental methodology and results. Finally, we offer conclusions in Section 6.
Note that throughout this paper, we use the terms  X  X omophily X ,  X  X ositive auto-correlation X ,  X  X ocal consistency X  and  X  X moothness X  interchangeably. We also use the terms  X  X nverse relationship X ,  X  X eg-ative auto-correlation X  and  X  X ack of homophily X  interchangeably. Figure 2: A preview of our results: classification performance of various approaches when half of the nodes in a graph data set are unlabeled. The solid blue (square icon) and red (cir-cle icon) lines on the top represent our ghostEdge approaches, which perform consistently well regardless of the degree of lo-cal consistency in the data set.
In recent years, there has been a great deal of work on models for learning and inference in relational data [7, 10, 11, 16, 18]. For within-network classification tasks where we have sparse labels, we categorize the previous work into two main groups: collective classification and graph-based semi-supervised learning.

Collective classification. Collective classification deals with la-bel sparsity by simultaneously labelling a set of related nodes, al-lowing estimates of neighboring labels to influence one another. Representative work in this line started with the seminal paper of Chakrabarti et al. on using hyperlinks to for hypertext classification [2]. Sen et al. [17] provide a careful empirical study of the various procedures for collective inference. Macskassy and Provost [12] provide a nice case-study of previous work in learning attributes of networked data. McDowell et al. [14] demonstrate that  X  X au-tious X  collective classification procedures produce better classifi-cation performance than  X  X ggressive" ones. They recommend only propagating information about the top-k most confidently predicted labels. One of the major advantages of collective classification lies in its powerful ability to learn various kinds of dependency struc-tures (positive vs. negative auto-correlation, different degrees of correlation, etc). However, as pointed out in [15], when the la-beled data is very sparse, which is quite common in the sparsely labeled networks that we are particularly interested in, the perfor-mance of collective classification might be largely degraded due to the lack of sufficient neighbors. This is exactly one major advan-tage of the proposed method, -we incorporate informative  X  X host edges X  into the networks to deal with sparsity issues. From this point of view, our method shares the similar spirit as the work by Macskassy [13]. However, in [13], the additional edges are calcu-lated based on attribute-similarity (specifically, text similarity). If such information is not available (which is quite common in many real applications and which is the case we are interested in), the method in [13] is not applicable. On the other hand, we can always leverage our  X  X host edges X  since they are based on the intrinsic structure of the networks. Lastly, the algorithm proposed in [13] does not learn the weights, instead it combines the weights through a heuristic.

Graph-based semi-supervised learning. The problem of within-network classification can also be thought of as the graph-based semi-supervised learning problem. Here, the basic idea is to es-timate a function on the graph which satisfies two kinds of con-strains: (1) the consistency with the label information and (2) the smoothness over the whole graph. The methods in this area mainly vary in the different ways to balance these two constraints. For ex-ample, Zhu X  X  Gaussian random field (GRF) method [23] puts a hard constraint on the label consistency and then achieves the smooth-ness by the harmonic function. Zhou X  X  global and local consistency method [21] combines the two kinds constraints by a regularization parameter and solves a quadratic optimization problem. For more on graph-based semi-supervised learning, we refer the reader to an excellent survey by Zhu [22]. By exploring the global structure (i.e. smoothness) over the whole graph, graph-based semi-supervised learning methods usually outperform the traditional methods, par-ticularly when there are very few labeled nodes in the networks. However, the constraint on smoothness implicitly assumes positive auto-correlation in the graph, that is nearby nodes tend to share the same class labels (i.e., homophily). When such an underlying assumption does not hold (negative auto-correlation, the degree of auto-correlation being small, etc), the performance might be largely affected. This is another advantage of our  X  X host edge X  method, -it leverages the additional learning stage to recover the intrinsic cor-relation structure.
In this section, we explain the motivation behind using ghost edges for within-network classification and discuss how ghost edges are created. We then describe how we take advantage of ghost edges to improve classification performance.
The power of statistical relational learning (SRL) lies in the fact that networks generally exhibit predictable relationships between class labels of related nodes. Therefore, labeled nodes provide a great deal of information about their unlabeled neighbors. Suppose we have an unlabeled node, u , and we have a good understanding of the relationship between the class label of u and the class labels of u  X  X  neighbors, N . If all nodes in N are labeled, we should be able to do a very good job of predicting the label of u . However, suppose that very few nodes in N are labeled. There are two ways of looking at this problem: 1. Node u shares edges with plenty of other nodes in the net-2. There are plenty of labeled nodes in the network, but node u The first way of looking at the problem is addressed by techniques such as collective classification. Our approach, based on ghost edges , addresses the second way of looking at the problem.
The idea behind our approach is to add  X  X host edges" between labeled nodes and unlabeled nodes to allow the information from labeled nodes to inform our predictions on unlabeled nodes. Of course, in order for this to work, we must carefully select pairs of nodes to connect via ghost edges. In particular, the success of our approach relies on our ability to choose pairs of nodes with labels that relate to each other in a predictable way. Our conjecture is that nodes which are  X  X loser" in a network will tend to have more predictable relationships between their class labels, and that nodes which are directly connected by an observed edge are simply a spe-cial case of this.

Based on this conjecture, we create ghost edges as follows. We create a single ghost edge between every  X  labeled, unlabeled  X  pair of nodes in our graph. We then assign a weight to each ghost edge based on the proximity of the nodes that the edge connects. A higher weight indicates that the connected nodes are closer together in the network (i.e., have higher proximity). The following subsec-tion describes our approach to quantifying node proximity.
To calculate RWR scores between each pair of nodes, we use a variation on the fast random walk with restart method proposed by Tong et al. [19]. The end result is that the algorithm can quickly give the proximity score r i,j , indicating how easy it is to reach node j from node i . In more detail, r i,j gives the steady-state probability to find a particle at node j , when this particle does a random walk with restarts from node i . The score r i,j is high if there are several, high-weighted, short paths from i to j . A random walk with restarts (or, equivalently, personalized pageRank ) works as follows: a par-ticle starts at node i , moves randomly along the edges of the given graph, and with probability 1  X  c (say, c = 0 . 90 ), the particle flies back to the initial node i . For more details, see [19]. There is a subtle, but very important step in our RWR algorithm. The problem is to handle varying degrees of local label consistency, including cases where labels of neighbors are inversely related. In cases with high label consistency (i.e., nearby nodes have similar labels), all methods tend to do well. In cases where consistency is low, as in, say, a near-bipartite graph of dating relationships with mostly male-female edges, the semi-supervised learning methods will not work, exactly because they have the local consistency as-sumption hardwired in their optimization functions.

Similarly, if we add ghost edges carelessly, we will add a lot of incorrect edges. What are the right edges to add so that our method can easily handle cases with varying degrees of label consistency?
The idea is to do RWR, but to insist on even -length paths. We shall refer to it as the even-step RWR . Mathematically, this means that we replace the adjacency matrix A with its square B = A  X  A . Then, we compute the RWR scores using the B matrix.

Why does this approach work regardless of the degree of label consistency? The reasons are subtle: For the case of inverse class-label relationship, the immediate neighbors are exactly the ones we want to avoid, which is exactly what the even-step RWR does. For the case of high local consistency, social networks typically have triangles and high  X  X lustering coefficient." Thus, even if we only focus on even step paths, our random walk will still give high scores to nodes that are well-connected. In practice, we find that the even-step approach works well for intermediate consistency values as well.
 In conclusion, with the subtle technique of even-step RWR , our GhostEdge methods can handle varying degrees of local label con-sistency, as we show in the experiments section.
We propose two novel approaches to within-network classifi-cation: the ghost edge non-learning classifier ( ghostEdgeNL ) and the ghost edge learning classifier ( ghostEdgeL ). Both approaches are based on propagating class labels throughout a network using ghost edges created via random walk with restart (namely, even-step RWR). However, the two classifiers make use of ghost edges in different ways. There are two ways in which the approaches differ: 1. Use of available labels . GhostEdgeNL is a non-learning 2. Use of RWR scores . GhostEdgeNL makes use of all ghost Ghost edges can be added to any relational classifier. For Ghost-EdgeNL, we chose to use a simple relational neighbor classifier [11]. This classifier predicts the class of a node based entirely on the class labels of neighboring nodes and performs no learning. It estimates the probability of node u belonging to class c as the weighted proportion of neighboring nodes that belong to class c . GhostEdgeNL uses the proximity score on the ghost edge between nodes as a weight. GhostEdgeNL ignores observed edges. For GhostEdgeL, we chose a learning link-based classifier [10]. This classifier uses logistic regression (LR) to build a discrimina-tive model of node i  X  X  class given the class labels of nodes directly connected to i . Since LR expects a fixed-length feature vector, the set of neighboring class labels is summarized by a statistic such as the count or proportion of neighbors of each class.
 We initially implemented GhostEdgeL using a standard LR model. However, we found that LR often failed to appropriately weight features based on their predictiveness. We achieved better results using an ensemble of LR models we refer to as logForest . The logForest model is inspired by Breiman X  X  Random Forest classifier [1]. We use a bag of LR classifiers, where each is given a subset of log ( M ) + 1 of the M total features. For this study, our logForest model uses 500 LR classifiers.

GhostEdgeL divides ghost edges into six bin as follows: A con-tains ghost edges with scores in the top 3%, B gets edges scoring between the top 3%-6%, C between 6%-12%, D between 12%-25%, E between 25%-40%, and F between 40%-80%. There is no overlap between bins.

The GhostEdgeL classifier uses the following features: (1) count of neighbors of each class across observed edges and (2) count of neighbors of each class across ghost edges for each bin. So, for a binary classification problem with six bins, we have 14 features.
Like any relational learning method, GhostEdgeL learns the de-pendencies between class labels of neighboring nodes. However, in addition, the model learns how much weight to put on observed edges vs. ghost edges with different proximity scores, and the model can potentially learn different dependencies for each of these edge types.
In even-step RWR, the ranking vector ~r i = [ r i,j ] for a given labeled node i is defined as: where ~e i is the starting vector for the node i , c is the fly-out prob-ability, t is the iteration number, and A is the normalized graph Laplacian [3]. 2
We can use the following iterative strategy: For t = 2 , 3 ,..., do the following two steps:
The complexity for each step t is clearly O ( E ) . So, to get one ranking vector, the complexity is O (  X  t  X  E ) , where  X  number of iteration needed to reach the steady state. Overall, we need L such ranking vectors (so that we will get all U  X  L proximity scores). Therefore, the overall complexity is O ( L  X  E ) (omitting the constant  X  t ).

Next, we will justify that the above iterative procedure will actu-ally converge. To see this, we can re-write ~r i ( t ) as:
Since A is normalized graph Laplacian, we have  X  1  X   X  ( A )  X  1 , where  X  ( A ) is the eigenvalue of A [3]. Therefore, A bounded. On the other hand, c t  X  0 with t  X  infinity . Thus, k ( c t ) A (2 t ) ~e i k goes to 0 as t goes to infinity, which completes the proof.
Our problem setting is within-network classification in sparsely labeled networks. We compare several approaches to overcome label sparsity: (1) collective classification, (2) graph-based semi-supervised learning methods, and (3) our  X  X host edge" label prop-agation approach (ghostEdge). The experiments are designed to answer the following research questions:
We present results on four real-world data sets: political book co-purchases [9], Enron emails [4], Reality Mining cell-phone calls [5], and high-energy physics citations from arXiv (a.k.a. HEP-TH [8]). Our tasks are to identify neutral political books, Enron execu-tives, Reality Mining study participants, and HEP-TH papers with the topic  X  X ifferential Geometry, X  respectively.

Figure 3 summarizes our prediction tasks. The Sample column describes the method used to obtain our experimental subset of data ( time ), or sample a continuous subgraph via breadth-first search ( BFS ). The Task column indicates the class label we are trying to predict. The | V | , | L | , and | E | columns indicate counts of total nodes, labeled nodes, and total edges in each network, respectively. The P (+) column indicates the proportion of labeled nodes that have the class label of interest (e.g., 12% of the political books are neutral).
The normalized graph Laplacian A = I  X  D  X  1 / 2 WD  X  1 / 2 where I is the identity matrix, D is the diagonal degree matrix, and W is the weighted adjacency matrix.
Note that for the Enron and HEP-TH tasks we have labels for only a subset of nodes (which we refer to as  X  X ore X  nodes) and can only train and test our classifiers on these nodes. However, unlabeled nodes and their connections to labeled nodes may still be exploited for label propagation.
 Political Books BFS Neutral? 105 105 441 0.12 Reality Mining BFS In Study? 1000 1000 31509 0.08
From our methods, we use the two versions: (a) ghostEdgeNL which is the non-learning ghostEdge-based approach as described in Section 3.4.1 and (b) ghostEdgeL which is the learning ghostEdge-based approach described in Section 3.4.2.

The competing methods fall under two categories: (1) collective classification, and (2) graph-based semi-supervised learning meth-ods. Both categories of methods were designed to handle label sparsity.
On each classification task, we ran seven individual classifiers: 1. logForest , an ensemble logistic link-based model without 2. logForest+ICA , an ensemble logistic link-based model, which 3. wvRN , a relational neighbor model without collective clas-4. wvRN+RL , a relational neighbor model, which uses relax-5. GRF , the SSL Gaussian random field model 6. ghostEdgeNL , our ghostEdge-based classifier without learn-7. ghostEdgeL , our ghostEdge-based classifier with learning
We describe each of the competing classifiers next. logForest is an ensemble of logistic regression classifiers as de-scribed in Section 3.4.2. The model takes two features as input: the count of unique neighbors of the positive class and the count of unique neighbors of the negative class. Our base logForest classi-fier does not use collective classification. Therefore, any neighbors with missing class labels are simply ignored. logForest+ICA uses the base logForest classifier, but performs collective classification using the ICA algorithm described in Sec-tion 4.2.2. We tried the logForest classifier with both the ICA and RL collective classification algorithms across our range of classifi-cation tasks. The performances of the two algorithms were compa-rable, but ICA performed slightly better overall. This is consistent with previous results [12]. wvRN is the weighted-vote relational neighbor classifier [11]. Given a node i and a set of neighboring nodes, N , the wvRN clas-sifier calculates the probability of each class c for node n as: where N is the set nodes that neighbor n , Z = P n and w ( n,m ) is the weight on the edge between n and m . For the baseline wvRN model, w ( n,m ) is simply the number of observed edges between n and m .

Note that in cases where a node has no labeled neighbors, we will end up with P ( C i = c ) = 0 for all c . In such cases, we simply assign probabilities to each class based on priors observed in the training data. Our base wvRN classifier does not use collective classification. Therefore, any neighbors with missing class labels are simply ignored. wvRN+RL uses the base wvRN classifier, but performs collec-tive classification using the RL algorithm described in Section 4.2.2. We tried the wvRN classifier with both the ICA and RL collective classification algorithms across our range of classification tasks. The RL algorithm performed better overall. This is consistent with previous results [12].
 GRF uses the Gaussian random field approach of Zhu et al. [23]. We ported Zhu X  X  MATLAB code 3 for use in our experimental frame-work and double checked our results with the original MATLAB code. We made one small modification to Zhu X  X  original code to allow it to handle disconnected graphs. Zhu computes the graph Laplacian as L = D  X  cW , where c = 1 . We set c = 0 . 9 to ensure that L is diagonally dominant and thus invertible. We found that our change had no substantial impact on classification perfor-mance.
To perform collective classification, we use both the iterative classification algorithm (ICA) and relaxation labeling (RL) [12]. We also ran preliminary experiments using Gibbs sampling [6], which yielded results comparable to ICA. This is consistent with findings of other researchers [12, 17]. In our experiments, the log-Forest classifier performed better overall using ICA and the wvRN classifier performed better using RL. Therefore, we report results only for these combinations.

ICA initially assigns labels to unlabeled nodes, U , based on what is known in each unlabeled node X  X  local neighborhood. Nodes with no labeled neighbors are temporarily assigned a label of null . Then, until either all class labels have stabilized or a certain number of iterations have elapsed, a new label is assigned to each u based on the current label assignments of u i  X  X  neighbors. RL is similar to ICA except that instead of each u i having a current label assignment, u i has a current probability distribution on the set of labels. Thus, the uncertainty associated with a label assignment is retained until the algorithm terminates and a final label is assigned. Unlabeled nodes are initially assigned the prior distribution, ob-served in the training data. We perform simulated annealing to cat-alyze convergence.
For all results presented here, the basic experimental setup is the same. Each data set contains a set of core nodes for which we have ground truth (i.e., we know the true class labels). In all cases, classifiers have access to the entire data graph during both training and testing. However, not all of the core nodes are labeled. We vary
See http://pages.cs.wisc.edu/ jerryzhu/pub/harmonic_function.m. the proportion of labeled core nodes from 10% to 90%. Classifiers are trained on all labeled core nodes and evaluated on all unlabeled core nodes.

Our methodology is as follows. For each proportion of core nodes labeled, we run 20 trials and report the average performance. For each trial and proportion labeled, we choose a class-stratified random sample containing (1.0  X  proportion labeled)% of the core instances as the test set and the remaining core instances become the training set. Note that for proportion labeled less than 0.9 (or greater than 10 trials), this means that a single instance will neces-sarily appear in multiple test sets. The test sets cannot be made to be independent because of this overlap. However, we carefully choose the test sets to ensure that each instance in our data set occurs in the same number of test sets over the course of our experiments. This ensures that each instance carries the same weight in the overall evaluation regardless of the proportion labeled. Labels are kept on the training instances and removed from the test instances. We use identical train/test splits for each classifier.

Our experimental framework sits on top of the open source Weka system [20]. We implement our own network data representation and experimental code, which handles tasks such as splitting the data into training and test sets, labeling and unlabeling of data, and converting network fragments into a Weka-compatible form. We rely on Weka for the implementation of logistic regression and for measuring classifier performance on individual training/test trials.
We use the area under the Receiver Operating Characteristic (ROC) curve (AUC) as a performance measure to compare classifiers. We chose AUC because it is more discriminating than accuracy. In particular, most of our tasks have a hight class-skew and accuracy cannot adequately differentiate between the classifiers.
In this section, we discuss the results of our experiments. We assessed significance of the results using paired t-tests  X  0 . 05 are considered significant).
Figures 4 and 5, respectively, compare the performance of wvRN (with and without RL) to ghostEdgeNL and logForest (with and without ICA) to ghostEdgeL. We see a consistent and often dra-matic increase in performance over the baseline wvRN and log-Forest models due to the use of ghost edges. GhostEdgeNL sig-nificantly outperforms wvRN on Enron  X  0 . 7 , Political Books 0 . 3  X  0 . 7 , and HEP-TH and Reality Mining for all proportions la-beled. GhostEdgeL significantly outperforms logForest on Enron and Reality Mining  X  0 . 7 , Political Books  X  0 . 3 , and HEP-TH at all proportions labeled.

In many cases, the ghostEdge classifiers also outperform collec-tive classification. GhostEdgeNL significantly outperforms wvRN+RL on Enron  X  0 . 5 , HEP-TH  X  0 . 7 , and Reality Mining for all pro-portions labeled. GhostEdgeL significantly outperforms logFor-est+ICA on Enron  X  0 . 7 , HEP-TH at all proportions labeled, Po-litical Books  X  0 . 3 , and Reality Mining  X  0 . 5 .

Figure 2 (previewed in Section 1) compares the performance of various approaches to handle label sparsity with 50% of core nodes labeled: ghost edges, collective classification, and Gaussian ran-
It is an open issue whether the standard significance tests for com-paring classifiers (e.g., t-tests, Wilcoxon signed-rank) are applica-ble for within-network classification, where there is typically some overlap in test sets across trials. It remains to be seen whether the use of such tests produces a bias and the extent of any errors caused by such a bias. This is an important area for future study that will potentially affect a number of published results. dom fields. This figure demonstrates the robustness of the ghost-Edge methods across a range of data sets with varying degrees of local consistency among labels (see Figure 8). Both ghostEdgeNL and ghostEdgeL are consistently high performers across all tasks. All of the other methods perform poorly (i.e.,  X  30 AUC points from the top) on at least one of the data sets.

Figure 6 presents a more complete comparison of the approaches as the proportion of labeled nodes varies. Here, we see that the ghostEdge methods perform well in comparison to other approaches, regardless of the proportion of nodes labeled. For all data sets and proportions labeled, one of the ghostEdge classifiers is always the top performer (or tied at the top). We note that there are occasion-ally substantial differences in performance between ghostEdgeL and ghostEdgeNL. We present a more detailed discussion of learn-ing and non-learning methods in Section 5.2.
Figure 6 reveals a couple of interesting things about learning vs. non-learning classifiers. First, learning methods in general are hurt more than non-learning methods by a smaller proportion of labeled nodes because learning methods rely on training examples to gen-erate an accurate dependency model. Figure 7 shows the average number of training examples available for each data set at each pro-portion labeled. The Political books and HEP-TH data sets have very few training examples available at the lower proportions of labeled nodes (11 and 33, respectively at 0.1 labeled). Correspond-ingly, it is on these data sets that we see a dip in the learning meth-ods relative to the non-learning methods at lower proportions of labeled data.

The second thing to note is that the performance of both GRF and wvRN+RL on the Reality Mining task actually decreases as more labels are made available. This is because there is an inverse relationship between class labels of neighbors (see auto-correlation scores in Figure 8). So, these non-learning methods take whatever truth they are given and use it to make exactly the wrong decision. The more information they get, the worse they perform. We see this same effect in Figure 4 with wvRN. GhostEdgeNL overcomes this problem by using even-step RWR, as described in section 3.3. The relative performance of logForest+ICA increases with respect to GRF and wvRN+RL as label consistency decreases since the logForest model is able to learn dependencies among neighboring labels (Figure 2).
Figures 2 and 6 allows us to compare the performance of the collective classification approaches (i.e., wvRN+RL and logFor-est+ICA) and the GRF semi-supervised approach across data sets. On all tasks, the performance of wvRN+RL and GRF is essen-tially equivalent, although GRF does perform significantly better than wvRN+RL in terms of AUC on all data sets except HEP-TH. We do not report results for wvRN+ICA, but we found that wvRN+RL performed much better than wvRN+ICA overall. On the other hand, the logForest classifier demonstrated roughly equiv-alent performance regardless of the collective inference procedure used. These results are consistent with previous findings [12].
We focus on the problem of predicting node labels in a large graph, when (a) there are few labeled nodes and (b) local consis-tency (a.k.a. homophily) does not necessarily hold. To address the first problem, we introduce  X  X host edges X , by judiciously adding edges between nodes, according to RWR proximity. To address the second problem, we propose to bypass all the activation-spreading methods (which implicitly assume homophily), and instead we use a classifier on a carefully chosen set of features from observed, as well as  X  X host-edge X  neighbors. A subtle, but vital point is that we consider RWR not on the original matrix, but on its square. This change makes our method robust, regardless of the degree of ho-mophily. In other words, our method does well even when the local consistency assumption is not met.

We performed experiments on several real, publicly available data sets, measuring the AUC. The competitors were carefully cho-sen to be the state of the art. Our method is very robust, performing as well as or better than the best competitor across tasks. All other classifiers we evaluated perform poorly in some cases, depending on the degree of homophily. We also showed that the complex-ity of our approach is O( L  X  E ), where L is the number of labeled nodes and E is the number of edges. Therefore, the approach is suitable for large data sets, provided that known labels and edges are sufficiently sparse.
This work was performed under the auspices of the U.S. Depart-ment of Energy by Lawrence Livermore National Laboratory un-der contract DE-AC52-07NA27344 (LLNL-CONF-404625), and based upon work supported by the National Science Foundation un-der Grant No. IIS-0534205. This work is also partially supported by the Pennsylvania Infrastructure Technology Alliance (PITA), an IBM Faculty Award, a Yahoo Research Alliance Gift, with addi-tional funding from Intel, NTT and Hewlett-Packard. Any opin-ions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation, or other funding par-ties. [1] L. Breiman. Random forests. Machine Learning , 45(1):5 X 32, [2] S. Chakrabarti, B. Dom, and P. Indyk. Enhanced hypertext [3] F. Chung. Spectral graph theory. Number 92 in CBMS [4] W. W. Cohen. Enron email data set. [5] N. Eagle and A. Pentland. Reality mining: sensing complex [6] S. Geman and D. Geman. Stochastic relaxation, Gibbs [7] L. Getoor, N. Friedman, D. Koller, and B. Taskar. Learning
Av g De g ree for a Node in V [8] D. Jensen. Proximity HEP-TH database. [9] V. Krebs. Books about U.S. Politics. [10] Q. Lu and L. Getoor. Link-based classification. In ICML , [11] S. Macskassy and F. Provost. A simple relational classifier. [12] S. Macskassy and F. Provost. Classification in networked [13] S. A. Macskassy. Improving learning in networked data by [14] L. McDowell, K. M. Gupta, and D. W. Aha. Cautious [15] J. Neville and D. Jensen. Relational dependency networks. [16] J. Neville, D. Jensen, L. Friedland, and M. Hay. Learning [17] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Gallagher, and [18] B. Taskar, P. Abbeel, and D. Koller. Discriminative [19] H. Tong, C. Faloutsos, and J.-Y. Pan. Fast random walk with [20] I. H. Witten and E. Frank. Data Mining: Practical Machine [21] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [22] X. Zhu. Semi-supervised learning literature survey. Technical [23] X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised
