 We propose a ranking model to diversify answers of non-factoid questions based on an inverse notion of graph conne ctivity. By representing a collection of candidate answers as a graph, we posit that novelty, a measure of diversity, is inversely proportional to answer vertices X  connectivity. Hence, unlike the ty pical graph ranking models, which score vertices based on the d egree of connectedness, our method assigns a penalty score f or a candidate answer if it is strongly connected to other answers . That is, any redundant answers, indicated by a higher inter-sent ence similarity, will be ranked lower than those with lower inter-se ntence similarity. At the end of the ranking iterations, m any redundant answers will be moved toward the bottom on the rank ed list. The experimental results show that our method helps div ersify answer coverage of non-factoid questions according to F-sc ores from nugget pyramid evaluation. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing  X  abstracting methods Algorithm, Experimentation, Performance Answer ranking, negative voting, non-factoid questi on answering Unlike factoid questions, whose answer comprises a short text segment of 50 bytes or less, non-factoid questions are inherently more complex and require a paragraph-length answer. Generating answers for complex, non-factoid questions from a l arge sentence collection is a very challenging task. Since some a nswers contain more vital information to the questions while some answers are more trivial, it is crucial for question answering systems to present as many informative answers in the system response as possible. However, past research proposes that informativenes s is not a sole criterion for selecting answers. For instance, [7] suggest that a good question answering system should provide a mix ture of both informative and interesting answers in the system r esponse. Moreover, another important criterion is the novelt y of answers. In the process of selecting answers, some candidate an swers may contain redundant information compared to others. F or instance, the following statements  X  George Edward Foreman is the oldest boxer to win a major heavyweight title  X  and  X  Foreman won a major boxing championship at age 45  X  contain mostly the same information as  X  George Foreman becomes the oldest world champion in boxing history.  X  Thus, we approach the problem of generating answers for non-factoid questions from a diversification perspective. The focus of our work is on maximizing the informativeness and diversity of responses in a fix ed-length extracted answer list. We propose a novel sentence-level ranking model called DiverseRank for re-ranking candidate answers by their informativeness and novelty . Extracting a ranked list of informative answers for complex non-factoid questions have been a major research topic for quite some time. Many researchers have proposed methods to fin d informative answers. The earliest approach, which starts from d efinition question answering research, is based on handcrafte d lexico-syntactic patterns [4]. Apart from informativeness, other aspects of answers have been explored. For instance, Kor and C hua [7] propose a unigram language model constructed from v arious web snippets to find interesting answers. Despite impro vements in system performance, generating a list of answers fo r complex questions remains a challenging task due to the fac t that the answers do not easily fall into predictable semanti c classes. Several related works have been done in community/collabora tive question answering (CQA) domain. For example, many graph-bas ed answer ranking models have been proposed [15][5]; many of which are inspired by HITS algorithm [6]. Nevertheless, many studies in CQA domain have focused on expert finding problem i n community QA sites while answer diversification pro blem has not been addressed. Lastly, the main difference between our work and other graph-based sentence ranking models [13][16] is in voting strategy. Most graph-based ranking methods, e.g. Le xRank [13], are based on PageRank algorithm [1], which relies o n positive voting or recommendation between vertices. However, we hypothesize that novelty is inversely proportional to connectivity. Thus, our proposed method negatively votes down the score of any high-degree vertices. We propose the sentence ranking method called Diver seRank, which aims to address the aforementioned problems. The proposed method is motivated by the intuition that a good ca ndidate answer should be highly informative and novel with respect to other answer sentences. Conceptually, informativeness of a sentence is proportional to a number of important words --the more important words the sentence has, the higher its informativen ess. Next, we postulate that novelty, a unit representing diversi ty, is inversely proportional to connectedness. Thus, the most novel sentence in the answer list should have the minimum similarity with respect to the rest of the answers. Based on this assumption, we compute DiverseRank score for an answer sentence according to its informative score penalized by the redundancy score from the neighboring sentences. vertices representing each answer, EE is a set of edges representing the similarity between vertices, and E  X  V  X  V E  X  V  X  V , DiverseRank score of V i V i is defined as: Where d is a damping factor with a real-number value in [0 ,1] range. We use a standard value of d = 0.85 in this work. E ( V set of edges that connect to V i V i for a given similarity threshold. Binary discretization is performed for a given simi larity threshold to determine the value of E ( V i ). From the above equation, DiverseRank can be conceptually viewed as a form of  X  X nverse PageRank X  where each neighboring vertex casts a neg ative vote for the other vertex. The greater the degree a vertex h as, the lower the score it has in the end. Moreover, we postulate tha t the initial scores ( DR 0 ) play a significant role in determining the final DiverseRank scores. Thus, we assign informative sco re as a starting value for each sentence vertex. Then, an i terative computation continues until convergence where there are no changes in the overall graph ranking. According to the evaluation on the test data sets, the average number of iterat ions to reach convergence for 100 vertices is 60 given d =0.85. Equation 2 describes DiverseRank computation at the initialized stage (DR 0 ). As it can be seen, we replace the first componen t in equation 1 with the informative score measure denot ed Info ( V this work, we propose two measures to estimate sent ence informativeness; they are Info IDF and Info REL . DR 0 ( V i ) = (1  X  d ) Inf o ( V i )  X  d First, we model informativeness as a function of term rarity . That is, Info IDF computes the informative score of a sentence as a normalized sum of inverse document frequency ( idf ) of matching terms between question q and sentence s . where idf w i idf w i is an inverse document frequency of term w is the entire sentence collection. Evidently, this measure scores answer sentences that contain greater number of rar e words higher than the ones with a fewer number of rare words. In herently, rarity is also a direct indication of diversity. Alternatively, the informative score of a sentence can be derived from its conceptual relevance to a given question. We define conceptual relevance as a function of conceptual te rm frequency ( ctf ) of words in the sentence and words in the questio n; where, ctf is compute from a number of occurrences of a concep tual term (i.e. either single words or multi-word phrases) in verb-argument structures of a sentence [14]. It is based on the a ssumption that concepts that appear in a greater number of verb-ar gument structures contribute more to sentence meaning than those that appear in a fewer number of verb argument structure s. Where CT F w i ;q CT F w i ;q is a concept-based weight of w while CT F w i ; s CT F w i ; s is a concept-based weight of w compute CTF , we adopt Shehata et al. X  X  formulation [14] which defines a concept-based weight (henceforth CTF) of term i in sentence j as a linear combination of its normalized term fre quency and normalized conceptual term frequency. where tf i is a frequency of term i , ctf i is a conceptual term frequency of term i which is determined by summing the occurrences of i in the verb-argument structures of sentence j , s a term-frequency weighted vector of sentence j , and t conceptual term-frequency weighted vector of senten ce j . In a case where sentence informativeness is ignored , we simply initialize all sentence scores with a constant valu e 1. Thus, we define a purely diversity-based DiverseRank as foll ows: To determine the degree of sentence vertex , we per form binary discretization for a given similarity threshold  X  X  . Specifically, any vertex with edge weight below  X  X  will have its edge removed. In order to handle variability of answer expression, w e measure inter-sentence similarity at sentence semantics level. We fully describe our sentence-level structural similarity in section 3.2. In this work, in our previous work [1]. The overall process to generate a diversified answe r passage is as follows. First, a semantic role labeler is employed to extract verb-argument structures for each sentence. Then, we use a vector-space model to retrieve a list of top-500 relevant senten ces. For this, we combine text segments in both question topic and fr ee-form narration fields into a single query. Relevance sco re between sentences and query is derived from a cosine simila rity between CTF-weighted sentence vector and CTF-weighted query vector. Next, we construct conceptual term-document matrix where conceptual term features are taken directly from si ngle word tokens. To extract the single-word tokens, we remov e functional or non content-bearing words (articles, conjunctions, prepositions, etc.) from sentences but keep the cardinal numbers, and apply Porter Stemmer. Next, CTF weight is computed for ea ch conceptual term feature. Finally, the relevance sco re of a sentence is calculated from a cosine similarity between CTF-weighted sentence vector and CTF-weighted query vector. A li st of top-500 answer sentences is selected from the retrieved set . After a list of top-500 relevant sentences is retri eved, we partition sentences into k subtopics using k-means clustering. In this case, the value of k is simply determined by dividing a fixed answer length by an average length of sentence in the corp us [10]. The cluster centroid is computed from cosine similarity of the corresponding conceptual term vectors. The best res ult from 10 runs is chosen. Then, we create a list of 100 candi date answers by selecting the top sentences from each cluster in a round-robin manner. That is, given c i 22 C k where C k is a set of sentence clusters, candidate sentence i is selected from the top-scoring sentence from cluster c i while candidate sentence i +1 is selected from cluster c i +1 , and so on until the top sentence in the last clus ter c is selected. If the total number of selected sente nces is still less than 100 after the k th round, then we restart the process by selecting the next-top sentence from the first clus ter and so on. The selection step is terminated after the candidate an swer list contains 100 sentences. Lastly, we rerank the answers obtained from the pre vious step using our DiverseRank method. First, we compute the informative score for each sentence in the list. Then, we const ruct adjacency matrix where inter-sentence similarity value is der ived from the sentence-level structural similarity measure. Then, we build the sentence graph by discretizing the edges at similar ity threshold  X  = 0 : 4  X  = 0 : 4 . The reranking continues until the overall ranking reaches convergence. Finally, we terminate the process and truncate the answer list to 7,000 characters (following the stan dard procedure in related TREC evaluation). We adopt a standard protocol used in the automatic evaluation of system-generated answers to complex questions calle d the  X  X ugget pyramid X  [9] to assess the performance of our propo sed methods. In essence, the method calculates the system scores according to a weighted harmonic mean (F 1 measure) between nugget recall (NR) and nugget precision (NP). NR and NP are derived fr om summing the unigram co-occurrences between terms in each  X  X  nformation nugget X  and terms from each system-extracted answer . Pourpre scoring script 1.1c [8] is used to compute F-scores . Equation 7-10 describes the formulas to compute NR, NP, and F sco re. We conduct the experiment on complex interactive qu estion answering (ciQA) test set used in TREC 2006 ciQA ta sk. Unlike traditional TREC question answering data, ciQA data focus exclusively on complex relationship questions. A re lationship is defined as the ability of one entity to influence a nother, including both means to influence and the motivation for doin g so [5]. This type of questions reflects the information needs ge nerally faced by intelligence analysts, e.g. financial, movement of goods, family ties, communication pathways, organizational ties, co-location, common interests, and temporal. A total of thirty q uestion topics and a detail description of their information need are prepared by human judges at NIST (National Institute of Standar ds and Technology). Overall, there are 2,320.87 answer sen tences per test question on average. NIST assessors also create the answer key comprising a list of vital/okay information nuggets for each test question. On average, each question contains sixtee n answer nuggets. In this work, we compare the performance of four ba selines and eight specific DiverseRank variants. Table 1A and 1 B summarizes all methods compared in the evaluation. Table 1A. Summary of the baselines used in the eval uation The pyramid F-scores of the twelve methods are show n in table 2. Overall, the best DiverseRank method, REL+CS, signi ficantly outperform all four baselines (p-value&lt;0.05). This suggests that our proposed method are effective in generating a d iverse list of answers. For example, it outperforms SB and MMR by 24.71% and 48.30%, respectively. In addition, It also sign ificantly outperforms LexRank at p&lt;0.05 although the improvem ent is relatively smaller. Despite the fact that the notio n of graph connectivity in DiverseRank is opposite to that in LexRank, it performs very competitively, or even superior to Le xRank under certain conditions. Note that, despite the similar negative ranking mechanism, inverted LexRank (LexRank X ) produces inf erior F-score to DiverseRank. This suggests that DiverseRan k is not merely a backward ranking of LexRank. Within Divers eRank variants, the best methods also significantly outpe rform (p-value &lt; 0.05) other DiverseRank methods. When both informativeness and novelty are considere d in DiverseRank model, it produces the best result, com pared to the purely salient methods, e.g. SB, MMR, and LexRank. Moreover, evidence from the evaluation also suggests that the purely diversity-centric methods, e.g. 1+NG and 1+CS, perf orm poorer than the more inclusive methods. We believe that th e best DiverseRank variants work well since they rank answ ers in a more balance manner. averaging across all questions. The best methods ar e in bold. In this paper, we propose a novel ranking model to re-rank candidate answers of non-factoid questions using gr aph connectivity. Unlike the tradition graph ranking mo dels, our proposed method employs a negative voting strategy to iteratively adjust answer score by its degree of redundancy wit h other candidate answers. As a consequence, answers which are highly informative but too similar to others will be itera tively voted down. The final outcome is an answer passage which has a good balance of informativeness and novelty. We conduct an empir ical evaluation of our proposed method on complex questi on answering (ciQA) 2006 data. The results show that o ur DiverseRank method outperforms most baseline method s. Moreover, it performs very competitive against an e ffective graph ranking model, such as topic-sensitive LexRank. This work is supported in part by NSF Career grant (NSF IIS 0448023), NSF CCF 0514679, PA Dept of Health Tobacc o Settlement Formula Grant (No. 240205 and No. 240196 ) and PA Dept of Health Grant (No. 239667). [1] Achananuparp, P., Hu, X., and Yang, C.C. (2009) Add ressing [2] Brin, S. and Page, L. (1998) The anatomy of a large -scale [3] Carbonell, J. and Goldstein, J. (1998) The Use of M MR, [4] Harabagiu, S., Moldovan, D., Clark, C., Bowden, M., Hickl, [5] Jurczyk, P. and Agichtein, E. (2007) Discovering au thorities [6] Kleinberg, J.M. (1999) Authoritative sources in a h yperlinked [7] Kor, K. and Chua, T. (2007) Interesting nuggets and their [8] Lin, J., and D., Demner-Fushman (2005) Automaticall y [9] Lin, J., and D., Demner-Fushman (2006) Will Pyramid s Built [10] Liu, D., He, Y., Ji, D., and Yang, H. (2006) Multi-Document [11] Liu, Y., Bian, J., and Agichtein, E. (2008) Predict ing [12] Nenkova, A. and Vanderwende, L. (2005) The impact o f [13] Otterbacher, Erkan, G., and Radev, D.R. (2005) Usin g [14] Shehata, S., Karray, F., and Kamel, M. (2007) A con cept-[15] Suryanto, M. A., Lim, E. P., Sun, A., and Chiang, R . H. [16] Wan, X. and Yang, J. (2007) Towards a Unified Appro ach 
