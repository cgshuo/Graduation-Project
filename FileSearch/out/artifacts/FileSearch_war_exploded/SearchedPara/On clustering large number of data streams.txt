
Department of Computer Science, University of Sharjah, Sharjah, United Arab Emirates Department of Electrical and Computer Engineering, University of Sharjah, Sharjah, United Arab Emirates 1. Introduction
A data stream is a real-time, conti nuous, ordered (exp licitly by timestamp or implicitly by arrival time) sequence of data elements. Data streams have gained importance in recent years mainly because of the advances in hardware technology. These advances have made it easy to store and process numerous transactions and activities in an automated way. Applications where information naturally occurs as stream of data values include network monitoring [40], telecommunication systems [44], fi nancial trades [13,48], medical applications [39], and environmental applications [15]. For example, in the telecommunications arena, data streams can be used in monitoring traf fi c congestion on the Internet or in telephone communication infrastructure [16]. Data streams can also represent transaction logs and web logs [17,18]. In the fi nancial sector, data streams can be used for detecting trends of fi nancial markets [14]. In environmental applications, data stream processing can be used to track pollution such as oil spills and poisonous smokes [27].

Data streams are unbounded in size, continuous and have high arrival rate. That is data stream processing is different from conventional static or slow updating database systems. As a result, data streams presentnew challengesto data processingalgorithms that impose the following new requirements on data stream algorithms:  X  Process data in one pass to provide timely results.  X  Be incremental to adapt quickly to the high arrival rate of data.  X  Require nominal memory to process large volumes of data streams.  X  Be scalable to the number of streams to cater for new applications.

To be able to handle an in fi nite number of values, processing is restricted either to a window of elements, or to those data elements that have arrived in the last window of time. The former is called a count-based sliding window, while the latter is called a time-based or a timestamp-based sliding window [6]. Constraining all queries by sliding window allows continuous queries to be executed over unbounded streams in fi nite memory.

Due to the aboveconstrains, fi nding the different clusters of data streams where the data streams of each cluster have similar values is essential for many applications such as fi nding oil spills in environmental voice patterns in voice rec ongnition applications [3] and fi nding traf fi c congestions at certain times in telecommunication applications [16]. Thus, the goal of this paper is to ef fi ciently fi nd clusters in multiple data streams. This usually involves processing a large number of data streams. Two readings are considered similar when the Euclidean distance between the readings is less than some threshold, which is a user de fi ned parameter. We propose solutions for centralized environments, where sensors send their readings to one central processing node, called a sink (see Fig. 1).

The straight forward approach to fi nd the different clusters in multiple data streams is to compute the pair-wise distance between readings within time window w (see Fig. 2) across all streams. This process multi-dimensional grid-based clustering algorithm for data streams. To reduce the dimensionality of data streams, we use a linear transformation, Discrete Fourier Transformation (DFT), that packs most of the stream information in a few DFT coef fi cients. Thus, each data stream is represented by point in a multi-dimensional grid in the frequency domain. The proposed technique de tects the existence of clusters in real time and updates the clusters incrementally.

We summarize the main contributions of this paper as follows: 1. Propose a novel unsupervised grid-based (MG-jo in) algorithm to cluster multiple data streams. 2. Uses DFT to concentrate most of the stream information in a few coef fi cients and thus reduce the 3. Update the clustering result in an incremental fashion.
 The rest of this paper is organized as follows: related work on mining data streams are presented in Section 2. The proposed clustering technique is presented in Section 3. In Section 4, we provide an extensive experimental evaluation for the proposed t echnique. Finally, we conclude the paper and outline possible future work in Section 5. 2. Related work
Algorithms in data streams have received increased attention over the past few years. In this Section, we present some pieces of work that are most related to the algorithm proposed in this paper. These related work are categorized into three areas: Equi-join of data streams, pattern discovery in data streams and clustering data streams. 2.1. Equi-join of data streams
A large body of research in the data streaming area focused on the join operation. These works were are two approaches: two-way join and multi-way join. In a two-way join approach [26], investigated algorithms for evaluating moving window joins over pairs of unbounded streams with different arrival rates. They suggested using a symmetric join to reduce the execution cost and introduced a unit-time-basis cost model for performance evaluation. On the other hand [24], considered the problem of sharing the window join execution among multiple queries.

Research in multi-way join approach followed one of two sub-approaches: single join of streams and set of binary joins (two streams at a time). The system in [29] suggested a single m ulti-way join operator for all streams, which is called M-join, by generalizing the existing streaming binary join algorithm in [28]. Using a single M-join, an arrival from any input source can be used to generate and propagate results in a single step without having to pass thes e results through a multi-stage binary execution pipeline.

On the other hand, the use of set of binary joins may be used with, or without, sliding windows. For example [23], proposed a stream window join operator, which copes with unbounded and asynchronized multi-sensor data streams. Moreover [20], studied multi-way join processing of continuous queries over data streams; as a result, several algorithms for performing continuous incremental joins were proposed. However, the authors of [25] argued that the straightforward application of traditional pipelined query processing techniques to sliding window queries can result in inef fi cient and incorrect behavior. On the other hand [28], presented a continously adaptive continuous query (CACQ) implementation without sliding window.
 For distributed environment [5], addressed the problem of joining large number of data streams. They presented a variable-arity join operator specially designed for a dynamically-con fi gured large-scale sensor network with distributed processing capabilities [45] also focused on multi-way join sliding window processing over distributed data streams. They proposed a novel join algorithm based on two distributed data stream transfer models. Very recently [15], investigated the problem of processing join queries within a sensor network in distributed environment. 2.2. Pattern discovery in data streams
Pattern discovery is the process of searching for patterns in data streams which are similar to a given query stream, or part of a stream [35] proposed AWSOM (Arbitrary Window Stream mOdeling Method) to fi nd patterns in a single data stream. Their proposed method is based on using wavelets to represent the important features of the data. Later [36] introduced SPIRIT (Streaming Pattern dIscoverRy in multIple Time series) algorithm, which captures correlations and trends in collections of semi-in fi nite co-evolving data streams. The PIRIT method is an anytime single pass algorithm that is based on Principle Component Analysis.

Recently [40], use Dynamic Time Warping (DTW) distance to fi nd sub-sequences of a data stream that are similar to a given query sequence. DTW is a popular distance measur e, permittin g accelerations and decelerations, and it has been studied for fi nite stored sequences. However, in many applications such as network analysis and sensor monitoring, massive amounts of data arrive continuously and therefore it is infeasible to save all the historical data. They proposed SPRING that keeps summaries of the data without signi fi cantly sacri fi cing accuracy and at the same time it requires constant space and time per time-tick.
 2.3. Clustering data streams
Clustering algorithms arrange a data set into several disjoint clusters such that data streams in the same cluster are similar to each other and are dissimilar to data streams in other clusters according to some similarity metrics. Clustering data streams can be divided into two categories: single data stream and have been proposed that use the k-median approach [11,12,21,22] proposed algorithms that cluster a data stream in a single pass and use nominal space. Then, the system in [7] used an exponential histogram (EH) data structures to improve the algorithm proposed by [21].
 An improved incremental k-means algorithm for clustering binary data streams was proposed by [34]. Moreover [30], proposed a k-means based clustering algorithm that is highly scalable to data size. Then [31], proposed two algorithms: STREAM and LOCALSEARCH for data stream clustering and showed that the proposed clustering algorithm outperforms the commonly used k-means algorithm. CluStream, which is proposed by [1], addressedthe quality of clusters when the data evolves considerably over time, then, in [2] the same authors proposed HPStream, which is a projected clustering algorithm for high dimensional data streams that outperforms CluStream.

Other research works used density-based methods to cluster data. These density-based methods, such as DBSCAN [18], do not require a prior knowledge of the number of clusters. It locates regions of high density that are separated from one another by regions of low density. Recently, researchers have becomeinterested in using density-basedmethods for clustering evolving data streams. For example[10], presented a new approach called DenStream that discovers clusters with arbitrary shape and detects outliers. Also [42], proposed a density-based appro ach, which is an extension of conventional kernel density clustering of data streams with spatio-temporal features. TRAC-STREAMS algorithim [32], presented a different density-based approach for mining noisy and evolving data streams. [37] proposed a statistical grid-based approach to cluster data elements of a single data stream. The multi-dimensional data space of a data stream is dynami cally divided into a set of cells with different clusters in a single stream. On the other hand [8], developed an online version of the classical k-means to cluster multiple data streams [17] introduced a k-m eans based on-demand clustering method to cluster multiple data streams.

Recently [47], proposed an EM-based (Expectation Maximization) framework to effectively cluster distributed data streams. The framework handles the existence of noise and incomplete data records. In addition, the framework learns the distribution of underlying data streams by maximizing the likelihood of the data clusters and uses a test-and-cluster strategy to reduce the average processing cost. The same authors of [47] have proposed clusters X  tracking technique of data streams [46].

From the above discussion, all the methods proposed for clustering m ultiple data streams are either based on k-means, which require prior knowledge of the number of clusters, or inef fi ciently fi nd the clusters due to the high-dimensionality of the data streams. Our objective in this paper is to develop an algorithm that ef fi ciently divides multiple data streams into an unknown number of clusters. 3. Proposed solution
The goal of this study is to fi nd clusters across multiple streams in a sensor network, i.e. streams that show similar readings over a period of time. Since all natural phenomena in our applications, such as oil spill and smoke, could be of irregular (non-convex) shapes, supervised algorithms such as K-means cannot discover these non-convex clusters as these algorithms are known to be more appropriate for circular (convex) shape clusters. However, our proposed algorithm, like DBSCAN, can discover two non-convex clusters even if one cluster is partially, or totally, engul fi ng the other cluster. Thus, we propose unsupervised clustering technique called m ulti-way grid-based joi n algorithm (MG-join). We present a formal problem de fi nition in Section 3.1, brief discussion of DFT in Section 3.2, the MG-join algorithm in Section 3.3, an ef fi cient incremental processing of the proposed algorithm in Section 3.4, a proposed distributed solution in Section 3.5 and the complexity analysis in Section 3.5. 3.1. Formal de fi nition of the problem
Let N denote the number of data streams. Each stream consists of a sequence of readings with an ordered timestamp. Since streams can grow in fi nitely (unboundedin length), we considerreadings within aprede fi ned window of length w . The sliding window w contains the most w recent readings of the data streams. This type of sliding window is called the time-based or the timestamp-based sliding window. Given the length of the sliding window w and the current time point t , we consider the sub-sequences of data streams from time t  X  w +1 to the current time t . Readings with timestamps older than w are discarded. Streams are assumed to be synchronized in the sense that a new reading arrives at each stream at a constant time interval.

Based on the above description, a data stream is represented formally as w -dimensional vector: where x
The data streams will be updated each time new blocks of values arrive as shown in Fig. 5. Our objective is to fi nd clusters across multiple data streams. A cluster is detected when a group of sensors produces similar readings for a period of time. Two readings are considered similar when the Euclidean distance d between the readings is less than some threshold (determining this threshold is presented in Section 4). The Euclidean distance between stream x and stream y is calculated by Eq. (1):
The cluster g is a set of similar data streams such that
Where | g | is the total number of streams in g . The set of all the clusters found by the algorithm is represented by G .

Before we present the proposed MG-join algorithm, we present a brief background on DFT that is used by MG-join algorithm. Table 1 presents the symbols used in this Section. 3.2. Background on DFT
Data streams have high dimensions. Thus the fi rst step in the proposed approach is to transform the data streams to reduce the dimens ionality of the streams without sacri fi cing accuracy. The Discrete Fourier Transformation is used to concentrate the stream information in the fi rst few DFT coef fi cients. Thus, a DFT takes as input a fi nite sequence of real numbers in the time domain and transforms it into a frequency domain representation.

In particular, a DFT is widely used in signal processing and related fi elds to analyze the frequencies contained in signals. The DFT can be computed in practice using a Fast Fourier transform (FFT) algorithm. The FFT and the DFT terms are often used interchangeably, although there is a clear distinction: DFT refers to a mathematical transformation, regardless of how it is computed, while FFT refers to any one of several ef fi cient algorithms for the DFT.

The Discrete Fourier Transformation DFT of a stream x ( x = x 0 ,x 1 ,...,x complex numbers [33]:
The DFT preserves the Euclidean distance between two sequences x and y [4]. That means the distance in the native domain equals the distance in the frequency domain.
In this paper, the proposed MG-join algorithm fi nds the clusters in the frequency domain; therefore, it is important that the transformation method preserves the distance between the time domain and the frequency domain. 3.3. The proposed MG-join algorithm
The straightforward approach of fi nding similar streams is to compute the pair-wise distance between all the readings within w in all the streams. This process is costly, especially when the number of streams N is large. A more ef fi cient solution is to fi nd similar data streams by means of clustering. However, most of the prior works in clustering data streams use the k-means algorithm, which requires prior knowledge of the number of clusters. The proposed MG-join algorithm is an unsupervised grid-based clustering algorithm that has the following features:  X  No prior assumption of the number of clusters.  X  Clustering avoids the pair-wise distance computation.  X  Clustering is ef fi cient as it is performed in the frequency domain.  X  There is no need to maintain the data physically; instead we keep useful statistics of the data streams  X  Grid-based clustering is fast for large sets of data.

The MG-join algorithm (Algorithm 1) computes the DFT coef fi cients for the readings of the data streams in w . Then, for every new block of values received, the values within w are changed by adding the new block and removing the eldest block (see Fig. 8). Thus, the DFT needs to be updated in an ef fi cient way without recalculating the DFT coef fi cients of the reading in w . Therefore, we propose an incremental update mechanism of the DFT coef fi cients that is presented in Section 3.4.
In data transformation, the DFT coef fi cients are computed over w for all the streams. Each stream is represented by f DFT coef fi cients. Then, each stream is mapped into a point in f-dimensional grid. For example, if f =2 , then each stream is represented as a point in a 2-dimensional grid as shown in Fig. 6. If two points are close in the grid, then they represent two similar streams within this window.
After mapping the streams, the MG-join (Algorithm 1) takes as input the list of DFT coef fi cients for all the streams and two other parameters, CellWidth and MinP . CellWidth represents the width of each grid cell and MinP represents the minimum number of streams that can form a cluster. Determining the CellWidth and MinP is presented later in this section.
 The MG-join algorithm starts by building the grid structure in line 1 using the CellWidth parameter. the list of clusters using the Process-Grid algorithm (Algorithm 2).

The Process-Grid procedure takes cell with the largest number of streams as a fi rst candidate cell (line 1). The second candidate cell is the cell with the next largest number of streams, and so on. The current candidate cell should be an unlabeled cell (not a member of any cluster). For each candidate cell, the algorithm fi nds a new cluster through the Cluster-Construction (Algorithm 3) in line 2. This cluster will be considered if the number of streams in its member list is greater than the threshold MinP (lines 3 &amp; 4). The Process-Grid procedure repeats these steps recursively until all cells are processed. The Process-Grid procedure returns the list of the accurate clusters (G) to the main algorithm, MG-join.
The Cluster-Construction (Algorithm 3) takes as i nput a candidate cell as initial seed. The algorithm Construction algorithm makes sure that the neighboring cell is not empty (line 3) and not a member of algorithm repeats recursively to check the neighbors of the current neighbor untill there is no populated cell around any members of the current cluster (line 5). The algorithm returns the current constructed cluster to the Process-Grid procedure. For a cell in a 2-dimensional grid (Fig. 7), the gray cells are the neighbors of the cell i , where the number of neighbors is 3 2 -1. In a f-dimensional grid, the number of MinP and CellWidth are design parameters that need to be determined in advance before executing the MG-join algorithm. MinP is the minimum number of streams, in a cluster, that is considered signi fi cant to form a phenomenon. Thus, a group of less than MinP streams is considered isolated noise and removed. This parameter depends on the application under consideration and is determined by the user.
CellWidth determines the granularity of the grid and it depends on the density of the data. CellWidth is calculated empirically using training data set. We use a simple but effective heuristic called k -distance, which was used in [18], to determine CellWidth that is equal to the computed maximal distance d of stream. That is the CellWidth should be big enough to accommodate a minimum number of streams that form a cluster.

The k -distance heuristic is based on the observation that for a stream x, increasing k from 1 to the (levels off). Based on our experiments, a threshold k for our data sets is found to be 3 and the CellWidth value is 0.35. 3.4. Incremental processing of the proposed algorithms
Due to the high arrival rate and continuous nature of data streams, algorithms must be able to adapt rapidly to the fast changes in the streams by using an ef fi cient incremental mechanism. Updates in data streams come in the form of insertion of new data and elimination of old data. For ef fi cient updating, the sliding window w is divided equally into n blocks. Each block B is of size b . B is called the basic window. Thus w = nb .

Figure 8 illustrates an example of sliding windows and basic windows, where w contains 3 n and n contains 3 readings ( b =3) . That means for every 3 new readings, the algorithm updates the sliding window by eliminating the oldest block (oldest 3 readings), B 0 , and adding the newly arrived block (new arrival 3 readings), B All data streams are updated by receiving new blocks of values. When a new block arrives, the DFT coef fi cients need to be updated over the sliding window w .Foref fi cient computation of the DFT coef fi cients, the algorithm keeps a summary for each block. Then, in order to calculate the new DFT coef fi cients over w , the algorithm needs only to calculate the summary of the new block and eliminate the summary of the old block as follows: Let the expired block of data stream be and the newly arrived block of values be
Therefore, the old window and the new window are: coef fi cient of the new data stream. X new
Note that to update the DFT coef fi cient incrementally, the following value is stored for each block: When a new block of readings arrives, the algorithm calculates the necessary value of the new block (  X 
Partitioning the sliding window i nto smaller basic windows reduces the update cost. For each block, we need only to store one value (  X  is also reduced. On the other hand, the results will not be up-to-date at each clock tick. The delay is at most one block size, so this disadvantage is limited by small blocks [9]. However, small blocks increase the number of blocks and therefore increase the values that need to be stored. In addition, one can note that for short time intervals the sensor readings change slightly. Therefore, changes are more noticeable after a block of readings than for a single reading. 3.5. Complexity analysis
The complexity of the proposed MG-join algorithm consists of two parts: the data transformation and the MG-join algorithm. The time complexity of the DFT of one stream is known to be O ( wlogw ) [33], where w is the size of the sliding window. Since we have N streams, the DFT time complexity is O ( N wlogw ). To compute the complexity of the MG-join algorithm, we need to inspect its main operations, which are: 1. Mapping the stream to the grid. 2. Constructing clusters.
The fi rst operation is mapping the streams to the grid. A single pass through the N data streams seed. The time complexity of ordering the cells is equal to the number of cells in the grid. If the size of the grid is L ,where L is determined by CellWidth , then the time complexity of determining the initial case each stream is mapped to a separate cell in the grid and therefore the algorithm will need to process the whole list and thus the time complexity of selecting the initial seeds is O ( N ) .
Each time the algorithm constructs a single cluster from an initial seed, the algorithm needs to go through all the neighbors of the initial seed. Since th e number of the dimensions equals the number of DFT coef fi cients f , the possible number of neighbors for a cell in the grid is 3 f  X  1 .Intheworst case, each stream is mapped into separate cell in the grid, and for each cell the algorithm checks 3 f  X  1 neighbors. There are N occupied cells in the grid (each cell has one stream), so the time complexity is O ( N  X  (3 f  X  1)) .
 In summary, the complexity of the proposed MG-join algorithm is: In this paper, we compare the proposed algorithm with the DBSCAN algorithm. The description of the DBSCAN is presented in Section 4.2. The basic time complexity of the DBSCAN algorithm is O ( N * points in the Eps -neighborhood), where N is the number of streams. In the worst case, for each stream, the algorithm has to traverse all nodes of the R  X  -tree. Thus the complexity of the DBSCAN algorithm is O ( N 2 ) . However, if we assume the Eps -neighborhood is small as compared to the size of whole dataset, a small query region has to traverse only a limited number of paths in the R  X  -tree. Therefore the average set of N streams. For each of the N data streams, we have at most one query region. Thus the average run time complexity of DBSCAN in this case is O ( NlogN ).

Based on the used data sets, if the number of streams in a cluster is small, it is likely that the whole cluster would fi t in one R  X  -tree node and thus the search would cost O ( log N ). But if the number of worst case search would cost O ( N 2 ) . Note that the proposed MG-join worst case complexity is better than that of the DBSCAN. 4. Performance evaluation
We conducted experiments to measure the performance of the algorithms using synthetic data, which is described in Secion 4.1. The proposed MG-join algorithm is compared with the DBSCAN algorithm, which is described in Section 4.2 . The evaluation of the proposed multi-way grid-based join algorithm is presented in Section 4.3.

We use the object oriented programming language Java (JDK 6) to implement all the proposed algorithms. For the experiments with the DBSCAN algorithm, we use the WEKA 3 toolkit, a data mining with open source machine learning software written in Java [43]. All the experiments were conducted using a 2 GHz Pentium(R) M with 2 GB RAM, and Windows XP. 4.1. Description of synthetic data sets
We use synthetic data to evaluate the proposed algorithm. In general, an important advantage of synthetic data is that it helps in conducting experiments in a controlled way and hence to answer questions regarding speci fi c hypotheses related to the proposed algorithm and its behavior.
To evaluate the MG-join algorithm, data sets that contain clusters were generated as follows. First, a prototype p is generated for each cluster. This prototype is a stochastic process de fi ned by means of a second-order differential equation [8]: where t = 0,  X  t ,2  X  t ... .The u ( t ) is an independent random variable that is uniformly distributed in becomes smoother. The streams that should belong to a cluster are then generated by  X  X istorting X  the prototype horizontally. More precisely, a data stream x is de fi ned by where h are stochastic processes that are generated in the same way as the prototype p [8]. Figure 9 shows a prototype p and a data stream x .

In a similar way, the training data set has been collected and thus it follows the same distribution as that of the test data sets. The size of the training data set is about 20% of the size of the test data. 4.2. Description of the DBSCAN algorithm
The proposed MG-join algorithm is compared with the DBSCAN [18], which is a density based algorithm. The main idea in the DBSCAN is based on the idea of density reachability . A point q is density reachable from point p if q is within Eqs distance from p and there are at least Minpt points within Eps around p . Thus, p is considered a core point and it is assigned to a cluster. Non-core points are either assigned to a boundary of a cluster or labeled as noise (do not belong to any cluster).
DBSCAN is designed to discover clusters of arbitrary shape in noisy data by locating regions of high density that are separated from one another by regions of low density (see Algorithm 4). The DBSCAN algorithm requires two parameters: (1) Eps , the size of Eps neighborhood of points and (2) Minpts, the minimum number of points in the Eps -neighborhood to form a cluster. DBSCAN uses an R*-tree structure, which holds all the data points. The DBSCAN algorithm starts with an arbitrary starting point p that has not been visited and fi nds all the neighboring points within distance Eps of the starting point. If the number of neighboring points is greater than or equal to Minpts , the starting point is labeled with current cluster ID an added to the queue. The current cluster is expanded with the starting point (see Algorithm 5). Then, the neighbors of p are added to the queue. But, if the number of neighbor points of p is less than Minpts , p is considered noise. The algorithm recursively repeats the process for all the neighbors until a cluster is fully discovered. Then, the algorithms proceeds to process the unlabeled (unvisited) points in the R*-tree, if any, to try to discover other clusters. 4.3. Performance evaluation of MG-join
To evaluate the performance of the MG-join algorithm we investigated the following two aspects: the quality in terms of the purity of the produced clusters and the ef fi ciency in terms of system execution time. We compared the MG-join algorithm with the DBSCAN algorithm in both aspects. The input parameters of both algorithms are summarized in Table 2. DBSCAN uses MinPts that is equal to MinP , and Eps is the maximal distance of the MinPts + 1 nearest neighbor. However, the CellWidth is not equal to the Eps because the MG-join works with the transformed data in the frequency domain, and therefore the distance computation is done using the transformed data, which considers only f DFT coef fi cients. On the other hand, Eps for the DBSCAN is computed using the original data in the native domain which considers all the w components of the streams.
 4.3.1. Purity evaluation
The quality of the MG-join depends on the choice of the data set. Therefore, we test the algorithms on different data sets. The quality of the MG-join is m easured in terms of purity of the produced clusters. Before presenting the results of quality evaluation, we discuss the de fi nition of purity.
In the context of clustering, precision and recall are used for evaluating the clustering results. Let C is compared with a true cluster j in C where cluster j is the dominant cluster label of the members in C . Recall and precision are computed as follows [38]: where n the total number of streams in cluster i ,and n true cluster labels are known for the synthetic data sets.
 We use the precision , Eq. (6), to measure the purity of each calculated cluster by the MG-join algorithm. Then the average purity is the average of the precision of all the clusters produced in G [2,10,42]: where | G | is the total number of clusters generated by the MG-join. The experiments are conducted by varying the following parameters: the total number of the data streams N , the size of the sliding window w , the number of DFT coef fi cients f , and the number of clusters in data set C .

The experiment parameters are set as follows:  X  The total number of data sets is 250 divided into 5 groups.  X  The number of data streams in a data set is 200.  X  The size of the window w is fi xed to 60 values.  X  The maximum number of DFT coef fi cients used is 2.  X  The number of clusters in the data varies from 2 to 10.
 The Y-axis in Fig. 10 represents the purity value, while the X-axis represents the number of clusters. Five different groups of data stream are generated where each group contains 50 data sets and each data set contains 200 data streams. Each group of data sets contains different number of clusters. For example, the fi rst group (50 data sets) contains 2 clusters; the second group (50 data sets) contains 4 clusters, and so on.

Figure 10 shows that the MG-join algorithm gives exactly the same purity values as the DBSCAN both algorithms produce the same quality of cluste rs, the MG-Join algorithm uses only two DFT coef fi cients to represent each stream. Yet as we can see from Section 4.3.2, MG-join is much faster than DBSCAN.
 Note that as the number of clusters increases, the purity value decreases for both the DBSCAN and the MG-Join. That is because with a greater number of clusters in the data sets, the clusters become closer to each other. Thus, the algorithm tends to merge some of the clusters and therefore the purity of the clustering algorithms decreases. This is true for almost all the clustering algorithms.
We noted that in most cases two DFT coef fi cients were enough to get exactly the same results as theDBSCAN.Inafewcases,whereasigni fi cant overlap exists between clusters, more than two DFT coef fi cients are required for MG-join to achieve the same results as DBSCAN.

The above experiment is repeated for 400 data sets that are divided into 4 groups, where each group contains 100 data sets. Each group contains different number of clusters (from 4 to 10) as shown in Fig. 11 and some of these clustes are overlapping. Note that each data set contains 200 data streams. The computation of purity of clustering is repeated for different number of DFT coef fi cients representing the data streams in the group until the same purity of the DBSCAN for the group is achieved.
In Fig. 11, the X-axis shows the number of clusters. The Y-axis shows the percentage of the group of data sets that achieves the same purity value as DBSCAN using two, three, four, and fi ve DFT
In Fig. 12, the number of DFT coef fi cients required to achieve the same purity as that of the DBSCAN is follows: 80% of the dataset required only two DFT coef fi cients, 8% of the data required three DFT In summary, we showed that for most of the datasets two DFT coef fi cients are enough to achieve same clustering quality as DBSCAN. Yet the MG-join is much faster than DBSCAN as we see in the following section which presents ef fi ciency evaluation in terms of system time. 4.3.2. Execution time evaluatio n of the multi-way grid-based join
This section presents a comparison between the MG-join execution time and the DBSCAN execution time. We also present a comparison between the MG-join execution time and the k-means execution time. The system execution time of the MG-join algorithm is the total time used for DFT transformation, in addition to the time used to building the grid structure, mapping data streams to the grid, and cluster construction. On the other hand, for the DBSCAN and k-means the system execution time is the time used for fi nding the clusters using the data streams in the time domain.

There are different factors which can affect the system time. These factors include the number of data streams N in the data set, the window size w , and the number of DFT coef fi cients f . Experiments are conducted to evaluate the effect of those factors on the response time of the compared algorithms. The All other parameters are fi xed.

In this experiment, the number of data streams is varied from 100 to 1000. However, the other parameters are fi xed as follows: w = 60, f = 2. The test data contains 3 clusters.

The Y-axis in Fig. 13 shows the system execution time used by MG-join algorithm and the k-means algorithm. From this fi gure, we note that as the number of streams increases the system execution time of both algorithms increases. However, the time of the MG-join algorithm increases at a very small rate, while the k-means time increases at a high rate. This is because the k-means algorithm needs to compute the distance between each cluster centers and each data stream member of that cluster in every iteration until the algorithm converges. The time complexity of the k-means algorithm is O ( ICNw ), where I is number of k-means iterations, C is the number of clusters, N is the number of streams, and w is the window size. Increasing the number of data streams will increase the distance calculation time and thus increases its execution time. On the other hand, there is no direct distance calculation in the proposed MG-join algorithm. The distances are implicitly implemented in the grid structure. To specify k (number of clusters) for the k-means algorithm, we fi rst execute the MG-join algorithm, and then k is set as the number of generated clusters by the MG-join algorithm.

Next, we present the comparison between the proposed MG-join execution time and the DBSCAN execution time. In this experiment, the number of data streams is varied from 10 to 10000. However, the other parameters are fi xed as follows: w = 60, f = 2. The test data contains 4 clusters. The Y-axis in Figs 14 and 15 shows the system execution time used by the MG-join algorithm and DBSCAN algorithm. The two fi gures represent the same experiments, but different representation scales; Fig. 14 uses a logarithmic scale in the Y-axis, while Fig. 15 uses a linear scale. From these fi gures, we note the following: as the number of streams increases, the system execution time of both algorithms increases. However, the time of the MG-join algorithm increases much more slowly than the DBSCAN. Note also that for a small number of streams like 10, the DBSCAN algorithm is slightly faster than the MG-join algorithm, while for a large number of streams, the MG-join algorithm outperforms the DBSCAN algorithm.

The reasons is that for a fi xed number of clusters, a small N would make the clusters relatively small and thus the DBSCAN traverses a limited number of R  X  -tree paths with time complexity O ( NlogN ), see Section 3.5. But, when N increases, the clusters become relatively larger in size and thus the complexity of the DBSCAN becomes O ( N 2 ) . As a result, if the number of streams increases, the execution time of the DBSCAN increases quadratically. In contrast, the complexity of the MG-join is O ( N 3 f )+ O ( L ) . In this experiment, the size of the grid is fi xed and does not change with the number of streams. As the DFT coef fi cient f is fi xed, the value 3 f is fi xed. Thus, while the number of streams N increases, the execution time of the MG-join algorithm increases linearly with N ; as a result, it increases much more slowly than DBSCAN.
 The following set of experiments evaluates the effect of window size on the execution time of the MG-join and DBSCAN algorithms. In this experiment, we varied the window size w from 50 to 250. However, the other parameters are fi xed as follows: N = 1000, f = 2. The test data contains 4 clusters.
In Fig. 16, the Y-axis shows the system time, while the X-axis shows the window size. Figure 16 shows that as the window size increases, the execution time of both algorithms increases. The execution time for the MG-join algorithm increases slightly because the overhead of the DFT computation increases with the window size (time complexity of DFT O ( Nw log w )). Then, during cluster phase, a fi xed number of DFT components is used in the distance computation and thus the window size does not add overhead on the clustering phase. On the other hand, the execution time of the DBSCAN uses the whole window size in the computation of distances during the clustering phase thus the window size has a severe effect on DBSCAN X  X  execution time, as described in Section 3.5. Although, the time increases for both algorithms, the proposed MG-join algorithm greatly outperforms the DBSCAN algorithm.
The following experiments evaluate the effect of the number of DFT coef fi cients on the execution time of the proposed algorithm. Earlier in Section 4.3.1, we presented how the number of DFT coef fi cients affects the purity of the MG-join. We conclude that for most of our data sets, two DFT coef fi cients were enough to produce the same results as DBSCAN. Few datasets which have clusters that overlap each other require up to 5 DFT coef fi cients. We conducted experiments to test the effect of increasing the number of DFT coef fi cients on the system execution time. In this experiment, we varied the number of DFTs f from 2 to 7. However, the other parameters are fi xed as follows: N = 8000, w = 60. The test data contains 4 clusters.

The Y-axis of Fig. 17 shows the logarithm of the execution time, while the X-axis represents the number of the DFT coef fi cients, f . Note that the execution time of the DBSCAN is independent of f and the number of streams is fi xed. As the number of the DFT coef fi cients increases, the execution time for the MG-join algorithm increases. Recall the time complexity of the MG-join is O ( N 3 f )+ O ( L ) . the processing time exponentially. This is what is known in the literature as the curse of dimensionality. As shown in Fig. 17 for a small number of DFT coef fi cients, the MG-join algorithm is much faster than the DBSCAN algorithm. When the number of DFT coef fi cients increases above 6 (the cross-over point), the execution time of the MG-join algorithm becomes larger than the DBSCAN algorithm for the current experimental setup. Note that the cross-over point depends on the number of data streams N and window size w . For example, when N or w increases the cross-over point will increase above 6 DFT coef fi cients since the R*-tree of the DBSCAN will grow bigger and become slower. However, since we do not need a large number of DFT coef fi cients to achieve the same clustering quality as that of the DBSCAN (Section 4.3.1), we can conclude that the proposed MG-join is more ef fi cient than the DBSCAN when the number of data streams is large. 5. Conclusions
This paper proposes the MG-join algorithm to address the problem of fi nding clusters in multiple data streams. Due to the nature of data streams, new challenges have emerged. Thus, processing data streams is expensive in terms of memory and time. In addition to the characteristics of the data streams, we are interested in data streams generated by sensor networks that present additional challenges such as limited processing capabilities, low memory and power. The main contributions of this paper are: 1. Proposing the novel MG-join algorithm based on unsupervised grid-based method fi nd to clusters 2. The use of DFT to concentrate most of the stream information in a few coef fi cients and thus reduce 3. An incremental technique to update the streamed data.

The conducted experiments compared the proposed algorithm with the DBSCAN and k-means algo-rithms to evaluate its quality and ef fi ciency. The experiments X  results showed that the proposed MG-join algorithm produces a similar clustering quality as that of the DBSCAN by using two DFT coef fi cients in most of the cases. In addition, the experiments showed that the proposed MG-join algorithm is much faster than DBSCAN. The results also showed that the MG-join is scalable for a large number of data streams. Although increasing the number of DFT coef fi cients increases the cost of MG-join algorithm,
In the future, we plan to extend the method to fi nd clusters in multiple data streams that are not synchronized, try other transformation methods to reduce the dimensionality of the data streams, and MG-join algorithm.
 References
