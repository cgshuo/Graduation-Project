 With the proliferation of online distribution methods for videos, content owners require easier and more effective meth-ods for monetization through advertising. Matching adver-tisements with related content has a significant impact on the effectiveness of the ads, but current methods for select-ing relevant advertising keywords for videos are limited by reliance on manually supplied metadata. In this paper we study the feasibility of using text available from video con-tent to obtain high quality keywords suitable for matching advertisements. In particular, we tap into three sources of text for ad keyword generation: production scripts, closed captioning tracks, and speech-to-text transcripts. We ad-dress several challenges associated with using such data. To overcome the high error rates prevalent in automatic speech recognition and the lack of an explicit structure to provide hints about which keywords are most relevant, we use sta-tistical and generative methods to identify dominant terms in the source text. To overcome the sparsity of the data and resulting vocabulary mismatches between source text and the advertiser X  X  chosen keywords, these terms are then expanded into a set of related keywords using related term mining methods. Our evaluations present a comprehensive analysis of the relative performance for these methods across a range of videos, including professionally produced films and popular videos from YouTube.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Advertising, keyword selection, related term mining
The rapidly growing user base and movement towards on-line video distribution necessitates new methods for content owners to monetize their videos and for advertisers to effec-tively market their products. Traditionally, television net-works have monetized their content by selling time slots to advertisers, who in turn rely on estimated audiences and tar-get demographics to determine which programs they should advertise during. Advertisements online have the potential to be more directly relevant to the video content or the inter-est of viewers, since ads can be selected from a large pool of advertisements individually for each viewing. The effective-ness of online ads are also easier to quantify by measuring clicks from the viewers.

Current methods for selecting the advertising keywords for a video often rely on user supplied metadata, such as the video title, summary, comments, anchor text from adja-cent pages, and so on. This text is often sparse compared to the much richer video content, and many professionally pro-duced videos are only available online for a short period of time. It is difficult to adequately identify all of the keywords manually, leading to missed opportunities for ad placement.
In this paper we study the effectiveness of generating ad-vertising keywords using the content of the video. Note that we use the term keyword to refer to text of arbitrary length, which may be individual words or multi-word phrases. We focus on text sources such as production scripts, closed cap-tioning tracks, and automatically generated speech-to-text transcripts. Text sources tend to be more reliable than image-based analysis in practice today, and require signif-icantly less domain-specific knowledge or offline training.
Even with the text content for a video, several challenges remain. Identifying relevant keywords from text is non-trivial, and made more difficult when only error-filled speech transcripts are available. Methods for identifying advertis-ing keywords on Web pages often rely on external links and explicit structural markup or formatting [6, 12], which the text from a video lacks. Unlike documents, which gener-ally convey information through a single medium (text), the intended user experience for a video is communicated through both visual and auditory components. Dialog is of-ten sparse and may fail to captu re this complete experience, and the relevant keywords for advertisers may not necessar-ily directly appear in the text for a video, particularly when only dialog-based data is available.

We address these issues in two stages. In Section 2 we de-scribe statistical and generative models to determine a set of dominant keywords within a text source (script, closed cap-tioning track, or speech transcript). The vocabulary of the extracted keywords does not always coordinate well with the keywords advertisers have in mind. That is, while we may have a set of relevant keywords for the video, they may not overlap with the terms advertisers intend to bid on. To ad-dress this vocabulary impedance problem [9], we extract re-lated keywords from multiple data sources to increase the likelihood of matching an advertiser X  X  keywords. In both steps, keywords are identified and ranked without consult-ing an inventory of ads or advertiser supplied keywords. We evaluate each of the text inputs as sources for advertising keywords across a wide range of videos, including profes-sionally produced films and amateur videos on YouTube.
In the first stage of processing, we address the complex-ities of video-based text sources, such as scripts, and de-scribe methods for selecting keywords using statistical anal-ysis and topic modeling. We consider three sources of text data for a video: (1) complete movie scripts, which contain descriptions of the scenes, actions, and dialog, along with corresponding metadata (e.g. name of character speaking the dialog), typically formatted in a human readable layout, (2) closed captioning tracks (CC), which contain the text of the spoken dialog and timecodes indicating when that dia-log is spoken, and (3) speech transcripts (STT), which con-sist of a series of words, each with an associated timecode and duration. While scripts and CC tracks are manually generated, and thus highly accurate, speech transcripts are created through an automatic process which converts audio data to text, and frequently contain errors and omissions.
Understanding the semantics of a text element in a script is helpful when processing it. For example, character names appear frequently in a script prior to each of their lines of dialog, though we generally find them to be a poor choice for advertising keywords. We add semantics to each text segment of a script using a finite state machine based parser derived from conventional screenplay writing rules.
Movie scripts (and thus keywords extracted from their text) do not contain any associated timecodes. To add timecode information to script keywords, we generate the speech transcript and use the Levenshtein Word Edit Dis-tance [7] algorithm to find the best word alignment between script dialog and the STT transcript. Note that the parsing and alignment steps described in this section are specific to scripts. CC and STT input contain only text of the spoken dialog, and the corresponding timecodes are already present.
In the final step for a source text (script, CC, or STT), the timecoded text elements are used to build an N-gram tree that is pruned by N-gram term frequency to discover the most dominant terms, based in large part on the work of Chim and Deng [3]. In our experiments, we use N-grams of length N = 4, and evaluate the top M =20keywords.
The statistical N-gram method works well when keywords and phrases are repeated multiple times. While this is often the case for longer or well-formed text input, short or noisy text often results in the majority of (non-stopword) N-grams only being mentioned once. With this type of input, statis-tical models are unable to decipher which keywords are most important. To better handle short or noisy text input, we use a keyword selection method based on generative topic modeling. In this model, we assume that a video comprises a small number of hidden topics, which can be represented as keyword probabilities, and that a video X  X  text is gener-ated from some distribution over those topics. The highly probable keywords in those topics are likely to be most rep-resentative of the video content. We use Latent Dirichlet Allocation (LDA) [1] to learn the topics and corresponding topic-keyword probability dis tribution from the input text. We then combine these topics to form a ranked keyword list.
To discover the underlying topics in a video, we segment the input text into sentences and perform topic modeling with LDA. In our experiments, we set the number of topics K = 5 with the LDA parameters  X  =0 . 3and  X  =0 . 1. The resulting topic-term distribution  X  is a K x V matrix, where K is the number of topics, V is the size of the input vocabulary, and  X  [ i ][ j ] is the probability of keyword topic i . We form an ordered list of keywords k i for each topic, sorted by their probability in  X  [ i ]. This results in ranked lists of keywords, one per topic, which must then be merged into a single list to select the top M . While simply selecting the top M K keywords from each topic is one option, we describe a more general solution for merging multiple ranked lists when we discuss our approach to finding related keywords in Section 3.3.
We apply two filters, when possible, to remove frequently occurring words which are often not useful in the context of matching advertisements. From all input sources, key-words matching a list of English profanity are removed. We also find that main character names are often amongst the top ranked keywords, but generally do not retrieve relevant advertisements. When given a complete script, we remove character names from the keywords using a dictionary auto-matically constructed during the parsing and tagging stage. For closed captioning and speech transcripts, however, these names are unknown and thus may still appear in the top keywords. This is more common for closed captioning than speech transcripts, however, as spoken character names are less likely to be correctly transcribed by the STT engine.
The keywords selected by processing the source text can provide a useful set of terms to represent the content of a video. These keywords are limited, however, to the vocabu-lary used by the original script authors. Closed captioning and speech transcripts are limited further to only the spoken dialog. An advertiser may have a particular set of seman-tically related keywords in mind which do not necessarily overlap with any of the selected keywords. These vocabu-lary mismatches result in missed opportunities to connect advertisers with relevant content. In this section we investi-gate two simple techniques for identifying related terms to help bridge the gap between the vocabularies used in videos and keywords chosen by advertisers. We explore term min-ing approaches based on (1) co-occurring terms using the Web, and (2) the Wikipedia graph.
Table 1: Example related terms for keyword  X  X amera X 
Buckley et al. [2] noted that related terms will typically co-occur non-randomly in documents relevant to a query. To find candidate related keywords for term(s) T , we first sub-mit T as a query to a Web search engine. For each of the top 50 search results, we identify a set of relevant keywords and construct a vector space model M from the results. Based on the popular TF-IDF [10] term weighting, we compute the corpus frequency (CF) and inverse-document-frequency (IDF) weight for each term in M , and rank the keywords according to their CF*IDF score.
Graphical models for term expansion have been studied using random walks and multiple semantic links [4]. Within the text of a Wikipedia article, numerous inter-wiki links point to other Wikipedia pages, which allows us to model Wikipedia as a directed graph G = { V,E } . We use the link structure of the graph to both identify and rank candidate related terms. We require the relatedness between two arti-cle nodes a and b to be a symmetric relationship: a is related to b if and only if b is related to a . To identify candidate related terms for term T , we first locate the Wikipedia page with T as the title. Given the node t for T , we identify any nodes in the graph which form a direct cycle with t as can-didate related terms. That is, keyword k is related to t if ( t, k )and( k, t )arebothin E . We then approximate the rel-ative importance of terms by computing PageRank [8] over the Wikipedia graph. Candidate terms are assigned a score equal to their PageRank value, and ranked accordingly.
The CF*IDF ranking metric for search result keywords has no inherent range, whereas PageRank assigns a value to each node such that the score of all pages sums to one. To combine these two sources, we normalize scores by assigning a score to each term within a list based on its reciprocal rank. For an ordered list of terms l , we assign a score to the term at rank i as s l ( t i )=(1+log i )  X  1 , with any term not existing in the list assigned a score of 0. We may then combine the terms from any n ranked keyword lists into a single list, with a final score for each term t as S ( t )= n j =1  X  j where the weight placed on list j is defined as  X  j , such that j =1  X  j = 1. In our experiments, we placed equal weight (  X  =0 . 5) on both the search result and Wikipedia sources. Table 1 shows an example of the suggested related terms generated by the methods described above.
We conducted a user survey to evaluate the keywords cho-sen from the source text and related term mining across a range of videos including 12 films, 3 clips from news and edu-cational content, and 5 amateur clips from YouTube. Users were shown a 3-4 minute video (or film trailer) and a set of keywords. We show 5 of the top 20 keywords for each method and text source, and 1 of the top 10 related terms for each of those keywords, chosen and ordered at random. Users made binary assessments on the relevance of each key-word. Over 23 people participated in the survey (personally identifiable information was not required), with a minimum of 9 and average of 13 users evaluating each video.
We evaluate the generated keywords using two metrics (additional metrics are discussed in the full version of this paper [11]). The average relevancy of the keywords displayed to users we call the precision . The second metric we define is popularity , which serves as an indicator of how pertinent the keywords are to advertisers. We define precision and popularity as:
R i is the set of keywords judged relevant in evaluation i and K i ( S ) are the keywords displayed to the user for eval-uation i which come from source S . R ( S ) are the keywords from source S judged relevant by at least one user, and A  X  is the number of advertisers bidding for keyword k .Since we do not have an inventory of ads available to exactly know ,weestimateitwithaWebsearchengineusingthenum-ber of ads returned for query k . Although most commercial search engines limit 0  X  A  X  k  X  8, we are primarily concerned with relative performance across text sources. Note that, because the popularity of a keyword is meaningless if it is not relevant to the content, we compute popularity for the set of keywords identified as relevant by at least one user.
Table 2 shows the precision of the keyword selection meth-ods and their identified related terms. Cells in bold indicate a significant difference in performance ( p&lt; 0 . 05) between the two methods. For example, in Table 2, the precision of the statistical method on closed captioning tracks was higher than the generative method with p =0 . 037.

As we expected, for  X  X ell formed X  text such as scripts, the statistical method generally achieves higher precision. The generative method shows slightly better performance on the noisier speech transcripts, though the difference is not large enough to be statistically significant. The precision of re-lated terms is lower than the corresponding terms identified directly from the source text. Interestingly, we also see that closed captioning data outperforms full scripts. This may indicate that viewers more closely associate dialog with the main points or themes of a video than the additional props, scenery,andactionsdescribedinacompletescript.

We take a closer look at the performance for speech tran-scripts across three different video types in Table 3. Here we see that for the longer, professionally produced films, the sta-tistical method achieves marginally higher precision even on speech transcripts. The generative method performs signif-icantly better on the shorter news and user generated clips, which supports our earlier intuition that statistical methods alone would likely have insufficient data to find the best key-words in such cases. We also note that news and educational content, on which the speech-to-text engine is expected to be most accurate, achieves the highest precision.
Hauptmann X  X  work indicates that STT word error rates (WER) under 0.4 result in retrieval performance compara-ble (approx. 80% relative retrieval precision) to a perfect transcript [5]. We compute the average WER for films and news/educational videos (using the STT engine X  X   X  X efault X  language models), and compare the relative precision of STT with respect to CC in Table 4. User generated videos are not included because no  X  X orrect X  transcripts are available for the content. As expected, the average WER for news and educational videos is substantially lower, though still around 0.4. For this type of content, the relative precision of STT is 96% of the closed captioning. For the higher word error rate of films we can still achieve over 70% average relative precision. These results further support use of the statistical selection methods on longer text inputs and the generative methods on shorter text, and suggest that speech transcripts alone may be sufficient to find meaningful advertising key-words for videos where the background noise is reasonably contained and the STT language models are appropriately trained, such as news and educational content.
Popularity estimates the utility of keywords for advertis-ing by measuring the average number of ads returned when each relevant keyword is issued as a search query, shown in Table 5. Popularity is notably higher for related terms in most cases, suggesting they would be more beneficial for ad-vertising. While closed captioning was generally considered the most precise source of keywords, we also see it produces the least meaningful keywords for advertisers. This may be a result of character names appearing in the closed caption-ing keywords, which we noted earlier are filtered out from script input text and are less likely to retrieve relevant ads.
We also look closer at the popularity of keywords from speech transcripts in Table 6. In all cases, the keywords News/Educational 1.69 4.11 2.21 3.50 User Generated 1.89 4.83 2.63 4.75 identified through related term mining have higher popular-ity than the keywords from the source text by a statistically significant margin. It also again shows that news and educa-tional content contains less popular keywords for advertisers.
In this paper we have explored the suitability of a range of text sources for generating advertising keywords for video content. We have demonstrated that statistical N-gram keyword selection methods are effective when a sufficient amount of text data is available, while methods based on generative topic modeling perform better when the data is short or error prone, as is often the case with automatic speech recognition and user generated clips on sites such as YouTube. We have also shown that expanding the keywords from the source text with related term mining can substan-tially improve the likelihood of matching relevant and more marketable advertiser keywords. We used simple methods for identifying related terms to demonstrate improvements for advertising, though related works in term expansion (e.g. [4]) may provide even more relevant related keywords.
Although not studied in this short paper, clearly a trade-off between precision and popularity can be played using a combination of source and related keywords. Readers are encouraged to view the full version of this paper [11] for a discussion of precision-popularity tradeoffs, as well as addi-tional details and evaluations omitted for this short version.
