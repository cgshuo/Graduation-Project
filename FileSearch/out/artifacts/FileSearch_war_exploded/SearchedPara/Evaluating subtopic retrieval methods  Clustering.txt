 1. Introduction
In the past few years increasing research has been done on subtopic retrieval, i.e. the task of retrieving documents rele-users in discriminating the information contained in a conventional list of search results, due to their redundancy and lack of structure. The number of real user queries affected is potentially large, partly because informational queries have been expressed by very few words may have multiple interpretations, depending on the user intent or on the context in which it is issued. Furthermore, there is evidence that the retrieval of more information on specific subtopics of interest is often the primary goal of the efforts of searchers ( Xu &amp; Yin, 2008 ).

Most search systems are not able to deal with subtopic retrieval because they try to present results in descending order of ranking principle , although Robertson in his paper recognized the limitations of using a ranking scheme that works docu-ment-by-document:  X  X he major problem appears to lie in the way the principle considers each document independently of the rest X . It has been shown that assuming that the relevance judgment of any document is independent from the rele-account the semantic similarity between the documents presented to the user.

One well known strategy to capture the thematic structure of search results is to use clustering. Search results clustering  X  labels, such items can be accessed in logarithmic rather than linear time. Following Viv X   X  simo X  X  research and commercial web clustering engines have been deployed, surveyed by Carpineto, Osin  X  ski, Romano, and Weiss (2009b) .

To illustrate, in Figs. 1 and 2 we show the output produced by KeySRC, tem analyzing the first about 100 retrieved documents are shown on the left, with the original list of search results being displayed on the right. Each cluster is described by a label and by the number of documents contained in that cluster, which can be seen by clicking on the label.

The two queries, used as exemplifications throughout the paper, are  X  X xcalibur X  and  X  X nformation retrieval X . The query  X  X xcalibur X  is ambiguous, with several meanings such as the name of the sword belonging to King Arthur, the movie, the two documents are nearly duplicate and the meaning of documents is sometimes not clear from their title (e.g., those related to Arthurian legend). In comparison, the clusters created by KeySRC allow the user to discriminate between a similar or somewhat smaller number of distinct meanings with more ease; e.g., by reading the cluster labels rather than the document titles or snippets.

The query  X  X nformation retrieval X  is a broad query. The top hits returned by the search engine (see Fig. 2 ) mostly cover introductory (general) documents about information retrieval (e.g., wikipedia, answers.com, glossary) that presumably pres-tified by the clusters seem quite useful and give a good overview of some of the most interesting subtopics of information once, even though they were scattered through the original search list.

The other main approach to subtopic retrieval is diversification of search results. The inner mechanisms of clustering and cluster; diversification considers inter-document similarity when ranking, but just presents a result page like any normal search engine.
 results page with reduced redundancy. The diversification of results is usually achieved by way of re-ranking functions that trade relevance for novelty: the documents to be selected at each step are compared according to both their estimated rel-evance to the query and their dissimilarity in content to the subtopics covered by the documents already chosen.
In Fig. 3 we show the results of a diversification method on the same queries considered above, namely  X  X xcalibur X  and duced by KeySRC through a method described below in the paper. Fig. 3 suggests that the re-ordered lists were more useful  X  X xcalibur X  and  X  X nformation retrieval X , and the information retrieval subtopics were more focused.
Although there is a rich body of literature on both clustering and diversification, there have been so far no evaluation of search results using a unified evaluation framework.

In order to allow cross-comparison between methods producing different types of outputs (i.e., lists versus partitions), we adapt subtopic retrieval evaluation measures developed for ranked lists to clustered results. The considered evaluation mea-sures focus on complementary aspects of the subtopic retrieval performance, namely partial subtopic coverage and full sub-of systems that post-process search results. The two collections contain ambiguous and broad queries, respectively, and are equipped with document-level relevance judgments per subtopic. They have unique features, compared to the other few available data sets for subtopic retrieval evaluation. We compare three strategies of subtopic retrieval methods, namely clustering, diversification, and minimal subsets of results.
 whole list in such a way that it becomes a concatenation of multiple subsets of diverse documents. This approach is illus-by 10 subsets of diversified results, with 10 elements each.
The results of our experiments suggest that different strategies cater for complementary aspects of subtopic retrieval per-formance. We identify the suitability of each strategy for the various evaluation measures being analyzed and study which but in practice it may be difficult for them to improve over lists produced by modern search engines.
The remainder of the paper has the following organization. We first address the two main issues involved in the evalu-ation of subtopic retrieval performance, namely the definition of suitable measures and test collections that accommodate user needs at a finer grain than provided by conventional ranking systems. Then we discuss the main approaches to subtopic achieved on ambiguous and multi-topic queries, including an estimation of subtopic dissimilarity across test collections and a query-by-query analysis. We finally provide some conclusions. 2. Evaluation measures
In recent years, several measures have been proposed to evaluate subtopic information retrieval, the best known of which subtopic search length under k document sufficiency ( kSSL ), measuring the average number of items that must be examined ments we used these three measures, precisely described below. Other evaluation metrics are discussed in Section 2.4 . because we need to define a user X  X  behavior model that takes into account its more complex features. The typical approach is contrast, we assume that user searches are driven by the content of cluster labels. This seems more realistic and in closer agreement with the findings of some studies of search results clustering usage. Users seem to interact well with cluster la-1996; Chen &amp; Dumais, 2000; Koshman, Spink, &amp; Jansen, 2006 ).

In addition, we anticipate that the same output clusters may give rise to different sublists of examined items during the interaction between the user and the system, depending on the search task at hand. This approach is similar to the usage-a clustering engine. The exact order in which the information items displayed by the clustering system will be examined on the part of the user, for each evaluation measure, is detailed below. 2.1. Subtopic recall at rank n (S X  X ec@n)
Consider a query Q with h subtopics S 1 , ... , S h and a ranking d by the first n documents: 2.1.1. S X  X ec@n for clustered results to avoid retrieving multiple documents relevant to the same subtopic. Our exact modelization of browsing through the clus-label relevant to subtopics that have not been already retrieved, and quits an opened cluster as soon as a document relevant to the cluster X  X  subtopic has been found. When the last cluster has been seen, the search is continued on the full list of documents. At the end of this process, all the documents will have been (re-) ranked accordingly, and we can compute the corresponding S  X  Rec @ n value using Eq. (1) (counting examined cluster labels as document snippets). For example, nicholas clay X , and  X  X amelot classic car X , subsumed by  X  X xcalibur cars X .
 2.2. Subtopic precision at recall r (S X  X rec@r) ratio of the number of subtopics covered by the first n r 2.2.1. S X  X rec@r for clustered results
For clustered results, we assume the same search behavior model as that applied to S  X  Rec @ n in Section 2.1.1 . 2.3. Subtopic search length under k document sufficiency (kSSL)
Let p S ciency ( kSSL ) is defined as the mean of the positions p 2.3.1. kSSL for clustered results
Turning to clustered results, as in this case the goal is to retrieve multiple documents relevant to the same subtopic, the user will want to inspect each relevant cluster fully. Thus, we assume that the user examines sequentially the cluster list, aged over the set of subtopics, of the number of items (cluster labels or document snippets) that must be examined for each mirren nicholas clay X  and  X  X xcalibur film X , etc. 2.4. Summary and related measures
In Table 1 we summarize the main features of the evaluation measures used in the experiments. Note that these measures do not easily allow comparison across queries having a different number of subtopics. Dividing by the number of subtopics is 2003 ), but it is computationally very expensive.

We would also like to emphasize that these measures are not exhaustive, because with ambiguous or broad queries it is more difficult to recognize the underlying search intent and it may be necessary to accommodate multiple user needs. Other metrics have been recently proposed. The a  X  X DCG measure assumes graded relevance values are available and balances rel-to the single subtopics of a given query and then aggregating the results weighted by the subtopic probabilities. Intent (or subtopic) aware measures model a further evaluation dimension, i.e., when the user wants to retrieve single documents that cover multiple aspects in as much detail as possible.

These measures have become rapidly popular. Most notably, a  X  X DCG (with a =0.5) and intent-aware precision at retrie-to choose a suitable user X  X  browsing model for these search tasks.
 3. Test collections
For several years, the TREC 6 X 8 Interactive Track data (assembled in 2001 X 2003) has been the only test collection with document-level relevance judgments per subtopic. It contains 20 queries, with a focus on the instances of a given concept; e.g.,  X  X hat tropical storms  X  hurricanes and typhoons  X  have caused property damage and/or loss of life X .
Very recently, the gap between the growth in the availability of Web subtopic retrieval systems and the lack of appropri-ate tools for their evaluation has started to be addressed. Two early examples are the question X  X nswering collection adapted to subtopic information retrieval by Clarke et al. (2008) , and the ImageCLEFPhoto 2008 collection developed by Arni, Tang,
Sanderson, and Clough (2008) In both of these collections, however, the single query subtopics have been retrofitted to que-ries that did not intend to emphasize diversity. Subsequently, two larger test collections have been made available that ad-dress the subtopic retrieval task in a more principled manner, namely the ClueWeb09 data set used in the diversity task of
They contain fifty queries each, categorized as ambiguous or faceted, with the queries and their subtopics being based on information extracted from the usage logs of a commercial search engine. In particular, the subtopics were approximated as query reformulations estimated by means of clustering techniques.

In all these collections, however, the problem of re-ordering (or clustering) the search results is not separated from their initial document ranking, whereas we may want to evaluate the specific behavior of post-processing techniques. We argue that to ensure a more controlled comparison, the same set of search results should be provided as an input to all systems being evaluated.

To tackle these issues Carpineto et al. (2009b) and Carpineto and Romano (2010) introduced two new test collections, termed AMBIENT and ODP-239. The idea is to use several distinct very small collections of mostly relevant documents, one for each query, as opposed to having one large collection containing a small set of relevant documents to each query, with most documents being nonrelevant to any query. Thus, our approach closely mimics the scenario of post-retrieval pro-cessing of Web search results.
 AMBIENT and ODP-239 contain queries with multiple interpretations and multiple subtopics, respectively, derived from of AMBIENT and ODP-239 are artificial, because they are not based on real user queries. In contrast, ClueWeb09 and ImageC-
LEFPhoto 2009 have a much more realistic set of queries. On the other hand, the subtopics defined in the latter collections may reflect the biases of a certain population of users and/or those of the automatic procedure used to analyze the log and guide the subtopic development, whereas the subtopics in Ambient and ODP-239 may have a more complete and objective definition, as stated in world-class knowledge repositories. These two approaches are complementary.
We now describe each collection in turn as they will be used in our experiments. 3.1. AMBIENT
AMBIENT (which stands for AMBIguous ENTries) consists of 44 queries extracted from ambiguous Wikipedia entries, i.e., collected from Yahoo! as of mid-2007 and manually annotated with document-level relevance judgments per subtopic.
The average number of subtopics for each AMBIENT query is 17.95, according to Wikipedia definitions. Roughly half of them were present in the search results (i.e., 7.93 subtopics per query, on average), with an average number of relevant re-topics that were not contained in the Wikipedia list. In fact AMBIENT measures the ability to retrieve some subtopics not all possible subtopics of a query.

For example, query 10 of AMBIENT is  X  X xcalibur X , which has 26 distinct meanings according to Wikipedia (the complete turned by Yahoo!. 10.1 Excalibur is the mythical sword of King Arthur 10.2 Excalibur (film), a 1981 film about the legend of King Arthur 10.4 Excalibur (automobile), a type of X  X  X ontemporary classic X  X  retro-styled car 10.5 Excalibur (comics), both a Marvel Comics series set in the United Kingdom and a series featuring Professor X and 10.10 Excalibur (newspaper), the student newspaper of York University 10.11 Excalibur (novel), a fantasy novel by Sanders Anne Laubenthal 10.12 Excalibur (nightclub), a large and well-known nightclub in Chicago, founded in 1989 10.20 XM982 Excalibur, 155 mm extended range artillery projectile being developed by Raytheon and Bofors 10.21 Excalibur Hotel and Casino, a medieval-themed hotel-casino in Las Vegas, Nevada
The other retrieved documents were often about products and services not covered by the traditional Wikipedia mean-many users. More details on AMBIENT are provided in Carpineto et al. (2009a) . The collection can be downloaded from http://credo.fub.it/ambient/ . 5 3.2. ODP-239
ODP-239 combines the features of search results data with those of classification benchmarks. It consists of 239 queries, each with about 10 subtopics and 100 documents. The queries, subtopics, and their associated documents were selected from the Open Directory Project ( http://www.dmoz.org ) as of June 2009, in such a way that the distribution of documents across subtopics reflects the relative importance of subtopics. By ODP document we mean the items contained in the leaf nodes of the directory. Each documents consists of a title (which is the anchor text to the final Web page pointed to by ODP) and a very short description.

The exact procedure to select queries, subtopics, and documents from ODP was the following. For each of the 14 top cat-egories in ODP (we disregarded the  X  X egional X  and  X  X orld X  categories because they do not have thematic subcategories), we chose all their subcategories with at least 100 documents and six sub-subcategories. Such subcategories formed the set of
ODP-239 queries . For each query, we chose up to 10 sub-subcategories as ODP-239 subtopics , considering the most numerous ones. Then we selected the documents to be assigned to each subtopic. We decided to take into account the relative size of
ODP sub-subcategories, just as more popular subtopics get more top results in real web searches. For each subcategory, we randomly selected a set of 100 documents (among those labeled with that subcategory) imposing that the distribution of documents per sub-subcategory in such a set was the same as the distribution in the whole ODP collection. We also set the minimum number of documents per sub-subcategory equal to 4. Due to approximations, the final number of documents per query in ODP-239 was in some cases slightly larger or smaller than 100.

To illustrate, one top entry of Wikipedia is  X  X ports X , which has about one hundred children categories:  X  X dventure_Racing X , 10 subtopics (with the number of associated documents shown in parentheses). 221.1 Sports &gt; Cycling &gt; Organizations (22) 221.2 Sports &gt; Cycling &gt; Travel (21) 221.3 Sports &gt; Cycling &gt; Bike_Shops (18) 221.4 Sports &gt; Cycling &gt; Regional (10) 221.5 Sports &gt; Cycling &gt; Mountain_Biking (10) 221.6 Sports &gt; Cycling &gt; Racing (7) 221.7 Sports &gt; Cycling &gt; BMX (5) 221.8 Sports &gt; Cycling &gt; Human_Powered_Vehicles (4) 221.9 Sports &gt; Cycling &gt; College_and_University (4) 221.10 Sports &gt; Cycling &gt; Personal_Pages (4)
Unlike AMBIENT, all documents are relevant to at least one subtopic and the document-subtopic assignment comes for free. ODP-239 and AMBIENT have complementary aspects: the former collection deals with ambiguous queries and is suit-tion of search results clustering systems. The collection is available for download at http://credo.fub.it/odp239 . 4. Main approaches to subtopic retrieval and tested methods
We consider three main types of approach to subtopic retrieval that can be applied to post-process search results: clus-methods. The overall nine methods, although not exhaustive, are representative of a large spectrum of subtopic retrieval techniques. In the following we describe each approach and its relative methods in turn. 4.1. Clustering of search results
In the shift from data-centric to description-centric algorithms, a variety of clustering paradigms have been used, includ-(GST) ( Ukkonen, 1995; Andersson, Larsson, &amp; Swanson, 1999 ), which can be used to extract all the phrases contained in the search results in time linear with the size of the input.
 by its chaining mechanism for aggregating GST nodes, several other methods have been proposed that select the most infor-mative GST nodes and then build the clusters around them. In this paper we consider three representatives of this clustering known to perform well on browsing retrieval tasks ( Carpineto et al., 2009b ). We did not need to re-implement these algo-rithms because the results produced by the original versions on our test collections were made available to us by their authors. The three systems are briefly described below. 4.1.1. Lingo
Harshman, 1990 ), and finally documents are allocated to such frequent phrases. Lingo can be tested at http://search.car-rot2.org/stable/search . 4.1.2. Lingo3G
Despite similar names, Lingo and Lingo 3 G are two very different clustering algorithms. While Lingo uses SVD as the pri-mary mechanism for cluster label induction, Lingo 3 G employs a custom-built metaheuristic algorithm that aims to select well-formed and diverse cluster labels. Lingo 3 G is a commercial system developed by Carrot Search (see http://company.car-rot-search.com/lingo-applications.html for more information). 4.1.3. KeySRC
One of the most recent examples of description-centric clustering algorithm is KeySRC tracted from the generalized suffix tree built from the search results and merged through an improved hierarchical agglomer-ative clustering procedure, representing each phrase as a weighted document vector and making use of a variable dendrogram cut-off value. Hereafter, this clustering method will be referred to as KeySRC 4.2. Diversification of top search results
The general problem of minimizing the redundancy of a ranking of documents (or, dually, of maximizing their coverage one at a time choosing at each iteration the document with the best combined score of relevance and diversity.
A general formulation is the following. For each document d in a set of documents D , define a combined relevance-diver-sity rank for d with respect to a set of documents D  X  as: weighting factor for the importance of the relevance (or diversity). This function is a generalization of the harmonic mean
In this paper we use as relevance ranks those provided by the search engine, because they take into account several infor-mation sources that are not normally available to research ranking algorithms. Consider also that classical ranking algo-rithms such as the tf idf scheme are not directly applicable to post-ranking of search results because the query words are usually present in any retrieved document. The diversity component in Eq. (4) can be computed in various ways, e.g., ( Wang &amp; Zhu, 2009 ).
A second main approach to search results diversification is to explicitly model the query aspects  X  rather than assuming that similar documents will cover similar aspects  X  and then avoid choosing documents that can be assigned to the same query aspect. The proposed techniques for modeling query aspects leverage various sources including diverse query refor-mulations obtained from a query log ( Radlinski &amp; Dumais, 2006 ), the information content of terms relevant to the query methods spanning both of these approaches. The methods, detailed below, do not require external knowledge for training such as query logs or a classification taxonomy. In addition, they can be easily implemented and are computationally efficient. 4.2.1. Novelty based method the novelty based method consisted of the following steps. 4. Repeat steps 2 X 3 for n times ( n is a user-provided parameter). 4.2.2. Coverage based method
This method attempts to maximize the coverage of the total knowledge that exists on the Web about a given query ( Swaminathan et al., 2008 ). Our implementation included the same steps as the novelty based method, except that the value of Div ( d ) (step 2) is determined as follows.
 The diversity rank Div ( d j RRL ) is computed by finding the joint coverage score of the combined set of d and RRL , i.e., C ( d [ { RRL }), and then by ranking the documents in COL according to this score.
 The joint coverage score is given by: ber of documents containing the query and the term t to the number of documents containing the query, and the argument of the sum function is the entropy of the term relevance score. Notice that only words appearing together with query terms have non-zero relevance and contribute to the coverage score of the documents that contain them. Although this method has been devised for working with full documents, it is suggested that to reduce noise only the paragraphs containing the query turned by a search engine. 4.2.3. Cluster based method
As we hypothesize that documents grouped in a same cluster are relevant to the same subtopic and nonrelevant to the other subtopics, a possible approach to increasing diversity in early ranking is to promote documents belonging to different ated by KeySRC and added it to RRL . The list of clusters was visited top-down and the representative was the most relevant document in the cluster according to the ranking function used by the search engine. This diversification strategy is termed KeySRC L .

An alternative means of selecting the best representative in each cluster would be to use the medoid, namely the docu-ment closest to the centroid of the cluster. The distance between two documents might be computed using a (dis) similarity function based on the set of terms describing the documents. In our case, however, this is probably not very appropriate be-cause the documents have been clustered using transformed features (i.e., keyphrases) that do not always occur in the same form in the original representation of documents.
 4.3. Retrieval of minimal document sets imal document sets with maximum subtopic coverage. The idea is to create several subsets of diverse documents in such a way that each subset covers all subtopics and does not have redundant information within it. Each set can be seen as a big, composite document, or multimedia object ( Lee &amp; Park, 2009 ), which is distinguishable by its aggregated subtopics.
In our experiments we used three methods to generate and rank relevant document sets. These methods are a simple applied the corresponding basic algorithms used for diversification to the documents in the original ranking not chosen by earlier invocations of the algorithms. The diverse document subsets were ranked in the same order as they were generated.
The cluster based diversification method was iterated, choosing documents from clusters in a round robin fashion until all clusters became empty, at which point we appended the unclustered documents to the final re-ranked list in the same order as the original ranking. 4.4. Summary
In Table 2 we summarize the main features of the subtopic retrieval methods used in the experiments. The methods are grouped by retrieval strategy, namely clustering, diversification, and minimal set generation. Note that KeySRC
KeySRC L have not been used elsewhere. 5. Experimental setting
We set the number of clusters to ten, because in our experiments each query contained approximately 10 subtopics. Like-wise, we set the parameter n used in the diversification methods to ten. We did not try to optimize the method parameters.
For the clustering methods we used the default values of the on-line system versions, while for the diversification methods we set b = 0.5.

The evaluation of clustering performance using the measures described above raised a practical problem. In our model-ization of browsing through clustered results, we assumed that user decisions are based on the semantic meaning of the evant or nonrelevant to any subtopic of that query. For the ODP-239 collection, for example, it takes about 239 10 10 3 = 71,700 (possibly redundant) relevance judgments, where the four factors are, respectively, the number of queries, subtopics per query, clusters per query, and clustering systems. To reduce the manual labor, rather than using the full set of ODP-239 topics, we randomly selected 44 of them. The three clustering systems tested in our experiments (i.e., Lingo , Lingo 3 G , and KeySRC associated with each AMBIENT and ODP-239 query. Then, the relevance of each cluster label (produced by any system for any query) to any of the subtopics listed in AMBIENT and ODP-239 for each query was manually assessed by three external subjects using a web based evaluation tool developed for that purpose. The six methods for diversification, and minimal doc-ument sets retrieval were run on the initial list of results provided by AMBIENT and ODP-239, without any additional manual always set to 1.
 6. Results for ambiguous queries
In the upper part of Table 3 we report the results of the ten methods on AMBIENT, averaged over the query set. In addition down). The methods used to generate minimal sets are denoted by an ending asterisk, and the best results are displayed in bold. We now compare the results achieved by each subtopic retrieval strategy.
 margin on all data points. The best method was Lingo, with a gain over the baseline of 23.5% for k = 2, 23.6% for k = 3, and line. Using a two-tailed paired t test with a confidence level in excess of 95%, we found that the improvements of Lingo for large, ranging from 2.6e 7 (over Novelty for kSSL , k = 2) to 0.028 (over EP  X  for kSSL , k = 3).

On the other hand, clustering did not perform well on the other evaluation measures, including kSSL with k = 1. For these relevant document in the first position), whereas minkSSL satisfies a distinct subtopic).
 Turning to diversification methods, they achieved dissimilar results. The retrieval effectiveness of KeySRC coverage). The last finding is especially remarkable because KeySRC for kSSL , k = 1. The differences between KeySRC L and all other methods but KeySRC S  X  Prec @1.00).

By contrast, EP and Novelty were far behind KeySRC L and they were even almost always worse than baseline. The disap-
Eq. (4) . On the other hand, improving over the original list of results provided by commercial search engines may not be an easy matter, because it is likely that the diversity issue is being addressed in the first results page. subtopic retrieval.
 The strategy based on minimal sets generation seems to combine the advantages of diversification and clustering.
Although none of these methods obtained the best absolute value for any evaluation measure, a better balance between the improvements on partial subtopic coverage and on full subtopic retrieval. The performance of for the other evaluation measures was slightly inferior to the corresponding KeySRC best method of both the diversification and minimal sets generation strategies was based on clusters. This a very simple yet relatively novel approach, and there is probably much scope for improving the basic method used in our experiments. Our results suggest that further investigation of cluster based diversification is worthwhile. 7. Results for multi-topic queries
In the lower part of Table 3 we report the results of the ten methods on the ODP-239 test collection. Compared to the analogous results on ambiguous queries, the values show better performance of diversification and minimal sets, and worse performance of clustering.

The diversification and minimal sets methods were almost always better than baseline, but often by a small margin. Note by iteratively performing a random selection of the documents associated with each query in the ODP-239 collection. The most numerous subtopics were thus more likely to appear in the first results, which increased the scope for diversification policies. The fact that diversification did not clearly outperform random baseline was thus somewhat surprising. This was be analyzed on a quantitative basis in the next section. Other reasons for the disappointing performance of diversification (while many ODP-239 subtopics are best discriminated by multi-word concepts).

Table 3 shows that the single diversification and minimal sets methods had comparable retrieval effectiveness, although the novelty based and the coverage based methods were in general better than the cluster based method. The best absolute nificance was again evaluated using a two-tailed paired t test with 95% confidence limits. The differences obtained by S  X  Prec @.50), except over Novelty and Novelty  X  for S  X  Prec @ r measures.

The novelty based and coverage based methods probably benefited from the lack of nonrelevant results in the original document set. Another main observation concerning the results shown in the lower part of Table 3 is the low performance of clustering methods. This is discussed in the next section.

As already remarked, the results described in this section were obtained on a restricted version of ODP-239 test collection, formed by cutting down the number of queries from 239 to 44. In order to explore how the query reduction may have af-fected average performance results, we ran a further experiment. We evaluated the seven methods that did not require man-same method in Table 3 . This can be seen as an indication that the results obtained for the clustering methods on the 44 topics would scale to the whole collection. 8. Estimation of subtopic dissimilarity across test collections The kSSL results obtained by all clustering methods on ODP-239 were much worse than the corresponding values on
AMBIENT, both in absolute terms and relative to the baseline values. One explanation is that the classes of ODP-239 are more difficult to recover for a clustering method than those of AMBIENT. On the other hand, the comparatively better results achieved by the same clustering methods for S  X  Rec @ n and S  X  Prec @ r , together with the very good performance of KeySRC for the KeySRC clustering method).

It emerges that the disappointing kSSL results were mainly due to poor cluster labeling, because we observed that 54% of the subtopics were not covered by any cluster label. We hypothesized that ODP-239 subtopics were more difficult to label because their language models were more similar. To find experimental evidence for our hypothesis, we performed an esti-mation of subtopic dissimilarity across the two test collections using the symmetric Kullback X  X eibler divergence, also known as Kullback X  X eibler Distance (KLD). This measure, already used in information retrieval by Carpineto, De Mori, Romano, and Bigi (2001) and Bigi (2003) , among others, was adapted to our problem in the following manner.

Let D 1 , D 2 , be the composite documents obtained by taking the union of the documents relevant to subtopics S respectively. The KLD between S 1 and S 2 is given by: The probabilities are estimated using a back-off smoothing, i.e., where c ( t , D i ) is the number of times the term t occurs in D value of the smallest probability of a term in a subtopic, and w  X  1 The subtopic dissimilarity S  X  Diss for a given query with h subtopics is:
We computed the S  X  Diss value for each query, and then averaged the results over the set of queries in each collection. We found that the mean subtopic dissimilarity was equal to 9.27 for AMBIENT, whereas it was 6.15 for ODP-239. These results prove that the subtopic language models of ODP-239 were more similar, and thus it may be more difficult to find character-labeling may be more difficult than cluster optimization when the subtopic vocabularies overlap. 9. A query-by-query analysis
Table 3 shows that the mean performance of each method is more similar to that of the other methods in the same cat-egory. However, as both the clustering methods and the diversification methods use different mathematical functions, it is conceivable that they would present considerable performance variation on individual queries. We tested this hypothesis through a query by query analysis. For each query and for each tested method, we computed the differences between the retrieval performance of the method and that of the baseline, and then computed the minimum and maximum of such differences.
 by Yahoo! as a baseline and S  X  Rec @10 as an evaluation measure. The length of each bar depicts the range of performance variations over the baseline (in percentage) attainable by the three methods on each query. In the right picture we plot the of scope for performance improvement.

These results suggest trying combination strategies. A first attempt to combine the results of multiple search results clus-tering algorithms was made by Carpineto &amp; Romano (2010) with very good results. We are not aware of similar approaches clustering and diversification methods, with the goal of improving performance for those evaluation measures where either strategy, taken in isolation, usually performs poorly. 10. Conclusions
Recent research has focused on the definition of more effective subtopic retrieval strategies under two separate main paradigms and identify their relative strengths and weaknesses. In this paper we have taken a step forward to help fill this gap, by providing a unifying evaluation perspective. We compared the effectiveness of three main strategies to subtopic re-trieval from search results, using a range of complementary cross-paradigm measures and two suitable test collections with document-level relevance judgments per subtopic. The following major conclusions emerge from the experimental evaluation.

In subtopic retrieval there is probably no one-size-fits-all method, because multiple and potentially conflicting user needs pretation. It would be interesting to see how well the proposed measures and our experimental findings correlate to the user experience; e.g., by means of user evaluation studies focusing on subtopic retrieval. More research is then needed to under-stand whether the most suitable method for a given query and user can be automatically selected (e.g., by classification of ist and effectively address complementary user needs may be a key to expanding the scope and usability of subtopic retrieval facilities in standard search systems.
 Acknowledgments
The authors thank Stanislaw Osin  X  ski and Dawid Weiss for running Lingo and Lingo 3 G on the AMBIENT and ODP-239 test collections, and providing us with the results. We would also like to thank two anonymous reviewers for their valuable comments and suggestions.
 References
