 Ramon Lawrence * 1. Introduction
Hybrid hash join [1] and its variants are the standard hash joins implemented in database systems. Hybrid hash join is capable of joining at most two relations at a time. If a query requires more than two relations, the query optimizer must determine a suitable join ordering [2] of multiple binary join operators. Multi-way joins [3] have been proposed as an alter-native to binary join plans. A multi-way operator that joins several inputs simultaneously has numerous advantages. First, query optimization may be simplified as a single multi-way operator can replace multiple binary join plans and can adapt to the input sizes during processing more easily than re-optimizing [4,5] an existing query plan. A multi-way operator can also reduce the number of intermediate results produced compared to a binary plan which saves both processing and I/O time.
Another trend is to make join operators non-blocking. Hybrid hash join is blocking as it can only produce its first result proves query response time and overall performance, especially when inputs are read from network sources.
It is desirable to produce an early, multi-way hash join operator. The multi-way feature allows the operator to be self-optimizing and change its behavior in response to different input arrival rates. As an early join, the operator would produce results faster and be resistant to network delays. The application domains for an early, multi-way hash join are distributed databases, grid databases, and integration systems. These systems must answer queries where the input relations are dis-tributed across several sources, and the answer to the query must be exact and complete.

There are several early, multi-way stream join operators [6,7] . These operators implicitly assume that the inputs all fit in memory or load shedding is performed to resolve memory overflow conditions. For streaming joins, it is acceptable that not all join results are produced in response to limited processing capabilities. Such algorithms are not applicable where all query results must be produced.
 The only existing early, multi-way hash join that is guaranteed to produce all results is MJoin [3] . MJoin is an extension of
XJoin [8] and is limited to multi-way joins where all relations are joined on the same attribute. Although there are many rithm that supports such joins and guarantees that all results will be produced.
 In this paper, we present slice join which is an early, multi-way join operator that: Is not restricted to joins where all inputs have the same join attribute(s).
 Uses indirect partitioning [9] to reduce the number of disk operations for sequences of one-to-many joins. Allows early production of results for use in distributed query processing, streaming, and interactive queries.
Contains simplified duplicate detection that requires no timestamps for the majority of multi-way join plans and at most one timestamp per tuple in all cases.

Adapts to changing input arrival rates and maximizes the number of results produced over time by intelligent use of memory.

Slice join supports the multi-way join of relations related using a sequence of primary and foreign key relationships. In tuple in all other inputs, and there is not a many-to-many relationship between any two relations involved in the join. A plier is not possible as although LineItem is a determinant relation, there is a many-to-many relationship between tuples of work-based joins, where the inputs may arrive from other sources.

The contribution of this paper is a multi-way, hash-based, non-blocking join algorithm that expands the number of clas-ses of queries that can be processed using a multi-way join. The best current algorithm can only process multi-way joins where all inputs share the same join attribute(s) and the related binary plans are often inefficient. Slice join implements a modified form of indirect partitioning and a simpler duplicate detection policy that maximizes the use of memory and im-proves performance. Experimental results demonstrate that slice join is significantly more efficient than MJoin [3] and plans of binary join operators.

The organization of this paper is as follows: in Section 2 , we provide background information on hash join operators including those that support early and multi-way joins. Limitations associated with multi-way joins are in Section 3 . Section paper closes with future work and conclusions in Section 6 . 2. Background
In this section, we provide a brief history of hash-based join algorithms with specific focus on algorithms that support early production of results or multi-way joins. 2.1. Sample databases
The two databases used for illustration in the text are the TPC-H database and the Wisconsin Benchmark [10] . TPC-H is a decision-support and data warehouse benchmark developed by the Transaction Processing Performance Council (TPC). More information about the TPC-H benchmark can be found at [11] . The TPC-H schema diagram is in Fig. 1 . The SF in the diagram
GB. The largest relation, LineItem , has just over 6 million records. The Wisconsin database schema [10] is a single relation with an integer primary key and various numeric and string fields with domains that are a subset of the total number of tuples in the relation. 2.2. Hash joins
A hash join relates tuples in two relations using an equality condition between attributes (equi-join). The smaller relation based on size estimates during query optimization. The join starts by reading build relation tuples into memory. Each build relation tuple is hashed using a hash function on the join attribute. The hash function maps the attribute value into a par-tition index. Each partition contains tuples that map to that location. The number of partitions is estimated by the optimizer be some number of partitions that are memory-resident and others that are completely on-disk. The partitions that are memory-resident are organized into in-memory hash tables.

The algorithm then starts reading the tuples of the probe relation. Each probe relation tuple is hashed using the same hash the probe relation tuple probes the in-memory hash table and results may be produced. If the build partition is not in mem-of reading the probe relation, partitions that remain memory-resident have had their results generated and are flushed from memory. During the cleanup pass, each on-disk build partition is read into memory and probed with the corresponding probe partition. This description roughly describes Dynamic Hash Join (DHJ) [12] , which is a variant of hybrid hash join that flushes partitions on demand. Hybrid hash join [1] is similar except that a pre-defined build partition is selected to remain memory-resident. This partition occupies all of the additional space beyond what is needed for disk buffers. 2.3. Early result production
Hash joins have been modified to produce results early (before reading the entire build relation) by modifying how the input tuples are read and the hash table structure. The dual hash table structure used for early hash join algorithms consists of P partitions and B buckets where each partition has B = P buckets. A dual hash table consists of two separate hash tables, one for each input. When a tuple arrives from either input, it is hashed on its join attributes to produce a bucket index b tuple is then placed in the linked list at bucket index b index consists of a range of buckets. Results are produced early by allowing the join algorithm to read a tuple from any input at any time. An input tuple is hashed to a bucket location b responding bucket b i in the other hash table.
Available memory is used to buffer tuples of both input relations. When the available memory is exhausted, one partition must be flushed. This partition may be from either input. The differentiating feature of early hash join algorithms is the est partition, or flush the same partition index at both inputs (coordinated flushing [3] ). When XJoin flushes a partition to disk, it does not freeze or block that partition from buffering new tuples in memory in the future. Rather, a partition can be rebuilt in memory several times (and consequently flushed several times). Regular hash joins do not rebuild a partition
While inputs are being read, the algorithm will be generating output tuples. This is referred to as the memory-to-memory on-disk partition of one relation is read into memory, an in-memory hash table is built and then probed by the matching on-to-disk phase . Early join algorithms can further improve their result generation rate by performing a disk-to-memory phase when some inputs are not yet complete, but there are no input tuples that are waiting to be processed. In this phase, an on-disk partition is used to probe a matching in-memory partition. This allows an early join algorithm to handle delayed or blocked inputs while still generating results.

The other major performance impact for early hash joins is the duplicate detection strategy. It is possible that an early join algorithm will generate the same result tuple more than once. These duplicates must be detected. For example, a tuple may be generated in the memory-to-memory phase and later during the disk-to-disk phase.

In XJoin, timestamps are used to determine when a tuple has been produced before. Each tuple is assigned an arrival timestamp and a departure timestamp (when it leaves memory). If two tuples were in memory at the same time, as indi-cated by overlapping arrival X  X eparture timestamp ranges, the tuple is detected as being generated before. XJoin also tracks when on-disk partitions are used in disk-to-memory phases to prevent duplicates.

An improvement to XJoin is Early Hash Join (EHJ) [13] . Early Hash Join has a different flushing policy and a simplified duplicate detection policy. For a flushing policy, EHJ performs biased flushing that favors flushing partitions of the largest tion strategies based on join cardinality. For one-to-one and one-to-many joins, no timestamps are used. Rather, matching tuples are discarded if they cannot produce more results (e.g. a tuple on the many-side is discarded once it generates a re-sult). For many-to-many joins, only a single timestamp is used representing the arrival timestamp. A departure timestamp is not required as the tuple X  X  departure time from memory can be determined from the time the partition was flushed. Exper-imental results show that these flushing and duplicate detection policies improve EHJ X  X  performance over XJoin. 2.4. Multi-way joins
A multi-way join algorithm is capable of joining more than two inputs at a time. The first instance of a multi-way join was hash teams [14] used in Microsoft SQL Server. Hash teams allow multiple tables to be joined on the same attributes. The hash team coordinated the memory management and partition flushing for all inputs which allowed for improved performance.
Hash teams were extended to generalized hash teams [9] that allowed tables to be joined using indirect partitioning. Indirect partitioning allows a relation to be partitioned on an attribute that functionally determines the partitioning attribute. Consider query 1 in Fig. 2 that joins the Customer , Orders , and LineItem relations.

This is an example of a sequence of one-to-many joins with a single determinant table . A tuple of LineItem functionally determines a unique tuple in Orders that in turn uniquely determines a tuple in Customer .

We define a determinant join as a join consisting of a sequence of one-to-many joins where there exists a single determi-does not exist a many-to-many relationship between any relations. These joins can be performed using indirect partitioning.
As an example of indirect partitioning, assume we select five as the number of partitions. Fig. 3 from [9] shows how the relations are divided using indirect partitioning. The tuples of the three relations that will join together are always in the same partition index. By partitioning in this way, we can process each partition independently. We define a slice as the par-Lineitem (see Fig. 3 ).

In this example, both Customer and Orders can be directly partitioned on custkey . While partitioning the Orders relation, ping function is used to indirectly partition LineItem .

Consider the following example with P  X  3. A tuple of Customer with custkey = 5 is placed in partition 2 (5 mod 3 = 2). A tuple of Orders with custkey = 5 and orderkey = 4 will also get placed in partition 2, and we will record the mapping that not have a custkey attribute. However, using the orderkey field and the mapping function, the correct partition can be found. If the tuple has orderkey = 4, then using the mapping function, we can place this tuple in partition 2.
The major issue with indirect partitioning is that storing an exact representation of the mapping function is memory-intensive. In [9] , bitmap approximations are used that consume less space but introduce the possibility of mapping errors.
These errors do not affect algorithm correctness but do affect performance. The bitmap approximation works by associating a bitmap of a chosen size with each partition. A key to be stored or queried with the mapping function is hashed based on the multiple partition bitmaps which results in false drops .

Early production of results required for non-blocking joins can complicate the issues of building and maintaining these mapping functions. The bitmap implementation cannot be used for early result production due to undetected collisions as a map-
Key K2 also hashes to bitmap index 5. This causes an undetected collision as the lookup procedure only sees that the bit for index 5 at partition 2 is set and returns partition 2. However, K2 may not hash to partition 2, as we have not yet added K2 to the mapping function. An undetected collision is a missed required drop . When processing K2 that collides with K1 in the bit-map, we may not detect that K2 should be placed in multiple partitions when it arrives. Thus, the mapping is incorrect and is very hard to undo as we must wait until key K2 arrives to detect the collision and by then the tuple that required the lookup may have been evicted from memory.

Indirect partitioning does not support general star joins such as query 2 (in Fig. 4 ) because there is a many-to-many rela-tionship between the tuples of Part and Orders .

The issue is that given a tuple of LineItem it is impossible to determine a single partition that it should be placed in be-cause Orders and Part are partitioned on independent attributes. One solution is to perform multi-way partitioning using the
SHARP query processing algorithm [15] . In multi-way partitioning, Part and Orders are the build tables and partitioned as usual. LineItem as the probe relation is partitioned simultaneously on ( partkey,orderkey ) (in two dimensions). The number of partitions of the probe table is the product of the number of partitions in each build input. For example, if Part was par-titioned into 3 memory-sized portions and Orders partitioned into 5 memory-sized portions, then LineItem would be parti-tioned into 5 * 3 = 15 portions.

For a tuple to be generated in the memory phase, the tuple of LineItem must have both its matching Part and Orders por-tion in memory. Otherwise, the probe tuple is written to disk. The cleanup pass involves iterating through all partition com-binations. The algorithm loads on-disk portions of the probe relation once and on-disk portions of the build relation i a times may still be faster than materializing intermediate results, and the operator benefits from memory sharing during par-titioning and the ability to adapt during its execution.

None of the indirect or multi-way partitioning algorithms have been extended to support the early generation of results. 2.4.1. MJoin
MJoin [3] uses a single hash table for each input and is limited to supporting multi-way joins where all inputs are par-titioned on the same attributes. The probe methodology is determined for each input by performing the most selective memory. When inputs are blocked, MJoin switches to a disk-to-memory phase to produce additional results.
MJoin X  X  duplicate detection policy requires two timestamps per tuple representing its arrival and departure from mem-ory. The timestamps are used to detect when a tuple has been generated before either during the memory-to-memory phase or a disk-to-memory probe. An output tuple produced from the join of N inputs will consists of N component tuples , one from each source. MJoin performs a timestamp check for all possible pairs of the N component tuples. 3. Limits to multi-way joins
There are some fundamental efficiency limits to multi-way hash joins. If all relations fit in memory, then it is possible to perform a multi-way hash join for any set of relations that can be joined using binary hash joins. However, when the inputs same domain but not necessarily the same name (not restricted to natural joins).

The simplest multi-way join combines relations on shared join attributes. A common attribute join partitions relations on one or more shared attributes. For example, a common attribute join may involve three relations all of which have the attri-the same index using a one-pass join during the cleanup phase.
 tinct attribute join must involve more than two relations, as all equi-joins involving only two relations have common join attributes. Query 1 is a distinct attribute join.

Indirect partitioning allows for efficient processing of distinct attribute joins where there is a functional dependency chain between the relations. Query 1 can be processed using indirect partitioning while Query 2 cannot. Distinct attribute index at all inputs) that do not remain memory-resident can be processed individually using a one-pass join. General star joins, such as Query 2, can be processed using multi-dimensional partitioning as with the SHARP algorithm.
This is not as efficient as indirect partitioning because the build relations may have to be read multiple times during the cleanup phase. However, it still may be more efficient than the corresponding binary plans. There has been no published algorithm for evaluating general distinct attribute joins where there is no determinant relation. We must either use binary plans or materialize intermediate results similar to binary plans.

We will use join cardinality to further distinguish the multi-way join types. Joins are classified based on the relationship cardinalities of the inputs as follows. A multi-way join has one-to-one cardinality if for all inputs I , a tuple t determinant relation (many side) where a tuple from the determinant relation can join with at most one tuple in each of the other inputs. Many-to-many cardinality occurs when there exists at least one input I where a tuple t many-to-many relationship between any of the inputs. Star joins fall under the general one-to-many cardinality case.
For common attribute joins, join cardinality effects performance and duplicate detection. Slice join, but not MJoin, can process distinct attribute joins that are one-to-one or one-to-many (indirect partitioning). A summary of the different joins and what algorithms support them is in Fig. 5 .
 ated by the optimizer to determine the optimal multi-way join used. 3.1. Memory issues
The ratio of the memory size to the input size is a critical factor in join performance metrics including execution time, I/O operations performed, results produced over time, and number of results produced before the cleanup phase is performed. M is the main memory size allocated to the join. The probe relation is typically the largest relation (see Section 4.1 ).
A very low memory fraction results in almost no tuples being produced while the inputs are being read and almost all tuples produced during cleanup. A high memory fraction improves performance and tuples generated over time, and a mem-ory fraction above 1 results in an in-memory join for binary join plans. A memory fraction of 1 does not guarantee an in-memory join for MJoin because all inputs are treated as symmetric (unlike binary plans), and there is not enough memory relations at the same time. Since our definition of memory fraction excludes buffering the probe relation in memory, MJoin requires a memory fraction above 1 to guarantee an in-memory join.
 As a simplified, approximate illustration, consider a 3-way, common attribute, one-to-one join with a memory fraction F  X  0 : 2 (20%). In the ideal case, when memory is full we only keep the 20% of each input that is logically related together.
The algorithm will only output at 20% of the maximum rate (given full memory) and only 20% of the output tuples will be generated before cleanup. The actual output rate will be considerably lower if memory is not managed to keep logically related tuples together. It is possible to achieve higher than 20% output if tuples are discarded from memory when no longer required or by using disk-to-memory probes. 4. Slice join algorithm In this section, we explain the basic features of the slice join algorithm. Slice join performs the equi-join of N relations
R implemented as an array where each array entry is a pointer to a linked list of tuples that hash to that location. An example determine a bucket index in the range of 0 ; ... ; B 1.

The algorithm will accept a tuple from any input at any time. When the inputs are read from disk, the tuples are read in a round-robin fashion (by blocks not individual tuples), although the algorithm can control its performance by selecting alter-nate reading strategies as shown in [13] . When joining data streams, the tuples are processed as they arrive, and the algo-rithm may enter a disk-to-memory probe phase to generate additional results when no inputs are available to be processed.
When a tuple arrives, it is hashed to a bucket and inserted into the hash table for its input. It then probes the same bucket of all of the other hash tables. The probe sequence can vary based on the input and the current contents of memory. As probes are performed, intermediate results are generated. A probe terminates when the entire sequence has been performed or there are no intermediate tuples generated at any point during the sequence. At any time, if the memory usage is over the amount allocated to the operator, a partition is flushed.

The algorithm processes input tuples until there are none remaining at which point it enters the cleanup phase. During the cleanup phase, the on-disk partitions for each partition slice are loaded into memory and processed using a one-pass join.
The slice join algorithm is summarized in Fig. 7 . More details on the implementation of partitioning, flushing, and probing are in the following sections. 4.1. Partitioning
The algorithm determines the minimum number of partitions P based on the given memory size M for the join operator such that each partition slice will fit in memory for a one-pass join. Let there be N relations R each relation is denoted by j R i j , and the size of a tuple of that relation is given by size  X  R imum number of partitions is: duringthecleanupphase .Notethatwith N joinedrelations,only N 1mustbememoryresidentastheotherrelationistheprobe relation.Theproberelation R N istherelationwiththelargestsizeforcommonattributejoinsandisthedeterminantrelationfor fered in memory to complete the one-pass join. During the memory phase, all inputs can function as a probe relation.
The overhead factor considers both the overhead in the hash table structure and the storage of mapping functions for indi-rect partitioning. Overhead for the hash table structure is a small constant multiplied by the number of buckets in the hash head for each tuple or block of tuples for the pointers to construct the linked list.

Partition skew is handled similar to dynamic hash join [16,17] . Dynamic hash join allocates many more than the mini-mum number of partitions required, and partitions are dynamically flushed when memory is required. Allocating more par-example, if the minimum number of partitions is 10, slice join may use 20 partitions. This allows any individual partition to memory during the cleanup phase. 4.2. Probing
A probe is initiated when a tuple arrives from any input. The first task is to determine the correct bucket to probe in each hash table. For common attribute joins, every input tuple has the attributes required to compute the hash function to determine the bucket b to probe. Given bucket b , the tuple is used to probe that bucket in each of the other tables.

For distinct attribute joins using indirect partitioning, when a tuple arrives, its join attributes may not be the same attri-butes used for partitioning. In that case, its join attributes are used to lookup the correct bucket using a mapping function. mapping function is incomplete with no entry for the given join attribute(s), the tuple is added to an unknown list and no probe is performed.
 4.2.1. One-way probing
One-way probing only allows tuples to be generated using a single probe ordering. For a sequence of one-to-many joins, This probe may be initiated either when the tuple is received, before a flushing operation, or periodically by the algorithm.
One-way probing reduces the number of intermediate results generated but may result in a lower output rate. 4.2.2. Multi-way probing
In multi-way probing, each input initiates a probe and the probe sequence can vary by input. For MJoin [3] , a probe se-quence for an input is determined by sorting the join selectivities and performing the most selective joins first. This only tuple of Orders will probe Customer then Nation then finally LineItem . However, the best probe sequence at any point of time depends on the join selectivity and the number of tuples in each input.

We visualize join selectivities as a directed, weighted graph called the base graph . There is a directed edge e  X  X  n from node n i to node n j if there is a potential join between relations R number of distinct values of join attribute a . V  X  R i ; a  X  X j R
R input R j by the average number of tuples produced when probing R number of tuples in memory for each input, the average input arrival rate, and update the edge weights ( w tion is used to predict the best probe sequences for each input for the next time interval.

From the base graph, a derived graph is used to determine the optimal probe sequences for all inputs for a specified per-iod of time. The derived graph has a node n i for each relation R number of tuples in memory for relation R i and r i is the estimated arrival rate for relation R e  X  X  n i ; n j ; w 0 ij  X  from node n i to node n j is equal to w weight of the corresponding edge in the base graph, and t is a period of time. An edge weight w 0 the average expected number of intermediate tuples produced per probe over time t .

Using the derived graph, we can calculate the expected best probe sequence for each input during the specified time interval. A probe sequence is a series of N 1 edges e 1 ; e are w 1 ; w 2 ; ... ; w N 1 . Each weight w i is an edge weight from the derived graph (i.e. a w 0 optimal probe sequence at any point of time for an input is found by enumerating all the possibilities and selecting the one
The possible orderings are pre-computed, so no graph operations are ever performed. The number of intermediate tuples generated by a probe sequence is P N 2 i  X  1 w i Q i 1 j  X  0 Consider the join in Fig. 8 a where the arrival rates in tuples/second are Nation =1, Customer = 600, Orders = 6000, and Line-
Item = 5000. The base graph and derived graphs are in Fig. 8 b. For simplicity, assume the probe selectivities are the same throughout the join as given in the base graph, and that we re-calculate the probe orderings at one second intervals (i.e. erates 0 : 02 0 : 00034  X  0 : 0000068 output tuples. After 3 seconds, the rate for LineItem changes to 10,000 tuples/second.
Recalculating the probe sequence for Orders at 5 seconds results in probing Customer and Nation before LineItem . 4.2.3. Partition mapper implementation
Indirect partitioning implemented using bitmaps does not apply for early joins because inputs can be received in any or-der, and the mappers are used as they are being built. A mapping function is only complete after its input has been com-pletely read. A mapper is a dynamic index of (key, bucket index) pairs.
 We have tested two mapper implementations: Memory mapper : memory-resident chained hash table.
 Hybrid mapper : disk-resident index with small memory-resident hash table.

The hybrid mapper uses a small in-memory hash table storing recent mappings. When the hash table becomes full, it is et index) pair. For a mapping lookup, the lookup is first performed on the in-memory hash table then, if not found, on the mappings missed in the buffer previously. The LRU buffer has the benefit that it occupies less space and captures locality in the inputs.

The immediate resolution variation of the hybrid mapper performs a random I/O for each buffer miss of an arriving tu-ple. The delayed resolution approach treats any buffer miss as an unknown tuple and puts it in the unknown list. This avoids the random I/Os but increases the unknown list causing a slower cleanup and requires that the unknown list be probed using a sequential scan of the mapping file before flushing. We discuss the experimental results of the two ap-proaches in Section 5 .

Several optimizations are performed to minimize disk I/Os for the hybrid mapper. First, an in-memory bit array is main-fore writing so only a sequential pass of the disk file is required.
 The memory overhead of the approaches is in Fig. 9 assuming the mapper is allocated to handle E mapping entries. ptr-
Size is the size of a memory pointer, and entrySize is the size of a mapping entry that consists of a key value and bucket index. sizeBuffer is the number of mapping entries that are memory-resident in the LRU buffer. The space required for the mapping functions reduces the amount of memory available to store tuples in the hash tables and consequently the join output rate. The E = 8 term in the hybrid formula represents the space used for the bit array that detects occupied buck-ets on disk. 4.2.4. Handling unknown mappings
Every mapping function must handle the case of an unknown mapping where a request is made for a key that is not yet in the mapping. In this case, the tuple is placed in an unknown list and not inserted in the hash table. No probe is performed as there is no information on what partition to probe. A tuple whose mapping is unknown cannot produce an output tuple. The mapping would be known if the tuple it joins with had already arrived and created the required mapping. During flushing, titioning. Before the cleanup phase, the unknown list is read and partitioned (as the mapping functions are now complete). Two approaches for handling unknown mappings:
Lazy : The lazy approach stores the unknown tuples in a list structure that is bounded by a given memory size. When full, a ping and generate results.

Eager : The eager approach immediately resolves unknown tuples when the required entry is inserted in the mapping. This is done by organizing the unknown tuples into an in-memory hash table structure hashed by the lookup attribute in the mapping ( x for map  X  x  X ! y ). When the entry for x is added, the hash table is probed for all tuples waiting on entry x and results are generated. The hash table size is bounded. When full, a fraction of the hash table tuples are flushed to disk. 4.3. Flushing policy The flushing policy determines which partition to flush when memory is full. Slice join has two flushing policy variations. memory. Keeping entire slices in memory at cleanup also results in faster cleanup as the whole slice can be discarded as all join results would have already been generated.

The second variation is used with a background probe process and distinct attribute joins. This flushing policy first relation and is often the largest. This partition can be used as the probe partition for a disk-to-memory phase (streaming joins only).

For example using Fig. 6 , flushing slices would flush the same partition index at each source. For instance, flushing slice 4 Orders .

An important difference from MJoin is that when a partition is flushed from memory it is frozen [12,13] , and not rebuilt in memory. Thus, it is not possible to have a disk-to-memory phase for slice join when flushing entire slices as there will be no maximizes the number of tuples produced before cleanup. For streaming joins, flushing the largest partition first has benefits as more results can be generated using the disk-to-memory phase. 4.4. Duplication prevention
The duplicate prevention strategy depends only on the multi-way join cardinality. Duplicate prevention is achieved for one-to-many cardinality joins by discarding a tuple from the determinant relation (e.g. Lineitem ) once it produces a result. One-to-one joins are a special case of one-to-many joins. Once any tuple has produced an output, it is discarded.
Slice join only supports many-to-many cardinality joins for common attribute joins. Duplicates are prevented by assign-ing an arrival timestamp to each tuple of every relation. Timestamps are assigned using an increasing integer value that is used for all inputs. When a tuple arrives from any input, it is assigned the next integer timestamp. This timestamp must be flushed with the tuple when it is evicted from memory. To avoid the use of two timestamps used by MJoin [3] , the flushing departure timestamp (when it is flushed from memory) for each tuple. A departure timestamp for a tuple is either the time tition was flushed.

A result would have been generated before if all of its component tuples were in memory at the same time. All component flush timestamp. If any tuple arrives after the partition flush time, it will not find its matching tuples in memory, and the result tuple has not been generated before. When flushing the largest partition index first, a result tuple has been generated before if all its component tuples arrived before the last partition index was flushed or all tuples arrived before the entire slice was flushed and the tuple in the last input arrived last. When a disk-to-memory phase is added, the timestamp check is modified as discussed in the next section. Freezing partitions cuts the timestamp overhead in half and greatly simplifies duplicate detection. 4.5. Disk-to-memory phase
For streaming joins where disk I/Os are not the bottleneck, the output rate can be increased by using a disk-to-memory phase that produces results by joining on-disk partitions with in-memory partitions. The disk-to-memory phase allows the operator to generate results when inputs are delayed or blocked. Slice join only uses the disk-to-memory phase when using the flush largest policy. The disk-to-memory phase uses the on-disk file from the largest partition and probes the remaining partitions in memory at that index. Once an entire partition slice is flushed for all inputs, no disk-to-memory phase is pos-sible at that partition index.

For one-to-one and one-to-many joins, it is necessary to handle discards in on-disk probe files. The solution is to associate compared to all tuples is sufficiently high for a performance benefit. This avoids reading and re-writing a file every probe. The deleted bitmap does not have to be memory-resident and can be read when the file is read as the probe file. For many-to-many joins, slice join requires a single timestamp. Given a result tuple consisting of N component tuples
T ; T 2 ; ... ; T N , let maxArrivalAll be the maximum timestamp of all N tuples. Let maxArrival be the maximum timestamp of all tuples except T N . Let arrivalLast be the arrival timestamp of T
The disk-to-memory phase only applies with the flush largest policy which flushes all partitions of the last input ( N ) first then flushes slices (the same partition index of the remaining N 1 inputs). Let lastFlushTime be the timestamp when the partition index for tuple T N (the last input) was flushed. Let allFlushTime be the timestamp when the rest of the partitions at that index for the other inputs were flushed. Let lastProbeTime be the last time when the partition file of T as a disk-to-memory probe file. Note that lastFlushTime 6 the following is true: maxArrivalAll 6 lastFlushTime  X  all in memory at same time before any flush. maxArrivalAll 6 allFlushTime AND arrivalLast &gt; maxArrival  X  All tuples arrived before flushed slice except for the last one which arrived after all others and probed the remaining in-memory partitions. maxArrivalAll 6 allFlushTime AND lastProbeTime P maxArrival  X  The tuple was generated by the disk-to-memory phase where all component tuples except the last were in memory before flush and tuple was used as probe tuple in the disk-to-memory phase.

The disk-to-memory process is initiated when no inputs are available for processing, and it is estimated that a disk-to-memory probe will produce sufficient results to be valuable. The current configuration is to only switch to a disk-to-memory probe if at least 500 tuples will be generated during that probe. 4.6. Optimizer modifications and cost formulas
The optimizer must decide whether the performance of a multi-way plan is better than a binary plan of operators. The I/O cost formula for slice join (not including input reading costs) is Using this formula, a query optimizer can decide based on costs the benefit of using slice join versus a binary join plan.
The formula does not capture some of the other multi-way join benefits such as the adaptability to changing input arrival rates and the ability to dynamically share memory across all inputs joined.
 the selectivity for join i . Given N inputs, there are N 1 join selectivities. Assume the N inputs have arrivals rates r in tuples/sec. For an in-memory join, the number of tuples generated after time t is selectivities. The output rate between two time points t 1
Note that the maximum output rate of a plan with N inputs is directly proportional to the product of the fraction of the join i . If 20% of each relation has arrived, using Formula 3 we get T  X  t  X  X  1 have arrived. This explains the observation in [3] that sometimes an N -way join can be slower than two or more smaller multi-way joins. A multi-way join is back-end loaded and discards intermediate results that do not produce an output tuple.
Consequently, it does not  X  X  X pread out X  the work the same way binary joins that generate intermediate results would. If the join processor cannot perform as fast as required at the end of a multi-way join, the single N -way join could have substan-dard performance even though it may do less work throughout the join.

However, the case of a totally in-memory join is rare. Further, in a multi-user system there will be multiple joins execut-ing simultaneously and sharing resources. Not all joins will enter peak phase at the same time, and it is better for overall system performance to have each query do less work than optimize for single query performance. Further, the maximum cumulative output rate is the same regardless if a binary plan or multi-way joins are used. There is a fixed limitation based on the reading strategy from the inputs. The only difference is the number of intermediate results generated by the different plans. For instance, for a query joining 4 inputs executed using 3 binary joins, the maximum output rate could not exceed that of a four-way join (assuming no disk-to-memory process), and with limited memory would be much lower due to each join independently managing its memory. However, each individual join would generate intermediate results during the vidual query but would reduce the overall performance of a system processing many queries if saving intermediate results is more costly than re-generating them.

The output rate for joins that are not completely executed in memory is more complex. For this discussion, we will ignore the significant benefit of tuple discards which reduce the memory consumption of the slice join operator and consequently improve its I/O performance and output rate. The formulas apply directly to the many-to-many case. Assume the memory flushes. Until the first flush is performed, the number of tuples output is given by Formula 3 above, which can be re-written partitions in total that produce tuples.

The first flush occurs at time t f 0  X  M P N t ten to disk immediately. In general, flush time t f i  X  t up to each flush time t f i is given by:
The first term in Formula 4 represents the number of tuples generated by partitions that were totally memory-resident until the flush time. The second term is the number of tuples generated by each flushed partition.

Using this equation that returns the cumulative number of tuples output so far, it is possible to derive equations for the number of tuples produced between two flush times, the cumulative output rate, and the output rate in an interval between two flushes. Fig. 10 presents a four-way primary key join of relations of size 1,000,000 that all arrive at 1000 tuples/second and a memory size of 400,000 tuples (10% of all relation sizes). The data shows how the output rate builds as more inputs are received and peaks at the end of the join. The first partitions flushed generate very few tuples whereas the one that remains memory-resident generates the maximum number of tuples.

In summary, the optimizer can evaluate when slice join should be used in comparison to binary plans using the I/O cost ators as more intermediate results will be produced. Generation of intermediate results may improve individual query per-formance but is likely to degrade overall system performance in a multi-query system.
 5. Experimental results
Slice join was tested with a wide variety of query plans and memory settings. We compared slice join with MJoin [3] for query plans consisting of common attribute multi-way joins. A modified version of MJoin was also created that uses the same duplicate detection as slice join for one-to-one and one-to-many joins. The performance of both multi-way join algo-rithms is compared to binary join trees using XJoin [8] and EHJ [13] operators.

Two data sets are used for the experiments. The first data set is TPC-H with scale factor 1 GB. All key fields were replaced with random numbers, and the order of each relation was also randomized. For skewed data experiments, we used the skewed TPC-H generator developed by Microsoft Research [18] . The second data set consists of generated relations that fol-low the Wisconsin benchmark [10] . Each tuple has a size of 240 bytes, and a relation with size 1,000,000 tuples occupies 240 MB on disk. Each relation is randomized. Tuples are stored in binary form on disk.

The inputs are read from disk to the join operator which has a separate hard drive for temporary results. Tuples read from disk are read in a round-robin fashion at the maximum hard drive rate. To minimize random I/Os, round-robin reading from disk is performed at a larger granularity than a single tuple. In the experiments, 1000 tuples (multiple pages) are read from an input file before switching to the other. This preserves the balanced reading from all inputs while minimizing random I/O costs. Streaming joins are implemented by using a separate thread for each input that independently reads and buffers tuples distribution. The machine used for testing was an Intel Pentium 4 2.8 GHz with 4 GB of memory running Windows XP. All algorithms were written in Java. Both slice join and MJoin used the same multi-way hash table structure to prevent any dif-ferences in hash table implementation and performance. The page size used is 32K. Results are averages of five runs. 5.1. Tuning experiments
The tuning experiments tested some of the implementation decisions related to slice join: flushing policy, handling un-known mappings, and mapper implementations. 5.1.1. Optimal flushing policy
The major difference between MJoin and slice join is the flushing policy. The flushing policy affects the join output rate, the number of tuples generated before cleanup, and the overall performance in terms of time and I/Os. The flushing policies tested were:
Coordinated flushing (MJoin) : flush the same partition index from all inputs. The partition index is allowed to be rebuilt in memory.
 frozen and not rebuilt in memory.
 inputs). A partition index is frozen once flushed.

We conducted experiments testing the different flushing policies for disk-bound joins and streaming joins of various join
Gen is the percentage of output tuples generated before cleanup, Time is the overall execution time in seconds, and IOs is the number of tuple IOs in thousands. With no disk-to-memory phase and disk-bound joins, slice flushing is optimal in terms of results produced in memory and overall execution time. Flush largest initially performs better than coordinated flushing for smaller memory sizes, but as the memory allocated gets larger, flushing part of each partition slice results in longer cleanup times and fewer results generated.

The results for the same join but with slow, streaming input arrival rates are in Fig. 12 . In this table, we show the per-centage of tuples generated before cleanup ( BC% ), the percentage of tuples generated by the disk-to-memory phase ( BG%) , and the number of tuple IOs performed in thousands ( IOs ). With slow streams, execution time is directly dependent on the stream arrival rates, and there is little difference between the approaches.

A disk-to-memory phase is now valuable in generating additional results. Although the disk-to-memory phase with coor-dinated flushing generates an extra 5 X 10% of the results for memory fractions between 20% and 50%, this comes at a cost of 50 X 80% additional IOs. For a single user query, this extra local system burden may be acceptable, but it generally would be unacceptable for a multi-user integration system or query processing engine. For small memory sizes the disk-to-memory phase generates few extra results as the memory is heavily fragmented. The flush largest approach generates even more of its tuples during the disk-to-memory phase with a cost of up to double the base number of IOs. Interestingly, binary plans of
XJoin and EHJ operators (not shown) are competitive with the multi-way plans. XJoin especially exploits its disk-to-memory phase which generates a substantial number of its results.

Flushing a slice is optimal for streaming joins as well, and there is no overhead associated with the disk-to-memory phase. The disk-to-memory phase is less effective for N -way compared to binary joins as the probability that a probe tuple will find all its matching tuples in memory decreases as the number of sources increases.

For a many-to-many (inflating) join (see Fig. 13 ), flush slice generates significantly more results than the other two ap-proaches with no disk-to-memory phase. For most memory sizes, flush slice is also about 10% faster because the cleanup phase is faster. Although the disk-to-memory phase improves MJoin X  X  relative performance, the overall time is still longer with fewer results generated before cleanup. For the many-to-many join, the binary plans are three to five times slower due to the generation and flushing to disk of intermediate tuples. Note that the plans do not generate 100% of the results at a 100% memory fraction as memory fraction is defined in terms of the build relations and thus 100% memory fraction is only sufficient to store 75% of all relations.

For disk-bound determinant joins, flush slice and flush largest generate approximately the same number of tuples for all flushing an entire slice is the best policy even for determinant joins. 5.1.2. Handling unknown mappings
The lazy and eager approaches were compared. Unknown tuples were allocated 20% of the memory available to the hash table. If the unknown tuples did not occupy the full 20%, data tuples were allowed to use the memory. The combination of data and unknown tuples are not allowed to exceed the memory given to the hash table. For all memory sizes, the eager approach is superior as it produces the most tuples before cleanup and has the lowest overall time.
 5.1.3. Probe ordering
The experiment tested Query 1 as an in-memory join and eager unknown resolution at maximum rate. The optimal solu-tion has the probe sequence for Orders of probing LineItem then Customer . The suboptimal ordering probes Customer before LineItem . One way probing probed only on arrivals from LineItem .

The suboptimal ordering performs 10% more probes and generates 12% more intermediate tuples than the optimal order-ing. One way probing performs 12% fewer probes and generates 1% fewer tuples than the optimal ordering at a slight sac-rifice of join output rate. Probe ordering has a minor performance impact compared to the flushing policy when handling joins larger than the memory size. 5.1.4. Mapper implementation
The memory and hybrid mapper implementations are tested for Query 1. This requires a mapping function for Orders with 1,500,000 entries. The hybrid mapper is given 15% of the memory allocated to the join operator. By memory fraction 25%, the hybrid mapper is a memory mapper as the hash table fits in the 15% memory allocation. For smaller memory sizes, the hy-brid mapper generates more tuples before cleanup as it occupies less space, but has a slower output rate due to the disk IOs performed.

The mapper is a critical component for processing distinct attribute joins. The ideal case is for sufficient memory to com-pletely store the mapper in memory. The hybrid mapper is beneficial for small memory sizes (&lt;10%) and for streaming joins.
Of the two mappers, the delayed resolution mapper that did not perform a random I/O on a cache miss was the best ap-proach. A hybrid mapper should only be used with memory limitations or with slow input arrival rates. Inputs that exhibit locality improve its performance as the buffer hit rate is higher. 5.2. Common attribute joins
These experiments performed a four-way common attribute join where the inputs were four randomly permuted copies of the Wisconsin relations. Binary joins were each allocated one third of the memory available. The result has 1,000,000 tuples. 5.2.1. Maximum rate join
The results of this four-way join when all inputs are read at maximum rate from disk are shown in Fig. 14 a. The memory fraction was 20% (144 MB) of the build relation sizes (720 MB). Slice join is 25% faster than both versions of MJoin and 30 X  40% faster than binary plans. Slice join also generates more results before cleanup as shown in the previous discussion on flushing policies.

Fig. 14 b compares the overall execution time of slice join versus the other algorithms for various memory fractions. Reg-ular MJoin (with timestamps) is considerably slower for all memory sizes. Modified MJoin is slower for smaller memory frac-execution time for slice join and Y is the execution time for another join algorithm. For example, at memory fraction 40%, memory fraction of 50% (137 s versus 20 s). This chart shows two important results. First, it is critical that early join algorithms exploit join cardinality to improve their performance. Without discarding tuples when they produce results, flushing in terms of time and results generated. 5.2.2. Streaming joins
We tested streaming joins with various input arrival rates. For slow streams, although slice join is more space efficient and generates more tuples before cleanup than MJoin, the overall times are dominated by the arrival rates and are within time becomes less of a factor. The scenario presented is where inputs 1 and 2 arrive initially at 20,000 tuples/second then switch to 50,000 tuples/second after 500,000 tuples are read. Inputs 3 and 4 start at 50,000 tuples/second and switch to 20,000 tuples/second after 500,000 tuples are read. The output rate for 20% memory is in Fig. 15 . 5.2.3. Inflating joins
For many-to-many (inflating) joins on identical attributes, there is a large difference between slice join and MJoin and binary operators. Fig. 16 a shows the results for a four way join with an inflation factor of 2 with memory fraction 50%. phase is faster because partition slices that were never flushed are discarded immediately. The binary plans are considerably slower because of the generation and flushing of intermediate tuples. 5.3. Distinct attribute joins MJoin cannot be applied when the joins are on distinct attributes, so slice join is compared to binary plans consisting of
EHJ and XJoin operators. Both the EHJ and XJoin implementations were configured to have the same duplicate prevention techniques as slice join. The major differences between the binary plans and slice join are that slice join uses indirect par-titioning to avoid intermediate tuple materialization and manages memory to maximize its output rate.
Unlike the common attribute join case where each binary join operator was allocated the same amount of memory, for these experiments the optimal memory allocation differed for each operator. Since the Customer build table is significantly smaller than the Orders build table, once sufficient memory for all of the Customer table was available, any left over that would usually be allocated to that join was given to the Orders join. This is an ideal case for binary operators which is not often achievable due to cost optimizers assigning static memory sizes to all joins and the difficulty in sharing memory across operators dynamically during execution. 5.3.1. Maximum rate join
Fig. 17 a illustrates the comparison of slice join versus a binary tree of operators for Query 1 with a memory fraction of 20% at maximum rate. Slice join has better overall execution time and output rate. Fig. 17 b displays the percent difference be-tween the output rate of slice join and XJoin/EHJ. Slice join outperforms the binary plans and is producing results faster. By the end of the join, slice join has a higher overall rate that is 50% faster than XJoin and 60% faster than EHJ.
We also tested if skew in the data set would affect the results. The experiment used TPC-H data generated using a Zipfian of the extra partitions required to guarantee against recursive partitioning. The extra partitions require more page buffers and increase the random I/Os when flushing these page buffers. Importantly, slice join functions without requiring recursive partitioning and maintains its relative advantage over the binary algorithms. Note in Fig. 18 b that at 30% memory fraction the Customer table is completely in memory resulting in the significant improvement for the binary plans. 5.3.2. Streaming joins
Various fixed and variable rate streaming join configurations were tested. For all stream speeds and memory fractions where the Customer build table is not completely memory resident for the binary plans, slice join has the same or faster over-all time primarily because its cleanup phase is faster. For binary plans, two cleanup phases must be performed which involve the generation of intermediate tuples.

Slice join X  X  output rate is generally faster than the binary plans overall. However, slice join does not always have a faster head of storing the mapping function for slice join. This mapping function consumes space that would otherwise be used to buffer tuples in memory and increase the join output rate. The advantage of the mapping function is avoiding IOs due to extra partitioning, but this benefit has less of an impact at the start of the join.

The results for a fast stream where all inputs arrive at 100,000 tuples/second are in Fig. 19 a. The results for a variable 100,000 for LineItem but changing to 50,000 after 1,000,000 LineItem tuples were read. These figures show the percentage difference in output rate of EHJ and XJoin compared to slice join.
 faster. Slice join generates more results faster throughout the join by keeping partition slices in memory as compared to the binary plans that independently keep partitions in memory.
 will not find a mapping and be put in the unknown list. The tuples in the unknown list are flushed to disk as required to save mapping is available, and the join output rate increases beyond the binary plans. In comparison, the binary operators do not require mapping of LineItem tuples that will be partitioned and joined with Orders then Customer . Although the binary plans do not guarantee that an incoming LineItem tuple will find its matching tuples in the other relations (due to both the tuple arrival ordering and the join flushing strategies), on average more of the earlier arriving LineItem tuples will find matches.
Thus, one additional factor to consider when selecting slice join over binary plans is the arrival rate of the relation used to generate the mapping ( Orders in this case). The faster the mapping can be generated, the better the overall performance relative to the binary plans. 5.4. Multi-way joins on the NEXRAD data set
Slice join was tested on a real-world data set extracted from the NEXRAD archive system [19] , which is a multi-institution research project involving the University of Iowa, Princeton University, the US National Climatic Data Center (NCDC), and
Unidata. The goal of this project is to produce an Internet-accessible archive and analysis system that processes the weather radar data collected by over 150 radars in the United States. This data is used for severe weather forecasting (hurricanes and tornadoes) as well as rainfall estimation and flood prediction.
 The current archive has over 8 million radar data files (representing 8 terabytes of data) and millions of derived products.
Each file and product is replicated at several locations. For the experiment, we extracted 1 million radar data files each of information on the radar data files including descriptive metadata, derived products, and the locations of raw data files and products. The sizes of the files are in Fig. 20 .
 An example query is in Fig. 21 that returns the locations of all data products derived from raw radar files produced since future as the metadata tables themselves will be distributed across institutions due to performance and political reasons.
MJoin cannot process this join as all relations are not joined on the same attributes. Slice join is compared to binary plans of XJoin and EHJ operators. The data was streamed into the operator at 100,000 tuples/second for all three inputs. The results for 20% memory and for all memory sizes are in Figs. 22 a and b, respectively.

Slice join generates results faster than the binary plans. It is also faster overall except for the larger memory fractions where the entire RadarData table can fit into memory and consequently the binary plans do not flush intermediate results overall rate as its cleanup phase is faster. 5.5. Summary of results
Overall, slice join supersedes MJoin as it allows both common and distinct attribute joins, while providing equivalent or better performance over all memory sizes, even when MJoin is adapted to use slice join X  X  duplicate detection policy. A major up phase begins. This difference occurs because slice join generates more tuples during the memory phase and eliminates tuples from memory during duplicate detection. Although MJoin performs coordinated flushing, it allows a flushed partition to be rebuilt in memory, which has the effect of leaving fragments of many partitions both memory and disk-resident. It is memory and disk-resident. Although MJoin benefits from slice join X  X  duplicate detection, it cannot take full advantage of it due to its flushing policy.

Slice join, and multi-way joins in general, generate results faster and are more efficient than binary plans when joining on a common attribute, especially for many-to-many (inflating joins). This improvement is because binary plans materialize and flush intermediate tuples.

Slice join has the ability to process distinct attribute, determinant-type, multi-way joins. This greatly expands the number associated I/O operations. The cost formulas for slice join and the binary plans are used to estimate the relative performance and decide when to use each operator. The slice join formula includes the overhead of the mapping function which is directly dependent on the cardinality and key size of the mapped relation ( Orders in the experiments). Slice join has better perfor-mance with a smaller mapping function which occurs when the size of Orders is smaller. Although slice join is not universally faster than binary plans for distinct attribute joins, slice join offers significant performance improvement when binary plans require multiple partitioning steps even with the overhead of the mapping function. Slice join has an even greater impact mal amount of memory to each join in the binary plan. Slice join manages memory allocation dynamically at run-time and there is no concern on how to allocate memory between operators at compile or execution time. 6. Conclusion
This paper presented slice join, a multi-way join operator capable of joining inputs even if they do not share common attributes. Slice join has a simpler duplicate detection policy than MJoin and has better performance for common attribute joins. Slice join is applicable to many more join plans and has superior performance to the equivalent binary plans. Slice join X  X  self-adaptive nature avoids the effect of poor estimates during join tree selection by the optimizer. It is better than uted query processing where subquery results are sent over the network to the system performing the join. Slice join pro-duces results even with network delays, which allows local processing to occur while results are retrieved over the network.
This parallelism results in faster execution times and is valuable when the database query is part of a larger execution pipe-line. Future work includes testing the performance of slice join in federated and integration systems.
References
