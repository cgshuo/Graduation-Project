
School of Computer Science, Shaanxi Normal University, Xi X  X n, Shaanxi, China
School of Electronic Engineering, Xidian University, Xi X  X n, Shaanxi, China
School of Information Systems, Computing and Mathematics, Brunel University, London, UK
College of Information Engineering, Shenzhen University, Shenzhen, China CAS Research Centre on Fictitious Economy and Data Science, Chinese Academy of Sciences, Beijing, China 1. Introduction
Standard support vector machines (SVMs) [1 X 4], introduced by Vapnik et al., are an excellent tool for binary classification problems and have been successfully and widely applied in many fields [5 X 10]. One key research topic about SVMs has been to develop the efficient learning algorithms and models. Over the past few decades, many improvements to SVMs have emerged, such as lagrangian support vector machines (LSVM) [11], a smooth support vector machine for classification (SSVM) [12], reduced SVMs (RSVM) [13], least squares support vector machine classifier (LS-SVM) [14], proximal support vector machine classifiers (PSVM) [15], and the generalized eigenvalue proximal SVMs (GEPSVM) for multiclass classification problems [16].

Recently, Jayadeva et al., motivated by GEPSVM, proposed a twin support vector machine (TWSVM) classifier for binary classification problem [17]. TWSVM determines two nonparallel hyperplanes by solving two smaller and related Quadratic Programming Problems (QPPs), in which each hyperplane is closer to one of the two classes and is as far away as possible from the other class. The strategy of solving two smaller QPPs rather than solving a single large QPP in traditional SVMs makes the learning speed of TWSVM approximately four times faster than that of a classical SVMs, whilst overcoming the potential problem of the exemplar unbalance in binary classification problems by introducing two penalty variables for two classes. Some extensions of TWSVM have been made, including a smooth TWSVM [18], least squares twin support vector machines for pattern classification (LS-TSVM) [19], nonparallel plane proximal classifier (NPPC) [20,21], and twin support vector machine for regression (TSVR) [22]. In addition, Cong et al. applied TWSVM to text independent speaker recognition, and obtained better results than that of traditional SVMs obtained [23].

SVMs were originally developed for binary classification problems. How to effectively extend it to multiclass classification problem s is still an ongoing research issue. Currently there are two kinds of approaches for multiclass SVMs. One is by constructing and combining several binary classifiers, while several algorithms have been proposed based on these two types of approaches. In particular the follow-ing models are widely discussed: One-Versus-All support vector machines (OVA-SVMs) [2,24] where one class is separated from the remaining classes; One-Versus-One SVMs (OVO-SVMs) [24] where any one class is separated from any other class; Error-correcting-output code SVMs (ECOC SVMs) [25] where error correcting codes are used for improving the generation ability; directed acyclic graph SVMs (DAGSVMs) proposed in [26,27], in which the training phase is the same as One-Versus-One sup-port vector machines by solving k ( k  X  1) / 2 binary SVMs, but its testing phase is different from the One-Versus-One SVMs; all-at-once SVMs [2] where all the decision functions are determined at once; multicategory proximal support vector machine classifiers (MPSVM) [28] which extend PSVM [15] to sion of LS-SVM [14] for multicategory.

However, the speed in learning a model and the method for dealing with the potential unbalance of ex-emplars in different classes are still two main problems for multiclass classification problems in SVMs. TWSVM overcomes the exemplar unbalance problem in two classes by choosing two different penalty lems. We combine TWSVM with the well known and simple one-versus-all methodology which has been very popular in solving multiclass classification problems to propose one-versus-all twin support vector machine classifiers (OVA-TWSVM) for multiclass classification problems. In our algorithm we combine the advantages of TWSVM and the well-known one-versus-all approach to learn a classifier for the multiclass classification problem. Our OVA-TWSVM consists of solving k QPPs, one for each class, so that we obtain k nonparallel hyperplanes for k classes, respectively. In OVA-TWSVM we use one-versus-all approach to construct a TWSVM classifier, where in i th TWSVM classifier, we only solve one QPP corresponding to the i th class to determine the hyperplane for the i th class. We overcome the unbalance problem of exemplars existing in i th TWSVM by choosing the proper penalty variable C i for the i th class which TWSVM supported. Solving one QPPs in one TWSVM classifier guarantees the speed of learning the model for the k -category classification problems. Extensive experimental com-parisons of OVA-SVMs, OVO-SVMs, DAGSVMs and our OVA-TWSVM classifier have been made on six UCI benchmark datasets. Experimental results show that our OVA-TWSVM classifier achieved better performances than OVA-SVMs, while still giving comparable performances to OVO-SVMs and DAGSVMs.
 The paper is organized as follows: Section 2 introduces the basic notations we use in this paper. Section 3 briefly describes TWSVMs and its properties. Section 4 introduces our proposed multiclass TWSVM classifier and gives the detailed analysis for it. At the same time, linear and nonlinear OVA-TWSVM classifier algorithms are described in Subsections 4.1 and 4.2, respectively. In Section 5 we demonstrate the experimental results of our OVA-TWSVM and other three SVMs for multiclass classi-fication. Finally, the paper is concluded in Section 6. 2. Notations
In this paper, all vectors will be column vectors unless transformed to a row vector by a prime super-script . A column vector of ones in real space of arbitrary dimension will be denoted by e .Foramatrix scalar (inner) product of two vectors x and y be denoted by x y and the 2-norm of x will be denoted by x .Formatrix A  X  R m  X  n and B  X  R n  X  k ,thekernel K ( A, B ) maps R m  X  n  X  R n  X  k into R m  X  k .In in
R m . We will make use of the following Gaussian kernel that is frequently used in SVM literatures in our experiments: will be denoted as I in our paper. 3. Twin support vector machines
In this section, we give a brief outline of TWSVM. Consider a binary classification problem of classi-fying m 1 data points belonging to class + 1and m 2 data points belonging to class  X  1in n -dimensional real space R n . Let matrix A  X  R m 1  X  n represent the data points of class + 1andmatrix B  X  R m 2  X  n represent the data points of class  X  1. The linear TWSVM classifier aims at generating two nonparallel hyperplanes in R n : such that each hyperplane is closer to datapoints of one class and furthest from the datapoints of the other class. A new data point is assigned to class + 1or  X  1 depending on its proximity to the two nonparallel hyperplanes. The concept of linear TWSVM is geometrically depicted in Fig. 1 for a simple two dimensional example on an artificial dataset.

The idea of linear TWSVM is to solve the following pair of QPPs Eqs (2) and (3), where C 1 ,C 2 &gt; 0 vectors of ones of appropriate dimensions.
The first term in the objective functions of (2) or (3) is the sum of squared distances from the hyper-plane is closer than this minimum distance of 1. The second term of the objective function minimizes  X  1.

The Wolfe dual of QPPs Eqs (2) and (3) are as follows Eqs (4) and (5) in terms of the Lagrangian where H =[ Ae 1 ] and G =[ Be 2 ] . where P =[ Ae 1 ] and Q =[ Be 2 ] .

The nonparallel hyperplanes Eq. (1) can be obtained from the solution of QPPs Eqs (4) and (5), as given in the following Eqs (6) and (7), respectively.
TWSVM was also extended to handle nonlinear classification problems by considering the following nonparallel kernel-generated surfaces Eq. (8).
 where C = nonlinear TWSVM is depicted in Fig. 2 for a simple two dimensional example on an artificial dataset.
The primary QPPs of nonlinear TWSVM corresponding surfaces Eq. (8) are given in Eqs (9) and (10).
The Wolfe duals of QPPs Eqs (9) and (10) are as follows Eqs (11) and (12), respectively. where S =[ K ( A, C ) e 1 ] , R =[ K ( B,C ) e 2 ] . where L =[ K ( A, C ) e 1 ] , N =[ K ( B,C ) e 2 ] .

The two surfaces can be obtained from the solutions of QPPs Eqs (11) and (12), as given in Eqs (13) and (14), respectively. given by x w (1) + b (1) =0 or K ( x ,C ) w (1) + b (1) =0 . Taking motivation from standard SVMs, one an important role in determining the required hyperplane and vice versa.

In linear or nonlinear TWSVM, solving two dual QPPs has the advantage of bounded constraints and reduced number of parameters as QPP Eqs (4) or (11) has only m 1 parameters and QPP Eqs (5) or (12) has only m 2 parameters, when compared with the conventional SVMs which have m = m 1 + m 2 parameters. As a result, TWSVM is approximately four times faster than the conventional SVMs. This is because the complexity of the conventional SVMs is no more than O ( m 3 ) , but each dual problem solved in TWSVM is roughly of size m/ 2 . Thus, the ratio of run times is approximately
In addition, TWSVM requires solving only one quadratic problem which corresponds to the important class when handling preferential classification problems that have traditionally been handled by the FSVM [30] and FPSVM [31] approaches. In many instances, m 1 m 2 and a classifier may be obtained very rapidly by solving the smaller problem. TWSVM can also choose different penalty parameters C 1 and C 2 in terms of classification with unbalanced data sets. 4. One-versus-all twin support vector machines
In this section, we propose a new k -category classifier, one-versus-all twin support vector machine classifier, which we will term as OVA-TWSVM. As mentioned earlier, TWSVM obtains two nonparallel hyperplanes by solving two comparative smaller QPPs, one for each class. Based on this idea, we extend TWSVM to solve multicategory data classification problems.

Given a dataset containing m datapoints represented by A  X  R m  X  n , each element is labeled by one of i  X  X  1 , 2 ,...,k } and m = m QPP Eq. (17). Therefore, minimizing it means to keep the data points of class i clustered around the hyperplane. The second term of the objective function minimizes the sum of error variables, thus trying to minimize to be at a distance of at least 1 from points of the other k  X  1 classes. 4.1. Linear one-versus-all twin support vector machines
The linear OVA-TWSVM classifier obtains k nonparallel hyperplanes by solving k QPPs, one for each class, around which the corresponding data points get clustered. We can classify points according to which hyperplane a given point is closest to.
 The Lagrangian corresponding to the QPP Eq. (17) is given by multipliers. The Ka rush-Kuhn-Tucker (K.K.T) necessary and s ufficient optimalit y conditions [3] for Eq. (18) are given by Since  X  0 , from Eq. (21) we get Eq. (25). Next, combining Eqs (19) and (20) leads to Eq. (26). Then we define Eq. (27), with these notations, Eq. (26) can be rewritten as Eq. (28).
 Because E E is always positive semidefinite, we can introduce a regularization term  X I ,  X &gt; 0 ,totake care of problems due to possible ill-conditioning of E E . Here, I is an identity matrix of appropriate dimensions. Therefore, Eq. (28) can be modified to Eq. (29). However, in the following, we shall continue to use Eq. (28) with the understanding that, if needed, Eq. (29) is to be used for the determination of u i .

Using Eq. (18) and K.K.T. conditions, we can obtain the Wolfe dual of QPP Eq. (17) as follows: 1 , 2 ,...,k ) planes given by Eq. (31) it lies closest to, i.e.,
According to TWSVM, we can define such patterns of the other k  X  1 classes for which 0  X  j C i important role in determining the required hyperplane.
 For clarity, our linear OVA-TWSVM is described in the following algorithm 1.
 Algorithm1 Linear OVA-TWSVM (i) Start with i =1 . (ii) Iterate (iii), (iv) and (v) until i = k . (iii) Define A and (v) Define E =[ A i ,e i ] ,and F =[ 4.2. Nonlinear one-versus-all twin support vector machines
In this section, we extend our linear OVA-TWSVM to nonlinear OVA-TWSVM by considering the following k kernel generated surfaces Eq. (33).
 where K is an appropriately chosen kernel. The primal two QPPs of nonlinear OVA-TWSVM can be modified to the QPPs as showed in Eq. (34).  X  e The Lagrangian corresponding to the problem Eq. (34) is given by the following Eq. (35), We can obtain the K.K.T conditions for Eq. (35) as the following Eqs (36) to (41).
 Since  X  0 , from Eq. (38) we have the Eq. (42). Combining Eqs (36) and (37), we get the Eq. (43). Define Then, Eq. (43) can be modified as Eq. (45), The Wolfe dual QPPs of Eq. (34) is given as follows Eq. (46), Once the k QPPs Eq. (46) are solved to obtain the k hyperplanes of Eq. (33), a new pattern x is assigned to class i ( i =1 , 2 ,...,k ) in a similar way to the linear case.
 Here, we will give an explicit statement of our nonlinear OVA-TWSVM algorithm.

Given a dataset containing m data points represented by A  X  R m  X  n , each element is labeled by one Algorithm2 Nonlinear OVA-TWSVM 4.3. Complexity analysis of one-versus-all twin support vector machines
In the OVA-SVMs classifier for k -category data classification, it requires solving k Wolfe dual QPPs, one of which contains m parameters, so the complexity of the conventional one-from-rest classifier is no more than k  X  m 3 . However, OVA-TWSVM only solves k Wolfe duals of QPP Eq. (30) for linear or m/k . Thus, each Wolfe dual QPP of Eqs (30) or (46) contains of m runtime of OVA-SVMs to OVA-TWSVM is approximately as: That is, our OVA-TWSVM classifier is approximately ( k k  X  1 ) 3 times faster than traditional OVA-SVMs the OVA-SVMs will degenerate to classical SVMs and has the complexity of m 3 , whilst OVA-TWSVM to TWSVM and has 2  X  ( m 2 ) 3 complexity, so the proportion of runtime between them is m 3 2  X  ( m 5. Experimental results and their analysis
To evaluate our OVA-TWSVM classifiers we investigate results in terms of accuracy and execution time on several synthetical datasets and some publicly available benchmark data sets from UCI ma-chine learning repository [32], which are commonly used in evaluating machine learning algorithms. All experiments are implemented in MATLAB 7.0 environment.

We first compare the performance of traditional OVA-SVMs and our OVA-TWSVM on synthetic datasets. After that we compare both linear and nonlinear kernel classifiers of OVA-SVMs, OVO-SVMs, DAGSVMs and our OVA-TWSVM on benchmark datasets from UCI machine learning repository. For the implementation of all algorithms we have used the optimizer code  X  X p.dll X  from Gunn SVM tool box [33]. Generalization error is determined by following the standard 10-fold cross-validation method-ology [34]. 5.1. Numerical experiments on artificial datasets
The OVA-TWSVM was tested on two synthetically generated datasets which are linear and nonlinear separable, respectively. Theses datasets are illustrated in Figs 3 and 4 respectively. The performances of our OVA-TWSVM on the two synthetic datasets are shown in Figs 5 and 6, respectively. Figure 5 is the result of our OVA-TWSVM classifier with a linear kernel on the simple two dimensional example OVA-TWSVM classifier with an RBF kernel on the synthetic data set which is nonlinear separable. From Figs 5 and 6, we can observe that the three classes are well separated. Tables 1 and 2 summarize the training accuracy and training time in seconds on the two synthetic datasets, respectively. From the results in these two tables, we can see that on simple synthetic datasets our OVA-TWSVM outperforms traditional OVA-SVMs in terms of both classification accuracy and run time. This result supports the theoretical analysis presented in Section 4.3 which suggested that OVA-TWSVM should be faster than traditional OVA-SVMs. While these results are promising, it is necessary to examine whether these results also apply to more complex real world data sets. The next section therefore describes further experiments which were conducted with benchmark datasets from the UCI machine learning repository. 5.2. Numerical experiments on UCI datasets
Now we summarize the performances of our OVA-TWSVM on some benchmark data sets available from the UCI machine learning repository. The properties of each dataset from UCI machine learning extended to include comparison with two further multiclass classifiers: one-versus-one support vector machines (OVO-SVMs) and directed acyclic graph support vector machines (DAGSVMs). Section 5.2.1 linear classifiers.
 5.2.1. Numerical experiments using linear classifiers
Here we compare the performances of OVA-SVMs, OVO-SVMs, DAGSVMs and our OVA-TWSVM classifier with a linear kernel.

The value of C in each method is chosen using a tuning set extracted from the training set. In order to find an optimal value for C the following tuning procedure is employed on each fold:
A random tuning set of the size of 10% of the training data is chosen and separated from the training dataset. The remaining 90% of the training data is trained by above four methods using values for C equals to 2 i ,where i =0 , 1 ,..., 25 .Thevalueof C that gives the highest accuracy on the tuning set will be chosen.

OVA-SVMs, OVO-SVMs, DAGSVMs and our OVA-TWSVM are trained using the chosen C on all accuracy of 10-fold cross validation of four methods with a linear kernel where the best accuracy is in bold.
 From the accuracy data shown in Table 4 it is clear that the OVA-TWSVM outperforms the traditional OVA-SVM for all data sets in terms of accuracy. However, the picture is less clear when we compare OVA TWSVM to the other methods. For the Vowel data set the 10-fold test accuracy of OVO-SVMs and DAGSVMs are much higher than that of OVA-SVMs and OVA-TWSVM, and the OVO-SVMs gets the highest accuracy on this data set; and on Segment and Wine data sets the OVO-SVMs and DAGSVMs methods get a slightly better test accuracy than OVA-SVMs and our OVA-TWSVM do, and these two methods achieve the best performance, respectively, on these two data sets; while about Glass, Iris and Vehicle three data sets, that our OVA-TWSVM has got the best accuracy on testing data sets. In summary, the OVA-TWSVM method performs best on three data sets, OVO-SVMs on two, and DAGSVMs only on one. 5.2.2. Numerical experiments using nonlinear classifiers For the nonlinear case, we compare nonlinear OVA-SVMs, OVO-SVMs, DAGSVMs and our OVA-TWSVM classifier. In all experiments, a RBF kernel function is used. In order to find the optimal value for C and for the RBF kernel function parameter  X  , a tuning procedure similar to that employed in the linear case is performed. Values of C are taken equal to 2 i ,i =5 , 6 ,..., 35 . Values for  X  are taken methods in order to reduce even more the computational time while maintaining the accuracy achieved by using the full kernel. For the Segment dataset, we have employed a rectangular kernel [13] with an would fit in memory (2310  X  350 instead of 2310  X  2310).
 Table 5 shows the testing accuracy of 10-fold cross validation experiments of four methods with an RBF kernel. The bold figure in each row means the highest accuracy on the data set at that row.
Experimental results in Table 5 show that OVA-TWSVM again outperforms traditional OVA-SVMs in all conditions. Overall there is not much difference between the test accuracy of the four methods on all the six data sets. Our OVA-TWSVM outperforms the other three SVMs on Glass and Iris datasets; while OVO-SVMs is the best one on Wine, Vowel, Vehicle and Segment datasets.
 Table 6 displays the average training time of 10-fold cross validation experiments in seconds of OVA-SVMs, OVO-SVMs, DAGSVMs, and our OVA-TWSVM with an RBF kernel function on six benchmark datasets from UCI machine learning repository, where the bold figures are the best whist the minimum training time.

In terms of training time, we can see from Table 6 that the experimental data from benchmark data sets again supports the theoretical findings and findings from the synthetic data sets, with OVA-TWSVM outperforming traditional OVA-SVMs in all conditions. When contrasting it with OVO-SVMs and DAGSVMs, OVA-TWSVM is doing much better on Glass and Vehicle datasets while OVO-SVMs fare better on the other datasets.

In fact, the OVO-SVMs and DAGSVMs methods have the same training procedure, where we have to Table 6 that among the OVO-SVMs, DAGSVMs, and OVA-TWSVM methods, the OVA-TWSVM is a little slower on the training time on most occasions. However, our OVA-TWSVM is dramatically faster than traditional OVA-SVMs on training time. 6. Conclusion
In this paper, we extend TWSVM to solve multi-category data classification problems, and pro-pose one-versus-all twin support vector machine (OVA-TWSVM) classifiers for multiclass classification problems. In OVA-TWSVM, we solve quadratic programming problems and obtain nonparallel hyper-planes, one for each class. In each TWSVM, we only solve the quadratic programming problem for analysis of our OVA-TWSVM uncovers its efficiency. Experimental results on synthetic datasets and on benchmark datasets from UCI machine learning repository show that our OVA-TWSVM classifier achieves consistently better performances than traditional OVA-SVMs. The picture becomes more mixed when OVA-TWSVM is compared to OVO-SVMs and DAGSVMs, but the results are promising as there are benchmark datasets where OVA-TWSVM outperforms these methods. Future work would include improving the computational efficiency of obtaining the optimal parameters for the kernel function of SVMs.
 Acknowledgements
We are most grateful to A Frank and A Asuncion for the assistance of the useful benchmark data sets as well as A Rakotomamonjy et al. who provide the helpful SVM tool box. We would also like to thank Jayadeva et al. who provide the TWSVM codes for reference. This work is supported in part by the grant of the Fundamental Research Funds for the Central Universities of GK201102007 in PR China, and is also supported by Natural Science Basis Research Plan in Shaanxi Province of China (Program No. 2010JM3004), and is at the same time supported by Chinese Academy of Sciences under the Innovative Group Overseas Partnership Grant as well as Natural Science Foundation of China Major International Joint Research Project (NO.71110107026).
 References
