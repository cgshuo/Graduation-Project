 More effective information access is beneficial to deal with the increasing amount of broadcast news speech. Many attempts have been made in the past decade to build news browser, spoken document retrieval system, and summarization or question answering system to effec-tively handle the large volume of news broadcast speech (e.g., the recent DARPA GALE program). Structural in-formation, such as story segmentation or speaker cluster-ing, is critical for all of these applications. In this paper, we investigate automatic identification of the speakers X  roles in broadcast news speech. A speaker X  X  role (such as anchor, reporter or journalist, interviewee, or some soundbites) can provide useful structural information of broadcast news. For example, anchors appear through the entire program and generally introduce news stories. Re-porters typically report a specific news story, in which there may be other guest speakers. The transition be-tween anchors and reporters is usually a good indicator of story structure. Speaker role information was shown to be useful for summarizing broadcast news (Maskey and Hirschberg, 2003). Anchor information has also been used for video segmentation, such as the systems in the
In this paper, we develop algorithms for speaker role identification in broadcast news speech. Human tran-scription and manual speaker turn labels are used in this initial study. The task is then to classify each speaker X  X  turn as anchor , reporter , or other . We use about 170 hours of speech for training and testing. Two approaches are evaluated, an HMM and a maximum entropy classi-fier. Our methods achieve about 80% accuracy for the three-way classification task, compared to around 50% when every speaker is labeled with the majority class la-
The rest of the paper is organized as follows. Related work is introduced in Section 2. We describe our ap-proaches in Section 3. Experimental setup and results are presented in Section 4. Summary and future work appear in Section 5. The most related previous work is (Barzilay et al., 2000), in which Barzilay et al. used BoosTexter and the max-imum entropy model to classify each speaker X  X  role in an English broadcast news corpus. Three classes are used, anchor, journalist, and guest speaker, which are very similar to the role categories in our study. Lexical features (key words), context features, duration, and ex-plicit speaker introduction are used as features. For the three-way classification task, they reported accuracy of about 80% compared to the chance of 35%. They have in-vestigated using both the reference transcripts and speech recognition output. Our study differs from theirs in that we use one generative modeling approach (HMM), as well as the conditional maximum entropy method. We also evaluate the contextual role information for classifi-cation. In addition, our experiments are conducted using a different language, Mandarin broadcast news. There may be inherent difference across languages and news sources.

Another task related to our study is anchor segmen-tation. Huang et al. (Huang et al., 1999) used a recog-nition model for a particular anchor and a background model to identify anchor segments. They reported very promising results for the task of determining whether or not a particular anchor is talking. However, this method is not generalizable to multiple anchors, nor is it to reporters or other guest speakers. Speaker role detection is also related to speaker segmentation and clustering (also called speaker diarization), which was a benchmark test in the NIST Rich Transcription evalua-tions in the past few years (for example, NIST RT-04F http://www.nist.gov/speech/tests/rt/rt2004/fall/). Most of the speaker diarization systems only use acoustic infor-mation; however, in recent studies textual sources have also been utilized to help improve speaker clustering re-sults, such as (Canseco et al., 2005). The goal of speaker diarization is to identify speaker change and group the same speakers together. It is different from our task since we determine the role of a speaker rather than speaker identity. In this initial study, instead of using automatic speaker segmentation and clustering results, we use the manual speaker segments but without any speaker iden-tity information. 3.1 Hidden Markov Model (HMM) Figure 1: A graphical representation of the HMM ap-proach for speaker role labeling. This is a simple first order HMM.

The HMM has been widely used in many tagging prob-lems. Stolcke et al. (Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation. In speaker role detection, the ob-servation is composed of a much longer word sequence, i.e., the entire speech from one speaker. Figure 1 shows the graphical representation of the HMM for speaker role identification, in which the states are the speaker roles, and the observation associated with a state consists of the utterances from a speaker. The most likely role sequence  X  R is:  X  R = argmax where O is the observation sequence, in which O sponds to one speaker turn. If we assume what a speaker says is only dependent on his or her role, then:
From the labeled training set, we train a language model (LM), which provides the transition probabilities in the HMM, i.e., the P ( R ) term in Equation (1). The vo-cabulary in this role LM (or role grammar) consists of dif-ferent role tags. All the sentences belonging to the same role are put together to train a role specific word-based N-gram LM. During testing, to obtain the observation prob-abilities in the HMM, P ( O is used to calculate the perplexity of those sentences cor-responding to a test speaker turn.

The graph in Figure 1 is a first-order HMM, in which the role state is only dependent on the previous state. In order to capture longer dependency relationship, we used a 6-gram LM for the role LM. For each role spe-cific word-based LM, 4-gram is used with Kneser-Ney smoothing. There is a weighting factor when combin-ing the state transitions and the observation probabilities with the best weights tuned on the development set (6 for the transition probabilities in our experiments). In addi-tion, in stead of using Viterbi decoding, we used forward-backward decoding in order to find the most likely role tag for each segment. Finally we may use only a subset of the sentences in a speaker X  X  turn, which are possibly more discriminative to determine the speaker X  X  role. The LM training and testing and HMM decoding are imple-mented using the SRILM toolkit (Stolcke, 2002). 3.2 Maximum Entropy (Maxent) Classifier A Maxent model estimates the conditional probability: where Z g ( R i ,O ) are indicator functions weighted by  X  , and k is used to indicate different  X  X eatures X . The weights (  X  ) are obtained to maximize the conditional likelihood of the training data, or in other words, maximize the entropy while satisfying all the constraints. Gaussian smoothing (variance=1) is used to avoid overfitting. In our experi-ments we used an existing Maxent toolkit (available from http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit. html).

The following features are used in the Maxent model:  X  bigram and trigram of the words in the first and the  X  bigram and trigram of the words in the last sentence  X  bigram and trigram of the words in the first sentence Our hypothesis is that the first and the last sentence from a speaker X  X  turn are more indicative of the speaker X  X  role (e.g., self introduction and closing). Similarly the last sentence from the previous speaker segment and the first sentence of the following speaker turn also capture the speaker transition information. Even though sentences from the other speakers are included as features, the Max-ent model makes a decision for each test speaker turn in-dividually without considering the other segments. The impact of the contextual role tags will be evaluated in our experiments. 4.1 Experimental Setup We used the TDT4 Mandarin broadcast news data in this study. The data set consists of about 170 hours (336 shows) of news speech from different sources. In the original transcripts provided by LDC, stories are seg-mented; however, speaker information (segmentation or identity) is not provided. Using the reference transcripts and the audio files, we manually labeled the data with segmentation is generally very reliable; however, the role annotation is ambiguous in some cases. The interanno-tator agreement will be evaluated in our future work. In this initial study, we just treat the data as noisy data.
We preprocessed the transcriptions by removing some bad codes and also did text normalization. We used punc-tuation (period, question mark, and exclamation) avail-able from the transcriptions (though not very accurate) to generate sentences, and a left-to-right longest word match approach to segment sentences into words. These words/sentences are then used for feature extraction in the Maxent model, and LM training and perplexity cal-culation in the HMM as described in Section 3. Note that the word segmentation approach we used may not be the-state-of-art, which might have some effect on our experiments. 10-fold cross validation is used in our experiments. The entire data set is split into ten subsets. Each time one subset is used as the test set, another one is used as the development set, and the rest are used for training. The average number of segments (i.e., speaker turns) in the ten subsets is 1591, among which 50.8% are anchors. Parameters (e.g., weighting factor) are tuned based on the average performance over the ten development sets, and the same weights are applied to all the splits during test-ing. 4.2 Results
A HMM and Maxent : Table 1 shows the role iden-Table 1: Automatic role labeling results (%) using the HMM and Maxent classifiers.

B Contextual role information : In order to investi-Table 2: Impact of role sequence information on the HMM and Maxent classifiers. The combination results of the HMM and Maxent are also provided.

C System combination : For system combination, we In this paper we have reported an initial study of speaker role identification in Mandarin broadcast news speech us-ing the HMM and Maxent tagging approaches. We find that the conditional Maxent generally performs slightly better than the HMM, and that their combination out-performs each model alone. The HMM and the Max-ent model show differences in identifying different roles. The impact of contextual role information is also exam-ined for the two approaches, and a significant gain is ob-served when contextual information is modeled. We find that the beginning and the end sentences in a speaker X  X  turn are good cues for role identification. The overall classification performance in this study is similar to that reported in (Barzilay et al., 2000); however, the chance performance is quite different (35% in that study). It is not clear yet whether it is because of the difference across the two corpora or languages.

The Maxent model provides a convenient way to in-corporate various knowledge sources. We will investi-gate other features to improve the classification results, such as name information, acoustic or prosodic features, and speaker clustering results (considering that the same speaker typically has the same role tag). We plan to examine the effect of using speech recognition output, as well as automatic speaker segmentation and cluster-ing results. Analysis of difference news sources may also reveal some interesting findings. Since our working hypothesis is that speaker role information is important to find structure in broadcast news, we will investigate whether and how speaker role relates to downstream lan-guage processing applications, such as summarization or question answering.

