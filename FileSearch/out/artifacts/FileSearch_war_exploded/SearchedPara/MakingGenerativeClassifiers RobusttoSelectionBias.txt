 This pap er presen ts approac hes to semi-sup ervised learning when the lab eled training data and test data are di eren tly distributed. Speci cally , the samples selected for lab eling are a biased subset of some general distribution and the test set consists of samples dra wn from either that general dis-tribution or the distribution of the unlab eled samples. An example of the former app ears in loan application appro val, where samples with repa y/default lab els exist only for ap-pro ved applican ts and the goal is to mo del the repa y/default beha vior of all applican ts. An example of the latter app ears in spam ltering, in whic h the lab eled samples can be out-dated due to the cost of lab eling email by hand, but an unlab eled set of up-to-date emails exists and the goal is to build a lter to sort new incoming email.

Most approac hes to overcoming suc h bias in the litera-ture rely on the assumption that samples are selected for lab eling dep ending only on the features, not the lab els, a case in whic h pro vably correct metho ds exist. The missing lab els are said to be \missing at random" (MAR). In real applications, however, the selection bias can be more se-vere. When the MAR conditional indep endence assumption is not satis ed and missing lab els are said to be \missing not at random" (MNAR), and no learning metho d is pro vably alw ays correct.

We presen t a generativ e classi er, the shifted mixture mo del (SMM), with separate represen tations of the distri-butions of the lab eled samples and the unlab eled samples. The SMM mak es no conditional indep endence assumptions and can mo del distributions of semi-lab eled data sets with arbitrary bias in the lab eling. We presen t a learning metho d based on the exp ectation maximization (EM) algorithm that, while not alw ays able to overcome arbitrary lab eling bias, learns SMMs with higher test-set accuracy in real-w orld data sets (with MNAR bias) than existing learning metho ds that are pro ven to overcome MAR bias.
 G.3 [ Probabilit y and Statistics ]: statistical computing; H.2.8 [ Database Managemen t ]: Database Applications| data mining ; I.2.6 [ Arti cial Intelligence ]: Learning| parameter learning Algorithms, Economics, Theory semi-sup ervised learning, sample selection bias, reject infer-ence, generativ e classi ers
It is often the case that lab eled data available for training in a particular data mining task is not represen tativ e of the population in whic h the resulting mo del is to be used. Suc h a data set is said to su er from \sample selection bias," since data from the general population were lab eled in a biased way. In addition to the biased lab eled data set, it is often easy to obtain an unlab eled data set randomly sampled from the general population. We presen t metho ds for incorp orat-ing the unlab eled data into the learning pro cess to impro ve the accuracy of generativ e classi ers that ignore selection bias.

Situations necessitating learning in the face of sample se-lection bias can arise in man y con texts. For example, lend-ing institutions create mo dels of who is likely to repa y a loan from training sets consisting of people in their records who were given loans in the past; however, the institution only appro ved loan applications of those it judged likely to repa y a loan. Learning from only appro ved applican ts yields an incorrect mo del because the training set is a biased sample of the general population of applican ts, whic h is the popula-tion in whic h the mo del is to be used. The lending institu-tion also has the dataset consisting of all applican ts, rejected and appro ved, whic h is assumed to be a random sample of people who apply for loans. Algorithms that overcome the bias of the lab eled data set by incorp orating the rejected ap-plican ts in the learning pro cess are called \reject-inference" algorithms [4] [6] [7] [8].

Another common example of a learning problem that must deal with sample selection bias occurs in medical and epi-demiological domains [1] [24] [18] [17]. We would like to dev elop a mathematical mo del for a particular treatmen t (or exp osure{as in epidemiology) that predicts the exten t to whic h it will a ect a particular patien t, measured per-haps by the exp ected lifetime increase, or a probabilit y of surviv al. To create suc h a mo del, we would use a database describing man y patien ts and their resp onses to the treat-men t. However, unless the study is a carefully con trolled randomized study , any suc h database will be biased by sam-ple selection, since it only con tains patien ts whom doctors recommended for the treatmen t; certainly whether or not a doctor recommends someone for a particular treatmen t is re-lated to how much that treatmen t is exp ected to bene t the patien t (Epidemiological studies are also sub ject to related but di eren t bias, suc h as the di eren t income levels of ex-posed and unexp osed people, whic h is correlated with access to high-qualit y care [24].) Learning a resp onse mo del for the general population should tak e into accoun t di erences be-tween the distribution of patien ts exp osed to treatmen t and the general distribution of all poten tial patien ts.
This bias in the lab eling can also arise when the act of lab eling data is exp ensiv e, and therefore datasets are infre-quen tly updated. For example, to build a spam-detecting classi er, a human must hand-lab el eac h email. The dis-tribution of emails may change over time and the spam l-ter will lose optimalit y, as it is optimal for old data. This is the \concept-drift" scenario, and the decrease in perfor-mance due to the out-dated training set can be mitigated by utilizing unlab eled up-to-date data, whic h will be dra wn from the population in whic h the spam lter is to be used (since they are up-to-date), and easy to obtain (since they are unlab eled). Unlik e the previous two examples in whic h the goal was to build a mo del of the general population (i.e. the lab eled and unlab eled samples), the goal in overcoming concept-drift is to build an up-to-date mo del that best clas-si es new samples, not samples from the general population, whic h consists of old and new samples.

The next section describ es di eren t types of sample selec-tion bias, whic h are distinguished by their conditional inde-pendence assumptions, and intuitiv e reasons for why these particular types of bias migh t be presen t in di eren t real world situations. Section 3 clari es the distinction between mo deling the general population and mo deling the unlab eled population, and pro vides metho ds for overcoming MAR bias in both scenarios. Section 4 presen ts a novel metho d for that can overcome bias that does not assume missing lab els are MAR. Section 5 discusses prior work related to the metho ds presen ted here. Sections 7 and 8 demonstrate that our meth-ods sho w impro vemen t over existing metho ds on real-w orld data sets.
Learning problems that require overcoming sample selec-tion bias are usually presen ted in terms of a semi-lab eled data set in whic h the lab eled samples are not a random sub-set. The bias presen t in a semi-lab eled data set is character-ized by the conditional indep endence relationships between the features, lab els, and presence/absence of a lab el. We use Bayesian net works as an intuitiv e tool to organize the dif-feren t conditional indep endence relationships [16] [14]. Fol-lowing are the de nition of eac h variable in our net works, with an example in paren theses of what the variable would represen t in the example of loan applications [21].
In [21], an additional variable h represen ted hidden fea-tures not available in either the lab eled training data or the unlab eled training data. h is omitted here because these net works only include variables and interactions that are di-rectly mo deled in our metho ds.
In the case that samples are selected from the general population for lab eling at random, no sample selection bias is presen t, and the lab els of the unlab eled data are said to be \missing completely at random" (MCAR) [13]. This situation is represen ted by the follo wing Bayesian net work.
Note that this is the traditional \semi-sup ervised learn-ing" scenario, where the goal is to use the unlab eled data to reduce the variance in the estimated mo del, as opp osed to overcoming di erences in the distributions.
In this type of bias, the selection variable s is in uenced only by the observ able features. Therefore, the lab eled data may constitute a biased sample of the general population, but the mec hanism for lab eling samples is learnable. In the literature, this type of bias case is called \missing at random" (MAR) [10] [13].

When lab els are MAR, whether a sample is lab eled, s , is indep enden t from the actual lab el y , given the features x . This is represen ted by the most general Bayesian net work:
Since we allo w only the observ able features to in uence the selection. Thus the class and selection are conditionally indep enden t: whic h implies the constrain ts
In this graph, observing x d-separates y and s , so the conditional indep endence relationships are preserv ed. Se-lection may dep end on x , but given x , y adds no additional information about selection, or, equiv alen tly, given x , kno w-ing s gives no information about the outcome (for example, whether or not someone is a bad borro wer).

This could arise in practice if samples are selected for la-beling with a formal selection mo del (suc h as a logistic re-gression), as a lending institution migh t use to appro ve loan applications. Alternativ ely, this bias is presen t when no ac-tual mo del has been used to select samples, but the concept discriminating between lab eled and unlab eled data is still learnable.
This type of bias in the lab eling is completely general: there are no assumptions about conditional indep endence relationships between the features, lab els, and lab eling. In this case, the missing lab els are said to be \missing not at random" (MNAR) [13].

There are two distinct goals to learning an unbiased mo del given a semi-lab eled data set with biased lab eling. The rst, general population mo deling , is represen ted in the loan application example in whic h the goal is to mo del the gen-eral population of accepted and rejected applican ts given a data set con taining only accepted applican ts, i.e. to learn a classi er of the form p ( y j x ), given only lab eled examples ( x; y; s = 1) and unlab eled examples ( x; s = 0).
The other, unlab eled population mo deling , app ears in the concept-drift scenario, for example, in spam ltering. We are given lab eled examples of out-dated email, and un-lab eled examples of up-to-date email. Unlik e general popu-lation mo deling, we assume the object is to learn a classi er that accurately mo dels incoming email, whic h is dra wn from the same distribution as the up-to-date, unlab eled samples. The goal is to learn an up-to-date mo del, p ( y j x; s = 0) given only lab eled examples ( x; y; s = 1) and unlab eled examples ( x; s = 0).
In both mo deling tasks, general population mo deling and unlab eled population mo deling, the lab eled samples can be weigh ted to appro ximate a sample dra wn from the appro-priate distribution, assuming MAR bias.
The follo wing lemma sho ws how the join t distribution of the lab eled samples is related to the general population:
Lemma 1. Under MAR bias in the labeling, if all probabilities are non-zer o.

Proof. Use the assumed conditional indep endence rela-tionship of MAR bias, then apply Bayes' Rule: kno wn as the \inverse probabilit y of treatmen t weigh t" (IPTW), where \treatmen t" means the sample is selected for lab eling [24] [17]. This lemma is explored in the con text of data mining in [26].
The follo wing lemma sho ws how the join t distribution of the lab eled samples is related to the distribution of unlab eled samples:
Lemma 2. Under MAR bias in the labeling, p ( x; y j s = 0) = p ( s = 1) 1 p ( s = 1) if all probabilities are non-zer o.

Proof. Apply Bayes' rule, use the assumed conditional indep endence relationship, then substitute equation 1. p ( x; y j s = 0) = p ( s = 0 j x; y )
This is related to imp ortance sampling [11], whic h calcu-lates exp ectations over a distribution di eren t from the dis-tribution of the actual samples using imp ortance weigh ts. If Figure 1: General population mo deling with Lemma 1. a) A syn thetic lab eled data set is generated with 5000 points and a curv ed decision boundary (for clarit y, only some points are sho wn). A logistic regression classi er is learned viewing the hidden lab els to demonstrate the theoretical best perfor-mance (solid line, repro duced in plots b) and c) as a dashed line for comparison). b) Some lab els are hidden (plotted as blac k dots), on the left with high probabilit y and on the righ t with low probabilit y. The classi er estimated from only the lab eled points (solid line) is not an accurate boundary in the gen-eral population. c) Lemma 1 pro vides weigh ts for the logistic regression loss function and estimates a boundary nearly iden tical to the one estimated from all points (viewing hidden lab els), therefore mini-mizing loss in the general population. samples X are distributed according to p ( x ), the exp ecta-tion of a function f ( x ) over a di eren t distribution p given by the weigh ted sum of f ( x ) over the samples: where the densit y ratio is the imp ortance weigh t. Under MAR sample selection bias, the densit y ratio p ( x j s =0) would be used for imp ortance sampling reduces to the ex-ample weigh t given by Lemma 2. This lemma is deriv ed di-rectly from the imp ortance weigh t in [2]. Similarly , Lemma 1 is deriv ed from imp ortance sampling weigh ts in [3].
Selecting samples based only on the features preserv es the decision boundary because y ? s j x . This would seem to imply that discriminativ e classi ers are robust to MAR bias since they mo del decision boundaries; however, an analysis of the loss function of discriminativ e classi ers sho ws that the lemmas are useful for impro ving exp ected performance in the general and unlab eled populations. This is useful under a common data mining assumption, that the the mo del is missp eci ed, i.e. the decision boundary is not kno wn to conform to the particular parametric mo del in use.
The goal of training a discriminativ e classi er f ( x ), mo d-eling p ( y = 1 j x ) is to minimize the exp ectation of the loss function L ( f ( x ) ; y ) over the distribution of some test set: where the exp ectation is over the join t distribution of x and y , in this case, the general population, and the sum is over the sampled data, indexed by i . Note that when y is binary , this form ulation of loss encompasses maxim um-lik eliho od, L ( f ( x ) ; y ) = y log f ( x ) + (1 y ) log(1 f ( x )), and minim um squared error, L ( f ( x ) ; y ) = ( f ( x ) y ) 2 .
Since lab els are missing for some samples, this loss cannot be directly estimated; however, Lemma 1 sho ws that it is equal to the loss over the weigh ted distribution of lab eled samples: where E x;y j s =1 indicates the join t distribution of x and y among the lab eled samples. Similarly , Lemma 2 pro vides weigh ts to estimate the loss over the distribution of unla-beled samples. The exp ected loss over the target distribu-tion can be minimized by minimizing the weigh ted loss over the distribution of lab eled samples. [25] explores the issues of maxim um likeliho od estimation under bias and the use of a weigh ted loss function for overcoming it.

This weigh ted loss metho d rst learns a mo del of the lab el-ing mec hanism p ( s j x ) from the lab eled and unlab eled sam-ples. The mo del is then calibrated 1 and used with the ap-propriate lemma to create example weigh ts for eac h lab eled sample. An unbiased discriminativ e classi er is then learned by minimizing the loss of these weigh ted (lab eled) samples, ignoring the unlab eled samples.
 Figure 1 demonstrate how a semi-lab eled data set with MAR bias in the lab eling can result in a discriminativ e clas-si er that is not optimal for the target distributions (in the gure, the general population is used as an example), and how the lemmas can pro vide weigh ts for the loss function to overcome the bias.

Interactions between mo del missp eci cation and MAR bias are explored in [20]. Speci cally it is sho wn that when the mo del is correctly speci ed, maxim um-lik eliho od estimation
Here, a mo del is calibrated if the estimate ^ p = p ( s = 1 j x ) is equal to the prop ortion of samples whose value of s is 1, among those samples for whic h the mo del output is (near) ^ p . [27] is asymptotically unbiased. When the mo del is missp eci ed, the asymptotically unbiased maxim um-lik eliho od estimate is sho wn to maximize the log-lik eliho od equation weigh ted with the imp ortance sampling weigh t (densit y ratios), whic h reduce to lemmas 1 and 2.
Generativ e classi ers mo del the join t densit y p ( x; y ), fac-tored into a class prior probabilit y p ( y ) and a class-conditional densit y mo del p ( x j y ). Unlik e discriminativ e classi ers, gen-erativ e classi ers are not inheren tly robust to MAR bias in the lab eling. This is because the densit y of features within eac h class among the lab eled samples, p ( x j y; s = 1), is not guaran teed to be the same in the general population, p ( x j y ). The speci c densit y mo del used in our exp erimen ts are Gaussian mixture mo dels (GMMs) and are explained in Section 6; however, our metho ds are general and can be used with whic hev er densit y mo del is appropriate for the learning task at hand.

Both lemmas have straigh t forw ard applications to over-coming MAR bias with generativ e classi ers:
The class priors and class conditional densities are com-bined to form a classi er using Bayes' Rule:
This elegan t metho d is pro vably correct when the missing lab els are MAR. Practically , though, it is only useful to the exten t that the classi er used to create the weigh ts can pro-vide accurate probabilit y estimates. Similarly , if one of the probabilities in the denominator of equations 1 or 2 is zero, the weigh t for that sample is in nite. This can be avoided in practice with a maxim um sample weigh t.
The lack of guaran tees regarding the relationship between the lab eled and unlab eled samples under MNAR bias con-icts with the intuition that the underlying concepts should be similar. For example, the map from words in emails to the spam/ham lab el is exp ected to drift over time, but not completely change. The approac h presen ted here is moti-vated by this intuition.

We can use Lemma 2 to create a generativ e classi er for the unlab eled data, p ( x j y; s = 0) and p ( y j s = 0), that is robust to MAR bias; however, this classi er will have the same decision boundary as one for the lab eled data and for the general population, since p ( y j x; s = 0) = p ( y j x; s = 1) = p ( y j x ) under the conditions of MAR bias. The shifted mix-ture mo del (SMM) uses Lemma 2 to estimate the generativ e classi er p ( x j y; s = 0) and p ( y j s = 0), then allo ws the pa-rameters of this mo del to shift to increase the likeliho od of the unlab eled data.

The complete SMM consists of two generativ e classi ers: a mo del of the lab eled data, p ( x j y; s = 1) and p ( y j s = 1), whic h is estimated directly from the lab eled samples and a mo del of the unlab eled data, p ( x j y; s = 0) and p ( y j s = 0), whic h is initialized with Lemma 2 and shifted with the EM algorithm. In addition, the probabilit y of selection p ( s ) can be estimated directly from the lab eled and unlab eled data.
The log-lik eliho od of mo del parameters given the unla-beled data ( x i ; s i = 0), for i = 1 ::N is Since no lab el is given for these samples, their probabilit y is the sum over the possible lab els y . The logarithm of the sum mak es direct maximization dicult, so the likeliho od it is iterativ ely increased with the EM algorithm. [9] sho wed that this is bounded from below by the exp ectation of the so-called \complete data" log-lik eliho od: where z iy is the probabilit y that unlab eled sample i was generated by class y . Re-estimating parameters to increase this lower bound also increases the log-lik eliho od. Eac h iter-ation j of the EM algorithm uses the curren t estimate of the parameters j to estimate Z iy , and then uses that matrix to update the parameters.

If run to con vergence, the EM algorithm nds a good clus-tering of the data; however, in real-w orld data mining tasks, it is not necessarily the case that the natural clusters in the data corresp ond well to the class lab els. For this reason, it is imp ortan t not to allo w the parameters to shift too far from their initialization, or the mixture comp onen ts will represen t the natural clustering of the data as opp osed to the densit y of the classes of unlab eled data. This is accomplished by only running the EM algorithm for a few iterations (we use 5), as well as updating the parameters using an \inertia" factor to slow the parameter evolution. In iteration j of the EM algorithm, when the M-step estimates new param-eters 0 , the actual parameters used in the next iteration are a com bination of the curren t parameters and the new parameters: After impro ving the likeliho od of the mo del p ( x j y; s = 0) and p ( y j s = 0) given the unlab eled data, the mo del better represen ts the classes within the population of unlab eled samples. Our exp erimen ts used = 0 : 99.

For the unlab eled population mo deling task, the genera-tive mo del of the unlab eled data can be used directly . For the general population mo deling task, the two generativ e classi ers are com bined to mo del the join t and test samples can be classi ed using the de nition of conditional probabilit y
Since shifting the parameters poten tially changes the de-cision boundary , the resulting generativ e classi er can gen-erate samples with MNAR bias. In Sections 7 and 8, the SMM is demonstrated to partially overcome MNAR bias in practice.
Sample selection bias is also kno wn as \co variate shift," esp ecially in regression literature [22] [23]. An approac h to overcoming covariate shift in regression tasks is given in [22]. Their mo del assumes ( x; y ) pairs in the training set are gen-erated by two indep enden t pro cesses, P 1 ( x; y ) and P but that the test data is only generated by the rst pro cess, P ( x; y ). The goal is to estimate mixture mo del parameters that separate e ects that are exp ected to be characteris-tic only of the test distribution from those characteristic of both test and training data. As with the SMM, the pa-rameters are estimated by maximizing the likeliho od of a mixture mo del with the EM algorithm; however, this frame-work assumes training instances are dra wn from the general population, and test data are selected not at random from that population.

A di eren t approac h to unlab eled population mo deling, in this case under MAR bias, is given in [2]. Here, Lemma 2 is deriv ed directly from the imp ortance weigh ts as op-posed to our deriv ation from the conditional indep endence assumptions of MAR bias. A discriminativ e classi er for the unlab eled population and selection probabilities are learned with a single con vex optimization. The con vexit y require-men t, however, precludes the use of more exible semi-and nonparametric mo dels.

The idea of learning mo del parameters from a training set and then adjusting them to better accoun t for the samples from a di eren t target distribution is used in [5]. A small set dra wn from the target distribution is used to adjust the parameters learned from a large training set with a di er-ent distribution; however, in their approac h both sets are la-beled. In a maxim um entrop y framew ork, they use the large training set to learn a prior distribution over mo del param-eters, whic h are updated using data dra wn from the target distribution; this is analogous to our initialization of the shifted mixture comp onen ts based on the lab eled data. To limit parameter evolution, eac h mo del parameter is alowed to be shifted away from its prior up to a maxim um distance that is based on that parameter's variance estimated from the large training set. This is used in natural language pro-cessing for so-called \domain adaptation," learning a mo del when the target and training domains are di eren t.
A general approac h to overcoming MNAR bias is given in [19]. Similar to the metho d suggested by Lemma 1, their metho d creates examples weigh ts based on a selection mo del learned from the data; however, their selection mo del p ( s = 1 j x; y ) is dep enden t on the class lab el. If these selection probabilities were kno wn exactly , the weigh ts would correct for MNAR bias; however, it is not pro ven that this metho d can alw ays estimate reliable selection mo dels. Despite this lack of a guaran tee, the metho d is demonstrated to be useful for overcoming MNAR bias in a real-w orld data mining task.
To learn a selection mo del p ( s = 1 j x; y; ), where is the set of mo del parameters, they observ e that for any function g ( x ) of the features, the exp ectation of g ( x ) in the general population matc hes the weigh ted exp ectation over the dis-tribution of lab eled samples: With resp ect to a data set in whic h samples 1 through M are lab eled and M + 1 through M + N are unlab eled, this is expressed empirically as
The metho d estimates 2 &lt; k by selecting k functions, g ( x ) ; :::; g k ( x ), and demanding equation 4 be satis ed for all g i ( x ). This metho d has the poten tial to overcome MNAR bias, though it requires solving nonlinear sim ultaneous equa-tions, requires a parametric selection mo del, and requires careful selection of the functions g i ( x ) to insure the unique-ness, stabilit y, and robustness of the estimate.

A similar metho d based on kernel means matc hing is given in [12], however it requires missing lab els to be MAR. This metho d does not require any mo del of the selection mec h-anism, but estimates optimal example weigh ts i for eac h lab eled sample directly .

Let : X ! F be a map from the feature space X to the kernel feature space F , and let : } ! F be the operator whic h maps probabilit y distribution P 2 } to its exp ec-tation under transformation , called the kernel mean. It is sho wn in [12] that weigh ts ( x ) for eac h example x are opti-mal when the kernel mean of the distribution of the general population P r 0 is equal to the weigh ted mean of the distri-bution P r of the lab eled samples: Con vergence results and pro of of con vexit y are also given.
Both metho ds correct di erences between the distribu-tion of the lab eled data and the general distribution using example weigh ts. In [19] these weigh ts are determined by an explicit selection mo del, and in [12] are estimated directly , but both are found by requiring that some exp ectation in the general population be equal to the weigh ted exp ectation over the lab eled data.
The metho ds for overcoming selection bias presen ted here are built on generativ e classi ers, whic h require accurate multiv ariate densit y estimation. Gaussian mixture mo dels (GMMs) comprise a class of semiparametric densit y esti-mators, and are exible enough to mo del man y real-w orld distributions. GMMs estimate the densit y at x , p ( x ) as a weigh ted sum over the j C j comp onen t densities, Eac h comp onen t in C has a multiv ariate normal distribu-tion: where d is the dimensionalit y, and the parameters of com-ponen t c are the mean c and the covariance matrix c .
The probabilistic form ulation of this densit y mo del allo ws the application of maxim um likeliho od (ML) metho ds, suc h as the exp ectation maximization (EM) algorithm [9]. Sim-ilar to the application of EM to the shifted mixture mo del, estimating a GMM with EM iterativ ely impro ves the log-likeliho od of the N data points, by bounding it from below and impro ving the bound, whic h is the exp ectation of the so-called \complete data" log-lik eliho od : where z ic is the probabilit y that point x i is generated by comp onen t c , whic h is analogous to the class variable y used in estimating the SMM.

To t a GMM, the parameters for eac h comp onen t, p ( c ), , and c are initialized and the follo wing two steps are it-erated: E-step: Use the curren t parameters to estimate the matrix Z = f z ic g = p ( c j x i ). M-step: Use Z to maximize equation 5 over the parameters.

Typically , this is iterated for some maxim um num ber of steps or until a con vergence criterion is satis ed.
There are sev eral kno wn diculties applying EM to Gaus-sian mixture mo dels.

Initialization. We start with a (uniform) random distri-bution over comp onen t resp onsibilities. This is equiv alen t to starting at the M-step with a random Z matrix (with rows that sum to 1). We found that in general, classi ers could be estimated to have reliably high test-set accuracy by initializing them with a general mixture mo del of the fea-tures. For example, in a binary classi cation task mapping features x to lab el y 2f 1 ; 0 g , a GMM mo deling p ( x ) would be estimated using EM and its parameters would be used to initialize the class-conditional densities p ( x j y = 0) and p ( x j y = 1), whic h would then both be impro ved with EM.
Local minima. Our solution to avoid local minima in the solution is to run the EM algorithm sev eral times under di eren t random initializations, and pick the mo del with the highest likeliho od. Eac h mo del was estimated by running EM until the relativ e change in the log-lik eliho od of the data was less than 10 7 . We found picking the best of 25 mo dels to pro vide sucien tly stable results.

Non-in vertible covariance matrix. There are two common ways for EM to pro duce a singular . Data sets in whic h some samples have the exactly the same value for a particular feature are particularly prone to this problem be-cause it is possible for one comp onen t to tak e resp onsibilit y only for a subset of these samples and con tain no variance for that feature. To overcome this issue, uniform random noise in [ 0 : 25 ; 0 : 25] is added to the binary and integer-v alued features. Initial tests with logistic regression sho wed this to have minimal impact on class-separabilit y.

Another reason a covariance matrix migh t be singular is if that comp onen t claims resp onsibilit y for too few points; a d dimensional covariance matrix estimated from few er than d samples is singular. Our solution was to stop updating comp onen ts in the M-step whic h were found to have resp on-sibilit y for few er than 2 d points. The probabilistic resp on-sibilit y of comp onen t j for sample i is given by Z ij , and for the purp ose of avoiding singular covariance matrices, eac h point is assigned to the comp onen t with highest partial re-sponsibilit y. Our exp erimen ts used only 6 comp onen ts and this rarely occurred. When it does, the resulting mixture mo del usually has lower likeliho od than the other 25 and is rejected.
The ADUL T data set con tains demographic information collected for a census. The target value is predicting whether income exceeds $50,000 ann ually , using features measuring education, household statistics, rep orted capital gains and losses, emplo ymen t and marital status. 1
This data set con tains features similar to what migh t be in a loan application appro val system. The target variable is analogous to a repa y/default lab el. Marital status is used to select whic h samples are lab eled (married) and whic h have hidden lab els (unmarried), analogous to whic h loan appli-cations were appro ved (selected for lab eling). In this sense, marital status is interpreted as a unquan ti able measure of resp onsibilit y whic h would not be ocially recorded in a real loan application (and, in fact, may not be legal), but whic h nev ertheless migh t in uence the ultimate decision to accept or reject the application. It is not immediately clear how the distribution of income of married people is related to the general distribution of income, even given the other demographic information.

Unlik e data sets collected from actual loan appro val records, this data set con tains lab els for every sample, so test set ac-curacy can be directly measured.

Testing the type of bias. Whic h type of bias is induced by this metho d of sample selection? It is possible that mar-ital status is not related at all to income and therefore that the lab els are MCAR; however, this is easily sho wn to be
The categories in eac h categorical feature were com bined to form a binary feature. For example, the categorical feature \coun try of origin" is transformed into the binary feature \is nativ e-b orn US citizen." \Capital gains" and \capital losses" were transformed by log(1 + x ). After all transformations, noise is added as describ ed in the previous section, to pre-vent Gaussian comp onen ts in the mixture mo dels from los-ing all variance in a particular feature. The lab els of the unmarried samples are hidden, the \marital status" feature is remo ved, and 40% of the samples are held out as a test set for validation. This yields 12798 lab eled samples, 14342 unlab eled samples, and 18092 test samples (married and un-married).
 Table 1: ADUL T data set: General population mo d-eling Table 2: ADUL T dataset: unlab eled population mo deling false by comparing the two probabilities:
Another possibilit y is that marital status is random given the observ able features, in whic h case the selection for la-beling is still only dep enden t on observ able features (MAR). This is testable using a discriminativ e classi er. The accu-racy of a logistic regression trained on the biased (married) data but tested on unbiased data sho ws the MAR scenario to be not plausible, even when using Lemma 1 to weigh t the likeliho od function: the accuracy of a mo del learned from the biased training set is 74.2%, but the accuracy a similar classi er learned from an unbiased training set (viewing the \hidden" lab els) whic h is 80.7% on the same test set. The increase in the training set size is not to be cause of the increase in performance, rather, it is because the observing the hidden lab els remo ves all bias.

Since the decision boundaries are di eren t, p ( y j x; s = 1) 6 = p ( y j x ), the missing lab els are MNAR.
 Main results. Table 1 sho ws the test set accuracy of the SMM is signi can tly better than the accuracy of a logistic regression that ignores the bias in both general population mo deling and unlab eled population mo deling. Generativ e classi ers estimated using the rew eigh ting lemmas also sho w impro ved performance over logistic regression; however the SMM is better able to mo del the unlab eled data, impro ving classi cation accuracy of the test sets. For these tests mix-ture mo dels with 6 comp onen ts are used as densit y estima-tors. The test-set accuracies are averages over 10 random test/train splits (using 60% for training, 40% for testing), with the standard deviation in paren theses. The California Housing (CA-HOUSING) is based on US Census data and con tains 20640 records about house values in California [15]. Eac h record describ es a localized area in California and con tains the follo wing features: median INCOME, median house AGE, total ROOMS, total BED-ROOMS, POPULA TION, HOUSEHOLDS, LONGITUDE, and LATITUDE, in addition to the median VALUE of houses in eac h area [19]. The target is to predict whether the house value is above the median house value in all of California or the rest of California to test general population mo deling or unlab eled population mo deling, resp ectiv ely.

With the CA-HOUSING data set, lab els in the training set are hidden dep ending on the geographic location where that sample was observ ed. Samples are lab eled only if they are appro ximately within 0 : 40 degrees of longitude (appro x-imately 22.4 miles) of the ocean, and above the 36th parallel (whic h is below the San Francisco Bay area). The ma jor-ity of these houses are concen trated in the San Francisco peninsula and in Marin Coun ty. The longitude and latitude features are then remo ved from the data set. 2
This represen t a scenario in whic h a mo del of house prices is being built for California and census data are available for the entire state, however only house prices in the coastal northern California are available.

Testing the type of bias. Houses on the San Fran-cisco peninsula and in Marin Coun ty are on average more exp ensiv e than in the rest of the state, indicating bias in the lab eling is presen t:
With the biased training set, the test accuracy of a logistic regression classi er weigh ted with Lemma 1 is 74.8%. A similar classi er learned from lab eled and unlab eled samples (viewing the \hidden" lab els) yields 80.5% on the same test set. Since the decision boundaries are di eren t, p ( y j x; s = 1) 6 = p ( y j x ), and therefore the missing lab els are MNAR.
Main result. Tables 3 and 4 sho ws that the SMM has sig-ni can tly higher test-set classi cation accuracy in both the general population mo deling task and the unlab eled popula-tion mo deling task. As with the ADUL T data set, generativ e classi ers using GMMs and weigh ted data sho ws higher test set accuracy than logistic regression. The SMM is a better mo del of the unlab eled population than re-w eigh ted GMMs, resulting in highest test set accuracy classifying houses not in coastal northern California (unlab eled population), and an increase of classifying houses throughout the state (general population). For these tests mixture mo dels with 6 comp o-nen ts are used as densit y estimators. The test-set accuracies are averages over 10 random test/train splits (using 60% for training, 40% for testing), with the standard deviation in paren theses
Both exp erimen ts sho w that the shifted mixture mo del im-pro ves the generativ e classi er estimated using the weigh ts
Coastal homes are found by creating 50 strata in the lat-itude with equal num bers of samples. Within eac h stra-tum the coast is de ned as the sample with lowest longitude (most western), and the samples within 0.4 degrees of lon-gitude of the western most house in eac h stratum were de-ned as \coastal" . After the samples not from coastal north-ern California were hidden, LONGITUDE and LATITUDE were remo ved from the data set. ROOMS, BEDR OOMS, POPULA TION, and HOUSEHOLDS were log-transformed.
 Noise is added to AGE as with the ADUL T data set, to avoid singular covariance matrices in the Gaussian comp onen ts of the mixture mo del. Again, 40% of the data are held out in a test set, yielding 2159 lab eled samples, 10228 unlab eled samples, and a test set size of 8253 samples.
 Table 3: CA-HOUSING data set: general popula-tion mo deling Table 4: CA-HOUSING data set: unlab eled popu-lation mo deling from the lemmas. This more accurate mo del of unlab eled data results in better test-set classi cation accuracies in both the general population mo deling task, and the unla-beled population mo deling task. Lemmas 1 and 2 allo wed estimation of generativ e classi ers that had better test-set accuracies than logistic regressions trained on the same bi-ased data, but to a lesser exten t than the SMM.

These exp erimen ts are both plausible real-w orld mo del-ing scenarios. The exp erimen t with the ADUL T data set is analogous to the loan application appro val problem, where income level represen ts repa y/default beha vior. Marital sta-tus is analogous to the decision of a lending institution to appro ve a loan. It was sho wn to be partially predictable from the other features in the data set, but also to have additional in uence on the target variable, given those features. The SMM can partially capture this additional in uence and im-pro ve test-set classi cation accuracy .

The CA-HOUSING exp erimen ts sho wed that the SMM can learn a mo del of housing prices throughout California based on a biased lab eled subset that is more accurate than a logistic regression that ignores the bias in the lab eling.
Most approac hes to overcoming selection bias in the liter-ature rely on the MAR assumption, whic h implies the deci-sion boundary is the same for the training and test sets. In real world applications, however, the conditional indep en-dence requiremen ts that de ne MAR are rarely perfectly satis ed. Despite this shortcoming, we demonstrated that the re-w eigh ting lemmas can be used to estimate generativ e classi ers that are more robust to selection bias than simple decision boundaries, even when the selection is demonstra-bly MNAR.

By shifting the parameters of a generativ e classi er mo d-eling the unlab eled data to increase the likeliho od, the SMM can achiev e accuracy higher than what is guaran teed by the lemmas, in both the general population mo deling task and the unlab eled population mo deling task.

E ectiv e use of the SMM requires con trolling the evolu-tion of parameters as the likeliho od given the unlab eled data is impro ved. This was acomplished by limiting EM to 5 it-erations and using the inertia parameter, = : 99. Future work will include more sophisticated metho ds for con trolling parameter shifts, as well as investigating the e ects of only allo wing some parameters to change (e.g. mixture comp o-nen t means but not covariance matrices).

Future work will investigate the e ects of mo dulating the exibilit y of the learning algorithm, as manifested in the num ber of comp onen ts, both on creating classi ers for p ( s = 1 j x ) to learn the weigh ts, and for the nal classi cation task, learning p ( y j x ) or p ( y j x; s = 0). In particular, the e ect of the qualit y of the weigh ts on the resulting classi er will be investigated.
This work was supp orted by Fair Isaac. [1] K. Benson and A. J. Hartz. A comparison of [2] S. Bic kel, M. Br  X  uckner, and T. Sche er.
 [3] S. Bic kel and T. Sche er. Diric hlet-enhanced spam [4] W. J. Boyes, D. J. Ho man, and S. A. Low. An [5] C. Chelba and A. Acero. Adaptation of maxim um [6] G. Chen and T. Astebro. The economic value of reject [7] D. A. Cobb-Clark and T. Crossley . Econometrics for [8] J. Cro ok and J. Banasik. Does reject inference really [9] A. Dempster, N. Laird, and D. B. Rubin. Maxim um [10] A. J. Feelders. An overview of mo del based reject [11] Glynn, Peter W. and Iglehart, Donald L. Imp ortance [12] J. Huang, A. Smola, A. Gretton, K. M. Borgw ardt, [13] R. J. A. Little and D. B. Rubin. Statistic al analysis [14] K. Murph y. Dynamic Bayesian Networks: [15] R. Pace and R. Barry . Sparse spatial autoregressions. [16] J. Pearl. Graphical mo dels for probabilistic and causal [17] J. M. Robins, M. A. Hernan, and B. Brum bac k. [18] P. R. Rosen baum and D. B. Rubin. The cen tral role of [19] S. Rosset, J. Zhu, H. Zou, and T. Hastie. A metho d [20] H. Shimo daira. Impro ving predictiv e inference under [21] A. Smith and C. Elk an. A bayesian net work framew ork [22] A. Stork ey and M. Sugiy ama. Mixture regression for [23] M. Sugiy ama and K.-R. M  X  uller. Mo del selection under [24] A. J. Treno, P. J. Gruenew ald, and F. W. Johnson. [25] K. Yamazaki, M. Kawanab e, S. Watanab e, [26] B. Zadrozn y. Learning and evaluating classi ers under [27] B. Zadrozn y and C. Elk an. Transforming classi er
