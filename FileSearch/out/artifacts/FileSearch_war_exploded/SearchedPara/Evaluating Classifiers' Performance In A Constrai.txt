 ABSTRACT 
In this paper, we focus on methodology of finding a classifier with a minimal cost in presence of additional performance constraints. ROCCH analysis, where accuracy and cost are intertwined in the solution space, was a revolutionary tool for two-class problems. We propose an alternative formulation, as an optimization problem, commonly used in Operations Research. 
This approach extends the ROCCH analysis to allow for locating optimal solutions while outside constraints are present. Similarly to the ROCCH analysis, we combine cost and class distribution while defining the objective function. Rather than focusing on slopes of the edges in the convex hull of the solution space, however, we treat cost as an objective function to be minimized over the solution space, by selecting the best performing 
Programming framework provides a theoretical and computational methodology for finding the vertex (classifier) which minimizes the objective function. 
Consider a problem, where classifiers performance has to be evaluated taking into account additional constraints related to error rates. Such constraints often arise from implementation. 
They could, for example, involve a limited workforce to resolve or restrictions in cost of incentives for responders. An application example used throughout this paper involves an attrition model among its customers. 
Naturally, one of the implementation concerns is limited availability of resources, such as phone representatives. Traditionally, a model is selected first and summarized by a not made or distributed for profit or commercial advantage and that requires prior specific permission and/or a fee. SIGKDD 02, July 23-26, 2002, Edmonton, Alberta, Canada. 
Copyright 2002 ACM 1-58113-567-X/02/0007 $5.00. modeler into several performance buckets. Then, the implementation team will eyeball the thresholds of the performance buckets and pick the threshold that matches constrained resources the closest. Such business practice often results in a sub-optimal solution being selected. If the constraints are known a-priori, they could be built into the system evaluating classifiers performance. If they are not known till the implementation time, a system analogous to the ROCCH could be accomplished in the following steps. First we note that a convex combination of two classifiers is also a viable classifier. Then, we remove dominated classifiers. Finally, we impose additional performance constraints if any. Those additional constraints also form a convex hull. Intersection of the two regions, is a new convex hull, the potential solution space. Finally, similarly to the 
ROCCH, we treat a combination of costs and class probabilities as an objective function to be minimized over the solution space. 
Due to existence of additional constraints, however, iterating slopes of the edges of the convex hull is no longer computationally efficient because not all vertices of the convex hull are known. The theory of linear programming provides computational tools for finding the optimal solution(s) without explicit knowledge of all vertices. 2. ROC CONVEX HULL H BRID CLASSIFIERS 
Suppose we construct a series of k-1 classifiers by varying a positive decision threshold from never , and gradually increasing probability of decision Yes . By allowing more instances to be classified as positive, each new threshold will increase the number of correctly classified positive instances, but it may also increase rate), each new classifier will be positioned up and to the right from the previous one. Connecting each pair of points with a line segment generates a convex set, where each vertex represents a classifier (Figure 1). 
Table 1 represents a set of classifiers obtained from a neural networks model for a bank attrition problem. Figure 1 shows the resulting convex set 
Figure 2 shows a set of classifiers obtained by varying thresholds for two logistic regression models for the attrition data. In this representation, we can visually recognize dominated classifiers. 
They are positioned inside the convex region, while potential candidates for optimal solution are on the boundary. It is easy to boundary, which will outperform a dominated classifier. Figure 3 shows a hybrid classifier obtained by removing dominated points. Any point positioned inside the bounded region, can be outperformed by some point on the boundary. The boundary forms a hybrid classifier, or a set of potential candidates for an optimal solution. 
Figure 3. ROCCH for the two logistic regression models The hybrid classifier obtained from our two logistic regression models formed a convex set in the (FP rate, TP rate) plane. In general, this doesnt have to be the case. Figure 4 provides such an example. Point B, where the region boundary transitions from the neural networks model to the logistic model. The boundary dips below the line from A to C. This can be fixed by creating a new classifier B on the (A, C) line. For example, if we want a point half way between A and C, the new classifier is obtained by using classifiers A and B randomly, each with probability 0.5. Provost and Fawcett describe a similar scheme, using random sampling to create a new classifier. In general any convex combination of two classifiers becomes a new classifier. For any point X on a line segment created by classifiers nl, ~,, we can always construct a new classifier, which would correspond to such point. We start by describing such point as convex combination X=arq+ (1-a)r~ where 0--&lt; a_&lt;l 
Then, we randomly partition the population being classified into 2 groups in proportions a, 1-a and apply an appropriate model for each group. o.s .[ .:/ [ o.4 ~. ,/~r " X -'*-~'N'~ ~ 0.3 -I ." 
Figure 4. Boundary of ROC space for two classifiers is not A new, convex boundary for the attrition problem is shown iN 
Figure 5. 0.7-0.5 02 BASIC TERlVIINOLOG 
The following terminology is used throughout the remaining sections. -Two classes: positive (y) and negative (n) with probabilities respectively p and 1-p -Classification decision: positive (Y) and negative (N) -FP, TP, FN, TN represent number of instances of each kind: false positive, true positive, false negative and true negative respectively -Rates of those instances are represented as follows: FP rate = p(Y n) TP_rate = pCY y) FN-_rate = p (N y) TN_rate = p(N n) -In the ROC space, the horizontal axis represents FP_rate, and the vertical axis represents TP_rate. We will use x to denote FP__rate and y to denote TP_rate Unit cost of a false positive error = c(Y n) -Unit cost of a false negative error = c(N y) 
In defining the cost, we need to start with the expected number of errors and transform the resulting formula into the ROC space terms, where the variables are error rates. The following scheme visualizes interdependencies between terms. 
Let M be the total number of instances being classified. For given p, x and y we can calculate the expected number of classified cases of each kind as follows. COST FUNCTION 
We now apply the cost function to the attrition problem and look for a classifier on the boundaries of the convex hull, which will minimize the total cost. 
A false negative classification means that we won t recognize an attriting customer and loose the account. The cost of loosing a customer is tied to net income after taxes (NIAT) this customer costs related preventive action. The line of business decided to assign error costs as shown in Table3. 
Given cost of a false negative error c(N y), and a false positive error c(Y n), the total expected cost Where Eft'N) is the expected number of false negatives and 
E(FP) is the expected number of false positives. 
We want to minimize the expected total cost in term of decision variables x and y. 
Minimize EC = c(Y n) E(FP) + c(N y) E(FN) =c(Y n) (l-p) M x+c(N y) p M (1 y) =c(Y n) (l-p) M x+c(N y) p M-c(N y) pMy 
After subtracting the constant (not dependent on the decision variables) term and dividing by M, this is equivalent to: 
Minimize (EC -c(N y) p )/M = This is equivalent to maximizing the opposite function Maximize C =-c(Y n) (l-p) x+c(N y) py 
C in space (x,y) is a collection of parallel lines with slopes depending on misclassification costs and the a-priori probability of the positive class. In the attrition example we are analyzing, p=3.75%. We are now ready to calculate slopes for the cost lines. maximize C, we can start at the bottom of the convex hull and move the line up, as far as possible. Each point of the region touching the line in a given position has the same cost. This defines iso-performing lines. the attrition problem, and cost function moving up through the 
A. The theory of Linear Programming shows, that for a convex and bounded region, the objective fimction is maximized either at one of the vertices, or on a line segment joining two vertices 
Bazaara [1]. Thus, we can always pick one or more best performing classifiers. As in the ROCCH analysis, we have iso-performing lines, which help visualize performance of classifiers under various cost structures. In the Linear Programming setting, however, the feasible region c~tn incorporate any additional constraints. We are gaining flexibility and control. 5. ADDING ADDITIONAL CONSTRAINTS 
In the attrition example, some implicit constraints are already built into the ROC space. Error rates are non-negative and cannot exceed 1. Those constraints bound the convex set and guarantee existence of an optimal solution. In addition, a planned calling campaign is limited by customer service resources availability. A phone call scenario included approaching a customer to find out if indeed their intention was to leave the bank, and to attempt to entice them to stay. Naturally, there are limited resources bank can devote to this undertaking. Traditionally, modelers would select the best model, and any additional constraints would be imposed at the implementation time, based on the pre-selected model. But if a modeler is aware of the constraints, they can be built into the system evaluating model s performance. The line of business, for which these classifiers were developed, determined that they could handle calling 20% of their customer base. This resulted in the constraint need to formulate it in terms of the decision variables x and y. 
Geometrically, this inequality is represented by a half plane in the (x,y) space. Figure 7 shows a case of the attrition problem with an additional constraint. The new convex set is the intersection of intersection of the previous convex set with the new one: the area to the left (down) of the dotted line. Figure 7. The capacity constraint bounds the feasible region the new feasible region. We need to slide the cost line back down to the feasible region. 
It will touch the feasible region at the point where the hybrid defines a new classifier. The new classifier is now optimal, as shown in Figure 8. 
In the next section we show how to find the new vertex. Here, we will point out an actual implementation of the new solution as outlined by Fawcett and Provost in [3]. 
Assume that the intersection point divides the line segment proceed as follows. 1. With probability ~ use classifier A 2. With probability 1-~ use classifier B 
If A and B were obtained from the same model by varying the decision threshold, this process can be simplified by finding an appropriate threshold between A and B. 6. LINEAR PROGRAMMING FORMULATION 
A linear program is a constrained optimization problem, where the objective function, as well as all constraints, are linear. We need to select values for all decision variables so that all constraints are satisfied and the objective function is minimized (or maximized). In this case, we need to pick a point(s) in the 
ROC space which will minimize the cost function (or maximize the modified cost fimction). Decision variables are then the coordinates (x,y) of points in the ROC space. Some of the constraints arise from the classifiers performance. 
In the ROC space, points under consideration need to be below the boundary of the convex region. Additional constraints are usually related to implementation and/or quality issues. 
Finally, we have non-negativity constraints and bounding constraints, since the ROC variables have to be between 0 and 1. 
A canonical form of a linear (maximization) program takes the following format. 
The theory of linear programming assures us, that if the set of constraints forms a convex and bounded set (called the feasible the feasible set is created as a conjunction of several linear inequalities. Not all vertices are known explicitly. A number of computational techniques have been designed to find optimal solutions without explicitly iterating over the vertices. More details can be found in [1] and [2]. 
The theory of Linear Programming also aids an analysis of a solution found, if we want to play some what if scenarios. At a point of optimality, som, constraints will be satisfied as binding . That is, the left-hand-side will be equal to the right hand side. Others will be satisfied as an inequality, leaving slack, or room for improvement. 
In a two-dimensional case, if two constraints are found binding at an optimal solution, the classifier at the intersection of those constraints is optimal. An analysis of slack at neighboring points (called marginal analysis or sensitivity analysis), often provides insight into alternative solutions and how close they are to optimality. All commercially available optimization packages, including a module that comes with Excel, will provide slack information for all constraints. We are showing an example of 
Excel sensitivity analysis report in the Appendix. In that example, the capacity constraint, which was based on workforce availability, is binding. That means that any further improvement of classifier s performance will require additional workforce resources. Additional resources will provide slack on the capacity constraint. This will allow the optimal solution to move along the convex hull boundary in a direction improving the objective function. 
We will now formulate the problem of looking for optimal classifier in the presence of additional constraints, as a linear programming problem. We already have a formal representation of the objective function. Maximize C (x, y) = -c(Y n) (l-p) x + c(N y) p y 
Now we need to formulate the set of constraints related to the classifiers and add a set of additional, bounding and non-negativity constraints. 
Given two classifiers Pi and Pj on the boundary, with error rates points P~ Pj has the slope and the equation Y Yi = m ( x xi) 
The feasible region is positioned below the line, so it is determined by a collection of inequalities which can be rearranged as The optimization problem can now be defined as follows. 
Given classifiers Pi with error rates (xi, Yi), i = 1,2, k, such that xi -&lt; xi+l Maximize C(x,y)=-c(Y n) (l-p) x+c(N y) p y 
Such that 
In the absence of additional constraints, all vertices are known a priori. In such case, it is computationally efficient, to explicitly calculate expected cost of each classifier and choose the one with the smallest cost, rather than actually slide the lines over the feasible region. When additional constraints are introduced, the situation changes. 
Finding new vertices, formed by the additional intersection points, can get cumbersome. Fortunately, the theory and practice of mathematical programming provides efficient algorithms for finding optimal solutions without a need to iterate over all vertices. There are also efficient solvers on the market, which can solve optimization problems. An add-on to Excel provides one ones described here, and it is available in almost any business setting. Figure 9 shows the original attrition problem, without the additional constraint, solved in Excel. Spreadsheet cell labeled 
Cost contains the cost formula. Value of that cell is maximized, by changing decision variables x and y, as long as all constraints are satisfied. Point (0.64, 0.96) minimizes the objective function. Figure . Excel optimi er solves the optimi ation problem Slopes of line segments on the boundaries are calculated in column 3. Note that at point (0.64, 0.96), slope changes from 0.55 (0.64, 0.96) would have been selected as optimal by the traditional ROCCH analysis as well. Figure 10 shows a new solution, after the additional capacity constraint was added. The new solution is (0.15, 0.44). For all practical purposes, a business setting often prefers speed and expediency of delivery, to an optimal solution, as classifier s performance is near optimal. In case of the attrition problem, we in close proximity of the optimal solution (0.15, 0.44) and still within the feasible region. A selection of near optimal solution was, in this case, a simple decision because this was a relatively simple problem with just one additional constraint. While analyzing the new optimal solution, we notice that the true positive class, only 44% (less than a hal0 is classified correctly. This is not a desirable performance. 54% of defecting customers remain unrecognized and without being contacted will leave the bank. Solution analysis (see Appendix) shows that the capacity constraint is binding at the optimal point. We cannot increase the True Positive rate y, without leaving the feasible region and violating the capacity constraint. Any further improvements in the attfitors recognition rate will require additional resources so that the capacity constraint can be relaxed. The optimization template set up in Excel allowed us to play a number of what if scenarios. In a final compromise, the line of business decided to be classified as positive. In return, we added a new missed campaign resulting in limiting a campaign size) 
