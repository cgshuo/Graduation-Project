 To realize services that provide serendipity, this paper as -sesses the surprise of each user when presented recommen-dations. We propose a recommendation algorithm that fo-cuses on the search time that, in the absence of any rec-ommendation, each user would need to find a desirable and novel item by himself. Following the hypothesis that the de-gree of user X  X  surprise is proportional to the estimated sea rch time, we consider both innovators X  preferences and trends f or identifying items with long estimated search times. To pre-dict which items the target user is likely to purchase in the near future, the candidate items, this algorithm weights ea ch item that innovators have purchased and that reflect one or more current trends; it then lists them in order of decreasin g weight. Experiments demonstrate that this algorithm out-puts recommendations that offer high user/item coverage, a low Gini coefficient, and long estimated search times, and so offers a high degree of recommendation serendipitousness. H.3.3 [ Information filtering ]: Information filtering by rank-ing Algorithms, experimentation Personalization, Ranking, Serendipitous Recommendation s, Innovator, User Flow, Collaborative filtering, Trend
Collaborative filtering(CF) aims to improve the user X  X  ex-perience and discovery by providing a better interface to th e potentially overwhelming set of choices. The volume of item s now exceeds the ability of any individual to accurately asse ss their desirability. This information overload problem is a lso serious for many service providers and thus has heightened demands for personalized recommendations. CF is used in web-based services, where there are a vast numbers of items such as Web pages, digital music, and video contents for browsing(download or purchase), and is regarded as one of the most promising recommendation algorithms.

Traditional CF algorithms focus on optimizing accuracy measurements such as precision/recall and neglect other fa c-tors, e.g, novelty and serendipity. For example, many CF algorithms use similarity scores between all pairs of users in an attempt to improve the top-N precision, and present items in proportion to their popularity among like-minded users [10] [12]. However, the most popular items in any log collection are the ones that a given user will recognize with high probability, or be broadly familiar with. Hence, highl y accurate recommendations appear far too obvious and of little help to users, and seem to fail to enhance user satis-faction [3] [17]. In fact, recommendation systems that ap-propriately discount popularity lead to an increase in the total sales volume [2] [4]. Moreover, novel and serendipito us recommendations are necessary to broaden the user X  X  view in the recommendation flow.

In this paper, we aim to emphasize the surprise of each user with the recommendation, instead of posing the typi-cal CF task: how well will the user like a candidate item? For this goal, our algorithm focuses on the estimated search time that the user would take to find the item by himself. This algorithm is based on the assumption that an item re-cently purchased by the innovator will surprise the followe r (the other user) more than other items. Namely, this item will have a longer estimated search time than other items meaning that the purchased item would be time-consuming for the user to find. Following this idea, we extend the Per-sonal Innovators Degree (PID) [8] into the Personal Innova-tor Probability (PIP) for identifying innovators and define a User flow probability (UFP) for measuring how likely the user is to purchase the item just after purchasing an arbi-trary item. In ranking candidate items for a given user, this algorithm weights each item by both PIP and UFP scores of the most latest purchases of his/her personal innovators and including trends.

One key advance of our algorithm is that it reduces the time spent by the user in seeking novel items compared to the conventional alternatives. This earned time is in propo r-tion to the difference between the estimated purchase time when the target user is likely to purchase the item without any recommendation and the time when this item is recom-mended to the user. To weight items in proportion to the value of this difference, we use the time lead and the number of users in both PIP and UFP. We define PIP using the pur-chase time offset, the number of users, and the relationship over multiple steps. Likewise, UFP is determined by using these time factors and the multiple steps. Consequently, we weight item serendipitousness by innovators and then item novelty by trends, where this weight is in proportion to the length of estimated search time.
Novelty and serendipity metrics measure the degree of  X  X on-obviousness X  X f recommendations with the goal of avoi d-ing X  X herry-picking X  X 6]. The system of [16] can infer wheth er an item, one that is considered relevant, contains any novel information as indicated by five proposed measures that are intended to capture redundancy. Yang et al [15] define nov-elty in terms of user knowledge, and his/her degree of in-terest in an item. Ziegler et al [17] propose a topical di-versification approach to balance and diversify personaliz ed recommendation lists; the goal is to use intra-list similar -ity to reflect the complete spectrum of the user X  X  interests. Fouss et al [5] incorporate Euclidean commute time distance, which is one of random-walk-based techniques, when com-puting the similarities of nodes. Recently, Celma et al [2] presented two complementary methods to analyze and eval-uate novel recommendations. The item-centric evaluation method involves analyzing the item-based recommendation network to detect whether the intrinsic topology of the net-work has a pathology that hinders novel recommendations. The user-centric evaluation aims to measure the perceived quality of the recommendations.

Although these approaches focus on providing novel and serendipitous recommendations, they ignore the dynamics, which describes the changes in user preferences over time. For example, new items are different from old items in terms of their serendipity, even if they share almost the same pop-ularity. While the former items are novel and as such are unknown to many people, the latter items have been recog-nized but not purchased by them. Identifying these differ-ences is valuable if the serendipitous recommendation flow is to retrieval worthwhile items for the user.

To distinguish these differences, recommendation algo-rithms need to consider both the change in user preferences and the trends of items in computing user-user and item-item relationships, respectively. In this paper, we use PIP , which measures how much earlier the innovator purchased the item, to compute user-user relationships and UFP, which measures how likely the user will be to purchase each item just after purchasing an arbitrary item to determine item-item relationships.
Here, we intuitively explain why the estimated purchase time is useful for improving serendipitousness and explain the ideas behind our algorithm.

A serendipitous recommendation helps the user find a sur-prisingly interesting item he might not have otherwise dis-covered [6]. The distinction between novelty and serendip-itousness is important when evaluating collaborative filte r-ing algorithms, because the potential for serendipitous re c-ommendations is one facet of collaborative filtering that Figure 1: Reducing search time by basing recom-mendations on the estimated purchase time: For example,  X  t a 2 denotes the estimated purchase time when user a would purchase item i 2 without a rec-ommendation traditional content-based information filtering systems d o not provide. To provide a clear example of the difference between novelty and serendipitousness, consider the case in which a simple recommendation system presents movies to a user whose favorite director is  X  X ames Cameron X . If the system recommends his latest movie  X  X vatar(2009) X  to these users, this movie is certainly novel, but probably not serendipitous. Since the user is his fan, he is likely to alre ady know of that movie via advertisements, discussion groups, or like-minded friends, or will find it in the near future. On the other hand, however, a recommender system that shows  X  X rincess Mononoke(1997) X , which has themes similar to  X  X vatar X , and which is not an obvious recommendation, provides serendipity to the user. Because this is a foreign movie produced by a different director, many users would take much longer for find it by themselves in the absence of any help. Consequently, serendipitous recommendations need to find the users with well-proven predictive ability (innovators) and use their logs.

First, we propose to use  X  X ime X  as a novel view for quanti-fying serendipity. As shown in Figure 1, time here means the difference between the time at which target user u a would purchase items (estimated time) i 2 (  X  t a 2 ), i 3 (  X  t times at which these items are recommended to user t 0 . Our algorithm is based on the assumption that this difference is useful as a measure of serendipity, since items with long es-timated search times will surprise the user. Accordingly, t he proposed recommendation algorithm ranks items in decreas-ing order of this difference; it yields serendipitous items a nd greatly reduces the time need for the user to find these items. u a than |  X  t a 2 -t 0 | . Moreover, i 3 seems to offer more serendip-ity to u a than i 2 . That is, serendipitous recommendations are real time-savers for consumers.

Second, we focus on X  X nnovative X  X onsumers for identifying the user logs that contain serendipitous items. In Figure 1, u b is an innovator for u a . Among like-minded consumers, innovators become aware of items well before their release and purchase these items soon after their release. Accord-ing to Rogers X  innovator theory, the early consumers who focus on undiscovered or unreleased items are called  X  X nno-vative X  [13]. The purchase logs of innovators include more serendipitous items that other like-minded consumers woul d like to acquire but have not yet become aware of.
Actually, many innovators can be found in CD/DVD pur-chase logs. Innovators are able to discover even poorly-marketed movies (e.g. Napoleon Dynamite(2004), The Vis-itor(2007) and The Hangover(2009), which become popu-Figure 2: Typical temporal flow of consumers ob-served in purchase history logs: The direction de-notes the purchase order time lar long after its release) ahead of others. These movies, once  X  X iscovered, X  are rapidly acquired by like-minded fol -lowers. Since Cameron likes the films of Hayao Miyazaki, like X  X rincess Mononoke X , X  X ausicaa: Valley of the wind(19 84) X , and  X  X aputa: Castle in the Sky(1986) X , where themes are pacifism, harmony with nature, flying aircraft, floating cas-tles, etc, and has watched these films, innovative consumers get to know this fact and then watch these movies, too. Con-sequently, innovators have the shortest offset, |  X  t u i innovator u i for item j .

Although innovators discover serendipitous items ahead of others, these items lose their serendipitousness over time . In Figure 1, i 3 better satisfies u b  X  X  preference than i 2 will be purchased i 3 after i 2 . Therefore we incorporate the trend of items in computing item-item relationships. Fig-ure 2 shows a simple example of consumer transitions for three items: i a , i 1 and i 2 . Let i a be the specified item, i.e., the item observed in the log of the target user. Since CF processes user purchase logs to decide which candidate items are to be recommended to the target user, its quality depends on how to weight items i 1 and i 2 for the user who has purchased i a given the preference order of this user.
Conventional algorithms based on item-item similarity com -pute the similarity scores among items using the number of consumers who purchased both items. This is because the underlying assumption of these algorithms is that those who agreed in terms of past behavior will tend to agree in the future, and the similarity score is proportional to the de-gree of agreement. For example, in Figure 2, each item of set i 1  X  3 has been purchased by the same users in common with i a , and thus each item is treated equally according to the similarity defined for i a . Accordingly, CF assigns equal weights to i 1  X  3 .

We, however, need to differentiate these items by focusing on the order of item purchase time. Here, i 3 was purchased by three users after i a , while i 2 was purchased by three users before i a . As stated before, we naturally assume that i better matches the future preferences of users who have just purchased i a than i 1 or i 2 , and will be preferred by them. Accordingly, i 3 is a more appropriate item for a user who has purchased i a than i 1 or i 2 . Therefore, information on item purchase time is useful in identifying the trend in item s. Consequently, it is more reasonable to compare the number of common users as well as purchase times in computing item-item relationships.
The aim of personal innovator probability (PIP) is to iden-tify the innovators. We extend PID to model how innova-tors are followed by multiple users and steps for ranking how likely each user is to be an innovator among all users. This approach is based on the assumption that if u c is an inno-vator for u a and u d is an innovator for u c , u d seems to be more useful to u a than u c . The aim of PID is to weight user logs appropriately given a target user and to identify logs that match the precedence preference of the target user . PID consists of two time factors and one item factor, and is given by: Here, C ab is the set of common items that both u a and u b have purchased, r ( i ; b, a ) denotes the degree of u b personal innovator to u a for common item i , w ( i ; a ) denotes how recently u a purchased common item i , and v ( i ) denotes how important common item i is as a discriminator of like-minded users. We describe each of the factors below: where T i denotes the release time of i and  X  t i denotes the average time i was purchased after its release, t a,i and t denote the times at which u a and u b purchased i , respec-tively. where e a,i denotes the time passed since u a purchased i , and  X  e a denotes the average of e a,i over all items purchased by u . where U i denotes the number of users who purchased i (taken from the logs).

It is clear that determining the user-user relationships vi a common items will encounter the problem of data sparse-ness, since the ratio of users who share common items de-creases rapidly in inverse proportion to the number of items .
To solve this problem, our approach is to model the in-novator relationship as an ergodic Markov chain; this guar-antees the convergence of the power of the Markov chain-based matrix as follows: First, we define innovator proba-bility p ( b | a ) according to the Bayesian rule: where p ( b | a ) represents the probability that, given u likely u b is to be regarded as the best innovator among all users for u a . The innovator probability p ( b | a ) is a relative measure since it is normalized over all users and selects b from among all users, while PID is a absolute measure. Sec-ond, we define the revised innovative probability  X  p ( b, a ) for implementing the ergodic Markov chain. Moreover, we consider the revised innovator probability as the primitive matrix  X  P for making the Markov chain ergodic: where  X  P is the innovator probability matrix consisting of p ( b | a ); e is the column vector of all ones and  X  is a weight parameter. By this definition,  X  P is constructed as a primi-tive stochastic matrix that models the probabilities of per -sonal innovator transition and that converges to a stationa ry distribution. Finally, we calculate the innovator relatio n-ship over multiple steps and paths over the users. Since short paths indicate a more direct relationship among users and more important innovators, we consider assigning lower weights to long paths:  X  P = ((  X   X  P )+ where 1 N ! assigns lower weight to longer paths,  X  is a pa-rameter to control the effects of transitivity, and exp (  X   X  ) is a normalization factor. The larger  X  is, the more strongly are longer paths weighted. Clearly, PIP enables us to iden-tify users who could not, according to PID, be regarded as innovators. We call innovator relationship  X  p ( b | a ) the Person-alized Innovator Probability P IP ( u b , u a ), and use it instead of P ID ( u b , u a ).
The aim of user flow probability (UFP) is to follow trends, and detect novel items by employing the idea discussed in the previous section. UFP weights each item appropriately given a specified item; it is an estimate of how many con-sumers will purchase this item after the specified item. We consider the function of consumers who have purchased item b at t after a in assessing transition probability p ab , and use this probability to predict which item will be purchased.
Similar to PIP, we model the probability by using CTMC, which satisfies the Markov property; it takes its value from a set called the state space. Continuous-time Markov pro-cesses are most easily defined by specifying the transition rates q ij , and these are typically given as the ij -th elements of the transition rate matrix Q , which contains all infor-mation about the transitions of the Markov chain. For the probabilities to be conserved, i.e., to add up to one, the off-diagonal elements of Q must be non-negative and the diagonal elements, which we call jump-rate, must satisfy which represents the chain from item i with rate q i . When the stochastic process leaves item i , it will next settle on item j with probability p ij , which is independent of the time spent at item i , and that satisfies p ii = 0 and P j 6 = i Accordingly, we gain We assume that the stay period at item i follows an exponen-tial distribution with shift-rate q i . According to the property of exponential distributed random variables, t i with rate q is given by Figure 3 shows the age of a DVD rating, i.e., the time passed since the release of an item as the horizontal axis, and the number of users who rated a DVD of that age as the vertical axis as determined from actual online music and video down-load services in Japan. Details of the data will be described in a later section. The fitting lines, exponential functions of time, are also shown, where the number of users who evaluated an item decreases exponentially as the item ages. This figure supports our approach of utilizing exponential distributions.

Given the jump-rate, we calculate the transition probabil-ity from item i to item j as where t u,ij denotes the time that user u took to move from item i to item j . Determining q ij from Eq. (10), we gain To predict the trends at time t = t f , we calculate the probability of the user flows from item i to others in the trix U with where P ( t ) denotes the transition matrix with the ij -th ele-ments of matrix p ij . Formally, when the state space is finite, the transition probability can be estimated by using where I is the identity matrix. The solution is We then get the transition matrix function that satisfies the Kolmogorov forward and backward equations. If Q can be diagonalized by Q = MDM  X  1 , then For large Q , a Taylor approximation can also be used, Consequently, UFP consists of u ( i b | i a ,  X  ) and is given by: Unlike the traditional item-item relationship based on pai r-wise similarity, UFP ranks how similar users will become finally; it prevents novel items from being overwhelmed by well-known items with high popularity. Figure 3: Distribution of the number of users who rated a DVD at a specific age of the DVD (time passed since its release) in Netflix. The horizontal axis corresponds to DVD age (measured in days) while the vertical axis shows the number of users who rated DVDs of that age.
We identify the informative logs for target user u a by us-ing PIP and then recommend items to u a according to the weight based on both PIP and UFP. By using PIP defined in Eq. (8), p ( i b | u a ), the probability that u a will purchase item i b , is defined as follows: where,  X  ( i a | u j ) represents the evidence that u j has pur-chased item i a , namely,  X  ( i a | u j ) = 1 when i a is in the pur-chase history logs of u j , otherwise  X  ( i a | u j ) = 0. Clearly, p ( i b | u a ) becomes high when there are many personal inno-vators with high PIP and high UFP who purchased i . When p ( i b | u a ) is computed for all items, i , we can simply recom-mend the top-N items, the N largest p ( i b | u a ), to u order to avoid creating a list of  X  X rivial X  recommendations , we must remove items that have already been purchased by the target user, from the recommendation list before pre-senting it to the target user.

A recommendation algorithm based on UFP and PIP is shown in Figure 4. This figure shows that we can calcu-late PIP over each pair of users by comparing just their purchase history logs in a manner similar to conventional methods. The computational cost of this process is linear, O ( I ), against the number of items, I , and thus is no more expensive than the conventional methods. The very low cal-culation costs of UFP and PIP mean that they can be easily introduced into any personalized recommendation service.
We conducted simulations on the following four data sets, two of which consist of purchase histories obtained from rea l on-line music and video download services in Japan, one from the rating logs in Netflix, and one from search query logs.
 Input: user logs Output: item ranking p ( i | u a ) N i : Number of users who purchased item i I : Total number of unique items observed in user logs N u : Number of items purchased by user u U : Total number of unique users observed in user logs
T 0 : Present time (=the time to recommend items) calculate  X  t i and  X  e u for all items and users for i = 1 to I do end for for u = 1 to U do end for calculate P IP ( u b , u a ) on each pair of u a and u b for a = 1 to U do end for calculate UF P ( i b , i a ) on each pair of u a and u b for a = 1 to I do end for rank items for each target user u a for a = 1 to U do end for
The first purchase log data set is a group of music down-load purchase records from April 1st, 2005 to July 31st, 2006 . It lists 44,527 items purchased by 84,620 users. Each pur-chase record consists of title of music purchased, artist na me, CD album title, purchase time stamp, and price. The sec-ond set is a set of movie video download purchase records from September 1st, 2005 to February 28th, 2006. It lists 4,064 items purchased by 7,537 users. Each purchase record consists of the title of the video purchased, director X  X  nam e, purchase time stamp, and price. Most items in these data sets are  X  X ewly released X  items that became available for purchase from the service as soon as they were first released as a CD or Video (DVD). Netflix contains a set of 100,480,507 rating records from Nov 11th, 1999 to Dec 31st, 2005, that list 17,770 movies rated by 480,189 users. We first selected only those users who rated at least 20 movies and movies that were rated by at least 100 users. This pre-processing downsized the data set to 85,730,203 rating records from Nov 11th, 1999 to Dec 31st, 2005; it consists of 9,264 movies rated by 136,589 user s. Each rating record consists of movie title id, user id, ratin g, and timestamp.

Unlike the first two purchase log data sets, Netflix con-sists of user rating logs of movies with multi-valued rating s. We have to convert the ratings to binary values, namely the value is 1 (purchase) if the user rates an item, and 0 (no purchase) otherwise and made two data sets, Netflix(o), Netflix(p), in the same manner as in [8].
The data set Query was generated from a search engine server log from April 1st, 2006 to May 31st, 2006. This data set consists of 35,325,842 query records, each of which consists of query keyword id, user id, and timestamp.
Our approach aims to predict which piece of music, video or movie (query) a user will purchase (submit) given the past purchase history of the user. We conducted simulations to evaluate the predictive performance of recommendations vi a K fold cross-validation where the original data was parti-tioned into K subsamples at random. Of the K subsamples, a single subsample was retained as the validation data for testing the model, and the remaining K -1 subsamples were used as training data. We repeated this process K times, with each of the K subsamples being used just once as the validation data. Each subsample was divided into two peri-ods: a learning period and a test period. We call the data in the test period the test data and that in the learning period the learning data; K was set to 10.

In the simulations, we treated each user in the test data as a target user to whom we applied each of the recom-mendation methods by using user logs collected from the learning data. We then presented the top N ranked items to the target user and confirmed that these recommended items existed in the test data. This is the traditional rank-ing based recommendation scenario. To evaluate the quality of the proposed method, we used top N precision, a measure commonly used for evaluating the predictive performance of CF [9].
We applied a total of nine proposed and conventional rec-ommendation methods to the data sets and compared the precision of their top-N recommendations ( N values were 1, 5 and 10). Popular recommends the most popular items dur-ing the last one month of the learning period and thus it is not personalized to the user. Pearson and Cosine are based on user similarity as measured by Pearson X  X  correlation coe f-ficient and cosine similarity, respectively. On the other ha nd, Item is based on content similarity as measured by Pear-son X  X  correlation coefficient proposed in [1]. bPLSA is based on Probabilistic Latent Semantic Analysis [7] with Bernoul li distribution. MEA is the maximum entropy approach pro-posed in [11]. EABIF represents the early-adoption-based information-flow approach proposed in [14]. PIP+UFP is the proposed method. In this paper, we set  X  = 1 in Eq. (7),  X  = 1 and N = 5 in Eq. (8). The results are shown in Ta-ble 1.

In addition to the top-N precision above, we also evalu-ated performance by two coverage measures, the Gini coeffi-cient, the elapsed time, and the difference time. The values of the two coverage terms, Gini coefficient, avg elapsed and avg difference results are shown in Table 2.
As the coverage measures, we calculated the percentage of the number of unique items appearing in the top-N recom-mendation list from the total number of unique items; we denote this coverage measure as  X  X tem coverage X (IC). The item coverage of a recommender system is a measure of the size of the item domain in the system from which the sys-tem can recommend [6]. Systems with lower coverage may be less valuable to users, since they can offer only limited choices in assisting users to make decisions. We also calcu-lated the percentage of the number of users to whom each method could recommend any item from the total number of all users who purchased any item in the test period; we denote this coverage measure as  X  X ser coverage X (UC).
The Gini coefficient is a measure of the statistical disper-sion of the distribution of users over items and is defined as the ratio of the areas on the Lorenz curve diagram; it takes values between 0 and 1. A low Gini coefficient indicates that the distribution is flat, while a high Gini coefficient indicat es that the distribution is extremely biased: 0 corresponds to perfect equality (every item has been purchased by exactly the same number of users) and 1 corresponds to perfect in-equality (where one item is purchased by all users, while none of the other items are purchased by any user). In other words, a result with a high Gini coefficient means that a few particular items tend to be ranked highly by most users and thus recommendations are not strongly personalized.
The elapsed time measures how much time has passed from each item X  X  release day to its day of purchase by each user. A short elapsed time indicates that the item is novel. We measured the average over all user-item pairs in the test data that each method could predict and denote this as avg elapsed (AE).
The difference time is the difference between the observed time when each user u purchased item i in the test data without recommendations t ui and the time when divided into two periods (learning/test) t 0 (Music:July 1st, 2006, Video:February 15th, 2006, Netflix(h),Netflix(p):Decembe r 1th, 2005, Query:May 31th, 2006) and defined over each user and each item, as discussed in Section 2. Therefore, an item with a large difference, | t ui  X  t 0 | , indicates a time-consuming item for u ; that is, it takes a long time for u to find this item on his/her own in the absence of any recommendation. We measured the average over all user-item pairs that each method could predict and denote this as the avg difference (AD).
From Table 1, we can see that PIP+UFP offers the high-est accuracy like PID . These results can be explained by the characteristics of the data sets; they consist of  X  X ux-ury items X . When an individual user desires and purchases p &lt; 0 . 05 , are marked with  X ** X ,  X * X , respectively. these items, they are generally motivated by their prefer-ences rather than their needs. PIP+UFP differentiates like-minded users by using time decay factors to assign higher weights to more recently purchased items, and introduces the trend of items into item ranking. Consequently, the pro-posed method ranks items that match the latest preferences, which improves the precision.
From Table 2, we can see that PIP + UFP exhibits the highest user/item coverage, the lowest Gini coefficient, clo se to 0, and the longest average difference. Although the top-N results don X  X  show the significant improvement of the pro-posed algorithm, this result indicates that PIP + UFP can rank and recommend various different items with much less bias than the other methods. In fact, those conventional al-gorithms, which do not consider the dynamics of user pref-erence or the user-user relationship, achieve lower item co v-erage and higher Gini coefficients, close to 1. They use logs of like-minded users who are later adopters as well as earlie r adopters and thus recommend generally popular and trivial items. With regard to user coverage, the algorithms that em-ploy the dynamics of the user-user relationship offer slight ly lower values than the others. Although users who are in-novators are not covered by EABIF, PID anyway, PIP + UFP algorithms can recommend some novel items to these innovators, most of these items have not yet been purchased by the others, which raises precision.
Here, we discuss the potential of the proposed algorithm from the viewpoints of serendipity and novelty in recommen-dation flow.

Although we are not able to distinguish the effect of inno-vators from that of trends from our experiments completely, the fact that the PIP improves AD, supports our assumption that innovators know more serendipitous items than the oth-ers. As shown in Table 2, EABIF and PID , which include the time factors, offers longer AD than others (i.e. those without time factors). The main difference between PID and PIP is that the former considers only pairwise compar-ison, while PIP uses an ergodic Markov chain to model how innovators are followed through multiple steps. According ly, PIP enables us to discover more items that would take more time to discover because of these multiple steps than is pos-sible with the other methods; it can recommend surprisingly interesting items that might not otherwise be discovered.
As shown in Table 2, the results that the proposed method yields the shortest, on average, elapsed time indicate that it is best at identifying novel items. Because these items are so new th they are not yet generally known to many other like-minded users, users would take some time for them to find the items by themselves. Accordingly they are novel items and their popularity is low. Since UFP suppresses well-known items in the recommendations, it cannot offer high top-N precision in the simulation comparison. How-ever, PIP improves the IC and ED; it can identify more novel items since its Gini coefficient is close to 0.
The contribution of this paper to the recommendation re-search field is to show the impact of trends on the transition probability of items, and that the estimated time offset to purchase in the absence of recommendations is a useful met-ric of serendipitousness. Our approach offers improvements on the user/item coverage, the Gini coefficient, the elapsed time, the difference in estimated time, and the predictive performance simultaneously.

Following the hypothesis that items that innovators have recently purchased offer serendipity and will be adopted by their like-minded users more willingly than items purchase d by others, we use PIP for identifying innovators and define UFP that estimates how likely the user is to finally purchase each item.

In future work, we would like to extend the proposed model to incorporate user specialty and level of expertise in a particular domain, and apply this extended model to personalized information retrieval more generally. [1] J. K. B. Sarwa, G. Karypis and J. Riedl. Item-based [2] ` O. Celma and P. Lamere. A new approach to [3] D. Cosley, S. Lawrence, and D. M. Pennock. Referee: [4] D. Fleder and K. Hosanagar. Blockbuster culture X  X  [5] F. Fouss, J.-M. Renders, A. Pirotte, and M. Saerens. [6] J. L. Herlocker, J. Konstan, L. Terveen, and J. Riedl. [7] T. Hofmann. Collaborative filtering via gaussian [8] N. Kawamae, H. Sakano, and T. Yamada. Personalized [9] J. Konstan, B. Miller, J. H. D. Maltz, L. Gordon, and [10] G. Linden, B. Smith, and J. York. Amazon.com [11] D. Pavlov and D. Pennock. A maximum entropy [12] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [13] E. M. Rogers. Diffusion of Innovations . The Free [14] X. Song, C. Lin, B. Tseng, and M. Sun. Personalized [15] Y. Yang and J. Z. Li. Interest-based recommendation [16] Y. Zhang, J. Callan, and T. Minka. Novelty and [17] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and
