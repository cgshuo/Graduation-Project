 In this paper, we extensively study the use of syntactic and semantic structures obtained with shallow and deeper syntactic parsers in the answer passage reranking task. We propose several dependency-based structures enriched with Linked Open Data (LD) knowledge for representing pairs of questions and answer passages. We use such tree structures in learning to rank (L2R) algorithms based on tree kernel. The latter can represent questions and passages in a tree fragment space, where each substructure represents a powerful syn-tactic/semantic feature. Additionally since we define links between structures, tree kernels also generate relational features spanning question and passage structures. We derive very important find-ings, which can be useful to build state-of-the-art systems: (i) full syntactic dependencies can outperform shallow models also using external knowledge and (ii) the semantic information should be de-rived by effective and high-coverage resources, e.g., LD, and in-corporated in syntactic structures to be effective. We demonstrate our findings by carrying out an extensive comparative experimenta-tion on two different TREC QA corpora and one community ques-tion answer dataset, namely Answerbag. Our comparative analysis on well-defined answer selection benchmarks consistently demon-strates that our structural semantic models largely outperform the state of the art in passage reranking.
 I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  Text analysis; Language parsing and understanding ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Re-trieval models Algorithms, Experimentation Question Answering; Learning to Rank; Kernel Methods; Struc-tural Kernels; Linked Data  X  Professor at University of Trento, DISI.

Previous work has shown that advanced natural language pro-cessing can positively impact the accuracy of Question Answering (QA) systems. As shown by the experience in the TREC QA task, e.g., [38], the selection of the right passage expressing the answer requires to consider the relation between the question and the pas-sage text. In other words, it is not enough measuring the similarity between question and passage, but it is also important analyzing how some concepts in the questions, e.g., constituting the focus, are structured along with other concepts in the answer passage. For the question focus, sports stadium is linked to the property the eighth wonder of the world . Since in the AP the property above is also stated for Astrodome , we infer that the latter is the correct answer. The inference is about finding two related statements in Q and in AP regarding the Q focus, where the entities in the state-ments may not necessarily match. Automatically selecting the right properties to be considered is a difficult task that must exploit the relationship between Q and AP. For example, the property Titans used to be called the Oilers can be important but it is not useful for answering the question. It follows that the relation between Q and AP for property selection is necessary.

Since manually selecting properties, i.e., generating rules for any pairs of Q and AP is rather difficult or even impossible, automatic feature engineering approaches based on kernel methods, e.g., [31], have been developed, where syntactic and semantic tree represen-tations of the Q/AP pairs are used in kernel-based L2R algorithms, e.g., SVMs [33]. The role of kernels was to implicitly generate syn-tactic patterns (i.e., tree fragments) to be used as features in SVMs.
However, this approach would not be able to solve the example above since, to answer the question, Astrodome has to be identified as a stadium. This is essential to derive that the focus of Q, sta-dium , and Astrodome in AP both own the same property, i.e., to be  X  the eighth wonder of the world ". Without this link many incorrect
The solution (provided in [13]) for solving this case is the use of semantic resources: the lexical answer type (LAT) of the ques-ple for the rest of the paper. 2 For example see http://en.wikipedia.org/wiki/ Eighth_Wonder_of_the_World#Things_labeled_ as_the_Eighth_Wonder_of_the_World against the one of the answers for improving candidate selection, e.g., for Astrodome the LAT is stadium . More in general, seman-tic resources help to reduce data sparseness and thus they enable matches between question and answer passages. However, finding the focus word and its category may be difficult and error prone and most importantly does not allow for solving non-factoid questions.
In this paper, we carried out an extensive study on the use of syntactic and semantic structures enriched with LD semantics for answer passage reranking. To make the feature design step eas-ier, we adopt L2R models based on SVMs and structural kernels, which provide SVMs with structural patterns, automatically gener-ated from Q/AP syntactic structures. The latter are enriched with relational semantic knowledge of LD by adding links between the entities in the Q and AP. In particular, we followed the steps below:
First, we design a representation for the Q/AP pair by engineer-ing a pair of shallow syntactic trees connected with relational nodes (i.e., those matching the same words in the question and in the an-swer passages). This approach capitalizes on our successful models proposed in [31, 32, 29, 37] but we also explore deeper linguistic structure such as dependency trees.

Secondly, we use YAGO [16], DBpedia [4] and WordNet [12] to match constituents from QA pairs and use their generalizations in our semantic structures. Following our previous work in [37], we employ word sense disambiguation to match the right entities in YAGO and DBpedia, and consider all senses of an ambiguous word from WordNet. For example, our system automatically de-rives that Astrodome is a stadium and thus such concept is con-nected to the question structure: stadium has been billed as  X  X he eighth wonder of the world" , through the word stadium . This way, we obtain connected structures of pairs of texts, which potentially contain patterns useful for capturing the relatedness of question and answer passage.

Thirdly, we apply structural kernels to the above structures by exploiting SVMs for automatically learning classification and rank-ing functions. In particular, we apply the Partial Tree Kernel (PTK) [24], which can generate the richest space of tree fragments.
Next, we experiment with three different corpora, (i) the stan-dard TREC QA corpus for passage reranking, (ii) a QA benchmark built for testing sentence reranking [42], and (iii) a community QA (i) traditional feature vectors, (ii) automatic semantic labels derived by statistical classifiers, e.g., question classifiers, and (iii) relational structures enriched with LD relations. The results show that our methods greatly improve on strong IR baseline, e.g., BM25, up to 101%, and on state-of-the-art reranking models, up to 16.0% (rela-tive improvement), e.g., in MAP.
 In the remainder of this paper, Sec. 2 reports on related work, Sec. 3 describes our proposed classification and reranking frame-work, Sec. 4 illustrates our basic representation approach. Sec. 5 describes our new algorithms to carry out semantic matching using LD, Sec. 6 shows how we use LD matches for defining relational structures, Sec. 7 presents our learning to rank models based on tree kernels, Sec. 8 illustrates our experiments and finally, Sec. 9 derives the conclusions.
A referring work for our research is the IBM Watson system [13] (hereafter referred as Watson). This is an advanced QA pipeline based on deep linguistic processing and semantic resources. Wat-son uses deep syntactic parsing components and a predicate-argument http://www.answerbag.com/
Figure 1: Kernel-based pair classification/reranking framework structure (PAS) builder [22]. Additionally, it uses several semantic resources, e.g., Wikipedia and PRISMATIC [11] combined in a ma-chine learning-based reranker. The Watson system is very accurate and effective but it requires to hand-craft rules, which is typically very costly. Our approach instead can automatically build syntac-tic/semantic patterns also exploiting LD, which can be used both for matching and generalizing words.

Our baseline model is an updated version of our previous sys-tem developed in [31]. However, as pointed out in the introduction, such model does not encode dependency structure and the seman-tic information from LD. Moreover, our previous several attempts of using semantic information, e.g., Latent Dirichlet Allocation, WordNet, Latent Semantic Analysis, failed to improve the struc-tural model. In contrast, we show that our LD approach can effec-tively encode knowledge improving on passage reranking.
More traditional work in QA using semantics and syntax can be observed in [15, 35]. However, the complexity of the method in [15] along with an obscure fine manual tuning, have made adaption or just replication of such systems rather complex. Recent stud-ies on passage reranking, exploiting structural information, were carried out in [19], whereas other methods explored soft matching (i.e., lexical similarity) based on NE types [1]. [28, 17] applied question and answer classifiers for passage reranking. In this con-text, several approaches focused on reranking the answers to defi-nition/description questions, e.g., [34, 36].

Next, the models developed in [2, 3] demonstrate that linguis-tic structures improve QA but the proposed approaches again are based on handcrafted features and rules. In contrast, our method is based on automatic feature engineering, resulting rather adaptable to different application domains.

Regarding answer sentence rerankers, [42] proposed a proba-bilistic quasi-synchronous grammar, inspired by machine transla-tion, which allows to model Q/AP relations by means of syntac-tic transformations. [41] designed a probabilistic model to learn tree-edit operations on dependency parse trees. [14] employed a computationally expensive tree kernel-based heuristic to identify tree edit sequences which could serve as good features for a logis-tic regression model. [43] further improved the [14] approach by proposing a faster dynamic-programming based algorithm for fea-ture extraction and extending the feature set with WordNet features. [45] proposed a model which, in addition to syntax, incorporates features based on rich lexical semantic knowledge, including syn-onymy, antonymy, hypernymy and semantic similarity, obtained from a number of external systems and resources. In our work, we encode semantic knowledge directly into syntactic tree repre-sentations of Q/AP and use PTK to learn the syntactico-semantic patterns. More recently, the answer selection task was tackled with deep learning, e.g. convolutional deep neural networks [30] and stacked bidirectional Long Short-Term Memory model [40].
Our framework uses the relations between a question (Q) and its answer passage (AP) to rerank passages. The basic schema is dis-played in Figure 1: given a Q we submit it as a query to a search en-gine, which retrieves a set of candidate AP. Each Q/AP text pair is processed by an NLP pipeline which performs basic tokenization, sentence splitting, lemmatization, stopword removal. Various NLP form more involved linguistic analysis, e.g., part-of-speech (POS) tagging, chunking, Named Entity (NE) recognition, constituency and dependency parsing, etc.
 The Q/AP text pair is processed by a Wikipedia link annotator. It automatically recognizes n-grams in plain text, which may be linked to Wikipedia and disambiguates them to Wikipedia URLs. The input text is supposed to be short thus we concatenate the pair members together to provide a larger disambiguation context to the annotator. These annotations are then used to produce computa-tional structures (see Sec. 4) input to pair classifiers. The semantics of such relational structures can be further enriched by adding links between the constituents of the two pieces of text. The relational links can be generated by: (i) matching lemmas as in [31]; (ii) matching the constituent types based on LD as in [37]; (iii) match-ing the question focus type derived by the question classifiers with the type of the target NE as in [31, 32, 29]. The resulting pairs of trees connected by semantic links are then used to train a kernel-based classifier Sec. 7.
In our study, we design and compare several syntactic and se-mantic structural representations of pairs of short texts, including dependency-and shallow chunk-based tree representations.
Shallow chunk-based tree (CH) . Similarly to [31, 32, 29], we represent a pair of short texts as two trees with lemmas at leaf level and their part-of-speech (POS) tags at the preterminal level. Preter-minal POS-tags are grouped into the chunk nodes and the chunks are further grouped into sentences. For example, Figure 2 shows the shallow tree representation of the two Q and AP reported in the introduction.

Dependency-based tree (DT1) [32]. We structure the output of dependency parsers to design a new grammatical relation centered tree. This is a dependency tree altered so that grammatical relations become nodes. Lemmas and their POS tags are allocated at the leaf and the preterminal levels, respectively. Finally, we add  X :: X  and the first letter of the respective POS tags to the leaves, e.g.,  X  X orld::n X . Figure 3 illustrates a DT1 representation of the question from the running example.
 Dependency-phrase based tree (DT2) . We further generalize DT1 with an additional layer of chunk label nodes between the POS and the grammatical relation node layers. Lemmas in the same chunk or in the  X  X bject of preposition X  ( pobj ) or the  X  X ossession modifier X ( poss ) relations are grouped under the same chunk node. Figure 4 provides an example of a DT2 structure.

Lexical-centered dependency tree (DT3) [8]. Finally, we engi-neer DT3 in which dependency relations rel(head,child) are repre-sented by a parent and a child node labeled head::pos and child::pos , respectively (lemmas are specialized with the first letter of their POS tags, i.e., ::pos ). We add the information about the name of the relation, rel , and POS of its child as the rightmost children, with GR and POS tags prepended, respectively. For example, we encode the relation nsubjpass(bill,stadium) by creating a parent-child pair of nodes labeled bill::v and stadium::n and adding chil-dren labeled as GR-nsubjpass and POS-NN to the latter. Figure 5 provides an example of an DT3 structure.
Previous work has shown the importance of encoding informa-tion about relations between question and answer passage into their structural representations, e.g., [2, 31]. Two basic methods to en-rich our proposed structures with relational information are de-scribed hereafter.

Lexical relations (REL). Structural relations in both kinds of trees are encoded using the REL tag, which links the related struc-tures in the two texts. Our previous work [31] used hard string match and soft-matching methods such as WordNet-based or La-tent Dirichlet Allocation-based semantic relatedness metrics to find related lemmas. However, soft matching did not improve the mod-els simply using hard-matching. The POS tag and the chunk pos tags in all representations are marked with REL label. In DT3 we mark the POS and grammar relation nodes. For example, Figure 2 shows that the lemma  X  X orld" occurs in both Q and AP, (we high-lighted this with the solid line box), thus the related POS and chunk nodes are marked with REL.

Question Focus-based relations (FREL). Semantic relations specific to QA can be derived from the question focus and cate-gory. These are encoded using the REL-FOCUS-&lt; QC &gt; &lt; QC &gt; is substituted with a question class in the specific examples. As in [31], we use statistical classifiers to derive focus and cate-gories of the question and of the NEs in the AP. We consider HUM, LOC, ENTY, NUM, ABBR and DESC question classes [20]. Ques-tion focus and AP chunks, which contain NEs of type compati-tags to their label. Figure 2 shows an example of such label in the dotted box, however, note that in this case the statistical classifier has determined the focus incorrectly, as it should be stadium , in-stead of sport . No named entities (NEs) of classes compatible with the question class ENTY are in AP, therefore there are no REL-FOCUS-ENTY tags in it.
Encoding relational information between Q and AP, i.e., links between words or constituents, is essential for improving passage reranking. Our previous work [32, 31, 29] has only defined two table. We use the following mappings: Person, Organization HUM ,ENTY ; Misc ! ENTY ; Location ! LOC ; Date, Time, Money, Percentage, Set, Duration ! NUM basic approaches based on string matching or question classifiers. The methods clearly suffer from (i) coverage problems, i.e., due to word mismatch and (ii) the non-perfect coverage and accuracy of classifiers and named entity recognizers (NER). Additionally, other attempts to use semantic matching, e.g., based on WordNet, have failed. Our approach targets entities defined in LD, it highly in-creases coverage and at the same time avoids errors of classifiers. In this work, we employ robust and accurate entity match algo-rithms we defined in [37].

More specifically, we detect semantic relations between word se-quences in two texts, T ent and T gen . We look for word sequences, e.g., noun phrases, in T ent and T gen , which denote two classes or a class and an entity, which are in a isSubclassOf or isa relation. Here, class is a set of entities, where two classes, C 1 and a subClassOf relation if C 1  X  C 2 . Similarly, an entity C are in isa relation if e 2 C . We call such relation between word sequences a Type Match (TM). For instance, the word sequences stadium and Astrodome in the running Q/AP example are in the TM relation, since stadium denotes a class of all the stadiums, and Astrodome is one particular stadium. We extract knowledge about entities, classes and their relations from the DBpedia, YAGO and WordNet datasets available as LD. They suggest using HTTP Unique Resources Identifiers (URIs) for naming  X  X hings", i.e. classes , entities , relationships , and us-and SPARQL query language for representing and querying knowl-edge. In RDF, knowledge is represented as a set of (subject, predi-http://www.w3.org/DesignIssues/LinkedData. html http://www.w3.org/TR/rdf-concepts/ cate, object) triples forming a directed graph, where subjects/objects are vertices and predicates are edges.

LD publishers reuse a number of vocabularies and ontologies cabulary, which contains basic elements for knowledge description. For example, its predicates rdf:type and rdfs:SubClassOf , denote the isa and subClassOf relationships that we described above. rdf:label predicate is utilized to describe human-readable names of the things uniquely identified by the URIs. For instance, in YAGO, the URIs for the stadium class and the As-trodome entity are yago:wordnet_stadium_104295881 10 and yago:Reliant_Astrodome , respectively, and the triple expressing their relationship is (yago:Reliant_Astrodome, rdf:type, yago:wordnet_ stadium_ 104295881) .

LD published under an open license is called Linked Open Data (LOD). In this work we use three large cross-domain datasets, namely WordNet, DBpedia and YAGO. WordNet is a manually created lexical database which groups synonymous words into synsets, and stores information about their relationships, e.g., hyponymy, hyper-nymy or meronymy. Since WordNet was created and is maintained manually, its coverage of NEs and domain-specific classes is lim-ited.
 YAGO was automatically created by combining WordNet and Wikipedia. YAGO construction algorithm converts WordNet synset hypernymy/hyponymy hierarchy into a class hierarchy and adds leaf Wikipedia categories as leaf classes to it using a heuristic category-to-synset mapping algorithm. The Wikipedia pages which belong to these categories become individuals of the resulting YAGO classes. A set of heuristics is employed to obtain facts about them from Wikipedia pages.

DBpedia was also automatically created using Wikipedia as a primary source of knowledge, and similarly to YAGO, Wikipedia pages are converted to individuals, and a set of heuristics is used to extract knowledge about them from Wikipedia infoboxes and categories. Hierarchy of classes in DBpedia was manually con-structed. The classes are populated with individuals using infobox-class mappings.
 Algorithm 1 Type Match algorithm 1: TM ; 4: for all type 2 getTypes( uri , LD d ) do
In this work, we employ Algorithm 1 to detect token sequences (hereafter denoted as anchors ) in TM relation. The algorithm takes two short texts, T ent and T gen , an LD dataset, LD d , and an empty set, TM , as input. It (i) scans T ent for the anchors that refer to classes or entities in LD d (line 2); (ii) for each anchor, T ent , it detects the URIs of the corresponding entities in 3); (iii) for each uri the algorithm extracts a list of types that gen-nally, it iterates through chunks in T gen and human-readable labels http://www.w3.org/TR/rdf-schema/ yago: is a shorthand for http://yago-knowledge.org/ resource/ of types extracted in the previous step and checks them for string match (line 5-6). If the last tokens 12 in the chunk ch , the last tokens of the human-readable label of a type, type.label extracted for a ent from T ent , we set a ent and a gen to be in TM relation, and add a tuple { ( a ent ,a gen ) } to the TM set.
For instance, if T ent = AP and T gen = Q from our run-ning example and LD d = YAGO , we: (i) scan AP for all the anchors, which may denote YAGO classes or entities and ex-tract their URIs. In our example, the Astrodome token in AP is an anchor with the respective YAGO URI yago:Reliant_ Astrodome . (ii) We extract the URIs and human-readable names of the classes with which the URIs extracted in step (i) are in rdf:type or rdfs:subClassOf relation. For ex-ample, yago:Reliant_Astrodome is in rdf:type relation with the yago:wordnet_stadium_104295881 class with the humane-readable label stadium . (iii) We look for the occur-rences of the human-readable names of the classes extracted in step (ii) in the Q text. In our example, the human-readable la-bel stadium occurs in the question. Therefore, we detect a TM relation between the Astrodome anchor from AP and the stadium anchor from Q, i.e., in line 8 of Algorithm 1 ( a ent ,a gen ( X  Astrodome  X  ,  X  stadium  X ) . We describe the details of the im-plementations of getAnchors , getURIs , getT ypes procedures in the subsection below.
 Detecting anchors and referent entities/classes. We detect the sequences of tokens in text, a ent , which constitute entities and classes in an external knowledge source, i.e., YAGO, DBpedia and WordNet. This is not as simple as a dictionary look-up as the a ent are ambiguous. However, we benefit from the fact that (i) YAGO and DBpedia are aligned with Wikipedia pages on entity-level by construction; and (ii) there are several so-called wikifica-tion algorithms, which find references to Wikipedia pages in plain text, and disambiguate them to correct Wikipedia pages [9, 23]. Thus, we wikify text to both detect about anchors in T ent extract URLs of the Wikipedia pages they refer to. In order to have a richer disambiguation context, we concatenate T ent T gen before passing it to the Wikification tool. We convert the Wikipedia page names to YAGO entities URIs by using YAGO yago:hasWikipediaUrl property. We obtain DBpedia URIs by the Wikipedia page prefix, http://en.wikipedia.org/ wiki/ , with http://dbpedia.org/resource/ . For in-stance, in our running example the wikification tool maps the astrodome token from AP to the http://en.wikipedia. org/wiki/Reliant_Astrodome page and YAGO con-tains a triple (yago:Reliant_Astrodome, yago:hasWikipediaURL,  X  X ttp://en. wikipedia. org/ wiki/ Reliant_Astrodome") .
In case of WordNet, we consider all noun phrases in T ent the anchors and all the WordNet synsets containing them to be their references, i.e., we have more than one reference for an anchor. Extracting generalization classes and their names. In case of YAGO and DBpedia, we employ RDFS predicates rdf:type and rdfs:subClassOf to extract URIs of classes which generalize the entities/classes returned by the getURIs procedure directly or transitively. The URIs are HTTP unique identifiers. We ex-tract their human-readable names by exploiting the rdfs:label property. For example, the output of getT ypes ( uri = yago: Reliant_Astrodome , LD d = YAGO ) includes yago: tions. For example, the human-readable name of one of the YAGO classes of Astrodome in the running example AP is  X  X overed sta-dium". We would miss the match with the  X  X tadium" in Q, if we only looked for the entire class name in it.

Figure 6: Fragments of a CH tree annotated in TM ND mode. wordnet_stadium_104295881 with the human-readable la-bels such as bowl , stadium and arena .

In case of WordNet, generalizations are all the hypernyms of the uri -s output by the getURIs procedure, and their human-readable names are all words belonging to the hypernym synsets.
Encoding relational information in Q/AP is about establishing links between the Q and AP structures, in turn, this is about finding and characterizing matching between text constituents. The previ-ous section has shown powerful methods using LD to establish a match between entities, possibly coming from two different pieces of text, e.g., Q and AP. Considering that in LD instances and classes are defined, we can also define different types of the match occur-ring between entities.
We start from the basic structures in Fig. 2 and we enrich them as the sets of nodes corresponding to the tokens composing two an-chors, a ent and a gen (from two texts T ent and T gen ), respectively. We define the following different types of relational information: (i) Untyped (TM N ) . We add a leaf sibling node labeled TM to the parent of each node in L ent and L gen . (ii) Direction-typed (TM ND ) . By construction, one of the anchors in TM relation, a gen , refers to a class in the LD dataset that con-tains an entity or generalizes a class to which the other anchor, i.e., a ent refers to. We reflect this fact by adding sibling nodes labeled TM-PARENT to the parents of all nodes in L gen , and sibling nodes labeled TM-CHILD to the parents of nodes in L ent . In our run-ning Q/AP example, stadium is the generalization of the anchor Astrodome . Therefore, as Figure 6 shows, we mark stadium as TM-PARENT and Astrodome as TM-CHILD. (iii) Focus-typed (TM NF ) . If one of the anchors is also the ques-tion focus, we add sibling nodes, TM-FOCUS, to the parents of nodes in L ent and L gen . Otherwise, we use TM N . (iv) Combo (TM NDF ) . We apply both TM ND and TM NF strate-gies thus adding two different types of nodes.

TM ND , TM NF , TM NDF are more specific than TM N , there-fore we expect them to provide more discriminative patterns. We use the techniques described above for encoding the TM relations into CH, DT1 and DT2 structures, while in DT3 we add a TM node to matched lexicals as a child. Figure 6 illustrates the TM egy applied to CH.
Section 3 has shown the importance of using REL tags. In our early work [31], these were generated using hard lemma matching. This may result in low coverage as, e.g., it does not capture syn-onyms or different variants of the same name. Wikification tools handle this problem, as typically they have precomputed sets of different variants of names for the same page, extracted from inter-nal Wikipedia links and redirection pages. Therefore, if a pair of anchors in T ent and T gen are annotated with the link to the same type of information. Figure 7: Some fragments generated by PTK or equivalently by page by a wikification tool, we consider them to be matching and we mark them with REL tags in the structural representations of T ent and T gen as described in Sec. 4.1.
The above structures can be used by a domain expert to design machine learning features for training relational classifiers. Given the complexity of this task, we can rely on kernel machines, e.g., SVMs, for automatic feature generation. These classify a test in-put x x x using the function: h ( x x x )= P the model parameters estimated from the training data, y i The latter computes the similarity between two objects. If we use kernel functions, we do not need to represent objects with features and thus we do not need to design features at all. In case of the con-volution tree kernels, K counts the number of common subtrees be-tween two trees T 1 and T 2 without explicitly considering the whole fragment space. The general equations for convolution tree kernels is: TK ( T 1 ,T 2 )= P and N T ( n 1 ,n 2 ) is equal to the number of common fragments rooted in the n 1 and n 2 nodes, according to several possible definitions of the atomic fragments. We use the constituting PTK [24], which defines any possible set of connected nodes as features. These cap-ture dependencies between structure elements, e.g., Fig. 7 shows fragments generated for the question tree of Fig. 2.
The above section has shown how we can build classifiers based on kernels. However, the same framework can be easily adapted for learning to rank problems. In particular, we can design a func-tion deciding which Q/AP pair is more probably correct than the others, where correct Q/AP pairs are formed by an AP contain-ing a correct answer to Q and a supporting justification. For this purpose, we adopt the following kernel for (preference) reranking: P
K ( h o 1 ,o 2 i , h o 0 1 ,o 0 2 i )= K ( o 1 ,o 0 1 )+ K ( o K ( o 2 ,o 0 1 ) . In our case, o i = h Q i ,AP i i and o where Q and AP are the trees defined in the previous section, and K ( o i ,o 0 j )= TK ( Q i ,Q 0 j )+ TK ( AP i ,AP 0 j ) . tree kernel function, e.g., PTK. Finally, we also add ( ~ ~ FV ( o 2 ))  X  ( ~ FV ( o 0 1 ) ~ FV ( o 0 2 )) to P K , where ~ tional feature vector representing Q/AP pairs. This enables the use of standard features.
We evaluated our structural representations on the passage and sentence retrieval subtasks. We investigated the impact of the dif-ferent structures described in this paper on several datasets of dif-ferent nature. Moreover, we compared the impact of various pre-processing annotation pipelines on the final system accuracy. We utilized three different datasets for testing our models: TREC QA 2002/2003. TREC QA tasks provide questions along with the answer keys, which can be used to select the passages containing correct answers where the passages are extracted from a given text corpus. We used a total of 824 questions from years the supporting answer passages.
 TREC13. Factoid open-domain TREC QA corpus prepared by [42]. The training data was assembled from the 1,229 TREC8-12 ques-tions. The answers for the training questions were automatically marked in sentences by applying regular expressions, therefore the dataset can be noisy. The test data contains 100 questions, whose work, we remove the questions that have only correct/only incor-rect candidate APs, thus reducing the dataset to 68 questions. We used 10 answer passages for each question for training our classi-Answerbag. It is a collaboratively constructed question-answer resource. Some of the answers are marked as  X  X rofessionally re-searched X  meaning that they have been professionally edited and fact-checked thus making them high-quality QA training data. We used 2,000 questions for training and 1,000 for testing, using 10 and 50 passages, respectively.
 LD datasets. We used the core RDF distribution of YAGO2 18 , Feature Vectors. We used several similarity functions between the pairs of texts, computed over various input representations to form a feature vector, as described hereafter: Term-overlap features: a cosine similarity over the text pair: sim COS ( T 1 ,T 2 ) are composed of word lemmas, bi-, three-an four-grams, POS-tags. PTK over tree representations: similarity based on the PTK score computed for the structural representations of T 1 and T 2 ( T 1 ,T 2 )= PTK ( T 1 ,T 2 ) , where the input trees can be both the de-pendency trees and/or the shallow chunk trees. Search engine rank-ing score: when experimenting with TREC QA and Answerbag, we also use a ranking score of our search engine assigned to AP. Learning Models. We used SVM-Light-TK 21 to train our models. The toolkit enables the use of structural kernels [24] in SVM-Light [18]. We used default PTK parameters as described in [31] and the polynomial kernel of degree 3 on standard features.
 Pipeline. We built the entire processing pipeline on top of the UIMA framework.We included many off-the-shelf NLP tools wrap-and Stanford CoreNLP [21] tools for sentence detection, tokeniza-tion, POS-tagging and NE recognition; Illinois chunker [27], Stan-ford CoreNLP Lemmatizer, and question class and focus classifiers trained as in [31]. For dependency parsing we used Stanford depen-dency parser (version 2.0.3) and UIMA wrappers provided by the DKPro toolset [10] for the Mate [6] (v3.5), ClearNLP [7] (v2.0.2, http://trec.nist.gov/data/qamain.html https://catalog.ldc.upenn.edu/LDC2002T31 https://code.google.com/p/jacana/ . works experimeting on TREC13 [45, 40, 42], we used the same evaluation setting as described in footnote 7 in [45]. In this setting the gold judgment file contains 4 extra questions not covered by the test set, thus resulting in lowering the upper bound of MAP and MRR to 94.44 instead of 100 http://www.mpi-inf.mpg.de/yago-naga/yago1_ yago2/download/yago2/yago2core_20120109. rdfs.7z http://semanticweb.cs.vu.nl/lod/wn30/ http://dbpedia.org/Downloads39 http://disi.unitn.it/moschitti/Tree-Kernel. htm https://opennlp.apache.org/index.html Table 2: Comparing the impact of the preprocessing pipelines in ontonotes model) and Malt [25] (v1.7.2, linear model) dependency parsers. Moreover, we used annotators for building new sentence representations starting from tools X  annotations. For example, we generated annotations with shallow chunk-based structural presen-tations using POS-tag and chunk annotation output by the previous annotators in the pipeline.
 Search engines. We retrieved answer passages for TREC QA 2002/ 2003 from the AQUAINT corpus and Answerbag answer collec-scoring model. We retrieved 50 passages for each question. Wikification tools . We used the Wikipedia Miner 24 (WM) [23] with their confidences ranging from 0 to 1. To have high coverage with Wikipedia links, we used all the links with confidence exceed-ing 0.2 and 0.05 for WM and ML, respectively.
 QA metrics. We used common QA metrics: Precision at rank 1 (P@1) i.e., the percentage of questions with a correct answer ranked at the first position, and Mean Reciprocal Rank (MRR). We also report the Mean Average Precision (MAP).
 Significance tests. We used paired two-tailed t-test for evaluating the statistical significance of the cross-validation experiments. and  X  indicates the significance levels of 0.05 and 0.1, respectively. resentations enriched with question class, NE and question focus information provided by the statistical classifiers.

Here, BM25 refers to the performance of the Terrier search en-gine, V is a baseline reranker only employing the feature vectors described in Section 8.1. In the following lines, CH, DT1, DT2, DT3 correspond to the eponymous structures described in Section 4. CH+V+REL+FREL and DT1+V+REL+FREL indicate the sys-tems employing, CH and DT1 representations, respectively whereas http://terrier.org/ http://sourceforge.net/projects/ wikipedia-miner/files/wikipedia-miner/ wikipedia-miner_1.1 http://www.machinelinking.com/wp we do not have more decimal digits for such results, we converted them to xx.00. +REL , +FREL indicate that the structures are linked with the REL and FREL approaches (Sec. 4.1). DT3 Q +DT2 A means that ques-tion and answer passage are represented by DT3 and DT2, respec-tively. +V indicates that a feature vector is added to the tree repre-sentation.

In our experiments on TREC 2002/2003, CH+V+FREL performs comparably to DT1+V+FREL and is outperformed by the other de-pendency structures. In our intuition, this new outcome may be due to the performance of the dependency parser employed for prepro-cessing [26].

This intuition motivated us to evaluate the impact of four dif-ferent parsers for building DT3 Q +DT2 A . The results in Table 2 show that ClearNLP parser is outperformed by Mate, Malt and Stanford, where the latter always obtains the best result. Addition-ally, we compared the impact of using sentence splitters, tokenizers and POS-taggers from two basic preprocessing pipelines, Stanford CoreNLP (Stanford pos ) and OpenNLP (OpenNLP pos ), on the final performance of the simpler CH structure. The results show that Stanford pos outperforms OpenNLP pos .
In these experiments, we evaluated the accuracy achieved by the baseline systems enriched with wikiREL (Section 6.2), and TM-relations. First, we ran an evaluation of all possible LD sources combinations with different TM-match knowledge techniques in cross-validation on TREC 2002/2003 and then we ran the resulting best systems on TREC13 and Answerbag.
We tested the impact of LD relations in structures using (i) basic lexical relations, i.e., CH+V+REL, and (ii) the lexical and seman-tic relations, i.e., CH+V+REL+FREL, described in Section 4.1. For these experiments, we (i) performed 5-fold cross-validation on TREC 2002/2003 and (ii) employed the preprocessing pipeline based on OpenNLP sentence splitter, tokenizer, POS tagger and Illinois chunker.
 Tables 3 and 4 report the results using CH+V+REL and CH+V+ REL+FREL, respectively. Since wikiREL improves coverage, we preferred to add it to all TM types of relations. Thus, after the dou-ble line, the tables display the baselines (in bold) enriched ( X + X ) with different models based on TM N , TM ND , TM NF , TM NDF i.e., different TM-encoding strategies (see Section 6). Addition-ally, D , Y , W denote that DBpedia, YAGO and WordNet, respec-tively, were used as the LD dataset, when detecting TM matches. We use + to indicate a combination of several LD datasets. We will reuse such notation in all the following tables. We evaluate statistical significance of the results obtained when using TM as compared to the results reported in the CH+V+REL+wikiREL and CH+V+REL+FREL+wikiREL lines for tables 3 and 4.

The tables show that all the systems exploiting LD knowledge, excluding those using DBpedia only, outperform the strong CH+V +REL and CH+V+REL+FREL baselines. Note that CH+V+REL+ Table 3: Results in 5-fold cross-validation on TREC QA corpus, each wikiREL systems enriched with TM tags perform comparably to CH+V+REL+FREL, i.e., using question and focus classifiers, and in some cases even outperform it. Thus LD models can avoid the use of training data and language/domain specific classifiers.
Additionally, the tables show that we typically obtain better re-sults when using YAGO2 and/or WordNet. In our intuition this is due to the fact that these resources are large-scale, have fine-grained class taxonomy and contain many synonymous labels per class/entity thus allowing us to have a good coverage with TM-links. The DBpedia ontology that we employed in the D experi-ments is more shallow and contains fewer labels for classes, there-fore the amount of discovered TM matches is not always sufficient for increasing performance. YAGO2 provides better coverage for TM relations between entities and their classes, while WordNet WordNet in our experiments in [31] but we employed supersenses whereas in this paper we use hypernymy relations. Moreover, we used a different technique to incorporate semantic match into the tree structures. This allowed our new models to relatively improve the old ones by about 16%, e.g., 37.16 vs. 32.0 in MAP. Next, using different TM-knowledge encoding strategies, i.e., TM N , TM ND , TM NF , TM NDF , results in small performance vari-ations. This means that encoding LD information is basically sim-pler than what we thought. We further tested the impact of the dif-ferent encoding strategies when using the Stanford parser. Table 5 reports results of the structures built by such parser and enriched with LD, according to different TM encoding techniques.
Finally, the last three lines of Tab. 4 show our attempt to en-code LD information in a feature vector, i.e., VL. This refers to the number of TM matches between the Q and AP, for different types of TM. It is basically the unstructured version of our LD models. As it can be seen, V+VL only slightly improves V and experiments Table 4: Results in 5-fold cross-validation on TREC QA corpus, all CH+V+VL+REL+FREL is about 4 points absolute less than the best model using LD in structures, i.e., +TM NDF . This confirms that semantic knowledge requires to be used in syntactic structures.
Comparison with TREC challenge. An approximate (as we used five-fold cross-validation) comparison can be attempted with the results from TREC 2003 for the  X  X est passages tasks X  described in TREC-overview [39]. Thanks to LD our system achieves an ac-curacy (Precision@1) of 36.59, which would allow it to be ranked 3rd in the official evaluation, i.e., higher than MultiText-system (ac-curacy=35.1) and below the systems of LCC and Singapore (68.5 and 41.9, respectively). However, such top two systems used many handcrafted rules, resources and heuristics, which also prevent re-searchers to replicate them. In contrast, we generated features au-tomatically, we do not design rules, and all our technology is al-ready off-the-shelf, except for some missing components that we will make freely available to facilitate replicability of our results.
The first two lines of Table 6 report our chosen baselines whereas the last six lines show the accuracy of the systems that typically ex-hibited the best accuracy on our experiments on TREC QA 2002/2003 corpus. The relative improvement when using LD and Wikipedia is lower. This is likely due to the fact that the textual overlap between questions and answers on Answerbag is higher than that on TREC QA 2002/2003, thus the generalization provided by our structures is less important. Note, that we also experiment with DT1 struc-ture built with OpenNLP pipeline and it is outperformed by the one constructed using Stanford preprocessing tools (see Table 1).
Table 7 reports the accuracy of the baseline CH+V+REL+FREL, along with the combinations with wikiREL and TM knowledge. The results on the TREC13 data further confirm the usefulness of LD-based TM knowledge, which allows us to obtain a statistically significant improvement of around 3.5 points in terms of MRR as compared to the baseline CH+V+REL+FREL. The comparison of our models with the state-of-the-art results, reported by Tab. 8, shows that our kernel-based rerankers outperforms the very recent best model, i.e., Wang and Nyberg (2015), by 2.17% absolute in MRR and 1.31% in MAP. Moreover, when we add LD informa-tion, the improvement in MAP increases to 2.73% absolute.
Computational complexity. The complexity of the type match procedure for a short text pair, T ent and T gen , described in Sec-tion 5.2 is O ( k  X  | T gen |  X  | T ent | ) . | T | is the length of text terms of words. k is a LD-dataset specific constant denoting the maximal amount of generalizations per all uri -s corresponding to a word. For example, in case of WordNet this would be the maximal amount of hypernyms for all the senses of a given word.
Wikipedia annotation time. Wikipedia annotation time depends on the algorithms used to perform wikification, tools and tech-niques for Wikipedia data storage and retrieval, and hardware ca-pacity. In our experiments we used (i) a local installation of Wikipedia web-service optimized for fast text processing, ML, which we ac-cess using REST API. We store preprocessed version of Wikipedia to be used by WM 1.0 in a 5.5.17 MySQL database.

Table 9 provides information about the running time of Wikipedia annotation on two randomly selected subsets of text pairs with dif-ferent length. Size column reports number of Q/AP pairs processed, Avg length reports their average length in words, and the two last columns report the average time required to process one pair in sec-onds (and total time required to process the full corpus in minutes in parentheses). The speed exhibited by the ML service shows that the methods relying on linking to Wikipedia are scalable for large amounts of data given the efficient implementation of the wikifica-tion tool.

Type data extraction time. We use Jena TDB 0.9.0 29 RDF triple store to store local YAGO, WordNet and DBpedia data. Jena 2.00GHz processor, 64-bit operating system, 640gb 2.5" Sata II Hard Drive with 5400 RPM http://jena.apache.org/documentation/tdb/ index.html 12 Intel R Xeon R Processor X5670 processors, with 94GB RAM Table 7: Results of CH+FREL plus the best LD models on TREC13 results reported, for example, in [5]. We store YAGO2 and Word-Net+DBpedia data in separate triple stores.

In our case, extracting all the generalizations of the URI, i.e. sending a set of SPARQL queries to a triple store, took 18.22, 36.28 and 68.59 milliseconds for WordNet, YAGO and DBpedia, respec-tively. In average, we extracted 32 type-type label pairs per anchor from WordNet, 49 from YAGO and 26 from DBpedia.

This paper proposes a study on syntactic structures enriched with semantic information from statistical classifiers and knowledge from LD for passage reranking. In particular, YAGO, DBpedia and Word-Net are used to match constituents from QA pairs. Such matches are used to enrich semantic structures. The experiments with TREC QA and the above models also combining traditional feature vec-tors and the improved relational structures greatly outperform a strong IR baseline, i.e., BM25, by 101%, and previous state-of-the-art reranking models, e.g., up to 16% in MAP. Differently from previous work, our models can effectively use semantic knowledge in statistical learning to rank methods. It should be stressed that our experiments have shown that simply using semantic information as features (even if extracted from a powerful resource as LD) does not significantly improve BM25. It is really necessary to encode semantic features in syntactic structures and then generate syntac-tic/semantic relational patterns between question and answer pas-sage (to be used as features in the reranker).

Our promising results open interesting future directions in de-signing novel semantic structures and using innovative semantic representations in learning algorithms for IR applications. Addi-tionally, jointly using deep neural networks with our approach is an interesting and promising research direction.
 This work has been partially supported by the EC project CogNet, 671625 (H2020-ICT-2014-2, Research and Innovation action) and by an IBM Faculty Award. Many thanks to the Center for Ad-vanced Studies of Trento of IBM Italia for the fruitful discussions on the early draft of this manuscript. under Linux, with 64-bit jdk1.7.0_02 and 1TB SATA 6.0Gb/s 2.5" hard drive with 7200 RPM. [1] E. Aktolga, J. Allan, and D. A. Smith. Passage reranking for [2] M. W. Bilotti, J. L. Elsas, J. Carbonell, and E. Nyberg. Rank [3] M. W. Bilotti and E. Nyberg. Improving text retrieval [4] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, [5] C. Bizer and A. Schultz. The berlin sparql benchmark. [6] B. Bohnet. Top accuracy and fast dependency parsing is not a [7] J. D. Choi and M. Palmer. Getting the most out of [8] D. Croce, A. Moschitti, and R. Basili. Structured lexical [9] A. Csomai and R. Mihalcea. Linking documents to [10] R. E. de Castilho and I. Gurevych. A broad-coverage [11] J. Fan, D. Ferrucci, D. Gondek, and A. Kalyanpur. Prismatic: [12] C. Fellbaum, editor. WordNet: An Electronic Lexical [13] D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek, [14] M. Heilman and N. A. Smith. Tree edit models for [15] A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, and [16] J. Hoffart, F. Suchanek, K. Berberich, and G. Weikum. [17] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar questions [18] T. Joachims. Optimizing search engines using clickthrough [19] B. Katz and J. Lin. Selectively using relations to improve [20] X. Li and D. Roth. Learning question classifiers. In [21] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. [22] M. C. McCord, J. W. Murdock, and B. Boguraev. Deep [23] D. Milne and I. Witten. An open-source toolkit for mining [24] A. Moschitti. Efficient convolution kernels for dependency [25] J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. K X bler, [26] M. Pasca. Open-Domain Question Answering from Large [27] V. Punyakanok and D. Roth. The use of classifiers in [28] F. Radlinski and T. Joachims. Query chains: Learning to rank [29] A. Severyn and A. Moschitti. Automatic feature engineering [30] A. Severyn and A. Moschitti. Learning to rank short text [31] A. Severyn, M. Nicosia, and A. Moschitti. Building [32] A. Severyn, M. Nicosia, and A. Moschitti. Learning [33] J. Shawe-Taylor and N. Cristianini. Kernel Methods for [34] D. Shen and M. Lapata. Using semantic roles to improve [35] S. Small, T. Strzalkowski, T. Liu, S. Ryan, R. Salkin, [36] M. Surdeanu, M. Ciaramita, and H. Zaragoza. Learning to [37] K. Tymoshenko, A. Moschitti, and A. Severyn. Encoding [38] E. M. Voorhees. Overview of the TREC 2001 Question [39] E. M. Voorhees. Overview of the trec 2003 question [40] D. Wang and E. Nyberg. A long short-term memory model [41] M. Wang and C. D. Manning. Probabilistic tree-edit models [42] M. Wang, N. A. Smith, and T. Mitamura. What is the [43] P. C. Xuchen Yao, Benjamin Van Durme and [44] X. Yao, B. Van Durme, and P. Clark. Answer extraction as [45] W.-t. Yih, M.-W. Chang, C. Meek, and A. Pastusiak.
