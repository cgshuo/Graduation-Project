 Today the integration of workflow technology into domains that belong to the natural sciences has recently gained increased interest and become a unifying mechanism for handling scientific data. 
Scientific workflows, while sharing commonalities with business workflows, are typically data-centric as opposed to task-centric business workflows [1]. Several sci-entific WFMSs, such as Kepler [2], ZOO [3], GridDB [4], are architected with a data-centric view of workflows. They can provide a data-centric interactive interface to manipulate workflows more conveniently, and allow users to inspect intermediate results in order to determine the next step of experiments [2], and so on. However, most of current WFMSs do not integrate tightly with data, so they can not satisfy the requirements of data-centric scientific experiments. 
Database technologies seem to be well suited to handle highly complex data man-agements. However, they have been utilized only to a limited extent [5]. To our knowledge, systems advocating a tighter integration of DBMS and WFMS only in-clude ZOO [3], GridDB [4] and [5]. Some technologies such as query optimization, fault-tolerance, view, and data provenance should be also applied to workflows in order to enhance the power of large magnitudes of data managements. For example, view mechanism can increase the degree of workflow automation, and combine dif-ferent experimental results in a view so as to compare them. Data provenance pro-files, tables [6]) generated by scientific workflows in order to make sense of and use them. The importance of data provenance has already been recognized in several scientific workflow projects, such as GridDB [4], Chimera [7], myGRID [8], CMCS [9]. As far as we know, the solutions of determining data provenance in the literature usually involve annotations that comprise of the derivation history of a data product and inversion that generates a  X  X everse X  query to find the origins supplied to derive a data product. Annotations may not scale well for fine-grained data as the complete annotations for the data may outsize the storage space required for the data itself. Inversion seems to be more optimal from a storage perspective since an inverse func-tion or query identifies the provenance for an entire class of data. However, it requires a reverse query to be generated and executed to compute provenance every time the provenance of a data product is required. 
In this paper, we present a scientific workflow framework integrated with object deputy model [10] which increases automation of scientific workflows by means of update propagation that is similar to view update, and can freely combine different experimental results by means of the object deputy algebra. Moreover, this framework can support incompleteness and uncertainty of workflow specifications. Most impor-tant of all, in our framework, it is easy to find sources of data products in terms of bi-directional pointers between the data products and their sources, not only saving a mass of storage space, but also decreasing extra computation cost. 
The remainder of this paper is organized as follows: In section 2, a scientific work-flow framework integrated with object deputy model is introduced. In section 3 we present provenance method in our framework and propose three kinds of schemas for data provenance. Experimental results and analysis are given in section 4. In section 5 we analyze and compare some related work. Finally, we present a summary of our contributions and our future work. Object deputy model [10] can satisfy the requirements stemming from complex, high performance scientific data managements with the concepts of deputy objects and deputy classes. In this section, we adopt the object deputy model to describe the proc-ess of scientific workflow executions. Deputy objects and deputy classes are called derived objects and derived classes in our framework. 2.1 Basic Concepts Scientific workflows consist of scientific data and scientific analysis programs ma-nipulating them. Initial inputs can be described as base scientific objects, intermediate results and outputs can be described as derived scientific objects, and scientific analy-sis programs can be defined as read methods of object attributes. Scientific classes have no definitions of functions. Definition 1. Each scientific object has an identifier and some attributes. The schema of scientific objects with the same attributes is defined as a class C = &lt; O , A &gt;. 1. O is the extent of C , O is one of instances of C . 2. A is the set of attribute definitions of C , ( T a : a ) A , where T a and a respectively Definition 2. A derived scientific object is derived from object(s) or other derived object(s). The latter is called source object(s) of the former. Source objects and derived objects are linked by bi-directional pointers between them. Derived objects have their own persistent identifiers, and can inherit some attributes from their source objects by switching operations without occupying storage space, and can also add their additional attributes. A derived cla ss defines the schema of derived objects with 2. A d A d + is the set of attribute definitions of C d . ues of derived object o d are computed through switching operations that need to read attribute values of source objects. In scientific computing environments, it is not al-lowed to update the inherited attributes, so the write method of these attributes is not defined. Switching operation for the read method of these attributes is defined as defined as According to above definitions, during th e course of each query, attribute values of derived scientific objects inherited from source objects are still computed through switching operations that need to communicate with the underlying information source. However, in scientific computing environments, outputs may be generated by long running scientific analysis programs, and information sources may be remote or values instead of re-generating them on each query. The definition of the read method for the inherited attribute of which value is materialized is changed as follows: 
That is, the inherited attribute values can be directly read from the derived object. Definition 3. Update propagation between scientific objects and their derived scientific objects. 1. If a scientific object o is added into class C , then all of derived scientific classes Based on the object deputy model, we have implemented a database system called TOTEM and designed an object deputy database language which can create various kinds of deputy classes, including SelectionDeputyClass, JoinDeputyClass, Union-DeputyClass, and GroupDeputyClass. 2.2 A High-Energy Physics Example In this section, we use the Atlas High-Energy Physics workflow [4] to explain our framework. This workflow consists of three programs: an event generator ( gen ); fast simulation ( atlfast ); and slower simulation ( atlsim ). Gen is invoked with inputting an integer parameter pmas , and produces an event file. The event file is then used to feed atlfase and atlsim , each simulating a detector X  X  reaction to the event, and producing a simulations are compared finally. The Atlas workflow is shown in figure 1(a). 
We adopt an object deputy database language [11] to set up the Atlas workflow in the following. Instead of encoding the workflow in a procedural script, we encode it with a schema definition language. Three programs: gen, atlfast and atlsim are de-fined as read methods of event , fImas and sImas respectively. 1. Create Class gC (pmas int); 2. Create SelectionDeputyClass evts as (Select gen(pmas) as (event int) from gC); 3. Create SelectionDeputyClass fC as (Select atlfast(event) as (fImas int) from 4. Create SelectionDeputyClass sC as (Select atlsim (event) as (sImas int) from 5. Create JoinDeputyClass compare as (Select fC.fImas, sC.sImas from fC, sC 6. or Select evts fC.fImas, evts  X  sC.sImas from evts 
Class evts derived from class gC stores event files produced by program gen , where parameter event contains the identifiers of the files. Likewise, fC and sC re-spectively store outputs of program altfast and atlsim , where outputs fImas and sImas are described as integer. We use JoinDeputyClass to compare results of two different simulations, atfast and atlsim . We can also achieve the same goal by select operation instead of explicitly deriving class compare. Implementation of the function for com paring mainly depends on cross-class query ( X   X   X )[11]. We can start from an initial object (base object or derived object) to one or more target objects by means of bi-directional pointers between them, and the query path length and the direction are not limited. Therefore workflows are composed by deriving classes in our framework. The internal data structures for Atlas workflows integrated with object deputy model are shown in figure 1(b), in which all pmas values change from 100 to 200. 
Most scientific data is not relational in nature, but the inputs and outputs to work-flows can be still represented as tables. In our framework, they are represented as stored in file format (such as XML), but it can be also represented as an object. More-allows users to specify a schema for data stored in the file system and query these data using a SQL-like language. It also allows executing external programs that process data in file systems. In TOTEM, files are described as one-attribute objects, which mainly record identifiers of the files. Actual files are stored in od_largeobject system table. 
Our framework does not need to materialize the intermediate results and outputs, thus reducing storage overhead and maintenance cost. This can optimize workflow executions using database techniques. However, in scientific environments, the de-rived scientific classes are usually generated by the outputs of possibly long running programs. As we have mentioned above, on each query, inherited attribute values of derived objects are computed through switching operations that need to read attribute values of source objects. In order to avoid re-computing them on each query, it might be best to materialize them. This prevents unnecessary re-execution of programs. The materialized method can be also supported in our framework. We emphasize that final data products must be materialized. 2.3 Workflow Automation and Flexibility Workflow automation is accomplished by insertion of values into the base classes. For example, the workflow is invoked by the following Insert statements. 
Insert into gC values (100) etc. 
Our model can automatically invoke an execution of workflow by means of update propagation according to definition 3. Once a scientific object (for example, gC101 ) is inserted into gC, then derived class evts of gC is checked. Because gC101 satisfies evts. In fact, the value of attribute event of evts101 does not occupy storage space, but shares with gC101 by means of the read method ( gen ) defined in attribute event . Likewise, the insertion of evts101 in evts makes derived objects fC101 and sC101 be added to fC and sC respectively, eventually resulting in addition of derived object compare101 . Thereby each input of class gC corresponds to a separate execution of the workflow. In addition, if base scientific objects need to be revised, then successive stages of the workflow can be automatically invoked and derived scientific objects evt0 , fC0 , and sC0 will be re-computed automatically. The mechanism, which is simi-lar to  X  X ush X  mechanism in materialized view, would increase the degree of WFMS automation. 
Scientific workflows are loosely-defined, that is, the complete structures of work-flows are difficult to determine in advance. Our framework allows creating derived classes for partial known experimental steps. By analyzing intermediate results, users are allowed to determine the next step of experiments by defining a new derived class for the new experimental task. For example, we can only define a derived class evts for the first experimental step. After experimental results generated by program gen are analyzed, we are allowed to define succeeding experimental steps by deriving class fC, sC and compare. Furthermore, in a scientific environment, a specification may change rapidly as the experimental results are analyzed even while a workflow is being executed. We can also dynamically change specifications of workflows by defining new derived classes instead of the old. 
The framework presented above is meant for tight integration with a database. The declaration and definition of workflows are in a SQL-like workflow manipulating language (object deputy database language), and the invocation and query are done in SQL. This will allow most WFMS management operations to be performed in a way analogous to traditional data management operations. In this section, we first present data provenance method of our framework, and argue that our method has the advantage over annotations and inversion. Then, we propose three kinds of schemas that trace data provenance. 3.1 Tracing Data Provenance There are two main approaches to representing provenance information, annotations and inversion. From database perspective, we prefer the latter because inversion mainly uses a data-oriented model of provenance. Provenance can be associated not products [12], including queries and functions defined by users. According to [2], intermediate data products should be also recorded in a scientific workflow system. We first give a definition about data provenance. is, 
In TOTEM, we adopt a system table called od_collate to store relationships be-tween source objects and derived objects. Thereby querying the origins of a data product can be directly switched to its source objects. Scientific analysis programs as read methods of object attributes are stored in od_switching system table. The internal data structures of od_collate and od_switching are shown in figure 2. 
Considering the example in section 2.2, if we want to compute provenance of ob-{&lt; gC0 &gt;, gen , &lt; evt0 &gt;, atlfast }.

Compared with either annotations or inversion, our method has the advantage over them. Firstly, annotations are attached to a data product, describing the derivation history of the data product, so annotations can be larger than the data itself even if the data is coarse-grained. In our method, derivation history of a data product can be directly constructed by bi-directional pointers between the data product and its sources, thus saving a mass of storage space. Secondly, our method shares some simi-larities with inversion, for example, we also require  X  X everse X  query to find the source data supplied to derive the data. However, it is not necessary to compute provenance using inverse queries or inverse functions because we can directly find source data of derived data by bi-directional pointers. This method also effectively avoids some issues about inexistence of inverse functions or inaccuracy of inverse computations. Finally, since source data may be remote or unavailable for some time, the inverse method usually requires storing additional auxiliary information in order to reduce or entirely avoid source accesses. In our method, derived objects can materialize some attribute values from source objects, thus avoiding source accesses. 3.2 Materialize Intermediate Products During tracing data provenance, intermediate data products also need to be queried, so choosing to materialize intermediate data products can help query intermediate results directly without re-computing them. In this section, we propose three kinds of schemas for materializing intermediate data products. 1. Materializing Nothing This schema is to materialize no intermediate data products. During tracing data origins, this schema retrieves all necessary information from source data, and then computes intermediate results. It incurs no extra storage or maintenance cost for in-termediate data products, but l eads to poor tracing performance. 2. Materializing Intermediate Data Products Compared with the first schema, materializing intermediate data products can im-Once users pose tracing provenance of a derived data product in class fC, the inter-mediate data products in class evts can be find at once without computing values by switching to class gC. However, intermediate data products may be large and be usually expensive to maintain, even lots of the intermediate data products may be irrelevant to final data products in which users are interested. Thus, extra storage and maintenance cost for intermediate data products increase. 3. Materializing Partial Intermediate Data Products An alternative way of decreasing storage and maintenance cost is to materialize par-tial intermediate data products, where only contains intermediate data products having derived objects. For example, we assume a part of event files are used to feed pro-which can greatly save storage spaces. Otherwise, in this schema, provenance query for final data products is only related with materialized intermediate data products, hence this schema does not affect tracing query performance. In this section, we will evaluate performance of the three proposed kinds of schemas for tracing query, maintenance cost and storage cost under the same environmental setting. 4.1 Experimental Model and Design We design a simple experiment with the architecture in figure 1(b). For simplicity, we assume there is only two-level selection derived class, where the second-level derived event generator ( gen ), and completely inherits objects in base scientific class gC; the second-level derived class fC stores results of the fast simulation ( atlfast ), and inherits about 40% of objects in evts. Scientific data analysis programs used in the experiment are some simple mathematic functions. In order to simulate the real environments, we assume that gen and atlfast consume 3 seconds and 2 seconds respectively. Three kinds of schemas used to materialize class evts respectively are represented by the symbol 1, 2 and 3. The experiment runs on a Celeron machine which has 2.0 GHz CPU, 256MB main memory, and Linux operating system. 
In our performance analysis we consider several performance metrics. The first is tracing query, where we use the average object tracing time as the metric. The second metric measures update maintenance cost, including the total time for maintaining the derived classes evts and fC. The third metric measures storage cost, including the storage spaces occupied by evts and fC. We adopt two types of operations, either tracing query or update maintenance, and compute the number of operations having been finished successfully during a given period, about half an hour. We assume that each operation only traces or updates an object. 4.2 Experimental Results and Analysis Our experiments compare the performance of three proposed kinds of schemas as base class size increases. We vary the size of base class from 100,000 to 500,000 objects. Figure 3(a) shows the average object tracing time of each schema. Figure 3(b) and figure 3(c) show the maintenance cost and the storage cost in each schema re-spectively. The x-axis represents the number of objects in a base class and the y-axis represents the relevant costs. 
From figure 3(a) we can see that the first schema achieves much lower tracing per-formance than the other two schemas, while the performance is identical for the latter two schemas. The longer the tracing time consumed by the first schema, the more the base class size increases. Because of no materialized intermediate data products in the first schema, in order to get values of the intermediate products, the system has to find their source objects by bi-directional pointers and computes their values, thus con-suming a mass of time. 
From the results in figure 3(b), we observe the third schema achieves the best main tenance performance, and maintenance cost of the second schema is close to that of the first schema when scaling up source class size. We divide into two cases to analyze the reasons. The first is that updating of objects in gC does not cause updating of derived objects in fC. In this case, the second schema requires consuming extra time (t 1 ) to maintain class evts completely materialized. The second is that updating of objects in gC will cause updating of derived objects in fC, all the three kinds of sche mas require maintaining the materialized derived class fC, while maintenance time (t cause the first schema has to find the top-level base objects, and then computes their values, thus consuming a mass of time. At beginning, t 2 is longer than t 1 . As the base schema is close to, even exceeds that of the first schema. 
It is evident that the storage cost of the second schema is highest, while the storage cost of the first schema is lowest in figure 3(c). 
From above analysis, we can know which schema to be adopted mainly depends on the requirements of practical applications. The first schema can be used in applica-tions which store terabytes of scientific data generated by short running programs, third schema can be especially useful when a base scientific class has wide objects but the final data products have only a small fraction. A few of research efforts have already been made to integrate some database tech-nologies into scientific workflow management systems [2,3,4,5]. The first system in this domain is ZOO [3]. In ZOO, the workflow is fully defined as an object-oriented database schema. The relationships between tasks and data are represented by ordi-nary object-oriented relationships. Invocation of workflow is triggered by active rules on these relationships. GridDB [4] and [5] share many similarities with ZOO; for example, workflows in these systems are architected with a data model. However, both [4] and [5] use the simpler relational model, and are mainly focused on the grid environment. In GridDB, the inputs and outputs of programs are modeled as relational tables. Programs and workflows can be represented as typed functions. So users can define programs and the relationships between their inputs and outputs in a schema definition language. Insertion of tuples in input table triggers automatically execution of programs and workflows. However, GridDB requires storing some function memo tables for automation of workflow executions, which will increase storage cost. [5] presents a workflow modeling language that tightly integrates workflow management systems and database management systems. Initial input data and programs are de-scribed as active relational tables, and derived data and programs are described as active views, so workflows are composed by declaring active views. Although this method does not cause extra storage, any section of the workflow must be invoked by issuing a SQL query on the corresponding views or tables, which will decrease auto-mation of workflow executions. 
As a critical component, data provenance has been studied by a lot of scientific workflows and database communities. To our knowledge, annotations and inversion are two main approaches to representing provenance information. At present, most of the workflow management systems more depend on annotations [7,8,9]; Chimera [7] analyzes the virtual data catalog and comes up with an abstract DAG representing the sequence of operations that produce that data. Some provenance systems [8,9] also provide semantic information using RDF and OWL in order to realize interaction of scientific workflows in collaborative environments. However, annotations may occupy a mass of storage spaces and even outsize the storage spaces required for the data itself, even if some systems such as Chimera [7] only record the immediately previous source cost. Inversion seems to be more optimal from a storage perspective especially for a large number of fine-grained data since an inverse function or query identifies the provenance for an entire class of data. Many database communities adopting this method such as [13,14,15,16] provide data-oriented provenance services to users. Any scientific workflow systems that use database queries and functions to model work-flows can apply such techniques. Trio [14] just uses the inverse method [16] to auto-matically determine the source data for tuples created by view queries or user defined functions. However, inversion used in these systems may not be the best way for not all functions has inverse functions [6]. [15] presents a framework for computing the approximate provenance based on weak inversion. The paper does not, however, pro-vide a mechanism for generating the weak inversion. Computing provenance using inverse methods usually requires accessing sources which are inaccessible or time consuming. By storing additional auxiliary information in the warehouse, [16] can compute provenance if a large amount of provenance information are required. In this paper, we have presented a DB-integrated scientific workflow framework which adopts the object deputy model to describe execution of a series of scientific tasks. In particular, the deputy objects, which are similar to view, improve workflow automation greatly and incur no extra storage or maintenance cost. The object deputy approach is superior in resolving automatic update maintenance and supporting dynamic or incomplete specification of workflows. Most important of all, data prove-nance method of this framework can provide much higher performance than annota-widely used in distributed grid applications and workflows, and grid has also become the platform for the creation, processing, and management of experimental data. Hence, scientific experimental environments tuned to an increasingly distributed and service-oriented grid infrastructure. How to seamlessly integrate our framework into the infrastructure is one of our future works. 
