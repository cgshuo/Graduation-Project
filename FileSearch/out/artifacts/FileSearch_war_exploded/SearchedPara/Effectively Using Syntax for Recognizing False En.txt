 { lucyv,arulm } @microsoft.com Recognizing the semantic equivalence of two frag-ments of text is a fundamental component of many applications in natural language processing. Recog-nizing textual entailment, as formulated in the recent whether some text sentence T entails some hypothe-sis sentence H .

The motivation for this formulation was to iso-late and evaluate the application-independent com-ponent of semantic inference shared across many ap-plication areas, reflected in the division of the PAS-CAL RTE dataset into seven distinct tasks: Informa-tion Extraction (IE), Comparable Documents (CD), Reading Comprehension (RC), Machine Translation (MT), Information Retrieval (IR), Question Answer-ing (QA), and Paraphrase Acquisition (PP). The RTE problem as presented in the PASCAL RTE dataset is particularly attractive in that it is a reasonably simple task for human annotators with high inter-annotator agreement (95.1% in one inde-pendent labeling (Bos and Markert, 2005)), but an extremely challenging task for automated systems. The highest accuracy systems on the RTE test set are still much closer in performance to a random baseline accuracy of 50% than to the inter-annotator agreement. For example, two high-accuracy systems are those described in (Tatu and Moldovan, 2005), achieving 60.4% accuracy with no task-specific in-formation, and (Bos and Markert, 2005), which achieves 61.2% task-dependent accuracy, i.e. when able to use the specific task labels as input.
Previous systems for RTE have attempted a wide variety of strategies. Many previous approaches have used a logical form representation of the text and hypothesis sentences, focusing on deriving a proof by which one can infer the hypothesis logical form from the text logical form (Bayer et al., 2005; Bos and Markert, 2005; Raina et al., 2005; Tatu and Moldovan, 2005). These papers often cite that a ma-jor obstacle to accurate theorem proving for the task of textual entailment is the lack of world knowledge, which is frequently difficult and costly to obtain and encode. Attempts have been made to remedy this deficit through various techniques, including model-building (Bos and Markert, 2005) and the addition of semantic axioms (Tatu and Moldovan, 2005).
Our system diverges from previous approaches most strongly by focusing upon false entailments; rather than assuming that a given entailment is false until proven true, we make the opposite assump-tion, and instead focus on applying knowledge-free heuristics that can act locally on a subgraph of syn-tactic dependencies to determine with high confi-dence that the entailment is false. Our approach is inspired by an analysis of the RTE dataset that sug-gested a syntax-based approach should be approxi-mately twice as effective at predicting false entail-ment as true entailment (Vanderwende and Dolan, 2006). The analysis implied that a great deal of syn-tactic information remained unexploited by existing systems, but gave few explicit suggestions on how syntactic information should be applied; this paper provides a starting point for creating the heuristics Similar to most other syntax-based approaches to recognizing textual entailment, we begin by rep-resenting each text and hypothesis sentence pair in logical forms . These logical forms are gener-language parsing and generation (Heidorn, 2000). Our logical form representation may be consid-ered equivalently as a set of triples of the form RELATION ( node i , node j ) , or as a graph of syntac-tic dependencies; we use both terminologies inter-changeably. Our algorithm proceeds as follows: 1. Parse each sentence with the NLPW IN parser, 2. Attempt an alignment of each content node in 3. Using the alignment, apply a set of syntactic Figure 1: Logical form produced by NLPW IN for the sentence  X  X ix hostages in Iraq were freed. X  4. If no syntactic heuristic matches, back off to In addition to the typical syntactic information pro-vided by a dependency parser, the NLPW IN parser provides an extensive number of semantic features obtained from various linguistic resources, creating a rich environment for feature engineering. For ex-ample, Figure 1 (from Dev Ex. #616) illustrates the dependency graph representation we use, demon-strating the stemming, part-of-speech tagging, syn-tactic relationship identification, and semantic fea-ture tagging capabilities of NLPW IN .

We define a content node to be any node whose lemma is not on a small stoplist of common stop words. In addition to content vs. non-content nodes, among content nodes we distinguish between en-tities and nonentities : an entity node is any node classified by the NLPW IN parser as being a proper noun, quantity, or time.

Each of the features of our system were developed from inspection of sentence pairs from the RTE de-velopment data set, and used in the final system only if they improved the system X  X  accuracy on the de-velopment set (or improved F-score if accuracy was unchanged); sentence pairs in the RTE test set were left uninspected and used for testing purposes only. Our syntactic heuristics for recognizing false entail-ment rely heavily on the correct alignment of words and multiword units between the text and hypothesis logical forms. In the notation below, we will con-sider h and t to be nodes in the hypothesis H and Figure 2: Example of synonym, value, and deriva-tional form alignment heuristics, Dev Ex. #767 text T logical forms, respectively. To accomplish the task of node alignment we rely on the following heuristics: 3.1 WordNet synonym match As in (Herrera et al., 2005) and others, we align a node h  X  H to any node t  X  T that has both the same part of speech and belongs to the same synset in WordNet. Our alignment considers mul-tiword units, including compound nouns (e.g., we align  X  X scar X  to  X  X cademy Award X  as in Figure 2), as well as verb-particle constructions such as  X  X et off X  (aligned to  X  X rigger X  in Test Ex. #1983). 3.2 Numeric value match The NLPW IN parser assigns a normalized numeric value feature to each piece of text inferred to cor-respond to a numeric value; this allows us to align  X 6th X  to  X  X ixth X  in Test Ex. #1175. and to align  X  X  dozen X  to  X  X welve X  in Test Ex. #1231. 3.3 Acronym match Many acronyms are recognized using the syn-onym match described above; nonetheless, many acronyms are not yet in WordNet. For these cases we have a specialized acronym match heuristic which aligns pairs of nodes with the following properties: if the lemma for some node h consists only of cap-italized letters (with possible interceding periods), and the letters correspond to the first characters of some multiword lemma for some t  X  T , then we consider h and t to be aligned. This heuristic allows us to align  X  X NDP X  to  X  X nited Nations Develop-ment Programme X  in Dev Ex. #357 and  X  X NC X  to  X  X frican National Congress X  in Test Ex. #1300. 3.4 Derivational form match We would like to align words which have the same root form (or have a synonym with the same root form) and which possess similar semantic meaning, but which may belong to different syntactic cate-gories. We perform this by using a combination of the synonym and derivationally-related form infor-mation contained within WordNet. Explicitly our procedure for constructing the set of derivationally-related forms for a node h is to take the union of all derivationally-related forms of all the synonyms of h (including h itself), i.e.: In addition to the noun/verb derivationally-related forms, we detect adjective/adverb derivationally-related forms that differ only by the suffix  X  X y X .
Unlike the previous alignment heuristics, we do not expect that two nodes aligned via derivationally-related forms will play the same syntactic role in their respective sentences. Thus we consider two nodes aligned in this way to be soft-aligned , and we do not attempt to apply our false entailment recog-nition heuristics to nodes aligned in this way. 3.5 Country adjectival form / demonym match As a special case of derivational form match, we soft-align matches from an explicit list of place  X  X weden X  and  X  X wedish X  in Test Ex. #1576. 3.6 Other heuristics for alignment In addition to these heuristics, we implemented a hy-ponym match heuristic similar to that discussed in (Herrera et al., 2005), and a heuristic based on the string-edit distance of two lemmas; however, these heuristics yielded a decrease in our system X  X  accu-racy on the development set and were thus left out of our final system. The bulk of our system focuses on heuristics for recognizing false entailment. For purposes of no-tation, we define binary functions for the existence of each semantic node feature recognized by NLP-W IN ; e.g., if h is negated, we state that N EG ( h ) = TRUE . Similarly we assign binary functions for the existence of each syntactic relation defined over pairs of nodes. Finally, we define the function ALIGN ( h, t ) to be true if and only if the node h  X  H has been  X  X ard-aligned X  to the node t  X  T using one of the heuristics in Section 3. Other notation is de-fined in the text as it is used. Table 1 summarizes all heuristics used in our final system to recognize false entailment. 4.1 Unaligned entity If some node h has been recognized as an entity (i.e., as a proper noun, quantity, or time) but has not been aligned to any node t , we predict that the entailment is false. For example, we predict that Test Ex. #1863 is false because the entities  X  X uwariya X ,  X 20 miles X , and  X 35 X  in H are unaligned. 4.2 Negation mismatch If any two nodes ( h, t ) are aligned, and one (and only one) of them is negated, we predict that the en-tailment is false. Negation is conveyed by the NEG feature in NLPW IN . This heuristic allows us to pre-dict false entailment in the example  X  X ertussis is not very contagious X  and  X ...pertussis, is a highly conta-gious bacterial infection X  in Test Ex. #1144. 4.3 Modal auxiliary verb mismatch If any two nodes ( h, t ) are aligned, and t is modified by a modal auxiliary verb (e.g, can , might , should , etc.) but h is not similarly modified, we predict that the entailment is false. Modification by a modal aux-iliary verb is conveyed by the MOD feature in NLP-W
IN . This heuristic allows us to predict false en-tailment between the text phrase  X  X ould constitute a threat to democracy X , and the hypothesis phrase  X  X onstitutes a democratic threat X  in Test Ex. #1203. 4.4 Antonym match If two aligned noun nodes ( h 1 , t 1 ) are both subjects or both objects of verb nodes ( h 0 , t 0 ) in their re-spective sentences, i.e., REL ( h 0 , h 1 )  X  REL ( t 0 , t REL  X  { SUBJ , OBJ } , then we check for a verb antonym match between ( h 0 , t 0 ) . We construct the set of verb antonyms using WordNet; we con-sider the antonyms of h 0 to be the union of the antonyms of the first three senses of LEMMA ( h 0 ) or of the nearest antonym-possessing hypernyms if those senses do not themselves have antonyms in WordNet. Explicitly our procedure for constructing the antonym set of a node h 0 is as follows: 1. ANTONYMS ( h 0 ) = {} 2. For each of the first three listed senses s of 3. return ANTONYMS ( h 0 ) In addition to the verb antonyms in WordNet, we detect the prepositional antonym pairs ( before/after , to/from , and over/under ). This heuristic allows us to predict false entailment between  X  X lack holes can lose mass... X  and  X  X lack holes can regain some of their mass... X  in Test Ex. #1445. 4.5 Argument movement For any two aligned verb nodes ( h 1 , t 1 ) , we con-sider each noun child h 2 of h 1 possessing any of Figure 3: Example of object movement signaling false entailment the subject, object, or indirect object relations to h , i.e., there exists REL ( h 1 , h 2 ) such that REL  X  { SUBJ , OBJ , IND } . If there is some node t 2 such that ALIGN ( h 2 , t 2 ) , but REL ( t 1 , t 2 ) 6 = REL ( h 1 we predict that the entailment is false.

As an example, consider Figure 3, representing subgraphs from Dev Ex. #1916: T H Here let ( h 1 , t 1 ) correspond to the aligned verbs with lemma kill , where the object of h 1 has lemma Prime Minister Robert Malval , and the object of t 1 has lemma conference . Since h 2 is aligned to some node t 2 in the text graph, but  X  OBJ ( t 1 , t 2 ) , the sen-tence pair is rejected as a false entailment. 4.6 Superlative mismatch If some adjective node h 1 in the hypothesis is iden-tified as a superlative, check that all of the following conditions are satisfied: 1. h 1 is aligned to some superlative t 1 in the text 2. The noun phrase h 2 modified by h 1 is aligned 3. Any additional modifier t 3 of the noun phrase If any of these conditions are not satisfied, we pre-dict that the entailment is false. This heuristic allows us to predict false entailment in (Dev Ex. #908): Here  X  X argest media and Internet company X  in T fails the reverse subset match (condition 3) to  X  X argest company X  in H . 4.7 Conditional mismatch For any pair of aligned nodes ( h 1 , t 1 ) , if there ex-ists a second pair of aligned nodes ( h 2 , t 2 ) such that the shortest path PATH ( t 1 , t 2 ) in the depen-dency graph T contains the conditional relation, then PATH ( h 1 , h 2 ) must also contain the conditional relation, or else we predict that the entailment is false. For example, consider the following false en-tailment (Dev Ex. #60): T H Here,  X  X exican X  and  X  X ross X  are aligned, and the path between them in the text contains the condi-tional relation, but does not in the hypothesis; thus the entailment is predicted to be false. 4.8 Other heuristics for false entailment In addition to these heuristics, we additionally im-plemented an IS-A mismatch heuristic, which at-tempted to discover when an IS-A relation in the hy-pothesis sentence was not implied by a correspond-ing IS-A relation in the text; however, this heuristic yielded a loss in accuracy on the development set and was therefore not included in our final system. 5.1 Lexical similarity using MindNet In case none of the preceding heuristics for rejec-tion are applicable, we back off to a lexical sim-ilarity model similar to that described in (Glick-man et al., 2005). For every content node h  X  H not already aligned by one of the heuristics in Sec-tion 3, we obtain a similarity score MN ( h, t ) from a similarity database that is constructed automatically (Richardson, 1997). Our similarity function is thus: sim ( h, t ) =
Where the minimum score min is a parameter tuned for maximum accuracy on the development set; min = 0 . 00002 in our final system. We then compute the entailment score:
This approach is identical to that used in (Glick-man et al., 2005), except that we use alignment heuristics and MindNet similarity scores in place of their web-based estimation of lexical entailment probabilities, and we take as our score the geomet-ric mean of the component entailment scores rather than the unnormalized product of probabilities. 5.2 Measuring phrasal similarity using the web The methods discussed so far for alignment are lim-ited to aligning pairs of single words or multiple-word units constituting single syntactic categories; these are insufficient for the problem of detecting more complicated paraphrases. For example, con-sider the following true entailment (Dev Ex. #496): H Here we would like to align the hypothesis phrase  X  X re monotheistic X  to the text phrase  X  X elieve there is only one God X ; unfortunately, single-node align-ment aligns only the nodes with lemma  X  X uslim X . In this section we describe the approach used in our system to approximate phrasal similarity via distrib-utional information obtained using the MSN Search search engine.

We propose a metric for measuring phrasal simi-larity based on a phrasal version of the distributional hypothesis: we propose that a phrase template P h (e.g.  X  x h are monotheistic X ) has high semantic simi-larity to a template P t (e.g.  X  x t believe there is only one God X ), with possible  X  X lot-fillers X  x h and x t , re-spectively, if the overlap of the sets of observed slot-fillers X h  X  X t for those phrase templates is high in some sufficiently large corpus (e.g., the Web).
To measure phrasal similarity we issue the sur-face text form of each candidate phrase template as a query to a web-based search engine, and parse the returned sentences in which the candidate phrase oc-curs to determine the appropriate slot-fillers. For ex-ample, in the above example, we observe the set of slot-fillers X t = { Muslims, Christians, Jews, Saiv-ities, Sikhs, Caodaists, People } , and X h  X  X t = { Muslims, Christians, Jews, Sikhs, People } .
Explicitly, given the text and hypothesis logical forms, our algorithm proceeds as follows to compute the phrasal similarity between all phrase templates in
H and T : 1. For each pair of aligned single node and un-2. Similarly, extract the slot fillers X h for each 3. Calculate paraphrase similarity as a function of We then incorporate paraphrase similarity within the lexical similarity model by allowing, for some un-aligned node h  X  P h , where t  X  P t : Our approach to paraphrase detection is most similar to the TE/ASE algorithm (Szpektor et al., 2004), and bears similarity to both DIRT (Lin and Pantel, 2001) and KnowItAll (Etzioni et al., 2004). The chief difference in our algorithm is that we generate the surface text search strings from the parsed logical forms using the generation capabilities of NLPW IN (Aikawa et al., 2001), and we verify that the syn-tactic relations in each discovered web snippet are isomorphic to those in the original candidate para-phrase template. In this section we present the final results of our sys-tem on the PASCAL RTE-1 test set, and examine our features in an ablation study. The PASCAL RTE-1 development and test sets consist of 567 and 800 ex-amples, respectively, with the test set split equally between true and false examples. 6.1 Results and Performance Comparison on Table 2 displays the accuracy and confidence-of the tasks for both the development and test sets.
Our overall test set accuracy of 62.50% rep-resents a 2.1% absolute improvement over the task-independent system described in (Tatu and Moldovan, 2005), and a 20 . 2% relative improve-ment in accuracy over their system with respect to an uninformed baseline accuracy of 50%.

To compute confidence scores for our judgments, any entailment determined to be false by any heuris-tic was assigned maximum confidence; no attempts were made to distinguish between entailments re-jected by different heuristics. The confidence of all other predictions was calculated as the ab-solute value in the difference between the output score ( H, T ) of the lexical similarity model and the threshold t = 0 . 1285 as tuned for highest accu-racy on our development set. We would expect a higher CWS to result from learning a more appro-priate confidence function; nonetheless our overall Table 2: Summary of accuracies and confidence-weighted scores, by task Alignment Feature Dev Test Synonym Match 0.0106 0.0038 Derivational Form 0.0053 0.0025 Paraphrase 0.0053 0.0000 Lexical Similarity 0.0053 0.0000 Value Match 0.0017 0.0013 Acronym Match 0.0017 0.0013 Adjectival Form 7 0.0000 0.0063 False Entailment Feature Dev Test Negation Mismatch 0.0106 0.0025 Argument Movement 0.0070 0.0250 Conditional Mismatch 0.0053 0.0037 Modal Mismatch 0.0035 0.0013 Superlative Mismatch 0.0035 -0.0025 Entity Mismatch 0.0018 0.0063 Table 3: Feature ablation study; quantity is the ac-curacy loss obtained by removal of single feature test set CWS of 0.6534 is higher than previously-reported task-independent systems (however, the task-dependent system reported in (Raina et al., 2005) achieves a CWS of 0.686). 6.2 Feature analysis Table 3 displays the results of our feature ablation study, analyzing the individual effect of each feature.
Of the seven heuristics used in our final system for node alignment (including lexical similarity and paraphrase detection), our ablation study showed that five were helpful in varying degrees on our test set, but that removal of either MindNet similarity scores or paraphrase detection resulted in no accu-racy loss on the test set.

Of the six false entailment heuristics used in the final system, five resulted in an accuracy improve-ment on the test set (the most effective by far was the  X  X rgument Movement X , resulting in a net gain of 20 correctly-classified false examples); inclusion of the  X  X uperlative Mismatch X  feature resulted in a small net loss of two examples.

We note that our heuristics for false entailment, where applicable, were indeed significantly more ac-curate than our final system as a whole; on the set of examples predicted false by our heuristics we had 71.3% accuracy on the training set (112 correct out of 157 predicted), and 72.9% accuracy on the test set (164 correct out of 225 predicted). In this paper we have presented and analyzed a sys-tem for recognizing textual entailment focused pri-marily on the recognition of false entailment, and demonstrated higher performance than achieved by previous approaches on the widely-used PASCAL RTE test set. Our system achieves state-of-the-art performance despite not exploiting a wide ar-ray of sources of knowledge used by other high-performance systems; we submit that the perfor-mance of our system demonstrates the unexploited potential in features designed specifically for the recognition of false entailment.
 Acknowledgments We thank Chris Brockett, Michael Gamon, Gary Kacmarick, and Chris Quirk for helpful discussion. Also, thanks to Robert Ragno for assistance with the MSN Search API. Rion Snow is supported by an NDSEG Fellowship sponsored by the DOD and AFOSR.

