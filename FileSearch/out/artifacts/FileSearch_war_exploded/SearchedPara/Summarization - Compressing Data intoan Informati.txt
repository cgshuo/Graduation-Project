 Summarization is a key data mining concept which involves techniques for find-ing a compact description of a dataset. Simple summarization methods such as tabulating the mean and standard deviations are often applied for data analy-sis, data visualization and automated report generation. Clustering [13, 23] is another data mining technique that is often used to summarize large datasets. For example, centroids of document clusters derived for a collection of text doc-uments [21] can provide a good indication of the topics being covered in the collection. The clustering based approach is effective in domains where the fea-tures are continuous or asymmetric binary [23, 10], and hence cluster centroids are a meaningful description of the clusters. However, if the data has categorical attributes, then the standard methods for computing a cluster centroid are not One such application is in the analysis of netflow data to detect cyber attacks. record has different features such as the IPs and ports involved, packets and bytes transferred (see Table 1). An important characteristic of netflow data is that it has a mix of categorical and continuous features. The volume of netflow data which a network analyst has to monitor is huge. For example, on a typical day at the University of Minnesota, more than one million flows are collected in every 10 minute window. Manual monitoring of this data is impossible and motivates the need for data mining techniques. Anomaly detection systems [9, 17, 4, 22] can be used to score these flows, and the analyst typically looks at only the most anomalous flows to identify attacks or other undesirable behavior. In a typical window of data being analyzed, there are often several hundreds or thousands of highly ranked flows that require the analyst X  X  attention. But due to the limited time available, analysts look at only the first few pages of results that cover the top few dozen most anomalous flows. If many of these most anomalous flows can be summarized into a small representation, then the analyst can analyze a much larger set of anomalies than is otherwise possible. For example, Table 2 shows 17 flows which were ranked as most suspicious by the MINDS Anomaly Detection Module [9] for the network traffic analyzed on January 26, 2003 (48 hours after the Slammer Worm hit the Internet) for a 10 minute window that contained 1.8 million flows. These flows are involved in three anomalous activities -slammer worm related traffic on port 1434, flows associated with a half-life game server on port 27016 and ping scans of the inside network by an external host on port 2048. If the dataset shown in Table 2 can be automatically summarized into the form shown in Table 3 (the last column has been removed since all the transactions contained the same value for it in Table 2), then the analyst can look at only 3 lines to get a sense of what is happening in 17 flows. Table 3 shows the output summary for this dataset generated by an application of our proposed scheme. We see that every flow is represented in the summary. The first summary S 1 represents flows { T 1 -T 10 , T 14 -T 16 } which correspond to the slammer worm traffic coming from a single external host and targeting several internal hosts. The second summary S 2 represents flows { T 12 , T 13 } which are the connections made to half-life game servers made by an internal host. The third summary, S 3 represents flows { T 11 , T 17 } which correspond to a ping scan by the external host. In general, such summarization has the potential to reduce the size of the data by several orders of magnitude.
 categorical features. We view summarization as a transformation from a given dataset to a smaller set of individual summaries with an objective of retaining the maximum information content. A fundamental requirement is that every data item should be represented in the summary . Our contributions in this paper are as follows  X   X  We formulate the problem of summarization of transactions that contain cate-gorical data, as a dual-optimization problem and characterize a good summary using two metrics  X  compaction gain and information loss . Compaction gain signifies the amount of reduction done in the transformation from the actual data to a summary. Information loss is defined as the total amount of infor-mation missing over all original data transactions in the summary.  X  We investigate two approaches to address this problem. The first approach is an adaptation of clustering and the second approach makes use of frequent itemsets from the association analysis domain [3].  X  We present an optimal but computationally infeasible algorithm to generate the best summary for a set of transactions in terms of the proposed metrics.
We also present a computationally feasible heuristic-based algorithm and in-vestigate different heuristics which can be used to generate an approximately good summary for a given set of transactions.  X  We illustrate one application of summarization in the field of network data where we show how our technique can be effectively used to summarize network traffic into a compact but meaningful representation. Specifically, we evaluate our proposed algorithms on the 1998 DARPA Off-line Intrusion Detection Evaluation data [15] and network data generated by SKAION Corp for the
ARDA information assurance program [1]. Summarization can be viewed as compressing a given set of transactions into a smaller set of patterns while retaining the maximum possible information. A trivial summary for a set of transactions would be itself. The information loss here is zero but there is no compaction. Another trivial summary would be the empty set  X  , which represents all the transactions. In this case the gain in compaction is maximum but the summary has no information content. A good summary is one which is small but still retains enough information about the data as a whole and also for each transaction.
 associated weight vector W such that each W i  X  W represents the weight of the feature F i  X  F . A set of transactions T , such that | T | = m , is defined using these features, and each T i  X  T has a specific value for each of the n features. Formally, a summary of a set of transactions can be defined as follows: Definition 1. ( Summary ) A summary S of a set of transactions T , is a set of individual summaries { S 1 , S 2 , . . . , S l } such that (i) each S j represents a subset of T and (ii) every transaction T i  X  T is represented by at least one S j  X  S . Each individual summary S j essentially covers a set of transactions. In the sum-mary S , these transactions are replaced by the individual summary that covers them. As we mentioned before, computing the centroid for data with categor-ical attributes is not possible. For such data, a feature-wise intersection of all transactions is a more appropriate description of an individual summary. Hence, from now on, an individual summary will be treated as a feature-wise inter-S data given in Table 4. The dataset shown is a set of 8 transactions that are described by 6 categorical features and 2 continuous features (see Table 1). Let all the features have equal weight of 1 8 . One summary for this dataset is shown in Table 5 as a set of 3 individual summaries. The individual summary S 1 covers only one transaction, T 8 .
 following metrics -Definition 2. ( Compaction Gain for a Summary ) Compaction Gain = m l . (Recall that m = | T | and l = | S | .) For the dataset in Table 4 and the summary in Table 5, Compaction Gain for S Definition 3. ( Information Loss for a transaction represented by an individual summary ) For a given transaction T i  X  T and an individual sum-mary S j  X  S that covers T i , loss ij = and 0 otherwise.
 The loss incurred if a transaction is represented by an individual summary will be the weighted sum of all features that are absent in the individual summary. Definition 4. ( Best Individual Summary for a transaction ) For a given transaction T i  X  T , a best individual summary S j  X  S is the one for which loss ij is minimum.
 The total information loss for a summary is the aggregate of the information lost for every transaction with respect to its best individual summary.
 T -T 4 are best covered by individual summary S 1 and each has an information loss of 4 8 . Transactions T 5 -T 7 are best covered by individual summary S 2 and each has an information loss of 5 8 . T 8 is represented by S 1 and S 3 . For T 8 and S , information loss = 4  X  1 8 = 1 2 , since there are 4 features absent in S 1 . For T 8 and S 3 , information loss = 0 since there are no features absent in S 3 . Hence the best individual summary for T 8 will be S 3 . Thus, we get that Information Loss for S = 4 8  X  4 + 5 8  X  3 + 0 = 31 8 = 3 . 875. loss , follow an optimality tradeoff curve as shown in Figure 1 such that increasing the compaction results in increase of information loss. We denote this curve as ICC ( Information-loss Compression-gain Characteristic ) curve.
 algorithm. The beginning and the end of the curve are fixed by the two trivial solutions discussed earlier. For any summarization algorithm, it is desirable that the area under its ICC curve be minimal. It can be observed that getting an optimal curve as shown in Figure 1 involves searching for a solution in exponential space and hence not feasible. But a good algorithm should be close enough to the optimal curve like 1 and not like 2 in the figure shown.
 problem since it involves two orthogonal objective functions. So a typical objec-tive of a summarization algorithm would be -for a given level of compaction find a summary with the lowest possible information loss. In this section, we present a direct application of clustering to obtain a summary for a given set of transactions with categorical attributes. This simple algorithm involves clustering of the data using any standard clustering algorithm and then replacing each cluster with a representation as described earlier using feature-wise intersection of all transactions in that cluster. The weights W are used to calculate the distance between two data transactions in the clustering algorithm. Thus, if  X  C is a set of clusters obtained from a set of transactions T by clustering, then each cluster produces an individual summary which is essentially the set of feature-value pairs which are present in all transactions in that cluster. The number of clusters here determine the compaction gain for the summary. while step 3 and 4 generate the summary description for each of the individ-ual clusters. For illustration consider again the sample dataset of 8 transac-tions in Table 4. Let clustering generate two clusters for this dataset  X  C 1 = { T ing the clustering based algorithm.
 of behavior in the data because they are captured well by the clusters. However, this approach performs poorly when the data has outliers and less frequent pat-terns. This happens because the outlying transactions are forced to belong to some cluster. If a cluster has even a single transaction which is different from other cluster members, it degrades the description of the cluster in the summary. For example, let us assume that another transaction T 9 as shown in Table 7 is added to the dataset shown in Table 4 and clustering assigns it to cluster C 1 . presence of this outlying transaction makes the summary description very lossy in terms of information content. Thus this approach represents outliers very poorly, which is not desirable in applications such as network intrusion detection and fraud detection where such outliers can be of special interest. In this section we propose an exhaustive search algorithm (shown in Figure 3) which is guaranteed to generate an optimal summary of given size l for a given set of transactions, T . The first step of this algorithm involves generating the powerset of T (= all possible subsets of T ), denoted by C . The size of C will be to select a subset, S which has following properties Property 1. (1) | S | = l , the size of this subset is equal to desired compaction level (2) The subset S covers all transactions in T (a set cover of T ) (3) The total information loss for S with respect to T is minimum over all other We denote the optimal summary generated by the algorithm in Figure 3 by S . The optimal algorithm follows the optimal ICC curve as shown in Figure 1. (for 4 transactions it would require searching a set of 65,536 subsets), which makes it computationally infeasible even for very small data sets. In this section we propose a methodology which simplifies each of the two steps of the optimal algorithm to make them computationally more efficient. We first present the fol-lowing lemma.
 Lemma 1. Any subset of T belonging to the optimal summary, S must belong to C c , where C c denotes a set containing all closed frequent itemsets 2 generated with a support threshold of 2 and T itself.
 Proof. This can be easily proved by contradiction. Suppose the optimal sum-mary contains a subset S i  X  C -C c . Thus there will be a subset S j  X  C c which  X  X loses X  S i , which means S j  X  S i and support ( S i ) = support ( S j ). We can replace S i with S j in S to obtain another summary, S 0 such that all the transactions represented by S i in S are represented by S j in S 0 . Since S j  X  S i , the infor-mation loss for these transactions will be lower in S 0 . Thus S 0 will have lower information loss than S for the same compaction which is not possible since S is an optimal summary.
 C . The result from Lemma 1 ensures that we can still obtain the optimal sum-mary from the reduced candidate set. But to obtain an optimal solution we still need to search from the powerset of C c , which is still computationally infeasible. Higher values of the support threshold can be used to further prune the number of possible candidates, but this can impact the quality of the summaries obtained. the exponential search by greedily searching for a good solution. This does not guarantee the optimal summary but tries to follow the optimal ICC curve (refer to Figure 1). The output (a subset of C c ) of Step 2 of our proposed algorithms satisfiy Property 1.1 and Property 1.2 mentioned in Section 4 but is not guar-anteed to satisfy Property 1.3.
 paction level while the information loss associated with this subset is approxi-mately minimal can be approached in two ways.  X  The first approach works in a top-down fashion where every transaction be-longing to T selects a  X  X est X  candidate for itself (based on a heuristic based function which will be described in later in this section ). The union of all such candidates is the summary for T .  X  The second approach works in a bottom-up fashion by starting with T as the initial summary and choosing a  X  X est X  candidate at every step and adding it to the summary. The individual summaries that are covered by the chosen candidates are replaced, thereby causing compaction.
 In this paper we will discuss only the bottom-up approach for summarization. An algorithm based on the top-down approach is presented in an extended tech-nical report [8]. Both of these approaches build a summary in an iterative and incremental fashion, starting from the original set of transactions T as the sum-mary. Thus from the ICC curve perspective, they start at the left hand corner ( compaction=1,loss=0 ). At each iteration the compaction gain increases along with the information loss. Each iteration makes the current summary smaller by bringing in one or more candidates into the current summary. The main idea behind the BUS algorithm is to incrementally select best can-didates from the candidate set such that at each step, for a certain gain in compaction, minimum information loss is incurred. The definition of a  X  X est X  candidate is based on a heuristic decision and can be defined in several different ways as we will describe later.
 closed frequent itemsets of T . As mentioned earlier, choosing a support threshold of 2 transactions while generating the frequent itemsets as well as the transac-tions themselves ensures that we capture patterns of every possible size. The rest of the algorithm works in an iterative mode until a summary of desired compaction level l is obtained. In each iteration a candidate from C c is chosen using the routine select best . We have investigated several heuristic versions of select best which will be described later. The general underlying principle for de-signing a select best routine is to ensure that the selected candidate incurs very low information loss while reducing the size of summary. After choosing a best candidate, C best , all individual summaries in current summary which are com-pletely covered 3 by C best are removed and C best is added to the summary (let -1, where size ( C best ) denotes the number of individual summaries in S c com-pletely covered by C best . This is denoted by gain ( C i , S c ) and represents the compaction gain achieved by choosing candidate C i for a given summary S c . replacing the summaries covered by C best ), we need to consider the transactions which will consider C best as their best individual summary (refer to Definition 4) in the new summary. For all such transactions, the difference in the loss when they were represented in S c by their best individual summaries and the loss when they are represented by C best in S 0 c is the extra loss incurred in choosing C best . This is denoted by loss ( C i , S c ).
 designed. Each of these approaches make a greedy choice to choose a candidate from the current candidate set C c which would lead to a locally optimal solution but does not ensure that the sequence of these choices will lead to a globally optimal solution . defined above, to score the candidates and choose one to be added to the cur-rent summary, S c . All candidates with gain ( C i , S c ) equal to or less than 1 are ignored (since adding them to the current summary would not result in any com-paction gain in the new summary). The remaining candidates are ordered using the candidate with highest gain ( C i , S c ) is returned as the best candidate . iteration by bringing in the candidates with lowest information loss. might eventually result in a sub-optimal solution. A simple counter-example shown in Figure 6 proves this. Let us consider a simple data set which has four has a unit weight). The aim is to obtain a summary of size 2 for this data set using the BUS algorithm. As shown in the figure, the candidate set C c contains 9 possible candidates. The BUS algorithm considers the transaction set, T as the initial summary, S c . The first iteration uses method 1 and (see Figure 7) chooses candidate C 5 as the best candidate, generating a new S c . The candidate set is rescored as shown in the figure. The next iteration chooses (see Figure 7) C 9 as the best candidate and reduces the size of S c to 2. The right side table in Figure 7 shows another size 2 summary for the same data set which has a smaller information loss. This shows that this method to score candidates might not result in an optimal solution. This method (as shown in Figure 9) also uses the quantities gain ( C i , S c ) and equal to or less than 1 are ignored (since adding them to the current summary would not result in any compaction gain in the new summary). The remaining candidates are ordered using gain ( C i , S c ). From among the candidates which turned as the best candidate .
 symmetry between information loss and compaction gain . The general idea be-hind method 2 is that at any iteration, the candidates which cover least number of individual summaries in the current summary S c , will also incur the lowest possible loss. Similar to method 1, this method for choosing the best candidate
T
T
T
T does not guarantee a globally optimal summary. Consider the transaction data set containing 6 transactions as shown in Figure 10. Figures 11-13 show the working of the BUS algorithm using method 2 to obtain a summary of size 3 for this data set. Figure 13( right ) shows an alternative summary S 0 c , which is of same size as S c but has a lower information loss.

T
T
T
T
T
T The third method to determine the best candidate makes use of a parameter k , which combines the two quantities gain ( C i , S c ) and loss ( C i , S c ) to obtain a single value, denoted by score ( C i , S c ). The method is shown in Figure 14. The first step calculates the score for each candidate using k s . The candidate with highest score is chosen as the best candidate. Initially k s is chosen to be 0. This favors the candidates with very small information loss. If all candidates with the highest score have size ( C i , S c ) &lt; = 1, then k s is incremented by a small value,  X  . This allows larger candidates to have higher score by offsetting the larger information loss associated with them, and thus be considered for selection into the summary.
 result in selection of larger candidates initially. Using this method again does not guarantee an optimal solution and depends on the initial value of k s and  X  . for each candidate after each iteration. The method 4, shown in Figure 15 does not require these computations but makes use of the quantity feature loss ( C i ) which refers to the weighted sum of the features missing in a candidate. Note that this quantity does not change over the iterations and hence is computed only once. The method to determine the best candidate uses parameter  X  , which defines a upper threshold on feature loss ( C i ). Only those candidates which have feature loss ( C i ) less than or equal to this threshold are considered for selection. Out of these the candidate with largest value of gain ( C i , S c ) ( &gt; 1) is selected as the best candidate. Initially  X  is chosen as 0. Thus only candidates with 0 fea-tures missing are considered. If all candidates which fall under  X  threshold have gain ( C i , S c )  X  1, then  X  is incremented by a small value,  X  . This allows larger candidates to be considered for selection into the summary. All the four methods discussed above select a candidate which they consider is the best with respect to a heuristic. We presented examples for first two methods where this choice might not always lead to a globally optimal solution . The last two methods make use of a user defined parameter and a wrong choice of this parameter can lead to suboptimal solutions. We have investigated each of these approaches and evaluated them on different network data sets (described in next section) and observed that all of them perform comparably in terms of the ICC curve characteristics with respect to each other. In this section we present the performance of our proposed algorithms on network data. We compare the performance of BUS with the clustering based approach to show that it performs better in terms of achieving lower information loss for a given degree of compaction. As we had mentioned earlier, the clustering based algorithm captures the clusters in the data and summarizes the transactions belonging to those clusters well. But the presence of infrequent patterns can ruin the cluster descriptions and result in high information loss. The experimental results presented in this section highlight this fact by choosing different data sets which have different characteristics in terms of the natural clustering in the data. We also illustrate the summaries obtained for different algorithms to make a qualitative comparison between them. The algorithms were implemented in GNU-C++ and were run on the Linux platform on a 4-processor intel -i686 machine. We ran our experiments on four different artificial datasets as listed in Table 8. The first two data sets, AD 1 and AD 2 were artificially generated such that they contained 5 clusters such that each cluster contained the same transaction replicated 200 times. AD 1 contained only these pure clusters. AD 2 contained outliers injected with respect to each of the cluster. The SKAION and DARPA data sets were generated by DARPA [15] and SKAION corporation [1] respec-tively, for the evaluation of intrusion detection systems. The DARPA dataset is publicly available and has been used extensively in the data mining community as it was used in KDD Cup 1999. The SKAION data was developed as a part of the ARDA funded program on information assurance and is available only to the investigators involved in the program. Both these datasets have a mixture of normal and attack traffic. The DARPA data set was a subset of the week 4, Friday, training data containing only attack related traffic corresponding to the following attacks -warezclient, rootkit, ffb, ipsweep, loadmodule and multihop . ution. We measure the distribution of the data using the lof (local outlier factor) score (see [6]). The distribution for lof scores for AD 1 data set is shown in Figure 16(a). Since all transactions belong to one of the 5 clusters, all transactions have a lof score of 1. Similar plot for data set AD 2 in Figure 17(a) shows that some of the transactions have a higher lof score since they are outliers with respect to the clusters. Figure 18(a) gives the distribution of the lof (local outlier fac-tor) score (see [6]) for the transactions in the SKAION dataset. The lof score reflects the outlierness of a transaction with respect to its nearest neighbors. The transactions which belong to tight clusters tend to have low lof scores while outliers have high lof scores. For the SKAION dataset we observe that there are a lot of transactions which have high outlier scores. The lof distribution for the DARPA dataset in Figure 18(e) shows that most of the transactions belong to tight clusters, and only a few transactions are outliers.
 We ran the clustering based algorithm by first generating clusters of different sizes using the CLUTO hierarchical clustering package [14]. For finding the sim-ilarity between transactions, the features were weighted as per the scheme used for evaluating the information loss incurred by a summary. We then summarized the clusters as explained in Section 3. For BUS, we present the results using frequent itemsets generated by the apriori algorithm with a support threshold of 2 as the candidates. The BUS algorithm was executed using method 1 (see in Table 9. These weights reflect the typical relative importance given to the different features by network analysts. The continuous attributes in the data were discretized using equal depth binning technique with a fixed number of in-tervals (= 75) and then used as categorical attributes. Figures 16(b) and 17(b) show the ICC curves for the clustering-based algorithm and BUS on data sets AD 1 and AD 2 respectively. Since AD 1 contains 5 pure clusters, both schemes show no information loss till the compaction gain is 200 (summary size = 5). For compaction more than 200, the information loss increases sharply, since the 5 clusters were chosen to be distinct from each other. Hence no larger summary could be found which could merge any two clusters efficiently. In the second data set AD 2, there are 100 outlying transactions. The figure shows that the perfor-mance of the clustering based approach degrades rapidly as the compaction gain is increased. This happens because some of the outlying transactions are forced to belong to the natural clusters, which makes the cluster description very lossy, and hence incurs a large information loss for all members of that cluster. rithm and BUS on the DARPA and SKAION data sets respectively. From the two graphs we can see that BUS performs better than the clustering-based ap-proach. We also observe that the difference in the curves for each case reflects the lof score distribution for each dataset. In the SKAION dataset there are a lot of outliers which are represented poorly by the clustering-based approach while BUS handles them better. Hence the difference in the information loss is very high. In the DARPA dataset, most of the transactions belong to well-defined clusters which are represented equally well by both the algorithms. Thus, the difference in information loss for the two algorithms is not very high in this case. quent patterns and outliers in the data, we plot the information loss for transac-tions which have lost a lot of information in the summary. Figure 18(c) shows the difference in the ICC curves for the transactions in the DARPA dataset which have lost more than 70% information. The graph shows that for BUS, none of the transactions lose more than 70% information till a compaction gain of about 220, while for the clustering based approach, there are considerable number of transactions which are very poorly represented even for a compaction gain of 50. A similar result for the SKAION dataset in Figure 18(g) shows that BUS generates summaries in which very few transactions have a high loss, which is not true in the case of the clustering based approach.
 the transactions which have lost less than 70% of information for the DARPA dataset. This plot illustrates the difference in behavior of the two algorithms in terms of summarizing the transactions which belong to some frequent pattern in the data. The clustering based approach represents these transactions better than BUS. A similar result can be seen for the SKAION dataset in Figure 18(h). 7.3. Qualitative Analysis of Summaries
In this section we illustrate the summaries obtained by running the clustering based algorithm (see Table 10), and BUS using frequent itemsets (see Table 11) on the DARPA dataset described above. This dataset is comprised of different attacks launched on the internal network by several external machines. The tables do not contain all the features due to the lack of space. However, the information loss was computed using all the features shown in Table 9.

S 1 and S 3 correspond to the icmp and udp traffic in the data. Summaries S 2 ,
S 4 and S 6 represent the ftp traffic on port 20, corresponding to the warezclient, loadmodule and ffb attacks which involve illegal ftp transfers. S 5 represents traffic on port 23 which correspond to the rootkit and multihop attacks. The rest of the summaries, S 7 -S 10 , do not have enough information as most of the features are missing. These cover most of the infrequent patterns and the outliers which were ignored by the clustering algorithm. Thus we see that the clustering based algorithm manages to bring out only the frequent patterns in the data. The summary obtained from BUS gives a much better representation of the data.
Almost all the summaries in this case contain one of the IPs (which have high weights), which is not true for the output of the clustering-based algorithm.
Summaries S 1 and S 2 represent the ffb and loadmodule attacks since they are launched by the same source IP. The warezclient attack on port 21 is represented by S 3 . The ipsweep attack, which is essentially a single external machine scanning a lot of internal machines on different ports, is summarized in S 6 . S 5 summarizes the connections which correspond to internal machines which replied to this scanner. The real advantage of this scheme can be seen if we observe summary S 9 which is essentially a single transaction. In the data, this is the only connection between these two machines and corresponds to the rootkit attack. The BUS algorithm preserves this outlier even for such a small summary because there is no other pattern which covers it without losing too much information. Similarly, S 10 represents 5 transactions which are icmp replies to an external scanner by 5 internal machines. Note that these replies were not merged with the summary S 5 but were represented as such. Thus, we see that summaries generated by BUS algorithm represent the frequent as well as infrequent patterns in the data. Compression techniques such as zip, mp3, mpeg etc. also aim at reduction in data size. But compression techniques are motivated by system constraints such as processor speed, bandwidth and disk space. Compression schemes try to reduce the size of the data for efficient storage, processing or data transfer. Summa-rization, on the other hand, aims at providing an overview of the data, thereby allowing an analyst to get an idea about the data without actually having to analyze the entire data.
 tion of frequent itemsets [2, 20, 11, 19, 7, 5]. However, their final objective is to approximate a collection of frequent itemsets with a smaller subset, which is different from the problem addressed in this paper, in which we try to represent a collection of transactions with a smaller summary.
 nity, and has been addressed mostly as a natural language processing problem which involves semantic knowledge and is different from the problem of summa-rization of transaction data addressed in this paper. Another form of summa-rization is addressed in [12] and [16], where the authors aim at organizing and summarizing individual rules for better visualization while not addressing the issue of summarizing the data.
 and Karypis [24]. This paper proposes an algorithm (SUMMARY) to find a set of frequent itemsets (based on a support threshold), which is called a summary-set , for a given set of transactions. The summary-set is found by determining the longest frequent itemset which covers a transaction, for each transaction and then taking union of all such longest frequent itemsets. The summary-set is then used to determine clusters for the given data set by treating each member of the summary-set as a cluster such that all transactions which considered that member as their longest representation belong to the same cluster. The authors claim that the summary-set determined using the SUMMARY algorithm is a good summary of the entire data set. Indeed this method can be viewed as one instance of the top-down approach for computing summaries. However there are several shortcomings associated with it as discussed below  X  The summary-set does not guarantee to cover every transaction belonging to the data set. Outlying transactions which do not match with any other trans-action on any feature will not exist in any frequent itemset (even if the support threshold is 2 transactions). Such transactions will not have any representa-tive in the summary-set and will be completely lost. This would be highly undesirable in applications where outliers are of great significance to analysts.
As mentioned in the introduction, every transaction has to have some form of representation in the final summary. We must note that the real objective of the summary-set algorithm (as stated in the paper) is to find clusters.  X  This method does not try to explicitly trade-off compaction gain for infor-mation loss. The compaction gain is very indirectly controlled by the support threshold parameter. Thus to reduce the size of summary-set , the support threshold can be increased. But this would result in more and more infrequent transactions getting completely lost. The two schemes presented for summarizing transaction datasets with categor-ical attributes demonstrated their effectiveness in the context of network traffic analysis. A variant of our proposed two-step approach is used routinely at the University of Minnesota as a part of the MINDS system to summarize several thousand anomalous netflows into just a few dozen summaries. This enables the analyst to visualize the suspicious traffic in a concise manner and often leads to the identification of attacks and other undesirable behavior that cannot be captured using widely used intrusion detection tools such as SNORT.
 actions in the data set are equally important. In several applications, the transac-tions might have different levels of importance. In such cases it will be desirable for higher ranked transactions to incur low information loss while lower ranked transactions can tolerate a little higher information loss. the network flows are ranked based on their anomaly scores. The main challenge that arises in adapting our proposed summarization techniques to this problem is how to incorporate the knowledge of ranks while scoring the candidates to achieve the above stated objective. We are currently investigating a few possible approaches in this direction.
 The authors thank Gaurav Pandey and Shyam Boriah for their extensive com-ments on an earlier draft of the paper.
 Center contract number DAAD19-01-2-0014, by the ARDA Grant AR/F30602-03-C-0243 and by the NSF grant IIS-0308264. The content of this work does not necessarily reflect the position or policy of the government and no official endorsement should be inferred. Access to computing facilities was provided by the AHPCRC and the Minnesota Supercomputing Institute.

