 The classical probabilistic models attempt to capture the Ad hoc information retrieval problem within a rigorous proba-bilistic framework. It has long been recognized that the primary obstacle to effective performance of the probabilis-tic models is the need to estimate a relevance model. The Dirichlet compound multinomial (DCM) distribution , which relies on hierarchical Bayesian modeling techniques, or the Polya Urn scheme, is a more appropriate generative model than the traditional multinomial distribution for text docu-ments. We explore a new probabilistic model based on the DCM distribution, which enables efficient retrieval and ac-curate ranking. Because the DCM distribution captures the dependency of repetitive word occurrences, the new proba-bilistic model is able to model the concavity of the score function more effectively. To avoid the empirical tuning of retrieval parameters, we design several parameter esti-mation algorithms to automatically set model parameters. Additionally, we propose a pseudo-relevance feedback algo-rithm based on the latent mixture modeling of the Dirichlet compound multinomial distribution to further improve re-trieval accuracy. Finally, our experiments show that both the baseline probabilistic retrieval algorithm based on the DCM distribution and the corresponding pseudo-relevance feedback algorithm outperform the existing language mod-eling systems on several TREC retrieval tasks.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms, Theory
The classical probabilistic retrieval model [16, 13] of infor-mation retrieval has received recognition for being theoreti-cally well founded. For the probabilistic retrieval models, we estimate two probabilistic models for each query: relevant class and non-relevant class. The probability ranking princi-ple [16] suggests ranking documents by the log-odds ratio of being observed in the relevant class against the non-relevant class. Robertson [16] has proved that ranking documents by the odds of being generated by the relevant class against non-relevant class optimizes the retrieval performance under the word independence condition.

The problem of effectively estimating the relevant and non-relevant models remains a major obstacle in the practi-cal applications of the probabilistic retrieval models. Vari-ous approaches for the estimation of relevance models have been considered in previous literature. The Binary indepen-dence retrieval model [13] treats each document as a binary vector over the vocabulary space and assumes independence between words. The 2-Poisson model [4] treats the term frequency as a mixture of 2-Poisson distributions, but ig-nores document length. Robertson and Walker [14] approx-imate the 2-Poisson model to account for several influential variables, including document length. The classical prob-abilistic retrieval models face the major challenge of effec-tively estimating the relevance model to take into account the variables influencing retrieval performance, and resort to different approximation techniques to model the relevant class.

However, the perceived limitation of the probabilistic re-trieval model led to the development of the language models [12, 3, 6]. These language modeling approaches focus on ef-fective estimation techniques for document modeling, and have been shown excellent retrieval accuracy and efficient implementation in practice. Language modeling approaches treat each document in the collection as a unique model and the query as strings of text randomly sampled from these models. The ranking is based on the probability of the query being generated by the document distribution. The Unigram language model is based on the multinomial distribution; several smoothing techniques [20] have been developed to avoid zero probabilities of non-occurring words.

A major limitation of the language models has been the lack of a clear connection with the explicit modeling of rel-evance. This gap has occasioned effort to relate these two models [7, 8]. Lafferty and Zhai [7] have demonstrated the probability equivalence of the language model to the proba-bilistic retrieval model under some very strong assumptions, which may or may not hold in practice. Language mod-eling approaches apply query expansion to incorporate in-formation from (pseudo) relevance feedback documents [19], while the probabilistic retrieval approaches treat (pseudo) relevance feedback as model adjustment. Thus, probabilis-tic approaches based on document generation have the ad-vantage of being able to inherently improve the estimation of the probabilistic models by exploiting both the positive and negative feedback information. Considering the ad-vantages of the probabilistic model, Lavrenko and Croft [8] proposed a novel technique for estimating the relevance model from the top ranked retrieved documents by com-bining language modeling estimation techniques with prob-abilistic model framework. Nevertheless, they essentially mod-el the pseudo relevance feedback process, and their model relies on a baseline language modeling approach.
Lewis [9] has pointed out the connection between the probabilistic retrieval model in information retrieval and the Naive Bayes classification model in machine learning. He also discussed the fact that the multinomial distribution performs well in the Naive Bayes classification in the con-text of text classification, but very poorly in the context of ranking documents in a search engine. Consequently, an open research problem that remains to be solved is the following:  X  X hat is a reasonable distributional model to be applied in probabilistic retrieval models. X  The multinomial distribution assumes word independence, and cannot cap-ture word burstiness: the phenomenon that if a word ap-pears once, it is more likely to appear again. In contrast, the Dirichlet compound multinomial (DCM) distribution [10, 1] has been shown effective in accommodating word burstiness, and achieves better performance in text classification and text clustering. Relying upon hierarchical Bayesian model-ing, the DCM distribution integrates out the parameters of the multinomial distribution. The DCM distribution is also motivated by the Polya urn scheme [5], which intuitively ex-plains how it captures word burstiness. The classical proba-bilistic model makes strong word independence assumptions, and thus has an incorrect model of relevant documents [7]. The DCM distribution relaxes the independence assumption by accounting for dependence among repetitive word occur-rences, and this is a better distribution for probabilistic mod-els. In our paper, we apply the DCM distribution as the generative source the new probabilistic model. Our model significantly simplifies the resulting form of the score func-tion by taking the log-odds ratio, and the new score function is very efficient to implement in practice.

The optimal setting of the retrieval parameters is usually achieved by practical tuning. Since both the query and doc-ument collection vary in practical retrieval scenarios, it is un-realistic to tune parameters for different retrieval tasks. Ap-plying already tuned parameters for another retrieval tasks may not perform consistently well. Therefore, automatically setting of retrieval parameters is critical to accommodating various retrieval tasks. Zhai and Lafferty [21] have derived a general two-stage parameter estimation method for the language model. We propose several parameter estimation approaches that explicitly capture the different impacts of document collection and query on the optimal settings of re-trieval parameters. We first estimate the non-relevant model by fitting a DCM distribution to the whole document collec-tion by using three approaches, and then estimate the query interpolation parameter which controls the relevance model generation.

Pseudo-relevance feedback has been demonstrated to be one of the most efficient approaches to improve retrieve ac-curacy. Pseudo-relevance feedback assumes that the top ranked retrieved documents are relevant, and reformulates the query representation by using these documents. Vari-ous pseudo-relevance feedback algorithms have been applied to several retrieval models. For the vector space model ap-proaches, the original query is expanded with the centroid of feedback documents [15]. For the language model ap-proaches, Zhai and Lafferty [19] have proposed several model based feedback algorithms, which expand the original query representation by the relevant topic terms from the feed-back documents. In this paper, we design a pseudo-relevance feedback algorithm based on a mixture of two DCM distri-butions: feedback relevant model and collection model. We resort to the EM algorithm to estimate the feedback rele-vant model and enrich the original relevant model without feedback. We also propose several insightful improvements on the EM algorithm to find a better local optimum.
In summary, this paper focuses on an effective probabilis-tic model which applies advanced text document modeling and estimation techniques. The main contributions of this paper are three fold: 1. A formal probabilistic retrieval model based on the 2. Several approaches that effectively estimate parame-3. A pseudo-relevance feedback algorithm based on the
In this section, we analyze the probabilistic retrieval model based on the multinomial distribution to shed some light on the intuition of using the DCM distribution. In the language modeling framework, documents are modeled as the multi-nomial distributions capturing the word frequency occur-rence within the documents. We model the relevant model and non-relevant model in the probabilistic retrieval model as two multinomial distributions. We define the parameters of relevant and non-relevant document language model as  X 
R and  X  N . The probability of document d l generated by relevant class is defined as the multinomial distribution: In the above equation, c ( w m ,d l ) is the term frequency of word w m in document d l ; n ( d l )= m c ( w m ,d l ) is the length of the document d l ; V is vocabulary size. The Non-relevant model P ( d l |  X  N ) is defined in the same way. The score func-tion of the probabilistic retrieval model based on the multi-nomial distribution can be derived from taking the log-odds ratio of two multinomial distributions.

Score MN ( d l )=log P ( d l Since the relevant class contains the query information,  X  larger than  X  m N for any word w m occurring in the query. Con-sequently, Score MN increases linearly with term frequency c ( w m ,d l ). However, the change in the relevance score caused by increasing term frequency from 1 to 2 should be larger than that caused by increasing term frequency from 2 to 3. In another word, score function is a concave function of term frequency such that score increase rate decreases with term frequency. Therefore, the probabilistic model based on the multinomial distribution violates the concavity constraint of the score function. The classical probabilistic model [13] uses binary indexing to avoid this linearity of term frequency in the score function. Another drawback of the score func-tion (2) is the inability to capture document length.
Consequently, the multinomial distribution is not an ap-propriate distribution for the probabilistic model. Because the multinomial distribution assumes the independence of the word repetitive occurrences, it results in a score function which incorporates undesired linearity in term frequency. To capture the concave property and penalize document length in the score function, a more appropriate distribution should be able to model the dependency of word repetitive occurrences (burstiness) that is if a word appears once, it is more likely to appear again. The Dirichlet compound multinomial (DCM) distribution [11, 10], which is motivated by the Polya urn scheme, is able to capture word burstiness, and thus better addresses the need to capture score function concavity and document length.
In the Bayesian hierarchical modeling framework, the Dir-ichlet distribution is a commonly used conjugate prior dis-tribution of the multinomial distribution. The Dirichlet dis-tribution for the parameters of the relevant class is where  X  R denotes the parameters of the multinomial distri-bution;  X  m R denotes the parameters of the Dirichlet distribu-tion; S R = V m =1  X  m R . We define p (  X  N |  X  N ) similarly as in Equation (3).

Hierarchical Bayesian modeling treats the generation of a document in the following way: A sample is drawn from the Dirichlet distribution to generate a multinomial distribution, and then a document is generated by the multinomial dis-tribution. This hierarchical Bayesian model is called the Dirichlet compound multinomial (DCM) distribution [11, 10, 1]. In the DCM distribution, the actual parameters are the Dirichlet parameters  X  m R and  X  m N , because the multino-mial parameters  X  m R and  X  m N have been integrated out. The DCM distribution for the relevant class is defined below.
P ( d l |  X  R )= P ( d l |  X  R ) P (  X  R |  X  R ) d X  R = n ( d l )! = n ( d l )! The first line of the above equation is derived by directly multiplying the Dirichlet distribution (3) with the multino-mial distribution (1). The third line of the above equation unnormalized version of the Dirichlet distribution with pa-rameters  X  m R + c ( w m ,d l ). We can derive the DCM distri-bution for the non-relevant class p ( d l |  X  N )inthesameway. Equation (4) looks fairly daunting, yet it can be significantly simplified by taking the log-odds ratio.

The Bayesian hierarchical modeling perspective does not provide very intuitive insights on how the DCM distribution captures word burstiness. The DCM distribution, however, also arises naturally from the Polya urn scheme [5], which ex-plains the intuition. The Polya (DCM) distribution models the following scenario: Consider an urn filled with colored balls, with one color for each word in the vocabulary, thus, the generation of a document can be simulated as draw-ing color balls from the urn. The multinomial distribution models the standard draw with replacement scheme, and the Polya (DCM) distribution models the draw with one addi-tional replacement scheme. Consequently, following the sec-ond scheme, words that have already been drawn are more likely to be drawn again, which is word burstiness.
The classical probability ranking principle [16] suggests ranking the documents by the log-odds ratio of their proba-bilities of being generated by the relevant class against the non-relevant class. Using the DCM distribution (4), we can rank documents by the following score function:
The first line of the above equation is derived by canceling the common term n ( d l )! / m c ( w m ,d l )! in the denomina-tor and numerator. The second line of the above equation is derived by noticing that  X ( s + n ) /  X ( s )= n  X  1 i =0 The score function (5) consists of two components: the first term depends on all the words occurring in the documents and the second term depends on the document length n ( d l Therefore, the complexity of scoring all the documents in the collection depends on the total term occurrences (including repetition) in the collection. Nevertheless, appropriate ini-tialization will significantly reduce the computational com-plexity, and we will further analyze the computation issue below.

In the information retrieval tasks, little information re-garding the user X  X  retrieval intent is available. Construct-ing the relevant model and non-relevant model without any relevance feedback information is a challenging problem in implementing the classical probabilistic model. User query and collection distribution are the only information available to the retrieval system. Intuitively, the initial parameters of the relevant model should capture the information in the query. Because there is limited amount of text contained in query, smoothing becomes extremely critical. The smooth-ing method involves a linear combination of the non-relevant model and the query, and is similar to the document smooth-ing approaches in the language model [20]. where  X  controls the degree of smoothing in the relevant model; c ( w m ,q ) is the term frequency of word w m in query q . Based on this initialization, a word not occurring in the query has the same parameter in the relevant model as the non-relevant model. By plugging Equation (6) into Equation(5), we obtain the final score function of the DCM retrieval algorithm .
 Score ( d l )= where n ( q )isthelengthofquery q . The above score func-tion consists of two components: The first term depends on the frequency of word co-occurring in the query and the doc-ument (that is term frequency); the second term depends on the query length and the document length. The first term of the score function increases with the term frequency, and the score increase rate is larger for smaller term frequen-cies because of the concavity of the log function. This is consistent with the intuition that repeated occurrences of query terms in the document have less impact on the rel-evance than their first occurrence, which is the basic term frequency constraint in [2]. The second term decreases with document length. This indicates that if two documents con-tain the same number of query terms, the shorter document is more likely to be relevant because it contains fewer non-relevant terms. This intuition also agrees with the length normalization constraint in [2].

Now we analyze the computational efficiency of the score function (7). The first term of the score function depends only on the terms co-occurring in the query and document, and consequently inverted indexing can significantly speed up the computation. The second term of this score function can be pre-computed, because this term only requires information on query length and document length. Thus, this model is very efficient to implement for large scale dataset.
In this section, we propose three approaches to estimate the non-relevant model from the document collection. The first approach is to fit a DCM distribution to the collection, and then compute the maximum likelihood estimate (MLE). To reduce the computation, the second approach is to fit an approximated DCM distribution to the collection, and then compute the MLE. The third approach is to fit the DCM distribution to the collection, and then compute the Leave-one-out estimate. Applying the maximum likelihood estimator based on the DCM distribution is the most straight forward approach to estimate the non-relevant model. We can not derive a closed-form solution for the maximum likelihood parameter values for the DCM model. An iterative gradient descent optimiza-tion method can be used to estimate the vector by comput-ing the gradient of the DCM log likelihood. The maximum likelihood estimate [10, 11] can be computed using the fixed point iteration. where Digamma function  X  is defined as  X (  X  )= d d X  log  X (  X  ). A detailed proof is given in [11].
Although the estimation procedure can be done off-line, the above formulation is still very inefficient when the col-lection size is large. The DCM distribution can be approx-imated by the EDCM distribution [1] to reduce the com-putation, when the dimension is very large. The EDCM distribution is defined as Based on the EDCM distribution, we can derive the maxi-mum likelihood estimate by the following steps.
 We can use the fixed point iteration to calculate S N based on the first Equation, and calculate  X  m N based on the sec-ond Equation after the fixed iteration algorithm converges. The calculation of the maximum likelihood estimate for the EDCM distribution is more efficient than that of the DCM distribution, because only S N is calculated in the fixed point iteration in the MLE of the EDCM distribution, while all the  X 
R are calculated in that of the DCM distribution.
An alternative to estimate the non-relevant model is to maximize the leave-one-out (LLO) likelihood [21, 11] instead of the true likelihood. The LLO likelihood, based on the cross validation criterion, is the product of the probability of each word given the distributional model constructed by the remaining data with the target word excluded. The LLO log-likelihood [11] for the DCM distribution is l (  X  | C )= ability of observing outcome c ( w m ,d i ) given the remaining data. This probability is derived from where d i \ w m represents document d i without one occur-rence of word w m . Equation (11) does not involve any spe-cial functions, so it is very efficient to implement. After taking the first derivative, a convergent fixed-point iteration can be used to solve the above optimization problem.
Manually tuning the free parameters to accommodate dif-ferent retrieval tasks dominates much of the research in in-formation retrieval. Automatically estimating the retrieval parameters improves the robustness of the retrieval system significantly. Because  X  is query dependent, we process the estimation of interpolation parameter  X  online after the user sends a query. Thus, the computational requirement in esti-mating the parameter  X  is more demanding than in estimat-ing the non-relevant model. Here we apply the EDCM dis-tribution given by Equation (9) to expedite the estimation of the parameter  X  . In order to estimate the parameter  X  , we approximate the relevant model space by the set of doc-uments which contain all the terms in the query. Thus, we maximize the log likelihood of the set of documents over pa-rameter  X  with the EDCM distribution. By plugging Equa-tion (6) into Equation (9), we get where C indicates the set of documents containing all the terms in the query. A closed-form solution is not feasible for the above optimization problem. In such a case, gradient descent approaches provide an alternative avenue for esti-mating parameter  X  . Zhai and Lafferty [21] used the whole collection as the relevant space in the second stage estima-tion, and thus their model requires expensive computation.
A natural way to estimate the relevant model from pseudo relevance feedback documents is to assume that the feed-back documents are directly generated by the relevant DCM distribution. In addition, the feedback documents also in-clude general English words besides the words relevant to the search topic. Therefore, a more reasonable model would be a mixture model that generates a feedback document by mix-ing the query topic model with a collection language model. We define two latent generative model components based on the DCM distributions: z FR and z N . z FR is the feedback relevant model variable, which represents terms occurring in the feedback documents and pertinent to the user X  X  search intent. z N is the collection model variable, which represents the general English words occurring frequently in the whole collection. The parameters of the z N are consistent with the parameters of the non-relevant class  X  N m ,whichcanbe estimated by one of the three approaches discussed in Sec-tion 2.3. Thus, a document is generated by picking a word either from the feedback relevant model z FR or the collec-tion model z N . The goal of this algorithm is to estimate the feedback relevant model z FR and use the most frequently oc-curring terms in z FR to enrich the original relevant model.
In order to speed up the computation, we employ the ap-proximated DCM distribution in Equation(9) as the under-lying generative sources. Thus, The log likelihood function of the feedback document is where F is the feedback documents set. We fix the back-ground collection model z N , and apply the EM algorithm to estimate the z FR . The detailed EM step is listed in Table 1. In the E-step, we calculate P ( z k | d i ,w m ), the probability of term w m in document d i belongs to generative model z k .In the M-step, we use P ( z k | d i ,w m ) to calculate both the proba-bility P ( z k | d i ) and the relevant feedback model distribution parameters S FR and  X  m FR .

The simple mixture model in [19] fixes the mixing co-efficients P ( z k | d i ) across all the feedback documents, even though some feedback documents presumably have more noise than others. Without fixing the mixing coefficients, the EM algorithm will converge to the local optimum where P ( z FR | d i ) = 1. To address this problem, we propose three improvements to the EM algorithm. These are the criti-cal elements which lead to improvement of the algorithmic performance.

First, the traditional language modeling approach uses the original collection model p ( w m | C ), whose parameters are estimated from the whole collection. Since the feed-back documents set F contains fewer terms than the whole collection, directly applying the collection model results in estimated background collection model will cause the EM al-gorithm to converge to P ( z FR | d i ) = 1. Zhai and Lafferty [19] noticed this convergence problem, but they did not explic-itly point out the underlying reason. Instead, they solved the problem by fixing P ( z FR | d i )and P ( z N | d i ). More re-cently, Tao and Zhai [17] addressed this problem by using early stopping to avoid converging to P ( z FR | d i )=1. In contrast to their Ad hoc approach, we use the reduced col-lection estimate , where we only count the word occurring in P ( w m | z N )=  X  m N /S Reduced N . Thus, we avoid fixing P ( z and the EM algorithm still converges to a desirable local op-timum. Moreover, updating the mixing coefficients P ( z k helps to converge to the local optimum quickly. Thus, this algorithm is very efficient and requires fewer EM iterations than the simple mixture model.

Second, a deterministic annealing procedure [18] allows the EM algorithm to find better local optimum of the like-lihood function . Deterministic annealing is also proposed for the EM clustering based on the approximated DCM dis-tribution in [1], which shows that deterministic annealing leads to a substantial better results. We replace the original Expectation step with where T is a temperature parameter. In each iteration, we decrease T  X   X T until the EM steps converge. The parame-ter  X  is a large value close to one, and we set  X  =0 . 96 in the experiments. This shows that the effect of T is to dampen the posterior probabilities such that they will get closer to the uniform distribution with decreasing T .

Third, the simple mixture model [19] does not involve the original query in any way during the EM iteration. In stead, it interpolates the estimated feedback model with original query model by using a fixed interpolation coefficient. A query regularization approach was proposed in [17] for lan-guage model based relevance feedback to reduce the defi-ciency. We apply a query regularization approach similar to [17]. In this algorithm, we treat query q as a relevant document occurring  X  times in the M-step of the algorithm. Therefore, the parameter  X  controls the relative weight we k  X  X  FR,N } k  X  X  FR,N } | add the original query to the feedback relevant model. In the experiments, we will show how the parameter  X  influences the retrieval performance. After the algorithm converges, we interpolate  X  FR with  X  N to obtain the expanded query. We scale down the value of the largest  X  m R to the same value of the parameter  X  in Equation (6) by multiplying  X  max m  X  m
To evaluate our DCM retrieval algorithm and its pseudo-relevance feedback algorithm described in the previous sec-tions, we experimented with three TREC datasets. The first one is the TREC 2003 HARD track, which uses part of the AQUAINT dataset plus two additional datasets (Congres-sional Record (CR) and Federal Register (FR)). We do not have the additional datasets in the TREC 2003 HARD track. Our results are still comparable to other published TREC 2003 HARD results, although the data are a little differ-ent. The second one is the TREC 7 dataset, which contains data from the TREC Disk 4 and 5 (excludes Congressional Record). The third one is the TREC 8 dataset. For all these tracks, we use the topic titles as queries on all the 50 topics, because they are closer to the actual queries used in real applications. Data pre-processing is standard: terms were stemmed using the Porter Stemming and stop words were removed by using standard stop word list.
 We employed the Lemur Toolkit as our retrieval system. To measure the performance of the retrieval algorithms, we used three standard Ad hoc retrieval measures: (1) Mean Average Precision (MAP), which is calculated as the average of the precision after each relevant document is retrieved, reflects the overall retrieval accuracy. (2) Precision at 10 documents (Pr@10): this measure gives us the precision for the first 10 documents.
To evaluate the effectiveness of our DCM retrieval algo-rithms, We experimented with 6 variants of the DCM re-trieval algorithm, and the notations are shown in Table 2. we compared the DCM retrieval algorithms with the prob-abilistic model based on the multinomial distribution(MN), the Dirichlet prior smoothing language model (DP) [20] and the two stage smoothing retrieval model (TS) [21].
In order to obtain a fair comparison, we pursued 5-fold cross-validation on the DP algorithm, the MN algorithm and DCM retrieval algorithm with tuned parameters (DCM-F-T, DCM-A-T, DCM-L-T), and then compared their cross-validation performance (CVP) with the DCM retrieval al-gorithms with automatic parameter estimation DCM-F-E, DCM-A-E, DCM-L-E and the TS algorithm (these four al-gorithms are parameter free). For the probabilistic model based on multinomial distribution, we use collection multi-nomial model as non-relevant model, and interpolate the query multinomial model with collection multinomial model to generate the relevant model. We separated 50 queries into 5 parts, where each part contains 10 queries. For the k th set of queries, we trained the parameters to optimize the re-trieval performance for the other 4 sets of queries, and used this set of the parameters to test on k th ( k =1 , 2 , 3 , 4 , 5) set of queries to obtain the retrieval performance measure for k th part. The cross-validation performance is the average performance on the 5 test query sets.

In Table 3, we show the experimental results of these re-trieval algorithms and indicate the best performance in bold. From the results, all our DCM retrieval algorithm variants consistently outperform the DP algorithm and the TS al-gorithm. The MN algorithm performs worst among all the algorithms, because it ignore the concavity of score func-tion and document length. The DCM retrieval algorithms with LLO estimate (DCM-L-T and DCM-L-E) have the best performance among all the variants of the DCM retrieval al-gorithm. We observe that S L N &gt;S F N &gt;S A N ,where S and S A N are the sum of non-relevant parameters estimated by the LLO estimator based on the full DCM, the MLE based on the full DCM, and the MLE based on the EDCM respectively. We can rank these three estimation approaches in terms of retrieval accuracy from the most accurate to the least accurate: LLO estimate, MLE based on Full DCM dis-tribution, and MLE based on the approximated DCM distri-bution. As indicated in [1], the parameter S N = N m =1  X  indicates the burstiness of the distribution. Increasing S decreases burstiness and vice versa. Since the non-relevant model captures the information in the large TREC collec-tions, less burstiness is more suitable. The parameter S plays the same role as the parameter  X  in the Dirichlet prior smoothing language model. The experiments in [20] show that the retrieval algorithm performs constantly well when the parameter  X  is large. This indicates why the LLO esti-mate performs consistently better than other approaches.
In this section, we compare the DCM pseudo relevance feedback algorithm (DCM-PR) with language model pseudo relevance feedback algorithms, including the simple mixture model (SMM) [19], the divergence minimization algorithm (DM) [19], and the regularized mixture model (RMM)[17].
The simple mixture model and the divergence minimiza-tion model are the standard language model based feedback algorithms [19] with strong performance. The simple mix-ture model algorithm models the feedback documents as a mixture of feedback topic model and background collection model. It uses the EM algorithm to estimate the feedback topic model and interpolates with the original query model. The divergence minimization algorithm models the relevance feedback in an optimization framework, and tends to mini-mize the divergence between the feedback topic model and the feedback documents, and at the same time maximize the divergence between the feedback topic model and the back-ground collection model. It also interpolates the original query model with the feedback topic model. The regularized mixture model [17] is the most up to date pseudo-relevance algorithm, which uses query regularization technique and performs an early stopping of the EM iteration.

We use the Dirichlet prior language model with a tuned parameter (DP) as the baseline retrieval model for the sim-ple mixture model (SMM), divergence minimization model (DM) and regularized mixture model (RMM) algorithms. We use the DCM probabilistic retrieval model with the LLO estimate and tuned parameter  X  (DCM-L-T) as the baseline retrieval model for the new pseudo relevance feedback algo-rithm (DCM-PR). We trained these algorithms on TREC 2003 HARD dataset, and set parameter values by optimiz-ing the performance on TREC 2003 HARD dataset. For the SMM and DM algorithms, the tuning parameters are the weighting parameter  X  and the interpolation parame-ter  X  . We varied both  X  and  X  from 0 to 1.0 with step 0.2. For the TREC 2003 HARD dataset, the SMM algo-rithm performed best when  X  =0 . 8and  X  =0 . 8; the DM algorithm performed best when  X  =0 . 8and  X  =0 . 4. The RMM algorithm performed best when  X  =30 , 000,  X  =0 . 9 and  X  = 2; the DCM-PR algorithm performs best when the query regularization parameter  X  = 125 and annealing damping parameter  X  =0 . 96. We fixed these parameter settings for all the remaining datasets, and chose the top 20 terms with the largest probabilities in the feedback rel-evant model for all these algorithms. We compare these algorithms by setting feedback documents size equal to 10 and 30 respectively. The results are shown in Table 4. The DCM-PR algorithm consistently outperforms the SMM, DM and RMM algorithms on all the three datasets in terms of MAP and Pr@10 with significant improvement. The bene-fits of pseudo relevance feedback decreases as we increase the feedback document size from 10 to 30, because more noise is introduced to the model as more documents are used for pseudo-relevance feedback beyond a limit.
We study the robustness of query regularization parame-ter  X  for the DCM pseudo-relevance feedback algorithm in this section. In the DCM pseudo-relevance feedback algo-rithm,  X  indicates the confidence of the original query model. The larger  X  is, the larger weight the original query terms have in the feedback relevant model. In another word, the estimated topic model has a larger impact on the terms in the query if parameter  X  is larger.

In the previous experiments, we set the  X  = 125. We con-ducted another set of experiments by fixing feedback docu-ments number equal 10 and varying parameter  X  .InFigure 1, we plotted the MAP for several values of  X  for the DCM pseudo-relevance model. The performance is insensitive to the setting of the parameter  X  as long as the prior strength  X  is set to be larger than 50. The performance insensitivity to the parameter  X  ensures the robustness of the model.
The main contribution of this paper is a new retrieval al-gorithm based on the probabilistic model framework and advanced document modeling and estimation techniques. The probabilistic model framework guarantees the theoreti-cal rigorousness, and the advanced document modeling and estimation techniques lead to efficient retrieval and accurate ranking. We have proposed applying the DCM distribution as the generative source in the probabilistic model, and this distribution is able to capture the dependency among repeti-tive word occurrences. To reduce the parameter tuning step, we have proposed several estimation approaches that auto-matically set the parameters of retrieval ranking function. To further improve the retrieval accuracy, we have proposed a pseudo-relevance feedback algorithm based on the DCM distribution. We have evaluated the algorithm on various TREC datasets. The experimental results show that our algorithm outperforms the existing language model based algorithms significantly.

There are two interesting research directions that will help better understand the role of the DCM retrieval model. First, it would be promising to apply the DCM retrieval model in the relevance feedback context, where the negative feedback documents will help the DCM retrieval algorithm to gain additional benefits. Second, the DCM distribution only accounts for the burstiness among repetitive terms, and ignores burstiness between different but related terms. A new distributional model which is able to capture the bursti-ness between different terms would be a promising direction to further improve the text retrieval accuracy.
We acknowledge support from Cisco, University of Cal-ifornia X  X  MICRO program. We also appreciate comments from Charles Elkan, Tom Minka and anonymous reviewers. [1] C. Elkan. Clustering documents with an exponential [2] H. Fang, T. Tao, and C. Zhai. A formal study of [3] F.Song and W.B.Croft. A general language model for [4] S. Harter. A probabilistic approach to automatic [5] N. Johnson, S. Kotz, and N. Balakrishnan. Discrete [6] J. Lafferty and C. Zhai. Document language models, [7] J. Lafferty and C. Zhai. Probabilistic relevance models [8] V. Lavrenko and W. B. Croft. Relevance-based [9] D. Lewis. Naive (bayes) at forty: The independence [10] R. Madsen, D. Kauchak, and C.Elkan. Modeling word [11] T. Minka. Estimating a dirichlet distribution. [12] J. Ponte and W. B. Croft. A language modeling [13] S. Robertson and K. S. Jones. Relevance weighting of [14] S. Robertson and S. Walker. Some simple effective [15] J. Rocchio. Relevance feedback in information [16] S.E.Robertson. The probability ranking principle in ir. [17] T. Tao and C. Zhai. Regularized estimation of mixture [18] N. Ueda and R. Nakano. Deterministic annealing EM [19] C. Zhai and J. Lafferty. Model-based feedback in the [20] C. Zhai and J. Lafferty. A study of smoothing methods [21] C. Zhai and J. Lafferty. Two-stage language models
