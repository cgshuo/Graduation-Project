 NGO XUAN BACH, NGUYEN LE MINH, TRAN THI OANH, and AKIRA SHIMAZU In recent years, a new research field called Legal Engineering has been proposed in the 21st Century COE Program, Verifiable and Evolvable e-Society [Katayama 2005, 2007; Katayama et al. 2008]. Legal Engineering serves to exam and verify whether a law has been established appropriately according to its purpose, whether a law con-tains contradictions, whether the law is consistent with related laws, and whether the law has been modified, added, and deleted consistently. There are two important goals of Legal Engineering. The first goal is to help experts make complete and con-sistent laws, and the other is to design an information system which works based on laws.

Legal Engineering regards that laws are a kind of software for our society. Specif-ically, laws such as pension law are specifications for information systems such as pension systems. To achieve a trustworthy society, laws need to be verified for their consistency and contradiction.

Legal texts have some specific characteristics that make them different from other daily-use documents. Legal texts are usually long and complicated. They are composed by experts who spent a lot of time to writing and checking them carefully.
One of the most important characteristics is that legal texts usually have some spe-cific structures at both sentence and paragraph levels. At the sentence level, a law sen-tence can roughly be divided into two logical parts: requisite part and effectuation part [Bach 2011; Bach et al. 2011; Tanaka et al. 1993]. At the paragraph level, a paragraph usually contains a main sentence 2 and one or more subordinate sentences [Takano et al. 2010]. In this article, we will consider the task of analyzing logical structures of legal texts, in which we first recognize logical parts in legal articles and then group related logical parts into logical structures.

In the remainder of this section, we first explain about logical parts and logical structures 3 . We next describe related work, the motivation of this work, and the overview of this article. A logical part is a clause or phrase in law sentences that conveys a part of the meaning of legal texts. Each logical part contains a specific kind of information according to its type. Three main types of logical parts are antecedent part , consequent part ,and topic part 4 . A logical part in consequent type describes a law provision; a logical part in antecedent type indicates cases (or the context) the law provision can be applied; and a logical part in topic type describes subjects related to the law provision.
A logical structure is a pair of two high-level logical parts, a requisite part and an effectuation part in the form: Each requisite part or effectuation part consists of several logical parts. This is the reason why we call them high-level logical parts. In a simple case, the requisite part only consists of one antecedent part ;andthe effectuation part only consists of one consequent part .

Figure 1 shows four cases of law sentences and their logical parts. Four cases are divided depending on the relationships between the topic part and the antecedent part and the consequent part. In Case 0, there is no topic part. The requisite part only consists of the antecedent part, and the effectuation part only consists of the conse-quent part. In Case 1, the topic part depends on the antecedent part. The requisite part is composed from the topic part and the antecedent part, while the effectuation part only consists of the consequent part. In Case 2, the topic part depends on the con-sequent part. The requisite part consists of the antecedent part, while the effectuation part is composed from the topic part and the consequent part. And in Case 3, the topic part depends on both the antecedent part and the consequent part. The requisite part is composed from the topic part and the antecedent part, while the effectuation part is composed from the topic part and the consequent part.

Logical structures in four cases can be expressed as follows:  X  X ase0: A  X  C  X  X ase1: T  X  A  X  C  X  X ase2: A  X  T  X  C  X  X ase3: T  X  A  X  T  X  C
In this work, we first recognize logical parts, then group logical parts into logical structures. We do not recognize the relations between logical parts. In this sense, a logical structure can be seen as a group of some logical parts.
 This subsection presents related work and places our work in the scope of the legal text processing field. First, Subsection 1.2.1 presents studies on legal text processing in general. Then, Subsection 1.2.2 focuses on studies on analyzing logical structures of Japanese legal texts, which are closest to our work. 1.2.1. Studies on Legal Text Processing. Studies on legal text processing can be cat-egorized into several topics including Legal Ontology Learning, Legal Information Extraction, Legal Semantic Annotation, Automatic Identification of Legal Terms, Le-gal Knowledge Modeling, Legal Argumentation, Legal Automatic Summarisation, and Fundamental NLP Tasks for Legal Texts 5 . In the following, we describe a short sum-mary of previous studies according to each topic 6 .  X  Legal Ontology Learning . Lame [2004] presents a general method using NLP tech-niques for recognizing legal concepts and semantic relations among them, the main components of an ontology. Using this method, the author built an ontology of
French laws, which is dedicated to information retrieval. Saias and Quaresma [2005] present a methodology to automatically create an OWL (Ontology Web Language) ontology from a set of legal documents in five steps: 1) Definition of an initial top-level ontology, 2) Identification of concepts referred in the legal documents and extraction of its properties, 3) Identification of relations between the identified con-cepts, 4) Creation of an ontology using the identified concepts and relations, and 5) Mergence of the created ontology with the initial ontology. Both Lame [2004] and
Saias and Quaresma [2005] identify components of legal ontologies from the anal-ysis of legal texts. The method of Saias and Quaresma [2005], however, can exploit initial documents which are enriched with instances to build the final ontology. The authors claim that this process allows the definition of semantic Web agents able to query the semantic content of these documents.

A rule-based method for extracting and analyzing definitions from parsed texts is presented in Walter and Pinkal [2006]. They evaluated this method on a corpus of about 6,000 German court decisions and reported an experiment exploring the use of extraction results to improve the quality of text-based ontology learning. V  X  olker et al. [2008] describe the open-source software Text2Onto2, a framework for ontology learning from open-domain unstructured text.  X  Legal Information Extraction . Walter [2008] presents a rule-based method which uses dependency parse trees to extract definitions from German court decisions. Mc-
Carty [2007] presents a method to compute semantic interpretations of legal texts from the output of a syntactic parser. The author introduced an initial legal corpus consisting of federal civil cases in the appellate courts in the United States; however, neither experiment nor evaluation was described.  X  Legal Semantic Annotation . Brighi et al. [2008] present an approach for the automatic annotation of modificatory provisions of Italian laws. They adopted a rule-based algorithm to fill the semantic roles of the semantic frame associated with the modificatory provision; however, this work is still in a prototypal stage.
Spinosa et al. [2009] present a system for the automatic consolidation of Italian legislative texts. The goal of the system is to be used as a support of an editorial consolidating activity. The proposed approach to consolidation is metadata-oriented (XML-based) and based on NLP techniques. The system was implemented and evaluated on Italian textual amendments.  X  Automatic Identification of Legal Terms . Pala et al. [2010] describe a project on identification of legal terms. The goal of this project is to build an electronic dictionary of Czech law terms. They also presented a legal database including approximate 50,000 Czech law documents.  X  Legal Knowledge Modeling . Nakamura et al. [2007] describe a rule-based system which translates legal texts into logical forms. Their logical formalization conforms to Davidsonian Style, which is suitable for languages allowing expressions with zero-pronouns such as Japanese. The system achieved 78% accuracy in terms of deriving predicates with bound variables.  X  Legal Argumentation . Moens et al. [2007] describe an investigation on the detection of arguments in legal texts. They considered the detection task as a classification problem and built a classifier using a set of annotated arguments. Various kinds of features were evaluated including lexical, syntactic, semantic, and discourse properties of texts.

Wyner et al. [2010] present recent approaches to automatic identification of legal arguments, which use Context Free Grammar, ontologies, and NLP techniques.  X  Legal Automatic Summarisation . Grover et al. [2003] present a method for auto-matic summarisation of legal documents in two steps: 1) sentences in the legal documents are classified according to their rhetorical role, and 2) sentences are selected to form a summary based on their rhetorical role.  X  Fundamental NLP Tasks for Legal Texts . These studies investigate fundamental
NLP tasks (such as morphology, syntactic parsing, chunking and so on) on legal do-mains. Pala et al. [2007] explore the morphology of the Czech law texts on a corpus of approximate 50,000 Czech law documents including Constitution, acts, public notices, and court judgements. Venturi [2010] describes an investigation on syn-tactic and lexical characteristics of legal language (Italian and English laws) with respect to ordinary language. According to the author, understanding these char-acteristics of specialized languages has practical importance in the development of domain-specific applications. 1.2.2. Studies on Analyzing Logical Structures of Japanese Legal Texts. Analyzing logical structures of legal texts can be considered as a subtopic of legal knowledge modeling in which we try to model knowledge conveyed in legal documents.

There have been some studies analyzing logical structures of Japanese legal texts. [Tanaka et al. 1993] describe the standard structure of legal provisions based on the principle of legal condition-effect . Tanaka [1998] analyzes semantic functions of the legal-effect X  X  restrictive part and its semantic restriction to the provision.
Muramatsu et al. [2002] describe a tool that displays logical structure of legal sentence from tagged legal sentences. This tool consists of two functions: 1) tagging support function, which labels automatically for gross structure, and displays tag can-didates for logical structure; and 2) logical structure display function, which shows a logical structure of a legal sentence based on the tag information.

Recently, a new research field called Legal Engineering has been proposed in the 21st Century COE Program, Verifiable and Evolvable e-Society Katayama [2005, 2007] and Katayama et al. [2008]. Several works have been conducted in this program. Bach et al. [2011] present the RRE task 7 , which analyzes logical structures of legal texts at the sentence level. In the RRE task, the goal is to recognize logical parts given an input law sentence. This task considers two types of sentences ( implication type equivalence type ) and seven kinds of logical parts (three kinds of topic parts , antecedent part , consequent part , left equivalent part ,and right equivalent part ).
Compared to the RRE task, which analyzes logical structures of legal texts at the sentence level, our task is more difficult in some points.  X  In the RRE task, we only consider a single sentence. We assume that all logical parts in a sentence belong to the same logical structure. In this task, we consider multiple sentences. A logical structure consists of several logical parts in different sentences.
A logical part also can belong to multiple logical structures.  X  In this task, we also consider cases that a logical part contains other logical parts (embedded relationship is possible).

Several machine learning models have been proposed to deal with the RRE task [Bach et al. 2011], in which the task is modeled as a sequence learning problem. Exper-imental results showed that Conditional random files (CRFs) [Lafferty et al. 2001] can solve the task relatively well. They achieved nearly 90% in F National Pension Law corpus.

Bach et al. [2010] describe an investigation on contributions of words to the RRE task. Authors presented a method to evaluate the importance of words in the task and found that words that have strong relations to the logical structure of law sentences are very important for machine learning models. Kimura et al. [2009] focus on dealing with legal sentences including itemized and referential expressions. They presented a rule-based method for substituting referent phrases. These works, however, only analyze logical structures of legal texts at the sentence level.

At the paragraph level, Takano et al. [2010] classify a legal paragraph into one of six predefined categories: A , B , C , D , E ,and F . Among six types, Type A , B ,and C correspond to cases in which the main sentence is the first sentence, and subordinate sentences are other sentences. In paragraphs of Type D , E ,and F , the main sentence is the first or the second sentence, and a subordinate sentence is an embedded sentence in parentheses within the main sentence.

This work is the next study following to the 21st Century COE Program, Verifiable and Evolvable e-Society. Analyzing logical structures of legal texts is an important task in Legal Engineering. The outputs of this task will be beneficial to people in understanding legal texts. They can help understanding in the following ways. (1) What does a law sentence say? (The consequent part of a logical structure is the (2) What cases the law sentence can be applied? (The antecedent part is the answer (3) What subjects are related to the provision described in the law sentence? (The topic
This task is the preliminary step, which supports other tasks in legal text processing (translating legal articles into logical and formal representations, legal text summari-sation, legal text translation, question answering in legal domains, etc) and serves legal text verification, an important goal of Legal Engineering. For example, in the task of translating legal articles into logical and formal representations, we can do the following. (1) Recognizing logical parts and logical structures in the legal articles. (2) Translating each logical part into a formal representation. (3) Combining formal representations of logical parts in the same logical structures
Until now, most researches have focused on analyzing logical structures of legal texts at the sentence level. Analyzing law sentences individually is not enough to un-derstand a legal document. In most cases, to understand a law sentence, we need to understand related sentences or its context. To our knowledge, however, no existing research addresses the task at the paragraph level. Analyzing logical structures of le-gal paragraphs is therefore a next step in the research of Legal Engineering to achieve its goals: helping people in understanding legal texts and making computers able to process legal texts automatically. In this article, we address the task of learning logical structures of legal articles at the paragraph level. We propose a two-phase framework to complete the task, in which we recognize logical parts in the first phase and logical structures in the second phase. We also describe experimental results on legal data.

Our main contributions can be summarized in the following points. (1) Introducing a new task to legal text processing, learning logical structures of para-(2) Presenting an annotated corpus for the task, the Japanese National Pension Law (3) Proposing a two-phase framework and providing solutions to solve the task. (4) Evaluating our framework on the real annotated corpus.

The rest of this article is organized as follows. Section 2 describes our task and its two subtasks: recognition of logical parts and recognition of logical structures .In Section 3, we present our solution for the first subtask: multi-layer sequence learning model for recognizing logical parts. Section 4 describes our solution for the second subtask: integer linear programming for recognizing logical structures. Experimental results on legal articles are described in Section 5. Finally, conclusions and directions for further research are presented in Section 6. Learning logical structures of paragraphs in legal articles is the task of recognition of logical structures between logical parts in law sentences. A logical structure is formed from a pair of a requisite part and an effectuation part . These two parts are built from other kinds of logical parts such as topic parts , antecedent parts , consequent parts , and so on [Bach 2011; Bach et al. 2011] 9 . Usually, consequent parts describe a law provision, antecedent parts describe cases in which the law provision can be applied, and topic parts describe subjects which are related to the law provision. In this article, a logical structure can be defined as a set of some related logical parts.
Figure 2 shows two cases of the inputs and outputs of the task. In the first case (a), the input is a paragraph of two sentences, and the outputs are four logical parts, which are grouped into two logical structures. In the second case (b), the input is a paragraph consisting of four sentences, and the outputs are four logical parts, which are grouped into three logical structures. We have the two following remarks. (1) A logical part may contain other logical parts. For example, in case (a), logical part (2) A logical part can belong to multiple logical structures. For example, in case (b),
An example in natural language 10 is presented in Figure 3. In this example, the first logical part describes a provision and other logical parts describe cases in which the provision can be applied. There are three cases, so we have three logical structures. Let s be a law sentence 11 in the space of law sentences S ,then s can be represented by a sequence of words s = [ w 1 w 2 ... w n ]. A legal paragraph x in the legal paragraph space X is a sequence of law sentences x = [ s 1 s 2 ... s For each paragraph x , we denote a logical part p by a quad-tuple p b , e ,and k are three integers which indicate position of the beginning word , position of the end word ,and sentence position of p ,and c is a logical part category in the set of predefined categories C . Formally, the set P of all possible logical parts defined in a paragraph x can be described as follows.
 In the above definition, l is the number of sentences in the paragraph x ,and len the length of the k th sentence.
In this subtask, we want to recognize some non-overlapping (but possibly embedded ) logical parts in an input paragraph. A solution for this task is a subset y does not violate the overlapping relationship. We say that two logical parts p are overlapping if and only if they are in the same sentence e &lt; e that p 1 is embedded in p 2 if and only if they are in the same sentence b  X  b
Figure 4 illustrates an example of overlapping and embedded relationships. In this example, the law sentence consists of nine words w 1 , ... (from 2 to 5), p 2 (from 1 to 7), and p 3 (from 7 to 9). Among three logical parts, p p are overlapping, and p 1 is embedded in p 2 .
 Formally, the solution space can be described as follows.
 The learning problem in this subtask is to learn a function R : X training samples { ( x i , y i ) | x i  X  X , y i  X  Y ,  X  i
In our task, we consider the following types of logical parts. (1) An antecedent part is denoted by A . (2) A consequent part is denoted by C . (3) A topic part which depends on the antecedent part is denoted by T (4) A topic part which depends on the consequent part is denoted by T (5) A topic part which depends on both the antecedent part and the consequent part (6) The left part of an equivalent statement is denoted by EL . (7) The right part of an equivalent statement is denoted by ER . (8) An object part, whose meaning is defined differently in different cases, is denoted (9) An original replacement part, which will be replaced by other replacement parts In the second subtask, the goal is to recognize a set of logical structures given a set of logical parts. We recall that a logical structure is a set of some related logical parts.
Let G = &lt; V , E &gt; be a complete undirected graph with the vertex set V and the edge set E . A real value function f is defined on E as follows. In this subtask, each vertex of the graph corresponds to a logical part, and a complete subgraph corresponds to a logical structure. The value on an edge connecting two ver-tices expresses the degree that the two vertices belong to one logical structure. The positive (negative) value means that two vertices are likely (not likely) to belong to one logical structure.

Let G s be a complete subgraph of G ,then v ( G s ) and e the set of edges of G s , respectively. We define the total value of a subgraph as follows.
Let be the set of all complete subgraphs of G . The problem becomes determining asubset  X  that satisfies the following constraints. (1)  X  g  X  , | v ( g ) | X  2, (2)  X  g  X  v ( g ) = V , (3)  X  g 1 , g 2  X  | v ( g 1 )  X  v ( g 2 )  X  v ( g 1 ) = v (4)  X  g  X  ,  X  h  X  , h = g v ( h ) = V ,and (5) g  X  f ( g )  X  maximize.

Constraint 1), minimal constraint , says that each logical structure must contain at least two logical parts. There is a case that a logical structure contains only a conse-quent part. Due to the characteristics of Japanese law sentences, however, our corpus does not contain such cases. A logical structure which contains a consequent part will also contain a topic part or an antecedent part or both of them. So a logical structure contains at least two logical parts. Constraint 2), complete constraint , says that each logical part must belong to at least one logical structure. Constraint 3), maximal con-straint , says that we cannot have two different logical structures such that the set of logical parts in one logical structure contains the set of logical parts in the other logi-cal structure. Constraint 4), significant constraint , says that if we remove any logical structure from the solution, Constraint 2) will be violated. Although Constraint 3) is guaranteed by Constraint 4), we introduce it because of its importance. This section presents our model for recognizing logical parts. We consider the recogni-tion problem as a multi-layer sequence learning problem. First, we give some related notions. Let s be a law sentence, and P be the set of logical parts of s , P ={ p 1 , p 2 , ... , p m } . Layer 1 ( s ) parts in P , which are not embedded in any other part. Layer P where Figure 5 illustrates a law sentence with four logical parts in three layers: Part 1 and Part 2 in Layer 1 , Part 3 in Layer 2 , and Part 4 in Layer
Let K be the number of layers in a law sentence s , our model will recognize logical parts in K steps. In the k th step we recognize logical parts in Layer model the recognition problem as a sequence labeling task in which each word is an element. Logical parts in Layer i  X  1 will be used as input sequence in the i first step, we use original sentence as input).

Figure 6 gives an example of labeling for an input sentence. The sentence consists of three logical parts in two layers. In our model, we use IOE tag setting: the last element of a part is tagged with E , the other elements of a part are tagged with I ,andan element not included in any part is tagged with O . According to Bach et al. [2011], IOE tag setting is the most suitable setting for recognizing logical parts in Japanese law sentences. There are two reasons which may explain why the IOE setting is suitable for recognizing logical parts. First, in Japanese, important words usually occur at the end of a phrase, and important Bunsetsu usually occur at the end of a sentence. A Bunsetsu always depends on another Bunsetsu which stands to its right. Second, a logical part tends to finish at a punctuation mark (comma or period). So a punctuation mark has a high probability of being the last element of a logical part.
Let K  X  be the maximum number of layers in all law sentences in training data. We learn K  X  models, in which the k th model is learned from logical parts in the Layer training data, using Conditional random fields [Kudo 2010; Lafferty et al. 2001]. In the testing phase, we first apply the first model to the input law sentence, and then apply the i th model to the predicted logical parts in Layer This subsection provides an introduction to Conditional random fields (CRFs), the learning method which we choose for our multi-layer sequence learning model, and explains why CRFs is suitable for the subtask.

Conditional random fields (CRFs) [Lafferty et al. 2001; Sutton and McCallum 2006] are undirected graphical models, which define the probability of a label sequence y given an observation sequence x as follows. on the entire observation sequence x and the labels at positions i and i the entire observation sequence x and the label at position i in the label sequence y ; and  X  k are parameters of the model, which are estimated in the training process; and Z ( x ) is a normalization factor.

Training CRFs is commonly performed by maximizing the likelihood function with respect to the training data using advanced convex optimization techniques such as L-BFGS [Byrd et al. 1994]. And inference in CRFs, that is, searching the most likely output label sequence of an input observation sequence, can be done by using Viterbi algorithm [Forney 1973].
 In our multi-layer sequence learning model, we choose CRFs as the learning method. There are some reasons why we choose CRFs. The first reason comes from the nature of the first subtask. This subtask is modeled as a multi-layer sequence learning problem, in which we learn a sequence learning model for each layer, and CRFs is a suitable framework for sequence learning tasks. The second reason comes from the advantages of CRFs. CRFs is a discriminative method, it has all the advantages of Maximum Entropy Markov models (MEMMs) [McCallum et al. 2000] but does not suffer from the label bias problem [Lafferty et al. 2001]. The last reason is that CRFs have been applied successfully to many NLP tasks such as POS tagging, chunking, named entity recognition, syntax parsing, information retrieval, information extraction, analyzing logical structures of legal texts at the sentence level, and so on [Bach et al. 2011; Kudo et al. 2004; Lafferty et al. 2001; Peng and McCallum 2006; Sha 2003]. This section describes our method for recognizing logical structures using integer lin-ear programming (ILP). Suppose that G is a subgraph of G such that G contains all the vertices of G and the degree of each vertex in G is greater than zero, then the set of all the maximal complete subgraphs (or cliques) of G will satisfy the minimal , complete ,and maximal constraints, and also the significant constraint in most cases. We also note that, a set of cliques that satisfies all these four constraints will form a subgraph that has two properties like properties of G .

Let be the set of all such subgraphs G of G , the subtask now consists of two steps. (1) Finding G = argmax G  X  f ( G ) ,and (2) Finding all cliques of G .

Each clique found in the second step will correspond to a logical structure. Figure 7 illustrates two examples of graphs and their cliques. The graph on the left-hand side consists of three nodes 1, 2, and 3, and two cliques { 1, 2 the right-hand side consists of five nodes 1, 2, 3, 4, and 5, and two cliques and { 4, 5 } .

Recently, some researches have shown that integer linear programming (ILP) formulations is an effective way to solve many NLP problems such as semantic role labeling [Punyakanok et al. 2004], coreference resolution [Denis and Baldridge 2007], summarization [Clarke and Lapata 2008], dependency parsing [Martins et al. 2009], and so on. The advantage of ILP formulations is that we can incorporate non-local features or global constraints easily, which are difficult in traditional algorithms. Although solving an ILP is NP-hard in general, some fast algorithms and tools now available. So we can apply ILP to many NLP problems [Martins et al. 2009].
In this work, we exploit ILP to solve the first step. Let N be the number of vertices of G , we introduce a set of integer variables { x ij } 1 values of { x ij } are set as follows. ILP formulations for the first step can be described as the following.

Maximize: Subject to: The last constraint guarantees that there is at least one edge connecting to each vertex in G .

The second step, finding all cliques of an undirected graph, is a famous problem in graph theory. Many algorithms have been proposed to solve this problem efficiently. In this work, we exploit the Bron-Kerbosch algorithm, a backtracking algorithm. The main idea of the Bron-Kerbosch algorithm is using a branch-and-bound technique to stop searching on branches that cannot lead to a clique [Bron and Kerbosch 1973]. Although Bron-Kerbosch is a famous algorithm, we present it here for clarity.
First, we describe three sets, which play an important role in the algorithm (1) The set compsub contains the nodes already defined as a part of the clique. (2) The set candidates contains all the nodes which serve as an extension to compsub (3) The set not contains all the nodes that have already processed at an earlier stage,
The pseudocode of the Bron-Kerbosch algorithm is presented as Algorithm 1. If there is an element in not connecting to all nodes in candidates , we cannot get a maximal clique from the present compsub (because we always miss that element in not ). So the algorithm will terminate as early as possible.

The remaining problem is how to define the value function f . Our solution is that first we learn a binary classifier C . This classifier takes a pair of logical parts as the input and outputs + 1 if two logical parts belong to one logical structure, otherwise it will output  X  1. Then, we define the value function f for two logical parts p (correspond to node i and node j in the graph) as follows. Function f will receive a value from  X  0.5 to + 0.5. This subsection presents machine learning models and features that we use to learn the binary classifier C . Many classification methods have been proposed including tra-ditional methods, such as k -NN [Cover and Hart 1967], decision tree [Safavian and Landgrebe 1991], naive Bayes [Mitchell 1997], and more recent advanced models, such as Maximum Entropy model (MaxEnt) [Berger et al. 1996] and support vector ma-chines (SVMs) [Vapnik 1998]. All of them can be used in our framework. Among these, we chose two classification methods to complete our framework: MaxEnt and SVMs. Both of them have been applied successfully to many NLP tasks. While SVMs are cho-sen because they are a very powerful method, MaxEnt is another good choice. It does not only performs better than SVMs in some particular cases, but also is very fast in both training and inference. In the following, we give a briefly introduction to MaxEnt and SVMs. 4.2.1. Maximum Entropy Model. Maximum Entropy model (MaxEnt) [Berger et al. 1996; Ratnaparkhi 1996] is a method of estimating the conditional probability p model outputs a label y given a context x . where f i ( x , y ) refers to a feature function;  X  i is a parameter of the model; and Z is a normalization factor. To capture statistic information, this method requires that the model accord with some constraints (properties). Among the models that satisfy these constraints, the MaxEnt method chooses the model with the flattest probability distribution (the model with the highest entropy).

To solve the constrained optimization problem, we first convert the primal problem to a dual optimization problem using the method of Lagrange multipliers [Berger et al. 1996]. Then the solution of the dual optimization problem can be found by applying the improved iterative scaling method [Berger et al. 1996; Darroch and Ratcliff 1972] or LBFGS method [Nocedal 1980] 14 .
 Maximum Entropy model has been applied successfully to many NLP task including POS tagging [Ratnaparkhi 1996], chunking [Koeling 2000], syntactic and semantic dependency parsing [Zhao and Kit 2008], statistical machine translation [Berger et al. 1996; Dyer 2009], and so on. 4.2.2. Support Vector Machines. Support Vector Machines (SVMs) is a statistical machine learning technique proposed by Vapnik et al. [Boser et al. 1992; Cortes and Vapnik 1995; Vapnik 1998]. To choose a hyperplane separating samples in a classification task, SVMs use a strategy that maximizes the margin between training samples and the hyperplane. SVMs have been demonstrated their performance on a number of problems in areas, including computer vision, handwriting recognition, pattern recognition, and statistical natural language processing. In the field of natural language processing, SVMs have been applied to text categorization [Joachims 1998], word sense disambiguation [Lee and Ng 2002], text chunking [Kudo and Matsumoto 2001], syntactic parsing [Nivre et al. 2006], semantic parsing [Nguyen et al. 2006], discourse parsing [Hernault et al. 2010], and so on, and achieved very good results.
In the cases where we cannot separate training samples linearly (because of some noises in the training data, for example) we can build the separating hyperplane by allowing some misclassifications. In those cases, we can build an optimal hyperplane by introducing a soft margin parameter, which trades off between the training error and the magnitude of the margin [Vapnik 1998].

SVMs can also deal with non-linear classification problems. First, the optimization problem is rewritten into a dual form, in which feature vectors only appear in the form of their dot products. By introducing a kernel function K product of x i and x j in the dual form, SVMs can solve non-linear cases. The following lists some kinds of kernel functions which are normally used.  X  Linear. K x i , x j = x i  X  x j .  X  Polynomial. K x i , x j =  X  x i  X  x j + r d ,  X &gt; 0.  X  Radial basic function (RBF). K x i , x j = exp  X   X  x i  X   X  Sigmoid. K x i , x j = tanh  X  x i  X  x j + r .
 Here,  X  , r ,and d are kernel parameters. 4.2.3. Features for Learning Binary Classifier. With a pair of logical parts, we extracted the following features (and combinations of them):  X  Categories of two parts.  X  Layers of two parts.  X  The positions of the sentences that contain two parts (the first sentence or not).  X  Categories of other parts in the input paragraph.

Table I shows features in details. Note that, in line number 22, we consider other logical parts in the input paragraph. Each such logical part will correspond to one feature. It is similar to line numbers 23, 24, and 25. Figure 8 shows the framework architecture of our whole system. In the learning pro-cess, the goal is to learn a multi-layer sequence model and a binary classifier. First, information about logical parts and logical structures is extracted from annotated Japanese National Pension Law (JNPL) corpus 15 . Then we conduct preprocessing us-ing CaboCha tool [Kudo and Matsumoto 2002]. The output of this process is then input into two feature extraction modules. Features extracted from sequence feature extrac-tion module are used to learn multi-layer sequence model. To learn this model, we exploited Conditional random fields with CRF++ tool [Kudo 2010]. Features extracted from relation feature extraction module are input into a machine learning model to learn the binary classifier. In this article, we present experimental results with two learning methods. Maximum Entropy model with maxent tool [Tsuruoka 2006] and Support vector machines with LIBSVM tool [Chih-Chung and Chih-Jen 2011; Hsu et al. 2010].
In testing process, the goal is to produce logical parts and logical structures given a new legal paragraph. First we conduct preprocessing on the input legal paragraph. It is then input into the sequence feature extraction module and multi-layer sequence learning model to produce logical parts. Logical parts are input into the relation feature extraction module and binary classifier to generate a weighted graph. From the weighted graph, an ILP formulation is then created. The solution of this ILP formulation produces a subgraph, from which logical structures are extracted using Bron-Kerbosch algorithm.
 We have built a corpus, Japanese National Pension Law (JNPL) corpus, which consists of 83 legal articles 16 of Japanese national pension law. The structure of JNPL is shown in Figure 9. The law consists of articles, articles consist of paragraphs, and paragraphs contain sentences. A sentence may belong to items, sub-items, or sub-sub-items of a paragraph.

Figure 10 shows the relationship between a law sentence and logical parts. A law sentence may contain some logical parts, and a logical part may be embedded in an-other one.

In our corpus, a logical part is annotated with information about its type (kind of part) and formula-id (logical parts with the same id will belong to one logical struc-ture). An example of annotated sentence in the JNPL corpus is shown in Figure 11.
We employed two people 17 in a data-making company, who analyzed and annotated our corpus. The corpus consists of 83 legal articles, which contain 119 paragraphs with 426 sentences. On average, each paragraph consists of 3.6 sentences. The total number of logical parts is 807, and the number of logical structures is 351. On average, each paragraph consists of 6.8 logical parts and 3 logical structures.

We focus on paragraphs in Type A , B ,and C defined in Takano et al. [2010]. In those types, the first sentence of each paragraph is the main sentence, which usually contains more logical parts than other sentences. The other sentences often have a few logical parts, and in most cases these logical parts only appear in one layer. The first sentences usually contain logical parts in two layers.

Table II shows some statistics on the number of logical parts of each type. Main types of parts are A (35.4%), C (30.7%), T 2 (14.1%), ER (7.1%), and EL (6.8%). Five main types of parts make up more than 94% of all types. We divided the JNLP corpus into 10 sets, and conducted 10-fold cross-validation tests for all experiments in this article. For the first subtask, we evaluated the performance of our system by precision, recall, and F 1 scores as follows.
For the second subtask, we used MUC precision, recall, and F in Vilain et al. [1995]. We summarize them here for clarity.

Let P 1 , P 2 , ... , P n be n predicted logical structures, and G answers or gold logical structures. To calculate the recall, for each gold logical struc-ture G i ( i = 1, 2, ... , m ) ,let k ( G i ) be the smallest number such that there exist k predicted structures P i 1 , P i 2 , ... , P i k ( G
To calculate the precision, we switch the roles of predicted structures and gold struc-tures. Finally, F 1 score is computed in a similar manner as in the first subtask.
Table III shows two examples of the evaluation method for Subtask 2. In two examples, we have five input logical parts numbered 1, 2, 3, 4, and 5, and the system predicts three logical structures { 1, 2, 3 } , { 1, 4 } ,and answer (gold) consists of two logical structures { 1, 2, 3, 4 second case, the correct answer consists of three logical structures and { 2, 4 } .

To score the whole system, due to the predicted logical parts may differ from the cor-rect logical parts, we need to modify the MUC scores. Let P ical structure G i ( i = 1, 2, ... , m ) ,let D i be the set of logical parts in G included in the set of predicted logical parts. D i = p  X  the smallest number such that there exist k ( G i ) predicted structures P which satisfy G i  X   X  k ( G i ) j = 1 P i j  X  D i . To calculate the precision, we switch the roles of predicted structures and gold structures.
 Two examples of the evaluation method for the whole system are shown in Table IV. In the first case, the set of predicted logical parts (output predicted by our system in Subtask 1) consists of three logical parts 1, 2, and 3, while the correct logical parts are 1, 2, 3, and 4. Logical part 4 is not included in the set of predicted logical parts. Therefore, when calculating recall, we need to subtract 1 from each factor in the nu-merator. In the second case, the set of predicted logical parts is unchanged, while the correct logical parts are 1, 2, and 4. The set of predicted logical parts does not contain logical part 4, and the set of gold logical parts does not contain logical part 3. Hence, we need to subtract 1 from each factor in the numerator when calculating both recall and precision. 5.3.1. Baseline: Filter-Ranking Perceptron Algorithm. We chose the Filter-Ranking (FR) Per-ceptron algorithm proposed by Carreras et al. [2002] and Carreras and Marquez [2005] as our baseline model because of its effectiveness on phrase recognition problems, espe-cially on problems that accept the embedded relationship 18 algorithm to recognize logical parts in law sentences one by one in an input paragraph.
The idea of the FR-perceptron algorithm is to build a recognition model with two components. The first component, operating at word level, is a filtering function F , which identifies a set of candidate logical parts for an input law sentence s , F is the set of all possible logical parts). The second one, operating at part level, is a score function which produces a real value score for a logical part. The recognizer will use the score function to search an optimal coherent subset from the candidate set F
The filtering component F is only used to reduce the search space. Instead of search-ing in the space P ,the R function only searches in a subset F function F is a begin-end classification for each logical part category: a word is con-sidered as c-begin if it is likely to begin a category-c logical part, and as c-end if it is likely to end a category-c logical part. Each pair of c-begin word w paragraph. Suppose that h c b and h c e are begin and end classification functions for each category c , the filtering function F can be described as follows.

The FR-perceptron algorithm uses linear functions for the begin and end classifi-cation functions in the filtering component, and the score function in the recognition component. To learn parameter vectors for these functions, a perceptron-like algo-rithm was introduced [Carreras and Marquez 2005; Carreras et al. 2002]. The main advantage of the FR-perceptron algorithm is that it can learn parameters for both classification functions and score function simultaneously. Moreover, using dynamic programming for inference is a guarantee for finding the best solution. For begin/end predictors, we got features of words, POS tags, and Bunsetsu tags. In Japanese, a sentence is divided into some chunks called Bunsetsu. Each Bunsetsu includes one or more content words (noun, verb, adjective, etc.) and may include some function words (case-marker, punctuation, etc.) [Murata et al. 2000]. The Bunsetsu tag of a word indicates whether the word appears at the beginning of a Bunsetsu or not. We obtained following features in a window size 2: f [  X  2], f [ words, POS tags, and Bunsetsu tags), f [  X  2] f [  X  1], f [ f [  X  2] f [  X  1] f [0], f [  X  1] f [0] f [ + 1], f [0] f [ + ample, if f is word feature then f [ 0] is the current word, f [ and f [  X  1] f [ 0] is the co-occurrence of them. Moreover, with begin predictor, we use a feature for checking whether this position is the beginning of the sentence or not. Sim-ilarly, with end predictor, we use a feature for checking whether this position is the end of the sentence or not.

With each logical part candidate, we extract following kinds of features. (1) Length of the logical part. (2) Internal structure: this feature is the concatenation of the top logical parts, punc-(3) Uni-gram of words and part-of-speech tags. (4) Bi-gram of words and part-of-speech tags. (5) Tri-gram of words and part-of-speech tags.

Experimental results of the baseline model are shown in Table V. The baseline model achieved pretty good results in three main types of parts: C (69.70%), A (65.30%), and T (69.70%). In all types of parts, the precision score was better than the recall score (80.10% and 61.69% in type C ; 82.35% and 50.91% in type EL; 66.67% and 21.05% in type ER; 79.19% and 55.59% in type A ; 100% and 16.67% in type RepO; 82.14% and 60.53% in type T 2 ; and 50.00% and 8.33% in type T 3 ). Overall, the model achieved 79.70% in the precision score, but only 52.54% in the recall score. These results led to alow F 1 score (63.33%). 5.3.2. Experimental Results of Multi-Layer Sequence Learning Model. We focused on para-graphs in Type A , B ,and C defined in Takano et al. [2010], where the first sentences of paragraphs usually contain logical parts in two layers, and other sentences con-tain logical parts in one layer. We divided sentences into two groups. The first group consists of the first sentences in paragraphs, and the second group consists of other sentences. We set the number of layers k to 2 for sentences in the first group, and to 1 for sentences in the second group. To learn sequence labeling models, we used CRFs [Kudo 2010; Lafferty et al. 2001].

Experimental results on the JNPL corpus are described in Table VI. We conducted experiments with four feature sets: words; words and POS tags; words and Bunsetsu tags; and words, POS tags, and Bunsetsu tags. To extract features from source sen-tences, we used the CaboCha tool [Kudo and Matsumoto 2002], a Japanese morpho-logical and syntactic analyzer. The best model (the model used word and Bunsetsu tag features) achieved 74.37% in the F 1 score. It improves 11.04% in the F in error rate) compared with the baseline model.

Table VII shows experimental results of our best model in more detail. Our model got good results on most main parts: C (78.98%), A (80.42%), and T got low results on the other types of parts. It is understandable because three types of logical parts C , A ,and T 2 make up more than 80%, while six other types only make up 20% of all types. Similar to the baseline model, the precision score was better than the recall score in all types of parts. However, the recall score was improved significantly (69.76% compared with 52.52%).
 5.4.1. Baseline: a Heuristic Algorithm. Our baseline is a heuristic algorithm to solve this subtask on graphs. This is an approximate algorithm which satisfies the minimal , complete , maximal ,and significant constraints (described in Subsection 2.2). The main idea of our algorithm is picking up as many positive edges as possible, and as few negative edges as possible. We consider the following two cases. (1) There is no positive value edge on the input graph, and (2) There are some positive value edges on the input graph.

The heuristic algorithm in the first case is presented as Algorithm 2. In this case, because all the edges have negative values, we build logical structures with as few logical parts as possible. In this case, each logical structure contains exactly two logical parts. So we gradually choose two nodes in the graph with the maximum value on the edge connecting them.

An example of the first case is illustrated in Figure 12. The maximum value on an edge is  X  0.1, so the first logical structure will contain node 1 and node 3. The second logical structure contains node 2 and node 4 19 .
 The main idea of the heuristic algorithm in the second case is described as Algorithm 3. Note that, e ( G ) represents the set of edges and v nodes in a graph G . In this case, we first consider the subgraph which only contains non-negative value edges. In this subgraph, we repeatedly build logical structures with as many logical parts as possible. After building successfully a logical structure, we re-move all the nodes and the edges constituting the logical structure. When we have no positive edge, we will build logical structures with exactly two logical parts.
An example of the second case is illustrated in Figure 13. First, we consider the subgraph with positive edges (graph G 1 in the algorithm). This subgraph consists of five nodes { 1, 2, 3, 4, 5 } and four edges { ( 1, 2 ) , ( structure with three nodes { 1, 2, 3 } . We remove these nodes and the positive edges connecting to these nodes. We have two nodes { 4, 5 } with no positive edges. Now we build logical structures with exactly two nodes.
 The set V consists of nodes that do no appear in the logical parts. In this case, V ={ 4, 5 } . We divide V into two sets V 1 and V 2 . The set V one positive edge connecting to them, and the set V 2 contains nodes that all edges connecting to them have negative scores. In this case, V 1 consider node 4. Among edges connecting to node 4, the edge So we have the second logical structure with two nodes { 2, 4 5, and we have the third logical structure with two nodes 5.4.2. Experimental Results. We conducted experiments on this subtask in two settings. In the first setting (gold-input setting), we used annotated logical parts as the inputs to the system. The purpose of this experiment is to evaluate the performance of the graph-based method on Subtask 2. We used MUC precision, recall, and F evaluate results in this setting. In the second setting (whole-task setting), predicted logical parts outputted by the Subtask 1 were used as the inputs to the system. The purpose of this experiment is to evaluate the performance of our framework on the whole task. We used a modification version of MUC precision, recall, and F (presented in Subsection 5.2) to evaluate results in this setting.

Table VIII shows experimental results on the second subtask in the first setting (gold-input setting). The proposed method using ILP formulation outperformed the baseline (heuristic) algorithm in both experiments with MaxEnt and SVMs. When us-ing MaxEnt as the classification method, our proposed model achieved 79.59% in the F 1 score while the baseline model only achieved 75.89% (improved 3.70% in the F score, 15.35% in error rate). When using SVMs with the RBF kernel as a classification method, the proposed model achieved 80.08% in the F 1 score compared with 74.86% of the baseline model (improved 5.22% in the F 1 score, 20.76% in error rate).
Table IX shows experimental results on the second subtask in the second setting (whole-task setting). Once again, the proposed model using ILP formulation outper-formed the baseline model in both experiments with MaxEnt and SVMs. When using MaxEnt as the classification method, our proposed model achieved 55.73% in the F score while the baseline model only achieved 51.12% (improved 4.61% in the F 9.43% in error rate). When using SVMs with the RBF kernel as classification method, the proposed model achieved 58.36% in the F 1 score compared with 50.47% of the base-line model (improved 7.89% in the F 1 score, 15.93% in error rate).
 This subsection discusses about our framework, the limitation of the system, and in-troduces a simple method to improve the performance of the system.

We proposed a pipeline framework, where the output of the first subtask will be used as the input of the second subtask. Specifically, logical parts outputted from the first subtask are input to the second subtask to build logical structures. The first step will affect to the second step, and therefore contribute significantly to the quality of the whole system. Experimental results showed that our system achieving 80.08% in F score when using correct logical parts, while only achieving 58.36% in F using logical parts recognized in the first subtask. Hence, in this subsection we focus on the first subtask, recognition of logical parts . We take into account the question: How to improve the first subtask, and therefore improve the quality of the whole system? 5.5.1. A Joint Decoding Algorithm. In our multi-layer sequence learning model, logical parts recognized in the first layer will be used as the input sequence to recognize logical parts in the second layer. The limitation of this method is that logical parts in different layers are recognized independently. When we recognize logical parts in the second layer, we can access information about logical parts in the first layer. When recognizing logical parts in the first layer, however, we have no information from logical parts in the second layer. Information is transmitted only in one direction.

A possible solution for this problem is follows. When recognizing logical parts in the first layer, we store N -best outputs (we call them candidates ) and use them to recognize logical parts in the second layer. The candidate that produces the best result (consid-ering outputs in both layers) will be selected. By doing this, we can indirectly access information from logical parts in the second layer when recognizing logical parts in the first layer. A simple version of this method is the joint decoding algorithm for multi-layer sequence learning model, which is presented as Algorithm 4. In this algorithm, the condition to select the best candidate is maximizing the joint probability, which is the product of probabilities outputted by recognition models when recognizing logical parts in two layers.

We conducted experiments on Subtask 1 using the joint decoding algorithm with different values of N (1, 2, 3, 4, 5, 10, 20, 50, 100, and 1000). Experimental results are shown in Table X. When N equals to 1, it becomes the separately decoding algorithm. When N equals to 2, the joint decoding algorithm improved a little bit (0.71% when using word and POS tag features, 0.83% when using word and Bunsetsu tag features, and 0.03% when using word, POS tag, and Bunsetsu tag features). However, there is no difference when we continue to increase the value of N . 5.5.2. Oracle Results. A natural question is that whether or not the list of N -best out-puts ( N -best list) contains the correct output? If the answer is No , we cannot find the correct result by this method. To answer this question, we conducted experiments in oracle setting. In this setting, if the N -best list contains the correct output, we assume that the system can find the output. Otherwise, the system returns the output with the highest probability.

Experimental results in oracle setting are presented in Table XI. When we used the 1000-best list, we got 79.45% in F 1 score (improved 5.08% compared with 1-best list). However, the result was the same for 10000-best list. From experimental results, we have some conclusions as follows.  X  The upper bound of the multi-layer sequence learning model is about 80% in F  X  In most cases, the 1000-best list will contain the correct answer. If the correct an-We have introduced the task of learning logical structures of paragraphs in legal ar-ticles, a new task which has been studied in research on Legal Engineering. There are two main goals of this task: 1) to help people in understanding legal documents; 2) to support other tasks in legal text processing, and therefore make computers be able to process legal texts automatically. We presented the Japanese National Pension Law corpus, an annotated corpus of real legal articles for the task. We also described a two-phase framework with multi-layer sequence learning model. In the first phase, we presented a multi-layer sequence learning model for recognizing logical parts that uses CRFs as the learning method. In the second phase, we proposed a graph-based method for recognizing logical structures and exploited ILP formulation to solve the optimization problem on the graph. To learn weights of graphs, we chose MaxEnt and SVMs as our learning methods.

We conducted 10-fold cross-validation tests on the Japanese National Pension Law corpus. Experimental results showed that our proposed model outperformed a baseline model in both subtasks: recognition of logical parts and recognition of logical struc-tures . For the first subtask, our model achieved 74.37% in the F 63.33% of the baseline model. For the second subtask, when using gold logical parts as the input, our model achieved 80.08% in the F 1 score, compared with 75.89%f the baseline model. When using the output of the first subtask as the input, our model achieved 58.36% in the F 1 score, compared with 51.12% of the baseline model. Our results provide a baseline for further researches on this interesting task.
In the future, we will continue to focus on the first subtask: recognition of logical parts . A direction is to investigate methods that can train recognition models in dif-ferent layers simultaneously. The advantage of such joint learning models is that they can exploit information of logical parts in several layers at the same time. Another direction is to study how to formulate the problem of Learning Logical Structures of Paragraphs in Legal Articles as a single task, in which we can recog-nize logical parts and logical structures simultaneously.

We also investigate how to exploit the results of this task to support other tasks in legal text processing.

