 The popularity of social image sharing platforms ( e.g., Flickr and instagram ) leads to a large number of images accessible online. More importantly, many of these images are annotated by their owners or viewers using freely chosen keywords (also known as tags) for sharing or self-referencing among many other purposes. Many tags refer to high level concepts ( e.g., scene, object, and opinion) which bridges the semantic gap between the low-level visual content of an image and the high-level semantic meaning perceived by human being from the image. The availability of such socially tagged images makes it possible to search for images that best match keyword queries through an interface similar to general Web search engines ( i.e., Tag-based Image Retrieval or TagIR TagIR is challenging and has attracted attention from both academic and industry. On the one hand, the tags are provided by common users from uncon-trolled vocabulary for different purposes of tagging and with different under-standings of the relevance between an image and a tag, or even different under-standings of the semantic meaning of a tag. On the other hand, the keyword queries given by users for image search are usually very short with average 2.2 tags for each search [ 6 ]. One approach to partially addressing the challenge is to group the search results into conceptual clusters. The grouping may be based on the knowledge embedded in the social tagging or other knowledge sources. For example, Fig. 1 gives example outputs for two keyword queries  X  X yramid X  and  X  X ock X  respectively. For the former, image results for Egyptian pyramid and image results for Louvre Pyramid 2 are grouped into two concepts to best match the possible search intent. For the latter, image results are grouped into the concept of rock stone and rock music respectively because of the different semantics of the word  X  X ock X . The search results are obtained from a Concept-Aware Social Image Search system named Casis [ 7 ]. Other example image search systems that summarize search results into conceptual clusters include Flickr tag cluster, Google Image search by subject, and Bing image search. Figure 2 cap-tures the search results of query  X  X ock X  from Bing image search the top row of the search results enumerates the (sub-)concepts of rock such as  X  X ocks and minerals X  and  X  X ock music X , and the column on the right lists the  X  X elated topics X  of the search query.
 images matching user X  X  search intent. However, existing search engines do not support further exploration of search results following user X  X  search intent (which becomes apparent once user clicks the image cluster of her choice). Given a user is interested in an image cluster, for example  X  X ock music X , what other image clusters shall be recommended to the user to view? problem is analogous to the problem of finding/recommending similar images for a given image. However, an image cluster contains much richer information than any single image. The key research issues include: (i) generating candidate image clusters for recommendation, (ii) representing image clusters, (iii) computing similarity or relatedness between two image concept clusters, and (iv) ranking and presenting the recommended image clusters. Among these research issues, we mainly focus on image cluster representation and similarity computation between image clusters. Note that, this problem is different from the problem of finding  X  X elated topics X  of the original query (see the right column of Fig. 2 ). In the latter, the user has not reveal her search intent and the related topics are recommended purely based on the search query ( e.g.,  X  X ock X  in Fig. 2 ). In our problem definition, the image clusters to be recommended are based on the image cluster from the search results that is selected by the user.
 by its low-level visual features and is annotated by tags. Therefore, an image cluster can be represented in two feature spaces, one for low-level visual features and the other for textual keywords. For the former, we first represent image concepts using a vector of concept-specific visual representativeness values. For the latter, an image cluster is represented by the tags derived from all its member images. We further consider the overlaps between two image clusters ( i.e., images that are contained in both clusters) in image similarity computation. Maximal Marginal Relevance (MMR) algorithm is adopted in the ranking process. In our experiments conducted on NUS-WIDE dataset, we evaluated the impact of using visual and semantic features in image concept cluster recommendation and analyzed the results. We believe our findings will pave better understanding on the problem of concept-based image search. Besides social image search, the concept of using multiple similarities for search result recommendation can be applied to search-ing other types of domain-specific resources ( e.g., questions and answers in healthcare).
 The rest of the paper is organized as follows. In Sect. 2 , we survey the related studies. The image cluster recommendation is detailed in Sect. 3 . Section 4 reports the experimental setting, evaluation criteria and the experimental results. Finally, Sect. 5 concludes this paper. Next, we review the related work in concept cluster generation and the study in visual representativeness. Clustering Image Search Results. There are two approaches for grouping images into conceptual clusters when both visual content and tag are considered in the clustering process, namely early fusion [ 9 ]and late fusion [ 4 ]. The former exploits tags and visual content of images jointly in the clustering process and the latter considers one feature space and then the other. There are also approaches which utilize one type of the features only, e.g., tag. In [ 7 ], the Concept-Aware Social Image Search ( Casis ) system detects concepts based on tag co-occurrences using graph clustering algorithm. More specifically, given a query, the best matching images from the image collection and then constructs tag co-occurrence graph based on the frequent tags associated with images in the result set. Graph clustering algorithm ( e.g., BorderFlow) is then employed to partition the graph into concepts, each of which is represented by a small number of representative tags. The tags describing the two concepts shown in Fig. 1 (a) and (b) are { cairo, egypt, giza, sphinx } and { architecture, france, louvre, night, paris } respectively. Images are then retrieved using the representative tags for each concept as shown in Fig. 1 . In this study, we use Casis engine to get concepts and the corresponding image clusters as shown in Fig. 1 . Visual-Representativeness. To recommend visually related image clusters, the best way is to find the visual characteristics of the image cluster, e.g., in terms of color, shape, texture, or others. For instance, the two image clusters in Fig. 1 (a) and (b) share architectures in similar shapes. However, extracting effec-tive and representative low-level visual features remains a challenging research problem [ 3 ]. In [ 5 ], the notion of visual-representativeness is proposed to eval-uate the effectiveness of a tag in describing the visual content of its annotated images. The main idea is to consider whether the set of images associated with a tag expresses visual cohesiveness compared to a similar sized set of images ran-domly drawn from a large image collection. In the following, we briefly review the centroid-based separation and cohesion measures.
 Let D c = { x 1 ,x 2 ,...,x n } be a cluster of n images and collection. The centroid-based separation measure is defined by the following equation: where Cent ( D c ) is the centroid of D c and dist (  X  ,  X  the two centroids represented by visual feature vectors. A larger centroid-based separation means that the image cluster has its visual feature vector more dis-tinguished from the image collection. The centroid-based cohesion measure, on the other hand, reflects whether the images in a cluster are visually cohesive, defined in the following equation.
 Each image cluster in [ 5 ] is defined based on a tag ( i.e., the set of images anno-tated with the tag) so as to evaluate the visual-representativeness of a tag. uate whether an image cluster has certain visual characteristics by computing the separation (and cohesion) measures using different types of visual features including color, textual, edge and other features. In this section, we detail image cluster recommendation. Given an image cluster D mend image clusters ranked by their similarity (or relatedness) to D ing order. Note that, an image cluster D r could be recommended because images in D r demonstrate similar visual characteristics as D c in any of the visual feature space include color, shape, texture, contained object or others, e.g., image clus-ter  X  X unset X  or  X  X each X  can be recommended to the query cluster  X  X unrise X . A cluster may also be recommended because of semantic relation, e.g., image clus-ter of  X  X pple X  or  X  X trawberry X  maybe recommended to query cluster  X  X anana X  for all being fruits. Therefore, both visual similarity and semantic similarity need to be considered in the evaluating the similarity between two image clusters. 3.1 Similarity Between Image Clusters We consider two types of similarities, namely visual similarity and semantic similarity .  X  Visual Similarity. We consider 6 types of commonly used low-level visual features and compute centroid-based separation and centroid-based cohesion on each type of visual features. As the result, a 12-dimensional feature vector is obtained to describe each image cluster. The 6 types of features include 64-D color histogram (LAB), 144-D color auto-correlogram (HSV), 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments (LAB), and 500-D bag of visual words (SIFT) features 4 . Manhattan distance is used in Eqs. 1 and 2 as the distance function. The visual similarity between two image clusters is the cosine similarity calculated on the two 12-dimensional feature vectors.

Note that, we use the visual-representativeness measures instead of using directly these 6 low-level features for capturing the visual characteristics of image clusters. For instance, if one cluster demonstrates strong appearance of red color ( e.g.,  X  X pple X ) and another cluster demonstrates strong appearance of yellow color ( e.g.,  X  X anana X ), we consider these two clusters both demon-strate strong characteristics in terms of color. Using visual-representativeness will therefore be able to recommend this kind of image clusters. On the other hand, two clusters having similar low-level visual features will have similar visual-representativeness values by definition.  X  Semantic Similarity. The semantic similarity between two image clusters is computed based on the tag distributions of the image clusters. More specif-ically, we consider all tags associated with all images in an image cluster. A tag feature vector is built consisting of all unique tags used to annotate any image in the cluster and each tag is weighted by the number of times it is used to annotate the images in the cluster. The semantic similarity between two image clusters is the cosine similarity of the tag feature vectors. With visual similarity and semantic similarity, we are able to get the image clusters that are most similar to the query cluster for recommendation. However, it is observed that two image clusters may share a large number of images, hence the corresponding similarity between them becomes very large regardless of in visual or in semantic feature space. The recommendation becomes meaningless if redundancy is not considered among image clusters. Thus, we reduce the sim-ilarity between concepts if they share a large number of identical images. The ratio of shared identical images is calculated using Jaccard coefficient between the two image clusters.
 To summarize, the similarity between image clusters D c and D by the following equation: ilarity respectively;  X  is a parameter control the weight of two similarity; and J (  X  ,  X  ) denotes the Jaccard coefficient. 3.2 Recommendation The most straightforward approach for recommendation for image cluster D is to rank the similar image clusters in descending order based on the similar-ity defined in Eq. 3 . Then top ranked clusters are recommended depending on the number of clusters needed in recommendation. However, the recommended image clusters may contain many overlaps with image duplications Maximal Marginal Relevance (MMR) that is commonly used in summarization generation to avoid such redundancy [ 1 ]. MMR takes both relevance and novelty into account. An image cluster is recommended if it is similar to the query image cluster and novel with respect to the existing clusters that have already been recommended.
 More specifically, let D c be the query image cluster and S be the set of clusters that have already been recommended. The score of the next image cluster D to be recommended is computed using Eq. 4 where Sim ( D c similarity between D c and D c and max D c  X  S Sim ( D c ,D larity between D c with an image cluster that has been recommended. Parameter  X  balances the two similarity values. 4.1 Dataset The NUS-WIDE dataset [ 2 ] is used in our experiment. The dataset consists of 269,648 socially tagged images from Flickr, annotated by more than 425,000 distinct tags.
 Query and Candidate Image Clusters. In this dataset, 81 concepts (each of which corresponds to a tag) have been manually annotated, including 31 concepts for object and 33 concepts for scene . We randomly selected 30 tags as queries whose image clusters are query clusters for image cluster recommendation. To get the candidate image clusters for recommendation, we used the 2569 popular tags as queries to get all their image clusters as candidate clusters. Each of the 2569 tags has been used to annotate at least 1 % (or 270) images in the dataset. Next, we describe how to obtain image clusters using Casis based on the frequent tags associated with resultant images of a keyword query. The tag co-occurrence graph is then cluster into tag concepts. Each tag concept is a collection of tags, e.g., { cairo, egypt, giza, sphinx concept in Fig. 1 (a). For the 30 tags to be used in testing, 59 tag concepts are obtained; and for the 2569 popular tags, 4660 tag concepts are obtained. We then retrieve the images for each tag concept T c = { t is a tag. It is reported that for multi-tag queries ( e.g., a query contains multiple tags), binary match gives good search results [ 6 ]. We therefore retrieve images that are annotated by all tags in T c . However, depending on the number of tags in a tag concept, there might be limited number of images containing all its tags. We then take images annotated with any combination of m  X  tags in T c . This process continues till there is no tags left in each subset. As the result, the retrieved images are ranked by their matching scores to the tag concepts based on the number of matching tags in descending order. Figure 3 reports the number of images in the resultant image clusters for the 4660 tag concepts from the popular tags. In our experiments, maximum top-500 images in each image cluster are considered and the image clusters with fewer than 100 images are ignored. 4.2 Evaluation Criteria We consider two aspects in evaluating the effectiveness of a recommendation, namely Relatedness and Diversity .
 Relatedness is the number of the recommended image clusters that are related to the query image cluster. For each query cluster, the top-5 recommended image clusters are manually evaluated. The range of relatedness measure is therefore 0 to 5 depending on the number of related image clusters towards the query image cluster.
 Diversity reflects the differences among the recommended image clusters, which is the number of distinct visual topics contained in the recommended image sets against the query image cluster. Similarly, for each query cluster, the value for diversity ranges from 0 to 5. If the value is 0, then all the 5 recommended image clusters are redundant with respect to the query image cluster. A value of 5 means that the five recommended image clusters cover different topics and are distinct from the query image cluster.
 Because relatedness and diversity cover two different aspects, we adopt F -measure as the overall recommendation effectiveness for a query image cluster D .
 4.3 Evaluation Results For each of the 59 query image clusters generated by the 30 randomly selected query tags (see Table 1 ), we evaluate the top-5 image clusters recommended from the more than 4000 candidate image clusters. Throughout the evaluation para-meter  X  is set to 0.7 in Eq. 4 . We evaluate three versions of Eq. 3 by setting  X  to be 1, 0, and 0.5 respectively. That is, we evaluate the impact of considering visual similarity, semantic similarity, and a combined visual and semantic sim-ilarity with equal importance. The evaluation results of relatedness, diversity, and F 1 on the tags of scene category, object category, and both categories are reported in Fig. 4 (a), (b) and (c) respectively.
 Relatedness: Shown in Fig. 4 , visual similarity alone performs poorly on tags of scene category, and slightly better on tags of object category. One possible reason is that images in scene category tend to be more diverse visually, leading to less obvious visual characteristics than images in object category. Semantic similarity achieves much better values for relatedness measure. Combining both visual and semantic similarity does not give better results than semantic similarity alone, although the difference is marginal.
 Diversity: Considering semantic similarity alone, however, results in poorer diver-sity,showinFig. 4 , for tags in both scene and object categories. One possible reason is that users tend to choose general and ambiguous tags when annotating images in order to minimize their efforts in choosing appropriate words [ 8 ]. The general and ambiguous tags make the semantic similarity less effective in identifying different image clusters, based on the noisy tag annotation alone. As expected, combining both visual and semantic similarity, better diversity is achieved.
 F -Measure: Reflected by the F -measure which considers both relatedness and diversity with equal importance, both visual and semantic similarities are essen-tial in image cluster recommendation. Overall, the poorer performance by using visual similarity alone against semantic similarity probably has its root in the ineffectiveness of existing visual features in representing the visual content of the images [ 3 ]. Grouping image search results into conceptual clusters may greatly improves user experiences in image search. However, existing image search engines do not support concept-based image exploration to let users browse the related image clusters of any image cluster that matches user search intent. In this paper, we take the first step to evaluate two different similarities between image clusters from visual aspect and semantic aspect. We conducted manual evaluation using 59 image clusters as query concepts to search for the most relevant image clusters from more than 4000 candidates. Using the two proposed measures, relatedness and diversity, we show that both visual and semantic similarities are essential for recommending image clusters. Our results also show that visual similarity is relatively weak in finding related image clusters. This calls for more study on better ways of representing image clusters in visual space.

