 Display ads on the Internet are often sold in bundles of thousands or millions of impressions over a particular time period, typically weeks or months. Ad serving systems that assign ads to pages on behalf of publishers must satisfy these contracts, but at the same time try to maximize overall quality of placement. This is usually modeled in the literature as an online allocation problem ,where contracts are represented by overall delivery constraints over a fi-nite time horizon. However this model misses an important aspect of ad delivery: time homogeneity. Advertisers who buy these pack-ages expect their ad to be shown smoothly throughout the purchased time period, in order to reach a wider audience, to have a sustained impact, and to support the ads they are running on other media (e.g., television). In this paper we formalize this problem using several nested packing constraints, and develop a tight ( competitive online algorithm for this problem. Our algorithms and analysis require novel techniques as they involve online computa-tion of multiple dual variables per ad. We then show the effective-ness of our algorithms through exhaustive simulation studies on real data sets.
 F. 2. 2 [ Theory of Computation ]: Nonnumerical Algorithms and Problems; H.4.0 [ Information Systems ]: General Online Matching, Ad Allocation, Display Ads, Smooth Delivery
Advertisers purchase bundles of thousands or even millions of display ad impressions from web publishers, and these bundles of-ten represent just one piece of an overall seasonal marketing cam-paign. Think for example of a national clothing store: the summer campaign is carefully designed to reach as many customers as pos-sible through similarly-themed ads on myriad different websites,  X 
This work was done when the author was interning at Google Re-search, New York.
 TV stations and other media. It is essential that customers see a consistent message across media, and that throughout the time pe-riod customers are being exposed to ads.

Display ad serving systems that assign ads to pages on behalf of web publishers must satisfy the contracts with advertisers, respect-ing targeting criteria and delivery goals. Modulo this, publishers try to allocate ads intelligently to maximize overall quality (measured for example, by clicks). This has been modeled in the literature represented by edge weights, and contracts are enforced by overall delivery constraints: advertiser i may not receive more than N impressions. There is a ( 1  X  1 / e ) -approximate algorithm known for this problem using primal-dual techniques. While advertisers certainly appreciate getting high-quality placements, this treatment of the problem could lead to very undesirable allocations; noth-ing in the model prevents the publisher from showing a particular ad only on one day out of a month-long campaign, or only between 1:00 and 2:00am. We note that, most current ad serving systems im-plement the smooth delivery constraint in some form to avoid such scenario, however their techniques do not achieve any bounded ap-proximation guarantee in the adversarial case.
 Our Results and Techniques. In this paper we consider the prob-lem of time-homogenous online ad assignment in a display ad serv-ing system. We introduce smooth delivery constraints, where ad-vertiser i may not receive more than N ( i , t ) impressions before time t . The objective is to maximize the overall weight of assignment while respecting these constraints. (We give a more formal model in the next section.) We give a ( 1  X  1 / e ) -approximate algorithm using primal-dual techniques for the online ad assignment prob-lem under arbitrary smooth delivery constraints in the free disposal model [7]. Building on the previous primal-dual algorithms that maintain one dual variable per advertiser, our algorithm and its analysis require new techniques for handling an entire vector of dual variables for each advertiser. Thankfully the online portion of the algorithm remains simple and can be implemented efficiently.
Our main technical contribution is how we handle the update of multiple, interacting dual variables simultaneously. In previous primal-dual algorithms, each incoming ad is assigned to the adver-tiser that minimizes an adjusted weight, equal to the edge weight minus the dual variable corresponding to that advertiser. The art is then to come up with a dual update rule that will maintain dual feasibility and a bounded gap between the primal and dual solu-tions. The natural extension to smoothness constraints subtracts off an entire vector of dual variables from the edge weight for each ad-vertiser, and thus the design space for update rules is considerably larger, with complex interactions between the dual variables. To overcome this challenge, we derive careful accounting rules for the dual variables, where each variable corresponds to a set of impres-sions, and has a value equal to the exponential average of the set associated with it (or possibly the marginal value of that set over a later set). The heart of the algorithm lies in a merge operation where the impression sets assigned to two dual variables are joined, and we must prove a key technical lemma about a certain property of exponential average functions: for two sequences S 1 and S weighted exponential average  X  1 and  X  2 , the weighted exponential average of their merged sequence is at most  X  1 | S 1 | +  X 
Finally, we study the performance of our algorithms on real data for several web publishers against algorithms in [3, 7, 8], as well as some natural heuristics used in practice to ensure smooth delivery. Because free disposal is a stylized model largely necessitated by worst-case analysis, we look at overall metrics as well as smooth delivery metrics. Our algorithm matches [3, 7, 8] in the total weight of the assignment, it ensures substantially better delivery guaran-tees than both, and improves on the average quality of assignment (average CTR). It is contrary to perception that the delivery guar-antees of our algorithm are substantially better than the heuristics used in practice for smooth delivery, as they are specifically tailored towards this objective. Thus, not only are our algorithms applicable to adversarial settings with traffic spikes, they also achieve superior performance on real-world data sets and metrics.
 Related Work. Our work is closely related to online ad alloca-tion problems, including the Display Ads Allocation (DA) prob-lem [1, 6, 7, 11], and the AdWords (AW) problem [5, 8]. In both of these problems, the publisher must assign online impressions to an inventory of ads, optimizing efficiency or revenue of the alloca-tion while respecting pre-specified contracts. In the AdWords(AW) problem [5, 8], advertiser i has a budget B ( i ) on the total spend, and assigning impression j to advertiser i consumes w ( i budget; whereas in the DisplayAds (DA) problem, advertiser i is associated with capacity N ( i ) , and assigning impression j to ad-vertiser i consumes 1 unit from his capacity. In both problems, the objective is to maximize the total welfare. These problems have been studied in the stochastic as well as the adversarial arrival model. For the adversarial arrival model, a (1-1/e) approximation is known for both problems [5, 7, 8]. In the stochastic arrival model, a ( 1  X   X  ) -approximation has been recently developed using primal-dual techniques [1,5,6,11]. Control-based adaptive algorithms have also been applied in the stochastic model; they achieve asymptotic optimality following an updating rul e inspired by the primal-dual algorithms [10]. We note that none of the stochastic algorithms provide a guaranteed approximation in the adversarial model. As a real world application can experience unexpected traffic spikes, the desirable algorithms should be able to deal with unexpected traffic spikes while performing better for stochastic models [9].
We describe some of the other related work. Alaei et al [2] con-sider the problem of balanced allocation in the context of off-line ad allocation problem. Recently, Chen et al [4] gave a scheme that enables advertisers to alter their bids in real time.

We would like to mention that that none of these results con-sider multiple budgets or capacity constraints per advertiser. Our analysis requires a new set of techniques as we need to deal with computing multiple dual variables per ad in an online manner. Organization We formally define the smooth delivery model for the Display Ads Allocation problem in Section 2. In Section 3, we describe our algorithm and prove the main result. Finally, we highlight important experimental observations in Section 4.
In this section, we formally define the Display Ads Allocation problem under smooth delivery constraint .Thereare n advertisers, A , A 2 ,..., A n . Impressions arrive one-by-one; when an impression arrives, it can be assigned to one of the advertisers. We denote the j th impression by I j and the welfare of assigning I j to A The total number of impressions is m .

The period of impressions X  arrival is split into t intervals and an advertiser specifies the nature of impressions X  delivery over these intervals. In particular, A i does not wish to be assigned more than N ( i , k ) impressions in first k intervals 1 , for each 1 refer to such capacity constraint as the smooth delivery constraint . The objective is to maximize the total welfare of the impressions assigned subject to the advertisers X  capacity constraints.
We denote the number of impressions that arrive in first k in-tervals by M ( k ) , and the interval in which I j arrives by T assume N ( i , k + 1 )  X  N ( i , k ) for each k and N ( i
To enforce the capacity constraints, we apply the free disposal model [7], which says that capacity constraints are not strictly en-forced during the run of the algorithm, rather they are used to deter-mine the final value of the solution. In the smooth delivery setting, that means the following: Let S i be the set of impressions assigned to advertiser i .Nowlet S i  X  S i be the maximum-weight subset such that S i has no more than N ( i , k ) impressions from the first k intervals, for every k . Then, S i is used in the objective function to determine the value of the solution.
In this section, we present a primal-dual based algorithm for the display ads problem with smooth delivery constraints. The LP for the offline matching problem is given below. The variable x is the fraction of I j assigned to A i . The objective function is to maximize total welfare, the first set of constraints are the capac-ity constraints that guarantee the smooth-delivery of impressions, and the second set of constraints indicate that each impression is assigned to at most one advertiser.

For each 1  X  i  X  n , 1  X  k  X  t :  X  1  X  j  X  M ( k ) x ( i , The following LP is its dual.
 For each 1  X  i  X  n , 1  X  j  X  m :  X  T ( j )  X  l  X  t  X  ( i , For each 1  X  i  X  n , 1  X  k  X  t :  X  ( i , k )  X  0
We first illustrate the overall structure of dual variables. There are t beta variables corresponding to an advertiser, one for each of t capacity constraints. A beta variable  X  ( i , k ) goes through three phases during the course of the algorithm: (a) inactive, (b) active, and (c) dead. It is inactive at the start of the algorithm, it becomes active when the first impression in the k th interval arrives and may
In practice, the advertiser only specifies the value of total delivery and its expected delivery pattern. The display ad serving system computes the values of N ( i , k ) for 1  X  k  X  t based on this infor-mation. However, the algorithm and its approximation factor are independent of the way N ( i , k ) s are computed. variable is 0 when it is inactive or dead.

At any given state in the algorithm, for any i , k ,let F the largest integer k such that k &lt; k and  X  ( i , k ) is active. If no such k exists, then we define F ( i , k ) to be 0. During the course of the algorithm, an active beta variable for an advertiser is associated with a (sub)set of impressions (already) assigned to him. If is active when I j arrives, then the set of impressions associated with it is the set of impressions assigned to A i from the ( F interval up to the k th interval. We denote this set by S the sets associated with an advertiser X  X  active beta variables form a partition over impressions assigned to him up to that stage.
We propose three different ways by which the value of an ac-tive beta variable is determined; and they correspond to three algo-rithms that we design, namely (a) smooth-greedy (b) smooth-avg, (c) smooth-exp: we compute a candidate value  X  ( i , k ) for an active beta variable  X  ( i , k ) in these three different algorithms as follows. 2. Smooth-Greedy :  X  ( i , k ) is the smallest weight of an impres-3. Smooth-Avg :  X  ( i , k ) is the average weight of impressions in We illustrate the smooth-avg algorithm in Section 3.1 and its anal-ysis is given in Section 3.2. The smooth-greedy algorithm is ex-actly same as the smooth-avg algorithm (other than computing the value of an active beta variable) and we skip its analysis for lack of space. The smooth-exp algorithm uses the same basic frame-work as the smooth-avg algorithm with certain crucial differences; we illustrate the differences in the smooth-exp algorithm, and its analysis in Section 3.3.

Our results for smooth-greedy and smooth-avg hold even when t is as large as m, i.e. the advertiser can specify his capacity con-straint from the beginning to the arrival of each impression. For the ( 1  X  1 / e ) -approximation, the smooth-exp algorithm makes certain assumptions (to be described later).
We now illustrate the smooth-avg algorithm. At the start of the algorithm, (a) for each i , k ,  X  ( i , k )= 0, (b) the set S N ( each j , 1  X  j  X  m , z j = 0. Algorithm 1 illustrates one iteration of the algorithm: the incoming impression is I j and the current inter-val is k . The algorithm either assigns the impression to one of the advertisers or leaves it unassigned.

Given an iteration of the algorithm, we use  X  st art ( i , to indicate the value of a beta variable at the start and the end of define F st art ( i , k ) , F end ( i , k ) ,  X  st art ( i S end ( i , k ) similarly to indicate corresponding quan tities at the start and the end of the iteration respectively, and F ( S ( i , k ) refer to their current values. We define  X  (  X  (  X  end ( i , k )  X   X  st art ( i , k ) .

We now provide a simple interpretation of the algorithm. We consider an instance with 2 intervals. Up to the end of the first interval,  X  ( i , 1 ) is the average weight of impressions in S Algorithm 1 One Iteration of Smooth-Avg Algorithm
Find advertiser A i which maximizes w ( i , j )  X   X  k  X  l if  X  k  X  l  X  t  X  ( i , l ) &lt; w ( i , j ) then end if Figure 1: Smooth-Avg Algorithm. The left box represents the first interval and the right box represents the second. The height of the shaded area indicates the value of corresponding beta variable. In the second interval, there are two phases. (1) In the first phase, S ( i , 2 ) is the set of top impressions assigned to A i in the second interval. In this phase, erage weight of impressions in S ( i , 1 ) . During this phase, S ( i over  X  ( i , 2 ) . (2) When the average weight of impressions in S ( i , 2 ) average weight of impressions in S ( i , 1 ) for the first time, then (a) the set S ( i , 1 ) merges with S ( i , 2 ) and S ( i , 2 top N ( i , 2 ) impressions assigned to A i so far.

Figure 3.1 illustrates these phases of the algorithm.
The goal of this section is to prove the following theorem:
T HEOREM 3.1. The approximation ratio of the smooth-avg al-gorithm is 1 / 2 .
 We first establish some important properties of beta variables. We call an iteration to be merge-free if no beta variables change their state to dead in the iteration. We note an important property of a merge free iteration.

F ACT 1. In a merge-free iteration of the algorithm, at most two beta variables change their value. Further more, if A i is assigned the impression and the current interval is k, then and remaining beta variables remain unchanged.
 In the following lemma, we establish an important invariant main-tained by the algorithm.

L EMMA 3.1. For any i , l, when a beta variable  X  ( i , l then  X  ( i , l )=  X  l  X  p  X  t  X  ( i , p ) .

P ROOF . Given a beta variable  X  ( i , l ) , we first consider the l terval of the algorithm. In the l th interval,  X  ( i , p )=  X  ( i , l ) remains unchanged from the start of the ( l + 1 ) th until  X  ( i , l ) becomes dead. Hence it suffices to show that, while  X  ( i , l ) is still active,  X 
Consider the j th iteration of the algorithm, and let the corre-sponding interval be k ( &gt; l ) . There are two cases based on the type of the iteration. If that iteration is merge-free, then as active, F ( i , k )  X  l , and by using Fact 1, we get  X  l  X 
Now we consider the case when the j th iteration involves a merge operation. In this case, the algorithm ensures that This proves the lemma.

We now prove the correctness of the smooth-avg algorithm by establishing two properties (a) the dual at the end of the algorithm is feasible, and (b) in every iteration, (the change in the dual)/(the change in the primal)  X  2. The feasibility of the dual solution fol-lows from the following lemma:
L EMMA 3.2. The sum  X  l  X  p  X  t  X  ( i , p ) never decreases during the course of the algorithm for any i , l.

P ROOF . We prove the claim by induction on iterations of the al-gorithm. We consider j th iteration of the algorithm and let k be the corresponding interval. If I j is not assigned to any advertiser, then the claim follows by the induction assumption as no beta variable is changed. Now we consider the case when it is assigned to We show t hat  X  l  X  p  X  t  X  ( i , p ) increases for F end remains unchanged otherwise. The latter follows as the algorithm ensures, Now we show that  X  l  X  p  X  t  X  ( i , p ) increases for F k . If the iteration is merge free and I j is assigned to  X  (  X  ( i , k )) &gt; 0,  X  (  X  ( i , F st art ( i , k )))=  X   X  (  X  ( beta variables remain unchanged. Thus the claim holds in this case. Now we consider the case when the iteration is not merge free. Let {  X  ( i , k 1 ) ,  X  ( i , k 2 ) ,...,  X  ( i , k d ) } (with k of beta variables that die in this iteration. We note that F k 1 for 2  X  l  X  d , F st art ( i , k )= k d and F st art ( i
Applying the precondition for merge operation on the last merge operation of the iteration, we get that the average weight of impres-sions in the set  X  2  X  p  X  d S st art ( i , k p )  X  S ( i S sions in S end ( i , k )=  X  1  X  p  X  d S st art ( i , k p )  X  is greater than  X  st art ( i , k 1 )=  X  k 1  X  p  X  k  X  st art as the other beta variables in the sum are 0. This completes the proof of the lemma.
 We are now ready to prove Theorem 3.1:
P ROOF . (Theorem 3.1) We compute the change in the primal and dual in the j th iteration of the algorithm. If I j remains unas-signed, then the primal and the dual solutions do not change. When it is assigned to A i , the change in the primal is w ( i it suffices to show that the change in the dual is at most 2 w min ) to establish the theorem.

We first consider the case when the j th iteration is merge-free. In this case, by Lemma 3.1, we have  X  (  X  ( i , k ))=  X   X  (  X  ( the total change in the dual solution is Moreover z j = w ( i , j )  X   X  ( i , k )  X  w ( i , j )  X  w in the dual is at most 2 ( w ( i , j )  X  w min ) .

Now we consider the case when the j th iteration is not merge free. Let S st art ( i , k 1 ) , S st art ( i , k 2 ) ,..., with S st art ( i , k ) in the current iteration, such that k As a result, the beta variables  X  ( i , k 1 ) ,  X  ( i , k their state to dead. For simplicity, we denote F end ( i , the iteration by k 0 .
 The value of  X  end ( i , k ) is the average weight of impressions in S Now we compute the average weight of impressions in S end The average weight of impressions in S st art ( i , k l ) |
S st art ( i , k l ) | = N ( i , k l )  X  N ( i , k l  X  1 ) for 1 for an active beta variable  X  ( i , l ) ,  X  st art ( i , l As  X  ( i , p )= 0for p &gt; k ,weget Thus we get we get  X  as other beta variables are 0. Thus we get, The change in the dual is equal to =  X  (  X  ( i , k )) N ( i , k )+  X  = w ( i , j )  X  w min + z j  X  2 ( w ( i , j )  X  w min ) This completes the proof of the theorem.
In this section, we illustrate our result for the smooth-exp algo-rithm. The algorithm uses the same basic framework as the smooth-avg algorithm; we illustrate the differences between the two algo-rithms and its correctness proof in this section.

We first establish an important property of weighted exponential average functions in the following lemma, that will be crucial to the smooth-exp algorithm.

L EMMA 3.3. Given two sequences S 1 and S 2 , with weighted ex-ponential averages  X  1 and  X  2 respectively, the weighted exponen-tial average of their merged sequence S 1  X  S 2 is at most
P ROOF .Let n 1 = | S 1 | , n 2 = | S 2 | ,and n = n 1 + n S nential average is no more than  X  1 n 1 +  X  2 n 2 n property:
P ROPERTY 1. For every 1  X  i  X  n,  X  1  X  j  X  i c j  X   X  1  X   X  for a sequence of size n 1 . Similarly, we define  X  ( i , The weighted exponential averages of S and S are  X  1  X  i  X  order, using Property 1, we get, which proves the lemma. So it remains to establish the existence of sequence S .

Let  X  = n 1 / n . For any sequence Q = q 1 , q 2 , q 3 ,...., r  X | Q | ,wedefine Prefix ( Q , r )=  X  We construct S as follows: for every i , 1  X  i  X  n where
R ( 1 , i )= Prefix ( S 1 , i  X  )  X  Prefix ( S 1 , ( i  X  1 )  X  )
R ( 2 , i )= Prefix ( S 2 , i ( 1  X   X  ))  X  Prefix ( S 2 , ( Now we check properties of S : (a) as S 1 and S 2 are sorted, S is also sorted in an ascending order, and (b) as the sum of first i elements in S is the sum of first i  X  n 1 / n and i  X  n 2 / n elements from S S respectively, it easily follows that S satisfies Property 1.
Now it remains to establish the bound on the exponential weighted average of S . We first note that  X  1 , i.e. the exponential average of S , can be written as = It suffices to show that the weighted exponential average of se-Then, by symmetry, the weighted exponential average of sequence R 2 = R ( 2 , 1 ) , R ( 2 , 2 ) ,..., R ( 2 , n ) is upper bounded by definition of S , we get the required upper bound on its weighed exponential average.

The weighted exponential average of R 1 can be written as a +  X  Let Z 1 ( i ) and Z ( i ) be multipliers of term ( a i  X  a S and R 1 respectively. As  X  1  X  i  X  n  X  ( 1 , i )=  X  1  X  i and  X  i  X  2 , ( a i  X  a i  X  1 )  X  0, it suffice to show that Z each 2  X  i  X  n 1 .Thenwehave We note an important property in the following lemma, it can be easily proven by differentiation.

L EMMA 3.4. For any fixed c , 0  X  c  X  1 , the function ( 1 is non-increasing in n.
 By choosing c =( n 1  X  i + 1 ) / n 1 ,weget Z 1 ( i )  X  Z pletes the proof.
 Algorithm: The algorithm is similar to the smooth-avg algorithm with one exception: the difference occurs in merge operation due to different behavior shown by exponential average function com-pared to  X  X verage X  function. For any two sequences S 1 and S exponential average  X  1 and  X  2 , the merged sequence can have ex-ponential average less than  X  1 | S 1 | +  X  2 | S 2 | | ates issue in the feasibility of dual if we follow the same algorithm as smooth-avg. We handle the merge operation as follows: in the iteration where  X  ( i , k ) merges with  X  ( i , F ( i , signed to A i before the merge operation. Let S ( i , k )= w ( i , j ) \ w min ,and  X  ( i , k ) be its weighted exponential average. Let to  X  ( i , k ) as result of the merge operation  X  | S st art ( i , k ) | +(  X  st art ( i , k )+  X  st art ( Similar to the smooth-avg algorithm, among the variables that are active at the end of the iteration, the value changes for only two of them:  X  ( i , k ) and  X  ( i , F end ( i , k )) .Furthermore, changed such that, we have Thus they continue to hold the marginal over the next active beta variable. Now we note important properties ensured by the algo-rithm.

F ACT 2. In the k th interval, (a) before any merge operation in-volving  X  ( i , k ) , the algorithm ensures  X  ( i , k )=  X  ( merge operation involving  X  ( i , k )  X   X  ( i , k ) , and (c) decreases.

F ACT 3. For any i , l, when a variable  X  ( i , l ) is active, then  X   X  p  X  t  X  ( i , p )  X   X  ( i , l ) .

F ACT 4. During the course of the algorithm,  X  l  X  p  X  t  X  ( not decrease for any i , l.
 It can be checked that these properties continue to hold for a merge-free iteration. Now we consider an iteration in which  X  ( with  X  ( i , F ( i , k )) ; we have: (a) the current interval is k and  X  ( i ,&gt; k )= 0, (b)  X  st art ( i , k )  X   X  st art (c)  X  st art ( i , F ( i , k ))  X   X  st art ( i , k )+  X  st art the weighed exponential average of S end ( i , k ) after merging is upper bounded by  X  | S st art ( i , k ) | +  X  st art ( i , F ( i , k )) | S  X  | S st art ( i , k ) | +(  X  st art ( i , k )+  X  st art ( checked that the merge operation does not increase the value of the dual solution and the dual continues to remain feasible. When more than one merge operation take place in the same iteration for an advertiser, we treat it as a sequence of merge operations such that in step, one beta variable merges with  X  ( i , k ) , and each step is treated in the same way as discussed above. We now establish the performance ratio of the algorithm.

T HEOREM 3.2. If ( N ( i , k )  X  N ( i , k  X  1 )) is large for every i then the performance ratio of the smooth-exp algorithm is 1 / e ) .

P ROOF . We compare the change in the primal and the dual solu-tion in the j th iteration of the algorithm, let k be the corresponding interval. If I j remains unassigned, then the primal and the dual so-lutions do not change. If I j is assigned to A i , then the change in the change in the dual is  X  (  X  ( i , k )) N ( i , k )  X   X  (  X  ( Furthermore, we have  X  (  X  ( i , k ))  X   X  end ( i , k )  X   X  st art ( i , k )  X   X  Thus the change in the dual is Using  X  st art ( i , p )= 0for p &gt; k and  X  st art ( i , change in the dual is bounded by Thus the change in the dual is at most e / ( e  X  1 ) times the change in the primal. When the iteration involves a merge operation, we con-sider two stages involved in the iteration, (a) in the first stage, we add the impression to S ( i , k ) ,  X  ( i , k ) increases and creases and the change in this step is bounded by ( e / ( w min ) , (b) as the value of  X  ( i , F ( i , k )) is now negative,  X  ( i , F ( i , k )) merge (potentially followed by a sequence of more merge operations), and using Lemma 3.3, there is no increase in the dual objective in this step. This completes the proof.
Recall that, the algorithm in [7] requires each advertiser to have a large capacity to get a ( 1  X  1 / e ) approximation. We now show that, even a stronger version of this assumption does not suffice to get a ( 1  X  1 / e ) -approximation in the smooth delivery setting, and the assumption made in Theorem 3.2 is indeed required. We defer its proof to the full version.

L EMMA 3.5. For the Display Ads Allocation problem, no de-terministic algorithm can do better than 1 / 2 , even when N large for every i and large k.
We study the performance of our algorithm on display ads data for a set of 10 anonymous publishers working with Google (Dou-bleClick) display advertising system. The data set is collected over a period of one week for each publisher. The number of impres-sions in a data set range from 200 thousands to 3 millions. Weight of an Advertiser-Impression Pair: We associate the CTR for an impression-advertiser pair with the advertiser X  X  welfare from assign impressions subject to the capacity constraints so that the (expected) number of clicks is maximized.
 The Delivery Model and Penalty: We assume a linear delivery model for every advertiser, i.e. the capacity of A i after arrival of j impressions is N ( i , j )= j m N ( i ) . We measure under-delivery and over-delivery of the assignment at 200 equally spaced milestones during the course of the algorithm.

As there is a penalty for under-delivery of impressions to a pub-lisher, the algorithm should keep the under-delivery as small as pos-sible. Even though there is no penalty for assigning impressions more than the capacity of the advertiser in the free-disposal model, the impressions that are not assigned can be used for other proper-ties in Google, hence it is imperative to keep even the over-delivery of impressions as small as possible.
 Algorithms Compared: We analyze the performance of smooth-avg, smooth-exp and smooth-greedy against the two reference al-gorithms: Pd-Avg: We compare against a baseline online matching algorithm with free disposal [7] considering the overall capacity constraint but ignoring the smooth delivery constraints. In particular, we con-sider the pd-avg online matching algorithm developed by [7]. The algorithm is as follows: For advertiser A i ,let S ( i top N ( i ) impressions assigned to A i ; if there are less than N pressions assigned to A i , then we add elements with weight 0 so that | S ( i ) | = N ( i ) . The algorithm associates a dual variables A , where the value of  X  ( i ) is the average weight of impressions in S ( i ) . The algorithm assigns the impression to the advertiser which maximizes w ( i , j )  X   X  ( i ) . We expect this algorithm to achieve a better total welfare, but more overdelivery/underdelivery as it does not obey the tighter smooth delivery constraints.
 A Heuristic: It is a natural baseline heuristic used in practice that adopts the pd-avg algorithm from [7] to ensure smooth delivery. It characteristics of impressions delivery for a sample data-set. is very similar to the algorithm described above, the only differ-ence is that in this heuristic, before arrival of I j , the capacity of A is considered to be j m N ( i ) (instead of N ( i ) ). Therefore, in the algorithm above, S ( i ) for heuristic algorithm is a set of weights corresponding to the top j m N ( i ) impressions assigned to A . We expect this algorithm to have better delivery guarantee, and worse total weight of the assignment.

We use 7 intervals in the implementation of smooth algorithms, and we measure delivery guarantees at 200 equally spaced mile-stones.

Results and Metrics: The average statistics over 10 data sets can be found in the Table 1. It uses the following set of metrics for the comparison. (1) Total Welfare is the total weight of the assignment regardless of the delivery constraint, relative to the pd-avg algorithm. (2) Capacity Capped Welfare is the total weight of the assignment taking the smooth capacity constraint into consideration, relative to the pd-avg algorithm. This is the most relevant metric. (3) Average edge weight in the assignment, relative to the pd-avg algorithm. (4) Under-delivery/over-delivery at the end of the algorithm as a fraction of the expected delivery. (5) Accumulated under-delivery/over-delivery summed over 200 mile-stones, as a fraction of the accumulated expected delivery.
These statistics for individual data-sets can be found in the Fig-ure 2. We also plot histograms of data-set based on the performance metrics (Figure 2), they show the number of data-sets achieving a certain performance for the given metric relative to the pd-avg algorithm. The under-delivery/over-delivery histograms show the marginal under-delivery/over-delivery compared to the pd-avg al-gorithm, measured as a fraction of the total expected delivery. All other plots measure the performance metric as a fraction of the pd-avg algorithm.

Overall, smooth-exp and smooth-avg algorithms ensure signifi-cantly less over-delivery (by about 15% of the expected delivery) than the pd-avg algorithm and the heuristic, and have a matching under-delivery guarantee. Although they (smooth-exp and smooth-avg algorithms) achieve a marginally lower overall weight of the assignment, they ensure significantly larger average edge weight in the assignment .

Delivery Guarantees of Smooth Algrorithms: It is interesting to note that smooth-avg and smooth-exp have less over-delivery and more average edge weight in the assignment for every data set com-pared to the pd-avg and the heuristic algorithm. This is contrary to expectations since the heuristic is specifically designed to ensure smooth delivery, at a cost of less overall weight of the assignment. While smooth-exp has under-delivery less than the two reference algorithm on all datasets, smooth-avg performs better in 8 out of 10 datasets. Furthermore, smooth-avg has less over-delivery than smooth-exp for every data set.

We plot values of various performance metrics measured at 200 observation points for a sample data-set (Figure 2). The perfor-mance is measured relative to the pd-avg algorithm. In each plot, we can observe 7 concave regions, they correspond to the seven in-tervals. At the beginning of the interval, the current beta variable is Metric 1Interval 7Intervals 49 Intervals Tot al Wel far e 109.41 100 98.66 Average Edge Weight 94.04 100 101.19 Over-delivery 26.29 11.50 9.85 Accumulated Over-delivery 31.68 13.24 9.96 Total Under-delivery 24.91 26.15 26.84 Accumulated Over-delivery 35.52 36.16 36.87 Figure 4: The figure on the left illustrates the reason for the less over-delivery of smooth-algorithms. The figure on the right is the delivery curve for a sample data-set over different values of intervals. zero and hence impressions are assigned as a faster rate. As the cur-rent beta variable increases, the rate of assignment of impressions reduces.
 Smooth-Avg vs Smooth-Exp Algorithm: Even though the smooth-exp algorithm has a better approximation guarantee, its implemen-tation requires storing all impressions assigned to an advertiser so far and the update rules are computationally expensive as it in-volves computing the weighted exponential average of a set. In comparison, the smooth-avg algorithm only needs to maintain the value of beta variables and its update rules are simple. Furthermore, the performance of the smooth-avg algorithm is no worse than the smooth-exp algorithm on data. This makes the smooth-avg algo-rithm a better candidate for practical applications.
 Number of Intervals and Smoothness: We analyze the effect of increasing the number of intervals. Towards this, we use three dif-ferent values of intervals: 1, 7 and 49. We use smooth-avg al-gorithm and measure over-delivery and under-delivery at 200 uni-formly spaced milestones. The average statistics can be found in the Table 2. These statistics for individual data-sets are given in the Figures 2 and 3. We also plot histograms of data-sets based on these performance parameters, and the values of various per-formance metrics measured at 200 observation points for a sample data-set (Figure 3). The performance is measured relative to the smooth-avg algorithm with 7 intervals.

We observe that the overall smoothness of the delivery increases as we increase the number of intervals. Though the average edge weight and the delivery guarantees improve with the increase in the number of intervals from 1 to 7, the corresponding gains by increasing the number of intervals to 49 are marginal and it leads to the drop the total weight of the assignment. We note that the over-delivery for every data-set reduces as we increase the number of intervals from 1 to 7 as well as from 7 to 49.

In the pd-avg algorithm, the value of a beta variables increase from 0 to to its final value by the end of the algorithm and an im-pression is assigned only when its weight is more than the beta variable X  X  value. If we assume an IID model of arrival, then as the value of a beta variable increases, the rate of delivery decreases as less impressions have weight more than the value of the beta vari-able. Thus the effective delivery curve is concave . With multiple intervals, the beta value is reset to 0 multiple times, thus the even-tual delivery curve is made up of multiple small concave regions and its sum of distance form a linear expected delivery is less. (See Figure 1 for an illustration.)
