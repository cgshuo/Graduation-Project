 Anne M. Denton  X  Jianfei Wu Abstract The representation of multiple continuous attributes as dimensions in a vector space has been among the most influential concepts in machine learning and data mining. We consider sets of related continuous attributes as vector data and search for patterns that relate a vector attribute to one or more items. The presence of an item set defines a subset of vectors that may or may not show unexpected density fluctuations. We test for fluctuations by studying density histograms. A vector X  X tem pattern is considered significant if its density histogram significantly differs from what is expected for a random subset of transactions. Using two different density measures, we evaluate the algorithm on two real data sets and one that was artificially constructed from time series data.
 Keywords Pattern mining  X  Pattern significance  X  Significance of classification  X  Vector space representation  X  Gene expression analysis  X  Time series subsequences 1 Introduction Recent decades have not only brought a startling growth in the amount of available data, data collections have also become increasingly diverse. Conventional algorithms in statistics and machine learning often assume that each object under consideration is characterized by a set of attributes and X  X n supervised learning X  X  class label. If the attributes are continu-ous, vector space representations have often been helpful. However, modern real data sets can be much more complex. While there may still be sets of continuous attributes that lend themselves to vector space representations, there may also be a large number of items of information that are known about each object. Item data, as they are considered in market basket research [ 4 ], can be represented as binary variables, and as such, could function as class labels. However, the objective may not be to predict any one or all of these item attri-butes. A user may rather want to know which items or items sets are related to the vector data.

Examples of such a problem statement can be found in Bioinformatics among many other application domains. Results of time course gene expression experiments can be viewed as multiple continuous attributes. At the same time, genes have properties that can be character-ized as items, such as the presence of a protein domains or function. Researchers who conduct gene expression experiments may want to link the experimental outcome with a property that is shared by the genes, rather than working with a simple list of differentially expressed genes. Previous work has demonstrated the relevance of such results to biological researchers [ 18 ]. In our first evaluation we take gene expression data from cell cycle experiments in yeast and look for patterns with respect to the occurrence of protein domains, e.g. the G3DSA domain that is discussed in Sects. 3.3 and 4.2 . This information can help establishing the role of the protein domain as part of the cell cycle process.

Our second evaluation is based on data from combinatorial chemistry experiments on polymeric coatings. Researchers working on coatings try to find chemical compositions that result in desirable outcomes for several experimental quantities. Hence they are interested in patterns involving several or all of these experimental outcomes. Since the data are very noisy, the outcomes for any one combination of input parameters may not be reliable. Our approach allows identifying individual parameter settings (items) or combinations of parameters (item sets) that produce subsets of samples (objects), for which the experimental outcomes (vector data) show strong patterns.

Figure 1 illustrates the problem of interest. Each of the circles represents an object or transaction. The spatial position of the object corresponds to a vector attribute, that is X  X n this example X  X wo-dimensional. In general, a vector attribute is composed of D continuous attributes that are assumed to form a vector space. In Fig. 1 , circles that are solid black represent objects, for which a particular item is present. Only a single item is represented in this image, but the process can be applied to many items. In the left panel the solid black circles are close together. For each of the solid circles all of the other circles can be considered to be  X  X lose X , or neighbors, according to the closeness criteria we develop in Sect. 3.5.2 (i.e., they are within a range of 0.4 of the total in each dimension for at least one of the dimensions). The histogram under the left panel reflects the observation that there are six objects that have the item of interest, each of which has five neighbors that also have the item. The right panel shows objects with the same vector data as the left panel, but the item data are associated with different objects. Although the vector data are identical, the item data look far more distributed, and the histogram shows that relevant objects only have two to four neighbors. The setting on the left side illustrates what we consider a vector X  X tem pattern.

Classification techniques are not inherently designed to test whether the input data are related to the class label but they can be used for the purpose: if the classification outcomes differ significantly from what would be expected by random chance one may assume that input and output are related. Such a test can be applied to a large number of items, and sets of items, as potential class labels. Any item set for which the prediction based on the given vector attribute is significantly better than random can be considered to show a vector X  X tem pattern. We use this approach as a comparison method to our new algorithms and demonstrate that our histogram-based techniques can be more effective and computationally efficient than classification-based techniques.

Several pattern mining algorithms have been developed to work on combinations of con-tinuous and categorical data [ 13 , 42 , 47 ]. The assumption behind such algorithms is that each of the continuous attributes represents a separate fact that may be independently related to any combination of the categorical attributes. This assumption is, however, not always valid. Multiple continuous attributes often represent a coherent concept that may or may not be related to items in the database. Gene expression experiments may be performed as time course experiments, in which different attributes correspond to the same overall experimental condition and differ only in the time that has passed since the beginning of the experiment. Feature vectors in image analysis and word vectors in text mining, are other examples of continuous data for which similarity is normally determined from a combination of many or all of the attributes.
 In our algorithm, we use the existence of an item set as a filter to derive subsets of objects. For each of those subsets we create a density histogram, based on the vector data, that summa-rizes the number of neighbors to each point. Two criteria for a neighborhood are considered. One of them uses the number of points that are within a predefined range for a fraction of the total dimensions. We call this measure subspace-based since it considers membership within any one of a class of subspaces. The other one, which we call product-based, uses the scalar product of the vectors to define the concept of neighborhood. In both cases, we compare the observed density histogram with an expected histogram.

Two strategies are discussed for deriving the expected histogram based on assumptions of randomness. A resampling approach averages over a large number (50 in the evaluation) of randomly selected subsets of the same size as the observed subset. Theoretical approaches are also presented that are based on underlying noise models. For the subspace-based measure the only assumption for creating the noise model is that the dimensions of the vector space are independent. The noise model underlying the theoretical distribution for the product-based approach further assumes that attribute values follow a Gaussian distribution.

To quantify the effectiveness of our approach, we use a data set that is artificially con-structed from time series data in conjunction with random walk data. This evaluation allows us to quantitatively study the effect of parameter choices on the quality of the vector X  X tem pattern identification. We demonstrate efficiency and effectiveness of the presented approaches com-pared with ones that are based on evaluating the outcome of classification. 2 Related work Significance of classification results has been extensively studied, albeit not in the pattern on twofold cross validation, that treats the confusion matrix [ 38 ] as a contingency table. We do not consider permutation-based techniques [ 25 , 41 ], since they would add to the compu-tational complexity of the already computation-intensive classification-based approach and are thereby not competitive, regardless of other concerns that have been raised [ 28 ].
Many algorithms for finding frequent or characteristic patterns have been developed for algorithms consider groups of continuous attributes together as units in the pattern mining process. The concept of vector item patterns was first introduced in [ 48 ]. A related algorithm based on pairwise similarity has been shown to assist in determining relevant functional groups in Escherichia Coli [ 18 ].

Much work has been done on clustering of gene expression data to find the genes that show this work does not directly relate genes to domain or functional information, it is worth not-ing that functional information is sometimes used to improve clustering results [ 33 , 35 ]. The coherence of subsets of genes and samples has been used to define clusters in [ 31 ]. Simulta-neous identification of functional groups and the genes that belong to them has been achieved through biclustering techniques [ 12 ]. Such approaches assume that the experiments fall into multiple groups, which is not assumed in our approach. The significance of clustering results has been studied in the context of validation of clustering approaches [ 50 ], and functional information has been considered in this context [ 8 ]. Expression patterns have also been used to identify differentially expressed genes in time course experiments [ 7 , 17 , 22 , 46 ].
The neighborhood criterion for the subspace-based measure is motivated as follows. It has frequently been observed that the Euclidean distance and other L p norms are problematic in high dimensions [ 2 ], and new distance measures have been developed [ 1 ] that only consider a subset of dimensions over which points are close. We use a similar concept of requiring a fraction of dimensions to be within a given range. We show that such a distance measure can be interpreted as a subspace-based evaluation. In contrast to conventional subspace clustering based on axis-parallel projections [ 3 ], our distance measure evaluates similarity in any one of a class of projections. More importantly our algorithm searches for subsets of the data that show inhomogeneities rather than looking for clusters in the full set of points. Note that the normalization used for the subspace measure is related to quantile normalization [ 9 ]inthat the quantile of a point determines its normalized value, assuming a constant reference distri-bution. Such a normalization results in distances that are closely related to the mass-distance discussed in [ 51 ].

The evaluation on time series data was motivated by a controversy within the time series data mining community: In 2003, Keogh et al. [ 37 ] made the claim that, given the set of all subsequences of a time series, clusters could not be found using k -means or hierarchical clus-tering. While much work has subsequently been done to understand this claim [ 11 , 16 , 24 , 29 ], the finding of data-set-specific patterns in time series subsequence data remains difficult. The question asked in this manuscript is slightly different from the topic of the above-cited work:  X  X iven a random sample of subsequences, can a relationship be established with the label that identifies their sequence of origin? X  From the time series data mining perspective, this may be considered a rather academic question. However, the role of time series data in this manuscript is very practical: the goal is to provide an opportunity for testing the effectiveness and efficiency of the algorithm under a variety of data set compositions. 3 Algorithm 3.1 Vector and item data We consider D continuous attributes, x i  X  R , 0  X  i &lt; D , that are known to be related based on background knowledge, as one  X  X ector X  attribute x with domain dom ( x ) = R D . The set of occurring data points (extant domain) is V  X  R D .Inprinciple,adatasetcanhave arbitrarily many vector attributes. Each of the vector attributes is considered separately in the pattern mining step. Note that the continuous attributes that form vector attribute x do not have to come from the same source. Different combinations of attributes can, furthermore, be considered separately. In the evaluation on gene expression data, we determine patterns involving four sets of experiments as separate vector attributes and also search for patterns involving the full set of all continuous attributes.

Similar to the original formalism of Agrawal et al. [ 4 ] we consider item data as binary attributes B ( i ) , 0  X  i &lt; M , that represent the presence of item i , with M distinct items occurring in the database. Conjunctions of the binary item attributes are considered and the usual downward closure statement based on support remains valid. However, the signifi-cance measure of vector X  X tem patterns does not satisfy the downward closure property. A Generalization to multiple vector attributes is straightforward. 3.2 Outline of the algorithm To find patterns among vector and item data we perform the following steps Normalization Vector data are normalized such that, for random data, each point has on average the same number of neighbors. For the subspace-based density definition in Sect. 3.5 this means that each individual dimension is normalized to follow a constant distribution. Vector data, for which dimensions are independent, will therefore fill the entire space homo-geneously. Details are given in Sect. 3.5.1 . For the product-based density that is discussed in Sect. 3.6 , z-normalization is chosen, such that each dimension of the vector data has mean  X  = 0 and standard deviation  X  = 1.
 Density histogram computation For each item, a density histogram analogous to the bot-tom part of Fig. 1 is calculated. Density histograms are at the center of the algorithm and are discussed in Sect. 3.3 . A support threshold for items is applied. Item sets can be combined using standard a-priori pruning [ 5 ] since their support is unaffected by the vector data. Computation of expected histogram For each item, significance is determined in compar-ison with an expected histogram. The expected histogram can either be calculated based on theoretical considerations or through random sampling. Both approaches are can be used with either of the two neighborhood definitions (subspace-and product-based) and are discussed in Sects. 3.4  X  3.6 .
 Determining significance Once the observed and expected histograms are known, the sig-nificance is determined using a  X  2 goodness-of-fit test. Those domains that are significant at the 5% level are returned as having a strong pattern with the vector data under consideration. Considering multiple vector data sets The process can be repeated for different vector data sets. 3.3 Density histograms The goal of the density histogram computation is to summarize the distribution of data points with respect to the item or item set of interest. Each binary attribute B ( i ) defines a subset of the vector data. For each of these subsets the density is summarized. The sub-relation R ( i ) defined by item B ( i ) is given through the selection (  X  ) and the corresponding set of points is given through a projection (  X  ) to vector attribute V
For each V ( i ) , we summarize the density distribution through a histogram. We assume that a neighbor selector function c has been defined that has the following property
For the density definition of conventional density-based clustering with a uniform kernel, the neighbor selector would be where d is the diameter of the hypersphere that defines a neighborhood, and  X  is the Heaviside step function. Note that c is of type integer and does not satisfy the normalization conditions of a kernel function.

Given c( x , y ), a density histogram of V can be defined such that each x , for which the sum of neighbors is equal to k , contributes 1 to h k where  X  i , j is the Kronecker delta, which is 1 when i = j and 0 otherwise. Figure 2 shows the histogram for a real protein domain (filled columns) and for an average over random sub-sets of transactions (empty columns). The difference between both histograms is significant according to a  X  2 -goodness-of-fit test, as will be further discussed in Sect. 4.2 . That means that for this example we conclude on the existence of a vector X  X tem pattern. 3.4 Determining significance We search for vector X  X tem patterns by comparing observed histograms with their expected counterparts. Hence, we must first determine the expected distribution for the given number of transactions. The noise models that allow determining an expected distribution are dis-cussed in Sects. 3.5 and 3.6 . In both cases, the probability p of a point y being a neighbor of point x does not depend on x . That means that the probability of observing k neighbors is simply governed by a binomial distribution. Given the number of transactions N i that are selected by the item attribute of interest b i , the expected number of observations of density k (i.e. k neighbors to a given point) is: This defines an expected density histogram, with which the observed histogram can be com-pared.

Alternatively, we can derive the expected distribution by averaging over a large number of randomly picked subsets of the actual data (in the evaluation we use 50). This step ensures that the expected distribution appropriately reflects the real distribution even when the dis-tribution of the complete data set deviates from the fully random model. Such deviations can occur even in the noise model for the subspace-based algorithm. The noise model for that algorithm assumes that dimensions are independent. Correlations among attributes can potentially still lead to inhomogeneous distributions in multi-dimensional subspaces and the full space. The gene expression data set also suffers from many missing values.

We compare the observed and expected distribution using a standard  X  2 goodness-of-fit test. Bins at both ends of the distribution are merged until the expected number is at least 5. If intermediate bins have an expected number smaller than 5 then pairs of bins are merged until no more bins have an expected number &lt; 5 (one recommended strategy according to [ 39 ]). A vector X  X tem pattern is considered significant if the  X  2 goodness-of-fit test yields a p value &lt; 0.05.

The process of constructing density histograms and determining their significance does not depend on whether individual items are used as a selection criterion or whether items are combined into item sets. Once an item set has been formed, the existence of all items itself provides a binary attribute that can be used in the same way as a single attribute can. The downward closure properties that are known for item set mining remain valid. The evaluation on coatings data makes substantial use of combining items into item sets. Unfortunately, the density-histogram-based significance measure does not satisfy downward closure. Hence, it is not possible to exclude candidate vector X  X tem patterns for reasons other than that the item support alone is lower than a given support threshold. 3.5 Subspace-based density 3.5.1 Rank-order normalization The basic idea for normalizing attributes such that they are evenly distributed in each dimen-following definition where  X ( x ) is the Dirac delta function and &gt; 0 is a number smaller than all differences between attribute values. Note, however, that this transformation maps equal values into equal values. That means that some attribute values may occur multiple times, and the modeling through a constant distribution is less accurate.

This problem is more serious than it may initially appear: most experimental data are not available with arbitrary precision. The data set we used for the gene-expression evaluation only lists two digits after the decimal point. Considering that the data set has over 7,000 records, some values occur more than 100 times. We cannot uniquely decide on a rank order among these, nor do we expect that the experimental precision is high enough to make such a rank order meaningful. However, assigning the same normalized value multiple times results in a poor fit with the model of a constant distribution. In practice, we assign a random order-ing to the records. This is not expected to increase the inherent error since experiments are typically designed such that the resolution of the output values is higher than the precision of the experiment. 3.5.2 Subspace-based neighbor selector We now introduce a neighbor selector for which we require a fixed fraction of dimensions to match rather than the uniform one of Eq. ( 4 ). We consider a vector y to be a neighbor of vector x if the number of dimensions in which both are similar exceeds a threshold tD , with D being the total number of dimensions and t  X  ( 0 , 1 ] . that for t = 1 this definition requires values in each dimension to satisfy c 1 d ( x i , y i ) = 1. the MAX metric, i.e. the distance is determined by the maximum of distances in individual dimensions. In practice, we found it useful to limit t to t  X  X  0 . 2 , 0 . 8 ] .

The neighbor selector of Eq. ( 8 ) can also be interpreted as being subspace-based. Requiring that a point be close in a fraction of t dimensions is equivalent to saying that the point should be within any one of the axis-parallel projections of a hypercube that satisfy the requirements of the one-dimensional selector c 1 d ( x i , y i ) and the maximum number of dimensions over which projections have been performed ( 1  X  t ) D . Figure 3 illustrates the subspaces for two dimensions in which a minimum of 1 dimension is required to match, corresponding to a t in the interval ( 0 , 0 . 5 ] . In the light shaded areas only one of the dimensions respectively corresponds to regions within the range of the data point x .

It is important for our concept that neighbor distributions of randomly distributed points only depend on the number of points and not on the position of the central point with respect to the normalization volume. For that reason we have to ensure that the volume for which c subspace ( x , y ) = 1 does not depend on x . We do so by shifting this volume such that it does not extend beyond the boundaries of the volume defined by the normalization. The dashed Note how this area exceeds the boundaries of the square with side length 1 over which the normalized data extend. The shaded area illustrates the shifted area, which is entirely enclosed in the square. In general, the one-dimensional neighborhood selector c 1d ( x i , y i ) is defined as
This definition is not symmetric in x and y , i.e. point y may be within range of point x without point x being within range of point y . We can use such a measure since we are only interested in the statistical properties of sets of points, and do not make statements about individual points. Figure 4 shows the subspace setup in 3 dimensions. An example volume with r = 0 . 5and t = 0 . 5 is highlighted. In three dimensions, t = 0 . 5 means that a data point must be within the specified range for at least 2 dimensions. 3.5.3 Expected distribution for subspace-based density The expected distribution for the subspace-based density can be derived as follows. We first need the probability of a data point being within the given range for exactly one axis par-allel projection of the D -dimensional space. Let us assume that the projected space is d -dimensional. The probability of a point being within range r of the total interval [ X  0 . 5 , 0 . 5 ] for any one dimension is p = r . The probability of being within range r for d dimensions is p = r d . We assume that the point is not within range for the remaining D  X  d dimensions. The probability of being in precisely the subspace under consideration is The number of subspaces with d dimensions is D ! / d ! ( D  X  d ) ! . Hence, the probability of finding a point within any subspace of at least d = tD dimensions, where  X   X  is the ceiling operator, is given by the cumulative binomial probability density function where Matlab notation was used in the second line. Together with Eq. ( 6 ) this defines an expected density histogram. In Fig. 5 an averaged distribution is plotted against the theoreti-cal binomial distribution for the gene expression data set. For this plot, imputation was used and missing values were replaced by zeros before normalization. It can be seen that, although the fit is not perfect, p values based on both types of evaluation are closely related.
Although Eqs. ( 11 )and( 6 ) are both governed by underlying binomial distributions, they represent entirely different properties of the system. Equation ( 11 ) makes a statement about one point relative to another point. The probability of the point being within the neighborhood of the other is given by a binomial distribution, because the D dimensions of the difference vector are considered to be independent. The point is only required to be within range for d out of a total of D dimensions. Equation ( 6 ) makes a statement about all points in one vector subset V i . We evaluate the neighborhoods for each one of the points as center. For each center, we know that any one other point is in a neighborhood with probability p . The probability that k points out of the total of N i points are within the neighborhood is given by a binomial distribution. We sample the distribution by picking every point as center once, hence the prefactor N i . 3.5.4 Theoretical considerations regarding parameter choices The subspace-based algorithm allows, in principle, choice of the two parameters, r (range) and t (threshold). Fortunately, one of the parameters can be chosen based on fundamental considerations alone: Our algorithm is expected to be most sensitive to local density fluctua-model with uniform distribution, p depends on input parameters t and r as shown in Fig. 6 for D = 20. The slope of the distribution is largest for rD = d , which can be seen as follows: The maximum of the slope of a cumulative density function is found at the maximum of the corresponding probability density function, which is at D  X  d = ( 1  X  r ) D ,or d = rD for the distribution in Eq. ( 11 ). Since all our data sets have D &gt; 10, we ignore the ceiling operator in d = tD and choose r = t for all of our experiments, regardless of whether tD is an integer. The choice of the remaining parameter r = t is somewhat data-set-dependent. In Sect. 4.4.4 we show that for time series data r = t = 0 . 3 is an appropriate choice. 3.6 Product-based density 3.6.1 Product-based neighborhood selector For the product-based neighborhood selector we consider a vector y to be a neighbor of x if
Figure 7 illustrates the concept. The shaded area represents the volume, within which points are considered neighbors. For t = 0 all those points are neighbors for which the product is positive, i.e. the angle between vectors x and y is acute. Often it is desirable to pick t &gt; 0 to enforce a greater similarity between neighbors. For example, for text data, using the bag-of-words model, for which all elements of the vectors are positive, the product would always be positive. For data that follow a multi-dimensional Gaussian distribution with  X  = 0and  X  = 1, setting t = 1 corresponds to requiring that the component of y in the direction of x is at least as large as one standard deviation. Note that the norm of vector y is on average
The rationale behind using a measure that involves the absolute value rather than cosine similarity or the Pearson correlation coefficient is that in many application domains the absolute value is relevant. In the gene expression example, log expression ratios with a small absolute value may indicate noise. Likewise in the coatings data that we consider, researchers are interested in those coatings that strongly exhibit the properties of interest.
In the following we assume that the data are z -normalized. For z -normalization, the mean is subtracted and vectors are divided by their standard deviation. This normalization is dif-ferent from the rank-order normalization in Sect. 3.5.1 but we will see in the next section that it will also allow us to derive a theoretical density distribution, albeit under more restrictive assumptions for the distribution of noise. 3.6.2 Expected distribution for product-based density To derive an expected distribution for the product-based density measure we assume that data follow a Gaussian (normal) distribution. Many data of interest, including gene expression experiments that are used in our first evaluation, are known to not closely follow a Gaussian distribution. For this reason we use a resampling-based analysis in that part of the evaluation.
For data that follow a Gaussian distribution reasonably well, the use of the theoretical computation of the expected density distribution leads to a substantial speed-up. The proba-bility of a data point y having projected value in the direction of x that is greater than t (i.e. ( y  X  x )/ | x | &gt; t ) can be calculated based on the observation that a distribution that is Gaussian in all dimensions is spherically symmetric. Hence, if all attributes follow a Gaussian distri-bution with  X  = 0and  X  = 1, then the distribution of data points in the direction of x will have those same properties, and the probability of a data point y having ( y  X  x )/ | x | &gt; t is where erf is the error function and the denominator tion is defined as an integral over exp (  X  t 2 ) which corresponds to a Gaussian distribution  X  exp (  X  x 2 /( 2  X  2 )) with  X  = 1 / density histogram. 3.7 Summary of algorithm The complete algorithm can be seen as Algorithm 1. For simplicity, only a single item is considered, but the extension to item sets is straight forward: the output of any frequent item set mining algorithm can be used as input to the algorithm. Since the significance measure does not provide any closure guarantees, the only opportunity for pruning lies in the standard downward closure property of the item set support. Note that the algorithm requires a support threshold of no less than 10 since the  X  2 goodness of fit test requires at least two bins with at least 5 instances. Even at a support threshold of 10 the significance of some vector X  X tem patterns cannot be determined, and those are not returned.

Algorithm 1 describes the randomization variant of the algorithm. An iteration is per-formed over all items and, for each item, densities are evaluated at the location of each point that has the given item.  X  X indPts X  selects those vectors for which the item is present, and  X  X andSubset X  selects a random subset of the same number of points.  X  X umberOfNeighbors X  function is given by a sum over Eq. ( 8 ) for the subspace or ( 12 ) for the product algorithm. The faster version of the algorithm uses Eq. ( 6 ) to determine the comparison distribution based on probability p , which is given in Eq. ( 11 ) for the subspace algorithm, and in Eq. ( 13 ) for the product algorithm. 4 Experimental evaluation The algorithm is implemented in MATLAB, allowing us to access many of the statistics func-tions directly as part of the Statistics Toolbox. For simplicity, we use a bitvector representation for items, since vectors can be handled much more easily in MATLAB than sets. We did not encounter memory limitations for the data sets we consider. 4.1 Classification-based comparison algorithm As a comparison algorithm we use the Classification Tree method that is provided in the Statistics Toolbox in MATLAB. Using two-fold cross-validation we derive a confusion matrix. We interpret the confusion matrix as a contingency table of the predicted vs. actual outcome and calculate the statistical significance of this contingency table using a  X  2 -test with Yates correction [ 49 ]. We can use this direct approach since we only want to test the significance of the relationship between actual and predicted outcomes rather than the signifi-cance of a reported accuracy or other measure as is often the goal when testing the significance of classification outcomes. 4.2 Evaluation on gene expression data In our first evaluation we use gene expression data sets from cell cycle experiments on yeast [ 46 ] as vector data, which are available at [ 45 ]. Table 1 summarizes the properties of the four data sets from separate experiments. So far, biologists who do time-course experiments are faced with cumbersome gene lists that are difficult to interpret. They may notice that several of the genes that are similarly expressed also have certain interesting domains or functional annotations. Such reasoning corresponds to a two-step process, in which the expression data are first analyzed with traditional techniques and relationships with domain or functional data are studied as a separate step. We use this hypothetical two-step process for comparison purposes. Our approach removes the artificial separation of the two steps and directly returns the domain or functional information.
Item data come from the Interpro database, in which information on protein domains, motifs, and other kinds of sequence signatures is collected and combined [ 40 ]. For sim-plicity, we refer to all sequence signatures as domains. Yeast domains are available at [ 44 ]. We limit our study to those domains for which significance can be evaluated without violating the constraint of having at least 5 instances in at least 2 bins, limiting the set to 307 domains.

We evaluate effectiveness by applying the algorithm separately to the four data sets cor-responding to different experiments. For each data set, we determine the domains that are significantly related to the vector data at the 5% level. We then compare the two sets using a  X  2 -test on the contingency table of the results. We use a  X  2 -test with Yates correction for all our tests on contingency tables to account for possibly small cell values [ 49 ]. All results are based on the product algorithm with resampling and t = 0 . 4. As an exam-ple, the results for the Cdc28 data set and the Elu data set have the following contingency table:
The contingency table shows that for almost two thirds of the predictions there is agreement between both data sets. This is a reasonably good result, considering, that the experiments were performed independently. The p value for this contingency table is 4.52E-8, indicating that it would be highly unlikely to get such an overlap accidentally.
 Ta b l e 2 summarizes the mutual overlap and p values for all data sets we considered. The combination of all data sets, resulting in a single data set with 72 columns (All) is also shown. The diagonal represents the number of domains found as significant for each of the techniques. The upper triangular matrix shows the overlap between any two of the experi-ments. Set below is a lower triangular matrix that gives the p value of the contingency table corresponding to the two data sets. p values that are below the 5% significance level are rendered in bold face.

We evaluated values of t from 0 through 2 in intervals of 0.4 as well as three runs of the results with respect to the following conventional approach for all data sets: as comparison approach we created contingency tables of genes that have a particular domain compared with genes that have been identified as cell cycle genes in [ 46 ]. Significance is determined using a  X  2 test with Yates correction. 54 domains are significant in this analysis. Note that the total number of domains that are considered significant is larger for our own evaluation, which identified up to 190 domains as significant. Column  X  X p X  in Table 2 refers to this conventional approach.
We now look at an example domain in more detail. The domain G3DSA:2.130.10.90, which was used as an example in Fig. 2 , occurs 117 times in the yeast data set. Comparing the occurrence of this domain with the Spellman subset, we get the following contingency table:
The p value based on  X  2 -test with Yates correction is 0.071, which is not considered significant, and the number of occurrences of domain G3DSA:2.130.10.90 in the genes from the Spellman set (9 times) is actually smaller than would be expected by chance (15.1 times). In our own analysis, using the combined data set, this domain has p value that is below the precision of the double precision data type in MATLAB and is hence considered significant, which is in accordance with the impression gained from Fig. 2 .

The overall consistency among our results and the consistency with the conventional evaluation give a strong indication that our algorithm is able to extract reliable expression X  domain patterns. Table 2 shows that the relationship between results from our algorithm is consistently significant, even when expression data are from independent experiments. Over-lap is particularly high for comparisons with the full data set. Naturally, any individual data set is dependent of the full data set. The high overlap in results is noteworthy rather because it shows that our algorithm performs well for up to 72-dimensional vector data. 4.3 Evaluation on coatings data As a second evaluation we consider data from high-throughput, combinatorial chemistry experiments on polymeric coatings. The data set was provided by Dr. Dean Webster at the NDSU Center for Nanoscale Science and Engineering [ 19 , 20 ]. For this data set the item data are choices taken in the design of the polymeric materials. Several independent choices are possible, e.g., the target molecular weight or the chemical composition of the polymers may be varied. Each possible combination of those choices is used in the experimental design, corresponding to a factorial experimental design. Vector data are constructed from the exper-imental outcomes of multiple experiments that together determine the quality of the coatings.
The motivation for using vector X  X tem pattern mining techniques on this data is that researchers are interested in coatings that show beneficial properties for a combination of experimental outcomes. Any one of the experiments does not sufficiently characterize the properties of the coatings. The  X  X ector X  concept is well-suited towards representing the com-bined experimental objective through a coherent concept. The representation of experimental input choices as items allows studying groups of experimental settings together. An individual experiment can be used to draw conclusions on a complete set of input parameters. However, the experimental data are noisy and a single data point may not provide reliable informa-tion. Even a small group of experiments may not conclusively indicate, which of the input choices are important for the desired result. The transaction set for a particular item, such as a particular chemical modification, includes many experiments, and is therefore more robust towards noise. If a pattern emerges by only considering one experimental choice, this can give specific insights into that particular choice. Item sets that consist of more than one item, limit the number of experiments but still relate to a class of experimental settings.
The data set [ 19 , 20 ] has the following properties. Each of the experiments, has three input variables: M n for PDMS (molecular weight of polydimethylsiloxane, PDMS), PCL Length (length of polycaprolactone blocks attached to PDMS) and Siloxane Content (percentage of PDMS), as well as six measurements, three of which relate to physical properties before water immersion and the other three to the same properties after immersion. The research goal of the coatings researchers is to develop coatings, for which water immersion leads to desirable changes. For that reason the differences between the experimental measurements before and after water immersion are the relevant experimental output variables: WCAI (Water Contact Angle Increment after water immersion), MICAI (Methylene Iodide Contact Angle Increment after water immersion) and AFI (Average Force at release Increment after water immersion). These three outcomes together form the vector data.
 Items are constructed from the three input variables, M n for PDMS, Siloxane Content and PCL Length. In the original data set, M n for PDMS has nine different possible values, and Siloxane Content and PCL Length each have four values. Currently, our algorithm is only set up to automatically consider conjunctions of the item attributes, as is the case for standard association rule mining. To achieve sufficient support for the statistical evaluation, we group neighboring intervals in a preprocessing step as shown in Table 3 . In principle, the process of forming intervals could be integrated into the pattern mining process. This step, however, depends on the nature of the items as numeric values and would not generalize widely.
Figures 8 , 9 ,and 10 show the experimental and resampled histograms for three example item sets. The middle panel of each figure shows a parallel-coordinate-representation [ 30 ]of the attributes. Each dimension corresponds to one horizontal position and the value for that dimension is given in vertical direction. Each data point is represented by one polyline that connects the values for each of the three dimensions.
Figure 8 shows the results of item set {MN_123, PCL_1} ( M n for PDMS = 2500, 5000 or 7500 and PCL Length = 0), which is statistically significant by both the product algorithm ( p value 3.1E-5) and the subspace algorithm ( p value 1.8E-10). The middle panel shows the three output variables after applying z-normalization per attribute. We can see that both the WCAI and the MICAI are positive for most of the instances in this item set, and AFI is negative or close to zero. This result is of interest to coatings researchers: a coating is considered good if it shows no change or an increase in the water contact angle (WCAI zero or positive) and no change or a decrease for the adhesion force (AFI zero or negative). Since these observations are based on 22 experiments and are shown to be statistically significant, the result is a substantial improvement over simple visual inspection of the data.
Figure 8 is an exceptionally clear example of a vector X  X tem pattern. Many item sets are not considered significant by the algorithms. Figure 9 shows an example of an item set for which the corresponding vector X  X tem pattern is not significant. This item set is defined by the single item PCL_34 (PCL length = 3 or 4).

Figure 8 is defined by a 2-item-set, i.e. two input variables determine the selection of objects, M n for PDMS and Length of PCL. A natural question is whether the corresponding 1-item-sets are significant. Since there are no downward or upward closure guarantees for the significance calculation, we have to consider those item sets separately. Figure 10 shows the results of item set PCL_1 (PCL length = 0, i.e., without addition of PCL blocks). This vec-tor X  X tem pattern is considered statistically significant by the subspace algorithm ( p value 0) but not by the product algorithm ( p value 0.24). The middle panel shows that the patterns observed for this item set are similar to those in Fig. 8 but not nearly as consistent. Similar observations hold for the 1-item-set MN_123 ( M n for PDMS = 2500, 5000 or 7500), which also is insignificant according to the product algorithm ( p value 0.12).

A further observation from Fig. 10 is that the results of the two algorithms are not always identical. In this case the difference between both algorithms can be understood by noting that two values of opposite sign that are both small can be considered as neighboring values by the subspace algorithm, but would decrease the similarity measure of the product algo-rithm. The results of the subspace algorithm overlap largely with those of product algorithm. Only few cases show the kind of discrepancy observed in Fig. 10 , i.e. a vector X  X tem pattern is determined to be significant by the subspace algorithm but not by the product algorithm or vice verse. 4.4 Evaluation on time series data Finally, the algorithms are tested on time series subsequence data. The motivation for this depending on the composition of the data set. Sometimes artificial data are used for this purpose. In this work, we use a data set that is artificially constructed from real data to avoid making assumptions on distributions. While the process of assembling the time series data sets is artificial, any patterns that are found are based on the real distributions in the time series data. Since the time series vary in their properties, the averaging over results from all of them, makes the conclusions less dependent on any one distribution than if we had artificially constructed data based on a single distribution. The need for an evaluation based on artificially constructed data is particularly urgent for our task since labels are unavailable for most practical data sets.

The task of relating time series subsequence data to their time series of origin can be expected to show a substantial degree of difficulty. Clusters of time series subsequences are notorious for being independent from the time series that was used for clustering [ 37 ]as discussed in the Sect. 2 . 4.4.1 Evaluation measures For this evaluation, we extract subsequences from nine different time series as well as from random walk data. The process is illustrated in Fig. 11 . Nine items are constructed, such that they can be expected to show a vector X  X tem pattern with the subsequence data. We will refer to these items as  X  X ime series items X , since each one of them is associated with one of the original time series. The binary item variable is constructed such that it is 1 for the subse-quences of its associated time series and 0 for all other subsequences, including the random walk subsequences. Nine other items, which we call  X  X andomized items X , are constructed through random selection of subsequences.

This setup allows a quantitative evaluation of the algorithms, using classification-style measures. We know that we expect a time series item to be identified as showing a vector X  item pattern. If it is correctly recognized as such, we consider the outcome as true positive (TP). When a time series item is not considered to be related to the subsequence data, we record a false negative (FN) outcome. Likewise, if a randomized item is identified as being related to the time series data we consider the outcome as false positive (FP). A randomized item that is correctly found to not be related to the time series data constitutes a true negative instance (TN). Based on these outcomes, standard classification measures, such as accuracy [(TP+TN)/(TP+FP+TN+FN)], sensitivity [TP/(TP+FN)] and specificity [TN/(TN+FP)] [ 26 ] can be evaluated.

Although the evaluation uses classification concepts it is important to note that the prob-lem as such is not set up as a classification task. The identification of significant vector X  X tem relationships does not require any training. It is also important to distinguish this evaluation step from the classification-based vector X  X tem pattern discovery. For the comparison method discussed in Sect. 4.1 , classification-based evaluation is used in two places: Initially the items are considered as class labels, and the vector data as input attributes. The tree-based classifier is used to predict the item using two-fold cross-validation. The confusion matrix derived from this classification step is interpreted as a contingency table for determining the significance of the vector X  X tem relationship, and a p value lower than 0 . 05 is considered as significant. This prediction is then evaluated in a separate classification-style analysis. A significant rela-tionship is considered as a true positive instance of a vector item pattern provided the item is a time series item. For a random item, a prediction as significant would count as a false positive. 4.4.2 Data sets This evaluation uses the same nine time series as [ 15 ]. Eight of those data sets come from the UCR time series repository [ 36 ], ( buoy sensor , balloon , glass furnace , steamgen , speech , with the data. One time series ( ecg ) was collected independently: The Ecg series (MIT-BIH Arrhythmia Database: mitdb100) originates from PhysioBank [ 23 ].Thesamepreprocess-ing steps were done as in [ 15 ]: the buoy sensor series was compressed by averaging over 4 consecutive values and the ecg series by averaging over 20 consecutive values.

Subsequences are extracted using a sliding window approach, and differences between successive data points are considered as the dimensions of the vector space. Windows of length 17 are used resulting in a 16-dimensional vector space. Different dimensions d = 2 n with n being an integer have been considered, and d = 16 was chosen based on what dimen-sionality can be expected for real data sets. The dimensionality of the gene expression data set was, on average 18, suggesting that d = 16 was a reasonable choice. The dimensionality of the coatings data set was lower. However, given the noisiness of the time series data, the overall accuracy deteriorated with smaller window sizes, and the differences between algorithms became less clear.

Note that through the process of taking differences, the random walk data become random data with a Gaussian distribution. z -normalization is applied to each data point, i.e. the mean is subtracted, and the result divided by the standard deviation. 1000 data points are col-lected from each time series and randomly selected subsets of Npts = 40 subsequences are used in the evaluation, i.e. the absolute support of all items is Npts .Since Nsets = 9 time series are used, Nsets  X  Npts subsequences are non-random. These are combined with Nrand  X  Nsets  X  Npts random subsequences, where the number of batches of random data, Nrand , is varied. A total of 2  X  Nsets items are constructed, half of which are based on the original time series and the other half are assigned by randomly sampling the subsequences. This process is repeated 50 times and all measures are reported as averages over those 50 runs. Unless otherwise stated, the product algorithm uses a threshold value of t = 1 . 8, and the subspace algorithm a threshold values of t = 0 . 3. These choices are motivated in Sect. 4.4.4 . 4.4.3 Effectiveness In this evaluation, we keep the number of points from each of the time series (item support) fixed at Npts = 40 and vary the number of random points in batches of Nsets  X  Npts or 360. Figure 12 shows the accuracy of the product and subspace algorithms, for both the theoretical and resampled distribution, as well as the classification-based evaluation. It can be seen that the accuracy of all variants of our algorithm except one outperform the classification-based comparison algorithm. The subspace algorithm, using the theoretical distribution, does not perform well when few or no random data have been added. This can be understood by not-ing that time series data are likely to show correlations among dimensions that are shared by time series from multiple sources. As random walk data are added, which result in randomly distributed data after normalization, the effect becomes less dominating. The resampling approach takes these fluctuations into account and only considers item sets as significant if their histograms differ significantly from what is found in random subsets. It can be seen that resampling alleviates the problem and the subspace algorithm with resampling outperforms the classification-based comparison algorithm at all evaluation points.

Overall, the product algorithm, which is not directly affected by the correlations, still performs better. Notice also that the product approach provides similarly good results for the theoretical and resampled distribution. This information is very valuable since the execution time of both algorithms differs by approximately a factor of n , with n being the number of resampled histograms used (50 in this evaluation).

The subspace measure was designed for applications, in which some dimensions may not be involved in patterns, as is the case for the coatings data set. The time series subsequence extraction process inherently treats all dimensions equal and can, therefore, not benefit from this quality. For the coatings data the subspace algorithm was able to find patterns that could not be found by the product algorithm.
 Data sets are constructed such that the number of positive and negative items is equal. Hence, accuracy is an acceptable measure of effectiveness of the algorithms. Nevertheless, it is worth looking at sensitivity and specificity separately to understand where the strengths and weaknesses of the respective algorithms lie. Figure 13 shows that for the histogram-based algorithms the sensitivity is consistently high, but for no or few random data, the specificity is low. This observation supports the statement that the low accuracy of the subspace algorithm at low fractions of random data is due to false positives rather than false negatives. These false positives can be explained through correlations in the fluctuations of the attributes. The sensitivity of all histogram-based algorithms remains stable even for little or no added random data. 4.4.4 Choice of parameters Both the subspace and the product algorithms have one free parameter. The subspace algo-rithm inherently could be considered as having two, the range within which a point is con-sidered neighbor within any one dimension as well as the minimum fraction of dimensions for which a point has to be a neighbor. However, we showed in Sect. 3.5.4 that those should be considered equal from theoretical considerations alone.

We determine the choice of the one remaining threshold parameter based on accuracy for three data set compositions. Figure 14 shows the results: The diamond symbol represents the data set composition of the second data point of Fig. 12 ,andthe + symbol those of the last data point. The  X  symbol corresponds to item support 160 and 2 batches of random data. It can be seen that overall a threshold parameter of 1.8 provides the highest accuracy for the product algorithm, and a threshold parameter of 0.3 for the subspace measure. The results that are reported earlier in this section are based on these parameter values. It should, how-ever, also be noted that, especially for the product measure, other parameter choices provide similarly high accuracy for at least some of the data set compositions. 4.5 Performance The algorithm scales linearly with the number of items and does not depend on the size of the data set. Figure 15 shows that for the evaluation of Fig. 12 the computation time for all histogram-based algorithms remains constant. The computation time of the classifica-tion-based algorithm, in contrast, does increase with the overall size of the data set. The histogram-based approaches using theoretical distributions are almost an order of magni-tude faster than the classification-based approach even for the smallest data set size. The resampling-based approaches, are slower by about a factor of 50 as expected by the need of calculating 50 resampled histograms. That makes them slower than the classification-based approach for all but the largest data set size. Note, however, that the accuracy results in Fig. 12 show that for this data set there is no need to use a resampling-based evaluation. 5 Conclusions We have presented algorithms for data mining of vector X  X tem patterns based on density histograms. Subsets of the vector data, which are defined by the item data are the basis for the histograms. Two density measures are used, one based on subspaces, as well as a product-based measure. Histograms based on the observed densities at data points are compared with their expected counterparts. Effectiveness and efficiency of our approach have been demon-strated using cell-cycle gene expression data, coatings data and time series subsequence data. For time series subsequence data we have shown that our algorithm outperforms a classifica-tion-based comparison approach in both accuracy and computation time. This work presents an approach toward relating continuous vector data and item data that is potentially valuable in many application areas.
 References Author Biographies
