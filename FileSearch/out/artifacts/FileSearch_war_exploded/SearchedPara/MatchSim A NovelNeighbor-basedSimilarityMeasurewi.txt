 The problem of measuring similarit y between web pages arises in man y imp ortan t Web applications, suc h as searc h engines and Web directories. In this pap er, we prop ose a novel neigh bor-based similarit y measure called Matc hSim, whic h uses only the neigh borho od structure of web pages. Technically , Matc hSim recursiv ely de nes similarit y between web pages by the average similarit y of the maxim um matc h-ing between their neigh bors. Our metho d extends the tradi-tional metho ds whic h simply coun t the num bers of common and/or di eren t neigh bors. It also successfully overcomes a sev ere coun terin tuitiv e loophole in SimRank, due to its strict consistency with the intuitions of similarit y. We give the computational complexit y of Matc hSim iteration. The accuracy of Matc hSim is compared with others on two real datasets. The results sho w that the metho d performs best in most cases.
 Categories and Subject Descriptors: H.3.3 Information Searc h and Retriev al: Clustering; Information ltering General Terms: Algorithms, Measuremen t Keyw ords: Similarit y Measure, Link Analysis, Web Min-ing, Graph Algorithm
In the past few years, man y popular Web applications have been requiring e ectiv e and ecien t algorithms to au-tomatically estimate web page similarities, suc h as searc h engines (e.g., Google's \similar pages" service) and web page classi cation services (e.g., Yaho o! Directory). According to the di eren t kinds of input, there are basically two comple-men tary approac hes: text-b ased and link-b ased .
The text-b ased metho ds, originated from IR (information retriev al), use the textual con ten t of web pages to extract similarities. The most notable are the cosine similarity and the TFIDF mo dels [17]. New metho ds have been prop osed for various of domains [3, 16, 5]. A problem of these metho ds is that they usually require large storage and long comput-ing time due to the need for full-text comparison, whic h causes serious scalabilit y problem when dealing with huge amoun t of and exp onen tially gro wing web pages. The link-based metho ds use the hyperlinks whic h are mo delled by the web graph , with vertices corresp onding to web pages and di-rected edges to the hyperlinks.

In this pap er, we focus on the neighb or-b ased metho ds, a subset of the link-b ased metho ds, whic h share a simple intuition that \web pages are similar because they have sim-ilar neigh bors" . Therefore, the main task of these metho ds is to estimate the similarit y between groups of neigh bors. The other subset, the graph-b ased metho ds, consider the whole structure of the web graph. These metho ds include the Maximum Flow/Minmum Cut [14] that originate from graph theory , the Companion [4] whic h are deriv ed from the HITS algorithm [10], and PageSim [13] whic h is based on the feature propagation of web pages, etc. Relativ ely, the neighb or-b ased metho ds are usually much easier to imple-men t and faster in running time.

The motiv ation of this work is to dev elop ecien t and ef-fectiv e neigh bor-based similarit y measures for the Web ap-plications. The main con tribution of the pap er is that we prop ose a novel neigh bor-based similarit y measure called MatchSim whic h recursiv ely de nes the similarit y between web pages by the average similarit y score of the maxim um matc hing between their neigh bors. Moreo ver, exp erimen ts on two real datasets are conducted whic h demonstrates the good performance of our metho d.

The rest of the pap er is organized as follo ws. Section 2 gives a brief review on related work. Section 3 describ es the Matc hSim algorithm in detail, including the de nition, computation, and time complexit y. Section 4 and 5 rep ort the exp erimen tal results and conclude the work resp ectiv ely.
The link structure of the Web, whic h has been greatly in uenced by researc h in the elds of social net work and ci-tation analysis, has been widely used to exploit imp ortan t information inheren t in the Web. Successful link-based al-gorithms include PageRank [15] and HITS.

A num ber of link-based similarit y measures have been prop osed in the past few years. Traditional algorithms in-cludes Co-citation [18], biblio graphic coupling [9], and Jac-card Measure [7]. The Maximum Flow/ Minimum Cut and Authority algorithms were dev elop ed for measuring the sim-ilarit y of scien ti c pap ers in a citation graph [14]. The Sim-Rank algorithm was prop osed to measure similarit y of the structural con text \in any domain with object-to-ob ject re-lationships" [8]. It is a recursiv e re nemen t of co-citation, based on the assumption that \two objects are similar if they are referenced by similar objects" . The Jac card mea-sure [7] and Adamic/A da [1] were also applied to the link prediction problem underlying social net work evolution us-ing only link information in [12]. Interested readers may refer to [12], whic h con tains an exhaustiv e list of link-based similarit y measures.

A review, or even a listing of all the uses of similarit y measures is imp ossible. We summarize the de nitions of some well-kno wn neigh bor-based metho ds in Table 1, whic h are latter used to compare with Matc hSim in performance exp erimen tally . In the table, sim ( a; b ) denotes the similarit y score between pages a and b , and I ( a ) and O ( a ) the set of in-link and out-link neigh bors of web page a resp ectiv ely. The notations will be used throughout the pap er.
 Bibliographic Coupling j O ( a ) \ O ( b ) j
Traditional neighb or-b ased metho ds lack of exibilit y since they only consider how man y exactly same (and/or di eren t) neigh bors two pages have. The recen t prop osed SimRank mak es an extension by taking also the similar neigh bors into accoun t. More precisely , SimRank de nes the similarit y be-tween pages by the average of the overall similarit y scores between their neigh bors.

For example, given the graph snipp et and kno wn simi-larit y scores in Fig. 1, we want to measure the similarit y score sim ( a; b ) between pages a and b . Traditional metho ds will rep ort that sim ( a; b ) = 0 since the num ber of com-mon neigh bors is 0, whic h is inaccurate. SimRank outputs sim ( a; b ) = 2 (0 ; 1) is a deca y factor. However, there is a sev ere loophole. If we remo ve the most similar neigh bors ( a 2 sim ( a; b ) increases to sim ( a 1 ; b 1 ) = 1 = 0 : 6 , whic h is ob-viously coun terin tuitiv e.

The problem results from the overall sum of similarit y scores between neigh bors. In fact, we can see that a and b are similar simply because their neigh bors are quite matche d , i.e., neigh bors ( a i ; b i ) ( i = 1 ; 2) are similar resp ectiv ely.
This idea is inspired by the exp erience that people esti-mate how similar two objects are by the similarities of their pairwise \matc hed features" (same or similar features). Be-cause measuring the similarit y between one person's nger and others' hair mak es no sense. In web graph, pages are objects, and their neigh bors are their \features" . The simi-larit y between two neigh bors re ects how matc hed they are. Therefore, measuring similarit y between pages by the simi-larities of their (pairwise) matche d neigh bors would be more reasonable. Figure 1: Measuring similarit y between a and b based on their neigh bors. ( sim ( a 1 ; b 1 ) = 0 : 6 , sim ( a 1 ; b 2 ) = sim ( a 2 ; b 1 ) = 0 : 1 , sim ( a 2
Next question is how to nd those matche d neigh bors. It can be easily mo delled by the classic weighte d assignment problem , in whic h two groups of neigh bors form a bipartite graph, and the similarit y scores between them are weigh ts. The aim is to nd a matc hing between the neigh bors with maxim um sum of similarit y scores.

The assignmen t problem has been studied for man y years, and various algorithms have been dev elop ed to implemen t it. This pap er adopts the famous Kuhn-Munkres (K-M) algo-rithm [11] (also kno wn as the Hungarian metho d). We refer to article [6] for a complete overview of nding maxim um matc hings in bipartite graphs.
We mo del the Web graph as a directed graph G = ( V; E ) with vertices V represen ting web pages v i ( i = 1 ; 2 ; ; n ) and directed edges E represen ting hyperlinks among web pages. Giv en two pages a and b in a web graph of size n , we obtain a weigh ted bipartite graph G a;b = ( I ( a ) + w ( u; v ) = sim ( u; v ). Based on the recursiv e intuition of \similar pages have similar neigh bors" , Matc hSim measures the similarit y between pages by \the average similarit y of the maximum matching between their neigh bors" . Formally , the Matc hSim score between two di eren t pages a and b is de ned by In the cases that j I ( a ) j = 0 or j I ( b ) j = 0, since there is no way to infer any similarit y, we simply de ne sim ( a; b ) = 0. If a = b , we have sim ( a; b ) = 1, whic h is obviously .
In Eq. (1), c W ( a; b ) denotes the weigh t of maxim um matc h-ing between I ( a ) and I ( b ), i.e., where m ab is a maxim um matc hing between I ( a ) and I ( b ). c
W ( a; b ) can be calculated using algorithms for the assign-men t problem. This pap er adopts the Kuhn-Munkres (K-M) algorithm. Since the K-M algorithm alw ays con vert I ( a ) and I ( b ) to be \equally-sized" before computing m ab , we de ne l ab , j m ab j = max ( I ( a ) ; I ( b )). Obviously , any matc hing between I ( a ) and I ( b ) is of size l ab . Therefore, in Eq. (1), a maxim um matc hing between the neigh bors of a and b .
For a graph G of size n , we compute the n 2 Matc hSim scores iterativ ely. For eac h iteration k , we can keep the n scores sim k ( ; ), where sim k ( a; b ) is the score between a and b in iteration k . We successiv ely compute sim k +1 ( ; ) based on sim k ( ; ). That is, on eac h iteration k + 1, we update the sim k +1 ( a; b ) using the similarit y scores from the precious iteration k . Formally speaking, we compute sim k +1 ( a; b ) from sim k ( ; ) as follo ws: where c W k ( a; b ) is computed based on the scores sim k
The Matc hSim computation starts with sim 0 ( a; b ) = 1 for a = b and sim 0 ( a; b ) = 0 for a 6 = b . The Matc hSim score between a and b is de ned as lim k !1 sim k ( a; b ). We pro ved that the limiting values exist and are unique, i.e., the Matc hSim iteration con verges. Due to space limitation, the detailed pro of of con vergency is omitted in this pap er. In our exp erimen ts, the Matc hSim computation con verges within 15 iterations.
Time Complexit y . For any two pages a and b in a web graph G = ( V; E ) of size n , we adopt the K-M algorithm to compute c W ( a; b ) in Eq. (1), and so the corresp onding time In eac h iteration, Matc hSim invokes the K-M algorithm n times. Supp osing there are a total of K iterations and let L = max a;b 2 V ( l ab ) = max a 2 V ( Ia ), the time complexit y of Matc hSim is thus O ( Kn 2 L 3 ).
 Space Complexit y . Matc hSim has to store n 2 Matc h-Sim scores. Moreo ver, the K-M algorithm invoked needs to store the similarit y matrix of two pages, the size of whic h is O ( L 2 ). Therefore, the space complexit y of Matc hSim is O ( n 2 ) + O ( L 2 ) = O ( n 2 + L 2 ). In the exp erimen ts, we compare the accuracy of Matc h-Sim( MS ) to those of sev eral well-kno wn neigh bor-based meth-ods, including Co-citation( CC ), Bibliographic Coupling( BC ), Jaccard Measure( JM ), and SimRank( SR ).
We run the algorithms on the follo wing two di eren t kind of real-w orld datasets. All text in our datasets is in English. 1. The Computer Web (CW) dataset is a set of web 2. The Google Scholar (GS) dataset is a citation http://sc holar.go ogle.com
For any vertex v in graph G , a similarit y measure A would pro duce a list of top N vertices most similar to v (excluding v itself ), whic h is denoted by top A;N ( v ). Let the sym bol scor e A;N ( v ) denote the average score to v of the top A;N ( v ). We consider the average value of scor e A;N for all v 2 V as the qualit y of the top N results pro duced by algorithm A , whic h is denoted by ( A; N ). That is, ( A; N ) = (
A good evaluation of the similarit y measures is dicult without performing extensiv e user studies or having a reli-able ground truth. In this pap er, we use two di eren t evalu-ation metho ds as rough metrics of similarit y to measure the accuracy of the algorithms. For the CW dataset, we use the cosine TFIDF, a traditional text-based similarit y function. For the GS dataset, we use the \Related Articles" pro vided by Google Scholar. Certainly , neither of these metrics are guaran teed to be perfect, but based on our observ ation, they are satisfying generally . (1) Cosine TFIDF Similarit y: The cosine TFIDF sim-ilarit y score of two web pages u and v is just the cosine of the angle between TFIDF vectors of the pages [2], whic h is de ned by where W tu and W tu are TFIDF weigh ts of term t for web pages u and v resp ectiv ely. k v k denotes the length of page v , whic h is de ned by k v k =
Therefore, for the CW dataset, we de ne and T ( A; N ) = ( A; N ) whic h measures the average co-sine TFIDF score of top N similar web pages returned by algorithm A . (2) Related Articles: For an article v in citation graph G , the list of its \Related Articles" returned by Google Scholar is denoted by RA ( v ). We de ne related N ( v ) = f top N related articles v i j v i 2 RA ( v ) \ V g :
The precision of similarit y measure A at rank N is:
Therefore, for the GS dataset, we simply de ne and P ( A; N ) = ( A; N ) whic h measures the average pre-cision of algorithm A at top N . (3) Overall Accuracy(O A) and Relativ e OA(ROA): OA and ROA are designed for measuring the \overal accu-racy" of an algorithm A over the top N rankings, and the \relativ e overall accuracy" between two algorithms A and B , resp ectiv ely. The de nitions are as follo ws.
 OA ( A; N ) = 1 Figure 2: Accuracy curves of the neigh bor-based similarit y measures on the GS dataset Figure 3: Accuracy curves of the neigh bor-based similarit y measures on the CW dataset
In this part, we compare the accuracy of Matc hSim with other neigh bor-based similarit y measures on the CW and GS datasets. The de nitions of the algorithms are given in Section 2. We set = 0 : 8 in SimRank.

Figures 2 and 3 plot the curv es of P ( A; N ) and T ( A; N ) on the GS and CW datasets, resp ectiv ely. To compare the overall accuracy of the algorithms with that of Matc hSim, we also sho w the ROA ( ; M S; 50) values of the algorithms in Table 2. From the results, we can see that Matc hSim outp erforms all the other algorithms in almost all cases on both of the GS and CW datasets in term of accuracy .
To e ectiv ely measure similarit y between web pages, we prop ose a novel link-based metho d called MatchSim , whic h recursiv ely de nes the similarit y between web pages by the average similarit y of the maximum matching between their resp ectiv e neigh bors. Exp erimen ts on two di eren t real-world datasets are conducted to sho w the e ectiv eness of the metho d.

There are a num ber of avenues for future work. (1) The eciency of Matc hSim needs to be impro ved to enable it to cop e with the entiret y of the Web. Possible approac hes in-clude neigh borho od pruning and appro ximation algorithms. (2) Matc hSim can be easily extended to the \bipartite" ver-sion, whic h is applicable to the recommender systems. In fact, this is our on-going work.
The work describ ed in this pap er is supp orted by gran ts from the Researc h Gran t Council of the Hong Kong Spe-cial Administrativ e Region, China (Pro ject No.: CUHK 4128/08E and CUHK 4158/08E). This work is also aliated with the Microsoft-CUHK Join t Lab oratory for Human-cen tric Computing and Interface Technologies.
