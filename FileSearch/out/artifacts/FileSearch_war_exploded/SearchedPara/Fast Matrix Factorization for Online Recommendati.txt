 This paper contributes improvements on both the effective-ness and efficiency of Matrix Factorization (MF) methods for implicit feedback. We highlight two critical issues of ex-isting works. First, due to the large space of unobserved feedback, most existing works resort to assign a uniform weight to the missing data to reduce computational com-plexity. However, such a uniform assumption is invalid in real-world settings. Second, most methods are also designed in an offline setting and fail to keep up with the dynamic nature of online data.

We address the above two issues in learning MF models from implicit feedback. We first propose to weight the miss-ing data based on item popularity, which is more effective and flexible than the uniform-weight assumption. However, such a non-uniform weighting poses efficiency challenge in learning the model. To address this, we specifically de-sign a new learning algorithm based on the e lement-wise A lternating L east S quares (eALS) technique, for efficiently optimizing a MF model with variably-weighted missing data. We exploit this efficiency to then seamlessly devise an incre-mental update strategy that instantly refreshes a MF model given new feedback. Through comprehensive experiments on two public datasets in both offline and online protocols, we show that our eALS method consistently outperforms state-of-the-art implicit MF methods. Our implementation is available at https://github.com/hexiangnan/sigir16-eals. Matrix Factorization, Implicit Feedback, Item Recommen-dation, Online Learning, ALS, Coordinate Descent
User personalization has become prevalent in modern rec-ommender system. It helps to capture users X  individualized NExT research is supported by the National Research Foundation, Prime Minister X  X  Office, Singapore under its IRC@SG Funding Initiative.
 preferences and has been shown to increase both satisfac-tion for users and revenue for content providers. Among its various methods, matrix factorization (MF) is the most popular and effective technique that characterizes users and items by vectors of latent factors [15, 32]. Early work on MF algorithms for recommendation [14, 27] have largely fo-cused on explicit feedback, where users X  ratings that directly reflect their preference on items are provided. These works formulated recommendation as a rating prediction problem for which the large volume of unobserved ratings ( i.e., miss-ing data) are assumed to be extraneous for modeling user preference [12]. This greatly reduces the modeling workload, and many sophisticated methods have been devised, such as SVD++ [15] and timeSVD [14].

However, explicit ratings are not always available in many applications; more often, users interact with items through implicit feedback , e.g., users X  video viewing and prod-uct purchase history. Compared to explicit ratings, implicit feedback is easier to collect for content providers, but more challenging to utilize due to the natural scarcity of negative feedback. It has been shown that modeling only the ob-served, positive feedback results in biased representations in user profiles [4, 12]; e.g., Marlin et al. [18] finds that users listen to music they expect to like and avoid the genres they dislike, leading to a severe bias in the observed data.
To solve the problem of lacking negative feedback (also known as the one-class problem [21]), a popular solution is to model all the missing data as negative feedback [12]. However, this adversely degrades the learning efficiency due to the full consideration of both observed and missing data. More importantly, the low efficiency makes it even more dif-ficult to deploy implicit MF method online [29]. In practical recommender systems where new users, items and interac-tions are continuously streaming in, it is crucial to refresh the underlying model in real-time to best serve users. In this work, we concern the above two challenging problems of the MF method  X  implicit feedback and online learning. We note that we are not the first to consider both aspects for MF, as a recent work by Devooght et al. [4] has proposed an efficient implicit MF method for learning with dynamic data. However, we argue that Devooght X  X  method [4] models miss-ing data in an unrealistic, suboptimal way. Specifically, it assigns a uniform weight to the missing data, assuming that the missing entries are equally likely to be negative feedback. However, such an assumption limits model X  X  fidelity and flex-ibility for real applications. For example, content providers usually know which items have been frequently featured to users but seldom clicked; such items are more likely to be true negative assessments and should be weighted higher than others. In addition, Devooght X  X  method learns param-eters through gradient descent, requiring an expensive line search to determine the best learning rate at each step.
We propose a new MF method aimed at learning from im-plicit feedback effectively while satisfying the requirement of online learning. We develop a new learning algorithm that efficiently optimizes the implicit MF model without impos-ing a uniform-weight restriction on missing data. In par-ticular, we assign the weight of missing data based on the popularity of items, which is arguably more effective than the previous methods [4, 12, 23, 30, 31] that are limited by the uniformity assumption. Our eALS algorithm is fast in accounting for missing data  X  analytically K times faster than ALS [12] where K denotes number of latent factors  X  the same time complexity with the recent dynamic MF solution [4]. This level of efficiency makes eALS suitable for learning online, for which we develop an incremental update strategy that instantly refreshes model parameters given new incoming data. Another key advantage of eALS is that it works without learning rate, bypassing the well-known difficulty for tuning gradient descent methods such as [4] and Stochastic Gradient Descent (SGD) [25].

We summarize our key contributions as follows. 1. We propose an item popularity-aware weighting scheme 2. We develop a new algorithm for learning model pa-3. We conduct extensive experiments with both offline
Handling missing data is obligatory for learning from im-plicit data due to the lack of negative feedback. To this end, two strategies have been proposed  X  sample based learn-ing [21, 25] that samples negative instances from missing data, or whole-data based learning [12, 30] that treats all missing data as negative. Both methods have pros and cons: sample-based methods are more efficient by reduc-ing negative examples in training, but risk decreasing the model X  X  predictiveness; whole-based methods model the full data with a potentially higher coverage, but inefficiency can be an issue. To retain model X  X  fidelity, we persist in the whole-data based learning, developing a fast ALS-based al-gorithm to resolve the inefficiency issue.

For existing whole-data based methods [4, 12, 23, 30, 31], one major limitation is in the uniform weighting on missing entries, which favors algorithm X  X  efficiency but limits model X  X  flexibility and extensibility. The only works that have con-sidered non-uniform weighting are from Pan et al. [20, 21]; however their cubic time complexity w.r.t. K makes it un-suitable to run on large-scale data [23], where a large number of factors needs to be considered to gain improved perfor-mance [31].

To optimize MF, various learners have been investigated, including SGD [14, 25], Coordinate Descent (CD) [4, 32], and Markov Chain Monto Carlo (MCMC) [26]. SGD is the most popular one owing to the ease of derivation, however, it is unsuitable for whole-data based MF [12] due to the large amount of training instances (the full user X  X tem inter-action matrix is considered). ALS can be seen as an in-stantiation of CD and has been widely used to solve the whole-based MF [12, 20, 21, 30]; however, its inefficiency is the main obstacle for practical use [23, 31]. To resolve this, [23] describes an approximate solution to ALS. Recently, [4] employs the Randomized block Coordinate Descent (RCD) learner [28], reducing the complexity and applying it to a dy-namic scenario. Similarly, [31] enriches the implicit feedback matrix with neighbor-based similarly, followed by applying unweighted SVD. Distinct from previous works, we propose an efficient element-wise ALS solution for the whole-data based MF with non-uniform missing data, which has never been studied before.

Another important aspect for practical recommender sys-tem lies in handling the dynamic nature of incoming data, for which timeliness is a key consideration. As it is pro-hibitive to retrain the full model online, various works have developed incremental learning strategies for neighbor-based [13], graph-based [9], probabilistic [3] and MF [4, 5, 17, 27] methods. For MF, different learners have been studied for online updating, including SGD [5, 27], RCD [4] and dual-averaging [17]. To our knowledge, this work is the first at-tempt to exploit the ALS technique for online learning.
We first introduce the whole-data based MF method for learning from implicit data, highlighting the inefficiency is-sue of the conventional ALS solution [12, 21]. Then we describe eALS, an element-wise ALS learner [26] that can reduce the time complexity to linearity w.r.t. number of factors. Although the learner is generic in optimizing MF with all kinds of weighting strategies, the form introduced is costly in accounting for all missing data and is thus unre-alistic for practical use. This defect motivates us to further develop eALS to make it suitable for learning from implicit feedback (details in Section 4.2).
We start by introducing some basic notation. For a user X  item interaction matrix R  X  R M  X  N , M and N denote the number of users and items, respectively; R denotes the set of user X  X tem pairs whose values are non-zero. We reserve the index u to denote a user and i to denote an item. Vector p denotes the latent feature vector for u , and set R u denotes the set of items that are interacted by u ; similar notations for q i and R i . Matrices P  X  R M  X  K and Q  X  R N  X  K denote the latent factor matrix for users and items.

Matrix factorization maps both users and items into a joint latent feature space of K dimension such that interac-tions are modeled as inner products in that space. Mathe-matically, each entry r ui of R is estimated as: The item recommendation problem is formulated as estimat-ing the scoring function  X  r ui , which is used to rank items. Note that this basic model subsumes the biased MF [14], commonly used in modeling explicit ratings: where b u ( b i ) captures the bias of user u (item i ) in giving (receiving) ratings. To recover it, set p u  X  [ p B u ,b q i  X  [ q to make notations simple and also to enable a fair compari-son with baselines [4, 12] that also complied with the basic model.

To learn model parameters, Hu et al. [12] introduced a weighted regression function, which associates a confidence to each prediction in the implicit feedback matrix R : J = where w ui denotes the weight of entry r ui and we use W = [ w ui ] M  X  N to represent the weight matrix.  X  controls the strength of regularization, which is usually an L 2 norm to prevent overfitting. Note that in implicit feedback learning, missing entries are usually assigned to a zero r ui value but non-zero w ui weight, both crucial to performance.
Alternating Least Square (ALS) is a popular approach to optimize regression models such as MF and graph reg-ularization [10]. It works by iteratively optimizing one pa-rameter, while leaving the others fixed. The prerequisite of ALS is that the optimization sub-problem can be analyti-cally solved. Here, we describe how Hu X  X  work [12] solves this problem.

First, minimizing J with respect to user latent vector p u is equivalent to minimizing: where W u is a N  X  N diagonal matrix with W u ii = w ui . The minimum is where the first-order derivative is 0: where I denotes the identity matrix. This analytical solution is also known as the ridge regression [23]. Following the same process, we can get the solution for q i .
As we can see, in order to update a latent vector, in-verting a K  X  K matrix is inevitable. Matrix inversion is an expensive operation, usually assumed O ( K 3 ) in time complexity [12]. As such, updating one user latent vector takes time O ( K 3 + NK 2 ). Thus, the overall time complex-ity of one iteration that updates all model parameters once is O (( M + N ) K 3 + MNK 2 ). Clearly, this high complexity makes the algorithm impractical to run on large-scale data, where there can be millions of users and items and billions of interactions.

Speed-up with Uniform Weighting . To reduce the high time complexity, Hu et al. [12] applied a uniform weight to missing entries; i.e. , assuming that all zero entries in R have a same weight w 0 . Through this simplification, they can speed up the computation with memoization: where W 0 is a diagonal matrix that each diagonal element is w . As Q T Q is independent of u , it can be pre-computed for updating all user latent vectors. Considering the fact that W u  X  W 0 only has |R u | non-zero entries, we can compute Eq. (4) in O ( |R u | K 2 ) time. Thus, the time complexity of ALS is reduced to O (( M + N ) K 3 + |R| K 2 ).

Even so, we argue that the O (( M + N ) K 3 ) term can be a major cost when ( M + N ) K  X  |R| . In addition, the O ( |R| K 2 ) part is still much higher than in SGD [25], which only requires O ( |R| K ) time. As a result, even with the ac-celeration, ALS is still prohibitive for running on large data, where large K is crucial as it can lead to better generaliz-ability and thus better prediction performance. Moreover, the uniform weighting assumption is usually invalid in real applications and adversely degrades model X  X  predictiveness. This thus motivates us to design an efficient implicit MF method not subject to uniform-weights.
The bottleneck of the previous ALS solution lies in the ma-trix inversion operation, which is due to the design that up-dates the latent vector for a user (item) as a whole. As such, it is natural to optimize parameters at the element level  X  optimizing each coordinate of the latent vector, while leav-ing the others fixed [26]. To achieve this, we first get the derivative of objective function Eq. (2) with respect to p  X  X  component of latent factor f . By setting this derivative to 0, we obtain the solution of p uf :
Similarly, we can get the solver for an item latent factor:
Given the closed-form solution that optimizes one param-eter with other fixed, the algorithm iteratively executes it for all model parameters until a joint optimum is reached. Due to the non-convexity of the objective function, critical points where gradients vanish can be local minima.

Time Complexity . As can be seen, by performing opti-mization at the element level, the expensive matrix inversion can be avoided. A raw implementation takes O ( MNK time for one iteration, directly speeding up ALS by eliminat-ing the O ( K 3 ) term. Moreover, by pre-computing  X  r ui we can calculate  X  r f ui in O (1) time rather than O ( K ). As such, the complexity can be further reduced to O ( MNK ), which is the same magnitude with evaluating all the user X  item predictions.
We first propose an item-oriented weighting scheme on the missing data, and follow with a popularity-aware weight-ing strategy, which is arguably more effective than the uni-form weighting for the recommendation task. Then, we de-velop a fast eALS algorithm to optimize the objective func-tion that significantly reduces learning complexity compar-ing with the conventional ALS [12] and generic element-wise ALS learner [26]. Lastly, we discuss how to adjust the learn-ing algorithm for real-time online learning.
Due to the large space of items, the missing entries for a user are a mixture of negative and unknown feedback. In specifying the weight w ui of missing entries, it is desired to assign a higher weight to the negative feedback. However, it is a well-known difficulty to differentiate the two cases. In addition, as the interaction matrix R is usually large and sparse, it will be too consuming to store each zero entry an individualized weight. To this end, existing works [4, 12, 23, 30, 31] have applied a simple uniform weight on missing entries, which are, however, suboptimal and non-extendable for real applications.

Considering the ease of content providers in accessing neg-ative information of the item side ( e.g., which items have been promoted to users but receive little interaction), we believe it is more realistic to weight missing data based on some item property. To capture this, we devise a more fine-grained objective function as follows: where c i denotes the confidence that item i missed by users is a true negative assessment, which can serve as a means to encode domain knowledge from practitioners. It is clear that the first term denotes the prediction error of the observed entries, which has been widely adopted in modeling explicit ratings [15, 27]. The second term accounts for the miss-ing data, which acts as the role of negative instances and is crucial for recommendation from implicit feedback [12, 25]. Next, we present a domain-independent strategy to deter-mine c i by leveraging a ubiquitous feature of modern Web 2.0 systems.
Existing visual interfaces of many Web 2.0 systems show-case popular items in their recommendations. All other fac-tors being equal, popular items are more likely to be known by users in general [10], and thus it is reasonable to think that a miss on a popular item is more probable to be truly irrelevant (as opposed to unknown) to the user. To account for this effect, we parametrize c i based on item X  X  popularity: where f i denotes the popularity of item i , given by its fre-quency in the implicit feedback data: |R i | / P N j =1 |R c determines the overall weight of missing data. Exponent  X  controls the significance level of popular items over un-popular ones  X  when  X  &gt; 1 the weights of popular items are promoted to strengthen the difference against unpop-ular ones; while setting  X  within the lower range of (0 , 1) suppresses the weight of popular items and has a smoothing effect. We empirically find  X  = 0 . 5 usually leads to good results. Note that the uniform weighting is a special case by setting  X  to 0 with w 0 = c 0 /N .

Relationship to Negative Sampling . Our proposed popularity-aware weighting strategy has the same intuition with Rendle X  X  popularity-based oversampling [24] for learn-ing BPR, which basically samples popular items as nega-tive feedback with a higher probability. However, [24] em-pirically shows the oversampling method underperforms the basic uniform sampler. We suspect the reason comes from the SGD learner, which will result in more gradient steps on popular items, due to oversampling. As a result, popular items may be over-trained locally at the expense of less pop-ular items which would then be under-trained. To resolve this, tricks like subsampling frequent items [19] and adaptive learning rates like Adagrad [6] have been adopted in other domains. As the focus of this paper is on whole-data based implicit MF, we do not further explore the details of SGD. It is worth pointing out that our proposed eALS learner avoids these learning issues by an exact optimization on each model parameter.
We can speed up learning by avoiding the massive re-peated computations introduced by the weighted missing data. We detail the derivation process for p uf ; where the counterpart for q if is achieved likewise.

First, we rewrite the p uf update rule Eq. (5) by separating the observed data part:
Clearly, the computational bottleneck lies in the summa-tion over missing data portion, which requires a traversal of the whole negative space. We first focus on the numerator: By this reformulation, we can see that the major computa-tion  X  the P N i =1 c i q if q ik term that iterates over all items  X  is independent of u . However, a na  X   X ve implementation repeatedly computes it unnecessarily, when updating the la-tent factors for different users. Clearly, we can achieve a significant speed-up by memoizing it.

We define the S q cache as S q = P N i =1 c i q i q T i , which can be pre-computed and used in updating the latent factors for all users. Then, Eq. (9) can be evaluated as: which can be done in O ( K + |R u | ) time.

Similarly, we can apply the cache to speed up the calcu-lation of denominator:
X
To summarize the above memoization strategy, we give the update rule for p uf with the use of S q cache: p Algorithm 1: Fast eALS Learning algorithm.
 Input : R , K ,  X  , W and item confidence vector c ;
Output : Latent feature matrix P and Q ;
Randomly initialize P and Q ; for ( u,i )  X  X  do  X  r ui  X  Eq. (1) ; . O ( |R| K ) while Stopping criteria is not met do 5 for u  X  1 to M do . O ( MK 2 + |R| K ) 6 for f  X  1 to K do 7 for i  X  X  u do  X  r f ui  X   X  r ui  X  p uf q if ; 8 p uf  X  Eq. (12) ; . O ( K + |R u | ) 9 for i  X  X  u do  X  r ui  X   X  r f ui + p uf q if ; 12 S p  X  P T P ; . O ( NK 2 ) 13 for i  X  1 to N do . O ( NK 2 + |R| K ) 14 for f  X  1 to K do 16 q if  X  Eq. (13) ; . O ( K + |R i | ) end return P and Q Table 1: Time complexity of implicit MF methods.
 Method Time Complexity ALS (Hu et al. [12]) O (( M + N ) K 3 + |R| K 2 ) BPR (Rendle et al. [25]) O ( |R| K )
IALS1 (Pil  X aszy et al. [23]) O ( K 3 + ( M + N ) K 2 + |R| K ) ii-SVD (Volkovs et al. [31]) O (( M + N ) K 2 + MN log K )
RCD (Devooght et al. [4]) O (( M + N ) K 2 + |R| K ) eALS (Algorithm 1) O (( M + N ) K 2 + |R| K ) Similarly, we can derive the update rule for q if : q where s p kf denotes the ( k,f ) th element of the S p cache, de-fined as S p = P T P .

Algorithm 1 summarizes the accelerated algorithm for our element-wise ALS learner, or eALS . For convergence, one can either monitor the value of objective function on train-ing set or check the prediction performance on a hold-out validation data.
Time Complexity . In Algorithm 1, updating a user latent factor takes O ( K + |R u | ) time. Thus, one eALS iter-ation takes O (( M + N ) K 2 + | R | K ) time. Table 1 summarizes the time complexity (of one iteration or epoch) of other MF algorithms that are designed for implicit feedback.
Comparing with the vector-wise ALS [12, 21], our element-wise ALS learner is K times faster. In addition, our pro-posed eALS has the same time complexity with RCD [4], being faster than ii-SVD [31], another recent solution. RCD is a state-of-the-art learner for whole-data based MF, which performs a gradient descent step on a randomly chosen la-tent vector. Since it requires a good learning rate, the work [4] adaptively determines it by a line search in each gradient step, which essentially chooses the learning rate that leads to the steepest descent among pre-defined candidates. A major advantage of eALS has over RCD is that it avoids the need for a learning rate by an exact optimization in each param-eter update, arguably more effective and easier to use than RCD. The most efficient algorithm is BPR, which applies the SGD learner on sampled, partial missing data only.
Computing the Objective Function . Evaluating the objective function is important to check the convergence of iterations and also to verify the correctness of implementa-tion. A direct calculation takes O ( MNK ) time, requiring a full estimation on the R matrix. Fortunately, with the item-oriented weighting, we can similarly exploit the sparseness of R for acceleration. To achieve this, we reformulate the loss of the missing data part that causes the major cost: By reusing S q and the prediction cache  X  r ui , we can calculate the objective function in O ( |R| + MK 2 ) time, much faster than with direct calculation.

Parallel Learning . The iterations of eALS can be easily parallelized. First, computing the S caches (line 4 and 12) is the standard matrix multiplication operation, for which modern matrix toolkits provide very efficient and parallelized implementation. Second, in updating the latent vectors for different users (line 5-11), the shared parameters are either independent with each other ( i.e.,  X  r ui ) or remaining un-changed ( i.e., S q ). This nice property means that an exact parallel solution can be obtained by separating the updates by users; that is, letting different workers update the model parameters for disjoint sets of users. The same parallelism can also be achieved in updating item latent vectors.
This is an advantage over the commonly-used SGD learner, which is a stochastic method that updates model parame-ters given a training instance. In SGD, different gradient steps can influence with each other and there is no exact way to separate the updates for workers. Thus, sophisti-cated strategies are required to control the possible losses introduced by parallelization [7]. Our proposed eALS solu-tion optimizes by coordinate descent where in each step a dedicated parameter is updated, making the algorithm em-barrassingly parallel without any approximate loss.
In practice, after a recommender model is trained offline on historical data, it will be used online and will need to adapt to best serve users. Here, we consider the online learning scenario that refreshes model parameters given a new user X  X tem interaction.

Incremental Updating . Let  X  P and  X  Q denote the model parameters learnt from offline training, and ( u,i ) denotes the new interaction streamed in. To approximate the model parameters in accounting for the new interaction, we per-form optimization steps for p u and q i only. The underlying assumption is that the new interaction should not change  X  and  X  Q too much from a global perspective, while it should Algorithm 2: Online Incremental Updates for eALS.

Input :  X  P ,  X  Q , new interaction ( u,i ) and its weight w
Output : Refreshed parameters P and Q ;
P  X   X  P ; Q  X   X  Q ; ;
If u is a new user do Randomly initialize p u ;
If i is a new item do Randomly initialize q i ; r ui  X  1;  X  r ui  X  Eq. (1); w ui  X  w new ; while Stopping criteria is not met do 6 update user( u ); ; . O ( K 2 + |R u | K ) 7 update cache( u , S p ); ; . O ( K 2 ) 8 update item( i ); ; . O ( K 2 + |R i | K ) 9 update cache( i , S q ); ; . O ( K 2 ) end return P and Q change the local features for u and i significantly. Particu-larly, when u is a new user, executing the local updates will force p u close to q i , which meets the expectation of latent factor model. The new item case is similar.

Algorithm 2 summarizes the incremental learning strat-egy for eALS. For the stopping criteria, our empirical study shows that one iteration is usually sufficient to get good re-sults. Moreover, it is important to note that after updating a latent vector, we need to update the S cache accordingly.
Weight of New Interactions . In an online system, new interactions are more reflective of a user X  X  short-term inter-est. Comparing to the historical interactions used in offline training, fresh data should be assigned a higher weight for predicting user X  X  future action. We assign a weight w new each new interaction (line 4 of Algorithm 2) as a tunable pa-rameter. Later in Section 5.3, we investigate how the setting of this parameter impacts online learning performance.
Time Complexity . The incremental update for a new interaction ( u,i ) can be done in O ( K 2 +( |R u | + |R It is worth noting that the cost depends on the number of observed interactions for u and i , while being independent with number of total interactions, users and items. This lo-calized complexity make the online learning algorithm suit-able to deployment in industrial use, as the complex software stack that deals with data dependencies [29] can be avoided.
We begin by introducing the experimental settings. Then we perform an empirical study with the traditional offline protocol, followed by a more realistic online protocol. Datasets . We evaluate on two publicly accessible datasets: Yelp 1 and Amazon Movies 2 . We transform the review dataset into implicit data, where each entry is marked as 0/1 indi-cating whether the user reviewed the item. Since the high sparsity of the original datasets makes it difficult to evaluate recommendation algorithms ( e.g., over half users have only one review), we follow the common practice [25] to filter out
We used the Yelp Challenge dataset downloaded on Octo-ber 2015 that contained 1.6 million reviews: http://www.yelp. users and items with less than 10 interactions. Table 2 sum-marizes the statistics of the filtered datasets.
 Methodology . We evaluate using two protocols: -Offline Protocol . We adopt the leave-one-out evalu-ation, where the latest interaction of each user is held out for prediction and the models are trained on the remain-ing data. Although it is a widely used evaluation protocol in the literature [9, 25], we point out that it is an artifi-cial split that does not correspond to the real recommenda-tion scenario. In addition, the new users problem is averted in this evaluation, as each test user has a training history. Thus this protocol only evaluates an algorithm X  X  capability in providing one-shot recommendation for existing users by leveraging the static history data. -Online Protocol . To create a more realistic recom-mendation scenario, we simulate the dynamic data stream. We first sort all interactions in chronological order, training models on the first 90% of the interactions and holding out the last 10% for testing. In the testing phase, given a test in-teraction ( i.e., a user X  X tem pair) from the hold-out data, the model first recommends a ranked list of items to the user; the performance is judged based on the ranked list. Then the test interaction is fed into the model for an incremental update. Note that with the global split by time, 14% and 57% test interactions are from new users for the Yelp and Amazon dataset, respectively. Overall this protocol evalu-ates an online learning algorithm X  X  effectiveness in digesting the dynamic new data.

To assess the ranked list with the ground-truth (GT) item that user actually consumed, we adopt Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). We trun-cate the ranked list at 100 for both metrics. HR measures whether the ground truth item is present on the ranked list, while NDCG accounts for the position of hit [9]. We report the score averaged by all test interactions.
 Baselines . We compare with the following methods: -ALS [12]. This is the conventional ALS method that optimizes the whole-data based MF. Due to the high time complexity, this method is infeasible in a real-time dynamic updating scenario, so we only evaluate it with the offline protocol. -RCD [4]. This is the state-of-the-art implicit MF method that has the same time complexity with eALS and is suit-able for online learning. For the line search parameters, we use the suggested values in the authors X  implementation 3 -BPR [25]. This is a sample-based method that opti-mizes the pair-wise ranking between the positive and nega-tive samples. It learns by SGD, which can be adjusted to online incremental learning by [27]. We use a fixed learning rate, varying it and reporting the best performance.
Parameter Settings. For the weight of observed in-teractions, we set it uniformly as 1, a default setting by previous works [4, 25]. For regularization, we set  X  as 0.01 for all methods for a fair comparison. All methods are im-plemented in Java and running on the same machine (Intel Xeon 2.67GHz CPU and 24GB RAM) in a single-thread for a fair comparison on efficiency. As the findings are consis-tent across the number of factors K , without any particular outlier, we only show the results of K = 128, a relatively large number that maintains good accuracy.
We first study how does the weighting scheme on missing data impact eALS X  X  performance. Then we compare with the whole-data based implicit MF methods ALS and RCD, as well as the sample-based ranking method BPR.
In Section 4.1, we propose an item popularity-aware weight-ing strategy, which has two parameters: c 0 determines the overall weight of missing data and  X  controls the weight dis-tribution. First, we set a uniform weight distribution ( i.e.,  X  = 0), varying c 0 to study how does the weight of miss-ing data impact the performance. For Yelp (Figure 1a), the peak performance is achieved when c 0 is around 512, corresponding to that the weight of each zero entry is 0 . 02 ( w 0 = c 0 /N ); similarly for Amazon (Figure 1c), the optimal c 0 is around 64, corresponding to w 0 = 0 . 0001. When c becomes smaller (where w 0 is close to 0), the performance degrades significantly. This highlights the necessity of ac-counting for the missing data when modeling implicit feed-back for item recommendation. Moreover, when c 0 is set too large, the performance also suffers. Based on this ob-servation, we believe the traditional SVD technique [2] that treats all entries equally weighted will be suboptimal here.
Then, we set c 0 to the best value (in the case of  X  = 0), varying  X  to check the performance change. As can be seen from Figure 1b and 1d, the performance of eALS is gradually improved with the increase of  X  , and the best result is reached around 0.4. We further conducted the one-sample paired t -test, verifying that the improvements are statistically significant ( p -value &lt; 0.01) for both metrics on the two datasets. This indicates the effectiveness of our popularity-biased weighting strategy. Moreover, when  X  is Figure 3: NDCG of whole-data based MF across K . larger than 0.5, the performance starts to drop significantly. This reveals the drawback of over-weighting popular items as negative instances, thus the importance of accounting for less popular items with a proper weight.

In the following experiments, we fix c 0 and  X  according to the best performance evaluated by HR, i.e., c 0 = 512 , X  = 0 . 4 for Yelp and c 0 = 64 , X  = 0 . 5 for Amazon. We performed the same grid search of w 0 for RCD and ALS and reported the best performance.

Convergence . Figure 2 shows the prediction accuracy with respect to number of iterations. First, we see that eALS achieves the best performance after converge. All improvements are statistically significant evidenced by the one-sample paired t -test ( p &lt; 0 . 01). We believe the benefits mainly come from the popularity-aware objective function, as both ALS and RCD apply a uniform weighting on the un-knowns. Second, eALS and ALS converge faster than RCD. We think the reason is that (e)ALS updates a parameter to minimize the objective function of the current status, while RCD updates towards the direction of the negative gradient, which can be suboptimal. On Amazon, RCD shows high but turbulent NDCG in early iterations, while the low hit ratio and later iterations indicate the high NDCG is unstable. Finally, we point out that in optimizing the same objective function, ALS outperforms RCD in most cases, demonstrat-ing the advantage of ALS over the gradient descent learner.
Accuracy vs. Number of Factors . Figure 3 shows the prediction accuracy with varying number of factors K . We only show the evaluation by NDCG as HR admits the same trend. First, eALS consistently outperforms ALS and RCD across K , demonstrating the effectiveness of our eALS method ( n.b. although the three methods seem to perform on par for Amazon at K = 128, their difference can be clearly seen in Figure 2d). Second, all methods can be im-proved significantly with a larger K . Although a large K might have the risk of overfitting, it can increase model X  X  representation ability thus better prediction. Especially for large datasets that can have millions of users and billions of interactions, a large K is particularly important for the accuracy of MF methods.
 Efficiency. Analytically, ALS X  X  time complexity is O (( M + N ) K 3 + |R| K 2 ), while eALS and RCD are K times faster. To compare their efficiency empirically, we show the actual training time per iteration in Table 3.
 Table 3: Training time per iteration of different whole-based MF methods with varying K .

As can be seen, with the increase of K , ALS takes much longer time than eALS and RCD. Specifically, when K is 512, ALS requires 11.6 hours for one iteration on Amazon, while eALS only takes 12 minutes. Although eALS does not empirically show K times faster than ALS due to the more efficient matrix inversion implementation (we used the fastest known algorithm [1] with time complexity around O ( K 2 . 376 )), the speed-up is already very significant. More-over, as RCD and eALS have the same analytical time com-plexity, their actual running time are in the same magnitude; the minor difference can be caused by some implementation details, such as the data structures and caches used.
Figure 4 plots the performance of BPR with different learning rates 4 in each iteration. Note that we only run eALS for 100 iterations, which are enough for eALS to con-verge. First, it is clear that BPR X  X  performance is subjected
We have also tried other intermediate values of learning rates, and the findings are consistent. Thus, to make the figure more clear, we only show three selected values. to the choice of learning rate  X  Figure 4a and 4b show that a higher learning rate leads to a faster convergence, while the final accuracy may be suffered. Second, we see that eALS significantly outperforms BPR on the Yelp dataset evaluated by both measures ( p &lt; 0 . 001). For Amazon, eALS obtains a much higher hit ratio but a lower NDCG score, indicat-ing that most hits occur at a relatively low ranks for eALS. Comparing with the performance of other whole-based MF methods ALS and RCD (Figure 2), we draw the conclusion that BPR is a weak performer in terms of the prediction recall, while being a strong performer in terms of the preci-sion at top ranks. We think BPR X  X  strength in ranking top items is due to its optimization objective, which is a pair-wise ranking function tailored for ranking correct item high. In contrast, the regression-based objective is not directly op-timized for ranking; instead, by account for all missing data in regression, it better predicts user X  X  preference on uncon-sumed items, leading to a better recall. This is consistent with [2] X  X  finding in evaluating top-K recommendation.
We notice that BPR shows unusual NDCG spike in early iterations on the Amazon dataset, however the performance is unstable and goes down with more iterations. The same phenomenon was also observed for another gradient descent method RCD on the same dataset (see Figure 2d). We hy-pothesize that it might be caused by some regularities in the data. For example, we find some Amazon users review on a movie multiple times 5 . In early iterations, BPR ranks these repeated items high, leading to a high but unstable NDCG score. There might be other reasons responsible for this, and we do not further explore here.
In the evaluation of online protocol, we hold out the latest 10% interactions as the test set, training all methods on the remaining 90% data with the best parameter settings evi-denced by the offline evaluation. We first study the number of online iterations required for eALS to converge. Then we show how does the weight of new interactions impact the performance. Lastly, we compare with dynamic MF meth-ods RCD and BPR in the online learning scenario.
Figure 6 shows how does eALS X  X  accuracy change with number of online iterations. Results at the 0-th iteration benchmark the performance of the offline trained model, as no incremental update is performed. First, we can see that the offline trained model performs very poorly, highlight-
Due to user X  X  repeat consumption behaviours, we do not exclude training items when generating recommend list. ing the importance of refreshing recommender model for an online system with dynamic data. Second, we find most performance gain comes from the first iteration, and more iterations do not further improve. This is due to the fact that only the local features regarding to the new interaction are updated, and one eALS step on a latent factor can find the optimal solution with others fixed. Thus, one iteration is enough for eALS to learn from a new interaction incre-mentally, making eALS very efficient for learning online. Figure 6: Impact of online iterations on eALS.

We have also investigated number of online iterations re-quired for baselines RCD and BPR. RCD shows the same trend that good prediction is obtained in the first iteration. While BPR requires more iterations, usually 5-10 iterations to get a peak performance and more iterations will adversely hurt the performance due to the local over-training.
To evaluate how does the weight of new interactions ef-fect the online learning algorithms, we also apply the same weight w new on RCD. Note that the original RCD paper [4] does not consider the weight of interactions; we encode w the same way with eALS, revising the RCD learner to opti-mize the weighted regression function.
 Figure 7: Impact of w new on eALS and RCD in online learning evaluated by NDCG.

Figure 7 shows the performance evaluated by NDCG (re-sults of HR show the same trend thus omitted for space). Setting w new to 1 signifies that new interaction is assigned a same weight with the old training interaction. As expected, with a modest increasing on w new , the prediction of both models is gradually improved, demonstrating the usefulness of strengthening user X  X  short-term interest. The peak per-formance is obtained around 4, where eALS shows better prediction than RCD. Overly increasing w new will adversely hurt the performance, admitting the utility of user X  X  histor-ical data used in offline training. Overall, this experiment indicates the importance of balancing user X  X  short-term and long-term interest for quality recommendation.
With the simulated data stream, we show the performance evolution with respect to number of test instances in Figure 5. First, eALS consistently outperforms RCD and BPR evi-denced by both measures, and one-sample paired t-test ver-ifies that all improvements are statistically significant with p &lt; 0 . 001. BPR betters RCD for Yelp, while underper-forms for Amazon. Second, we observe the trend that the performance of dynamic learning first decreases, and then increases before becoming stable. This is caused by the new users problem  X  when there are few feedback for a user, the model can not personalize the user X  X  preference effectively; with more feedbacks streaming in, the model can adapt itself to improve the preference modeling accordingly. To show this, we further breakdown the results of eALS by number of past interactions of test user in Figure 8.
 Figure 8: Results breakdown of eALS by # of past interactions of test user. Note: Interaction # &gt; 0 denotes the performance for non-cold-start users.

It is clear that when there are no historical feedback for a test user ( i.e., user cold-start cases), the performance is very poor  X  no better than random. After the first interac-tion streams in, the prediction is significantly improved; and with more interactions, the performance is further improved. This highlights the importance of incorporating instanta-neous user feedback into the model, especially for cold-start or sparse users that have few history in training.
We study the problem of learning MF models from im-plicit feedback. In contrast to previous work that applied a uniform weight on missing data, we propose to weight missing data based on the popularity of items. To address the key efficiency challenge in optimization, we develop a new learning algorithm  X  eALS  X  which effectively learns parameters by performing coordinate descent with memo-ization. For online learning, we devise an incremental up-date strategy for eALS to adapt dynamic data in real-time. Experiments with both offline and online protocols demon-strate promising results. Importantly, our work makes MF more practical to use for modeling implicit data, along two dimensions. First, we investigate a new paradigm to deal with missing data which can easily incorporate prior do-main knowledge. Second, eALS is embarrassingly parallel, making it attractive for large-scale industrial deployment.
We plan to study the optimal weighting strategy for online data as a way to explore user X  X  short-term interest. Along the technical line, we explored the element-wise ALS learner in its basic MF form and solved the efficiency challenge in handling missing data. To make our method more applica-ble to real-world settings, we plan to encode side information such as user social contexts [8] and reviews [9] by extending eALS to more generic models, such as collective factoriza-tion [11] and Factorization machines [26]. In addition, we will study binary coding for MF on implicit data, since a recent advance [32] has shown that discrete latent factors are beneficial to collaborative filtering for explicit ratings.
The strength of eALS can be applied to other domains, owing to the universality of factorizing sparse data matrices. For example, recent advances in natural language process-ing [16] have shown the connection between neural word em-beddings and MF on the word X  X ontext matrix. This bridge nicely motivates several proposals to use MF to learn word embeddings; however, when it comes to handling missing data, they have either ignored [22] or equally weighted the missing entries, similar to traditional SVD [16]. It will be in-teresting to see whether eALS will also improve these tasks. The authors would like to thank the additional discussion and help from Steffen Rendle, Bhargav Kanagal, Immanuel Bayer, Tao Chen, Ming Gao and Jovian Lin.
