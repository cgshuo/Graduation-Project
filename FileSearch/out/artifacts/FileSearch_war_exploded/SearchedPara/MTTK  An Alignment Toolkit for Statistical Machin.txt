 Parallel text alignment procedures attempt to iden-tify translation equivalences within collections of translated documents. This can be be done at various levels. At the finest level, this involves the alignment of words and phrases within two sentences that are known to be translations (Brown et al., 1993; Och and Ney, 2003; Vogel et al., 1996; Deng and Byrne, 2005). Another task is the identification and align-ment of sentence-level segments within document pairs that are known to be translations (Gale and Church, 1991); this is referred to as sentence-level alignment, although it may also involve the align-ment of sub-sentential segments (Deng et al., ) as well as the identification of long segments in either document which are not translations. There is also document level translation which involves the iden-tification of translated document pairs in a collection of documents in multiple languages. As an example, Figure 1 shows parallel Chinese/English text that is aligned at the sentence, word, and phrase levels.
Parallel text plays a crucial role in multi-lingual natural language processing research. In particu-lar, statistical machine translation systems require collections of sentence pairs (or sentence fragment pairs) as the basic ingredients for building statistical word and phrase alignment models. However, with the increasing availability of parallel text, human-created alignments are expensive and often unaf-fordable for practical systems, even at a small scale. High quality automatic alignment of parallel text has therefore become indispensable. In addition to good alignment quality, several other properties are also desirable in automatic alignment systems. Ideally, these should be general-purpose and language in-dependent, capable of aligning very different lan-guages, such as English, French, Chinese, German and Arabic, to give a few examples of current in-terest. If the alignment system is based on statis-tical models, the model parameters should be esti-mated from scratch, in an unsupervised manner from whatever parallel text is available. To process mil-lions of sentence pairs, these models need to be ca-pable of generalization and the alignment and esti-mation algorithms should be computationally effi-cient. Finally, since noisy mismatched text is often found in real data, such as parallel text mined from web pages, automatic alignment needs to be robust. There are systems available for these purposes, no-tably the GIZA++ (Och and Ney, 2003) toolkit and the Champollion Toolkit (Ma et al., 2004).

This demo introduces MTTK, the Machine Trans-lation Toolkit. The toolkit can be used to train statis-tical models and perform parallel text alignment at different levels. Target applications include not only machine translation, but also bilingual lexicon in-duction, cross lingual information retrieval and other multi-lingual applications. MTTK is a collection of C++ programs and Perl and shell scripts that can be used to build statisti-cal alignment models from parallel text. Respective of the text to be aligned, MTTK X  X  functions are cat-egorized into the following two main parts. 2.1 Chunk Alignment Chunk alignment aims to extract sentence or sub-sentence pairs from parallel corpora. A chunk can be multiple sentences, a sentence or a sub-sentence, as required by the application. Two align-ment procedures are implemented: one is the widely used dynamic programming procedure that derives monotone alignment of sentence segments (Gale and Church, 1991); the other is divisive clustering procedure that begins by finding coarse alignments that are then iteratively refined by successive binary splitting (Deng et al., ). These two types of align-ment procedures complement each other. They can be used together to improve the overall sentence alignment quality.

When translation lexicons are not available, chunk alignment can be performed using length-based statistics. This usually can serve as a start-ing point of sentence alignment. Alignment qual-ity can be further improved when the chunking pro-cedure is based on translation lexicons from IBM Model-1 alignment model (Brown et al., 1993). The MTTK toolkit also generates alignment score for each chunk pair, that can be utilized in post process-ing, for example in filtering out aligned segments of dubious quality. 2.2 Word and Phrase Alignment After a collection of sentence or sub-sentence pairs are extracted via chunk alignment procedures, sta-tistical word and phrase alignment models can be estimated with EM algorithms. MTTK provides im-plementations of various alignment, models includ-ing IBM Model-1, Model-2 (Brown et al., 1993), HMM-based word-to-word alignment model (Vogel et al., 1996; Och and Ney, 2003) and HMM-based word-to-phrase alignment model (Deng and Byrne, 2005). After model parameters are estimated, the Viterbi word alignments can be derived. A novel computation performed by MTTK is the genera-tion of model-based phrase pair posterior distribu-tions (Deng and Byrne, 2005), which plays an im-portant role in extracting a phrase-to-phrase transla-tion probabilities. MTTK is designed to process huge amounts of par-allel text. Model parameter estimation can be car-ried out parallel during EM training using multiple CPUs. The entire parallel text is split into parts. During each E-step , statistics are collected paral-lel over each part, while in the M-steps, these statis-tics are merged together to update model parame-ters for next iteration. This parallel implementation not only reduces model training time significantly, it also avoids memory usage issues that arise in pro-cessing millions of sentence pairs, since each E-Step need only save and process co-occurrence that ap-pears in its part of the parallel text. This enables building a single model from many millions of sen-tence pairs.

Another feature of MTTK is language indepen-dence. Linguistic knowledge is not required during model training, although when it is available, per-formance can be improved. Statistical parameters are estimated and learned automatically from data in an unsupervised way. To accommodate language diversity, there are several parameters in MTTK that can be tuned for individual applications to optimize performance. A typical example of using MTTK is give in Fig-ure 2. It starts with a collection of document pairs. During pre-processing, documents are normalized and tokenized into token sequences. This prepro-cessing is carried out before using the MTTK, and is usually language dependent, requiring, for exam-ple, segmenting Chinese characters into words or ap-plying morphological analyzing to Arabic word se-quences.
 Statistical models are then built from scratch. Chunk alignment begins with length statistics that can be simply obtained by counting the number of tokens on in each language. The chunk aligning procedure then applies dynamic programming to de-rive a sentence alignment. After sorting the gener-ated sentence pairs by their probabilities, high qual-ity sentence pairs are then selected and used to train a translation lexicon. As an input for next round chunk alignment, more and better sentence pairs can be extracted and serve as training material for a bet-ter translation lexicon. This bootstrapping procedure identifies high quality sentence pairs in an iterative fashion.

To maximize the number of training words for building word and phrase alignment models, long sentence pairs are then processed further using a di-visive clustering chunk procedure that derives chunk pairs at the sub-sentence level. This provides addi-tional translation training pairs that would otherwise be discarded as being overly long.

Once all usable chunk pairs are identified in the chunk alignment procedure, word alignment model training starts with IBM Model-1. Model com-plexity increases gradually to Model-2, and then HMM-based word-to-word alignment model, and finally to HMM-based word-to-phrase alignment model (Deng and Byrne, 2005). With these models, word alignments can be obtained using the Viterbi algorithm, and phrase pair posterior distributions can be computed in building a phrase translation ta-ble.
 In published experiments we have found that MTTK generates alignments of quality comparable to those generated by GIZA++, where alignment quality is measured both directly in terms of Align-ment Error Rate relative to human word alignments and indirectly through the translation performance of systems constructed from the alignments (Deng and Byrne, 2005). We have used MTTK as the basis of translation systems entered into the recent NIST Arabic-English and Chinese-English MT Evalua-tions as well as the TC-STAR Chinese-English MT evaluation (NIST, 2005; TC-STAR, 2005). MTTK Version 1.0 is released under the Open Source Educational Community License 1 . The tools and documentation are available at http://mi.eng.cam.ac.uk/  X  wjb31/distrib/mttkv1/ .
