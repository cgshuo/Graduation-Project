 Nowadays, more and more applications need to manage huge volume of data and serve large amount of requests concurrently. As it is too hard for a single machine to support the whole system operations, these applications are mostly installed on clus-ters containing multiple data nodes. Thus, the whole data in an application needs to be partitioned and then stored in different machines. The goal of a partitioning algorithm is to achieve low response time and high throughput by minimizing inter-node com-munications. 
During data partitioning, partition key selection, which would select attributes for processing nodes, and concurrent Read and Write operations over multiple partitions affect query performance [2]. For example, an application, which has partitioned data by the Location attribute but accesses data with queries containing many joins on the Date column, would undoubtedly has an expensive communication cost. 
Previously, there have been many efforts on the problem of data partitioning [1, 2, most of them utilize some simple strategy, while some others rely on a commercial database system for this selection. 
In this work, we discuss how to select partition keys automatically for tables in ap-plications and propose ASAWA ( A utomatic S election based on A ttribute W eight tions. Thus it would be better to partition datasets according to the attributes in search condition defined in query statements. The basic process of ASAWA would first compute an aggregated score for each attrib ute by identifying the data dependency among tables and finding out associated attributes, and then select partition keys with top scores evaluated by grouping features in workload schema. Combined both data and workload information predefined, it is expected to reduce communication cost among data nodes and improve system performance. 
The contributions of this paper are summarized as follows: 1. We propose an automatic partition key selection strategy. 2. We implement the strategy within a novel algorithm for choosing partition 3. We conduct a series of experiments on TPC-H [3] to show both the efficiency The remainder of this paper is organized as follows. Section 2 presents related works illustrates the automatic partition key selection method ASAWA. Using three kinds of show the efficiency of ASAWA. Finally, we have briefly summarized the contribu-tions in this work and discussed works in the future in Section 5. Data partitioning [4] is known as one of the most famous technologies for its efficien-cy in large-scale application scalability. Traditionally, there are 3 kinds of partitioning [7], any of which needs to select a partition key for each relation as the basis of parti-tioning algorithms. In recent years, there have emerged some new researches, which graph of relations in data into fine-grained partitioning results. In [2], it has proposed a method to have a skew-aware partitioning solution based on large-scale neighbor-hood search. It would like to select the most frequently accessed column in the work-when there are too many columns in a large amount of tables, the search space would much more suitable for the applications with fixed data and workloads. 
The ideal solution is supposed to partitio n data without manual assignment, while the performance is as well as the best-known. In most research works, they assume [10] that the proper partitioning keys have been chosen, and concentrate on partition-commercial database systems (e.g. Oracle [11], DB2 [12], SQL Server [13], etc.) have mentioned the importance of partition key selection and have their own mechanisms for recommendation, most of them are no t open sources and combined deeply with underlying database optimizers. Therefore, they could not be applied on others direct-ly. Based on some guidelines [14, 15], it is mostly operated manually in cases that the data management architecture is new or open source. 
Basically, there are four kinds of partition key selection strategies:  X  Primary Key Selection: This is the most widely used partition key selection  X  Random Selection: This occurs when partition keys are specified without suffi- X  Exhaustive Selection: This would like to try all possible partition key groups to  X  Analytical Selection: This is based on data and workload schema analysis, As data amount increasing rapidly and workload requests variously, this work would the requirements from data and workload. It prompts ASAWA method for automatic partition key selection, which not only takes the data and workload schemas into con-sideration, but also combined with design and execution information. To illustrate the problem of partition key selection, consider the following motivating management configuration that consists of many storage node s and to partition data to suitable nodes so that the workloads could be executed in parallel. As the first step in data partitioning, a critical problem is how to select partition keys for relationships in the data center to minimize communication costs as much as possible. 3.1 Problem Description this work, we focus on applications with well defined data schema and static work-load pattern which could be obtained with access frequencies for queries in it. The partition key selection problem can be generally formulated as follows: attribute PKS x ( 1  X  x  X d ) for each table as the partition key . Here,  X  Q y ( 1  X  y  X w ) is a predefined query pattern with an access frequency AF y in work- X  B z ( 1  X  z  X s ) is the storage bound of node z.
 distribute datasets into data nodes uniformly. The best partition key selection solution should partition data to nodes and complete queries within the least response time. 3.2 ASAWA Method The basic process of ASAWA would first compute an aggregated score for each attribute based on identifying the dependency among tables and finding out the cor-mized by grouping features in workload schema. Database Analysis Data Schema As mentioned in Section 3.1, we assume that the data schema is well defined to guar-antee data integrity for the database and all of the basic information (including attribute name, data type, constraints in the table, etc.) could be obtained. 
To guarantee the efficiency of ASAWA, we would like to select the attributes with eign Key and columns declared Unique or Not Null . The weight of affect in dimension of data schema for an attribute could then be assigned equally as the values of w tioned above. The values of weights could be the same or different, which depend on specific applications. The existences of the 4 kinds of statements are present as e the integrity statement on the relative attribute. 
For a given attribute in the data schema, its weight in dimension of data schema is defined integrity statements on it and the values of weights in a relative sequence: w DS (GA) =  X  (w DS (PK)  X  e DS (GA PK ) + w DS (FK)  X  e DS (GA FK ) Scale Factor.

A scale factor is the number used as a multip lier in scaling. This is mainly used in data generation. In ASAWA, the weight of affect for a given attribute in dimension of scale factor is as in Formula (2): in which SF MIN = min{ SF 1 , SF 2 ,..., SF d }. Obviously, min(w SF (GA))=1.
In some applications, each table X  X  volume might increase in the same growth rate, which means that the scale factor for each of them is 1. Thus, w SF of each table is 1. In For example, in TPC-H, the scale factor coefficient of table Lineitem is 6000000, while Orders X  X  is 15000000 and Supplier X  X  is 10000. According to Formula (2), w
An extreme case is that, there are tables that too tiny to be worth partitioning. For example, in an OLAP application with Star Schema, the volume of fact tables in-creases fast while dimension tables always stay the same. These tiny tables are consi-considerations on system performance. 
Thus, the total weight of a given attribute in database is present in Formula (3): Workload Analysis Query Pattern Query pattern is a piece of code that will make a reasonably normal query expression compile. As mentioned in Section 3.1, we assume that each query would be executed using the same structure with variable parameters. This ensures that the attributes touched in each query is the same in every execution of it. As mentioned in [10], The most relevant operations that present the attributes touched in SQL statements include equi-joins, Group By operations, duplicate elimination, and selections. The weights of w
QP (SelectConstant) and w QP (SelectHostVariable) relatively, while the existences as e
QP (Join), e QP (GB), e QP (DR), e QP (SelectConstant) and e QP (SelectHostVariable) respectively, which are the counts of their appearance in the query. 
For a given attribute in a given query, the weight of it in dimension of data schema is shown as w QP (GA GQ ) in Formula (4): Access Frequency Access frequency represents how many times an application runs in a given period. Its measurement is decided by the designer, e.g. an hour, a day, et al. In ASAWA, we assume the access frequency is steady for queries in the workload and define that the weight of affect in dimension of access frequency is as in Formula (5): in which AF MIN = min{ AF 1 , AF 2 ,..., AF w }. Obviously, min(w AF (Q y ))=1.
For example, we assume that the access frequency of Q 1 and Q 11 is 7000 and 150 respectively during an hour in TPC-H, while the minimized access frequency is 3 on Q w
Thus, the total weight of a given attribute in workload is present in Formula (6): According to the above analysis, the general weight of a given attribute w(a) could be calculated as presented in Formula (7): ASAWA Algorithm Although there have been many partition key selection strategies for manual operation Here we propose ASAWA ( A utomatic S election based on A ttribute W eight A nalysis) method to solve this issue. load information predefined, it takes attributes with data integrity constrains into con-sideration to reduce the complexity in prac tice and is expected to reduce communica-tion cost among data nodes and benefit for system performance. In this section, we have shown results of our experiments using different partition key selection strategies and compared with the results with analysis. Input: Output: Solution PKSD = {PKS 1 , PKS 2 ,..., PKS x } (1  X  x  X d ), in which PKS j (1  X  j  X  x) is an attribute in table T x  X  D.
 Procedure ASAWA: 4.1 Environment and Configuration In the experiment, we have setup 3 data nodes, each of which is equipped with 2.66GHz * 8 cores, more than 300G disk, and CentOS 6.1 x86-64 with Open SSH 5.2 inside. As the scenario mentioned in Section 3, we select Greenplum Database (GPDB for short) 4.2.0.0 [26] as the platform. The dataset and queries used are gener-ated from TPC-H 2.14.2, which has well-defined data schema and query patters to degree of complexity, and give answers to critical business questions. The node with 32G main memories is chosen as the master node, while the other two nodes have 16G main memory and are set as slave nodes. Each of them has 8 segments. 4.2 Experimental Procedure A complete experimental procedure is shown in Fig. 2. It would be executed in every round of each strategy on a generated dataset with a specific scale factor. To be fair, this procedure should be repeated in each round of every case. Input: Output: Procedure: 
It needs to select partition keys based on different strategies in the first step. In this work, we implement three partition key selection strategies for comparison, which are based on Primary Key, Random Assigned and ASAWA respectively. The detail de-scriptions of the first two strategies could be found in Section 2. For ASAWA, we set w
QP (DR) = 0.08, w QP (SelectConstant) = -0.05 and w QP (SelectHostVariable) = 0.05 as w
DS (GA) in this work. Then we could run Procedure ASAWA shown in Fig. 1 for partition key selection for case ASAWA with the specific scale factor in this round. 
In the TPC-H Schema, there are two tables named Nation and Region respectively that are not affected by scale factors. For the size for both of them are fixed and quite selected strategy case for performance consideration. 4.3 Results and Analysis As a matter of fact that the servers may have many different networks and application strategy with relative dataset and workload in a specific scale should be executed ware environment, the scale factor of dataset is set to be 1, 10 and 100 respectively. We run query set 5 times within the newly partitioned dataset in every round. Overall Results As a decision support oriented benchmark, it is hard to predict the access frequency of queries in TPC-H. We first all of the 22 queries in the data set of scale factor =1, 10 and 100 respectively. The results are shown in Figure 3. 
From the series figures shown in Fig. 3, we could see that in each scale factor round, Case 2: Random Strategy always runs the longest time and shows the worst performance, Case 1: Primary Key Strategy is faster than Case 2 and shows fine per-formance, while Case 3: ASAWA is even better than Case 1 and always shows the best performance in the 3 scales of datasets. Besides the results shown in Figure 3, we Case 2 shows obviously longer data loading time than the others. 
The result verifies again that the performance in queries is affected by the key se-lection switch. Also, we found that compared with Case 1, the advantages of Case 3 in Figure 3(c) are not so distinct than it in Figure 3(a) and 3(b). This is because GPDB has implemented a mechanism named Motion to do live migration and eliminate par-distance. But this would add extra workload to data nodes and are not common im-plemented in every product. Specific Query Results query in the data set of scale factor =1, 10 and 100 respectively. From these figures, we could see that for most TPC-H queries in each scale factor round, Case 2 runs the longest time and shows the worst performance, Case 1 shows fine performance than the partition strategy should take the actual workloads of applications into considera-tion to get much better performance. Due to identify tuples specifically but having not considered the business seman tical information, Primary Keys strategy always obtains reasonable but limited performance. For improper partition keys selection, Random strategy leads to bad system performance. Having taken data and workload features into consideration, ASAWA shows the best performance in all of these 3 strategies. Besides, we have intentionally selected Q1, Q5 and Q13 from the 22 queries of TPC-H, in which Q1 is represented for queries on a single large table, Q5 for queries on multi tables and Q13 for queries only on small tables, to take a continuous running shown in Fig.5 indicates that, for the queries that data distributed originally and mem-ory could not cached, the executing time does not change too much, like Q1 and Q5, while for the queries that memory cacheable as Q13, the executing time is the most at tion operation (like live migration) in GPDB, Round 3-7 show steady results. Ob-viously, if similar queries could be scheduled closer, which our strategy want to achieve, executing performance would be improved a lot. As more and more applications need to manage data in huge volume and response large amount of requests concurrently, the whole dataset for an application need to be less, there are few works addressing this topic. In this work, we present an automatic partition key selection strategy, called ASAWA. It would first compute an aggregated score for each attribute based on identifying the dependency among tables and finding scores optimized by grouping features in workload schema. In this way, close tuples, i.e. co-appearing in queries frequently, would be probably put into the same partition. Hence the inter-node joins could be greatly reduced and the system performance could be improved. We then conduct a series of experiments over TPC-H queries with fectiveness of ASAWA method. 
For most applications with huge data volume and great concurrent requests, parti-applications, this would be undoubtedly more meaningful and useful for the widely use of database system products and release the burden of database administrators or use empirical weight values assigned in ASAWA in the experiments. In the future work, we would like to adjust the values by learning methods to get a more practical weight set. Of course, there must be some differences among application features and visions. We would like to explore an automatic partitioning method combined with both ASAWA and proper partitioning algorithms in further research. Acknowledgements. This work is supported by State Key Laboratory of Software Development Environment Open Fund under Grant No.SKLSDE-2012KF-09, the National Science Foundation of China under Grant No. 61003086, and the Graduate Student Scientific Research Foundation of Renmin University of China under Grant No. 42306176. The authors would like to thank the anonymous reviewers for their helpful comments and valuable suggestions. 
