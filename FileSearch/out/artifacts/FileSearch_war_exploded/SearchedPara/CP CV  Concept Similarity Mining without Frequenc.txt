 Domain specific ontologies are heavily used in many ap-plications. For instance, these form the bases on which similarity/dissimilarity between keywords are extracted for various knowledge discovery and retrieval tasks. Existing similarity computation schemes can be categorized as (a) structure-or (b) information-based approaches. Structure-based approaches compute dissimilarity between keywords using a (weighted) count of edges between two keywords. Information-base approaches, on the other hand, leverage available corpora to extract additional information, such as keyword frequency, to achieve better performance in simi-larity computation than structure-based approaches. Unfor-tunately, in many application domains (such as applications that rely on unique-keys in a relational database), frequency information required by information-based approaches does not exist . In this paper, we note that there is a third way of computing similarity: if each node in a given hierarchy can be represented as a vector of related concepts, these vectors could be compared to compute similarities. This requires mapping concept-nodes in a given hierarchy onto a concept-space. In this paper, we propose a concept propagation (CP) scheme, which relies on the semantical relationships between concepts implied by the structure of the hierarchy to anno-tate each concept-node with a concept-vector (CV). We refer to this approach as CP/CV. Comparison of keyword similar-ity results shows that CP/CV provides significantly better (upto 33%) results than existing structure-based schemes. Also, even if CP/CV does not assume the availability of an appropriate corpus to extract keyword frequency informa-tion, our approach matches (and slightly improves on) the performance of information-based approaches.
This work is supported by an NSF ITR Grant, ITR-0326544;  X  ILearn: IT-enabled Ubiquitous Access for Edu-cational Opportunities for Blind Individuals  X  X ndanRSA Grant  X  Ubiquitous Environment to Facilitate Access to Text-books and Related Materials for Adults and School Age Chil-dren who are Blind or Visually Impaired  X .
 Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. Categories and Subject Descriptors: H.3.1 [INFOR-MATION STORAGE AND RETRIEVAL][Content Analy-sis and Indexing]: Indexing methods General Terms: Algorithms, experimentation, human fac-tors.
 Keywords: Mining keyword similarities, concept hierar-chies, concept propagation.
Recently, there has been growing research on integrat-ing keyword search into databases. Kacholia et al. [15], BANKS [5], XRank [13], ObjectRank [4], and XSEarch [7] all rely on structural analysis, information retrieval tech-niques, as well as keyword similarities for ranking database query results. Naturally a particular challenge of these and other techniques, which aim to apply keyword search on do-main specific databases, is that keyword (or concept) simi-larities in specific domains need to be properly measured for these to be effective.

Ontologies and taxonomies are used in diverse areas of sci-ence, including biology and medicine, as well as in various standardization and information integration efforts, where it is important to be precise about the relationships of con-cepts. Given a concept taxonomy for a particular appli-cation domain, software systems can represent and orga-nize data pertinent to this domain more effectively than without any prior knowledge of the relationships between concepts in this domain. Furthermore, ontologies enable sharing and integration of data from different domains and data sources. The effectiveness of ontologies in enabling domain-specific treatment of knowledge-application, lead to proliferation of domain specific ontologies, such as UMLS [1] (for medical concepts), TOVE [12] (for enterprise modeling), PLINIUS [37] (for material science) and GENSIM [16] (for molecular biology and biochemistry). Our goal in this pa-per is to develop an effective measure of similarity between concepts in such a concept taxonomy.
Semantic similarity measures quantify relatedness be-tween two words or concepts. Many traditional knowledge-driven applications (such as text classification [33], word sense disambiguation [2], and data mapping [29, 6]) re-quire mining of such semantic similarity/ dissimilarity val-ues between concepts in a given domain. Therefore, the study of semantic relationships between words in a language has a long history in psychological theory, natural language processing, and knowledge management. There are vari-Figure 1: An IS-A hierarchy:  X  X us X  and  X  X ab X  are more closely related than  X  X us X  and  X  X inibike X . ous general purpose efforts, such as WordNet [22, 23] and FrameNet [3], to model the lexical knowledge underlying a language in the form of a hierarchical taxonomy, where the structure of the graph represents the knowledge about the relatedness of the words. Intuitively, highly related words are grouped together and the path between two different concept-nodes in the hierarchy reflects how these are re-lated in the real-world. The parent/child edges in these hierarchies may correspond to different relationships, such as IS-A or PARTS-OF. An example is presented in Figure 1.
If we consider the WordNet segment presented in Fig-ure 1, we can intuitively see that the two concepts,  X  bus  X  X nd  X  cab X  , are more closely related to each other than concepts,  X  bus  X  X nd X  minibike  X . In the last decade various measures for estimating the semantic similarity of keywords in a given taxonomy are proposed. These measures can be roughly categorized into structure-based (or, edge-based) methods and information-based methods. In structure-based meth-ods, the semantic similarity between two words is measured by the shortest distance between them [26] or the sum of the edge weights along this shortest path [27]. Information-based methods leverage available corpora to extract addi-tional information, such as keyword frequency, to achieve better similarity evaluation than those approaches that rely only on the structural analysis of a hierarchy. For example, [28] estimates the similarity between two concepts using the information content (i.e., negative logarithm of the probabil-ity of encountering an instance of the concept in the corpus) of the concepts subsuming them. When using [28], the sim-ilarity between  X  bus  X  X nd X  minibike  X  in Figure 1 would be determined by the information content of the node  X  motor vehicle  X , which subsumes both words in the hierarchy.
Generally speaking, given an appropriate corpus, information-based methods show a better performance than structure-only approaches. However, information-based methods need an appropriate (large and representative) cor-pus. Such a large corpora is usually available in text-retrieval applications: the collection of documents that is going to be indexed can be used to extract keyword fre-quency information. However, in many other applications, such a large corpora can not be expected to be readily avail-able: for example, in applications that rely on a relational database, the concepts that are used as unique-keys in the data can not have frequency information that will sup-port information-based methods for similarity computation. Even for non-unique attributes, the value frequencies (which can depend on how a schema is normalized) do not carry the same information as in text collections. In these cases, the similarities between these concepts have to be extracted from the available concept hierarchy or taxonomy. Thus, in this paper, we focus on the challenge of learning concept (a) concept hierarchy (b) concept space Figure 2: (a) An IS-A hierarchy where each node represents a concept; (b) the corresponding concept-space. The concept-vectors (CVs) representing where the nodes are mapped in the space are com-puted through concept-propagation, CP. In the pro-posed approach, similarities are measured using the concept-vectors in this concept space. The map-ping from the concept hierarchy to the concept-space (i.e., concept propagation process) relies on the fact that degrees of generality computed in both representations should be identical similarities from a given hierarchy without having to rely on a large corpus for frequency information extraction.
In this paper, we note that there is a third way of min-ing similarities of keywords/concepts: if each concept-node in a given hierarchy could be represented as a vector, then these vectors could be compared to compute concept simi-larity values (Figure 2). Essentially, such concept vectors would capture the semantic information (necessary for sim-ilarity computation), otherwise inherent but hidden within the structure of the hierarchy.

Based on this observation, we propose a concept-propagation (CP) scheme which leverages the semantical relationships between concept-nodes (implied by the struc-ture of the concept hierarchy) to annotate each node with a concept-vector (CV). The CVs are then used for similarity computations. We refer to this novel approach as CP/CV. In particular,
Similarities between the concept-nodes are then computed in the resulting concept-space using cosine similarities [39] of the resulting concept-vectors. In Section 7, we experimen-tally evaluate the proposed CP/CV similarity mining tech-nique. Comparison of similarity results shows that CP/CV provides significantly better (upto 33%) results than exist-ing structure-based schemes. Furthermore, CP/CV matches (in fact slightly improves) the performance of information-based approaches, without needing a representative corpus with appropriate frequency information.
Here, we present the related work in the domains of mining semantic similarities of concepts in a taxonomy and spreading activation and propagation-based approaches used in Web indexing and mining.
 Mining of Concept Similarities: There have been a number of proposals for measuring semantic similarities be-tween keywords in a taxonomy. As mentioned in the intro-duction, these approaches can be classified into two cate-gories: structure-based or information-based methods. [26] proposes that the conceptual distance between two concept-nodes should be defined as the shortest path between two nodes in the taxonomy and that this should satisfy metric distance properties. This approach proved to be very use-ful in small and specific domains, such as medical semantic nets. However, it ignores that (a) the semantic distance be-tween neighboring nodes are not always equal and that (b) the varying local densities in the taxonomy can have strong impacts on the semantic distance between concept-nodes. To overcome these shortcomings, [27] associate weights to the edges in the hierarchy: the edge weight is affected both by its depth in the hierarchy and the local density in the taxonomy. To capture the effect of the depth, [38] estimates the conceptual distance between two concepts, c 1 and c 2 counting the number of edges between them, and normal-izing this value using the number of edges from the root of the hierarchy to the closest common ancestor of c 1 and c
Information-based methods, on the other hand, measure the semantic similarity between two concepts based on the amount of information content of the common ancestors of two given concept-nodes [28, 19, 20, 14]. The information content of a concept is defined as the negative logarithm of the probability of encountering an instance of the concept in the given corpus. For example, in [28], similarity between two concepts, c 1 and c 2 , is defined as where Subsume ( c 1 ,c 2 ) is the set of concepts that subsume c 1 and c 2 ,and P ( c ) is the probability of encountering an instance of concept, c , in a corpus. Recently, [21] applied a similar approach for mining similarities of Web pages. Their approach is based on the information-based analysis of the hierarchy of Web pages, though they generalize their results to non-hierarchical Web data as well.
 Propagation and Spreading Activation: Spreading ac-tivation is a general scheme used for propagation of knowl-edge on data represented in the form of graphs. In spreading activation, activation of one concept in a given node in the graph will spread to several or many related nodes. Spread-ing activation is used heavily in information retrieval [8] and Web mining [11]. For example, [11] presents a method to improve Web pages annotations using spreading activation (of available annotations) over the Web graph. In a work related to similarity mining, [18] proposes measuring the semantic similarity between words in a semantic network using spreading activation approach. Unlike our approach, however, [18] assumes that initial weights (frequency infor-mation obtained from a corpus) are available to be spread.
Propagation is also used in Web mining. One successful approach for organizing web query results based on avail-able web structure is topic distillation proposed in [17]. The basic idea in topic-distillation is to consider the structure of the Web and propagate scores between pages in a way to organize topic spaces in terms of smaller sets of hub and authoritative pages. Other methods propagate the term frequency values or (given a query) the relevance score it-self. For instance, given a query [34] propagates the rele-vance score between web pages connected with hyperlinks. [36, 32] on the other hand, propagate the term frequency values between neighboring pages. Recently, [25] proposes a generic relevance propagation framework, which brings together techniques from [34] and [36], as well as differ-ent propagation methods: hyperlink-level/sitemap-level and score-level/term-frequency-level propagation for indexing.
In CP/CV, we map concept-nodes in a hierarchy into a concept-space for similarity computation. This section presents the vector interpretation of the concept-nodes on which the CP/CV approach is based.
In various domains, including text mining and informa-tion retrieval, concepts are usually represented as vectors in a feature (or keyword) space. For instance, latent se-mantic indexing (LSI) [9] and principal component analysis (PCA) [35], analyze the keywords of the documents in a corpus to identify ( mutually independent ) concepts that are dominant in the corpus. The resulting dominant concepts are represented as vectors in the keyword space.

In this work, we also aim to represent concept-nodes in a given hierarchy as vectors in a concept-space. Yet, there are fundamental differences between the above approaches and CP/CV: first of all, LSI and PCA extract concept vec-tors from term-document matrix (i.e., the corpus); secondly, the concepts identified through LSI and PCA are mutually independent (i.e., their mutual similarities are zero).
Note that one major advantage of the vector representa-tion of concepts is that we can leverage various operations that are proven to be well supported within the vector-space model. One such operation, used successfully in the informa-tion retrieval and text mining literatures, is the use of cosine similarity to compute the similarity of two vectors [39].
Let H ( N, E ) denote a concept hierarchy, where N is the set of concept-nodes (corresponding to the concepts) and E is the set of edges between the parent/child pairs in H .
Definition 3.1 (Concept-Space (CS)). A concept hierarchy, H ( N, E ) ,with m concept-nodes has a corre-sponding concept-space (CS) with m concept-dimensions.
Definition 3.2 (Concept-Vector (CV)). Given a hierarchy, H ( N, E ) , and the corresponding m -dimensional concept-space, CS, each concept-node in n i  X  N maps to a concept-vector, V n i =[ w n i , 1 ,w n i , 2 , ....., w n denotes the weight of the k-th concept-dimension of V n i Example 3.1. Figure 3(a) presents a concept hierarchy. Since the number of nodes in the hierarchy is 4, the corre-sponding concept-space has 4 dimensions (Figure 3(b)).
Concept-vectors provide a mechanism through which sim-ilarity between concepts can be measured. Given two concept-nodes, n i and n j , and their concept-vectors, V Figure 3: (a) A concept hierarchy where each node represents a concept; (b) nodes of the graph are an-notated with concept vector s( w n j ,i denotes the con-tribution of the concept associated with node n i to the concept associated with node n j ) V j , we can compute the cosine similarity, sim cos ( n i ,n between these two concept-nodes as [39] However, Definition 3.2 does not state how to compute the concept-vectors. It is clear that simply setting the concept-vector for node n i as V n i =[0 , 0 ,..., 1 ,..., 0], which includes a non-zero value only for concept c i , would not be helpful. If this scheme was used, similarity between any two concept-nodes, n i and n j where i = j , would be computed as zero.
In this paper, we propose a concept propagation (CP) method for propagating the concepts (i.e., weights of concept-dimensions), between neighboring nodes to iden-tify concept-vectors. Naturally, this concept propagation process must be governed by (and preserve) the underly-ing semantic structure of the concept hierarchy. Therefore, as discussed in Section 1.3, CP process is governed by the constraint that the concept-vectors obtained through propa-gation should have the same (generality) semantics inherent in the concept hierarchy. Therefore, we need mechanisms to quantify the generality value between two concept-nodes in the hierarchy (Section 4) as well as two concept-vectors in the concept-space (Section 5). This observation is used in Section 6 for propagation.
One way to understand the semantic relationship between nodes in a concept hierarchy is to study the generality di-rections and degrees between two adjacent nodes. Since in IS-A hierarchies ancestors are more general than their de-scendants, the generality direction (from child to parent) is clear. However, the degree of generality between a given pair of nodes is not self evident. Thus, in this section, we develop a method for quantifying the generality degree be-tween concept-nodes in a given hierarchy.
As observed in the literature, the degree of generality be-tween neighboring concepts in a concept hierarchy is related to the local density of the hierarchy as well as the depth of the nodes [27]. Prior work (such as [27]) used this obser-vation for associating weights to edges for structure-based computation of concept similarities.

We note that although generality degree between two con-cepts is related to their similarities , these two are not equiv-alent 1 . Information-based methods overcome this by supple-menting the available structural information with informa-tion content extracted from an available corpus.
We can informally state the two basic properties of con-cept hierarchies, that we leverage for measuring the degree of generality between concept-nodes, as follows: These properties are used in the literature (for instance in the information-based approaches, such as [28]) to directly compute concept similarities. Unlike previous works (such as [27, 28]), where density and depth are used for measuring similarities between concepts directly, we aim to use these only for measuring the degree of generality of one node rela-tive to the other. Thus, CP/CV does not fall into the same pitfall of directly substituting generality for dissimilarity as the earlier structure-based schemes do.

We represent the semantic coverage of a concept hierarchy as a range, [0 , 1]. Since the root of the hierarchy subsumes all the other nodes, this range corresponds to the root X  X  share. Naturally, (a) since a parent node subsumes its children en-tirely, the concept range of the parent covers the concept ranges of its children, and (b) as one moves down in the con-cept hierarchy, the concept-nodes get more specialized and they subsume less concepts. Thus, the sizes of the concept ranges corresponding to the nodes decrease monotonically as one moves deeper in the hierarchy. Using these two ob-servations, the size of the concept range, share n i , of a node n can be recursively defined as follows: share n i = Here n p is the parent of n i in the hierarchy and the number children of n p ( num children ( n p )) gives the local density of the hierarchy relative to node n i .

Figure 4 provides a sample co ncept hierarchy and shows how the concepts in the hierarchy share the corresponding concept range. The root node of the hierarchy is the most general concept and occupies the entire range, while the deeper nodes are more specific and have smaller shares. Note that, in the absence of any prior or external/corpus-based knowledge, the children of a given concept-node are assumed to split the concept range of the parent uniformly 2 .
In the experiments section, we show that edge weights that represent the degree of generality does not significantly im-prove similarity computation.
In our future work, we will investigate whether incorporat-ing corpus-based knowledge for more informed splits of the concept-range improve sCP/CVresultsornot. Figure 4: (a) A sample concept hierarchy and (b) the corresponding split of the unit concept range (we assume uniform splits in the absence of any prior or external/corpus-based knowledge)
Given the above definition of shares of concept-nodes, we are ready to define the degree of generality between two nodes in a concept hierarchy.
 Definition 4.1 (Degree of Gen. (by Structure)).
 Given two concept-nodes, n i and n j (where n i is an ancestor of n j ) and the corresponding shares, share n i and share of the concept range, we can define the degree of generality of n i relative to n j based on the structure information inherent in the hierarchy as
Since n i is an ancestor of n j , the degree of generality of n relative to n j , G str n i ,n j , is greater than 1.0.
Example 4.1. Let us consider the example hierarchy and the corresponding concept range split given in Figure 4. In this example, by Definition 4.1, we can compute G str n 1 ,n 3 we can state that the degree of generality of n 1 relative to n 6 is twice as large as that of n 1 relative to n 3 .
As stated before, unlike prior work, we do not use the rel-ative degree of generality as a substitute for measuring sim-ilarity . Instead, CP/CV uses the correspondence between degrees of generality computed by structure (Definition 4.1) and the degrees of generality computed in the correspond-ing concept-space (next section) to map concept-nodes into concept-vectors, which then enable similarity computations.
Let us consider two concept-nodes, n i and n j ,inagiven hierarchy. Let V n i and V n j be the corresponding concept-vectors. Naturally, ensuring that these concept-vectors pre-serve the degrees of generality (computed as in the previous section) between n i and n j requires a similar way of quan-tifying the degree of generality between V n i and V n j in the concept-space. In this section, we propose such a mecha-nism for computing the degree of generality between two concept-vectors.

One way to think of the degree of generality of one node relative to the other is in terms of constraints imposed on them by their concept-vectors: intuitively, the statement that  X  node n i is more general than node n j  X  X anbeinter-preted as n i being less constrained than n j by its concept-vector.

Example 5.1. Let us consider two nodes, n a and n b , where n a is an ancestor of n b . Let us assume that D n a three non-zero concept-dimensions (corresponding to con-cepts c 1 , c 2 ,and c 3 ), while D n b has two non-zero concept-dimensions (corresponding to c 2 and c 3 ): In order to be able to benefit from this observation in mea-suring degrees of generality, we need to be able to quantify how well a given vector can be interpreted as the disjunction of the corresponding concepts.

Extended boolean model [31] of vector spaces associate well defined disjunctive and conjunctive semantics to docu-ment in order to be able to answer boolean queries on vector data. We will use a similar treatment of the vector space to quantify how well a given vector represents the disjunc-tion of the corresponding concepts. Let O =[0 ,..., 0] de-note the origin of the concept-space. O , corresponds to a (hypothetical) concept-vector where all concept-dimensions are zero valued. In other words, O can be interpreted as (  X  c 1  X  X  c 2  X  X  ...  X  X  c m )orequivalentlyas  X  ( c 1  X  c 2  X  Since O corresponds to  X  ( c 1  X  c 2  X  ...  X  c m ), how much a vector V represents a disjunct can be measured by | V  X  X | i.e., the length, | V | ,ofthevector V .

Thus, we can generalize the observation in Example 5.1 as follows: Let us be given two concept-vectors, V n i and V j . Let the concept-dimensions of V n i and V n j be denoted as some dimensions that are non-zero in both V n i and V n j These dimensions are denoted as the common dimensions, D in more general than n j , then the vector V n i should be longer than the vector V n j . In other words, we can define the degree of generality of node n i relative to n j based on the relative lengths of the corresponding vectors.
 Definition 5.1 (Degree of Gen. in Concept-Space). Given two nodes, n i and n j , and their corresponding then we can define the degree of generality (in the concept-space) of n i relative to n j as
The purpose of propagation is to identify the concept-vectors that represent the concept nodes. The process re-peatedly enriches the concept-vector s of the nodes by en-abling neighboring nodes to exchange concept weights. Be-fore the propagation process starts, concept-vectors of the nodes are simply initialized with the concepts corresponding to each node; i.e., if the node n i in the concept hierarchy corresponds to the concept c i , then the initial concept-vector of this node is V n i =[0 , 0 ,..., 1 ,..., 0], where the only non-zero weight is associated with the concept-dimension, c i
Naturally, propagation of weights of concept-dimensions moves the concept-vectors in the concept-space. Yet, after the propagation, the semantic properties, such as the degrees of generality of the nodes, should be preserved. Therefore, the propagation process should be done in such a way that for any pairs of parent/child nodes, the resulting degree of the generality (i.e., G cs ) based on the vector-space model should be same with the degree of generality (i.e., G computed based on structure. In other words, for all neigh-boring (parent/child) n i and n j inthenodehierarchy, must hold after propagation. Since, initially, each concept-vector has only one single non-zero concept-dimension, while when propagation process ends, the concept-vectors have non-zero weights for more than one dimension, we refer to this process as concept propagation (CP).
Using the preservation of generality principle stated above, we first develop an algorithm to propagate concepts (or concepts weights) between parent and children nodes in the concept hierarchy. We will extend this to the propaga-tion of concepts in a whole hierarchy in Subsection 6.3.
The propagation algorithm is governed by a per-neighbor propagation degree which governs how much concept weights two adjacent nodes in a hierarchy should exchange. Definition 6.1 (Propagation Degree,  X  ).
 Let us consider a parent node, par , its children, chld i  X  Chld ( par ) , and the corresponding concept-V between the parent node and its children are such that after propagation, we obtain concept-vectors , V [ w w and for all chld i  X  Chld ( par ) we have Since, per Section 4, the parent concept subsumes all child concepts, we can set the propagation degrees from children to parent (i.e.,  X  child i  X  par ) to 1.0. In order to compute the propagation degrees from parent node to its children (i.e.,  X  par  X  chld i ), on the other hand, we rely on the preservation Figure 5: (a) A sample hierarchy, (b) initial concept-vectors and propagation degrees, (c) concept-vectors and propagation degrees after the first it-eration and (d) status after the second iteration of generality principle: after propagation, the degree of gen-erality computed in the concept space (Section 5) should be the same with the degree of generality computed based on structure (Section 4). That is, for each chld i  X  Chld ( par ), we need to ensure that where G cs par,chld i denote the degree of generality between the concept-vectors of the parent and child vectors after the propagation. Since during the propagation the parent in-herits all concepts from its children, after the propagation, we also have Therefore, using Definition 5.1, we can restate the preserva-tion of generality principle as follows: By using this equation and the value of G str par,chld i ,com-puted in Section 4, we can solve for the propagation degree (  X 
As shown in Figure 5, concept propagation is an itera-tive process, repeated until all concepts have chance to get propagated across all nodes: 1. The three nodes in Figure 5(a) each correspond to a 2. Each node is initially annotated with a concept-vector 3. After the first concept propagation step, the concept-4. Since concept-vectors have been shifted in space in the
By iteratively propagating concepts between a parent node and its children as described in the previous section, we enable the nodes X  concept-vectors to get enriched based on their semantic relationships relative to each other. How-ever, in a large concept hierarchy, considering only parent and its children nodes is not sufficient. The propagation pro-cess should not be limited to nodes between a parent and children, but should be performed iteratively on the entire hierarchy.

For representational and computational convenience, in this section we represent the various parameters involved in concept propagation as matrices.
Let us be given a concept hierarchy H ( N, E ), where nodes in N denote individual concepts and E represents the set of edges between pairs of nodes in N .
 Definition 6.2 (Propagation Adjacency Matrix).
 Given a concept hierarchy graph H ( N, E ) ,itscorresponding propagation adjacency metric, M , is defined as follows:
Note that the diagonal values of M are all equal to 0.
For convenience, we also represent all concept-vectors cor-responding to the nodes in the hierarchy in the form of a single matrix:
Definition 6.3 (Concept-Vector Matrix). Given a concept hierarchy graph, H ( N, E ) ,thecorresponding concept-vector matrix , CV ,isamatrix,where k -th column of CV corresponds to the concept-vector of node n k  X  N .
Since there are m concept-nodes and since each concept-vector has m dimensions, the size of CV is m  X  m .
Given a propagation adjacency matrix, M , and a concept-vector matrix, CV , we execute propagation using a concept propagation operator: Definition 6.4 (Concept Propagation Operator).
 The concept propagation operator,  X  , is such that, given a concept-vector matrix CV and a propagation adjacency matrix M , is a matrix, where P [ k, i ] is the cumulative propagation weight for concept-dimension, c k , in the concept-vector, V After the an iteration with  X  operator, the computed cumu-lative propagation weights in P = CV  X  M should be added to the original values in CV . Based on Definition 6.1, the entry P [ k, i ] can be computed as follows: In other words, the concept propagation operator is where CV . M is the matrix product of CV and M .Thus,the new enriched concept-node matrix after the first iteration of concept propagation is equal to
CV 1 = CV + P = CV +( CV  X  M )= CV + CV . M = CV . ( I + M where I is the identity matrix. Since all diagonal values in are zero, we will use M I , where all diagonal values are 1 and all non-diagonal entries are those in M ,todenote I + M .
We can generalize this process as follows: Let d be the diameter (the greatest number of edges between any nodes) in the graph, S ( N, E ). Then, the final concept-vector ma-trix can be computed by repeated application of the  X  (i.e., matrix multiplication) as follows: where M I m is the propagation adjacency matrix computed for the m th iteration.
Since, after the d th iteration, all nodes are enriched with all concepts (with appropriate weights), the construction of the concept-vectors is complete and the process stops.
In this section, we describe the experiments we carried out to evaluate the effectiveness of the concept propagation method we introduced in this paper. First we describe the experimental setup and then we will discuss the results.
Our aim in this paper is to develop a technique that can be used to mine concept (keyword) similarities, without the help of external information (such as a representative cor-pus). Naturally, the most reasonable way to evaluate the efficiency of CP/CV technique proposed in this paper is to discover the correlation of the resulting concept similarity judgments with human common sense. Thus, to evaluate the performance of our approach effectively, we need (a) a concept hierarchy, (b) ground truth (i.e., a user study of sim-ilarities) on this hierarchy, and (c) representative implemen-tations of alternative (i.e., information-based and structure-based) approaches. Fortunately, such a concept hierarchy (WordNet), appropriate user studies [24, 28] on this con-cept hierarchy, and already re ported similarity results [26, 28] for WordNet do exist: Concept Hierarchy. As discussed in Section 2, Word-Net [22, 23] is a lexical reference system for English nouns, similarity values returned by different approaches are not normalized) verbs, adjective, and adverbs. Here, we use the WordNet version 2.0, composed of a total 115424 synsets that repre-sent concepts. Among these synsets, we focus on the 79689 noun synsets and the IS-A relationship between them. Note that WordNet is not a domain specific ontology. How-ever, using WordNet, we are able to concretely and fairly compare the performance of the proposed algorithm against existing structure-and information-based approaches. Ground Truth. Commonly used ground truth data to evaluate methods for computing the semantic similarity be-tween words comes from an experiment carried by Miller and Charles [24]. The authors did a user study where as-sessors were given 30 pairs of words and asked to rate these words for similarity in meaning on a scale from 0 (dissim-ilar) to 4 (highly similar). In 1999, Resnik [28] replicated the experiment by Miler and Charles. The results obtained from this second study were highly correlated (0.9015) with the Miller and Charles study. The high correlation provides support for the validity of both user tests, yet the imperfect agreement between the studies indicates that 0 . 9 (i.e., the agreement between two user studies on the same data) is es-sentially an upper bound for meaningful correlation degrees.
The first three columns in Table 1 contain the word-pairs used in these two studies and the corresponding similarity values (MC for Miler and Charles X  study and RR for the Resnik X  X  replication of the experiment) 3 . We use both Miler and Charles and Resnik data sets as ground truth. Information-and Structure-based Similarity Results for our Comparative Study. A major advantage of using
Among the 30 pairs of words used by Miler and Charles, two pairs are missing in WordNet; thus, we use only 28 pairs. MC and RR ground truth data is that there are already various published works that evaluate their algorithms using this data set. Therefore, we can directly compare our results with these published results.

Table 1 also presents results for the same word pairs using representatives of different approaches. The column titled Info contains the semantic similarity values measured by the information-based method reported in [28]. The column titled Stru cnt lists the semantic similarity values computed using a structure-based method (edge counting [26]).
We have also considered an alternative structure-based method, where edges are weighted based on the depth and local density [27]. Since [27] does not contain enough ex-periment data and enough details regarding how exactly the edge weights are set, we implemented this method our-selves as follows: Each word in WordNet has possibly several senses. For example the word,  X  X ar X  has five different senses and each sense of this word has a corresponding concept-node. Let us consider two words, wd i and wd j and the cor-responding sets of concept-nodes, con ( wd i )and con ( wd We computed the similarity between wd i and wd j using the weighted edge sum method, Stru wght , as also done in [28]: 2 maxW eight  X  min where,  X  The column titled Stru wgth lists the semantic similarity val-ues computed using (our implementation of) the structure-based method with weighted edges [27].

Note that there are other approaches which leverage other available information for improved similarity evaluation. For example, [18] proposes a spreading activation based ap-proach which assumes the availability of initial edge weights that are extracted using the frequency information obtained from a corpus. [19] leverages not only IS-A relationships, but also PARTS-OF relationships between concepts. [19] also assumes the existence of training data (pairs of words from [30]) to learn and tune various parameters. Since CP/CV does not assume any such extra information, we do not compare it against [18, 19]. Incorporation of such extra knowledge in CP/CV is part of our future work. Implementation of CP/CV on WordNet. Given two words, wd i and wd j , and the corresponding sets of concept nodes, con ( wd i )and con ( wd j ), we computed the CP/CV similarity between wd i and wd j as where V n i and V n j are the concept-vectors (CVs) obtained through concept propagation (CP). During the computa-tion of concept propagation degrees, the distances in the vector spaces are calculated using the 1-norm (manhattan distance) measure, because under this model, equations are guaranteed to be solvable.
 We carried similarity value computations on two graphs: Complete WordNet (79,689 noun concept nodes) and partial WordNet, consisting of the minimum subtrees that contain all the words in the user study (10,121 concept nodes). Ta-ble 1 also contains the semantic similarity values returned by CP/CV for both partial and complete hierarchies. Evaluation Metric (Correlation Coefficient). As in [28], to evaluate concept propagation methods, we use the correlation with the ground truth as the performance metric. Given m -pairs of observations ( x 1 ,y 1 ) , ( x 2 ,y 2 ) ...... ( x the correlation coefficient, r , is defined as and measures how closely (linearly) related x observations are with the y observations [10]. The correlation value of 1 . 0 means that there is a perfect linear relationship between the two data sets, while the value of 0 . 0 indicates that two data sets are not linearly related.
With the above experiment setup, we compared correla-tion coefficients obtained though CP/CV with correlation coefficients returned by various alternative approaches. Ta-ble 2 lists the correlation coefficients of various approaches against; i.e., Miler and Charles X  X , MC, and Resnik replica-tion, RR, user studies. To make sure that the results we report are statistically significant, we verified these corre-lation values using Fisher X  X  Z-test [10]. According to this Table 2: Correlation coefficients from various ap-proaches against the ground truth from the user studies, MC and RR test, our statistical confidence for all the coefficient values reported in Table 2 is higher than 95%.

The main observation from this table is that CP/CV pro-vides better results than all alternatives for both partial and complete hierarchies and against ground truth from both user studies.

CP/CV provides a correlation coefficient value of 0.8138 against Miler and Charles X  X  Rating (MC); this is a significant improvement against both structure-based methods, edge counting and sum of edge weights (21.4% and 16.6% respec-tively). The correlation coefficient value obtained against the Resnik user study is even higher: 0.8639 on the com-plete hierarchy. Note that con sidering that the correlation agreement between two user studies is about 0 . 9, this cor-relation between CP/CV and RR is quite high. This also translates to larger improvements against structured-based methods (33.1% and 28.7% improvement on edge counting and sum of the edge weight methods, respectively).
Note that, for both user studies, performance of CP/CV seems to be slightly better than even the performance of the information-based methods. As expected (since information-based methods can leverage an external corpus, while CP/CV only uses the hierarchy itself), the difference in improvement is less pronounced (only upto 6%). Since our goal, in this paper, is to address the needs of the cases (such as relational databases with unique keys) where suf-ficient frequency information to support information-based approaches is not readily available, we do not further inves-tigate the impact of this 6% gain. However, the fact that CP/CV matches the performance of information-based ap-proaches without having to rely on frequency information provides a strong validation for the proposed approach.
Comparison of the CP/CV values in Table 2 computed on the partial and complete WordNet hierarchies highlight that there is some value in propagating concepts even though they are not in the immediate neighborhood (partial trees) of the words of interest. Figure 6 shows this more explic-itly. Here, the curve represents the correlation coefficient values between RR user study and CP/CV, obtained at dif-ferent iterations of the CP algorithm (Section 6). The first value on the curve corresponds to 0 th iteration (no prop-agation), the next value to 1 st iteration (i.e., propagation between immediate neighbors), and so on. Since it takes at least k iterations of CP for concept-nodes at distance k from each other to exchange weights, this plot also shows the effect of the distance (of concept-nodes) in contributing to the concept-vectors of each-other. As shown in Figure 6, CP/CV observes significant improvements during the first 3 iterations. Beyond these, the correlation coefficient values becomes more or less stable and the maximum correlation value is observed after the 10 th iteration. After this point, the obtained correlation coefficient values start to decrease slightly with additional iterations (though a new stable point Figure 6: Correlation coefficients (between RR user study and CP/CV on the complete hierarchy): there is a significant improvement in the first few iter-ations and the best result is obtained around the 10th iteration; after 10th iteration context-vectors get over-propagated is reached around 18 th iteration which is also the maximum depth of the WordNet). We refer to the slightly drop af-ter the 10 th iteration as over-propagation and conjecture that this is because of the average depth WordNet has been passed. Note that, in general, it should be possible to iden-tify (without having to use ground truth data) the point at which the performance starts dropping simply by checking if the inter-iteration correlations (i.e., correlations between the similarity values returned by consecutive iterations of CP) starts dropping. Our future work will also involve in-vestigation of the over-propagation issue.
In this paper, we first proposed a vector representation for the concept-nodes in an IS-A hierarchy. We then pre-sented two methods for quantifying the generality relation-ships between concept-nodes: one measures generality di-rectly in the hierarchy, while the other uses concept-vectors. We conjectured that the degree of generality between two nodes should be the same whether it is measured in the hierarchy or in the concept-space. Using this observation, we developed a concept propagation algorithm (CP) to map concept-nodes in an hierarchy into a concept-space so that the similarities between nodes can be measured using the re-sulting concept-vectors (CV). Experiments showed that the CP/CV algorithm provides a significant improvement in re-sults when compared with structure-based methods and as good or better results than information-based methods.
