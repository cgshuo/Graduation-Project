 Estimating the quality of machine translation output, called quality estimation (QE) (Specia dict quality scores/categories for unseen machine-translated sentences without reference translations at various granularity levels (sentence-level/word-level/document-level). Quality estimation is of growing importance in the field of machine transla-tion (MT) since MT systems are widely used and the quality of each machine-translated sentence is able to vary considerably.

Previous research on QE, addressed as a re-gression/classification problem to compute quality scores/categories, has mainly focused on feature ex-traction and feature selection. Feature extraction is to find the relevant features, such as baseline fea-tures (Specia et al., 2013) and latent semantic index-ing (LSI) based features (Langlois, 2015), captur-ing various aspects of quality from source and target is to select the best features by using selection al-gorithms, such as Gaussian processes (Shah et al., 2015) and heuristic (Gonz  X  alez-Rubio et al., 2013), among already extracted features. Finding desirable features has played a key role in the QE research.
In this paper we present a recurrent neural net-works approach for estimating the quality of ma-chine translation output at sentence level, which does not require manual effort for finding the best relevant features. The remainder of this paper is or-ganized as follows. In Section 2, we propose a re-current neural networks approach using a sequence of vectors made by the prediction method as input for quality estimation. And we describe the pre-diction method using bi-directional recurrent neural networks architecture in Section 3. In Section 4, we report evaluation results, and conclude our paper in Section 5. Because recurrent neural networks (RNNs) have the strength for handling sequential data (Goodfellow et al., 2015), we apply RNNs to estimate the quality score of translation.

The input of the final RNN is a sequence of vec-tors that have quality information about whether tar-get words in a target sentence are properly translated from a source sentence. We will refer to this se-quence of vectors as quality vectors ( q y Each quality vector q y about how well a target word y j in a target sentence y = ( y 1 , ... ,y tence 3 x = ( x 1 , ... ,x erated from the prediction method (of Section 3).
To predict a quality estimation score (QE score) as an HTER score (Snover et al., 2006) in [0,1] for each target sentence, a logistic sigmoid function is used such that where s is a summary unit of the whole quality vec-tors and W mary unit.

To get the summary unit s , the hidden state v j em-ploying p gated hidden units for the target word y j is computed by The gated hidden unit (Cho et al., 2014) for the ac-tivation function f is used to learn long-term depen-dencies of translation qualities for target words. We consider the QE score as the integrated/condensed value reflecting the sequential quality information of sequential target words. Because the last hidden state v tors, we fix the summary unit s to the last hidden state v In this section, we detail the ways to get the quality vectors ( q y use a neural networks approach for making quality vectors, we use an alternative based on large-scale parallel corpora such as Europarl. We modify the word prediction method of RNN Encoder-Decoder (Cho et al., 2014) using parallel corpora to make the quality vectors.

In subsection 3.1, we describe the underlying word prediction method of RNN Encoder-Decoder. We i) extend the prediction method to use the ad-ditional backward RNN architecture on target sen-tence in subsection 3.2 and ii) modify to get the qual-ity vectors ( q y
Figure 1 is the graphical illustration of the pro-posed RNNs approach. 3.1 Word Prediction Method of RNN RNN Encoder-Decoder proposed by Cho et al. (2014) is able to predict the target word y j given a source sentence x and all preceding target words { y 1 ,...,y j  X  1 } by using a softmax function. And it is extended by Bahdanau et al. (2015) to use infor-mation of relevant source words for predicting the target word y j such that g is a nonlinear function predicting the probabil-RNN on target sentence and contains information of preceding target words { y 1 ,... ,y j  X  1 } . c j is the context vector which means relevant parts of source sentence associated with the target word y j . ~s j  X  1 and y j  X  1 are related to all preceding target words { y 1 ,...,y j  X  1 } , and c j is related to x in the word pre-diction function of (3). 3.2 Additional Backward RNN Architecture on Bahdanau et al. (2015) introduce bi-directional RNN architecture only on source sentence to ex-tend RNN Encoder-Decoder. In our proposed QE model, bi-directional RNN architecture is used both on source and target sentence. By applying bi-directional RNN architecture both on source and tar-get sentence, we can fully and bi-directionally uti-lize source and target sentence for predicting target words, such that which is the extended version of (3) using the addi-
To reflect further all following target words { y j +1 ,... ,y T y } when predicting the target word y j , the hidden state s j +1 of the backward RNN and the next target word y j +1 are added. [ ~s j  X  1 ; s j +1 ] and [ y x in the word prediction function of (4).
 trices of softmax function. K y is the vocabulary sizes of target language and q is the dimensionality of quality vectors. l is the dimensionality of maxout units such that where  X  t j,k is the k -th element of a vector  X  t j . And  X  t trix on target sentence. m and n are the dimensional-ity of word embedding and hidden states of forward and backward RNNs. The hidden state s j +1 of the backward RNN and next target word y j +1 are used
From the extended prediction method of (4), the probability of the target word y j is computed by us-ing information of relevant source words in source sentence x and all target words y = y target word y j in target sentence. 3.3 Quality Vectors on Target Sentence Word prediction method predicts the probability of target words as a number between 0 and 1. But we want to get quality vectors of q -dimensionality which have the more intrinsic quality information for target words.

To make quality vectors, we regard that the prob-ability of the target word y j involves the quality in-formation about whether the target word y j in target sentence is properly translated from source sentence. . . . y . . .
 . . . y j  X  [ ] . . .
 the quality vector q y puted by where  X  is an element-wise multiplication. All of quality information about possible K y target words at position j of target sentence is encoded in t j . Thus, by decoding t j , we are able to get quality vec-tor q y of target sentence. Figure 2 and 3 show the ways to compute the quality vector q y The proposed RNNs approach was evaluated on the tence level of English-Spanish.
 step process. First, by using English-Spanish paral-lel corpus of Europarl v7 (Koehn, 2005), we trained bi-directional RNNs having 1000 hidden units on source and target sentence to make quality vectors. Next, by using the training set of WMT15 QE task, to predicte QE scores we trained the final RNN that use the quality vectors generated in previous step as the input and have 100 hidden units.

Table 1 and 2 present the results of the proposed approach (Bi-RNN) and the official results for the ity Estimation Shared Task at sentence level. At both variants of the task, the proposed RNNs ap-proach achieved the performance over the baseline performance. Also our experiments showed that the performance of the proposed RNNs approach is in-cluded to the best performance group (at the scoring variant of Table 1) or is close to the best performance group (at the ranking variant of Table 2). This paper proposed a recurrent neural networks ap-proach using quality vectors for estimating the qual-ity of machine translation output at sentence level. This approach does not require manual effort for finding the best relevant features which the previous QE research has mainly focused on.

To make quality vectors we used an alterna-tive prediction method based on large-scale paral-lel corpora, because the QE training data were not enough. By extending the prediction method to use bi-directional RNN architecture both on source and target sentence, we were able to fully utilize the bi-directional quality information from source and tar-get sentence for quality estimation.

The proposed RNNs approach achieved a per-formance comparable to the existing state-of-the-art models at sentence-level QE. Our experiments have showed that RNNs approach is a meaningful step for QE research. Applying RNNs approach to word-level QE and studying other ways to make quality vectors better are remained for the future study. This research was supported by the MSIP (Ministry of Science, ICT and Future Planning), Korea, un-der the  X  X CT Consilience Creative Program X  (IITP-2015-R0346-15-1007) supervised by the IITP (Insti-tute for Information &amp; communications Technology Promotion)
