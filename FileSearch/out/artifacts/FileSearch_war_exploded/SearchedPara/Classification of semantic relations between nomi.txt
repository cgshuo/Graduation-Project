 Roxana Girju  X  Preslav Nakov  X  Vivi Nastase  X  Stan Szpakowicz  X  Peter Turney  X  Deniz Yuret Abstract The NLP community has shown a renewed interest in deeper semantic analyses, among them automatic recognition of semantic relations in text. We present the development and evaluation of a semantic analysis task: automatic recognition of relations between pairs of nominals in a sentence. The task was part of SemEval-2007, the fourth edition of the semantic evaluation event previously known as SensEval. Apart from the observations we have made, the long-lasting effect of this task may be a framework for comparing approaches to the task. We introduce the problem of recognizing relations between nominals, and in particular the process of drafting and refining the definitions of the semantic relations. We show how we created the training and test data, list and briefly describe the 15 participating systems, discuss the results, and conclude with the lessons learned in the course of this exercise.
 Keywords Semantic relations Nominals Classification SemEval 1 Introduction The semantic evaluation exercises X  X reviously called SensEval, now SemEval X  are a forum for the evaluation of approaches to the semantic analysis tasks that are most sought after in the NLP community. Initially focused on word sense disambiguation in English, the list of tasks grew to include multi-lingual lexical semantics, and then other aspects of semantic analyses, such as semantic role labeling. We present Task 4 in the fourth edition of SensEval/SemEval-2007. 1 It tackles yet another aspect of semantic analysis of texts: the classification of semantic relations between simple nominals X  X ouns or base noun phrases other than named entities. For example, honey bee could be an instance of the P RODUCT  X  P
RODUCER relation. The classification is performed in the context of a sentence in a written English text.
 Algorithms that classify semantic relations could be applied in Information Retrieval, Information Extraction, Summarization, Machine Translation, Question Answering, Paraphrasing, Recognizing Textual Entailment, Thesaurus Construc-tion, Semantic Network Construction, Word Sense Disambiguation, and Language Modelling. As the techniques for semantic relation classification mature, some of these applications are being tested. For example, Tatu and Moldovan ( 2005 ) applied semantic relation classification in their entry in the PASCAL Recognizing Textual Entailment challenge and significantly improved a state-of-the-art algorithm.
The literature shows a wide variety of methods of nominal relation classification (Lapata 2002 ; Moldovan et al. 2004 ; Lapata and Keller 2005 ; Girju et al. 2005 ; Kim and Baldwin 2005 ; Nakov and Hearst 2006 ; Nastase et al. 2006 ; Pantel and Pennacchiotti 2006 ; Pennacchiotti and Pantel 2006 ; Turney 2005 ). They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst ( 2001 ) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. ( 2002 ) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz ( 2003 ) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with five classes at the top and 30 classes at the bottom; other researchers (Turney and Littman 2005 ; Turney 2005 ; Kim and Baldwin 2005 ; Nastase et al. 2006 ) have used their class scheme and data set. Moldovan et al. ( 2004 ) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al. 2005 ). Stephens et al. ( 2001 ) propose 17 classes targeted to relations between genes. Lapata ( 2002 ) presents a binary classification of relations in nominalizations.
In past work on the classification of semantic relations, most class schemes have been designed to maximize coverage (include a wide range of semantic relations) and minimize overlap (have clearly distinct classes). The ideal class scheme would be exhaustive (include all relations) and mutually exclusive (no overlapping classes). We have rejected this ideal for Task 4. The literature suggests no such class scheme that could satisfy all needs. For example, the gene X  X ene relation scheme of useful for general text.

We sought a motivating application for semantic relation classification, the theme of Task 4. We chose relational search (Cafarella et al. 2006 ). We envision a kind of search engine that can answer queries such as  X  X  X ist all X such that X causes asthma X  X  (Girju 2001 ). With this application in mind, we have decided to focus on semantic relations between nominals (nouns and base noun phrases) other than named entities. Cafarella et al. ( 2006 ) focus on relations between named entities, but the same tasks are interesting for common nouns. For example, consider the task of making a list of things in a given relation with some constant thing:  X  list all X such that X causes cancer  X  list all X such that X is part of an automobile engine  X  list all X such that X is material for making a ship X  X  hull  X  list all X such that X is a type of transportation  X  list all X such that X is produced from cork trees
Relational search does not require an exhaustive and exclusive set of classes of semantic relations. Each class, such as X causes Y , can be treated as a single binary classification problem; for example, either X causes Y or it does not. For Task 4, we separate binary classification task.

In Sect. 2 we explain how we defined our seven relations, and in Sect. 3  X  X ow we built the data sets. Section 4 introduces the teams and systems that participated in Task 4, gives the baseline systems, and presents the performance of all systems on the testing data. Section 5 investigates six questions about the performance of the systems in Task 4. We summarize the lessons learned in Sect. 6 and conclude in Sect. 7 . 2 Defining semantic relations between nominals It is to be expected in manual annotation that clear guidelines are essential to the success of the effort. In Task 4, the most important element of the guidelines were the definitions of the relations. We have tried to pin down as carefully as possible the type of nominals to which a relation applies, and the right conditions. We have found that the annotations have been greatly affected by such additions to the relation definition proper as positive and negative examples with explanations, or notes on usage.

We began by choosing a set of relations to be exemplified in the data. Each task organizer proposed three relations. Seven relations made the cut: C AUSE  X  X  FFECT , I W
HOLE ,C ONTENT  X  X  ONTAINER . For each of these, one task organizer wrote a draft definition and supplied positive and near-miss examples to help illustrate the point. This went to two other organizers who served as reviewers of the definition and later annotators of the data for this relation.

There ensued an iterative process of refining the definitions, based on the comments of the two reviewers. The discussion (with all task organizers taking part) was very intense and productive. It resulted in the inclusion of two more segments in the definition. The restrictions clarify some aspects of the linked concepts that help identify situations when the relation does not apply. The notes identify potential ambiguities and extensions of the definition. Figure 1 shows our partial definition of the semantic relation C ONTENT  X  X  ONTAINER . 3 Building the annotated data sets Ours was a new evaluation task, so we began with guidelines for data set creation Szpakowicz ( 2003 ) have built contains not only relation labels but also part-of-speech and WordNet sense annotations. Similarly, Moldovan et al. ( 2004 ) and Girju et al. ( 2005 ) give the annotators an example of each phrase in a sentence along with WordNet senses and position of arguments. Our annotations include all these, to support a variety of methods. Since we worked with relations between nominals, the part of speech was always noun . WordNet 3.0 on the Web supplied sense index tags.

For each of the seven relations, we based data collection on wild-card search patterns that Google allows. We built such Google queries manually, starting with the patterns proposed by Hearst ( 1992 ) and Nakov and Hearst ( 2006 ). For example, instances of C ONTENT  X  X  ONTAINER come up in response to queries such as  X  X * contains * X  X ,  X  X * holds * X  X ,  X  X  X he * in the * X  X . Following the sample sizes in the English Lexical Sample Task at SensEval-3, we set out to collect 140 training and at least 70 test examples per relation, so we had to invent many different queries to ensure variety. We also aimed to collect a balanced number of positive and negative examples. The use of heuristic patterns to search for both positive and negative examples should naturally result in negative examples that are near misses. We believe that near misses are more useful in supervised learning than negative examples that are generated randomly.

Figure 2 illustrates the annotations. We tag the target nominals. For SemEval-2007 Task 4, we have defined a nominal as a noun or base noun phrase, excluding named entities. A base noun phrase, such as lawn or lawn mower , is a noun with pre-modifiers. We also exclude complex noun phrases (for example, with attached prepositional phrases X  the engine of the lawn mower ).

The procedure was the same for each relation. One person gathered the sample sentences, aiming for a similar number of positive and negative examples, and tagged the entities. Two other people manually annotated the sentences with WordNet senses and classified the relations. The detailed definitions and the preliminary discussions of positive and negative examples served to maximize the agreement between the annotators, who classified the data independently and then discussed every disagreement and looked for consensus. Table 1 shows the statistics for the data collected. 2
The annotators tagged relation instances as true or false. The average inter-annotator agreement was 70.3% after initial independent annotation X  X olumn Original Agreement in Table 1 . The average agreement on WordNet sense labels was 71.9%. Next, the annotators sought a consensus. The definition of each relation was revised to cover explicitly cases where there had been disagreement. Column Percent Retained in Table 1 shows the percentage of instances retained from the original set of collected examples, after the definition of the corresponding relation has been revised and the annotators have agreed on the tags. This percentage could be seen as measuring the agreement between the annotators after revising the definitions. The annotated data sets for each of the seven relations consist only of the examples on whose tag the annotators agreed either initially or after discussing and revising the definition of the relation.

We split each data set into 140 training and no fewer than 70 test examples. (We published the training set for the C ONTENT  X  X  ONTAINER relation as development data 2 months before the test set.) Table 2 shows the size of the training and test sets for each relation and the percentage of positive examples in each set. 4 The participants The task of classifying semantic relations between nominals was the second most successful task at SemEval-2007, attracting the participation of 14 teams who submitted 15 systems. Section 1 lists the systems, the authors and their affiliations, and gives a brief description of each system. Section 2 discusses the three baseline systems and their performance. Section 3 presents the performance of each system in terms of precision, recall, F -measure, and accuracy.

In Table 3 , we distinguish four categories of systems, based on whether the system uses WordNet senses or Google queries. The column Uses WordNet? tells us only whether a system uses the WordNet sense labels in the data sets. A system may use WordNet internally for varied purposes, but ignore our sense labels; such a system would be in category A or C . Based on the input used, each submitted system may have up to four variations X  A , B , C , and D .
 4.1 The systems UVAVU (category B) Sophia Katrenko, Willem Robert van Hage University of Amsterdam and TNO Science and Industry Free University Amsterdam.

Method : similarity measures in WordNet; syntactic dependencies; lexical patterns; logical combination of attributes.
 CMU-AT (category B) Alicia Tribble, Scott E. Fahlman Carnegie Mellon University.
 Method : WordNet; manually-built ontologies; Scone Knowledge Representation Language; semantic distance.
 ILK (categories A, B) Caroline Sporleder, Roser Morante, Antal van den Bosch Tilburg University.

Method : semantic clusters based on noun similarity; WordNet supersenses; grammatical relation between entities; head of sentence; WEKA.
 FBK-IRST (category B) Claudio Giuliano, Alberto Lavelli, Daniele Pighin, Lorenza Romano Fondazione Bruno Kessler X  X RST.

Method : shallow and deep syntactic information; WordNet synsets and hyper-nyms; kernel methods; SVM.
 LCC-SRN (category B) Adriana Badulescu Language Computer Corporation.

Method : named entity recognition; lexical, semantic, syntactic features; decision tree and semantic scattering.
 UMELB-A (category B) Su Nam Kim, Timothy Baldwin University of Melbourne.

Method : sense collocations; similarity of constituents; extending training and test data using similar words.
 UMELB-B (category A) Su Nam Kim, Timothy Baldwin University of Melbourne.

Method : similarity of nearest-neighbor matching over the union of senses for the two nominals; cascaded tagging with decreasing thresholds.
 UCB (categories A, C) Preslav Nakov, Marti Hearst University of California at Berkeley.
 Method : paraphrasing verbs, prepositions and coordinating conjunctions from the Web and from the sentence; KNN-1.
 Team includes a Task 4 organizer .
 UC3M (categories A, B, C, D) Isabel Segura Bedmar, Doaa Sammy, Jose  X  Luis Mart X   X  nez Fer-na  X  ndez University Carlos III of Madrid.
 Method: WordNet path; syntactic features; SVM.
 UCD-S1 (category B) Cristina Butnariu, Tony Veale University College Dublin.

Method : lexical-semantic categories from WordNet; syntactic patterns from corpora, SVM.
 UCD-FC (categories A, B, C, D) Fintan Costello University College Dublin.
 Method : WordNet; additional corpus of tagged noun compounds; Naive Bayes. UCD-PN (category B) Paul Nulty University College Dublin.

Method : WordNet supersenses; Web-based frequency counts for specific joining terms; WEKA (SMO).
 UIUC (category B) Brandon Beamer, Suma Bhat, Brant Chee, Andrew Fister, Alla Rozovskaya, Roxana Girju University of Illinois at Urbana Champaign.

Method : features based on WordNet, NomLex-PLUS, grammatical roles, lexico-syntactic patterns, semantic parses.
 Team includes a Task 4 organizer .
 UTD-HLT-CG (category D) Cristina Nicolae, Garbiel Nicolae, Sanda Harabagiu University of Texas at Dallas.

Method : lexico-semantic features from WordNet, VerbNet; semantic features from a PropBank parser; dependency features.
 UTH (category A) Eiji Aramaki, Takeshi Imai, Kengo Miyo, Kazuhiko Ohe University of Tokio.

Method : joining phrases; physical size for entities; Web-mining; SVM. 4.2 The baselines We used three baseline algorithms for comparison. They guess the class of an example ( X  X  X rue X  X  or  X  X  X alse X  X ) only from the probability distribution of the classes in the test data. The majority algorithm always guesses either  X  X  X rue X  X  or  X  X  X alse X  X , whichever is the majority in the test data. This maximizes accuracy. The alltrue algorithm always guesses  X  X  X rue X  X , which maximizes recall. The probmatch algorithm randomly guesses  X  X  X rue X  X  with probability that matches the distribution of  X  X  X rue X  X  in the test data, which balances precision and recall. Table 4 presents the performance of the three baseline algorithms. 4.3 The results Table 5 shows the performance of the systems in terms of precision, recall, F -measure, and accuracy, macro-averaged over all relations. We computed these measures as described in Lewis ( 1991 ).

From Table 4 , we have maximum baseline values of 64.8% for F -measure and 57.0% for accuracy. In Table 5 , F -measure and accuracy are marked in bold when they are greater than or equal to these maximum baseline values. Many of the systems are above the maximum baselines. 5 Discussion We performed various analyses of the results, which we summarize here in six questions.
 5.1 Is inter-annotator agreement a bound on performance? The highest accuracy on Task 4, averaged over the seven relations, was 76.3% (see Table 5 , UIUC team). The initial agreement between annotators, averaged over the seven relations, was 70.3% (see Table 1 ). This means that the average initial agreement between annotators, before revising the definitions, is not an upper bound on the accuracy that a system can achieve. That the initial agreement between annotators is not a good indicator of the achievable accuracy is also supported by the low correlation of 0.15 between the Accuracy column in Table 6 and the Original Agreement column in Table 1 .

As we explained earlier, two annotators independently labeled each sentence and then compared their labels and revised the definition of the given relation until they came to agreement on the labeling. We believe that the final revisions are much more precise than the initial versions, and that annotators using the final definitions would achieve a much higher agreement than the 70.3% that we report in Table 1 . 5.2 Does more training data help? Overall, the results suggest that more training data improves the performance. There were 17 cases in which we had results for all four possible amounts of training data. Let X i be the four system categories A i , B i , C i and D i with four possible amounts of training data: systems X 1 use training examples 1 X 35, X 2  X 1 to 70, X 3  X 1 to 105 and X  X  1 to 140. We considered average F -measure differences, F ( X 4 ) X  F ( X i ) where X = A to D , i = 1 to 3. All differences for these 17 sets of results are statistically significant:
F ( X 4 ) -F ( X 1 ): N = 17, avg = 8.3, std = 5.8, min = 1.1, max = 19.6, t -value = -5.9, p -value = 0.00001.

F ( X 4 ) -F ( X 2 ): N = 17, avg = 4.0, std = 3.7, min =-3.5, max = 10.5, t -value = 4.5, p -value = 0.0002.

F ( X 4 ) -F ( X 3 ): N = 17, avg = 0.9, std = 1.7, min =-2.6, max = 4.7, t -value = 2.1, p -value = 0.03. 5.3 Does WordNet help? The statistics show that WordNet is important, but the contribution varies across systems. Three teams submitted 12 results both for A 1 -A 4 and B 1 -B 4 . The average difference in F -measure, F ( B i ) -F ( A i ), i = 1 to 4, is significant:
F ( B i ) -F ( A i ): N = 12, avg = 6.1, std = 8.4, min =-4.5, max = 21.2, t -value = -2.5, p -value = 0.01.
 The results of the UCD-FC system actually went down when WordNet was used. The statistics for the remaining two teams, however, are a bit better:
F ( B i ) -F ( A i ): N = 8, avg = 10.4, std = 6.7, min =-1.0, max = 21.2, t -value = -4.4, p -value = 0.002.
 5.4 Does knowing the query help? Overall, knowing the search queries used for data collection did not seem to improve the results. Three teams submitted 12 results both for A 1 -A 4 and C 1 -C 4 . The average F -measure difference, F ( C i ) -F ( A i ), i = 1 to 4, is not significant:
F ( C i ) -F ( A i ): N = 12, avg = 0.9, std = 1.8, min =-2.0, max = 5.0, t -value = -1.6, p -value = 0.06.

Again, the UCD-FC system differed from the other systems in that the A and C scores were identical, but even averaging over the remaining two systems and eight cases does not show a statistically significant advantage:
F ( C i ) -F ( A i ): N = 8, avg = 1.3, std = 2.2, min =-2.0, max = 5.0, t -value = -1.7, p -value = 0.07. 5.5 Are some relations harder to classify? Table 6 shows the best results for each relation in terms of precision, recall and F -measure, per team and system category. In Table 7 , column F BL presents the baseline F -measure (alltrue), while Acc BL gives the baseline accuracy score (majority). For all seven relations, the best system significantly outperforms the baseline. The category of the best-scoring system is B 4 except the submission by ILK, whose B 4 system scored second on the O RIGIN  X  X  NTITY relation.
 Table 6 suggests that some relations are more difficult to classify than others. The best F -measure ranges from 83.7% for P RODUCT  X  X  RODUCER to 68.6% for O RIGIN  X  X  NTITY . The absolute difference between the best and the baseline F -measure ranges from 23.3% for P ART  X  X  HOLE to 3.7% for P RODUCT  X  X  RODUCER , while for accuracy the difference ranges from 31.0% for C ONTENT  X  X  ONTAINER to
The F column of Table 6 shows the best result for each relation, but similar differences among the relations can be observed when all results are pooled. In Table 7 , the Average rank column computes the average rank of each relation in the ordered list of relations generated by each system. For example, P RODUCT  X  X  RODUCER is often listed as the first or the second easiest relation (with an average rank of 1.7), while O RIGIN  X  X  NTITY and T HEME  X  X  OOL are identified as the most difficult relations to classify (with average ranks of 6.0). 5.6 How important is the sentence context? For some pairs of nominals, we may be able to guess the appropriate semantic relation from the nominals alone, without looking at the sentence in which they appear. For other pairs, we need to look at the sentence in order to determine the relation. In this section, we estimate the importance of sentence context for the test data in Task 4.

WordNet defines default as  X  X  X n option that is selected automatically unless an alternative is specified X  X . Many word pairs have a default semantic relation, assumed to hold unless there is evidence to the contrary. For example, the default relation for the pair h flu, virus i is C AUSE  X  X  FFECT (virus, flu). For the pair h grain, alcohol i , the default relation is O RIGIN  X  X  NTITY (grain, alcohol).

A sentence that contains a given word pair may imply or explicitly state a semantic relation between the words in the pair, which may agree or disagree with the default relation. For example, in the sentence  X  X  X he mutant virus gave him a severe flu. X  X , the relation between flu and virus in the sentence is C AUSE  X  X  FFECT (virus, flu), which agrees with the default relation. On the other hand, in the sentence  X  X  X e poured alcohol on the grain and put it on fire. X  X , the relation between grain and alcohol disagrees with the default relation O RIGIN  X  X  NTITY (grain, alcohol).
One should recognize a default relation if it is what one would expect given the two nominals involved, without knowing the context X  X he remainder of the sentence. For example, in Fig. 3 , we expect an I NSTRUMENT  X  X  GENCY relation between guys and sledgehammer . The label Default =  X  X  X rue X  X  means that the target relation, I NSTRUMENT  X  X  GENCY (e2, e1), is indeed what we would expect for these nominals, without looking at the sentence.

Two annotators independently labeled 105 sentences from the test data X 15 sentences for each relation X  X ccording to whether the semantic relation was the default relation for the target pair. The initial agreement between the annotators, before comparing their labels and achieving consensus, was 83%. Table 8 shows the correspondence between the Default label and the label for the target relation (such
One third (35/105) of the sentences have default relations (Def = true). When a If an algorithm could perfectly guess the Default label, it would achieve 83% ((32 ? 55)/105) accuracy on guessing the truth or falsity of the relation. In principle, therefore, an algorithm could achieve 83% accuracy on our SemEval-2007 task if it looked only at the nominals and ignored the context.

If the target relation is the default relation (Def = true), then it is almost certain that the target relation is true (32/35 &amp; 91%). In other words, it is very unusual for a sentence directly to contradict our default assumptions (Def = true but Rel = false) on this dataset.

The variation in the truth or falsity of the Default label across the seven relations (Def = true) amount to roughly 33%. 6 Lessons learned Data annotation brought up very interesting issues. It confirmed some and invalidated others among our intuitions and preconceptions about semantic relations and what is relevant in identifying them. We had initially assumed that semantic relations are clear-cut, so that a pair of connected nominals is an instance of one and only one relation. We had also assumed that context need not unduly influence the relation to be assigned: linguistic knowledge and world knowledge give us enough evidence on how two particular concepts are related, so yet another sentence is unlikely to cause a fundamental change.

The annotation process has shown that semantic relations, like concepts, are vague. Certain examples are more prototypical of a particular relation, while others are closer to the gray boundary areas. Sometimes more than one relation may apply, as for the spatial relations C ONTENT  X  X  ONTAINER and P ART  X  X  HOLE , where an entity situations, refining the relation definition has helped sharpen the boundaries and thus facilitate the decision process.

How semantic relations depend on context is also a matter of degree. Certain relations X  X uch as P ART  X  X  HOLE or H YPERNYM  X  X  YPONYM  X  X re less context-depen-dent, and the relation arises from intrinsic properties of the concepts: a hand is part of a human body, an engine is part of a car, a cat is an animal, and papers are written materials. Even among the less context-dependent relations there are permanent and episodic ones: a cat is always an animal, but Hilary Clinton will be a senator only for a limited time. To decide whether a pair is an instance of C ONTENT  X  X  ONTAINER , for example, the context constrains the concepts to their required functionality (Labov 1973 ): a basket may or may not fill the role of container:  X  X  X he basket of apples was sitting on the counter X  X , but  X  X  X he apple fell next to the basket. X  X  The type of relation X  X ntrinsic or extrinsic, permanent or episodic X  X ay therefore affect the annotation process and our ability to identify and agree on the annotations. 7 Conclusion We have described a new semantic evaluation task, Classification of Semantic Relations between Nominals , and our work towards the goal of constructing a framework and a benchmark data set to allow for comparisons of methods for this task.

The success of the task X  X easured in the interest of the community and the performance of the participating systems X  X hows that the framework and the data are useful resources. This collection is now freely accessible, 3 to encourage further research in this domain and integration of semantic relation algorithms in high-end NLP applications, such as Machine Translation, Summarization and Question Answering.

The semantic relation annotation task has brought interesting insights into the nature of relations and the types of features relevant to their recognition. Like concept , relation is a vague notion. At the gray boundary area, some relations overlap. Their type X  X ntrinsic to the linked concepts or extrinsic, episodic or permanent X  X ay affect the ability of people and systems to recognize them; the relation type may also affect the features useful in describing the instances in this learning task. The data included both lexical and contextual information X  X ordNet senses, the query used and the sentential context X  X ll of it meant to facilitate the analysis of useful sources of information for determining the semantic relations between nominals. The higher results of the participating systems that used WordNet sense information have demonstrated the advantages of using such lexical-semantic information.

The discussions between annotators during the process of building the data are reflected in the definitions of the relation, which are also freely accessible. These can also serve as guidelines and examples for future annotation efforts, and for further analysis into the nature of semantic relations.
 References
