 Preference-based recommendation systems have transformed how we consume media. By analyzing usage data, these methods un-cover our latent preferences for items (such as articles or movies) and form recommendations based on the behavior of others with similar tastes. But traditional preference-based recommendations do not account for the social aspect of consumption, where a trusted friend might point us to an interesting item that does not match our typical preferences. In this work, we aim to bridge the gap be-tween preference-and social-based recommendations. We develop social Poisson factorization (SPF), a probabilistic model that incor-porates social network information into a traditional factorization method; SPF introduces the social aspect to algorithmic recommen-dation. We develop a scalable algorithm for analyzing data with SPF, and demonstrate that it outperforms competing methods on six real-world datasets; data sources include a social reader and Etsy. Recommender systems; probabilistic models; social networks.
Recommendation has become a core component in our online experience, such as when we watch movies, read articles, listen to music, and shop. Given information about what a user has consumed (e.g., items viewed, marked as  X  X avorites, X  or rated), the goal of recommendation is to suggest a set of unobserved items that she will like.

Most recommendation systems aim to make personalized sugges-tions to each user based on similar users X  histories. To solve this problem, matrix factorization algorithms are the workhorse methods of choice [20, 32]. Factorization algorithms use historical data to uncover recurring patterns of consumption, and then describe each user in terms of their varying preferences for those patterns. For example, the discovered patterns might include art supplies, holiday decorations, and vintage kitchenware; and each user has different preferences for each category. To perform recommendation, fac-torization algorithms find unmarked items of each user that are characteristic of her preferences.
 Figure 1: Observed and recommended items 1 for an Etsy user. The user is shown in the center, with friends on the sides. The top row is training items and the bottom row is the top recommen-dations from our model (SPF). Some items are recommended because they are favorites of the friends, and others because they match the general preferences of the user.

Many applications of recommendation contain an additional source of information: a social network. This network is increas-ingly available at the same platforms on which we read, watch, and shop. Examples include Etsy, Instagram, and various social readers. Researchers have found that users value the opinions of their friends for discovering and discussing content [18, 33], and online access to their network can reinforce this phenomenon.

Factorization approaches, however, cannot exploit this informa-tion. They can capture that you may enjoy an item because it matches your general preferences, but they cannot capture that you may enjoy another because your friend enjoyed it. Knowing your connections and what items your friends like should help better predict what you will enjoy.

In this paper we develop social Poisson factorization (SPF), a new Bayesian factorization method that accounts for the social aspect of how users consume items. (SPF is based on Poisson factorization [11], a new model that is particularly suited for implicit data.) SPF assumes that there are two signals driving each user X  X  clicks: her latent preferences for items (and the latent attributes of each) and the latent  X  X nfluence X  of her friends. 2 From observed Etsy product images courtesy of Amber Dubois and Ami Lahoff. Used with permission.
There is a large body of research literature on peer influence [22, 6, 30]. In this work we use the term to indicate the latent change in consumption due to social connections. data X  X hich contains both click histories and a social network X  SPF infers each user X  X  preferences and influences. Subsequently, it recommends items relating both to what a user is likely to be interested in and what her friends have clicked.

Figure 1 gives the intuition. The user is in the center. She clicked on items (on the top, connected to the user), has friends (to either side), and those friends have clicked on items too (top and bottom, connected to each friend). From this data, we can learn both about her preferences (e.g., for handmade soap) and about how much she is influenced by each of her friends (e.g., more strongly by the friend on the left). SPF recommends items on the bottom, based on both aspects of the data. It is important to be able to explain the origins of recommendations to users [15], and SPF can tell the user why an item was recommended: it can indicate friends ( X  X ou always trust Sally X ) and general item attributes ( X  X ou seem to like everything about ninjas X ) to describe the source of recommendations.
We use the language of users clicking on items. This is just a convenience X  X ur model applies just as easily for users purchasing, rating, watching, reading, and  X  X avoriting X  items. Our goal is to predict which of the unclicked items a user will want to click.
In the following, we develop the mathematical details behind the model (Section 2), derive an efficient learning algorithm (based on variational inference) for estimating it from data (Section 2, Appendix), and evaluate it on six real-world data sets (Section 3). In all cases, our social recommendation outperforms both traditional factorization approaches [11, 29] and previous recommendation methods that account for the network [14, 17, 24, 25, 34]. Related work. We first review previous research on using social networks to help recommend items to users. A crucial component of SPF is that it infers the influence that users have with each other. In previous work, some systems assume that user influence (sometimes called  X  X rust X ) is observed [27]. However, trust information beyond a binary yes/no is onerous for users to input, and thus observing trust beyond  X  X ollowing X  or  X  X riending X  is impractical in a large system. Others assume that trust is propagated [2] or computed from the structure of the network [10]. This is limited in that it ignores user activity, which can reveal the trust of a user for some parts of the network over others; SPF captures this idea. Information diffusion [8, 12] also relies on user activity to describe influence, but focuses on understanding the widespread flow of information. A final alternative is to compute trust from rating similarities be-tween users [9]. However, performing this computation in advance of fitting the model confounds general preference similarity with instances of influence X  X wo people with the same preferences might read the same books in isolation.

Other research has included social information directly into vari-ous collaborative filtering methods. Ref. [36] incorporates the net-work into pairwise ranking methods. Their approach is interesting, but one-class ranking methods are not as interpretable as factor-ization, which is important in many applications of recommender systems [15]. Refs. [25, 28, 34] have explored how traditional fac-torization methods can exploit network connections. For example, many of these models factorize both user-item data and the user-user network. This brings the latent preferences of connected users closer to each other, reflecting that friends have similar tastes. Refs [24, 35] incorporate this idea more directly by including friends X  latent representations in computing recommendations made for a user.
Our model has a fundamentally different approach to using the network to form recommendations. It seeks to find friends with different preferences to help recommend items to a user that are outside of her usual taste. For example, imagine that a user likes an item simply because many of her friends liked it too, but that it falls squarely outside of her usual preferences. Models that adjust their friends X  overall preferences according to the social network do not allow the possibility that the user may still enjoy this anomalous item. As we show in Section 3, using the social network in this way performs better than these previous approaches.
In this section we develop social Poisson factorization (SPF). SPF is a model for recommendation; it captures patterns in user activity using traditional signals X  X atent user preferences and latent item attributes X  X nd estimates how much each user is influenced by his or her friends X  observed clicks. From its estimate of influence, SPF recommends clicked items by influential friends even when they are not consistent with a user X  X  factorization-based preferences.
We first review Poisson factorization and give the intuition on our model. Then, we formally specify our model, describe how to form recommendations, and discuss how we learn the hidden variables. Background: Poisson factorization. SPF is based on Poisson factorization (PF) [11], a recent variant of probabilistic matrix fac-torization for recommendation. Let r ui be the count of how many times user u clicked item i . 3 PF assumes that an observed count r comes from a Poisson distribution. Its rate is a linear combination of a non-negative K -vector of user preferences u and a non-negative K -vector of item attributes  X  i , The user preferences and item attributes are hidden variables with Gamma priors. (Recall that the Gamma is an exponential family distribution of positive values.) Given a matrix of observed clicks, posterior inference of these hidden variables reveals a useful factor-ization: latent attributes describe each item and latent preference describe each user. These inferences enable personalized recom-mendations.

PF relates to the GaP topic model [5], and can be viewed as a type of Bayesian non-negative matrix factorization [21]. Ref. [11] shows that PF realistically captures patterns of user behavior, lends itself to scalable algorithms for sparse data, and outperforms traditional matrix factorization based on Gaussian likelihoods [11, 29]. Social Poisson factorization. In many settings, users are part of an online social network that is connected to the same platforms on which they engage with items. For some, such as Etsy, these networks are innate to the site. Others may have external data, e.g., from Facebook or LinkedIn, about the network of users.

We build on PF to develop a model of data where users click on items and where the same users are organized in a network. Social Poisson factorization (SPF) accounts for both the latent preferences of each user and the click patterns of her neighbors.

Consider the user whose items are shown in Figure 1. The intu-ition behind SPF is that there can be two reasons that a user might like an item. The first reason is that the user X  X  general preferences match with the attributes of the item; this is the idea behind Poisson factorization (and other factorization approaches). For example, the user of Figure 1 may inherently enjoy handmade soap. A second reason is that the user has a friend who likes the item, or perhaps a collection of friends who all like it. This possibility is not exposed by factorization, but captures how the user might find items that are outside of her general preferences. Without learning the influence of friends in Figure 1, the system could easily interpret the woven box as a general preference and recommend more boxes, even if the user doesn X  X  usually like them.
The theory around PF works on count data, but Ref. [11] shows that it works well empirically with implicit recommendation data, i.e., censored counts, as well. Figure 2: A conditional directed graphical model of social Pois-son Factorization (SPF) to show considered dependencies. For brevity, we refer to the set of priors a and b as ; for example,
SPF captures this intuition. As in PF, each user has a vector of latent preferences. However, each user also has a vector of  X  X nfluence X  values, one for each of her friends. Whether she likes an item depends on both signals: first, it depends on the affinity between her latent preferences and the item X  X  latent attributes; second, it depends on whether her influential friends have clicked it. Model specification. We formally describe SPF. The observed data are user behavior and a social network. The behavior data is a sparse matrix R , where r ui is the number of times user u clicked on item i . (Often this will be one or zero.) The social network is represented by its neighbor sets; N.u/ is the set of indices of other users connected to u . Finally, the hidden variables of SPF are per-user K -vectors of non-negative preferences u , per-item K -vectors of non-negative attributes  X  i , and per-neighbor non-negative user influences uv . Loosely, uv represents how much user u is influenced by the clicks of her neighbor, user v . (Note we must set the number of components K . Section 3 studies the effect of K on performance; usually we set it to 50 or 100.)
Conditional on the hidden variables and the social network, SPF is a model of clicks r ui . Unlike many models in modern machine learning, we specify the joint distribution of the entire matrix R by the conditionals of each cell r ui given the others, where r u;i denotes the vector of clicks of the other users of the i th item. 4 This equation captures the intuition behind the model, that the conditional distribution of whether user u clicks on item i is governed by two terms. The first term, as we said above, is the affinity between latent preferences u and latent attributes  X  second term bumps the parameter up when trustworthy neighbors v (i.e., those with high values of uv ) also clicked on the item. Figure 2 shows the dependencies between the hidden and observed variables as a conditional graphical model.

To complete the specification of the variables, we place gamma priors on all of the hidden variables. We chose the hyperparameters of the gammas so that preferences, attributes, and influences are sparse. (See Section 3 for details.) Forming recommendations with SPF. We have specified a prob-abilistic model of hidden variables and observed clicks. Given a U I click matrix R and a U U social network N , we analyze
We are specifying an exponential family model conditionally. This leads to a well-defined joint if and only if the natural parameters for each conditional are sums and products of the sufficient statistics of the corresponding conditionals of the conditioning set [3]. In our case, this is satisfied. the data by estimating the posterior distribution of the hidden pref-erences, attributes, and influences p. 1 W U ; X  1 W I ; 1 W U posterior places high probability on configurations of preferences, attributes, and influence values that best describe the observed clicks within the social network.

From this posterior, we can form predictions for each user and each of their unclicked items. For a user u and an unclicked item j , we compute where all expectations are with respect to the posterior. For each user, we form recommendation lists by making predictions for the user X  X  set of unclicked items and then ranking the items by these continuous-valued predictions. This is how we can use SPF to form a recommendation system.
 Learning the hidden variables with variational methods. Social PF enjoys the benefits of Poisson factorization and accounts for the network of users. However, using SPF requires computing the posterior. Conditioned on click data and a social network, our goal is to compute the posterior user preferences, item attributes, and latent influence values.

As for many Bayesian models, the exact posterior for SPF is not tractable to compute; approximating the posterior is our cen-tral statistical and computational problem. We develop an efficient approximate inference algorithm for SPF based on variational meth-ods [4, 19], a widely-used technique in statistical machine learning for fitting complex Bayesian models. 5 With our algorithm, we can approximate posterior expectations with very large click and network data (see Section 3).

Variational inference approximates the posterior by solving an optimization problem. We define a freely parameterized distribu-tion over the hidden variables, and then fit its parameters to be close to the posterior distribution. We measure  X  X loseness X  by the Kullback-Leibler divergence, which is an assymetric measure of distance between distributions. Finally, we use the fitted variational distribution as a proxy for the posterior, for example to compute the expectations we need on the right-hand side of Eq. 2.

We use the mean-field variational family, where each latent vari-able is independent and governed by its own varitional parameter. The latent variables are the user preferences u , item attributes  X  and user influences uv . The variational family is q.; X ;/ D Y This is a flexible family. For example each cell of each user X  X  pref-erence vector uk is associated with its own variational parameter uk . Thus, when fit to be close to the model X  X  posterior, the vari-ational parameters can capture each user X  X  unique interests, each item X  X  unique attributes, and each friend X  X  unique influence value.
With the family in place, variational inference solves the follow-ing optimization problem, q .; X ;/ D arg min Note that the data X  X he clicks and the network X  X nter the variational distribution through this optimization. Finally, we use the resulting variational parameters of q . / as a proxy for the exact posterior. This lets us use SPF to perform recommendation.

In the appendix we describe the details of how we solve the problem in Eq. 4 to find a local optimum of the KL divergence. We
Source code available at https://github.com/ajbc/spf. NCRR Etsy, where our alternate model SF achieves top performance. use a form of alternating minimization, iteratively minimizing the KL divergence with respect to each of the variational parameters while holding the others fixed. This leads to a scalable iterative algorithm, where each iteration runs on the order of the number of non-zero entries of the matrix. (In Section 3 we empirically compare the runtime of SPF with competing methods.) We now turn to an empirical study of SPF. In this section we study the performance of SPF. We compared SPF to five competing methods that involve a social network in recommendation [14, 17, 24, 25, 34] as well as two traditional fac-torization approaches [11, 29]. Across six real-world datasets, our methods outperformed all of the competing methods (Figure 3). We also demonstrate how to use SPF to explore the data, characterizing it in terms of latent factors and social influence. Finally, we assess sensitivity to the number of latent factors and discuss how to set hyperparameters on the prior distributions. Datasets and preprocessing. We studied six datasets. Table 1 summarizes their attributes. The datasets are:
These datasets include both explicit ratings on a star scale and binary data. Content consumption is binary when the data is implicit (a news article was viewed) or when the system only provides a binary flag (favoriting). With implicit data, non-Poisson models require us to subsample 0 X  X  so as to differentiate between items; in these instances, we randomly sampled negative examples such that each user has the same number of positive and negative ratings. Note that Poisson-based models implicitly analyze the full matrix without needing to pay the computational cost of analyzing the zeros [11].
For each dataset, we preprocessed the network. We removed network connections where the users have no items in common. Note this advantages both SPF and comparison models (though SPF can learn the relative influence of the neighbors).

Our studies divided the data into three groups: approximately 10% of 1000 users X  data are held-out for post-inference testing, 1% of all users X  data are used to assess convergence of the inference algorithm (see Appendix), and the rest is used to train. One exception is Ciao, where we used 10% of all users X  data to test. Competing methods. We compared SPF to five competing mod-els that involve a social network in recommendation: RSTE [24], TrustSVD [14], SocialMF [17], SoRec [25], and TrustMF [34]. also include probabilistic Gaussian matrix factorization (PMF) [29], because it is a widely used recommendation method. For each of these, we used the parameter settings that achieved best performance according to the example fits published on the LibRec website.
We can think of SPF having two parts: a Poisson factorization component and a social component (see Eq. 1). Thus we also compared SPF to each of these components in isolation, Poisson factorization [11] (PF) and social factorization (SF). SF is the influ-ence model without the factorization model. 7 We note that SF is a contribution of this paper.

Finally, we compare to two baselines, ordering items randomly and ordering items by their universal popularity.
 Metrics. We evaluate these methods on a per-user basis. For each user, we predict clicks for both held-out and truly unclicked items, and we rank these items according to their predictions. We denote the user-specific rank to be rank ui for item i and user u . A better model will place the held-out items higher in the ranking (giving smaller rank ui values on held-out items). We now introduce the normalized cumulative reciprocal rank (NCRR) metric to gauge this performance.

Reciprocal rank (RR) is an information retrieval measure; given a query, it is the reciprocal of the rank at which the first relevant document was retrieved. (Larger numbers are better.) Users  X  X uery X  a recommender system similarly, except that each user only has one query (e.g.,  X  X hat books should I read? X ) and they care not just about the first item that X  X  relevant, but about finding as many relevant items as possible.

Suppose user u has held out items D u . 8 We define the cumulative reciprocal rank to be: CRR can be interpreted as the ease of finding all held-out items, as higher numbers indicate that the held-out items are higher in the list. For example, a CRR of 0.75 means that the second and fourth items are in the held-out set, or are relevant to the user.
We used the LibRec library (librec.net) for all competing methods.
Social factorization has a technical problem when none of a user X  X  friends has clicked on an item; the resulting Poisson cannot have a rate of zero. Thus we add a small constant D 10 10 to the rate in social factorization X  X  model of clicks.
With binary data this is simply the full set of heldout items. When items have non-binary ratings, we threshold the set such to include only highly rated items ( 4 or 5 in a 5 -star system).

CRR behaves similarly to discounted cumulative gain (DCG), except it places a higher priority on high-rank items by omitting the log factor X  X t can be thought of as a harsher variant of DCG. Like DCG, it can be also be normalized. The normalized cumulative reciprocal rank (NCRR) is where the ideal variant in the denominator is the value of the metric if the ranking was perfect. To evaluate an entire model, we can compute average NCRR over all users, 1 U P u NCRR u . We will use this metric throughout this section.

Performance measured by NCRR is consistent with performance measured by NDCG, but NCRR is more interpretable X  X imple re-ciprocals are easier to understand than the reciprocal of the log.
Note we omit root-mean-square error (RMSE) as a metric. Im-provements in RMSE often do not translate into accuracy improve-ments for ranked lists [1, 7, 23, 31], especially with binary or im-plicit data. Our end goal here is item recommendation and not rating prediction X  X  X hich movie should I watch next? X  is inherently a ranking problem X  X hus we treat the predictions as means to an end.
We evaluate SPF by considering overall performance and perfor-mance as a function of user degree. We also show how to explore the data using the algorithm.
 Performance. Figure 3 shows the performance of SPF against the competing methods: the previous methods that account for the social network, social factorization (SF), Poisson factorization (PF), and the popularity baseline. (We do not illustrate the random baseline because it is far below all of the other methods.) SPF achieves top performance on five of the datasets. On the one remaining dataset, Etsy, the social-only variant of our model (SF) performs best.
Notice the strong performance of ranking by popularity. This highlights the importance of social factorization. It is only social Poisson factorization that consistently outperforms this baseline.
We measured runtime with the Ciao data set to get a sense for the relative computational costs. Figure 4 shows the runtime for all of the methods at various values of K . The Poisson models are average in terms of runtime.

Finally, using the Ciao and Epinions data, we break down the performance of SPF, SF, and PF as a function of the degree of each user; the results are shown in Figure 5. 9 All models perform better on high-degree users, presumably because these are higher activity users as well. Overall, SPF performs better than SF because of its advantage on the large number of low-degree users.
Smoothed with GAM. http://www.inside-r.org/r-doc/mgcv/gam Figure 4: Training and testing runtimes for multiple models on Ciao data, with the number of latent factors K ranging from 1 to 500. Each dot represents a full cycle of training and evaluating. SPF performs with average runtime.
 Figure 5: Performance on Ciao and Epinions broken down as a function of degree; grey in background indicates density of users. SPF and SF perform similarly, with SPF doing slightly better on a large number of low-degree users and SF doing better on a low number of high-degree users.
 Interpretability. It is important to be able to explain the origins of recommendations to users [15]. Items recommended with SPF have the advantage of interpretability. In particular, we use auxiliary variables (see Appendix) to attribute each recommendation to friends or general preferences; we then use these attributions to explore data.
When items are recommended because of social influence, the system may indicate a friend as the source of the recommendation. Similarly, when items are recommended because of general prefer-ences, the system may indicate already clicked items that exhibit that preference. On the Etsy data, learned item factors included coherent groupings of items such as mugs, sparkly nail polish, children X  X  toys, handmade cards, and doll clothes. Thus, SPF explains the rec-ommended the handmade soap in Figure 1 as coming from general preferences and the others items as coming from social influence. The social and preference signals will not always be cleanly sepa-rated; SPF attributes recommendations to sources probabilistically.
Figure 6 shows how the proportion of social attribution (as op-posed to general preference attribution) changes as a function of user degree on Ciao and Epinions. We observe that Epinions attributes a larger portion of behavior to social influence, controlled for user degree. Similarly, we can compute the contribution of users to their friends X  behavior. Figure 7 shows social contribution as a function of indegree; here we see that Epinions users with higher indegree have lower social contribution than low-indegree users.
 Figure 6: The proportion of social attribution (vs. general preference attribution) as a function of user degree. Attributions are calculated on all training data from Ciao and Epinions. Epinions attributes a larger portion of rating to social influence. Figure 7: Contribution to friends X  behavior as a function of indegree, calculated on all Epinions training data. Users with higher indegree have lower social contribution.
The details of our methods requires some decisions: we must choose the number of latent factors K and set the hyperparameters. Choosing the number of latent factors K . All factorization models, including SPF, require the investigator to select of the number of latent factors K used to represent users and items. We evaluated the sensitivity to this choice for the Ciao dataset. (We chose this dataset because of its smaller size; ranking millions of items for every user is computationally expensive for any model.) Figure 8 shows per-user average NCRR K varies from 1 to 500; SPF performs best on the Ciao dataset with K D 40 , though is less sensitive to this choice than some other methods (such as PF). Hyperparameters. We also must set the hyperparameters to the gamma priors on the latent variables. The gamma is parameterized by a shape and a rate. We followed [11] and set them to 0.3 for the priors on latent preferences and attributes. We set the hyperparame-ters for the prior on user influences to .2;5/ in order to encourage the model to explore explanation by social influence. In a pilot study, we found that the model was not sensitive to these settings. Does learning influence matter? We can easily fix each user-friend influence at 1, giving us local popularity among a user X  X  social connections. We compared fitted influence against fixed influence on both Ciao and Epinions and found that SPF with fitted influence performs best on both datasets.

In the case of cold-start users, where we know the user X  X  social network but not their click counts on items, SPF will perform equiva-lently to SF with fixed influence. SPF in this cold-start user scenario performs better than competing models. Figure 8: Model performance on Ciao data (measured as NCRR averaged over all users) as a function of number of latent fac-tors K . The dotted vertical line at K D 40 indicates the best performance for Poisson family models.
We presented social Poisson factorization, a Bayesian model that incorporates a user X  X  latent preferences for items with the latent influences of her friends. We demonstrated that social Poisson fac-torization improves recommendations even with noisy online social signals. Social Poisson factorization has the following properties: (1) It discovers the latent influence that exists between users in a social network, allowing us to analyze the social dynamics. (2) It provides a source of explainable serendipity (i.e., pleasant surprise due to novelty). (3) It enjoys scalable algorithms that can be fit to large data sets.

We anticipate that social Poisson factorization will perform well on platforms that allow for and encourage users to share content. Examples include Etsy, Pinterest, Twitter, and Facebook. We note that our model does not account for time X  X hen two connected users both enjoy an item, one of them probably consumed it first. Future work includes incorporating time, hierarchical influence, and topical influence. We thank Prem Gopalan, Jake Hofman, Chong Wang, Laurent Charlin, Rajesh Ranganath, and Alp Kucukelbir for their insights and discussions. We thank Etsy, Diane Hu in particular, for shar-ing data. We also thank LibRec X  X  creator, Guibing Guo. DMB is supported by NSF BIGDATA NSF IIS-1247664, ONR N00014-11-1-0651, and DARPA FA8750-14-2-0009. TER is supported by NSF CNS-1314603, by DTRA HDTRA1-10-1-0120, and by DAPRA under SMISC Program Agreement No. W911NF-12-C-0028. [1] X. Amatriain, P. Castells, A. de Vries, and C. Posse. [2] R. Andersen, C. Borgs, J. Chayes, U. Feige, A. Flaxman, [3] B. C. Arnold, E. Castillo, and J. M. Sarabia. Conditional [4] C. Bishop. Pattern Recognition and Machine Learning . [5] J. Canny. GaP: a factor model for discrete data. In SIGIR , [6] D. Crandall, D. Cosley, D. Huttenlocher, J. Kleinberg, and [7] P. Cremonesi, Y. Koren, and R. Turrin. Performance of [8] N. Du, L. Song, H. Woo, and H. Zha. Uncover topic-sensitive [9] S. Fazeli, B. Loni, A. Bellogin, H. Drachsler, and P. Sloep. [10] J. Golbeck and J. Hendler. FilmTrust: Movie [11] P. Gopalan, J. M. Hofman, and D. M. Blei. Scalable [12] A. Guille, H. Hacid, C. Favre, and D. A. Zighed. Information [13] G. Guo, J. Zhang, D. Thalmann, and N. Yorke-Smith. Etaf: [14] G. Guo, J. Zhang, and N. Yorke-Smith. TrustSVD: [15] J. L. Herlocker, J. A. Konstan, and J. Riedl. Explaining [16] M. Hoffman, D. Blei, C. Wang, and J. Paisley. Stochastic [17] M. Jamali and M. Ester. A matrix factorization technique with [18] J. Johnstone and E. Katz. Youth and popular music: A study [19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. [20] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization [21] D. D. Lee and H. S. Seung. Algorithms for non-negative [22] J. Leskovec, A. Singh, and J. Kleinberg. Patterns of influence [23] D. Loiacono, A. Lommatzsch, and R. Turrin. An analysis of [24] H. Ma, I. King, and M. R. Lyu. Learning to recommend with [25] H. Ma, H. Yang, M. R. Lyu, and I. King. SoRec: Social [26] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King.
 [27] P. Massa and P. Avesani. Trust-aware recommender systems. [28] S. Purushotham, Y. Liu, and C.-C. J. Kuo. Collaborative topic [29] R. Salakhutdinov and A. Mnih. Probabilistic matrix [30] S. Shang, P. Hui, S. R. Kulkarni, and P. W. Cuff. Wisdom of [31] P. Singh, G. Singh, and A. Bhardwaj. Ranking approach to [32] X. Su and T. M. Khoshgoftaar. A survey of collaborative [33] I. P. Volz. The impact of online music services on the demand [34] B. Yang, Y. Lei, D. Liu, and J. Liu. Social collaborative [35] M. Ye, X. Liu, and W.-C. Lee. Exploring social influence for [36] T. Zhao, J. McAuley, and I. King. Leveraging social In this appendix, we describe the details of the variational inference algorithm for SPF. This algorithm fits the parameters of the varia-tional distribution in Eq. 3 so that it is close in KL divergence to the posterior. We use coordinate ascent, iteratively updating each parameter while holding the others fixed. This goes uphill in the variational objective and converges to a local optimum [4].
To obtain simple updates, we first construct auxiliary latent vari-ables z . These variables, when marginalized out, leave the original model intact. Recall the additive property of the Poisson distribu-tion. Specifically, if r Poisson .a C b/ then r D z 1 C z z 1 Poisson .a/ and z 2 Poisson .b/ . We apply this decomposi-tion to the conditional click count distribution in Eq. 1. We define Poisson variables for each term in the click count: The M and S superscripts indicate the contributions from matrix fac-torization (general preferences) and social factorization (influence), respectively. Given these variables, the click count is deterministic, where V Dj N.u/ j and the index v selects a friend of u (as opposed to selecting from the set of all users).

Coordinate-ascent variational inference is derived from the com-plete conditionals, i.e., the conditional distributions of each variable given the other variables and observations. These conditionals de-fine both the form of each variational factor and their updates. For the Gamma variables X  X he user preferences, item attributes, and user influence X  X he conditionals are The complete conditional for the auxiliary variables is z ui j ; X ;; R ; N Mult . r ui ; ui / where (Intuitively, these variables allocate the data to one of the factors or one of the friends.) Each variational factor is set to the same family as its corresponding complete conditional.

Given these conditionals, the algorithm sets each parameter to the expected conditional parameter under the variational distribu-tion. (Thanks to the mean field assumption, this expectation will not involve the parameter being updated.) Note that under a gamma distribution, E  X  X  D a = b ; where a and b are shape and rate pa-rameters. For the auxiliary variables, the expectation of the indicator is the probability, E  X z ui  X  D r ui ui .
 Algorithm 1 shows our variational inference algorithm. It is O.N.K C V // per iteration, where N is the number of recorded user-item interactions (click counts, ratings, etc.). K is the number of latent factors, and V is the maximum user degree. (Note that both K and V are usually small relative to N .) We can modify the algorithm to sample users and update the variables stochastically [16]; this approach scales to much larger datasets than competing methods. Algorithm 1 Mean field variational inference SPF 1: initialize E  X  X ; E  X  X  X  randomly 2: for each user u do 3: for each friend v 2 N.u/ do 5: while log L &gt;  X  do F check for model convergence 7: for each user u do 10: init. preferences ;a u to prior a for all factors
To assess convergence, we use the change in the average click log likelihood of a validation set.
