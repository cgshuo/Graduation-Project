 Query logs record the actual usage of search systems and their analysis has proven critical to improving search engine functionality. Yet, despite the deluge of information, query log analysis often suffers from the sparsity of the query space. Based on the observation that most queries pivot around a single entity that represents the main focus of the user X  X  need, we propose a new model for query log data called the entity-aware click graph . In this representation, we decom-pose queries into entities and modifiers, and measure their association with clicked pages. We demonstrate the benefits of this approach on the crucial task of understanding which websites fulfill similar user needs, showing that using this representation we can achieve a higher precision than other query log-based approaches.
 H.3.3 [ Information Search and Retrieval ]; H.3.1 [ Content Analysis and Indexing ] Algorithms, Experimentation click graph, query logs, website similarity
For both search engine providers and Web site owners, query logs are among the most valuable sources of informa-tion on how users interact with online content. For search engines, query logs are indispensable for providing such cru-cial services as query completion and  X  X lso try X  suggestions.  X  P art of this work was done while the author was visiting Yahoo! Research Labs, Barcelona For site owners, search referrals and site logs make it pos-sible to discover the information needs of their users and optimize their presence on the web.

Most applications of query log analysis, however, suffer from the notable sparsity of the query space. In the case of Web search, Baeza-Yates [1] shows that 44% of the queries occur only once even when considering a full year of data. Query frequencies follow a power law, which means that a large fraction of the queries that appear more than once have very low frequency in general, and consequently offer a small number of clicks that can be used to determine their relationship to other queries.

As an illustration of the problem, Figure 1 shows the sets of queries that two hypothetical websites may receive in a query log. If we would try to measure the similarity 1 of these websites based on the overlap of these sets, we would find that no queries are shared between the two sites. However, it should be clear that the two websites are related based on the intent of these queries.

I n order to alleviate the effects of sparsity, practical stud-ies on the click graph usually exclude low frequency queries or treat queries as bag of words [1], which destroys the se-mantics of queries resulting in data loss and/or bias [9].
In this paper, we propose the entity-aware click graph model for query log representation, which relies on break-ing up queries into a named entity and a modifier, i.e. the remaining words that are not recognized as part of a named entity. Based on the manual annotation of 264 randomly se-lected queries from Yahoo! Search query logs, it was shown in recent work [20] that over 62% of the queries contain the name of an entity or type of entity that the user is trying to locate. In most of these queries, the name of the entity or type is surrounded by additional modifiers that narrow the search context, e.g. by specifying additional characteristic of the entity sought or by expressing the intent of the user with regard to the named entity or type. Context, however, is typically rather short, as users  X  accustomed to the con-
I n this work we interchangeably use the terms similarity and relatedness to loosely mean semantic relatedness. junctive semantics of queries  X  try to keep the number of q uery words to a minimum.

We will show that it is indeed possible to capture large portions of query log data using this model and that this treatment preserves the structure that is inherent to the query log data, while reducing sparsity. Figure 2 shows how this relatively simple parsing alleviates the sparsity of the query space.
 Figure 2: Queries from Figure 1 broken down into entity a nd modifier
As a demonstration of this model, we consider the prob-lem of finding websites that provide similar services to a user. We will show that a simple entity spotting approach that leverages Linked Data on the Web can be effective in identifying website intent. In our current study we will use a high quality subset of named entities provided by Freebase, a repository of collaboratively managed structured data ex-posed as Linked Data on the Web. By focusing on the en-tity and context dimensions separately, we will show that we can also identify two broad classes of websites, i.e. websites centered around particular entities vs. websites providing generic services.
Query log analysis is an important task in information retrieval and web mining research [16, 19, 5, 8].
Graph representations of query logs . One of the most widely used approaches for query log analysis is to model the query log as a graph. In a query similarity graph , two queries are connected by an undirected edge weighted by a given query similarity function (e.g. keyword overlap). As an alternative, Baeza-Yates and Tiberi [2] define a simi-larity metric based on common clicked URLs that has been shown to outperform keyword overlap. A second class of approaches models both queries and clicks. Craswell and Szummer introduce the click graph as a bipartite graph be-tween queries and URLs to improve search [8].

In practice, all of the above approaches suffer from the sparsity of the query space. In order to obtain a more con-nected graph, researchers often perform cleaning steps (e.g. removing all infrequent queries) resulting in a loss of infor-mation and a bias towards frequent queries. An alternative is to treat queries as a bag of words [2], which destroys the semantics of many queries. In comparison, our entity-aware click graph model leads to less loss of information and pre-serves query structure, while considerably attenuating the sparsity problem. h ttp://freebase.com
Query interpretation . The most common form of query interpretation is classification against a generic or domain specific taxonomy. Jansen, Booth and Spink [12] expanded on previous work in query intent classification (Broder, 2002 [6]; Rose and Levingson, 2004 [21]) to detail query intents in three hierarchical levels, and approached manually de-veloped rules to classify 1.5 million queries. Li et al. [13] also target the problem of query intent classification. They model the query logs as a click graph and infer class mem-berships of unlabeled queries from those of labeled ones ac-cording to their proximities in a click graph. Hu et al. [11] present an alternative methodology to intent classification that uses the Wikipedia graph instead of the click graph.
We do not perform explicit query classification in our work. We annotate queries with entity identifiers from well-known Linked Data sources, separating mentions of entities from modifiers, and use this new model to classify websites. A key insight of our work is the orthogonality of these dimen-sions which we will illustrate with examples (see Section 3). This leads to potential interpretation of queries  X  and by proxy, clicked websites  X  across two dimensions: the enti-ties and the modifiers found in a query. We will show that by separating these dimensions, we are able to outperform alternative methods that treat queries as a whole or as a bag of words on the task of computing website similarity.
Named Entity Recognition . A key aspect of our ap-proach is the recognition of the internal composition of queries containing named entities and context words. Several ap-proaches exist for the task of Named Entity Recognition (NER)[17] in the context of natural language text and in the context of query logs [18, 14, 10]. Determining the best entity extraction technique is out of the scope of this work. We focus on how to use the extracted entities and context words for analyzing the query logs.
A typical representation of a query log is the click graph, a bi-partite graph where the nodes are queries Q = { q 1 , ..., q and URLs  X  or, simplifying, the sites S = { s 1 , ..., s | S | ing the URLs. An edge connects a query q a to a site s c if there has been at least one search session in the data where a user clicked on a URL in that site after issuing the query, but before issuing another query.

The entity-aware click graph models relationships between entities and modifiers appearing in queries and clicked sites, and can be defined as the union of two bipartite-graphs. Let E = { e 1 , ..., e | E | } be the set of all entities and M = { m 1 , ..., m | M | } the set of all modifiers.
We define CG entity = ( E  X  S, ( e i , s j )) where an edge ( e , s i ) exists if a user searched with keywords containing the entity e i and visited site s j . Analogously, CG modifier edges ( m k , s l ) relating modifiers and sites. The entity-aware click graph is then defined as CG = CG entity  X  CG modifier
A key feature of the entity-aware click graph is that enti-ties and modifiers represent two distinct dimensions of the query space. As an illustration, we show the difference in which content vs. service-oriented websites behave along these two dimensions. The most common queries in our dataset (see Section 4 for more details) include X  X ahoo mail X ,  X  X acebook login X ,  X  X oogle search engine X ,  X  X ank of america online X  and  X  X elly blue book X . When breaking queries into entities and modifiers, the most common entities include  X  X ahoo X ,  X  X oogle X ,  X  X ells fargo X ,  X  X ank of america X  and X  X ace-book X , while the most common modifiers include  X  X yrics X ,  X  online X ,  X  X ames X ,  X  X ail X  and  X  X ank X . From the examples we can see that entities represent the topical content of the queries and the clicked websites, while modifiers commonly specify some service provision aspect with regard to the en-tity searched for and the website clicked.

In order to quantify the range of entities and modifiers that a website interacts with, we observe for each site the en-tropy of the probability distribution over entities and mod-ifiers. Entropy is a measure of the  X  X nformativeness X  of a probability distribution. The more concentrated is the dis-tribution, the less is its entropy; the more diffuse it is, the greater is its entropy. Formally, for a specific website s , we compute the negative entropies as: H e ( s ) = H ( E | s ) = P i P ( e i | s ) log P ( e i | s ). We compute H m analogously for mod-ifiers.

W ebsites that are associated with several entities will have a high H e , while those that are linked to few entities will have a low H e . In this sense, H e may be seen as a measure of entity specificity of a website: a more specialized con-tent provider exhibits a lower negative entropy with regard to entities. Conversely, H m can be seen as a measure of modifier specificity and, given the commonly observed mod-ifiers, possibly indicating the range of services provided by a website.

In Figure 3, we show a two-dimensional plot where each point represents a website. The vertical axis marks the en-tropy of the entity-conditional distribution P ( C | w ) and the horizontal axis marks P ( E | w ) the conditional distribution with regard to modifiers. Figure 3 shows that websites cover a wide range of values on both scales and there is a spread of sites across all four quadrants. For example, hp.com and sonyericsson.com are both on the bottom-right quadrant, not because they co-occur with the same entities or modi-fiers, but because they exhibit a similar behavior with regard to our query model. They both represent brands and there-fore relate to a small range of entities (names of the brand and models) while offering a wide-range of information from technical specifications to multimedia.
Website similarity analysis is useful for understanding the structure of the Web at a macro level and can answer ques-tions typically asked in the context of competitive analysis. Using a similarity measure, search engines can group the-matically similar sites and also extend the suggestions to other sites that are not present in the search result, but contain similar content (for informational queries) or pro-vide similar services (for transactional queries).
Given a bipartite click-graph, computing a website simi-larity graph is analogous to computing the query similarity graph [1]. This can be done, for example, by measuring the overlap of queries received by two web sites. In graph terms, this can be achieved by folding the bipartite click graph into a similarity graph, a one-mode graph where nodes are web sites and there is an edge between a pair of nodes if there was a path of length two connecting them in the bipartite graph. It is common to weight the resulting edges by the number of such paths.

As the example in figures 1 and 2 illustrate, website simi-larity graphs obtained from click graphs may fail to capture all relevant relationships. We propose to address this prob-lem using the aforementioned entity-aware click graph, as partitioning queries reveals relationships that were obscured by the monolithic treatment of queries in the regular click graph.

In the following, we define separate website similarity graphs based on the similarity between queries, entities, modifiers or the individual keywords that led to a click on two websites. Formally, we will represent each website using the vectors V different vector spaces spanned by queries, keywords, enti-ties and modifiers, respectively. In each case, we apply a tf-idf weighting to the coordinates instead of using simple counts as in the example. Then we compute the similarities between each pair of vectors in each of these spaces, e.g.
Based on the similarities, we define four website similar-the nodes represent websites and edges represent similari-ties based on similarity of queries, keywords, entities and modifiers, respectively, weighted by the corresponding simi-larity measure (e.g. Sim query for SG query ).

Since SG entity and SG modifier contain only partial infor-mation from each query-click pair, we also created similarity graphs containing the union of nodes and edges from entities and modifiers. We argue that for the task of website simi-larity, if we knew that a website focused on certain entities, it would be more important to find other websites focusing on the same entity, relegating the modifiers to a more aux-iliary role. Conversely, for service-focused websites it would be more important to give more weight to modifiers.
For websites that have been very entity-specific in the query logs, the fact that two websites share the same entity is more perplexing (therefore more informative). Thus, while building a union of SG entity and SG modifier , we can use a measure of perplexity to weigh individual entity/modifier components of a query. In Information Theory, perplexity is defined in terms of entropy as 2 H . Thus, we model the dy-namics between the query parts (entity, modifier) by the ra-tio between their perplexities: pRatio e ( s ) = 2  X  H e ( s ) Similarly, we define pRatio m by substituting the numerator by 2  X  H m ( s ) . A higher pRatio e results in a lower pRatio and vice versa.

We then generate SG ratio where each website is repre-sented by a vector resulting from the union of V e and V m after applying a scalar multiplication of each by the corre-sponding pRatio ( s ) weight, resulting in: V ratio ( s ) = V pRatio e ( s )  X  V m  X  pRatio m ( s ).

For comparison, we also tested a simple union of SG entity and SG modifier , which we reference as SG union .
For our experiments, we created a query log dataset by sampling 45,815,323 successful query sessions from the Jan-uary 2009 query logs of of Yahoo! Search. We have consid-ered a query session successful when the session has ended with a click [7]. We obtained a list of entities from Freebase data from May 2009 containing 5,600,250 named entities. We created our dictionary E from all entities provided, in-cluding both the high-quality curated entities and the user-defined ones, which tend to be less trustworthy.

From our query log, we have collected query hits consist-ing of the last query and the clicked URL. We then parsed the query keywords and collected every query hit that con-tained one of the entities in E followed or preceded by a mod-ifier. When faced with ambiguity, we pick the most frequent entity. The most frequent sense is a competitive baseline for word sense disambiguation, hard to beat in non-specialized domains by far more sophisticated approaches [15]. We in-dexed E in main memory and performed the extraction pro-cess in a distributed fashion using Hadoop. 3 After the ex-traction step we were left with 6,703,821 query hits where the query included at least one entity.

We used Pig over Hadoop to generate counts and simple statistics over this dataset. Although computing similarities between all pairs of click-graph nodes scales quadratically in complexity, we have used a similarity engine similar to Bayardo et al. [3] to speed up the computation. Distribution over a cluster allows us to scale linearly with the number of computers, allowing Web scale analysis.

Baselines . Based on the related work, we have two base-lines for our evaluation. We compute website similarities by treating entire queries as units ( SG query ), or breaking up queries into individual query words ( SG word ).

We also compare our results with a third baseline based on social tagging metadata. User provided tags from Deli-cious, for example, have been already shown to result in 8% better accuracy in classifying pages against the Open Direc-tory Project (ODP) taxonomy than representations based on the HTML content of Web pages [22]. Compared to our query log data, Delicious is also a strong baseline in that many websites have attracted significant tagging data over the many years of the existence of service, while our query log data covers only one month of activity.
 Using the Yahoo! BOSS API 4 we retrieved the top public Delicious tags T ( s ) for a site and the counts associated with each tag. Hence, we define Sim delicious ( s i , s j ) as the Jac-card coefficient between the tagsets of s i and s j in Delicious: Sim delicious = | T ( s i )  X  T ( s j ) | / | T ( s i )  X  T ( s
Gold standard . We used the ODP taxonomy as a gold standard, given that it has been extensively employed in h ttp://hadoop.apache.org http://developer.yahoo.com/search/boss/ evaluations [2, 9, 4]. ODP can provide a grounding to mea-sure website relatedness: websites that are in the same ODP categories are considered related. We use ODP to judge if each predicted edge in our similarity graphs connects related websites.

Let S odp be the set of websites from ODP that have been clicked more than 10 times in January 2009 in our logs. We will note as C ( s ) ,  X  s  X  S odp , the set of categories for each website s in the gold standard (ODP), and Rel odp ( s i , s function that assign a relevance score according to the gold standard. One natural choice would be to assign 1 if  X  c  X  C ( s i )  X  C ( s j ), i.e. the two sites share a category in ODP and 0 otherwise. However, such a measure ignores the taxonomy nature of ODP. Since ODP categories are defined as paths (e.g  X  Regional/Europe/Spain  X ), Baeza and Tiberi defined an ODP similarity function as the length of the common prefix between the categories of c i and c j divided by the longest path [2]. We considered two websites related if their categories overlapped by at least 2/3.

Results . We evaluate one website similarity graph at a time. For each website s i in S odp , we collect all edges ( s s , s j  X  S odp from the similarity graph, and assign a score with regard to the gold standard Rel odp .
 Table 1: Precision at 5 ( P @ 5) results and average number of edges returned Avg ( | E | ) for each similarity graph.
We use P@n as defined by Deng et al. for query similarity evaluation [9] and apply it to our website similarity evalu-ation, so that: P @ n = P n i =1 Rel odp ( s i , s j ) /n . We compute P@5 and average the values over all websites. This assesses the performance on our main intended task, i.e. suggesting a small number of similar websites to show to users in an online scenario.

Table 1 shows the performance of each similarity func-tion. The SG ratio graph provides a statistically significant improvement over the SG query and SG word baselines. The SG ratio graph is also a significant improvement over its com-ponents SG entity and SG modifier . Significance was tested with the Wilcoxon Matched-pair Signed-Ranks Test.
Table 2 focuses on the strongest similarity edges identified for each similarity graph ( Sim &gt; 0 . 9). The table shows the percentage of edges returned that were correct ( P ) and on average how similar were the categories of the websites connected by those edges ( Avg ( Sim ( s i , s j ))). The difference between the two measures is that the latter also captures partial matches. The results show that SG ratio returns less exact matches, i.e. pairs of sites whose categories are exactly the same, but shows higher overall quality of the returned results based on partial matching.
 Table 2: Precision of top correctly identified edges ( Sim &gt; 0.9).
Recall was less than 0.05% for all networks. We consider t his a natural result, as edge recall is limited by the query log data, i.e. it only measures the proportion of ODP we cover with a sample of one month of query log data. We note that, our objective is not to reconstruct ODP. Instead, we focus on finding, amongst websites being searched by users , those that are similar to each other. ODP here figures solely as a previously assigned, manually generated judgment of each website similarity edge.

Discussion . There are two major ways in which a correct similarity edge was not counted in this study. Many edges were found by our method but there was no classification available for (at least one of) the websites. We call this the  X  X nclassified websites X  problem. For those unclassified sites we could expect the precision to behave as in the subset of hosts that is in ODP.

Furthermore, since ODP is a manually created resource, it is a known fact that its classification is not exhaustive: users may use different category names, and possibly fail to include a website into a relevant category. Some websites were found by our methods but not computed as correct since they were not in overlapping categories in ODP. We call this the  X  X nsufficient classification X  problem.
The separation in entity and modifier can cause over spe-cialization in some graphs. In SG entity , for lufthansa-usa. com there is only an association with the website lufthansa. com with a similarity greater than 0.9, and these sites are associated because both are highly related to the entity  X  X ufthansa X  (and that is not the case for any other site). In the SG modifier graph the site lufthansa-usa.com is simi-lar to 46 other sites with a similarity greater than 0.9 and only one these sites are not an actual airline (but a travel site). In the SG word graph there is only one related site ( lufthansa.com ) with a similarity greater than 0.9, and there are only 7 sites with a similarity greater than 0.3 (six airline sites and one travel site).

As expected, Sim word yielded mistakes related to the dis-tortion of semantics in queries. For example, for the web-sites all-about-halloween.com and catlitterboxes.net we observe Sim word = 0 . 78, since there were queries search-ing for recipes of X  X at Litter Cake X , a type of cake commonly baked for Halloween. Breaking up that entity into words will make it seem closely related to websites that are focused on pets.
In this paper we have proposed an entity-aware repre-senting of query log data by interpreting search queries as consisting of an entity and modifiers. The method is scal-able to the entire Web. It is an unsupervised approach of query log interpretation, where computation can be done offline, distributed over a Hadoop cluster. We showed that this representation alleviates the problems of query sparsity in comparison to other models of query log data, while pre-serving enough of the semantics of queries to be useful in applications. We showed that even taken in isolation, enti-ties and modifiers are good predictors of website similarity. Moreover, the entity-aware models offer an insight into why those websites are related, namely provision of similar con-tent or similar services. This kind of analysis can be useful to several applications including query intent detection, search result diversification, vertical selection, amongst others.
By recognizing named entities originating from the Se-mantic Web, in the future we will also be able exploit the fact that background knowledge not only provides names of entities, but also provides the types of entities as well as the relationships among entities. As an example, moving from entities to types will allow characterizing websites at higher level of abstractions while the inter-relationships of entities could be exploited to improve measures of query similarity. [1] R. Baeza-Yates. Relating content through web usage. [2] R. Baeza-Yates and A. Tiberi. Extracting semantic [3] R. J. Bayardo, Y. Ma, and R. Srikant. Scaling up all [4] S. M. Beitzel, E. C. Jensen, D. D. Lewis, [5] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, [6] A. Broder. A taxonomy of web search. SIGIR X 10 . [7] D. Ciemiewicz, T. Kanungo, A. Laxminarayan, and [8] N. Craswell and M. Szummer. Random walks on the [9] H. Deng, I. King, and M. R. Lyu. Entropy-biased [10] J. Guo, G. Xu, X. Cheng, and H. Li. Named entity [11] J. Hu, G. Wang, F. Lochovsky, J.-t. Sun, and Z. Chen. [12] B. J. Jansen, D. L. Booth, and A. Spink. Determining [13] X. Li, Y.-Y. Wang, and A. Acero. Learning query [14] X. Li, Y.-Y. Wang, and A. Acero. Extracting [15] D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. [16] Q. Mei, D. Zhou, and K. Church. Query suggestion [17] Nadeau, David, Sekine, and Satoshi. A survey of [18] M. Pa  X sca. Weakly-supervised discovery of named [19] B. Poblete, C. Castillo, and A. Gionis. Dr. searcher [20] J. Pound, P. Mika, and H. Zaragoza. Ad-hoc object [21] D. E. Rose and D. Levinson. Understanding user goals [22] A. Zubiaga, R. Mart  X  X nez, and V. Fresno. Getting the
