
Chinese Academy of Sciences listwise ranking methods have very competitive ranking per formances [10]. of the objects.
 performance improvement. This validates the correctness o f our theoretical analysis. We review the permutation-level ranking framework propose d in [16]. whose elements are permutations of objects, and P learn a function that can minimize the expected risk R ( h ) , defined as, where l ( h ( x ) ,y ) is the true loss such that sort ( g ( x g mation of the true loss. In this way, the expected risk become s where g ( x ) = ( g ( x problem in many real applications. We next describe the real ranking problem, and then propose t he top-k ranking framework. 3.1 Top-k ranking problem [4, 5, 14]. We refer to it as the top-k ranking problem. 3.2 Top-k true loss positions in a ranked list, referred to as the top-k true loss. true loss is more general than the permutation-level 0-1 los s. With Eq.(5), the expected risk becomes where G all the permutations have the same top-k true loss; G ranking methods with the top-k true loss in the next section. loss functions in existing methods are consistent, and how t o make them consistent if not. 4.1 Statistical consistency Bayes ranker as defined in Eq.(7).
 Bayes ranker), when the training sample size approaches infi nity. We denote Q ( P ( y | x ) , g ( x )) as Q ( p , g ) , g ( x ) as g and P ( y | x ) as p of a permutation set in which each permutation ranks object i before object j , i.e., Y y the following definitions.
 Definition 1.  X  P Definition 2. A top-k subgroup probability space  X  i and j , if  X  y  X  Y have p same as in y .
 j : (1)  X  ( g ,y ) =  X  (  X   X  1 G k (  X  inequality holds; otherwise,  X  ( g ,y ) =  X  ( g , X   X  1 G k ( y (1) ,y (2) ,...,y ( k )) 6 = G k (  X  that if the top-k subgroup probability on a permutation y  X  Y  X  properties as shown below.
 G k ( y (1) ,y (2) ,...,y ( k )) and j with g top-k true loss as defined in Eq.(5). 7 and its detailed proof. For the detailed proof of Theorem 6, please refer to [15]. k minimizes Q ( p , g ) in Eq.(8), then g Proof. Without loss of generality, we assume i = 1 , j = 2 , g 0 First, we prove g Q ( p , g 0 )  X  Q ( p , g ) = X The first equation is based on the fact g 0 =  X   X  1  X  Q ( p , g 0 )  X  Q ( p , g ) = X where G Since g the optimality of g . Therefore, we must have g Second, we prove g Q ( p , g ) with respect to g 1 and g 2 to zero and compare them 3 , we have, After some algebra, we obtain, since at least one of components in the sum is negative accord ing to Definition 3. 4.2 Consistency with respect to k We discuss the change of the consistency conditions with res pect to various k values. First, we have the following proposition for the top-k subgroup probability space. der preserving with respect to objects 1 and 2 , then we have p and p p Proposition 8 holds for this case while the opposite does not . Second, we obtain the following proposition for the surroga te loss function  X  . then it is also top-( k + 1) subgroup order sensitive on the same set. g 1 &lt; g 2  X  (  X  ( order sensitive is a special case of top-2 subgroup order sensitive. According to the above propositions, we can come to the follo wing conclusions. function may not be consistent with the top-1 true loss.
 probabilities of permutations are p respectively, where p these surrogate loss functions. We will make discussions on this in the next subsection. 4.3 Consistent surrogate loss functions aforementioned surrogate loss functions are given as follo ws. 4.3.1 Likelihood loss The likelihood loss is the loss function used in ListMLE [16] , which is defined as below, defined with the Luce model [11]) in the above definition: It can be proved that the modified loss is top-k subgroup order sensitive (see [15]). 4.3.2 Cosine loss The cosine loss is the loss function used in RankCosine [13], which is defined as follows, where the score vector of the ground truth is produced by a map ping function  X  which retains the order in a permutation, i.e.,  X   X  the modification, the cosine loss becomes top-k subgroup order sensitive (see [15]). 4.3.3 Cross entropy loss The cross entropy loss is the loss function used in ListNet [3 ], defined as follows, and P (  X  | x ; g ) are the permutation probabilities in the Luce model. top-k subgroup order sensitive (see [15]). cally, we used OHSUMED, TD2003, and TD2004 in the LETOR bench mark dataset [10] to perform some experiments. As evaluation measure, we adopted Normal ized Discounted Cumulative Gain problems.
 We chose ListMLE as example method since the likelihood loss has nice properties such as con-corresponds to the original likelihood loss in ListMLE.
 Neural Network, since the model has been widely used [3, 13, 1 6]. The experimental results are summarized in Tables 1-3.
 Table 1: Ranking accuracies on OHSUMED order preserving for many different k values.
 Next, we take Top-10 ListMLE as an example to make comparison with some other baseline meth-N@1 and P@1, it significantly outperforms all the other metho ds on all the datasets. the original surrogate loss functions. Our experiments have shown that with the proposed modificati ons, algorithms like ListMLE can significantly outperform their original version, and also m any other ranking methods. with the top-k true loss. [14] C. Rudin. Ranking with a p-norm push. In Proc. of COLT , pages 589 X 604, 2006.
