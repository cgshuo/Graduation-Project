 Inference in topic models typically involves a sampling step to associate latent variables with observations. Unfortu-nately the generative model loses sparsity as the amount of data increases, requiring O ( k ) operations per word for k topics. In this paper we propose an algorithm which scales linearly with the number of actually instantiated topics k the document. For large document collections and in struc-tured hierarchical models k d k . This yields an order of magnitude speedup. Our method applies to a wide variety of statistical models such as PDP [ 16, 4] and HDP [ 19 ].
At its core is the idea that dense, slowly changing distri-butions can be approximated efficiently by the combination of a Metropolis-Hastings step, use of sparsity, and amortized constant time sampling via Walker X  X  alias method.
 Sampling; Scalability; Topic Models; Alias Method
Topic models are some of the most versatile tools for mod-eling statistical dependencies. Given a set of observations x i  X  X , such as documents, logs of user activity, or com-munications patterns, we want to infer the hidden causes motivating this behavior. A key property in topic models is that they model p ( x ) via a discrete hidden factor, z via ment. In this case it leads to Gaussian and Dirichlet mixture models [14 ]. When z is a vector of topics associated with in-dividual words, this leads to Latent Dirichlet Allocation [3 ]. Likewise, whenever z indicates a term in a hierarchy, it leads to structured and mixed-content annotations [ 19 , 2, 4, 12 ].
One of the key obstacles in performing scalable inference is to draw p ( z | x ) from the discrete state distribution associated with the data. A substantial improvement in this context was provided by [ 22 ] who exploited sparsity to decompose the collapsed sampler [9 ] for Latent Dirichlet Allocation. As a result the sampling cost can be reduced from O ( k ), the total number of topics to O ( k d + k w ), i.e. the number k topics occurring for a particular word w and k d for a partic-ular document d . This insight led to an order of magnitude improvement for sampling topic models, thus making their implementation feasible at a large scale. In fact, the strat-egy is sufficiently robust that it can be extended to settings where the topic smoother depends on the words [15 ].
For small datasets the assumption k d + k w k is well sat-isfied. Unfortunately, as the number of documents grows, so does the number of topics in which a particular word occurs. In particular k w  X  k , since the probability of observing any particular topic for a given word is rarely nonzero: Assume that the probability of occurrence for a given topic for a word is bounded from below by  X  . Then the probability of the topic occurring at least once in a collection of n docu-ments is given by From this it follows that k w = O ( k ) for n = O (  X   X  1 In other words, for large numbers documents the efficiencies discussed in [22 ] vanish. This is troubling, since in many in-dustrial settings n can be in the order of billions to trillions. Consequently, with increasing amounts of data, the time to process individual documents increases due to loss of spar-sity, thus leading to a superlinear increase in runtime.
On the other hand, the topic-sparsity for a given document essentially remains unchanged, regardless of the total num-ber of related documents that are available. This is due to the fact that the number of tokens per document is typically less than O ( k ). For instance, microblogs contain only dozens of words, yet admit to thousands of topics. 1 This situation is exacerbated when it comes to hierarchical and structured topic models, since there the number of (sub)topics can grow considerably more rapidly. Hence the use of sparsity is cru-cial in designing efficient samplers.
The present paper proposes a new decomposition of the collapsed conditional probability, in conjunction with a Obviously, this approach would not work to infer topics for Dostojevski X  X  War and Peace . That said, a plain topic model is an unlikely candidate to represent very long documents. Metropolis-Hastings [7 ] scheme and the use of the alias updates for random variables. This method is highly ver-satile. It defers corrections to the model and avoids renor-malization. This allows us to apply it to both flat and hier-archical models. Experimental evaluation demonstrates the efficacy of our approach, yielding orders of magnitude accel-eration and a simplified algorithm.

While we introduce our algorithm in the context of topic models, it is entirely general and applies to a much richer inference problems the model parameters only change rela-tively slowly during sampling. For instance, the location of cluster centers, the definition of topics, or the shape of au-toregressive functions, only change relatively slowly. Hence, if we could draw from a distribution over k outcomes k times, Walker X  X  alias method would allow us to generate samples in amortized constant time. At the same time, the Metropolis Hastings algorithm allows us to use approximations of the correct probability distribution, provided that we compute ratios between successive states correctly. Our approach is to draw from the stale distribution in constant time and to accept the transition based on the ratio between successive states. This step takes constant time. Moreover, the pro-posal is independent of the current state. Once k samples have been drawn, we simply update the alias table. In honor of the constitutent algorithms we refer to our technique as the Metropolis Hastings Walker (MHW) sampler.
We begin with a brief introduction to topic models and the associated inference problems. This includes a short motiva-tion of sampling schemes in the context collapsed samplers [9 , 18] and of stochastic variational models [ 21]. It is followed by a description of extensions to hierarchical models.
In LDA [ 3] one assumes that documents are mixture dis-tributions of language models associated with individual topics. That is, the documents are generated following the graphical model below: For each document d draw a topic distribution  X  d from a Dirichlet distribution with concentration parameter  X  For each topic t draw a word distribution from a Dirichlet distribution with concentration parameter  X  For each word i  X  { 1 ...n d } in document d draw a topic from the multinomial  X  d via Draw a word from the multinomial  X  z di via The beauty of the Dirichlet-multinomial design is that the distributions are conjugate. This means that the multino-mial distributions  X  d and  X  k can be integrated out, thus This yields a Gibbs sampler to draw p ( z di | rest) efficiently. The conditional probability is given by Here the count variables n td ,n tw and n t denote the num-ber of occurrences of a particular (topic,document) and (topic,word) pair, or of a particular topic respectively. More-over, the superscript  X   X  di denotes said count when ignoring ing the (topic,word) combination at position ( d,i ). Finally,  X   X  := P w  X  w denotes the joint normalization.

At first glance, sampling from ( 5) appears to cost O ( k ) time since we have k nonzero terms in a sum that needs to be normalized. [22 ] devised an ingenious strategy for exploiting sparsity by decomposing terms into p ( z di | rest)  X   X  w  X  t As can be seen, for small collections of documents only the first term is dense, and more specifically, P t  X  t / ( n can be computed from P t  X  t / ( n t +  X   X  ) in O (1) time. That is, whenever both n td and n tw are sparse, sampling from p ( z di | rest) can be accomplished efficiently. The use of packed index variables and a clever reordering of (topic,count) pairs further improve efficient sampling to O ( k w + k d ).
Stochastic variational inference [ 11] requires an analogous sampling step. The main difference being that rather than  X  tw associated with the conjugate variational distribution. Unfortunately this renders the model dense, unless rather careful precautions are undertaken [11 ] to separate residual dense and sparse components.

Instead, we devise a sampler to draw from p ( z di | rest) in amortized O ( k d ) time. We accomplish this by using Here the first term is sparse in k d and we can draw from it in O ( k d ) time. The second term is dense, regardless of the number of documents (this holds true for stochastic varia-tional samplers, too). However, the  X  X anguage model X  p ( w | t ) does not change too drastically whenever we resample a sin-gle word. The number of words is huge, hence the amount of change per word is concomitantly small. This insight forms the basis for applying Metropolis-Hastings-Walker sampling.
To illustrate the fact that the MHW sampler also works with models containing a dense generative part, we describe its application to the Poisson Dirichlet Process [ 4, 16]. The model is given by the following variant of the LDA model:
In a conventional topic model the language model is sim-ply given by a multinomial draw from a Dirichlet distribu-tion. This fails to exploit distribution information between topics, such as the fact that all topics have the same common underlying language. A means for addressing this problem is to add a level of hierarchy to model the distribution over  X  t via Q t p (  X  t |  X  0 ) p (  X  0 |  X  ) rather than Q t model is depicted above.
 The ingredients for a refined language model are a Pitman-Yor Topic Model (PYTM) [17 ] that is more appropriate to deal with natural languages. This is then combined with the Poisson Dirichlet Process (PDP) [16 , 4] to capture the fact that the number of occurences of a word in a natu-ral language corpus follows power-law. Within a corpus, the frequency of a word is approximately inversely proportional to its ranking in number of occurences. Each draw from a Poisson Dirichlet Process PDP( b,a, X  0 ) is a probability dis-tribution. The base distribution  X  0 defines the common un-derlying distribution shared across the generated distribu-tions. Under the settings of Pitman-Yor Topic Model, each topic defines a distribution over words, and the base dis-tribution defines the common underlying common language model shared by the topics. The concentration parameter b controls how likely a word is to occur again while being sampled from the generated distribution. The discount pa-rameter a prevents a word to be sampled too often by im-posing a penalty on its probability based on its frequency. The combined model described explicityly in [5 ]: As can be seen, the document-specific part is identical to LDA whereas the language model is rather more sophisti-cated. Likewise, the collapsed inference scheme is analogous to a Chinese Restaurant Process [ 6, 5]. The technical diffi-culty arises from the fact that we are dealing with distribu-tions over countable domains. Hence, we need to keep track of multiplicities, i.e. whether any given token is drawn from  X  i or  X  0 . This will require the introduction of additional count variables in the collapsed inference algorithm.
Each topic is equivalent to a restaurant. Each token in the document is equivalent to a customer. Each type of word corresponds each type of dish served by the restaurant. The same results in [6 ] can be used to derive the conditional probability by introducing axillary variables: The conditional probability is given by: p ( z di = t,r di = 0 | rest)  X  if no additional  X  X able X  is opened by word w di . Otherwise Here S N M,a is the generalized Stirling number. It is given by [4]. Moreover we have m t = P w m tw , and s t = P t s tw
Similar to the conditional probability expression in LDA, these two expressions can be written as a combination of a sparse term and a dense term, simply by splitting the factor (  X  t + n dt ) into its sparse component n dt and its dense counterpart  X  t . Hence we can apply the same strategy as before when sampling topics from LDA, albeit now using a twice as large space of state variables.
To illustrate the efficacy and generality of our approach we discuss a third case where the document model itself is more sophisticated than a simple collapsed Dirichlet-multinomial. We demonstrate that there, too, inference can be performed efficiently. Consider the two-level topic model based on the Hierarchical Dirichlet Process [19 ] (HDP-LDA). In it, the topic distribution for each document  X  d is drawn from a Dirichlet process DP( b 0 ,H (  X  )) governing the distribution over topics. In other words, we add an extra level of hierar-chy on the document side (compared to the extra hierarchy on the language model used in the PDP). More formally, the joint distribution is as follows: By construction, DP( b 0 ,H (  X  )) is a Dirichlet Process, equiva-lent to a Poisson Dirichlet Process PDP( b 0 ,a,H (  X  )) with the discount parameter a set to 0. The base distribution H ( . ) is often assumed to be a uniform distribution in most cases.
At first, a base  X  0 is drawn from DP( b 0 ,H (  X  )). This gov-erns how many topics there are in general, and what their overall prevalence is. The latter is then used in the next level of the hierarchy to draw a document-specific distribution  X  that serves the same role as in LDA. The main difference is that unlike in LDA, we use  X  0 to infer which topics are more popular than others.

It is also possible to extend the model to more than two levels of hierarchy, such as the infinite mixture model [ 19 ]. Similar to Poisson Dirichlet Process, an equivalent Chinese Restaurant Franchise analogy [6 , 19] exists for Hierarchi-cal Dirichlet Process with multiple levels. In this analogy, each Dirichlet Process is mapped to a single Chinese Restau-rant Process, and the hierarchical structure is constructed to identify the parent and children of each restaurant.
The general (collapsed) structure is as follows: let N j be the total number of customers in restaurant j and n the number of customers in restaurant j served with dish t . When a new customer (a token) enters restaurant j with the corresponding Dirichlet Process DP ( b j ,H j (  X  )), there are two types of seating arrangement for the customer: In the event that the customer sits at a new table, a phan-tom customer is sent upstream the hierarchy to the parent restaurant of j , denoted by j 0 , with corresponding Dirichlet Process DP ( b j 0 ,H j 0 (  X  )). The parent restaurant then decides the seating arrangement of the phantom customer under the same rules. This process repeats, until there is no more par-ent restaurant or any of phantom customer decides to sit in an existing table in any parent restaurant along the path.
We use the block Gibbs sampler given in [6] as it allows us to extend our approach for multi-level Hierarchical Dirichlet Process, and performs better than the samplers given in [19] and the collapsed Gibbs sampler given in [4 ], as measured in convergence speed, running time, and topic quality. than keeping track of relative assignments of tables to each other (and the resulting multiplicities and infrequent block moves) it simply keeps track of the level within the hierarchy of restaurants at which an individual customer opens a new table. The advantage is that this allows us to factor out the relative assignment of customers to specific tables but rather only keep track of the dishes that they consume. The obvious downside being that a small number of customers can be blocked from moves if they opened a table at a high position of the hierarchy that other customers depend upon. Improving mixing in this context is subject to future work.
In the setting studied above we only have a two-level HDP: that of the parent DP tying all documents together and the DP within each document, governing its topic distribution. We use z di  X  N to denote the topic indicator of word i at position d and u di  X  X  0 , 1 } to indicate whether a new table is opened at the root level (i.e. u di = 1). Moreover, define s td to be the table counter for document d , i.e. the number of times a table serving topic t has been opened, and let s be the associated counter for the base DP, associated with tables opened at the parent level. Finally, let s := P t s the total number of tables opened at the root level.
Clearly the situation where s t = 0 and u di = 0 is impossi-ble since this would imply that we are opening a new table at document d while there is no matching table available at the root level. Hence for the collapsed sampler we only need to consider the following cases: This amounts to the following (unnormalized) conditional probabilities. See [ 6] for further details. Expressions for the generalized form are analogous. Both forms contain a fraction with its numerator being the sum of a sparse term m tw and a dense term  X  w . Therefore, the conditional probability can be decomposed to a dense term multiplied by  X  w , and a sparse term multiplied by m tw . Ap-plying the same methodology, the sampling complexity of a multi-level HDP can be reduced to O ( k w ).
We now introduce the key components for the MHW al-gorithm and how to use it in sampling topics. They consist of the alias method [20, 13] and a simplified version of the Metropolis-Hastings sampler [ 7].
Typically, when drawing from a distribution over l out-comes, it is accepted that one would need to perform O ( l ) work to generate a sample. In fact, this is a lower bound, since we need to inspect each probability at least once before we can construct the sampler. However, what is commonly overlooked is that there exist algorithms that allow us to draw subsequent samples from the same distribution in O (1) time. This means that drawing l samples from a distribution over l outcomes can be accomplished in O (1) amortized time per draw. We make extensive use of this fact.

Denote by p i with i  X  X  1 ...l } the probabilities of a distri-bution over l outcomes from which we would like to sample. The algorithm works by decomposing a distribution over l events into l bins of equal probability by pairing at most two events per bin. Since it  X  X obs X  from the probabilities p i Hood X  method [ 13 ]. The algorithm proceeds as follows: 1: GenerateAlias ( p,l ) 2: Initialize L = H =  X  and A = []. 3: for i = 1 to l do 4: if p i  X  l  X  1 then 5: L  X  L  X  X  ( i,p i ) } 6: else 7: H  X  H  X  X  ( i,p i ) } 8: end if 9: end for 10: while L 6 =  X  do 11: Extract ( i,p i ) from L and ( h,p h ) from H 12: A  X  [ A, ( i,h,p i )] 13: if p h  X  p i &gt; l  X  1 then 14: H  X  H  X  X  ( h,p h  X  p i ) } 15: else 16: L  X  L  X  X  ( h,p h  X  p i ) } 17: end if 18: end while 19: return A This yields an array A containing triples ( i,h,p h ) with p h &lt; l  X  1 . It runs in O ( l ) time since at each step one event is removed from the list. And the probabilities remain un-changed, as can be seen by induction. All we need to do now is to draw a random element from A and flip a biased coin to accept h or i with probability lp h and 1  X  lp h respectively. 1: SampleAlias ( A,l ) 2: bin = RandInt( l ) 3: ( i,h,p ) = A [bin] 4: if lp &gt; RandUnif(1) then 5: return h 6: else 7: return i 8: end if
Note that the alias method works since we are implicitly exploiting parallelism inherent in CPUs: as long as l does not exceed 2 64 are guaranteed that even an information the-oretically inefficient code will not require more than 64 bit, which can be generated in constant time.
Whenever we draw l identical samples from p it is clear that the above algorithm provides an O (1) sampler. How-ever, if p changes, it is difficult to apply the alias sampler directly. To address this, we use rejection sampling and Metropolis-Hastings procedures. Rejection sampling pro-ceeds as follows: 1: Rejection ( p,q,c ) 2: repeat 3: Draw i  X  q ( i ) 4: until p ( i )  X  cq ( i )RandUnif(1) 5: return i Here p is the distribution we would like to draw from, q is a reference distribution that makes sampling easy, and c  X  1 of samples to draw via Rejection ( p,q,c ) is c , provided that a good bound c exists. In this case we have the following: Lemma 1 Given l distributions p i and q over l outcomes satisfying c i q  X  p i , the expected amortized runtime complex-ity for drawing using SampleAlias ( A,l ) and rejecting using Rejection ( p i ,q,c i ) is given by O 1 l P l i =1 c i .
Proof. Preprocessing costs amortized O (1) time. Each rejection sampler costs O ( c i ) work. Averaging over the draws proves the claim.
 In many cases, unfortunately, we do not know c i , or comput-ing c i is essentially as costly as drawing from p i itself. More-over, in some cases c i may be unreasonably large. In this sit-uation we resort to Metropolis Hastings sampling [ 7] using a stationary proposal distribution. As in rejection sampling, we use a proposal distribution q and correct the effect of sam-pling from the  X  X rong X  distribution by a subsequent accep-tance step. The main difference is that Metropolis Hastings can be considerably more efficient than Rejection sampling since it only requires that the ratios of probabilities are close rather than requiring knowledge of a uniform upper bound on the ratio. The drawback is that instead of drawing iid samples from p we end up with a chain of dependent sam-ples from p , as governed by q .

For the purpose of the current method we only need to concern ourselves with stationary distributions p and q , i.e. special case below. For a more general discussion see e.g. [ 8]. 1: StationaryMetropolisHastings ( p,q,n ) 2: if no initial state exists then i  X  q ( i ) 3: for l = 1 to n do 4: Draw j  X  q ( j ) 6: i  X  j 7: end if 8: end for 9: return i As a result, provided that p and q are sufficiently similar, the sampler accepts most of the time. This is the case, e.g. when-ever we use a stale variant of p as the proposal q . Obviously, which holds, e.g. whenever we incorporate a smoother.
In combining both methods we arrive at, what we believe is a significant improvement over each component individu-ally. It works as follows: 1: Initialize A  X  GenerateAlias ( p,l ) 2: for i = 1 to N n do 3: Update q as needed 4: Sample j  X  StationaryMetropolisHastings ( p,A,n ) 5: end for Provided that the sampler mixes within n rounds of the MH-procedure, this generates draws from up-to-date versions of p . Note that a further improvement is possible whenever we can start with a more up-to-date draw from p , e.g. in the case of revisiting a document in a topic model. After burn-in the previous topic assignment for a given word is likely to be still pertinent for the current sampling pass.
 Lemma 2 If the Metropolis Hastings sampler over N out-comes using q instead of p mixes well in n steps, the amor-tized cost of drawing n samples from q is O ( n ) per sample. This follows directly from the construction of the sampler and the fact that we can amortize generating the alias table. Note that by choosing a good starting point and after burn-in we can effectively set n = 1.
We now have all components necessary for an accelerated sampler. The trick is to recycle old values for p ( w di | z when they change slightly and then to correct this via a Metropolis-Hastings scheme. Since the values change only slightly, we can therefore amortize the values efficiently. We and extend it to hierarchical models subsequently.
We now design a proposal distribution for ( 6). It involves computing the document-specific sparse term exactly and approximating the remainder with slightly stale data. Fur-thermore, to avoid the need to store a stale alias table A , we simply draw from the distribution and keep the samples. Once this supply is exhausted we compute a new table. Alias table: Denote by the alias normalization and the associated probability dis-tribution. Then we perform the following steps: 1. Generate the alias table A using q w . 2. Draw k samples from q w and store them in S w . 3. Discard A and only retain Q w and the array S w . Generating S w and computing Q w costs O ( k ) time. In par-ticular, storage of S w requires at most O ( k log 2 k ) bits, thus it is much more compact than A . Note, however, that we need to store Q w and q w ( t ).
 Metropolis Hastings proposal: Denote by
P the sparse document-dependent topic contribution. Com-proposal distribution To perform an MH-step we then draw from q ( t ) in O ( k amortized time. The step from topic s to topic t is accepted with probability min(1 , X  ) where  X  = Note that the last fraction effectively removes the normal-ization in p dw and q w respectively, that is, we take ratios of unnormalized probabilities.
 Complexity: To draw from q costs O ( k d ) time. This is so since computing P dw has this time complexity, and so does the sampler for p dw . Moreover, drawing from q w ( t ) is O (1), hence it does not change the order of the algorithm. Note that repeated draws from q are only O (1) since we can use the very same alias sampler also for draws from p dw . Finally, evaluating  X  costs only O (1) time. We have the following: Lemma 3 Drawing up to k steps in a Metropolis-Hastings proposal from p ( z di | rest) can be accomplished in O ( k tized time per sample and O ( k ) space. Following the same steps as above, the basic Poisson Dirichlet Process topic model can be decomposed by ex-ploiting the sparsity of n dt . The main difference to before is that we need to account for the auxiliary variable r  X  X  0 , 1 } rather than just the topic indicator t . The alias table is: q w ( t,r ) := Likewise, the sparse document-specific contribution is p dw ( t,r ) := n dt As previously, computing p dw ( t,r ) only costs O ( k which allows a proposal distribution very similar to the case in LDA to be constructed: As before, we use a Metropolis-Hastings sampler, although before by using the ratio of current and stale probabilities (the latter given by q ). As before in the context of LDA, the time complexity of this sampler is amortized O ( k d ).
Due to slight differences in the nature of the sparse term and the dense term, we demonstrate the efficacy of our ap-proach for sparse language models here. That is, we show that whenever the document model is dense but the lan-guage model sparse, our strategy still applies. In other words, this sampler works at O ( k w ) cost which is beneficial for infrequent words.
 For brevity, we only discuss the derivation for the two level HDP-LDA, given that the general multi-level HDP can be easily extended from the derivation. Recall ( 9). Now the alias table is now given by: and the exact term is given by As before, we engineer the proposal distribution to be a com-bination of stale and fresh counts. It is given by using straightforward Metropolis-Hastings acceptance ra-tios. We omitted the subscript w di = w for brevity. The same argument as above shows that the time complexity of our sampler for drawing from HDP-LDA is amortized O ( k w ).
To demonstrate empirically the performance of the alias method we implemented the aforementioned samplers in both their base forms that have O ( k ) time complexity, as well as our alias variants which have amortized O ( k time complexity. In addition to this, we implemented the SparseLDA [ 22 ] algorithm with the full set of features includ-ing the sorted list containing a compact encoding of n tw n dt , as well as dynamic O (1) update of bucket values. Be-yond the standard implementation provided in MalletLDA AliasLDA, AliasHDP and AliasPDP.
 Figure 2: Perplexity as a function of runtime (in sec-onds) for PDP, AliasPDP, HDP, and AliasHDP on GPOL (left) and Enron (right).
 Figure 3: Runtimes of SparseLDA and AliasLDA on
PubMedSmall (left) and NyTimes (right). by [22 ], we made two major improvements: we accelerated the sorting algorithm for the compact list of encoded values to amortized O (1); and we avoided hash maps which sub-stantially improved the speed in general with small sacrifice of memory efficiency (we need an inverted list of the indices and an inverted list of the indices of the inverted lists). In this section these implementations will be referred as LDA which is O ( k ), SparseLDA which is O ( k w + k d ), AliasLDA which is O ( k d ), PDP at O ( k ) [5], AliasPDP at O ( k d ), HDP at O ( k ) [6], and AliasHDP at O ( k w
All our implementations are written in C++11 in a way that maximise runtime speed, compiled with gcc 4.8 with -O3 compiler optimisation in amd64 architecture. All our ex-periments are conducted on a laptop with 12GB memory and an Intel i7-740QM processor with 1.73GHz clock rate, 4  X  256KB L2 Cache and 6MB L3 Cache. Furthermore, we only use one single sampling thread across all experiments. Therefore, only one CPU core is active throughout and only 256KB L2 cache is available. We further disabled Turbo Boost to ensure all experiment are run at exactly 1.73GHz clock rate. Ubuntu 13.10 64bit served as runtime.

We use 5 datasets with a variety in sizes, vocabulary length, and document lengths for evaluation, as shown in Ta-ble 1 . RedState dataset contains American political blogs crawled from redstate.com in year 2011. GPOL contains a subset of political news articles from Reuters RCV1 collec-tion. 2 We also included the Enron Email Dataset, 3 . NY-Times contains articles published by New York Times be-tween year 1987 and 2007. PubMedSmall is a subset of ap-proximately 1% of the biomedical literature abstracts from PubMed. Stopwords are removed from all datasets. Further-more, words occurring less than 10 times are removed from NYTimes , Enron , and PubMedSmall . NYTimes , En-ron , and PUBMED datasets are available at [ 1].
We evaluate the algorithms based on two metrics: the amount of time elapsed for one Gibbs sampling iteration and perplexity. The perplexity is evaluated after every 5 it-erations, beginning with the first evaluation at the ending of the first Gibbs sampling iteration. We use the standard held-out method [10 ] to evaluate test perplexity, in which a small set of test documents originating from the same collec-tion is set to query the model being trained. This produces an estimate of the document-topic mixtures  X   X  dt for each test document d . From there the perplexity is then evaluated as:  X  ( W | rest) := p ( w d | rest) = Here we obtain the estimate of p ( w i = w | z di = t, rest) from the model being trained. To avoid effects due to variation in the number of topics, we hardcoded k = 1024 for all experi-ments except one (GPOL) where we vary k and observe the effect on speed per iteration. We use fixed values for hyper-parameters in all our models, setting  X  =  X  = 0 . 1 for LDA, a = 0 . 1, b = 10, and  X  = 0 . 1 for the PDP, and b 0 = b  X  = 0 . 1 for the HDP. For alias implementations, we fix the number of Metropolis-Hasting sampling steps at 2, as we observed a satisfactory acceptance rate (over 90%) at these settings. Only a negligible improvement in perplexity was observed by raising this value. Furthermore, we did not ob-serve degraded topic quality even when Metropolis-Hasting
Reuters Vol. 1, English language, 1996-08-20 to 1997-08-19
Source: www.cs.cmu.edu/  X enron sampling step was reduced to n = 1, and in all our experi-ments the perplexity almost perfectly converges at the same pace (i.e. along number of iterations) with the same algo-rithm without applying alias method (albeit with much less time per iteration).
 Table 1: Datasets and their statistics. V: vocabulary size; L: total number of training tokens, D: number of training documents; T: number of test documents. L/V is the average number occurrences of a word.
 L/D is the average document length.
Figure 6 shows the overall performance of perplexity as a function of time elapsed when comparing SparseLDA vs AliasLDA on the four larger datasets. When k is fixed to 1024, substantial performance in perplexity over run-ning time on all problems with the exception of the Enron dataset, most likely due to its uniquely small vocabulary size. The gap in performance is increased as the datasets become larger and more realistic in size. The gain in per-formance is noted in particular when the average document length is smaller since our sampler scales with O ( k d ) which is likely to be smaller for short documents.

Figure 2 gives the comparison between PDP, HDP and their aliased variants on GPOL and Enron. By the time AliasPDP and AliasHDP are converged, the straightforward sampler are still at their first few iterations.
In the following we evaluate the performance in two sep-arate parts: perplexity as a function of iterations and run-time vs. iterations. We first establish that the acceleration comes at no cost of degraded topic quality, as shown in Figure 6. The convergence speed and converged perplex-ity of AliasLDA, AliasPDP, and AliasHDP almost perfectly match the non-alias counterparts. This further shows that our choice of relatively small number of Metropolis-Hasting steps (2 per sample) is adequate.

The improved performance in running time of our alias implementations can be seen in all phases of sampling when compared to non-alias standard implementations (LDA, PDP, HDP). When compared to SparseLDA (Figure 3), the performance gain is salient during all phases on larger datasets (except for the early stage in Enron dataset), and the performance is very close on small datasets (0 . 35s per it-eration on AliasLDA vs. 0 . 25s per iteration on SparseLDA). As the size of the data grows AliasLDA outperforms SparseLDA without degrading topic quality, reducing the amount of time for each Gibbs iteration on NYTimes corpus by around 12% to 38% overall, on Enron corpus by around 30% after 30 iterations, and on PubMedSmall corpus by 27%-60% throughout the first 50 iterations. Compared to SparseLDA, the time required for each Gibbs iteration with AliasLDA grows at a much slower rate, and the benefits of reduced sampling complexity is particularly clear when the average length of each document is small. Figure 4: Comparison of SparseLDA and AliasLDA on GPOL when varying the number of topics for k  X  X  256 , 1024 , 2048 , 4096 } .
 Figure 5: Average runtime per iteration when com-pared on { 10% , 20% , 40% , 75% , 100% } of the PubMedS-mall dataset for SparseLDA and AliasLDA.

The gap in performance is especially large for more so-phisticated language modelsl such as PDP and HDP. The running time for each Gibbs iteration is reduced by 60% to 80% for PDP, and 80% to 95% for HDP, an order of magni-tude on improvement.
When the number of topics k increases, the running time for an iteration of AliasLDA increases at a much lower rate than SparseLDA, as seen from Figure 4 on dataset GPOL since k d is almost constant. Even though the gap between SparseLDA and AliasLDA may seem insignificant at k = 1024, it becomes very pronounced at k = 2048 (45% improvement) and at k = 4096 (over 100%) This con-firms the observation above that shorter documents benefits more from AliasLDA in the sense that the average docu-ments length L/D relative to the number of topics k be-comes  X  X horter X  as k increases. This yields a more sparse n and lower k d for a document d on average.
Figure 5 demonstrates how the gap in running time speed scales with growing number of documents in the same do-main. We measure the average runtime for the first 50 Gibbs iterations on 10%, 20%, 40%, 75%, and 100% of PubMedS-mall dataset. The speedup ratio for each subset is at 31%, 34%, 37%, 41%, 43% respectively. In other words, it in-creases with the amount of data, which conforms our in-tuition that adding new documents increases the density of n tw , thus slowing down the sparse sampler much more than the alias sampler, since the latter only depends on k d rather than k d + k w .
 Figure 6: Perplexity as a function of runtime (left) and number of iterations (right) for LDA, SparseLDA, and LDA, PDP and HDP, both with and without using the Alias method. We see consid-erable acceleration at unchanged perplexity.
In this paper, we described an approach that effectively reduces sampling complexity of topic models from O ( k ) to O ( k d ) in general, and from O ( k w + k d ) (SparseLDA) to O ( k (AliasLDA) for LDA topic model. Empirically, we showed that our approach scales better than existing state-of-the-art method when the number of topics and the number of documents become large. This enables many large scale ap-plications, and many existing applications which require a scalable distributed approach. In many industrial applica-tions where the number of tokens easily reaches billions, these properties are crucial and often desirable in design-ing a scalable and responsive service. We also demonstrated an order of magnitude improvement when our approach is applied to complex models such as PDP and HDP. With an order of magnitude gain in speed, PDP and HDP may be-come much more appealing to many applications for their superior convergence performance, and more sophisticated representation of topic distributions and language models.
For k = 1024 topics the number of tokens processed per second in our implementation is beyond 1 million for all datasets except one (NYTimes), of which contains substan-tially more lengthy documents. This is substantially faster than many known implementations when measured in num-ber of tokens processed per computing second per core, such as YahooLDA [ 18], and GraphLab, given that we only utilise a single thread on a single laptop CPU core.

Acknowledgments: This work was supported in part by a resource grant from amazon.com , a Faculty Research Grant from Google, and Intel. [1] K. Bache and M. Lichman. UCI machine learning [2] D. Blei, T. Griffiths, and M. Jordan. The nested [3] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet [4] W. Buntine and M. Hutter. A bayesian review of the [5] C. Chen, W. Buntine, N. Ding, L. Xie, and L. Du. [6] C. Chen, L. Du, and W. Buntine. Sampling table [7] J. Geweke and H. Tanizaki. Bayesian estimation of [8] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter. [9] T. Griffiths and M. Steyvers. Finding scientific topics. [10] G. Heinrich. Parameter estimation for text analysis. [11] M. Hoffman, D. M. Blei, C. Wang, and J. Paisley. [12] W. Li, D. Blei, and A. McCallum. Nonparametric [13] G. Marsaglia, W. W. Tsang, and J. Wang. Fast [14] R. M. Neal. Markov chain sampling methods for [15] J. Petterson, A. Smola, T. Caetano, W. Buntine, and [16] J. Pitman and M. Yor. The two-parameter [17] I. Sato and H. Nakagawa. Topic models with [18] A. J. Smola and S. Narayanamurthy. An architecture [19] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical [20] A. J. Walker. An efficient method for generating [21] C. Wang, J. Paisley, and D. M. Blei. Online [22] L. Yao, D. Mimno, and A. McCallum. Efficient
