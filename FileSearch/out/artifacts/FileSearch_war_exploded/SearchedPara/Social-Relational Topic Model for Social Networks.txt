 Social networking services, such as Twitter and Sina Weibo, have tremendous popularity in recent years. Mass of short texts and social links are aggregated into these service plat-forms. To realize personalized services on social network, topic inference from both short texts and social links plays more and more important role. Most conventional topic modeling methods focus on analyzing formal texts, e.g., pa-pers, news and blogs, and usually assume that the links are only generated by topical factors. As a result, on social network, the learned topics of these methods are usually af-fected by topic-irrelevant links. Recently, a few approaches use artificial priors to recognize the links generated by the popularity factor in topic modeling. However, employing global priors, these methods can not well capture the dis-tinct properties of each link and still suffer from the effect of topic-irrelevant links. To address the above limitations, we propose a novel Social-Relational Topic Model (SRTM), which can alleviate the effect of topic-irrelevant links by an-alyzing relational users X  topics of each link. SRTM jointly models texts and social links for learning the topic distribu-tion and topical influence of each user. The experimental results show that, our model outperforms the state-of-the-arts in topic modeling and social link prediction.
 Topic Modeling; Social Networks; Social Link Generation
Exploring users X  topics on social networks from rich texts and social links is important for marketing activities in real applications. Although a lot of works, e.g., Link-LDA [1], RTM [2] and RankTopic [3], have been proposed for this task, most of them assume that the social links are purely caused by topical factors and are used as the supplement of texts in topic inference. Clearly, this assumption is not suitable for social networks, since many topic-irrelevant fac-tors which can lead to a link generation. For example, on Figure 1: A toy example of Twitter data, which shows Twitter, President Obama associates with political topics and has a lot of followers. But, as far as we know, many of Obama X  X  followers are not really interested in politics, and these following relationships may be caused by the so-called bandwagon effect [4]. As a result, if we could not recog-nize such topic-irrelevant links which are not really caused by users X  interesting topics, the learned topics tend to be affected by these noise links and are not reliable to reveal personalized interests.

Recently, some approaches are proposed for dealing with topic-irrelevant links in topic modeling. For example, FLDA [5] embeds a Bernoulli prior to judge whether following links on Micro-bloging platforms are generated by popularity fac-tors. However, since the factors of link generation on so-cial networks are various, the methods which only rely on prior knowledge and can not well reveal the characteristics of each link. Intuitively, many topic-irrelevant links can be indicated by the texts of their relational users. As shown in Figure 1, Obama and Nancy have published similar po-litical views in their tweets, and they follow each other may because of having similar political topics. While Cristiano mainly publishes tweets about football, he follows Obama may be caused by adoring the political celebrity. Therefore, the topical similarity of users, may be in turn to indicate whether the links between users are caused by topical fac-tors.

In this paper, for alleviating the effect of topic-irrelevant links and obtaining reliable topics, we propose a novel SRTM, which can assess whether a specific link on social networks is caused by topical factors, and jointly model the texts and so-cial links into a unified generative process. Moreover, except for individual topic distribution, our model can learn the individual influence on different topics and the global pop-ularity of each user. To systematically assess our model, we conduct comparisons with several state-of-the-arts in topic modeling using the perplexity metric and social link predic-tion with ranking metrics.
Recently, a mass of hybrid data that contain textual in-formation and social information are aggregated into social networking Websites. Topic models, such as Link-LDA [1] and RTM [2], which can discover a given number of top-ics from data sources, are often used to process such hybrid data. Link-LDA views the citations of a document as a kind of words, and uses LDA [6] to deal with such special words. With a pure topic relational assumption, Chang et al. pro-pose RTM model for document networks, which draws topics for citations according to relational topic distributions of ci-tations. However, the social links on social networks, e.g., Micro-blogs, are more complex than the citations. Many factors may lead to the generation of a social link, such as the bandwagon effect or the marketing advertisement. Thus, FLDA [5] improves Link-LDA by introducing a Multinomial-Bernoulli prior to assess whether a following relationship is caused by topical factors. Different from FLDA using an artificial prior to analyze social links, we leverage the simi-larity of relational topic distributions of each link to measure whether the link is generated by topic-irrelevant factors.
Ranking based models [3, 7] are another direction for deal-ing with hybrid data. They usually use topic modeling for analyzing the textual information, and conduct ranking al-gorithms, e.g., pagerank [8], on the structure information. For example, RankTopic [3] uses basic topic model to ex-plore the topic distribution of each node from its texts, and then conducts Topic Sensitive PageRank [9] on the citation network to obtain more reliable topics. In [7], Yan et al. learn the topic distribution of tweets using LDA, and then compute the topical influence of each tweet on a heteroge-neous graph by pagerank. However, since ranking based models often assume that the links are purely generated by topics, their learned topics tend to be effected by the topic-irrelevant effect. Moreover, due to the large scale social net-works, these models often suffer from a slow convergence. In this section, we present the Social-Relational Topic Model (SRTM), which jointly models the texts and social links. Through this model, we estimate the topic-irrelevant links, infer the topic distribution for each user, and analyze the users X  influence on different topics.
Our model consists of three major components with each capturing one perspective of our targets. For one following relationship l , the graphical representation of our model is shown in Figure 2. The model embeds two LDA models for processing the texts of relational users on either side of the model. In the middle part, the model estimates whether the link associates with topical factors and analyzes users X  influence on different topics. More specifically, in SRTM, each user u is viewed as a mixture of K latent topics, i.e.,  X   X  R K , corresponding to the texts and social links. SRTM Figure 2: Graphical representation of SRTM. The gray generates a following relationship and estimates whether it is a topic-irrelevant link, by taking relational topic distri-butions, i.e.,  X  u and  X  v into account. In a nutshell, the generative process of SRTM is summarized in Algorithm 1, where  X  is a sigmoid function, i.e.,  X  ( x )=1 / (1 + exp ( and  X  u,v =  X  u  X  v +  X  .
 Algorithm 1 Generative process of SRTM 1: Draw  X   X  Dir ( ); 2: for all each topic k = { 1 , 2 , ..., K } do 3: Draw  X   X  Dir (  X  ); 4: Draw  X   X  Dir (  X  ); 5: end for 6: for all each user i = { 1 , 2 , ..., M } do 7: Draw topic proportions  X  i |  X   X  Dir (  X  ); 11: end for 12: end for 13: for all each user u = { 1 , 2 , ..., M } do 19: else 21: end if 22: end for 23: end for
To generate texts of users, each user is treated as a mix-ture of latent topics from which words are drawn. Similar to LDA, for the u -th user, the model first draws the topic dis-tribution  X  u from a Dirichlet prior with a hyper-parameter  X  . Then, to generate the n -th word in the texts of the user, a topic assignment z u,n is drawn from  X  u . Finally, the word w u,n is picked from the topic-word distribution  X  z u,n .
Clearly, the topic distribution is affected by the social in-formation, and the generative mechanism of following rela-tionships are different from the words in textual contents. In Algorithm 1, we sculpture a two-stage stochastic process for the generation of following relationships. Specifically, we first assess whether a following relationship is related to users X  topics, using a switch variable  X  . For a following re-lationship l u,v ,thereisaswitch  X  u,v which is drawn from a Bernoulli prior. If  X  u,v = 1, the following relationship is assumed to be associated with the user X  X  topics, then a topic assignment x u,v is drawn according to the relational topic distributions, i.e.,  X  u and  X  v . Indicated by x following relationship l u,v is sampled from the Multinomial distribution  X  x u,v , which corresponds to the topic-specific influence of users. If  X  u,v = 0, the following relationship is viewed to be generated by topic-irrelevant factors, and is sampled from another Multinomial distribution  X  ,which mainly indicates the global popularity of users. Moreover, according to the observation in Figure 1, the similarity of textual contents can help us to estimate whether the social link associates with topics. Therefore, for the following re-lationship l u,v ,webring  X  u and  X  v to be the preconditions of  X  u,v . More specifically, for drawing the value of  X  calculate the inner product of  X  u and  X  v and use it as the parameter of Bernoulli prior.
Given M users and the hyper-parameters  X  ,  X  ,  X  , and  X  , the joint probability distribution for the observed variables of the model can be written as where  X  = {  X ,  X ,  X ,  X ,  X ,  X ,  X ,  X , ,  X  } is the set of pa-rameters. F u is a set which contains all users followed by u -th user, and G = { l u,v | u  X  M, v  X  F u } is the social graph. W represents the observed word set.

To deal with the coupling of variables in our model, we use collapsed Gibbs sampling to learn variables distributions in Eq.1. Since, in SRTM, the distribution of following re-lationships is a joint distribution of two-level mixtures and simultaneously associates with two topic mixtures, we need to take both  X  u and  X  v into account when computing the posterior distributions of x , which is the topic distribution on following relationships. More specifically, the posterior distributions for Gibbs sampling in SRTM are given where z uw denotes the topic of the w -th word for the u -th user, and x u,v is the topic of the v -th link for the u -th user. Let z  X  ( uw ) denote the topics for all words except z and x  X  ( u,v ) follow an analogous definition. We use  X  a factor indicator (topical or non topical) of the v -th link for the u -th user. Moreover, for recording the intermediate process, we bring several counters into the above equations, i.e., W kw , V kv , d uk and s uk . W kw is the number of times that the w -th word is assigned to the k -th topic and V the number of times that the v -th user is assigned to the k -th topic. d uk records the number of times that the u -th user is assigned to the k -th topic from texts. s uk is the number of times that the u -th user is assigned to the k -th topic from following links.

After the sampling algorithm runs for an appropriate num-ber of iterations (until the chain has converged to a station-ary distribution), the estimates for the parameters, i.e.,  X  ,  X  ,  X  and  X  can be obtained via the following equations: where L u denotes the number of following links of the u -th user.  X   X  ,v, 0 denotes the number of times that the v -th user is followed by other users because of non-topical factors, and  X   X  ,  X  , 0 is the total number of times that the following behaviors are caused by non-topical factors in the dataset.
After the procedure of parameter inference, using the es-timated parameters, we can construct a function to describe the generative process of social links in SRTM. In this func-tion, we take the individual topic distribution, individual topic-specific influence and user popularity into account. More specifically, given a user u and a candidate v ,wecan calculate the value which indicates the likelihood of u fol-lowing v as below, Notice that including proposed SRTM model, the latent space models, e.g., LDA, Link-LDA and FLDA, can eas-ily be embedded in this function. The value computed from this function describes the generation likelihood of the social link from user u to user v .
To investigate the performance of SRTM, we perform ex-periments on two real word datasets, i.e., Sina Weibo and Twitter, which contain a mass of following links and user published short texts. Sina Weibo is a popular Micro-bloging service of China, and the dataset we used contains 1,704,142 users and their related information, i.e., published statuses and social links. The dataset of Twitter is collected from twitter.com, which contains 5,847,699 users, 24,257,080 sta-tuses and 126,964,950 social links. We randomly split train-ing (80%) and testing (20%) data for experiments. Figure 3: The perplexity of LDA styled models on Sina
We conduct comparisons with several state-of-the-art meth-ods, including MF [10], LDA [6], Link-LDA [1] and FLDA [5]. Among these compared methods, MF and LDA only take links or texts into account, while Link-LDA and FLDA take both links and texts into account. In order to evalu-ate the LDA styled models, we use the perplexity which is widely used for topic modeling. To further study our model, we embed it into the TRA framework for social link predic-tion, and use Area Under the ROC Curve (AUC) and Mean Average Precision (MAP) as metrics.

Figure 3 demonstrates the effectiveness of compared meth-ods on topic modeling, in term of perplexity. Since the train-ing texts usually can not represent all topical information and some topics can be revealed by social links, LDA only taking texts into account can not well model such hybrid data. Since Link-LDA treats links as a kind of words and FLDA introduces a prior for the bandwagon effect of social links, they improve performance of LDA. Owing much to estimating topic-irrelevant links with relational topic distri-butions, our model yields lower perplexity than both Link-LDA and FLDA on two datasets.

In addition, we show keywords and associated influencers of several topics, which are explored by SRTM from Sina Weibo dataset. Table 2 indicates that our model can not only explain latent properties of users using the keywords extracted from texts, but also find out the users who are the influencers on individual topic. For example, under the topic of Economy, SRTM can not only provide keywords, e.g., investment, market and bank, to describe this topic, but also can discover the influencers, e.g., Xianping Lang who is a famous economist in China.

To evaluate performance of link generation, Table 1 shows the comparison evaluated by ranking metrics on Sina Weibo with two different factor dimensionalities. Owing much to take topic-irrelevant links, popularity of users and topic-specific influence of users into account, our model consis-tently outperforms the compared methods in terms of MAP@N and AUC. In particular, on MAP@N, improvements of SRTM over compared methods are real significant. This observa-tion indicates that SRTM is very suitable for top-N rec-ommendation, which is the most fundamental problem in practical applications. Figure 4 indicates the precision-recall curves on two datasets. Since short texts on social network contain much noise and lack formal linguistic structures, the performance of LDA is often worse than MF which only Figure 4: Precision-Recall curves on Sina Weibo Table 2: A sample of topics and their influencers dis-takes social links into account. Furthermore, we notice that Link-LDA and FLDA, jointly modeling the texts and links, have great superiority over LDA and MF only considering texts or links. This phenomenon reveals that on social net-work, the texts and social links can be used to complement each other in topic inference. In all experiments, due to well estimating topic-irrelevant links, SRTM consistently outper-forms Link-LDA and FLDA. This work is jointly supported by National Basic Research Program of China (2012CB316300), and National Natural Science Founda tion of China (61403390, U1435221, 61175003, 61420106015).
