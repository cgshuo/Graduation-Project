 Matthew Tesch mtesch@cs.cmu.edu Jeff Schneider jeff.schneider@cs.cmu.edu Howie Choset choset@cs.cmu.edu Many real-world optimization tasks take the form of an optimization problem where the number of objec-tive function samples can be severely limited. This often occurs with physical systems which are expen-sive to test, such as choosing optimal parameters for a robot X  X  control policy, or with design optimizations which take considerable effort to evaluate, such as us-ing computational fluid dynamics simulations to test aircraft wing designs. In cases where the objective is a continuous real-valued function, the use of Bayesian se-quential experiment selection metrics such as expected improvement has lead to efficient optimization of these objectives. A particular advantage of expected im-provement is that it requires no tuning parameters. We are interested in the problem setting where the ob-jective is not a deterministic continuous-valued func-tion, but a stochastic binary valued function. In the case of a robot, instead of choosing parameters which maximize locomotive speed, the task may be to choose the parameters of a policy which maximize the prob-ability of successfully moving over an obstacle, where the success of this task is stochastic due to environ-mental factors or small uncontrollable variations in the commanded movements of the robot.
 Inspired by the success of Bayesian optimization for continuous problems, we propose using a similar framework for the stochastic binary setting. This pa-per begins with a definition of the stochastic binary optimization problem and a brief overview of back-ground material. We describe several existing algo-rithms which could be applied to this problem and pro-pose a selection metric for stochastic binary functions based on expected improvement. Finally, we present a summary of results from a comparison of these meth-ods on a set of synthetic test functions, and apply the proposed method to learn robust motions for a snake robot to overcome obstacles.
 The primary contributions of this paper are the defini-tion of the stochastic binary optimization problem, the application of Gaussian process classification (GPC) to adapt Bayesian optimization methods to this problem setting, and the definition of the expected improve-ment for stochastic binary outputs. Secondary con-tributions include the comparison of methods on syn-thetic test functions and the optimization of a new locomotive capability on a real snake robot.
 For optimization problems where each function evalu-ation is expensive (either requiring significant time or resources) the choice of which point to sample becomes more important than the speed at which a sample can be chosen. To this end, Bayesian optimization of such functions relies on a function regression method, such as Gaussian processes (GPs) (Rasmussen &amp; Williams, 2006), to predict the entire unknown objective from limited sampled data. Given the information pro-vided by this prediction of the true objective, the cen-tral challenge is the exploration/exploitation tradeoff  X  balancing the need to explore unknown areas of the space with the need to refine the knowledge in areas that are known to have high function values. Met-rics such as the upper confidence bound (Auer et al., 2002), probability of improvement (  X  Zilinskas, 1992), and expected improvement (Mockus et al., 1978) at-tempt to trade off these conflicting goals. A compre-hensive survey on past work in this subject is given by Jones (2001). The existing literature primarily fo-cuses on deterministic, continuous, real-valued func-tions, rather than stochastic ones or ones with binary outputs.
 Active learning (c.f. the survey of Settles (2009)), how-ever, is primarily focused on learning the binary class membership of a set of unlabeled data points. In gen-eral, this work focuses on accurately learning the class membership of all of the unlabeled points with high confidence, which is inefficient if the loss function is asymmetric, i.e. if it is more important to identify successes than failures. Of particular interest is the active binary-classification problem discussed in (Gar-nett et al., 2011); this problem focuses on finding a Bayesian optimal policy for identifying a particular class, but assumes deterministic class membership. A particularly relevant set of subtopics in the bandit literature is continuous-armed bandits (Agrawal, 1995; Auer et al., 2007; Kleinberg &amp; Upfal, 2008) or met-ric bandits (Bubeck et al., 2011b); these both have a similar problem structure to that described in our work. Metric-armed bandits embed the  X  X rms X  of the classic multi-arm bandit problem into a metric space, allowing a potentially uncountably infinite number of arms. These arms are often constrained to generate re-sponses via an underlying (often Lipschitz continuous) probability function. The focus of bandit work is min-imizing asymptotic bounds on the cumulative regret in an online setting, whereas we are not concerned with errors incurred during training, but rather the perfor-mance of the algorithm recommendation after an of-fline training phase. The recent work of (Bubeck et al., 2011a) begins to address the problem we describe here by investigating bounds on the simple regret (predic-tive quality of the model after training) as compared to bounds on cumulative regret, but the results in this paper aim to characterize the spaces in which cumula-tive regret can be minimized rather than the definition of practical algorithms for the simple regret case. The problem we attempt to solve is analagous to min-imizing simple regret for a continuous-armed bandit that recieves a 1/0 binomial reward, with a budget of n function evaluations.
 More formally, we state it as follows: given an in-put (parameter) space X  X  R and an unknown func-tion  X  : X  X  [0 , 1] which represents the underlying binomial probability of success of an experiment, the learner sequentially chooses a series of points x = { x 1 ,x 2 ...x n | x i  X  X } at which to run these exper-iments. After choosing each x i , the learner recieves feedback y i where y i = 1 with probability  X  ( x i ) and y = 0 with probability 1  X   X  ( x i ). Note that the choice of x i is made with knowledge of { y 1 ,y 2 ...y i  X  1 } . The goal of the learner is to recommend, after n experi-ments, a point x r which minimizes the (typically un-known) error, or simple regret , max x  X  X  X  ( x )  X   X  ( x this is equivalent to maximizing  X  ( x r ). 4.1. Bayesian Optimization In Bayesian optimization of a continuous real-valued deterministic function, the goal is to find x best which maximizes 1 the function f : X  X  R . The process relies on a probabilistic model  X  f of the underlying function f which is generated from the data; often GPs are used for this model.
 The optimization also relies on a selection metric (sometimes referred to as an infill criterion) which, at each iteration, selects the next point to sample. The algorithm is essentially an iterative process  X  at each step i , fit a model based on x and y , select a next x i and evaluate x i on the true function f to obtain y i see Alg. 1. The critical parameter then is the metric which is maximized to choose the next point.
 The idea of expected improvement (Mockus et al., 1978) has been used as such a selection metric, and has been popularized by Jones in his Efficient Global Algorithm 1 Bayesian Optimization x := space-filling design of k points y := {} for i = 1 to k do end for for i = k + 1 to n do end for Optimization algorithm (1998) 2 . Given a function es-timate  X  f , improvement is defined as where y best was the maximizer of the previously sam-pled y . Because the GP defines  X  f as a posterior dis-tribution over potential f , the expectation over these function estimates defines the expected improvement,
EI( x ) = E [ I (  X  f ( x ))] , (2) where p x f is the posterior probability density function deviation of this pdf. 4.2. Gaussian Processes for Classification One of the key ideas behind Bayesian optimization is the probabilistic modeling of the unknown func-tion. Below we briefly describe the adaptation of GPs, which provide such a model in the continuous regres-sion case, to a classification setting. This provides a similar probabilistic model for the underlying function in the stochastic binary case. More in-depth coverage of these ideas may be found in (Rasmussen &amp; Williams, 2006).
 Adapting GPs for a space of binary response variables uses concepts from linear binary classification. Lin-ear logistic regression and linear probit regression use the logistic and the probit, respectively, as response functions  X  to convert a linear model with a range of (  X  X  X  ,  X  ) to an output that lies within [0 , 1] (i.e., a valid probability). Therefore, given a linear regression model y = w T x , the predicted class probability  X   X  ( x ) is  X  ( wx ). The choice of w for the latent linear regres-sion model is typically accomplished via maximizing the likelihood of the data given the model.
 Similarly, a GP can generate outputs in the range (  X  X  X  ,  X  ), and by using a response function  X  can con-vert these outputs to values which can be interpreted as class probabilities. In particular, the latent GP  X  f defines a Gaussian probability density function p x f for each x  X  X (as well as joint Gaussian pdfs for any set of points in X ). We define the corresponding proba-bility density over class probability functions as p x  X  . Note that although the response function  X  maps from the latent space F to the class probability space  X , p  X , not a 0 / 1 sample). Instead, due to the change of variables, In this work, we will assume that  X  is the standard nor-mal cumulative density function; however, any mono-tonically increasing function mapping from R to the unit interval can be used.
 Finally, because we do not observe values of f directly, the inference step for conditioning our GP posterior on the sampled observations x = { x i } and y = { y i } requires computing the following integral to determine the posterior  X  f at x  X  : p (  X  f  X  | x , y ,x  X  ) = In this equation, f  X  represents the GP prior on the la-tent function at x  X  . Unfortunately, the second term in the integrand represents a non-Gaussian likelihood which makes this integral analytically intractable; ap-proximate inference methods for GP classification rely on approximating this with a Gaussian. Advan-tages and disadvantages of different approximations are discussed in (Nickisch &amp; Rasmussen, 2008); we use Minka X  X  expectation propagation (EP) method (2001) due to its accuracy and reasonable speed.
 4.2.1. Expectation of Posterior on Success linearity of  X  . Because of this, the expectation of the posterior over the success probability, E [ p x  X  ], is not gen-erally equal to  X  ( E [ p x f ]). To calculate the former, we use the definition of expectation along with a change-of-variables substitution (  X  =  X  ( f ) and  X  y =  X  ( z )) to take this integral in the latent space (where approxi-mations for the standard normal CDF can be used): E [ p x  X  ] = As noted in section 3.9 of (Rasmussen &amp; Williams, 2006), if  X  is the Gaussian cumulative density function, this can be rewritten as For notational simplicity, we define  X   X  ( x ) = E [ p use later in the paper. Using GPC to model this problem allows us to in-fer a posterior probability distribution  X   X  over the un-known true function  X  from observing several ( x i ,y i ), and also to obtain a posterior over a latent function  X  f . Although this latent function could technically be used for experiment selection, it does not have a di-rect probabalistic interpretation except through the response function  X  .
 As baselines to compare against the binary expected improvement metric we propose in  X  6, we use a uni-form random experiment selection method along with the following approaches.
 First, as upper confidence bounds (UCB) methods are often used in bandit and expensive optimization prob-lems (e.g., (Auer et al., 2002)), we compare against UCB on the latent function  X  f , with  X  a tuneable met-ric parameter: For comparison, we also test the standard expected improvement metric in the latent space, EI f , and on a GP directly fit to the binary data. For the former, be-cause we are not directly observing the sampled func-tion value we must redefine the y best term in the im-provement quantity from Eqn. (1) as where x is all sampled x i . This represents the latent space projection of the maximizer of  X   X  ( x ) at the pre-viously sampled points.
 Finally, we compare against the continuous-armed bandit algorithm UCBC (Upper Confidence Bound for Continuous-armed bandits) proposed in (Auer et al., 2007). This algorithm divides X into a set of n equal-sized intervals, and runs the multi-arm bandit UCB algorithm to select the interval from which to sam-ple. The point to sample is then chosen uniformly at random from this interval. Recommendations for how to choose the algorithm parameter n are given in the paper. In the case of stochastic binomial feedback, the no-tion of improvement that underlies the definition of expected improvement must change. Because the only potential values for y i are 1 and 0, after the first 1 is sampled y best would be set to 1. Because there is no possibility for a returned value higher than 1, the im-provement (and therefore the expected improvement) would then be identically zero for each x  X  X . Instead, we query the GP posterior at each point in x , and let As the 0 and 1 responses are samples from a Bernoulli distribution with mean  X  ( x ), we define the improve-ment as if we could truly sample the underlying mean. Choosing this rather than conditioning our improve-ment on 0/1 is consistent with the fact that our  X   X  max represents a probability, not a single sample of 0/1. In this case, To calculate the expected improvement, we follow a similar procedure to that in  X  4.2.1 to calculate the ex-pecation of I  X  (  X  ( x )): Unfortunately, the marginalization trick that allowed us to evaluate this integral and obtain a solution only requiring the Gaussian CDF in the case of  X   X  (Eqn. (7)) does not work because these integrals are not from  X  X  X  to  X  ; fortunately these are one dimensional integrals regardless of the dimension of X and are easy to nu-merically evaluate in practice. 7.1. Synthetic Test Functions To validate the performance of our expected improve-ment metric for stochastic binary outputs, we created several synthetic test functions on which we could run a large number of optimizations. Shown in Fig. 1 are three of these functions, these exhibit properties such as multiple local optima and a narrow global optimum to challenge optimization algorithms; moreover, they are stochastic (  X  ( x ) /  X  X  0 , 1 } ) over much of X . 7.2. Experimental Setup To compare the various algorithms, we allowed each algorithm to sequentially choose a series of x = { x 1 ,x 2 ...x 50 } , with feedback of y i generated from a Bernoulli distribution with mean  X  ( x ) (according to the test function) after each choice of x i . This was completed 100 times for each test function.
 For our random selection baseline, at each step i , a ran-dom point was chosen and evaluated. For the baseline EI f and UCB f metrics (which used the latent GP) as well as the proposed EI  X  metric, the standard Bayesian optimization framework described in Alg. 1 was used with an initial Latin hypercube sampling of 5 points. The UCB baseline tests were run with various values of the  X  parameter from 0 . 5 to 10; 1 was found to work as well or better than other values and was used for the comparison here. The maximization of the met-ric was done by evaluating the metric on a dense grid over the space; in practice and in higher dimensions one would typically apply another global optimization method to obtain the maximizer.
 Often in the Bayesian optimization framework, the co-variance function and hyperparameters of the GP are chosen at each iteration through likelihood maximiza-tion; we chose to use a simple squared exponential co-variance with fixed hyperparameters (length scale of e 0 . 75 and signal variance of e 5 ) to reduce the variance in algorithm performance due to optimization of this likelihood function. The GP inference step (including the expectation propagation step) was done using the Gaussian Processes for Machine Learning MATLAB software package (Rasmussen &amp; Williams).
 For comparison with the continuous-armed bandit lit-erature, we implemented the UCBC algorithm de-scribed in (Auer et al., 2007); the algorithm param-eter of n was chosen as recommended therein for un-known functions, n = ( T/ln ( T )) 1 / 4 = 2, assuming the number of samples T = 50. We also ran UCBC with n = 10, but did not get appreciably different perfor-mance.
 Our binary Bayesian optimization MATLAB code used to run these experiments, including implementa-tions of the algorithms described herein, is available at http://www.mtesch.net/ICML2013/ , along with more complete results with varied algorithm param-eters on a wider variety of benchmark test functions. 7.3. Measuring Performance To obtain a measure of the algorithm X  X  performance at step i , we use the natural Bayesian recommenda-tion strategy of choosing the point which has the high-est expected probability of success E [ p x  X  ] given knowl-edge only of the sampled points { x 1 ,x 2 ...x i } and { y 1 ,y 2 ...y i } . In practice, one may wish to optimize a utility function that also considers risk (e.g., the un-certainty in that probability).
 After choosing x best = argmax X E [ p x  X  ], this point is evaluated on the underlying true success probability function  X  , and the resulting value  X  ( x best ) is given as the expected performance of the algorithm at step i . For the random selection and UCBC 3 algorithms which do not have a notion of  X   X  , a GP was fit to the data collected by the algorithm to obtain this  X   X  using the same parameters as for the Bayesian optimization algorithms. 7.4. Comparison of Results In Fig. 2, we plot the average performance over 100 runs of the proposed stochastic binary expected im-provement EI  X  as well as the random baseline and the continuous-armed bandit UCBC algorithm. As ex-pected, the knowledge of the underlying function grew slowly but steadily as random sampling characterized the entire function. The focus of EI  X  on areas of the function with the highest expectation for improvement led to a more efficient strategy which still chose to ex-plore, but focused experimental evaluations on more promising areas of the search space. Notably, EI matched or outperformed tuned versions of all other algorithms tested, without requiring a tuning parame-ter.
 The UCBC algorithm worked well for simple cases (test function 1 had a significant region with high prob-ability of success) but faltered as the functions became more difficult to optimize. One challenge with this al-gorithm is that there is no shared knowledge between nearby intervals  X  if a function is continuous, the per-formance at interval k is likely to be similar to that of k  X  1 for a large enough number of intervals. Another challenge is the dependence on a tuning parameter for the number of intervals. It is likely that different val-ues for this parameter would significantly affect the re-sults; we chose the parameter recommended by (Auer et al., 2007) ( n = 2), but also varied this parameter (to n = 10) and obtained comparable performance on test functions 1 and 3 and slight improvements on test function 2. This reinforces the authors X  observations that bandit algorithm parameters which produce the best theoretical bounds do not always translate to ef-ficient algorithm performance.
 Another challenge is that UCBC is not defined for higher dimensions; the natural extension would be to use a grid of area elements instead of a set of in-tervals, but the choice of n for each dimension isn X  X  clear; for this reason we limit the results herein to one-dimensional test functions.
 We also note that EI  X  outperforms the na  X  X ve use of Bayesian optimization techniques on the latent GP  X  f , as shown in Fig. 3. This is largely true because the interpretation of variances on the latent function when used in the classification framework are unintuitive  X  the variance  X  f  X  is not based solely on the sampled points as in the regression case; instead larger values of  X  f  X  tend to have larger variances due to the nonlinear mapping into the space of probabilities  X   X  .
 This problem is especially apparent in test function 3, where the local maxima are given in fairly wide area likely to be sampled during the initial space-filling de-sign, whereas the global maximum is narrower; be-cause the variance of the latent function continues to be high at high values of the mean, and drop off very slowly, both EI f and UCB f tend to focus remaining evaluations in this localized area.
 Because EI  X  instead uses the posterior in the under-lying success probability space, the variance decreases near the local maxima as expected, and the algorithm explores other areas of X with potential for improve-ment.
 Finally, standard EI fit directly to the binary data per-forms remarkably well, although is slower to converge than EI  X  . However, this method required additional careful model selection; we shown the best results af-ter carefully fitting a noise term in the diagonal of the covariance; poor selection or omittance of this term resulted in performance far below any baseline shown. The goal of this work was to find task parameters that have a high expected success probability and to do so with a small number of experiments that give only binary (success/failure) feedback. The task that mo-tivated this goal was improving the locomotion of a snake robot (Wright et al., 2012) so that it could reli-ably overcome obstacles encountered in the field, such as the dimensional lumber in these experiments. Inspired by the approach taken in (Tesch et al., 2012), a master-slave system was set up to record an expert X  X  input to move the robot over an obstacle. Using a sparse function approximation of the expert X  X  input, we created a 7 parameter model that was able to over-come obstacles of various sizes, albeit unreliably  X  the same parameters would sometimes result in success and sometimes failure. We found that parameters of this model were difficult to optimize by hand to pro-duce reliable results.
 Using the EI  X  metric in the Bayesian optimization framework described above, 2 and 3 dimensional sub-spaces of the model were searched to identify regions of the parameter space that resulted in a robust motion over the obstacle that was used to record the original unreliable motion. In each of these cases, running 40 experiments at 20 points 4 resulted in the recommen-dation of a parameter setting which produced robust, successful motions; the resulting motion is shown in Fig. 4. After completing the first 2D optimization, it was noted that after the optimization found a successful solution it would not sample other areas of the space. This is because as  X   X   X  approaches 1, the maximum pos-sible improvement approaches 0, as does EI  X  ; this dis-courages selection of points that are not near the cur-rent maximum. While this technically meets the ob-jective of finding a robust solution, there is utility in the use of the remaining experiments to find other ro-bust solutions in the parameter space. To accomplish this goal, we modified the selection metric to not select any point which had a high confidence in its estimate of the true probability. To measure this confidence in being within of the mean at a point x , we can take the integral We reran the optimization over the same 2 D param-eter space, not considering points where, for = . 1, C ( x ) &gt; = 90%. As seen in Fig. 5, this generated a more diverse solution set which provided a more rich set of motions for the robot. In this paper, we have defined the stochastic binary optimization problem for expensive functions, and pre-sented a novel use of GPC to frame this problem as Bayesian optimization. We also presented a new opti-mization algorithm that computes expected improve-ment in the stochastic binary case, outperforming sev-eral baseline metrics as well as a leading continuous-armed bandit algorithm. Finally, we used our algo-rithm to learn a robust motion for moving a snake robot over an obstacle.
 The problem we define is not limited to the demon-strated snake robot application, but applies to many expensive problems with parameterized policies and stochastic success/failure feedback. This includes vari-ations of applications where continuous-armed bandits are currently used, such as auction mechanisms and oblivious routing (see references in (Kleinberg, 2004)), which could contain an offline training phase penaliz-ing simple rather than continuous regret.
 One promising topic that builds upon the current work is the application of these methods to trans-fer/multiple task learning. For the robot application, this could include using knowledge from optimization for a single obstacle to improve the learning rate for other similar obstacles, such as different heights and widths of the beam demonstrated here. A final future goal is the derivation of theoretic convergence guaran-tees for the binary stochastic expected improvement metric.
 The authors thank Roman Garnett for his insights into the stochastic binary optimization problem. We also appreciate the reviewer comments, especially the sug-gestion of the direct GP fit to the 0/1 data baseline. Agrawal, Rajeev. The Continuum-Armed Bandit
Problem. SIAM Journal on Control and Optimiza-tion , 33(6):1926 X 1951, November 1995. ISSN 0363-0129. doi: 10.1137/S0363012992237273.
 Auer, Peter, Cesa-Bianchi, N, and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Machine learning , pp. 235 X 256, 2002.
 Auer, Peter, Ortner, Ronald, and Szepesv  X ari, C. Im-proved rates for the stochastic continuum-armed bandit problem. Learning Theory , 2007.
 Bubeck, S  X ebastien, Munos, R  X emi, and Stoltz, Gilles.
Pure exploration in finitely-armed and continuous-armed bandits. Theoretical Computer Science , 412 (19):1832 X 1852, April 2011a. ISSN 03043975. doi: 10.1016/j.tcs.2010.12.059.
 Bubeck, S  X ebastien, Munos, R  X emi, Stoltz, Gilles, and Szepesv  X ari, Csaba. X -Armed Bandits. Journal of Machine Learning Research , 12:1655 X 1695, 2011b. Garnett, Roman, Krishnamurthy, Yamuna, Wang, Donghan, Schneider, Jeff, and Mann, Richard.
Bayesian optimal active search on graphs. In Work-shop on Mining and Learning with Graphs , 2011. ISBN 9781450308342.
 Jones, Donald R. A taxonomy of global optimiza-tion methods based on response surfaces. Journal of Global Optimization , 21(4):345 X 383, 2001.
 Jones, Donald R., Schonlau, Matthias, and Welch,
William J. Efficient Global Optimization of Expen-sive Black-Box Functions. Journal of Global Opti-mization , 13(4), 1998. ISSN 0925-5001.
 Kleinberg, Robert. Nearly tight bounds for the continuum-armed bandit problem. In Advances in
Neural Information Processing Systems , pp. 697 X  704, 2004.
 Kleinberg, Robert and Upfal, Eli. Multi-Armed Ban-dits in Metric Spaces. In STOC  X 08 Proceedings of the 40th annual ACM symposium on Theory of com-puting , pp. 681 X 690, 2008. ISBN 9781605580470. Minka, Thomas P. A family of algorithms for approxi-mate Bayesian inference . Phd thesis, Massachusetts Institute of Technology, 2001.
 Mockus, J, Tiesis, V, and Zilinskas, A. The applica-tion of Bayesian methods for seeking the extremum. Towards Global Optimization , 2:117 X 129, 1978. Nickisch, Hannes and Rasmussen, CE. Approxi-mations for binary Gaussian process classification.
Journal of Machine Learning Research , 9:2035 X  2078, 2008.
 Rasmussen, Carl Edward and Williams, Christopher
K. I. Gaussian Processes for Machine Learn-ing. URL http://www.gaussianprocess.org/ gpml/code/gpml-matlab.tar.gz .
 Rasmussen, Carl Edward and Williams, Christopher K. I. Gaussian Processes for Machine Learning . The MIT Press, 2006. ISBN 026218253X.
 Settles, Burr. Active Learning Literature Survey.
Technical report, University of Wisconsin X  X adison, 2009.
 Tesch, Matthew, O X  X eill, Alex, and Choset, Howie.
Using Kinesthetic Input to Overcome Obstacles with Snake Robots. In International Symposium on
Safety, Security, and Rescue Robotics , 2012.  X  Zilinskas, Antanas. A review of statistical models for global optimization. Journal of Global Optimization , 2(2):145 X 153, June 1992. ISSN 0925-5001. doi: 10. 1007/BF00122051.
 Wright, C., Buchan, A., Brown, B., Geist, J., Schw-erin, M., Rollinson, D., Tesch, M., and Choset,
H. Design and Architecture of the Unified Modu-lar Snake Robot. In 2012 IEEE International Con-ference on Robotics and Automation , St. Paul, MN,
