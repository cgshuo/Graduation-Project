 MODE Lab, University of Washington, Seattle, WA.
 Hamiltonian Monte Carlo (HMC) (Duane et al., 1987; Neal, 2010) sampling methods provide a powerful Markov chain Monte Carlo (MCMC) sampling algorithm. The methods define a Hamiltonian function in terms of the tar-get distribution from which we desire samples X  X he po-tential energy  X  X nd a kinetic energy term parameterized by a set of  X  X omentum X  auxiliary variables. Based on simple updates to the momentum variables, one simu-lates from a Hamiltonian dynamical system that enables proposals of distant states. The target distribution is in-variant under these dynamics; in practice, a discretiza-tion of the continuous-time system is needed necessitating a Metropolis-Hastings (MH) correction, though still with high acceptance probability. Based on the attractive proper-ties of HMC in terms of rapid exploration of the state space, HMC methods have grown in popularity recently (Neal, 2010; Hoffman &amp; Gelman, 2011; Wang et al., 2013). A limitation of HMC, however, is the necessity to com-pute the gradient of the potential energy function in order to simulate the Hamiltonian dynamical system. We are in-creasingly faced with datasets having millions to billions of observations or where data come in as a stream and we need to make inferences online, such as in online advertis-ing or recommender systems. In these ever-more-common scenarios of massive batch or streaming data, such gradi-ent computations are infeasible since they utilize the entire dataset, and thus are not applicable to  X  X ig data X  problem-s. Recently, in a variety of machine learning algorithms, we have witnessed the many successes of utilizing a noisy estimate of the gradient based on a minibatch of data to scale the algorithms (Robbins &amp; Monro, 1951; Hoffman et al., 2013; Welling &amp; Teh, 2011). A majority of these developments have been in optimization-based algorithm-s (Robbins &amp; Monro, 1951; Nemirovski et al., 2009), and a question is whether similar efficiencies can be garnered by sampling-based algorithms that maintain many desir-able theoretical properties for Bayesian inference. One at-tempt at applying such methods in a sampling context is the recently proposed stochastic gradient Langevin dynamics (SGLD) (Welling &amp; Teh, 2011; Ahn et al., 2012; Patterson &amp; Teh, 2013). This method builds on first-order Langevin dynamics that do not include the crucial momentum term of HMC.
 In this paper, we explore the possibility of marrying the efficiencies in state space exploration of HMC with the big-data computational efficiencies of stochastic gradients. Such an algorithm would enable a large-scale and online Bayesian sampling algorithm with the potential to rapidly explore the posterior. As a first cut, we consider simply applying a stochastic gradient modification to HMC and assess the impact of the noisy gradient. We prove that the noise injected in the system by the stochastic gradient no longer leads to Hamiltonian dynamics with the desired tar-get distribution as the stationary distribution. As such, even before discretizing the dynamical system, we need to cor-rect for this effect. One can correct for the injected gradi-ent noise through an MH step, though this itself requires costly computations on the entire dataset. In practice, one might propose long simulation runs before an MH correc-tion, but this leads to low acceptance rates due to large de-viations in the Hamiltonian from the injected noise. The efficiency of this MH step could potentially be improved using the recent results of (Korattikara et al., 2014; Bar-denet et al., 2014). In this paper, we instead introduce a stochastic gradient HMC method with friction added to the momentum update. We assume the injected noise is Gaus-sian, appealing to the central limit theorem, and analyze the corresponding dynamics. We show that using such second-order Langevin dynamics enables us to maintain the desired target distribution as the stationary distribution. That is, the friction counteracts the effects of the injected noise. For discretized systems, we consider letting the step size tend to zero so that an MH step is not needed, giving us a sig-nificant computational advantage. Empirically, we demon-strate that we have good performance even for set to a small, fixed value. The theoretical computation versus ac-curacy tradeoff of this small-approach is provided in the Supplementary Material.
 A number of simulated experiments validate our theoretical results and demonstrate the differences between (i) exact HMC, (ii) the na  X   X ve implementation of stochastic gradient HMC (simply replacing the gradient with a stochastic gra-dient), and (iii) our proposed method incorporating friction. We also compare to the first-order Langevin dynamics of SGLD. Finally, we apply our proposed methods to a classi-fication task using Bayesian neural networks and to online Bayesian matrix factorization of a standard movie dataset. Our experimental results demonstrate the effectiveness of the proposed algorithm. Suppose we want to sample from the posterior distribution of  X  given a set of independent observations x  X  X  : where the potential energy function U is given by Hamiltonian (Hybrid) Monte Carlo (HMC) (Duane et al., 1987; Neal, 2010) provides a method for proposing sam-ples of  X  in a Metropolis-Hastings (MH) framework that efficiently explores the state space as compared to stan-dard random-walk proposals. These proposals are gener-ated from a Hamiltonian system based on introducing a set of auxiliary momentum variables, r . That is, to sample from p (  X  |D ) , HMC considers generating samples from a joint distribution of (  X ,r ) defined by If we simply discard the resulting r samples, the  X  sam-ples have marginal distribution p (  X  |D ) . Here, M is a mass matrix, and together with r , defines a kinetic energy term. M is often set to the identity matrix, I , but can be used to precondition the sampler when we have more information about the target distribution. The Hamiltonian function is defined by H (  X ,r ) = U (  X  ) + 1 2 r T M  X  1 r . Intuitively, H measures the total energy of a physical system with posi-tion variables  X  and momentum variables r .
 To propose samples, HMC simulates the Hamiltonian dy-namics To make Eq. (4) concrete, a common analogy in 2D is as follows (Neal, 2010). Imagine a hockey puck sliding over a frictionless ice surface of varying height. The potential en-ergy term is based on the height of the surface at the current puck position,  X  , while the kinetic energy is based on the momentum of the puck, r , and its mass, M . If the surface is flat (  X  U (  X  ) = 0 ,  X   X  ), the puck moves at a constant ve-locity. For positive slopes (  X  U (  X  ) &gt; 0 ), the kinetic energy decreases as the potential energy increases until the kinet-ic energy is 0 ( r = 0 ). The puck then slides back down the hill increasing its kinetic energy and decreasing poten-tial energy. Recall that in HMC, the position variables are those of direct interest whereas the momentum variables are artificial constructs (auxiliary variables).
 Over any interval s , the Hamiltonian dynamics of Eq. (4) defines a mapping from the state at time t to the state at time t + s . Importantly, this mapping is reversible, which is important in showing that the dynamics leave  X  invari-ant. Likewise, the dynamics preserve the total energy, H , so proposals are always accepted. In practice, however, we usually cannot simulate exactly from the continuous system of Eq. (4) and instead consider a discretized system. One common approach is the  X  X eapfrog X  method, which is out-lined in Alg. 1. Because of inaccuracies introduced through the discretization, an MH step must be implemented (i.e., the acceptance rate is no longer 1). However, acceptance rates still tend to be high even for proposals that can be quite far from their last state.
 Algorithm 1: Hamiltonian Monte Carlo There have been many recent developments of HMC to make the algorithm more flexible and applicable in a va-riety of settings. The  X  X o U-Turn X  sampler (Hoffman &amp; Gelman, 2011) and the methods proposed by Wang et al. (2013) allow automatic tuning of the step size, , and num-ber of simulation steps, m . Riemann manifold HMC (Giro-lami &amp; Calderhead, 2011) makes use of the Riemann ge-ometry to adapt the mass M , enabling the algorithm to make use of curvature information to perform more effi-cient sampling. We attempt to improve HMC in an orthog-onal direction focused on computational complexity, but these adaptive HMC techniques could potentially be com-bined with our proposed methods to see further benefits. In this section, we study the implications of implementing HMC using a stochastic gradient and propose variants on the Hamiltonian dynamics that are more robust to the noise introduced by the stochastic gradient estimates. In all s-cenarios, instead of directly computing the costly gradient  X  U (  X  ) using Eq. (2), which requires examination of the entire dataset D , we consider a noisy estimate based on a minibatch  X  D sampled uniformly at random from D :  X   X 
U (  X  ) =  X  We assume that our observations x are independent and, appealing to the central limit theorem, approximate this noisy gradient as Here, V is the covariance of the stochastic gradient noise, which can depend on the current model parameters and sample size. Note that we use an abuse of notation in E-q. (6) where the addition of N (  X ,  X ) denotes the introduc-tion of a random variable that is distributed according to this multivariate Gaussian. As the size of  X  D increases, this Gaussian approximation becomes more accurate. Clearly, we want minibatches to be small to have our sought-after computational gains. Empirically, in a wide range of set-tings, simply considering a minibatch size on the order of hundreds of data points is sufficient for the central limit theorem approximation to be accurate (Ahn et al., 2012). In our applications of interest, minibatches of this size still represent a significant reduction in the computational cost of the gradient. 3.1. Na  X   X ve Stochastic Gradient HMC The most straightforward approach to stochastic gradient HMC is simply to replace  X  U (  X  ) in Alg. 1 by  X   X  U (  X  ) . Re-ferring to Eq. (6), this introduces noise in the momentum update, which becomes  X  r =  X   X   X  U (  X  ) =  X   X  U (  X  ) + N (0 , 2 V ) . The resulting discrete time system can be viewed as an -discretization of the following continuous stochastic differential equation: Here, B (  X  ) = 1 2 V (  X  ) is the diffusion matrix contributed by gradient noise. As with the original HMC formulation, it is useful to return to a continuous time system in order to derive properties of the approach. To gain some intuition about this setting, consider the same hockey puck analogy of Sec. 2. Here, we can imagine the puck on the same ice surface, but with some random wind blowing as well. This wind may blow the puck further away than expected. Formally, as given by Corollary 3.1 of Theorem 3.1, when B is nonzero,  X  (  X ,r ) of Eq. (3) is no longer invariant under the dynamics described by Eq. (7).
 Theorem 3.1. Let p t (  X ,r ) be the distribution of (  X ,r ) at time t with dynamics governed by Eq. (7) . Define the entropy of p t as h ( p t ) =  X  R  X ,r f ( p t (  X ,r )) d X dr , where f ( x ) = x ln x . Assume p t is a distribution with density and gradient vanishing at infinity. Furthermore, assume the gradient vanishes faster than 1 ln p p increases over time with rate  X  t h ( p t (  X ,r )) = Eq. (8) implies that  X  t h ( p t (  X ,r ))  X  0 since B (  X  ) is a pos-itive semi-definite matrix.
 Intuitively, Theorem 3.1 is true because the noise-free Hamiltonian dynamics preserve entropy, while the addi-tional noise term strictly increases entropy if we assume (i) B (  X  ) is positive definite (a reasonable assumption due to the normal full rank property of Fisher information) and strictly increases over time. This hints at the fact that the distribution p t tends toward a uniform distribution, which can be very far from the target distribution  X  .
 Corollary 3.1. The distribution  X  (  X ,r )  X  exp (  X  H (  X ,r )) is no longer invariant under the dynamics in Eq. (7) . The proofs of Theorem 3.1 and Corollary 3.1 are in the Supplementary Material.
 Because  X  is no longer invariant under the dynamics of Eq. (7), we must introduce a correction step even before considering errors introduced by the discretization of the dynamical system. For the correctness of an MH step (based on the entire dataset), we appeal to the same argu-ments made for the HMC data-splitting technique of Neal (2010). This approach likewise considers minibatches of data and simulating the (continuous) Hamiltonian dynam-ics on each batch sequentially. Importantly, Neal (2010) alludes to the fact that the resulting H from the split-data scenario may be far from that of the full-data scenario af-ter simulation, which leads to lower acceptance rates and thereby reduces the apparent computational gains in simu-lation. Empirically, as we demonstrate in Fig. 2, we see that even finite-length simulations from the noisy system can diverge quite substantially from those of the noise-free sys-tem. Although the minibatch-based HMC technique con-sidered herein is slightly different from that of Neal (2010), the theory we have developed in Theorem 3.1 surrounding the high-entropy properties of the resulting invariant distri-bution of Eq. (7) provides some intuition for the observed deviations in H both in our experiments and those of Neal (2010).
 The poorly behaved properties of the trajectory of H based on simulations using noisy gradients results in a complex computation versus efficiency tradeoff. On one hand, it is extremely computationally intensive in large datasets to insert an MH step after just short simulation runs (where deviations in H are less pronounced and acceptance rates should be reasonable). Each of these MH steps requires a costly computation using all of the data, thus defeating the computational gains of considering noisy gradients. On the other hand, long simulation runs between MH steps can lead to very low acceptance rates. Each rejection corre-sponds to a wasted (noisy) gradient computation and simu-lation using the proposed variant of Alg. 1. One possible di-rection of future research is to consider using the recent re-sults of Korattikara et al. (2014) and Bardenet et al. (2014) that show that it is possible to do MH using a subset of data. However, we instead consider in Sec. 3.2 a straightforward modification to the Hamiltonian dynamics that alleviates the issues of the noise introduced by stochastic gradients. In particular, our modification allows us to again achieve the desired  X  as the invariant distribution of the continuous Hamiltonian dynamical system. 3.2. Stochastic Gradient HMC with Friction In Sec. 3.1, we showed that HMC with stochastic gradients requires a frequent costly MH correction step, or alterna-tively, long simulation runs with low acceptance probabili-ties. Ideally, instead, we would like to minimize the effect of the injected noise on the dynamics themselves to allevi-ate these problems. To this end, we consider a modification to Eq. (7) that adds a  X  X riction X  term to the momentum up-date: Here and throughout the remainder of the paper, we omit the dependence of B on  X  for simplicity of notation. Let us again make a hockey analogy. Imagine we are now playing street hockey instead of ice hockey, which introduces fric-tion from the asphalt. There is still a random wind blowing, however the friction of the surface prevents the puck from running far away. That is, the friction term BM  X  1 r help-s decrease the energy H (  X ,r ) , thus reducing the influence of the noise. This type of dynamical system is commonly referred to as second-order Langevin dynamics in physic-s (Wang &amp; Uhlenbeck, 1945). Importantly, we note that the Langevin dynamics used in SGLD (Welling &amp; Teh, 2011) are first-order, which can be viewed as a limiting case of our second-order dynamics when the friction term is large. Further details on this comparison follow at the end of this section.
 Theorem 3.2.  X  (  X ,r )  X  exp(  X  H (  X ,r )) is the unique sta-tionary distribution of the dynamics described by Eq. (9) . Proof. Let G = is an anti-symmetric matrix, and D is the symmetric (d-iffusion) matrix. Eq. (9) can be written in the following decomposed form (Yin &amp; Ao, 2006; Shi et al., 2012) d The distribution evolution under this dynamical system is governed by a Fokker-Planck equation  X  p t (  X ,r )=  X  T { [ D + G ] [ p t (  X ,r )  X  H (  X ,r ) +  X  p See the Supplementary Material for details. We can ver-ify that  X  (  X ,r ) is invariant under Eq. (10) by calculating e to the existence of diffusion noise,  X  is the unique station-ary distribution of Eq. (10).
 In summary, we have shown that the dynamics given by Eq. (9) have a similar invariance property to that of the o-riginal Hamiltonian dynamics of Eq. (4), even with noise present. The key was to introduce a friction term using second-order Langevin dynamics. Our revised momen-tum update can also be viewed as akin to partial momen-tum refreshment (Horowitz, 1991; Neal, 1993), which al-so corresponds to second-order Langevin dynamics. Such partial momentum refreshment was shown to not greatly improve HMC in the case of noise-free gradients (Neal, 2010). However, as we have demonstrated, the idea is cru-cial in our stochastic gradient scenario in order to counter-balance the effect of the noisy gradients. We refer to the resulting method as stochastic gradient HMC (SGHMC). C As we previously discussed, the dynamics introduced in E-q. (9) relate to the first-order Langevin dynamics used in S-GLD (Welling &amp; Teh, 2011). In particular, the dynamics of SGLD can be viewed as second-order Langevin dynamics with a large friction term. To intuitively demonstrate this connection, let BM  X  1 = 1 dt in Eq. (9). Because the fric-tion and momentum noise terms are very large, the momen-tum variable r changes much faster than  X  . Thus, relative to the rapidly changing momentum,  X  can be considered as fixed . We can study this case as simply: The fast evolution of r leads to a rapid convergence to the stationary distribution of Eq. (11), which is given by N ( MB  X  1  X  U (  X  ) ,M ) . Let us now consider a change in  X  , with r  X  N ( MB  X  1  X  U (  X  ) ,M ) . Recalling BM  X  1 = 1 we have which exactly aligns with the dynamics of SGLD where M  X  1 serves as the preconditioning matrix (Welling &amp; Teh, 2011). Intuitively, this means that when the friction is large, the dynamics do not depend on the decaying series of past gradients represented by dr , reducing to first-order Langevin dynamics.

Algorithm 2: Stochastic Gradient HMC 3.3. Stochastic Gradient HMC in Practice In everything we have considered so far, we have assumed that we know the noise model B . Clearly, in practice this is not the case. Imagine instead that we simply have an es-timate  X  B . As will become clear, it is beneficial to instead introduce a user specified friction term C  X  B and consid-er the following dynamics The resulting SGHMC algorithm is shown in Alg. 2. Note that the algorithm is purely in terms of user-specified or computable quantities. To understand our choice of dy-namics, we begin with the unrealistic scenario of perfect estimation of B .
 Proposition 3.1. If  X  B = B , then the dynamics of Eq. (13) yield the stationary distribution  X  (  X ,r )  X  e  X  H (  X ,r ) Proof. The momentum update simplifies to r =  X  X  X  U (  X  ) dt  X  CM  X  1 rdt + N (0 , 2 Cdt ) , with friction term CM  X  1 and noise term N (0 , 2 Cdt ) . Noting that the proof of Theorem 3.2 only relied on a matching of noise and fric-tion, the result follows directly by using C in place of B in Theorem 3.2.
 Now consider the benefit of introducing the C terms and revised dynamics in the more realistic scenario of inaccu-rate estimation of B . For example, the simplest choice is  X  B = 0 . Though the true stochastic gradient noise B is clearly non-zero, as the step size  X  0 , B = 1 2 V goes to 0 and C dominates. That is, the dynamics are again governed by the controllable injected noise N (0 , 2 Cdt ) and friction CM  X  1 . It is also possible to set  X  B = 1 2  X  V , where mated using empirical Fisher information as in (Ahn et al., 2012) for SGLD.
 C The complexity of Alg. 2 depends on the choice of M , C and  X  B , and the complexity for estimating  X   X  U (  X  )  X  X enoted as g ( |D| ,d )  X  X here d is the dimension of the parameter s-pace. Assume we allow  X  B to be an arbitrary d  X  d pos-itive definite matrix. Using empirical Fisher information estimation of  X  B , the per-iteration complexity of this esti-mation step is O ( d 2 |  X  D| ) . Then, the time complexity for the (  X ,r ) update is O ( d 3 ) , because the update is dom-inated by generating Gaussian noise with a full covari-ance matrix. In total, the per-iteration time complexity is the matrices to be diagonal when d is large, resulting in time complexity O ( d |  X  D| + d + g ( |  X  D| ,d )) . Importantly, we note that our SGHMC time complexity is the same as that of SGLD (Welling &amp; Teh, 2011; Ahn et al., 2012) in both parameter settings.
 In practice, we must assume inaccurate estimation of B . For a decaying series of step sizes t , an MH step is not required (Welling &amp; Teh, 2011; Ahn et al., 2012) 1 . Howev-er, as the step size decreases, the efficiency of the sampler likewise decreases since proposals are increasingly close to their initial value. In practice, we may want to tolerate some errors in the sampling accuracy to gain efficiency. As in (Welling &amp; Teh, 2011; Ahn et al., 2012) for SGLD, we consider using a small, non-zero leading to some bias. We explore an analysis of the errors introduced by such finite-approximations in the Supplementary Material.
 C Adding a momentum term to stochastic gradient descent (SGD) is common practice. In concept, there is a clear rela-tionship between SGD with momentum and SGHMC, and here we formalize this connection. Letting v = M  X  1 r , we first rewrite the update rule in Alg. 2 as Define  X  = 2 M  X  1 ,  X  = M  X  1 C ,  X   X  = M  X  1  X  B . The update rule becomes Comparing to an SGD with momentum method, it is clear from Eq. (15) that  X  corresponds to the learning rate and 1  X   X  the momentum term. When the noise is removed (via C =  X  B = 0 ), SGHMC naturally reduces to a stochastic gradient method with momentum. We can use the equiv-alent update rule of Eq. (15) to run SGHMC, and borrow experience from parameter settings of SGD with momen-tum to guide our choices of SGHMC settings. For example, we can set  X  to a fixed small number (e.g., 0 . 01 or 0 . 1 ), s-elect the learning rate  X  , and then fix  X   X  =  X   X  V / 2 . A more sophisticated strategy involves using momentum schedul-ing (Sutskever et al., 2013). We elaborate upon how to se-lect these parameters in the Supplementary Material. 4.1. Simulated Scenarios To empirically explore the behavior of HMC using exact gradients relative to stochastic gradients, we conduct ex-periments on a simulated setup. As a baseline, we consider the standard HMC implementation of Alg. 1, both with and without the MH correction. We then compare to HMC with stochastic gradients, replacing  X  U in Alg. 1 with  X   X  U , and consider this proposal with and without an MH correction. Finally, we compare to our proposed SGHMC, which does not use an MH correction. Fig. 1 shows the empirical distri-butions generated by the different sampling algorithms. We see that even without an MH correction, both the HMC and SGHMC algorithms provide results close to the true distri-bution, implying that any errors from considering non-zero are negligible. On the other hand, the results of na  X   X ve s-tochastic gradient HMC diverge significantly from the truth unless an MH correction is added. These findings validate our theoretical results; that is, both standard HMC and S-GHMC maintain  X  as the invariant distribution as  X  0 whereas na  X   X ve stochastic gradient HMC does not, though this can be corrected for using a (costly) MH step. We also consider simply simulating from the discretized Hamiltonian dynamical systems associated with the vari-ous samplers compared. In Fig. 2, we compare the result-ing trajectories and see that the path of (  X ,r ) from the noisy system without friction diverges significantly. The modifi-cation of the dynamical system by adding friction (corre-sponding to SGHMC) corrects this behavior. We can al-so correct for this divergence through periodic resampling of the momentum, though as we saw in Fig. 1, the cor-responding MCMC algorithm ( X  X aive stochastic gradient HMC (no MH) X ) does not yield the correct target distribu-tion. These results confirm the importance of the friction term in maintaining a well-behaved Hamiltonian and lead-ing to the correct stationary distribution.
 It is known that a benefit of HMC over many other MCM-C algorithms is the efficiency in sampling from correlated distributions (Neal, 2010) X  X his is where the introduction of the momentum variable shines. SGHMC inherits this property. Fig. 3 compares SGHMC and SGLD (Welling &amp; Teh, 2011) when sampling from a bivariate Gaussian with positive correlation. For each method, we examine five different settings of the initial step size on a linear-ly decreasing scale and generate ten million samples. For each of these sets of samples (one set per step-size setting), we calculate the autocorrelation time 2 of the samples and the average absolute error of the resulting sample covari-ance. Fig. 3(a) shows the autocorrelation versus estima-tion error for the five settings. As we decrease the step-size, SGLD has reasonably low estimation error but high autocorrelation time indicating an inefficient sampler. In contrast, SGHMC achieves even lower estimation error at very low autocorrelation times, from which we conclude that the sampler is indeed efficiently exploring the distribu-tion. Fig. 3(b) shows the first 50 samples generated by the two samplers. We see that SGLD X  X  random-walk behavior makes it challenging to explore the tails of the distribution. The momentum variable associated with SGHMC instead drives the sampler to move along the distribution contours. 4.2. Bayesian Neural Networks for Classification We also test our method on a handwritten digits classifica-tion task using the MNIST dataset 3 . The dataset consists of 60,000 training instances and 10,000 test instances. We randomly split a validation set containing 10,000 instances from the training data in order to select training parame-ters, and use the remaining 50,000 instances for training. For classification, we consider a two layer Bayesian neu-ral network with 100 hidden variables using a sigmoid unit and an output layer using softmax. We tested four meth-ods: SGD, SGD with momentum, SGLD and SGHMC. For the optimization-based methods, we use the validation set to select the optimal regularizer  X  of network weights For the sampling-based methods, we take a fully Bayesian approach and place a weakly informative gamma prior on each layer X  X  weight regularizer  X  . The sampling procedure is carried out by running SGHMC and SGLD using mini-batches of 500 training instances, then resampling hyperpa-rameters after an entire pass over the training set. We run the samplers for 800 iterations (each over the entire training dataset) and discard the initial 50 samples as burn-in. The test error as a function of MCMC or optimization iter-ation (after burn-in) is reported for each of these methods in Fig. 4. From the results, we see that SGD with mo-mentum converges faster than SGD. SGHMC also has an advantage over SGLD, converging to a low test error much more rapidly. In terms of runtime, in this case the gradien-t computation used in backpropagation dominates so both have the same computational cost. The final results of the sampling based methods are better than optimization-based methods, showing an advantage to Bayesian inference in this setting, thus validating the need for scalable and effi-cient Bayesian inference algorithms such as SGHMC. 4.3. Online Bayesian Probabilistic Matrix Collaborative filtering is an important problem in web ap-plications. The task is to predict a user X  X  preference over a set of items (e.g., movies, music) and produce rec-ommendations. Probabilistic matrix factorization (PM-F) (Salakhutdinov &amp; Mnih, 2008b) has proven effective for this task. Due to the sparsity in the ratings matrix (user-s versus items) in recommender systems, over-fitting is a severe issue with Bayesian approaches providing a natural solution (Salakhutdinov &amp; Mnih, 2008a).
 We conduct an experiment in online Bayesian PMF on the Movielens dataset ml-1M 5 . The dataset contains about 1 million ratings of 3,952 movies by 6,040 users. The num-ber of latent dimensions is set to 20. In comparing our stochastic-gradient-based approaches, we use minibatches of 4,000 ratings to update the user and item latent matri-ces. We choose a significantly larger minibatch size in this application than that of the neural net because of the dra-matically larger parameter space associated with the PMF model. For the optimization-based approaches, the hyper-parameters are set using cross validation (again, we did not see a performance difference from considering MAP esti-mation). For the sampling-based approaches, the hyperpa-rameters are updated using a Gibbs step after every 2 , 000 steps of sampling model parameters. We run the sampler to generate 2,000,000 samples, with the first 100,000 samples discarded as burn-in. We use five-fold cross validation to evaluate the performance of the different methods. The results are shown in Table 1. Both SGHMC and S-GLD give better prediction results than optimization-based methods. In this experiment, the results for SGLD and S-GHMC are very similar. We also observed that the per-iteration running time of both methods are comparable. As such, the experiment suggests that SGHMC is an effective candidate for online Bayesian PMF. Moving between modes of a distribution is one of the key challenges for MCMC-based inference algorithms. To address this problem in the large-scale or online setting, we proposed SGHMC, an efficient method for generat-ing high-quality,  X  X istant X  steps in such sampling meth-ods. Our approach builds on the fundamental framework of HMC, but using stochastic estimates of the gradient to avoid the costly full gradient computation. Surprisingly, we discovered that the natural way to incorporate stochas-tic gradient estimates into HMC can lead to divergence and poor behavior both in theory and in practice. To address this challenge, we introduced second-order Langevin dy-namics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribu-tion as the invariant distribution of the continuous system. Our empirical results, both in a simulated experiment and on real data, validate our theory and demonstrate the practi-cal value of introducing this simple modification. A natural next step is to explore combining adaptive HMC techniques with SGHMC. More broadly, we believe that the unifi-cation of efficient optimization and sampling techniques, such as those described herein, will enable a significant s-caling of Bayesian methods.
 Ahn, S., Korattikara, A., and Welling, M. Bayesian pos-terior sampling via stochastic gradient Fisher scoring.
In Proceedings of the 29th International Conference on Machine Learning (ICML X 12) , pp. 1591 X 1598, July 2012.
 Bardenet, R., Doucet, A., and Holmes, C. Towards scaling up Markov chain Monte Carlo: An adaptive subsampling approach. In Proceedings of the 30th International Con-ference on Machine Learning (ICML X 14) , volume 32, p-p. 405 X 413, February 2014.
 Duane, S., Kennedy, A.D., Pendleton, B.J., and Roweth,
D. Hybrid Monte Carlo. Physics Letters B , 195(2):216  X  222, 1987.
 Girolami, M. and Calderhead, B. Riemann manifold
Langevin and Hamiltonian Monte Carlo methods. Jour-nal of the Royal Statistical Society Series B , 73(2):123 X  214, 03 2011.
 Hoffman, M.D. and Gelman, A. The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo. arXiv , 1111.4246, 2011.
 Hoffman, M.D., Blei, D. M., Wang, C., and Paisley, J. Stochastic variational inference. Journal of Maching Learning Research , 14(1):1303 X 1347, May 2013.
 Horowitz, A.M. A generalized guided Monte Carlo algo-rithm. Physics Letters B , 268(2):247  X  252, 1991. Korattikara, A., Chen, Y., and Welling, M. Austerity in MCMC land: Cutting the Metropolis-Hastings budget. In Proceedings of the 30th International Conference on Machine Learning (ICML X 14) , volume 32, pp. 181 X 189, February 2014.
 Levin, D.A., Peres, Y., and Wilmer, E.L. Markov Chain-s and Mixing Times . American Mathematical Society, 2008.
 Neal, R.M. Bayesian learning via stochastic dynamics. In
Advances in Neural Information Processing Systems 5 (NIPS X 93) , pp. 475 X 482, 1993.
 Neal, R.M. MCMC using Hamiltonian dynamics. Hand-book of Markov Chain Monte Carlo , 54:113 X 162, 2010. Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization , 19(4): 1574 X 1609, January 2009.
 Patterson, S. and Teh, Y.W. Stochastic gradient Rieman-nian Langevin dynamics on the probability simplex. In
Advances in Neural Information Processing Systems 26 (NIPS X 13) , pp. 3102 X 3110. 2013.
 Robbins, H. and Monro, S. A stochastic approximation method. The Annals of Mathematical Statistics , 22(3): 400 X 407, 09 1951.
 Salakhutdinov, R. and Mnih, A. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In Proceedings of the 25th International Conference on Machine Learning (ICML X 08) , pp. 880 X 887, 2008a.
 Salakhutdinov, R. and Mnih, A. Probabilistic matrix factor-ization. In Advances in Neural Information Processing Systems 20 (NIPS X 08) , pp. 1257 X 1264, 2008b.
 Shi, J., Chen, T., Yuan, R., Yuan, B., and Ao, P. Relation of a new interpretation of stochastic differential equations to Ito process. Journal of Statistical Physics , 148(3): 579 X 590, 2012.
 Sutskever, I., Martens, J., Dahl, G. E., and Hinton, G. E. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Con-ference on Machine Learning (ICML X 13) , volume 28, p-p. 1139 X 1147, May 2013.
 Wang, M.C. and Uhlenbeck, G.E. On the Theory of the
Brownian Motion II. Reviews of Modern Physics , 17(2-3):323, 1945.
 Wang, Z., Mohamed, S., and Nando, D. Adaptive Hamil-tonian and Riemann manifold Monte Carlo. In Proceed-ings of the 30th International Conference on Machine
Learning (ICML X 13) , volume 28, pp. 1462 X 1470, May 2013.
 Welling, M. and Teh, Y.W. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICM-L X 11) , pp. 681 X 688, June 2011.
 Yin, L. and Ao, P. Existence and construction of dynamical potential in nonequilibrium processes without detailed balance. Journal of Physics A: Mathematical and Gen-
