 We study the novel problem of efficiently computing the update distance for a pair of relational databases. In analogy to the edit distance of strings, we define th e update distance of two databases as the minimal number of set-orie nted insert, delete and modifica-tion operations necessary to transfor m one database into the other. We show how this distance can be computed by traversing a search space of database instances connected by update opera-tions. This insight leads to a family of algorithms that compute the update distance or approximations of it. In our experiments we observed that a simple heuristic performs surprisingly well in most considered cases. Our motivation for studying distance measures for databases stems from the field of scientific databases. There, replicas of a single database are often maintained at different sites, which typi-cally leads to (accidental or planned) divergence of their content. To re-create a consistent view, these differences must be resolved. Such an effort requires an understanding of the process that pro-duced them. We found that minimal update sequences of set-oriented update operations are a proper and concise representation of systematic errors, thus giving valuable clues to domain experts responsible for conflict resolution. H.2.8 [ Database Management ]: Database Applications  X  Scien-tific databases.
 Contradicting databases, conflic t resolution, update distance. Today, many scientific databases overlap in their sets of repre-sented objects due to redundant da ta generation or data replica-tion. For instance, in life science research it is common practice to distribute the same set of samples, such as clones, proteins, or patient X  X  blood, to different labor atories to enhance the reliability of analysis results. Whenever overlapping data is generated or administered at different sites, there is a high probability of dif-ferences in results. These differences do not need to be accidental, but could be the result of differe nt data production and processing workflows. For example, the three protein structure databases OpenMMS [5], MSD [6], and Columba [19] are all copies of the Protein Data Bank PDB [4]. Howe ver, due to different cleansing strategies, these copies vary s ubstantially. Thus, a biologist is often faced with conflicting copies of the same set of real world objects and with the problem of so lving these conflicts to produce a consistent view of the data. Many inconsistencies are highly sy stematically due to different controlled vocabularies, different measurement units, different abbreviations, or due to consis tent bias during experimental analysis. Learning about the reasons that led to inconsistencies is valuable information when asse ssing the quality of contradicting values to resolve conflicts. Unfo rtunately, in most cases only the final databases are accessible w ithout any additional knowledge about the data generation or ma nipulation processes. Assuming that conflicts do not occur randomly, but follow specific (but unknown) regularities, patterns of the form  X  X F condition THEN conflict X  provide valuable knowledge to facilitate their under-standing. A domain expert could ev aluate these patterns to assess the correctness of conflicting values and therefore for conflict resolution. In [16] we propose an algorithm for finding such pat-terns using association rule mining. In this paper, we develop a di fferent approach for describing  X  X egular differences X  among contra dicting databases: The detec-tion of minimal update sequences that transform one database into the other. We consider SQL-lik e update operations because these are the fundamental operations for the manipulation of data stored in relational databases. Given a pa ir of contradicting databases, each operation may (i) directly represent an update operation that has been applied to one of the da tabases if they both evolved from a common ancestor (as in the PDB example), or (ii) describe sys-tematic differences between the databases. Consider the example in Figure 1 showing the fictitious results of two groups investigat-ing the same set of owls (identified by their IDs). For transform-ing DBOwl into OWLBase Latin species names must be replaced with common names. We also need to modify the color for all snowy owls, as DBOwl uses a finer grained distinction in colors. Therefore, regardless of how th ese databases were generated, there are update operations that allow a user to infer possible rea-sons for the existing conflicts. 
Figure 1: Contradicting databases resulting from different We call the number of operations within a minimal update se-quence the update distance between the two databases. Each up-date sequence as long as the update distance is one of the simplest possible explanations for the observed differences. Following the  X  X ccam X  X  Razor X  principle, we conclude that the simplest expla-nations are usually the most likely. Minimal update sequences give valuable clues on what ha s happened to make a database different from its original state. Therefore, the update distance is a semantic distance measure, as it is inherently process-oriented in contrast to purely syntactic meas ures such as counting different values. The update distance is defined analogously to the edit distance for strings [14], i.e., the minimal number of edit operations that trans-form one string into another. In contrast to editing strings, set-oriented database updates strongl y influence each other thus giv-ing importance to the order of ope rations. To give an idea of the complexity of the problem, consider transforming databases r into r 2 of Figure 2. A first approach solves each conflict individu-ally (Figure 2a). The five resu lting operations could be executed in any order. A greedy approach, solving as many conflicts as possible with each operation, results in a total of four update op-erations (Figure 2b). However, the minimal update sequence for transforming r 1 into r 2 has three update operations (Figure 2c). troduce new conflicts, because these conflicts can be used as dis-criminating conditions in later update operations. The first opera-tion in Figure 2c temporarily in creases the total number of con-flicts, but this is compen sated in later operations. Figure 2: Different update sequences for a pair of databases. In this paper, we present exact and approximate algorithms for computing the update distance a nd for finding minimal sequences of update operations for a pair of databases. Even though we con-sider only a restricted form of updates (namely those where at-tribute values are set to consta nts), our algorithms for computing the exact solution require exponential space and time. However, we also present greedy strategies that lead to good results in all examples we considered. The rest of this paper is struct ured as follows: The next section discusses related work. Section 3 gives our definition of update distance and derives an upper a nd lower bound, which are impor-tant for optimization. In Secti on 4, algorithms for finding minimal sequences are presented. Secti on 5 discusses fa st approximate algorithms. In Section 6 we present experimental results, and Section 7 concludes our paper. Th ere is also a longer version of this paper available for further details [17]. To the best of our knowledge the problem of finding minimal sequences of set-oriented update operations for relational data-bases has not been considered be fore. The main areas of related work are: definitions of distan ce for databases, representing dif-ferences of databases, and fi nding patterns in conflicting data. There are other definitions of database distance based on the modification of databases. One comes from the area of computing consistent query answers for inc onsistent databases [1]. Here, a repair for a database r violating a set of integrity constraints IC is defined as a database r X  that satis fies IC and has minimal distance to r. In this context, the distance is defined as the number of tuples that are contained in one database, but not in the other. Thus, the distance definition is very different from ours, as we need to find the actual update sequences for transforming one database into the other. Bohannon et al. presented an algorithm for finding a repair that is the cheapest under a cost model for modifications and up-dates, also leading to a quite different notion of distance [7]. While their approach needs to consider the order of their opera-tions they do not consider set-oriented update operations. The latter is also true for other approaches that represent differ-ences between databases. Chawathe et al. and Labioet et al. use sequences of insert, delete, a nd update operations to represent differences between database snap shots [13] or hierarchically structured data [8]. Both appro aches consider only operations that affect a single tuple. Since databases are manipulated with (set-oriented) SQL commands, we consider our problem as more natu-ral than a tuple-at-a-time approach . Several other applications use so called  X  X pdate deltas X  to repr esent differences between data-bases. In database versioning they are used as memory effective representation of different databa se versions [10]. However, ver-sioning collects the actual operations during execution instead of having to rediscover them from two given versions. Other methods for finding patterns in contradictory data to sup-port conflict resolution are presented in [16] and [11]. In [11], the authors distinguish between cont ext dependent and context inde-pendent conflicts. Context depende nt conflicts represent system-atic disparities that are conseque nces of conflicting assumptions or interpretations. Context indepe ndent conflicts are idiosyncratic in nature and are consequences of random events, human errors, or imperfect instrumentation. A ccordingly, we consider only con-text dependent conflicts. However, in contrast to [11], we do not consider complex data conversi on rules for conflict resolution. Instead we always use one of the conflicting values as solution. We consider the discovery of c onflict conversion rules as future work in Section 7. However, we do consider the conflict causing context to be identifiable as data patterns. In [16], we use a fre-quent itemset mining algorithm to find conflicts frequently occur-ring together with certain patterns in the data. These patterns can only provide a static view on the differences and do not take into account possible dependencies between conflicts. In the approach presented here, such dependencies are implicitly captured by the order of update operations within a (minimal) update sequence. A prerequisite of our approach is the ability to identify identical real-world objects in different databa ses. In general, this is a diffi-cult problem with a long history of research [12, 15, 22] that we consider as orthogonal to our problem. Throughout this paper, we assume the existence of a source-spanning object identifier. These identifiers may have been assigned to tuples by a preceding dupli-cate detection step. In this section we formally de fine the update distance between two relational databases. Each da tabase consists only of a single relation r, following the same schema R(A 1 , ..., A n ). The domain of each attribute A is denoted by dom (A). Without loss of gener-ality we assume dom (A) =  X  for all A  X  R and that A primary key of each r. We will use ID as a synonym for A ples are denoted by t and attribute values of a tuple are denoted by t[A]. We use t{j} to refer to the tuple with primary key value j, j = 1, ..., m. As mentioned before, we do not consider the problem of assigning unique key values that a llow the association of tuples representing the same real-world object in different databases. In the following, we introduce the necessary concepts to define update sequences. Section 3.1 defi nes matches and conflicts be-tween two databases. Section 3. 2 introduces the types of basic operations we assume as possi ble updates and assembles them into update sequences. These types naturally lead to the notion of shortest update sequences. In S ection 3.3, we define upper and lower bounds for update distances wh ich we use later for pruning. Our basic update operations are e ssentially SQL updates that set attributes selected by conjuncti ons of equality patterns to con-stants. Even with this simple model, computing minimal update distances is not trivial, as we shall see in Section 4. Thus, we leave extensions to databases c onsisting of multiple relations and to more powerful updates operations for future work. A pair of tuples from databases r 1 and r 2 is called a matching pair if they posses identical primary key values. The set of all match-ing pairs between databases (i.e., relations) r 1 and r M(r 1 , r 2 ). Let m = (t 1 , t 2 ) be a matching pair from M(r different tuples from m are denoted by tup 1 (m) and tup equal primary key value of both tuples is denoted by id (m). A pair of databases r 1 and r 2 is called overlapping if M(r that we are only interested in finding regularities between the overlapping parts of da tabases. We do not consider finding regu-larities within the sets of tuples missing in either of the databases. Therefore, we assume |r 1 | = |r 2 | = |M(r 1 , r 2 )|. Within a matching pair several conflicts may occur. We represent each conflict by the matching pair m and the attribute A in which the conflict occurs. D
EFINITION 1 (S ET OF C ONFLICTS ): The set of conflicts between a pair of databases r 1 and r 2 , denoted by C(r 1 tuples (m, A) where a conflict in a ttribute A of pair m exists, i.e., A pair of databases r 1 and r 2 is called contradicting , if there exists at least one conflict between them, i.e., C(r 1 , r ample, consider the contradicti ng databases of Figure 2. The set of matching pairs contains ten el ements, and there are five con-flicts, i.e., C(r 1 , r 2 ) = {((t 1 {4}, t 2 {4}), A ((t {6}, t 2 {6}), A 3 ), ((t 1 {7}, t 2 {7}), A 3 ), ((t 1 {8}, t Update operations are used to m odify existing databases. They can be considered as functions that map databases onto each other. Let  X  (R) denote the infinite set of databases following schema R that satisfy the primary key constraint. An update op-eration is a mapping  X  :  X  (R)  X   X  (R). For relational databases there are three types of basic update operations, namely insert, delete, and modify [20]. For space limitations, we consider only modify operations and not insert and delete operations for this paper. This restriction also has a practical reason: Consider an object being stored (with conflicts) in two databases r modifications that are necessary to unify both descriptions. We are not interested in an update sequence that, for transforming r into r 2 , first deletes t 1 and then inserts t 2 ever, our algorithms can be extended to include insert and delete operations, as discussed in [17]. Before defining sequences of upda te operations, we first fix the expressiveness of our ba sic update operations. D
EFINITION 2 (T ERM ): A term  X  over schema R is a pair (A, x), with attribute A  X  R and value x  X  dom (A). We define attr (  X  ) = A and value (  X  ) = x.  X  A term can be interpreted as a Boolean-function on tuples. A tu-support of  X  in r. Terms are combined to patterns selecting the set of tuples affected by an update. D
EFINITION 3 (P ATTERN ): A pattern  X  over schema R is a set of terms over R, where all terms are defined over different attributes, i.e., A tuple t satisfies  X  if it satisfies each term within  X  . A pattern is therefore a conjunction of terms. Th e empty pattern is satisfied by each tuple of a database. Analogous to the definitions above,  X  (r) denotes the set of tuples selected by  X  . A pattern describes which tuples should be changed by a modification operation. D tion  X  = (  X  ,  X  ) is a term-pattern pair with attr (  X  )  X  R / ID. The definition of a modification operation (i) excludes the pri-mary key attribute from being modi fied, and (ii) allows only one attribute to be modified. When a pplied to a database r, a modifica-does not necessarily exist a reverse operation for each modifica-tion operation. For example, the operation  X  = ((A 2 , 7), {(A when applied to database r 1 of Figure 2. We need at least six modification operations to undo this single operation. D
EFINITION 5 (U PDATE S EQUENCE ): A list of update operations  X  = &lt;  X  1 , ...,  X  k &gt; is called an update sequence . When applied to data-base r 1 , an update sequence generate s (or derives) database  X  (r  X  (r 1 ) =  X  k (...  X  2 (  X  1 (r 1 ))...).  X  Obviously, the order of operations within an update sequence is important. For example, update sequences  X  1 = &lt;  X  1 = &lt;  X  2 ,  X  1 &gt; with  X  1 = ((A 2 , 7), {(A 3 , 1)}) and  X  1)}) have different results when applied to database r 1 The first sequence results in a database, where the value for at-quence the operation  X  1 has no effect, as the pattern is no longer satisfied by any of the tuples after applying operation  X  We call  X  a transformer for r 1 and r 2 , iff  X  (r 1 of update operations within a seque nce is called its length and is denoted by |  X  |. Figure 2 lists three update sequences (a-c) of dif-ferent length, which are transfor mers for the databases shown. D and r 2 , an update sequence  X  with  X  (r 1 ) = r transformer for r 1 and r 2 if there does not exist another trans-former  X   X  with  X   X (r 1 ) = r 2 and |  X   X  X  &lt; |  X  |. Clearly, minimal transformers for a pair of databases are not unique. The set of all minimal transformers for r noted as T(r 1 , r 2 ). D
EFINITION 7 (U PDATE D ISTANCE ): For a pair of databases r r minimal transformer for r 1 and r 2 .  X  Note that the update distance is not a metric as it is not a symmet-ric relation. Based on our definitions, we are ready to define the main problem considered in this paper. P
ROBLEM S TATEMENT : Given a pair of databases r 1 and r minimal transformers from r 1 to r 2 . This subsection defines upper and lower bounds for the update distance. They are later utilized as pruning criteria. D
EFINITION 8 (R ESOLUTION D ISTANCE ): For a pair of databases r of conflicts between them, i.e.,  X  R (r 1 , r 2 ) = |C(r Transforming a database r 1 into a database r 2 requires the conflicts between them to be solved by replacing conflicting values in r with their according values from r 2 . Since each tuple can be se-lected individually by a pattern containing its primary key as a term, we can define modification operations that change exactly one value in the database. Therefore, the resolution distance is an upper bound for the update distance. L
EMMA 1: For each pair of databases r 1 and r 2 there exists a trans-former  X  of length  X  R (r 1 , r 2 ). P ROOF : Omitted for space limitations.  X  To define a lower bound as well, we recognize that each update operation modifies only one attri bute according to our definition. We subsume the conflicts that are potentially solvable using a single modification operation within a conflict group . D
EFINITION 9 (C ONFLICT G ROUP ): Given a pair of databases r r . A conflict group  X  is an attribute-value pair (A, x) with attr (  X  ) between r 1 and r 2 having the following property: Thus, all conflicts represented by a conflict group  X  occur in the same attribute A and have the same solution x. These conflicts are hence solvable using a modification operation with  X  as the modi-tween a pair of databases. According to this definition, the only conflict group for the databases in Figure 2 is  X  = (A 3 L
EMMA 2: For each pair of databases r 1 and r 2 , there cannot exist a transformer for r 1 and r 2 that is shorter than |K(r 1 this lower bound as LB(r 1 , r 2 ). P operation  X  per conflict group  X   X  K(r 1 , r 2 ), with  X  =  X  . For the example in Figure 2 the upda te distance is three, as shown by the update sequence in c). The lower bound of the update dis-tance is one and the upper bound is five. This section describes the TRANSIT algorithms to determine the set of minimal TRANS formers for contrad I cting da r . Both algorithms essentially enumerate the space of all data-bases reachable by applying sequences of modifications to r Doing so efficiently poses seve ral challenges for which we de-scribe solutions. First, we introduce transition graphs as formal-izations of the search problem. Since many update sequences lead to the same database state, dupli cate detection is of outmost im-portance. We describe a hashi ng scheme for efficient duplicate checking. We show how we use the upper and lower bounds de-fined in Section 3.3 to prune the search space, leading to a branch and bound algorithm. We then descri be a breadth-first strategy for traversing the search space and briefly sketch a depth-first strat-egy. In Section 4.2, we show how -given a database state -the set of all possible modification opera tions can be computed using a mining algorithm that computes closed frequent itemsets. determining all databa ses derivable from r o by a single modifica-tion operation, called level-1 data bases. Level-2 databases are computed by using all level-1 da tabases as starting point for an-other modification. This pro cess continues until we reach r level at which the target is reached first reflects the minimal num-ber of modification operations necessary to derive r t the update distance. To determine T(r o , r t ) the algorithm also needs to enumerate all other sequences that are of the same length. We maintain the sequence of modification operations with each database. Since multiple sequences may generate the same database, level-n databases may ha ve an update distance that is actually shorter than n. We later treat the detection of duplicated databases. Since we enumerate all possible modifications at each level and for each database, we ensure that our first match with r defines the shortest possible sequence. We represent the search space using a directed labeled graph, called the transition graph . Vertices of this graph are databases connected by directed edges that represent modification opera-tions. D
EFINITION 10 (T RANSITION G RAPH ): For two databases r the transition graph G T = (V, E) with vertices V and edges E is defined as follows: V is the set of all databases derivable from r using an update sequence of length shorter than or equal to the update distance  X  U (r o , r t ). This implies that r t  X  V. E is the set of all edges e = (r 1 , r 2 ,  X  ) for which  X  (r 1 ) = r 2 noted by target (e).  X  D
EFINITION 11 (P ATH ): A path  X  = &lt;e 1 , ..., e graph G T = (V, E) is a sequence of edges from E, with source (e = target (e i-1 ) for all 1 &lt;i  X  p. Two databases r nected by  X  if source (e 1 ) = r 1 and target (e p ) = r Each path between two databases r 1 and r 2 defines a transformer  X  (r 1 ) = r 2 . In accordance to D EFINITION 6 a path is minimal if no shorter path between the sa me two databases exists. The TRANSIT-algorithms iterativ ely construct the transition graph starting with database r o as the only vertex. Figure 3 shows an example. Levels are outlined by horizontal lines and derivable databases are only shown at the level of their update distance from r o . Vertices and edges on the minimal paths between the origin and target are encl osed within a gray box. Figure 3: An exemplified transiti on graph as generated by the 
TRANSIT algorithm without p erforming any pruning of Duplicates at different levels of the graph may introduce cycles. Since the corresponding edges -delineated by dotted lines for clarity in Figure 3 -cannot be part of a minimal transformer, they are not included in the graph. Intra-level duplicates may result in multiple edges between two verti ces on adjacent distance levels. As we will show in Section 6, a large portion of all generated databases are duplicates. For example, the operations  X  applied to database r 1 of Figure 2. Also, many update sequences derive a database from itself. For example, the update sequence &lt;((A 2 , 0), {(A 1 , 1)}), ((A 2 , 1), {(A 1 , 1)})&gt; derives r a 2-step update sequence. We must detect duplicates efficiently to avoid unnecessary explosion of the search space. Duplicate detection requires comp arison of entire databases. To reduce the number of duplicate checks, we compute a hash value for each database and maintain a hash table for generated data-bases. Complete database comp arisons are only performed when the hash values of two databases are equal, which drastically re-duces the number of (expensive) fu ll database comparisons at the price of having to maintain the hash table. We currently employ the followi ng hash function for databases: Without a loss of generality we assume the IDs to be integers in the range 1, ..., m. We number the attribute values of the particu-lar tuples in the following order 0:t{1}[A 1 ], 1:t{1}[A (m*n)-1:t{m}[A n ], called the cell index. With each database we maintain a list of the conflicting values with an order based on this cell index. We select k valu es from this list, having cell index where the i-th digit is the value of cell c i modulo 10. We also tested a hash function based on a histogram of the attribute values occurring within a database, but we found the later scheme infe-rior in our experiments. The TRANSIT-algorithms try to avoid generating the complete transition graph. The number of ve rtices outside of the minimal transition graph in Figure 3 shows that many of the generated databases are not part of any mi nimal transformer. This observa-tion is supported by the following example: E
XAMPLE 1: Enumerating the complete transition graph with du-plicate detection for the databases shown in Figure 2 results in a graph with 294,998 vertices a nd 768,333 edges. However, only two minimal paths from r 1 to r 2 exist, which together contain only six vertices.  X  This large difference suggests that pruning is essential. In TRAN-SIT, pruning uses the upper and lower bounds for the update dis-tance as defined in Section 3.3. Let  X  denote the current upper bound for the update distance between r o and r t The update distance  X  U (r o , r) is maintained with each vertex in order to avoid recalculation. We decrease  X  whenever a database r is generated with (  X   X  (r o ) = r with |  X  | =  X  U (r o , r). Then, L EMMA 1 guarantees the exis-tence of a transformer  X   X (r) = r t with length |  X  following simple Lemma proofs the existence of a transformer  X   X  X  (r o ) = r t having length |  X   X  X  | =  X  U (r o , r) +  X  L
EMMA 3: Given transformers  X  1 (r 1 ) = r 2 and  X  exists a transformer  X  3 (r 1 ) = r 3 with length |  X  3 P ROOF : Omitted for space limitations.  X  Each time the current bound  X  is decreased we remove all data-bases r from the transition graph with insufficient bound, i.e., for which  X  U (r o , r) + LB(r, r t ) &gt;  X  . The previous approach resembles a branch and bound behavior. Therein, we can explore the search space either in breadth-first or in depth-first manner. We first describe a breadth-first algorithm (see Figure 4). The algorithm generates all data bases derivable by update se-quences of increasing length. Within the branch step a database is chosen for processing. We genera te all databases that are deriv-able from this database by a si ngle modification operation. Next, in the bound step the current bound is decreased if possible and databases are pruned as described. After finishing the processing of the current database we chose the next database for processing from the remaining, untested databa ses in the graph. We process all databases at the current level first before proceeding to data-bases at the next level. We continue until r t is reached and no untested database remains. 1 TRANSIT-BFS(r o , r t ) { 2 G T := ({r o }, {}); 3 V P := V(G T ); 4  X  U := 0; 5  X  :=  X  R (r o , r t ); 6 while (r t  X  V P ) { 7  X  U :=  X  U + 1; 8 V C := {}; 9 for each r i  X  V P do { 10 MDF := modifier (r i , r t ); 11 for each  X   X  MDF do { 12 r new :=  X  ( clone (r i )); 13 if ((LB(r new , r t ) +  X  U )  X   X  ) { 14 if (r new  X  V(G T )) 15 V(G T ) := V(G T )  X  {r new }; 16 E(G T ) := E(G T )  X  (r i , r new ,  X  ); 17 V C := V C  X  {r new }; 18 if ((  X  R (r new , r t ) +  X  U ) &lt;  X  19  X  :=  X  R (r new , r t ) +  X  U ; 20 prune V P , V C , G T ,  X  ; 21 } 22 } else if (r new  X  V C ) { 23 E(G T ) := E(G T )  X  (r i , r new ,  X  ); 24 } } } } 25 V P := sort (V C ); 26 } 27 output min_paths (G T , r o , r t ); 28 } Figure 4 shows the corresponding algorithm TRANSIT-BFS. Each database from the previous level, maintained in V P , is proc-essed while enumerating the current level ( lines 9-24). The data-bases at the current level, maintained in V C , afterwards become the candidates for the enumeration of the next level ( line 25). We sort the candidates in ascending order of their lower and upper bounds. This is done with the inte ntion of decreasing the current bound  X  as soon as possible, therefore avoiding the unnecessary insertion of databases that are pruned afterwards. After reaching the destination the algorithm returns the set of minimal paths in are interested in calculating the update distance only, the algo-rithm can terminate immediately after r t is derived for the first time (check for equality after line 12). Processing a database starts by determining the set of possible modification operations ( line 10  X  see Section 4.2 for details). Each of the operations is applied to a copy of the database, as modification operations alte r the given database ( line 12). The resulting database is added to the transition graph and to V does not already occur within the graph ( lines 14-17). Otherwise, the database is a duplicate. It is an intra-level duplicate, if it also occurs in V C . In this case the database has been derived before at the current distance level. Intra-level duplicates add additional edges. Otherwise, no changes occur. The transition graph may also be constructed in depth-first man-ner. We refer to the corres ponding algorithm as TRANSIT-DFS. Within this algorithm, we immediately proceed to the next dis-tance level after finishing the processing of the current database, i.e., generating all databases deri vable with a single modification operation. We chose that databa se from all generated ones with the smallest lower bound as the ne w current database. Pruning is performed as described above. Th e depth-first approach finds a first solution after processing fe wer databases then the breadth-first approach. This solution is not necessarily optimal. Therefore, after reaching the target database, TRANSIT-DFS needs to return to the previous databases and te st them as candidates in depth-first manner. The algorithm continue s until all databases that have not been pruned by the bounding step have been tested. Compared to TRANSIT-BFS, duplicate detection is more complicated be-cause identical databases may be generated multiple times at de-creasing distance levels. Every time a database is derived at a lower level once more, it must be considered as a candidate again and cannot be rejected as a duplicate. The experiments of Subsection 6. 1 show the advantage of using the branch and bound approaches over enumerating the complete transition graph. According to D EFINITION 4, a modification operation is a pair of modification term  X  and modification pattern  X  . For a given data-base r the set of modification ope rations is then defined as the Cartesian product of the set of po ssible modification terms and the set of modification patterns. Modification terms are attribute-value pairs (D EFINITION specify the attribute to be modified and the value to be set by a modification operation. Every non-ke y attribute is modifiable and every value in the attribute domai n is a valid modification value. Therefore, the set of modification terms for a database r is defined as the union of the sets of modification terms for each non-key attribute A  X  R, A  X  ID, that is A  X  dom (A). A problem with this definition is the infinite size of dom (A) that leads to an infinite set of modification terms and therefore to an infinite set of modifica-tion operations. Consequently, our algorithm would not terminate. We must therefore restrict the se t of possible modification values. This is accomplished by permitting only the following values in modification terms for attribute A: For example, the set of modi fication terms for attribute A databases of Figure 2 is {(A 3 , 0), (A 3 , 1), (A 3 , 2)}, where 2 is the Skolem constant. During TRANSIT , all Skolem constants used are maintained within a separate list for each attribute. Let P(r) denote the set of patterns  X  that select a non-empty set of tuples from r, i.e.,  X  (r)  X   X  . If we regard a tuple t as a set of terms (A, t[A]) with one term for each attribute A  X  R, P(r) is effi-ciently computable using a freque nt itemset mining algorithm [2]. Very likely, this set contains pairs of patterns  X  and  X  1 (r) =  X  2 (r). For example, the patterns  X  = {(A 2 , 1), (A 3 , 1)} select the same set of tuples from database r of Figure 2. Using both patterns for enumeration of modification operations would result in operations with equal effect. We avoid such redundancy by restricting P(r) to the set of closed patterns , denoted by P C (r). The following definition is taken from [3, 18]: D
EFINITION 12 (C LOSED P ATTERN ): Given a database r, a pattern  X  pattern  X   X   X   X  with  X   X (r) =  X  (r).  X  Pattern  X  2 from the example above is a closed pattern for database r of Figure 2. Pattern  X  1 is not a closed pattern, because  X  and  X  1 (r 2 ) =  X  2 (r 2 ). A closed pattern  X  represents exactly those terms that are common for every tuple in  X  (r) (when viewing the tuples as sets of terms). Following this definition there are no two closed patterns  X  P (r),  X  1  X   X  2 , that select equal subsets of r. L
EMMA 4: Given a database r. For each pattern  X   X  P(r) there exists a pattern  X   X   X  P C (r) with  X  (r) =  X   X (r). P ROOF : Omitted for space limitations.  X  Based on L EMMA 4 the set of closed patterns for a database r uniquely defines the set of selectable subsets of r. Therefore, it is sufficient to use P C (r) extended by the empty pattern instead of P(r) as the set of modification pa tterns. We add the empty pattern to P C (r) in order to allow modificati ons of the complete database at once. We are able to use existing methods for mining closed itemsets like CHARM [23], CLOSET+ [21], or FARMER [9]. Within our implementation we currently use CHARM [23]. When enumerating modification ope rations for a given database, we are only interested in those operations that actually change the database. A modification operation has no effect if the modifica-tion term also occurs within the modification pattern. In this case, all selected tuples already possess the new value in the modified attribute. These operations are removed from the final result. The described TRANSIT-algorithms are only applicable for small databases. For larger databases the search space of derivable da-tabases is enormous due to the la rge number of closed patterns for each database. The maximum number of closed pattern for a da-tabase r is 2 |r| -1, i.e., each non-empty subset of r defines a differ-ent closed pattern. While this is the worst case, there are at least |r| closed patterns for each database, as each tuple forms a closed pattern by itself. The large number of closed patterns results in an even larger number of modifica tion operations and therefore de-rivable databases. Despite e liminating duplicates and pruning over 95% of the generated data bases immediately (see Section 6.1) processing the remaining da tabases is still too expensive. Within this section we describe heuristics which do not necessar-ily find the exact solution but are able to handle databases of al-most arbitrary size. We analyze the quality of the computed re-sults in Section 6.2. A first simple heuristic is applying a greedy algorithm, called GREEDY-TRANSIT. The algorithm returns a single transformer by selecting at each level the modification operation that reduces the number of conflicts most. Given a pair of databases r we start by enumerating all modification operations for r each operation  X  we then determine the number of conflicts be-tween databases  X  (r o ) and r t , i.e., the resolution distance  X  The modification operation whose re sulting database has the least resolution distance is selected as the next operation in the final transformer. Ties are broken randomly. The resulting database becomes the next starting point, i.e., the new r o merate all modification operations and choose the operation that reduces the number of conflicts mo st. This is continued until the target database is reached. The described procedure ensures that the database chosen as start-ing point always contains fewer conflicts with r t previous databases. Therefore, ne ither cycles nor duplicated data-bases at different levels may occu r. However, the assumption that the database with the fewest conflicts has the potential of reaching the destination first is not always correct. For the databases of Figure 2 the resulting transformer has a length of four. The main challenge for the greedy algorithm is the enumeration of modification operations. Enum erating the complete set of modification operations is infeasible for large databases due to the large number of closed patterns. Ho wever, it is also not necessary. We avoid enumerating modificati on operations that are no candi-date for the final transformer by interleaving the mining for closed patterns with determination of resolution distances. The algorithm is outlined in Figure 5. Every time the pattern mining algorithm identifies a new closed pattern  X  ( line 3), we enumerate all modification operations using  X  that are able to reduce the number of conflicts more than the currently best operation  X  ( lines 4-10). These are modification operations (i) that have a modification term that equals one of the current conflict groups  X   X  K(r o , r t ), and (ii) select more t uples belonging in conflict group  X  then the reduction in the number of conflicts by  X  tained in minsup ). The minsup value is also used as support con-straint in the pattern mining al gorithm to avoid enumeration of patterns that do not select a sufficient set of tuples. tain a conflict belonging in conflict group  X  , i.e., This number can easily be derive d while scanning the databases to determine the initial set of terms for closed pattern mining. We further define sup (  X  ,  X  ) as the minimum sup (  X  ,  X  ) for all the terms  X   X   X  . This is the maximal number of conflicts the pattern can potentially solve from conflict group  X  . Whenever an operation is enumerated that performs better than  X  max we are able to increase minsup and thereby avoid further enumeration of patterns that cannot solve more conflicts than the new  X  max ( lines 7-10). GREEDY-TRANSIT calls greedyNext () for each database r o result of greedyNext () can be empty as we use 2 as the initial minsup . In this case we solve one of the existing conflicts ran-domly using the tuple where the conflict occurs as closed pattern. Another heuristic is based on solving the conflicts within each conflict group independently. We use the sum of necessary opera-tions for conflict solution of each conflict group as an approxima-tion of the update distance. This is called the group solution cost . The approximation completely disregards the possible impact that the modification of values for so me of the tuples may have on solving conflicts for other tuples. 1 greedyNext (r o , r t ) { 2  X  max :=  X  ; minsup := 2; 3 while (  X  = nextPattern (minsup)) { 4 for each  X   X  K(r o , r t ) { 5 if ( sup (  X  ,  X  )  X  minsup) { 6 r c := (  X  ,  X  )(r); 7 if (  X  R (r o , r t ) X   X  R (r c , r t )  X  minsup) { 8  X  max := (  X  ,  X  ); 9 minsup := (  X  R (r o , r t ) X   X  R (r c , r 10 } } } } 11 return  X  max ; 12 } Figure 5: Determine the modification operation that results in Determining the minimal number of modification operations nec-essary to solve the conflicts w ithin a conflict group still is expen-sive, as shown in Section 6.1. Th erefore, we restrict the set of valid modification operations fo r approximating the update dis-tance by considering only operations that reduce the number of conflicts and that do not introduce new conflicts or alter conflict-ing values in existing conflic ts. The entire algorithm, called TRANSIT-APPROX can be found in [17] but is omitted here due to space limitations. The group solution cost may also be used as a replacement for the lower bound within the algorithms TRANSIT-BFS and TRAN-SIT-DFS. We thus might miss the exact solution. However, in all our experiments this heuristic co mputed the exact solution. The corresponding algorithms are called TRANSIT-BFS (GS) and TRANSIT-DFS (GS), respectively. This section discusses some of the results of our experiments applying the defined algorithms to different pairs of databases. We implemented all algorithms using Java  X  J2SE 5.0. The code and experimental data is available from the authors. Computing minimal update sequen ces using the algorithms de-scribed in Section 4 is feasible only for small databases. We used several small synthetic databases pairs in our experiments. In this give an idea of the complexity of the algorithms. We refer the interested reader to [17] for further results. The necessary computation to determine the set of minimal up-date sequences that transform r 1 into r 2 is shown in Table 1a. The first two columns list the numbe r of databases processed and modification operations executed for building the transition graph. We also list the overall num ber of databases added to the graph together with the number of databases generated as dupli-cates. Table 1a shows a huge difference between the number of modifi-cation operations executed and the number of databases added to the graph or being identified as duplicates for both approaches. This observation indicates that the majority of the generated data-bases are pruned due to their upper and lower bounds. The num-ber of pruned databases is between 80-95% for TRANSIT-BFS and above 95% for TRANSIT-DFS. Still, due to the enormous number of modification operations and databases derivable at each node of the transition graph execution time and memory requirements prevent an application of the algorithms for larger databases. The depth-first approach is inferi or for the databases of Figure 2. The optimal solution requires the insertion of conflicts at first. Thus, the depth-first approach ge nerates a first solution that is longer that the actual update distan ce. It therefore tests several databases at distance levels above the actual update distance. This is reflected by comparing the number of databases added and tested in columns 1 and 3 of Table 1a. For TRANSIF-DFS more databases are tested than added to the graph, due to those data-bases that are added once but tested several times at decreasing distance levels. a) TRANSIT-BFS vs. TRANSIT-DFS 
DFS 4,275 603,971 4,204 4,417 4,483 b) TRANSIT-BFS (GS) vs. TRANSIT-DFS (GS) Comparing the numbers of data bases added and tested for TRANSIT-BFS shows the general problem of this approach: a large number of databases that are added to the graph are never processed afterwards. Our experime nts show that pruning of data-bases once added to the graph is not very effective for both ap-proaches. Therefore, the number of databases in the graph grows linearly with the number of added databases. We observe that in general the memory requirement for the breadth-first approach is higher than that for the depth-first approach. Table 1b shows the necessary effort to determine the set of mini-mal transformers when using the group solution cost as the lower bound for both branch and bound algorithms. In our experiments, this heuristic always computes the correct update distance, but does not find all minimal update sequences. Compared to the numbers in Table 1a, the effort regarding data-bases tested and added is si gnificantly lower for TRANSIT-BFS (GS) and TRANSIT-DFS (GS). As a downside, the computa-tion cost may increase due to the computation of the group solu-tion cost. Therefore, despite the extremely high accuracy the com-putation cost (and not the memory requirements) prevents us from applying this heuristic to larger databases. Finding an efficient method for group solution cost computation would yield in a sig-nificant runtime improvement. We therefore consider improving the efficiency of group selection co st computation as future work. While computing the exact solution is only possible for small databases the greedy algorithm a nd the approximation allow com-putation of update sequences and di stance for large databases. In order to assess the accuracy of GREEDY-TRANSIT and TRAN-SIT-APPROX we used a database of 10 attributes and 100 tuples and modified it using arbitrary update sequences of length be-tween 5 and 50. We then computed the update distance between the original and the resulting database using both algorithms. The results are shown in Figure 6. The shown values are averaged over ten runs. The dark area above the lower bound highlights the location of the exact solution between the lower bound and the length of the sequences that gene rated the databases. The greedy approach and the approximation are both surprisingly accurate for short update sequences. For longer update sequences the accuracy decreases but remains in reasonable bounds. Overall, the GREEDY-TRANSIT outperforms TRANSIT-APPROX in accu-racy. 
Figure 6: Comparing the a ccuracy of GREEDY-TRANSIT The accuracy of GREEDY-TRANSIT decreases, as the number of conflicts increases. We used a s econd database of 20 attributes and about 800 tuples and modified it using update sequences as described above. Figure 7 shows the resulting greedy update dis-tances when limiting modification operations within the se-quences to such whose patterns sel ect at least 10, 20, or 50 tuples each. The number of conflicts in troduced by theses sequences increases as the selectivity of th e patterns increases. This resulted in database pairs with over 3, 500 conflicts between them. The decrease in accuracy shown by GREEDY-TRANSIT is even lar-ger when using TRANSIT-APPROX. Execution time of the branch a nd bound approaches lies between a few milliseconds and up to four minutes for the databases we considered. The execution time increases dramatically for larger databases. GREEDY-TRANSIT ha d execution times between two seconds for short sequences and one minute for long sequences in the experiments of Figure 7. We also applied this algorithm on the protein structure databases Co lumba and OpenMMS, having over 20,000 tuples each and nearly 100,000 conflicts between them. The resulting update sequence contained over 10,000 update op-erations and computation took more than 24 hours. The result shows a clear disadvantage of the greedy approach as over 97% of the operations in the sequences so lved less than 10 conflicts. Still, by interpreting the operations at the start of the sequence, we discovered update operations that describe the commonly known differences between the databases like usage of different value representations or vocabularies in some of the attributes. 
Figure 7: Accuracy of GREEDY-TRANSIT for update se-We described several algorithms for determining update se-quences of SQL-like update operations that transform one data-base into another one. If conflicts between two databases are due to systematic manipulation, th e operations within update se-quences are valuable to domain experts interested in solving the conflicts. Minimal sequences may al so be used as retrospective documentation of manipulations performed on a given database. Due to the complexity of the problem, exact solutions are only feasible for very small databases. Therefore, we investigated sev-eral heuristics that, in our expe riments, found near-optimal solu-tions and are applicable also to larger data sets. In our current research work we investigate several directions. A major challenge is to reduce th e memory requirements of our algorithms. For instance, instead of holding entire databases in main memory, one could represen t a database by its generating operations plus the hash key. Th is reduces memory consumption while increasing the execution time for duplicate checks. Another considerable cost factor is the necessary computation of closed patterns to find all valid modifica tions. However, deriving the set of closed patterns for a database from the set of closed patterns from its predecessor database usi ng some incremental approach could be highly advantageous sin ce both database vary only very little. We also consider several extensi ons to our approach. First, en-hancing the expressiveness of update operations, including modi-fications like SET A = f (A) as described in [11], would be very important; yet the cost of finding such functions is probably pro-hibitive. Second, assuming that tw o contradicting databases have been derived from a single ancestor database, it is natural to ask the following question (studied in biology under the term phy-logenetics): Given two database r 1 and r 2 , compute the database r whose update distance to r 1 plus its update distance to r mal. Thus, we could actually reconstruct the original database. [1] M. Arenas, L. Bertossi, J. Chomicki. Consistent Query An-[2] R. Agrawal and R. Srikant. Fast Algorithms for Mining As-[3] J. Bayardo, Jr. Efficiently mining long patterns from data-[4] H.M.Berman, J.Westbrook, Z. Fe ng, G. Gilliland, T.N. Bhat, [5] T.N. Bhat, et al. The PDB data uniformity project , Nucleic [6] H. Boutselakis, et al. E-MSD: the European Bioinformatics [7] P. Bohannon, W. Fan, M. Flaster, and R. Rastogi. A Cost-[8] S. Chawathe, H. Garcia-Molina. Meaningful change detec-[9] G. Cong, A.K.H. Tung, X. Xu, F. Pan, and J. Yang. [10] P. Dadam, V.Y. Lum, H.-D. Werner. Integration of Time [11] W. Fan, H. Lu, S.E. Madnick, and D. Cheung. Discovering [12] M.A. Hernandez, S.J. Stolfo. The merge/purge problem for [13] W. J. Labio and H. Garcia-Molina. Efficient Snapshot Dif-[14] V.I. Levenshtein. Binary codes capable of correcting inser-[15] A.E. Monge, C.P. Elkan. An efficient domain-independent [16] H. M X ller, U. Leser, and J.-C. Freytag. Mining for Patterns [17] H. M X ller, J.-C. Freytag, and U. Leser. On the Distance of [18] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. Discover-[19] K. Rother, H. M X ller, S. Trissl, I. Koch, T. Steinke, R. [20] G. Vossen. Data Models, Database Languages and Data-[21] J. Wang, J. Han, and J. Pei. CLOSET+: searching for the [22] W. Winkler. The state of record linkage and current re-[23] M.J. Zaki and C.-J. Hsiao. CHARM: An efficient algorithm 
