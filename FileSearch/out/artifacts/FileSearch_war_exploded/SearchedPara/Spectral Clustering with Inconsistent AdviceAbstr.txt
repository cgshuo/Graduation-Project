 Tom Coleman colemant@csse.unimelb.edu.au James Saunderson j.saunderson@ugrad.unimelb.edu.au Anthony Wirth awirth@csse.unimelb.edu.au The University of Melbourne, Victoria 3010 Australia Clustering is an exploratory data analysis problem which asks us to form groups of related objects. Al-though humans have a good intuition for clustering in two dimensions, if the data is in a higher dimensional space, it can be hard to visualise. In this paper, we will focus on the problem of clustering data into two clusters, subject to a balance criterion and advice . 1.1. Clustering with advice It is sensible for clustering algorithms to be able to incorporate must-link and cannot-link advice 1 , as it is known in the constrained clustering community. For example, in biology, when experimentally clustering proteins (or genes etc), it is often practical to test as-sociations of individual pairs. However, there is no guarantee that the advice we generate in this way will be correct. Additionally, it is well known that hu-man and biological  X  X xperiments X  are often subject to noise. If we have enough noisy advice, that advice will be inconsistent X  X hat is, there is no way to cluster the data which agrees with all the advice.
 In that case, the objective naturally becomes to re-spect as much advice as possible. In fact, if we ignore all the data apart from the advice, we have the the 2-correlation clustering problem ( 2CC ), known to be NP-hard (Bansal et al., 2004). In this context, we can think of the advice graph  X  X hich has an edge for each piece of advice, labelled with a + for a must-link, and a  X  otherwise. 1.2. Balanced clustering A natural problem to solve when clustering in general is the normalised cut (Shi &amp; Malik, 2000). Normalised cut ( Ncut ) asks us to find a cut which minimises inter-cluster affinity whilst maximising intra-cluster affinity in a sensible way. In the two cluster case, this is to minimise the quantity measures the affinity across the cut and vol( S ) is the total degree (out-affinity) of all the nodes within S . Our aim for the paper is to attempt to optimise both the Ncut and 2CC criteria simultaneously. To do so we will need to relax the problems X  X e cannot hope to solve them combinatorially. 1.3. Relaxed versions of the clustering Traditionally, spectral approaches were used for the Ncut problem, as they led to fast algorithms. Re-cently there has been a trend towards tighter, semidef-inite programming (SDP)-based, relaxations. We will demonstrate how to alter the basic spectral clustering algorithm, and the SDP techniques, to integrate and deal sensibly with inconsistent advice.
 Suppose that the advice is consistent (it is simple to check this fact). Once we know that this is the case, it is sensible to constrain the space of the solutions that we explore only to contain clusterings that are consistent with this advice. 1.4. Existing approaches Spectral Clustering Spectral Clustering appeared first in the literature in the 1970s. Much of the recent popularity of the technique was instigated by the con-nection to Ncut as shown by Shi and Malik (2000). Advice Advice (instance-level constraints) for clus-tering problems was introduced to the machine learning community in the work of Wagstaff and Cardie (2000) who developed a variant of the classic k -means algorithms to incorporate advice.
 Kamvar, Klein and Manning (2003) integrated advice into the spectral formulation by directly changing en-tries of the Laplacian matrix. Xing et al. (2003) im-prove on this idea by essentially changing the Lapla-cian in a more consistent way. They do this by finding a metric that best agrees with the advice. These meth-ods do not directly exploit the nature of the spectral algorithm.
 SDP relaxations for N-cut Xing and Jor-dan (2003) outline a SDP formulation for the Ncut problem for multiple clusters and highlight the con-nection to spectral clustering. De Bie and Cristian-ini (2006) provide an SDP which is easier to deal with, and demonstrate that the subspace trick can also be used to introduce advice to this problem.
 The subspace trick De Bie, Suykens and De Moor (2004) outline a subspace trick to integrate advice into spectral clustering by constraining a so-lution to be within the subspace of solutions which agree with the advice. This approach was also pre-viously mentioned in the work of Yu and Shi (2001). This technique leaves the spectral algorithm essentially unchanged; it now just searches for eigenvectors in a different subspace. However it is not necessarily ap-parent from their work how to extend this technique to inconsistent advice. This is the key issue addressed in this paper. 1.5. Addressing Inconsistency So how can we apply the subspace trick when the ad-vice we have is no longer consistent? As a first approach ( Method One ), we could sim-ply try to solve 2CC defined by the advice, and reject any advice that this solution fails to respect. A good solution to 2CC will ensure that we minimise the num-ber of such edges that we will have to ignore. Then the advice that remains will be consistent, and we can then use the subspace trick. Or indeed, we could use any other constraint-based clustering algorithm in this way.
 However, this idea has some problems. A toy example of inconsistent advice in Figure 1 shows that deleting any one of the three edges will result in an optimal so-lution to 2CC . However, one specific cut (namely sep-arating node 3 from nodes 1 and 2) has a much better Ncut cost. So, in forcing a particular optimal solution to 2CC , we are constraining our Ncut solution too much. A second approach ( Method Two ) that solves this problem is to calculate the cost of an approxi-mately optimal solution to 2CC . Rather than force our Ncut solution to be consistent with this 2CC so-lution, instead we simply require that our Ncut solu-tion has the same correlation clustering cost. This ap-proach will give the Ncut side of our algorithm some room to move, avoiding situations like Figure 1. This technique will be outlined in section 4.1.
 A third approach ( Method Three ) is to allow the al-gorithm to differ from the optimum correlation cluster-ing cost, but only by some a given factor. So now the Ncut side of the problem has some breathing space in which to find a good solution, whilst we are still forcing a very good solution to 2CC . This approach is developed in section 4.2. 2.1. Normalised Cut To define the Ncut problem, we begin with an edge-weighted affinity graph with associated affinity ma-trix A . Distant edges may have zero affinity X  X e can represent this by deleting the connecting edge, which will speed up the computation.
 We represent a 2-clustering by a vector v , where each coordinate represents a datapoint, and is either +1 or  X  1 depending on cluster assignment. The Ncut value becomes: where d is the vector of vertex degrees, and L ( X ) is the Laplacian of matrix X . Recall that if e is the vector consisting of all ones and diag( x ) is the matrix with vector x on the main diagonal and zeros elsewhere, then L ( X ) = diag( Xe )  X  X . 2.2. Spectral Clustering In the spectral relaxation, instead of assigning  X  1 to each vertex, we assign a real number v . If v = ( v 1 , . . . , v n ) then Ncut relaxes to P1. Spectral clustering min where D = diag( d ). This relaxation is correct because v T L ( A ) v is invariant under translations of v so we can add the constraint d T v = 0 without changing the op-timum cost. With this constraint, the denominator of (1) can be simplified as in P1 .
 It turns out that v = D 1 2 u is an optimum solution of P1 , where u is the eigenvector of D  X  1 2 L ( A ) D  X  corresponding to the smallest non-zero eigenvalue. 2.3. The SDP formulation In the two cluster case, De Bie and Cristianini (2006) devised an efficient relaxation of Ncut to a semidef-inite program. In this case, instead of assigning  X  1 to the vertices we assign vectors v i of some com-mon length. If X is the Gram matrix of these vectors (i.e. X ij = v T i v j ) then the relaxation is P2. De Bie and Cristianini SDP min s.t.
 where A  X  B = trace( AB ) for matrices of appropriate dimension. Here the free variable q is the common (squared) length of the v i and (2) is a scaling con-straint corresponding to the denominator of the Ncut objective function (1).
 Importantly, given any X 0 we can find v 1 , . . . , v n such that X ij = v T i v j ; we can thus convert a solution to P2 to an assignment of vectors to the vertices of the graph.
 Spectral clustering can be recovered from P2 by re-moving the constraints of (3) (see Goemans (1997)). 2.4. The  X  X ubspace trick X  The subspace trick of De Bie, Suykens and De Moor (2004) gives a method for incorporating con-sistent advice into spectral and SDP relaxations of Ncut . As an example, consider spectral clustering and suppose we have two  X  X locks X  of independent ad-vice. The first that two vertices, say v 1 and v 2 , should be in the same cluster and both should be in a differ-ent cluster to v 3 , the second that vertices v 4 and v 5 should be in the same cluster. Then it makes sense to constrain the solution vector v so that v 1 = v 2 and v 4 = v 5 guaranteeing that these pairs of vertices end up in the same cluster after rounding. It also makes sense to constrain v so that v 3 =  X  v 2 =  X  v 1 . This can be done by assuming v has the form where u  X  R n  X  3 . The identity matrix corresponds to vertices for which we have no advice and so should not constrain. Given an advice graph, in the form of + (must-link) or  X  (cannot-link) edges between datapoints, the corre-lation clustering problem asks us to cluster the data-points so that the number of pieces of advice (i.e. edge labels) that are disobeyed is minimised. 3.1. 2CC  X  the combinatorial problem In general, unlike the affinity graph, the advice graph is not connected. So we can solve correlation clustering independently on each connected component. We call the vertices in a connected component an advice block . We assume without loss of generality that the order on the vertices ensures that the vertices within each block are consecutive. Here we will deal with the problem of solving 2CC for a single advice block B with m vertices. In later sections, we will consider multiple advice blocks.
 If e is an edge within B let w e  X   X  1 correspond to the the sign of e . As for Ncut , we assign v i =  X  1 to each vertex depending on the cluster in which we place that vertex. For convenience, let E ij be the matrix with a 1 in the ( i, j ) entry and zeros everywhere else. Our immediate aim is to find, in terms of v , a simple expression for the number of constraints violated by the labelling.
 Consider a single edge e = { i, j } of B with label w e . Define and note that M e 0 because its eigenvalues are 0 and 2. Now So if we define M B = v T M B v = 4  X  (# pieces of advice violated by v ) . Thus 2CC is essentially Note that a clustering that satisfies all the advice in a block will have cost zero. Also observe that v T v = m is a constant so we could replace the objective function with ( v T M B v ) / ( v T v ) without changing the optimum vector. 3.2. Relaxations of 2CC Recall that our overall aim is to constrain any algo-rithm we have for (approximately) solving Ncut to produce clusterings which are, in terms of 2CC cost, not much worse than the optimum.
 Since we cannot hope to solve (8) exactly, we will in-stead solve a relaxed version of it. In this paper we consider two relaxations which arise in much the same way as the spectral and SDP relaxations of Ncut . P3. Spectral relaxation of correlation clustering min Observe that the solution of P3 is given by any non-zero vector in the  X  min -eigenspace of M B . P4. SDP relaxation of correlation clustering min s.t.  X  i  X  [ m ] X ii = 1 (9) For either relaxation, if the advice is consistent, the relaxation produces a solution of the same cost (zero) as the optimal solution to the combinatorial problem (8). This is because any solution of the original prob-lem is a feasible point of the relaxed problem, and the relaxed problem has non-negative cost as M B 0. In this section, we give the details of Method Two and Method Three , introduced in Section 1.5, for both the spectral and SDP relaxations of Ncut . Throughout, let B 1 , . . . , B p be the advice blocks of the advice graph. Let v B denote the projection of v onto the coordinates involved in advice block B . Along sim-ilar lines, if u B is a vector of length |B|  X  n associated with the advice block B , define f u B to be the length n vector that agrees with u B in the appropriate coordi-nates and has zeros elsewhere. For a |B|  X  |B| matrix M
B we also define 4.1. Combining 2CC and Ncut: Method Two Let opt j denote the optimum cost of the SDP relax-ation of 2CC ( P4 ) for block j . For the SDP relax-ation, we can add the constraint that for each advice block, the 2CC cost of point X is at most q opt j . (The scaling by q is necessary because in P4 the vari-ables satisfy X ii = 1 whereas in P2 the variables sat-isfy X ii = q .) This forces the new SDP ( P5 ) only to consider points of minimum SDP-relaxed 2CC cost. P5. Method Two (SDP version) min s.t.
 In the spectral case, the analogous thing to do would be to add the following constraints to the spectral re-laxation of Ncut .  X  j  X  [ p ] But doing so would mean the problem would no longer be an eigenvalue problem X  X n fact it would be an SDP X  X hich would undermine the main strength of spectral clustering, its speed.
 Luckily, the condition in (11) is equiva-lent to the condition that each v B the  X  min -eigenspace of M B P6. Method Two (spectral version) min s.t. d T v = 0 (12) The constraints (12) and (13) are forcing v to be in some linear subspace of R n . So the problem can then be solved using the subspace trick. Details of how to do this are in Appendix A. 4.2. Combining 2CC and Ncut: Method Three The main drawback of Method Two is that it does not give the algorithm much freedom to balance the trade-off between the 2CC and the Ncut problem. If the advice is quite inconsistent, then forcing the algo-rithm to follow solutions of a relaxation of 2CC too closely will result in poor performance.
 Above, we forced the algorithm to produce a (relax-ation of) a clustering that had cost at most the mini-mum cost of the appropriate relaxation of 2CC . Now we introduce a parameter f  X  1 which tells us the fac-tor by which we are willing to exceed the 2CC cost. This is straightforward to introduce to the SDP for-mulation. We simply replace the constraints (11) of P5 with In the spectral formulation, the constraints we actually want to add are  X  j  X  [ p ] but, again we cannot add these and still have an eigen-value problem. Unfortunately in this case we cannot get a constraint equivalent to (15) by the subspace trick. So, in the interests of producing a practical al-gorithm, we approximate (15) by  X  j  X  [ p ] v B where the (  X  f  X  min )-eigenspace of M B all eigenvectors of M B If v satisfies (16) then it satisfies (15), but the converse does not necessarily hold.
 Replacing the constraints in (13) of P6 with the con-straints in (16) gives our final spectral algorithm for clustering with inconsistent advice. It can again be solved with the subspace trick, using the techniques outlined in Appendix A, because all the constraints simply force v to be in some linear subspace of R n . 5.1. Experiment Setup In order to test the performance of the algorithms on real world datasets, we used six of the UCI repository datasets (Asuncion &amp; Newman, 2007). All datasets are multi-dimensional binary classification problems. Both datasets were stripped of incomplete records, and in one case (the Spambase dataset), sampled down to 500 datapoints. In each case, the two clusters were of different sizes. This contributed to the mediocre performance of the pure spectral algorithm. This gives us reason to believe that adding advice will help the situation.
 For reasons of speed, our experiments primarily use the spectral version of each of the algorithms. Re-laxed solutions are rounded to clusterings by cutting at zero. This ensures that advice respected in the re-laxed solution is respected in the final clustering. Advice We generated two different  X  X ypes X  of syn-thetic advice for these problems to get a sense of how the algorithms perform. The first we call Dense  X  X ere we are generating around n pieces of advice. We are generating that advice in a dense fashion X  X e concen-trate all advice within 5 separate groups of 20 data-points. This simulates a few sets of experiments done on some small subset of the total dataspace. Each piece of advice agrees with the actual classification in-dependently with some probability p .
 The second type of advice is the Complete case X  here we are simulating pairwise comparisons that are relatively cheap, but quite noisy. So we generate a piece of advice for each pair of datapoints, and thus our advice graph is complete. 2CC Algorithms In order to test Method One , we need to solve 2CC on each advice block. In the Dense case, we use a tight, strongly performing SDP relaxation (Agarwal et al., 2005). In the complete case, we use the simple 3-approximation algorithm of Bansal, Blum and Chalwa (2004) with a final local search step (see also our other paper (Coleman et al., 2008)).
 For each advice type on each dataset, we ran spectral clustering with no advice (as a baseline), Method One (as a second baseline), and then spectral clus-tering with every different meaningful f value from 1 upwards. That is, every increment in f that added one additional eigenvector to a single block, until all eigenvectors were added (which is exactly the same as the no advice case).
 5.2. Results Dense advice Figure 2 displays the results of the Dense advice problem on the Heart Disease dataset with p = 0 . 75. We can see that the advice here is sufficiently inconsistent that algorithms which follow it closely (i.e. Method One and Method Two ) perform far worse than the algorithm that ignores it completely (that is, spectral alone). But we can see that by increasing f and striking a balance between ignoring advice and respecting it too strongly, we can achieve results that outperform either extreme. We also note that two other datasets, Congressional Voting Records and Australian , perform simi-larly.
 Figure 3 shows the results of running very similar ad-vice (again p = 0 . 75) on the Spambase dataset. Here we can see that algorithms that strictly follow the ad-vice outperform algorithms that ignore it, quite sig-nificantly. It is perhaps unsurprising then that when we allow the algorithm more and more freedom to ig-nore the advice we move toward the baseline no advice score. This highlights the fact that these algorithms are not always of use X  X here needs to be enough inac-curacy in the advice that attempting to follow it is not a great idea.
 However, if we lower p to be 0 . 65, the situation changes, and we get a scenario as demonstrated by Figure 4. Here as for the Heart Disease case, using only the 2CC solution is worse than using no advice at all, and for a large range of f values, the compromise of using some advice is better than either extreme. Here the Haberman dataset performs similarly. The difference in this case is that for high f values, very poor performance is exhibited. We will discuss this in the next section.
 Finally, we consider the Hepatitis dataset (Figure 5). Here we see new behaviour, as our algorithms only begin to perform well for high f values.
 Complete Advice Figures 6 and 7 show the results of the experiments on the two datasets with Com-plete advice. We first notice that in order to get meaningful experiments, we needed to set p extra-ordinarily low X  X ll the way down to p = 0 . 53. If p is much higher than this, advice is so complete that any incorrect edges will be vastly overshadowed by cor-rect ones, and simply solving 2CC on the instance will give 100% accuracy.
 However, with p = 0 . 53 and the problem interesting, we can see that things are similar to the Dense case. Again, when f is low, we start at the 2CC -baseline, and as f increases we move towards and above the no-advice baseline. An interesting point is that the 2CC baseline is around 0 . 5 in both cases. Note that this is an extremely low score X  X he advice alone is useless for solving the problem, yet it is still a useful addendum for the spectral method.
 As we saw in the Dense case, one interesting difference between the two datasets is the way the performance drops off as f increases. For Heart Disease , the performance seems to asymptote to the no-advice case as we increase f (as we would expect). However, in both cases for the Spambase data, there is a huge dropoff in performance for high end f values. We have no explanation currently for this phenomenon. We have presented a new algorithm that uses incon-sistent advice in spectral clustering. This paper is the first to do so. Initial experiments indicate that in many situations our methods are successful, however further theoretical and experimental work is needed. For example, given a clustering problem with incon-sistent advice, how do we know when to use Method Three rather than Method Two ? And if we are to use Method Three , how do we decide which value of f to choose? This paper was intended as a largely theoretical work X  X xperiments were performed to give prelimi-nary evidence that the techniques work. Certainly a more thorough comparison to existing work is needed X  X  technique similar to Method One could be used in order to compare our algorithms to other approaches that can only deal with consistent con-straints. These will be tested in the full version of the paper.
 Additionally, in this paper we focused on clustering into two clusters. This is for two reasons. First, there is no obvious way to express cannot-link advice when we have more than two clusters X  X he approach used here does not generalise nicely. Furthermore, we do not know of an SDP relaxation of the problem which fits into the framework of this paper for the case of more than two clusters. Future work will try to ad-dress these problems.
 Thanks to Ian Davidson for encouraging us to pursue this problem, and to the anonymous reviewers of the draft version. This work was supported by the Aus-tralian Research Council through Discovery Project Grant DP0663979.
 Agarwal, A., Charikar, M., Makarychev, K., &amp;
Makarychev, Y. (2005). O ( algorithms for min UnCut , min 2CNF deletion , and directed cut problems. Proceedings of the
Thirty-Seventh Annual ACM Symposium on Theory of Computing , 573 X 581.
 Asuncion, A., &amp; Newman, D. (2007). UCI machine learning repository.
 Bansal, N., Blum, A., &amp; Chawla, S. (2004). Correla-tion clustering. Machine Learning , 56 , 89 X 113. Coleman, T., Saunderson, J., &amp; Wirth, A. (2008). A local-search 2-approximation for 2-correlation clus-tering. Submitted.
 De Bie, T., &amp; Cristianini, N. (2006). Fast SDP Relax-ations of Graph Cut Clustering, Transduction, and
Other Combinatorial Problems. The Journal of Ma-chine Learning Research , 7 , 1409 X 1436.
 De Bie, T., Suykens, J., &amp; De Moor, B. (2004). Learn-ing from general label constraints. Joint IAPR In-ternational Workshops on Structural, Syntactic, and Statistical Pattern Recognition , 671 X 679.
 Goemans, M. (1997). Semidefinite programming in combinatorial optimization. Mathematical Program-ming , 79 , 143 X 161.
 Kamvar, S., Klein, D., &amp; Manning, C. (2003). Spec-tral learning. Proceedings of the Seventeenth Inter-national Joint Conference on Artificial Intelligence (IJCAI-2003) , 561 X 566.
 Shi, J., &amp; Malik, J. (2000). Normalized cuts and im-age segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 22 , 888 X 905. Wagstaff, K., &amp; Cardie, C. (2000). Clustering with instance-level constraints. Proceedings of the Seven-teenth International Conference on Machine Learn-ing , 1103 , 1110.
 Xing, E., &amp; Jordan, M. (2003). On Semidefinite
Relaxation for Normalized K-cut and Connections to Spectral Clustering . Computer Science Division, University of California.
 Xing, E., Ng, A., Jordan, M., &amp; Russell, S. (2003).
Distance metric learning, with application to clus-tering with side-information. Advances in Neural Information Processing Systems , 15 , 505 X 512. Yu, S., &amp; Shi, J. (2001). Grouping with Bias . Carnegie Mellon University, the Robotics Institute.
 In this section we explain how to implement the spec-tral version of Method Two and Method Three using the subspace trick.
 Let W B (  X  f  X  min )-eigenspace of M B respectively denote the nullspace and range space of a matrix X . Then the problem can be written as follows: P7. Spectral clustering with inconsistent advice min s.t. Let where the dimension of I is the number of vertices not involved in any advice. Then we can replace the constraints (17) and (18) with Suppose Y is a matrix satisfying R ( Y ) = C and Y T DY = I . Then if we let v = Y u it is clear that v  X  R ( Y ) which is what we want. So P7 becomes A solution of (19) is given by taking u to be an eigenvector corresponding to the smallest eigenvalue of Y T L ( A ) Y . The solution to the original problem is then v = Y u .
 Elementary linear algebra shows Y generated thus is satisfactory: 1. Let N be a matrix whose columns are an or-2. Let R be a matrix whose columns are an orthonor-
