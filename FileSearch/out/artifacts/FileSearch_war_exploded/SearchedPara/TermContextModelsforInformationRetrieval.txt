 At their heart, most if not all information retriev al mo dels utilize some form of term frequency . The notion is that the more often a query term occurs in a documen t, the more likely it is that documen t meets an information need. We examine an alternativ e. We prop ose a mo del whic h assesses the presence of a term in a documen t not by looking at the actual occurrence of that term, but by a set of non-indep enden t supp orting terms, i.e. context . This yields a weigh ting for terms in documen ts whic h is di eren t from and complemen tary to tf -based metho ds, and is bene cial for retriev al.
 H.3.3 [ Information Searc h and Retriev al ]: Retriev al mo dels Algorithms, Exp erimen tation Maxim um entrop y, conditional random elds, con text-based retriev al
At the core of almost every mo dern ad hoc retriev al algo-rithm is a reliance both on local prop erties (statistics from within a documen t) as well as with global prop erties (statis-tics from across a collection). For example, Okapi BM25 [13] uses term frequency ( tf ) and documen t length ( dl ) as local prop erties, and inverse documen t frequency ( idf ) and aver-age documen t length as global prop erties. Ponte's language mo deling [11] uses tf and dl for local maxim um likeliho od estimation, and com bines these with a global risk factor a measure of how far the curren t documen t's tf is from the collection-wide normalized mean. In more recen t language mo deling work [17], the local maxim um likeliho od estimate is com bined with a global \fallbac k" mo del, or p ( t j C ), the probabilit y of the term app earing in the entire collection.
The motiv ation for idf , ^ R , and p ( t j C ), all global prop-erties, comes from the understanding that local prop erties alone are insucien t. No matter the algorithmic framew ork, some com bination of local and global prop erties are required for prop er retriev al. It is with this understanding that we have dev elop ed another useful global prop erty: term con-text . Unlik e the aforemen tioned global prop erties, whic h are all based on coun ts across documen ts in whic h a term is presen t, we create a mo del that predicts whether or not the term should be presen t in a documen t whether or not it is , based on global con textual usage patterns. It is a mea-sure of how good a t a term is for a particular documen t, in uenced by how that term is used in other documen ts in a collection.

For example, supp ose a user issues the query \fuel". Fur-thermore, supp ose there are two same length, same \fuel" term frequency documen ts in the collection. However, one of the documen ts talks about energy and gas and coal, whereas the other documen t talks about how various interest rates and tax cuts fuel the econom y. Ceteris paribus, tf -based metho ds have no reason to prefer one documen t over an-other. However, our mo dels may learn that the documen t on energy con tains a more con textually standard usage of fuel in a particular collection than the documen t on the econom y, and thus will be rank ed higher when utilizing term con text mo dels.

Lest this previous example mislead the reader, this is more than just word sense disam biguation. Take for example the query term \emacs". There is really only one lexical sense of this word. And yet documen ts con taining emacs migh t encompass a whole range of sub jects, everything from keystrok e shortcuts to programming environmen ts to email integration. If there is a particular asp ect or set of asp ects that dominate in a collection, those will be learned by con-text mo deling and documen ts con taining those asp ects will be given a boost in the rankings.

We note at the outset that while we are computing global con textual statistics, what we are doing is neither query ex-pansion (via global con text analysis) nor pseudo-relev ance feedbac k (via local con text analysis) [16]. We do not ac-tually add the terms gas or coal to the fuel query . We do not mo dify weigh ts on fuel based on the top n documen ts from an initial retriev al run. This distinction is subtle but imp ortan t, and will be explained further in later sections.
This pap er is structured as follo ws. In section 2 we dis-cuss related work. Section 3 con tains a description of our mo del, while section 4 sho ws how we apply the mo del to ad hoc retriev al. The exp erimen tal set up and evaluation is done in section 5. Future work is prop osed in section 6 and conclusions are dra wn section 7.
The parametric form that we use for the term con text mo del is the familiar log-linear, also kno wn as exp onen tial or maxim um entrop y mo del, and is related to conditional random elds [6]. CRFs can be though of as a structured form of maxim um entrop y. Con versely , conditionally trained maxim um entrop y can be though t of as an unstructured, or \0 th order", CRF [8]. We prefer this latter persp ectiv e, and in section 3 will describ e our mo del in these terms.
Maxim um entrop y and related mo dels have long been used in information retriev al applications, from early work by [5] to more recen t work by [4, 10] and even [9]. However, these mo dels only consider coun ts and dep endencies amongst the terms in the query , and do not mak e use of the larger sta-tistical language prop erties of a term.

Other maxim um entrop y and related mo dels do mak e use of statistical language but have not been applied to the task of ad hoc retriev al [1, 2, 3]. The mo del with the most sim-ilar form to ours is [14], though again the application of that mo del is quite di eren t. Another closely related work is [7]. While the application is music information retriev al, notes instead of words, the idea of situating notes in their prop er con text is analogous to situating terms in their con-text. Finally , [8] uses maxim um entrop y to learn man ually-annotated text lab els for documen ts (via \so cial tagging"), using the documen t as the con text for the lab el. Determin-ing whether a particular term is a good lab el for a documen t is similar to our approac h. The di erence is that the lab els we are learning are not external to the con ten t of the docu-men ts. This will become clearer in the next section.
Our goal is to assess the presence of a term in a documen t based not on the actual observe d occurr ence of that term , but on the evidenc e of a set of supp orting terms, or context, from that document .

One possible solution is to use term co-o ccurrence. If a \supp ort" term co-o ccurs frequen tly with the \target" (query) term, one may be able to use that supp ort term as a substitute for the target. However, in most cases no one term is a perfect substitute for another, so multiple terms are needed. This presen ts a problem: When multiple terms are added to a query , precision at low recall can be adv ersely a ected [16]. Furthermore, supp ort terms are not indep en-den t. Using straigh tforw ard co-o ccurrence mak es the pair-wise indep endence assumption. This is only sometimes, and clearly not necessarily , true. We believ e that by mo deling the statistical dep endencies between supp ort terms we can increase precision at low recall for some queries without ad-versely a ecting precision for other queries. Therefore, we turn to a framew ork that allo ws us to selectiv ely mo del the interactions between the individual supp ort terms.
Supp ose we have a lexicon of k terms extracted from some corpus of interest. We create for eac h term i in the lexi-con two binary random variables, t x i for the observ ed value of that term, and t y i for the unobserv ed (hidden, inferred) value. When given a documen t d from our corpus, the ob-serv ed variables f t x 1 : : : t x k g are instan tiated by assigning a value of \0" when the observ ed frequency of the term is zero in that documen t, and a value of \1" when the observ ed fre-quency is greater than zero. Now, for eac h term's hidden binary variable t y i we de ne the context H y i as the set of observ able variables for all terms in the vocabulary other than the i th term itself:
Terms in H y i are the only ones that can be examined when we are making the prediction regarding t y i . In other words, we assume that the probabilit y of term t y i occurring in d is completely determined by H y i in our mo del.
This also means that eac h term t y i is conditionally inde-penden t of all other terms t y j 6 = i , given f t x 1 : : : t prop ose a \bag of term-con text-mo dels" approac h. How-ever, it is imp ortan t to stress that we do not want to assume indep endence among the conditioning variables; we still al-low arbitrary dep endencies within the H y i con text.
A well-kno wn adv antage of the random eld framew ork is that it allo ws arbitrary dep endencies between the target t and its con text H y i . Features may be simple or complex, based on everything from term frequencies to the locations of commas. However, for the sak e of simplicit y we will de-liberately restrict allo wed dep endencies to binary questions of the form: \ does term t x j occur in this document? ".
We will also allo w generalizations where a question is ask ed about some subset S of the terms in H y i . The an-swer to a question of this form will be called the feature function f S , and S will be referred to as the supp ort of f . For a given supp ort S 2 H y i , the feature function f S de ned as the conjunction of answ ers about the individual terms in t x j 2 S :
De ned in this manner, our feature functions are alw ays boolean, and equal to 1 if all the terms de ned by S occur in the documen t. A feature function alw ays includes the target term t y i . This is not a fallacy , since t y i will nev er actually be considered a part of its own con text. Presence of t y the feature serv es only to tie the occurrences of terms in S to the term t y i . Figure 1 is an example of a term con text mo del for a single term, t y i .
There are a num ber of di eren t forms we could choose for computing the probabilities P ( t y i j H y i ), but it turns out that for random elds there is a natural form ulation of the dis-tribution that is given by the maxim um-en trop y framew ork. Supp ose we are given a set F of feature functions that de ne the structure of the eld. The maxim um-en trop y principle states that we should select the parametric form that is: (i) consisten t with the structure imp osed by F and (ii) mak es the least amoun t of unwarran ted assumptions | that is the most uniform of all distributions consisten t with F . The family of functions that satis es these two criteria is the exp onen tial (or log-linear) family , expressed as: Figure 1: Graphical represen tation of a Term Context
In equation (3), the set of scalars = f f : f 2 Fg are the Lagrange multipliers for the set of structural con-strain ts F . Z i is the normalization constan t that ensures that our distribution sums to unit y over all possible values
For a general random eld, the partition function Z i is exceptionally hard to compute since it involves summation over all possible con gurations of the system, whic h is ex-ponen tial. In our case, our assumption of no dep endencies between hidden variables f t y 1 : : : t y k g mak es computation of the partition function extremely simple: Z i only needs to be computed for t y i = 0 and t y i = 1.
The ultimate goal of this pro ject is to dev elop a probabil-ity distribution ^ P ( t y i j H y i ) that will accurately predict the presence of term t y i in a documen t. There exist a num-ber of di eren t measures that could indicate the qualit y of prediction. We choose one of the simplest | log-lik eliho od of the training data. Giv en a training set T of documen ts d , the log-lik eliho od is simply the average logarithm of the probabilit y of pro ducing term i in T :
If we examine the parametric form in equation (3), we note that there are two things on whic h the mo del dep ends. The rst and (in our opinion) foremost is the structure of the eld F , represen ted as a set of constrain ts or feature functions f 2F . These constrain ts represen t most signi -can t dep endencies between the variables of the eld. The second thing we learn is the set of weigh ts = f f g , one for eac h feature f 2F . We kno w that and F are intimately intert wined and we need to learn them sim ultaneously , but for the sak e of clarit y we split the discussion in two sec-tions. This section will describ e how we can incremen tally induce the structure F of the eld, starting with a very at, meaningless structure and generalize to more interesting re-lationships.

The eld induction pro cedure closely follo ws the algorithm describ ed in [3], the primary di erence being that we are dealing with a conditional eld, whereas Della Pietra et al use a join t mo del. We start with a eld that con tains only that term without any dep endencies: F 0 = f t y i g . We will incremen tally update the structure F by adding the features g that result in the greatest impro vemen t in the objectiv e function. Supp ose F k = f f S g is the curren t eld structure. Also assume that the corresp onding weigh ts k are optimized with resp ect to F k . We would like to add to F k a new feature g that will allo w us to further increase the likeliho od of the training data. In order to do that we rst need to form a set of candidate features G that could be added. We de ne G to be the set of all single term extensions of the curren t structure F :
In other words, we form new candidate features g tak-ing an existing feature f and attac hing a single observ able term t x j . Naturally , we do not include as candidates any features that are already mem bers of F . Now, follo wing the reasoning of Della Pietra, we would like to pick a candidate g 2G that will result in the maxim um impro vemen t in the objectiv e function.

First, let ~ E [ g ] denote the empiric al or target exp ected value of g , whic h is simply how often the feature actually occurs in the training data T :
Similarly , our estimate ^ P ( t y i j H y i ) gives rise to the pre-dicte d exp ectation ^ E [ g ] for the function g . Predicted ex-pected value is simply how often our mo del \thinks" that g should occur in the training set: Now, supp ose that previous log-lik eliho od based only on F k was L ^ , the new likeliho od of the training data would be:
As our feature functions are binary , the weigh t can be determined in closed form by di eren tiating the new log-likeliho od L ^ P + f g g with resp ect to and nding the root of the deriv ativ e:
Kno wing also allo ws us to compute the resulting im-pro vemen t, or gain , in log-lik eliho od in closed form:
In the previous section we describ ed how we can automati-cally induce the structure of a random eld by incremen tally adding the most promising candidate feature g 2 G . How-ever, since the features f 2 F are not indep enden t of eac h other, adding a new feature will a ect the balance of exist-ing features, and therefore the objectiv e function. We may further impro ve the objectiv e by re-optimizing the weigh ts.
Assume now that the structure F con tains all the desired features. We adjust the set of weigh ts so that the ob-jectiv e function L ^ P is maximized by computing the partial deriv ativ es of L ^ P with resp ect to eac h weigh t f 0 inten tion of driving these deriv ativ es to zero:
There is no closed-form solution for setting the weigh ts to their optimal values, so we utilize an iterativ e pro cedure, a variation of gradien t descen t:
Note that while ~ E [ f ] is computed only once for eac h fea-ture f , we have to re-compute the value ^ E [ f ] after every update. This mak es the learning pro cedure exp ensiv e. How-ever, learning is guaran teed to con verge to the global opti-mum; L ^ P is \ -con vex with resp ect to the weigh ts f .
We are nally ready to bring together the comp onen ts of the previous subsections into one algorithm for automatic induction of a con text mo del for term t y i : 1. Initialization 2. Weight Update 3. Feature Induction
While the general mo del creation algorithm is describ ed above, in practice we had to imp ose a few limitations due to the intense computational resources required. The rst lim-itation is on the candidate feature vocabulary f t x 1 : : : t Rather than using the entire vocabulary , we use the 500 terms with the highest documen t frequency , that also occur at least once with the target term t y i . This subset is of course di eren t for every t y i .

We stem and con ate terms using Porter [12], so that there is a single variable t x i (as well as a single variable t for all terms that share the same root, i.e. car/cars. We do not want \cars" to be a supp ort feature in our term con text mo del of \car", as this defeats the purp ose of learning a good general con text mo del. The second limitation is on iterations. In step (2b), we iterate 12 times, rather than until no noticeable change in likeliho od. In step (3e), we iterate 30 times, inducing exactly 30 features, for a total of 31 features including the &lt; null &gt; con text seed feature. There is no reason for the these num bers beyond intuition. Our feeling was that 30 felt like a good \handful" of supp ort features, not too man y so as to over t and not to few so as to miss the prop er con text. We certainly did not optimize these parameters, nor try other parameters. Future work can address this.

The nal limitation we imp ose is on the size of eac h fea-ture. Recall from equation (2) that a feature may include any num ber of observ able terms t x . While we have done some exploratory runs allo wing two and three supp ort terms, the retriev al results presen ted in section 5 were done using mo dels limited to features with a single supp ort term. Fig-ure 2 sho ws mo dels created for four di eren t terms, trained on the 131,000 LA Times documen ts in TREC volume 5.
For an insigh tful aside, in Figure 2 we sho w co-o ccurrence scores across from f feature weigh ts. While we do no want to give the impression that con text mo dels and cooccurrence are directly comparable, because cooccurrences assume pair-wise indep endence whereas con text mo dels use the entire set of terms, there is one interesting thing to note: The cooccurrence scores and f weigh ts do not yield the same relativ e ordering. Our approac h is taking into accoun t the dep endencies between supp ort features, and down-w eigh ts features that are already \co vered" by previous features.
For example, in the mo del for \hydro electric", the term \riv er" has the 2nd highest co-o ccurrence score, after \dammed". And yet it is 15th in terms of its f weigh t. This is because the terms \dammed" and \riv er" are themselv es not inde-penden t. When it comes to predicting hydro electric, occur-rences of \dammed" already cover most of the occurrences of \riv er". Therefore \riv er", while strongly asso ciated with \hydro electric" by itself, is not as imp ortan t within a con-text that already includes \dammed".

Another imp ortan t distinction between term-con text and co-o ccurrence is the use of negativ e features. We do not go into details here, but hin t at the usefulness of this in sec-tion 6. Further exploration on the relationship between term con text mo dels and co-o ccurrence is desirable, but beyond the scop e of this pap er.
Now that we have a framew ork for creating term con text mo dels we wish to use them for ad hoc retriev al. This is only one of man y possible uses for these mo dels, but have chosen this task to demonstrate their e ectiv eness. We be-gin at index time by creating con text mo dels for all terms in the collection vocabulary , indep enden t of any query . Once eac h term's con text mo del is learned, we iterate once more through the entire collection using the learned mo dels to as-sess the degree to whic h eac h documen t's con text supp orts the belief in eac h term (explained in section 4.1). The result of this calculation is a single value or context score for ev-ery term-do cumen t pair in the collection. These values are stored in inverted lists alongside tf values.

At retriev al time, this precomputed con text score is com-bined with a standard tf -based approac h, in our case Okapi BM25 (explained in section 4.2). The mixture between this tf -based score and the con text mo del score yields and over-all score for a query term in a documen t (explained in sec-tion 4.3). The follo wing sections con tain additional details.
At index time, after mo dels are trained, a per term-do cumen t con text score is is calculated using the entire set of 31 feature functions F from the con text mo del for term t i in documen t d :
Note that we do these calculations on the same collection on whic h we train the mo del. This is not a fallacy . This split is not necessary as it poses no more problem than \testing" an idf weigh t \trained" on the same corpus one is searc h-ing. Our mo dels are not directly intended for prediction of unseen values. Lik e idf , they are mean t to capture certain discriminating characteristics of the very corpus they will be used to searc h. Evaluation is done not on the mo del's classi cation accuracy , but on retriev al precision and recall.
Recall that equation (3) is estimated based on exp ected values. By now going through eac h documen t individually , we are determining how eac h of the documen ts in the collec-tion deviate from this exp ected value. There will be occur-rences of a term that are irregular, or \out of con text", as trained on the collection as a whole. Suc h occurrences will have a lower score than occurrences that are more con tex-tually standard. The mo del may assign a low con text score even if the tf of that term is high. There will also be other documen ts in whic h the tf for a term is low, but the con-text score in that documen t is high. In a manner somewhat unc hained from the actual tf of a query term, we are deter-mining whether that term is a good t for that documen t.
We use the Okapi BM25 scoring function [13] as our base-line. BM25 has consisten tly been one of the top scoring functions in TREC style evaluations, and if we can impro ve upon it then we will sho w value in our metho ds. The exact form (for term t i in documen t d ) is given by equation (13) below, where k 1 =2.0 and b =0.75. tf is the frequency of t in d , dl is the length of d , avgd l is the average documen t length across all documen ts in the collection, N is the total num ber of documen ts in the collection, and n is the num ber of documen ts in whic h the query term is found.
As men tioned in section 1, this score com bines both local features ( tf and dl ) with global features ( avgd l , N and n ) to come up with an overall weigh ting for a query term.
In this section we will bring together the ideas introduced in this pap er into a comprehensiv e whole. From section 4.1 we have a scoring function that increases as the con text in a documen t moves toward the conceptually standard usage across the collection. From section 4.2 we have a scoring function that increases as term frequency increases. What we need is a scoring function that increases both as term frequency and con textual cen tralit y increase.

This can be accomplished through data fusion techniques, wherein we use a linear com bination of BM25 and the term con text mo del as the score for a documen t. The idea is that the BM25 approac h, whic h is primarily tf -based, is going to give a somewhat di eren t ranking than the TCM approac h, whic h is con text-based. The \mistak es" that BM25 mak es are hop efully di eren t than TCM, and vice versa. When you fuse the scores the relev ant documen ts should percolate to the top, while the spurious matc hes should drop out.
Returning to the example on the query term \fuel" from section 1, a documen t with ve occurrences of fuel, but that talks about how interest rates fuel a housing bubble, is going to be lowered in the rankings. At the same time another documen t that only men tions fuel once, but does so in the con text of energy and gas, is going to be boosted in the rankings. A documen t with ve occurrences of fuel, that also talks about it in the con text of energy , should be rank ed fairly high.

While man y fusion techniques are available, we use a sim-ple linear com bination of the scores (MIX):
We understand that, at the momen t, this is not a for-mally justi ed metho d for com bining the two scores, as TCM ranges from 0.0 to 1.0 while BM25 is non-negativ e (greater than zero). How does a documen t with a BM25 score of 4.0 and a TCM score of 0.9 compare with another documen t's 4.3 BM25 score and 0.6 TCM score? While it is not yet clear, we are encouraged by the though t that, for any two documen ts with a similar BM25 score (i.e. similar tf and documen t length), di ering TCM scores pro vide a di eren-tiating factor. And vice versa.

Future work will address this com bination of TCM with other features, but what we sho w in this pap er is simply that there is value in the TCM approac h: It works. Just as various com binations tf and idf took years to dev elop, so will better metho ds for utilizing TCM be dev elop ed in the future. Furthermore, as detailed in the next section, we found that results are fairly robust with resp ect to a wide range of settings. This indicates that term context as an index-time globally-informed feature is a good measure of the relev ance of a documen t to a query term.
We use standard TREC datasets [15] for evaluation. Our dataset consists of the 131k documen ts from the LA Times in TREC volume 5 with 143 title-only queries from topics 301 to 450. Actually , there are 150 topics in this set, but sev en queries had no relev ant documen ts in the LA Times collection so are omitted from the evaluation. We presen t two mo dels: BM25 and MIX. For all the terms in the query , we sum the weigh ts over all terms t i in the query Q given by eac h of these metho ds. Documen ts are sorted in decreasing order by these scores.
Results are sho wn in gure 3, with the mixing param-eter set to 0.5. In general, MIX outp erforms BM25. Recall is sligh tly better, and average precision is about the same, but the highest gains are in the most imp ortan t area: preci-sion at low recall. These gains are 5-6% and are statistically signi can t. We feel these results justify the approac h. How-ever, in the next section we will pro vide a more in-depth analysis and see that the results are even better than they rst app ear.

These results are also quite robust with resp ect to the mixing parameter . We tested mixture weigh ts ranging in step wise 0.1 interv als from 0.0 to 1.0. Statistically signi -can t impro vemen ts in roughly the same amoun ts and areas were obtained using ranging from 0.3 to 0.8, with the best mixture around 0.6 or 0.7. So the results we sho w are cer-tainly not the best ones possible, but rather than tune the mixing parameter, we chose the \maxim um entrop y" value of =0.5 to demonstrate the robustness of the approac h.
While the results in the previous section clearly demon-strate the utilit y of our metho d, we decided to tak e a closer look to better understand what was happ ening. Precision at low recall impro ves 5-6%, but is this the whole story? Due to the nature of information retriev al and the large variabil-ity among queries, there is more than one way of getting this statistically signi can t increase. One poten tial expla-nation is that the ma jorit y of the queries could do sligh tly better, but the minorit y do much worse, for an overall pos-itiv e average. This is not so bene cial, as a user trades the possibilit y of only sligh t impro vemen t for harsh deteriora-tion in retriev al qualit y. The bene ts do not out weigh the risks.

However, we noticed while examining the results by hand that there are a large num ber of queries in whic h precision at low recall exhibits no real change, either positiv e or neg-ativ e. This intrigued us. Our mo dels use global information to capture the cen tralit y of a term's con textual usage. For a good num ber of query terms, there is either not a lot of variation in con textual usage or there is already a high cor-relation between term frequency and con textual cen tralit y. In those cases, term con text mo deling is not going to pro-vide any bene t over BM25. But on the ip side, it also will likely do no worse, either. How migh t this a ect the results we are seeing?
We decided to further examine this in the follo wing man-ner: Figure 4 con tains the exact same 143 queries as gure 3. However, they are split into two parts. On the left are all the queries (87 total) in whic h there was exactly 0% di erence at interp olated 0.0 recall between BM25 and MIX. On the
Figure 3: Overall results: All 143 righ t are all the queries (56 total) in whic h were was non -zero di erence at interp olated 0.0 recall. In other words, these are all the queries in whic h con text information either did better or worse than BM25 alone.

First, we see from the 87 \no change" queries that our in-tuition is fairly correct. There are a large num ber of queries on whic h con text mo deling has little to no e ect. Rel j ret is sligh tly better, precision at 0.5 is sligh tly worse (though still not statistically signi can t), and at all other levels and on average there is practically no change. On the other hand, for those queries in whic h there is di erence at 0.0 recall, better or worse, the term con text approac h sho ws a huge, signi can t 37% impro vemen t at 0.0 and 17% impro vemen t at 0.1 recall. Average precision impro vemen t is also statis-tically signi can t. We saw similar patterns of impro vemen t when we brok e down the results from other ( =0.3 to 0.8) mixture parameters as well.

These are imp ortan t results. They indicate that when our technique works, it works extremely well. And, equally imp ortan tly, when our technique does not work, it does little to no hurt. Risk is minimized, whic h a desirable feature of any retriev al system.

Finally , please note the di erence in absolute values be-tween the two query subsets. The \no change" subset has much higher precision than the \impro ved" subset. We think this is further evidence of the robustness of our ap-proac h: When the original query is already well-p erforming, con text mo dels manage not to monk ey with a good thing. On the other hand, when the original query yields poor results, con text mo dels manage to impro ve the query im-mensely . Con text mo dels may even be useful for predicting whic h queries will and will not perform well.
There are a num ber of possible areas one could use term con text mo dels. We have tried to mak e absolutely clear that the term con text mo deling done in this pap er is not the same as query expansion or pseudo-relev ance feedbac k. However, we ackno wledge that one could actually utilize con text mo d-els in supp ort of some of these techniques. For example, supp ort feature weigh ts are curren tly trained per term at index time, indep enden t of an actual query . One could eas-ily envision a pseudo-relev ance re-training of these weigh ts post-query time, on the top n retriev ed documen ts. That would then alter the con text score in the remainder of the collection, and may impro ve retriev al.

A second possibilit y involves query expansion. Returning to the example from section 1, supp ose we have a mo del for fuel in whic h gas and coal are the two highest-w eigh ted supp ort features. We are already using fuel's con text score alongside the BM25 score. However, that does not stop us from adding the BM25 scores for the terms gas and coal to the query , i.e. doing expansion. The key point is that we can also mix the con text scores for gas and coal into the nal score. So we would not just be adding more docu-men ts with high tf for gas and coal; we would be boosting documen ts whic h con tained con texts that best supp orted all three terms: fuel, gas and coal. [16] rep orts that one of the disadv antages of query expansion is that \individual queries can be signi can tly degraded by expansion." If we do ex-pansion not just by adding more terms, but by taking into accoun t the cen tralit y of those terms' con text, we may be able to impro ve without risking signi can t degradation.
Perhaps the most interesting possibilit y for term con text involves richer mo deling. Recall from equation (2) that fea-tures may be conjuncts of one or more supp ort terms t x j We did an exp erimen t in whic h we induced a mo del for the term \food" using the LA Times collection. In this mo del, the feature \drug" has a small, positiv e f weigh t. There is clearly a relationship between these two terms, as evidenced by phrases suc h as \Food and Drug Administration". But both terms are used in enough other con texts that the an-ity is small.

At this point, co-o ccurrence alone has nothing more to say about these terms other than their sligh t anit y. However, in our exp erimen t the con text mo del automatically learned a very interesting multiple supp ort term feature: f \drug" AND \police" g . It added this feature to the mo del with a strong negativ e weigh t.

This means that if \drug" is a documen t by itself there is a sligh tly higher likeliho od of \food" belonging there. However, if both \drug" and \police" are in the documen t, there is strong supp orting evidence against \food". (The term \police" by itself, without \drug", carries little evi-dence one way or the other.) By taking into accoun t the non-indep endence of supp ort features, the con text mo del implicitly disam biguates between \drug" as a medication and \drug" as an illegal narcotic, and the e ect that has on \food". This is something that straigh t co-o ccurrence can-not do. It is also something that the con text mo del learned by itself, automatically , with no domain kno wledge. We should be able to tak e adv antage of this for retriev al.
We are encouraged by this example to pursue richer mo d-els of con text. Maxim um entrop y and random eld mo dels are, as has been oft noted, ideal framew orks for the integra-tion of all types of evidence. In the future, we hop e to add more useful con textual features to our mo del. As we have sho wn, having a good statistical mo del of a term's con text is useful for retriev al.
Term con text mo dels o er a new tool for assessing term presence in a documen t. By determining whether a doc-umen t pro vides supp ort (con text) for a term, rather than using the observ ed frequency of that term, we have created a fundamen tally di eren t metho d for assessing similarit y be-tween a query term and a documen t. Furthermore we have sho wn that this approac h is useful for retriev al, with huge impro vemen ts in precision at low recall. Though we are not the rst to use maxim um entrop y or random eld mo dels for information retriev al, we are the rst to use them in this manner. By explicitly mo deling the con text of a term we open the doors to man y new applications.
We wish to thank Victor Lavrenk o for some of his written descriptions of the maxim um entrop y framew ork, borro wed here (with permission) from earlier collab orations in whic h we applied similar mo dels to the problem of music informa-tion retriev al. [1] D. Beeferman, A. Berger, and J. La ert y. Text [2] A. L. Berger, S. A. Della Pietra, and V. J.
 [3] S. Della Pietra, V. Della Pietra, and J. La ert y. [4] W. Grei and J. Ponte. The maxim um entrop y [5] P. Kan tor and J. Lee. The maxim um entrop y principle [6] J. La ert y, A. McCallum, and F. Pereira. Conditional [7] V. Lavrenk o and J. Pic kens. Polyphonic music [8] A. McCallum and N. Ghamra wi. Collectiv e multi-lab el [9] D. Metzler and W. B. Croft. A mark ov random eld [10] R. Nallapati. Discriminativ e mo dels for information [11] J. M. Ponte and W. B. Croft. A language mo deling [12] M. Porter. An algorithm for sux stripping. Program , [13] S. Rob ertson, S. Walker, S. Jones, [14] R. Rosenfeld. A maxim um entrop y approac h to [15] E. Voorhees and D. Harman. Overview of the sixth [16] J. Xu and W. B. Croft. Query expansion using local [17] C. Zhai and J. La ert y. A study of smo othing
