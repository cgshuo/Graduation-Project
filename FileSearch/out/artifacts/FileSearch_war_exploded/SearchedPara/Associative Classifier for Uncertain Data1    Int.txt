 In recent years, due to advances in technol ogy and deep understanding of data acquisi-tion and processing, uncertain data has attracted more and more attention in the litera-ture. Uncertain data is ubiquitous in many real-world applications, such as environmen-tal monitoring, sensor network, market analysis and medical diagnosis [1]. A number of factors contribute to the uncertainty. It m ay be caused by imprecision measurements, network latencies, data staling and decision errors [2,3]. Uncertainty can arise in cat-egorical attributes and numeric attributes [1,2]. For example, in cancer diagnosis, it is often very difficult for the doctor to exactly classify a tumor to be benign or malignant due to the experiment precision limitation. Therefore it would be better to represent by probability to be benign or malignant [2].

Associative classifiers are relatively easy for people to understand and often out-perform decision tree learners on many classi fication problems [5,7,8]. However, data uncertainty may render many conventional classifiers inapplicable to uncertain classi-fication tasks. Consequently, the following adaptations are required to ensure that the extension to CBA [5] can classify uncertain data: Firstly, due to uncertainty, we need to modify the initial definition of support and confidence of associative rules [11] to mine association rules from uncertain data. S econdly, CBA only utilizes the rule with the highest confidence for classification. For uncertain data, an instance may be partially covered by a rule. We define the weight of instance covered by a rule and introduce multiple rules classification , and this help to improve the performance of the proposed algorithm. To the best of our knowledge, this is the first work devoted to associative classification of uncertain data.

To sum up, in this paper, based on the expected support [4], we extend CBA algo-rithm [5] and propose an associative classifier, uCBA, for uncertain data. We perform experiments on real datasets with uncertainty, and the experimental results demonstrate that uCBA algorithm perform well even on highly uncertain data.
 This paper is organized as follows. In the next section, we survey the related work. Section 3 gives problem statement. Section 4 illustrates the proposed algorithm in detail. The experimental results are shown in Section 5. And finally, we conclude the paper and give future work in Section 6. A detailed survey of uncertain data mining techniques may be found in [9].In the case of uncertain data mining, studies include clustering [23,24,25], classification [2,3,10,22], frequent itemsets mining [4,12,13,16,17] and outlier detection [26]. Here, we mainly focus on associative classification of uncertain data.

At present, existing works about classification of uncertain data all fall into exten-sions to traditional classification algorithms. Qin et al. proposed a rule-based algorithm to cope with uncertain data [2]. Later, in [3], Qin et al. presented DTU, which based on decision tree algorithm, to deal with uncertain data by extending traditional measure-ments, such as information entropy and information gain. In [10], Tsang et al. extended classical decision tree algorithm and proposed UDT algorithm to handle uncertain data which is represented by probability density function (pdf). In [22], Bi et al. proposed Total Support Vector Classification (TSVC), a formulation of support vector classifi-cation to handle uncertain data. Associative classifiers for certain dataset have been widely studied [5,7,8]. However, the problem studied in this paper is different from works mentioned above, we consider classification of uncertain data from the perspec-tive of association rule mining and propose an associative classifier for uncertain data, uCBA.

Recently, there have been some studies on frequent itemset mining from uncertain transaction databases. In [4], Chui et al. extended Apriori [11] algorithm and proposed U-Apriori algorithm to mine frequent itemsets from uncertain data. U-Apriori com-putes the expected support of itemsets by summing all itemset probabilities. Later, in [12], they additionally proposed a probabilistic filter in order to prune candidates early. Leung et al. proposed the UF-growth algorithm in [13]. Like U-Apriori, UF-growth computes frequent itemsets based on the expected support , and it uses the FP-tree [14] approach in order to avoid expensive candidate generation. In [15], Aggarwal et al. ex-tended several classical frequent itemset mi ning algorithms to study their performances when applied to uncertain data. In [16], Zhang et al. proposed exact and sampling-based algorithms to find likely frequent items in streaming probabilistic data. In [17], Ber-necker et al. proposed to find frequent itemsets from uncertain transaction database in a probabilistic way. Different from studies in [4,12,13], work in [17] mined probabilis-tic frequent itemsets by means of the probabilistic support . All the works mentioned above belong to the framework of mining frequent itemsets from uncertain transaction databases, and do not consider mining association rules from uncertain data. In this pa-per, we apply the expected support [4] to mine associative rules from uncertain data, and then perform associative classification for uncertain data.
 At present, there are few works about mining associative rules from uncertain data. In [18], Weng et al. developed an algorithm to mine fuzzy association rules from un-certain data which is represented by possibility distributions. In their study, there are relations between all the possible values of a categorical attribute,and they provide a similarity matrix to compute the similarity between values of this attribute. While re-cent studies in the literature about uncerta in data management and mining generally base on possible world model [6]. In this paper, we integrate possible world model [6] into mining association rules from uncertain data. For simplicity, in this paper, we only consider uncertain categorical attributes, and fol-lowing studies in [2,3], we also assume the class label is certain. 3.1 A Model for Uncertain Categorical Data When dealing with uncertain categorical attribute, we utilize the same model as stud-ies in [1,2,3] to represent uncertain categorical data. Under the uncertain categorical model, a dataset can have attributes that ar e allowed to take uncertain values [2]. And we call these attributes Uncertain Categor ical Attributes, UCA. The concept of UCA was introduced in [1]. Let X  X  write A u c i for the i th uncertain categorical attribute, and V tribute value of A u c i can be represented by the probability distribution over V i ,andfor-tain attribute can be viewed as a special cas e of uncertain attribute. In this case, the P 3.2 Associative Rules for Uncertain Data Let D be the uncertain dataset, Y be the set of class labels, and y  X  Y be a class ( A possible values of each UCA to a set of attribute-value pairs. With these mappings, we can view an uncertain instance as a set of (attr ibute, attribute-value) pairs and a class label.
 Definition 1. Let an item x be the (attribute, attribute-value) pair, denoted as x = ( A i ,v ik ) ,where v ik is a value of attribute A v ik is the value of i th attribute of t j .
 Following the definition of rule in [8], we can define a rule for uncertain data: Definition 2. An associative rule R for uncertain data, is defined as R : x 1  X  x 2  X  X  X  X  X  x  X  y .Here X = x of R .
 An uncertain instance t satisfies R if and only if it satisfies every item in R .If t satisfies R , R predicts the class label of t to be y . If a rule contains zero item, then its antecedent is satisfied by any instance.

In U-Apriori [4] algorithm, to handle uncertain data, instead of incrementing the support counts of candidate itemsets by their actual support, the algorithm increments the support counts of candidate itemsets by their expected support under the possible world model.
 Definition 3. The expected support of antecedent, X ,ofrule R on uncertain dataset D can be defined as [4]: where p j ( x ) is the existence probability of item x in t j , p t j ( x ) &gt; 0 . 0 .
Accordingly, we can compute the expected support of R , expSup ( R ) , as following: Rule R is considered to be frequent if its expected support exceeds  X  s  X | D | ,where  X  s is a user-specified expected support threshold.
 Definition 4. For association rule R : X  X  y on uncertain data, with expected support expSup ( X ) and expSup ( R ) , its confidence can be formalized as: The intuition behind conf idence ( R ) is to show the expected accuracy of rule R un-der the possible world model. Rule R is considered to be accurate if and only if its confidence exceeds  X  c ,where  X  c is a user-specified confidence threshold.
 Weight of Uncertain Instance Covered by a Rule. Due to data uncertainty, an instance may be partially covered by a rule. The intuition to define this weight is twofold. On one hand, inspired by pruning rules based on database coverage [7], in classifier builder algorithm, instead of removing one instance from the training data set immediately after it is covered by some selected rule like CBA does, we let it stay there until its covered weight reached 1.0, which ensures that each training instance is covered by at least one rule. This allows us to select more rules. When classifying a new instance, it may have more rules to consult and better chance to be accurately predicted. On the other hand, in multiple rules classification algorithm, we utilize this weight to further control the number of matched rules using to predict a test instance. In this paper, we follow the method proposed in [2] to compute the weight of instance covered by rule.
 in the rule sequence as following: can be formalized as following: Multiple Rules Classification. When classifying an uncerta in instance, a natural ap-proach is directly following CBA [5], which only utilizes the rule with the highest con-fidence for classification, i.e. single rule classification . However, this direct approach ignores uncertain information in the instance, and hence may decrease the prediction accuracy.

Note that CMAR [7] performs classification based on a weighted  X  2 analysis by us-ing multiple strong associative rules, we can follow the idea of CMAR and modify the formula of weighted  X  2 by using the expected support of rules. In this paper, we refer to methods in [8,10,2] to predict class label, which classifies the test instances by com-bining multiple rules. Similar to works in [8,10,2], which use the expected accuracy of rules on training dataset [8,10] or the class probability distribution of training instances that follow at the tree leaf to classify the test instance [2], we utilize the confidence of rules to compute the class probability distribution of test instance t to predict the class label, which can be formalized as: where rs is the set of rules that t matches. This method is also similar to the idea of CMAR.
 Pruning Rules based on Pessimistic Error Rate. Since there are strong associations in some datasets, the number of association rules can be huge, and there may be a large number of insignificant association rules, which make no contributions to classification and may even do harm to classification by introducing noise. Consequently, rule prun-ing is helpful. Following CBA [5], we also utilize pessimistic error rate (PER) based pruning method presented in C4.5 [19] to prune rules, which is formalized as: Here under the uncertain data scenario, r is the observed error rate of rule R l , r = c is confidence level given by the user. Based on CBA [5], we propose uCBA algorithm for associative classification of uncer-tain data. It consists of three parts, a rule generator (uCBA-RG), a classifier builder (uCBA-CB), and multiple rules classification (uCBA-MRC). 4.1 Algorithm for uCBA-RG Conventional CBA mines associative rules based on Apriori [5] algorithm, while uCBA generates uncertain associative rules based on U-Apriori [4] algorithm. And the general framework of uCBA-RG is the same with that of CBA-RG [5], the differences lie in their ways to accumulate th e support counts: if instance t matches R l , then for CBA-RG, support of antecedent X increase incrementally by 1, i.e. X . condSupCount ++. Furthermore, if R l and t have the same class label, then support of rule also increase incrementally by 1, i.e. X . ruleSupCount ++ [5]. While for uCBA-RG, if t matches R l , then the expected support of X increase incrementally by probability of t covered by R l , i.e. X . expSup += P ( t, R l ) , P ( t, R l ) can be computed following formula (5). Expected support of R l updates similarly. The algorithm is omitted here for lack of space. 4.2 Algorithm for uCBA-CB Here we give our uCBA-CB algorithm, which is illustrated in Algorithm 1. It has three steps:
Step 1 (Line 1): According to relation  X  *  X  for rules in CBA [5], we sort the set of generated rules R based on expected support and confidence , which guarantees that we will choose the highest precedence rules for our classifier.

Step 2 (line 2-23): Selecting rules for the classifier from R following the sorted sequence. For each rule R l , we traverse D to find those instances covered by R l (line 7) and compute probability (line 8) and weight (line 9) of instances covered by R l .If weight of instance covered by R l is greater than 0, then records the weight and marks R l classifier (line 15,16). Meanwhile, we need to update the weight of instances covered by R l (line 17,18,19). For uncertain data, we should ensure that the total weight of each Algorithm 1. Classifier Builder for uCBA instance covered by all the matching rules is not greater than 1.0 (line 7). Similar to CBA-CB [5], we also select a default class for each potential rule in the classifier (line 20). We then compute and record the total errors that are made by the current C and the default class (line 21). When there is no rule or no training instance left, the rule selection procedure terminates.

Step 3 (line 24-26): Selecting the set of the most predictable rules on training dataset as the final classifier. Same with CBA [5], for our uCBA-CB, the first rule at which there is the least total errors recorded on D is the cutoff rule. 4.3 Algorithm for uCBA-MRC Here we introduce the algorithm of multiple rules classification in uCBA, which is given in Algorithm 2, it has two steps: matched rules (line 3). Note that, if we use all the matched rules to predict the class label, and do not filter rules with low pr ecedence, it may introduce noise and decrease the prediction accuracy. And we will validate t his idea in the experi mental study. There-fore, in our uCBA-MRC, under the circumstances of ensuring that the weight of each test instance covered by rules is less than 1.0, we further constrain the number of mul-tiple rules not to exceed a user-specified threshold (line 4). When t j satisfies R l ,we compute the probability (line 5) and weight (line 6) of instance covered by R l .Ifthe covered weight is greater than 0, then we update the weight of instance covered by the matched rules (line 8), and insert R l into the multiple rule set (line 10). Step 2 (line 14-15): Predict the class labe l of instance according to formula (6). Algorithm 2. Multiple-Rule Classification for uCBA In order to evaluate the classification performance of our uCBA algorithm, we perform experiments on 21 datasets from UCI [20]Repository. At present, for simplicity, our algorithm only consider uncertain categorical attribute. For datasets with numeric at-tributes, each numeric attribute is first dis cretized into a categorical one using method as in [5]. At present, there are no standard and public uncertain datasets in the literature. Experiments on uncertain data in the literature all perform on synthetic datasets, which means researchers obtain uncertain dataset via introducing uncertain information into certain dataset [2,3,10].

For all of the experiments in this paper, w e utilize the model introduced in Section 3.1 to represent uncertain dataset. For example, as described in [2], when we introduce 20% uncertainty, this attribute will take the original value with 80% probability, and take other values with 20% probability. Meanwhile, we utilize Information Gain, IG, to select the top K attributes with maximum IG values, and transform these top K attributes into uncertain ones. And the uncertain dataset is denoted by Top K uA (Top K uncertain Attribute).

Our algorithms are implemented in Java based on the WEKA 1 software packages, and the experiments are conducted on a PC with Core 2 CPU, 2.0GB memory and Windows XP OS. We set expected support threshold to 1% and confidence threshold to 50%. Following CBA [5], we also set a limit of 80,000 on the total number of candidate rules in memory. As in [2,3,10], we measure t he classification performance of the pro-posed classifier by accuracy. All the experimental results reported here are the average accuracy of 10-fold cross validation. 5.1 Performance of uCBA on Uncertain Datasets In this group of experiment, we evaluate the performance of uCBA algorithm on differ-ent level of uncertain dataset. In the following, U T represents dataset with T % uncer-tainty, and we denote certain dataset as U0. Note that, uCBA algorithm is equivalent to the traditional CBA when applying to certain dataset, that is to say, when we set T =0 , our uCBA algorithm performs the same as CBA does.

Table 1 gives the performance for uCBA on Top2uA and different level of uncer-tainty (U0-U40) dataset. In Table 1, column CBA represents accuracy of CBA on cer-tain datasets; column Single represents accuracy of uCBA X  X  single rule classification ; and column Multiple represents accuracy of uCBA X  X  multiple rules classification .
As shown in Table 1, with increasing of un certain level, the accuracy of uCBA de-grades to some extent. It can be observed from Table 1 that, in most cases, the accuracy of Multiple exceeds that of Single ; and on all the uncertainty datasets, the averaged accuracy of Multiple is higher than that of Single . It can be observed from Table 1 that, uCBA per-forms differently for different datasets. For some datasets, for example, balance-scale, labor and sonar, when introducing uncertainty into the datasets, uCBA-MRC can im-prove the prediction accuracy comparing w ith the accuracy of CBA. For most datasets, the performance decrement a re within 7%, even when data uncertainty reaches 30%. The worst performance decrement is for the nur sery dataset, the classifier has over 94% ac-curacy on certain data, reduces to around 67.5% when the uncertainty is 10%, to 67.4% when the uncertainty is 20%, and to 67. 3% when the uncertainty reaches 30%.
The similar experiment results could be observed when we set K to 1, 3 and other values, and are omitted here for limited space. Overall speaking, the accuracy of uCBA classifier remains relatively stable. Eve n when the uncertainty level reaches 40% (U40), the average accuracy of uCBA-MRC (83.5%) on 21 datasets is still quite comparable to CBA (88.0%), and only decreases by 4.5% on accuracy. The experiments shows uCBA is quite robust against data uncertainty. Meanwhile, the di fference of accuracy of the two methods among different experiment setti ngs is significant on paired-sample t-Test [21], which means Multiple is more robust than Single . 5.2 Parameter Analysis on coverThreshold In this group of experiment, we analyze the effect of parameter, coverT hreshold ,on accuracy. As discussed earlier, this paramete r controls the number of rules for classi-fication. Generally speaking, for uncertain data classification, if we use very few rules for classification, the instance may not be fully covered by rules, and this may lead to bad classification performance; on the other hand, if we use too many rules for classifi-cation, we may introduce noise and it may also lead to bad classification performance.
As an example, we analyze the effect of coverT hreshold on accuracy over 5 un-certain datasets with different level of uncertainty. From Fig.1, we can see that when the number of rules for classification exceeds 5, the performance of uCBA over the 5 datasets tend to be stable. Therefore, we set this parameter to 5 in all of the experiments in this paper. 5.3 Time and Space Analysis of uCBA Here we analyze the number of association rules generated and the time token to gen-erate these rules. We select 5 datasets from UCI Repository to perform this experiment, and analyze time and space consumption over Top1uA, Top2uA with U20 uncertain dataset. In Table 2, column w/o pru represents without rule pruning, and column pru represents rule pruning with PER following fomula (7).

We can see from Table 2 that PER can greatly reduce the number of rules, and prune insignificant rules. Meanwhile, we can also see that the number of association rules on uncertain data is larger than that on certain data, this is because there are many uncertain information in uncertain data. It is shown that under the same level of uncertainty, the more the number of uncertain attributes, the longer it takes to mine association rules. Data uncertainty is prevalent in many real-w orld applications. In this paper, based on the expected support , we extend CBA algorithm and propose an associative classifier, uCBA, for uncertain data classification task. We redefine the support, confidence, rule pruning and classification strategy of CBA to build uncertain associative classifier. Ex-perimental results on 21 datasets from UCI Repository demonstrate that the proposed algorithm yields good performance and has satisfactory performance even on highly uncertain data.
 At present, our proposed algorithm only considers uncertain categorical attributes. We will consider uncertain numeric attributes in our future work.

