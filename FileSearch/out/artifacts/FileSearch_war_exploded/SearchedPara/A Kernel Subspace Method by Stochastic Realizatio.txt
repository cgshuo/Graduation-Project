 numerous fields, such as system control.
 and reliable numerical schemes.
 be executed by a procedure similar to that in the linear case. and the conditional covariance matrix of x and y conditioned on z by  X  the row space of A .  X  0 denotes the transpose of a matrix  X  , and I 2.1 Problem Description and Some Definitions dimensions n construct from observed input-output data, as a nonlinear s tate-space system: realization with reproducing kernel Hilbert spaces.
 maps  X  Mercer kernels k order random variables u ( t ) , y ( t ) and w ( t ) , and F as and the Hilbert spaces generated by these random variables a s: U functionals of the joint process in the feature spaces (  X  2.2 Optimal Predictor in Kernel Feature Space First, we require the following technical assumptions [3,5 ]. !! A A space U linear case.
 P described below are fulfilled: (1) There is no feedback from  X  (2) U  X  P reproducing kernel Hilbert spaces. As U + Y we can obtain Y  X  ture space U  X  U t = U kernel Hilbert spaces. Therefore, U +  X  Using proposition 1, we now obtain the following representa tion result. T in feature space f  X  ( t ) based on P  X  in which  X  and  X  satisfy the discrete Wiener-Hopf-type equations P 2.3 Construction of State Vector Let L L has the entries  X  orthogonal.
 We define the extended observability and controllability ma trices  X  be the n -dimensional vector process  X  of predictor  X  f  X  ( t ) in Eq. (4) has the form contained in P  X  In analogy with the linear case [3,5], the output process in f eature space  X  minimal stochastic realization with the state vector x ( t ) of the form constant matrices and e ( t ) :=  X  ( y ( t ))  X  (  X  ( y ( t )) | P  X  2.4 Preimage define the feature maps  X   X  U because X direct sum of the oblique projections as As a result, we can obtain the following theorem.
 T
HEOREM 2. Under assumptions 1 and 2, if rank  X   X  sented in the following state-space model: where  X  e ( t ) := y ( t )  X  y ( t ) / X  X  coefficient matrix of the nonlinear regression from  X  e ( t ) to e ( t ) 1 . 3.1 Realization with Finite Data f  X  ( t ) / U  X  + T
HEOREM 3. Under assumptions 1 and 2, if rank( X   X  expressed by the following nonstationary state-space mode l: where the state vector  X  x and  X  e approximated state vector  X  x from x ( t ) in Eq. (8); however, when T  X   X  , the difference between  X  x of the following Algebra Riccati Equation (ARE): P Moreover, the Kalman gain K  X  converges to where  X   X  respectively. 3.2 Using Kernel Principal Components Let z be a random variable, k denote  X  principal components u form an orthonormal basis of a d be described as the linear combination U expansion coefficients. A such that A components is given by C  X 
 X  z =  X  z  X  0 z = I m ), we can derive the following equation: where  X   X  A  X  where  X   X  ant can be obtained by replacing G  X  the square root matrix of  X   X  where k ( p ( t )) :=  X  in the previous sections. First, Eq. (11) can be approximate d as where A and G that all data is centered. First, using the Gram matrices G G and G Step 2 Calculate the SVD of the normalized covariance matrix (cf. E q. (19)) Step 3 Estimate the state sequence as (cf. Eq. (20)) Step 4 Calculate the eigendecomposition of the Gram matrices G Step 5 Calculate the covariance matrices of the residuals 5th-order Runge-Kutta method with a sampling time of 0.05 se conds: principal components n pc acquired by our algorithm, in which the parameters were set a s  X   X  x = 1 . 0 errors [2] where y s involved much time and effort for tuning. tions and the solid lines represent the simulated values. Acknowledgments References
