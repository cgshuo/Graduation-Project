 1. Introduction 1.1. Distributed learning
Learning from example is one of the most popular paradigm of machine learning. It deals with the problem of identifying regularities between a number of independent variables and a target or dependent categorical variable in a given dataset ( Tsoumakas et al., 2004 ). If such a dataset is categorical then one faces a classification problem. In such case, the task of the learning problem is to find a model (or function) that describes and distinguishes data classes or concepts. The model is called classifier ( Han and Kamber, 2001 ). Finally, a classification model role is to predict the class of objects whose class labels are unknown ( Han and Kamber, 2001 ).

Classification is also considered as a typical task of data mining, where data mining is a component of the knowledge discovery in databases (KDD) ( Frawley et al., 1991 ). Within the KDD process, viewed as an iterative sequence of steps, data mining is preceded by the data reduction step ( Frawley et al., 1991 ).

There exist numerous learning algorithms. However, the applicability of various approaches to the machine learning is, in practice, limited. Most important barrier and at the same time a challenge to machine learning is posed by a very large datasets and their complexity. The development of efficient techniques that scale up to a very large datasets is one of the main challenges of machine learning ( Fu and Kechadi, 2006 ). Majority of approaches to the machine learning base on the assumption that all training data can be pooled together in a centralized data repository. In the real life there are, however, numerous cases, where the data volume is huge and is physically and geographi-cally distributed. Such collections of data can be encountered in meteorological, financial, medical, industrial or science domains. In practice it is often unrealistic or unfeasible to pool together the distributed data for centralized processing. Unfortunately, moving all data into a centralized location can be very time consuming, costly from the commercial point of view, or may not be feasible due to restricted communication bandwidth among sites or high expenses involved, or some other reasons ( Klusch et al., 2003; Silva et al., 2005 ). Thus, applying algorithms that require pooling together data from the distributed data sources may not be possible ( Kargupta et al., 1999; Silva et al., 2005 ).
The need to extract potentially useful patterns out of separated, distributed data sources created a new, important and challenging research area, known as distributed data mining (DDM) or knowledge discovery from multi-databases ( Kargupta et al., 1999 ). In DDM the process of finding a classification model is referred to as the distributed learning. In general, knowledge discovery from multi-databases is considered to be a more complex and difficult task than knowledge discovery from a single database, and one of the main challenges is to create systems that learn automatically from distributed datasets ( Kargupta et al., 1999; Silva et al., 2005; Zhang et al., 2003 ).
Several approaches to distributed learning have been proposed. Among them there are: integration of independently derived hypotheses by combining different classifiers to make global classification decisions ( Chawla et al., 2001 ), model aggregating Bayesian classifiers ( Dash and Cooper, 2001 ), or global level combination of learning results obtained by using meta-learning ( Prodromidis et al., 2000 ). In Tsoumakas et al. (2004) it is concluded that, meta-learning methodologies view data distribution as a technical issue and treat distributed data sources as a part of a single database. It is also pointed out that such an approach offers a rather narrow view of the distributed data mining, since the data distributions in different locations are often not identical. In Tsoumakas et al. (2004) an approach to clustering local classification models induced at physically distributed databases is proposed. Stacking with respect to the number of distributed nodes is another approach investigated in
Wei et al. (2009) . Approaches, focusing on improving the performance of the distributed learning based on a boosting technique, have been proposed in Lazarevic and Obradovic (2002) and Min (2008) . An alternative approach to distributed learning is the concept of the genetic-based machine learning methods, also called learning classifier systems (LCSs). The architecture of the distributed learning classifier system has been proposed in Dam et al. (2008) . Several approaches for learning from the distributed datasets have been also discussed in Cao (2009) .

Distributed data mining is expected to perform partial analysis of data at each of the separate sites, called local sites, individually and at the same time. Then, the partial analysis results can be moved to other sites where they are merged into the global knowledge model. Such approach can be seen as a mean of addressing the scalability issue of mining large datasets. In such case the local level analysis can be carried out by simple sampling of data ( Morgan et al., 2003; Kuri-Morales and
Rodriguez-Erazo, 2009 ). However, one disadvantage of such an approach is that the sampled set may not sufficiently represent the local level data.

Selecting out of the distributed databases only the relevant data can eliminate or reduce the above problems and speed up the global knowledge extraction process. Such approach is much more efficient computationally than moving all distributed datasets into the centralized site for learning the global model.
As it was mentioned in Stolfo et al. (1997) learning models based on the reduced datasets combined later into a meta-model seem to be one of the most successful approaches to distributed data mining. It also means that in such case the data reduction plays an important role in the distributed learning.

In general, the objective of data reduction is to find patterns, also called prototypes or references vectors, or regularities within certain attributes (see, for example Liu et al., 1998 ). Data reduction can be achieved by selection of instances, by selection of attributes/features or by simultaneous reduction of both dimensions ( Bhanu and Peng, 2000; Kuri-Morales and Rodri-guez-Erazo, 2009 ). When in DDM the data reduction is carried out by instance selection, the datasets are replaced by the reduced sets of local prototypes. The reduced datasets enable either pooling the data together and using some mono-database mining tools or effectively applying some meta-learning techniques.
The process of learning from the distributed data can be further complicated by differences of structures and features within the considered distributed datasets ( Riesen and Bunke, 2009 ). The differences between the considered distributed datasets can be natural ( Silva et al., 2005 ). It means that it can result from their design or from the vertical fragmentation of data ( Silva et al., 2005; Skillicorn and McConnell, 2008 ).
Such differences can also result from introducing simultaneously reduced datasets in the two dimensions, through selecting instances and removing irrelevant attributes at each local site. In consequence the reduced datasets at each site are not necessarily homogenous and may contain values of different sets of features with only some features common among the sites.
To sum up, the aim of the distributed data reduction is to select prototypes at each local site by simultaneously reducing datasets in the two dimensions. If the reduced datasets are heterogeneous in the above sense then some further techniques would be required to deal with such situation. One possible approach is applying, at the global level a special combiner strategy, responsible for combining and integrating a number of classifiers learned from heterogeneous sets of prototypes.

Another possible approach is to assure that prototypes obtained at each local site are homogenous in the discussed sense and characterized by the identical set of attributes. In such case, if at the local level the data reduction assumes simulta-neously reducing dataset in the two dimensions, a special technique for assuring the selection of homogenous prototypes is required. Such technique can be based, for example, on the distributed feature discrimination test using tree induction algorithm supported by some special agent X  X ediator whose role is to facilitate the communication process between agents involved in the distributed data mining process ( Bala et al., 2003; Kim et al., 2010 ).

Agent-oriented software engineering seems to be an attractive tool for implementation of methods and systems dedicated for the distributed learning (see, for example Klusch et al., 2003;
Symeonidis et al., 2007 ). In such an approach each site can have one or more associated agents processing the local data and communicating the results to other agents that control and manage the discovery knowledge process. For example, in Edward and Davies (1993) , Guo et al. (1997) , Prodromidis et al. (2000) ,
Sian (1991) , Stolfo et al. (1997) and Tozicka et al. (2008) different distributed agent-based data-mining systems, denoted, respec-tively, as PADMA, JAM, Kensington, Papyrus, MALE, ANIMALS and
MALEF have been proposed. The example of a system that was developed for mining heterogeneous data set is BODHI ( Bailey et al., 1999 ). In Albashiri et al. (2009) the EMADS conceptual framework and its current implementation are shown. EDMAS is a hybrid peer-to-peer agent-based system comprising a collection of collaborating agents, including a collection of classifier data mining agents that exist in a set of containers. These containers may be distributed across a network.

Although several examples of agent-based distributed data mining systems have been recently described, effective methods for learning from heterogeneous and massive datasets, including these obtained by data reduction, are still an active field of research (see also, for example Kargupta et al., 1999; Caragea et al., 2003; Zhang et al., 2003; Yang et al., 2009; Kim et al., 2010 ).
Systems of cooperative information agents for data mining tasks in distributed environments appear to be also a natural vision for the near future to be realized ( Klusch et al., 2003; Cao, 2009 ). 1.2. Learning from distributed data  X  the problem formulation
The problem of learning from data can be formulated as follows: given a dataset D , a set of hypothesis H , a performance criterion P , the learning algorithm L outputs a hypothesis h that optimize P . In pattern classification application, h is a classifier (i.e. decision tree, artificial neural network, na X   X ve Bayes, k -nearest neighbor, etc.). The data D consists of N training examples. Each example consists of a set of attribute/feature values and is labeled with a class. The class label of each example can take any value from a finite set of decision classes C  X  { c :l  X  1, y , k }, which has cardinality k . The set of attributes (or features) A , common to all examples, has the total number of attributes equal to n . The goal of learning is to produce a hypothesis that optimizes the performance criterion (e.g. function of accuracy of classification, complexity of the hypothesis, classification cost or classification error).

In the distributed learning a dataset D is distributed among data sources D 1 , y , D K , with N 1 , y , N K examples, which are stored in separated sites, where K is a number of sites and where the following properties holds: P K i  X  1 N i  X  N , and where all attributes distributed learning a set of constraints Z can be imposed on the learner. Such constraints may for example prohibit transferring data from separated sites to the central location, or impose a physical limit on the amount of information that can be moved, or impose other restrictions to preserve data privacy.

When data reduction is carried out subsets D 1 , y , D K are replaced by the reduced datasets S 1 C D 1 , y , S K C D K patterns. When data reduction is carried out in both dimensions, i.e. by instance selection and feature selection simultaneously, reduced datasets are likely to become heterogeneous. In such case A , y , A K are sets of features which values are stored at sites 1, y , K , respectively. Obviously, there is possibility that some attributes can be shared across more than one reduced dataset S where i  X  1, y , K . Thus, the goal of data reduction is to find subset S from given D i by reducing of number of examples or/and feature selection and with retaining essentially extractable knowledge and preserving the quality of data mining results.

To sum up, the task of the distributed learner L d is to output a hypothesis h C H optimizing P using operations allowed by Z and using data located in K subsets S 1 C D 1 , y , S K C D K and where each S i is described by set of attributes A i such that ( i , j : A i a A j . 1.3. Scope of the work
The main contribution of the paper is proposing and evaluating a novel agent-based framework for the distributed learning. The approach is to extend the agent-based distributed learning classifier proposed in Czarnowski and J  X  edrzejowicz (2009) by adding features such that data reduction is carried out in two dimensions. The process involves two stages:
At the local level the selection of prototypes from distributed data is carried out. Prototypes are selected in both dimensions through selecting instances and removing irrelevant attributes.
At the global level integration and pooling of the selected prototypes and producing the global classifier takes place.
There are two variants of executing the above process. The first is the distributed data reduction through agent collaboration with a view to obtain homogonous set of locally selected prototypes is discussed. In this case, the instance selection is carried out independently at each site through applying the agent-based population search but the feature selection is managed and coordinated through the process of interaction and collaboration between agents. The second requires a special strategy for constructing combiner classifiers used independently at separate sites, producing a set of heterogeneous datasets. The paper discusses both variants proposing strategies for the feature selection coordination and strategies to obtain the global classifier induced from a heterogeneous set of prototypes.

The paper is organized as follows: Section 2 explains the proposed agent-based data reduction approach to the distributed learning and provides details on how interaction among agents is achieved when the data reduction is carried on in parallel at each distributed data site. Section 3 describes the computational experiment set up and experiment results. Finally, Section 4 contains conclusion and suggestions for feature research. 2. Agent-based approach to learning classifier from distributed data 2.1. Main features of the proposed approach
The data reduction through instance or feature selection belongs to the class of NP-hard problems ( Hamo and Markovitch, 2005 ). Various approaches the data reduction, including those dedicated to the distributed data mining, can be found in Dash and Liu (1997) , Garcia et al. (2008) , Kohavi and John (1997) , Raman (2003) , Rozsypal and Kubat (2003) , Skalak (1994) , Wurst and Morik (2007) , Vucetic and Obradovic (2000) , Kuri-Morales and Rodriguez-Erazo (2009) , Nanni and Lumini (2009) and Viera et al. (2010) . None of them can be considered as superior or guaranteeing satisfactory results in the process of learning classifiers.

To overcome some of the difficulties posed by computational complexity of the distributed data reduction problem it is proposed to apply the population-based approach. The approach, integrating features such as: heuristics like evolutionary compu-tation ( Michalewicz, 1996 ), local search algorithms ( Glover, 1990 ), population learning algorithm ( J  X  edrzejowicz, 1999 ), has the ability to solve difficult combinatorial optimization problem.
The proposed approach is an implementation of different optimization procedures under the umbrella of the asynchronous team of agents (A-Team). An approach based on the A-Team concept can be superior to other distributed learning procedures through exploiting inherent features of A-Teams including cooperation, coordination and scalability when multiple agents search, in parallel at the local level, for the best prototypes.
The A-Team concept was originally introduced by Talukdar et al. (1996) . The design of the A-Team architecture was motivated by other architectures used for optimization including blackboard systems and genetic algorithms. Within the A-Team, multiple agents achieve an implicit cooperation by sharing a population of solutions, also called individuals, to the problem to be solved. An A-Team can also be defined as a set of agents and a set of memories, forming a network in which every agent remains in a closed loop. All the agents can work asynchronously and in parallel. Agents cooperate to construct, find and improve solu-tions which are read from the shared, common memory.

In our case the shared memory is used to store a population of feasible solutions to the data reduction problem. Each solution is represented by the set of prototypes, i.e. by the compact representation of the dataset from a given local level. A feasible solution to the data reduction problem at a local site is encoded as a string consisting of numbers of selected reference instances and numbers of selected features.

The first part of such string, where numbers of selected instances are stored, is constructed during the initial population generation phase. When the initial population is generated at first the instances are grouped into clusters using the procedure proposed in Czarnowski and J  X  edrzejowicz (2004) . Then instances with identical values of this coefficient are grouped into clusters. Further on, selection of the representation of instances through population-based search is carried out by the team of agents for each cluster. The selection and removal of the remaining instances constitute basic steps of the first stage of the proposed procedure. The other part of the string representing a solution to the data reduction problem and containing numbers of the selected features is, at this stage, generated randomly and can be coordinated by a special agent with a view to unify the set of selected features.

When the initial population of solution has been created the team of agents is used to find the best solution at the local level and then the agent responsible for managing the stages of data mining is activated. In general, all the required steps of the proposed approach are carried out by program agents of the four following types: global manager  X  agent responsible for managing the process of distributed learning, optimizing agents  X  agents that are implementations of the improvement algorithms, solution manager  X  agent responsible for managing the popula-tion of solutions, feature manager  X  agent responsible for the feature selection coordination.

Basic functionality of the presented framework realized by the above agents is shown on the use case diagram in Fig. 1 and described in a detailed manner in the following section. 2.2. Agents involved in solving the distributed data reduction problem
Data reduction is carried out, in parallel, for each distributed data site. Data reduction, carried out at a data site, is an independent process which can be seen as a part of the distributed data learning. Each data reduction subproblem is solved by two main types of agents responsible for instance and feature selection namely a solution manager and a set of optimizing agents. Each optimizing agent is an implementation of some improvement algorithm, and the problem of data reduction at local sites is solved by an A-Team, that is a team of optimizing agents, possibly of different kinds supervised by the solution manager.

The solution manager is responsible for organizing the data reduction process at a local site through managing the population of solutions called individuals and updating them when appro-priate. Each solution manager is also responsible for selecting the best obtained solution for the supervised data reduction subpro-blem for randomly generating the initial population of solutions and storing it in the common memory. During the data reduction process the solution manager continues reading individuals (solutions) from the common memory and storing them back after attempted improvement until a stopping criterion is met.
During this process the solution manager keeps sending randomly drawn individuals (solutions) from the common memory to optimizing agents. Each optimizing agent tries to improve the quality of the received solution and afterwards sends it back to the solution manager, which, in turn, updates common memory by replacing a randomly selected individual with the improved one.

To solve the data reduction problem four kinds of optimizing agents have been implemented each representing different improvement procedure: local search with tabu list for instance selection, simple local search for instance selection, local search with tabu list for feature selection and hybrid local search for simultaneous instance and feature selection. If, during the search, an agent successfully has improved the received solution then it stops and the improved solution is transmitted to the solution manager. Otherwise, agents stop searching for an improvement after having completed the prescribed number of iterations.
When the process of improving a solution is carried out, the modified solution replaces the current one if it is an improvement. Evaluation of the solution is carried out by estimating classifica-tion accuracy of the classifier, which is created taking into account the instances and features indicated by the solution. 2.3. Agent responsible for coordinating the feature selection
Solutions to subproblems are not independent. Outcomes of the data reduction at local sites influence overall quality of the distributed learning. Hence, some form of coordination between solutions at local sites is required. Such coordination is possible by introducing the feature manager.

While data reduction subproblems are being solved in parallel the feature selection is coordinated by the special agent called the feature manager. The feature manager is also responsible for the final integration of features selected locally by optimizing agents. The feature manager actions include receiving candidate features form solution mangers, overseeing all data reduction subpro-blems and deciding on the common set of features to be used at both the local and the global levels. Such a feature set is called a winning set of features.

The feature manager performs its tasks in collaboration with solution managers supervising data reduction processes at local sites. Such a collaboration involves information exchange and decision making. Each solution manager, after having supervised a prescribed number of iterations within the data reduction process at a local site is obliged to send to the feature manager a set of the candidate features together with its fitness. This set contains features from the current best local solution. The fitness is calculated as the estimated classification accuracy of the classifier induced from the reduced dataset at the local site. The number of iterations carried out by a local A-Team is a parameter set by the user. One iteration cycle covers all A-team activities between two successive replacements of a single solution from the common memory by the improved one received from an optimizing agent.

After having receiving all candidate features from local sites the feature manager decides on the winning set of features. Its decision is based on a predefined strategy selected from the set of possible strategies by the user. Once the winning set of features have been chosen the feature manager passes the outcome of its decision to all solution managers, whose role now is to update the respective individuals by correcting accordingly the structure of features in the strings representing solutions in the current population.

The simplest strategy for deciding on the winning set of features is to accept all features that have been proposed at the local level. In such case the procedure of updating features is equivalent to adding to each string representing the best solution at a local site numbers of features that have been selected from the best solutions at other sites. Another strategy for deciding on the winning set of features considered in the paper is the feature voting. In such case the winning set contains features that have been proposed by the majority of the local level solutions.
Functional interactions between the agents used for solving the distributed data reduction problem with the feature selection coordination are shown in the sequence diagram ( Fig. 2 ). 2.4. Agent responsible for managing the process of distributed learning
The proposed approach deals with several data reduction subproblems solved in parallel. The parallel process is managed by the global manager, which is activated as the first one within the learning process. This agent is responsible for managing all stages of the data mining. At the first step it identifies the distributed learning task that is to be solved and allocates optimizing agents to local sites using the available agent migration procedure. Than the global manager runs, in parallel, all subtasks, that is data reduction processes at local sites.
When all the subtasks have been solved, solutions from the local level are used to obtain a global solution. It requires that the global manager has skills to induce the global classifier. When the prototypes obtained from local sites are homogenous, i.e. when the prototypes are described by the same set of attribute the global manager creates the global classifier, called also a meta-classifier, using a selected machine learning tool. When the local level solutions are represented by heterogeneous set of prototypes, then the global classifier can be induced by applying one of the possible combiner (meta-) classifier strategies. 3. Computational experiment 3.1. Computational experiment setting
The aim of the computation experiment study was to evaluate to what extent the combiner constructing strategies contribute to increasing classification accuracy of the global classifier induced from the set of prototypes selected at autonomous distributed sites. The experiment was also expected to investigate how the choice of strategy for deciding on the winning set of features can influence the classification accuracy of the global classifier.
In the reported research the following combiner constructing strategies have been considered: bagging combiner strategy  X  this strategy is based on the bagging approach as described in Quinlan (1996) . The boot-strap sample is obtained by uniformly sampling instances from the given reduced dataset S i with replacement. From each sample set a classifier is induced. This procedure is carried out independently for each reduced dataset S 1 , y , S K , and next the global classifier is generated by aggregating classifiers repre-senting each sample set. The final classification of a new object x is obtained by the uniform voting scheme on h ij , where h classifier learned on j trial where j  X  1, y , T , i  X  1, the number of the bootstrap samples. In this case the voting decision for a new object x is computed as h  X  x  X  X  argmax where h ( x ) is the class label predicted for object x by ij th classifier. The details of the strategy are shown as Algorithm 1. Algorithm 1 . Bagging combiner strategy
Input : S i , with N i examples and correct labels; T -the number of bootstrap iterations; Output : h -classifier; Begin End AdaBoost combiner strategy  X  this strategy is based on the
AdaBoost approach ( Quinlan, 1996 ). Weak classifiers are constructed from the reduced datasets. The boosted classifier h is combined from all weak classifiers h ij , which are induced on the j th boosting trial ( j  X  1, y , T ) and based on the i th reduced dataset ( i  X  1, y , K ). In particular, the combined deci-sion is taken through summing up the votes of the weak classi-fiers h ij . The vote of each h ij is worth log  X  1 = b ij b ij  X  e ij =  X  1 e ij  X  is a correct classification factor calculated from the error e ij of the weak classifier h ij . Thus the voting decision for a new object x is computed as h  X  x  X  X  argmax The details of the strategy are shown as Algorithm 2. Algorithm 2 . AdaBoost combiner strategy
Input : S i , with N i examples and correct labels; T specifying the number of boosting trials; Output : h -classifier; Begin
End simple voting strategy  X  at the global level the set of all prototypes from different sites are further reduced in the feature dimension to obtain prototypes with features belong-ing to the subset of attributes A 0 , where A u  X [ K i  X  1 that the decision for new object x is computed by the classifier induced from the global set of prototypes S u  X [ K i  X  1 S each example is a vector of attribute values with attributes A 0 . The details of the simple voting strategy are shown as Algorithm 3.
 Algorithm 3 . Voting strategy Input : A i , S i ; Output : h -classifier; Begin
End hybrid feature voting strategy  X  after integrating local level solution features are selected independently by two feature selection techniques i.e. by the forward and backward sequential selection (FSS and BSS) ( Kohavi and John, 1997 ).
The ten cross validation approach (10CV) has been used. The learning and the validation sets are obtained by randomly splitting the global set of prototypes. In each of the 10CV runs the feature selection process is carried out. The final set of features is obtained through the unanimous voting mechanism. The details of the strategy are shown as Algorithm 4.
 Algorithm 4 . Hybrid feature voting strategy Input : A i , S i ; Output : h  X  classifier; Begin End
The second considered group of strategies includes strategies for deciding on the winning set of features. It has been decided to implement and evaluate the following strategies: static feature selection (Algorithm 5), dynamic feature selection (Algorithm 6).
 Algorithm 5 . Static feature selection
Begin End Algorithm 6 . Dynamic feature selection Begin
End while // solution manager  X  i-th local level For each solution manager do
End for
End 3.2. Experiment results and their discussion
Classification accuracy of the global classifiers obtained using the above described strategies have been compared with: results obtained by pooling together, without data reduction, all instances from distributed databases, into the centralized database, results obtained by pooling together all instances selected from distributed databases through the instance reduction only.
Generalization accuracy has been used as the performance criterion. The learning tool used was C 4.5 algorithm ( Quinlan, 1993 ).

The experiment involved three datasets  X  Customer (24,000 instances, 36 attributes, 2 classes), Adult (30,162, 14, 2) and Waveform (30,000, 21, 2). For the first two datasets the best known reported classification accuracies are 75.53% and 84.46%, respectively ( Asuncion and Newman, 2007; EUNITE, 2002 ). The above datasets have been obtained from Asuncion and Newman (2007) and EUNITE (2002) . The reported computational experi-ment was based on the 10 cross validation approach. At first, the available datasets have been randomly divided into training and test sets in approximately 9/10 and 1/10 proportions. The second step involved a random partition of the previously generated training sets into training subsets each representing a different dataset placed in the separate location. Next, each of the obtained datasets has been reduced. Then the reduced subsets have been used to compute the global classifier using the proposed strategies. The above scheme was repeated ten times, using a different dataset partition as the test set for each trial.
The experiment has been repeated four times for four different partitions of the training set into a multi-database. The original data set was randomly partitioned into 2, 3, 4 and 5 multi-datasets of approximately similar size.

For the combiner strategy based on bagging, the parameter T (number of bootstraps) was set to 3 and 5. Choosing such small values of T was inspired by good results obtained by Quinlan for the C 4.5 classifier with bagging ( Quinlan, 1996 ). In the case of the AdaBoost-based strategy the value of T (number of boosting rounds) was arbitrary set to 10.

All optimizing agents have been allowed to continue iterating until 100 iterations have been performed. The common memory size was set to 100 individuals. The number of iterations and the size of common memory had been set out experimentally at the fine-tuning phase. The whole process of searching for the best solution stopped when there were no improvements of the best solution during the last 3 minutes of computation.

The experiment has been repeated four times for the four different partitions of the training set into a multi-database.
The respective experiment results averaged over ten cross validation runs are shown in Table 1 . The results cover several independent cases. In the first case only reference instance selection at the local level has been carried out, and next, the global classifier has been computed based on the homogenous set of prototypes. The next cases concern the results obtained by the strategies for deciding on the winning set of features, i.e. the strategies for coordination between solutions at local sites, and by strategies for learning combiner classifiers.

It should be noted that data reduction in two dimensions (selection of instances and features) assures better results in comparison to data reduction only in one dimension, i.e. instance dimension, and that the above conclusion holds true indepen-dently from the combiner strategy used at the global level and from the strategy for the winning feature selection at the local level. It has been also confirmed that learning classifiers from distributed data and performing data reduction at the local level produces reasonable to very good results in comparison with the case in which all instances from distributed datasets are pooled together.

Additionally, it can be observed that combiner strategies produce reasonable to very good results compared with other cases presented in Table 1 . Out of the investigated combiner strategies the strategies based on the simple and hybrid feature voting have performed best. Next are placed the AdaBoost-based strategy, the Bagging-based strategy and the dynamic feature selection strategy.

For example, pooling all instances from distributed datasets assures classification accuracy of 73.32%( 7 1.42), 82.43%( and 71.01%( 7 0.8) for customer, adult and waveform datasets, respectively. On the other hand, the global classifier based on instance selection only assures classification accuracy of 75.21%, 87.1% and 80.67%. These results can still be considerably improved using the hybrid feature voting strategy assuring classification accuracy of 78.15%, 92.48% and 82.04%, respectively, for the investigated datasets.
 However, merging the two strategies as in cases G and J in Table 1 may further improve the classification accuracy. In the first case the AdaBoost algorithm was applied after the hybrid feature voting strategy that resulted in improving the results in the six cases. When the AdaBoost algorithm was applied to the results of feature selection coordinated by dynamic strategy, the improvement took place in three cases. Based on these results it can be concluded that the best results can be obtained by hybridization of different approaches to distributed learning when data reduction in two dimensions is carried on. In this case the hybridization is understood in the context of an applying more then one techniques of selecting the final set of features or constructing combiner classifiers or both.
 To reinforce our conclusions the experiment results, shown in Table 1 , have been used to perform the one-way analysis of variance (ANOVA), where the following null hypothesis was formulated: choice of the general strategy, when the data reduction in two dimensions is carried out, does not influence the classifier performance. It was established, at 5% significance level, that our hypothesis should be rejected in all cases. In addition, the Tukey test has confirmed that in case of feature selection at the local level there are no statistically differences between mean performances of the feature selection strategies. 4. Conclusions
The paper proposes an agent-based framework for the distributed machine learning. It has been assumed that the data reduction is carried out in two dimensions (i.e. selection of instances and features) at the local level of the distributed learning. Hence, strategy for constructing the combiner classifier at the global level and strategy for coordinating feature selection at the local level are needed. Several strategies of both types have been investigated and experimentally evaluated using three benchmark datasets. Experiment results allow to draw the following conclusions:
Quality of the machine learning outcome depends on the strategy used for the construction of the combiner classifier.
Quality of the machine learning outcome depends also on the approach used to obtain the homogenous prototypes at the global level of distributed learning.

The collaboration between agents working locally on the data reduction process can improve the distributed data mining quality and assure homogenous set of prototypes obtained from local sites.

The scope of the reported experiment does not allow to conclude that some of the investigated strategies always outperform the others. However, hybrid feature voting at the global level or dynamic feature selection strategy, possibly combined with the AdaBoost algorithm should be seriously considered as a potential winner strategy for solving the distributed data mining problems.

Future work will focus on evaluating other combiner classifier strategies in terms of classification accuracy and computation costs and on carrying out more extensive experiments with the use of additional datasets. So far, the experiment was carried out with the C 4.5 learning algorithm as the classification tool. In the future work other learning algorithms should be tested. The authors also belief that the proposed agents X  cooperation scheme gives possibility to develop and implement more advanced strategies for choosing the appropriate winning feature set common for all the distributed sites.
 Acknowledgements This research has been supported by the Polish Ministry of Science and Higher Education with grant for years 2008 X 2010. References
