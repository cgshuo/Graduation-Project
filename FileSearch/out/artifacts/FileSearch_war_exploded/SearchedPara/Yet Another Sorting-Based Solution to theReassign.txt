 Web Search Engines are always facing serious performance challenges with the rapid growth of web data. First, full text index for tens of billions of pages needs to be stored in disk or main memory, which requires a huge amount of storage; sec-ond, tens of thousands of queries should be processed in hundreds of milliseconds. Inverted file has been proved to be the bes t way to index large and variable-length web data compared to signature file and bitmap in [1]. Inverted file is composed of the lexicon and the posting lists. The lexicon contains terms tokenized from the documents. For each term in the lexicon, a posting list is associated containing information about document frequencies, document identifiers, term frequencies and positions. The general form of inverted file is as follows: where f t i represents the document frequency of term t i (number of documents in which it appears), and d i 1 ,d i 2 ,...,d if t taining t i . For simplicity here, we leave out the frequencies and positions in the posting list.

The scale of data has caused a critical dependence on compression of the inverted file. Index compression not only reduces the storage space, but more importantly, gives chance to keep a larger fraction of inverted file in main memory to improve overall query processing throughput[2].

For efficient storage, inverted files use the d-gap representation instead of the original document identifier, because most compression regimes achieve higher compression ratio for smaller integers. We sort the document identifiers (do-cIDs) in each posting list by ascending order, and then take the differences among successive identifiers (d-gap). Finally, we encode the d-gaps with certain compression regimes introduced in [3,4,5,6,7,8,9].
 Meanwhile, researchers propose another approach called Document Identifier Reassignment, to enhance the compression ratio of inverted files. Generally, iden-tifiers are assigned to documents by indexed order, while the docID reassignment is concerned with reassigning docIDs to optimize the distribution of d-gaps and maximize the compression ratio of the re sulting inverted file. A good docID as-signment stated in [10] is to create loca lity by assigning consecutive docIDs to similar documents; this produces clusters of many small values interrupted by a few large values and results in better compression.

Prior work showed that compressed index size can be substantially reduced through reassignment of docIDs. The first solution to this problem named B&amp;B is proposed in [10] by first building a similarity graph from inverted index and then recursively splits the whole graph into small subgraphs until every sub-graph becomes a singleton. Finally, identifiers are reassigned to documents by depth-first traversal of the graph. Another approach proposed in [11] consid-ers the problem as Traveling Salesman Problem (TSP). This approach tries to find a traversal from the document similarity graph that maximizes the sum of distances (document similarity) betw een consecutive documents. The maximal traversal gives the optimal order of docIDs.

However, the B&amp;B and TSP-based solutions are limited on small data sets; they operate on a dense graph with O ( n 2 )edgesfor n documents. Based on the assumption that similar documents share a large prefix of their URLs, [12] just simply assign docIDs to web pages according to the alphabetical or-der of the URLs (URL sorting-based). But for documents without URLs (e.g., FBIS or LATimes collections) or that URL similarity is irrelevant to docu-ment similarity (e.g., Wikipedia, Twitter), URL sorting-based approach does not apply.

Despite a number of recent publications on this topic, there are still many open challenges. Our goal in this paper is to illustrate a new solution to the reassignment of document identifiers. Our main contributions are as follows: 1. We present another sorting-based approach named TERM sorting that scales 2. We study three different ways of sorting the terms in our approach. Ex-3. We evaluate our approach with TSP-based approach on the small data sets 4. We compare our approach with URL sorting-based approach on the large The rest of this paper is organized as follo ws: Section 2 provid es some technical background and reviews related work. In Section 3 we introduce our TERM sorting-based approach in detail. Section 4 shows the results of our evaluation. Section 5 provides conclusion and future work. In this section, we first outline several known index compression regimes used in our later experiments, then we disc uss related work on the reassignment of document identifiers. 2.1 Index Compression Techniques The basic idea of index compression is to compress a sequence of integers (d-gaps) into smaller size. We now provide brief description of some compression regimes to keep the paper self-contained.
 Variable-Byte Coding(VB): VB coding [4] represents an integer in variable bytes, where the highest bit of each byte called status bit indicating whether the next byte follows the current one, and the rest 7 bits in binary encoding. Simple9(S9): The Simple9 coding [13] divides 32-bit word into 4 status bits and 28 data bits, where the 4 status bits can represent 9 different situations of the data bits: 28 1-bit numbers, 14 2-bit numbers, 9 3-bit numbers (1 bit unused) or 7 4-bit numbers and so on. Anh and Moffat extend 32-bit to 64-bit (named S8b ) in [14] with 4 status bits indicating 16 cases of the 60 bits available for data.
 PForDelta(PFD): This compression regime [6] supports fast decompression while also achieving a good compressed size. It first determines a value b so that most of the values to be encoded (say, 90%) are less than 2 b and thus can fit into a fixed bit field of b bits each. The remaining values, called exceptions which are larger than 2 b , are coded separately. If we compress 32 values each time, then we need (32  X  b ) bits to store ordinary values, followed by exceptions compressed in fixed-length coding. For the original positions of the exceptions, we store the offset value to the next exception. Ahead of the fixed bit field, there is a value indicating the offset of the first exception. 2.2 Document Identifier Reassignment The compressed size of the whole inverted file is a function of all the d-gaps being compressed, which greatly depends on how we assign the docIDs.
A common assumption in prior work is that docIDs should be assigned such that similar documents (i.e., documents with a lot of common terms) are close to each other. We classify the related work into three categories: 1) cluster-based, which partitions the documents into clusters by similarity, then assigns consecutive docIDs to documents in the same cluster, 2) TSP-based, which finds the maximal distance by traversing the document similarity graph, then assigns docIDs according to the trav ersal order and 3) URL sorting-based, which assigns docIDs to documents by the alphabetical order of the URLs.
 Cluster-Based: B&amp;B algorithm proposed in [10] starts from a previously built inverted file, and then constructs a docu ment similarity graph (DSG) where the vertices correspond to documents and the edges weighted by the cosine similarity between each pair of documents. The B&amp;B algorithm recursively splits DSG into smaller subgraphs, which represent smaller subsets of the collection, until all subgraphs become singleton. The identifiers are finally reassigned according to the depth-first visit of the resulting tree.

On the basis of B&amp;B, a lightweight k-means-like cluster algorithm is pro-posedin[15].Itscansthewholecollectionktimes,andateachtimechooses the longest document as the center, then adds the remaining documents to the center according to the Jaccard similarity. Finally, we get k clusters with ( N/k ) documents each. Identifiers are reassi gned to the documents according to the order added to these clusters.
 TSP-Based: In [11], Shieh et al. proposed an approach based on the Travelling Salesman Problem (TSP) which attempts to find a tour of DSG to maximize the sum of all the travelled edge weights. The experiments show a great improvement of compression ratio.

To improve the efficiency of TSP, Blan co et al. introduced SVD to reduce the dimensionality of DSG in [16]. However, SVD technique is still quadratic in the number of documents.

Ding et al. proposed a new framework in [17] for scaling TSP-based approach by Locality Sensitive Hashing, which obtains a reduced sparse graph. This tech-nique achieves improved compression ratio while scaling to tens of millions of documents. However, for large collections it still takes a long time to hash and sample from the whole DSG.
 URL Sorting-Based: This approach is proposed by Silvestri in [12], which sim-ply sorts the documents by alphabetical order of their URLs, and then reassigns docIDs according to the new order. This is the simplest and fastest approach, and performs well for large web collections with almost unlimited scalability. 3.1 Motivation In Section 2, we discussed three categories of approaches to the problem of docID reassignment. Of all these approaches, the TSP-based approach achieves the best compression ratio most of the time; however , it is limited to fairly small data sets. The URL sorting-based approach performs not as well, but it can scale to large collections and performs well in many co mmercial search engines. However, the URL sorting-based approach is only applicable to web pages where the similarity between pages can be inferred from the URLs.

Inspired by the URL sorting-based approach, we propose the TERM sorting-based approach to the reassignment of docIDs which concerns the length of the posting list. The advantages of our solution is as follows: 1) it sorts the documents by terms instead of the URLs and performs better than URL sorting-based approach, 2) it takes much less space and time than TSP-based approach while achieving comparative compression ratio and 3) it is applicable and scalable for large collections. 3.2 Methodology Given the posting list in Equation (1), we first sort the document identifiers in ascending order and then get it X  X  d-gap form: Since most coding regimes encode an integer x with O (log( x )) bits, let  X  ( t i )be the required bits of t i  X  X  posting list in Equation (2), Our goal is to minimize t the best permutation which optimizes the distribution of overall d-gaps and gains most in compression ratio.

The simplest way is to exhaustively permute the N documents and find the best reassignment which achieves the best compression ratio. But the time com-plexity of this brute force method is O ( N !) which is not feasible, especially for a large N . It is proved in [18] that finding the best permutation is an NP-complete problem.

However, from Equation (3) we observe that:  X  term t i with different document frequency (DF) has different impact on the  X   X  ( t i ) is composed of continued product of d-gaps which would be decreased Based on the above observations, we propose an approximate polynomial-time solution to the docID reassignment problem, named TERM sorting-based solu-tion. At the first step, we sort the terms in every document by DF; and then sort the documents by the presence of terms. F inally, we reassign docIDs according to the new order of the sorted documents. 3.3 Choosing Order Of Terms As discussed in previous subsection, we need to arrange the terms in every document before sorting the documents. In this paper, we study three different ways of sorting the terms. In the simplest way, we use the original order of terms (TERM-ORIGIN) in every documen t. Besides, we sort the terms in each document by descending order (TERM-DESC) and ascending order (TERM-ASC) of DF.
 TERM-ORIGIN: Simply considered as a random order of terms.
 TERM-DESC: Assuming that the terms with larger DF (i.e., longer posting list) have more impact on the compression ratio, documents should be reordered according to the presence of these ter ms first to generate more 1-gaps in the corresponding posting lists.
 TERM-ASC: Just like the above one, but emphasizes on optimizing the shorter posting lists of terms with smaller DF.

For example, given a 4  X  4 matrix in Fig. 1, we have four terms t 1 ,t 2 ,t 3 and t 4 tokenized from documents d 1 ,d 2 ,d 3 and d 4 .Element e ij in the matrix represents presence(= 1 ) or absence(= 0) of term t i in document d j and numbers in brackets indicate the DF of each term. Fig. 2 illustrates the TERM-DESC ( left )orderandTERM-ASC( right ) order of the terms. 3.4 Sorting the Documents After sorting the terms, we then sort the documents by the presence of terms to generate more 1-gaps, following the observations in subsection 3.2. Consider the matrix in Fig. 1 and Fig. 2, the document sorting results are illustrated in Fig. 3.
After sorting the documents, we reassign the identifiers to the documents according to the new order. For exam ple, in Fig. 3 we get the new order of documents under TERM-DESC ( middle ): d 2 ,d 4 ,d 3 and d 1 . During the docID reassignment procedure, new docID 1 , 2 , 3 , 4 are reassigned to d 2 ,d 4 ,d 3 and d 1 accordingly. 3.5 Time Complexity Analysis Previous studies like TSP-based and B&amp;B approach focused on finding clustering property of the n documents through computing the similarities between docu-ments, which takes O ( n 2 ) time and occupies large additional space for document similarity graph [15,10,11,17].

We avoid the computational bottlenec k of constructing document similarity graph by just sorting the terms and the documents in place. Our TERM sorting-based approach consists of two steps: 1) so rting terms and 2) sorting documents. It is quite difficult to calculate the exact time complexity under real situations. To estimate the time complexity, here we simply assume that all the documents are of the same length | D | , i.e., average length of all the documents.
At step 1, sorting terms in n document requires O ( n  X  | D | X  log | D | ) time on average using quicksort. Step 2 requires O ( n log n ) comparisons between docu-ments using quicksort and each comparison takes O ( | D | ) time on average, so the time complexity of step 2 is O ( | D | X  n log n ). As the | D | is around hundreds and n scales to thousands or millions or even billions in real data, the time complexity of our TERM sorting-based approach is O ( | D | X  n log n ). In this section, we evaluate the performance of our TERM sorting-based ap-proach. Our experiments are set up as follows: first, we introduce the experi-mental setup; then, we compare our approach with TSP-based approach and URL sorting-based approach in both compression size and run-time for the doc-ument reassignment procedure. 4.1 Experimental Setup In our experiments, we use four data sets. Table 1 summarizes the statistics: the size of collection, number of terms, documents and postings.
  X  FBIS: The TREC FBIS coll ection consists of 130 , 471 documents from For- X  LATimes: The TREC LATimes collection consists of 131 , 896 documents  X  WT2g: A general Web crawl, used by the TREC 1999 Web track.  X  Wiki: This is a dump of English version of Wikipedia taken on April 11, We ran our experiments on a Xeon 2GHz PC with 128GB of main memory. The operating system was Linux and all the experimental code was written in C++. We didn X  X  introduce stemming or stop words in our experiments. Since our approach explicitly try to optimize docID compression which do not affect much frequency and position compression, we focus on total index size due to docID reassignment throughout this section. 4.2 Reducing in Sum of Logarithm In this subsection, we sum up the logarithm of the d-gaps in each data set and give a quantitative method-independent evaluation of the benefits that accrue from the docID reassignment. Fig. 4 shows the results in four data sets mentioned above. It is showed that docID reassignme nt significantly reduces the sum of the logs of the d-gaps when the terms are sorted in descending order. It is worth noticing that docID reassignment does not achieve improvements in FBIS. We think this is because the optimization th rough reassignment is limited compared to the scale of posting lists. 4.3 Comparing with TSP-Based Approach In this subsection, we compare five reassignment methods: an original order of docIDs without reassignment (ORIGIN), TSP-based approach (TSP), TERM-ORIGIN, TERM-DESC and TERM-ASC based on our TERM sorting approach.

Table 2 and Table 3 show the compression size and run-times of the five approaches in four compression regimes on FBIS and LATimes.
We show the number of bits per docID in VB, S9, S8b and PFD. The last column shows the run-time to do the reassignment required in each approach. The best reassignment for each compression regime is bolded.

From the experimental results we see that: 1) TERM sorting-based approach performs better with TERM-DESC, 2) TSP gets better compression ratio in VB, S9 while TERM-DESC performs better in S8b and PFD and 3) TSP takes about 22 hours for FBIS and 24 hours for LATimes, while our TERM sorting-based approaches only need about 5 seconds and 8 seconds respectively.

To the best of our knowledge, PFD is widely used in many commercial search engines for its fast decoding speed and high compression ratio. Considering the compression ratio and run-time in both reassignment and decoding, we think TERM-DESC is a better choice for FBIS and LATimes. 4.4 Comparing with URL Sorting Approach In this subsection, we compare our TERM sorting-based approach with URL sorting-based approach (URL) on WT2g and Wiki data sets. (The TSP-based approach is not referred here because it is limited to small data sets.)
Table 4 shows the results on WT2g data set. As discussed in [17], URL-based approach performs not so well for the less densely sampled set of web pages like WT2g and outperforms the others only in S9 and S8b. Again in PFD, TERM-DESC improves the compression ratio by 3.7%.
 Table 5 shows the results on Wiki data set. From this table, we see that TERM-DESC improves the compression ratio by 2 . 5%, 9 . 6%, 10 . 6% and 8 . 5% in VB, S9, S8b and PFD respectively, comp ared to the URL-sorting based method. In this paper, we propose the TERM sorting-based approach to the reassignment of document identifiers.We compare our approach with the TSP-based approach and show that the compression ratio of our approach is comparative with the TSP, with run-time significantly decr eased. Compared with the URL sorting-based approach, our approach improves the compression ratio up to 10 . 6%.
However, there are still several open questions for future research. First, the order of the terms still needs further study. Second, current studies focus on maximizingthenumberofsmallgaps,anditisanopenquestiononhowto minimize the compressed size of inverted fi le which also contains term frequencies and positions. We leave this to our future work.
 Acknowledgment. This work is supported by the National Science Foundation of China under Grant No. 61070111 and the Strategic Priority Research Program of the Chinese Academy of S ciences under Gra nt No. XDA06030200.

