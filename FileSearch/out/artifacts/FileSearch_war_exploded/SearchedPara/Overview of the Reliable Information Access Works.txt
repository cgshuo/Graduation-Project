 Donna Harman  X  Chris Buckley Abstract The Reliable Information Access (RIA) Workshop was held in the summer of 2003, with a goal of improved understanding of information retrieval systems, in particular massive cross-system failure analysis on 45 of the TREC topics and also performed cross-system experiments on pseudo-relevance feedback. This paper presents an overview of that workshop, along with some preliminary conclusions from these experiments. Even if this workshop was held 6 years ago, the issues of improving system performance across all first widely published full papers for the workshop.
 Keywords Information retrieval Relevance feedback Failure analysis 1 Introduction The field of information retrieval has always closely modeled the application of a person wide variety in the types of information that users seek, but a huge variation in how those evaluations of information retrieval systems must mirror this in test collections by having numbers of test questions (called  X  X  X opics X  X  in TREC).

Despite the wide variety in the topics used in TREC, the graph in Fig. 1 shows that the average retrieval effectiveness approximately doubled in the first 7 years of TREC. This means, for example, that retrieval engines that could retrieve three good documents within the top 10 documents in 1992 were now likely to retrieve six good documents in the top 10 documents retrieved for the same search. The figure plots retrieval effectiveness for one well-known early retrieval system, the SMART system of Cornell University. The SMART were comparable with it, so the graph is representative of the increase in effectiveness for the field as a whole.

Figure 1 also shows a flattening of the improvements by TREC-7. Note that in general this flattening appeared for all of the systems and there was considerable discussion as to the cause of this performance ceiling. One issue is simply that researchers put more effort into the new tasks being run in the later TRECs, such as cross-language retrieval or web searching. But there was agreement that a major factor in this flattening or ceiling effect is work well for one topic do not work well for others, leaving no improvement in perfor-mance on average. In the early TRECs, new techniques such as better weighting and pseudo-relevance feedback improved performance on most topics, therefore improving the averages. However, at some point, there were no new ideas that seemed to improve performance for the majority of topics X  X ence the flat curves.

Topic variation is reflected in many ways such as: 1. a wide variation across topics in the average precision score for the best performing 2. a wide variation in performance across topics for a given system (or system variant), 3. a wide variation in performance across topics of the effectiveness of particular devices
Figure 2 clearly illustrates the first two of these variation problems. First, the perfor-mance of the best system for each of the 50 topics varies from almost perfect performance to an average precision of barely 0.1. Past experiments (Voorhees and Harman 1997 ) have shown that this performance variation is not correlated with the number of relevant doc-document set being searched, and the retrieval system. When specific systems are exam-ined, a second source of variation can be seen in Fig. 2 . The results for the OKAPI system in TREC-8 show a wide variation in performance scores across the different topics, and PIRCS system, shows the same types of variations, but with performance different than both the best system and the OKAPI system.

Table 1 illustrates the third example of topic variation. The table shows the number of description only and title only) for three different systems. Further examination of the data reveals that topics that work best at a particular length for one group did not necessarily work best at that length for the other groups.

Because retrieval approaches can work well on one topic but poorly on another, determination in advance of which approach would work well for a given topic would allow tailoring of the systems to each topic. Unfortunately, despite many efforts (Cronen-Townsend et al. 2002 ; Yom-Tov et al. 2005 ), no one knows how to choose good approaches on a per topic basis. The major problem in understanding retrieval variability is that it is caused by a number of factors. There are topic factors due to the topic statement itself and to the relationship of the topic to the document collection as a whole. There are system dependent factors including the specific algorithms and implementation details. In separate out the topic variability factors from the system variability.
The goal of the Reliable Information Access (RIA) Workshop was to understand the retrieval variability. Comparative analysis of the different systems was to enable system workshop was sponsored by ARDA in their summer workshop series. 2 Workshop description focused; additionally experiments needed to concentrate on techniques that are common to statistics in some manner as a core of their systems, with the common technique of matching the words in the input questions against words in the documents. In general this implies that improvements must come from either re-weighting the importance of existing word matches, or from adding new words to the query that can be used for matching. Thus query expansion has been a central focus of statistical information retrieval throughout its performance on average . However while query expansion works well on average, there are several different mechanisms that could cause this improvement. Systems are in effect tuned to emphasize some choice(s) of these mechanisms, such as different term weighting methods, different query expansion methods, etc.

In a pre-workshop meeting in March of 2003, it was decided to focus the workshop investigation on one type of query expansion, that of pseudo-relevance feedback (also ranked documents are relevant and uses these documents in the feedback process. The documents can then be mined for expansion terms or for re-weighting of existing terms or both. Between March and June, the various systems were installed at MITRE (the location of the workshop) and discussion continued on the details of what would be done during the the relationship of improved retrieval as input to a question-answering system. This part of the workshop is not further covered here; see (Collins-Thompson et al. 2004 ) for more on this.
The final organization of the RIA workshop featured two approaches to the investiga-tion of system and topic variability X  X  massive comparative failure analysis and a series of tightly controlled experiments examining variants of pseudo-relevance feedback.
For the massive comparative failure analysis, each system contributed one representa-retrieved documents was done. The analysis goal was to discover why systems fail on each topic. Were failures due to system dependent problems such as query expansion weak-nesses or system algorithm problems, or were the problems more inherent to the topic? For each topic, what would be needed to improve performance for each system? How could this be predicted by the system?
For the controlled set of experiments, the systems performed a large number of vari-ations in the pseudo-relevance feedback technique. In some sets of experiments the sys-tems changed their own tuning parameter settings. In other experiments each system used as the source of expansion terms documents from each of the other systems, or used the actual expansion terms determined by other systems. The overall goal of the analysis was expansion efforts on each topic.
For each of these two approaches the workshop participants collected enormous amounts of data. Only a small portion of the analysis of the data could be completed during the workshop. The preliminary analysis that has been done has already produced a number ( http://ir.nist.gov/ria ), and hopefully will enable useful research for years to come.
By its very nature, the RIA workshop required participation from a large number of groups and experts. Bringing together seven of the top research systems in one location with both high-level theoretical expertise and also practical system expertise was difficult, especially given the 6-week duration of the workshop. There were two groups of partic-ipants; the senior experts who generally were present for 1 X 2 weeks of the workshop spread out over several trips, and the graduate students who for the most part were at the workshop for the full 6 weeks. Altogether, there were 28 people from 12 organizations that participated. The seven systems represented at RIA were CMU (from Carnegie Mellon University); City (from City University, London); CLJ and FullCL (from Clairvoyance Corporation); Sabir (from Sabir Research); UMass (from University of Massachusetts at Amherst); Albany (from University of New York at Albany); and Waterloo (from University of Waterloo), with the workshop being coordinated by NIST and held at MITRE Corporation.

The Appendix gives the organizations, people, and software that contributed to the workshop, along with detailed descriptions of each system as written by the participants. Note that this was an open workshop environment where everybody was constantly con-sessions and one or two of the system experiments.
 followed by summaries of each of the controlled experiments. Section 5 is a summary of the data that was collected and that is available on the website. Section 6 gives some very others. The paper concludes with a retrospective summary of lessons learned in terms of how to organize and run such a workshop, and also a set of suggested experiments to continue this work. 3 Massive comparative failure analysis The failure analysis investigation was an attempt to discover why current research infor-improve effectiveness. What follows is a short summary; readers are referred to the paper meeting it was decided that all groups would submit a standard retrieval run that in some what could be in the run as long as it was completely automatic. These runs became the basis of the failure analysis.

This failure analysis was a major activity of the workshop; with 90 min to 2 h per day cedure was adopted, using a wide variety of tools. The major tool was the Waterloo User retrieved in a top set, or that were non-relevant, but were retrieved in the top set. Given the large time requirements for failure analysis (from 11 to 40 person-hours per topic), it was obvious that not all 150 topics could be examined (only 45 topics were actually finished). It was decided to focus on topics where the systems in general scored below the overall MAP average and where there was a large variance among system scores.

The first conclusion of the failure analysis was that the root cause of poor performance retrieving different documents in general, all systems were missing the same aspect in the top documents. The other major conclusion was that for well over half the topics studied, current technology should be able to improve results significantly. This suggests it may be more important for research to discover what current techniques should be applied to conclusions, see the paper in this issue (Buckley in press). 4 Controlled retrieval experiments 4.1 Design of experiments The retrieval experiments in the RIA workshop were a large investigation into how dif-ferent systems vary while performing a single query expansion task, that of pseudo-relevance feedback. Pseudo-relevance feedback was chosen as the target task for several reasons. First, it is known to have a high degree of topic variance; within any one system it works very well on some topics but hurts performance on other topics. Most systems find a mild average benefit to the use of pseudo-relevance feedback. Secondly, most systems have used it at some point in their research; thus the implementation effort required for experimentation was minimized. And, finally, it has a number of important parameter settings that systems in practice set to different values, and that can be changed easily.
In a typical pseudo-relevance feedback task, systems automatically expand the original query by adding terms that occur in documents (or passages) that the system determined were closely related to the query. On each topic, a system 1. Performs an initial retrieval with terms from the text of the original topic, 2. Without any user looking at them (thus  X  X  X seudo-relevance X  X ), the system assumes that 3. The system chooses N terms from the top X documents and adds them to the original 4. All terms are reweighted, 5. The new expanded query is re-run against the entire document collection, and a 6. In a live system, these documents would then be given the user. In the experimental
For these retrieval experiments, variations of each of the possible parameter choices were studied. These included the number of documents to draw expansion terms from ( X ), the number of expansion terms to add ( N ), the choice of the expansion documents, and the choice of the expansion terms. There is an inherent system performance of each system due to their weighting, indexing, and matching algorithms. The major goal of the analysis was dependent variability. Different expansion approaches work well on different topics. If it is determine the success of an expansion approach and each system can adjust its approach and parameters based upon those topic dependent factors.

Somewhat more formally, evaluation scores can be explained in terms of the topic, the inherent system, and the run (system parameter settings). the effect of the interaction between topic and run; est , the effect of the interaction between system and topic; and estr , the interaction of all three parameters, which is ignored here.
In the basic sets of experiments, there were altogether 150 topics, seven systems, and about 100 different runs for a total of 105,000 data points. One goal of the experiments was to look at etr , the interaction of the topic and run. This could be used to classify topics according to what sort of approach and parameters should be used. Ideally, this classifi-cation could be matched to a classification based on topic information alone. In that case, there would be an effective decision procedure for how to choose the approach and parameters on a per topic basis.

Another major goal of these retrieval experiments was simply to increase the under-standing of what is happening with query expansion and pseudo-relevance feedback. Most research groups have experimented extensively with pseudo-relevance feedback at some point or another, but because pseudo-relevance feedback is so topic and system dependent, it has been very hard to analyze why it works or doesn X  X  work on particular topics. Most groups have been content to just optimize for maximum average performance.

When query expansion improves performance, it tends to be because one or more of the following is added: 1. better weighting to original query terms 2. synonyms 3. one or two good related words 4. a large number of related words that establish that some aspect of the topic is present 5. specific examples of general original query terms topics but not to other sets. Until it is known how important each of these effects is, the systems cannot adjust to improve expansion performance. The goal here was to understand for a system what worked for individual topics as compared to all other approaches that much easier to compare against other system results than to attempt to judge whether an approach succeeded or failed on some absolute basis. 4.2 Brief descriptions of each experiment There was very little time for analysis of the experiments during the workshop, but Readers should refer to these publications for more information.

Each experiment listed below includes a brief description, the experimental goal, the leader and the participating systems, the basic methodology, a summary of the results (and reference to other publications on these results), and some suggestions for further analysis. Note that these suggestions were made at the time of the experiments and therefore represent excellent leads into further research.

TREC data ( http://trec.nist.gov ) was used in the workshop, with most of the work being done with the 150 topics created for the ad hoc tracks in TRECs 6, 7, and 8 (topics 301 X  450), against the TREC disks 4 and 5 (without the Congressional Record sub-collection which was used only in TREC 6). This topic set is usually considered the  X  X  X est X  X  one for experimentation, both because the topic generation methodology used in TREC was stable by this point and because it is the only set with 150 topics against the same data. Note that additional runs were made as part of the database collection for other sets of topics (see each system using their  X  X  X ormal X  X  stopword and stemming techniques. 4.2.1 bf_base  X  Description: Basic investigation of pseudo-relevance feedback  X  Goal: Establish whether pseudo-relevance feedback works for the participating systems  X  Leader: Andres Corrada-Emmanuel  X  Participants: All 8 systems (2 from Clairvoyance)  X  Methodology: Perform 4 runs per group:  X  Results and Comments:  X  Future Analysis: none suggested at the workshop 4.2.2 bf_numdocs  X  Description: Vary the number of documents from which added terms are extracted in a  X  Goal: Along with bf_numterms, one of the two major experiments in pseudo-relevance  X  Leader: Jesse Montgomery  X  Participants: All 8 systems  X  Methodology: Perform 36 pseudo-relevance feedback runs, expanding by 20 terms  X  Results: The short paper presented at SIGIR2004 (Montgomery and Evans 2004 )  X  Future Analysis: 4.2.3 bf_numdocs_relonly  X  Description: Vary the number of potential documents from which added terms are  X  Goal: This is a paired experiment with bf_numdocs. The goal was to determine how  X  Leaders: Rob Warren, Ting Liu, David Evans  X  Participants: All 8 systems  X  Methodology: Perform 36 pseudo-relevance feedback runs, expanding by 20 terms  X  Results: This is an upper-bound experiment. Among other things, it simulates having  X  A short paper at SIGIR2004 (Warren 2004 ) discussed the following additional results.  X  Future Analysis: It would be interesting to investigate if there is any way to 4.2.4 bf_numterms  X  Description: Vary the number of terms added to the original query by pseudo-relevance  X  Goal: Along with bf_numdocs, one of the two major experiments investigating pseudo- X  Leader: Paul Ogilvie  X  Participants: All 8 systems  X  Methodology: Perform 37 pseudo-relevance feedback runs with expansion based on the  X  Results: Average behavior was different for each of the systems. This issue contains a  X  Future Analysis: This is the major experiment which needs to be understood on a per 4.2.5 bf_pass_numterms  X  Goal: Understand how passage retrieval differs from document retrieval in the  X  Leader: Zhenmei Gu, Ming Luo  X  Participants: 4 systems X  X ity, CMU, Waterloo, FullClarit  X  Methodology: The same methodology as the bf_numterms experiment, except each  X  The FullClarit and Waterloo systems already expand queries by considering passages;  X  Results: Both CMU and City got very mild average improvement (1 X 2%) over the  X  A short paper presented at SIGIR2004 (Gu 2004 ) added the following observations: 4.2.6 bf_swap_doc  X  Description: Each system used the top documents found by initial runs of other systems  X  Goal: Determine how much the initial retrieval strategy of each system affects whether  X  Leader: Tom Lynam  X  Participants: All 8 systems  X  Results and Comments: A separate paper in this issue (Clarke et al. in press) provides  X  Future Analysis: The effects of swapping documents is complex: there is a need to look 4.2.7 bf_swap_doc_term  X  Description: Each system used both top documents and expansion terms found by other  X  Goal: Determine how much term selection algorithms of each system affect whether  X  Leader: Tom Lynam, Ting Liu  X  Participants: 7 systems participated (CLJ did not)  X  Methodology: This was a challenging experiment to perform (and explain). Please see  X  Future Analysis: There has been no topic analysis or categorization done for these runs. 4.2.8 bf_swap_doc_cluster; bf_swap_doc_hitiqa  X  Description: These were the first two of three small experiments in which the source of  X  Goal: Investigate the effect that criteria other than initial retrieval have on expansion  X  Leaders: Jesse Montgomery (bf_swap_doc_cluster), Sean Ryan (bf_swap_doc_hitiqa)  X  Participants: 5 systems X  X lbany, City, FullClarit, Sabir, Waterloo  X  Methodology (bf_swap_doc_cluster): This experiment was an upper bound experi- X  Results (bf_swap_doc_cluster): The results are shown in Table 4 . The most interesting Table 6 shows a short summary of all of the controlled experiments. 5 Run database One of the major resources for future research produced by the workshop is the database of runs. This database is now stored on the NIST system ( http://ir.nist.gov/ria ) and a paper in this issue (Soboroff in press) describes the web site in detail.

Each group produced well over a hundred evaluated retrieval runs on the standard collection of 150 topics used in TRECs 6, 7, and 8, as described in the previous section. Then the major experiments were all rerun (replicated) for each group on the TREC 5 ad hoc task, about 95 runs. In addition, 2 key experiments (bf_numdocs and bf_numterms, TREC 1, 2, 3, 4. Finally, one run was made for each group on the merged document collection formed from the news articles in TRECs 1 X 8, using all available topics (1 X 450). Altogether, there are 4,088 run results in the database, taking up over 22 gigabytes of disk space (Zhenmei Gu and Luo Ming were responsible for the run replications).

The replicated runs have not yet been examined in any detail; that lies in the future. The main purpose of the replicated runs was to validate the experimental analysis done on the dependent on the particular topics and documents of the standard collection, but hold true on other collections as well. In addition to the validation purpose, the replicated runs are themselves useful for research as described below.

The primary difficulty in studying topic and collection variability has been the fact that evaluated retrieval runs from a single version of a system on large numbers of topics have not been available. The 50 topics in a typical TREC experiment run on a single collection have not been sufficient. The results from the 400 topics run here will provide the first good represent the entire universe of topics, especially given the rather stylized nature of TREC topics, but it is enough to investigate how topics group together, both in their character-istics and in their resulting search behavior.

The runs done for the merged document collection (TRECs 1 X 8 news articles) should be a useful resource for research in themselves, even though there are only 6 runs total. The standard pseudo-relevance feedback approach (bf) for each group was used to retrieve the available for each topic; only the documents from the two (out of five total) volumes of the TREC disks used during the year the topic was introduced were ever judged. Research that can be done using these runs includes  X  Does retrieval improve when documents from outside the target collection are used for  X  Does the ranking of systems for ad hoc retrieval on the same document collection agree  X  Can a valid evaluation methodology be devised for comparing runs when there is only 6 Preliminary experiments in topic categorization One of the major goals for the workshop was to understand how topics differ from each other, and how this affects system performance. An initial approach to this, unfortunately not even started until the final week of the workshop, was to automatically assign topics to categories based upon performance scores and other features. What follows are some initial experiments and some very preliminary results that are only meant to suggest further work.
For these experiments, each of the topics were  X  X  X cored X  X  based on various features, such as those below. Note that some of these scores are system-dependent and therefore there will be a topic score for each system. 1. Non-relevance-dependent features: 2. Relevance-dependent features: 6.1 Experimental method For the purposes of this initial investigation, the interest was in the extremes of scores for each feature. Was the behavior of the topic different for those topics which were given a high score for the feature, as opposed to those topics given a low score? Given the feature score for each topic, the 150 topics were divided into three categories:  X  Positive: The top 30 topics according to the feature score  X  Negative: The bottom 30 topics according to the feature score  X  Neutral: The remaining (90) topics
Some of the more natural measures, such as MAP scores, were system dependent as well as topic dependent. This could have been handled by averaging the measure across system dependence was handled by a voting mechanism in a two step process. 1. Step One: For each system, divide the topics into the three above categories. 2. Step Two: Vote on the above categorization among the systems (normally there were 7
The parameters X and Y were chosen by hand on a per feature basis to give roughly 30 topics in each of the PositiveVote and NegativeVote categories. 6.2 Categorization experimental results There were a total of 20 categorization experiments done, with 14 investigated in some detail, including one based upon the manual topic failure analysis. All of these experiments and the data are available from the web site.

Much more work needs to be done, but several interesting results have already been discovered. The following result discussions look at the intersection of two categorizations and concentrate on correlation between the Positive (or PositiveVote) categories defined by two different feature scores. 6.2.1 Similar document rankings among all systems versus pseudo-relevance feedback The document rankings for each topic for the 8 standard runs were compared against each other by using the  X  X  X nchormap X  X  measure. This (newly defined) measure is a general, asymmetric, pairwise ranking comparison measure that emphasizes the top elements in the two rankings. Anchormap computes the similarity of a pair of system retrieval rankings in the following manner. The top X (here 30) documents of Ranking A are used as the only relevant documents to calculate a MAP score for Ranking B. If those top documents of A are near the top of B, then anchormap will be high and the rankings are considered similar. Anchormap is a general measure, but was originally a measure to specifically look at how the top X documents used for feedback in the initial run of a pseudo-relevance feedback experiment are dispersed throughout the ranking for the feedback run.

In this particular categorization of topics, anchormap was used in its general form, computed over the 56 pairs of feedback runs for the 8 systems, and averaged for each topic. The topics were then sorted by this average anchormap score, and divided into Positive, chormap were compared against the categories produced by the top MAP scores. The Pearson correlation between the topics in the Positive groups was an extremely high 0.557, i.e., the topics for which the systems found the same top documents were indeed the topics that the systems got the best scores on. Out of the 30 topics with the most similar rankings, 19 of them were in the top 26 highest scoring topics and 0 topics were in bottom 24 scoring topics. Conversely, of the 30 topics with least similar rankings, 0 were among the top MAP systems or approaches get similar top documents, then the topic can be considered easy and standard techniques should work well. 6.2.2 Similar rankings among all systems versus pseudo-relevance feedback improvement This categorization comparison was the same as before except instead of comparing an-chormap similarities against the top scoring topics, they were compared against the topics for which pseudo-relevance feedback improved the most. Here the correlation among Positive categories was a very high 0.327. This would indicate that if systems or approaches get similar documents, then pseudo-relevance feedback is likely to help.
An interesting investigation would be to use the anchormap similarity and like approaches to detect and correct the problem of a system missing aspects of a topic. For instance, instead of anchoring the map score in the top documents of a base run and an expansion run, anchor it in only the top documents that have some threshold similarity to a topic aspect. The absolute value of the map score of a base run counting only the docu-retrieved, and the anchormap similarity, given those documents with the aspect, of the base run and expanded run will indicate whether the expansion is moving toward or away from an aspect. 6.2.3 Similar rankings between base run and feedback run versus pseudo-relevance To explore the pseudo-relevance feedback improvement more, instead of comparing the similarity among the rankings of 8 different systems, compare the ranking similarity between the initial run and the pseudo-relevance feedback run of the same system. Topics were categorized by the voting procedure described previously which chooses topics for which most systems agree have the same sort of ranking similarity. The correlation among positive groups was again a very high 0.371. This would imply that the topic results are feedback remain the near the top of the expanded search ranking.

This seems to make sense, since the top documents of the initial search were used for expansion terms and weighting in the expanded search. If different documents were retrieved then it X  X  very possible that the new search got off-topic by over-emphasizing one aspect of the top initial documents. 6.2.4 Similar rankings between base run and feedback run versus pseudo-relevance This comparison was the same as above except directly comparing whether pseudo-relevance feedback improves performance. The Positive groups had a high correlation of 0.287, again suggesting that pseudo-relevance feedback should be used when the initial top documents remain stable in their rankings. 6.2.5 Clarity versus pseudo-relevance feedback MAP The Clarity measure was used on the CMU base run to categorize topics and this was then compared against MAP scores. The correlation among Positive groups was 0.167. Since Clarity can predict hardness of a topic, this strongly suggests that the anchormap approaches, with a much higher correlation, should also be able to predict hardness. That remains for future work.

Note that it may be fairer to compare Clarity against MAP score of baseline systems mild improvement but in the same ballpark. 6.2.6 Clarity versus pseudo-relevance feedback improvement It has never been claimed that Clarity can predict pseudo-relevance feedback improvement without modification of the Clarity measure. Indeed, the RIA investigations showed a correlation among Positive categories of only 0.038. The correlation between the Positive Clarity category and the Negative improvement category was .098, substantially higher. 6.2.7 Topic rare term versus pseudo-relevance feedback MAP If the topic contained a comparatively rare term, then it was more likely to be easy. The score for each topic here was the maximum idf of any of its original topic terms, with the topic scores then being sorted and divided into the normal Positive, Negative, and Neutral categories. The correlation between Positive categories was 0.229. 6.2.8 Topic rare term versus pseudo-relevance feedback improvement If the topic contained a rare term, as measured by the maximum idf of all original topic correlation between Positive categories was 0.038, or roughly neutral. What was quite improvement category was 0.294 (like Clarity, higher than between Positive categories). For a very substantial number of topics with rare terms, pseudo-relevance feedback hurts. 6.3 Preliminary categorization conclusions Overall, the results of the initial categorization efforts surpassed expectations. There were high correlations between a number of categories, including several described above that should be able to be transformed into a predictive process that gives insight as to what sort of retrieval approaches are likely to be successful on a particular topic.

As yet, there are no real results comparing the categories determined by the manual developed. 7 Summary of research results and suggested future work There are many detailed results and suggested further work given in the previous sections; emphasized. These are drawn from the work above, and from the half-day review dis-cussions that each 2-week workshop session ended with. 1. Current research IR systems are failing for the same reason on individual topics. They 2. Current system failures are dominated by presence or absence of topic aspects in the 3. The data is now available for understanding why pseudo-relevance feedback improves 4. Automatic (non-relevance-based) categorization of topics is needed as topics have to 7. At a lower level of analysis, the massive data should support finding the expansion 8 Conclusion and retrospective thoughts The RIA workshop presented a very special opportunity to the IR community to start work on understanding how and why systems vary in performance across questions (topics). Once there is a better understanding of this, then there will be more robust IR systems, which will in turn lead to better QA systems. The initial work has been done, what remains is further analysis of the results by the entire IR community.

The workshop was both a major effort and a major success, although there was never enough time to do everything. One of the major successes was simply the act of bringing so many systems and graduate students together to work on a common task. The enthusiasm and the daily interaction of the seven groups led not only to better understanding of the various systems but to increased awareness of many different IR issues. The logistics of focusing on both a failure analysis and a common set of experiments turned out to be a good use of the 6 weeks. The early decision to create a large data set for later analysis, and management of such a large group of work, and in providing an excellent record of what was done, allowing for future analysis.
There were two issues that created problems for the workshop, both involving the lack setting up such a workshop are huge; even though the systems were set up at MITRE only system changes to run the experiments, but the building of failure analysis modules and the organization and creation of the results data. Some of this could have been done difficulty of the topic variation problem. It had been expected that the early experiments would lead to some hypotheses that could then be tested and would lead to more concrete conclusions. This did not happen and became part of the reason that there was so little time for analysis or categorization experiments.
 A short workshop was held at SIGIR 2004 to discuss recommendations for the future. The following list is the outcome of that workshop. Note that the list is relatively unedited in that these are various ideas as opposed to an ordered list.  X  What could be done differently next time  X  additional work with the number of documents experiments  X  additional work with the number of terms to add experiment  X  additional swapping experiments Appendix  X  Carnegie Mellon University  X  City University, London (City)  X  Clairvoyance Corporation  X  Sabir Research  X  University of Massachusetts at Amherst  X  University of New York at Albany  X  University of Waterloo  X  NIST  X  MITRE and others References
