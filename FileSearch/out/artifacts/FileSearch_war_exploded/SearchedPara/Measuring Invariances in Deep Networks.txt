 we propose to use a suite of  X  X nvariance tests X  that directly measure the invariance properties of features; this gives us a measure of the quality of features l earned in an unsupervised manner by a deep learning algorithm.
 Our work also seeks to address the question: why are deep lear ning algorithms useful? Bengio and LeCun gave a theoretical answer to this question, in which th ey showed that a deep architecture is necessary to represent many functions compactly [1]. A seco nd answer can also be found in such work as [2, 3, 4, 5], which shows that such architectures lead to useful representations for classi-Our observations lend credence to the common view of invaria nces to minor shifts, rotations and deformations being learned in the lower layers, and being co mbined in the higher layers to form progressively more invariant features.
 In computer vision, one can view object recognition perform ance as a measure of the invariance of the underlying features. While such an end-to-end system per formance measure has many benefits, it can also be expensive to compute and does not give much insi ght into how to directly improve such video tests have the potential to examine changes in oth er variables such as illumination. We responses to sinusoidal gratings; however, the natural vid eo approach enables us to test invariance to a wide range of transformations while the grating test onl y allows changes in stimulus position, orientation, and frequency.
 Our proposed invariance measure is broadly applicable to ev aluating many deep learning algorithms for many tasks, but the present paper will focus on two differ ent algorithms applied to computer vision. First, we examine the invariances of stacked autoen coder networks [2]. These networks sequences, and examines more complex variations such as out -of-plane changes in viewing angle. We find that when trained under these conditions, stacked aut oencoders learn increasingly invariant Next, we show that convolutional deep belief networks (CDBN s) [5], which are hand-designed to be stacking of autoencoders are important for gaining increas ing invariance. Deep architectures have shown significant promise as a techn ique for automatically learning fea-tures for recognition systems. Deep architectures consist of multiple layers of simple computational elements. By combining the output of lower layers in higher l ayers, deep networks can represent progressively more complex features of the input. Hinton et al. introduced the deep belief network, work using an autoencoder neural network in each layer [2, 3, 6]. Ranzato et al. and Lee et al. explored the use of sparsity regularization in autoencodin g energy-based models [7, 8] and sparse convolutional DBNs with probabilistic max-pooling [5] res pectively. These networks, when trained subsequently in a discriminative fashion, have achieved ex cellent performance on handwritten digit good features for classification tasks even when trained on d ata that does not include examples of the classes to be recognized [5, 9].
 Some work in deep architectures draws inspiration from the b iology of sensory systems. The human features [10]. Lee et al., for example, compared the respons e properties of the second layer of a erty of the visual system is a progressive increase in the inv ariance of neural responses in higher ably invariant to transformations of the image, responding equally well to images from different perspectives, at different scales, and even responding to t he text  X  X alle Berry. X  While we do not may well turn out to respond also to other stimuli than Halle B erry related ones), they nonetheless show impressive selectivity and robustness to input transf ormations.
 Computational models such as the neocognitron [13], HMAX mo del [14], and Convolutional Net-work [15] achieve invariance by alternating layers of featu re detectors with local pooling and sub-sampling of the feature maps. This approach has been used to e ndow deep networks with some more complicated invariances using this fixed architecture . Additionally, while deep architectures provide a task-independent method of learning features, co nvolutional and max-pooling techniques are somewhat specialized to visual and audio processing. in any way to the invariance tests. 3.1 Stacked autoencoder The majority of our tests focus on the stacked autoencoder of Bengio et al. [2], which is a deep network consisting of an autoencoding neural network in eac h layer. In the single-layer case, in response to an input pattern x  X  R n , the activation of each neuron, h where h ( x )  X  R m is the vector of neuron activations, W bias vector, and tanh is the hyperbolic tangent applied comp onentwise. The network output is then computed as where  X  x  X  R n is a vector of output values, W vector. Given a set of p input patterns x ( i ) , i = 1 ,  X  X  X  , p , the weight matrices W using backpropagation [16, 17, 18] to minimize the reconstr uction error P p Following [2], we successively train up layers of the networ k in a greedy layerwise fashion. The first layer receives a 14  X  14 patch of an image as input. After it achieves acceptable leve ls of reconstruction error, a second layer is added, then a third, and so on.
 In some of our experiments, we use the method of [11], and cons train the expected activation of the hidden units to be sparse. We never constrain W hold in practice. 3.2 Convolutional Deep Belief Network We also test a CDBN [5] that was trained using two hidden layer s. Each layer includes a collection of  X  X onvolution X  units as well as a collection of  X  X ax-pooli ng X  units. Each convolution unit has a receptive field size of 10x10 pixels, and each max-pooling u nit implements a probabilistic max-like operation over four (i.e., 2x2) neighboring convoluti on units, giving each max-pooling unit an The model is regularized in a way that the average hidden unit activation is sparse. We also use a small amount of L Because the convolution units share weights and because the ir outputs are combined in the max-the input, and otherwise respond weakly when it is absent. An invariant neuron, then, is one that invariant, it might continue to respond strongly even as the image rotates.
 for each hidden unit such that all units fire at the same rate wh en presented with random stimuli. calculating its firing rate in response to a set of transforme d versions of that input. More formally, a hidden unit i is said to fire when s by our test for that hidden unit and s sign term s use high values to indicate the presence of the feature that t hey detect. We therefore choose s maximize the invariance score. For hidden units that are reg ularized to be sparse, we assume that s = 1 , since their mean activity has been regularized to be low. We define the indicator function f otherwise.
 A transformation function  X  ( x,  X  ) transforms a stimulus x into a new, related stimulus, where the transformation parametrized by  X   X  R n .) In order for a function  X  to be useful with our invariance be the number of degrees by which x is rotated.
 x , that is degrees.
 The global firing rate is the firing rate of a hidden unit when applied to stimuli draw n randomly from a distribution P ( x ) : Using these definitions, we can measure the robustness of a hi dden unit as follows. We define the set Z as a set of inputs that activate h the hidden unit, measure of the robustness of the neuron X  X  response to the tra nsformation  X  .
 Our invariance score for a hidden unit h The numerator is a measure of the hidden unit X  X  robustness to transformation  X  near the unit X  X  opti-mal inputs, and the denominator ensures that the neuron is se lective and not simply always active. In our tests, we tried to select the threshold t same activation value (up to machine precision), it is somet imes not possible to choose t  X  ( x,  X  ) , and  X  .
 N of
N . We discard the (1  X  p ) worst hidden units because different subpopulations of uni ts may be more than proportion p of their hidden units to such a task.
 by deep networks, it could be applied to virtually any kind of feature in virtually any application domain. We use as input an image I of a grating, with image pixel intensities given by ment our invariance measure, we define P ( x ) as a distribution over grating images. We measure defining  X  ( x,  X  ) to change  X  by  X  . 1 generate the images, it shares the difficulty faced by a numbe r of other methods for quantifying data that systematically varies a large variety of image par ameters.
 Our second suite of invariance tests uses natural video data . Using this method, we will measure the degree to which various learned features are invariant t o a wide range of more complex image parameters. This will allow us to perform quantitative comp arisons of representations at each layer of a deep network. We also verify that the results using this t echnique align closely with those obtained with the grating-based invariance tests. 6.1 Data collection Our dataset consists of natural videos containing common im age transformations such as transla-the NORB dataset [21] where the viewpoint changes in large in crements between successive images, our videos are taken at sixty frames per second, and thus are s uitable for measuring more modest are reduced in size to 320 by 180 pixels and whitened by applyi ng a band pass filter. Finally, we adjust the constrast of the whitened images with a scaling co nstant that varies smoothly over time and attempts to make each image use as much of the dynamic rang e of the image format as possible. Each video sequence contains at least one hundred frames. So me video sequences contain motion order to focus on the relevant transformation. 6.2 Invariance calculation of transformation. This obviates the need to define a complex  X  capable of synthetically performing operations such as 3-D rotation. 7.1 Stacked autoencoders 7.1.1 Relationship between grating test and natural video t est Sinusoidal gratings are already used as a common reference s timulus. To validate our approach 2-D (in-plane) rotation. Our 2-D rotations were captured by hand-rotating a video camera in natural environments, which introduces small amounts of other type s of transformations. To verify that the problem is not that rotation when viewed far from the imag e center resembles translation, we Figure 3: Our invariance measure selects networks that lear n edge detectors resembling Gabor func-tions as the maximally invariant single-layer networks. Un regularized networks that learn high-frequency weights also receive high scores, but are not able to match the scores of good edge detec-tors. Degenerate networks in which every hidden unit learns essentially the same function tend to receive very low scores. are certainly not well-approximated by translation. 7.1.2 Pronounced effect of sparsity and weight decay We trained several single-layer autoencoders using sparsi ty regularization with various target mean activations and amounts of weight decay. For these experime nts, we averaged the invariance scores sparsity regularization, we assume s to be the region where the problem is constrained enough that the autoencoder must throw away best-scoring network receives a score of 32.41. 7.1.3 Modest improvements with depth of autoencoders using only weight decay. The majority of suc cessful image classification results in layer 1 and layer 2 of the CDBN. Autoencoder weight images are taken from the best autoencoder at each depth. All weight images are contrast normalized indep endently but plotted on the same spatial scale. Weight images in deeper layers are formed by making li near combinations of weight images in shallower layers. This approximates the function comput ed by each unit as a linear function. search space more densely. We trained a total of 7 3 networks with weight decay at each layer set to s for each hidden unit to maximize the invariance score, since there was no sparsity regularization to impose a sign on the hidden unit values.
 After performing this grid search, we trained 100 additiona l copies of the network with the best mean invariance score at each depth, holding the weight deca y parameters constant and varying only the random weights used to initialize training. We foun d that the improvement with depth was limited compared to the increase that can be gained with the c orrect sparsity and weight decay. 7.2 Convolutional Deep Belief Networks We also ran our invariance tests on a two layer CDBN. This provides a measure of the effec-tiveness of hard-wired techniques for achiev-ing invariance, including convolution and max-pooling. The results are summarized in Table 1. These results cannot be compared directly to the results for autoencoders, because of the dif-ferent receptive field sizes. The receptive field sizes in the CDBN are smaller than those in the autoencoder for the lower layers, but larger than those in the autoencoder for the higher layers due to the pooling effect. Note that the great-est relative improvement comes in the natural image tests, which presumably require greater sophistication than the grating tests. The single test with the greatest relative improvement is the 3-D (out-of-plane) rotation test. This is the most complex transformation included in our tests, and it is where depth provides the greatest percentagewise increase. In this paper, we presented a set of tests for measuring invariances in deep networks. We defined a general formula for a test metric, and demonstrated how to implement it using syn-thetic grating images as well as natural videos which reveal more types of invariances than just 2-D (in-plane) rotation, translation and fre-quency.
 At the level of a single hidden unit, our firing rate invariance measure requires learned fea-trade-off between precision and recall in a detection probl em. As learning algorithms become more advanced, another appropriate measure of invariance may be a hidden unit X  X  invariance to object information with categories in the Caltech 101 dataset [22] . We found that none of our networks gave good results. We suspect that current learning algorit hms are not yet sophisticated enough to categories, but this ability will become measurable in the f uture.
 At the network level, our measure requires networks to have a t least some subpopulation of hidden top-scoring proportion p of hidden units when calculating the network score. Such a qu alification is necessary to give high scores to networks that decompose t he input into separate variables. For example, one very useful way of representing a stimulus woul d be to use some subset of hidden units its identity. Even though this would be an extremely powerfu l feature representation, a value of p set too high would result in penalizing some of these subsets for not being invariant. identity.
 A surprising finding in our experiments with visual data is th at stacked autoencoders yield only modest improvements in invariance as depth increases. This suggests that while depth is valuable, mere stacking of shallow architectures may not be sufficient to exploit the full potential of deep architectures to learn invariant features.
 Another interesting finding is that by incorporating sparsi ty, networks can become more invariant. features. For example, one promising approach that we are cu rrently investigating is the idea of learning slow features [19] from temporal data.
 We also document that explicit approaches to achieving inva riance such as max-pooling and weight-algorithms that automatically discover much more invarian t features without relying on hard-wired strategies.
 Acknowledgments This work was supported in part by the National Science Found ation under grant EFRI-0835878, and in part by the Office of Naval Researc h under MURI N000140710747. Andrew Saxe is supported by a Scott A. and Geraldine D. Macomb er Stanford Graduate Fellowship. We would also like to thank the anonymous reviewers for their helpful comments.
 [1] Y. Bengio and Y. LeCun. Scaling learning algorithms towa rds ai. In L. Bottou, O. Chapelle, [2] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Gr eedy layer-wise training of deep [3] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y . Bengio. An empirical evaluation of [5] H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. Convolution al deep belief networks for scalable [6] H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin. E xploring strategies for training deep [7] M. Ranzato, Y-L. Boureau, and Y. LeCun. Sparse feature le arning for deep belief networks. In [8] M. Ranzato, F.-J. Huang, Y-L. Boureau, and Y. LeCun. Unsu pervised learning of invariant [9] Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packe r, and Andrew Y. Ng. Self-taught [10] D.J. Felleman and D.C. Van Essen. Distributed hierarch ical processing in the primate cerebral [11] H. Lee, C. Ekanadham, and A.Y. Ng. Sparse deep belief net work model for visual area v2. In [13] K. Fukushima and S. Miyake. Neocognitron: A new algorit hm for pattern recognition tolerant [14] M. Riesenhuber and T. Poggio. Hierarchical models of ob ject recognition in cortex. Nature [15] Y. LeCun, B. Boser, J.S. Denker, D. Henderson, R.E. Howa rd, W. Hubbard, and L.D. Jackel. [16] P. Werbos. Beyond regression: New tools for prediction and analysis in the behavioral sci-[18] D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learni ng representations by back-[20] L. Wiskott and T. Sejnowski. Slow feature analysis: Uns upervised learning of invariances. [21] Y. LeCun, F.J. Huang, and L. Bottou. Learning methods fo r generic object recognition with [22] Li Fei-Fei, Rod Fergus, and Pietro Perona. Learning gen erative visual models from few train-
