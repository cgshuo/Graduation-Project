 Retrieving similar documents from a large-scale text corpus according to a given document is a fundamental technique for many applications. However, most of existing indexing techniques have difficulties to address this problem due to special properties of a document query, e.g. high dimension-ality , sparse representation and semantic issue . Towards ad-dressing this problem, we propose a two-level retrieval solu-tion based on a document decomposition idea. A document is decomposed to a compact vector and a few document spe-cific keywords by a dimension reduction approach. The com-pact vector embodies the major semantics of a document, and the document specific keywords complement the dis-criminative power lost in dimension reduction process. We adopt locality sensitive hashing (LSH) to index the compact vectors, which guarantees to quickly find a set of related documents according to the vector of a query document. Then we re-rank documents in this set by their document specific keywords. In experiments, we obtained promising results on various datasets in terms of both accuracy and performance. We demonstrated that this solution is able to index large-scale corpus for efficient similarity-based docu-ment retrieval.
 H.3.1 [ Information Systems ]: Information Storage and Retrieval X  Content Analysis and Indexing Algorithms, Performance, Experimentation document decomposition, similarity search, indexing
In many applications, a fundamental technique is to find similar documents according to a given document from a large-scale text corpus, e.g. a movie recommendation system needs to find similar movies/users to a given movie/user, in both content-based and collaborative filtering scenarios [30]; a blogger wants to know who pirates his blog pages or to do the cross reference; a patent lawyer want to find similar patents as reference cases. Although lots of retrieval technologies have been developed, most of them are designed for short query retrieval, e.g. 2-10 query terms. A document query, e.g. including 2000 query terms, is very different from a short query, as well as corresponding indexing techniques. Following [35], we term this problem as Query By Document (QBD) in this paper.

The QBD problem has three important properties, high dimensionality , sparse representation and semantic issue , which should be comprehensively considered in the solution. First of all, the document query contains lots of terms, so it is naturally a high-dimension indexing problem. Secondly, the document query is often represented as a language vec-tor, which is quite sparse, and it will incur new challenges when mixed with the high dimension property. At last, a document query often has its inherent semantics, which is quite important information in the similar document match-ing. This property determines the difference between tradi-tional near-duplicate detection and our QBD problem. The former often focus on the syntax level matching, and try to find exactly the same document only with some minor syn-tactic difference; the latter, needs to consider the syntactic level matching and semantic level matching together, and try to find related documents from the user perspective.
A simple solution for QBD is a linear scan approach, in which both documents in corpus and a query document are represented by high-dimensional vectors [3], and cosine sim-ilarity could be adopted to rank documents. However, this solution have two drawbacks: i) for large-scale corpus, its computational cost is unacceptable, ii) direct matching of keywords may encounter some language problems, i.e. syn-onymy and polysemy [13], which also loses the important semantic information.

Inverted files is an efficient index structure for large-scale text corpus [39]. In inverted files, words are primary keys, and documents containing the same word are organized as one row of the table. Thus, given a query, documents con-taining all query terms can be obtained efficiently. For a t ypical query which contains 2-10 terms, this operation is very efficient [6]. However, inverted files cannot solve the QBD problem. Given a long document query, the compu-tational cost to merge hundreds of rows of the table is un-acceptable. Moreover, it is not necessary to require that the returned documents contain all terms of a query doc-ument. An optional solution is to select a set of represen-tative words/phrases from the query document, and then search related documents via the inverted files with the con-structed  X  X hort query X  [35]. [35] proposed to score words by TF-IDF weights and mutual-information. However, the se-lection of representative words largely loses information of a query document, especially the semantic information. More-over, inverted files cannot solve those language problems we mentioned above.

The QBD essentially is a special high-dimensional index-ing (HDI) problem which has been extensively studied in literatures. A basic idea of HDI approaches is to partition a feature space to lots of  X  X ins X . If two documents appear in the same bin, they are deemed as similar documents. Typ-ical HDI approaches can be categorized as tree-based, e.g. kd-tree [3], and hash-based approaches, e.g. locality sen-sitive hashing (LSH) [1]. Due to the well-known curse of dimensionality, tree-based approaches only can be applied to  X  X ow X  dimensional spaces (say, less than 30) [22]. To find the nearest neighbors in a high-dimensional space by a tree-based approach, we have to perform back-tracking operation frequently in the search process, as will degenerate a such method to be a linear scan. LSH partitions a feature space by a group of carefully designed hash functions. It has been demonstrated to be an effective technique for visual features of images [20]. However, lots of experiments including ours show it is not good at indexing sparse feature vectors, e.g. TF-IDF vectors in QBD problem, because the L 2 distance of sparse features is not as reliable as that of dense features [1] 1 . Most of hash-based approaches are designed for de-tecting near-duplicate documents [18], rather than retriev-ing similar documents, e.g. shingling algorithms [8] and a random projection-based approach [9].

Dimension reduction (DR) approaches are effective solu-tions to the QBD problem, e.g. matrix factorization based approaches (e.g. latent semantic indexing (LSI) [13], non-negative matrix factorization (NMF) [21]), and topic models (e.g. pLSI [19] and LDA [5]), in which high dimensional fea-ture vectors are projected to a low dimensional latent space by minimizing some kinds of reconstruction error. Because the new representations of documents often are dense and compact, many similarity measures, e.g. cosine similarity, and HDI approaches, e.g. LSH, work well. Moreover, those language problems, i.e. synonymy and polysemy , which are often encountered in direct keywords matching-based solu-tions, can be alleviated [13].

However, DR approaches have an obvious drawback if they are applied to address QBD alone. DR approaches often reduce the discriminative power of document vectors. For example, two news stories, a basketball related and a football related, are likely to be projected to the same or two very near points in the latent space because  X  X ports X  are their major semantics (preserved in DR), while basketball and football are their minor semantics (ignored in DR), as
Ev en if good parameters for sparse vectors exist, the pro-cess to finding them largely depends on designer X  X  experience and extensive experiments. shown in Figure 1. These minor semantics exist as resid-ual which is abandoned in a DR process. This observation inspired us that the residual of documents can complement the loss of X  X haracteristics X  X f documents. For a document, if we can selectively preserve some keywords, which major lead its residual, and along with the compact vector obtained by DR, we can preserve its major semantics as well as most of its discriminative power in a still compact compound repre-sentation.

With the above analysis, we proposed a decomposition-based two-level retrieval approach to address the QBD prob-lem in this paper. Firstly, we utilize a principle document decomposition model(DDM)[10] to decompose the original document into three parts of words, i.e. background words, topic related words and document specific words. Back-ground words have no information to distinguish documents, topic related words embody the inter-class differences which are major semantics of a document, and document specific words embody the intra-class differences which are minor but most distinguishing elements of a document. As shown in Figure 1, topic related words are major results of dimen-sion reduction, while document specific words are residual. Then, we design an effective two-level index and rank schema based on the document decomposition results. In the index-ing process, the compact and dense topic related vectors can be indexed by a HDI approach, e.g. LSH in this pa-per, and the document specific keywords can be indexed by inverted/forward files, as shown in Figure 3. In the rank-ing process, we first get a set of candidate documents by search in LSH index. Documents in this set are likely to talking about the same topics as the query. Then, we re-rank them by their document specific keywords. The top ranked documents are likely to talking about the same de-tails of the same topics. [27] proposed a similar document decomposition idea with a model called PCP based on the PCA, which can also be included in our unified two-stage index and re-rank framework.
 The remaining part of the paper is organized as follows. In Section 2, we introduce related works. In Section 3, we con-tinue to introduce our document decomposition idea. Then the two-level index and rank schema is presented in Sec-tion 4. A set of practical experiments are conducted and the corresponding results are analyzed in section 5. Finally, conclusions and future work are given in the last Section.
Near-duplicate detection is an important related work. It is a classical technique devoting to find duplicate pages or duplicate documents, which are strikingly similar with only some minute difference, e.g. words, fonts, bold types. Near-duplicates are widespread in email, retrieval results, news pages and images. The state-of-the-art solution is considered as the shingling algorithm in [8] and the random projection projection based approach in [9]. Moreover, there are lots of adaptive approaches to satisfy various requirement, such as [24, 11, 12, 14, 18], all of them gain great success in their own scenarios. As mentioned before, the goal of near-duplicate is not exactly the same with our QBD problems, in short, one focus on the syntactic similarity, and the other needs to consider semantic similarity and syntax similarity together. Locality sensitive hashing(LSH) [1] is frequently utilized in near-duplicate detection and is demonstrated effective in the Fi gure 1: Illustration of the document decomposi-tion idea. The cube denotes a vector space of doc-uments, the triangle is a simplex spanned by three bases, i.e. topics, a rectangle point denotes a doc-ument, and a circle point denotes an embedding of a document in the topic simplex. The distance of a document to the simplex is its residual. compact vector indexing, therefore, our approach adopt LSH to index the compact topic element of document.

Phrase extraction based matching is another related work to QBD problem, which tries to extract some representa-tive phrases from the query document, and then to find related documents containing these representative phrases. Schemas to extract representative phrases are various: [25, 26] utilize statistical information to identify suitable phrases; [16, 33] incorporate the relationships between phrases to identify phrases; [32, 31] apply learning algorithms in the extracting process. [35] is the nowadays phrase extracting based matching approach, which identifies the phrase set by the TF-IDF weight and the mutual information, and then refines the set with the help of phrase association in the wikipedia link graph. Phrase extraction based matching is a quite effective solution to our problem, but the selection of phrases loses much useful information, especially the se-mantics in the document. In our approach, the document specific words actually are representative phrases which are extracted by a principle language model, and as a comple-ment, the topic elements maintain the semantic information in document.

Topic model based retrieval is also a related work, which often combines topic model element and document language element together. This solution is quite popular nowadays, and [36, 37, 38, 23, 34] are all successful works of this class. The semantic information in document is somewhat com-plemented in the topic model element, so this solution has been demonstrated effective in many applications. However, there are still two challenges in these existing works: one is that the query is often seen as a bag of words, this assump-tion is reasonable in the short query scenario, but in our document query application, the semantic structure of the query also needs to be considered; the other one is that when facing the document query scenario with large-scale corpus, the matching efficiency will be a serious issue. To address these two challenges, our approach proposes the document decomposition idea and two-level indexing schema.
In this section, we present the document decomposition idea and its implementation. It is noted the purpose of de-composing a document is to find a new representation which can be easier indexed.
Our document decomposition idea is originated from di-mension reduction approaches. To make the presentation easy to understand, we use LSI to illustrate our basic idea [13]. By LSI, a document, d (a W -dimensional sparse vector, where W is the size of vocabulary), is decomposed to where  X  is the mean of features, X is a W  X  k matrix, k is the number of principle components ( k  X  W for dimen-sion reduction puspose), w is a k -dimensional coefficient vec-tor, and  X  is a W -dimensional vector. Column vectors of X is a group of so-called base vectors, which span a low-dimensional latent space; w is the projection of a document in the space;  X  is the residual which is the  X  X istance X  of the document to the space. Since, the mean vector,  X  , does not contain any discriminative information (i.e. stopwords), it can be removed from the representation of a document. Af-ter removing  X  , the inner product of a document d in corpus and a query document q could be computed by &lt; d, q &gt; = &lt; Xw d +  X  d &gt;&lt; Xw q +  X  q &gt; = w Here, we use a property of LSI decomposition: the residual (  X  ) is orthogonal to principle components ( X ). Thus, the multiplication of X and  X  is null. Considering the physical meanings of w and  X  , we can get an effective approximation to the inner product. For a good LSI decomposition, entries of  X  usually are very close to zero. Thus, even if we only keep a few large entries of the two  X  vectors respectively, the inner product of two documents is likely to not changing. However, in this way, we can greatly reduce the storage for documents. To store a raw document vector, d , we need | d storage cells, where | d | is the length of the document, while to store the new appropriate representation, we only need k + t storage cells, where t is the number of  X  entries we kept. k + t is much less than | d | in practice. Moreover, in Section 4, we will show that a good property of this approx-imation enables an efficient indexing solution. In summary, a document is represented by: where w is a k -dimensional compact vector, and  X  is a few keywords.

It is noted that, for short documents, the new represen-tations may increase their storage cost (i.e. k + t &gt; | However, we argue that even in this case, the proposed ap-proach has benefits. For a short document, the information containing in it is likely to be insufficient if we only consider words in it alone. In this case, LSI can find complementary information for short documents [29, 34] because they con-sider all words in the whole corpus. Relationships among words can help short documents to complement their infor-mation.

Actually, the document decomposition idea is very gen-eral. Any dimension reduction approach, if only it can find the most significant residual words explicitly, can be adopted to implement the idea, e.g. the LSI approach pre-sented above. However, we recommend to implement it by a probabilistic topic model in this paper. Although the com-putations in the topic model looks much more complex than Fi gure 2: A graphical representation of document decomposition model (DDM). matrix factorization-based approaches, it can help us bet-ter understand the proposed solution 2 . Indeed, the two ap-proaches are equivalent. [15] proved the equivalence of pLSI [19] and non-negative matrix factorization [21].
A topic model often describes a generative process of a document [5, 19]. It assumes that words of a document are drawn from a set of topic distributions. Document de-composition model (DDM) [10], as shown in Figure 2, is an extension of LDA [5], which is first proposed to model dif-ferent aspects of a document. The DDM model assumes a document is generated by this process: 1. Draw  X   X  Dirichlet (  X  ) 2. Draw  X   X  Dirichlet (  X  2 ) 3. Draw  X   X  Dirichlet (  X  ) 4. For each word of the document where M ult stands for a multinomial distribution. The key idea of DDM is to use a switch variable x to control the generation of words. x takes value 1, 2 or 3, which con-trols a word is drawn from either a topic distribution ( z and  X  z ), a document specific distribution (  X  ), or a corpus background distribution (  X  ). It provides a nature way to partition a document into three parts of words. Intuitively, a word which widely appears in all documents is likely to be a background/stop word; a word which only appears in a few documents but seldom appears in other documents is likely to be a document specific word; a word which widely appears in documents with a common semantic but seldom appears in other documents without this semantic is likely to be a topic related word. It is interesting to note that doc-ument specific words are likely to have the largest TF-IDF values in a document.

From the aspect of matrix factorization, the work of DDM can be understood as simultaneously finding a group of basis vectors (  X  1: K , a set of distributions over vocabulary), and
Th e computational cost of the two approaches are almost the same. coefficients (  X  , topic mixture proportion). The conditional probability of a word given a document is computed by p ( w | d ) = Thus, the decomposition can be shown in a matrix factor-ization manner d = This decomposition has the same format as Eq. 1.
To estimate parameters of a graphical model, Expecta-tion Maximization (EM) algorithm is often adopted [19, 5]. However, in DDM, direct optimization to model parameters by EM algorithm is intractable [5, 10]. We adopt Monte Carlo EM [2] instead. In E step, rather than analytically compute the posterior of hidden variables, we draw sam-ples from their posterior distributions. In M-step, we ap-proximate the expectation by a finite sum over samples and maximize the expectation with respect to model parameters. Instead of uisng a full Markov Chain Monte Carlo (MCMC) approach [2], a collapsed Gibbs sampling approach is used to draw samples from posterior [17]. Considering the con-ditional independencies between variables, we only need to sample hidden variables x and z , while integrate out other hidden variables. The Gibbs sampling equations are easy to obtain p ( x i = 1 , z = k | x i , z i , w,  X ) = p ( x i = 2 | x i , z i , w,  X ) = 2 + n p ( x i = 3 | x i , z i , w,  X ) = 3 + n wh ere  X  denotes all hyper parameters, i.e.  X ,  X ,  X  1 ; 2 ; 3 means all words except for the current word w i , n j;k d; i notes the number of words generated when x = j and z = k in document d , n j;w i d; i denotes the number of times of word w i generated when x = j in document d , and n 1 ;k i;w i de-notes the number of times of word w i assigned to z = k . To update hyper-parameters in M step, we adopt the fix-point iteration algorithm proposed in [28].
In practice, we only can learn a model with a small pro-portion of documents of a large corpus, and then apply it to infer hidden variables of remainder documents. For a re-trieval system, we also need to infer latent variables of a query document. For DDM, the inference algorithm is simi-lar as the estimation algorithm. However, model parameters will be fixed to be those obtained in model estimation step. The Gibbs sampling equations are p ( x i = 1 , z = k | x i , z i , w,  X ) = p ( x i = 2 | x i , z i , w,  X ) = 2 + n p ( x i = 3 | x i , z i , w,  X ) = 3 + n I t is remarkable that the inferences of latent variables of different documents are independent to each other. From above equations, we can easily verify this fact. Only statis-tics inside a document take part in the computation of Gibbs sampling, that is, we can distribute documents to different processors and parallel decompose them. Thus, the scala-bility of DDM is not an issue.

Once we have inferred latent variables of a document, we can easily get its decomposition results.  X  is the compact vector as the vector obtained in the LSI-based approach. To find out document specific words of a document, we compute this conditional probability p ( x i = 2 | w i , d ) = p ( w i T o reduce computation, we approximate the posterior by  X  d;w i . With this probability, we can control the number of document specific words to be an acceptable value, e.g. 15 in our experiments. An advantage of DDM beyond matrix factorization-based approaches is that it can explicitly de-compose words of a document to be three categories. This is very helpful when we analyze experimental results.
So far we have proposed two mathematically solid ap-proaches to implement the document decomposition idea, but actually we can implement it by many heuristic ways. The only requirement to an approach is that it must be able to get a compound representation of a document like in Eq. 3. We give some examples which will be compared in our experiments.

Under the framework of topic models , we need to con-struct a set of topics(  X  1: K of DDM), and then run the infer-ence algorithm of DDM to decompose documents. For this purpose, we can obtain topics by many existing approaches, such as LDA [5] and pLSI [19]. A heuristic but very practi-cable method is to get topic vectors from web pages of ODP by counting word frequencies in documents of each category. An ODP category may be mapped to a topic or a few top-ics. These topic vectors are likely to be more reasonable than those got by purely unsupervised models.

Under the framework of matrix factorization , we can perform the factorization in many methods, e.g. LSI [13] and NMF [21]. If only we can get a decomposition like Eq. 1, we can extract the co-efficient w as a representation of topic related words and keep the top N largest entries of  X  as document specific words.
We have represented both documents in corpus and a query document by vector pairs &lt; X  ,  X &gt; , in which  X  is a compact vector and  X  is a few keywords. In this section, we will present indexing and ranking approaches based on this compound representation.
The similarity of two documents is computed by a linear combination of the two components: where  X  : 1 and  X  : 2 mean the word ratios of topic related words and document specific words in a document respectively. They can be got by the inference algorithm. In experiments, we compute sim (  X  d ,  X  q ) and sim (  X  d ,  X  q ) by inner products. This simple ranking function does not introduce additional parameters.

If we keep all words in  X  (  X  is equivalent to the full residual  X  in Eq. 2), this similarity measure exactly recovers the Eq. 2. Thus, this ranking approach is named as  X  X ccurate rank-ing X  in this paper. It defines the upper-bound of retrieval accuracy of the compound representations. In following sec-tions, we will develop an index-based approximation to it, which is required to be as accurate as it but with much lower computational and memory cost.

Similar as Eq. 4, [34] linearly combines probabilities ob-tained by a traditional language model and a topic model, which achieved best performance in lots of short query re-trieval tasks. Without changing its meanings, its ranking function can be rewritten as p ( q | d ) =  X  where z denotes a topic in a LDA model and  X  is a coefficient to balance the two terms. By simple derivation, we can prove this function is equivalent to Eq. 4 when the probability the conditional probabilities of different words/topics given the query q are equal. This assumption is reasonable for a short query because we do not have enough information to infer hidden variables only with a few terms. However, for a real document query, we have enough information to infer hidden variables to extract more knowledge from it. Thus, our work can be deemed as an extension of [34] to dealing with long document queries.
By fully considering the characteristics of the new repre-sentations of documents, we designed an efficient indexing structure as shown in Figure 3. We build two separate in-dices for topic related words (the compact vector  X  ) and document specific words (  X  ). The two index are used to approximately compute the similarity given in Eq. 4.
Indexing topic related words . Because topic related words of a document are represented by a compact and dense vec-tor, which is an appropriate data style for LSH, we choose LSH [1] to index them. Previous research [20] and our ex-periments demonstrated the LSH algorithm is good at in-dexing compact and dense feature vectors but not good at indexing high-dimensional sparse feature vectors, like TF-IDF features of documents. LSH algorithm will assign a group of hash values to a document. These hash values can be deemed as a new discrete representation of a document. We further adopt inverted list to index them to enable fast similarity computing [20]. Therefore, the memory cost of LSH index is only L  X  D , where L (e.g. 30 in our experi-ments) is the number of hash functions of LSH.

Indexing document specific words . Because we can select the most salient document specific words according to the conditional probability p ( x i = 2 | w i , d ), we can control the number of document specific words. In experiments, we achieved good retrieval accuracy with only 15 document spe-cific words. Thus, we adopt a simple structure to organize document specific words, i.e. forward list. In the forward is  X  w . Therefore, the memory cost of the forward list index is only 15  X  D , where D is the number of documents. for removing moisture, or for simulating gravitational effects.
The functionalities of the two components of the com-pound representations imply an efficient two-level ranking approach. The similarity sim (  X  d ,  X  q ) distinguish documents belonging to different categories, while sim (  X  d ,  X  q ) only is a complement of the first similarity which can further dis-tinguish documents belonging to the same category. Thus, the first similarity is a major component, and the second similarity is a minor component. This observation naturally motivated a two-level ranking approach. In the first stage, we use sim (  X  d ,  X  q ) to select the top k most similar documents as a candidate set. Because we index  X  by LSH, we can get a set of related documents much more quickly than a linear scan approach. In the second stage, we use sim (  X  d ,  X  q rerank documents in this set. Actually, multiple-level rank-ing approaches are very popular in IR systems [7]. In our experiments, we set the size of candidate set to 50. The time complexity to respond a query is 30  X  # collision + 15  X  50, where # collision is the average number of docu-ments in the same bucket in inverted index of LSH. In LSH, # collision only slightly changes with the increasing of the numbers of documents it indexes [1]. Thus, the solution can answer a query almost in constant time, which is indepen-dent to the number of documents the system indexes.
It is noted that, by this index and ranking approach, we only can get an approximate results as the accurate rank-ing(Eq. 4) because: i) LSH is an approximate nearest-neighbor algorithm; ii) only a few most salient document specific words are kept in index. Although it is an approx-imate algorithm, its retrieval accuracy is almost as good as the linear ranking in our experiments. The small sacrifice of accuracy significantly shortens the response time and re-duces memory cost.
To the best of our knowledge, there are no widely ac-knowledged criterions on what is a scalable indexing solu-tion. However, the following aspects must be useful when checking the scalability of an indexing solution.
Parallelizable . It implies the indexing of documents must be independent, and its ranking function only uses features within a document and some simple global statistics (e.g. PageRank [6]).

Efficient . The time to building index (e.g. in a few hours) and responding users X  queries (e.g. in a few milliseconds) must be acceptable.

Large Capability . Single machine can index several mil-lion documents (e.g. Google indexed 5 million pages by one machine in 2003 [4]).

It is easy to check that our indexing and ranking solution can meet these requirements. In our experiments, it takes about 8 hours to decompose 5 million documents and build index by one 3G CPU. The index size of 5 million pages is about 3.5GB (including LSH index, forward files and neces-sary metadata of documents). The average time to answer a query is about 13ms if the top 50 results are returned. Thus, theoretically, the proposed solution can index any number of documents and provide realtime search service.
We now present the experiment evaluation of our ap-proach. Section 5.1 and Section 5.2 introduce the utilized data sets and the related experiment setup respectively. Sec-tion 5.3 evaluates performance of decomposition based ap-proaches, and Section 5.4 evaluates performance of the in-dexing solution. At last, the scalability issues are discussed in the Section 5.5 to show the efficiency of our approach.
We evaluated the proposed approaches on four datasets shown in Table 1: 20 newsgroups (20NG), Wikipedia pages (Wiki), a randomly selected subset of Wikipedia (SWiki) and a web corpus with 5 million pages (5M) which was ran-domly sampled from the web TREC X 08 corpus. In 20NG, documents belonging to the same category are labeled as relevant documents. Wikipedia organizes documents in a tree structure. If two leaf nodes, i.e. documents, have the same parent or are linked by a  X  X ee also X  hyperlink, they are labeled as similar documents. SWiki dataset is used to investigate the affection of corpus size to our approaches. For the 5M dataset, we do not have ground truth. The purpose of collecting this dataset is to evaluate scalability of our approaches. In the preprocessing, we performed Porter X  X  stemming and removed words whose document frequencies (DF) are less than 4.
We implemented four baseline approaches: TF-IDF with cosine similarity (TFIDF) [3], directly indexing TF-IDF vec-tors by LSH (LSH) [1], similarity computed only by top N TF-IDF words(TopN) [35], and linearly combining sim-ilarity computed by top N TF-IDF words and similarity g ot by topic mixtures obtained by LDA (TopNLDA) [34]. TopNLDA is one of the state-of-the-art algorithms for web search and also an instantiation of the proposed document decomposition idea. To simplify the notation, we refer to document specific words as DS, and topic related words as TS. We refer to Eq. 4 as DDM, and our index scheme as DDM+Index. We randomly sampled 200 documents from each dataset as queries. In the following experiments, we will adopt the macro-average precision of the top 10 search results for the 200 queries to measure algorithms X  perfor-mance, which is computed by: wh ere N is the number of queries and T i is the number of related documents in the top 10 search results for the ith query.

Our experimental goals include three-fold: 1) performance of the DDM and some practical implementations (rank by Eq. 4); 2) the drop of performance of the index and ranking solution; 3) scalability of the solution.
In this section, we evaluate the best performance that decomposition-based approaches can obtain, which is achieved by Eq. 4.
A common concern about a topic model based algorithm is that the number of topics may significantly affect its per-formance. How to determine the number of topics is still an open problem. Thus, we run a group of experiments to in-vestigate the effects of topic numbers. In these experiments, we fixed the number of DS words to 15. The results on the three dataset are shown in Figure 4.

The decomposition based approaches significantly outper-formed baseline approaches. On the three dataset, the LSH approach almost consistently got the worst results. This fact indicates that LSH is not a good approach to index high-dimensional sparse feature vectors. The accuracy of TS only approach increases with the number of topics, while the accuracy of DS only approach drops. Because in this process more and more DS words will be absorbed into topics, the TS is becoming more and more powerful, while the DS is be-coming weaker and weaker. In extreme cases, both DS and TS only approaches degenerate to the TF-IDF approach.
Intuitively, the best results of an approach on the three dataset will be obtained under very different number of top-ics because the complexities of them are very different. How-ever, we found all their best results were achieved when topic numbers are around 250. The result may be explained by the way we obtain TS and DS words. There are no criterions to define what is a TS word but not a DS word and vice versa. If we increase the number of topics in DDM, some words which previously are classified as DS words will be re-labeled as TS words, and vice versa. Thus, for a dataset, if it can be well represented by the latent space, DS words could be very discriminative or rare words which represent minor seman-tics of documents, otherwise, DS words will be more like TS words which represent major semantics of documents. Thus, the proposed decomposition-based approaches are not sen-sitive to dataset or number of topics.

All approaches based on the document decomposition idea outperform the baseline approaches. As we have mentioned, words with largest TF-IDF values are likely to be DS words. However, the TopNLDA got worse accuracy than decompo-sition based approaches. It is probably because the top N TF-IDF words may be redundant to the topics obtained by LDA, while DDM can decompose a document to two sets of words which are complementary to each other. More-over, the accuracy of ODP+DS, is close to DDM. This fact further demonstrates the power of the document decomposi-tion idea. Because curves for the two other DDM-based ap-proaches (LDA+DS and LSI+DS) almost overlapped with ODP+DS and DDM, we did not draw them in this figure.
Theoretically, the number of DS words should be different for different kinds of documents. For example, Wikipedia documents usually are long documents with complex seman-tics, while documents of 20NG usually are short and sim-ple documents. Thus, DS words for Wikipedia documents should be more than 20NG. However, after we performed experiments we got different answers which are shown in Figure 5. By setting the number of topics to be 250 and the number of DS words to be 15, almost all approaches significantly outperformed baseline approaches. Even when we set the number of DS words to be 5, their performances are better than TopN. The accuracies of TopNLDA is worse than DDM based approaches again. This result could be ex-plained by the reason we discussed in last section. It again demonstrates that DS words are more meaningful than top N TF-IDF words in this ranking framework. Thus, we con-clude that 250 topics and 15 DS words are an appropriate setting for a QBD problem.
In this section, we evaluate performance of the index so-lution.
We built index for documents according to the proposed indexing scheme and adopted the two-level ranking approach to rank documents. Because we only evaluate the precision of the top 10 search results, we set the size of candidate set to be 50 in the LSH retrieval step. LSH index has two parameters, i.e. L and k . L is the number of hash functions, which is set to 30; k is the length of hash code, which is set to 6. The results are shown in Figure 6. For all index-based approaches, their curves are only slightly lower than corresponding accurate approaches (the DDM curve). We assert that the two approaches, i.e. DDM and DDM+index, are statistically the same. We run t -test to evaluate this hypothesis. With 99.5% confidence, the hypothesis is true. Therefore, we conclude that the indexing solution is a good approximation to the accurate approaches. perform LSI decomposition) and LDA model, respectively
Figure 7 shows the results with different number of DS words. Curves of DDM and DDM+index almost overlap to-gether at all points. When the number of DS words is bigger than 10, the accuracies of all our approaches are very close. These results further demonstrate that the index solution is a good approximation to the accurate approach.

Combining results of these experiments, we can conclude: 1) DDM outperform other baseline approaches; 2) accuracies of the proposed practical approaches (e.g. ODP+DS) are only slightly worse than DDM; 3) The drop of accuracy of the index solution is acceptable.
The efficiency of a retrieval system could be measured from two-fold, i.e. time and memory cost to build index and answer a query. In our system, a standard index machine is a workstation with a dual-core CPU and 4G bytes RAM, by which we plan to index around 5 million web pages and provide realtime search service. Thus, we mainly measure cost of our approaches under this hardware setting.
In our system, building index consists of three steps: train a model by a set of documents, infer the left documents and build index as the proposed indexing scheme. The model training is very time-consuming on a large corpus Table 2: Inference time for three dataset (#top-ics=250) wit h huge lexicon. It took us about 50 hours to train a model with 100,000 documents (its vocabulary consists of 163,871 terms). Fortunately, we can get the model done once and for ever. The inference time of the three dataset are listed in Table 2. The time for long documents (e.g. Wiki) are slightly longer than short documents (e.g. 20NG). The time per document on all dataset is no longer than 4.5ms. As we have mentioned, the inference could be performed parallel. By a dual-core computer, it takes around 3.2 hours to infer 5 million documents. After we get inferring results, index can be built in around 70 minutes.

In a real system, index should be able to be loaded in memory to enable fast search [6, 4]. The LSH index size of the 5M (#topics=250, LSH parameters L=30 and k=6 [1, 20]) is around 2.1GB. We use a &lt; word id , weight &gt; pair to represent a document specific word in the forward files. The size of forward files and lexicon of the 5M is around 800MB. Thus, all index data can be loaded in memory. SWiki 20NG, Wiki, and SWiki Fi gure 8: Response time under different size of in-dex
The average response time as a function of number of re-turned search results is shown in Figure 8. The response time includes the time to inferring a query document, i.e. about 4ms, and the time for ranking. The response time is almost a constant time, which almost does not increase with the number of documents that the system indexes. If we run accurate ranking algorithm on the 5M pages dataset, the time to answer a query is around 25s. The proposed in-dex solution can accelerate the search speed 100-1000 times, while its results are almost as accurate as the  X  X ccurate X  ranking approach (Eq. 4). Our system is able to answer more than 20 queries per second.
We have proposed a solution for large-scale query by docu-ment (QBD) application, which essentially is a fast approach to compute nearest neighbors in a high-dimensional sparse feature space. Our theoretic analysis and experimental re-sults demonstrate: 1) The document decomposition idea can solve problems caused by long document queries. Based on this idea, we can represent a document to be a compact vector as well as a few keywords. Retrieval results based on the new representation are better than full text linear scan approach. 2) The indexing and two-level ranking solution signifi-cantly reduces both computation and memory cost of the system, while it can provide search results as accurate as the accurate ranking algorithm (Eq. 4). The whole index-ing solution is a good trade-off among accuracy, cost and performance. 3) The solution has the same architecture as web search engines. Thus, it can be applied to index large-scale text corpus to support QBD-based applications. [1] A. Andoni and P. Indyk. Near-optimal hashing [2] C. Andrieu, N. de Freitas, A. Doucet, and M. I. [3] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern [4 ] L. A. Barroso, J. Dean, and U. H  X  olzle. Web search for [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [6] S. Brin and L. Page. The anatomy of a large-scale [7] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and [8] A. Z. Broder, S. C. Glassman, M. S. Manasse, and [9] M. S. Charikar. Similarity estimation techniques from [10] C. Chemudugunta, P. Smyth, and M. Steyvers. [11] J. Cho, N. Shivakumar, and H. Garcia-Molina. [12] A. Chowdhury, O. Frieder, D. Grossman, and M. C. [13] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [14] F. Deng and D. Rafiei. Approximately detecting [15] C. H. Q. Ding, T. Li, and W. Peng. Nonnegative [16] K. T. Frantzi. Incorporating context information for [17] T. L. Griffiths and M. Steyvers. Finding scientific [18] M. R. Henzinger. Finding near-duplicate web pages: a [19] T. Hofmann. Probabilistic latent semantic indexing. In [20] P. Indyk and N. Thaper. Fast image retrieval via [21] D. D. Lee and H. S. Seung. Algorithms for [22] T. Liu, A. W. Moore, and A. G. Gray. New algorithms [23] X. Liu and W. B. Croft. Cluster-based retrieval using [24] D. P. Lopresti. Models and algorithms for duplicate [25] C. D. Manning and H. Schutze. Foundations of [26] O. Medelyan and I. H. Witten. Thesaurus based [27] K. Min, Z. Zhang, J. Wright, and Y. Ma.
 [28] T. P. Minka. Estimating a dirichlet distribution. [29] X. H. Phan, M. L. Nguyen, and S. Horiguchi. Learning [30] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. [31] T. Tomokiyo and M. Hurst. A language model [32] P. D. Turney. Learning algorithms for keyphrase [33] O. Vechtomova and M. Karamuftuoglu. Approaches to [34] X. Wei and W. B. Croft. Lda-based document models [35] Y. Yang, N. Bansal, W. Dakka, P. Ipeirotis, [36] C. Zhai and J. D. Lafferty. Model-based feedback in [37] Y. Zhang, J. P. Callan, and T. P. Minka. Novelty and [38] Y. Zhang, W. Xu, and J. Callan. Exact maximum [39] J. Zobel and A. Moffat. Inverted files for text search
