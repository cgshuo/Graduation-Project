 Ensemble of classifiers are known to perform better than individual classifiers when they are accurate and diverse [5], [1 9]. In text categorization, they are proven to perform better in some cases [6]. However, they are not efficient due to computational workload. For instance, in news portals, it is a burden to train a new ensemble model or test new documents. Various ensemble selection methods are proposed to overcome this problem [3]. The main idea is to increase the efficiency by reducing the size of ensemble without hurting the effectiveness. Besides, it can increase the effectivenes s if selected classifiers are more accurate and diverse than base classifiers. In this work, we study these two aspects of ensemble selection by giving the accura cy (i.e. effectiveness) results [16].
Ensemble selection mainly consists of t hree stages: constructing base clas-sifiers (ensemble members), selecting target classifiers among base classifiers, and combining their predictions. Base classifiers are constructed homogeneously or heterogeneously. Homogeneous classifiers are trained by the same algorithm and constructed by data partitioning methods in which training documents are manipulated [5], [6]. Heteregeneous classifiers are usually created by training dif-ferent algorithms on the training set [3]. There are also mixed constructions in which data is partitioned and different algorithms are applied separately. There are various ensemble selection approaches [17]. In general, they search for an optimal subset of ensemble members. Searching evaluation is done with a val-idation (hillclimbing or hold-out) set, which can be used either in training or as a separate part of training set. Lastly, ensemble predictions are combined by simple/weighted voting, mixture of experts or stacking [17]. Voting is the most popular approach. It combines predictions of ensemble based on sum, produc-tion or other rules. It is called weighted when each prediction is multiplied by a coefficient.

Construction of base classifiers, training them and getting predictions from each of them require a considerable amount of time in text categorization when there are huge numbers of text documents. This becomes crucial when text documents become longer as experienced in news portals. There is a need for pruning as many base classifiers as possible. Therefore, in this study, we examine ensemble pruning in text categorization by applying different data partitioning methods for construction of base classifiers and popular classification algorithms to train them. We select a simple ranked-based ensemble pruning method in which base classifiers are ranked (ordered) according to their accuracy in a sep-arate validation set and then pruned pre-defined amounts. We choose to use weighted voting to combine predictions of pruned ensemble.

Our answers to the following questions are the contributions of this study: 1. How much data can we prune without hurting the effectiveness using data 2. Which partitioning and categorization methods are more suitable for ensem-3. How do English and Turkish differ in ensemble pruning? 4. Can we increase effectiveness with ens emble pruning in the text categoriza-The rest of the paper is organized as follows. Section 2 gives the related work on ensemble selection. Section 3 explains th e experimental design and the datasets used in our study. Section 4 gives the experimental results. Finally, Section 5 concludes the paper. There are several ensemble selection st udies. Tsoumakas et al. [17] give a taxon-omy and short review on ensemble select ion. Their taxonomy divides ensemble selection methods into search-based, clustering-based, ranked-based, and other methods. Search-based methods apply greedy search algorithms (forward or backward) to get the optimal ensemble. Clustering-based methods employ a clustering algorithm and then prune clusters. Ranked-based methods rank en-semble members once, and then prune a pre-defined amount of members. Our approach also uses a ranked-based selection approach that examines different pruning levels.

Margineantu and Dietterich [11] study search-based ensemble pruning consid-ering memory requirements. Classifiers constructed by the AdaBoost algorithm are pruned according to five different meas ures for greedy search based on accu-racy or diversity. Their results show that it is possible to prune 60-80% ensemble members in some domains with g ood effectiveness performance.

Prodromidis et al. [14] define pre-pruning and post-pruning for ensemble se-lection in fraud detection domain using meta-learning. In our study, their pre-pruning corresponds to forward greedy search and post-pruning means backward greedy search. They produce their base classifiers in a mixed way such that they divide the train data into data partitions by time divisions and then apply dif-ferent classification algorithms including decision trees to these partitions. They get up to 90% pruning with 60-80% of the original performance.

Caruana et al. [3] employ forward greedy s earch with heteregeneous ensembles on binary machine learning problem s. They show that their selection approach outperforms traditional ensembling methods such as bagging and boost-ing. Caruana et al. [2] then examine some unexplored aspects of ensemble selection. They indicate th at increasing validation set size improves performance. They also show that pruning up to 80-90% ensemble members rarely hurts the performance.

Mart  X   X nez-Mu  X  noz and Su  X  arez [12] examine search-based ensemble pruning with bagging. They use CART trees and three different measures for forward greedy search. They show that 80% members can be removed with Margin Distance Minimization (MDM). Hern  X  andez-lobato et al. [7] study search-based ensemble pruning with bagging on regression problems. They decide to use 20% of en-semble members by looking regression errors. Mart  X   X nez-Mu  X  noz and Su  X  arez [13] use training error defined in boosting in order to use in greedy search of en-semble pruning. Results are similar with the work by Hern  X  andez-lobato et al. [7].

In a recent work, Lu et al. [10] introduce ensemble selection by ordering ac-cording to a heuristic measure based o n accuracy and diversity. Similar to our study, they prune the ordered (ranked) ensemble members using a pre-defined number of ensemble sizes. They compare their results with bagging and the approach used by Mart  X   X nez-Mu  X  noz and Su  X  arez [12]. Their method usually out-performs the others when 15% and 30% of ensemble members are selected.
Our study is different from the above studies in terms of the production method of ensemble members, the way of ensemble selection, and the domain to which ensemble selection applied. We introduce a novel approach that examines data partitioning ensembles in ensembl e selection. We also examine different classification algorithms that are popular in text categorization for ensemble selection. Our ensemble selection method is also simple such that we do not use greedy search or a genetic algorithm.
 3.1 Experimental Design Figure 1 represents the ensemble select ion process used in this study. Firstly, the training set is divided into two separate parts. The base training set is used for training the base classifiers (i.e the ensemble). We construct the ensemble by dividing the base training set with homogeneous (in which base classifiers are trained by the same algorithm) data partitioning methods.

We apply four different partitioning methods: bagging, random-size sampling, disjunct, and fold partitioning [6].  X  Bagging [1] creates ensemble members each of size N documents by randomly  X  Disjunct partitioning divides the training set into k equal-size partitions  X  Fold partitioning divides the training set into k equal-size partitions and k-1  X  Random-size sampling is similar to bagging, but the size of each ensemble The base classifiers are then trained with four popular machine learning algo-rithms: C4.5 decision tree [15], KNN (k-n earest Neighbor) [4] , NB (Naive Bayes) [8], and SVM (Support Vector Machines) [18]. KNN X  X  k value is set as 1 and the default parameters are used for other classifiers.
After constructing the ensemble we decide to use simple solutions for ensemble selection since constructing data partitioning ensembles is a time-consuming process for large text collections. We choose ranking-based ensemble pruning that does not use complex search algorithm s of other ensemble selection methods. Each ensemble member is ranked accordin g to their accuracy on the validation set. We use a distinct part of the training set for the validation. The size of the validation set is set as 5% of the training set since we observe reasonable effectiveness and efficiency and accordingly, 5% of each category X  X  documents are chosen randomly without replacement. After ranking, we prune the ranked-list 10% to 90% by 10% increments.

For the combination of the pruned base classifiers, we choose weighted voting that avoids the computational overload of stacking, mixture of experts etc. Class weight of each ensemble member is take n as its accuracy performance on the validation set. If the validation set of a class is empty (when number of documents in a class is not enough), then simple voting is applied.

Considering each four data partitioning methods with four classification algo-rithms, we use a thorough experimental approach and repeat the above ensemble pruning procedure for 16 different scenarios. All experiments are repeated 30 times and results are averaged. Document s are represented with term frequency vectors. Ensemble size (i.e data partitioning parameter) is set as 10 and the most frequent 100 unique words per category a re used to increase efficiency. We use the classification accuracy for e ffectiveness measurement.
 3.2 Datasets We use the following two datasets in the experiments: Reuters-21578 and BilCat-TRT (http://cs.bilkent.edu.tr/  X  ctoraman/datasets/ensemblePruning). The for-mer one is a well-known benchmark dataset [9]. After splitting it with ModApte, eliminating multi-class documents and choosing the 10 most frequent topics, we get 5,753 training and 2,254 test news articles. The latter consists of 3,184 train-ing and 1,660 test Turkish news articles. We choose these two datasets to observe the performance in both English and Turkish. 4.1 Pruning Results The four questions given in introduction are answered in this section. Firstly, Figure 2 gives the results of how much ensemble member we can prune with different data partitioning and categorization methods. These figures can be interpreted either heuristically or statistically. In heuristic way, one can look at Figure 2 and choose an appropriate pruning degree regarding some accuracy reduction. In general, fold partitioning seems to be more robust to accuracy reduction while disjunct partitioning is the weakest one. Similarly, NB and SVM are more suitable for ensemble pruning while C4.5 prunes the least number of base classifiers.

One can also apply some statistical methods to obtain a pruning degree re-garding no accuracy reduction. We apply unpaired two-tail t-test between each pruning degree and traditional ensembling to check whether accuracy reduc-tion is statistically significant. We apply unpaired t-test until difference becomes statistically significant. Pruning degrees regarding no accuracy reduction with unpaired t-test are listed in Table 1. We can prune up to %90 ensemble members using fold partitioning and NB on both datasets. Disjunct partitioning seems to be the worst method for ensemble pruning with no accuracy reduction. Similar to heuristic observations, we get better pruning degrees when either NB or SVM is used. Small amount of ensemble members are pruned using C4.5 and KNN with no accuracy reduction.

Table 1 also suggests that all partitioning and categorization methods prune similar number of ensemble members in both English and Turkish when no accuracy reduction is considered. How ever, NB prunes more or equal number of ensemble members with all partitioning methods in English than those of Turkish.

In some pruning degrees, we observe that ensemble pruning even increases accuracy of traditional ens embling. Table 2 and 3 list accuracies of traditional ensembling and highest increased accura cy that we can obtain by ensemble prun-ing using Reuters-21578 and BilCat-TRT respectively. If any degree of ensemble pruning makes no increase in accuracy, then we only give its traditional ensem-bling accuracy. We also give the pruning degree in which we get the highest accuracy within parentheses. Note that these pruning degrees are not the same as those in Table 1. Unpaired t-test is applied for all comparisons between tra-ditional ensemble and pruning X  X  highest i ncreased accuracy. Results show that, in general, it is possible to increase accuracy with NB and SVM when ensemble pruning is applied. The combination when highest accuracies are seen is bagging with SVM on both datasets. Fold with SVM and random-size with SVM are almost as good as bagging with SVM.
 4.2 Pruning-Related Parameters Ranked-based ensemble pruning explained in Section 3.1 is a simple strategy that depends on choosing an appropriate validation set and ensemble size. In the previous experiments, validation set is chosen as 5% of the original training set and ensemble size is set as 10. These decisions are chosen for simplicity. However, other decisions may affect th e accuracy result of ranked-based ensemble pruning. The following experiments are conducted on only b agging wi th SVM.
Different validation set sizes on both datasets are examined in Figure 3. Val-idation size experiments are conducted by 90% pruning of 10 base classifiers. We randomly select news documents for each category between 1% and 50% of the original training set and set this separate part as validation set. Figure 3 shows that if validation set size is eith er too small or too big, accuracy becomes reduced. Optimal validation set size is somewhere between 5% and 10% of the original training set.

Ensemble size is another parameter for ensemble pruning. Figure 4 displays pruning accuracies of different number of base classifiers between 10 and 50. In ensemble set size experiments, valid ation set size is selected as 5% of the original training set. Accuracy is slightly increased with increasing number of base classifiers as expected. Moreover, accuracy reduction due to pruning be-comes lower as ensemble size increases. However, efficiency is reduced due to the additional workload of training base classifiers. Thus, one should consider the trade-off between reduction in effici ency and increase in accuracy. In this work we study ensemble pruning in text categorization. Ensembles are cre-ated with different data partitioning methods and trained by four different pop-ular text categorization algorithms. The controlled experiments are conducted on English and Turkish datasets. We plan to perform further experiments with additional datasets. However, our statisti cal tests results provide strong evidence about the generalizability of our results. The main goals are to find how many ensemble members we can prune in text categorization without hurting accuracy, which data partitioning methods and categorization algorithms are more suit-able for ensemble pruning, how English and Turkish differ in ensemble pruning, and lastly whether we can increas e accuracy with ensemble pruning.
This study employs data partitioning methods with several classification al-gorithms in ensemble pruning. The main results of this study are: 1. Up to 90% of ensemble members can be pruned with almost no decrease in 2. NB and SVM prune more ensemble members than C4.5 and KNN. Using 3. Pruning results are similar for both English and Turkish. 4. It is possible to increase accuracy with ensemble pruning (See Table 2 and 3). We also examine the effect of different ense mble and validation set sizes. It is seen that using 5-10% of the training set for validation is an appropriate decision for both datasets. We also find that accuracy reduction becomes smaller as ensemble size increases.

In future work different ensemble selection methods and validation measures can be studied. Additional test collections in other languages can be used in further experiments.

