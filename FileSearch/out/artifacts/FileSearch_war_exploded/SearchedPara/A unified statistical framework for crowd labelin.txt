 Jafar Muhammadi  X  Hamid R. Rabiee  X  Abbas Hosseini Abstract Recently, there has been a burst in the number of research projects on human computation via crowdsourcing. Multiple-choice (or labeling) questions could be referred to as a common type of problem which is solved by this approach. As an application, crowd labeling is applied to find true labels for large machine learning datasets. Since crowds are not necessarily experts, the labels they provide are rather noisy and erroneous. This challenge is usually resolved by collecting multiple labels for each sample and then aggregating them to estimate the true label. Although the mechanism leads to high-quality labels, it is not actually cost-effective. As a result, efforts are currently made to maximize the accuracy in estimating true labels, while fixing the number of acquired labels.

This paper surveys methods to aggregate redundant crowd labels in order to estimate unknown true labels. It presents a unified statistical latent model where the differences among popular methods in the field correspond to different choices for the parameters of the model. Afterward, algorithms to make inference on these models will be surveyed. Moreover, adap-tive methods which iteratively collect labels based on the previously collected labels and estimated models will be discussed. In addition, this paper compares the distinguished meth-ods and provides guidelines for future work required to address the current open issues. Keywords Crowdsourcing  X  Human computation  X  Mechanical turk  X  Labeling  X  Latent model  X  Inference 1 Introduction Crowdsourcing has become a popular field of research during recent years. In crowdsourcing, a group of people are asked to contribute in performing a task that cannot be individually done with the same ease [ 24 ].  X  X ikipedia X , for instance, is a well-recognized crowdsourcing system, in which thousands of Internet users participate in the creation of the largest world-wide encyclopedia. Crowdsourcing brings about a large number of applications, ranging from funding (e.g., KickStarter which funds creative projects) and forecasting (e.g., Threadless which estimates the success rate of T-shirt designs in the market) to organization (e.g., Digg which organizes Internet links) and human computation.

Human computation is defined as harnessing human intelligence to solve computational problems beyond the scope of existing Artificial Intelligence (AI) algorithms [ 36 ]. One of the main reasons that AI has not yet achieved all human capabilities is that it lacks common sense knowledge, whatever facts a human knows (captures, saves, and uses) [ 66 ]. In 1992, the problem was originally proposed by Marvin Minski in the context of slow progress in processing natural languages [ 47 ]. He stated that computers do not have access to the meanings of the words and objects, as humans do. With the  X  X OPE X , as an example, someone can pull something, but not push it, or he can wrap something, but not eat it, etc. Even a child can describe more than a hundred applications of a rope, or any other object or words, in a few minutes. But, a computer cannot do so. Creating common sense knowledge base is a very demanding task, because (i) a huge amount of information must be captured, (ii) there is no proper method to represent the knowledge, (iii) updating the facts is very difficult, and (iv) there is a lack of efficient methods to use and make inference about that knowledge [ 18 , 43 ].
When large-scale dramatic problems have to be overcome within a limited budget, human intelligence can be harnessed using crowdsourcing. For example, it is common place to find true labels for large Machine Learning (ML) datasets through human computation. It means that human computation using crowdsourcing is becoming the spurt of AI and ML research. As the following section show, most human computation applications can be described as a set of labeling problems. Therefore, solving labeling problems using crowdsourcing, or briefly  X  X rowd labeling X , is becoming a hot research topic. Although crowd labeling is an important and growing field of inquiry, there has not been any notable survey concerning the area. Therefore, we believe that such surveys are not only helpful, but necessary as well.
This paper is aimed at introducing crowd labeling and its complexities, and surveying the literature that attempts to overcome those complexities. Then, it presents a unified statistical latent model where the differences among popular methods in the field correspond to different choices for the parameters of the model. The paper has both practical and theoretical contri-butions. It systematically exposes the main aspects of crowd labeling methods. Moreover, it compares the performance of different methods using comprehensive experiments.

The reminder of this paper is organized as follows: In Sect. 2 , we introduce crowd label-ing. Section 3 is where the target problem of the paper will be formally defined. Section 4 presents the assumed latent models in label aggregation process. Section 5 intends to survey the inference algorithms of latent models. Section 6 describes adaptive methods. Section 7 includes experimental results. Finally, concluding remarks, open issues, and guidelines for future work are presented in the last section. 2 Crowd labeling There are some constraints in using crowdsourcing for human computation. Since the par-ticipants are not experts, the problems are supposed to be normally small, simple, and well formed. In addition, due to human error and bias, it is mandatory to ensure that collected responses are adequately reliable. Dividing the main problem into several micro-problems is a helpful way to resolve the first challenge. And, there must be a quality control mechanism to overcome the second one.
Labeling (or multiple-choice) questions are the simplest well-known type of micro-problems in which the answering process is to simply choose one of the provided choices. Most of other more complex types of problems are convertible to this type [ 6 , 21 , 29 , 38 ]. And finally, there are efficient quality assuring mechanisms addressing labeling questions. Many applications have used labeling micro-problems to utilize humans X  power; for exam-ple, evaluating the accuracy of the results returned by search engines [ 23 ]; shortening and proofreading of documents [ 6 ]; quantifying the ability of native listeners to perform speaker recognition [ 59 ]; classifying galaxies from the Sloan Digital Sky Survey [ 52 ]; semantic simi-larity detection in maps [ 5 ]; and solving object recognition problems using visual 20 question games [ 10 ].

In order to remove noise, bias, and errors, the solutions provided by humans should be validated. In multiple checking mechanisms, the solution of each problem is requested from multiple labelers. Then, collected responses are aggregated (e.g., using majority voting) to find the final solution of that problem. In  X  X ollaborative tasks X , users build on or evaluate each other X  X  answers [ 50 ]. For example, in  X  X ind-fix-verify X  mechanism, some users specify the problems ( X  X ind X  operation), other users solve them ( X  X ix X  operation), and some others are supposed to confirm that the provided solutions are valid ( X  X erify X  operation) [ 6 , 37 ]. And, in Game With A Purpose (GWAP) scenarios, context-dependent proprietary procedures are used for quality assurance (see the next paragraph for more information). Multiple checking is the most frequently mechanism used for quality assurance in labeling problems.
Another issue in crowd labeling is finding and utilizing the humans. There is a large number of micro-problems in each application pertinent to crowd labeling. A large number of humans are required to solve these problems. Let us consider Content Based Image Retrieval (CBIR) using a game for example. ESP method [ 64 ] involves a large number of humans in an interesting game which subsequently results in some tagged images. In this game, the system randomly pairs two online users who neither know, nor communicate with each other. An image is displayed to the players, and during a specified period of time, the players independently guess the image content by presenting text tags. Considering the constraint that they are not allowed to use the words presented in a taboo list, the players win the game only if one of them presents a tag which has been already presented by the other (the taboo list is provided by the system in order to exclude the obvious tags or the tags previously obtained for that image in other games). The resulting tag is then used as a new annotation for that image. In this approach, a huge number of game players are required to collect tags for images in a real-world multimedia search engine database.

Since GWAP is not applicable to any given problem, labor marketplaces are used in most cases. Such marketplaces require a marginal cost to provide human responses to micro-problems. Amazon Mechanical Turk (MTurk) 1 is a famous crowdsourcing marketplace. It has been illustrated that the quality of the majority vote of multiple responses collected by MTurk is at least as good as that of answers provided by individual experts [ 62 ]. CrowdFlower, 2 CastingWords, 3 CrowdSpring, 4 Microworkers, 5 and MobileWorks 6 are other samples of labor marketplaces.
Push and pull are two approaches for task routing in crowdsourcing platforms. In the former, the system determines which tasks should be assigned to each user. In the latter, the system does not enforce explicitly its decisions, but rather does task routing implicitly by setting up the environment for workers [ 36 ]. Since game players have no choice in deciding which questions they are asked, the push approach is suitable for GWAP. There has also been some research projects on push approaches in labor marketplaces [ 12 , 25 , 26 ]. However, none of the famous marketplaces use this approach. In typical marketplaces, the problems are designed in a way which maximizes the probability of being selected by groups of users. This requires being aware of the criteria considered by the users in selecting the problems. Samples of such criteria are the time of importing the problems to the pool, the expiration time set for each problem, and the reward assigned to each problem. The proper values relevant to example, in order to assess the role of reward in MTurk for a set of problems [ 6 ], imported several problems with various amount of rewards to the system and measured the performance of the system in each case. They found that decreasing the amount of the reward does not affect the quality of the solutions, while it increases the waiting time. 3 Problem configuration The target problem of the paper is using a crowdsourcing marketplace (e.g., MTurk) to find high-quality labels for a large set of samples (e.g., providing labels for an ML dataset). The following assumptions are taken into considerations: (i) Budget is limited, (ii) a multiple checking mechanism is used for quality assurance, and iii) the pull approach is used for task routing (there is no task assignment mechanism). In addition, there is no limitation on the number of classes. However, in order to avoid complexities in tracking the subject, in some cases binary labeling will be considered. In most cases, the resulting algorithms and equations of binary labeling problems are extensible to general labeling problems.
Let us assume that there are samples X ={ x i } i = 1 : N with unknown true labels Y = { y x , i.e., the i th row of A is denoted by A i . The unseen labels in the matrix A are shown by 0. The total number of all collected labels for all samples (the budget) is limited, i.e., { X  y } i = 1 : N which maximizes P (  X  Y = Y | A ) .

The base method for integrating the collected labels and estimating the  X  y i sismajority voting. Label aggregation has been recently done on the basis of more efficient methods which are surveyed in the following sections. 4 Latent models To classify each sample, the labels provided by multiple individuals are aggregated. Majority voting is the base method for label aggregation. More advanced methods are capable of taking into account factors such as user capabilities and problem difficulties. These methods build statistical latent models that determine how the labels are generated by labelers. The graphical models of these latent models are shown in Fig. 1 . In these graphs, observed variables are depicted using shaded nodes and blank nodes represent latent variables. Moreover, arrows are used to represent the dependency among random variables. The plate structure is used to act as a for loop to represent repetition. As it can be seen, there are totally N samples with unknown true labels y i and there are R responses to each of them provided by users.
Figure 1 a represents the latent model for majority voting. According to this model, it is assumed that the provided labels for a sample depend only on the true label. That is, users provide correct answers to questions with a common known probability  X  (the users X  accuracy in answering questions is  X  ). In this method, Since log (.) is a monotonically increasing function, it may be shown that Considering  X   X  1, the familiar equation of majority voting appears. This indicates that P ( y i = y | A i ) is merely depends on  X  and the number of y votes. To realize the role of  X  and the number of participants in majority voting, suppose that z 1 ,..., z 2 L + 1 denote the collected labels for sample x i , in a binary labeling problem. Using ( 1 ), the correctness probability of the aggregated solutions is: where q indicates the probability that the majority vote is the correct answer, i.e., the proba-bility that more than L labelers propose correct labels. Moreover,  X  y i is the estimated label by majority voting for x i ,and k indicates the possible number of incorrect answers. According there is a growth in q , but a decrease in the rate of changes. For example, increasing the number of labelers leads to more significant results when  X  = 0 . 7, compared with the case where  X  = 0 . 9[ 60 ]. Figure 2 shows the value of q for different values of  X  , based on the number of collected labels per sample.

Majority voting is an intuitive and simple method. Moreover, its low computational cost makes it a frequently used method for label aggregation in crowd labeling. However, as it is depicted in Fig. 2 ,when  X  is small, a large number of labels are required to estimate the true labels accurately. Furthermore, it has some simplifying assumptions which are not true in real world. In most labor marketplaces, there are various kinds of people with different accuracies in answering, but majority voting does not consider the quality of responses. As an example, there are spammers that try to maximize their income by answering questions as fast as possible, without caring about the true answers.

More advanced methods, consider user capabilities. As shown in Fig. 1 b, these methods model the capability of user u j by a (set of) random variable(s)  X  j . Different methods have been proposed for modeling the user capabilities. One of the simplest methods is to model each user X  X  capability by a single random variable which corresponds to the user X  X  accuracy where  X  j  X  [ 0 , 1 ] .  X  j = 1 indicates that the user u j is an expert,  X  j 0 . 5 means that the user u j is a spammer, and  X  j &lt; 0 . 5 shows that u j is an adversary user. These methods simultaneously estimate the true labels and user X  X  capabilities by making inference on the model using the collected labels. The model reduces to majority voting if  X  j is considered equal for all users. The likelihood that the label provided by user u j is the correct label is a Bernoulli distribution, that is: In order to make the inference tractable, a Beta distribution,  X ( a , b ) ,on  X  j is usually chosen as the prior, which is conjugate to the labels X  likelihood, i.e., Bernoulli distribution [ 40 ]. The hyperparameters a and b represent the prior knowledge about the accuracy of users. Actually, the  X ( a , b ) prior over  X  j can be interpreted as observing a + b pseudo-labels from u j which a indicates that the most of the users are spammers, we should set a = b , and for the case that accuracies of most users is more than 0 . 5, a should be set greater than b . Moreover, a + b indicates the uncertainty in our prior knowledge about user accuracies. Therefore, in the case that we do not have any prior knowledge about the accuracy of the users, choosing a = b = where  X  0 + is appropriate. This is equivalent to Haldane distribution, 1  X 
When users have different accuracies, but a user can answer questions of different cat-egories with almost the same accuracy, the method leads to significant improvements. In addition, since the number of latent variables is small, a few observations are sufficient to efficiently estimate the latent variables. However, the method fails when the expertise of users on different categories varies a lot.

A common method to model the user expertises on different categories of problems is to model the capability of user u j by a confusion matrix,  X  j [ 15 ]. The element  X  j kl of the matrix measures the probability that the user u j may provide label l for a given problem whose true label is k . In binary labeling problems, confusion matrix reduces to sensitivity and specificity measurements, which indicate the proportion of actual positive and negative samples that are accurately recognized, respectively. In this method, Beta and its multivariate generalization, Dirichlet, are also considered as prior distribution over  X  j [ 39 , 40 , 54 ]. This method leads to very good results in applications that include problems with almost the same level of difficulties. Hence, this method is very popular and is followed in many other researches [ 28 , 31 , 61 ]. However, it should be noted that the number of latent variables in this method is more than the previous method, and hence, it requires more user labels to estimate the capability of users, i.e., its data complexity is higher than the previous method.
More complex methods consider problem properties beside user capabilities. There is evidence that it is useful for some applications to take the difficulty level of problems into account. For example, the experiments on different categories of problems in [ 65 ]depictthat using the same users for all problem categories leads to different results. In addition, in some categories the majority vote is better than the answers of the best user in the system, while in some others the opposite is true. Furthermore, in some problems the diversity of user X  X  answers is very high, while in some others it is low [ 11 ]. The general latent model for such methods is depicted in Fig. 1 c. As shown in the figure, in these methods, the difficulty level of problem x i is modeled by a (set of) random variable(s)  X  i . Different methods have been proposed for modeling the user capabilities and problem difficulty levels. In GLAD method [ 68 ], the difficulty level of problem x i is modeled by single random variable  X  i  X  X  0 ,  X  ) and the capability of user u j is modeled by r j  X  (  X  X  X  , + X  ) . In addition, the label generation model is assumed as the following logistic model, According to this model, the probability of providing correct label for sample x i by user u j increases as 1 / X  i decreases, and itapproaches 0 . 5as1 / X  i increases. Similarly, this probability that u j is a spammer and r j &lt; 0 indicates that u j is an adversary).

Moreover, Gaussian prior distribution is considered over r j . In addition, in order to ensure that  X  i can only take positive values, it was re-parametrized as e  X  and a Gaussian distribution is considered as the prior over  X  . GLAD X  X  ability to find spammers and adversary users, accompanied by its capability to model users with different levels of ability and problems with different levels of difficulty, makes it a suitable choice for label aggregation in real datasets. Moreover, its data complexity is low. However, it cannot distinguish user expertises in labeling different categories of problems. An equivalent method to GLAD is also proposed in [ 14 ], which assumes that  X  i s are known.

DARE is another method for modeling both user expertises and problem difficulty levels [ 4 ]. The method assumes that  X  j = r j and  X  i ={  X  i , X  i } ,where r j and  X  i respectively indicate the expertise of user u j and difficulty level of problem x i ,and  X  j determines how discriminative x i is. DARE assumes that if the user u j does not know the true answer of a question, she will answer it randomly. Moreover, according to this model, the user knows the true label of x i with probability p ij = ( distribution function, i.e., where C is the number of classes. In order to make the inference tractable, Gaussian priors are assumed over both user expertise and problem difficulty, and Gamma distribution was chosen as the prior over discriminations. DARE is a complex model and is suitable for the case that problems are very diverse in difficulty and ambiguity levels, and also the users answer to the best of their abilities, and hence, significant results are not expected in most general crowd labeling applications when using this method.

In [ 56 ], the GLAD method is extended. This method assumes that many other factors may be important in label aggregation. In this method, the difficulty of problem x i and expertise of user u j are respectively modeled by a set of random variables  X  i and  X  j .Itis also assumed that these factors are not pre-defined, but they emerge naturally from the data. In addition, a variable  X  j is also considered for user u j , which shows her bias. According to these assumptions, the label generation model is revised to: Although these factors are able to capture a wide range of variability in the correctness probabilities of labels, but since they are not pre-defined, they do not necessarily have high-level semantic meaning. Moreover, these latent factors do not share attributes across problems or labelers and lots of user labels are required to estimate their values. In order to solve these issues, the method assumes that there are sets of pre-defined features to be used in modeling problems and labelers. For example, demographic properties such as age, sex, and level of education are samples of such predefined features for labelers. In addition, the method assumes that the values of the features are known for all problems and labelers. Using the provided features, the method models the latent factor vectors as a linear combination of the pre-defined features and an unknown set of weights; that is,  X  j =  X  j W  X  ,  X  i =  X  i W  X  , and  X  j =  X  j w  X  ,where denotes the values of features set and W ( w ) is the weight matrix (vector) relating the latent factors to the features set. The main issue of this approach is the need to define the features sets and also providing their values for all labelers and problems. An almost identical method is proposed in [ 67 ]. 5 Inference algorithms As it was discussed in Sect. 4 , crowd labeling models can be formulated under a unified probabilistic framework. In this framework, each model corresponds to a generative process of the data using a set of latent random variables. This process specifies the joint probability distribution of hidden and observed random variables. Random variables can be categorized intothreegroups:Observedvariables( x ),e.g.,providedlabels A ,querylatentvariableswhose values we wish to know ( y ), e.g., the true labels of samples Y , and the nuisance latent variables ( z ), e.g., user capabilities  X  [ 58 ]. The common goal of all discussed methods is inferring the hidden structure that most likely generated the observed data. There are different approaches for making inference on these models and estimating the values of desirable latent variables.
In a fully Bayesian approach, the posterior distribution of the desired random variables given observed variables is computed by marginalizing over the nuisance random variables and conditioning on the observed variables, i.e., Here, we assumed that the latent variables are discrete. In the continuous case, the summa-tions are replaced by integration. Although this approach is quite simple since it only uses marginalization and conditioning, the required integrals are analytically intractable in most models, such as crowd categorization ones. However, utilizing independence relations among random variables can facilitate this process. Graphical models provide a graphical represen-tation for the dependency relations between random variables. Belief Propagation (BP) [ 49 ] is a Bayesian inference algorithm on graphical models which utilizes the independence rela-tions to reduce computational complexity. BP is a message-passing algorithm. It operates by sending local messages (the beliefs) between adjacent variables in the graphical model. The message v i  X  j is the belief of v i of what v j should be, which is based on messages to v i of all neighbors except v j . The messages are updated iteratively and in a parallel manner. A BP-like algorithm for crowd categorization is heuristically introduced in [ 34 ] for the model depicted in Fig. 1 b. Let x i  X  j and u j  X  i be real-valued messages from questions to labelers and from labelers to questions, respectively. Initializing u 0 j  X  i randomly from Gaussian dis-tribution N ( 1 , 1 ) or deterministically by u j  X  i = 1, the algorithm updates the messages at t -th iteration via, and the labels are estimated via  X  s t i = sign  X  x t i ,where  X  x t i = j  X   X  all neighbors of l in the graph, and S \ k excludes k from the set S . x and y play the roles of weighted majority votes and user reliabilities, respectively. In [ 40 ], BP is used to make inference on the graphical model depicted in Fig. 1 b. The authors show that the rule set in ( 9 ) is a special case of their algorithm when Haldane prior distribution is assumed as the prior over user reliabilities.

Singular Value Decomposition (SVD) is also beneficial to draw inferences from crowd categorization of binary problems. When A is a ( l , r ) -regular bipartite graph with l = r ,the rules in ( 9 ) are very similar to the ones in power iteration [ 8 ] which is a method to compute the leading singular vectors of a matrix. At the presence of matrix A N  X  R and two vectors u  X  R N and v  X  R R , power iteration starts with a random initialized v , and then it iteratively updates u and v according to: It is known that randomized u and v converge linearly to the leading left and right singular vectors. Inspired by this similarity, the following algorithm is proposed for binary catego-rization problems in [ 35 ]: 1. Compute the left and right singular vector of A , corresponding to the top singular values 2. Since both ( u , v ) and (  X  u ,  X  v ) are valid pairs of leading singular vectors, the mass of Note that the algorithm and the power iteration rules are not exactly the same. In the updating rules attributed to the algorithm, the received signals from the destination will be excluded ( X  \ j  X  X  in the algorithm). But, these signals are presented in the power iteration. In other words, the power iteration rules are indeed the simplified versions of those in the algorithm, because the latter approximates all different u i  X  j with a common u i .

Although BP reduces the computational complexity of Bayesian inference, still making inference in more complex models such as Fig. 1 c is intractable. Hence, approximate infer-ence algorithms are the favored solutions in these cases. Approximate Bayesian inference algorithms can be categorized into deterministic and nondeterministic classes. Nondetermin-istic methods are based on sampling methods such as Markov Chain Monte Carlo (MCMC) [ 44 ]. Although these methods are broadly applicable and can be applied to a wide range of distributions, in practice, Monte Carlo methods are computationally expensive and their com-putational demands often limit their use to small-scale problems. Therefore, using sampling-based methods such as MCMC are not very common in crowd categorization problems in which the size of the datasets are usually large, and hence, deterministic approaches such as variational methods have received great attention in this area. Variational approximation is a family of inference algorithms that approximate the desired posterior p ( y | x ) with a member of a family of distributions q ( y ) , working with which is tractable, e.g., Gaussian distributions [ 9 ]. In these methods, the measure for selecting the most appropriate q ( y ) from the set of candidate distributions is a member of the alpha family of divergences [ 2 ]. Mean field approximation is one of the most applicable variational methods [ 9 ]. It approximates the optimizes each of the base density functions such that their product becomes closer to the desired posterior. Mean field measures the distance between the approximated function and the posterior using Kullback X  X eibler divergence measures the distance between the approximated function and the posterior. Another variational inference framework is Expectation Propagation (EP) [ 45 , 46 ]. Similar to mean field approximation method, this method is also based on minimization of KL divergence between approximated function and the posterior but of the reverse form (note that the KL divergence is not symmetric). This makes the approximation rather different. Mean field is zero-forcing, i.e., underestimates the posterior variance, whereas EP is zero-avoiding, i.e., the approximated function overestimates the posterior variance [ 9 ]. In [ 40 ], mean field approximation is used to make inference on the graphical model presented in Fig. 1 b. This method approximates the posterior distribution P ( Y ,  X  | A ) by the factorized method uses block coordinate descent method to estimate factors f i ( y i ) sand g j ( X  i ) sthat alternatively optimizes the factors. The authors show that their approximation method is closely related to EM algorithm, which will be described in the sequel. Moreover, in [ 4 ]an approximation inference method based on EP is used to calculate the marginal distributions on the underlying factor graph corresponding to the model depicted in Fig. 1 c by iteratively calculating messages along edges that propagate information across the factor graph.
Another approach for estimating the latent variables of probabilistic models is finding a point estimate for query latent variables by optimizing an objective function such as the log-likelihood function, In order to find likelihood function, we have to marginalize over nuisance variables. Since this is time-consuming, a common method Expectation Maximization (EM) is used. EM is a two-stage iterative algorithm for finding maximum likelihood estimates. In crowd categorization models, EM iteratively (i) estimates the true labels according to the current estimates of model parameters and (ii) re-estimates the model parameters based on the current beliefs about the true labels. For example, in [ 15 ] the likelihood is, where is the set of all user X  X  confusion matrices (  X  j ik s are the elements of the confusion number of labels C l which is assigned to problem x i by users u j . Also, T iq = 1, if the label C q is a true one for the problem x i , and it is zero, otherwise. Maximization of L is a complicated task, and hence, EM algorithm helps to estimate the parameters. In E step, true labels are estimated by user X  X  confusion matrices and prior probabilities included in the previous step. C k is the label of x i with the following probability: Maximizing the likelihood through the estimated labels in M step leads to the following estimations for parameters: Based on prior probabilities for parameters in the latent model, the joint probability can replace the likelihood in EM.

As the last category of methods, we describe methods which use Matrix Factorization (MF) to estimate missing (unseen) values of sparse matrices before making inferences. The goal of MF is to factorize the matrix A ; for example, as factors U k  X  N and V k  X  R ,where and P (  X  ) indicates the observed elements of the given matrix. Then, A = U T V contains the observations and estimates of unseen entries. The described SVD-based method in the earlier paragraphs is based on MF. It approximates the target matrix with a rank-1 matrix, but it uses sum-squared distance between all entries of target and estimated matrices. When A is sparse, the distance should only be computed for the observed entries of the target matrix. This may lead to a complex nonconvex optimization problem [ 63 ]. In such situation, it is possible to use Probabilistic Matrix Factorization (PMF) which is a well-known approach in collaborative filtering [ 57 ]. It induces a latent feature vector for each person and example, in order to infer unobserved user ratings for all examples. In a similar way, PMF is used in crowd categorization to estimate the unseen labels. PMF can be considered as an inference algorithm on the model depicted in Fig. 1 c. In PMF, column vectors U i and V j , respectively, represent k -dimensional labeler-specific and question-specific latent feature vectors, and the conditional distribution over the collected labels, using the Gaussian distribution, is defined as, Also, zero-mean spherical Gaussian priors are considered for latent feature matrices, i.e., To estimate model parameters, PMF maximizes the log-posterior distribution over feature matrices with hyper-parameters. The maximization is equivalent to minimizing the following squared error with L 2 regularization, where  X  U =  X  U  X  ,  X  V =  X  V  X  ,and 2 F denote the Frobenius norm. U and V are found by gradient descent algorithm, and missing values of A are inferred by taking the inner product of estimated U and V . The equation in ( 19 ) is the same equation as in ( 16 ) with an additional regularization term. Finally, it should be noted that any inference algorithm can be used to estimate the true labels from the collected and estimated unseen labels. For example, majority voting as well as [ 15 ] methods are used in [ 30 , 31 ], respectively.

In Table 6 (in Sect. 8 ), you can see the inference algorithm types of all major surveyed method. 6 Cost-efficient methods Crowd labeling methods can be categorized into three main groups: one-shot, inductive, and adaptive. So far, all methods that have been discussed are one-shot; that is, a number of labels are collected for a set of problems, and the true labels are estimated by aggregating the user labels.

Inductive methods collect a set of labels for a limited number of samples and use the 33 , 53 , 54 , 69 ]. Moreover, these methods can also collect labels in an active manner. That is, each sample is either classified by the classifier, or its label is estimated using crowd labeling and is used to improve the classifier [ 20 , 48 , 60 ]. In inductive methods, each sample must be described as a feature vector. Moreover, a criterion is required to decide whether to acquire the label from the classifier or the crowd [ 22 ]. Hence, the success rate of inductive methods depends on (i) the transformation method from the object space to the feature space, (ii) the type of classifier model, (iii) the decision criterion, and (iv) the quality of estimated labels using collected labels from the crowd. The first three factors are out of the scope of this paper. Therefore, we do not survey inductive methods.

Adaptive methods spend budget economically and collect the labels iteratively. These methods collect a set of labels in each step, based on the assumed latent model and current collected labels. They use the new acquired labels to re-estimate the true labels and update the latent model; that is, the updated model in each step participates in selecting samples in the next step. It is obvious that an oracle procedure selects samples for which acquiring new labels maximizes the overall performance. In the rest of this section, we describe adaptive methods for crowd labeling.

In this section, we assume that the labeling cost is equal for all samples, and only one label is requested for one of the samples in each step, based on a sample selection criterion. The simplest criterion selects the sample with minimum number of currently collected labels. The result is an approximately equal number of labels for all samples. This criterion is called  X  X niform sample selection X . As samples with low-difficulty levels need fewer labels, in most cases uniform criterion wastes the budget.

Heterogeneity of collected labels is a nonuniform criterion. It selects a sample that enjoys the minimum heterogeneity of its current collected labels. Considering the same level of reliabilities for all users and uniform prior over possible labels, heterogeneity of collected labels for sample x i can be measured using the entropy of P ( y i | A i ) , which follows a beta distribution [ 27 , 60 ]. Therefore, heterogeneity criterion selects sample x i  X  to request a new label where, Entropy has a bias toward selecting the samples with more labels. For example, in first steps it never selects samples with only one label in their current labels set. Therefore, using entropy results in having a few samples with many labels and several samples with few labels. Moreover, since entropy does not consider the labeler capabilities, it does not request more labels for samples with heterogeneous wrong labels.
 The uncertainty in estimated labels is another nonuniform criterion for sample selection. In [ 60 ], uncertainty is defined as the probability that the majority label of a sample is not true, i.e., ties and the considered prior distribution over possible labels is uniform. In this configuration, P ( y i | A i ) follows a Beta distribution  X ( a + 1 , b  X  1 ) where a and b are the number of collected labels from different categories. Therefore, where I x ( m , n ) is the cumulative distribution function of the beta distribution.
Both entropy and uncertainty criteria can be extended to consider user reliabilities by using more complex models described in Sect. 4 . The main weakness of both of these criteria is that they choose samples only based on current collected labels, i.e., they are blind to the result of their decisions. The methods that are aware of their decisions must measure the effect of selecting a sample by using a defined indicator. The proposed criterion in [ 4 ]defines the uncertainty in estimates of user model parameters as the indicator. It selects a sample which acquiring a new label is expected to maximize the reduction in the defined indicator. The method measures the uncertainty in estimates of model parameters by the entropy of posterior distributions of the parameters. The method assumes that P ( X  j | A ) = N ( X  j , X  j ) next provided label by the crowd. Since a ij is not known, the criterion uses the expectation, i.e., the criterion selects sample x i  X  to request a new label where, where H indicates the entropy function. It can be shown that if Gaussian distributions are considered over user model parameters, then [ 4 ], This criterion, which is called as AccsExpEntRed in the experiments, leads to find more accurate user model parameters.

To focus on more accurately estimated labels, the future indicator can be defined as the uncertainty in estimates of true labels. The uncertainty can be measured by using the incorrectness probability of the estimated labels. The criterion selects sample x i  X  to request a new label where,
The main goal of sample selection is to identify challenging problems and spend money efficiently when collecting labels. A criterion to distinguish difficult problems from simple ones is that getting a new label for challenging samples does not necessarily reduce the uncertainty in their estimated labels. Specially, when there are many unreliable labelers and the observations are not sufficient to accurately estimate their capabilities. Therefore, a method to recognize difficult problems is to choose a sample for which getting a new label is expected to maximize the uncertainty in its estimated label, i.e., such a criterion selects We refer to the two proposed criteria as ExpAccUnc and AccUnc in the experiments. 7 Experimental results 7.1 Test setup We implemented the major surveyed methods, and then, we compared them using human and synthetic datasets. All reported results are averaged over 10 runs. Available labels for each samples are shuffled in each run, and all methods are used the same shuffled data in that run. Every time an algorithm requests a label for a sample, the first unseen label of that sample is returned. After getting on the average one new label per each sample, the accuracy of the method is re-calculated. In addition, the labels are converted to ordered ranks, in Probabilistic Matrix Factorization (PMF). Each data matrix is factorized by using decomposition ranks k the factors with lowest estimation error are selected as the result. The estimation error is considered as the mean squared errors of the known values and their corresponding estimates. Moreover, in the PMF X  X  iterative algorithm U and V are initialized with small Gaussian random values. In addition, the algorithm stops when error becomes less than 0 . 01 or number of iterations reaches 1,000. Furthermore, the number of iterations in Belief Propagation (BP) is considered as the average number of labels per sample. Moreover, BP stops when the estimated labels remain unchanged from the previous step, or the number of iterations reaches 100.

The implemented methods are listed in Table 1 . The abbreviation column indicates the short names given to methods which appear in subsequent plots and result tables. Note that since models with multidimensional parameters [ 56 , 67 ] require external information about labelers X  or problems X  features, we did not consider them in our experimental comparisons. 7.2 Datasets The implemented methods are compared using the following datasets:  X  Recognizing textual entailment (RTE) In RTE dataset [ 62 ], users are given two sentences  X  Temporal event recognition (TEMP) In TEMP dataset [ 62 ], the users must choose one  X  Duchenne In each face image of the Duchenne dataset [ 68 ], MTurk users are asked  X  Synthetic datasets Multiple synthetic datasets are generated to evaluate methods on multi-Table 2 contains the properties of the human datasets. Figure 3 depicts the ratio of labelers with various levels of accuracies, for each dataset. The ratio of provided labels by the labelers with various levels of accuracies is also depicted in the figure. These properties indicate that the Duchenne is the most challenging dataset, because there are fewer crowd labels per sample; it is full-rank; adversary and unreliable labelers have provided most of the collected labels; the ratio of correct crowd labels to all crowd labels is less than other datasets. In a similar manner, it may be inferred that TEMP is the easiest dataset. 7.3 Results and analyses The results of comparing different methods on three datasets 2-class-Duchenne, 3-class-RTE and 4-class-TEMP datasets as samples are shown in Tables 3 , 4 ,and 5 , respectively. 7 In all tables, the best results are given in bold. In addition, the results of one-shot methods on 2-and 3-class samples for RTE dataset are depicted in Fig. 4 .
 One-shot methods TEMP is the easiest dataset. According to our expectations, the simpler methods lead to good results. Experiments show that the methods that only model user capabilities lead to best results, but DARE which models the problem difficulty levels has poor performance (Table 5 ). In RTE, we observed almost the same results as the TEMP dataset (Table 4 ;Fig. 4 ). Since Duchenne is the most challenging dataset and it contains ambiguous problems, we expect to have best results using methods that model both user capabilities and problem difficulty levels. The results meet our expectations (Table 3 ). PMF-based methods The role of matrix completion (using PMF) prior to label aggregation isshowninTable 3 and is illustrated in Fig. 5 . PMF estimates a sparse matrix using a low-rank complete matrix. Since all used datasets form almost full-rank data matrices, we do not expect significant results from these methods. Experimental results are compatible with our expectations. In addition, our experiments show that PMF re-estimate the known values well, but it is not able to estimate unseen labels, perfectly. The main reason is that PMF uses mean squared error to measure the distance between the original and the estimated matrices, which is unsuitable for categorical data.
 Adaptive methods The results show that almost all methods lead to good results in case of having sufficient crowd labels. Adaptive methods lead to better results than the one-shot methods, when a few number of labels per sample are available. In TEMP, the easiest dataset, entropy and uncertainty criteria lead to best results, while the other more complex criteria did not add any value to these simple ones (Table 5 ). In RTE, the criteria that considers the future had better results than the simple criteria (Table 4 ;Fig. 6 ). And, in Duchenne that has most adversarial users, as we expected AccUnc leads to best results, while entropy and uncertainty had poor performances (Table 3 ). 8 Conclusions In this paper, we addressed crowd labeling and surveyed the literature using a unified prob-abilistic framework. Crowd labeling is a popular and growing field which concentrates on solving labeling problems using human crowds. Although this approach has revealed signif-icant performance in different applications, there is no survey associated with its technical aspects. This paper attempts to introduce the challenges associated with this approach and investigates existing methods to overcome these challenges. Since most of these methods model crowd labeling as a statistical inference problem, we proposed a general probabilistic model and showed that all surveyed methods can be considered as a special case. We then described inference algorithms on these models. Sample selection criteria are introduced for the case when crowd labels are collected adaptively. In addition, the applications suited to the methods and the relationship among them has been studied. Finally, we compared crowd labeling methods based on their merits and demerits and confirmed our analysis using extensive experiments. The surveyed methods are summarized in Table 6 along with their parameters and inference algorithms.

Several directions of future research are possible. Most of the current crowd labeling models are designed to be used in a wide range of applications, and hence, they need to be very complex. However, these methods make many simplifications for tractability which are not true in many cases. There are two approaches to overcome this challenge: Relaxing the simplifying assumptions and designing task-driven models. There are many aspects in crowd labeling which should be considered in designing a model. For example, most of the existing methods cannot be applied to multiclass problems [ 34 , 68 ]. Moreover, the correlation among users are usually ignored [ 1 , 51 , 54 ]. In order to overcome these issues in general, more complex models are required. However, complex models need much more labels to estimate the true labels which is not cost-efficient. Also, analyzing these methods and providing error bounds is not possible in most of these methods. Therefore, it seems reasonable to switch to application-specific models [ 56 , 67 ] which can use the problem domain properties to simplify the model without nonrealistic assumptions.

There are various applications that can benefit from crowd labeling, but are not compatible with the configuration of current methods. For example, there are different online classifi-cation tasks that can utilize crowd labeling to reduce their costs. However, these methods need a pool of available labelers that can provide labels for the incoming data. To the best of our knowledge, there are no marketplaces that support this kind of task, and there are very limited number of methods that are designed for online classification using crowd labeling [ 7 ]. Online label acquisition, user expertise modeling in different categories of problems, and handling concept drift in data streams using crowd labeling are samples of requirements of new applications such as market prediction and online advertisement [ 3 ].

Connecting crowd labeling with related theoretical fields is necessary. For example, since collected crowd labels can be presented as a sparse matrix, working on theories and methods of sparse representation for application on crowd labeling may be a fruitful topic for future work. Probabilistic Matrix Factorization (PMF) has recently been used to estimate unseen crowd labels [ 30 , 31 ]. We note that using PMF for crowd labeling requires further work. For example, as our experimental results showed, PMF leads to poor results, because it has its roots in collaborative filtering (CF) which innately differs from crowd labeling; that is, it deals with ranking data and there are strong correlations between the rows and also the columns of the data matrix in CF. Here, we provide some guidelines to adjust matrix factorization for crowd labeling. Since the data matrix of real crowd datasets is almost full-rank and eliminating the wrong labels reduces the rank of the matrix to 1, outlier detection is a crucial element to consider before factorization. As a solution, outlier detection can be considered as a term in the objective function of the factorization problem [ 70 ]. Moreover, a proper distance function must be used in the objective function instead of sum-squared distance, which is not suitable for categorical data.
 References
