 1. Introduction similarity measure.

Fo r the proposed specific choice of a similarity measure we generate a k -means like 332 KO GAN, TEBOULLE AND NICHOLAS similarity measure by the squared Euclidean distance.

The outline of the paper is the following. In Section 2 we present general batch and Appendix. 2. k -means like algorithms k -means type clustering algorithm: R . Consider a partition ={  X  1 ,..., X  k } of the set, i.e., that satisfies the following basic properties: norm vectors one has d ( x , y ) = 1  X  x = y .
 DATA DRIVEN SIMILARITY MEASURES 333
To describe the relation between q and d we define a centroid c of a cluster  X  by the exposition in what follows we shall primarily address the case  X  X pt = min X  keeping 1. Set t = 0. 3. Apply (4) to recompute centroids. 6. Stop.
 334 KO GAN, TEBOULLE AND NICHOLAS x  X  the problem presented by Example 2.1. We now briefly discuss the incremental k -means  X  j is made by the batch k -means algorithm based on the sign of DATA DRIVEN SIMILARITY MEASURES 335 reassignment is performed. To formally present the procedure described above we need additional definitions.
 {  X  assigning this vector to an existing cluster  X  j of .
 function.
 v ariation one has
To escape the local minimum trap we augment iterations of the batch k -means by an squared distance like function. 2. Generate the partition nextKM ( ( t ) ). 3. Generate the partition nextFV ( ( t ) ). 4. Stop.
 Due to the additional Step 3 Algorithm 2 always outperforms batch k -means in cluster 336 KO GAN, TEBOULLE AND NICHOLAS (here x = ( x [1] ,..., x [ n ]) T  X  R n ).
 f amily of distance-like functions and an optimization technique that solves (4). 3. Optimization approach to a simple solution of the centroid computation problem.
 define m v ectors DATA DRIVEN SIMILARITY MEASURES 337 set S  X  R n is defined by (see e.g., (Rockafellar 1970)): and a simplex  X  R k is otherwise. Keeping in mind inf x f ( x ) = X  sup x { X  f ( x ) } we get { c centroids at once.
 ( c ,..., c c [ n ]i s denoted by  X  c due to (8) one has Furthermore, a straightforward computation shows that { . x fined through the kernel 338 KO GAN, TEBOULLE AND NICHOLAS and non-negative scalars  X  and  X  :
Note that when  X  = 0 and  X  = 1, the second term in (11) side of (12) reduces to  X  resulting (12).

The choice of the distance (11) which combines a squared Euclidean distance with the development of various optimization algorithms, see (Auslender et al. 1999).
To justify the application of the gradient equation (9), we consider the two cases: 1.  X &gt; 0 , X  = 0.
 DATA DRIVEN SIMILARITY MEASURES 339 2.  X &gt; 0.
 3.1. Centroid computation the vector  X  c
A straightforward computation (see (10)) leads to the following: To simplify the above equation, define (for convenience we omit the indices): 340 KO GAN, TEBOULLE AND NICHOLAS We remind the reader (see (7)) of the following: of clusters should be decreased, and c j should not be recomputed. We now assume that in the non-negative solution of quadratic Eq. (19) formula: and gives through (20), the centroid c j = m (  X  j ). 4. Experimental results Dhillon and Modha (2001). In one of the experiments of Dhillon and Modha (2001) the spherical k -means algorithm was applied to a data set containing 3893 documents. This data set contains the following three document collections (available from ftp://ftp.cs.cornell.edu/pub/smart ):  X  Medlars Collection (1033 medical abstracts),  X  CISI Collection (1460 information science abstracts),  X  Cranfield Collection (1400 aerodynamics abstracts).
 DATA DRIVEN SIMILARITY MEASURES 341 by the algorithm. After removing stopwords (Dhillon and Modha 2001) reported 24,574 4,099 w ords to construct the vector space model (Berry and Browne 1999).
Our data set is a merger of the three document collections (available from http://www. cs.utk.edu/~lsi/ ):  X  DC0 (Medlars Collection 1033 medical abstracts)  X  DC1 (CISI Collection 1460 information science abstracts)  X  DC2 (Cranfield Collection 1398 aerodynamics abstracts) The Cranfield collection tackled by Dhillon and Modha contained two empty documents. These two documents have been removed from DC2. The other document collections are identical.
 row. We regard the document collection corresponding to the  X  X ominant X  column as the the k -means like algorithm with the distance function (11). When the outcome of sPDDP 342 KO GAN, TEBOULLE AND NICHOLAS DATA DRIVEN SIMILARITY MEASURES 343 the  X  X xtreme X  values of the objective functions.

While the number of selected terms is only 600 (i.e., only about 15% of the number of 344 KO GAN, TEBOULLE AND NICHOLAS generated by sPDDP further improves the confusion matrix (see Tables 5 X 7). We pause briefly to discuss some properties of the (  X ,  X  ) algorithm. 2. In the extreme case  X  = 1,  X  = 0 the classical k -means algorithm is recovered. 3. When  X  = 0,  X  = 1 and the document vectors are normalized in l 1 norm the (  X ,  X  )
T able 8 summarizes clustering results for the sPDDP algorithm and the combinations of We next show by an example that this indication is not necessarily always correct. The graph below shows the number of misclassified documents for 600 selected terms and 25 The graph indicates the best performance for  X   X  = 500, 600, 700, 1000, and 1100. choices of index terms. 5. Concluding remarks and future work DATA DRIVEN SIMILARITY MEASURES 345 with  X  X ntermediate X  parameter values.
 algorithm. We plan to focus on the following problems: 4. We plan to run the experiments on a variety of large document collections. A ppendix special case x i 1 = 1, i = 1 ,..., m only.
 x  X   X  1.  X   X  1 a cluster obtained from the cluster  X  1 be removing x from  X  1 , 2.  X  + 2 a cluster obtained from the cluster  X  2 by assigning x to  X  2 . Our goal is to evaluate for two extreme cases of the (  X ,  X  ) algorithm described below. 346 KO GAN, TEBOULLE AND NICHOLAS Squared euclidean norm (  X  = 0 ) follows: Information X  X heoretical distance (  X  = 0 ) step may in fact be much lower than the cost reported in Berkhin and Becher (2002).
First we consider cases of vector  X  X emoval X  and  X  X ddition X  separately.  X   X  ={ x
Since ( p  X  1) c  X  = p c  X  x one gets the following: DATA DRIVEN SIMILARITY MEASURES 347
Finally,  X   X  ={ x
We use the identity ( p + 1) c + = p c + x to obtain: and q (  X  )  X  q (  X  + )isgivenby The expression for 348 KO GAN, TEBOULLE AND NICHOLAS follows in a straightforward manner from (23) and (24): is about the same as that of (22). Indeed, 4. The document vector x is always sparse, hence the number of operation required to associated with steps 1 and 3 above.
 Acknowledgments of the results.
 References DATA DRIVEN SIMILARITY MEASURES 349
