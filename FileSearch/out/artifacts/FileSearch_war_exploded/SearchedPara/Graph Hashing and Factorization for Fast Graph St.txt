 Graph stream classification concerns building learning models from continuously growing graph data, in which an essential step is to explore subgraph features to represent graphs for effective learn-ing and classification. When representing a graph using subgraph features, all existing methods employ coarse-grained feature rep-resentation, which only considers whether or not a subgraph fea-ture appears in the graph. In this paper, we propose a fine-grained graph factorization approach for Fast Graph Stream Classification ( F GSC ). Our main idea is to find a set of cliques as feature base to represent each graph as a linear combination of the base cliques. To achieve this goal, we decompose each graph into a number of cliques and select discriminative cliques to generate a transfer ma-trix called Clique Set Matrix ( M ). By using M as the base for for-mulating graph factorization, each graph is represented in a vector space with each element denoting the degree of the corresponding subgraph feature related to the graph, so existing supervised learn-ing algorithms can be applied to derive learning models for graph classification.
 H.2.8 [ Database Applications ]: Data mining Algorithms, Experimentation, Performance Graph stream, matrix factorization, mining algorithm
Recent advances in data acquisition have resulted in an increas-ing number of applications involving stream of graph data, exam-ples include Bioinformatics and social networks or information net-works [2]. To support graph classification, existing methods com-monly employ an occurrence-based feature representation model, in which a set of subgraph features is used to represent the graph data by using the occurrence of the subgraph in the graph (either 0/1 occurrence or actual number of occurrences) as the feature value. In this paper, we refer to this occurrence based representation model as coarse-grained representation model . An example is shown in Fig. 1. While the coarse-grained feature representation has been popularly used for graph classification, it has a number of disad-vantages, including: Computation burden for isomorphism val-idation: Because subgraphs are selected as features to represent the graph, the expensive subgraph mining and isomorphism valida-tion process can not be avoided (graph isomorphism is known to be NP-hard) [4]. Severe and unbounded information loss: Be-cause the coarse-grained representation does not characterize the degree of closeness of the subgraph pattern related to the graph, it suffers information loss in representing the graph. Furthermore, for a graph stream with continuously growing volumes ( i.e. an increasing number of graphs) and changing structures ( e.g. new nodes may appear in the coming graphs), the disadvantage of the existing coarse-grained graph feature model is even greater. This is because (1) stream volumes continuously increase, which makes it computationally expensive to explore subgraph features, and (2) the changes in the graph stream (such as new structures) make the subgraph features incapable of representing graphs.

In this paper, we propose a fine-grained graph factorization frame-work for efficient graph stream classification. Being fine-grained, our mapping framework relies on a set of discriminative frequent cliques, instead of frequent subgraph patterns, to represent the graph. Such a fine-grained representation ensures that the final instance-feature representation is sufficiently close to the original graph, with theoretical guarantee. To solve the problem, our algorithm (1) finds a set of frequent graph cliques as the base; and (2) uses graph factorization to calculate a linear combination of the graph cliques to best represent a graph. Compared to the traditional coarse-grained representation model, a clear advantage of our method is that it can ensure minimal information loss for graph representation, and also avoids expensive subgraph isomorphism validation process. As shown in Fig.1, the original graphs are represented in more details by using fine-grained feature mapping. Even though g 3 does not appear in G 1 , there is still a feature value (0.09) because the two nodes of g 3 indeed appear in G 1 .
D efinition 1. Connected Graph : A graph is represented as G = E  X  X  X V denotes a set of edges, and L = { l 1 ; l 2 ;  X  X  X  ; l set of symbol labels for vertices and edges. A connected graph is a graph such that there is a path between any pair of vertices.
Definition 2. Adjacency Matrix : An adjacency matrix of G (which contains Q unique nodes) is denoted by A ( G )  X  R where each entry a ij denotes the weight of the edge from vertex v to vertex v j . And a ii means the label of node v i .

We use adjacency matrix to represent a graph and assume that each edge has a default weight 1. A graph G i is a labeled graph if a class label y i  X  X  is assigned to G i . For the binary classification problem, we have y i  X  Y = { X  1 ; +1 } . A graph G i is either labeled (denoted by G L i ) or unlabeled (denoted by G U i
Definition 3. Graph Stream : A graph stream S contains an in-creasing number of graphs arriving in a streaming fashion with S = { G 1 ; G 2 ;  X  X  X  ; G i ;  X  X  X } . At any particular time point, we can col-lect a batch of graphs ( B ) for analysis. Formally, S = where B j = { G j 1 ; G j 2 ;  X  X  X  ; G j k
The aim of graph stream classification is to learn a classification model from S , at any particular time point, by scanning the graph stream only once, and predicting the class labels of future arrival graphs in the graph stream with maximal accuracy.
Given a graph encoded by an adjacency matrix, our goal is to use discriminative feature-patterns to precisely represent the graph. In this paper, we propose to use cliques as feature-patterns to rep-resent graphs. There are several advantages of using cliques as feature-patterns: (1) Finding cliques does not require a compli-cated subgraph mining process (which is computationally expen-sive). The special structure allows us to develop a fast algorithm to find cliques. (2) Because two nodes v i and v j in a clique are connected, there is no need to describe edges in the clique, so we can simply use all nodes appearing in a clique to form a vector to represent the clique.

We assume that the size of the feature set (which is a clique set), denoted by N , and the size of graph node space, denoted by Q , are given. Our model is parameterized by w z and m jz , where w denotes the expected weight of clique c z , and m jz represents the occurrence of vertex j in clique c z . The expected weight of the edge that lies between vertices i and j in clique c z can then be re-written by m iz w z m jz . If we sum up the cliques in a given clique-pattern set C , where | C | = N , the expected weight of the edge that lies between vertices i and j can be represented as
From Eq. (1), we have an expected adjacency matrix e A as fol-lows, where M is a Q  X  N binary matrix and W is a N  X  N di-agonal matrix.
In Eq.(2), each row of M means a node index which is the same as the node index in e A ( G ) . Each column of M corresponds to the occurrences of the nodes in a selected clique-pattern c  X  C . Figure 2: T he framework of F GSC for graph stream classification
The decomposition of the adjacency matrix e A ( G ) can be inter-preted in terms of overlapping weight theory. More specifically, denote the i th column of M as M  X  i . We let  X  i = M  X  i Eq. (2) can then be rewritten as: We refer to  X  i as the basic element. Because the i th column of M is a clique,  X  i is the binary matrix in which each diagonal el-ement means a vertex of the graph that does or does not appear in this clique and remaining nodes in the binary graph matrix repre-sent edge information in the clique. We use W  X  R N  X  N to denote the clique interaction matrix, which can also be considered as the clique degree matrix. As a result, Eq.(3) means that a graph can be represented as a set of cliques with different weight values ( w For a given M , we can decompose a graph into a diagonal ma-trix W = [ w 1 ; w 2 ;  X  X  X  ; w N ] T , which can be used to represent the graph. This is equivalent to mapping graphs into a vector space. In reality, M is a non-square matrix and W k may not exist such that A ( G k ) = M W k M T for a given graph G k .
 Therefore, we need to find an optimal f W k that satisfies, By using squared loss to measure the relaxation error, we have, where RE (  X  ) is the function of computing relaxation error. Then the optimization problem of Eq.(4) can be formulated as
To solve Eq.(6), by combining Eqs.(2) and (3), we have
We define vec (  X  ) as the matrix vectorized operator.
Let P = [ vec ( X  1 ) ; vec ( X  2 ) ;  X  X  X  ; vec ( X  N )] . Then where diag (  X  ) denotes the main diagonal of a matrix.
To solve Eq.(9), we employ the General Linear System Theorem proposed in [6].

T HEOREM 1. General Linear System Theorem : Let there exist a matrix B such that B y is a minimum 2-norm least-squares solu-tion of a linear system Ax = y . Then it is necessary and sufficient that B = A  X  , the Moore-Penrose generalized inverse of matrix A.
Based on Theorem 1, the smallest 2-norm least-squares solution of the above linear system (Eq.(9)) is where P  X  is the Moore-Penrose generalized inverse of matrix P .
By using the above factorization results, we can use diag ( to map each graph G k into a vector space, which is called fine-grained representation for graph G k . Because our factorization process ensures that the multiplication of Clique Set Matrix M and the diagonal matrix W can best approximate the adjacency matrix of graph G k with A ( G k )  X  e A ( G k ) = M f W k M T .
The proposed graph stream classification framework ( F GSC ), as illustrated in Fig. 2, contains three key steps: (1) Clique Min-ing, (2) Clique Set Matrix and Graph Factorization and (3) Graph Stream Classification. We will propose the details of these three parts in following subsections.
Our first step is to select a set of cliques from each graph in the stream. In stream scenarios, the node set of the graph stream can be extremely large and unlimited. In addition, new nodes and structures may appear in the graphs, and it is therefore necessary to compress each incoming graph into a fixed node space. We use a random hash function to map the original unlimited node set onto a significantly compressed node set  X  of size Q . In other words, nodes in the compressed graph of G k will be re-indexed by { 1 ;  X  X  X  ; Q } . Because multiple nodes in G k may be hashed to the same index, the weight of one edge in the compressed graph is set to the number of edges between the original nodes which are hashed onto the compressed nodes. After hashing, each graph G k  X  S is transferred into a compressed graph G  X  k for clique mining. We de-fine this node hashing strategy using G  X  k := NodeHash ( G Algorithm 1 C lique Mining Input: g raph G k 1: G  X  k  X  NodeHash ( G k ; Q ); 4: while The weight of each edge in G  X  k unequal to 0 do 6: for all c  X  C G  X  7: c  X  MinimalW eight ( c ); 8: end for 10: G  X  k  X  UpgradeW eights ( G  X  k ) 11: end while An example of clique mining is illustrated in Fig. 3, where ste p (B) shows the compressed graph.

To discover cliques from compressed graphs, we propose the fol-lowing approach. According to the graph compression results, the weight of one edge in the compressed graph denotes the number of edges which exist in the original graph. Our clique mining process can take this information into consideration. More specifically, our maximal clique set mining process includes: Finding Maximal Cliques: We first discover maximal cliques, C
G  X  k , from the compressed graph G  X  k (where no maximal cliques pressed graph G  X  k are unique, finding maximal cliques from G very efficient (we use the Bron-Kerbosch maximal clique finding algorithm in our experiments). An example of the maximal clique c is shown in Fig. 3 (C) upper panel; Clique Extraction: The weight of each edge in the discovered maximal clique c  X  C G  X  edges in c . After that, we decrease the edge weights of compressed graph G  X  k by the weight of the corresponding edges in C generate a new graph G  X  k . An example of the updated graph G after extracting clique c 1 is shown in Fig. 3 (C) lower panel; Loop: If the weight of any edge in the updated graph G  X  k means that the edge has already been removed, and no longer needs to be considered in the following clique mining process. Continu-ing the loop between the above two steps will discover all cliques until all weight values of edges in G  X  k are equal to 0. An example is shown in Fig. 3 (E) lower panel.
 Algorithm 1 describes the detailed process of clique mining, where MaximalCliques ( G  X  k ) finds the maximal cliques from G MinimalW eight ( c ) sets input clique c  X  X  edge weight to the small-est weight value of all edges in c . UpgradeW eights ( G  X  the weight values of graph G  X  k .
In this subsection, we introduce the detailed process for finding a set of discriminative frequent cliques to form the Clique Set Matrix ( M ) for graph factorization.
To find a good set of cliques to form base M for graph factoriza-tion, we use correlations between each clique and graph labels ( i.e. similar to Information Gain [5] and G-test score [7]) to find cliques that are highly correlated to the classification task. For fast discrim-inative clique finding, we use an  X  X n-memory X  Clique-class table  X  Figure 4: A n example of  X  X n-memory X  Clique-class table  X  , where Q = 4 with ( Q + |Y| + 2) columns to count the frequency of each clique with respect to each class label. An example of an  X  X n-memory X  clique-class table is shown in Fig. 4.

For each graph G k in the stream, we first use Algorithm 1 to collect its clique set C k out . After that, for each clique in C c k;j , we apply a random hash function h (  X  ) to the string of ordered edges in c k;j to generate an index H k;j  X  { 1 ; 2 ;  X  X  X  ;  X  } , where  X  is a control parameter. Then we check the second-last column of  X  to validate whether there is a value equal to H k;j . In other words, we only compare the nodes of c k;j with the nodes on  X  po H k;j =  X  po ; o = Q + |Y| + 1 . If they are the same, we update the information by adding 1 to the corresponding class Label column to which G k belongs. If c k;j does not appear in the current  X  , a new row is added at the end of  X  to record this clique. The hashing helps accelerate clique matching, because instead of comparing all cliques, we only compare cliques with the same hashing results.
Algorithm 2 shows the procedure of discriminative frequent clique-pattern finding, in which CliqueMining (  X  ) is Algorithm 1, opera-tor  X  is a function to find cliques whose frequencies are equal to or greater than  X  . Sort ( X ) sorts cliques in descending order according to their objective scores and T op ( X  ; m ) returns top m rows from  X  . Algorithm 2 F inding Discriminative Frequent Cliques Input: T raining graph set G , frequency threshold  X  and feature 1: for all G k  X  X  do 2: C k out  X  CliqueMining ( G k ); 3:  X   X   X  4: end for 5:  X   X  Select ( X  ;  X  ); 6: Sort ( X ); 7: M X  T op ( X  ; m ); Output: M ;
O nce the Clique Set Matrix M has been suitably generated, we can convert each compressed graph G  X  k into the same vector space by using the factorization results proposed in Section 3. The fac-torization will generate diagonal matrix f W k under objective arg min W ||M W M T  X  A ( G  X  k ) || 2 and diag ( f W k ) can then be used as fine-grained representation for G  X  k .
Given the vector representation of each graph in a graph batch ( B i ), we can use generic learning algorithms, such as Nearest Neigh-bors (NN), Naive Bayes or support vector machines (SVM), to train a classifier. The classifier is then used to predict class labels for new graphs. As soon as the new graphs arrive and a new graph batch is collected and labeled, we can easily update the classification model by training a new classifier from the new batch.
We validate the performance of the algorithm on the following two real-world graph streams.
 DBLP Stream 1 : DBLP is a computer science bibliography. Each record in DBLP denotes one publication with a number of attributes such as the paper ID, authors, year, title, abstract, and reference ID [8]. In our experiments, we build a graph stream with binary classification tasks by using papers published in a list of selected conferences. The classification task is to predict whether a paper belongs to the field of DBDM (Database and data mining: SIG-MOD, VLDB, ICDE, EDBT, PODS, DASFAA, SSDBM, CIKM, DEXA, KDD, ICDM, SDM, PKDD and PAKDD with 9530 pa-pers) or CVPR (Computer vision and pattern recognition: ICCV, CVPR, ECCV, ICPR, ICIP, ACM Multimedia and ICME with 9926 papers). In our experiments, we represent each paper as a graph, with each node denoting a paper ID or a keyword and each edge representing the citation relationship between papers or keywords appearing in the paper X  X  title. More specifically: (1) each paper ID is a node; (2) if a paper A cites another paper B , there is an edge between A and B (we use undirected edge); (3) each keyword in the title is also a node; (4) each paper ID node is connected to the keyword nodes of the paper; and (5) the keyword nodes of each paper are fully connected with one another. In the experiments, all papers are arranged in chronological order to form a graph stream. IBM Sensor Stream 2 : This stream contains information about local traffic on a sensor network which issues a set of intrusion attack types. Each graph constitutes a local pattern of traffic in the sensor network where nodes denote the IP-addresses and edges correspond to local traffic patterns (local traffic flows between IP-addresses) [1]. Each graph is associated with a particular intrusion type and there are over 300 different local patterns in the dataset. We only choose 20 typical intrusion types (10 types of  X  X WEST X  pattern and 10 types of  X  X NMP X  pattern) which are considered as class labels in our experiments. Our goal is to classify a traffic flow pattern as one of the 20 intrusion types. Baseline Methods: We evaluate the efficiency and effectiveness of our graph stream classification framework, F GSC (FG+Stream), with two baseline methods.  X  Coarse-grained representation-based method (CG+Stream) This method uses traditional coarse-grained feature representation to represent graph data (the selected cliques and the classification methods are the same as our proposed method, except that CG+Stream uses 0/1 occurrence as the feature value). This method is similar to a clique hashing-based graph stream classification method in [3].  X  2-D edge hash-compressed stream classifier (EH+Stream) This method employs a 2-D random edge hashing scheme to construct an  X  X n-memory X  summary for the sequentially presented graphs [1]. The first random-hash scheme is used to reduce the size of the edge set. The second min-hash scheme is used to dynamically update a number of hash-codes, which is able to summarize frequent pa t-terns of co-occurrence edges observed so far in the graph stream. A simple heuristic is used to select a set of most discriminative frequent patterns to build a rule-based classifier for classification.
In the experiments, we use 10-fold cross-validation to evalu-ate the performance of the graph stream classification. Because EH+Stream uses Nearest Neighbor classifier (NN) as the classifier, we cannot apply other learning algorithms to this method. So for the other two methods (FG+Stream and CG+Stream), we also use Nearest Neighbor (NN) to form an ensemble and predict the class labels of graphs in a future chunk. The default parameter settings are as follows: Batch size | D | = { 600 ; 800 ; 1000 } (for DBLP) and { 300 ; 400 ; 500 } (for IBM), feature size | m | = { 62 ; 142 ; 307 } with frequency threshold  X  = { 2% ; 1% ; 0 : 5% } (for DBLP) and (for IBM). Results on different chunk sizes | D | : In Figs. 5, we report the al-gorithm performance by using different numbers of graphs in each chunk | D | (varying from 1000, 800, to 600 for the DBLP stream and from 500, 400, to 300 for the IBM stream).
 Overall, the results in Fig. 5 (a)-(c) show that the accuracies of FG+Stream on DBLP stream are better than that of CG+Stream and EH+Stream. The EH+Stream method has the worst performance, and its accuracy is slightly over 50%, which is nearly random pre-dictions. The accuracy of FG+Stream, on all three experiments, consistently outperforms CG+Stream and EH+Stream, even though the accuracy fluctuates to a large extent on some chunks (such as Batch 14 in Fig. 5 (b)). Recall that the only difference between FG+Stream and CG+Steam is that the former uses fine-grained fea-ture representations whereas the latter uses 0/1 occurrence (both methods have the same set of discriminative cliques). Therefore, the better performance of FG+Stream, compared to CG+Stream, can be attributed to the fact that fine-grained representation pro-vides more accurate information to describe each single graph and ensures minimal information loss for graph representation. As we have explained in Section 3 (Graph Factorization), FG+Stream uses a linear combination of cliques to represent each graph, so even if some features do not appear in a specific graph, they may still be represented by using a small value to denote its correlations to the graph. By contrast, CG+Stream can only use zero value to denote feature not appearing in the graph. The detailed feature informa-tion in fine-grained representation ensures good accuracy for graph classification.

The results in Fig. 5 (g)-(i) show that, for the IBM stream, the ac-curacies of FG+Stream and CG+Stream are almost identical across the whole stream. This is mainly because that each graph in the IBM stream is composed of only a few nodes (most graphs con-tain less than four nodes). As a result, the features mined from the training set contain only a few nodes ( e.g. one or two nodes) and the overlap between any two features ( i.e. the same nodes and edges shared by two features) is less frequent. The feature weight values calculated by our graph factorization method thus degrade to 0/1 occurrence of the features. As a result, the vector represen-tation obtained by using FG+Stream is almost the same as that of CG+Stream, which results in the same classification accuracy. The accuracy of EH+Stream is significantly worse than the other two methods. As shown in Fig. 5 (g), no graph is correctly classified for chunks 14-15, 17-19, 21, and 25-27. This demonstrates that F igure 6: Accumulated system runtime by using NN classifier, where for graphs with very few nodes and edges, using clique hashing, as EH+Stream does, cannot obtain good classification accuracy. Results on different numbers of features | m | : In Fig. 5 (d)-(f) and (j)-(l), we report the algorithm X  X  performance w.r.t. different number of features in each chunk.

As expected, FG+Stream has the best performance on the DBLP stream. In addition, the results in Fig. 5 (d)-(f) show that increas-ing the number of features in the DBLP stream actually increases the accuracy of the CG+Stream method, while the accuracy of FG+Stream method remains relatively stable. This is because CG+Stream directly uses select clique features to represent a graph whereas FG+Stream uses a combinations of clique features for representa-tion. As a result, FG+Stream has less dependence on the number of features selected for graph representation.

Interestingly, even though the accuracy of all three methods fluc-tuates to a large extent across the whole graph stream, the results in Fig. 5 (j)-(l) show for each method, the number of features does not have significant impact on the classification accuracy. This may suggest that for IBM stream, a small number of features (such as special IP-addresses or IP-address connections) may already pro-vide sufficient information for classification. Using additional fea-tures may not necessarily provide new information to effectively improve the classification accuracy. In Fig. 6, we report the system runtime performance (efficiency). The results show that FG+Stream always requires less time than CG+Stream. This is mainly because FG+Stream avoids the expen-sive subgraph isomorphism validation process for vector genera-tion, which is very time-consuming. EH+Stream performs better than the other two methods on the IBM stream and has a very fast runtime for the first few chunks. This is because EH+Stream al-ways keeps a feature table of fixed size. The features in the table will be replaced once a better feature is detected. The size of graphs in the IBM stream is very small (only several nodes) compared to the DBLP stream, so features are found which lead to less feature valuation and replacement for updating.

Overall, the results show that FG+Stream linearly scales to the number of chunks. The runtime of FG+Stream is acceptable com-pared to its high classification accuracy. As a result, FG+Stream is a practical solution for handling real-world high speed graph streams.
In this paper, we proposed to address graph representation and classification. We argued that in graph stream scenarios, the data volumes and the structures of the graphs may constantly change. The existing subgraph feature-based representation model is not only inefficient but also ineffective for graph stream classification. To solve the problem, we proposed a graph factorization-based fine-grained representation model, where the main objective is to use linear combinations of a set of discriminative cliques to rep-resent graphs for learning. The optimization-oriented factorization approach ensures minimum information loss for graph representa-tion, and also avoids the expensive subgraph isomorphism valida-tion process. Based on this idea, we proposed a novel framework for fast graph stream classification. Experiments on two real-world graph streams validated the proposed design for effective graph stream classification.
This research is sponsored by an Australian Research Council (ARC) Discovery Project under grant No. DP130102748. [1] C. C. Aggarwal. On classification of graph streams. In Proc. [2] C. C. Aggarwal and H. Wang. Managing and Mining Graph [3] L. Chi, B. Li, and X. Zhu. Fast graph stream classification [4] G. H. Dietmar. On the randomized complexity of monotone [5] S. Kullback. Information theory and statistics . Courier Dover [6] D. Serre. Matrices: Theory and Applications . Springer, New [7] R. R. Sokal and F. J. Rohlf. Biometry: the principles and [8] L. Tang, H. Liu, J. Zhang, and Z. Nazeri. Community
