 Influence-driven diffusion of information is a fundamental process in social networks. Learning the latent variables of such process, i.e., the influence strength along each link, is a central question towards understanding the structure and function of complex networks, modeling information cas-cades, and developing applications such as viral marketing.
Motivated by modern microblogging platforms, such as twitter , in this paper we study the problem of learning influence probabilities in a data-stream scenario, in which the network topology is relatively stable and the challenge of a learning algorithm is to keep up with a continuous stream of tweets using a small amount of time and mem-ory. Our contribution is a number of randomized approx-imation algorithms, categorized according to the available space (superlinear, linear, and sublinear in the number of nodes n ) and according to different models (landmark and sliding window). Among several results, we show that we can learn influence probabilities with one pass over the data, using O ( n log n ) space, in both the landmark model and the sliding-window model, and we further show that our algo-rithm is within a logarithmic factor of optimal.

For truly large graphs, when one needs to operate with sublinear space, we show that we can still learn influence probabilities in one pass, assuming that we restrict our at-tention to the most active users.

Our thorough experimental evaluation on large social graph demonstrates that the empirical performance of our algorithms agrees with that predicted by the theory. H.2.8 [ Database Management ]: Database Applications -Data Mining Social network analysis, Social Influence, Streaming, Ran-domized approximation algorithms.

Data from social networks and social media is generated continuously, creating streams that many applications need to process in real-time. Analysis of these social streams in real-time predicates the need for fast learning methods, which use a small amount of memory, and are capable of adapting to changes in the data distribution of the social network.

Diffusion of information drive by social influence is a fun-damental process in social networks. Modeling and inferring latent information-influence variables is a central question towards understanding the structure and function of com-plex networks. Accordingly, a lot of research has been de-voted in studying models of information diffusion and devel-oping methods to learn their parameters. On the other hand, many of the proposed methods employ computationally-intensive techniques, such as EM-type schemes or approxi-mation algorithms, which assume that one can operate with random access on the whole data, or can make many passes over the data. Obviously, such techniques are not suitable for nowadays applications that require processing and min-ing of large-scale data in continuous streams.

In this paper we propose STRIP, a suite of streaming methods for computing the influence strength along each link of a social network , that is, we learn the probability that each user influences each of his or her social contacts, friends, followers, etc. As a conceptual framework to compute the influence probabilities of the network edges, we are adopting the frequentist definition of Goyal et al. [13]. The brute-force computation of influence probabilities under this framework requires space that is proportional to the overall activity in the network, e.g., all the  X  X weets X  that all users have posted.
We show how to efficiently estimate influence probabilities using much less space, and with one pass over the activity log. In particular, we express the space requirements of our algorithms as a function of the number of nodes in the net-work ( n ). This should be contrasted with other quantities in the network, such as the total number of edges ( m ), or the total number of actions performed at the network nodes. One should note that in modern social-microblogging appli-cations, such as twitter , not only the number of users is expected to be much less than the size of the activity log, but also the set of users is much more stable while the ac-tions performed by the users are continuous, rapid, and time-varying data streams. It is also worth noting that we aim at approximating a set of m probability values, one for each network edge, while expecting to keep a constant amount of memory for each network node. This is achieved by the sketching nature of the STRIP framework: our algorithms maintain a constant-size sketch for each network node, and the influence probability of an edge ( u,v ) can be estimated directly by the sketches of the nodes u and v .

In addition to the algorithms we develop, we also present a number of theoretical results concerning the lower bounds for the required space complexity. In particular we study the landmark model , where the influence probabilities are computed with respect to the whole activity history of each user, and the sliding-window model , which are appropriate for forgetting data items that are not relevant, and thus adapting better to the behavioral changes occurring in the network.
 Paper contributions and roadmap. The paper is orga-nized in three main technical sections corresponding to the three memory settings: superlinear in the number of nodes n (Section 4), linear (Section 5), and sublinear (Section 6). Sections 4 and 5 are further divided in subsections present-ing the STRIP methods based on the landmark model and the sliding-window model.

Our contributions are summarized as follows: We present some background and related work in Section 2. In Section 3 we give some preliminaries. We perform and discuss an empirical validation of the new methods proposed in Section 7, and finally Section 8 concludes the paper.
To the best of our knowledge this is the first work on learning influence probabilities from data streams. Stream learning. Data streams are large read-once se-quences of data that change with time. Actions of users in social networks are an example of data streams. Data streams are usually so large that any exact computation is prohibitive in terms of memory and time. Thus stream learning methods are becoming popular since they allow real-time analytics on evolving data, under restrictions of time and memory. These methods usually rely on approxi-mate algorithms that can obtain large gains in memory and time complexity, giving away little accuracy, by using sketch structures [6, 1], or applying sampling techniques, or a com-bination of both.
 Learning influence probabilities. Detecting and esti-mating social influence strength among the users of social networks, is becoming a hot research topic in the computa-tional social science as well as in the marketing literature. The amount of interest that this computational problem is attracting is justified by the great business potentialities of applications such as viral marketing, for which estimating influence strength is a needed preliminary step.

Given a social network, whose nodes are users and arcs represent social relations among the users, we can asso-ciate each arc ( u,v ) with a probability p uv representing the strength of influence exerted by u on v . Or in other terms, the probability that a tweet posted by u will be  X  X etweeted X  by v . In this setting, a basic computational problem is that of selecting the set of users to be targeted by the viral mar-keting campaign: those are the users more likely to generate a large viral cascade. The first algorithmic treatment of the problem was provided by Domingos and Richardson [10]. Later, Kempe et al. [16] introduced influence maximization as a discrete optimization problem: given a budget k , find the set of k nodes that maximizes the expected number of active nodes at the end of the process. The activation of nodes is governed by a probabilistic propagation model. For instance, in the Independent Cascade propagation model, when a node u first becomes active, say at time t , it has one chance of influencing each inactive neighbor v with probabil-ity p uv . If the tentative succeeds, v becomes active at time t + 1. Following this seminal work [16], considerable effort has been devoted to develop methods for improving the ef-ficiency of influence maximization [18, 7, 14]. The majority of this literature assumes the input social graph has already the influence probabilities associated to links, and does not address how to compute them.

Saito et al. [21] were the first to study the problem of learn-ing the probabilities for the independent cascade model from a set of past observations, formalizing it as likelihood max-imization and applying Expectation Maximization (EM) to solve it. However, the iterative nature of EM methods is re-ally not suited for stream processing. In this paper we adopt instead the simpler frequentist definition of Goyal et al. [13]. The learning procedure defined in [13] assumes that the in-put propagation data is stable and sorted by the item and then by time. Of course the real stream of events does not come sorted by any criteria except time, so that the propa-gations corresponding to different items arrive intertwined. The learning algorithm needs two scans of this fixed and sorted database. Moreover, it keeps in memory the whole propagation of an item, plus one counter for each node and a couple of counters for each link.
We consider a social network, represented as a directed graph G = ( V,E ) with users corresponding to vertices and edges to social connections, such that the edge ( u,v ) denotes that v follows u . The maximum in-degree of a vertex in G is denoted by  X . Let A be a set of actions . The input is provided as stream S of triples ( u,a,t u ). Each triple denotes that user u performed action a  X  A at time t u . The number of actions in S is denoted as s . An action a propagates within  X  time units from user u to user v if ( u,v )  X  E , and a is performed by u and then by v within  X  time units, i.e., there exist triples ( u,a,t u ) , ( v,a,t v )  X  X  with 0 &lt; t v are interested how influential is a given user u , i.e., how probable is that actions propagate from user u to a user v .
We consider two standard streaming models: In the land-mark model one is interested in the whole history of the stream starting from a given time point to the present. Let A u denote the set of actions performed by user u . We also define A  X  u 2 v to be the set of actions propagated from u to v within  X  time units, and A u | v the set of actions performed by either u or v . We set max( A, X  ) to be the maximum number of actions that can be performed by a user within  X  time units. To estimate the influence probability p uv between the users u and v , we use the so-called Jaccard model, pro-posed by Goyal et al. [13], and define p uv = s ( A  X  u 2 v hereinafter, s ( A ) denotes the size of a set A . In time-based sliding windows only the most recent actions are considered for a user-defined time threshold. Denote the window by W and the set of actions performed within the window by A W The notation for the actions performed by a user within the window W extends in the obvious way the notation for the landmark model. We set w = arg max u  X  V s ( A W u ).
Note that if we are able to keep in main memory the whole stream, we can easily compute the influence probabilities defined above. However, for high-frequency streams of very large volume this is not possible and we thus need to de-velop stream-processing algorithms computing approximate estimates of the influence probabilities. Naturally, our solu-tion extends techniques developed for streaming algorithms. We review the relevant background below.
 Probabilistic approximation. We say that an algorithm returns an (  X , X  ) -approximation of some quantity q , if it re-turns a value  X  q , such that (1  X   X  ) q  X   X  q  X  (1 +  X  ) q , with probability at least 1  X   X  for any 0 &lt;  X , X  &lt; 1. Min-wise independent functions. A family F of func-tions from U to Z is k -wise independent if for f : U  X  Z chosen uniformly at random from F for z = s ( Z ), distinct u i  X  U and any c i  X  Z and k  X 
A family H of functions from U to a finite totally ordered set S is called (  X ,k ) -min-wise independent if for any X  X  U and Y  X  X , with | Y | = k and 0 &lt;  X  &lt; 1, for a function h chosen uniformly at random from H it holds We use the notation h : U  X  [0 , 1] to denote that h maps U to a finite subset D of [0 , 1]. For h being pairwise in-dependent and | D | = | U | 3 , the probability of collision, i.e., We thus assume that h : U  X  [0 , 1] is injective with high probability and h ( u ) can be described using O (log | U | ) bits. For the analysis of our algorithms we will use the following
Fact 1. Let c 1 ,c 2 ,c 3 ,c 4 be constants larger than 1. For any 0 &lt;  X  &lt; 1 there exists an  X  0 =  X (  X  ) such that Min-wise independent hashing. Min-wise independent permutations [4, 8] is a powerful technique for estimating A and B , subsets of a ground set U . In particular, let  X  be a random permutation of the elements in U . Define  X  ( A ) = min x  X  A {  X  ( x ) } , the smallest element of A under the permutation  X  , and  X  ( B ) similarly. It is easy to see that Pr[  X  ( A ) =  X  ( B )] = J ( A,B ), namely, the indicator variable that the smallest elements in  X  ( A ) and  X  ( B ) are identical yields an unbiased estimator of the Jaccard similarity.
The approach can be generalized to estimating the frac-the elements in A  X  B . For instance, for a random permuta-tion  X  , we can obtain an unbiased estimator of the influence probability p uv by keeping track of a u = arg min x  X  A u and a v = arg min x  X  A v {  X  ( x ) } , and then checking whether a u = a v and a v has been performed within  X  time units of a u . By the estimator theorem [19], with O ( 1  X  2 p independent estimators one obtains an (  X , X  )-approximation of p uv . The applicability of the approach on a streaming setting, follows from the fact that, as shown by Broder et al. [4], the random permutations can be replaced by effi-ciently computable hash functions h : A  X  [0 , 1].

A modification of the approach stores the k smallest hash values instead of a single hash value and returns as an es-in A  X  B with the k smallest hash values. By choosing  X  to be the function that returns the number of propagated actions in the sample, one can show that for k = O ( and an (  X ,k )-independent hash function, for some constant  X  &lt; 1 / 2, the scheme yields an (1  X   X  )-approximation for edges with influence probability at least p with probability more than 1/2. Standard application of Chernoff bounds yields that the median of O (log 1  X  ) independent estimators is an (  X , X  )-approximation.

The latter scheme is more suitable for our purposes. Addi-tionally, a recent improvement of the approach exponentially reduced the evaluation time of an (  X ,k )-independent hash function from O ( k ) to O (log 2 k ) [12]. For the theoretical analysis of our algorithms we use the following
Fact 2. Let A k be the set of size-k subsets of A and let more than 2 / 3 of these satisfy a certain property  X  . Let X  X  A k be a set satisfying  X  . Then there exists a constant  X  &lt; 1 / 2 such that an (  X ,k ) -independent hash function h maps the elements of X to the k smallest hash values with probability more than 1 / 2 . The function h can be described in O ( k ) machine words and evaluated in time O (log 2 k ) . Sliding-time windows. Datar et al. [9] presented an al-gorithm for estimating the number of 1 X  X  in a stream of bits over time-based sliding windows. The algorithm relies on a data structure based on exponential histograms . We ad-just the approach such that we can apply min-wise indepen-dent hashing over sliding windows. As a result we can keep track of the minimum element, according to a random per-mutation  X  , in the sliding window. In the analysis of our algorithms we will use the following
Fact 3. Let W be a time-based window and w be the number of actions performed within W . Keeping an expo-nential histogram with O (log w/ (  X  )) buckets for each user, one can compute the minimum hash value of at least (1  X   X  ) w of the actions in W .
In this section we assume that the whole graph fits in memory and for each user we can store all actions performed within  X  time units. We present algorithms for the landmark and the sliding-window model. The algorithms provide an (  X , X  )-approximation for the influence probability p uv of all ( u,v )  X  E , and the complexity does not depend on p uv .
The superlinear-space landmark-model algorithm, shown as Algorithm 1, works as follows. For each user u we keep a data structure Q u recording the performed actions together with a hash table H u recording the time each action in Q has been performed. For each directed edge ( u,v ) we keep a counter c ( A u 2 v ) recording the number of actions propagated from u to v within time  X  , respectively. For each user u we keep a sketch MinHash u of the actions. The sketches of the users u and v will be used to estimate s ( A u | v ) [1]. For a pair-wise independent hash function h : A  X  [0 , 1] the sketch for user u consists of the k smallest hash values of the actions performed by u . For each incoming triple ( u,a,t u ) we up-date Q u and H u by first deleting all actions performed more than  X  time units ago and adding the pair ( a,t u ). Then we evaluate the hash value of the action performed and up-date MinHash u , i.e., we add h ( a ) to u  X  X  sketch if it is smaller than the largest value or there are less than k hash values in the sketch. Next we check which of u  X  X  neighbors have already performed a within time  X  and increment c ( A u 2 v At the end we estimate s ( A u | v ) as proposed in [1] by re-turning k/h k , where h k is the k -th smallest hash value in MinHash u  X  MinHash v .

Theorem 1. Let G = ( V,E ) be a connected graph over n vertices and m edges, and S be a stream of s actions. There exists an algorithm computing an (  X , X  ) -approximation of the influence probability of all edges in expected amortized time O ( s ( X  + log n  X  ) + m  X  2 ) and space O ( m + n (max { A, X  } + log n  X  )) in only one pass over S .

Proof. For a user u we keep a queue Q u recording the performed actions together with the corresponding time as well as a hash table H u containing the actions. For each user we also maintain the k actions with the smallest hash values seen so far. Clearly, Q u and H u consists of O (max { A, X  } ) pairs since we maintain the invariant that the first and last action in Q are preformed within  X  time units. Also each pair is added and deleted exactly once to a given Q u and H thus each incoming action is processed in expected constant amortized time. We observe that storing the incident edges for each user u in a hash table, we can access the neighbors of a user u in expected time bounded by O ( X ).

Update maintains the k smallest hash values of actions performed by each user. One can use a priority queue, this Algorithm 1: STRIP -Superlinear Memory ProceedStream
Input : Social graph G = ( V,E ), stream S of s actions,
Load G in memory. for ( u,a,t )  X  X  do EstimateUnionSize Input : vertex u , vertex v , int k
MinHash  X  MinHash u  X  MinHash v h k  X  the k -th smallest value in MinHash return k/h k EstimateInfluenceProbabilities
Input : Social graph G = ( V,E ) for ( v,u )  X  E do however incurs an additional log k -factor for the running time. Instead, for each user u we maintain the k -th smallest hash value h k min . Then for an incoming triple ( u,a,t ) we check whether h ( a ) &lt; h k min and if so, we store h ( a ) in a aux-iliary data structure MinHash aux u . Once there are k values in MinHash aux u , we find the median in MinHash u  X  MinHash and update MinHash u to contain the k minimum hash values. The median can be found in O ( k ) time by a deterministic al-gorithm [2], thus we update MinHash u in constant amortized time. Looking-up in a hash table takes expected constant time. For each edge ( u,v ) Estimate ( u,v ) computes h the k -th smallest hash value in MinHash ( u )  X  MinHash ( v ). As shown by Bar-Yossef et al. [1], for k = O ( 1  X  2 ) the value 1 /h k uv is an (1  X   X  )-approximation of s ( A u | v ) with proba-bility at least 2 / 3. Thus the median of log m  X  = O (log such estimators is an (  X ,  X  n )-approximation of p uv and by the union bound we have an (  X , X  )-approximation of the number of performed actions for all edges. The (  X , X  )-approximation of the influence probabilities then follows from Fact 1.
A straightforward extension of Algorithm 1 for the sliding window model is to combine it with the exponential his-togram approach using t = O ( 1  X  ) buckets. In each bucket we will keep the k smallest hash values and at the end, us-ing Fact 3, we will be guaranteed to have the k smallest hash values for at least (1  X  2  X  ) of the actions performed by each user. However, we can make use of the fact that hash values are random and thus it is very improbable that the k smallest actions will come all from the same bucket. We formalize this intuition in the next theorem and show that for a sufficiently random hash function it suffices to store only the smallest O ( 1  X  log 1  X  ) hash values per bucket.
Theorem 2. Let G = ( V,E ) be a connected graph over n vertices and m edges, S be a stream of s actions and W a time-based sliding window. There exists an algorithm computing an (  X , X  ) -approximation of the influence proba-bility of all edges over a time-based sliding window in ex-pected amortized time O ( s ( X  + log n  X  log 1  X  ) + m  X  2 O ( m log w 1  X  + n (max { A, X  } + 1  X  2 log 2 ( 1  X  ) log w log one pass over S .

Proof. We use a bit-counter over sliding windows to es-timate s ( A W u 2 v ). In order to estimate s ( A W the exponential histogram approach as follows. Consider a given user u and assume that s/he performs w actions within the window. We maintain t = 1  X  buckets of width 2 for 0  X  i  X  log w 2 t , each keeping track of 2 i  X   X w/ 2 actions.
Let k = O ( 1  X  2 ). Since h maps the w actions uniformly at random to values in [0 , 1], we expect k hash values to be at most k/w . By Markov X  X  inequality the probabil-ity that the k -th smallest value is larger than 3 k/w is at most 1/3. Next we bound the number of small hash val-ues in a given bucket. For a bucket recording at most  X w actions we expect 3 k X  = O ( 1  X  ) hash values in the bucket to be smaller than 3 k/w . For a fully random hash func-tion, by Chernoff bounds the probability that the number of hash values smaller than 3 k/w is more that O ( k X  log t ) is 1 / (8 t ). Thus, for uniformly distributed hash values in [0 , 1], a fraction of 1 / (8 t ) of the size-x subsets of the actions in a given bucket, for x = O ( k X  log t ) = O ( 1  X  log 1  X  ), will con-stitute of actions with hash values all smaller than 3 k/w . Thus, by Fact 2, there exists constant  X  &lt; 1 / 2 such that an (  X ,x )-independent hash function guarantees with proba-bility 1 / (6 t ) that the hash values for at most x actions in a given bucket are smaller than 3 k/w . By the union bound we find the k smallest hash values for the buckets of given width with probability at least 5 / 6, thus the total space to com-pute the smallest O ( 1  X  2 ) hash values is O ( 1  X  2 log Each action is processed in amortized time O (log 1  X  ).
From Fact 3 it follows that we might not have a record for  X w of the actions in A W u and A W v . In the worst case we thus have not considered 2  X s ( A W u | v ) different actions and we com-pute an (1  X   X  )-approximation of the quantity (1  X  2  X  ) s ( A with probability at least 2/3. Since the bit-counter for s ( A W u 2 v ) computes an (1  X   X  )-approximation of the number of actions propagated from u to v , by Fact 1 it follows that by rescaling  X  , i.e.,  X  :=  X /c , for some constant c &gt; 0, we have an (1  X   X  )-approximation of the influence probabilities for an edge ( u,v ) with probability at least 2/3. Thus, the success probability for detecting the k smallest hash val-ues and obtaining an (1  X   X  )-approximation from them is (5 / 6)  X  (2 / 3) &gt; 1 / 2. Taking the median of O (log mates yields the claimed result.
We now move to the more interesting case, presenting algorithms that require space linear in the number of ver-tices. Our algorithms are appropriate for dense networks, and when we assume that there is O ( n ) available space but not O ( m ), the so-called semi-streaming model [11]. From the empirical point-of-view it is known that social networks become denser over time [17] and it is conjectured that m =  X ( n 1+  X  ) for some constant  X  &gt; 0. Also, it is feasible to keep the number of actions for each user performed within  X  time units only for relatively small  X  . If one is interested in learning the probabilities over longer time intervals, one has to store a considerable fraction of the stream in memory.
Before providing our algorithms, we discuss that linear space in the number of nodes is necessary if one wants to learn the influence probabilities on all edges with certain accuracy. Intuitively, we need to store some information about each vertex in the social graph because the influence probability on a given edge ( u,v ) can be high even if the number of actions performed by u and v is very small.
The proof of the next theorem can be found in the full version of the paper 1 . It uses a simple reduction from the Bit-Vector Disjointness problem for binary vectors over n bits which is known to have communication complexity of  X ( n ) bits [15]. Note that the result holds for random-ized algorithms even if they are allowed to make a constant number of passes on the data stream.

Theorem 3. Let G = ( V,E ) be a connected graph over n vertices and m edges, and S be the action stream. Con-sider any randomized streaming algorithm A that makes a constant number of passes over E and S . Assume that A distinguishes with probability more than 1/2 the following two cases (1) all edges have influence probability at most 1 / ( d  X  1) , for any d  X  3 , vs. (2) there is an edge with influ-ence probability 1 / 2 . Then A needs  X ( n ) bits in expectation. Algorithm 2 is based on min-wise independent sampling. We assume h : A  X  [0 , 1] is t -wise independent, for t that will be specified later. For each user u we keep a sample MinHash u of the k actions with the smallest hash values per-formed by him/her together with the time-stamp the action was performed. For each incoming action ( u,a,t ) we eval-uate h ( a ) and update MinHash u to contain the k actions with smallest hash values. After processing the stream, for a given edge ( u,v ) we determine the (at most) k actions with the smallest hash values in MinHash u  X  MinHash v and in Prop( MinHash k u | v , X  ) we count how many of them propa-gated from u to v within  X  time units.

Theorem 4. Let G = ( V,E ) be a connected graph over n vertices and m edges, and S be a stream of s actions. There exists an algorithm returning an (  X , X  ) -approximation of the influence probability p uv of all arcs ( u,v ) with p p in amortized time O (( s log 2 ( 1  X p ) + m  X  2 p ) log O ( n  X  2 p log n  X  ) in one pass over S .

Proof. Assume that h is t -wise independent, for t to be specified later. Consider an edge ( u,v )  X  E . We are in-terested in those sets MinHash k u | v of k elements containing (1  X   X  ) p uv k actions that propagated from u to v . The num-ber of such sets follows hypergeometric distribution. We expect p uv k actions in MinHash k u | v to have propagated from u to v . With some algebra we bound the variance of the http://francescobonchi.com/STRIP.pdf random variable counting the number of propagated actions in MinHash k u | v . By Chebyshev X  X  inequality it follows that for k = O ( 1  X  2 p ) at most 1 / 3-rd of the k -size subsets will not yield a (1  X   X  )-approximation of p uv . From Fact 2 there exists a constant  X  &lt; 1 / 2 such that for h being (  X ,k )-independent, with probability at least 2 / 3 the k smallest hash values correspond to a set of size k which provides an (1  X   X  )-approximation. A standard application of Chernoff X  X  inequality and the union bound yields that the median of O (log n  X  ) estimates will be an (  X , X  )-approximation for all ( u,v ) with p uv  X  p .

The sets MinHash u for each user can be stored in an in-dexed array allowing constant time access. We maintain the k smallest hash values in a priority queue and update it for each newly performed action. We need h to be t -wise inde-pendent for t = O ( 1  X  2 p ) and by [12] h can be represented in space O ( 1  X  2 p ) and evaluated in time O (log 2 1  X p ). Algorithm 2: STRIP -Linear Memory Compute Samples
Input : stream S of actions, threshold  X  , a parameter k , for ( u,a,t )  X  X  do Single Estimate
Input : users u,v , a set of samples MinHash z for all
Let MinHash k u | v be the k entries with the smallest hash value in MinHash u  X  MinHash v
We extend the previous algorithm to estimate the influ-ence probabilities over sliding windows. The main idea is to guarantee that the required O ( 1  X  2 p ) smallest hash values of actions performed within the window are kept with con-stant error probability. We achieve this by adjusting the exponential-histogram technique.

Theorem 5. Let G = ( V,E ) be a connected graph over n vertices and m edges, S be a stream of s actions and W a time-based sliding window of at most w actions. There is an algorithm returning an (  X , X  ) -approximation of the influence probability p W uv of all edges with p in amortized time O (( s log 2 ( 1  X p ) + m  X  2 p ) log O ( n  X  2 p log 2 ( 1  X p ) log w log n  X  ) in one pass over S .
Proof. Assume for each i we maintain t buckets of width 2 , 0  X  i  X  log w 2 t for t  X  1  X  . Similarly to the proof of Theorem 2, we can show that storing O ( 1 p X  log 1  X p per bucket, we can compute with constant error probability p &lt; 1 / 4 the required smallest k = O ( 1  X  2 p ) hash values.
The construction of exponential histograms implies that in the last bucket, i.e., the bucket keeping track of the oldest actions, we might have recorded k hash values of actions not performed within the window. Consider a directed edge ( u,v ) and assume the worst case that we have no record of w/ (2 t ) actions performed by u within the window and all Algorithm 3: STRIP -Sublinear Memory ComputeSamplesInBuckets
Input : stream S of actions performed by users, int k , for ( u,a,t )  X  X  do Single Estimate
Input : users u,v , threshold  X  , a set of samples b u = g ( u ) ,b v = g ( v ) if |{ ( u,h ( a ) ,t u )  X  MinHash b u }| X  k/ 2 and |{ ( v,h ( a ) ,t v )  X  MinHash b v }| X  k/ 2 then of them have propagated from u to v . This happens when either for u or w we have not recorded hash values of actions performed within the window and propagating from u to v . Therefore, as shown in Theorem 4 we can obtain with error probability less than 1/2 an (1  X   X  )-approximation of additive approximation of  X p uv  X  w ks ( A 2 w and p uv  X  p we obtain that for t  X  1 p X  we will have an (1  X   X  )-multiplicative approximation with error probability p 2 &lt; 1 / 4. Similar reasoning applies to the case when we have no record of w/ (2 t ) actions performed by u and no of them propagated to v .

Thus, with error probability less than 1/2 we have an (1  X   X  )-approximation of the influence probability for a given arc. Running O (log n  X  ) copies in parallel and taking for each edge the median of the estimates, the approximation holds for all arcs with probability at least 1  X   X  . For the landmark model we combine the algorithm from Section 5.2 with hashing based algorithms for mining heavy hitters in data streams, e.g., [6]. The main idea in these algorithms is to distribute the heavy items to different bins by a suitably defined hash function. Then one shows that the contribution from non-heavy items in each bin is not significant and one can obtain high quality estimates for the heaviest items. In Algorithm 3 we apply the approach to active users but instead of estimating the activity, we are interested in obtaining a sample of the actions performed by active users. By recording the actions with the smallest hash values in each bin, we will show that with high probability a large fraction of those will be for actions performed by an active user.

Theorem 6. Let G = ( V,E ) be a connected graph over n vertices and m edges, and S be a stream of s actions. Let the user activity follow Zipfian distribution with parameter z and let ( u 1 ,u 2 ,...,u n ) be the users sorted according to their activity. There exists a one-pass streaming algorithm com-puting an (  X , X  ) -approximation of the influence probabilities of all edges ( u i ,u j ) with p u i u j  X  p for i,j  X  b in time T and space S such that:
We can extend the algorithm to handle the sliding window model under the assumption that the user activity within the window adheres to Zipfian distribution. We adjust the expo-nential histograms approach such that each bucket records a certain number of bins recording the smallest hash values. As in the previous sections, this incurs an additional cost of polylog ( 1  X p ,w ) to the space complexity of the algorithm.
The goal of our experiments is twofold. First, to show that the proposed approach is indeed applicable to learning influ-ence probabilities in real social networks and yields results that confirm the theoretical analysis. Second, to compare the algorithms for the different memory models. Note that we did not try to optimize the space usage since it heav-ily depends on low-level technical details. For example, in our Java implementation we worked with data structures provided by the java.util package. However, a hash table then automatically wraps primitive data types as objects, which considerably increases the space usage. While such details can greatly influence the performance, they are be-yond the scope of the present work.
 Details on the implementation. We used tabulation hashing [5] to implement the hash function. (See, e.g., [20] for details.) The approach is very efficient and each hash function can be stored in fast cache and evaluated in con-stant time. The scheme yields only 3-wise independence but recently Chernoff-like concentration on the estimates pro-vided by algorithms using tabulation hashing with a single hash function were shown [20]. Therefore, we worked with only one hash function instead of taking the median of sev-eral estimates provided by different functions.
 Experimental settings. The algorithms were imple-mented in Java and experiments performed on a Windows 7 machine with 3.30 GHz clocked processor and 4 GB RAM. We experiment on two real-world datasets. For both datasets we extend the available stream of events, by creat-ing synthetic data in a smart way: the synthetic stream is generated by an instance of the Independent Cascade model that fits the available real data, as explained next.
The first dataset is obtained by crawling the public time-line of Twitter and tracking the propagation of hashtags across the network. The second dataset has been crawled from Flixster 2 , one of the main web communities which allows to share ratings on movies and to meet other users with similar tastes. The propagation log records the time at which a user rated a given movie. An item propagates from v to u , if u rates the item shortly after the rating by v . The datasets basic statistics are reported in Table 1.
The user activity is quite skewed: the maximum num-ber of actions performed by a user in Twitter is 358 and in Flixster 11,542; in Twitter the 1,000 most active users per-form 68,191 actions, and in Flixster the skew is even more significant with 1,000 users performing 1,643,686 actions.
From the real datasets we create a larger stream of actions as follows. For a suitably chosen propagation threshold  X  we compute the influence probabilities among edges and also compute the  X  X tarting X  probability for each user to initiate a given action, i.e., how probable is that a given user performs an action without being influenced by its neighbors. Then we sequentially create new items. For a given new item, from the starting probabilities for each user we toss a biased coin and decide whether the user will perform the action. If performed, the action is then propagated to its neighbors according to the precomputed influence probability within time r , where r is a random number in (0 , X  ]. The average influence probability in Twitter is 0.0099 and the number of edges with influence probability at least 0.05 is 92,813. The numbers for Flixster are 0.0202 and 51,963, respectively.
For 100,000 items propagated through the Twitter net-work we obtain a stream of about 36 million actions. For the Flixster network we propagate 1,000,000 items obtain-ing a stream of about 27 million actions. The synthetic datasets exhibit similar characteristics to the original data with skewed activity among users and, not surprisingly, sim-ilar distribution of the influence probabilities.

For a directed edge ( u,v ), we denote the approximation of the influence probability p uv by  X  p uv . We evaluate the quality of the estimates of all edges ( u,v ) s.t. p uv  X  0 . 05 with respect to average relative error, Pearson correlation coefficient and Spearman rank correlation [3].
 Evaluation. Table 3 shows the quality of the estimates for varying number of samples. As expected, the best estimates are obtained for Algorithm 1, the superlinear space model, since the number of propagations for any ( u,v ) is computed exactly and we only estimate the number of different actions performed by u or v . The left-most plot in Figure 1 confirms that the quality of the approximation does not depend on the influence probability. Due to the sparsity of the graph structure of the two considered networks, the space com-plexity of Algorithm 1 is comparable to the space-usage by Algorithm 2. However, the running time is more than an order of magnitude larger and this is due to the fact that for each incoming triple ( u,a,t u ) we explicitly need to check whether v has performed a within  X  time units of t u for all arcs ( v,u ). Instead, in Algorithm 2 each incoming triple is processed in constant amortized time. The exact num-bers are in Table 2, where the space is the number of stored samples and the time is given in seconds. For the sublinear model we estimated the influence probabilities among the 1,000 most active users and worked with 3,000 bins. For a given number of samples x we then stored in each bin the 3 x smallest hash values of users hashed to the bin. The plots in Figure 1 confirm that despite of working with a single hash function there are no outliers in the estimates.

In Tables 4 and 5 we evaluated the quality of estimates for time-based sliding windows. We choose a time thresh-old for the window such that the number of actions in the in memory.

