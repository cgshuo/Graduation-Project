 The cold-start problem has attracted extensive attention among various online services that provide personalized rec-ommendation. Many online vendors employ contextual ban-dit strategies to tackle the so-called exploration/exploitation dilemma rooted from the cold-start problem. However, due to high-dimensional user/item features and the underlying characteristics of bandit policies, it is often difficult for ser-vice providers to obtain and deploy an appropriate algorithm to achieve acceptable and robust economic profit.
In this paper, we explore ensemble strategies of contextual bandit algorithms to obtain robust predicted click-through rate (CTR) of web objects. The ensemble is acquired by aggregating different pulling policies of bandit algorithms, rather than forcing the agreement of prediction results or learning a unified predictive model. To this end, we employ a meta-bandit paradigm that places a hyper bandit over the base bandits, to explicitly explore/exploit the relative importance of base bandits based on user feedbacks. Ex-tensive empirical experiments on two real-world data sets (news recommendation and online advertising) demonstrate the effectiveness of our proposed approach in terms of CTR. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Information Fil-tering; H.3.5 [Information Systems]: On-line Information Services; I.2.6 [Computing Methodologies]: Learning Keywords: Personalized Recommendation; Ensemble Rec-ommendation; Contextual Bandit; CTR Prediction; Meta Learning
Personalized recommendation services aim to identify pop-ular items and tailor the content according to users X  prefer-ences. In practice, a large number of users or items might be completely new to the system, which is referred to as the cold-start problem [21]. This issue is often recognized as an exploration/exploitation problem, in which we have to find a trade-off between two competing goals: maximizing users X  satisfaction in a long run, while exploring uncertainties of user interests [1]. For example, a news recommender should provide breaking news to users while maintaining user pref-erences based on aging news stories.

Theaforementionedissueisoftenmodeledasacontextual bandit problem [29]. A contextual bandit problem is a series of trials with a fixed number of arms. In each trial, the algorithm selects an arm to pull based on the given context. By pulling an arm, it can obtain a reward, drawn from some unknown distribution determined by the pulled arm and the context. The objective of bandit algorithms is to maximize the total obtained reward. In the cold-start situation, a recommender system does not have enough training data to build the predictive model. In such a case, people often use a bandit algorithm to solve the recommendation problem, where each trial can be treated as a user visit, and each arm is an item (e.g., a news article or advertisement). Pulling an arm is recommending that item, where the context is a set of user features. The reward is the user response (e.g., a click), which is also determined by the recommended item and user features. The objective of recommender systems is to maximize the total user response, which is equivalent to maximizing the total reward in bandit algorithms [13].
Recently, a series of algorithms have been reported to tackle the multi-armed bandit problem, including unguided exploration (e.g., -greedy [28], and epoch-greedy [12]), guided exploration (e.g., LinUCB [13], EXP4 [2], and Thompson sampling [6]). These existing algorithms can achieve promis-ing performance under specific settings. The performances of different policies vary significantly in many recommenda-tion applications. A common practice of picking the appro-priate policy is to first evaluate these policies and then select the best one to deploy. However, in the cold-start situation, it is often difficult to conduct an unbiased offline evaluation due to the deficiency of the historical data. For online eval-uation, e.g., A/B test, the user visit traffic has to be split to multiple buckets for different policies, and therefore the number of testing policies running in parallel is restricted in order to obtain acceptable daily income.

In our work, we explore the possibility of utilizing en-semble strategies to obtain a robust policy that can achieve acceptable CTR in various recommender systems. As the predictive result of each contextual bandit algorithm is the pulled arm (item), it is not appropriate to adopt the major-ity voting or consensus prediction as the ensemble. We hence resort to meta learning to build a hyper policy that adap-tively allocates the pulling chances to different base policies based on the estimation of their performance. The proposed ensemble bandit algorithms may not produce the optimal CTR of base policies, but it can always approach to the best one, which gives a robust mechanism for online personalized recommendation in the cold-start situations. In summary, the contribution of our work is three-fold:
The rest of this paper is organized as follows. In Sec-tion 2, we describe a brief summary of prior work relevant to contextual bandit problems, ensemble recommendation and meta learning. We then formulate the problem in Section 3, and present the detailed algorithmic description in Section 4. Extensive empirical evaluation results are reported in Sec-tion 5. Finally, Section 6 concludes the paper.
In our work, we employ ensemble strategies combined with a meta learning paradigm to stabilize the output of contex-tual bandit algorithms. In the following, we highlight the previous research that are most relevant to our work.
Contextual Bandits : When predicting the CTR of web data, the cold-start problem is often modeled as a contex-tual bandit problem with exploration/exploitation trade-off, where user features are regarded as contextual information. Typical solutions of this problem involve unguided explo-ration (e.g., -greedy [28], epoch-greedy [12]), guided explo-ration (e.g., LinUCB [13], EXP4 [2]) and probability match-ing (e.g., Thompson sampling [6, 22]). Most existing meth-ods require either a parameter to control the importance of exploration or prior information of Bayesian learning mod-els; however in practice, it is difficult to determine the op-timal value for the input due to the insufficiency of user feedbacks. Hence, the prediction performance of these algo-rithms is not stable along both exploration and exploitation phases, unless the selection policy/model converges.
Ensemble Recommendation : Ensemble based algorithms have been well explored to improve the performance of pre-diction [20, 27], and are often preferred in recommendation competitions, such as the Netflix Prize contest [11, 24] and KDD Cups [17, 30]. Typically, an ensemble method com-bines the prediction of different algorithms to obtain a fi-nal prediction [18], which is often referred to as  X  X lend-ing X  [10]. The most basic blending strategy is to acquire the final prediction based on the mean over all the predic-tion results or the majority vote. Learning based approaches have also been proposed to unify different recommendation algorithms [31]. In our work, to obtain a robust policy, we resort to ensemble strategies that assimilate the advantages of different contextual bandit algorithms.

Meta Learning : In machine learning community, the goal of meta learning is to accumulate experience on the perfor-mance of multiple learning algorithms [8]. Meta learning has been widely used in algorithm selection [16, 19]. Due to the uncertainty of learning algorithms, i.e., we do not know in advance the predictive performance, a lot of work has mod-eled algorithm selection as a multi-armed bandit problem [7, 25] and tries to balance the trade-off between exploring al-gorithm capabilities and exploiting the predictive power of algorithms. In addition, some recent research efforts [15, 23] focus on meta learning of exploration/exploitation strate-gies, where the base learners are bandit algorithms.
In this section, we formulate the problem studied in this paper. Let A denote the set of items (or bandit arms), A ommenders (or policies), where each recommender is a con-textual bandit algorithm with a specific parameter setting.  X  ( x )= a indicates that the policy  X  i pulls a with respect to x ,where a  X  X  and x is a context feature vector. Let D be the space of x and p ( x ) denote the probability density of x . After the pulling,  X  i receives a reward r ,where r value drawn from the conditional distribution p (  X | x ,a our work, we only consider r  X  X  0 , 1 } , i.e., a non-click/click
Each policy  X  i  X   X  aims to maximize the expected re-ceived reward denoted by E[ r  X  i ], where E[ TheexpectedrewardE[ r  X  i ] is known as the CTR of the policy  X  i , which is often used as the performance metric.
Existing studies propose different contextual bandit algo-rithms and show their empirical performances on various real-world data sets [5, 6, 13]. It is known that a bandit al-gorithm with different parameter settings can have different performance [22]. The choice of parameters depends on the distribution of the real data, which is often unknown in the cold-start situation. Therefore, given a set of policies  X , in-dividual polices in  X  can have very different performance for a particular recommender system. Let  X   X  denote the best policy in terms of the performance, i.e., For different recommendation problems,  X   X  is different and not known in advance. The goal of this paper is to develop an ensemble contextual bandit policy such that its perfor-mance can be close to the performance of  X   X  .
This section presents two ensemble bandit algorithms, Hy-perTS and HyperTSFB , for solving the contextual recommen-dation problem in the cold-start situation. The idea of these two algorithms is to distribute the trials to the base bandit policies. Given a set of policies  X  = {  X  1 , ...,  X  m } and a con-text x , both algorithms make two decisions to deicide which arm to pull: For Decision 1, if we know which base policy is the best one, i.e.,  X   X  , we can always select it. However, the performance of each policy is unknown at the beginning. To estimate their expected rewards, we need to select them and observe the received rewards. Same as Decision 2, the exploration-exploitation dilemma also exists in Decision 1. In this paper, we focus on Decision 1.

To address the policy selection problem in Decision 1, both of the proposed algorithms leverage non-contextual Thomp-son sampling [6, 26]. Generally, in each trial, the algorithms randomly select a policy  X  i  X   X , where the probability of selecting  X  i is equal to the probability of  X  i being  X   X  p (  X 
E[ r  X  i ]=max  X  pected reward of the policy  X  i . It is difficult to directly compute this probability [22]. Thus, in each trial, we ran-domly draw a value, denoted by  X  r  X  i , from the distribution of  X 
E[ r  X  i ]foreach  X  i  X   X , and then select the policy that has the maximum value of  X  r  X  i . In the following, we present two approaches to estimate E[ r  X  i ].
HyperTS estimates the expected reward E[ r  X  i ]ofeach policy  X  i  X   X  using Monte Carlo method. Concretely, let x ,..., x n be the contexts of n trials in which  X  i is selected. x ,..., x n are samples drawn from p ( x ). For an input context x ,  X  i pulls the arm a j and receives the reward r j ,where a is seen as a sample from p ( a =  X  i ( x j ) | x j ), r j sample drawn from p ( r | x j ,a j ). Thus, ( x j ,a j ,r ple drawn from the joint distribution p ( x ,a,r ), j =1 , ..., n The Monte Carlo estimate is  X  E[ r  X  i ]= 1 n n j =1 r j .The rewards r 1 , ..., r n  X  X  0 , 1 } are the sample drawn from the Bernoulli distribution p ( r ), which is a marginal distribution of p ( x ,a,r ). Therefore,  X  E[ r  X  where  X   X  i = n j =1 r j and  X   X  i = n  X   X   X  i . For the prior, Beta(1 , 1) is used. Algorithm 1 shows the pseudo-code of a Beta distribution. The selected policy is the one having the maximum r i .
 Algorithm 1 HyperTS ( X ) 1: for i =1 , ..., m do 3: end for 4: for t =1 , 2 , ... do 5: for i =1 , ..., m do 7: end for 12: else 14: end if 15: end for
In HyperTS , the expected reward of each base policy is estimated only from the feedback when that policy is se-lected. The feedback of the decision made by other policies is not utilized. If the number of policies in  X  is large, the total number of trials needed for exploring the performance of base policies will be large and the total reward will be smaller. To improve the estimation efficiency, we propose HyperTSFB ( HyperTS with shared feedback), an algorithm that fully utilizes every received feedback for expected re-ward estimation.

Given the context x , HyperTSFB requires each base policy  X  i  X   X  provide the probability of  X  i pulling the arm a , i.e., p ( a =  X  i ( x ) | x ). Then, even though the policy selected in the trial, HyperTSFB can still utilizes the feedback for x to estimate the expected reward of  X  i . For some policy, p ( a =  X  i ( x ) | x ) can be computed directly. For instance, if denotes random policy, then p ( a =  X  i ( x ) | x )=1 /k , if  X  i is -greedy ,then where a  X  is the arm that has the maximum predicted reward by the input x . For some policy, p ( a =  X  i ( x ) | x )canbedif-ficult to compute, e.g. contextual Thompson sampling. We can invoke  X  i ( x ) multiple times to estimate this probability according to the frequency of a being output.
 as Eq.(1) and importance sampling [32] can be leveraged for estimation.

E[ r  X  i ]= E r, x [ r  X  w  X  i a, x | a ] is the expected reward of  X  i Eq.(1) states that the expected reward estimation can be separated into multiple estimations for different arms. In the importance weight, p ( a | x ) is the probability of the arm a being pulled given the context x , In the implementation, we use a sampling-based method to obtain the value of p ( a | x ). For each given context x ,we invoke HyperTSFB multiple times and then estimate p ( a | x according to the frequency of a being selected. p ( a )isthe marginal probability of a being selected, which is simply approximated by the ratio of a being pulled in all previous trials done by HyperTSFB . In Eq.(1), the expected reward of  X  i by pulling a is E y
In Eq.(2), E r, x [ r | a ] is the expected reward of pulling arm a , which is also the overall CTR of a and determined by the popularity of a . The importance weight w  X  i a, x is proportional to the probability of  X  i pulling a given x .E x [ w  X  i a, x reflects how likely will  X  i pulls a if the reward of a is 1. Intuitively, Eq.(2) states that the expected reward of  X  i pulling a is determined by the popularity of a and the likeli-hood of  X  i pulling a if the reward of a is 1. Since r  X  X  we use a Beta distribution to model  X  E r, x [ r | a ], i.e. where  X  a is the number of trials that the received reward is 1 by pulling a ,  X  a is the number of trials that the received reward is 0 by pulling a . For the prior, uniform distribu-mean of importance weights in previous trials in which a is pulled and the reward is 1. Assuming for one policy and one arm those importance weights will converge in one dis-tribution, based on Central Limit Theory, the sample mean follows a normal distribution when the sample size is suffi-cient large, i.e. where  X  a,i and  X  2 a,i are the mean and variance of the distri-bution of the importance weights, n a,i isthesamplesizeto calculate the the mean and variance. If the sample size n is not sufficient large (less than 30 [9]), we draw  X  E x 1 ,a ] from uniform distribution U (0 , 1).

Algorithm 2 shows the pseudo-code of HyperTSFB .For trial t =1 , 2 , ... , given the context x t ,thesampledexpected reward of  X  i is r  X  i , which is calculated based on two other sampled values from each arm (Line 8 to 21). As for the re-ceived reward, r x t ,a , the estimated parameters of every base policy and arm a are updated (Line 24 to 34).
We verify our approaches on two real-world data sets, in-cluding news recommendation data ( Yahoo! Today News ) and online advertising data ( KDD Cup 2012, Track 2).
Yahoo! Today News data set is collected by Yahoo! Today module 1 . News articles were randomly displayed on the Ya-hoo! Front Page from October 2nd, 2011 to October 16th, 2011. The data set contains 28,041,015 user visit events to the Yahoo! Today Module . Each visit event is associated http://webscope.sandbox.yahoo.com/catalog.php.
 Algorithm 2 HyperTSFB ( X ) 1: for j =1 , ..., k do 3: for i =1 , ..., m do 5: end for 6: end for 7: for t =1 , 2 , ... do 8: for j =1 , ..., k do 10: end for 11: for i =1 , ..., m do 13: for j =1 , ..., k do 16: else 18: end if 20: end for 21: end for 26: for i =1 , ..., m do 30: end for 32: else 34: end if 35: end for with the user X  X  information, e.g., age, gender, behavior tar-geting features, etc., represented by a binary feature vector of dimension 136. This data set has been used for evaluating contextual bandit algorithms in other literatures [6, 13, 14]. 10 million user visit events are used in this evaluation. KDD Cup Online Advertising data set is published by KDD Cup 2012 2 . Each record of this data set is an ad impression, containing user profile, queries, ad information and click counts. In our work, the context is represented as a binary feature vector, each entry of which denotes whether a query token is contained in the search query. User pro-files, e.g., gender and age, are also appended to the context vector using the binary format. The dimension of the con-text features for this data set is 1,070,866. One issue of this data set is that the click information is extremely sparse due to the large pool of the ads. To alleviate this problem, we only select the top 50 ads with the most impressions. The generated data set contains 9 million user visit events.
The experiments on Yahoo! Today News data set is eval-uated by the Replayer method [14], which provides an un-biased offline evaluation by utilizing the historical log. It has been shown that the CTR estimated by this Replayer approaches the real CTR of the deployed online system if http://www.kddcup2012.org/c/kddcup2012-track2. the items in historical user visits are randomly and uni-formly recommended [14]. However, for KDD Cup data set, the search ads in historical logs are not uniformly rec-ommended. We hence evaluate this data set using a simu-lation method [6]. In the simulation method, we first train a logistic regression model for each ad using the entire data offline. Then, for each impression with context x , the click of an ad is generated with a probability (1+exp(  X  w T x )) Although the evaluated CTR is not the real CTR, it pro-vides a methodology for comparing different policies in such high dimensional data.
For evaluation purpose, we use the averaged reward as the metric, which is the total reward divided by the total number of trials, i.e., 1 n n t =1 r t ,where n is the number of trials. The higher the CTR, the better the performance. In the experiments, to avoid the leakage of business-sensitive information, we report the relative CTR, which is the overall CTR of an algorithm divided by the overall CTR of random selection. The base policies used for the ensemble contain multiple types of algorithms, including: In the experiments, the reward in a single recommendation activity is the user click, which is a binary value. There-fore, logistic regression is applied as the learning model in all policies (except for Random ). Since the contextual bandit algorithms are online algorithms, stochastic gradient ascent is used as the learning algorithm [4]. Notice that the algo-rithms digest the data in an online manner, hence all the user visits in the data sets are used for the testing purpose.
In our problem setting, we utilize ensemble strategies to obtain a unified policy. To evaluate the effectiveness of the ensemble, we empirically compare the following ensemble algorithms:
For each policy, we test its performance on the entire data to obtain the overall CTR. To emphasize the robustness of our proposed ensemble strategy, we also split the data into multiple time buckets, and evaluate how the policies perform on each individual time bucket.
For base policies, most of them are randomized except for LinUCB . For each trial, we randomly shuffle the pool of items to be recommended. Thus, the performance of Lin-UCB may vary in different runs. We run each policy 10 times, and calculate the mean, standard deviation, minimum and maximum of the overall CTR. Table 1 reports the results. The mean values of the top 5 base policies are highlighted in bold , and the comparable mean values of ensemble strate-gies are emphasized by bold *.

As depicted in the table, the performance of the base poli-cies varies significantly with different parameter values. Ex-cept for Random (pure exploration) and -greedy(0.0) (pure exploitation), all the base policies take into account both exploration and exploitation. If the parameter that controls the relative importance of exploration and exploitation is perfectly set, then the performance approaches to the op-timal; otherwise, the performance is relatively poor. How-ever in practice, it is often difficult to determine the optimal value for the input parameter of each policy, primarily due to the online learning process of the algorithms as well as the unknown data distribution for learning models.
Our proposed ensemble strategies can achieve compara-ble performance with the top ranked policies in terms of the overall CTR, by virtue of the bandit property of the hyper model. It is worthy to note that the two ensemble strategies have no parameter to choose. Intuitively, with more trials, the base policies with the bandit property can produce bet-ter results as there are more data used for learning. By employing the ensemble strategies that select base policies based on their corresponding overall CTR, we can certainly obtain a unified policy with acceptable CTR. From Table 1, we observe that: (1) The HyperRandom policy performs pure exploration on the base policies. This may work well at the beginning of the learning process; however, after a long run, it cannot obtain acceptable performance due to the random-ness of the policy selection. (2) The bandit-based ensemble strategies, i.e., HyperTS and HyperTSFB , are able to achieve comparable performance with the top ranked base policies.
The advantages of the meta-bandit policies involve two as-pects: (1) by exploring/exploiting multiple base policies that have different parameter settings, the meta-bandit policies are able to absorb the merits of good policies, and hence produce robust results in terms of the overall CTR; and (2) there is no parameter setting required for meta-bandit policies. HyperTS does not have the mechanism of sharing feedbacks among different policies, then for each base pol-icy, the digested click traffic may not be sufficient to pro-duce a high-quality learning model and an accurate CTR estimate. Without enough click traffic, the base policies that exhibit relatively poor performance at the beginning of the trails may have very limited opportunities to be ex-plored/exploited in the subsequent trials, even though they are good policies if running solely. This is the primary rea-son that the performance of the HyperTS policy is inferior to the one of the HyperTSFB policy, as indicated in Table 1.
Besides the overall CTR of each policy, we also evalu-ate the CTR on individual time bucket. The CTR on each bucket is calculated by the clicks collected in that bucket. The entire Yahoo! Today News data is split into 100 time buckets, where each bucket has 100,000 impressions on news articles. The KDD Cup data set is split into 90 time buck-ets, with 100,000 impression for each bucket. All the user visit events are order by the time.

For the purpose of illustration, we compare the ensem-ble strategies, i.e., HyperTS and HyperTSFB ,witheachtype of contextual bandit policies, including -greedy , LinUCB , Softmax , Epoch-greedy , TS and TSNR . At each time bucket, these policies are executed independently, and the relative CTR for each policy is calculated. The results for Yahoo! Today News data set and KDD Cup data set are reported in Figure 1 and 2, respectively. We can observe that the policy of HyperTSFB achieves consistent performance on both data sets. Although in some time buckets the relative CTR of Hy-perTSFB is slightly lower than the one of some specific base policies, its overall performance is quite robust compared with other baselines.

From Figure 1 we observe that the CTR curves of differ-ent policies have wide fluctuations. This is because the CTR estimated in Yahoo! news data is close to the real CTR in each bucket. The real CTR of an online recommender sys-tem usually varies over the time. For instance, popular news articles may become unpopular since the news is aging. User interests may change from daytime to nighttime. In con-trast, the CTR curves for KDD Cup data, as described in Figure 2, are quite flat except the first few time buckets, and the reason is straightforward. The click of KDD Cup data is simulated by a group of logistic regression models. The time factor is not included in those models. Despite differ-ent characteristics of the two data sets, our proposed meta-bandit policy performs in a very robust way. This further demonstrates the generalization capability of our proposed method in dealing with different recommendation problems. Another interesting phenomenon is that the CTR lift of HyperTSFB is significantly higher than the one of HyperTS at the first few time buckets on both Yahoo! Today News data and KDD Cup data. The reason here is straightforward: by sharing feedbacks among different policies, the data used for exploring/exploiting the policies become rich, and hence there are more data used for training the underlying learn-ing model and estimating the performance of base policies. Therefore, at the initial time buckets, HyperTSFB outper-forms HyperTS in terms of CTR.

To further demonstrate the robustness of our proposed methods, we consider to rank all the policies based on their CTR lift, and then examine if the result of HyperTSFB is in the top ranked list. Specifically in each time bucket, we rank the base and ensemble policies based on the CTR lift, and then count the number of times that a policy appears in the top@ k ranked list. Next, we calculate the ratio of this count with the total number of buckets for each policy. Finally, we rank the policies based on their ranking ratios. For this evaluation, 4 best performed policies of Yahoo! Today News data and KDD Cup data are reported in Table 2 and 3, respectively, in which we consider the top@1, top@3 and top@5 results. Table 2: CTR ranking on buckets for Yahoo! data. As observed in Table 2, our proposed policy HyperTSFB on Yahoo! Today News data always achieves the 1st place in the top@1, top@3 and top@5 results. Also in Table 3, Hyper-TSFB reaches the 2nd place of top@1, 3rd place of top@3, and 1st place of top@5. The results indicate that Hyper-TSFB is able to achieve promising performance in most time buckets. Such an observation further confirms the robust ca-pability of our proposed policy in handling different online recommendation problems.

Table 3: CTR ranking on buckets for KDD data.
In addition, the performance of base policies with dif-ferent parameter settings may vary significantly over dif-ferent experimental data. From Table 2, we observe that -greedy (0.01) and Epoch (100) perform very well on Ya-hoo! Today News data, indicating that for this data set, the bandit policies can have achieve striking performance with a limited exploration. Comparatively in Table 3, -greedy (0.0) and Epoch (500) are able to produce promising results, meaning that higher CTR can be achieved on KDD Cup data based on the policies with much less exploration.
In personalized recommender s ystems, the dilemma of ex-ploration/exploitation in the cold-start situation remains a challenging issue due to the uncertainty of user preferences. A lot of contextual bandit policies have been proposed to tackle this dilemma; however, the prerequisite of the input parameters limits the predictive power of the policies. In real-world applications, these policies cannot be easily eval-uated under different parameters as they may require too much web traffic and affect the profit of service providers.
In this work, we explore ensemble strategies of multi-ple contextual bandit policies to obtain robust predicted CTR. Specifically, we employ a meta-bandit paradigm that places a hyper bandit over the base bandits, to explicitly ex-plore/exploit the relative importance of base bandits based on user feedbacks. The proposed approach does not have the restriction on the number of policies being involved, and can always obtain an acceptable CTR close the the optimal. Ex-tensive empirical evaluation on two data sets demonstrates the efficacy of our proposed approach in terms of CTR. The work was supported in part by the National Science Foundation under grants D BI-0850203, HRD-0833093, CNS-1126619, and IIS-1213026, the U.S. Department of Home-land Security unde r grant number 2010-ST-06200039, Army Research Office unde r grants W911NF-10-1-0366 and W911NF-12-1-0431, and an FIU Dissertation Year Fellowship.
