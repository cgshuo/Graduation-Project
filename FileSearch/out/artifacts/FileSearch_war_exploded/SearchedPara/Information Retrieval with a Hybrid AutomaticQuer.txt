 1. Introduction Information retrieval (IR) systems are indispensable tools to fight information overload in the Internet age. Improvement in IR performance can be achieved through improved search procedures that better match the user X  X  information need and indexes documents. Research in this area looks at two fundamental issues. One, assuming that a user query represents imperfectly the user X  X  information need, how to enrich the query in order to obtain a better documents in a collection of documents (corpus) are relevant. A rich array of approaches has been used to develop different search procedures to address these issues. The vector-space approach is probably the most classic next to the Boolean search (Salton 1975). Other typical approaches include Bayesian classification (Bookstein and Swanson 1974, 1975), inference network (Turtle and Croft 1991), clustering (e.g. Willett 1988), data-fusion (e.g. Ng and Kantor 1998), and recently language modeling (e.g. Ponte and Croft 1998). Machine learning methods including genetic algorithm, classification trees, and neural network (e.g., Chen 1994) were also explored. In this light, it is safe to say that most IR research has focused primarily on improving the search procedure.

This paper reports on a research effort to improve the search procedure. We propose a hybrid IR procedure that builds on two well-known IR search approaches X  data fusion and 42 XU AND BENAROCH query expansion via r elevance-feedback . Data fusion improves IR performance by inte-grating the multiple search results yielded by running several different search mechanisms (e.g., search engines) on the same user query or by running a single search mechanism on several different user queries reflecting the same information need (Ng and Kantor 1998). Relevance-feedback runs a single search mechanism on a user query to obtain initial retrieval results based on which it then revises, or expands, the user query for follow-up re-trieval runs. These approaches, however, are not without their own limitations. A drawback of relevance-feedback is the way it typically revises a user query: it employs an adaptive learning method whose parameters must be set on a trial-and-error basis. As to data fusion, though it holds the promise of  X  X wo heads are better than one X  (Kantor 1992), one weakness is that it typically employs ad hoc results-integration methods that lack theoretical justifica-tion. In addition to the question of  X  X ow X  to integrate multiple search results in data fusion, the question of  X  X hen X  such integration will be effective is to a large degree unanswered.
Our hybrid IR procedure is designed to exploit the strengths of both the data fusion and relevance-feedback approaches, and at the same time it tries to resolve some of their weaknesses. In this procedure, we use the relevance-feedback results as surrogate queries. parameter selection problem in relevance feedback can be avoided. At the same time, integrating the multiple retrieval results with the  X  X um-cosine X  score is also based on the propositions that are theoretically and empirically justifiable, as will be shown later.
In testing the propositions that our procedure is based on, we draw three observations from an extensive empirical testing. First, our procedure X  X  retrieval performance is significantly improved over that of the traditional vector space model (Salton 1983). Second, while our procedure X  X  retrieval performance is comparable to that of relevance feedback, its ability to estimate the user X  X  information need is superior. Finally, we are also able to verify the conditions under which such improvements should be expected.

The rest of this paper is organized as follows. Section 2 reviews the literature in areas related to this study, and particularly, the research in data fusion and relevance feedback. Section 3 presents our hybrid IR procedure. Section 4 provides the propositions that our procedure is based on. Section 5 reports on the empirical testing results and implications. Section 6 offers concluding remarks and directions for future research. 2. Literature review 2.1. Data fusion Our proposed procedure integrates multiple retrieval results, hence built on the data fusion research. IR research on data fusion studies the integration of retrieval results from multiple IR systems. Two variations of data fusion systems can be observed in prior researches. The first kind employs different search mechanisms, but the same query. The retrieval results, which are typically in the form of a relevance score list, are integrated to produce the final ranking. Examples of such studies include (Ng and Kantor 1996, 2000, Bartell et al. 1994, Cohen 1998).
 HYBRID AUTOMATIC QUERY EXPANSION 43
A second variation of the data fusion strategy employs multiple queries but the same search mechanism (e.g. Belkin et al. 1993, Lee 1998). This variation can be traced back to Katzer et al. (1982) who found that, when a user states the same query using different query languages (e.g., Boolean query, natural language query), the documents retrieved have little overlap. Respectively, later research showed that retrieval performance could be improved by combining the retrieval results for multiple dissimilar queries (Fox and Shaw skills, because the multiple queries are typically constructed manually as Boolean queries. Why does data fusion have the potential to improve performance? Kantor (1992) and Saracevic and Kantor (1988) observed that given the same information task, different searchers, with their different queries, would retrieve similar sets of relevant documents, bu t dissimilar sets of nonrelevant documents. Therefore, even a mere voting scheme would make the shared relevant documents standing out. Kantor (1992) attributed this  X  X wo heads are better than one X  phenomenon to the  X  X nrichment effect X  of data fusion that enhances the odd of a relevant document being retrieved as contrasting to the nonrelevant one when multiple judges are used. Notice Kantor (1992) implicitly assumed that the probability of a relevant document being retrieved is larger than that of a nonrelevant one.

Theoretically promising as it is, research on data fusion at large has produced mixed results: sometimes it improves performance, and sometimes not (Ng and Kantor 1996, 2000, Ng et al. 1997). Ng and Kantor (1998), for example, identified some conditions under which data fusion might improve performance: the results returned by different retrieval mechanisms should have little overlap, and none of the retrieval mechanisms should be too ineffective. Although such conditions may provide some practical guidance to data fusion, they are at the search mechanism level; at the query level, they do not predict when multiple queries used with the same search mechanism would improve performance. Identifying such conditions is an important issue.
 results. Data fusion research tends to look at ad hoc methods for combining the results of different retrieval mechanisms. For example, assuming a document has been assigned different relevancy scores by different retrieval mechanisms, Fox and Shaw (1994) tried summation, maximum, and minimum as alternative ways to compute the aggregate rele-v ance score and found the summation to be the most effective. Lee (1998) normalized the scores from different retrieval runs and took the summation as final score. Bartell et al. (1994) and Cohen (1998) used a supervised learning approach to learn the  X  X redibility X  of each retrieval mechanism and then computed (sum up) the aggregate score accordingly.
Unfortunately, not only is the effectiveness of such ad hoc methods uncertain, but they also gives no formal justification. In an effort to search for a more theoretically solid inte-gration method, Manmatha and Feng (2001) showed that the relevance score distribution of relevant and nonrelevant documents can be modeled with normal and exponential distribu-tion respectively. Therefore, to classify a document as accurate as possible, or equivalently to minimize the Bayes misclassification error, an integrated score of a document should be the sum of posterior probabilities p ( rel | score ) from different search mechanisms, as-suming the search mechanisms are independent. As an implication, because the posterior probability can be viewed as a normalized score and is monotonically related to the original 44 XU AND BENAROCH score, this offers a foundation for summing up scores as an integration method, though not optimal. This reasoning, however, assumes different and independent search mechanisms. It does not address the scenario when multiple queries are constructed for the same informa-tion need and are submitted to the same search mechanism. In addition, its performance is dependent on the estimation of the probability distribution, which might be prone to errors when the number of relevant document is small (Manmatha and Feng 2001).

Our proposed method differs from prior research in a few ways. First, we attempt to address the integration of the retrieval results from a same retrieval mechanism with multiple queries. This is more practical because most of the time a searcher has access to only one search mechanism. Second and consequently, we assume the scores of each retrieval result are calculated in the same way. In this circumstance, still using summation of scores, we show when and how it can be justified. 2.2. Relevance-feedback Our proposed procedure uses multiple surrogate queries generated by relevance feedback, hence built on relevance feedback research as well. Relevance-feedback essentially seeks to re-estimate the user X  X  information need through query expansion (Efthimiadis 1996). Query e xpansion adds or deletes terms in the original query as well as adjusts term weights, with the hope that the revised query would better describe the user X  X  information need. Salton and Buckley (1990) discussed a variety of procedures for relevance-feedback. In r eal (or manual ) relevance-feedback, the user is asked to decide which of the initially query. This approach, of course, puts additional burden on the user. In automatic relevance-feedback (also called pseudo or blind relevance-feedback), only the few top-ranked retrieved documents are treated as relevant, without any user involvement; if, in addition, the few bottom-ranked retrieved documents are treated as nonrelevant and they do participate in the feedback, we have full automatic relevance-feedback .

Rocchio (1965), whose work continues to dominate the thinking underlying current relevance-feedback research, views the ultimate purpose of relevance-feedback to be one of estimating the  X  X ptimal X  user query. While the notion of  X  X ptimal query X  could be subjective, Rocchio (1965) originally proposed the following measure: where N is the total number of documents in the corpus, d i is a document vector, R is the set of relevant documents in the corpus, I is the set of nonrelevant documents in the corpus, and n is number of relevant documents. However, since n is not known beforehand, Rocchio (1965) also proposed an adaptive term weighting method for revising (improving) the user X  X  original query. This method can be described by the equation: HYBRID AUTOMATIC QUERY EXPANSION 45 where q and q 0 are the revised and the original user query represented as document vectors, d identified by manual or automatic feedback), and  X  ,  X  and  X  are coefficients.

The major difficulty with the adaptive learning method is the choice of coefficients,  X  ,  X  , and  X  . Since there is no way to decide which set of coefficient values will lead to the best performance, they are set on a trial-and-error basis. For example, Buckley and Salton (1993) set  X  ,  X  , and  X  to 8, 16, and 4, respectively. Buckley and Salton (1994) later recognized the limitation of such parameters and tried to mitigate it by proposing an alternative adaptive learning method. They used  X   X  1 / R and  X   X  1 / I as coefficients, where R and I are the size of the subsets of relevant and nonrelevant documents (in the latest retrieval results), respectively. Although this alternative method does not point out a way to set the coefficients, it does try to mitigate the bias introduced by the number of relevant and nonrelevant documents in the latest retrieval results. Interestingly, later, in TREC-6, Buckley et al. (1997) adopted a different set of values for these parameters, with  X  = 8 and  X  = 8. More recent example of such fine-tuning in action can be found in Crouch et al. (2002), where they held  X  constant and varied  X  in search for a good v alue.

In summary of the literature review, we observe that the ad hoc consolidation methods typically used in data fusion still lack theoretical justification, at least in the case of non-independent search mechanisms. The relevance feedback suffers the coefficient selection problem. It is our objective to build on the strengths, as well as resolving some weaknesses, of both data fusion and relevance-feedback IR procedures. In the next three sections, we present our procedure and then offer its theoretical propositions and empirical evidence in support of various critical elements of this procedure. 3. Proposed hybrid procedure 3.1. The procedure Our hybrid IR procedure can be summarized as follows: 1. Run a regular vector-space retrieval session with the original user query, q 0 ,t o produce 2. Select from D k  X  1 top-ranked documents automatically or manually, and then run 46 XU AND BENAROCH 3. Consolidate the retrieval results of all surrogate queries in the top-k ,t o produce a final
The similarity of our IR procedure with relevance-feedback and data fusion should be apparent. The rationale for Step 2 is anchored in relevance-feedback: because the top-ranked documents (ideally relevant ones) reflect how query terms are used in relevant documents, they can be seen as  X  X evisions X  of the user X  X  original query. Step 3 is inspired by data fusion: retrieval performance can be improved through the use of multiple queries and the integration of the retrieval results from these queries. However, Step 2 in our procedure does not use an adaptive learning method, thus avoiding the error-prone coefficient-selection problem. Instead, they are treated as separate queries. Moreover, the  X  X um-cosine X  consolidation method used in Step 3 can be more easily justified than some other ad hoc integration methods, as will be explained shortly.

This procedure resembles Lee X  X  study (Lee 1998) which also used multiple queries gen-erated from relevance feedback and resubmitted them to the same search mechanism. In his study, surrogate queries were generated with different relevance feedback methods, such as the Rocchio X  X  and the probabilistic relevance feedback. Results were integrated with the sum of normalized scores for each query. However, his study did not try to theoretically justify the integration method, and no conditions were identified as to when such procedure w ould work. Our study tries to resolve these issues, in the context when the  X  X aw relevant documents X  are used as surrogate queries.
 A sensitive issue in our procedure is the selection of a good set of top-k documents in Step 2. Ideally, we should strive to select the top-k documents such that they will all be relevant to the user X  X  true information need. However, this is only possible with manual selection of the top-k .W ith automatic selection, there is no guarantee that the top-k docu-ments retrieved in Step 2 are all relevant. The next discussion explains how our procedure addresses this important issue. 3.2. Top-k selection strategy The reason it is necessary to select a  X  X ure X  top-k is obvious. The existence of nonrelevant documents in the top-k can drive the  X  X evised X  query away from the user X  X  original intention. Therefore, the overall performance of our procedure would be critically affected by the HYBRID AUTOMATIC QUERY EXPANSION 47 manual and automatic.

W ith manual top-k selection, we can set k to different values (2, 3, etc.), which would result in a varying number of relevant documents in the top-k . The disadvantage of a manual strategy is the extra user effort it involves. Consequently, we limit the number of relevant documents that a user has to identify in our later discussed experiments. Even though our ultimate goal is to use an effective automatic top-k selection strategy, considering the manual strategy in this research allows us to compare the performance between them.

For automatic top-k selection we can use different heuristics to set a cutoff point in the retrieval results returned by Step 1. All documents that rank above the cutoff point are treated as relevant blindly, just like in relevance feedback. Next we introduce the two heuristics for automatic top-k selection used in out test. 3.2.1. Heuristic 1. The basis for the first heuristic for picking the top-k documents is as follows. Upon examining the sorted series of cosine scores obtained in Step 1, we no-by Manmatha and Feng (2001); and (2) the difference of the cosine scores for the top-bottom-ranked documents (see figure 1(b) for example). This phenomenon is understand-able because, if two documents are nonrelevant, both their cosine scores will be close to zero, and so the difference between them will be small. By contrast, two relevant doc-uments can catch different aspects of the user X  X  query to different degrees, and because relevant documents are small in number, we can expect more fluctuation in the difference of their cosine scores. On this basis, we define the first automatic top-k selection heuristic as follows.
 Heuristic 1. Fo ra document to be included in the top-k ,i t must meet the following criteria: 1. The document X  X  cosine score is above 50% of the highest cosine score. 48 XU AND BENAROCH 2. If we make a sliding window of five consecutive documents, the document must be
The parameters in Heuristic 1 (i.e., 50 and 20%) are selected based on our examination of the TREC topic 301 X 320 cosine series generated by the vector-space model. No optimization has been attempted for these parameters. But we did try to select the parameters so that the number of documents returned is around 10. In fact, when tested on TREC topic 301 X 350, the average number of documents returned by Heuristic 1 is nine (with a range of 1 to 39). 3.2.2. Heuristic 2. The second automatic top-k selection heuristic, Heuristic 2, is designed to be more prudent based on the following concept. For a query, if the subset of relevant documents and the subset of nonrelevant documents are reasonably separated from each other, a retrieval run with the vector-space model would produce a ranking of cosine scores with a mirrored S shape (see figure 1(a)), with the relevant documents having higher scores and the nonrelevant ones having lower scores. If we want to separate relevant and nonrelevant documents based on this curve, we may want to set the dividing point at the point where the slope is the steepest (i.e., where the second derivative of the curve is zero). On this basis, we define the second top-k selection heuristic as follows.
 Heuristic 2. Include in the top-k all documents up to the point when the difference between the cosine scores of two documents is the largest.

Fo re xample, applying Heuristic 2 to topic 301 of TREC data sets k at 5, because the difference between the scores of documents 5 and 6 is the largest. For TREC topic 301 X 350, this heuristic returns on average only 2 documents.

Heuristics 1 and 2 select a small set of top-k documents by setting a cutoff point in the ranked list returned by the vector space model in Step 1. There are surely other ways to select the top-k documents. For example, rather than setting a cutoff point, we can impose additional criteria that a document has to satisfy to be included in the top-k . These criteria could be the presence of keywords, clustering behavior, and so on. We further discuss this issue in Section 6.

Now that we have operationalized our hybrid procedure, let us proceed to look at its theoretical foundations and empirical assessment results. 4. Theoretical investigation Our hybrid procedure relies on two postulates: 1. The sum-cosine measure is adequate for consolidating the results of multiple queries, HYBRID AUTOMATIC QUERY EXPANSION 49
These postulates can be stated more precisely as a proposition and a hypothesis, respec-tively. The first postulate would be referred to as the sum-cosine-adequacy proposition , and the second postulate as the two-heads-are-better-than-one hypothesis .Wenextprovethe proposition and show why the hypothesis is generally reasonable. 4.1.  X  X um-Cosine X  proposition The sum-cosine-adequacy proposition states that the sum-cosine similarity measure gives an adequate ordering of documents by their relevancy to the user X  X  information need.
Assume there are k v ersions of expression of the same information need, hence k queries; also assume for a specific document d j , k queries generate k cosine scores, cos( q 0 , d j ), cos( q 1 , d j ), . . . , cos( q k  X  1 , d j ); if q i is normalized then it is easy to see: where  X  q is mean of k query vectors. This simple derivation shows that the sum-cosine documents. Because each query can be viewed as an independent representation of user X  X  information need (top k documents are created independently), the centroid can be viewed as an estimate of the user X  X  information need. Sum-cosine is adequate in the sense that it ranks the document in accordance to the estimate of user X  X  information need. Naturally, this leads us to the question: how good can the centroid of the top-k documents represent the user X  X  information need? We answer this question by looking at the next hypothesis.
Before we move to the next hypothesis, notice that (1) the underlying similarity score product will show similar property; (2) sum-cosine is not the only integration method; other monotonic transformation of d j  X   X  q will do exactly the same job. Therefore, the analysis of the proposition and the following hypothesis applies to a more general situation than the specific procedure we adopted here. 4.2.  X  X wo-heads-are-better-than-one X  hypothesis 4.2.1. User X  X  information need. The two-heads-are-better-than-one hypothesis states the following: assuming there is an exact description of the user X  X  information need, then the top-k documents in K (i.e., surrogate queries, including q 0 ), taken together, are a better representation of the user X  X  information need than q 0 alone. This hypothesis has much empirical support in the research on relevance-feedback X  X any studies show that feedback with the first few top-ranked documents could be beneficial (Ng and Kantor 1996, 2000, Ng et al. 1997). Likewise, our empirical testing (reported shortly) shows that, in all the cases tested, the centroid of the top-k documents is closer to the user X  X  information need than the original user queries.

To formally investigate the basis for this hypothesis, we must first define notions like 50 XU AND BENAROCH whether a document meets the user X  X  information need and is regarded as relevant is a subjective matter. We are not interested in the subjective or cognitive relevance in this study, rather we adopt a more objective perspective, i.e., the  X  X ystem relevance X  or  X  X opicality X  of a document as classified by a panel of domain experts. Readers interested in the distinction between cognitive and system relevance can refer to Schamber (1994) and Borlund (2002).
We therefore simply assume that a user has a fixed interest as well as a consistent judgment of the relevancy of a retrieved document. We also assume that, for any given user query, the collection of relevant documents in a corpus is enough to satisfy the user X  X  information need. In this light, we define the user X  X  true information need as the mean (centroid) of all relevant document vectors in the corpus: where R is the set of relevant documents and d i is a specific document in R .

Such definition, however, implicitly assumes that relevant documents will cluster around a centroid in the document space, as conjectured by van Rijsbergen (1979) in the famous  X  X luster hypothesis X . Though such hypothesis is not without criticism (Rorvig 1998), at least with TREC data that we used for testing, it is still safe to say so. Empirical evidence can be found in Rorvig (1998) in supporting of the clustering hypothesis with TREC topics. Given this definition, we can now measure the quality of any document vector or estimate of user X  X  information need vis-` a-vis the user X  X  true information need. 4.2.2. Estimate of user X  X  information need and its quality. To see how the top-k is better than the original query alone, we are interested in comparing the user X  X  information need with the original user query and the estimate constructed using the top-k documents.
In order to measure the estimate quality, let X  X  first examine the factors affecting the quality of an estimate by looking at a hypothetical example. Assuming we are interested in civilian death in the wars in Africa, let the original query be: where the number following a term is its frequency in the query. Also, let the mean of all relevant documents in the corpus be: Now, suppose that after a top-k sample was collected, we construct an estimate (using the relevance feedback procedure or the hybrid procedure) and get: terms from the top-k documents. For all terms in q 0 , the weights are actually closer to the HYBRID AUTOMATIC QUERY EXPANSION 51 mean after revision. In contrast, the term &lt; Lebanon:0.2 &gt; represents an additional noise term that in fact drifts the query towards the civilian death in the mid-east. At this point, it should be fairly clear that two competing factors determine the quality of an estimate X  X he accuracy gain for terms in  X  , and the drifting effect of additional noise terms.
Therefore, to assess the quality of an estimate, we look at two measures. To capture the ov erall accuracy gain, we can take the cosine of  X  and q , and see if it is better than the cosine of  X  and q 0 . And, to capture the impact of noise terms which were already in q 0 or were introduced by additional terms that appear in q but not  X  ,w e can measure the contribution of these terms to the total vector length. Our hypothesis can be respectively restated as: 4.2.3. Estimate quality in the hybrid procedure. To further investigate when and why the top-k documents in the hybrid procedure could better approximate the user X  X  information need, let us make two additional assumptions. First, we maintain the conventional vector space assumption that terms in a document are independent. Second, we also assume that all the top-k documents are relevant, to be able to treat them as a sample from the set of all relevant documents, R .G iv en these two assumptions, our next discussion distinguishes between two cases: K is a random sample of documents in R , and K is a non-random sample in R .
 Case 1: K is a random sample. Suppose that the original user query, q 0 ,isrel ev ant and that it is randomly generated by the same underlying process that generates the remaining relevant documents in K . Hence, for the random variable q 0 ,wehave: where E ( q 0 )i s the expected value of the original query, Var( q 0 )i sav ariance-covariance matrix, and  X  and are the mean and variance-covariance matrix of all relevant documents. If documents in K are a random sample from the population of relevant documents, R ,it is obvious that, on average, the estimate of R  X  X  mean from such a sample would be a more accurate estimate of the user X  X  information need than the user X  X  original query, regardless of the underlying distribution of terms in document vectors. Mathematically, if  X  q is the av erage vector of the top-k documents, it can be shown that: 52 XU AND BENAROCH in  X  q will have a weight close to zero, and they will account for a small portion the total v ector length.
 Case 2: K is a non-random sample .T op-k is generally not a random sample. Therefore the simple conclusion derived above cannot apply. Since terms in document vectors are independent, let X  X  distinguish between terms that appear in q 0 and terms that do not, so that we can separately examine their impact on final representation of the user X  X  information need.

Suppose that out of all the terms appearing in the top-k documents ( K ),  X  q q the top-k documents would be the vector (  X  q q represented as ( q 0 , 0), where 0 is a zero vector.
 We need to look at the competing forces affecting the quality of the final estimate. independent of those in q 0 , the top-k documents can still be viewed as a random sample, and accordingly they benefit the final estimation as in Case 1. They produce unbiased estimate. On one hand,  X  q a captures some features of the user X  X  information need not covered by q 0 . On the other hand, this estimate could also potentially introduce terms not in  X  (the mean of documents in R ), which will abduct the query, like the term &lt; Lebanon:0.2 &gt; in the example above. By contrast, for terms that do appear in q 0 , the top-k documents produce an uncertain estimate because they are not a random sample. Consequently, for those terms, we do not know whether or not they are getting closer to the mean of R  X  X  population. Combining all these three competing forces, the quality of final estimate is indecisive. The condition that  X  X wo-heads-are-better-than-one X  applies when the accuracy gained from adding more relevant terms over-compensates the possible introduction of noise terms and the possible deteriorating of terms in q 0 .

W ith the hybrid procedure or similar procedures that rely on top-k as an estimate of user X  X  information need, however, we still have three reasons to be optimistic. First, because the number of terms in q 0 is very small compared to the number of all terms in the revised query (which could potentially have all the terms in the top-k documents), we can argue the benefit gained from terms not in q 0 outweighs the possible loss from those terms in q 0 . Second, the impact of additional noise terms is limited. Because their true mean is zero (they are nonrelevant terms with zero weight in the user X  X  information need) and we use their unbiased estimate in the revised query, we expect the weight estimates of these terms to be close to zero. Finally, if a query is relatively short, the weights for terms in q 0 are usually up-biased. For example, assuming that a term appears in q 0 only once, a query with ten terms will give each term a 0.1 weight (by only frequency counting), which is usually higher than it could be with a document containing 200 terms. Although the top-k is not a random sample, it is reasonable to expect that the sample average will produce a less biased estimate than q 0 alone. The implication is that the average of the top-k documents produces a better estimate of the user X  X  information need even for terms in q 0 . This observation is actually supported by our empirical tests, as we will see shortly.

However, the above observations are based on the assumption that top-k are relevant. If this assumption is not met, the conclusion will be largely affected by the  X  X uality X  of the HYBRID AUTOMATIC QUERY EXPANSION 53 nonrelevant documents selected into top-k .I f they are close to the user X  X  information need in vector format, they may still contribute. Otherwise, they may distort the presentation of user X  X  information need.

Relevance feedback, as in Rocchio X  X  original idea, also assumes there is a true information need and more  X  X eads X  can provide better estimate of the information need. In approaching the true information need, later researchers typically implicitly assumed that the more recent documents are closer to the true need, hence heavier weight on  X  .Y et this presumption may not apply to many situations.

In summary, our hybrid procedure is based on two postulates. We first proved the propo-sition that the result integration measure X  X um of cosines X  X s an adequate measure for ranking the retrieval results of multiple queries. This is because the sum-cosine score ranks the documents by their closeness to the centroid of top-k documents which is used as an estimate of the user X  X  information need. We then showed that the ability of the top-k doc-uments to represent the user X  X  information need is affected by two competing forces: the accuracy gained by re-weighting and adding relevant terms, and the noise introduced by additional noisy terms. In the context of the hybrid procedure, we showed analytically that the top-k could be better than the original query alone given the assumption that top-k are all relevant. 5. Empirical tests Having seen the theoretical basis for our hybrid procedure, this section presents an empirical assessment of the procedure. This empirical assessment has two specific goals: 1. Test the performance of the hybrid procedure against that of several benchmark IR 2. Measure how well the top-k documents represent the user X  X  information need, compared
We introduce next the datasets and experimental design. Then, we present the performance results; because the performance of the hybrid procedure also depends on the strategy used to select the top-k ,w e report the results for different top-k selection strategies. Finally, we report on evidence supporting the hypothesis that the hybrid procedure yields a better estimate of the user X  X  information need. 5.1. Datasets and experimental design 5.1.1. Data sets. We used TREC 4&amp;5 data set for the test. The topics we used range from topic 301 to 450. For each query, the data set specifies which of the documents in the corpus are relevant (as established by the domain experts who constructed the set). We thus have a  X  X old standard X  for judging the performance of the compared IR procedures.
 (narrative) sections. We also noticed that there are some non-content-bearing words in the topic. For example, the &lt; narr &gt; section of topic 301 includes the words  X  Ar elevant 54 XU AND BENAROCH document must as a minimum identify  X  because of the writing style. These words are completely nonrelevant to the topic of  X  International Organized Crime . X  To address this,  X  X elevant X ,  X  X ocument X  and so on. It is possible that these words could be content-bearing in some contexts, but we expect very few such cases.

Fo r each document and query, we applied the regular preprocessing steps X  X top word deletion and Porter X  X  stemming X  X n order to convert the data sets into the document vector-space representation. We also used the following  X  ltc  X  TFIDF weighting scheme (Buckley et al. 1993, 1994) for both documents and queries: where w ij is the weight of term j in document i , f ij is the frequency of term j in document i , N is the total number of documents in the corpus, and n j is the number of documents which contains term j . 5.1.2. The runs. T able 1 lists all the test runs included in our experiment. The basis for our choice of runs is as follows. First, we select two IR procedures as benchmarks for performance comparison: the vector space model and relevance feedback. It is our interest to see whether the hybrid procedure improves on the vector space model, and how it differs from relevance feedback despite their similarities. Moreover, note that the retrieval results of the basic vector space run are used by both the standard relevance feedback procedure and the hybrid procedure. Second, because the retrieval effectiveness of the hybrid procedure automatic top-k selection using Heuristics 1 and 2 (see Section 3.2).

Fo r manual top-k selection, we simulate human selection behavior by checking each of the documents returned by the Vector Space (VStrec) run against the relevant document list for each query (or topic). If a document is found relevant, it is kept for feedback. This process continues until a desired number of relevant documents are found. Thus, the nonrelevant documents found in the process are not used for query revision. We also maintain that, for a query with n relevant documents, the number of relevant documents used for feedback should be less than n .F or each query, a single run returns only the first 1000 documents, as adopted by SMART experiments (Buckley et al. 1994). For all runs with manual top-k selection (in both relevance feedback and the hybrid procedure), the manually selected relevant documents are excluded from the corpus in the second round. These manually selected relevant documents are also excluded from the  X  X nswer list X  for performance ev aluation. In other words, we view these file as a  X  X raining set X . For all runs with automatic top-k selection, those documents are not excluded and they are blindly assumed relevant. Fo r automatic top-k selection, we include the runs AutoTop10RF, AutoTopKRF, and AutoTopKHybrid. The AutoTop10RF run blindly assumes the top 10 documents in the VStrec results as relevant and uses them for relevance feedback. We chose AutoTop10RF as a benchmark because it is a commonly used automatic relevance feedback procedure. However, we believe it is not effective to restrict the number of documents used for feedback HYBRID AUTOMATIC QUERY EXPANSION 55 introduce noise unnecessarily. Instead, a dynamically chosen cutoff number ( k ) might be better. Therefore, the AutoTopKRF runs use Heuristics 1 or 2 to pick the top-k documents for relevance feedback. The AutoTopKHybrid runs use the same two heuristics as AutoTopKRF, bu t they use our Sum-of-Cosines method to consolidate results. Thus, the AutoTopKRF runs can also help us to separate the improvement in performance due to the top-k selection strategy from the improvement due to the Sum-of-Cosines consolidation method in our hybrid procedure. 5.1.3. The IR procedures and parameters. Among the three procedures tested X  X ector space model, relevance feedback, and hybrid procedure X  X nly relevance feedback has pa-rameters that need to be set beforehand. There are mainly three parameters for relevance feedback: the alpha and beta used in the adaptive learning method, and the length of revised queries.

W ith the TREC data, the top few documents returned by the vector space model usually give a low precision. With the top 10 documents, the precision is about 0.32, which means that about three out of ten documents are relevant. This suggests that feedback information may unduly introduce noise terms. Some preliminary tests that gave more weight to beta 56 XU AND BENAROCH (e.g.  X  = 2,  X  = 8;  X  = 3,  X  = 7) yielded unsatisfactory result and confirmed this observation. Therefore, in our final test, we gave more weight to alpha (  X  = 7,  X  = 3), putting more trust in the user X  X  original query than the feedback information. Additionally, because the TREC documents are longer on average and adding all the terms to a revised query can introduce noise unnecessarily, we kept only the top 300 terms for feedback, as suggested in Buckley et al. (1994) and Buckley (1995). 5.1.4. Performance measures. In reporting the performance of the different runs, we pro-vide only the summarized measurements of all the queries tested. Some major measurements we used are summarized in Table 2. 5.2. Performance testing In what follows, we report on the performance test results of the hybrid procedure and the benchmark IR procedures, while distinguishing between different top-k selection strategies. 5.2.1. Results for manual top-k selection. We tested the hybrid procedure with manually selected top-k documents ( k = 1 , 2 , 3 , 4 , 10) used as surrogate queries, and compared it with relevance feedback with the same k documents. Table 3 summarizes the testing results. The hybrid procedure outperforms relevance feedback when a small number of top-k doc-uments ( k = 1 , 2 , 3 , 4 , 10) are used. The differences are statistically significant when one or two top documents are used. When more relevant documents are used ( k = 3 , 4 , 10), the improvement becomes insignificant. This is not surprising because, when more documents HYBRID AUTOMATIC QUERY EXPANSION 57 58 XU AND BENAROCH are used, the revised query of relevance feedback is closer to the average of the top-k ,in which case the influence of every individual document is becoming more and more equally strong as in the hybrid procedure.

Figure 2(a) compares precision at eleven recall levels for k = 2; same patterns were found for other levels of k .I t shows that the improvement in precision of the hybrid procedure is consistent at almost all recall levels. Table 4 summarizes the comparison in R -precisions. 5.2.2. Results for automatic top-k selection. We tested the hybrid procedure with au-tomatic top-k selection using Heuristics 1 and 2. Whereas Heuristic 1 selects about nine documents on average, Heuristic 2 is more restrictive and selects only about two documents on average. Since we expect the automatically selected top-k to include some nonrelevant documents, we examine the effect of the size of k using five runs. Table 5 and figure 2(b) summarize the results for these runs, suggesting two overall patterns: 1. Both the hybrid procedure and relevance feedback perform better than the vector space HYBRID AUTOMATIC QUERY EXPANSION 59 60 XU AND BENAROCH 2. The hybrid procedure performs worse than relevance feedback, and the difference is
The overall patterns suggested by Table 5 are quite puzzling. On one hand, they are con-sistent with other experiments reported in TREC showing that, a blind relevance feedback (as exemplified by AutoTop10RF here) helps to improve retrieval performance. So do rele-v ance feedback and the hybrid procedure with Heuristics 1 and 2. On the other hand, we are left with two troubling questions: (1) Why didn X  X  the hybrid procedure outperform relevance feedback, and (2) why didn X  X  Heuristic 2 (which is seemingly more prudent) lead to better performance? To answer these perplexing questions, we need to take a closer look at the parameters used for relevance feedback and several characteristics of the TREC datasets.
As mentioned above, we noticed that the R-precision is very low for the top a few documents in the vector space result: a precision of 32.8% at the top 5 documents, and 31.8% at the top 10. Given the low precision levels of automatically picked top-k , the way that these top-k documents are used becomes more important. Relevance feedback (with  X  = 7 and  X  = 3) emphasizes the original query more than the top-k documents. As a result, nonrelevant documents in the top-k ex ercise less influence on the revised query and on the final estimate of the user X  X  information need. In contrast, the hybrid procedure (with the sum-cosine scoring method) gives every document and the original query an equal v oting right, which turns out to be biased toward the nonrelevant documents. Based on these observations, it should be understandable why the hybrid procedure is slightly worse than relevance feedback.

The low precision with the few top documents also implies that the relevant and the nonrelevant documents are in fact quite similar to each other in the front portion of the returned list. A nonrelevant document is not necessarily of less value in query revision than a relevant but lower-ranked document. If top-k selection has to be done by setting a cutoff point, as in the blind relevance feedback and Heuristics 1 and 2, it should make the best use of those nonrelevant documents in addition to the relevant ones. Because Heuristic 1 on av erage sets the cutoff point at about 9, while Heuristic 2 sets it at 2, Heuristic 2 is in fact too restrictive and forgoes some useful nonrelevant documents. We believe this is another reason why Heuristic 2 is inferior to Heuristic 1 in this experiment. 5.2.3. Summary of performance testing. We tested our hybrid procedure and its bench-marks on TREC data with both manually and automatically selected top-k documents. We conclude from the results that:  X 
The hybrid procedure outperforms the vector space model with both manually and auto-matically selected top-k .  X 
The hybrid procedure outperforms relevance feedback when a small number of relevant documents are selected manually. However, when the top-k is automatically selected by setting a cutoff point, relevance feedback marginally outperforms the hybrid procedure.  X 
The overall performance of runs with manually selected top-k is better than that of the automatically selected top-k .F or both relevance feedback and the hybrid procedure, the quality of the top-k has a direct effect on the final IR performance. The existence of nonrelevant documents in the top-k could lead to severe query distortion.
 HYBRID AUTOMATIC QUERY EXPANSION 61  X 
The hybrid procedure is more sensitive to nonrelevant documents in the top-k ,but it makes better use of relevant documents. The practical benefit is that, with a manual strategy, the hybrid procedure requires less user effort to improve IR effectiveness.  X 
It is desirable to develop better schemes for automatic top-k selection so that the overall effectiveness of the hybrid procedure can be improved. 5.3. Testing the quality of user X  X  information need estimate The goal of this section is to verify the hypothesis we raised in Section 4, namely: using the top-k selected documents as multiple surrogate queries could be a better way to represent the user X  X  information need than the original query. Recall that the hybrid procedure uses the centroid of the top-k as an estimate of the user X  X  information need, and that the user X  X  true information need is in fact the mean of all relevant documents in the corpus. Therefore, our experiments need to verify that the centroid of the selected top-k documents is indeed a better estimate of the user X  X  information need than the original query.

To measure the quality of an estimate of the user X  X  information need, we need to refresh our memory about the factors affecting that estimate. Let us revisit the example in Section 4. Suppose that the original query, q 0 , is: After a top-k sample is collected, we revise q 0 (for both the hybrid procedure and relevance feedback) and get: Now, let the mean of all relevant documents in the corpus be: from the top-k documents. For all terms in q 0 , the weights in q are actually closer to those in  X  .B y contrast, &lt; Lebanon:0.2 &gt; represents a noise term that drifts the query away from its initial intention focusing only on civilian death in Africa. As apparent in this example (and our discussion in Section 4.2), the ability of q to estimate the user X  X  information need is affected by two competing forces: (1) the benefit of re-weighting terms in, and adding relevant terms to those in, q 0 ; and, (2) the query drift introduced by adding noise terms to those in q 0 .

To assess the effects of these two forces, we use the following measures: 1. cos( q ,  X  ): cosine between a revised query, q , and the user X  X  true information need,  X  . 62 XU AND BENAROCH 3. noise r atio : where the length of a vector is defined as the square root of the sum of
When reporting the result, first, rather than report these measures for each query, we report only the averages over all queries. Second, when calculating estimate of the user X  X  information need as well as the centroid of all relevant documents in the corpus (  X  ), rather than keeping all the terms, we keep only the top 300 terms with the largest weights (Buckley et al. 1994, Buckley 1995). This is because the  X  X eal X  centroid could have millions of terms of which the majority are too article-specific and have little relevance to the current topic. T able 6 reports the results for both manually and automatically selected top-k documents. HYBRID AUTOMATIC QUERY EXPANSION 63 produces better estimates than the original query and its counterpart relevance feedback, and the  X  X oiseRatio X  column shows that the hybrid procedure in general introduces less noise, with the exception of ManTop1Hybrid, which is slightly inferior to ManTop1RF. With automatic top-k selection, the picture is somewhat different. There is still improvement as measured by cos( q ,  X  ), though not statistically significant one. However, more noise is introduced by the hybrid procedure than relevance feedback, and this offsets the benefit gained through adding and re-weighting relevant terms. In Section 4.2.3, we also expected that query revision will make the terms in the original query less biased, despite the fact an unbiased estimate is not available. Apparently, Table 6 confirms such prediction.
As predicted by the analysis in Section 4.2, the hybrid procedure outperforms the vector space model and relevance feedback by better representing the user X  X  information need. From the tests we can conclude:  X 
Compared to the user X  X  original query, the top-k documents (manually or automatically selected) both offer a better estimate of the user X  X  information need.  X 
Compared to relevance feedback, the hybrid procedure can better approximate the user X  X  information need.  X 
When the top-k documents are selected manually, the improvement in estimate quality for the hybrid procedure does not come at the price query distortion. In fact, it even reduces this risk as measured by the noise ratio.  X 
When the top-k is automatically selected with low precision, the problem of introducing additional noise terms is more severe with the hybrid procedure than with relevance feedback. 6. Conclusion This research proposed a new hybrid IR procedure that combines the strengths of data-fusion and relevance-feedback procedures while avoiding some of their weaknesses. The hybrid procedure involves three main steps. First, several surrogate queries are generated automatically or manually by selecting from a few top-ranking documents retrieved for the initial user query. Then, individual retrieval runs are carried out for these surrogate queries. Finally, the retrieval results from these runs are integrated based on a sum-cosine score.

The hybrid procedure relies on two key postulates. One is that the sum-cosine measure ranking documents by similarity. Another postulate is that the top-k documents (the set of surrogate queries, including q 0 ) are a better representation of the user X  X  information need than q 0 alone. Our investigation of these postulates shows that they are theoretically and empirically justifiable. For the first postulate, we showed that the sum-cosine relevance score is an adequate method for integrating the scores assigned to a document by different ad-hoc result combination methods that have been typically used in data fusion research. In examining the second postulate, we showed that there are two competing forces in query revision. A revised query could be more accurate in weighting terms, but at the same time 64 XU AND BENAROCH it could also introduce noise. The quality of the final estimate depends on the net effect of these two forces. However, in the case of the hybrid procedure, we showed that when the surrogate queries are manually selected, it is generally reasonable to expect that they collectively produce better estimates of the user X  X  information need than the original query alone. Unfortunately, this may not hold in the case of automatically selected top-k .
All this said, we recognize two key limitations of the hybrid procedure that we plan to pursue in our future research. The most notable is the strategy used for automatic top-k selection. The heuristics we currently use for this purpose proved to be weaker than we e xpected. There are surely numerous ways to improve on these heuristics. Two examples include: imposing keyword-based criteria on the top-k selection step, and using clustering techniques to help better separate relevant and nonrelevant documents. Another limitation of the hybrid procedure is that it is more computationally intensive than standard relevance feedback. Finding a more efficient algorithm to implement the hybrid procedure could greatly enhance its appeal in real-world settings.
 References HYBRID AUTOMATIC QUERY EXPANSION 65
