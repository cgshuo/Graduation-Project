
Institute of Mathematical Sciences and Computation, University of S X o Paulo, S X o Carlos, Brazil LIADD  X  INESC Porto, University of Porto, Porto, Portugal 1. Introduction
Many applications produce a continuous and time dependent series of data. In this scenario, the batch learning followed by many Machine Learning (ML) algorithms is not appropriate, since it is unpractical, and sometimes even impossible to store all examples in memory [1]. This new situation of high produc-tion of data has led to a new research area known as Data Stream mini ng [12], where, in opposition to conventional ML, algorithms are designed to learn online, processing examples as they become available and discarding them afterwards [9].

Another important distinction between the data stream learning scenario and the conventional, static, learning scenario is that, in the former, data characteristics can change over time because of the con-tinuous operation of the system generating the data. In the conventional scenario, algorithms usually assume that data do not change with time and, therefore, after a learning model has been induced, it does not need to be modified in the future. On the other hand, an algorithm designed to learn over a Data Stream needs to take into account the non-static nature of data. Thus, models may need to be reinduced if changes occur in data characteristics.
Data change detection is an important issue for any activity in Data Stream mining, being frequently been approached under the names of behavior change detection and concept drift [10,11]. For super-vised learning, detecting changes in the behavior of data is crucial. Once the current data distribution is different from what has been seen in the past, the predictive model, induced from past data, will have its predictive performance decreased. When unsupervised learning is required, algorithms need to be able to introduce new data into the current model as to reflect the most recent behavior of the data stream, while maintaining, in an appropriated degree, previous knowledge.

This article is concerned with behavior change detection on unsupervised scenarios. Behavior change detection in such case is performed in a variety of ways [2,4,16,17]. Among the most interesting ap-proaches are the ones the apply clustering and novelty detection techniques [3,6,15,19]. These algo-rithms are designed in order to continuously adapt clustering structures when variations in the behavior of the stream are observed. Together with a novelty detection mechanism, these techniques usually in-form when a new event is considered to differ from the past observed behavior. However, some of these approaches are based on proximity algorithms, what assumes data is organized in hyper-spherical clus-ters. Others are based on the assumption that a single outlier example in the stream is a novelty event, which, we believe, does not reflect a real behavior change. Moreover, many algorithms differentiate nor-mal from novelty events based on static threshold values, what is not appropriate for streams of data changing over time.

This article proposes a new approach for automatic behavior change detection in unsupervised data streams based on clustering and novelty detection techniques. Our focus is on changes that alters the clustering structure by either changing the number of clusters, the proportion of examples per cluster or the arrival order of examples to the clusters. This approach is designed as to be independent of the num-ber and format of clusters present in data, what motivated the application of a density based clustering algorithm inspired by DenStream [5]. The novelty detection step is based on the use of two different entropy measures. The spatial entropy considers the partition produced by the clustering algorithm and how examples are distributed among clusters. On the other hand, the temporal entropy models temporal relations between examples and, therefore, captures existing dependencies in the arrival of examples through time. Regardless of the entropy measure used, novelties are detected by changes in entropy levels that surpasses a dynamic calculated threshold, making the approach suitable for processing con-tinuously changing streams, where it is unpractical to consider a constant threshold separating normal from novel events. Finally, a real behavior change is considered to be happening when novelties are detected in a sequence and, therefore, outliers are discarded.

The main objective of this work is to compare our density-based approach with a method that uses the same novelty detection mechanism, but with a proximity-based clustering instead. Our intent is to show that a density-based clustering algorithm can better capture clusters of different formats in data, therefore improving change detection. We also investigate the appropriateness of both temporal and entropy measures for streams containing different clustering characteristics.

This article is organized as follows: Section 2 presents a brief review of the literature on behavior change for unsupervised scenarios; Section 3 describes the main concepts of the DenStream clustering algorithm; M-DBScan, the proposed method for unsupervised behavior change detection, is explained in Section 4; experimental setup and analysis of the results obtained in a set of datasets are presented in Sections 5 and 6, respectively; finally, Section 7 discusses the main conclusions of this work and suggests future research directions.
 2. Change detection in data streams: Related work
Detecting when the data in a stream is changing and sub sequentially incorporating these changes in the learned model is not a trivial task [1]. Changes in an unsupervised stream can be approached in a variety of ways.

The first approach is to process the raw data in order to build visualization tools that help a human expert analyze data evolution. Works under this approach usually apply visualization techniques in a user-defined time horizon, making it possible for an expert to analyze the evolution of data and find particular trends. Since the objective is to provide the user with a visual tool for following data evolution through time, this approach is not suitable for automatically detecting and reporting behavior changes. The work of Aggarwal [2] is one example of this approach. It presents a framework for the diagnosis of multidimensional data streams that uses a concept called velocity density estimation. Based on this concept, two types of profiles, a spacial velocity profile and a temporal velocity profile, are generated. These profiles can be then visualized and analyzed in a user-defined time horizon.

The second approach makes use of time windows in order to detect data changes. This approaches usually monitor data distribution inside a window of most recent points, or compare successive win-dows. Some examples are the Adaptive WINdowing algorithm (ADWIN) [4] and the Fixed Cumulative Windows Model algorithm (FCWM) [17]. The window approach works fine for big streams of data. However, there are applications where it is not possible to afford the delay in change detection intro-duced by time windows. An interesting alternative is the Page Hinkley Test (PHT) [16]. PHT can be applied incrementally over the stream and was designed to detect changes in the average of Gaussian signals. The drawback of PHT is that it uses a constant threshold to report on the occurrence of changes.
The third approach is related to the the task of clustering the unsupervised data, where the algorithm is designed so as to naturally accommodate changes in the clustering structure being built. In this case, new clusters can be created in order to represent the new data arriving in the stream or pre-existing clus-ters can have their features adjusted incrementally. In this situation, data changes are being introduced into the model, but there is no report on when the behavior of data is presenting such changes. Some examples of clustering algorithms that incrementally update their decision models based on new data are CluStream [1], DenStream [5], Self-Organizing Maps (SOM) [14] and Recurrent Self-Organizing Maps (RSOM) [20].

CluStream [1], for example, is an algorithm divided in two phases. The first phase uses the concept of micro-clusters in order to summarize the data arriving at the stream. The second phase applies the K-means algorithm [13] to the center of the micro-clusters, producing a high level clustering structure. DenStream [5] is another algorithm that is divided in two phases. Since this algorithm is the base for our work, it will be described in more details in Section 3. The SOM [14] algorithm, on the other hand, uses the neuron as the basic learning unity. SOM incrementally updates its neurons based on the examples being processed. This way, the network incrementally changes its topology characteristics in order to represent regions of interest in the domain of the problem being learned.

The fourth way to approach changes in this scenario includes clustering approaches that are linked to a novelty detection mechanism. In this case, the objective is not only updating the clustering structure with the most up to date data, but also reporting on the occurrence of events that cause a variation on the current structure. The novelty detector checks how much the clustering structure has been changed with the arrival of a new example, for example, the creation of a new cluster to accommodate an incoming example. A novelty is declared when the level of change is higher than a pre-specified threshold. One interesting aspect of these methods is that they are incremental change detectors, i.e, consider only the current example in order to make a decision. Some algorithms that are based on this general idea are Growing k-means [6], OLINDDA [19], Sequential Leader Clustering (SLC) [7], Grow When Required Network (GWR) [15] and Self-Organizing Nove lty Detection Neural N etwork (SONDE) [3].

The GWR [15] algorithm, for example, adds neurons to a neural network when the network does not match the data distribution anymore. The algorithm uses the information about how many times a neuron has been activated in order to detect novelties. If a neuron that has not been activated before, i.e., a newly created neuron, or a neuron that has not been activated for some period of time is selected for activation, a novelty event is considered to be occurring. SONDE [3], an algorithm that uses a neural network for data clustering, detects novelties based on estimations of Markov Chains (MCs) and measurements of entropy variation between consecutive MCs. At every new example arriving in the stream, the algorithm uses the information about the neuron activated by this example, together with information about the neuron selected for activation in the last time instant, to update a transition matrix between clusters. This matrix is then used to estimate the transition probabilities of a MC represented by the clusters. Shannon X  X  entropy [18] is used to calculate an entropy level of the MC at every time instant. The current entropy level is compared to the previous level and, if the difference is larger than a pre-specified threshold, a novelty event is declared.

The majority of algorithms that follows the forth approach present one of three characteristics: 1. Implement proximity-based clustering algorithms. This assumption reduces the application of these 2. Consider that a single outlier example in the stream is a novelty event, which, we believe, does not 3. Assume a constant threshold in order to detect novelties, which, in many cases, is hard to establish
We believe that in order to design an unsupervised algorithm to automatically detect and report data changes, it is necessary to consider both an arbitrary clustering structure of data and the definition of change as a sequence of consecutive novelty events. Since clusters in a stream can have arbitrary shapes, a density-based clustering procedure can better capture the correct shapes of clusters, thus improving novelty detection. Change detection, on the other hand, cannot be described by a single novelty event, because streams of data may contain noise or outliers.

Moreover, the threshold separating normal from novel events should be dynamically adjusted. Estab-lishing the threshold apriori is difficult, with wrong choices largely affecting the quality of the novelty detection. Also, once the stream X  X  characteristics may change over time, so should the threshold separat-ing normal and novel events. As far as our knowledge goes, ours is the first algorithm two present these characteristics simultaneously. Our method also approachs change detection from both the perspective of spatial distribution (spatial entropy measure) and temporal relationships among data (temporal en-tropy measure). This makes it possible to compare both types of measures and derive conclusions about the suitability of each measure to different scenarios. Thus, in the next sections we will present the basic concepts in which our approach is based, as well as a detailed explanation on how the algorithm detects changes on unsupervised data.
 3. DenStream clustering
DenStream [5] is a density-based clustering algorithm designed to discover clusters in a data stream scenario. It has an online phase where data is summarized and an offline phase of macro-clustering. The algorithm implements a strategy to find arbitrarily shaped clusters in a data stream without the need to specify the number of clusters apriori . It also includes a mechanism to distinguish normal examples from outliers.

In its online phase, the algorithm summarizes data using the concepts of potencial-micro-clusters (p-micro-clusters) and outlier-micro-clusters (o-micro-clusters). Micro-clusters are, essentially, hyper-spherical clusters defined by a maximum radius value and a corresponding weight that represents how many data points fall into this micro-cluster. O-micro-clusters are micro-clusters that contain outlier examples and are kept in a separate  X  X utlier buffer X . The main difference between p-micro-clusters and o-micro-clusters is a restriction on the weight w .
 cluster ( CF 1 ) , the weighted squared sum of points ( CF 2 ) and a parameter value w that represents the weight of this micro-cluster. Micro-clusters are incrementally updated. Thus, if a new point p is merged into micro-cluster c , the information in c is updated by c =( CF 1 + p, CF 2 + p 2 ,w +1) .Otherwise,  X   X  [0 , 1] . The algorithm also uses two input parameters,  X  and  X  ,where  X  is the minimum weight of any micro-cluster and  X   X  [0 , 1] is the threshold of outlier relative to p-micro-clusters.
Everytime a new point p is available in the stream, the algorithm proceeds as follows: 1. Trytomerge p into its nearest p-micro-cluster c p .If r p , the new radius of c p is less or equal to , 2. Otherwise, try to merge p into its nearest o-micro-cluster c o .If r o , the new radius of c o is less or 3. Otherwise, create a new o-micro-cluster in the outlier buffer to accommodate p .

Since DenStream applies an exponential forgetfulness to its micro-clusters, the algorithm periodically checks the p-micro-clusters to make sure that all of them still hold to the assumption that w&gt; X  X  .If the weight of a p-micro-cluster is smaller than  X  X  , the corresponding p-micro-cluster is removed. The o-micro-clusters are also checked to remove those that do not have potential to grown into a p-micro-cluster. The weight of o-micro-clusters is compared with a lower weight limit,  X  . If the weight is smaller than  X  , the o-micro-cluster is removed.

It is important to notice that the micro-clustering phase of DenStream is a summarization phase, where points with similar characteristics are included in the same micro-cluster. How many micro-clusters will be formed and, also, how many points will fall into a particular micro-cluster, is dependent on the data characteristics and how the values of parameters ,  X  and  X  are defined.

In the offline phase (or macro-clustering phase), everytime a clustering request arrives, DenStream applies a modified version of the DBSCAN [8] algorithm, using the centers of the p-micro-clusters as virtual points. Since DBScan works only with the summarized information and not with all data points, it is much more applicable to an online scenario. The dense regions formed by the p-micro-clusters are analyzed by DBScan, creating a new set of clusters that can present arbitrary shapes. More details on the dynamics of DenStream, as well as results showing its applicability to discovering clusters of arbitrary shapes in data streams, can be found at the original paper [5]. 4. Density-based behavior change detection This section describes the approach proposed in this article for automatic behavior change detection. Like other methods previously proposed for detecting changes in unsupervised scenarios, the proposed method is based on the application of a clustering algorithm to the incoming data, followed by a novelty detection step to detect behavior changes. The proposed method assumes that: 1. The number of clusters in the stream is unknown: since in a data stream examples arrive continu-2. The format of clusters is arbitrary: it is not possible in many data stream applications to assume 3. Outliers can occur and do not reflect a real change: examples that clearly differ from the others The clustering procedure used in the proposed approach is an evolving method based on the Den-Stream [5] algorithm. As mentioned before, DenStream is able to find arbitrarily shaped clusters and does not require the number of clusters to be defined before hand. Besides, DenStream implements an efficient mechanism to eliminate outliers, preventing these examples from misleading the clustering structure. Some modifications to the original DenStream formulation were introduced in order to avoid error propagations from the clustering phase to the novelty detection phase. Novelty detection is per-formed by calculating an entropy level every time a new example is inserted in the clustering structure. Two different entropy measures are used, with different characteristics. Regardless of the entropy mea-sure used, novelties are detected based on the history of entropy values observed during the processing of the stream, and, thus, the process is less dependent on pre-specified thresholds than other similar strategies. The approach proposed in this work will be referred as M-DBScan (from Micro-C lustering DBScan). 4.1. Clustering of incoming data
As M-DBScan is based on DenStream, the concepts of p-micro-clusters and o-micro-clusters are used and they store the same information as the original DenStream, i.e., CF 1 , CF 2 and w . The micro-cluster structures are maintained incrementally. When a new point p arrives at the stream, if an existing p-micro-For all remaining p-micro-clusters, the values of CF 1 , CF 2 and w are not updated. Diferently from DenStream, no exponential forgetfulness is applied to the set of p-micro-clusters.

On the other hand, if no p-micro-cluster is selected to receive p , the outlier buffer is checked to see if there is any o-micro-cluster able to receive the example. If so, the values of CF 1 , CF 2 and w are updated as for the p-micro-clusters. If no o-micro-cluster is chosen, a new o-micro-cluster is created to receive p , with w = 1 as initial weight. All remainin g o-micro-clusters suffer exponential forgetfulness on their stored information, i.e, for any given o-micro-cluster c o , if no point is merged into c o in the time interval  X t
The algorithm uses the input parameters  X  ,  X  and ,where  X  is the minimum weight of a micro-cluster,  X   X  [0 , 1] is the threshold of outlier relative to p-micro-clusters and, finally, is the maximum boundary on the radius of a micro-cluster. The algorithm is detailed next.

The online maintenance of micro-clusters with the arrival of a new example p is as follows: 1. Trytomerge p into its nearest p-micro-cluster c p .If r p , the new radius of c p , is less than or equal 2. Otherwise, try to merge p into its nearest o-micro-cluster c o .If r o , the new radius of c o ,islessthan 3. Otherwise, create a new o-micro-cluster in the outlier buffer to accommodate p . Equations (1) and (2) are used in the original DenStream algorithm. Also like DenStream, the algorithm proceeds applying the DBScan algorithm [8] on top of the micro-clustering structure, using the p-micro-clusters as virtual points. The o-micro-clusters are not considered by DBScan. The macro-clustering procedure is called for every new example, differently from the original definition of DenStream where the macro-clustering is performed on an interval basis, where the interval is defined by the user.
At the initialization phase of the clustering algorithm, there are neither p-micro-clusters nor o-micro-clusters created. The first examples in the stream will form o-micro-clusters in the outlier buffer. As more examples arrive in the o-micro-clusters, those o-micro-clusters whose weight is larger than  X  X  are incrementally removed from the outlier buffer and become a p-micro-cluster.

By using this clustering procedure, no assumption needs to be done apriori on the number and formats of clusters present in data. Besides, the outlier buffer represents a filter that discards the majority of outliers in the stream. Other remaining outliers are taken care by the novelty detection procedure. 4.2. Novelty detection based on entropy levels
Entropy is a measure of the uncertainty associated with a random variable. The higher the value of the entropy measure, the higher the uncertainty about the outcome of the random variable, while small values of entropy stand for high degrees of certainty [18]. As an example, consider Fig. 1, where H ( X ) stands for the entropy associated with a coin flip and P ( X =1) is the probability of the random variable X assuming value one. In this example, X is the outcome of a coin flip and can, therefore, assume two possible values. A value of zero means a tails outcome, while a value of one represents a heads outcome. If the coin is fare, the probability of the outcome being heads is 0.5, leading to the highest value of uncertainty about the outcome of a coin flip. On the opposite side, if the coin is unfair and, for example, only outcomes heads, the probability P ( X =1) is equal to one and the entropy value is zero, meaning total certainty about the outcome of the coin flip.

This work proposes the use of two different entropy measures, one with a temporal bias and another with a more spatial bias. The first is based on the e ntropy measure used by t he SONDE algorithm [3]. The second is based on the purity of different sets, like, for example, the purity of leaf nodes in a decision tree. Both measures consider the clusters found in data at the current time instant t , which are created by the application of the DBScan algorithm to the set of p-micro-clusters. 4.2.1. Temporal entropy measure
This measure estimates a Markov Chain (MC) from the clusters found in data at time instant t . Each cluster is considered a state in the MC. The transition probabilities between states are updated according to the cluster selected to receive the example at the previous time instant and the cluster selected to receive the current example.

Let M be a MC with N states and p i,j be the transition probability between states i and j .State i represents the cluster selected to receive the example at time instant t  X  1 ,whilestate j represents the cluster selected at the current time instant t . In this work, the transition probabilities are estimated by an exponentially-weighted moving average, with a weighting factor  X  t  X  [0 , 1] . The weighting factor has a control influence on the transition probabilities of the chain. The higher the value of  X  t ,themore the transition probabilities are affected by cu rrent updates. On the other hand, if a small value of  X  t is used, the probabilities are changed more slowly, giving more weight to the past history of the chain. The transition corre sponding to states i and j in the chain are updated as in Eq. (3). All other transitions { a, b } = { i, j } at time instant t are updated as in Eq. (4). The entropy level of a chain is calculated using its updated transition probabilities. This entropy level is based on Shannon X  X  entropy [18] (Eq. (5)).
 As an example, consider Fig. 2 where a MC is represented at instants t and t +1 . In this example,  X  t is equal to 0 . 05 . Each circle, or state, in Fig. 2 represents a cluster. Suppose that the example that arrived through the stream at time instant t was assigned to the cluster at the right. The example at time instant t +1 is assigned to the cluster at the left, thus a new transition needs to be created starting at the rightmost cluster and ending at the leftmost cluster. All other existing transitions are updated in order to reflect the change. The entropy of the chain is modified as a consequence of the updated transition probabilities.
Since the entropy level is calculated using the transition probabilities of a MC, it has a temporal bias, i.e., this measure is sensitive to changes in the order of occurrence of events. Therefore, the use of this measure allows for the detection of temporal relations between consecutive examples in the stream. 4.2.2. Spatial entropy measure
The second entropy measure considers the spatial distribution of data into clusters and estimates the number of examples belonging to each cluster during the stream X  X  processing. The number of examples counting on the number of examples per cluster can be used. However, when the number of examples per cluster increase significantly, the addition of a new cluster does not produce a corresponding modification in the entropy measure. To solve this problem, an estimate on the number of examples per cluster using an exponentially weighted moving average is used. The cluster i that receives the example at time instant t has its number of examples updated as in Eq. (6), where  X  s  X  (0 , 1) is a weighting parameter for the moving average.
 All other remaining clusters have their values updated as in Eq. (7), where j = i represents the cluster index.
 Afterwards, the entropy is calculated as in Eq. (8) where G is the set of clusters at the current time instant and N is the number of clusters.
Figure 3 presents a simple example where the estimated number of examples per cluster at time instant t differs from the number at time instant t +1 , consequently modifying the entropy value. In this example a value of  X  s = 0.05 is considered.

Different from the temporal entropy measure, the spatial entropy models changes in the concentration of examples per clusters. The spatial entropy is, therefore, not concerned with temporal dependencies that might be presented in data. 4.2.3. Using entropy levels to detect novelties
Whether the temporal or spatial measure is being considered, entropy levels are used in order to decide if the arrival of an example produces a significant variation in the clustering structure.
The entropy calculated at time instant t , expressed as H t (  X  ) , is compared to a dynamically calculated threshold  X  . To define the value of  X  , a moving average on historical entropy values  X  Eq. (9), and a moving standard deviation  X  Eq. (10), are calculated, where  X  and  X   X  [0 , 1] are weighting factors. To estimated  X  , we will assume a Normal distribution on entropy values. Therefore, we will include a constant parameter  X  that represents the number of standard deviations from the average to consider in order to define the threshold. Equation 11 presents the final calculation. If
H t (  X  ) &gt; X  , then the entropy level H t (  X  ) at time instant t is considered to diverge from the average entropy level observed, indicating the occurrence of a novel event.

This procedure for detecting novelties automatically adjusts the threshold separating normal from novel events during the processing of the stream. As a consequence, it is more appropriate for detecting novelties in changing streams than other approaches that assume a constant threshold defined by the user.
 5. Experiments
This section presents the experiments carried out to access the performance of M-DBScan for change detection. For such, we evaluated our density-based approach for a set of data sets using both the tem-poral and spatial entropy measures. We compared the results obtained for each entropy measure with the results of a proximity-based method for clustering, using the same entropy measures for novelty detection. In the experiments, our focus was on detecting three different types of changes. The first two are changes that modify the clustering structure (structural change) by either changing the number of clusters or changing the format and proportion of examples per cluster. The last is a change in the arrival order of examples to belonging clusters (temporal change). This type of change is important for data streams with a time series behavior. Our hypothesis is that M-DBScan will present higher performance in the task of automatically detecting different types of changes when compared with the proximity based approach. The results of these experiments will also provide useful insights on the effectiveness of each entropy measure to detect different types of changes occurring in data. 5.1. Datasets used
Eight synthetical datasets were constructed to test the performance of the proposed approach. Each of these datasets has two numerical attributes and represents a data stream. Also, each dataset contains three behavior changes, occurring at the 1000 th , 2000 th and 3000 th examples. These datasets represent several possible alternatives in which data can be organized into clusters, as well as how these clusters evolve over time. We believe that these data are appropriate for evaluating the performance of a change detection algorithm because it is possible to know exactly how data is behaving. These datasets are also appropriate for evaluating how the two different entropy measures behave in different situations, for example, when clusters appear and disappear over time.

Further, one dataset was created using data from t he 10% partition from the KDD99 dataset on intru-sion detection. This dataset contains 3 behavior changes, occurring at the 1000 th and 2000 th examples. Next, we present the main characteristics of each dataset. 1. FixedDenSpatial: presents four arbitrarily shaped clusters from the beginning to the end of the 2. FixedDenTemporal: presents four arbitrarily shaped clusters from the beginning to the end of the 3. FixedSphericalSpatial: presents four hyper-spherical clusters from the beginning to the end of the 4. FixedSphericalTemporal: presents four hyper-spherical clusters from the beginning to the end of 5. VarDenAddCluster: presents a variable number of arbitrarily shaped clusters. Changes are of the 6. VarDenMergeCluster: presents a variable number of arbitrarily shaped clusters. Changes are of the 7. VarSphericalAddCluster: presents a variable number of hyper-spherical clusters. Changes are of 8. VarSphericalAddSubCluster: presents a variable number of hyper-spherical clusters. Changes are 9. StreamKDD99: this dataset was constructed using data from 3 di fferent attacks from the KDD99
Table 1 summarizes the characteristics of each synthetical dataset. For the StreamKDD99 dataset, some columns are marked as unknown. Since this dataset was constructed from data used primarily for classification tasks, it is hard to define correctly the number of clusters present in the data used in our experiments, as well as the format of these clusters. In what regards the type of change, we can safely assume that all the 3 changes can be categorized as both temporal and structural, because data from each attack is well differentiated from other attacks. Therefore, data from each type of attack will form a different set of clusters. When changing from one attack type to the other, changes will both alter the arrival order of examples to clusters and the proportion of examples per cluster. 5.2. Experimental setup
M-DBScan uses a clustering step that is based on the DenStream algorithm. Therefore, on the macro-clustering phase, a density-based clustering algorithm is used. This algorithm is a variation of the DB-Scan algorithm that uses the centers of the p-micro-clusters, built on the previous micro-clustering phase, as virtual points that will be divided into clusters. Our hypothesis is that using a density-based cluster-ing algorithm for the final clustering will produce better results when the data present clusters of arbi-trary shapes. In order to evaluate this assumption, we implemented another clustering strategy that is proximity-based, i.e., builds spherical format clusters.

The proximity-based strategy used works as follows. Instead of using the DBScan variation on top of the p-micro-clusters, we applied the K-means [13] algorithm. The examples submitted to K-means were virtual points represented by the centers of the p-micro-clusters. Since each p-micro-cluster c p has a center and a corresponding weight w c p , we created N c p copies of the example that represents c p .The constant.
 In the experiments that follow, we set up the constant c with the same value as k ,where k is the number of clusters for K-means. All examples in the set of virtual points were normalized in the interval [0 , 1] before being turned in to K-means. Regarding K-means centroid initialization, at the first execution of the algorithm we randomly selected k points in the interval [0 , 1] . We then stored these initial values in order to initialize the centroids always at the same positions every time K-means is executed.
It is important to notice that this implementation allows us to make a fair comparison between density-based and proximity-based approaches, since both of them are based on the same micro-clustering pro-cess. Therefore, differences are a consequence of the macro-clustering step only. Moreover, K-means is a well known clustering algorithm, being frequently used as reference. For the sake of simplicity this approach will be called, from this point forward, as M-Kmeans (from Micro-Clustering Kmeans).
Our first set of experiments compares M-DBScan with M-Kmeans, using the temporal entropy as a measure for novelty detection, in the nine datasets presented. The second experiment compares both approaches using the spatial entropy measure. The parameters used by M-DBScan were  X  = 10,  X  = 0.105,  X  = 0.03. This values configure the algorithm to promote o-micro-clusters to p-micro-clusters when the o-micro-cluster contains more than one example. It also establishes that, at every 102 exam-ples, the algorithm will check for the possible removal of o-micro-clusters from the outlier buffer. The value of parameter , which defines a maximum boundary for the micro-clusters, varies with each dataset used and is expressed in Table 2. These values were established empirically for each dataset. For detailed explanation on the meaning of each parameter, see Section 4.1. It is interesting to point out that addi-tional information on the particular data domain can facilitate parameter value initialization, specially the parameter . If data from the same domain is available apriori , can be better adjusted using these data, increasing the overall performance of the clustering algorithm.

Table 2 also presents the values used for k (the number of cluster to M-Kmeans), since different values were used for different datasets. As a rule of thumb, we selected k as the initial number of clusters present in each dataset, except for StreamKDD99 where k was set to 4.

For the experiments using the temporal entropy measure we used  X  t = 0.005,  X  = 0.05,  X  = 0.002 and  X  = 3. The experiments using the spatial entropy measure applied  X  s = 0.005,  X  = 0.05,  X  = 0.02 and  X  = 3. These values were chosen in order to apply a more conservative approach to novelty detection, where the past history of events have a stronger influence on the definition of the threshold separating normal from novel events. In all experiments, examples were considered to arrive at the stream at every 1 second.

A behavior change is considered detected when more than one novelty occur in a sequence. These con-secutive novelties are considered to represent a single change. Single novelties events are not considered as behavior changes.

In order to compare the results, three measures were used. The first one was the number of times a change was detected where it was actually not happening (false positive  X  FP). The second one was the number of times a change was detected, but with a significant delay (delayed detection  X  DD). A change detection is considered to be delayed (DD), if the delay is greater than 100 examples but lesser that 300 examples. Finally, the last measure was the number of times the change was not detected (non-detected change  X  ND). This measure includes the changes that were not detected at all and the changes that were detected with a delay greater than 300 examples. The lower the values in each one of these measures, the better the results.

Our hypothesis is that M-DBScan will produce better change detection results than M-Kmeans in the majority of the datasets used, specially on those containing clusters of arbitrary shapes and the ones with a changing number of clusters. 6. Results
The first experiment compares M-DBScan with M-Kmeans using the temporal entropy measure. Ta-ble 3 summarizes the results of this experiment. The results show that M-Kmeans was not able to detect 4 out of 27 existing behavior changes, while M-DBScan did not miss any change. It was observed that M-DBScan was capable of finding a much more stable and close to optimum clustering of the points. This happened because M-DBScan can find an arbitrary number of clusters, while M-Kmeans produced a fixed number of clusters during the entire stream processing. This is particularly important when the number of clusters in the stream is changing, what is reflected in the ND results for M-Kmeans, where all the ND happened in datasets with a varying number of clusters.

Building a more stable clustering structure during the stream X  X  processing also means building a more stable MC through time, which is directly linked to the FP occurrence. If the clustering structure is poor, there will be a high probability of occurrence of misleading transitions in the MC. These transitions will be reflected as false temporal novelties even for the simplest datasets, as, for example, FixedSpher-icalTemporal dataset. As can be seen in the results in Table 3, M-DBScan produced much less FP than M-Kmeans.

For the StreamKDD99 dataset both methods were able to detect the 2 existing behavior changes. This is due to the fact that data representing a particular network attack is reasonably well defined, presenting small variation. Moreover, different attacks produce data that significantly differ from data produced by other types of attacks. Regarding FP, M-DBScan detected the occurrence of one FP in the middle of the data representing the first attack (Satan). Taking a closer look to the results, we observed that this FP was actually a variation of the same attack, with different characteristics of what has been observed before in the stream. So the detection of this event would be actually desired, since it is a different behavior of the same type of intrusion.
Figures 4 and 5 present the temporal entropy variation with corresponding standard deviation and occurrence of novelties for dataset VarDenAddCluster. Behavior change detection is represented by peaks in the entropy curve that cause a sequence of novelties to occur.

Table 4 presents the delay for detection of each one of the three behavior changes. The delay for de-tecting a behavior change is important because, the sooner the change is detected, the sooner appropriate measures to deal with the results of this change can take place. As well as being able of detecting more behavior changes, M-DBScan also presented similar delays to M-Kmeans.

The next experiment compares M-DBScan with M-Kmeans using another entropy measure for novelty detection. This time, the spatial entropy measure was used. Table 5 presents the results obtained by both methods on the datasets used.

Results on Table 5 suggest that both methods were not able to detect a high number of behavior changes (ND). However, these results bring important insights on how the spatial entropy measure works. First of all, the results using both methods indicate that the spatial entropy measure was not appropriate when the change in data is of the temporal type (datasets FixedDenTemporal and Fixed-SphericalTemporal). This was expected, since the spatial measure does not measure the order of arrival of examples to clusters. It was also observed that the spatial entropy was very sensitive to the creation of new clusters, producing a high variation in entropy.

Results on the datasets with an increasing number of clusters suggest that M-Kmeans performance was worse than M-DBScan. While M-DBScan was able to accurately detect the majority of changes in these datasets, M-Kmeans or did not detect changes or detected them with a delay, as can be observed in Table 6. M-Kmeans also presented a high number of FP, while M-DBScan did not produce any FPs. Figures 6 and 7 present the spatial entropy variation with corresponding standard deviation and occurrence of novelties for dataset VarDenAddCluster.
For the StreamKDD99 dataset, again both methods performed similarly. The same conclusion draw from the temporal entropy applies to the spatial entropy.

Results in both the first and second experiments indicate that M-DBScan is more suitable to the task of detecting behavior changes than M-Kmeans. In order to better understand the influence of the two entropy measures used, we compared the results of M-DBScan gathered in the first and second exper-iments, i.e., using, respectively, the temporal and spatial entropy measures. Table 7 summarizes these results and simplifies comparisons between entropies.

As can be seen in Table 7, ND was reasonably higher in the spatial entropy. The majority of ND events happened in datasets containing temporal changes and changes in the format of clusters. As for the temporal changes, this was actually expected since the spatial entropy does not measure changes in the arrival order of examples. For the datasets containing changes in the format of clusters, we observed that ND occurrence was more related to the micro-clustering step than the novelty detection step. Since the micro-clustering step depends on input parameter to proceed, misclassifications of examples to wrong micro-clusters can occur if the value of is not appropriate. This happens because defines a maximum boundary for the radius of the micro-clusters (see Section 4.1). Errors in the micro-clustering phase propagate to the macro-clustering phase, resulting in a wrong clustering of the data as a whole. This errors can cause the spatial entropy not to detect variations in the proportion of examples per clusters, missing the true changes. The spatial entropy, however, was capable of detecting the majority of changes in the number of clusters occurring in the datasets.

As for the temporal entropy, this measure was more appropriate to detect the two types of changes where the spatial entropy fails. This is because these changes cause a modification in the transition prob-abilities of the Markov Chain (MC) that are almost instantaneous, even in some cases where the clus-tering structure is not optimal. Modifications in the MC are reflected in the temporal entropy measure, producing significant differences and consequently leading to novelty detection. The temporal entropy measure, however, presented a drawback in relation to the spatial entropy. Because the temporal entropy is less conservative in declaring novelties, the occurrence of FP in the temporal entropy was higher than in the spatial entropy. Since the spatial entropy requires more examples to produce a significant variation in entropy, it was not as affected by false positives. 7. Conclusions
In data stream mining, being capable of detecting and informing when the behavior of the stream is changing is an important consideration of many learning algorithms. In unsupervised learning, behavior change detection is performed mostly by the conjunction use of clustering algorithms and novelty de-tection strategies. However, some of these approaches are only capable of producing hypher-spherical clusters [3,15,19], while others consider a single event as a novelty [3,15]. Moreover, many of these approaches use a constant threshold separating normal from novel events [7,15]. None of them is ulti-mately appropriate to detecting real behavior changes in arbitrary streams of data because: i) data can organize itself in clusters of arbitrary formats; ii) a true behavior change is characterized by a sequence of novel events; iii) the threshold separating normal from novel events should be dynamically adjusted during the stream X  X  processing.

With this in mind, this article proposed a new method for detecting and informing when a behavior change is happening in the stream of data. This method uses a density-based clustering algorithm and a novelty detection mechanism based on entropy measures. Two different types of entropy measures were presented and explored in this work.

Results of experiments comparing our approach with a proximity-based method suggested the density-based approach proposed in this work was more appropriate for detecting different types of behavior changes in several datasets. Useful insight was also gathered about the two entropy measures used. The temporal entropy measure proved to be more appropriate for detecting the three types of changes presented in the datasets, while the spatial entropy is more effective for detecting changes in the number of clusters. The temporal entropy, however, is more sensitive to false positives than the spatial entropy when the number of examples in the stream is high. It was also observed that the delay for change detection was shorter for the temporal entropy than for the spatial entropy because changes produce modifications in the Markov Chain that are almost instantaneous.

Future work will focus on the comparison of M-DBScan with other changing detection mechanisms proposed in the literature. We also intend to further investigate the entropy measures used for novelty detection. Initially, we will investigate how to make the temporal entropy measure less sensitive to false positives. For the spatial entropy measure, we intend to explore alternatives to make the measure more appropriate to detect cluster shape changes, without modification of the proportion of examples per cluster.

The M-DBScan algorithm, as it is proposed in this work, is more appropriate for streams where the velocity of examples arriving is no less than one example per second. This occurs because the current approach applies the DBScan algorithm to every new data example. However, this is not suitable for very fast data streams because of the overhead introduced by the application of the density-based clustering algorithm. An improved version of M-DBScan able to deal with this problem is being investigated.
Finally, a complete system for online behavior change detection will be developed, with the possibility of using both entropies for making decisions on novelty events. We believe that such a behavior changing detection technique can be useful to many applications ranging from sensor monitoring in industries, to network traffic monitoring, user profile creation, health care applications, and many others. Acknowledgments
Rosane M. M. Vallim is funded by FAPESP, pro cess number 2010/11250-0. Jo X o Gama is funded by the Portuguese Foundation for Science and Technology, project KDUS ref. PTDC/EIA-EIA/098355/ 2008. The authors would also like to thank the brazilian research councils CNPq and Capes. References
