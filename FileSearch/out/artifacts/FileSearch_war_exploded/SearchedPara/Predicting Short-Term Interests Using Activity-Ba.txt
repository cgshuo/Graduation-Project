 A query considered in isolation offers limited info rmation about a searcher X  X  intent. Query context that considers pre -query activity (e.g., previous queries and page visits), can provi de richer infor-mation about search intentions. In this paper, we d escribe a study in which we developed and evaluated user interest m odels for the current query, its context (from pre-query session activity), and their combination, which we refer to as intent . Using large-scale logs, we evaluate how accurately each model predict s the user X  X  short-term interests under various experimental con ditions. In our study we: (i) determine the extent of opportunity f or using context to model intent; (ii) compare the utility of differ ent sources of behavioral evidence (queries, search result clicks, and Web page visits) for building predictive interest models, an d; (iii) investigate optimally combining the query and its context by le arning a model that predicts the context weight for each query. Ou r findings demonstrate significant opportunity in leveraging c ontextual in-formation, show that context and source influence p redictive ac-curacy, and show that we can learn a near-optimal c ombination of the query and context for each query. The findings can inform the design of search systems that leverage contextual i nformation to better understand, model, and serve searchers X  info rmation needs. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process, information filtering. Algorithms, Experimentation, Human Factors, Measure ment. Search context, short-term interests, interest mode ls. Search behavior resides within an external context that motivates the problem situation and influences interaction be havior for the duration of the search session and beyond [14]. Sat isfying search-ers X  information needs involves a thorough understa nding of their interests expressed explicitly through search queri es, or implicitly through search engine result page (SERP) clicks or post-SERP browsing behavior. The information retrieval (IR) c ommunity has theorized about context [14], developed context-sen sitive search models (e.g., [24][26]), and performed user studies investigating the role of context in the search process (e.g., [1 8]). Most IR systems assume that queries are context-ind ependent. This abstraction is necessary in Cranfield-style ev aluations where relevance judgments are gathered independent of any user or in-teraction context [31]. In larger operational syste ms such as Web search engines, scale constraints have often favore d simple con-text-independent approaches. Recent research sugges ts that this may be changing as log data and machine learning te chniques are applied to model activity-based context (i.e., context gleaned from prior user interactions) in applications such as qu ery suggestion [7], query classification [8], Web page recommendat ion [30], and Web search result ranking [32]. However, this resea rch is often specific to particular applications, and an assessm ent of the value of modeling activity-based context that is applicab le to a broad range of search and recommendation settings is requ ired. In this paper we describe a systematic study of the value of con-textual information during Web search activity. We construct interest models of the current query, its context c omprising pre-ceding session activity such as previous queries or previous clicks on search results, the combination of the query and its context (called intent ), and evaluate the predictive effectiveness of the se models using future actions. Figure 1 illustrates e ach of the mod-els and their role in representing users X  interests . Queries are de-picted as circles and pages as rectangles. The curr ent query is . 
Figure 1. Modeling search context and short-term fu ture Accurate understanding of current interests and pre diction of fu-ture interests are core tasks for user modeling, wi th a range of possible applications. For example, a query such as [ACL] could be interpreted differently depending on whether the y previous query was [knee injury] vs. [syntactic parsing] vs. [country mu-sic]. This contextual knowledge could be used to re-rank search results, classify the query, or suggest alternative query formula-tions. Similarly, an accurate understanding of curr ent and future interests could be used to dynamically adapt search interfaces to support different tasks. In our study we: (i) deter mine the fraction of search engine queries for which context could be leveraged, (ii) measure the value of different models and sources f or predicting future interests, and (iii) investigate learning th e optimal combina-tion of query and context on a per-query basis, and use the learned models to improve the accuracy of our predictions. We use a log-based methodology as logs contain behavioral eviden ce at scale and cover many classes of information needs. This i s important since performance differences may not hold for all search tasks. The remainder of this paper is structured as follow s. In Section 2 we present related work on implicit profile generat ion from user activity, on representing interests with topical ca tegories, on query analysis, and on the development of predictive inte rest models. Section 3 describes how we define and construct the models de-veloped for this study. We describe the study and t he findings from our analysis in Section 4. We discuss findings and their im-plications in Section 5 and conclude in Section 6. Although most search systems match user queries to documents, independent of the interests and activities of the searcher, there is a growing body of work examining how knowledge of a search-er X  X  interests and search context can be used to im prove various aspects of search (e.g., ranking, query suggestion, query classifi-cation). User interests can be modeled using differ ent sources of profile information (e.g., explicit demographic or interest profiles, or implicit profiles based on previous queries, sea rch result clicks, general browsing activity, or even richer desktop i ndices). Profiles can be based on long-term patterns of interaction, or on short-term session-level patterns. Here we review prior resear ch that exam-ines the use of short-term implicit profiles genera ted using user X  X  searching and browsing actions (queries, clicks on search results, and subsequent navigation to other pages). Further, we focus pri-marily on research that has used topical categories , such as the human-generated Web ontology provided by the Open D irectory Project (ODP, dmoz.org), to model user interests si nce this pro-vides a consistent representation for queries and W eb page visits. Several research groups have investigated personali zing search results using user profiles that consist of ODP-lik e topic catego-ries. In an early personalized Web search system, Outride , Pitkow et al. [21] used the ODP classes associated with br owser favorites and the last 1000 unique pages to represent a user X  s interests. They used this representation to both modify querie s and re-rank search results. Gauch et al. [13] learned a user X  X  profile from their browsing history, Speretta and Gauch [25] built pro files using just the search history, and Chirita et al. [9] and Ma e t al. [19] used profiles that user X  X  specified explicitly. In all c ases, interest pro-files were compared with those of search results an d used to alter the order in which results were presented to indivi duals. Personal profiles have also been used to create personalized versions of the PageRank ranking algorithm for setting query-independent pr iors on Web pages [16]. Such personalized document prior s can be combined with content match features between querie s and Web pages for the improved ranking of Web search result s. More re-cently, Bennett et al. [5] demonstrated how categor y-level features can be used to improve search ranking in the aggreg ate. In that work categories were not used to represent user pro files, but ra-ther to propagate user behavior for a small number of specific URLs that are clicked on for a query to a much larg er set of URLs that belong to the same topical category. Similarly , Xiang et al. [32] developed heuristics to promote URLs with the same topical category if successive queries in a search session were related by general similarity, and were not specializations, g eneralizations, or reformulations of the previous query. Using the context of user activities within a searc h session has also been used to improve query analysis. Short que ries are often ambiguous, so researchers have used previous querie s and clicks in the same session to build a richer models of int erests and im-prove how the search system interprets users X  infor mation needs. Cao et al. [7][8] represented search context by mod eling sessions as sequences of user queries and clicks. They learn ed sequential prediction models such as hidden Markov models and conditional random fields from large-scale log data, and applie d the models to query suggestion, query categorization, and URL rec ommenda-tion. Mihalkova and Mooney [20] used similar search session features to disambiguate the current query. Althoug h neither of these studies uses ODP categories, they are session -based and illustrate non-ranking applications of search sessi on information. Finally, several research groups have developed pre dictive models of user interactions within search sessions. Early Web recommen-dation systems such WebWatcher suggested new Web pages for individuals based on their recently-viewed pages [3 ]. Shen et al. [23] learned probabilistic models to predict the cl ass (from top-level ODP categories)of the next URL a searcher wou ld click on. They compared models for individuals, groups, and a ggregate search behavior patterns using long-term interactio n patterns. The best predictive accuracy was obtained with individu al or group models, but they did not explore richer combination s. Piwowarski and Zaragoza [22] explored three different predicti ve click models based on personalized and aggregate click informati on in a Web search setting, trying to predict relationships bet ween queries and clicked documents. They built a probabilistic user-centric model, a group model, and a global model, and a model that combined all three. The best of their models was able to achieve either moder-ate prediction accuracy (50% of the clicks) with hi gh recall (75% of the time), or very high accuracy of 98% but low recall (5% of the time). White et al. [30] compared the value of a variety of sources of contextual information for predicting fu ture interests at different time scales (one hour, one day, or one we ek). The best accuracy was obtained when recent actions (the so-c alled interac-tion context ) were used to predict interests in the following h our. Bailey et al. [4] focused on assigning ODP labels t o long and rare (previously-unseen) Web search engine queries. They used labels assigned to pages on browse trails extracted from t oolbar logs following seen queries and matched the unseen queri es to them. The research presented in this paper differs from t he previous work reviewed above in several important ways. Firs t, we exam-ine contexts from several sources: queries, URLs vi sited (from search engine results pages and in subsequent Web b rowsing), and learn optimal source combinations. Second, we focus on develop-ing models capable of accurately predicting the fut ure interests in a search session, and explore precision/coverage tr adeoffs. Such predictive models can be used for a wide variety of applications, including supporting pro-active changes to the inte rface to empha-size results of likely interest or to suggest conte xtually-relevant query alternatives, as well as more traditional app lications to rank-ing and filtering. Finally, we base our analyses on a large set of user searching and browsing sessions obtained from log data, thus addressing scale and representativeness issues. The scenario that we model in this investigation is that of a Web searcher who has just issued a query to a search en gine. It is at this point that the engine may leverage the recent search activity of the user to augment the current search query wit h a more so-phisticated representation of their search intent. Important ques-tions around which types of search activity sources can be used to build contextual models X  X s well as how and when to c ombine these sources X  X re answered in the study presented in this paper. We begin by describing the data used to both model search activi-ty and evaluate the predictive performance of the m odels. The primary source of data for this study was a pro prietary data set containing the anonymized logs of URLs visited by users who consented to provide interaction data through a wid ely-distributed browser plugin. The data set contained browser-base d logs with both searching and browsing episodes from which we extract search-related data. These data provide us with exa mples of real-world searching behavior that may be useful in unde rstanding and modeling search context. Log entries include a time stamp for each page view, and the URL of the Web page visited. To remove vari-ability caused by geographic and linguistic variati on in search behavior, we only include log entries generated in the English-speaking United States locale. The results describe d in this paper are based on URL visits during the last week of Feb ruary 2010 representing billions of Web page visits from hundr eds of thou-sands of unique users. From these data we extracted search ses-sions on the Bing Web search engine, using a session ext raction methodology similar to [29]. Search sessions begin with a query, occur within the same browser and tab instance (to lessen the effect of any multi-tasking that users may perform) , and terminate following 30 minutes of user inactivity. We use the se browser-based logs rather than traditional search-engine lo gs since they provide access to all pages visited in the search s ession preceding and succeeding the search query, information that i s important for our later analyses. The median session length was 1 9 actions (que-ries and Web page views) (mean=29 actions). The med ian dura-tion of a search session was 10 minutes 9 seconds ( mean=8 minutes 32 seconds). To augment the browser-based l ogs, we also mined Bing search engine logs to obtain the URLs of the top-ten search results returned for each query (to build qu ery models). We represented context as a distribution across cat egories in the ODP topical hierarchy. This provides us with a cons istent topical representation of queries and page visits from whic h to build our models. ODP categories can also be effective for re flecting topical differences in the search results for a query [5] o r a user X  X  interests [30]. Given the large number of pages present in ou r log data, we used automatic classification techniques to assign an ODP catego-ry label to each page. Our classifier assigned one or more labels to the pages based on the ODP using a similar approach to Shen et al. [23]. In this approach, classification begins w ith URLs present in the ODP and incrementally prunes non-present URL s until a match is found or miss declared. Similar to [23], we excluded pages labeled with the  X  X egional X  and  X  X orld X  top-l evel ODP categories, since they are location-based and are t ypically unin-formative for constructing models of user interests . To lessen the impact of small differences in the labels assigned, we also filtered to only use 219 categories at the top two levels of the ODP hierar-chy, referred to as hereafter. The coverage of the resulting ODP classifier with URL back-off was approximately 60%. To improve the coverage of the classifier we combined it with a text-based classifier, described in [5], that uses logistic re gression to predict the ODP category for a given Web page. When optimiz ed for the score in each ODP category, the text-based classif ier has a micro-average of 0.60. Predicted ODP category labels from this classifier were available for many pages in th e Bing search engine index. For URLs where only one classifier ha d labels, the most frequent label (for ODP lookup) or the most pr obable label (for the text-based classifier) was used. For URLs where both classifiers had a label, the label was determined b y first looking for an exact match in the ODP, then in the classifi ed index pages, and then incrementally pruning the URL and checking for a cate-gory label in the ODP or in the classified index pa ges. This classi-fier combination led to coverage exceeding 80% acro ss all URLs in our set. We did not attain 100% coverage since s ome URLs were in the hidden Web and not in the Bing index (e .g., because logins are required, the pages are dynamically gene rated, etc.). We use three sources to build models from search se ssions: 1. Query : ODP labels automatically assigned to the top-ten search results for the query returned by the engine used i n our study. 
Label assignment is described in more detail in Sec tion 3.4. 2. SERPClick : ODP labels automatically assigned to the search results clicked by the user during the current sear ch session. 3. NavTrail : ODP labels automatically assigned to Web pages th at the user visits following a SERP click. By examining the effectiveness of interest models b uilt with these sources, we can help determine their relative value and provide insight into which sources are the most important f or search en-gines to represent to attain good prediction perfor mance. For ex-ample, queries and SERP clicks are easy for search engines to capture, but post-SERP capturing browsing behaviors involves the deployment of client-side software such as a Web br owser plugin. In this study, we experiment with building models o f the search context for the current query using: (i) previous q ueries only; (ii) previous search engine activity only (previous quer ies and SERP clicks), and; (iii) all previous activity (previous queries, SERP clicks, and post-SERP Web page visits). We also com pare each of these source combinations against the current query alone. Three models were constructed to represent users X  s hort term interests: query (the current query), context (queries and Web pages viewed prior to the current query), and intent (a weighted combination of current query and context). The sequ ence of ac-tions following the current query in the session is used to develop the relevance model used as ground truth. Note that the  X  X odels X  are different from the  X  X ources X  described in the p revious section. The sources determine the information used in build ing the mod-els. The decision about which sources are used in c onstructing the models can be made based on availability (e.g., sea rch engines may only have access to queries and SERP clicks) an d/or desired predictive performance (more sources may lead to mo re accurate models, but may also contain more noise if searcher s deviate from a single task). All models represent user interests as a probability distribution across the ODP labels in . In the remainder of this subsection we provide more details on each of the m odels. Given the method for assigning ODP category labels to URLs, we assigned labels to a query as follows. For each que ry, we obtain the category labels for the top-ten search results returned by the Bing Web search engine at query time. Probabilities are assigned to the categories in by using information about which URLs are clicked for each query. We first obtain the normali zed click fre-quencies for each of the top-ten results from searc h-engine click log data gathered during all of 2009, and computed the distribu-tion across all ODP category labels. Search results without click information are ignored in this procedure. ODP cate gories in that are not used to label top-ranked results are a ssigned the prior probabilities for query models, as described in Sec tion 3.5. The context model is constructed based on actions t hat occur prior to the current query in the search session. Actions comprise que-ries, Web pages visited through a SERP click, or We b pages visit-ed on the navigational trail following a SERP click . A query mod-el is created for each previous query in the contex t using the method described in the previous subsubsection. A m odel for each Web page is created using the ODP category label as signed via the strategy described earlier (i.e., first check f or exact match in ODP, then check for exact match with logistic regre ssion classifi-er, etc.). The weight attributed to the category la bel assigned to the page is based on the amount of time that the us er dwells on the page. Dwell time has been used previously as a meas ure of user satisfaction with Web pages [1][11]. In a similar w ay, we assume that if a user dwells on a page for longer than 30 seconds, then the page contains useful content. However, instead of u sing a binary relevant/non-relevant threshold of 30 seconds we us ed a sigmoid function to smoothly assign weights to the categori es. Function values ranged from just above zero initially to one at 30 seconds. In addition to varying the probability assigned to the class based on page dwell time, we also assigned an exponential ly-decreasing weight to each action as we move deeper into the co ntext. That is, pre-query actions were weighted according to , where represents the number of actions before the current query. A simi-lar discount has been applied in previous work on o stensive rele-vance [6]. Using this function, we could assign the action imme-diately preceding the current query a weight of one and down-weight the importance of all preceding session acti ons, such that more distant events received lower weights. This is supported by previous work which suggests that the most recent a ction is most relevant for predicting the next action (e.g., [10] [23]). All page and query models in the context had their contribut ion toward the overall context model weighted based on this discou nt function. All of these models were merged and their probabili ties normal-ized so that they summed to one (after priors were assigned to unobserved categories). The resultant distribution over the ODP category labels in represents the user X  X  context at query time. The intent model is a weighted linear combination o f the query model (for the current query) and the context model (for the pre-vious actions in the search session). Since this mo del includes information from the current query and from the pre vious actions, it can potentially provide a more accurate represen tation of user interests than the query model or the context model alone. The intent model is defined as: where , , and represent the intent, context, and query models respectively, and represents the weight assigned to the context model. When combining the query and context models to form the intent model, by default = 0.5. However, as we will show, the optimal value of varies per query and can be accurately predict-ed using features of the query and its activity-bas ed context. The relevance model contains actions that occur following the current query in the session. This captures the  X  X uture X  as show n in Figure 1 and represents the ground truth for our predictions. The relevance model comprises a probability distrib ution over and is constructed in a similar way to the context model. The only difference between how the two models are built is that the rele-vance model considers future actions rather than pa st actions. In the relevance model, we weight the action immediate ly following the query X  X ypically another query or a SERP click X  X os t highly, and decrease the weight rapidly for each succeeding action in the session (using the same exponential decay function as the context model). This regards the next action as more import ant to the user than the other actions in the remainder of the sess ion. This seems reasonable as the next action may be most closely r elated to their interests for the search query. We use this relevan ce model as the ground truth for measuring the accuracy of our pred ictions of short-term user interests and for learning the opti mal combination of query and context for a query. User behavior has been shown to be a useful measure of search success in previous w ork [2][14]. Since the relevance model is automatically generate d, it can be used to evaluate performance on a large and diverse set of queries, but may contain noise associated with variance in s earch behavior. To handle missing values, each of the interest mode ls was as-signed a prior distribution across based on ODP categories as-signed to URLs in a held out set of 100,000 randoml y-selected search sessions from our data set, hereafter referr ed to as . In this set the number of sessions from any single use r was restricted to ten. Limiting the number of sessions per user le ssened the like-lihood that highly-active users would bias our samp le. Priors were tailored to the sources being used to construct eac h interest model. For example, the query models were always initializ ed with the query prior (generated from the ODP categories labe ls assigned across all URLs appearing in search result lists in ), whereas a context model X  X  priors are based on all sources use d to build the model (e.g., all search engine activity in ). We now describe our study, beginning with research questions. Three research questions drove our investigation: 1. What fraction of search engine queries could be impacted by the use of context information? 2. What is the predictive accuracy of the user inte rest models gen-erated from the current query, context, and intent? What is the effect of varying the source of context information on predic-tive accuracy? 3. Can we learn how best to combine query and conte xt models? In the remainder of this section we answer each que stion in turn, beginning with the extent of the opportunity offere d by context. We first investigate the potential opportunity of u sing short-term session context information for interpreting the cu rrent query. To do so we selected a random sample of one hundred th ousand search sessions, hereafter referred to as , from the data set de-scribed in Section 3.1 with the same ten-session-pe r-user limit as in . In , there were a total of 325,271 queries. Of these q ue-ries, 224,634 (69%) were not reached through sessio n-level revis-itation (e.g., through the browser  X  X ack X  button). This is important in estimating the opportunity for using contextual information, since it focuses attention on those queries that us ers actually sub-mitted. For queries that are repeated using the bac k button, search engines would probably not want to update search re sults when users returned to them. Indeed, previous work has s hown that care should be taken when adapting the content or the or dering of search results for repeat queries as this could lea d to user confu-sion and frustration [27]. We therefore focus on in tentionally-issued search queries in the remainder of our analy sis. Around 40% of the sessions in contained multiple queries and around 60% of queries had at least one preceding qu ery in the search session, providing an opportunity to generat e a context model. Given an ideal interest model construction m ethod that could build query/context/intent models for all enc ountered con-texts, we have opportunity to improve the search ex perience for a significant fraction (60%) of the search traffic th at search engines receive. 1 This is important, since implementing support for con-text modeling at scale is a potentially costly undertaking for search engine companies, who must also consider the investment in infrastructure required to serve such context ra pidly for many millions of search queries daily. The simple label assignment methods described in Section 3.2 cover almost 80% o f all con-texts, so we cover roughly 50% of all queries in ou r experiments. In the previous section we showed that context can cover a signif-icant portion of search engine queries. In this sec tion, we focus on how well each of our models  X  current query only ( ), context only ( ), and intent ( )  X  predicts the future short-term interests of the current user, represented by the relevance mode l or ground truth ( ). The advantage of using logs for this study is th at we can observe future actions and construct the relevance model automat-ically from them. As part of the analysis, we also vary the source of information used to construct the context model: either previ-ous queries, previous queries and SERP clicks, or a ll previous actions. In building the intent models in this sect ion we set the context weight ( ) to 0.5. For this analysis we used the same set of sessions used in the previous section ( ). Parametric statistical testing is performed where appropriate with alpha s et to .05. To evaluate the predictive accuracy of the model so urces we used the measure. This measure computes the harmonic mean o f precision and recall, and has been used successfull y in a number of search scenarios including prior work on evaluat ing context sources [30]. We prefer to alternatives such as Jensen-Shannon divergence since is easily interpretable and makes model com-parisons simpler. is defined in the standard way as: where we define precision and recall as: Note that it may also be possible to create contex ts from preced-ing actions beyond search sessions (e.g., previous Web page vis-its alone). See the work of White and colleagues [3 0] for details. where is the set of instantiated labels in the model bei ng tested and is the set of instantiated labels in the relevance model. Label instantiation occurs when category labels are assigned to a query or a Web page as described in Section 3.4. Wh en either or are empty, precision and recall equal zero. We use to measure the predictive accuracy of each of the quer y, context, and intent models. For each model, the recall depth is computed based on the number of categories which were instantiated in building the relevance model comprising all future session a ctivity. In order to compare the different models on the sam e queries, we focus on queries for which we could construct a mod el for the current query, its context, and its future (for rel evance), for all context sources. This equated to 30% of the queries in . We iterated over each query, and computed the models p er the ap-proach described earlier and calculated how accurat ely the models predicted the future (using the score). The findings of this analysis across all queries in our set are summariz ed in Table 1. Note that the contribution of the query model (colu mn ) is the same for all sources because no context is used. Th e findings un-der  X  X ccuracy X  in the table suggest that: (i) lever aging any con-text (columns and ) yields significant prediction accuracy gains over using no context ( ), in terms of predicting the rele-vance model; (ii) leveraging more sources yields mo re accurate predictions (improved predictions for both and as more con-text sources are included), and; (iii) the context ( ) and intent ( ) models do not differ in terms of overall accuracy. All observed differences in the scores between the three models were statis-tically significant using one-way analyses of varia nce (ANOVA) (all (2,122154)=6.91, all &lt;.001). In addition to comparing the overall scores, we also computed the fraction of queries for which each model outper formed the other models and the fraction of queries for which each source outperformed the other sources. This analysis allow s us to identify queries for which each of the models provides best performance. The percentage of queries for which each of the mod els and con-text sources wins is shown in Table 1 under  X  X ercen tage of que-ries best. X  Only queries where one model / context source was a clear winner are included under this heading in the table. Queries with ties, where all models failed to predict (arou nd 10% of the sample used) or two of more models / context source s had the same score (around 30% of queries) are not included. Th e columns , , and show the percent of queries for which that model for each context source  X  e.g., for the Query source, is best on 18% of queries, on 25%, and on 22%. The final col-umn shows differences for sources for the intent mo del. Here we see that the richer the context the better: the Query source is best Context source Accuracy ( ) Percentage of queries best None (i.e., current query only) 0.39  X  0.39 100%  X  100% 15% Query (i.e., all previous queries) 0.39 0.42 0.43 25% 18 % 22% 19% on 19% of the queries, the Query + SERPClick source is best for 22% of queries, and all sources are best for 26% of queries. At least two observations can be made from the find ings. First, as expected, there are sets of queries for which each of the models performs best, and second, there are queries for wh ich each of the model sources performs best. To understand more abo ut which queries each of models performed best on we visuall y inspected a sample of queries and their sessions to understand why the model or source performed so well. The findings of our an alysis revealed that in cases where the current query model ( ) wins, the query either has very specific search intent (e.g., [espn] , [webmd] , [call of duty 4 demo] ), or is in situations where the query represents t he first action after a noticeable shift in topical in terests within the search session. In cases where the context model ( ) wins, the query is a continuation of constant intent, the que ry is ambiguous (e.g., [amazon] ) or is detached from the search session (e.g., que -ry for [facebook] during a search session seemingly about Ama-zonian rainforests). In cases where the intent mode l ( ) wins, there is typically a consistent intent throughout the ses sion. To detect shifts in interests, we computed the cros s entropy ( ) of the query model versus the context model. Cross entropy is an information theoretic measure of the average number of bits need-ed to identify an event from a set of possibilities given two distri-butions. It has been used in previous work to compa re distribu-tions in order to make ranking decisions [5]. It is defined as: where and represent the current query model and the context model respectively, and and represent the probability as-signed to each of the category labels ( ) in the current query and context models. In the case of comparing query and context, the interpretation would be the number of bits on avera ge to encode the class of query that is actually issued next giv en predictions regarding what class would be issued using the prev ious context. In situations where the entropy between the query m odel and the context model is low, we would expect the query to be a continua-tion of the same intent (and perhaps more weight sh ould be placed on the context). In cases where it is high, we woul d expect there to be a shift in intents (and perhaps more weight s hould be placed on the current query). The Pearson X  X  correlation co efficient ( ) between the cross entropy and the scores for the context and the query models were 0.63 and 0.58 respectively, s uggesting that difference between the query and the context may be important in determining when contextual information should be l everaged. To gain insight into the breadth of interests assoc iated with a que-ry, we computed the click entropy of the query. Cli ck entropy has been used in previous work [28] to gauge the variab ility in intents associated with a query, manifested as clicks on se arch results. Low click entropy for a query suggests that searche rs generally target a single result URL. In contrast, a query wi th high click entropy indicates that a range of URLs are targeted . Interestingly, for queries in which the context and intent models outperformed the query models, the click entropy of the query wa s significantly higher (2.48 versus 2.12). This is consistent with the intuition that for ambiguous queries, knowing more about previous session context can be helpful. It also seems that query cl ick entropy pro-vides insight into when more weight should be place d on context, perhaps to help disambiguate search results of ambi guous queries. We will now explore varying the context weight for each query. In our analysis thus far we have assumed that the w eight assigned to the context in the intent model is always 0.5. H owever, as our findings in this section, and the findings of previ ous work in this area [32], have suggested, it is unlikely that the same context weights should be used for all queries. In the next section we pre-sent an investigation of whether we can automatical ly learn the optimal context weight on a per-query basis. To learn the optimal weight to assign to context wh en combining the context model and the query model we identified the optimal context weight ( ) for each query on a held out training set, creat-ing features for the query and the context that cou ld be useful in predicting , and then learning using those features. In this section we cover all three of these steps, beginnin g with the opti-mization task. We also present findings on improvem ents in pre-diction accuracy obtainable by learning , and results of predic-tion experiments using different model sources. The goal of the optimization is to determine the co ntext weight that minimizes the difference in distributions betw een the intent and the relevance models. To construct a set for le arning, we as-sume therefore that we are given a set of queries w ith their con-text, query, and relevance models collected from ob served session behavior. We first need to convert the knowledge of the future represented in the relevance model to an optimal co ntext weight that we then use for training a prediction model. T he function that we wish to minimize in this scenario is the cross-e ntropy, as de-fined in Equation 5, between the intent model and t he relevance model. In this case, the reference distribution is the relevance model, and the cross-entropy takes its minimal valu e (the entropy of the relevance distribution) when the intent mode l distribution is equal to the relevance distribution. The objective function used is: Here , , and represent the probability assigned to the th category by relevance, context, and current query m odels, respec-tively. Similarly, is the corresponding intent probability using w as the context weight. The first term in this equat ion is simply the cross-entropy between the relevance and intent distri-butions. The second term is a regularizer that pena lizes deviations from w =0.5. It is essentially a Gaussian regularization a pplied after a logit transform (which is monotone in and symmetric around =0.5). The regularizer also has the negligible effe ct of constraining the optimum to lie in the open interva l (0,1) instead of the closed interval [0,1]. After squaring then, the regularization term is convex. Since cross-entropy minimization is also known to be convex, for &gt; 0 , the resulting problem is convex and can be minimized efficiently to find an optimal value of . Besides keep-ing closer to 0.5, the regularizer is helpful in that w ithout it, small deviations in the distributions (e.g., due to floating point imprecision) can force the optimal weight to 0 or 1 although the value of the objective is essentially (near) flat. This adds a source of unnecessary noise to learning and is easily hand led through regularization. For our experiments, we set , and further exploration of this parameter remains as future wor k. To create a training set, we use the query, context , and relevance models to compute the optimal context weight per qu ery by min-imizing the regularized cross-entropy for each quer y independent-ly. Note that the relevance model is implicitly the labeled signal which optimization converts to a  X  X old-standard X  we ight to be used in learning and prediction. At query time, a search engine has access to a larg e number of features about the query and the activity-based sea rch context that could be useful for learning the optimal context we ight. Table 2 lists the features that were used in our prediction s. Features were divided into three classes: Query , capturing characteristics of the current query and the query model, including log-ba sed features based on search logs of the Bing search engine for the last week in January 2010, italicized in Table 2; Context , capturing aspects of the pre-query interaction behavior as well as featu res of the con-text models themselves, and QueryContext , capturing aspects of how the query model and context model compare. Feature Feature description QueryLength Number of characters in query QueryWordLength Number of words in query AvgQueryWordLength Average length of query words AvgClickPos Average SERP click position for query AvgNumClicks Average number of SERP clicks for query AvgNumAds Average number of advertisements shown on the SERP for query AvgNumResults Average number of total search results returned for the query AbandonmentRate Fraction of times query issued and has no SERP clic k PaginationRate Fraction of times query issued and next page of res ults requested QueryCount Number of query occurrences HasSpellCorrection True if search engine spelling c orrection is offered for query HasAlteration True if query is automatically modifi ed by engine (e.g., stemming) QueryEntropy Entropy of the query model ClickEntropy Click entropy of query based on distri bution of result clicks NumActions Number of queries and page visits (exclu des current query) NumQueries Number of queries (excludes current quer y) Time Time spent in session so far NumSERPClicks Number of search results clicked NumPages Number of non-SERP pages visited NumUniqueDomains Number of unique domains visited NumBacks Number of session page revisits NumSATDwells Number of page dwells exceeding a 30-s econd dwell time threshold AvgQueryOverlap Average percentage query overlap b etween all successive queries ContextEntropy Entropy of the context model The broad range of features used enabled us to capt ure many as-pects of search activity. These features were gener ated for each session in our set and used to train a predictive m odel to estimate the optimal weight to be placed on the context when building the intent model. We now describe the experiments perfo rmed to evaluate our predictions of the optimal context wei ght. We used Multiple Additive Regression Trees (MART) [ 12] to train a regression model to predict the optimal con text weight. MART uses gradient tree boosting methods for regres sion and classification. MART has several strengths, includi ng model in-terpretability (e.g., a ranked list of important fe atures is generat-ed), facility for rapid training and testing, and r obustness against noisy labels and missing values, that make it attra ctive for this task. We selected a new set of one hundred thousand search ses-sions with no overlap with the sessions used in the analysis pre-sented previously in this paper (referred to as ). From these sessions we selected around 45,000 queries for whic h we could construct a query, context, intent, and relevance m odel. We used all search session activity (queries, SERP clicks, and post-SERP Web page visits) since models constructed from thos e sources yielded the best predictive performance in our earl ier analysis and we wanted to see how well we could do given this ri ch source of information. We used 60% of those queries for train ing, 20% for validation, and 20% for testing, and performed ten-fold cross validation to improve result reliability. Note that in constructing the folds, we split by session so that all queries in a session are used for either training, validation, or testing. W e do not allow queries from the same session to be used in differe nt phases as this may bias our experiments. Pearson X  X  correlatio n ( ) and root mean squared error ( ) were used to measure our perfor-mance at predicting optimal . Correlation measures the strength of association between predicted and actual on a sc ale from 1, with one indicating a perfect correlation and ze ro indicating no correlation. is the square root of the mean of the squared deviations between the actual and predicted values and resides between zero and one. The ideal value of is zero, with larger values showing more errors. Following our an alysis, the average obtained across all experimental runs was 0.85 and the average across all runs was 0.15. 2 The weights assigned by the model to the top-15 features are shown in Table 3, normalized relative to the most predictive, QueryContextCrossEntropy . From Table 3 it appears that the most performant fe atures relate to the information divergence of the query models and the context models. This suggests that the strength of the rela tionship between the current query and the context is an important i ndicator of how much weight to assign to the context. Also importan t are features of the current query only and its context only. In Figure 2 we plot the predicted and optimal conte xt weight and show the line of best fit between them for a repres entative cross-validation run. Each point on the plot is a search query. From the figure it appears that we perform well when predict ions place a large amount of weight on the context. Indeed, alth ough the aver-age true optimal across all queries is 0.69, the average predicted is 0.75, suggesting that our predictions may overw eight context. For reference, the average performance of the mode l trained only on search engine interactions was =0.75, =0.19. Feature Class Importance QueryContextCrossEntropy QueryContext 1.00 JensenShannonDivergence QueryContext 0.86 ContextEntropy Context 0.69 ContextQueryCrossEntropy QueryContext 0.46 ContextJensenShannon Context 0.19 QueryEntropy Query 0.18 LastContextWeight Context 0.17 QueryCount Query 0.14 NumActions Context 0.12 FracQueryModelNonPrior Query 0.12 FracContextModelNonPrior Context 0.12 NumQueries Context 0.11 NumSERPClicks Context 0.05 QueryJensenShannon Query 0.04 ClickEntropy Query 0.04 Figure 2. Predicted context weight vs. optimal cont ext weight. Prediction error was highest for those queries in t he top-left corner of the figure. Inspection of the queries revealed t hat they were rare queries or contained typographical errors, which me ant that we could not generate some of the log-based query feat ures such as QueryCount , which from Table 3, appear to be important in our predictions. Prediction errors were also larger for queries at the beginning of search sessions, meaning that there wa s limited con-textual information was available to build interest models. In the next section we apply the estimates of the o ptimal context weight to the prediction of future interests tackle d in Section 4.3. Given that we are able to predict the optimal conte xt weight with good accuracy, we now investigate the impact on pre dictive accu-racy of utilizing the predicted optimal context wei ght in the intent model when combining the query model and the contex t model. To do this, we used a model trained on all sessions in , created Predicted context weight a test set of 100,000 randomly-chosen search sessio ns (not in or and limited to ten sessions per user), and chose t he optimal context weight for queries where could construct th e models. For each query in the test set, we then constructed the intent mod-el with the predicted context weight ( ) and computed the score between the intent model and the relevance mo del for each query. In Table 4 we present the average score using the per-query estimates computed from our predictive model and the av-erage score assuming that the context and query mod els each get a weight of 0.5, the optimal combination across all q ueries (com-puted by averaging over all sessions in ). To provide an up-per bound for , we computed the score that could be obtained if we used an oracle, the optimal for each query in the test set. This is shown in the last row of the table. The ora cle X  X  score is not one because it is based on the optimization des cribed earlier, and may contain noise. These scores give us a sense for the gain to be obtained from estimating the context weight p er query and how much lift we would get if we just applied a glo bal optimum (derived by averaging the optimal values over all q ueries), remov-ing the need for engines to deploy a run-time class ifier. Context weight source Percentage of oracle Default ( = 0.5) 0.49 75.3 Global optimum ( = 0.75) 0.52 80.0 Per-query optimum 0.56 86.1 Oracle 0.65  X  The findings suggest that the global optimum helps to obtain a performance that is close to the oracle (80%), but the per-query optimum, based on features of the query, the contex t, and their combination, achieves over 85% of the predictive ac curacy of the oracle. Both the global optimum and per-query optim um led to significant improvements in predictive performance over the de-fault, using paired -tests (global: (44873)=2.58, &lt;.01, per-query: (44873)=2.87, &lt;.01). These findings suggest significant benefit from optimizing context weights, even if se arch engines can only use the global optimum due to infrastructu re constraints. To evaluate how the amount of context and relevance information available to build the predictive model influences its prediction accuracy, we built models using different amounts o f context and relevance. In building the context model we used ei ther all previ-ous actions or the most recent previous action. In building the relevance model we use all future actions, the next action, or the last action in the session. Table 5 shows the avera ge performance. Table 5. Average predictive performance by model so urce. 
Relevance model source All actions =0.85, =0.15 =0.78, =0.19 Next action =0.83, =0.16 =0.77, =0.19 Last action =0.83, =0.16 =0.76, =0.19 We analyzed the models using the same methodology d escribed in Section 4.4.3, ran a 3  X  2 ANOVA with relevance model source and context model source as the factors, and examin ed the signifi-cance of pairwise differences using Tukey post-hoc testing. We found that when context models were built from only the previous action, performance was significantly lower than wh en all previ-ous actions were used (all &lt;.01). This suggests that additional preceding actions adds predictive signal. We found no significant difference for any source when varying the amount o f information used to generate the ground truth (first subsequent action, last action in session, or all actions), all  X  .12. It appears that the source of relevance information has only a marginal impact on how well the models estimate the weight to assign t o the context. In this section we demonstrated predictive value in context, varia-tion by source and by model, and have shown that we can learn to predict optimal context weights with good accuracy. Through a log-based analysis, we quantified the opp ortunity for using activity-based search context, compared the a ccuracy of interest models of the current query, context, and their combina-tion, and learned optimal weights to combine the qu ery and con-text models on a per-query basis. These findings ca n inform the design of search systems to leverage contextual inf ormation to better understand, model, and serve searchers X  info rmation needs. Context models based on recent search activity pres ent significant opportunity to improve search performance. We showe d that over 60% of queries had at least one preceding query. Th e simple method we used for generating category labels cover s almost 80% of all contexts, meaning that we can potentially im prove engine responses for roughly 50% of the queries in our exp eriments. We found that using context led improved the accura cy of predic-tions of future interests over the current query al one. This is in line with previous work, which has also demonstrate d the benefits of contextualized search [24][26]. Further, leverag ing increasing-ly-richer sources of contextual information (querie s, SERP clicks, and post-SERP Web page visits) improved predictive accuracy. We showed that there are distinct query sets for wh ich different interest models and sources perform most effectivel y, suggesting that query information is likely important in selec ting sources and/or model weights. Finally, we showed improvemen ts in pre-dictive accuracy by learning per-query context weig hts. By representing short-term session-contexts we are able to signifi-cantly improve our ability to model user intent. Th e richer and more accurate predictive models we developed can be used to interpret the query for a variety of search-related applications, including interface changes to emphasize results of likely interest, to suggest contextually-relevant query alternatives , or for ranking and filtering. Category-level information has alrea dy been shown to improve result relevance for just the current qu ery [5]. A direct extension of our work would be to use the context m odel (as-signed a weight based on features of the query and context) to improve the quality of search engine result ranking s, by promot-ing results that are consistent with the inferred u ser intent. There are several directions for improving model de velopment. The gains in the optimal context weight prediction performance when moving from previous action only to all preced ing actions suggests value in using multi-action context. To re duce noise from the context, we need to experiment with ways to sel ect only rele-vant actions and explore other context decay functi ons. The priors used in the models for each of the sources were bas ed on the be-havior of many users across many different queries. Personalized model priors based on a user X  X  search history could also be used so that predictions can be tailored to topics that interested them. Further work is needed to verify the accuracy of th e relevance models based on future actions. Manual labeling of a random sample of sessions (going beyond the visual inspect ion performed in this study) may be necessary to create a reliabl e ground truth and ensure that our session demarcation (which was temporally-based not topic-based) is accurate. These labels co uld provide an additional source of ground truth information for l earning the optimal combination of query and context and in eva luating pre-dictions or result rankings generated using such a combination. More generally, there are important opportunities t o develop new context-aware evaluation methodologies. Evaluation of search results is typically based on the query alone, and new judgment protocols and evaluation metrics (e.g., [16]) are n ecessary to also consider search context. The framework we have deve loped to represent and predict user interests could also be generalized by using other context features and alternative outcom e measures. In this paper we have described a study investigati ng the effec-tiveness of activity-based context in predicting us ers X  search inter-ests. We demonstrated that context can be captured and modeled for a significant portion of search queries, sugges ting that there lies significant opportunity in leveraging contextu al information. We explored the value of modeling the current query , its context, and their combination (which we refer to as intent ), and different sources of context (search queries, SERP clicks, an d post-SERP navigation). Our findings showed that intent models developed from many sources perform best overall. In addition , we found that all models and context sources have some set o f queries for which they provide the best performance. Thus, we a lso devel-oped techniques to learn the optimal combinations o f query and context models per query. Our findings demonstrate significant opportunity in leveraging short-term contextual inf ormation to improve search systems. Future work involves constr ucting more sophisticated user interest models, and the develop ment and de-ployment of search engine enhancements to ranking a nd result presentation that leverage context information effe ctively. [1] Agichtein, E., Brill, E. and Dumais, S. (2006). Imp roving [2] Agichtein, E., Brill, E., Dumais, S. and Ragno, R. (2006). [3] Armstrong, R., Freitag, D., Joachims, T. and Mitche ll, T. [4] Bailey, P., White, R.W., Liu, H. and Kumaran, G. (2 010). [5] Bennett, P., Svore, K. and Dumais, S. (2010). Class ification-[6] Campbell, I. and Van Rijsbergen, C.J. (1996). The o stensive [7] Cao, H., Jiang, D., Pei, J., He, Q., Liao, Z., Chen , E. and Li, [8] Cao, H., Hu, D.H., Shen, D., Jiang, D., Sun, J.-T., Chen, E. [9] Chirita, P., Nejdl, W., Paiu, R. and Kohlschutter, C. (2005). [10] Downey, D., Dumais, S., Liebling, D. and Horvitz, E . (2008). [11] Fox, S., Karnawat, K., Mydland, M., Dumais, S. and White, [12] Friedman, J.H., Hastie, T. and Tibshirani, R. (1998 ). Additive [13] Gauch, S., Chaffee, J. and Pretschner, A. (2003). O ntology-[14] Hassan, A., Jones, R. and Klinkner, K.L. (2010). Be yond [15] Ingwersen, P. and J X rvelin, K. (2005). The Turn: Integration [16] J X rvelin, K., Price, S., Delcambre, L. and Nielsen, M. (2008). [17] Jeh, G. and Widom, J. (2003). Scaling personalized web [18] Kelly, D., Dollu, V.D. and Fu, X. (2005). The loqua cious [19] Ma, Z., Pant, G. and Sheng, O. (2007). Interest-bas ed per-[20] Muhalkova, L. and Mooney, R. (2009). Learning to di sam-[21] Pitkow, J., Sch X tze, H., Cass, T., Cooley, R., Turn bull, D., [22] Piwowarski, B. and Zaragoza, H. (2007). Predictive user [23] Shen, X., Dumais, S.T. and Horvitz, E. (2005). Anal ysis of [24] Shen, X., Tan, B. and Zhai, C. (2005). Context-sens itive [25] Speretta, M. and Gauch, S. (2005). Personalizing se arch [26] Tan, B., Shen, X. and Zhai, C. (2006). Mining long-term [27] Teevan, J. (2008). How people recall, recognize, an d reuse [28] Teevan, J., Dumais, S.T. and Liebling, D.J. (2008). To per-[29] White, R.W. and Drucker, S.M. (2007). Investigating behav-[30] White, R.W., Bailey, P. and Chen, L. (2009). Predic ting user [31] Voorhees, E. and Harman, D. (2005). TREC: Experiment and [32] Xiang, B., Jiang, D., Pei, J., Sun, X., Chen, E. an d Li, H. 
