 University of Twente Tilburg University/ Netherlands Institute for Advanced Study in the Humanities and Social Sciences (NIAS) Carnegie Mellon University University of Twente/ Erasmus University Rotterdam
Language is a social phenomenon and variation is i nherent to its social nature. Recently, there has been a surge of interest within the computational linguistics (CL) community in the social dimension of language. In this article we present a survey of the emerging field of  X  X omputa-tional sociolinguistics" that reflects this increased interest. We aim to provide a comprehensive overview of CL research on sociolinguistic themes, featuring topics such as the relation between language and social identity, language use in s ocial interaction, and multilingual communi-cation. Moreover, we demonstrate the potential for synergy between the research communities involved, by showing how the large-scale data-driven methods that are widely used in CL can complement existing sociolinguistic studies, an d how sociolinguistics can inform and challenge the methods and assumptions used in CL studies. We hope to convey the possible benefits of a closer collaboration between the two communities and conclude with a discussion of open challenges. 1. Introduction
Science has experienced a paradigm shift alo ng with the increasing availability of large amounts of digital research data (Hey, Tansley, and Tolle 2009). In addition to the traditional focus on the description of natural phenomena, theory development, and computational science, data-driven exploration and discovery have become a dominant ingredient of many methodological framew orks. In line with these developments, the field of computational linguistics (CL) has also evolved.
 computational linguistics has primarily focused on capturing the informational di-mension of language and the structure of verbal information transfer. In the words of Krishnan and Eisenstein (2015), computational linguistics has made great progress in modeling language X  X  informational dime nsion, but with a few notable exceptions, computation has had little to contribute to our understanding of language X  X  social dimension. The recent increase in interest of computational linguists to study language in social contexts is partly driven by the ever increasing availability of social media data.
Data from social media platforms provide a strong incentive for innovation in the CL research agenda and the surge in relevant d ata opens up methodological possibilities for studying text as social data. Textual resources, like many other language resources, can be seen as a data type that is signalin g all kinds of social phenomena. This is related to the fact that language is one of t he instruments by which people construct their online identity and manage their socia l network. Of course, there are challenges as well. For example, social media language is more colloquial and contains more linguistic variation, such as the use of slang and dialects, than the language in data sets that have been commonly used in CL research (e.g., scientific articles, newswire text, and the Wall
Street Journal ) (Eisenstein 2013b). However, an even greater challenge is that the relation between social variables and language is typically fluid and tenuous, whereas the CL field commonly focuses on the level of literal meaning and language structure, which is more stable.
 the symbolic nature of the relation between them. With the language chosen a social identity is signaled, which may buy a speaker 1 something in terms of footing within a conversation; or, in other words, for speakers there is room for choice in how to use their linguistic repertoire in order to achieve social goals. This freedom of choice is often referred to as the agency of speakers and the linguistic symbols chosen can be thought of as a form of social currency. Speakers may thus make use of specific words or stylistic elements to represent themselves in a certain way. However, because of this agency, social variables cease to have an essential connection with language use. It may be the case, for example, that on average female speakers display certain characteristics in their language more frequently than their male counterparts. Nevertheless, in spe-cific circumstances, females may choose to de-emphasize their identity as females by modulating their language usage to sound more male. Thus, although this exception serves to highlight rather than challenge the commonly accepted symbolic association between gender and language, it nevertheless means that it is less feasible to predict how a woman will sound in a randomly selected context.

Just as with any violation of expectations, these creative violations communicate in-direct meanings. As these violations beco me conventionalized, they may be one ve-hicle towards language change. Thus, agency plays a role in explaining the variation in and dynamic nature of language practices, both within individual speakers and across speakers. This variation is manifested at various levels of expression X  X he choice 538 of lexical elements, phonological variants , semantic alternatives, and grammatical patterns X  X nd plays a central role in the phe nomenon of linguistic change. The audi-ence, demographic variables (e.g., gender, age), and speaker goals are among the factors that influence how variation is exhibited in specific contexts. Agency thus increases the intricate complexity of language that must be captured in order to achieve a social interpretation of language.
 each other. Sociolinguists traditionally work with spoken data using qualitative and quantitative approaches. Surveys and ethnographic research have been the main meth-ods of data collection (Weinreich, Labov, and Herzog 1968; Trudgill 1974; Milroy and
Milroy 1985; Eckert 1989; Milroy and Gordon 2003; Tagliamonte 2006). The data sets used are often selected and/or constructed to facilitate controlled statistical analyses and insightful observations. However, the resulting data sets are often small in size compared with the standards adopted by the CL community. The massive volumes of data that have become available from sources such as social media platforms have provided the opportunity to investigate language variation more broadly. The opportu-nity for the field of sociolinguistics is to identify questions that this massive but messy data would enable them to answer. Sociolinguists must then also select an appropriate methodology. However, typical methods us ed within sociolinguistics would require sampling the data down. If they take up the challenge to instead analyze the data in its massive form, they may find themselves open to partnerships in which they may consider approaches more typical in the field of CL.
 social perspective, an increased awareness of insights from the field of sociolinguis-tics could inspire modeling refinements and potentially lead to performance gains. Recently, various studies (Volkova, Wilson, and Yarowsky 2013; Stoop and van den
Bosch 2014; Hovy 2015) have demonstrated that existing NLP tools can be improved by accounting for linguistic variation due to social factors, and Hovy and S X gaard (2015) have drawn attention to the fact that biases in frequently used corpora, such as the Wall Street Journal , cause NLP tools to perform better on texts written by older people. The rich repertoire of theory and pra ctice developed by sociolinguists could influence the field of CL also in more fundamental ways. The boundaries of commu-nities are often not as clear-cut as they may seem and the impact of agency has not been sufficiently taken into account in man y computational studies. For example, an understanding of linguistic agency can explain why and when there might be more or less of a problem when making inference s about people based on their linguistic choices. This issue is discussed in depth in some recent computational work related to gender, specifically, Bamman, Eisenstein, and Schnoebelen (2014) and Nguyen et al. (2014), who provide a critical reflection on the operationalization of gender in CL studies.
 within CL encourages collaboration between sociolinguistics and CL in various ways.
However, the potential for synergy between the two fields has not been explored systematically so far (Eisenstein 2013b) and to date there is no overview of the common and complementary aspects of the two fields. This article aims to present an integrated overview of research published in the two communities and to describe the state-of-the-art in the emerging multidisciplinary field that could be labeled as  X  X omputational sociolinguistics. X  The envisag ed audiences are CL researchers interested in sociolinguis-tics and sociolinguists interested in computational approaches to study language use.
We hope to demonstrate that there is enough substance to warrant the recognition of computational sociolinguistics as an autono mous yet multidisciplinary research area.
Furthermore, we hope to convey that this is the moment to develop a research agenda for the scholarly community that maintains links with both sociolinguistics and compu-tational linguistics.
 survey in more detail as well as the potential impact of integrating the social dimensions of language use in the development of practical NLP applications. In Section 2 we discuss methods for computational sociolinguistics, in which we reflect on methods used in sociolinguistics and computational linguistics. In Section 3, on language and social identity construction, we discuss ho w speakers use language to shape perception of their identity and focus on computational approaches to model language variation based on gender, age and geographical location. In Section 4, on language and social interaction, we move from individual speakers to pairs, groups, and communities and discuss the role of language in shaping personal relationships, the use of style-shifting, and the adoption of norms and language change in communities. In Section 5 we discuss multilingualism and social interaction, in which we present an overview of tools for processing multilingual communication, such as parsers and language identification systems. We will also discuss approaches for analyzing patterns in multilingual commu-nication from a computational perspective. In Section 6 we conclude with a summary of major challenges within this emerging field. 1.1 Rationale for a Survey of Computational Sociolinguistics
The increased interest in studying a soci al phenomenon such as language use from a data-driven or computational perspective e xemplifies a more general trend in scholarly agendas. The study of social phenomena through computational methods is commonly referred to as  X  computational social science  X  (Lazer et al. 2009). The increasing inter-est of social scientists in computational methods can be regarded as illustrating the general increase of attention for cross-disciplinary research perspectives.  X  X ultidisci-plinary, X   X  X nterdisciplinary, X   X  X ross-disc iplinary, X  and  X  X ransdisciplinary X  are among the labels used to mark the shift from monodisciplinary research formats to models of collaboration that embrace diversity in the selection of data and methodological frameworks. However, in spite of various attempts to harmonize terminology, the adoption of such labels is often poorly supported by definitions and they tend to be used interchangeably. The objectives of res earch rooted in multiple disciplines often include the ambition to resolve real-world o r complex problems, to provide different perspectives on a problem, or to create cross-cutting research questions, to name a few (Choi and Pak 2006).
 fits in this trend. We will use the term computational sociolinguistics for the emerging research field that integrates aspects of soc iolinguistics and computer science in study-ing the relation between language and socie ty from a computational perspective. This survey article aims to show the potential of leveraging massive amounts of data to study social dynamics in language use by combining advances in computational linguistics and machine learning with foundational concepts and insights from sociolinguistics.
Our goals for establishing computational sociolinguistics as an independent research area include the development of tools to suppo rt sociolinguists, the establishment of new statistical methods for th e modeling and analysis of data that contains linguistic content as well as information on the social context, and the development or refinement of NLP tools based on sociolinguistic insights. 540 1.2 Scope of the Discussion
Given the breadth of this field, we will limit the scope of this survey as follows. First of all, the coverage of sociolinguistics topi cs will be selective and primarily determined by the work within computational linguistics that touches on sociolinguistic topics. For readers with a wish for a more complete over view of sociolinguistics, we recommend the introductory readings by Bell (2013), Holmes (2013), and Meyerhoff (2011). mediated formats is one of the primary driving factors for the emergence of computational sociolinguistics. A relevant research area is therefore the study of computer-mediated communication (Herring 1996). Considering the strong focus on speech data within sociolinguistics, there is much potential for computational ap-proaches to be applied to spoken language as well. Moreover, the increased availability of recordings of spontaneous speech and tr anscribed speech has inspired a revival in the study of the social dimensions of spoken language (Jain et al. 2012), as well as in the analysis of the relation between the verbal a nd the nonverbal layers in spoken dialogues (Truong et al. 2014). As online data increasingly becomes multimodal X  X or example, with the popularity of vlogs (video blogs) X  X e expect the use of spoken word data for computational sociolinguistics to increa se. Furthermore, we expect that multimodal analysis, a topic that has been the focus of attention in the field of human X  X omputer interaction for many years, will also receive a ttention in computational sociolinguistics. often analyzed in context. Therefore, much of the work on language use in settings with multiple speakers draws from foundations in discourse analysis (Hyland 2004; Martin and White 2005; De Fina, Schiffrin, and Bamberg 2006; Schegloff 2007), pragmatics (such as speech act theory [Searle 1969; Austin 1975]), rhetorical structure theory (Mann and
Thompson 1988; Taboada and Mann 2006), and social psychology (Giles and Coupland 1991; Postmes, Spears, and Lea 2000; Richards 2006). For studies within the scope of computational sociolinguistics that build upon these fields the link with the founda-tional frameworks will be indicated. Another relevant field is computational stylometry (Holmes 1998; Stamatatos 2009; Daelemans 2013), which focuses on computational models of writing style for various tasks such as plagiarism detection, author profiling, and authorship attribution. Here we limit ou r discussion to publications on topics such as the link between style and social variables. 1.3 NLP Applications
Besides yielding new insights into language use in social contexts, research in compu-tational sociolinguistics could potentially also impact the development of applications for the processing of textual social media an d other content. For example, user profiling tools might benefit from research on automatically detecting the gender (Burger et al. 2011), age (Nguyen et al. 2013), geographical location (Eisenstein et al. 2010), or affilia-tions of users (Piergallini et a l. 2014) based on an ana lysis of their linguistic choices. The cases for which the interpretation of the language used could benefit most from using variables such as age and gender are usually also the ones for which it is most difficult to automatically detect those variables. Nevertheless, in spite of this kind of challenge, there are some published proofs of concept that suggest potential value in advancing past the typical assumption of homogeneity of language use embodied in current NLP tools. For example, incorporating how language use varies across social groups has improved word prediction systems (Stoop and van den Bosch 2014), algorithms for cyberbullying detection (Dad var et al. 2012), and sentiment-analysis tools (Volkova,
Wilson, and Yarowsky 2013; Hovy 2015). Hovy and S X gaard (2015) show that POS taggers trained on well-known corpora su ch as the English Penn Treebank perform better on texts written by older authors. Th ey draw attention to the fact that texts in various frequently used corpora are from a biased sample of authors in terms of demographic factors. Furthermore, many NLP tools currently assume that the input consists of monolingual text, but this assumption does not hold in all domains. For example, social media users may use multiple language varieties, even within a single message. To be able to automatically proces s these texts, NLP tools that are able to deal with multilingual texts are n eeded (Solorio and Liu 2008b). 2. Methods for Computational Sociolinguistics
As discussed, one important goal of this ar ticle is to stimulate collaboration between the fields of sociolinguistics in particular an d social science research related to commu-nication at large on the one hand, and computational linguistics on the other hand. By addressing the relationship with methods from both sociolinguistics and the social sci-ences in general we are able to underline two expectations. First of all, we are convinced that sociolinguistics and related fields can help the field of computational linguistics to build richer models that are more effective for the tasks they are or could be used for.
Second, the time seems right for the CL commu nity to contribute to sociolinguistics and the social sciences, not only by developing and adjusting tools for sociolinguists, but also by refining the theoretical models wi thin sociolinguistics using computational approaches and contributing to the understanding of the social dynamics in natural language. In this section, we highlight challe nges that reflect the current state of the field of computational linguistics. In part these challenges relate to the fact that in the field of language technologies a t large, the methodologies of social science research are usually not valued, and therefore also not taught. There is a lack of familiarity with methods that could easily be adopted if understood and accepted. However, there are promising examples of bridge-building that are already occurring in related fields such as learning analytics. More specifically, in t he emerging area of discourse analytics there are demonstrations of how these practices could eventually be observed within the language technologies community as well (Ros X  in press; Ros X  and Tovares 2015; Ros X  et al. 2008).
 ences in goals and values between communities, as these differences strongly influence what counts as a contribution within each fi eld, which in turn influences what it would mean for the fields to contribute to one another. Towards that end, we first discuss the related but distinct notions of reliability and validity, as well as the differing roles these notions have played in each field (Section 2.1). This will help lay a foundation for exploring differences in values and perspectives between fields. Here, it will be most convenient to begin with quantitative approaches in the social sciences as a frame of reference. In Section 2.2 we discuss contrasting notions of theory and empiricism as well as the relationship between the two, as that will play an important and complementary role in addressing the concern over differin g values. In Section 2.3 we broaden the scope to the spectrum of research approaches within the social sciences, including strong quantitative and strong qualitative approa ches, and the relationship between CL and the social disciplines involved. This will he lp to further specify the concrete challenges that must be overcome in order for a meaningful exchange between communities to take place. In Section 2.4 we illustrate how these issues come together in the role of data, as 542 the collection, sampling, and preparation of data are of central importance to the work in both fields. 2.1 Validation of Modeling Approaches
The core of much research in the field of computational linguistics, in the past decade especially, is the development of new meth ods for computational modeling, such as probabilistic graphical models and deep lear ning within a neural network approach.
These novel methods are valued both for the creativity that guided the specification of novel model structures and the corresponding requirement for new methods of inference as well as the achievement of predictive accuracy on tasks for which there is some notion of a correct answer.
 both within sociolinguistics (and the socia l sciences in general) and the CL community, and there is a lot of overlap with respect to the types of methods used. For example, logistic regression is widely utilized by vari ationist sociolinguists using a program called VARBRUL (Tagliamonte 2006). Similarly, logistic regression is widely used in the
CL community, especially in combination wit h regularization methods when dealing with thousands of variables, for example for age prediction (Nguyen et al. 2013). As another example, latent variable modeling approaches (Koller and Friedman 2009) have grown in prominence within the CL community for dimensionality reduction, manag-ing heterogeneity in terms of multiple domains or multiple tasks (Zhang, Ghahramani, and Yang 2008), and approximation of semantics (Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004). Similarly, it has grown in prominence within the quantitative branches of the social sciences for modeling causality (Glymour et al. 1987), managing heterogeneity in terms of group effects an d subpopulations (Co llins and Lanza 2010), and time series modeling (Rabe-Hesketh, Skrondal, and Pickles 2004; Rabe-Hesketh and Skrondal 2012).
 differences in values. Whereas in CL there is a value placed on creativity and predictive accuracy, within the social sciences the related notions of validity and reliability underline the values placed on conceptual contributio ns to the field. Validity is primarily a mea-sure of the extent to which a research design isolates a particular issue from confounds so that questions can receive clear answers. This typically requires creativity, and fre-quently research designs for isolating issues effectively are acknowledged for this cre-ativity in much the same way that a novel graphical model would be acknowledged for the elegance of its mathematical formulatio n. Reliability, on the other hand, is primarily a measure of the reproducibility of a result and might seem to be a distinct notion from predictive accuracy. However, the connection is apparent when one considers that a common notion of reliability is the extent to which two human coders would arrive at the same judgment on a set of data points, whereas predictive accuracy is the extent to which a model would arrive at the same judgment on a set of data points as a set of judgments decided ahead of time by one or more humans.
 values of the two communities, the differences in values signified by the emphasis on creativity and predictive accuracy on the one side and reliability and validity on the other side nevertheless pose challeng es for mutual exchange. Validity is a multi-faceted notion, and it is important to proper ly distinguish it from the related notion of reliability. If one considers shooting arrows at a target, one can consider reliability to be a measure of how much convergence is achieved in location of impact of multiple arrows. On the other hand, validity is the extent to which the point of convergence centers on the target. Reproducibility of results is highly valued in both fields, which requires reliability wherever human judgme nt is involved, such as in the production of a gold standard (Carletta 1996; Di Eugenio and Glass 2004). However, before techniques from CL will be adopted by social science researchers, standards of validation from the social sciences will likely need to be addre ssed (Krippendorff 2013) . We will see that this notion requires more than the related notion of creativity as appreciated within the field of CL.
 istheextenttowhichtheessencethatsomeconstructactuallycapturescorresponds to the intended quantity. This aspect of validity is referred to as face validity .For example, the face validity of a sentiment analysis tool could be tested as follows. First, an automatic measure of sentiment would be a pplied to a text corpus. Then, texts would be sorted by the resulting sentiment scores and the data points from the end points andmiddlecomparedwithoneanother.Aret here consistent and clear distinctions in sentiment between beginning, middle, and end? Is sentiment the main thing that is captured in the contrast, or is something different really going on? Although the CL community has frequently upheld high standards of reliability, it is rare to find work that deeply questions whether the models are measuring the right thing. Nevertheless, this deep questioning is core to high-quality work in the social sciences, and without it, the work may appear weak.
 imental design manages extraneous variance effectively. If the design fails to do so, it affects the interpretability of the result . This notion applies when we interpret the learned weights of features in our models to make statements about language use.
When not controlling for confounding variables, the feature weights are misleading and valid interpretation is not possible. For example, many studies on gender prediction (see Section 3) ignore extraneous variables such as age, whereas gender and age are known to interact with each other highly. Where confounds may not have been properly eliminated in an investigation, again, the results may appear weak regardless of the numbers associated with the measure of predictive accuracy.
 that if you look at the same object through different lenses, each of which is designed to accentuate and suppress different kinds of details, you get more information than if you looked through just one X  X his is analogous to the value obtained through the use of ensemble methods like bagging . Triangulation is thus an important way of strengthening research findings in the social sciences by leveraging multiple views simultaneously rather than just using one in addressing a question. Sentiment analysis can again be used for illustration purposes. Consider a blog corpus for which the age of each individual blogger is available. Let X  X  assume that a model for predicting age allocated high weights to some sentiment-related w ords. This may be considered as evidence that the model is consistent with previous findings that older people use more words that express a positive sentiment. Another method could measure sentiment for each blog individually. If the measured sentiment would correlate with the age of bloggers across the corpus, the two methods for inve stigating the connection between age and sentiment would tell the same story and the confidence in the validity of the story would increase. This type of confirming evidence is referred to as an indication of convergent validity .

For this example, assume that a particular model for predicting political affiliation 544 placed high weights on some sentiment-related words in a corpus related to issues for which those affiliated with one political pers pective would take a different stance than those affiliated with another perspective, and this affiliation is known for all data points.
The experimenters may conclude that this evidence is consistent with previous findings suggesting that voters express more positive sentiment towards political stances they are in favor of. If this is true, then if the model is applied to a corpus where both parties agree on a stance, the measure of sentiment should become irrelevant. Assuming the difference in the role of sentiment bet ween the corpora is consistent with what is expected, the interpretation is strengthened. This is referred to as divergent validity because an expected difference in relationship is confirmed. Seeking convergent and divergent validity is a mark of high quality work in the social sciences, but it is rare in evaluations in the field of CL, and with out it, again, the results may appear weak from a social science perspective. In order for methods from CL to be acceptable for use within the social sciences, these perceived weaknesses must be addressed. 2.2 Theory versus Empiricism
In the previous section we discussed the importance placed on validity within the social sciences that stems from the goal of isolating an issue in order to answer questions. In ordertoclarifywhythatisimportant,itisnecessarytodiscussthevalueplacedon theory versus empiricism.
 1990s. Initially, approaches that combined symbolic and statistical methods were of interest (Klavans and Resnik 1996). But with the focus on very large corpora and new frameworks for large-scale statistical modeling, symbolic-and knowledge-driven methods have been largely left aside, though the presence of linguistics as an active force can still be seen in some areas of computational linguistics, such as tree banking.
Along with older symbolic methods that required carefully crafted grammars and lexicons, the concept of knowledge source has become strongly associated with the notion of theory, which is consistent with th e philosophical notion of linguistic theory advocated by Chomskyan linguistics and other formal linguistic theories (Green 1992;
Backofen and Smolka 1993; Wintner 2002; Schneider, Dowdall, and Rinaldi 2004). As knowledge-based methods have to a large extent been replaced with statistical models, a grounding in linguistic theory has become less and less valued. A desire to replace theory with empiricism dominated the zeitgeist and drove progress within the field. Currently, the term theory seems to be associated with old and outdated approaches.
It often has a negative connotation in contra st to the positive reception of empiricism, and contemporary modeling approaches are b elieved to have a greater ability to offer insights into language than symbolic modeling frameworks.
 of the extent to which it contributes towards theory. Theories may begin with human-originated ideas. But these notions are only treated as valuable if they are confirmed through empirical methods. As these metho ds are applied, theoretical models gain empirical support. Findings are ratified a nd then accumulated. Therefore, theories become storehouses for knowledge obtained through empirical methods. Atheoretical empiricism is not attractive within the soc ial sciences, where the primary value is on building theory and engaging theory i n the interpretation of models.
 of values must be addressed in order to avoid the fields talking at cross purposes.
To stimulate collaboration between fields, it is important not only to focus on task performance, but also to integrate existing theories into the computational models and use these models to refine or develop new theories. 2.3 Quantitative versus Qualitative Approaches
The social sciences have both strong qualitative and quantitative branches. Similarly, sociolinguistics has branches in qualitative r esearch (e.g., interactional sociolinguis-tics) and quantitative research (variationist sociolinguistics). From a methodological perspective, most computational sociolingu istics X  work has a strong resemblance with quantitative and therefore variationist sociolinguistics, which has a strong focus on statistical analysis to uncover the distribution of sociolinguistic variables (Tagliamonte 2006). So far, we have mostly reflected on methods used in CL and their commonality with the methods used in the quantitative branches in sociolinguistics and the social sciences, but the time is right for a greater focus on how qualitative methods may also be of use. Some thoughts about what that might look like can be found in the work of Ros X  and Tovares (2015), who explore the productive tension between the two branches as it relates to interaction analysis. The field of computational linguistics could benefit from exploring this tension to a greater degree in its own work X  X or example, by taking a deeper look at data through human eyes as part of the validation of constructed models. the extent to which the agency of speakers i s taken into account. As explained in the
Introduction, linguistic agency refers to t he freedom of speakers to make choices about how they present themselves in interaction. A contrasting notion is the extent to which social structures influence the linguistic choices speakers make. Regardless of research tradition, it is acknowledged that speake rs both have agency and are simultaneously influenced by social structures. The quest ion is which is emphasized in the research approach. Quantitative researchers believe that the most important variance is captured by representation of the social structure. They recognize that this is a simplification, but the value placed on quantification for the purpose of identifying causal connections between variables makes the sacrifice of a ccuracy worth it. In the field of CL, this valuing is analogous to the well-known saying that all models are wrong, but some are nevertheless useful. On the other side are researchers committed to the idea that the most important and interesting aspects of language use are the ones that violate norms in order for the speaker to achieve a goal. These researchers may doubt that the bulk of choices made by speakers can be accounted for by social structures. We see the balance and tension between the ideas of lang uage reflecting established social struc-tures and language arising from speaker agency within current trends in variationist sociolinguistics. Much of that work focused on the ways in which language variation can be accounted for by reference to social structures (Bell 2013). On the other hand, more recently, the agency of speakers is playing a more central role as well in variationist sociolinguistics (Eckert 2012).
 work that lacks rigor, one could argue that high-quality qualitative research has a separate notion of rigor and depth that is all its own (Morrow and Brown 1994). An im-portant role for qualitative research is to cha llenge the operationalizations constructed by quantitative researchers. To achiev e the adoption of CL methods and models by social science researchers, the challenges from the qualitative branches of the social sciences will become something to consider carefully.
 many studies within computational sociolinguistics also focus on the influence of social 546 structures. For example, work on predicting social variables such as gender (Section 3) is built on the idea that gender determine s the language use of speakers. However, such research ignores the agency of speakers: Speakers use language to construct their identity and thus not everyone might write in a way that reflects their biological sex.
Moving forward, it would make sense for resea rchers in computational sociolinguistics to reflect on the dominant role of social structures over agency. Some work in CL has already begun to acknowledge the agenc y of speakers when interpreting findings (Bamman, Eisenstein, and Schnoebelen 2014; Nguyen et al. 2014).
 els in the two fields is to reconsider the trade-off between maximizing interpretability X  typical of the social sciences and sociolinguis tics X  X nd maximizing predictive accuracy, typical of CL. Both fields place a premium on rigor in evaluation and generalization of results across data sets. To maintain a certain standard of rigor, the CL community has produced practices for standardization of metrics, sampling, and avoidance of overfit-ting or overestimation of performance through careful separation of training and testing data at all stages of model development. Within the social sciences, the striving for rigor has also produced statistical machinery for analysis, but most of all it has resulted in an elaborate process for validation of such modeling approaches and practices for careful application and interpretation of the results.
 models tend to be kept small and simple in terms of the number of parameters, fre-quently no more than 10, or at least no more than 100. Because the models are kept simple, they can be estimated on smaller dat a sets, as long as sampling is done carefully and extraneous variance is controlled. In the CL community, it is more typical for models to include tens of thousands of parameters or more. For such large models, massive corpora are needed to prevent overfitting. As a result, research in the CL community is frequently driven by the availa bility of large corpora, which explains the large number of recent papers on data from the Web, such as Twitter and Wikipedia.
Because of this difference in scale, a major focus on parallelization and approximate inference has been an important focus of work in CL (Heskes, Albers, and Kappen 2002), whereas interest in such methods has only recently grown within the social sciences. 2.4 Spotlight on Corpora and Other Data
Data collection is a fundamental step in the research cycle for researchers in both sociolinguistics and computational linguis tics. Here we will reflect on the differences in the practices and traditions within both fields and on the emerging use of online data. In the subsequent sections of this survey, there will be dedicated subsections about the data sources used in the specific studies relevant to the discussed themes (e.g., on identity construction).
 speech (also referred to as the vernacular ), that is, the kind of language used when speakers are not paying attention (Tagliamo nte 2006). A variety of methods have been used to collect data, including observatio n, surveys, and inter views (Tagliamonte 2006;
Mallinson, Childs, and H erk 2013). The soci olinguistic data sets are carefully prepared to enable in-depth analyses of how a speech community operates, carefully observing standards of reliability and validity as discussed previously. Inevitably, these data col-lection methods are labor-intensive and tim e-consuming. The resulting data sets are often small in comparison with the ones use d within computational linguistics. The small sizes of these data sets made the work in sociolinguistics of limited interest to the field of CL.

Herring (2007) defines CMC as  X  X redominantly text-based human X  X uman interaction mediated by networked computers or mobile telephony. X  The content generated in
CMC, and in particular when generated on social media platforms, is a rich and easy-to-access source of large amounts of informal la nguage coming together with information about the context (e.g., the users, social network structure, the time or geolocation at which it was generated) that can be used for the study of language in social contexts on a large scale. Examples include microblogs (Kooti et al. 2012; Eisenstein et al. 2014),
Web forums (Nguyen and Ros X  2011; Garley and Hockenmaier 2012), and online review sites (Danescu-Niculescu-Mizil et al. 2013b; Hovy, Johannsen, and S X gaard 2015). For example, based on data from Twitter (a popular microblogging site) dialectal variation has been mapped using a fraction of the time, costs, and effort that was needed in traditional studies (Doyle 2014). However, data from CMC are not always easy to collect. As an example, although text messa ging (SMS) is widely used, collecting SMS data has been difficult due to both technical and privacy concerns. The SMS4science project (D X rscheid and Stark 2011) aims to overcome these difficulties by asking people to donate their messages, collaborating with the service providers for the collection of the messages, and applying anonymization to ensure privacy.
 adjust their language use towards the expectations of the data collector. This phe-nomenon is known as the  X  X bserver X  X  paradox, X  a term first coined by Labov (1972):  X  X he aim of linguistic research in the community must be to find out how people talk when they are not being systematically observed; yet we can only obtain these data by systematic observation." In social media, t he observer X  X  paradox could potentially be argued to have lost much of its strength, ma king it a promising resource to complement traditional data collection methods. Altho ugh a convenient source of data, the use of social media data does introduce new challenges that must be addressed regardless of field, and this offers a convenient beginning to a potential exchange between fields. (Mislove et al. 2011; Nguyen et al. 2013). A better understanding of the demographics could aid the interpretation of findings, but often little is known about the users.
Collecting demographic info rmation requires significant effort, or might not even be possible in some cases because of ethical concerns. Furthermore, in many cases the complete data are not fully accessible through an API, requiring researchers to apply a sampling strategy (e.g., randomly, by topi c, time, individuals/groups, phenomenon [Herring 2004; Androutsopoulos 2013]). Sampling may introduce additional biases or remove important contextual information. These problems are even more of a concern when data sets are reused for secondary analysis by other researchers whose purposes might be very different from th ose who performed the sampling.
 threads) that do not correspond entirely with traditional analysis units (such as sen-tences and turns) (Androutsopoulos 2013). This raises the question about valid appli-cation of findings from prior work. Another complicating factor is that in social media the target audience of a message is often not ex plicitly indicated X  X amely, multiple au-diences (e.g., friends, colleagues) are co llapsed into a single context (Marwick and boyd 2011). Some studies have therefore treated the use of hashtags and user mentions as proxies for the target audience (Nguyen, T rieschnigg, and Cornips 2015; Pavalanathan and Eisenstein 2015a). Furthermore, although historically the field of sociolinguistics 548 started with a major focus on phonological variation (e.g., Labov 1966), the use of social media data has led to a higher focus on lexical variation in computational so-ciolinguistics. However, there are concern s that a focus on lexical variation without regard to other aspects may threaten the validity of conclusions. Phonology does impact social media orthography at both the word level and structural level (Eisenstein 2013a), suggesting that studies on phonological variation could inform studies based on social media text data and vice versa. For example, Eisenstein (2013a) found that consonant cluster reduction (e.g., just vs. jus ) in Twitter is influenced by the phonological context, in particular, reduction was less likely when the word was followed by a segment that began with a vowel.
 often been conceptualized as either public or private, in reality this distinction is not as absolute; for example, a user might discuss a private topic on a public social media site. In view of the related privacy issues, Bolander and Locher (2014) argue for more awareness regarding the ethical implications of research using social media data. other types of data that have been used within computational linguistics. Many devel-oped tools (e.g., parsers, named entity recognizers) do not work well because of the informal nature of many social media texts. Although the dominant response has been to focus on text normalization and domain adaptation, Eisenstein (2013b) argues that doing so is throwing away meaningful variation. For example, building on work on text normalization, Gouws et al. (2011) showed how various transformations (e.g., dropping the last character of a word) vary across different user groups on Twitter. As another example, Brody and Diakopoulos (2011) find that lengthening of words (e.g., cooooll )is often applied to subjective wor ds. They build on this observation to detect sentiment-bearing words. The tension between norma lizing and preserving the variation in text also arises in the processing and analysis of historical texts (see Piotrowski [2012] for an overview), which also contain many spelling v ariations. In this domain, normalization is often applied as well to facilitate the use of tools such as parsers. However, some approaches first normalize the text, but th en replace the modernized word forms with the original word forms to retain the original text. Another issue with social media data is that many social media studies have so far focused primarily on one data source. A comparison of the online data sources in terms of language use has only been done in a few studies (Baldwin et al. 2013; Hu, Talamadupula, and Kambhampati 2013).
 perspective is crowdsourcing. So far, crowdsourcing is mostly used to obtain large numbers of annotations (e.g., Snow et al. 2008). However,  X  X rowds X  can also be used for large-scale perception studies (i.e., to s tudy how non-linguists interpret messages and identify social characteristics of speakers [Clopper 2013]), and for the collection of linguistic data, such as the use of variants of linguistic variables. Within sociolinguistics, surveys have been one of the instruments to collect data and crowdsourcing is an emerging alternative to traditional methods for collecting survey data.
 research X  X or example, to study how English utterances are perceived differently across language communities (Makatchev and S immons 2011) and to obtain native-likeness ratings of speech samples (Wieling et al. 2014). For some studies, games have been developed to collect data. Nguyen et al. (2014) studied how Twitter users are perceived based on their tweets by asking players to guess the gender and age based on displayed tweets. Leemann et al. (2016) developed a mobile app that predicted the user X  X  location based on a 16-question survey. By also collecting user feedback on the predictions, the authors compared their data with the Linguis tic Atlas of German-speaking Switzerland, which was collected about 70 years before the crowdsourcing study. The mismatches between the Atlas data and self-reported data from the mobile app were seen to suggest linguistic change in progress.
 is less controlled and additional effort for quality control is often needed. Even more problematic is the fact that usually little is known about the workers, such as the com-munities they are part of. For example, Wieling et al. (2014) recruited participants using e-mail, social media, and blogs, which resulted in a sample that was likely to be biased towards linguistically interested people. H owever, they did not expect that the possible bias in the data influenced the findings much. Another concern is that participants in crowdsourcing studies might modulate t heir answers towards what they think is expected, especially when there is a monetary compensation. In the social sciences in general, crowdsourcing is also increasingly used for survey research. Behrend et al. (2011) compared the data collected using crowdsourcing with data collected from a tra-ditional psychology participant pool (under graduates) in the context of organizational psychology research and concluded that crow dsourcing is a potentially viable resource to collect data for this research area. Alth ough promising, the number of studies so far using crowdsourcing for sociolinguistic research is small and more research needs to be done to study the strengths and weaknesses of this data collection method for sociolinguistic research. 3. Language and Social Identity
We now turn to discussing computational approaches for modeling language varia-tion related to social identity. Speakers use language to construct their social identity (Bucholtz and Hall 2005). Being involved in communicative exchange can be functional for the transfer of information, but at the same it functions as a staged performance in which users select specific codes (e.g., la nguage, dialect, style) that shape their communication (Wardhaugh 2011). Consciously or unconsciously, speakers adjust their performance to the specific social context and to the impression they intend to make on their audience. Each speaker has a persona l linguistic repertoire to draw linguistic elements or codes from. Selecting from the re pertoire is partially subject to  X  X dentity work, X  a term referring to the range of activities that individuals engage in to create, present, and sustain personal identities th at are congruent with and supportive of the self-concept (Snow and Anderson 1987).
 but there are limitations (e.g., physical or genetic constraints) to the variation that can be achieved. For example, somebody w ith a smoker X  X  voice may not be able to speak with a smooth voice but many individual characteristics still leave room for variation. Although traditionally attributed an absolute status, personal features (e.g., age and gender) are increasingly considered social rather than biological variables.
Within sociolinguistics, a major thrust of research is to uncover the relation between social variables (e.g., gender, age, ethnicity, status) and language use (Eckert 1997;
Holmes and Meyerhoff 2003; Wagner 2012; Eckert and McConnell-Ginet 2013). The concept of sociolects , or social dialects, is similar to t he concept of regional dialects.
Where regional dialects are language variet ies based on geography, sociolects are based on social groups X  X or example, different groups according to social class (with labels such as  X  X orking class X  and  X  X iddle class X ), or according to gender or age. A study by Guy (2013) suggests that the cohesion between variables (e.g., nominal agreement, 550 denasalization) to form sociolects is weaker than usually assumed. The unique use of language by an individual is an idiolect , and this concept is in particular relevant for authorship attribution (e.g., Grieve 2007).
 tional linguistics have focused on automatica lly inferring social variables from text. This task can be seen as a form of automatic metada ta detection that can provide information on author features. The growing interest in trend analysis tools is one of the drivers for the interest in the development and refinement of algorithms for this type of metadata detection. However, tasks such as gender and age prediction do not only appeal to researchers and developers of trend mining tools. Various public demos have been able to attract the attention of the general public (e.g., TweetGenie and Meder 2014] and Gender Guesser 3 ), which can be attributed to a widespread interest in the entertaining dimension of th e linguistic dimension of identity work. The automatic prediction of individual features such as age and gender based on only text is a nontrivial task. Studies that have com pared the performance of humans with that of automatic systems for gender and age prediction based on text alone found that automatic systems perform better than humans (Burger et al. 2011; Nguyen et al. 2013).
A system based on aggregating guesses from a large number of people still predicted gender incorrectly for 16% of the Twitter users (Nguyen et al. 2014). Although most studies use a supervised learning approach, a recent study by Ardehaly and Culotta (2015) explored a lightl y supervised approach using soft constraints. They combined unlabeled geotagged Twitter data with soft constraints, such as the proportion of people younger or older than 25 years in a county according to Census data, to train their classifiers.
 geographical location have received the mos t attention, compared with other variables such as ethnicity (Pennacchiotti and Popescu 2011; Rao et al. 2011; Ardehaly and Culotta 2015) and social class. Labels for variables like social class are more difficult to obtain and use because they are rarely made explic it in online user profiles that are publically available. Only recently has this direction been explored, with occupation as a proxy for variables like social class. Occupation labe ls for Twitter users have been extracted from their profile description (Preo  X  tiuc-Pietro, Lampos, and Aletras 2015; Preo  X  tiuc-Pietro et al. 2015; Sloan et al. 2015). Preo  X  tiuc-Pietro et al. (2015) then mapped the derived occupations to income and Sloan et al. (2015) mapped the occupations to social class categories. However, these studies were limited to users with self-reported occupations in their profiles.
 not independent. For example, there are indications that linguistic features that are used more by men increase in frequency with age as well (Argamon et al. 2007). As another example, some studies have suggested that language variation across gender tends to be stronger among younger people and to fade away with older ages (Barbieri 2008).
Eckert (1997) notes that the age considered appropriate for cultural events often differs for men and women (e.g., getting married), which influences the interaction between gender and age. The interaction between thes e variables is further complicated by the fact that in many uncontrolled settings the gender distribution may not be equal for different age ranges (as observed in blogs [Burger and Henderson 2006] and Twitter [Nguyen et al. 2013]). Therefore, failing to c ontrol for gender while studying age (and vice versa) can lead to misinterpretation of the findings.
 variation related to social identity. This s ection will first focus on the data sets that have been used to investigate social identity and language variation in computational linguistics (Section 3.1). After surveying computational studies on language variation according to gender (Section 3.2), age (Section 3.3), and location (Section 3.4), we con-clude with a discussion of how various NLP tasks, such as sentiment detection, can be improved by accounting for language variation related to the social identity of speakers (Section 3.5). 3.1 Data Sources
Early computational studies on social identity and language use were based on for-mal texts, such as the British National Corpus (Koppel, Argamon, and Shimoni 2002;
Argamon et al. 2003), or data sets collected from controlled settings, such as recorded conversations (Singh 2001) and telephone conversations (Boulis and Ostendorf 2005;
Garera and Yarowsky 2009; Van Durme 2012), where protocols were used to coordinate the conversations (such as the topic). With the advent of social media, a shift is observed towards more informal texts collected from uncontrolled settings. Much of the initial work in this domain focused on blogs. The Blog Authorship Corpus (Schler et al. 2006), collected in 2004 from blogger.com, has been used in various studies on gender and age (Argamon et al. 2007; Goswami, Sarkar, and Rustagi 2009; Gianfortoni, Adamson, and
Ros X  2011; Nguyen, Smith, and Ros X  2011; Sap et al. 2014). Others have created their own blog corpus from various sources including LiveJournal and Xanga (Burger and
Henderson 2006; Nowson and Oberlander 2006; Yan and Yan 2006; Mukherjee and Liu 2010; Rosenthal and McKeown 2011; Sara wgi, Gajulapalli, and Choi 2011).
 than blogs. Burger et al. (2011) created a large corpus by following links to blogs that contained author information provided by the authors themselves. The data set has been used in various subsequent studies (Van Durme 2012; Bergsma and Van Durme 2013; Volkova, Wilson, and Yarowsky 2013). Others created their own Twitter data set (Rao et al. 2010; Eisenstein, Smith, and Xing 2011; Zamal, Liu, and Ruths 2012; Kokkos and Tzouramanis 2014; Liao et al. 2014). Where as early studies focused on English, re-cent studies have used Twitter data written i n other languages as well, for example,
Dutch (Nguyen et al. 2013), Spanish and Russian (Volkova, Wilson, and Yarowsky 2013), and Japanese, Indonesian, Turkish, and French (Ciot, Sonderegger, and Ruths 2013).
Besides blogs and Twitter, other Web sources have been explored, including LinkedIn (Kokkos and Tzouramanis 2014), IMDb (Otterbacher 2010), YouTube (Filippova 2012), e-mails (Corney et al. 2002), a Belgian social n etwork site (Peersman, Daelemans, and Vaerenbergh 2011), and Facebook (Rao et al. 2011; Sap et al. 2014; Schwartz et al. 2013). data sets to study the relation between social variables and language use.

Labeling. Data sets derived from uncontrolled se ttings such as social media often lack explicit information regarding the identit y of users, such as their gender, age, or loca-tion. Researchers have used different strategies to acquire adequate labels: 552 Sample selection. In many cases, it is necessary to limit the study to a sample of persons.
Sometimes the selected sample is directly related to the way labels are obtained, for example, by only including people who explic itly list their gender or age in their social media profile (Burger et al. 2011), who have a gender-specific first name (Bamman,
Eisenstein, and Schnoebelen 2014), or who have geotagged tweets (Eisenstein et al. 2010). Restricting the sample (e.g., by only including geotagged tweets) could poten-tially lead to biased data sets. Pavalanathan and Eisenstein (2015b) compared geotagged tweets with tweets written by users with self-reported locations in their profile. They found that geotagged tweets are more often written by women and younger peo-ple. Furthermore, geotagged tweets contain more geographically specific non-standard words. Another approach is random sampling, or as random as possible due to restric-tions of targeting a specific language (Nguyen et al. 2013). However, in these cases the labels may not be readily available. This increases the annotation effort and in some cases it may not even be possible to obtain reliable labels. Focused sampling is used as well, for example, by starting with social media accounts related to gender-specific behavior (e.g., male/female hygiene produc ts, sororities) (Rao et al. 2010). However, such an approach has the danger of creating biased data sets, which could influence the prediction performance (Cohen and Ruths 2013). 3.2 Gender
The study of gender and language variation has received much attention in sociolin-guistics (Holmes and Meyerhoff 2003; Eckert and McConnell-Ginet 2013). Various stud-ies have highlighted gender differences . According to Tannen (1990), women engage more in  X  X apport X  talk, focusing on establishing connections, whereas men engage more in  X  X eport X  talk, focusing on exchanging information. Similarly, according to Holmes (1995), in women X  X  communication the social function of language is more salient, whereas in men X  X  communication the referential function (conveying information) tends to be dominant. Argamon et al. (2003) make a distinction between involvedness (more associated with women) and informational (more associated with men). However, with the increasing view that speakers use language to construct their identity, such general-izations have also been met with criticism. Many of these studies rely on small sample sizes and ignore other variables (such as ethnicity, social class) and the many similarities between genders. Such generalizations contribute to stereotypes and the view of gender as an inherent property. 3.2.1 Modeling Gender. Within computational linguistics, researchers have focused primarily on automatic gender classification based on text. Gender is then treated as a binary variable based on biological characteri stics, resulting in a binary classification task. A variety of machine learning methods have been explored, including SVMs (Corney et al. 2002; Boulis and Ostendorf 2005; Nowson and Oberlander 2006; Mukherjee and Liu 2010; Rao et al. 2010; Gianfortoni, Adamson, and Ros X  2011; Peersman, Daelemans, and Vaerenbergh 2011; Fink, Kopecky, and Morawski 2012;
Zamal, Liu, and Ruths 2012; Ciot, Sonderegger, and Ruths 2013), logistic regression (Otterbacher 2010; Bergsma and Van Durme 2013), naive Bayes (Yan and Yan 2006;
Goswami, Sarkar, and Rustagi 2009; Mukherjee and Liu 2010), and the Winnow algorithm (Schler et al. 2006; Burger et al. 2011). However, treating gender as a binary variable based on biological characteristics assumes that gender is fixed and is something people have , instead of something people do (Butler 1990), that is, such a set-up neglects the agency of speakers. Many socio linguists, together with scholars from the social sciences in general, view gender as a s ocial construct, emphasizing that gendered behavior is a result of social conventions rath er than inherent biological characteristics. 3.2.2 Features and Patterns. Rather than focusing on the underlying machine learning models, most studies have focused on developing predictive features. Token-level and character-level unigrams and n -grams have been explored in various studies (Yan and Yan 2006; Burger et al. 2011; Sarawgi, Gajulapalli, and Choi 2011; Fink, Kopecky, and Morawski 2012; Bamman, Eisenstein, and Schnoebelen 2014). Sarawgi, Gajulapalli, and
Choi (2011) found character-level language models to be more robust than token-level language models. Grouping words by meaningf ul classes could improve the interpreta-tion and possibly the performance of models. Linguistic inquiry and word count (LIWC;
Pennebaker, Francis, and Booth 2001) is a dictionary-based word counting program originally developed for the English language. It also has versions for other languages, such as Dutch (Zijlstra et al. 2005). LIWC has been used in experiments on Twitter data (Fink, Kopecky, and Morawski 2012) and blogs (Nowson and Oberlander 2006;
Schler et al. 2006). However, models based on LIWC alone tend to perform worse than unigram/ n -gram models (Nowson and Oberlander 2006; Fink, Kopecky, and Morawski 2012). By analyzing the developed features, studies have shown that men tend to use more numbers (Bamman, Eisenstein, and Schnoebelen 2014), technology words 554 (Bamman, Eisenstein, and Schnoebelen 2014), and URLs (Schler et al. 2006; Nguyen et al. 2013), whereas women use more terms referring to family and relationship issues (Boulis and Ostendorf 2005). A discussion of the influence of genre and domain on gender differences is provided later in this section.
 tures capturing individual POS frequencies (Argamon et al. 2003; Otterbacher 2010) as well as POS patterns (Argamon et al. 2003; Schler et al. 2006; Argamon et al. 2009;
Bamman, Eisenstein, and Schnoebelen 2014). Men tend to use more prepositions (Schler et al. 2006; Argamon et al. 2007, 2009; Otterbacher 2010) and more articles (Nowson and
Oberlander 2006; Schler et al. 2006; Argamon et al. 2007; Otterbacher 2010; Schwartz et al. 2013), although Bamman, Eisenstein, and Schnoebelen (2014) did not find these differences to be significant in their Twitte r study. Women tend to use more pronouns (Argamon et al. 2003; Schler et al. 2006; Argamon et al. 2007, 2009; Otterbacher 2010;
Schwartz et al. 2013; Bamman, Eisenstein, and Schnoebelen 2014), in particular first person singular (Otterbacher 2010; Nguyen et al. 2013; Schwartz et al. 2013). A mea-sure introduced by Heylighen and Dewaele (2002) to measure for mality based on the frequencies of different word classes has been used in experiments on blogs (Nowson,
Oberlander, and Gill 2005; Mukherjee and Liu 2010). Sarawgi, Gajulapalli, and Choi (2011) experimented with prob abilistic context-free grammars (PCFGs) by adopting the approach proposed by Raghavan, Kovashka, and Mooney (2010) for authorship attribution. They trained PCFG parsers fo r each gender and computed the likelihood of test documents for each gender-specific PCFG parser to make the prediction. Bergsma,
Post, and Yarowsky (2012) experimented with three types of syntax features and found features based on single-level context-free-grammar (CFG) rules (e.g., the most effective. In some languages such as French, the gender of nouns (including the speaker) is often marked in the syntax. For example, a man would write je suis all X  , whereas a woman would write je suis all X e ( X  X  went X ). By detecting such je suis constructions, Ciot, Sonderegger, and Ruths (2013) improved performance of gender classificationinFrench.
 men tend to use longer words, sentences, and texts (Singh 2001; Goswami, Sarkar, and Rustagi 2009; Otterbacher 2010), and more swear words (Boulis and Ostendorf 2005; Schwartz et al. 2013). Women use more emotion words (Nowson and Oberlander 2006;
Schwartz et al. 2013; Bamman, Eisenstein, and Schnoebelen 2014), emoticons (Rao et al. 2010; Gianfortoni, Adamson, and Ros X  2011; Bamman, Eisenstein, and Schnoebelen 2014; Volkova, Wilson, and Yarowsky 2013), and typical social media words such as omg and lol (Schler et al. 2006; Bamman, Eisenstein, and Schnoebelen 2014).
 maiden names. Bergsma and Van Durme (2013) used such distinguishing attributes, extracted from common nouns for men and women (e.g., granny, waitress), to improve classification performance. Features based on first names have also been explored.
Although not revealing much about language use itself, they can improve prediction performance (Burger et al. 2011; Rao et al. 2011; Bergsma and Van Durme 2013).
Genre. So far, not many studies have analyzed the influence of genre and domain (Lee 2001) on language use, but a better understanding will aid the interpretation of observed language variation patterns. Using data from the British National Corpus, Argamon et al. (2003) found a strong correlation between characteristics of male and nonfiction writing and likewise, between female and fict ion writing. Based on this observation, they trained separate prediction models for fiction and nonfiction (Koppel, Argamon, and Shimoni 2002). Building o n these findings, Herring an d Paolillo ( 2006) investigated whether gender differences would still be ob served when controlling for genre in blogs.
They did not find a significant relation between gender and linguistic features that were identified to be associated with gender in previous literature, although the study was based on a relatively small sample. Similarly, Gianfortoni, Adamson, and Ros X  (2011) revisited the task of gender prediction on th e Blog Authorship Corpus. After controlling for occupation, features that previously were found to be predictive for gender on that corpus were no longer effective.
 prediction models by training and testing on different data sets. Although models tend to perform worse when tested on a different data set than the one used for training, studies have shown that prediction performa nce is still higher than random, suggesting that there are indeed gender-specific patte rns of language variation that go beyond genre and domain (Sarawgi, Gajulapalli, and Choi 2011; Sap et al. 2014). Gianfortoni,
Adamson, and Ros X  (2011) proposed the use of  X  X tretchy patterns, X  flexible sequences of categories, to model stylistic variation and to improve generalizability across domains.
Social Interaction. Most computational studies on gender-specific patterns in language use have studied speakers in isolation. Because the conversational partner network influence the language use of speakers, several studies have extended their focus by also considering contextual factors. For example, this led to the finding that speakers use more gender-specific language in same-gender conversations (Boulis and
Ostendorf 2005). On the Fisher and Switchboard corpus (telephone conversations), classifiers dependent on the gender of the conversation partner improve performance (Garera and Yarowsky 2009). However, exploiting the social network of speakers on
Twitter has been less effective so far. Features derived from the friends of Twitter users did not improve gender classification (but it was effective for age) (Zamal, Liu, and Ruths 2012). Likewise, Bamman, Eisenstein, and Schnoebelen (2014) found that social network information of Twitter users did not improve gender classification when enough text was available.
 gender classification itself. Some have used gender as a variable when studying other phenomena. In a study on language and power, Prabhakaran, Reid, and Rambow (2014) showed how the gender composition of a group influenced how power is manifested in the Enron corpus, a large collection of e-mails from Enron employees (described in more detail in Section 4.1). In a study on language change in online communities,
Hemphill and Otterbacher (2012) found tha t women write more like men over time in the IMDb community (a movie review site), which they explain by men receiving more prestige in the community. Jurafsky, Ranganath, and McFarland (2009) automatically classified speakers according to interactional style (awkward, friendly, or flirtatious) using various types of features, including l exical features based on LIWC (Pennebaker,
Francis, and Booth 2001), prosodic, and discourse features. Differences, as well as commonalities, were observed between genders, and incorporating features from both speakers improved classification performance. 3.2.3 Interpretation of Findings. As mentioned before, most computational approaches adopt a simplistic view of gender as an i nherent property based on biological 556 characteristics. Only recently has the comp utational linguistics community noticed the limitations of this simplistic view by acknowledging the agency of speakers. Two of these studies based their argumentation on an analysis of the social networks of the users. Automatic gender predictions on YouTube data correlated more strongly with the dominant gender in a user X  X  network t han the user-reported gender (Filippova 2012). Likewise, in experiments by Bamman, Eisenstein, and Schnoebelen (2014), incorrectly labeled Twitter users also had f ewer same-gender connections. In addition, they identified clusters of users who used linguistic markers that conflicted with general population-level findings. Anoth er study was based on data collected from an online game (Nguyen et al. 2014). Thousands of players guessed the age and gender of
Twitter users based on their tweets, and the results revealed that many Twitter users do not tweet in a gender-stereotypical way.
 average used more by men or women, individual speakers may diverge from the stereo-typical images that tend to be highlighted by many studies. In addition, gender is shaped differently depending on the culture and language, and thus presenting gender as a universal social variable can be misleadin g. Furthermore, linguistic variation within speakers of the same gender holds true as well. 3.3 Age
Aging is a universal phenomenon and understanding the relation between language and age can provide interesting insights in many ways. An individual at a specific time represents a place in history as well as a life stage (Eckert 1997), and thus observed patterns can generate new insights into la nguage change as well as how individuals change their language use as they move throug h life. Within computational linguistics, fewer studies have focused on language variation according to age than studies focusing on gender, possibly because obtaining age labels requires more effort than gender labels (e.g., the gender of people can often be derived from their names; cf. Section 3.1). Most of these studies have focused on absolute chronological age, although age can also be seen as a social variable like gender.
 forms, because at a young age the group pressu re to not conform to established societal conventions is the largest (Eckert 1997; Holmes 2013). In contrast, adults are found to use the most standard language, because for them social advancement matters and they use standard language to be taken seriously (Eckert 1997; Bell 2013). These insights can explain why predicting the ages of older people is harder (e.g., distinguishing between a 15-and a 20-year-old person based on their la nguage use is easier than distinguishing between a 40-and a 45-year-old person [Nguyen et al. 2013; Nguyen et al. 2014]).
Thus, age is an important variable to conside r, especially when we consider processes relevant for language evolution, since the degree of language innovation varies by age (Labov 2001). 3.3.1 Modeling Age. Afundamentalquestionis how to model age, and so far researchers have not yet reached a consensus. Eckert (1997) distinguishes between chronological age (number of years since birth), biological age (physical maturity), and social age (based on life events). Speakers are often grouped according to their age, because the amount of data is in many cases not sufficient to make more fine-grained distinctions (Eckert 1997). Most studies consider chronological age and group speakers based on age spans (Labov 1966; Trudgill 1974; Barbieri 2008). However, chronological age can be misleading because persons with the same chronological age may have had very different life experiences. Another approach is to group speakers according to  X  X hared experiences of time, X  such as high school students (Eckert 1997).
 specific language use based on the chrono logical age of speakers. An exception is
Nguyen et al. (2013), who explored classification into life stages. However, even when focusing on chronological age, the task can be framed in different ways as well.
Chronological age prediction has mostly been approached as a classification problem, by modeling the chronological age as a categorical variable. Based on this task formulation, various classical machine learning models have been used, such as SVMs (Rao et al. 2010; Peersman, Daelemans, and Vaerenbergh 2011), logistic regression (Rosenthal and McKeown 2011; Nguyen et al. 2013), and naive Bayes (Tam and Martell 2009).
 and experimental set-up. Experiments on the blog authorship corpus (Schler et al. 2006) used categories based on the following age spans: 13 X 17, 23 X 27, and 33 X 47, removing the age ranges in between to simplify the task. Rangel et al. (2013) adopted this approach in the Author Profiling task at PAN 2013. The following year, the difficulty of the task at PAN 2014 was increased by considering the more fine-grained categories of 18 X 24, 25 X 34, 35 X 49, 50 X 64 and 65+ years (Rangel et al. 2014). Zamal, Liu, and Ruths (2012) classified Twitter users into 18 X 23 and 25 X 30 years. Other studies explored boundaries at 30 (Rao et al. 2010), at 20 and 40 (Nguyen et al. 2013), at 40 (Garera and Yarowsky 2009), and at 18 years (Burger and Henderson 2006).
 aries. Peersman, Daelemans, and Vaerenbergh (2011) experimented with binary classi-fication and boundaries at 16, 18, and 25 years. Tam and Martell (2009) experimented with classifying teens versus 20s, 30s, 40s, 50s, and adults. Not surprisingly, in both studies a higher performance was obtained when using larger age gaps (e.g., teens versus 40s/50s) than when using smaller age gaps (e.g., teens versus 20s/30s) (Tam and
Martell 2009; Peersman, Daelemans, and Vaerenbergh 2011). Rosenthal and McKeown (2011) explored a range of splits to study differences in performance when predicting the birth year of blog authors. They related t heir findings to pre X  and post X  X ocial media generations.

However, it does have several limitations. First, selecting age boundaries has proven to be difficult. It is not always clear which catego ries are meaningful. Secondly, researchers have used different categories depending on the age distribution of their data set, which makes it difficult to make comparisons across data sets.
 variable, removing the need to define age categories. Framing age prediction as a regression task, a frequently used method has been linear regression (Nguyen, Smith, and Ros X  2011; Nguyen et al. 2013; Schwartz et al. 2013; Sap et al. 2014). Liao et al. (2014) experimented with a latent variable model that jointly models age and topics. In their model, age-specific topics obtain low standa rd deviations of age, whereas more general topics obtain high standard deviations. A nother approach that would remove the need to define age categories is the unsupervised i nduction of age categories. Analyzing the discovered age groups could shed more light on the relation between language use and age, but we are not aware of existing research in this area. 3.3.2 Features and Patterns. The majority of studies on age prediction have focused on identifying predictive features. Although some features tend to be effective across 558 domains, others are domain-specific (Nguyen, Smith, and Ros X  2011). Features that characterize male speech have been found to also increase with age (Argamon et al. 2007), thus, simply said, men tend to sound older than they are.

Daelemans, and Vaerenbergh 2011; Nguyen et al. 2013). Features based on part of speech are effective as well. For example, younger people tend to use more first-and second-person singular pronouns (e.g., I, you ), while older people more often use first person plural pronouns (e.g., we ) (Barbieri 2008; Rosenthal and McKeown 2011; Nguyen et al. 2013). Older people also use more prepositions (Argamon et al. 2009; Nguyen et al. 2013), determiners (Argamon et al. 2009; Nguyen, Smith, and Ros X  2011), and articles (Schwartz et al. 2013). Most of t hese studies focused on English and therefore some of these findings might not be applicable to othe r languages. For example, the effectiveness of pronoun-related features should also be s tudied in pro-drop languages (e.g., Turkish and Spanish).
 use more alphabetical lengthening (e.g., niiiice ) (Rao et al. 2010; Nguyen et al. 2013), more contractions without apostrophes (e.g., dont ) (Argamon et al. 2009), more Inter-net acronyms (e.g., lol ) (Rosenthal and McKeown 2011), more slang (Barbieri 2008;
Rosenthal and McKeown 2011), more swear words (Barbieri 2008; Nguyen, Smith, and Ros X  2011), and more capitalized words (e.g., HAHA ) (Rosenthal and McKeown 2011; Nguyen et al. 2013). Specific words such as like are also highly associated with younger ages (Barbieri 2008; Nguyen, Smith, and Ros X  2011). Younger people also use more features that indicate stance and emotional involvement (Barbieri 2008), such as intensifiers (Barbieri 2008; Nguyen et al. 2013) and emoticons (Rosenthal and
McKeown 2011). Younger people also use shorter words and sentences and write shorter tweets (Burger and Henderson 2006; Rosenthal and McKeown 2011; Nguyen et al. 2013). 3.3.3 Interpretation of Findings. Age prediction experiments are usually done on data sets collected at a specific point in time. Based on such data sets, language use is modeled and compared between users with different ages. Features that are found to be predic-tive or that correlate highly with age are us ed to highlight how differently  X  X ounger X  and  X  X lder X  people talk or write. However, the observed differences in language use based on such data sets could be explained in multiple ways. Linguistic variation can occur as an individual moves through life ( age grading ). In that case the same trend is observed for individuals at different time periods. Linguistic variation can also be a re-sult of changes in the community itself as it moves through time ( generational change ) (Sankoff 2006; Bell 2013). For example, suppose we observe that younger Twitter users include more smileys in their tweets. This could indicate that smiley usage is higher at younger ages, but that when Twitter use rs grow older they decrease their usage of smileys. Or, this could indicate a differ ence in smiley usage between generations (i.e., the generation of the current younger Twitter users use more smileys compared with the generation of the older Twitter users ). This also points to the relation between synchronic variation and diachronic change. Synchronic variation is variation across different speakers or speech communi ties at a particular point in time, and diachronic change is accumulation of synchronic variatio n in time and frequency. To have a better understanding of change, we need to unders tand the spread of variation across time and frequency. As is the case for gender, age can be considered a social variable and thus when only modeling chronological ag e, we are ignoring the agency of speakers and that speakers follow different trajectories in their lives. 3.4 Location
Regional variation has been extensively studi ed in sociolinguistics and related areas such as dialectology (Chambers and Tr udgill 1998) and dialect ometry (Wieling and Nerbonne 2015). The use of certain words, grammatical constructions, or the pronunciation of a word can often reveal where a speaker is from. For example, yinz (a form of the second-person pronoun) is mostly used around Pittsburgh, which can be observed on Twitter as well (Eisenstein 2015). Dialectology traditionally focuses on the geographical distribution of individual or small sets of linguistic variables (Chambers and Trudgill 1998). A typical approach involves identifying and plotting isoglosses , lines that divide maps into regions where specific values of the variable predominate. The next step involves identif ying bundles of isoglosses, often followed by the identification of dialect regions. Although these steps have usually been done manually, computational approaches have recently been explored as well. For example,
Grieve, Speelman, and Geeraerts (2011) demonstrated how methods from spatial analysis can be used for automating such an analysis.
 approaches, such as from computational linguistics, machine learning, and spatial analysis. A separate branch has also emerged, referred to as dialectometry (Wieling and Nerbonne 2015). In contrast to dialectology, which focuses on individual linguistic variables, dialectometry involves aggregatin g linguistic variables to examine linguis-tic differences between regions. Nerbonne (2009) argues that studies that focus on individual variables are sensitive to noise and that therefore aggregating linguistic variables will result in more reliable signals. This aggregation step has led to the in-troduction of various statistical methods, in cluding clustering, dimensionality reduc-tion techniques, and regression approaches (Wieling and Nerbonne 2010; Heeringa and Nerbonne 2013; Nerbonne and Wieling 2015). Recently, researchers within dialec-tometry have explored the automatic identifica tion of characteristic features of dialect regions (Wieling and Nerbo nne 2010), a task that aligns m ore closely with the ap-proaches taken by dialectologists.
 still small compared to data sets used in computational linguistics, similar statistical methods have been explored. This has created a promising starting point for closer collaboration with computational linguistics. 3.4.1 Modeling Geographical Variation. Within CL, we find two lines of work on computa-tionally modeling geographical variation.

Supervised. The first approach starts with documents labeled according to their dialect, which can be seen as a supervised learning approach. Most studies taking this approach focus on automatic dialect identification, which is a variation of automatic language identification, a well-studied research topic within the field of computational linguistics (Hughes et al. 2006; Baldwin and Lui 2010). Whereas some have considered automatic language identification to be a solved problem (McNamee 2005), many outstanding issues still exist (Hughes et al. 2006), including the identification of dialects and closely related languages (Zampieri et al. 2014, 2015). In studies on automatic dialect identifi-cation, various dialects have been explored, including Arabic (Elfardy and Diab 2013; Zaidan and Callison-Burch 2013; Darwis h, Sajjad, and Mubarak 2014; Huang 2015), Turkish (Do  X  gru X z and Nakov 2014), Swiss German (Scherrer and Rambow 2010), and
Dutch (Trieschnigg et al. 2012) dialects. 560
Unsupervised. An alternative approach is to start with location-tagged data to auto-matically identify dialect regions. Whereas the models are given labels indicating the locations of speakers, the dialect labels th emselves are not observed. In the context of modeling dialects, we consider it an unsupervised approach (although it can be considered a supervised approach when the task is framed as a location prediction task). The majority of the work in this area has used Twitter data, because it contains fine-grained location information in the form of GPS data for tweets or user-provided locations in user profiles.
 of automatically predicting the location of speakers. The set-up is thus similar to the set-up for the other tasks that we have surveyed in this section (e.g., gender and age prediction). Eisenstein et al. (2010) developed a topic model to identify ge-ographically coherent linguistic regions and words that are highly associated with these regions. The model was tested by predic ting the locations of Twitter users based on their tweets. Although the topic of text-based location prediction has received increasing attention (Wing and Baldridge 2011; Han, Cook, and Baldwin 2012), us-ing these models for the discovery of new sociolinguistic patterns is an option that has not been fully explored yet, since most studies primarily focus on prediction performance.
 that is essential in many of the studies that start with location-tagged data. In Wing and Baldridge (2011), locations are modeled using geodesic grids, but these grids do not always correspond to administrative or language boundaries. Users can also be grouped based on cities (Han, Cook, and Baldwin 2012), but such an approach is not suitable for users in rural areas or when the focus is on more fine-grained geographical variation (e.g., within a city). Eisenstein et al. (2010) model regions using Gaussian distributions, but only focus on the United States and thus more research is needed to investigate the suitability of this approach wh en considering other countries or larger regions. 3.4.2 Features and Patterns. Word and character n -gram models have been frequently used in dialect identificati on (Trieschnigg et al. 2012; Zaidan and Callison-Burch 2013;
King, Radev, and Abney 2014). Similarly, many text-based location prediction systems make use of unigram word features (Eisenstein et al. 2010; Wing and Baldridge 2011; Han, Cook, and Baldwin 2012).

Darwish, Sajjad, and Mubarak (2014) showed that for identifying Arabic dialects a better classification performance could be obtained by incorporating known lexical, mor-phological, and phonological differences in their model. Scherrer and Rambow (2010) also found that using linguistic knowledge improves over an n -gram approach. Their method is based on a linguistic atlas for the extraction of lexical, morphological, and phonetic rules and the likelihood of these for ms across German-speaking Switzerland.
Do  X  gru X z and Nakov (2014) ex plored the use of light verb constructions to distinguish between two Turkish dialects.
 tion performance, several studies have focu sed on automatically identifying charac-teristic features of dialects. Han, Cook, and Baldwin (2012) explored various feature selection methods to improve location pre diction. The selected features may reflect dialectal variation but this was not the focus of the study. The method by Proki  X  c,
 X  X ltekin, and Nerbonne (2012) was based on in-group and out-group comparisons using data in which linguistic varieties were al ready grouped (e.g., based on clustering).
Peirsman, Geeraerts, and Speelman (2010) compared frequency-based measures, such as chi-square and log-likelihood tests, with distributional methods. Automatic methods may identify many features that vary geographically such as topic words and named entities, and an open challenge is to separate this type of variation from the more sociolinguistically interesting variation s. For example, the observation that the word beach is used more often near coastal areas or that Times Square is used more often in New York is not interesting from the perspective of a sociolinguist.
 patterns of regional variation. Doyle (2014) analyzed the geographical distribution of dialectal variants (e.g., the use of double modals like might could )basedonTwit-ter data, and compared it with traditional so ciolinguistic data collection methods.
Starting with a query-based approach, he uses baseline queries (e.g., I ) for estimat-ing a conditional distribution of data given metadata. His approach achieved high correlations with data from sociolinguistic studies. J X rgensen, Hovy, and S X gaard (2015) studied the use of three phonological features of African American Vernacular
English using manually selected word pairs. The occurrence of the features was cor-related with location data (longitude and latitude) as well as demographic informa-tion obtained from the U.S. census bureau. Although these approaches start with attested dialect variants, automatic discovery of unknown variation patterns could potentially lead to even more interestin g results. To study how a word X  X  mean-ing varies geographically, Bamman, Dyer, and Smith (2014) extended the skip gram model by Mikolov et al. (2013) by adding contextual variables that represent states from the United States. The model then le arns a global embedding matrix and ad-ditional matrices for each context (e.g., state) to capture the variation of a word X  X  meaning.
 spreading of linguistic innovations geographically and over time on a large scale. A study by Eisenstein et al. (2014) based on tweets in the United States indicates that linguistic innovations spread through demographically similar areas, in particular with regard to race. 3.4.3 Interpretation of Findings. Labeling texts by dialect presumes that there are clear boundaries between dialects. However, it is not easy to make absolute distinctions between language varieties (e.g., languag es, dialects). Chamb ers and Trudgill (1998) illustrate this with the example of traveling from village to village in a rural area.
Speakers from villages at larger distances have more difficulty understanding each other compared with villages that are closer to each other, but there is no clear-cut distance at which speakers are no longer mutually intelligible. A computational ap-proach was taken by Heeringa and Nerbonne (2001) to shed mo re light on this puzzling example. Besides linguistic differences, boundaries between language varieties are often influenced by other factors such as politic al boundaries (Chambers and Trudgill 1998).
Therefore, deciding on the appropriate lab els to describe linguistic communication across different groups of speakers (in terms of language, dialect, minority language, regional variety, etc.) is an ongoing issue of d ebate. The arbitrariness of the distinction between a language and dialect is captured with the popular expression  X  X  language is a dialect with an army and navy X  (Bright 1997). Methods that do not presume clear dialect boundaries are therefore a promising alternative. However, such methods then rely on location-tagged data, which are usually only available for a portion of the data. 562 3.5 Text Classification Informed by Identity Information
So far, we have focused on automatically predicting the variables themselves (e.g., gender, age, location) but linguistic varia tion related to the identity of speakers can also be used to improve various other NLP tasks. Dadvar et al. (2012) trained gender-specific classifiers to detect instances of c yberbullying, noticing that language used by harassers varies by gender . To improve the prediction performance of detecting the power direction between participants in e-mails, Prabhakaran, Reid, and Rambow (2014) incorporated the gender of participants in e-mail conversations and the overall  X  X ender environment X  as features in their model. Volkova, Wilson, and Yarowsky (2013) studied gender differences in the use of subjective language on Twitter. Representing gender as a binary feature was not effective, but the use of features based on gender-dependent sentiment terms improved subje ctivity and polarity classification. Hovy (2015) found that training gender-or age-specific word embeddings improved tasks such as sentiment analysis and topic classification. 4. Language and Social Interaction
The previous section explored computatio nal approaches to the study of identity con-struction through language. We discussed variables such as gender, age, and geograph-ical location, thereby mostly focusing on th e influence of social structures on language use. However, as we also pointed out, speaker agency enables violations of conventional language patterns. Speakers do not act in isolation, but they are part of pairs, groups, and communities. Social interaction contexts produce the opportunity for variation due to agency. In response to the particulars of these social settings and encounters (e.g., the addressee or audience, topic, and socia l goals of the speakers), there is thus much variation within individual speakers. The variation that is related to the context of interaction will be the focus of this section.
 language use in pairs, groups, and communities (Section 4.1). Next, we discuss compu-tational approaches to studying how language reflects and shapes footing within social relationships (Section 4.2). Much of this work has revolved around the role of language in power dynamics by studying how speakers use language to maintain and change power relations (Fairclough 1989). We continue with a discussion on style-shifting (i.e., the use of different styles by a single speaker) in Section 4.3. We discuss two prominent frameworks within sociolinguistics, Audience Design (Bell 1984) and Communication
Accommodation Theory (Giles, Coupland, and Coupland 1991), and discuss how these frameworks have been studied within the computational linguistics community. Finally, we move our focus to the community level and discuss computational studies on how members adapt their language to conform to or sometimes diverge from community norms. One might speculate about how thes e micro-level processes might eventually become conventional, and therefore consider how these processes may lead to language change over time (Section 4.4). 4.1 Data Sources
Many of the types of data that are relevant for the investigation of concepts of social identity are also relevant for work on communication dynamics in pairs, groups, and communities. The availability of detailed interaction recordings in online data has driven and enabled much of the work on this topic within computational linguistics.
A variety of online discussion forums have been analyzed, including online cancer support communities (Nguyen and Ros X  2011; Wang, Reitter, and Yen 2014), a street gang forum (Piergallini et a l. 2014), and more recently di scussion forums in Massive Open Online Courses (Wen, Yang, and Ros X  2014a, 2014b). Review sites, such as
TripAdvisor (Michael and Otterbacher 2014), IMDb (Hemphill and Otterbacher 2012), and beer review communities (Danescu-Niculescu-Mizil et al. 2013b), have also been used in studies on language in online communities.
 is a large e-mail corpus with messages from Enron employees, which was made public during the legal investigation of the Enron corporation. The corpus has been used in various studies X  X or example, investigation s related to e-mail classification (Klimt and
Yang 2004) and structure of communication networks (Diesner and Carley 2005). In particular, in studies on language and social dynamics, the Enron e-mail corpus has featured in analyses of power relationships (Diehl, Namata, and Getoor 2007; Gilbert 2012; Prabhakaran, Rambow, and Diab 2012b; Prabhakaran, Reid, and Rambow 2014), since Enron X  X  organizational structure is known and can be integrated in studies on hierarchical power structures connected wi th quantitative capacity theories of power.
Such theories treat power as a stable characteristic that inheres in a person. An example theory within this space is Resource Dependency Theory (Pfeffer and Salancik 1978). viduals who are pursuing power), other res ources have also been explored, includ-ing Wikipedia Talk Pages (Bender et al. 2011; Bracewell, Tomlinson, and Wang 2012;
Danescu-Niculescu-Mizil et al. 2012; Swayamdipta and Rambow 2012), transcripts of political debates (Prabhakaran, John, and Seligmann 2013; Prabhakaran, Arora, and
Rambow 2014), and transcripts of Supreme Court arguments (Danescu-Niculescu-Mizil et al. 2012). 4.2 Shaping Social Relationships
Language is not only a means to exchange information but language also contributes to the performance of action within interact ion. Language serves simultaneously as a reflection of the relative positioning of spe akers to their conversation partners as well as actions that accompany those positions (Ribeiro 2006). Sometimes distributions of these actions can be considered to cohere to such a degree that they can be thought of as defining conversational roles (Yang, Wen, and Ros X  2015). At a conceptual level, this work draws heavily from a foundation in linguistic pragmatics (Grice 1975; Levinson 1983) as well as sociological theories of discourse (Tannen 1993; Gee 2011), which each provide a complementary view. Concep ts related to expectations or norms that provide the foundation for claiming such positions may similarly be described either from a philosophical perspective or a sociological one (Postmes, Spears, and Lea 2000).
In viewing interaction as providing a context in which information and action may flow towards the accomplishment of social g oals, speakers position themselves and others as sources or recipients of such information and action (Martin and Rose 2003).
When performatives (i.e., speech acts use d to perform an action) break norms related to social positions, they have implications fo r relational constructs such as politeness (Brown and Levinson 1987), which codifies rhetorical strategies for acknowledging and managing relational expectations while see king to accomplish extra-relational goals. In the remaining part of this section, we focus on computational studies within this theme. 564
We first discuss the general topic of automati c extraction of social relationships from text, and then focus on power and politeness.

Automatic Extraction of Social Relationships. Recognizing that language use may reveal cues about social relationships, studies within CL have explored the automatic extrac-tion of different types of social relationships based on text. One distinction that has been made is between weak ties (e.g., acquaintances) and strong ties (e.g., family and close friends) (Granovetter 1973). Gilbert and Karahalios (2009) explored how different types of information (including messages pos ted) can be used to predict tie strength on Facebook. In this study, the predictions were done for ties within a selected sample. Bak,
Kim, and Oh (2012) studied differences in self-disclosure on Twitter between strong and weak ties using automatically identified top ics. Twitter users disclose more personal information to strong ties, but they show m ore positive sentiment towards weak ties, which may be explained by social norms regarding first-time acquaintances on Twitter. data sets, enabling analyses of the extracted network structures. These studies have focused on extracting signed social networks , i.e., networks with positive and negative edges, for example based on positive and negative affinity between individuals or formal and informal relationships. Work within this area has drawn from Structural
Balance Theory (Heider 1946), which captures intuitions such as that when two indi-viduals have a mutual friend, they are likely to be friends as well, and from Status
Theory (Leskovec, Huttenlocher, and Kleinberg 2010), which involves edges that are directed and reflect status differences. Hassan, Abu-Jbara, and Radev (2012) developed a machine learning classifier to extract signed social networks and found that the extracted network structure mostly agreed with Structural Balance Theory. Krishnan and Eisenstein (2015) proposed an unsupervised model for extracting signed social networks, which they used to extract forma l and informal relations in a movie-script corpus. Furthermore, their model also ind uced the social function of address terms (e.g., dude ). To infer edge signs in a social network, West et al. (2014) formulated an optimization problem that combined two obj ectives, capturing the extent to which the inferred signs agreed with the predictions of a sentiment analysis model, and the extent to which the resulting triangles corre sponded with Status and Structural Balance Theory.

Power. Work on power relations draws from social psychological concepts of relative power in social situations (Guinote and Vescio 2010), in particular, aspects of relative power that operate at the level of individual s in relation to specific others within groups or communities. Relative power may be thoug ht of as operating in terms of horizontal positioning or vertical positioning: Horizon tal positioning relates to closeness and re-lated constructs such as positive regard, trust, and commitment, and vertical positioning relates to authority and related constructs such as approval and respect among individ-uals within communities. Within the areas of linguistics and computational linguistics, investigations have focused on how speakers use language to maintain and change power relations (Fairclough 1989). Operationalization and computational modeling of these two dimensions has important app lications in the field of learning sciences (Howley, Mayfield, and Ros X  2013).
 as it is reflected through language has focused on automatically identifying power relationships from text. Though some of the literature cited here is referenced in this work, the engagement between communities has remained so far at a simple level.
Fine-grained distinctions between families o f theories of power, and subtleties about the relationship between power and langua ge, are frequently glossed over. One way in which this is visible is in the extent to which the locus of meaning is treated as though it is in the text itself rather than an emergent property of the interaction between speakers.
Though some references to external power str uctures and transient power relationships are mentioned, much room remains for deeper reflection on the connection between power and language.
 normally centered around classification tasks. Earlier studies have focused on hierar-chical power relations based on the organizat ional structure, thereby frequently making use of the Enron corpus. Bramsen et al. (2011) extracted messages between pairs of participants and developed a machine learni ng classifier to automatically determine whether the messages of an author were UpSpeak (directed towards a person of higher status) or DownSpeak (directed towards a person of lower status). With a slightly different formulation of the task, Gilbert (2012) used logistic regression to classify power relationships in the Enron corpus and identified the most predictive phrases. Besides formulating the task as a classification tas k, ranking approaches have been explored as well (Diehl, Namata, and Getoor 2007; Prabhakaran, John, and Seligm ann 2013;
Nguyen et al. 2014). For example, Prabhaka ran, John, and Seligmann (2013) predicted the ranking of participants in political debates according to their relative poll standings. a company, treat power relations as static. Recent studies have adopted more dynamic notions of power. For example, Prabhakaran, Rambow, and Diab (2012b) discuss a setting with an employee in a Human Resour ces department who interacts with an office manager. The HR employee has power over the office manager when the situation is about enforcing an HR policy, but the power relation will be reversed when the topic is allocation of new office space. In their s tudy using the Enron corpus, they compared manual annotations of situational power with the organization hierarchy and found that these were not well aligned. Other studies have focused on a more dynamic view of power as arising through asymmetries with respect to needed resources or other goals, as characterized in consent-based theories of power such as exchange theory (Guinote and Vescio 2010). This would include such investigations as identifying persons who are pursuing power (Bracewell, Tomlinson, and Wang 2012; Swayamdipta and Rambow 2012) and detecting influencers (Huffaker 2010; Quercia et al. 2011; Biran et al. 2012;
Nguyen et al. 2014). This could also include studying how language use changes when users change their status in online communities (Danescu-Niculescu-Mizil et al. 2012). relations or roles of individuals have been co llected in different ways, such as based on the organizational structure of Enron (Bramsen et al. 2011; Gilbert 2012), the number of followers in Twitter (Danescu-Niculescu-Mizil, Gamon, and Dumais 2011), standings in state and national polls to study power in political debates (Prabhakaran, John, and Seligmann 2013), admins and non-admins in Wikipedia (Bender et al. 2011; Danescu-Niculescu-Mizil et al. 2012), and manual annotation (Biran et al. 2012; Prabhakaran and Rambow 2013; Nguyen et al. 2014).
 pragmatics related to speech act theory (Searle 1969; Austin 1975), which has most commonly been represented in what are typically referred to as conversation, dialog or social acts (Bender et al. 2011; Ferschke, Gurevych, and Chebotar 2012). Such cat-egories can also be combined into sequences (Bracewell, Tomlinson, and Wang 2012).
Other specialized representations are also us ed, such as features related to turn-taking 566 style (Swayamdipta and Rambow 2012; Prabhakaran, John, and Seligmann 2013), topic control (Strzalkowski et al. 2012; Nguyen et al. 2014; Prabhakaran, Arora, and Rambow 2014), and  X  X vert displays of power, X  which Prabhakaran, Rambow, and Diab (2012a) define as utterances that constrain the addr essee X  X  actions beyond what the underlying dialog act imposes.

Politeness. Polite behavior contributes to mainta ining social harmony and avoiding social conflict (Holmes 2013). Automatic classifiers to detect politeness have been developed to study politeness strategies o n a large scale. According to a politeness theory by Brown and Levinson (1987), three social factors influence linguistically polite behavior: social distance, relative power, and ranking of the imposition (i.e., cost of the request). Drawing from this theory, Peterson, Hohensee, and Xia (2011) performed a study on the Enron corpus by training classifiers to automatically detect formality and requests. E-mails that contained requests or that were sent to people of higher ranks indeed tended to be more formal. According to politeness theory, speakers with greater power than their addressees are expected to be less polite (Brown and
Levinson 1987). Danescu-Niculescu-Mizil et al. (2013a) developed a politeness classifier and found that in Wikipedia polite editors were more likely to achieve higher status, but once promoted, they indeed became less polite. In StackExchange, a site with an explicit reputation system, users with a h igher reputation were less polite than users with a lower reputation. Their study also rev ealed new interactions between politeness markings (e.g., please ) and morphosyntactic context. 4.3 Style Shifting
According to Labov (1972), there are no single-style speakers because speakers may switch between styles (style-shifting) depend ing on their communication partners (e.g., addressee X  X  age, gender, and social background). Besides the addressee, other factors such as the topic (e.g., politics vs. religion) or the context (e.g., a courtroom vs. family dinner) can contribute to style shifting. In e arly studies, Labov stated that "styles can be arranged along a single dimension, measured by the amount of attention paid to speech" (Labov 1972), which thus views style shifting as mainly something responsive.
The work by Labov on style has been highly influential, but not everyone agreed with his explanation for different speech styles. We will discuss two theories (Com-munication Accommodation Theory and Audience Design) that have received much attention in both sociolinguistics and computational linguistics and that focus on the role of audiences and addressees on style. Even more recent theories are emphasizing the agency of speakers as they use different styles to represent themselves in a certain way or initiate a change in the situation. Bes ides switching between styles, multilingual speakers may also switch between language s or dialects. This is discussed in more depth in Section 5.

Communication Accommodation Theory. Communication Accommodation Theory (CAT) (Giles, Taylor, and Bourhis 1973; Giles, Coupland, and Coupland 1991; Soliz and Giles 2014) seeks to explain why speakers accommodate 5 to each other during conversa-tions. Speakers can shift their behavior to become more similar (convergence) or more different (divergence) to their conversation partners. Convergence reduces the social distance between speakers and converging speakers are often viewed as more favorable and cooperative. CAT was developed in the 1970s and has its roots in the field of social psychology. Although CAT has been st udied extensively in controlled settings (e.g., Gonzales, Hancock, and Pennebaker 2010), only recently studies have been per-formed in uncontrolled settings such as Twi tter conversations (Danescu-Niculescu-
Mizil, Gamon, and Dumais 2011), online for ums (Jones et al. 2014), W ikipedia Talk pages and Supreme Court arguments (Danescu-Niculescu-Mizil et al. 2012), and even movie scripts (Danescu-Niculescu-Mizil and Lee 2011).
 pitch and gestures, to the words that are used. Within computational linguistics, re-searchers have focused on measuring lingu istic accommodation. LIWC has frequently been used in these studies to capture st ylistic accommodation X  X or example, as re-flected in the use of pronouns (Niederhoffer and Pennebaker 2002; Danescu-Niculescu-
Mizil and Lee 2011; Danescu-Niculescu-Mizil, Gamon, and Dumais 2011; Jones et al. 2014). Speakers do not necessarily converge on all dimensions (Giles, Coupland, and Coupland 1991); this has also been observed on Twitter (Danescu-Niculescu-Mizil,
Gamon, and Dumais 2011). Although earlier studies used correlations of specific fea-tures between participants, on turn-level or overall conversation-level (Niederhoffer and Pennebaker 2002; Scissors et al. 2009; Levitan, Gravano, and Hirschberg 2011), these correlations fail to capture the tem poral aspect of accommodation. The measure developed by Danescu-Niculescu-Mizil, Gamon, and Dumais (2011) is based on the increase in probability of a response containing a certain stylistic dimension given that the original message contains that specifi c stylistic dimension. Wang, Reitter, and
Yen (2014) used a measure based on repetition of words (or syntactic structures) be-tween target and prime posts. Jones et al. (2014) proposed a measure that takes into account that speakers differ in their tendency to accommodate to others. Similarly,
Jain et al. (2012) used a Dynamic Bayesian Model to induce latent style states that group related style choices together in a way that reflects relevant styles within a corpus. They also introduce global accommodation states that provide more context in identification of style shifts in interactions that extend for more than a couple of turns.
 out over time and computational approaches to measure accommodation have been used to study power dynamics (Danescu-Niculescu-Mizil, Gamon, and Dumais 2011;
Danescu-Niculescu-Mizil et al. 2012; Jones et al. 2014). In a study on power dynamics in Wikipedia Talk pages and Supreme court debates, Danescu-Niculescu-Mizil et al. (2012) found that people with a lower status accommodated more than people with a higher status. In addition, users accommodated less once they became an admin in Wikipedia. Using the same Wikipedia da ta, Noble and Fern X ndez (2015) found that users accommodated more towards users who occupied a more central position, based on eigenvector and betweenness centra lity, in the social network. Furthermore, whether a user was an admin did not have a significant effect on the amount of coordination that highly central users rece ived. From a different angle, Gweon et al. (2013) studied transactive exchange in debate contexts. Transactivity is a property of an assertion that requires that it displays reasoning (e.g., a causal mechanism) and refers to or integrates an idea expressed ear lier in the discussion. In this context, high concentrations of transactivity reflect a ba lance of power in a discussion. In their data, higher levels of speech style accommodati on were correlated with higher levels of transactivity. 568
Audience Design. In a classical study set in New Zealand, Allan Bell found that news-readers used different styles depending on which radio station they were talking for, even when they were reporting the same news on the same day. Bell X  X  audience design framework (Bell 1984) explains style shifting as a response to audiences and shares similarities with CAT. One of the differences with CAT is that different types of au-diences are defined from the perspective of the speaker (ranging from addressee to eavesdropper) and thus can also be applied to settings in which there is only a one-way interaction (such as broadcasting). Social media provides an interesting setting to study how audiences influence style. In many social media platforms, such as Twitter or Facebook, multiple audiences (e.g., frie nds, colleagues) are collapsed into a single context. Users of such platforms often imagine an audience when writing messages and they may target messages to different audiences (Marwick and boyd 2011).
 In a study on how audiences influence the use of minority languages on Twitter,
Nguyen, Trieschnigg, and Cor nips (2015) showed how charact eristics of the audience influence language choice on Twitter by analyzing tweets from multilingual users in the Netherlands using automatic language identification. Tweets directed to larger audiences were more often written in Dutch, w hereas within conversations users often switched to the minority language. In another study on audience on Twitter, Bamman and Smith (2015) showed that incorporating features of the audience improved sarcasm detection. Furthermore, their results suggested that users tend to use the hashtag #sarcasm when they are less familiar with their audience. Pavalanathan and
Eisenstein (2015a) studied two types of non-standard lexical variables: those strongly associated with specific geographical regio ns of the United States and variables that were frequently used in Twitter but considered non-standard in other media. The use of non-standard lexical variables w as higher in messages with user mentions, which are usually intended for smaller a udiences, and lower in messages with hashtags, which are usually intended for lar ger audiences. Furthermore, non-standard lexical variables were more often used in t weets addressed to individuals from the same metropolitan area. Using a different data source, Michael and Otterbacher (2014) showed that reviewers on the TripAdvisor sit e adjust their style to the style of preceding reviews. Moreover, the extent to which reviewers are influenced correlates with attributes such as experience of the reviewer and their sentiment towards the reviewed attraction. 4.4 Community Dynamics
As we just discussed, people adapt their la nguage use towards their conversation partner. Within communities, norms emerge over time through interaction between members, such as the use of slang words and domain-specific jargon (Nguyen and Ros X  2011; Danescu-Niculescu-Mizil et al. 2013b), or conventions for indicating retweets in
Twitter (Kooti et al. 2012). Community members use such markers to signal their affili-ation. In an online gangs forum, for example, graffiti style features were used to signal group affiliation (Pierga llini et al. 2014). To become a core member of a community, members adopt such community norms. As a result, often a change in behavior can be observed when someone joins a community. Multiple studies have reported that members of online communities decrease their use of first person singular pronouns (e.g., I ) over time and increase their use of first person plural pronouns (e.g., we ) (Cassell and Tversky 2005; Nguyen and Ros X  2011; Danescu-Niculescu-Mizil et al. 2013b), suggesting a stronger focus on the community. Depending on the frequency of use and social factors, local accommodation effects could influence how languages change in the long term (Labov 1994, 2001). Fine-grained, large-scale analyses of lan-guage change are difficult in offline settings, but the emergence of online commu-nities has enabled computational approaches for analyzing language change within communities.
 such as e-mail exchanges between students during a course (Postmes, Spears, and Lea 2000) and data from the Junior Summit  X 98, an online community where children from across the world discussed global issues (Cassell and Tversky 2005; Huffaker et al. 2006).
In these communities, members joined at the same time. Furthermore, the studies were based on data spanning only several months.
 online forums and review sites. Data from these communities typically span longer time periods (e.g., multiple years). Member s join these communities intermittently and thus, when new users join, community norms have already been established. Nguyen and Ros X  (2011) analyzed an online breast cancer community, in which long-time members used forum-specific jargon, high ly informal style, and showed familiarity and emotional involvement with other members. Time periods were represented by the distribution of high frequency words and m easures such as Kullback-Leibler diver-gence were used to study how language changed over time. Members who joined the community showed increasing conformity to community norms during the first year of their participation. Based on these observa tions, a model was developed to determine membership duration. Hemphill and Otterbacher (2012) also studied how members adopt community norms over time but focused specifically on gender differences. They studied changes in the use of various characteristics, such as hedging, word/sentence complexity and vocabulary richness, on IMDb a community in which men tend to receive higher prestige than women.
 community, communities themselves are als o constantly evolving. Kershaw, Rowe, and
Stacey (2016) identified and analyzed word innovations in Twitter and Reddit based on variation in frequency, form, and meaning. They performed their analyses on a global level (i.e., the whole data set) and on a community level, based on applying a community detection algorithm to the Reddi t data and grouping the geotagged tweets by geopolitical units.

Danescu-Niculescu-Mizil et al. (2013b) in two beer review communities. Language models were created based on monthly snapshots to capture the linguistic state of a community over time. Cross-entropy was then used to measure how much a cer-tain post deviated from a language model . Members in these communities turned out to follow a two-stage lifecycle: They first align with the language of the com-munity (innovative learning phase), however at some point they stop adapting their language (conservative phase). The point at which members enter the conservative phase turned out to be dependent on how long a user would end up staying in the community.
 study language change in communities in a quantitative manner. However, in such analyses biases in the data should be considered carefully, especially when the dy-namics and content of the data are not understood fully. For example, Pechenick,
Danforth, and Dodds (2015) call into question the findings on linguistic change based 570 on the Google books corpus, because of its bias towards scientific publications. Further-more, they point out that prolific authors in the data set can influence the findings as well. 5. Multilingualism and Social Interaction
Languages evolve through the interaction of speakers within and outside their speech communities. Within sociolinguistics, mul tilingual speakers and speech communities have been studied widely with respect to the contexts and conditions of language mixing and/or switching across languages. We use the term  X  X ultilingual speaker X  for someone who has a repertoire of various languages and/or dialects and who may mix them depending on contextual factors like occasion (e.g., home vs. work) and conversation partners (e.g., family vs. formal encounters). This section is dedicated to computational approaches for analyzing multilingual communication in relation to the social and linguistic contexts. We first star t with a brief introduction to multilingual communication from a sociolinguistic point of view. Later, we expand the discussion to include the analysis of multilingual communication using computational approaches. different languages. Weinreich (1953) was one of the first to explain why and how languages come into contact and evolve under each other X  X  influence in a systematic manner. Sociolinguists (Auer 1988; Gumperz 1982; Poplack, Sankoff, and Miller 1988;
Myers-Scotton 2002) have studied various aspects of language contact and mixing across different contact settings.
 always a consensus on the terminology. According to Gumperz (1982), language mixing refers to the mixing of languages within the same text or conversation. Wei (1998) describes language alternations at or above the clause level and calls it code-mixing.
Romaine (1995) differentiates between inter-sentential (i.e., across sentences) and intra-sentential (i.e., within the same sentence) switches. Poplack, S ankoff, and Miller (1988) refer to complete languages shifts of individual users as code-switching.
 words or fixed multi-word expressions) to more structural ones (e.g., morphological, syntactic borrowings). The duration and intensity of interaction between speakers of contact languages influence the types of switches. When the frequency of switched words increases in use, they may get established in the speech community and become borrowed/loan words (e.g., hip hop X  X elated Anglicisms in a German hip hop forum [Garley and Hockenmaier 2012]).
 collected in controlled or naturalistic settings (Auer 1988; Myers-Scotton 1995). Nowa-days, the wide-spread use of the Internet in multilingual populations provides ample opportunities for large-scale and in-depth analyses of mixed language use in online media (Paolillo 2001; Tsaliki 2003; Hinric hs 2006; Danet and Herring 2007; Hinnenkamp 2008). Still mo st of these studies focus on qualitative analyses of multilingual online communication with limited data in terms of size and duration.
 gual communication on a large scale (Section 5.1). Consequently, we discuss research on adapting various NLP tools to process mixed-language texts (Section 5.2). We conclude this section with a discussion of studies that analyze, or even try to predict, the use of multiple languages in multilingual communication (Section 5.3). 5.1 Data Sources
In sociolinguistics, conversational data i s usually collected by the researchers them-selves, either among small groups o fspeakersatdifferenttimes(Do  X  gru X z and Backus 2007, 2009) or from the same group of speakers longitudinally (Milroy and Milroy 1978; Trudgill 2003). The manual transcription a nd annotation of data is time-intensive and costly. Multilingual data from online env ironments is usually extracted in small volumes and for short periods. Automatic analysis of this type of data has been difficult for most languages, especially when resources or technical support are lacking. processing of mixed-language texts. Lui, Lau, and Baldwin (2014) and Yamaguchi and Tanaka-Ishii (2012) studied automatic language identification in mixed-language documents from Wikipedia by artificially concatenating texts from monolingual sources into multilingual documents. Howe ver, such approaches lead to artificial language boundaries. More recently, social media (such as Facebook [Vyas et al. 2014],
Twitter [Jurgens, Dimitrov, and Ruths 2014; Peng, Wang, and Dredze 2014; Solorio et al. 2014] and online forums [Nguyen and Do  X  gru X z 2013]) provide large volumes of data for analyzing multilingual communication in social interaction. Transcriptions of conversations have been explored by Solorio and Liu (2008b), however their data was limited to three speakers. Language pairs that have been studied for multilingual communication include English X  X indi (Vyas et al. 2014), Spanish X  X nglish (Solorio and
Liu 2008a, 2008b; Peng, Wang, and Dredze 2014), Turkish X  X utch (Nguyen and Do  X  gru X z 2013), Mandarin X  X nglish (Adel, Vu, and Sch ultz 2013; Peng, Wang, and Dredze 2014), and French X  X nglish (Jurgens, Dimitrov, a nd Ruths 2014). Besid es being a valuable resource for studies on multilingual social interaction, multilingual texts in social media have also been used to improve general purpose machine translation systems (Ling et al. 2013; Huang and Yates 2014).
 guages at the word level. Language identification is a well-researched problem in CL and we discussed it in the context of dialect identification in Section 3.4.1. Here, we discuss language identification for mixed-language texts. Several data sets are publicly available to stimulate research on language identification in mixed-language texts, including data from the shared task on Language Identification in Code-Switched Data (Solorio et al. 2014) covering four different language pairs on Twitter, romanized
Algerian Arabic and French texts from th e comments section of an online Algerian newspaper (Cotterell et al. 2014), Turkish X  X utch forum posts (Nguyen and Do  X  gru X z 2013) and Web documents in different languages (King and Abney 2013).
 challenges in the construction of data sets. More fine-grained annotations require more effort and sometimes the segments are so short that they can no longer be clearly attributed to a particular language. For example, annotating the language of named entities remains a challenge in mixed-langu age texts. Named entities have been labeled according to the context (King and Abney 2013), ignored in the evaluation (Elfardy and Diab 2012b; Nguyen and Do  X  gru X z 2013) or treated as a separate category (Elfardy and
Diab 2012a; Solorio et al. 2014). Annotation at the sentence-level is also challenging. For example, Zaidan and Callison-Burch (2013) a nnotated a large corpus for Arabic dialect identification using crowdsourcing. Their a nalysis indicated that many annotators over-identify their native dialect (i.e., they we re biased towards labeling texts as written in their own dialect). Elfardy and Diab (2012a) presented guidelines to annotate texts written in dialectal variants of Arabic an d Modern Standard Arabic on a word level. 572 5.2 NLP Tools for Multilingual Data
Most of the current NLP tools, such as parsers, are developed for texts written in a single language. Therefore, such tools are no t optimized for processing texts containing multiple languages. In this section, we di scuss the development of NLP tools that specifically aim to support the processing of multilingual texts. We start with research on automatic language identification, which is an important step in the preprocessing pipeline of many language-specific analysis tasks. Mixed-language documents have introduced new challenges to this task. W e then continue with a discussion of work on various other NLP tools (e.g., parsers, topic modeling).

Automatic Language Identification. Automatic language identification is often the first step for systems that process mixed-language texts (Vyas et al. 2014). Furthermore, it supports large-scale analyses of patterns in multilingual communication (Jurgens, Dimitrov, and Ruths 2014; Kim et al. 2014; Papalexakis, Nguyen, and Do  X  gru X z 2014).
Most of the earlier research on automatic language identification focused on document-level identification of a single language (Baldwin and Lui 2010). To handle mixed-language texts, more fine-grained approaches have been explored, ranging from language identification at the sentence (E lfardy and Diab 2013; Za idan and Callison-Burch 2013; Zampieri et al. 2014) and word level (Elfardy and Diab 2012b; King and
Abney 2013; Nguyen and Do  X  gru X z 2013; Solorio et al. 2014; Voss et al. 2014), approaches for text segmentation (Yamaguchi and Tanaka-Ishii 2012), and estimating the proportion of the various languages used within documents (Prager 1999; Lui, Lau, and Baldwin 2014). Depending on the application, different approaches may be suitable, but studies that analyze patterns in multilingual communication have mostly focused on word-level identification (Nguyen and Do  X  gru X z 2013; Solorio et al. 2014). Off-the-shelf tools developed for language identification at the document level (e.g., the TextCat program [Cavnar and Trenkle 1994]) are not effective for word-level identification (Alex 2005; Nguyen and Do  X  gru X z 2013). Language models (Elfardy and Diab 2012b; Nguyen and Do  X  gru X z 2013) and dictionaries (Alex 2005; Elfardy and Diab 2012b; Nguyen and
Do  X  gru X z 2013), which are also commonly used in automatic language identification at the document level, have been explored. Furthermore, the context around the words has been exploited using conditional random fields to improve performance on language identification at the word level (King and Abney 2013; Nguyen and Do  X  gru X z 2013).
Parsing. Early studies on language mixing within computational linguistics focused on developing grammars to model language mixing (e.g., Joshi 1982). However, the models developed in these early studies we re not tested on empirical data. The more recently developed systems have been validat ed on large, real-world data. Solorio and
Liu (2008b) explored various strategies to combine monolingual taggers to parse mixed-language texts. The best performance was obtained by including the output of the monolingual parsers as features in a machine learning algorithm. Vyas et al. (2014) studied the impact of different preprocessing steps on POS tagging of English X  X indi data collected from Facebook. Language identification and transliteration were the major challenges that impacted POS performance.

Language and Topic Models. Language models have been de veloped to improve speech recognition for mixed-language speech, by adding POS and language information to the language models (Adel, Vu, and Schultz 2013) or by incorporating syntactic inversion constraints (Li and Fung 2012). Peng, Wang, and Dredze (2014) developed a topic model that infers language-specific topic distribut ions based on mixed-language text. The main challenge for their model was aligning the inferred topics across languages. 5.3 Analysis and Prediction of Multilingual Communication According to Thomason (2001), Gardner-Chloros and Edwards (2004), and Bhatt and
Bolonyai (2011), social factors (e.g., attitudes and motives of the speakers, social and political context) are as important as linguistic factors in multilingual settings. Large-scale analysis of social factors in multilingual communication has only recently been possible with the availability of automatic language identification tools.
 choice at the user level, researchers have extracted network structures, based on follow-ers and followees (Eleta and Golbeck 2014; Kim et al. 2014), or mentions and retweets (Hale 2014), and analyzed the relation between the composition of such networks and the language choices of users. Users tweetin g in multiple languages are often found to function as a bridge between communities tw eeting in one language. Besides analyzing language choice at the user level, there is also an interest in the language choices for individual tweets. Jurgens, Dimitrov, and Ruths (2014) studied tweets written in one language but containing hashtags in another language. Automatic language identifi-cation was used to identify the languages of the tweets. However, as they note, some tweets were written in another language beca use they were automatically generated by applications rather than being a conscious c hoice of the user. Nguyen, Trieschnigg, and
Cornips (2015) studied users in the Netherlands who tweeted in a minority language (Limburgish or Frisian) as well as in Dutch. Most tweets were written in Dutch, but during conversations users often switched to the minority language. Mocanu et al. (2013) analyzed the geographic distributio n of languages in multilingual regions and cities (such as New York and Montreal) using Twitter.
 studies have explored the automatic prediction of language switches. The task may seem similar to automatic language identification, yet there are differences between the two tasks. Rather than determining the language of an utterance, it involves predicting whether the language of the next utterance is the same without having access to the next utterance itself. Solorio and Liu (2008a) were the first to predict whether a speaker will switch to another language in English X  X panish bilingual spoken conversations based on lexical and syntactic features. The approach was evaluated using standard ma-chine learning metrics as well as human evaluators who rated the naturalness/human-likeness of the sentences the system gen erated. Papalexakis, Nguyen, and Do  X  gru X z (2014) predicted when m ultilingual users switch between languages in a Turkish X  X utch online forum using various features, including features based on multi-word units and emoticons. 6. Research Agenda
Computational sociolinguistics is an emer ging multidisciplinary field. Closer collab-oration between sociolinguists and computa tional linguists could be beneficial to re-searchers from both fields. In this article, we have outlined some challenges related to differences in data and methods that must be addressed in order for synergy to be effective. In this section, we summarize the main challenges for advancing the field of computational sociolinguistics. The se fall under three main headings, namely, 574 expanding the scope of inquiry of the field, ad apting methods to increase compatibility, and offering tools. 6.1 Expanding the Scope of Inquiry
The field of computational linguistics has beg un to investigate issues that overlap with those of the field of sociolinguistics. The emerging availability of data that is of interest to both communities is an important factor, but in order for real synergy to come out of this, additional angles in the research agendas and tuning of the methodological frameworks in the respective communities would be needed.

Going Beyond Lexical and Stylistic Variation. Many studies within CL focus on lexical variation (e.g., Section 3 on social identity), possibly driven by the focus on prediction tasks. Stylistic variation has also received a ttention. Several of the discussed studies focus on variation in the usage of functional categories. For example, they zoom in on the usage of determiners, prepositions, and pronouns for studying linguistic style accommodation (in Section 4.3). Others use measures such as average word and sen-tence length (e.g., in Section 3). Advances in the area of stylometry (Stamatatos 2009) could inspire the exploration of more fine-grained features to capture style. Besides lexical and stylistic variation, linguistic variation also occurs on many other levels. Some computational studies have focused on phonological (Jain et al. 2012; Eisenstein 2013a;
J X rgensen, Hovy, and S X gaard 2015) and syntactic (Wiersma, Nerbonne, and Lauttamus 2010; Gianfortoni, Adamson, and Ros X  2011; Doyle 2014; Johannsen, Hovy, and S X gaard 2015) variation, but so far the number of studies is limited. In combination with the surge in availability of relevant data, these examples suggest that there seems to be ample opportunities for an extended scope.

Extending Focus to Other Social Variables. A large body of work exists on the model-ing of gender, age, and regional variation (cf. Section 3). Other variables, like social class (Labov 1966), have barely received any attention so far within computational sociolinguistics. Although it is more difficult to obtain labels for some social variables, they are essential for a richer understanding of language variation and more robust analyses.

Going Beyond English and Monolingual Data. The world is multilingual and multicultural, but English has received much more attentio n within computational sociolinguistics than other languages. There is a need for research to validate the generalizability of find-ings based on English data for other languages (Danet and Herring 2007). Furthermore, most studies within computational linguisti cs generally assume that texts are written in one language. However, these assumptions may not hold, especially in social media. A single user may use multiple languages, so metimes even within a syntactic unit, while most NLP tools are not optimized to process such texts. Tools that are able to process mixed-language texts will support the analysis of such data and shed more light on the social and linguistic factors invo lved in multilingual communication.

From Monomodal to Multimodal Data. Another recommendable shift in scope would be a stronger focus on multimedia data. Video and audio recordings with a speech track encapsulate a form of language in which the verbal and nonverbal dimensions of human communication are available in an integrated manner and they represent a rich source for the study of social behavior. Among the so-called paralinguistic aspects for which detection models and evaluation frameworks exist are age, gender, and affect (Schuller et al. 2010). The increasing volumes of recordings of spoken dialogue and aligned transcriptions, for example, in oral history collections (Boyd 2013; De Jong et al. 2014), meeting recording archives (Janin et al. 2003), and video blogs (Biel et al. 2013), can add new angles to the investigation of sociolinguistic variation. In particular, the study of the interaction between (transcribed ) speech, non-speech (laughter, sighs, etc.), facial expression, and gestures is a promising area for capturing and predicting social variables as well as the related affective layers. 6.2 Adapting Methodological Frameworks to Increase Compatibility
To make use of the rich repertoire of theory and practice from sociolinguistics and to contribute to it, we have to appreciate the met hodologies that underlie sociolinguistic research, for example, the rules of engagement for joining in the ongoing scientific dis-course. However, as we have highlighted in the methodology discussion (Section 2), the differences in values between the communities can be perceived as a divide. Whereas the CL community has experienced a history in which theory and empiricism are treated as the extreme ends of a spectrum, in the social sciences there is no such dichotomy, and empiricism contributes substantially to theory. Moving forward, research within computational sociolinguistics should build on and seek to partner in extending ex-isting sociolinguistic theories and insights. This requires placing a strong focus on the interpretability of the developed models. Th e feasibility of such a shift in attention can be seen when observing successes of applied c omputational sociolinguistics work that has been adopted in other fields like health communication (Mayfield et al. 2014) and education (Ros X  et al. 2008).

Controlling for Multiple Variables. Sociolinguistic studies typically control for multiple social variables (e.g., gender, age, social class, ethnicity). However, many studies in computational sociolinguistics focus on individual variables (e.g., only gender, or only age), which can be explained by the focus on social media data. The uncontrolled nature of social media makes it challenging to obtain data about the social backgrounds of the speakers and to understand the various biases that such data sets might have. The result is that models are frequently confounded, which results in low interpretability as well as limited justification for generalization to other domains.
 modeling approaches that take a step towar ds addressing these issues (Joshi et al. 2012, 2013). These approaches are very similar to the hierarchical modeling approaches used in sociolinguistic research to control for multiple sources of variation and thus avoid misattributing weight to extraneous variables. A stronger partnership within the field of CL between researchers interested in computational sociolinguistics and researchers interested in multi-domain learning would be valuable for addressing some of the limitations mentioned here. In this regard, inferring demographic variables au-tomatically (see Section 3) may also help, because predicted demographic variables could be used in structuring the models. Another approach is the use of census data when location data is already available. For example, Eisenstein et al. (2014) studied lexical change in social media by using cen sus data to obtain demographic informa-tion for the geographical locations. They justified their approach by assuming that lexical change is influenced by the demographics of the population in these loca-tions, and not necessarily by the demographic s of the particular Twitter users in these locations. 576
Developing Models that Generalize Across Domains. Many of the studies within the area of computational sociolinguistics have focused on a single domain. However, domain effects can influence the findings, such as which features are predictive for gender (e.g.,
Herring and Paolillo 2006). Studies considering multiple domains enable distinguishing variables that work differently in differen t contexts, and therefore improve the inter-pretation of the findings. Recently, several studies within the area of computational sociolinguistics have performed experimen ts across domains (Sarawgi, Gajulapalli, and
Choi 2011; Sap et al. 2014) and explored the effectiveness of domain adaptation ap-proaches (Nguyen, Smith, and Ros X  2011; Piergallini et al. 2014). Another approach involves reconsidering the features used in an attempt to include more features with a deep connection with the predicted variable of interest. For example, Gianfortoni,
Adamson, and Ros X  (2011) show that features such as n -grams, usually reported to be predictive for gender classification, did not perform well after controlling for occupation in a blog corpus, but pattern-based features inspired by findings related to gender-based language practices did.
 Using Sociolinguistics and the Social Sciences as a Source of Inspiration for Methodological
Reflection. Going forward, we need to appreciate where our work stands along an important continuum that represents a fundamental tension in the social sciences: qualitative approaches that seek to preserve the complexity of the phenomena of in-terest, versus quantitative approaches that discretize (but thereby also simplify) the phenomena to achieve more generalizability. For computational linguistics, a primarily quantitative field, work from research areas with a less strong or less exclusive focus on quantitative measures, such as sociolinguis tics and the social sciences, could serve as a source of inspiration for methodological re flection. In this survey, we have questioned the operationalizations of the concepts of g ender (Section 3.2), age (Section 3.3), and language variety (Section 3.4) as discrete and static categories, based on insights from sociolinguistics. More critical reflection on such operationalizations could lead to a deeper insight into the limitations of the dev eloped models and the incorrect predictions that they sometimes make. 6.3 Tuning NLP Tools to Requirements of Sociolinguistics Research
As a final important direction, we should consider what would be required for NLP tools to be supportive for sociolinguistic work.
 Developing Models That Can Guide Users of Data Analysis Systems in Taking Next Steps.
Sociolinguists are primarily interested in new insights about language use. In contrast, much of the work within CL is centered around highly specific analysis tasks that are isolated from scenarios of use, and the focus on the obtained performance figures for such tasks is fairly dominant. As Manning (2015) mentions: "there has been an over-focus on numbers, on beating the state of the art." Only for few analysis methods, validation of the outcomes has been pursued (e.g., have we measured the right thing?) in view of the potential for integration of the models outside lab-like environments. Fur-thermore many of the models developed within CL make use of thousands of features. As a result, their value for practical data ex ploration tasks is therefore often limited.
Sparse models, such as used in Eisenstein, Smith, and Xing (2011), that identify small sets of predictive features would be more suited for exploratory analysis. However, when the focus is on interpretability of the models, we must consider that the resulting average prediction performance of interpre table models may be lower (Piergallini et al. 2014).

Developing Pre-processing Tools to Support the Analysis of Language Variation. The perfor-mance of many developed NLP tools is lower on informal text. For example, POS tag-gers perform less well on texts written by certain user groups (e.g., younger people [Hovy and S X gaard 2015]) or on texts in certain language varieties (e.g., African
American Vernacular English [J X rgensen, Hovy, and S X gaard 2015]). One of the ap-proaches to improve the performance of tools has been to normalize the texts, but as
Eisenstein (2013b) argues, doing so is removing the variation that is central to the study of sociolinguistics. To support deeper sociolinguistic analyses and to go beyond shallow features, we thus need pre-processing tools, such as POS taggers that are able to handle the variation found in informal texts and that are not biased towards certain social groups. 7. Conclusion
Although the computational linguistics field has historically emphasized interpretation and manipulation of the propositional conte nt of language, another valid perspective on language is that it is a dynamic, social e ntity. Some aspects of language viewed from a social perspective are predictable , and thus behave much like other aspects more commonly the target of inquiry in the field, but we must acknowledge that linguistic agency is a big part of how language is used to construct social identities, to build and maintain social relationships, and even to define the boundaries of communities.
The increasing research on social media data has contributed to the insight that text can be considered as a data source that captures multiple aspects and layers of human and social behavior. The recent focus on text as social data and the emergence of computational social science are likely to incr ease the interest within the computational linguistics community on sociolinguistic top ics. In this article, we have defined and set out a research agenda for the emerging field o f  X  X omputational Sociolinguistics. X  We have aimed to provide a comprehensive over view of studies published within the field of CL that touch upon sociolinguistic theme s in order to provide an overview of what has been accomplished so far and where there is room for growth. In particular, we have endeavored to illustrate how the large-scale data-driven methods of our community can complement existing sociolinguistic studi es, but also how sociolinguistics can inform and challenge our methods and assumptions.
 Acknowledgments 578 580 582 584 586 588 590 592
