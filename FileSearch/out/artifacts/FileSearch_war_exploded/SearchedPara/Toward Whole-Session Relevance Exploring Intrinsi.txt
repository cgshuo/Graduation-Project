 Current research on web search has focused on optimizing and evaluating single queries. However, a significant fraction of user queries are part of more complex tasks [20] which span multiple queries across one or more search sessions [26, 24]. An ideal search engine would not only retrieve rele-vant results for a user X  X  particular query but also be able to identify when the user is engaged in a more complex task and aid the user in completing that task [29, 1]. Toward optimizing whole-session or task relevance, we characterize and address the problem of intrinsic diversity (ID) in re-trieval [30], a type of complex task that requires multiple in-teractions with current search engines. Unlike existing work on extrinsic diversity [30] that deals with ambiguity in intent across multiple users, ID queries often have little ambiguity in intent but seek content covering a variety of aspects on a shared theme. In such scenarios, the underlying needs are typically exploratory, comparative, or breadth-oriented in nature. We identify and address three key problems for ID retrieval: identifying authentic examples of ID tasks from post-hoc analysis of behavioral signals in search logs; learn-ing to identify initiator queries that mark the start of an ID search task; and given an initiator query, predicting which content to prefetch and rank.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  retrieval models, search process Algorithms, Experimentation Search session analysis, diversity, proactive search
Information retrieval research has primarily focused on improving retrieval for a single query at a time. However, many complex tasks such as vacation planning, comparative shopping, literature surveys, etc. require multiple queries to complete the task [20].
 remodeling ideas Table 1: Examples of intrinsically diverse search tasks, showing the first (initiator) query and several successor queries from the same search session.

Within the context of this work, we focus on one spe-cific type of information seeking need that drives interaction with web search engines and often requires issuing multiple queries  X  namely intrinsically diverse tasks [30]. Table 1 gives examples of two intrinsically diverse tasks observed in a commercial web search engine. Intrinsic diversity, where diversity is a desired property of the retrieved set of results to satisfy the current user X  X  immediate information need, is meant to indicate that diversity is intrinsic to the need it-self; this is in contrast to techniques that provide diversity to cope with uncertainty in query intent ( e.g. , [jaguar] ).
Intrinsically diverse tasks typically are exploratory, com-prehensive, survey-like, or comparative in nature. They may result from users seeking different opinions on a topic, ex-ploring or discovering aspects of a topic, or trying to ascer-tain an overview of a topic [30]. While a single, comprehen-sive result on the topic may satisfy the need when available, several or many results may be required to provide the user with adequate information [30]. As seen in the examples, a user starting with [snow leopards] may be about to engage in an exploratory task covering many aspects of snow leop-ards including their lifespan, geographic dispersion, and ap-pearance. Likewise when investigating remodeling ideas, a user may wish to explore a variety of aspects including cost, compliance with current codes, and common redecoration options. Note that the user may in fact discover these as-pects to explore through the interaction process itself. Thus intrinsic diversity shares overlap with both exploratory and faceted search [9, 37]. However, unlike the more open-ended paradigm provided by exploratory search, we desire a solu-tion that is shaped by the current user X  X  information need and is able to discover and associate relevant aspects for a topic automatically in a data-driven fashion. For example, for the query [snow leopards] , our goal is to enable deeper user-driven exploration of that topic, by proactively search-ing for the relevant information that the user might want during the course of a session on that topic, thus reducing the time and effort involved in manual reformulations, as-pect discovery, and so on.

To this end, we aim to design a system that addresses two key problems needed for ID retrieval: detecting the start of an ID task, and computing an optimal set of ID documents to return to the user given engagement on an ID task. For the former, the system must be capable of predicting when a user is likely to issue multiple queries to accomplish a task, based on seeing their first  X  X nitiator query X . To do this, we first develop a set of heuristic rules to mine examples of au-thentic intrinsic diversity tasks from the query logs of a com-mercial search engine. The resulting tasks provide a source of weak supervision for training classification methods that can predict when a query is initiating an intrinsically diverse task. With these predictive models, we characterize how ID initiators differ from typical queries. We then present our approach to intrinsically diversifying for a query. In par-ticular, rather than simply considering different intents of a query, we incorporate queries that give rise to related aspects of a topic by estimating the relevance relationship between the aspect and the original query. Given the intrinsically diverse sessions identified through log analysis, we demon-strate that our approach to intrinsic diversification is able to identify more of the relevant material found during a ses-sion given less user effort, and furthermore, the proposed approach outperforms a number of standard baselines.
The distinction between extrinsic and intrinsic diversity was first made by Radlinski et al. who coined these terms [30]. In contrast to extrinsically-oriented approaches, which diver-sify search results due to ambiguity in user intent, intrinsic diversification requires that results are both relevant to a single topical intent as well as diverse across aspects, rather than simply covering additional topical interpretations. Ex-isting methods like maximal marginal relevance (MMR) do not satisfy these requirements well ( cf. Sec. 5.3). Most di-versification research has focused primarily on extrinsic di-versity: this includes learning [39, 34] and non-learning ap-proaches [6, 40, 7, 8]. Recent work [2], however, indicates real-world Web search tasks are commonly intrinsically di-verse and require significant user effort. For example, con-sidering average number of queries, total time, and preva-lence of such sessions, common tasks include: discovering more information about a specific topic (6.8 queries, 13.5 min, 14% of sessions); comparing products or services (6.8 q, 24.8 m, 12%); finding facts about a person (6.9 q, 4.8 m, 3.5%); and learning how to perform a task (13 q, 8.5 m, 2.5%). Thus, improvements in retrieval quality that address intrinsically diverse needs have potential for broad impact. Some previous TREC tracks, including the Interactive, Novelty and QA tracks, studied intrinsic diversity-like prob-lems in which retrieval effectiveness was partly measured in terms of coverage of relevant aspects of queries, along with the interactive cost to a user of achieving good coverage. However, our task and data assumptions differ from these tracks. For example, the Interactive tracks focused more on coverage of fact-or website-oriented answers, while our def-inition of query aspect is broader and includes less-focused subtopics. In addition to optimizing rankings to allow ef-ficient exploration of topics, we also predict queries that initiate intrinsically diverse tasks, and show how to mine candidates for ID tasks from large-scale search log data.
Session -based retrieval is a topic that has become increas-ingly popular. For example, Radlinski et al. [31] studied the benefit of using query chains in a learning-to-rank framework to improve ranking performance. Others have proposed dif-ferent session-level evaluation metrics [17, 21]. Research in this area has been aided by the introduction of the Session track at TREC [22]; this has led to papers on session anal-ysis and classification [27]. In particular, He et al. use a random walk on a query graph to find other related queries, which are then clustered and used as subtopics in their diver-sification system [16]. In our re-ranking approach, we also use related queries to diversify the results, but maintain co-herence with the original query. Specifically, we identify a common type of information need that often leads to longer, more complex search sessions. However, in contrast to pre-vious work, rather than using the session interactions up to the current point to improve retrieval for the current query, we use a query to improve retrieval for a user X  X  future session and use sessions from query logs to evaluate the effective-ness of the proposed methods. While the TREC Session track evaluated the number of uncovered relevant examples for the final query, the emphasis was on the impact of ses-sion context up to the present query; in our case, we assume no previous context, but instead are able to characterize the need for intrinsic diversity based on the single query alone.
Session data has also been used to identify and focus on complex, multi-stage user search tasks that require multiple searches to obtain the necessary information [36, 24]. This has led to research on task-based retrieval [14, 15] where tasks are the unit of interest (as opposed to queries or ses-sions). Trail-finding research studies the influence of factors such as relevance, topic coverage, diversity and expertise [33, 38]. While these problems are certainly related to ours, tasks and trails tend to be more specialized and defined in terms of specific structures: e.g. tasks are characterized as a set or sequence of sub-tasks to be accomplished, while trails are defined in terms of specific paths of user behavior on the web graph. However, intrinsically diverse search sessions, e.g. as in Table 1, represent a broader, less structured category of search behavior. Similarly, our approach complements work on faceted search [23] and exploratory search [37] by pro-viding a data-driven manner of discovering common facets dependent on the particular topic.

Query suggestions are a well-established component of web search results with a large research literature: common approaches include using query similarity ( e.g. [42]) or query-log based learning approaches ( e.g. [19]). Query suggestions can play an important role for intrinsically diverse needs, be-cause they provide an accessible and efficient mechanism for directing users towards potentially multiple diverse sets of relevant documents. Therefore, query suggestion techniques that do not merely provide simple reformulation of the ini-tial query, but correctly diversify across multiple facets of a topic may be particularly helpful for intrinsically diverse needs. Thus, recent research on diversifying query sugges-tions [28] has partly inspired our retrieval approach.
Our approach is also motivated by recent work on interac-tive ranking . Brandt et al. [5] propose the notion of dynamic rankings, where users navigate a path through the search results, to maximize the likelihood of finding documents rel-evant to them. Our objective formulation closely relates to another recent work on two-level dynamic rankings [32], which studied the benefit of interaction for the problem of extrinsic diversity. Similarly, user interaction has been found to help in more structured and faceted search tasks [41, 13], in cases such as product search. However, while presenting interactive, dynamic rankings is one user experience that of-fers a way to surface the improved relevance to users, our techniques are more general: they may be used to present a summary of the topic to the user, recommend unexplored options, anticipate and then crowdsource queries to trade off latency and quality by prefetching, and more.

In contrast to previous work, we provide a way to not only identify complex search tasks that will require multiple queries but to proactively retrieve for future queries before the user has searched for them. Importantly, these future queries are neither simple reformulations nor completely un-related, but are queries on the particular task that the user has started. Finally, we introduce diversification methods which, unlike previous methods, maintain coherence around the current theme while diversifying. Using these methods we demonstrate that we can improve retrieval relevance for a task by detecting an intrinsically diverse need and providing whole-session retrieval at that point.
An intrinsically diverse task is one in which the user re-quires information about multiple, different aspects of the same topical information need. In practice, a user most strongly demonstrates this interest by issuing multiple queries about different aspects of the same topic. We are particu-larly interested in identifying the common theme of an in-trinsically diverse task and when a user initiated the task. We unify these into the concept of an initiator query where, given a set of queries on an intrinsically diverse task, the query among them that is most general and likely to have been the first among these set of queries is called the ini-tiator query. If multiple such queries exist, then the first among them from the actual sequence (issued by the user) is considered the initiator. We give importance to the tem-poral sequence since the goal is to detect the initiation of the task and provide support for it as soon as possible.
While previous work has defined the concept of intrinsic diversity, there has been no further understanding of the problem or means to obtain data. We now identify and ana-lyze authentic instances of intrinsically diverse search behav-ior, extracted from large-scale mining and analysis of query logs from a commercial search engine.
Intuitively, intrinsically diverse (ID) tasks are topically co-herent but cover many different aspects. To automatically identify ID tasks in situ where a user is attempting to ac-complish the task, we seek to codify this intuition. Further-more, rather than trying to cover all types of ID tasks, we focus on extracting with good precision and accuracy a set of tasks where each task is contained within a single search ses-sion. As a  X  X ession X  we take the commonly used approach of demarcating session boundaries by 30 minutes of user inac-tivity [35]. Once identified, these mined instances could po-tentially be used to predict broader patterns of cross-session intrinsic diversity tasks [24, 1], but we restrict this study to mining and predicting the initiation of an ID task within a search session and performing whole-session retrieval at the point of detection.

To mine intrinsically diverse sessions from a post-hoc anal-ysis of behavioral interactions signals with the search results, we developed a set of heuristics to detect when a session is topically coherent but covering many aspects. These can be summarized as finding sessions that are: (1) longer  X  the user must display evidence of exploring multiple aspects; (2) topically coherent  X  the identified aspects should be related to the same overall theme rather than disparate tasks or top-ics; (3) diverse over aspects  X  the queries should demonstrate a pattern beyond simple reformulation by showing diversity. Furthermore, since the user X  X  interaction with the results will be used in lieu of a contextual relevance judgment for evaluation, we also desire that we have some  X  X atisfied X  or  X  X ong-click X  results where we define a satisfied (SAT) click similar to other work as having a dwell of  X  30 s or termi-nating the search session [11, 12].

Given these criteria, we propose a simple algorithm to col-lect intrinsically diverse user sessions. Our algorithm uses a series of filters, explained in more detail below. When we re-fer to  X  X emoving X  queries, we mean they were treated as not having occurred for any subsequent analysis steps. For ses-sions, with the exception of those we  X  X emove X  from further analysis in Step 4, we label all other sessions as intrinsically diverse or regular ( i.e. , not ID). We identify the initiator query as the first query that remains after all query removal steps, and likewise a successor query is any remaining query that follows the initiator in the session. More precisely, we use the following steps (in sequence) to filter sessions: 1. Remove frequent queries: Frequent queries  X  such 2. Collapse duplicates: We collapse any duplicate of 3. Only preserve manually entered queries: To fo-4. Remove sessions with no SAT Document: Since 5. Ensure topical coherence: As ID sessions have a 6. Ensure diversity in aspects: Although we desire 7. Remove long queries: We observed a small frac-8. Threshold the number of distinct aspects: Fi-
Putting everything together, we ran this algorithm on a sample of user sessions from the logs of a commercial search engine from the period April 1 X  X ay 31, 2012. We used log entries generated in the English-speaking United States lo-cale to reduce variability caused by geographical or linguistic variation in search behavior. Starting with 51.2M sessions comprising 134M queries, applying all but the SAT-click fil-ter, with the Number of Distinct Aspects threshold at two, led to more than 497K ID sessions with 7.0M queries. These ID tasks accounted for 1 . 0% of all search sessions in our sam-ple, and 3 . 5% of sessions having 3 queries or more (14.4M sessions) 1 . Further applying the SAT-click filter reduced the number to 390K. Finally, focusing on the more complex ses-sions by setting the Number of Distinct Aspects filter to three, reduced this to 146K sessions.

Given that ID sessions require multiple queries, we hy-pothesize that ID sessions account for a disproportionately larger fraction of time spent searching by all users. To test this, we estimated the time a user spent in a session by the elapsed time from the first query to the last action ( i.e., query or click). Sessions with a single query and no clicks were assigned a constant duration of 5 seconds. Here, the time in session includes the whole session once an ID task was identified in that session. Our hypothesis was confirmed: while ID sessions with at least 2 distinct aspects represented 1.0% of all sessions, they accounted for 4 . 3% of total time spent searching, showing the significant role ID sessions play in overall search activity.

To assess the accuracy of our automatic labeling process, we sampled 150 sessions (75 each from the auto-labeled regu-lar and intrinsic sets) of length at least 2 queries. We ignored single query sessions since those are dominated by regular
Because we do not focus on more complex ID information seeking, such as tasks that span multiple sessions, the true percentage associated with ID tasks is likely to be larger. intents and there may be a bias in labeling. Two assessors were given instructions similar to the description in the first paragraph of Section 3, examples of ID sessions such as those in Table 1, and all of the queries in the session and asked to label each session as regular or ID. The assessors had a 79% agreement with an inter-rater  X  agreement of 0.5875. Using each assessor as a gold-standard and taking the average, on sessions of length two or greater our extraction method has a precision of 73.9% and an accuracy of 73.7% (overall accu-racy is higher because of single query sessions always being regular). Thus, with both good agreement and a moderate to strong accuracy and precision, the method provides a suit-able source of noisy supervised labels. With enough data, we can hope to overcome the noise in the labels (as long as it is unbiased) with an appropriate learning algorithm [3].
Given that we may want to alter retrieval depending on whether the user is seeking intrinsic diversity or not, we ask the question whether we can identify the initiator queries for intrinsically diverse tasks and treat this as a classifica-tion problem. In particular, while in Sec. 3 we used the behavioral signals of interaction between the initiator and successor queries of a session to automatically label queries with a (weak) supervised label, here we ask if we can predict what the label would be in the absence of those interaction signals  X  a necessary ability if we are to detect the user X  X  need for intrinsic diversity in an operational setting. Ulti-mately our goal is to enable a search engine to customize the search results for intrinsic diversity only when appropriate, while providing at least the same level of relevance on tasks predicted to be regular. Recognizing that in most operative settings, it is likely important to invoke a specialized method of retrieval only when confident, we present a precision-recall tradeoff but focus on the high precision portion of the curve.
Data: We used a sample of initiator queries from the in-trinsically diverse sessions described in Sec. 3.1 as our pos-itive examples, and the first queries (after removing com-mon queries as in Step 1 of Sec. 3.1) from regular sessions were used as negative examples. Note that since the la-bel of a query, e.g. [foo] , comes from the session context, it is possible that [foo] occurs in both positive and nega-tive contexts. In order to only train to predict queries that were clearly either ID or regular, we dropped such conflicting queries from the dataset; this only occurred 1 out of every 5K ID sessions. Also to weigh each task equally instead of by frequency, we sample by type: i.e., we treat multiple occurrences of a query in the positive (resp. negative) set as a single occurrence. Finally, we downsample to obtain a 1:1 ratio from the positive and negative sets to create a balanced set. Unless otherwise mentioned, the dataset was sampled to contain 61K queries and split into an 80/5/15 proportion (50000 training, 3000 validation, 8000 test) with no class bias.

Classification: We used SVMs[18] with linear kernels, unless mentioned otherwise. We varied the regularization parameter ( C ) over the values: { 10  X  4 , 2  X  10  X  4 , 5  X  10 ..., 500 , 10 3 } . Model selection was done using the validation Feature Set Examples Cardinality Coverage Normalized? Log? set by selecting the model with the best precision using the default margin score threshold ( i.e. , 0).

Features: The features are broadly grouped into 5 classes as shown in Table 2. Apart from the text and POS tag fea-tures, all other features were normalized to zero mean, unit variance. Features with values spanning multiple orders of magnitude, such as the number of impressions , were first scaled down via the log function. Due to the large scale of our data, coverage of some features is limited. In particu-lar, query classification was done similar to [4] by selecting the top 9.4M queries by frequency from a year X  X  query logs previously in time and then using a click-based weighting on the content-classified documents receiving clicks 2 . Like-wise Stats and QLOG features are built from four months X  worth of query logs and have limited coverage as a result. The query logs chosen to build these features were from pre-vious to April 2012 to ensure a fair experimental setting with no overlap with the data collection period of the intrinsically diverse or regular sessions. We found the coverage of these features to be roughly the same for both the positive and negative classes.

We also note that the cardinality of some feature sets will depend on the training set ( e.g. ,vocabulary size of Text grows with more training data); the values listed in Table 2 are for the default training set of 50,000 queries. Most of our experiments will use all of the 5 feature sets; the effect of using only a subset of the feature sets is explored in Sec. 4.3.
To begin with, we would like to know the precision-recall tradeoff that we can achieve on this problem. Figure 1 shows the precision-recall curve for a linear SVM trained on 50K examples with all the features. The result is a curve with clear regions of high precision, indicating that the SVM is able to identify initiator queries in these regions quite ac-curately. Furthermore, performance is better than random (precision of 50% since classes are balanced) along the entire recall spectrum.

As Table 3 shows, we are able to achieve relatively high precision values at low recall values. For example, we can identify 20% of ID tasks with 80% precision.
We next investigate the effect of using different subsets of the features on performance. The results are shown in Figure 2 and Table 4. First, we note that Stats, QLOG and ODP feature sets help identify only a small fraction of the initiator queries but do so with high precision. On the
For greater coverage this could be extended to a rank-weighted back-off as described in that paper.
 Table 3: Recall at different precision levels and vice-versa for predicting ID task initiation. Precision Figure 1: P-R curve for predicting ID task initiation. other hand, the Text and POS feature sets, which have high coverage, provide some meaningful signal for all the queries, but cannot lead to high precision classification. We also find that a combination of features, such as the Text and Stats features, can help obtain higher precision as well as higher recall than either alone. In fact, such combinations perform almost as well as using all features, which is the best out of all feature combinations. Precision Figure 2: Change in classification performance of initiator queries as feature sets are varied.
To further understand ID initiator queries, we identified the part-of-speech and text features most strongly associ-Table 4: Effect of feature set on precision &amp; recall. T=Text, S=Stats, P=POS, O=ODP, Q=QLOG ated with them, by computing each feature X  X  log-odds ratio (LOR) 3 compared to regular queries. Looking at the top-ranked features by LOR, we found that initiator queries are more likely to use question words (LOR=0 . 41); focus on proper nouns (0 . 40) such as places and people; use more  X  X iller X  words (particles) found in natural language (0 . 27); and when they use general nouns, these tend to be plu-ral (0 . 13) instead of singular (  X  0 . 052). Predominant text features indicated the importance of list-like nouns such as forms, facts, types, ideas (LOR=1 . 59 , 1 . 45 , 1 . 25 , 0 . 92); verbs that are commonly used in questions such as did (1 . 34); and words indicating a broad need such as information and man-ual (1 . 64 , 1 . 18). Strong negative features tend to encode ex-ceptions  X  such as the most negative word lyrics (  X  2 . 25) used to find words to specific songs.
While the previous section discusses the identification of queries that lead to ID tasks, in this section we discuss changes that can be made to the search results page to support queries for ID tasks. Specifically, we propose a re-ranking scheme that looks to satisfy not only the information need of the issued query, but also the future queries that the user is likely to issue later in the session on other aspects of the task. To the best of our knowledge, we are the first to address the problem of jointly satisfying the current query as well as future queries (unlike anticipatory search [25] which focuses solely on the latter).

We will use an interactive ranking-based paradigm here, using an approach related to the two-level rankings proposed in [32]. Given an issued query representing the start of an ID task, we consider rankings where each result can be at-tributed to some aspect of that task. We represent each aspect of the ID task by a related query of the issued query. One way this could be surfaced on a results page for a user is by placing the related query for an aspect adjacent to its corresponding search result. In such a setting, clicking on the related query could lead to results for that query being presented, thus enabling the user to explore documents for that aspect. This brings us to the question of how we find such a ranking.
We first describe precisely what we consider as an inter-active ranking. In response to an initial query q , an inter-active ranking y = ( y D , y Q ) comprises two parts: a ranking
The LOR can be thought of as an approximation to the weight in a single-variable logistic regression. of documents y D = d 1 ,d 2 ,... , which we refer to as the pri-mary ranking; and a corresponding list of related queries y
Q = q 1 ,q 2 ,... , which represent the aspects associated with the documents of the primary ranking. The i th query in the list, q i , represents the aspect associated with d i turally this can also be thought of as a ranked list of (doc-ument, related query) pairs ( d i ,q i ).

Given this structure, let us consider four conditions that comprise a good interactive ranking: 1. Since the documents in the primary ranking were dis-2. As document d i is associated with the aspect repre-3. Aspects should be relevant to the ID task being initi-4. At the same time, the aspects should not be repetitive
We now design a ranking objective function that satisfies these four conditions to jointly optimize the selection of doc-uments and queries ( y D , y Q ). Suppose we have an existing interactive ranking y ( k  X  1) that has k  X  1 (document, related query) pairs, and our goal is to construct a new ranking y ( k ) by adding an optimal (document, related query) pair to y Condition 1 above can be met by selecting d k such that R ( d k | q ) is large, where R ( d | q ) denotes the probability of rel-evance of document d given query q . Condition 2 can be met by selecting d k such that its relevance to the related query q , R ( d k | q k ), is large. Conditions 3 and 4 imply a standard diversification tradeoff, but here we have that the aspects q k should be related to the initial query q and diverse. If we use a similarity function between queries to estimate the relevance between queries, Condition 3 implies that the sim-ilarity function Sim ( q,q k ) between q k and q should be large. Condition 4 requires that the diversity should be maximized between q k and all previous queries Q = q 1 ,...,q k  X  1 Condition 3 and 4 can be jointly obtained by optimizing an MMR-like diversity function [6], Div ( q k , Q ), as described below. Intuitively, we would also like the change in the ob-jective function on adding document-query pair ( d k ,q k the ranking y to be no smaller than what we would gain if adding the pair to a larger ranking y  X  y 0 : that is, the ob-jective function should be monotone and submodular . Sub-modular objectives are desirable because they have the prop-erty that they can be optimized using a simple and efficient greedy algorithm which iteratively computes the next best ( d,q ) pair to add to the ranking. Using the greedy algo-rithm ensures that the computed solution is at least (1  X  times as good as the optimal.

We now consider the following objective satisfying the above conditions 4 : where Q is shorthand for the set of queries Q = { q 1 ,...,q
We omit the straightforward submodularity proof for space reasons. and Div (  X  ) is an MMR-like diversity function defined as Here,  X   X  [0 , 1] and  X  &gt; 0 are parameters, where  X  controls the tradeoff between related query aspect relevance and di-versity while  X  controls the rate at which returns diminish from additional coverage. Finally,  X  i refers to the discount factor for position i : we use the common 1 log counting.

This objective can be interpreted as maximizing an ex-pected utility (the exponential term) of covering related and diverse aspects where the expectation is over the maximum joint relevance of a document to both the initial query and the related query aspect. Furthermore, the joint probability is assumed to be conditionally independent to factor into the two relevance terms.

In this study, we define Sim ( x,y ) as the cosine similarity between word-TF representations of x and y , and Snip ( q is the bag-of-words representation of caption text from the top-10 search results for q j using relevance score R ( d | q alone. The MMR-like term appears within the exponent to ensure the objective is monotone.

Note that while the final objective optimizes for an inter-active ranking, the primary ranking itself aims to present re-sults from other aspects. We optimize this using the greedy algorithm presented in Algorithm 1, which we refer to as the DynRR method. In Alg. 1, the function RelQ ( q ) de-notes a function that returns related queries for query q , and Top ( y D ) returns the top element in the ranking y D . Algorithm 1 Greedy-DynRR(  X , X ,P (  X | X  ) ,q ) 1: ( y D , y Q )  X   X  2: for all q 0  X  RelQ ( q ) do 3: Next ( q 0 )  X  Document Ranking by R (  X | q )  X  R (  X | q 4: for i = 1  X  n do 5: bestU  X  X  X  X  6: for all q 0  X  RelQ ( q ) / y Q do 7: d 0  X  Top ( Next ( q 0 ) / y D ) 9: if v &gt; bestU then 10: bestU  X  v 11: bestQ  X  q 0 12: bestD  X  d 0 13: ( y D , y Q )  X  ( y D , y Q )  X  ( bestD,bestQ ) 14: return y
As the problem of presenting results for both the cur-rent as well as future queries is a new one, we first discuss the evaluation methodology used. In particular, we use two kinds of evaluation metrics:
Primary ranking metrics : To compare against stan-dard non-interactive methods of ranking, we simply evalu-ate the quality of the primary ranking, i.e., completely ig-nore the related query suggestions attributed to documents. Since our goal is whole-session relevance , documents are con-sidered relevant if and only if they are relevant to any query in the session. Given this notion of relevance, we compute the Precision, MAP, DCG and NDCG values.
 Table 5: Datasets used in re-ranking experiments
Interactive ranking metrics : To evaluate the offline effectiveness and accuracy of the predicted future aspects (queries) and results (documents), we need to assume some model of human interaction. Consider the following search user model: 1. Users begin at the top of the ranking. 2. They click/expand the related query attributed to a 3. On expanding the related query, the user views the top 4. Users ignore previously seen documents, and click on Under this user model, we can easily trace the ranking of documents that the user navigates and thus evaluate Preci-sion@10 and DCG@10 for this ranking. We refer to these metrics as PrecU k and DCGU k , and compare them with the primary Prec@10 and DCG@10 metrics.

We do not claim that this user model accurately captures all online users, nor that it is sophisticated. This is simply a well-motivated model for analyzing a rational user X  X  actions, assuming the user is relatively accurate at predicting the relevance of an aspect based on either the top document or its related query. This in turn is intended to inform us about trends and relative differences we may see in online studies.
DATA: To evaluate the efficacy of the method, we used the data obtained from mining the search logs, as described in Section 3. We used two main datasets shown in Table 5. To analyze impact when most of the sessions are ID and more complex, the MINED dataset is obtained directly from the filtering algorithm by setting the threshold on the Number of Distinct Aspects to be 5. To determine the re-ranking impact when sessions may be a mixture of both ID and regular sessions, the MIXED dataset was obtained by predicting when a session was ID using the classifier from Sec. 4 over a mixture of the MINED dataset sessions and a random sample of regular sessions of the same size. More specifically, the combined sessions were split in a 45-10-45 split of training-validation and test sets. The trained clas-sifier was used to classify the test set sessions as being ID or not, based on the initiator query. The sessions predicted as ID formed the MIXED dataset (prediction accuracy of 68.8% over the combined sessions); for those not predicted to be ID, we assume the standard ranking algorithm would be applied and thus relevance would be the same on those. The MIXED dataset is a reflection of an operational setting, where the query issued is used to predict if the resulting ses-sion will be an ID session or not, and the ones predicted to be ID are selected for re-ranking.

Obtaining Probability of Relevance: For our algo-rithm, we required the computation of the conditional rele-vance of a document given a query i.e., R ( d | q ). Thus, to en-
Table 6: The 21 features used to train R(d | q). able easier reproducibility by others, we learned a model us-ing Boosted Regression Trees , on a dataset labeled with the relevance-values for query-document pairs with 20,000 queries using graded relevance judgments (  X  60 documents per query). The features used are given in Table 6. Features were all normalized to 0 mean, unit variance. To obtain the final model, we optimized for NDCG@5.

Baselines: As baselines we used the following methods: We also computed performance of other baselines, such as MMR and relevance-based methods such as BM-25 (using the weighted anchor text), but found them to perform far worse than RelDQ and Baseline and hence do not present the results for such other baselines.

Related Queries: To study the effect of the related queries, we used four different sources: To ensure fairness, the graphs were constructed using data prior to April 2012. For most experiments, we only use the first 3 sources or only the second and third (which we distinguish by the suffix C+S ).

Settings: The parameters for DynRR were set by op-timizing for DCGU 3 on the training data 5 . All numbers reported here are for the test sets. We considered all SAT-clicked results in the session as relevant documents; since we compare relative to the baseline search engine, the as-sumption is that placing the SAT-clicked documents higher is better, rather than being an indication of absolute per-formance. Unless otherwise mentioned, the candidate docu-ment set for re-ranking comprises the union of the top 100 results (from the Baseline method) of the initiator query, and the top 10 results from each related query.
We varied the  X  parameter from 0 to 1 in increments of 0.1, while the  X  parameter was varied across the values { 0 . 1 , 0 . 3 , 1 , 3 , 10 } .
 PREC Mined Mixed DCG Mined Mixed PrecU 1 1.093 1.103 DCGU 1 1.075 1.074 PrecU 2 1.247 1.223 DCGU 2 1.188 1.153 PrecU 3 1.347 1.295 DCGU 3 1.242 1.190 PrecU 5 1.401 1.345 DCGU 5 1.254 1.204 Table 8: Interactive Performance of DynRR for dif-ferent user models (as ratios compared to the Base-line Prec@10 and DCG@10) Mined DCGU 3 34.4 13.0 1.6 9.9 2.7 0.1 DCG@10 19.6 5.2 0.3 12.7 3.8 0.3 Mixed DCGU 3 29.1 12.0 1.6 10.8 3.7 0.2 DCG@10 17.7 6.0 0.8 12.9 4.0 0.2 Table 9: % of sessions for which the metric perfor-mance of DynRR differs from the Baseline DCG@10 by more than a certain threshold.
Primary Evaluation: We first study the re-ranking with-out any interactivity i.e., using the primary ranking metrics to evaluate the quality of the top-level ranking. As seen in the results of Table 7, the re-ranking leads to improvements across the different metrics for both datasets. Thus, even without interactivity, the method is able to outperform the baselines in predicting future results of interest to the user, while also providing results for the current query. In particu-lar, we found the DynRR method works best using the C+S related queries (which we return to later) with 9-11% gains over the baselines at position 10 across the various metrics with 3-5% relative gains. We also find that the method im-proves on the MIXED dataset supporting the question of whether the method can be robustly used in practical sce-narios. Thus we improve an important segment of tasks while maintaining high levels of performance elsewhere; fur-ther improvements to the initiator classification model will improve the robustness further.

Interactive Evaluation: Next we evaluate the perfor-mance of the method while incorporating the interactivity. As seen in Table 8, the added interactivity leads to large increases in both the precision and DCG of the user paths navigated, across the different user models and datasets. In fact, we find 30-40% improvements in precision and 20-25% improvements in DCG, indicating that we are able to do a far better job in predicting future relevant results, and po-tentially, queries. These results also show that the method improvements are relatively robust to the user model.
Robustness: A key concern when comparing a new method against a baseline, is the robustness of the method. In par-ticular, we are interested in the number of queries that are either improved or hurt, when switching from the Baseline method to the proposed re-ranking method. This is par-ticularly crucial for the MIXED dataset, as we would want that the performance on non-ID sessions not be severely af-fected. Table 9 displays the % of examples for which the method either gains or loses above a certain threshold, com-pared to the Baseline method. We see that the number of gains far exceeds the number of losses, especially while com-paring the interactive metric. We should also note that for ASCO 1.179 1.144 1.580 1.386 1.802 Table 10: Performance change on varying the re-lated queries. All measures are @10 and reported as a ratio to the baseline values.
 AreQueriesID? .452 67.1 100 BestInitiatorQ .694 55.3 98.7
Table 11: Annotator agreement on TREC data. both datasets and both metrics, the DynRR method is sta-tistically significantly better than the Baseline method, as measured by a binomial test at the 99.99% significance level.
Effect of related query set: Next we study the impact of the related queries on the method performance, using the MINED dataset. To do so, we considered different combi-nations of the four related query sources: API( A ), Click-Graph( C ), Co-Session( S ) and Oracle( O ). Table 10 shows the results. As we clearly see, the related query source can make a significant impact on both the primary ranking per-formance and the interactive performance. One thing which stands out is the extremely strong performance using the Or-acle related queries, which suggest that any improvements we can make in the quality of the suggested related queries is likely to result in even better overall performance. On the other hand, we see that using the API related queries almost always hurts performance. In fact, simply using only the re-lated queries from the click-graph and the co-session data leads to much better performance than that compared to using the API queries as well. Further analysis reveals that this is due to two reasons: (a) In many cases, the queries re-turned by the API are spelling corrections or reformulations, with no difference in aspect; (b) most importantly though, there is little to no diversity in the queries obtained from the API compared to those from the other sources.
We also ran experiments using the publicly available TREC 2011 Session data (which was constructed with ID topics in mind) using only publicly reproducible components. To do so, three assessors labeled the different sessions as poten-Initiator Method Pr@1 Pr@3 DCG@1 DCG@3 Label Baseline 0.55 0.51 0.87 1.95 Label DynRR 0.61 0.5 1.13 2.09 Table 12: Absolute performance on TREC Session data.  X  indicates significance at p = 0 . 05 by a paired one-tailed t -test. tially being intrinsically diverse or not, based: a) only on the queries issued; and b) on the narration and title of the session as well. We also asked them to label their opinion on the query best suited to be the initiator query, among the queries issued. Annotators were provided the definition of ID sessions as described at the start of Section 3. We found good agreement among the different annotators for all the different labeling tasks, as seen from Table 11. In fact, in 63 of the 76 total sessions all three annotators agreed the ses-sions were ID based on the narration, title, and queries. We used a 50-50 training-test split on all sets, with the training data used for selecting the parameters of the ranking meth-ods. To obtain the conditional relevance R ( d | q ), we trained a regularized linear regression model with features based on the scores of two standard ranking algorithms: BM25 and TFIDF. As labeled data we used the TREC Web data from 2010 and 2011, by converting the graded relevance scores for relevant and above from the { 1 , 2 , 3 } scale to { 1 3 , 1 , 1 } . We used related queries from the Van Dang-Croft [10] method ( Q ) on the ClueWeb  X 09 anchor text, where the starting seed for the random walk would use the most similar anchor text to the query by TFIDF-weighted cosine if an exact match was not available. Our candidate document pool was set similar to the previous experiments. To evaluate, we again use the same metrics as before except using the TREC asses-sor relevance labels instead of clicks. We considered three different candidates for the initiator query: (a) Topic; (b) First query in the session; and (c) Labeled initiator query. As a baseline, we considered the method that ranked as per R ( d | q ). For the DynRR method, we used the titles of the top 10 results of a query (as per the baseline), as the snippet of the query, since snippets were not made available. The results for the primary metric comparison are shown in Ta-ble 12. As we see from the table, the method improves in precision and DCG in most cases, with particularly large im-provements when the title of the topic is used as the initiator query. This matches feedback the assessors gave us that the titles looked much more like the general queries issued by web users; in contrast, the TREC sessions would often start with a specific query before moving to a more general query. It could be that supplying the user with a well-formulated topic description before starting the search task influences the user to search for a particular aspect, rather than issue a more general query as they might when no topic description is explicitly formulated.
In this paper, we studied intrinsically diverse tasks that typically require multiple user searches on different aspects of the same information need. As this is just the first step into this problem, this also opens many interesting future di-rections  X  such as iterative ways to combine the mining and query identification process or extending these techniques to other related problems like exploratory search. In this work, we motivated the problem using real-world data and presented an algorithm to mine data from search logs us-ing behavioral interaction signals within a session. We then looked at the problem of identifying the queries that start these sessions, and treated it as a classification problem, along with an analysis of these queries. Finally, we pre-sented an approach to alter the rankings presented to the user, so as to also provide them information on aspects of the task for which the user will search in the future. We validated our approach empirically using search log data, as well as TREC data, demonstrating significant improvement over competitive baselines in both cases.
