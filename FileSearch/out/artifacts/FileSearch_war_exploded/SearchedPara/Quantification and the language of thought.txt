 Humans can learn and think about many kinds of concepts, incl uding natural kinds such as elephant and water and nominal kinds such as grandmother and prime num ber. Understanding the mental representations that support these abilities is a central c hallenge for cognitive science. This paper proposes that quantification plays a role in conceptual repr esentation X  X or example, an animal X qualifies as a predator if there is some animal Y such that X hunts Y . The concepts we consider are much simpler than real-world examples such as predator, but even simple laboratory studies can provide important clues about the nature of mental represen tation.
 Our approach to mental representation is based on the langua ge of thought hypothesis [1]. As pursued here, the hypothesis proposes that mental represen tations are constructed in a compositional language of some kind, and that the psychological complexit y of a concept is closely related to the length of its representation in this language [2, 3, 4]. F ollowing previous researchers [2, 4], we operationalize the psychological complexity of a concep t in terms of the ease with which it is learned and remembered. Given these working assumptions, t he remaining challenge is to specify the representational resources provided by the language of thought. Some previous studies have relied on propositional logic as a representation language [2, 5], but we believe that the resources of predicate logic are needed to capture the structure of man y human concepts. In particular, we suggest that the language of thought can accommodate relati ons, functions, and quantification, and focus here on the role of quantification.
 Our primary proposal is that quantification is supported by t he language of thought, but that quan-tification over objects is psychologically more natural tha n quantification over features. To test this idea we compare concept learning in two domains which are ver y similar except for one critical difference: one domain allows quantification over objects, and the other allows quantification over features. We consider several logical languages that can be used to formulate concepts in both do-mains, and find that learning times are best predicted by a lan guage that supports quantification over objects but not features.
 Our work illustrates how theories of mental representation can be informed by comparing concept learning across two or more domains. Existing studies work w ith a range of domains, and it is useful to consider a  X  X onceptual universe X  that includes these pos sibilities along with many others that have not yet been studied. Table 1 charts a small fragment of this u niverse, and the penultimate column shows example stimuli that will be familiar from previous st udies of concept learning. Previous studies have made important contributions by choosing a sin gle domain in Table 1 and explaining why some concepts within this domain are easier to learn than others [2, 4, 6, 7, 8, 9]. Comparisons across domains can also provide important information abou t learning and mental representation, and we illustrate this claim by comparing learning times acr oss Domains 3 and 4.
 The next section introduces the conceptual universe in Tabl e 1 in more detail. We then present a formal approach to concept learning that relies on a logical language and compare three candidate languages. Language OQ (for object quantification) supports quantification over ob jects but not fea-tures, language F Q (for feature quantification) supports quantification over f eatures but not objects, and language OQ + F Q supports quantification over both objects and features. We u se these lan-guages to predict learning times across Domains 3 and 4, and p resent an experiment which suggests that language OQ comes closest to the language of thought. Table 1 provides an organizing framework for thinking about the many domains in which learning can occur. The table includes 8 domains, each of which is defin ed by specifying some number of objects, features, and relations, and by specifying the ran ge of each feature and each relation. We refer to the elements in each domain as items , and the penultimate column of Table 1 shows items from each domain. The first row shows a domain commonly used by studies of Boolean concept learning. Each item in this domain includes a single object a and specifies whether that object has value v 1 (small) or v 2 (large) on feature F (size), value v 3 (white) or v 4 (gray) on feature G (color), and value v 5 (vertical) or v 6 (horizontal) on feature H (texture). Domain 2 also includes three features, but now each item includes three objects and each feature applies to only one of the objects. For example, feature H (texture) applies to only the third object in the domain (i.e . the third square on each card). Domain 3 is similar to Domain 1, but now t he three features can be aligned X  for any given item each feature will be absent (value 0) or pre sent. The example in Table 1 uses three features (boundary, dots, and slash) that can each be added t o an unadorned gray square. Domain 4 is similar to Domain 2, but again the feature values can be ali gned, and the feature for each object will be absent (value 0) or present. Domains 5 and 6 are simila r to domains 2 and 4 respectively, but each one includes relations rather than features. In Domain 6, for example, the relation R assigns value 0 (absent) or value 1 (present) to each undirected pair of objects.
 The first six domains in Table 1 are all variants of Domain 1, wh ich is the domain typically used by studies of Boolean concept learning. Focusing on six relate d domains helps to establish some of the dimensions along which domains can differ, but the final two d omains in Table 1 show some of the many alternative possibilities. Domain 7 includes two cate gorical features, each of which takes three rather than two values. Domain 8 is similar to Domain 6, but no w the number of objects is 6 rather than 3 and relation R is directed rather than undirected. To mention just a handfu l of possibilities which do not appear in Table 1, domains may also have categori cal features that are ordered (e.g. a size feature that takes values small, medium, and large), c ontinuous valued features or relations, relations with more than two places, and objects that contai n sub-objects or parts.
 Several learning problems can be formulated within any give n domain. The most basic is to learn a single item X  X or example, a single item from Domain 8 [4]. A sec ond problem is to learn a class of items X  X or example, a class that includes four of the items in D omain 1 and excludes the remaining four [6]. Learning an item class can be formalized as learnin g a unary predicate defined over items, and a natural extension is to consider predicates with two or more arguments. For example, problems of the form A is to B as C is to ? can be formulated as problems where the task is to learn a bina ry relation analogous ( , ) given the single example analogous ( A, B ) . Here, however, we focus on the task of learning item classes or unary predicates.
 Since we focus on the role of quantification, we will work with domains where quantification is appropriate. Quantification over objects is natural in case s like Domain 4 where the feature values for all objects can be aligned. Note, for example, that the st atement  X  X very object has its feature X  picks out the final example item in Domain 4 but that no such sta tement is possible in Domain 2. Quantification over features is natural in cases like Domain 3 where the ranges of each feature can be aligned. For example,  X  X bject a has all three features X  picks out the final example item in Dom ain 3 but no such statement is possible in Domain 1. We therefore fo cus on Domains 3 and 4, and explore the problem of learning item classes in each domain.
Domain specification # Objects O Features Relations Example Items
Ref. Figure 1: (a) A stimulus lattice for domains (e.g. Domains 3, 4, and 6) that can be encoded as a triple of binary values where 0 represents  X  X bsent X  and 1 represents  X  X resent. X  (b) If the order of the values in the triple is not significant, there are 10 disti nct ways to partition the lattice into two classes of four items. The SHJ type for each partition is show n in parentheses.
 Domains 3 and 4 both include 8 items each and we will consider c lasses that include exactly four of these items. Each item in these domains can be represented as a triple of binary values, where 0 indicates that a feature is absent and value 1 indicates that a feature is present. Each triple represents the values of the three features (Domain 3) or the feature val ues for the three objects (Domain 4). By representing each domain in this way, we have effectively adopted domain specifications that are simplifications of those shown in Table 1. Domain 3 is repr esented using three features of the form F, G, H : O  X  X  0 , 1 } , and Domain 4 is represented using a single feature of the for m F : O  X  X  0 , 1 } . Simplifications of this kind are possible because the featu res in each domain can be aligned X  X otice that no corresponding simplifications are possible for Domains 1 and 2. The eight binary triples in each domain can be organized into the lattice shown in Figure 1a. Here we consider all ways to partition the vertices of the lattice into two groups of four. If partitions that differ only up to a permutation of the features (Domain 3) or o bjects (Domain 4) are grouped into equivalence classes, there are ten of these classes, and a re presentative of each is shown in Figure 1b. Previous researchers [6] have pointed out that the stimuli i n Domain 1 can be organized into a cube similar to Figure 1a, and that there are six ways to partition these stimuli into two groups of four up to permutations of the features and permutations of the ra nge of each feature. We refer to these equivalence classes as the six Shepard-Hovland-Jenkins ty pes (or SHJ types), and each partition in Figure 1b is labeled with its corresponding SHJ type label. N ote, for example, that partitions 3 and 4 are both examples of SHJ type III. For us, partitions 3 and 4 ar e distinct since items 000 (all absent) and 111 (all present) are uniquely identifiable, and partition 3 ass igns these items to different classes but partition 4 does not.
 Previous researchers have considered differences between some of the first six domains in Table 1. Shepard et al. [6] ran experiments using compact stimuli (Domain 1) and distributed stimuli (Do-mains 2 and 4), and observed the same difficulty ranking of the six SHJ types in all cases. Their work, however, does not acknowledge that Domain 4 leads to 10 distinct types rather than 6, and therefore fails to address issues such as the relative compl exities of concepts 5 and 6 in Figure 1. Social psychologists [13, 14] have studied Domain 6 and foun d that learning patterns depart from the standard SHJ order X  X n particular, that SHJ type VI (Conce pt 10 in Figure 1) is simpler than types III, IV and V. This finding has been used to support the cl aim that social learning relies on a domain-specific principle of structural balance [14]. We will see, however, that the relative sim-plicity of type VI in domains like 4 and 6 is consistent with a d omain-general account based on representational economy. The conceptual universe in Table 1 calls for an account of lea rning that can apply across many domains. One candidate is the representation length approa ch, which proposes that concepts are mentally represented in a language of thought, and that the s ubjective complexity of a concept is determined by the length of its representation in this langu age [4]. We consider the case where a concept corresponds to a class of items, and explore the ide a that these concepts are mentally represented in a logical language. More formally, a concept is represented as a logical sentence, and the concept includes all models of this sentence, or all items that make the sentence true. The predictions of this representation length approach dep end critically on the language chosen. Here we consider three languages X  X n object quantification language OQ that supports quantifica-tion over objects, a feature quantification language F Q that supports quantification over features, and a language OQ + F Q that supports quantification over both objects and features . Language OQ is based on a standard logical language known as predicate lo gic with equality. The language includes symbols representing objects (e.g. a and b ), and features (e.g. F and G ) and these symbols can be combined to create literals that indicate that an object does ( F a ) or does not have a certain language includes two quantifiers X  X or all (  X  ) and there exists (  X  ) X  X nd allows quantification over objects (e.g.  X  x F x , where x is a variable that ranges over all objects in the domain). Fin ally, language OQ includes equality and inequality relations ( = and 6 = ) which can be used to compare objects and object variables (e.g. = xa or 6 = xy ).
 Table 2 shows several sentences formulated in language OQ . Suppose that the OQ complexity of each sentence is defined as the number of basic propositions i t contains, where a basic proposition Equivalently, the complexity of a sentence is the total numb er of ANDs plus the total number of ORs plus one. This measure is equivalent by design to Feldman  X  X  [2] notion of Boolean complexity when applied to a sentence without quantification. The compl exity values in Table 2 show minimal complexity values for each concept in Domains 3 and 4. Table 2 also shows a single sentence that achieves each of these complexity values, although som e concepts admit multiple sentences of minimal complexity.
 The complexity values in Table 2 were computed using an  X  X num erate then combine X  approach. We began by enumerating a set of sentences according to criteri a described in the next paragraph. Each sentence has an extension that specifies which items in the do main are consistent with the sentence. Given the extensions of all sentences generated during the e numeration phase, the combination phase considered all possible ways to combine these extensi ons using conjunctions or disjunctions. The procedure terminated once extensions corresponding to all of the concepts in the domain had been found. Although the number of possible sentences grows rapidly as the complexity of these sentences increases, the number of extensions is fixed and re latively small ( 2 8 for domains of size 8). The combination phase is tractable since sentences with the same extension can be grouped into a single equivalence class.
 The enumeration phase considered all formulae which had at m ost two quantifiers and which had a complexity value lower than four. For example, this pha se did not include the formula  X  too high). Despite these restrictions, we believe that the c omplexity values in Table 2 are identical to the values that would be obtained if we had considered all p ossible sentences.
 Language F Q is similar to OQ but allows quantification over features rather than objects . For example, F Q includes the statement  X  Q Q a , where Q is a variable that ranges over all features in the domain. Language F Q also allows features and feature variables to be compared fo r equality or inequality (e.g. = QF or 6 = QR ). Since F Q and OQ are closely related, it follows that the F Q complexity values for Domains 3 and 4 are identical to the OQ complexity values for Domains 4 and 3. For example, F Q can express concept 5 in Domain 3 as  X  Q  X  R 6 = QR R a .
 We can combine OQ and F Q to create a language OQ + F Q that allows quantification over both objects and features. Allowing both kinds of quantification leads to identical complexity values for Domains 3 and 4. Language OQ + F Q can express each of the formulae for Domain 4 in Table 2, and these formulae can be converted into corresponding form ulae for Domain 3 by translating each instance of object quantification into an instance of featur e quantification.
 Logicians distinguish between first-order logic, which all ows quantification over objects but not predicates, and second-order logic, which allows quantific ation over objects and predicates. The difference between languages OQ and OQ + F Q is superficially similar to the difference between first-order and second-order logic, but does not cut to the he art of this matter. Since language Table 2: Complexity values C and corresponding formulae for language OQ . Boolean complexity predicts complexity values for both domains that are identi cal to the OQ complexity values shown here for Domain 3. Language F Q predicts complexity values for Domains 3 and 4 that are ident ical to the OQ values for Domains 4 and 3 respectively. Language OQ + F Q predicts complexity values for both domains that are identical to the OQ complexity values for Domain 4.
 OQ + F Q only supports quantification over a pre-specified set of feat ures, it is equivalent to a typed first order logic that includes types for objects and fe atures [15]. Future studies, however, can explore the cognitive relevance of higher-order logic as de veloped by logicians. Now that we have introduced languages OQ , F Q and OQ + F Q our theoretical proposals can be sharply formulated. We suggest that quantification over obj ects plays an important role in mental representations, and predict that OQ complexity will account better for human learning than Bool ean complexity. We also propose that quantification over object s is more natural than quantification over features, and predict that OQ complexity will account better for human learning than both F Q complexity and OQ + F Q complexity. We tested these predictions by designing an exp eriment where participants learned concepts from Domains 3 and 4.
 Method. 20 adults participated for course credit. Each participant was assigned to Domain 3 or Domain 4 and learned all ten concepts from that domain. The it ems used for each domain were the cards shown in Table 1. Note, for example, that each Domain 3 c ard showed one square, and that each Domain 4 card showed three squares. These items are base d on stimuli developed by Sakamoto and Love [12].
 The experiment was carried out using a custom built graphica l interface. For each learning problem in each domain, all eight items were simultaneously present ed on the screen, and participants were able to drag them around and organize them however they liked . Each problem had three phases. During the learning phase, the four items belonging to the cu rrent concept had red boundaries, and the remaining four items had blue boundaries. During the mem ory phase, these colored boundaries were removed, and participants were asked to sort the items i nto the red group and the blue group. If they made an error they returned to the learning phase, and could retake the test whenever they were ready. During the description phase, participants wer e asked to provide a written description of the two groups of cards. The color assignments (red or blue) w ere randomized across participants X  in other words, the  X  X ed groups X  learned by some participant s were identical to the  X  X lue groups X  learned by others. The order in which participants learned t he 10 concepts was also randomized. Model predictions. The OQ complexity values for the ten concepts in each domain are sho wn in Table 2 and plotted in Figure 2a. The complexity values in Fig ure 2a have been normalized so that they sum to one within each domain, and the differences of the se normalized scores are shown in the final row of Figure 2a. The two largest bars in the differen ce plot indicate that Concepts 10 and 5 are predicted to be easier to learn in Domain 4 than in Dom ain 3. Language OQ can express Figure 2: Normalized OQ complexity values and normalized learning times for the 10 c oncepts in Domains 3 and 4. statements like  X  X ither 1 or 3 objects have F  X  (Concept 10 in Domain 4), or  X 2 or more objects have F  X  (Concept 5 in Domain 4). Since quantification over features is not permitted, however, analogous statements (e.g.  X  X bject a has either 1 or 3 features X ) cannot be formulated in Domain 3. Concept 10 corresponds to SHJ type VI, which often emerges as the most difficult concept in studies of Boolean concept learning. Our model therefore predicts t hat the standard ordering of the SHJ types will not apply in Domain 4. Our model also predicts that concepts assigned to the same SHJ type will have different complexities. In Domain 4 the model predicts that Concept 6 will be harder to learn than Concept 5 (both are examples of SHJ type IV), and that Concept 8 will be harder to learn than Concepts 7 or 9 (all three are examples of SHJ type V ).
 Results. The computer interface recorded the amount of time particip ants spent on the learning phase for each concept. Domain 3 was a little more difficult th an Domain 4 overall: on average, Domain 3 participants took 557 seconds and Domain 4 particip ants took 467 seconds to learn the 10 concepts. For all remaining analyses, we consider learni ng times that are normalized to sum to 1 for each participant. Figure 2b shows the mean values for the se normalized times, and indicates the relative difficulties of the concepts within each condition .
 The difference plot in Figure 2b supports the two main predic tions identified previously. Concepts 10 and 5 are the cases that differ most across the domains, and both concepts are easier to learn in Domain 3 than Domain 4. As predicted, Concept 5 is substantia lly easier than Concept 6 in Domain 4 even though both correspond to the same SHJ type. Concepts 7 through 9 also correspond to the same SHJ type, and the data for Domain 4 suggest that Concept 8 is the most difficult of the three, although the difference between Concepts 8 and 7 is not espec ially large.
 Four sets of complexity predictions are plotted against the human data in Figure 3. Boolean com-plexity and OQ complexity make identical predictions about Domain 3, and OQ complexity and OQ + F Q complexity make identical predictions about Domain 4. Only OQ complexity, however, accounts for the results observed in both domains.
 The concept descriptions generated by participants provid e additional evidence that there are psy-chologically important differences between Domains 3 and 4 . If the descriptions for concepts 5 and 10 are combined, 18 out of 20 responses in Domain 4 referred to quantification or counting. One representative description of Concept 5 stated that  X  X ed ha s multiple filled X  and that  X  X lue has one filled or none. X  Only 3 of 20 responses in Domain 3 mentioned qu antification. One representative description of Concept 5 stated that  X  X ed = multiple feature s X  and that  X  X lue = only one feature. X  Figure 3: Normalized learning times for each domain plotted against normalized complexity values predicted by four languages: Boolean logic, OQ , F Q and OQ + F Q .
 These results suggest that people can count or quantify over features, but that it is psychologically more natural to quantify over objects rather than features.
 Although we have focused on three specific languages, the res ults in Figure 2b can be used to evaluate alternative proposals about the language of thoug ht. One such alternative is an extension of Language OQ that allows feature values to be compared for equality. This extended language supports concise representations of Concept 2 in both Domai n 3 ( F a = H a ) and Domain 4 ( F a = F c ), and predicts that Concept 2 will be easier to learn than all ot her concepts except Concept 1. Note, however, that this prediction is not compatible with the dat a in Figure 2b. Other languages might also be considered, but we know of no simple language that wil l account for our data better than OQ . Comparing concept learning across qualitatively differen t domains can provide valuable information about the nature of mental representation. We compared two d omains that that are similar in many respects, but that differ according to whether they include a single object (Domain 3) or multiple objects (Domain 4). Quantification over objects is possible in Domain 4 but not Domain 3, and this difference helps to explain the different learning pattern s we observed across the two domains. Our results suggest that concept representations can incorpor ate quantification, and that quantifying over objects is more natural than quantifying over features.
 The model predictions we reported are based on a language ( OQ ) that is a generic version of first order logic with equality. Our results therefore suggest th at some of the languages commonly con-sidered by logicians (e.g. first order logic with equality) m ay indeed capture some aspects of the  X  X aws of thought X  [16]. A simple language like OQ offers a convenient way to explore the role of quantification, but this language will need to be refined and e xtended in order to provide a more accurate account of mental representation. For example, a c omprehensive account of the language of thought will need to support quantification over features in some cases, but might be formulated so that quantification over features is typically more costl y than quantification over objects. Many possible representation languages can be imagined and a large amount of empirical data will be needed to identify the language that comes closest to the l anguage of thought. Many relevant studies have already been conducted [2, 6, 8, 9, 13, 17], but t here are vast regions of the conceptual universe (Table 1) that remain to be explored. Navigating th is universe is likely to involve several challenges, but web-based experiments [18, 19] may allow it to be explored at a depth and scale that are currently unprecedented. Characterizing the lang uage of thought is undoubtedly a long term project, but modern methods of data collection may support r apid progress towards this goal. [1] J. A. Fodor. The language of thought . Harvard University Press, Cambridge, 1975. [2] J. Feldman. Minimization of Boolean complexity in human concept learning. Nature , 407: [3] D. Fass and J. Feldman. Categorization under complexity : A unified MDL account of human [4] C. Kemp, N. D. Goodman, and J. B. Tenenbaum. Learning and u sing relational theories. In J.C. [5] N. D. Goodman, J. B. Tenenbaum, J. Feldman, and T. L. Griffi ths. A rational analysis of [6] R. N. Shepard, C. I. Hovland, and H. M. Jenkins. Learning a nd memorization of classifications. [7] R. M. Nosofsky, M. Gluck, T. J. Palmeri, S. C. McKinley, an d P. Glauthier. Comparing models [8] M. D. Lee and D. J. Navarro. Extending the ALCOVE model of c ategory learning to featural [9] C. D. Aitkin and J. Feldman. Subjective complexity of cat egories defined over three-valued [10] F. Mathy and J. Bradmetz. A theory of the graceful comple xification of concepts and their [11] R. Vigo. A note on the complexity of Boolean concepts. Journal of Mathematical Psychology , [12] Y. Sakamoto and B. C. Love. Schematic influences on categ ory learning and recognition mem-[13] W. H. Crockett. Balance, agreement and positivity in th e cognition of small social structures. [14] N. B. Cottrell. Heider X  X  structural balance principle as a conceptual rule. Journal of Personality [15] H. B. Enderton. A mathematical introduction to logic . Academic Press, New York, 1972. [16] G. Boole. An investigation of the laws of thought on which are founded t he mathematical [17] B. C. Love and A. B. Markman. The nonindependence of stim ulus properties in human cate-[18] L. von Ahn. Games with a purpose. Computer , 39(6):92 X 94, 2006. [19] R. Snow, B. O X  X onnor, D. Jurafsky, and A. Ng. Cheap and fa st X  X ut is it good? Evaluating
