 Traditional data mining techniques are designed to model a single type of heterogeneity, such as multi-task learning for modeling task heterogeneity, multi-view learning for model-ing view heterogeneity, etc. Recently, a variety of real appli-cations emerged, which exhibit dual heterogeneity, namely both task heterogeneity and view heterogeneity. Examples include insider threat detection across multiple organiza-tions, web image classification in different domains, etc. Ex-isting methods for addressing such problems typically as-sume that multiple tasks are equally related and multiple views are equally consistent, which limits their application in complex settings with varying task relatedness and view consistency. In this paper, we advance state-of-the-art tech-niques by adaptively modeling task relatedness and view consistency via a nonparametric Bayes model: we model task relatedness using normal penalty with sparse covari-ances, and view consistency using matrix Dirichlet process. Based on this model, we propose the NOBLE algorithm using an efficient Gibbs sampler. Experimental results on multiple real data sets demonstrate the effectiveness of the proposed algorithm.
 G.3 [ Probability and Statistics ]: [nonparametric statis-tics]; I.5.1 [ Pattern Recognition ]: Models X  statistical Nonparametric Bayes modeling; multi-task multi-view; Gibbs sampler.
Nowadays, we are facing large amount of data in a va-riety of areas, such as social media, manufacturing, traffic analytics, etc. A common challenge in these areas is how to handle multiple types of data heterogeneity. For exam-ple, in social media, we may have micro-blogs coming from heterogeneous sources, such as Facebook and Twitter, and each micro-blog may be characterized by heterogeneous fea-tures, such as key words, hashtags, number of re-tweets, number of Facebook likes, etc; in manufacturing, we may have products from heterogeneous manufacturing lines, and each product may be characterized by heterogeneous envi-ronmental variables, such as temperature, pressure, etc; in traffic analytics, we can collect traffic information from het-erogeneous geographic locations (e.g., different states), and for each location, we may have heterogeneous traffic indica-tors, such as volume, GPS positions, etc.

Recent years have seen growing interest in addressing prob-lems with multiple types of data heterogeneity [22, 19, 14, 43, 20, 21, 42]. In particular, for problems with dual hetero-geneity, i.e., both task and view heterogeneity, researchers have proposed multi-task multi-view learning, or M 2 TV learning, to jointly learn in multiple related tasks with over-lapping, partially overlapping or completely different feature spaces [22, 43, 21, 42]. Compared with traditional multi-task learning [12, 46, 11, 38, 30], where the feature space is ho-mogeneous across different tasks, i.e., a single view, M 2 learning is able to handle heterogeneous feature spaces; com-pared with traditional multi-view learning [13, 24, 16, 28, 8], where the examples come from a homogeneous task, i.e., a single task, M 2 TV learning is able to leverage heterogeneous (related) tasks to improve the learning performance in each task.

A key question in M 2 TV learning is how to model the relatedness among multiple tasks/views. Existing methods for M 2 TV learning [22, 43, 21, 42] usually assume that all the tasks are equally related, and all the views are equally consistent. Therefore, they mainly focus on exploring vari-ous types of task relatedness and view consistency. In this paper, we go one step further, and study: (1) if all the tasks are equally related and all the views are equally consistent; (2) to what extent the multiple tasks are related and the multiple views are consistent. This is motivated by the fact that in many real applications, it is often not known a priori the degree of relatedness among multiple tasks and consis-tency among multiple views. In the adversarial cases where some tasks are negatively related to the others and some views are contaminated by noise, simply applying the exist-ing methods for M 2 TV learning may even hurt the perfor-mance. Although in traditional multi-task learning, there has already been some work accommodating various task relatedness [45, 46, 12, 11], to the best of our knowledge, our work is the first to study this problem in the context of M 2 TV learning.
 To this end, motivated by the successful application of Bayesian hierarchical modeling in multi-task learning and multi-view learning [19, 3, 5], we propose a nonparametric Bayes method for M 2 TV learning. In this method, task relatedness is modeled via a normal penalty that decom-poses the full covariance matrix into the Kronecker product, and view consistency is modeled via a matrix Dirichlet pro-cess. Furthermore, we design the NOBLE algorithm, which stands for NO nparametric B ayes LE arning with dual het-erogeneity. It is based on an efficient Gibbs sampler scalable to relatively high dimensions. The main contributions of this paper can be summarized as follows. 1. For the first time, in the context of M 2 TV learning, 2. We propose a nonparametric Bayes method for M 2 TV 3. We design the NOBLE algorithm based on an efficient 4. We compare the performance of our proposed NOBLE
The rest of the paper is organized as follows. In Section 2, we briefly review the related work. The nonparametric Bayes method for M 2 TV learning is proposed in Section 3, followed by the algorithm description of NOBLE in Section 4. Section 5 compares NOBLE with state-of-the-art meth-ods on real data sets. Finally, we conclude in Section 6.
In this section, we briefly review the related work in het-erogeneous learning and Dirichlet process mixture models.
The goal of heterogeneous learning is to leverage multiple types of heterogeneities (e.g., task heterogeneity, view het-erogeneity, instance heterogeneity, label heterogeneity, etc) to improve the performance of predictive modeling. For ex-ample, in [22, 23, 33, 43, 19, 21, 42], the authors jointly modeled the task and view heterogeneities; in [41] the au-thors jointly modeled the view and instance heterogeneities; in [26], the authors jointly modeled the instance and label heterogeneities.

For problems with both task and view heterogeneity, the authors of [22] focused on multiple tasks with completely different feature spaces, and proposed to construct a sin-gle prediction model in the shared induced space; the au-thors of [23] proposed to learn shared predictive structures on common views from multiple related tasks, and used the consistency among different views to improve the perfor-mance; the authors of [43] used co-regularization in each task to obtain a linear mapping, and used additional regu-larization functions across different tasks to impose task re-latedness; the authors of [19] proposed a latent probit model to jointly learn the domain transforms, and a probit classifier shared in the common domain; the authors of [42] proposed a large margin framework to address transfer learning prob-lems 1 with the same set of views in the source and target domains; the authors of [21] proposed a graph-based frame-work to model the relationship among multiple tasks/views, and designed an iterative algorithm IteM 2 to find the clas-sification function. The major difference between our work and the existing work is the following. Existing methods assume that all the tasks are equally related and all the views are equally consistent. Therefore, they mainly focus on exploring various kinds of task relatedness and view con-sistency. In our work, we go one step further, and study: (1) if all the tasks are equally related and all the views are equally consistent; and (2) to what extent the multiple tasks are related and the multiple views are consistent.
The problem of varying task relatedness has been stud-ied in traditional multi-task learning. For example, in [37], the authors proposed to use bipartite graphs to represent multi-task learning, and made use of Gaussian process to model varying task relatedness; in [12], the authors pro-posed a robust multi-task learning (RMTL) algorithm that learns multiple tasks simultaneously as well identifies the irrelevant tasks; in [46], the authors showed the equivalent relationship between alternating structure optimization and clustered multi-task learning; etc. However, the above meth-ods and analysis only apply in the multi-task setting, and it is not straightforward to extend them to M 2 TV learning.
In particular, Bayesian modeling has been widely used in multi-task learning and multi-view learning over the last decade. Research work dedicated to Bayesian hierarchical modeling has demonstrated effectiveness and improvement in performance [19, 3, 5]. The proposed methods have been successfully applied to different areas, such as information retrieval [7] and computer vision [30]. Typical approaches to transfer information among multiple tasks/views include: sharing hidden nodes in neural networks, placing a common prior in hierarchical models, sharing a common structure on the predictor space, and structured regularization in kernel methods, among others [19, 38, 9, 40, 39].
In this paper, we propose to use Dirichlet process (DP) prior to encourage view clustering in the context of M 2 TV learning. Before presenting our model, we briefly review DP mixture models. In a Bayesian mixture model, we assume that the true density of the response Y canbewrittenas a mixture of parametric densities, conditioned on a hidden parameter  X  . For example, in a Gaussian mixture,  X  corre-sponds to the mean  X  and variance  X  2 . The marginal prob-ability of an observation is given by a continuous mixture, f ( y )= T f ( y |  X  ) P ( d X  ) , where T is the set of all possible parameters and the prior P is a measure on that space. DP models uncertainty about the prior density P [17, 2]. If P is drawn from a Dirichlet process then it can be analytically integrated out of the conditional distribution of  X  T given  X  1:( T  X  1) ,where  X  T denotes the T th parameter for observa-tion y T . Specifically, the random variable  X  T has a Polya
Transfer learning is very similar to multi-task learning ex-cept that in transfer learning, we only care about the learn-ing performance in the target domain. urn distribution [6]: The above equation reveals the clustering property of the joint distribution of  X  1: T , where there is a positive probabil-ity that each  X  t will take on the value of another  X  t , leading some of the parameters to share values. This equation also makes clear the roles of  X  and G 0 . The unique values of  X  1:( T  X  1) are drawn independently from G 0 ; the parameter  X  controls how likely  X  T is to be a newly drawn value from G rather than to take one of values from  X  1:( T  X  1) . G 0 the distribution of a new component.

In a DP mixture,  X  is a latent parameter to an observed data point y [2]: P  X  DP(  X G 0 ) , X  t  X  P, y t |  X  t  X  f ( amining the posterior distribution of  X  1: T given y 1: T out its interpretation as an  X  X nfinite clustering X  model. Be-cause of the clustering property, observations are grouped by their shared parameters. Unlike finite clustering mod-els, however, the number of groups is random and unknown. Moreover, a new data point can be assigned to a new cluster that was not previously seen in the data.

However, the DP prior does not allow local clustering of tasks/views with respect to a subset of the feature vec-tor without making independence assumptions. Considering sample s ( s =1 ,...,n t ) from task (or view) t ( t =1 ,...,T ), suppose that the response variable is y ts and related feature vector is x ts with dimension n p by 1. A common strat-egy for such problem is to use a hierarchical model of the distribution of y given feature vector x , parameters f and  X  .  X  are global parameters and f =( f 1 ,..., f T )isavec-tor of task-specific (or view-specific) coefficients. We could specify independent DP priors for the coefficients [35, 14]: f tp iid  X  G t ,G t  X  DP (  X  j ,G 0 j )for p =1 ,...,n p . This ap-proach allows differential clustering of the coefficients for different feature components, however, independence is as-sumed across the feature components. This is unappealing, because f tp = f t p provides information that tasks t and t are similar, which should intuitively increase the probability that f tp = f t p ,for p = p . Motivated by this desire to borrow information across related feature components and tasks simultaneously, [35] propose a matrix stick-breaking process (MSBP) by assuming where G = { G tp ,p =1 ,...,n p ,t =1 ,...,T } is a matrix of random probability measures, and P is a probability mea-sure on ( X  , G ), with  X  the space of T  X  n p matrices with the ( t, p )th element a probability measure on ( X t , B t ). Here, is a  X  -algebra of subsets of  X  and B t is a Borel  X  -algebra of subsets of X t , f tp  X  X  t . The proposed MSBP allows sepa-rate clustering and borrowing of information for the different feature components through
To provide an intuitive explanation for the above formu-lation, we first consider the sticks W ph .If W ph is large for a particular index h  X  , then the corresponding parameters  X  note that this sharing among tasks is encouraged by large U this implies that if parameter sharing occurs for one predic-tor among the multiple tasks, then it is also likely that there will be sharing for other predictors. We can therefore gen-eralize the following key properties of MSBP: (i) if a given the tasks, it is more likely to be shared among other tasks; (ii) if sharing occurs between multiple predictors for a subset of tasks, then it is more encouraged that sharing will occur between other predictors within these tasks.
Suppose that we have T tasks and V views in total. For the v th view, there are d v features. For the t th task ( t = 1 ,...,T ), there are n t examples and each example can be represented as x ts =[( x ts 1 ) ,..., ( x tsV ) ] with label  X  y ( s =1 ,...,n t ), where () denotes vector transpose. x tsv R d v denotes the features from the v th view ( v =1 ,...,V ) of the s th example in the t th task, and  X  y ts is either dis-crete for classification problems, or real-valued for regres-sion problems. Notice that if a certain view is missing, the associated features will all be 0. Therefore, our problem set-ting is essentially the same as in [21] where some views are shared by multiple tasks, and some views are task specific. Without loss of generality, suppose that we know the output  X  y ,...,  X  y tm t of the first m t examples, where m t is usually much smaller than n t . Our goal is to leverage both the label information from all the related tasks, as well as the consis-tency among different views of a single task to predict the output of the remaining n t  X  m t examples.
In our proposed model, we first decompose each task into multiple single-view models. Each of them generates a pre-dictor based on the features in the single view, which can be used to make predictions on future unseen examples. Here we relax the common assumption in multi-view learn-ing [8, 29, 34] that different views are conditionally indepen-dent given the class label. To be specific, for the t th task ( t =1 ,...,T ), we use a mixture linear regression model for the estimated output  X  y ts ( s =1 ,...,n t ) by averaging the prediction results from all single-view models as follows: where f tv  X  R d v is the coefficient vector, and tsv  X  R observational error. Based on the above model, we estimate the task relatedness and the view consistency as follows. 1. Task Relatedness : Here we use a Gaussian process de-fined on ts to model the task relatedness. To be spe-cific, we assume that s = { ts } t =1 ,...,T  X  N(0 ,K ), where
K  X  R T  X  T is the kernel matrix of the Gaussian pro-cess, and it is the key to determining the various task relatedness. Different from [37], where only a single in-formation source is used to obtain the kernel function, in this paper, we fully leverage the multi-view property to estimate K inamorereliableway. Tobespecific,in order to estimate K , we define a task graph as follows: the graph consists of T nodes with each node represent-ing a single task; let B  X  R T  X  T denote the adjacency matrix of the graph, whose element in the t th row and where t, t =1 ,...,T ,and &lt;  X  ,  X  &gt; denotes vector inner product. For this graph, we can compute the Laplacian
 X = D  X  B ,where D  X  R T  X  T is a diagonal matrix with each diagonal element equal to the row sum of B .Using  X , we obtain K as follows: where both  X  and  X  2 are positive parameters. In par-ticular,  X  controls the overall sharpness of the distri-bution: large values of  X  mean that the distribution is more peaked around its mean. For more flexibility, we let  X   X  Ga( a, b ), which stands for Gamma distribution with shape parameter a and scale parameter b . It will be adapted to the data through adjusting the distribu-tion related parameters a and b .  X  2 controls the amount of regularization. For this parameter, we could use the following prior  X  2  X  IG( c, d ), which stands for Inverse-
Gamma distribution with shape parameter c and scale parameter d .

We would like to point out several important aspects of the proposed Gaussian process. First, the kernel matrix
K , whose elements indicate the similarity among various tasks, depends on the inverse of the regularized graph
Laplacian  X . Therefore, the relatedness between two tasks is global in the sense that it depends on all the tasks. Second, if we also have unlabeled data in addition to the labeled training data, all the unlabeled data can be used to define the adjacency matrix B (since it does not require label information), thus making it more robust to noise. Finally, the adjacency matrix B depends on the features from all the views through x ts . It tends to be more reliable if certain views have been contaminated by noise. 2. View Consistency : To estimate the various view con-sistency, we jointly model the coefficient vectors f tv ( v = 1 ,...,V ) through:  X   X   X  where  X  vv  X  R d v  X  d v + denotes the covariance matrix be-tween the v th and the v th views.  X  vv = X  v v .

Furthermore, a Dirichlet Process (DP) prior can be used here to encourage view cluster. However, without the conditional independence assumption, the DP prior does not allow local clustering of views with respect to a subset of the feature vectors. To address this problem, we extend the matrix DP prior [15] to define the covariance matrix
 X  vv , which encourages cross-view sharing of data. To be specific, we borrow information by incorporating depen-dency in the prior distributions for the matrices {  X  vv } We start by assuming for v  X  v  X  1,
Here F = { F vv ,V  X  v  X  v  X  1 } is a matrix of random probability measures. Let  X  be the space of symmetric V  X  V matrices and F will be a  X  -algebra of subsets of  X . P is a probability measure on ( X  , F ).

Next, our focus is on the specification of P . Assuming each element in F has a stick-breaking representation, i.e., where W vv = { W vv ,h ,h =1 ,...,  X  X  ,for V  X  v  X  v  X  1, is an array of random stick-breaking weights.
 X  h  X  R d v  X  d v + stands for the latent covariance matrix that is drawn from the base measure G , which usually takes the Inverse-Wishart (IW) distribution. Notice that similar to the usual Dirichlet Process,  X  vv equals to  X  h with probability proportional to W vv ,h l&lt;h (1  X  W vv ,l
Dependency within dimensions of F will be incorporated through dependent stick-breaking weights and the com-mon parametric prior G . For the stick-breaking weights, we decompose them as follows where both  X  vh and  X  v h are random variables with the same Beta distribution,  X &gt; 0 is a parameter in the Beta distribution, and  X  0 &gt; 0 is the scale parameter in the
Gamma distribution. In this way, we guarantee the sym-metric property: W vv ,h = W v v,h . Furthermore, accord-ing to [15], the definition of  X  vh ensures that Therefore, Equation (1) is a valid probability measure.
We use the following example to show the intuition of the above formulation. Let V =4,and V 1 ,...,V 4 stand for the four different views. Then the probability that two covariance matrices  X  V 1 V 2 and  X  V 1 V 3 are same can be computed as follows.

Furthermore, the conditional probability of these two ma-trices being the same given that  X  V 4 V 2 = X  V 4 V 3 can be computed as follows.
From the above equations, we can see that the probability and 1, depending on the value of the parameter  X  .Both converge to 1 in the limit as  X   X  0andto0as  X   X  X  X  .
For the sake of explanation, we assume that d v is a constant for v =1 ,...,V ; otherwise we fill in 0 values to make the dimensionality of each view equal. view 3, 4 are equally correlated in terms of the covariance matrices, then there will be an increased probability that view 1, 2 and view 1, 3 are equally correlated.

Finally, for the base measure G of the view covariance matrix, we consider the following degenerate distribution: where 0  X   X   X  1,  X  is the degrees of freedom of the
Inverse-Wishart distribution,  X  0  X  R d v  X  d v + is the scale matrix. When  X  vv falls into the I 0 cluster, the corre-sponding covariance matrix will be a zero matrix, and the nonsignificant f tv will be set to 0.

Figure 1 shows the graphical representation of the pro-posed model. To generalize, for each example s in the task t ( y st ), task relatedness is characterized through K where  X  controls the overall sharpness of the distribution and  X  controls the amount of regularization. On the other hand, there is a view-specific feature x tsv for the s example in the t th task and we use f tv to characterize v th view effect in the t th task. We extend the DP to characterize the co-variance matrix  X  vv in order to cluster the coefficients W and  X  characterize the stick weights; G characterizes the base measure for DP, which is a degenerate distribution with probability  X  to be the null matrix and with probability 1  X   X  to be an Inverse-Wishart distribution G 0 with degrees of freedom  X  and scale matrix  X  0 .
 Figure 1: Graphical representation for the proposed model.
In this section, we present the NOBLE algorithm, which stands for NO nparametric B ayes LE arning with dual het-erogeneity. It is based on an efficient Gibbs algorithm that is scalable to relatively high dimensions. For simplicity, we assume that n t = S in the following. In particular, each iteration of the Gibbs sampler draws samples through the following sequence. The joint likelihood of the samples is as follows: where with x ts =
The posterior distribution of f tv is proportional to com-bining the joint likelihood and the prior in Equation (1). Therefore we can update f tv jointly from the following con-jugate mulvariate normal distribution:
Similarly, by combining the joint likelihood as in Equation (2) and the prior Ga( a, b ),  X  is updated directly through the following Gamma distribution p (  X  | X  X  X  ):
In each iteration, given the prior IG( c, d ),  X  2 is drawn through:
Since the DP prior implies that D is almost surely dis-crete, the prior will automatically group the m coefficient-specific hyperparameters  X  vv into L clusters  X   X  l ,where L respond to  X   X  l = I d v  X  d v , and the other clusters will not be 0. We denote J vv = l if the ( v,v )th covariance matrix is clusteredinthe l th latent cluster. Our proposed prior can be seen more clearly through the equivalent stick breaking Extending the exact block Gibbs sampler of [36], the joint prior distributions of J vv and a latent variable  X  vv can be written as We implement the following exact block Gibbs sampler steps: (1). Sample  X  vv  X  uniform(0 ,W J vv ), for v  X  v  X  1with  X  (2). Sample J vv for v  X  v  X  1 from the multinomial con-After updating  X  h , with the relationship  X  vh  X  beta(1 , X  ) , X  Ga(1 , X  0 ), we sample  X  through where E ( x ;  X  )=  X  exp(  X   X x ) is the exponential density.
Based on the above discussion, the proposed NOBLE al-gorithm is summarized in Algorithm 1.
 Algorithm 1 NOBLE Algorithm Require: y ts , x tsv ,IN tsv , Kt =1 ,...,T , s =1 ,...,S , Ensure: the initial value for f tv ,  X  ,  X  2 , X  vv and  X  1: for i = 1 to Total number of iterations do 2: for t =1to T do 3: for v,v =1to V do 4: Update f through the multivariate normal distri-5: Update  X  through the Gamma distribution as in 6: Draw  X  2 directly from Inverse Gamma distribu-7: Update DP related parameters using exact block 8: Update  X  using truncated exponential distribu-9: end for 10: end for 11: end for
In this section, we present some experimental results show-ing the effectiveness of the proposed NOBLE algorithm and compare against the following algorithms 3 : 1. regMVMT [44]: an inductive multi-view learning algo-rithm for multiple related tasks through a co-regularized framework.
We did not compare with IteM 2 [21] since in our experi-ments, the features are not guaranteed to be non-negative. As shown in [43], the performance of IteM 2 is not satisfac-tory in this case. 2. SMTL [27]: a Bayesian semi-supervised learning frame-work for problems with multiple tasks using unlabeled data based on Markov random walk. 3. CASO [10]: a multi-task learning algorithm improving the ASO algorithm [1] through a novel regularizer. For all 4 algorithms, we repeat the experiments 10 times and report the average classification error 4 . For regMVMT, the parameters are optimized using cross-validation. For SMTL and CASO, the parameters are set according to [27] and [10] respectively. For the proposed NOBLE algorithm, we simply set non-informative hyperparameters as  X  0 =1,  X  =1 / 2,  X  =2 n p +1 and  X  0 = I n p without prior knowledge about the correlation among the tasks and the relative im-portance of each view in the predictive model of each task. We also performed convergence diagnostics, such as trace plots and Geweke X  X  convergence diagnostic for randomly se-lected parameters. No signs of adverse mixing have been found. All results are based on 3,000 Gibbs sampling itera-tions after a burn-in period of 2,000.

In our experiments, to generate multiple views from the original feature space, we adopt a similar strategy as in [25], and apply different linear/nonlinear dimensionality reduc-tion methods, including ICA with different functions (pow or order 3 polynomial kernel, Tanh, Gaussian, skew) [18], PCA based (PCA, Prob PCA [31], and kernel PCA), MDS, diffusion maps, Laplacian, and Laplacian Eigenmaps [32], resulting in 11 views total. 20 newsgroups data set. We first consider the 20 news-groups data set [4]. This data set consists of articles from 20 different newsgroups forming a hierarchical structure. Here we focus on the  X  X omp X  and  X  X ec X  categories (similar experi-mental results are observed for the other categories and thus omitted for brevity), and create 4 tasks from them. To be specific, for each task, we pick one subcategory from  X  X omp X  and X  X ec X  X espectively and randomly sample 100 articles from each subcategory to form 2 classes, each described by 53975 features.

To test the capability of our proposed algorithm to re-cover data sets with different sparsity, we experiment on data sets with various numbers of labeled examples: vary-ing from randomly selecting 20 to 180 observed samples and use the remaining as test set.

Figure 2 shows the comparison results of the 4 algorithms with varying training set size. Each subfigure shows the av-erage classification error for a single task. From these figures, we can see that the performance of NOBLE dominates the other methods, and the margin becomes more significant as the number of labeled examples increases. This is because NOBLE is able to learn from data: (1) if all the tasks/views are related, and (2) how much they are related to each other.
Figure 3 shows that by using the DP prior, we are able to partition the 11 views into 2 groups roughly: one consists of 7 views generated using ICA and PCA based dimensionality reduction methods, and the other consists of 4 views gener-ated using MDS, diffusion maps, Laplacian, and Laplacian Eigenmaps. In each iteration of the algorithm, there is a positive probability that  X  V i V j = X  V i V j for every j = j . Two views j = j are said to be clustered in terms of shar-ing covariance matrices if and only if  X  V i V j = X  V i V Figure 3 is the average over the iterations. The clustering
For the sake of clarity, we did not display the error bars. of the views encoded by the ties among the covariance ma-trices will simply be referred to as the  X  X lustering of the views X , although it should be understood that it is the data themselves that are clustered. The fact that our model in-duces ties among the views is the means by which it borrows strength across objects for estimation.
 WebKB data set. Next we test the performance of NO-BLE on WebKB data set, where the goal is to classify whether a web page is course related or not [8]. We also create 4 tasks from this data set, each including 200 web pages collected from the same university.
 Figure 3: NOBLE clustering probability for 11 views of the 20 newsgroups data.

Figure 4 shows the comparison results with varying train-ing set size. Similarly as before, we can see that the perfor-mance of NOBLE is better than the other 3 competitors in each of the 4 tasks.
 Email spam data set. Finally, we compare on the email spam data set from ECML 2006 discovery challenge. 5 The goal is to classify if each email is spam or ham. In prob-lem A, There are 3 users with 2,500 emails each, which are considered as 3 related tasks.

Comparison results are shown in Figure 5. On this data set, we also see improved performance of NOBLE over the competitors except for Task 1: when the training set size is small, NOBLE and CASO are pretty close to each other; when the training set size is large, the performance of NO-BLE is consistently improved whereas the performance of CASO fluctuates. We notice that throughout the exten-http://www.ecmlpkdd2006.org/challenge.html . sive experiments, regMVMT cannot perform well when the training sample size is small.
 We also test the computation time per iteration in NO-BLE as we vary the training set size, which is shown in Figure 6. From this figure, we can see that NOBLE scales linearly with respect to the total number of labeled exam-ples, thus it is scalable to relatively large data sets.
In this paper, we propose a nonparametric Bayes model for addressing problems with dual-heterogeneity, i.e., task heterogeneity (multiple related tasks) and view heterogene-ity (multiple views). Compared with state-of-the-art tech-niques which assume that the tasks are equally related and the views are equally consistent, we aim at answering the fol-lowing two questions: (1) Are all the tasks equally related and all the views equally consistent? (2) To what extent are the tasks related to each other, and the views consistent with each other? To this end, we make use of the normal penalty with sparse inverse covariances and the matrix DP prior to adaptively learn the task relatedness and the view consistency. Furthermore, we propose the NOBLE algo-rithm based on an efficient Gibbs sampler, which constructs predictors for all the tasks leveraging both the multi-task and multi-view nature. Experimental results on several real data sets show that NOBLE outperforms existing methods in M 2 TV learning. Figure 6: Computation time per iteration of NO-BLE  X  [1] R. Ando and T. Zhang. A framework for learning [2] C. Antoniak. Mixtures of dirichlet processes with [3] C. Archambeau, S. Guo, and O. Zoeter. Sparse [4] A. Asuncion and D. Newman. UCI machine learning [5] B. Bakker and T. Hesks. Task clustering and gating [6] D. Blackwell and J. MacQueen. Ferguson distributions [7] D. M. Blei, T. L. Griffiths, M. I. Jordan, and J. B. [8] A. Blum and T. M. Mitchell. Combining labeled and [9] D. Burr and H. Doss. A bayesian semiparametric [10] J.Chen,T.Lei,J.Liu,andJ.Ye.Aconvex [11] J. Chen, J. Liu, and J. Ye. Learning incoherent sparse [12] J. Chen, J. Zhou, and J. Ye. Integrating low-rank and [13] C. Christoudias, R. Urtasun, and T. Darrell. [14] L. Ding, A. Yilmaz, and R. Yan. Interactive image [15] D. Dunson, Y. Xue, and L. Carin. The matrix stick-[16] J. Farquhar, D. Hardoon, H. Meng, J. Shawe-Taylor, [17] T. Ferguson. A bayesian analysis of some [18] H. Gavert, J. Hurri, J. Sarela, and A. Hyvarinen. The [19] S. Han, X. Liao, and L. Carin. Cross-domain multitask [20] M. Harel and S. Mannor. Learning from multiple [21] J. He and R. Lawrence. A graphbased framework for [22] J. He, Y. Liu, and Q. Yang. Linking heterogeneous [23] X. Jin, F. Zhuang, S. Wang, Q. He, and Z. Shi. Shared [24] S. M. Kakade and D. P. Foster. Multi-view regression [25] A. Kumar, P. Rai, and H. D. III. Co-regularized [26] Y.-X. Li, S. Ji, S. Kumar, J. Ye, and Z.-H. Zhou. [27] Q. Liu, X. Liao, and L. Carin. Semi-supervised [28] I. Muslea, S. Minton, and C. A. Knoblock. Active + [29] K. Nigam and R. Ghani. Analyzing the effectiveness [30] J. O X  X ullivan and S. Thrun. Discovering structure in [31] M. Tipping and C. Bishop. Probabilistic principal [32] L. van der Maaten, E. Postma, and H. van den Herik. [33] H.Wang,F.Nie,H.Huang,S.L.Risacher,A.J.
 [34] W. Wang and Z.-H. Zhou. A new analysis of [35] Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. [36] C. Yau, O. Papaspiliopoulos, G. Roberts, and [37] K. Yu and W. Chu. Gaussian process models for link [38] K.Yu,A.Schwaighofer,andV.Tresp.Learning [39] K. Yu, A. Schwaighofer, V. Tresp, W. Ma, and [40] K. Yu, V. Tresp, and S. Yu. A nonparametric [41] D. Zhang, J. He, and R. D. Lawrence. Mi2ls: [42] D. Zhang, J. He, Y. Liu, L. Si, and R. D. Lawrence. [43] J. Zhang and J. Huan. Inductive multi-task learning [44] J. Zhang and J. Huan. Inductive multi-task learning [45] Y. Zhang and D.-Y. Yeung. A convex formulation for [46] J. Zhou, J. Chen, and J. Ye. Clustered multi-task
