 The process of training classifiers with small amounts of labeled data and relatively large amounts of unlabeled data is known as semi-supervised learning (SSL). In many applications, such as speech recognition, annotating training data is time-consuming, tedious and error-prone. SSL lends itself as a useful technique in such situations as one only needs to annotate small amounts of data for training models. For a survey of SSL algorithms, see [1, 2]. In this paper we focus on graph-based SSL [1]. Here one assumes that the labeled and unlabeled samples are embedded within a low-dimensional manifold expressed by a graph  X  each data sample is represented by a vertex within a weighted graph with the weights providing a measure of similarity between vertices. Some graph-based SSL approaches perform random walks on the graph for inference [3, 4] while others optimize a loss function based on smoothness constraints derived from the graph [5, 6, 7, 8]. Graph-based SSL algorithms are inherently non-parametric, transductive and discriminative [2]. The results of the benchmark SSL evaluations in chapter 21 of [1] show that graph-based algorithms are in general better than other SSL algorithms.
 Most of the current graph-based SSL algorithms have a number of shortcomings  X  (a) in many cases, such as [6, 9], a two class problem is assumed; this necessitates the use of sub-optimal extensions like one vs. rest to solve multi-class problems, (b) most graph-based SSL algorithms (exceptions in-clude [7, 8]) attempt to minimize squared error which is not optimal for classification problems [10], and (c) there is a lack of principled approaches to integrating class prior information into graph-based SSL algorithms. Approaches such as class mass normalization and label bidding are used as a post-processing step rather than being tightly integrated with the inference. To address some of the above issues, we proposed a new graph-based SSL algorithm based on minimizing a Kullback-Leibler divergence (KLD) based loss in [11]. Some of the advantages of this approach include, straight-forward extension to multi-class problems, ability to handle label uncertainty and integrate priors. We also showed that this objective can be minimized using alternating minimization (AM), and can outperform other state-of-the-art SSL algorithms for document classification.
 Another criticism of previous work in graph-based SSL (and SSL in general) is the lack of algo-rithms that scale to very large data sets. SSL is based on the premise that unlabeled data is easily obtained, and adding large quantities of unlabeled data leads to improved performance. Thus prac-tical scalability (e.g., parallelization), is very important in SSL algorithms. [12, 13] discuss the application of TSVMs to large-scale problems. [14] suggests an algorithm for improving the induc-tion speed in the case of graph-based algorithms. [15] solves a graph transduction problem with 650,000 samples. To the best of our knowledge, the largest graph-based problem solved to date had about 900,000 samples (includes both labeled and unlabeled data) [16]. Clearly, this is a fraction of the amount of unlabeled data at our disposal. For example, on the Internet alone we create 1.6 billion blog posts, 60 billion emails, 2 million photos and 200,000 videos every day [17]. The goal of this paper is to provide theoretical analysis of our algorithm proposed in [11] and also show how it can be scaled to very large problems. We first prove that AM on our KLD based objective converges to the true optimum. We also provide a test for convergence and discuss some theoretical connections between the two SSL objectives proposed in [11]. In addition, we propose a graph node ordering algorithm that is cache cognizant and makes obtaining a linear speedup with a parallel implementation more likely. As a result, the algorithms are able to scale to very large datasets. The node ordering algorithm is quite general and can be applied to graph-based SSL algorithm such as [5, 11]. In one instance, we solve a SSL problem over a graph with 120 million vertices. We use the phone classification problem to demonstrate the scalability of the algorithm. We believe that speech recognition is an ideal application for SSL and in particular graph-based SSL for several reasons: (a) human speech is produced by a small number of articulators and thus amenable to representation in a low-dimensional manifold [18]; (b) annotating speech data is time-consuming and costly; and (c) the data sets tend to be very large. and D , {D l , D u } . Here r i is an encoding of the labeled data and will be explained shortly. We are interested in solving the transductive learning problem, i.e., given D , the task is to predict the labels of the samples in D u . The first step in most graph-based SSL algorithms is the construction of an undirected weighted graph G = ( V,E ) , where the vertices (nodes) V = { 1 ,...,m } , m = l + u , are the data points in D and the edges E  X  V  X  V . Let V l and V u be the set of labeled and unlabeled vertices respectively. G may be represented via a symmetric matrix W which is referred to as the weight or affinity matrix. There are many ways of constructing the graph (see section 6.2 in [2]). In this paper, we use symmetric k -nearest neighbor (NN) graphs  X  that is, we first form w ij , [ W ] ij = sim( x i , x j ) and then make this graph sparse by setting w ij = 0 unless i is one of j  X  X  k nearest neighbors or j is one of i  X  X  k nearest neighbors. It is assumed that sim( x,y ) = sim( y,x ) . Let N ( i ) be the set of neighbors of vertex i . Choosing the correct similarity measure and |N ( i ) | are crucial steps in the success of any graph-based SSL algorithm as it determines the graph [2]. For each i  X  V and j  X  V l , define probability measures p i and r j respectively over the measurable space (Y , Y ) . Here Y is the  X  -field of measurable subsets of Y and Y  X  N (the set of natural numbers) is the space of classifier outputs. Thus | Y | = 2 yields binary classification while | Y | &gt; 2 implies multi-class. As we only consider classification problems here, p i and r i are multinomial the labels are known with certainty, then r j is a  X  X ne-hot X  vector (with the single 1 at the appropriate position in the vector). r j is also capable of representing cases where the label is uncertain, i.e., for example when the labels are in the form of a distribution (possibly derived from normalizing scores representing confidence). It is important to distinguish between the classical multi-label problem various values of the output. As p i , r i are probability measures, they lie within a | Y | -dimensional probability simplex which we represent using M | Y | and so p i , r i  X  M | Y | (henceforth denoted as M ). Also let p , ( p 1 ,...,p m )  X  M m ,M  X  ...  X  M ( m times) and r , ( r 1 ,...,r l )  X  M l . Consider the optimization problem proposed in [11] where p  X  = min Here H ( p ) =  X  P y p (y) log p (y) is the Shannon entropy of p and D KL ( p || q ) is the KLD between is convex [19]. (  X , X  ) are hyper-parameters whose choice we discuss in Section 5. The first term in C 1 penalizes the solution p i i  X  { 1 ,...,l } , when it is far away from the labeled training data D l , but it does not insist that p i = r i , as allowing for deviations from r i can help especially with noisy labels [20] or when the graph is extremely dense in certain regions. The second term of C 1 penalizes a lack of consistency with the geometry of the data, i.e., a graph regularizer. If w ij is large, we prefer a solution in which p i and p j are close in the KLD sense. The last term encourages each p i to be close to the uniform distribution if not preferred to the contrary by the first two terms. This acts as a guard against degenerate solutions commonly encountered in graph-based SSL [6], e.g., in cases where a sub-graph is not connected to any labeled vertex. We conjecture that by maximizing the entropy of each p i , the classifier has a better chance of producing high entropy results in graph regions of low confidence (e.g. close to the decision boundary and/or low density regions). To recap, C 1 makes use of the manifold assumption, is naturally multi-class and able to encode label uncertainty.
 As C 1 is convex in p with linear constraints, we have a convex programming problem . However, a closed form solution does not exist and so standard numerical optimization approaches such as interior point methods (IPM) or method of multipliers (MOM) can be used to solve the problem. But, each of these approaches have their own shortcomings and are rather cumbersome to implement (e.g. an implementation of MOM to solve this problem would have 7 extraneous parameters). Thus, in [11], we proposed the use of AM for minimizing C 1 . We will address the question of whether AM is superior to IPMs or MOMs for minimizing C 1 shortly.
 Consider a problem of minimizing d ( p,q ) over p  X  P ,q  X  Q . Sometimes solving this problem di-rectly is hard and in such cases AM lends itself as a valuable tool for efficient optimization. It is an The Expectation-Maximization (EM) [21] algorithm is an example of AM. C 1 is not amenable to optimization using AM and so we have proposed a modified version of the objective where (p  X  , q  X  ) = min In the above, a third measure q i ,  X  i  X  V is defined over the measurable space (Y , Y ) , W 0 = W +  X  I n , N 0 ( i ) = {{ i } X  X  ( i ) } and  X   X  0 . Here the q i  X  X  play a similar role as the p i  X  X  and hyper-parameter, plays an important role in ensuring that p i and q i are close  X  i . It should be at least intuitively clear that as  X  gets large, the reformulated objective ( C 2 ) apparently approaches the original objective ( C 1 ). Our results from [11] suggest that setting  X  = 2 ensures that p  X  = q  X  (more While the first term encourages q i for the labeled vertices to be close to the labels, r i , the last term encourages higher entropy p i  X  X . The second term, in addition to acting as a graph regularizer, also acts as glue between the p  X  X  and q  X  X . The update equations for solving C 2 (p , q) are given by p i ( y ) = where  X  i =  X  +  X  P j w 0 ij . Intuitively, discrete probability measures are being propagated between vertices along edges, so we refer to this algorithm as measure propagation (MP).
 When AM is used to solve an optimization problem, a closed form solution to each of the steps of the AM is desired but not always guaranteed [7]. It can be seen that solving C 2 using AM has a single additional hyper-parameter while other approaches such as MOM can have as many as 7. Further, as we show in section 4, the AM update equations can be easily parallelized.
 We briefly comment on the relationship to previous work. As noted in section 1, a majority of the previous graph-based SSL algorithms are based on minimizing squared error [6, 5]. While these objectives are convex and in some cases admit closed-form (i.e., non-iterative) solutions, they require inverting a matrix of size m  X  m . Thus in the case of very large data sets (e.g., like the one we consider in section 5), it might not be feasible to use this approach. Therefore, an iterative update is employed in practice. Also, squared-error is only optimal under a Gaussian loss model and thus more suitable for regression rather than classification problems. Squared-loss penalizes absolute error , while KLD, on the other-hand penalizes relative error (pages 226 and 235 of [10]). Henceforth, we refer to a multi-class extension of algorithm 11.2 in [20] as SQ-Loss .
 The Information Regularization (IR) [7] approach and subsequently the algorithm of Tsuda [8] use KLD based objectives and utilize AM to solve the problem. However these algorithms are motivated from a different perspective. In fact, as stated above, one of the steps of the AM procedure in the case of IR does not admit a closed form solution. In addition, neither IR nor the work of Tsuda use an entropy regularizer, which, as our results will show, leads to improved performance. While the two steps of the AM procedure in the case of Tsuda X  X  work have closed form solutions and the approach is applicable to hyper-graphs, one of the updates (equation 13 in [8]) is a special of the update for p ( n ) i . For more connections to previous approaches, see Section 4 in [11]. We show that AM on C 2 converges to the minimum of C 2 , and that there exists a finite  X  such that the optimal solutions of C 1 and C 2 are identical. Therefore, C 2 is a perfect tractable surrogate for C 1 . In general, AM is not always guaranteed to converge to the correct solution. For example, consider minimizing f ( x,y ) = x 2 + 3 xy + y 2 over x,y  X  R where f ( x,y ) is unbounded below (consider y =  X  x ). But AM says that ( x  X  ,y  X  ) = (0 , 0) which is incorrect (see [22] for more examples). For AM to converge to the correct solution certain conditions must be satisfied. These might include topological properties of the optimization problem [23, 24] or certain geometrical properties [25]. The latter is referred to as the Information Geometry approach where the 5-points property (5-pp) [25] plays an important role in determining convergence and is the method of choice here. Theorem 3.1 ( Convergence of AM on C 2 ). If Proof Sketch : (a) is the 5-pp for C 2 (p , q) . 5-pp holds if the 3-points (3-pp) and 4-points (4-pp) t  X  1 . Next we use the fact that the first-order Taylor X  X  approximation underestimates a convex function to upper bound the gradient of f ( t ) w.r.t t . We then pass this to the limit as t  X  1 and use the monotone convergence theorem to exchange the limit and the summation. This gives the 3-pp. The proof for 4-pp follows in a similar manner. (b) follows as a result of Theorem 3 in [25]. Theorem 3.2 ( Test for Convergence). If { (p ( n ) , q ( n ) ) }  X  n =1 is generated by AM of C 2 (p , q) and C C Proof Sketch : As the 5-pp holds for all p , q  X  M m , it also holds for p = p  X  and q = q  X  . We use fact that E ( f ( Z ))  X  sup z f ( z ) where Z is a random variable and f (  X  ) is an arbitrary function. The above means that AM on C 2 converges to its optimal value. We also have the following theorems that show the existence of a finite lower-bound on  X  such that the optimum of C 1 and C 2 are the same. Lemma 3.3. If C 2 (p , q; w 0 ii = 0) is C 2 when the diagonal elements of the affinity matrix are all zero then we have that Theorem 3.4. Given any A,B,S  X  M m (i.e., A = [ a 1 ,...,a n ] , B = [ b 1 ,...,b n ] , S = [ s then there exists a finite  X  such that Theorem 3.5 ( Equality of Solutions of C 1 and C 2 ). Let  X p = argmin argmin ( q  X p = p  X   X   X  = q  X   X   X  . Further, it is the case that if p  X   X   X  6 = q  X   X   X  , then And if p  X   X   X  = q  X   X   X  then  X   X   X   X   X  . One big advantage of AM on C 2 over optimizing C 1 directly is that it is naturally amenable to a parallel implementation, and is also amenable to further optimizations (see below) that yield a near linear speedup. Consider the update equations of Section 2. We see that one set of measures is held fixed while the other set is updated without any required communication amongst set members, so there is no write contention. This immediately yields a T  X  1 -threaded implementation where the graph is evenly T -partitioned and each thread operates over only a size m/T = ( l + u ) /T subset of the graph nodes.
 We constructed a 10-NN graph using the standard TIMIT training and development sets (see sec-tion 5). The graph had 1.4 million vertices. We ran a timing test on a 16 core symmetric multipro-cessor with 128GB of RAM, each core operating at 1.6GHz. We varied the number T of threads from 1 (single-threaded) up to 16, in each case running 3 iterations of AM (i.e., 3 each of p and q updates). Each experiment was repeated 10 times, and we measured the minimum CPU time over these 10 runs (total CPU time only was taken into account). The speedup for T threads is typically defined as the ratio of time taken for single thread to time taken for T threads. The solid (black) line in figure 1(a) represents the ideal case (a linear speedup), i.e., when using T threads results in a speedup of T . The pointed (green) line shows the actual speedup of the above procedure, typically less than ideal due to inter-process communication and poor shared L1 and/or L2 microprocessor cache interaction. When T  X  4 , the speedup (green) is close to ideal, but for increasing T the performance diminishes away from the ideal case.
 Our contention is that the sub-linear speedup is due to the poor cache cognizance of the algorithm. At a given point in time, suppose thread t  X  { 1 ,...,T } is operating on node i t . The collec-tive set of neighbors that are being used by these T threads are { X  T t =1 N ( i t ) } and this, along with nodes  X  T t =1 { i t } (and all memory for the associated measures), constitute the current working set . The working set should be made as small as possible to increase the chance it will fit in the mi-croprocessor caches, but this becomes decreasingly likely as T increases since the working set is monotonically increasing with T . Our goal, therefore, is for the nodes that are being simultane-ously operated on to have a large amount of neighbor overlap thus minimizing the working set size. Viewed as an optimization problem, we must find a partition ( V 1 ,V 2 ,...,V m/T ) of V that mini-the neighbors of V i would have maximal overlap with the neighbors of V i +1 . We then schedule the T nodes in V j to run simultaneously, and schedule the V j sets successively.
 Of course, the time to produce such a partition cannot dominate the time to run the algorithm itself. Therefore, we propose a simple fast node ordering procedure (Algorithm 1) that can be run once before the parallelization begins. The algorithm orders the nodes such that successive nodes are likely to have a high amount of neighbor overlap with each other and, by transitivity, with nearby nodes in the ordering. It does this by, given a node v , choosing another node v 0 from amongst v  X  X  neighbors X  neighbors (meaning the neighbors of v  X  X  neighbors) that has the highest neighbor overlap. We need not search all V nodes for this, since anything other than v  X  X  neighbors X  neighbors Algorithm 1 Graph Ordering Algorithm
Select an arbitrary node v . while there are any unselected nodes remaining do end while Figure 1: (a) speedup vs. number of threads for the TIMIT graph (see section 5). The process was run on a 128GB, 16 core machine with each core at 1.6GHz. (b) The actual CPU times in seconds on a log scale vs. number of threads for with and without ordering cases. has no overlap with the neighbors of v . Given such an ordering, the t th thread operates on nodes { t,t + m/T,t + 2 m/T,... } . If the threads proceed synchronously (which we do not enforce) the set of nodes being processed at any time instant are { 1 + jm/T, 2 + jm/T,...,T + jm/T } . This assignment is beneficial not only for maximizing the set of neighbors being simultaneously used, but also for successive chunks of T nodes since once a chunk of T nodes have been processed, it is likely that many of the neighbors of the next chunk of T nodes will already have been pre-fetched into the caches. With the graph represented as an adjacency list, and sets of neighbor indices sorted, our algorithm is O ( mk 3 ) in time and linear in memory since the intersection between two sorted lists may be computed in O ( k ) time. This is sometimes better than O ( m log m ) for cases where k 3 &lt; log m , true for very large m .
 We ordered the TIMIT graph nodes, and ran timing tests as explained above. To be fair, the time required for node ordering is charged against every run. The results are shown in figure 1(a) (pointed red line) where the results are much closer to ideal, and there are no obvious diminishing returns like in the unordered case. Running times are given in figure 1(b). Moreover, the ordered case showed better performance even for a single thread T = 1 (CPU time of 539s vs. 565s for ordered vs. unordered respectively, on 3 iterations of AM).
 We conclude this section by noting that (a) re-ordering may be considered a pre-processing (offline) step, (b) the SQ-Loss algorithm may also be implemented in a multi-threaded manner and this is supported by our implementation, (c) our re-ordering algorithm is general and fast and can be used for any graph-based algorithm where the iterative updates for a given node are a function of its neighbors (i.e., the updates are harmonic w.r.t. the graph [5]), and (d) while the focus here was on parallelization across different processors on a symmetric multiprocessor, this would also apply for distributed processing across a network with a shared network disk. In this section we present results on two popular phone classification tasks. We use SQ-Loss as the competing graph-based algorithm and compare its performance against that of MP because (a) SQ-Loss has been shown to outperform its other variants, such as, label propagation [4] and the harmonic function algorithm [5], (b) SQ-Loss scales easily to very large data sets unlike approaches like spectral graph transduction [6], and (c) SQ-Loss gives similar performance as other algorithms that minimize squared error such as manifold regularization [20]. Figure 2: Phone accuracy on the TIMIT test set (a,left) and phone accuracy vs. amount of SWB training data (b,right). With all SWB data added, the graph has 120 million nodes.
 TIMIT Phone Classification: TIMIT is a corpus of read speech that includes time-aligned phonetic transcriptions. As a result, it has been popular in the speech community for evaluating supervised phone classification algorithms [26]. Here, we use it to evaluate SSL algorithms by using fractions of the standard TIMIT training set, i.e., simulating the case when only small amounts of data are labeled. We constructed a symmetrized 10-NN graph ( G timit ) over the TIMIT training and devel-opment sets (minimum graph degree is 10). The graph had about 1.4 million vertices. We used the entire training set. In order to obtain the features, x i , we first extracted mel-frequency cepstral coefficients (MFCC) along with deltas in the manner described in [27]. As phone classification per-formance is improved with context information, each x i was constructed using a 7 frame context window. We follow the standard practice of building models to classify 48 phones ( | Y | = 48 ) and then mapping down to 39 phones for scoring [26].
 We compare the performance of MP against MP with no entropy regularization (  X  = 0 ), SQ-Loss, and a supervised state-of-the-art L2 regularized multi-layered perceptron (MLP) [10]. The hyper-parameters in each case, i.e., number of hidden units and regularization weight in case of MLP,  X  and  X  in the case of MP and SQ-Loss, were tuned on the development set. For the MP and SQ-Loss, the hyper-parameters were tuned over the following sets  X   X  { 1e X 8, 1e X 4, 0.01, 0.1 } and  X   X  { 1e X 8, 1e X 6, 1e X 4, 0.01, 0.1 } . We found that setting  X  = 1 in the case of MP ensured that p = q at convergence. As both MP and SQ-Loss are transductive, in order to measure performance on an independent test set, we induce the labels using the Nadaraya-Watson estimator (see section 6.4 in [2]) with 50 NNs using the similarity measure defined above.
 Figure 2(a) shows the phone classification results on the NIST Core test set (independent of the development set). We varied the number of labeled examples by sampling a fraction f of the TIMIT cases the MLP was trained fully-supervised. We only show results on the test set, but the results on the development set showed similar trends. It can be seen that (i) using an entropy regularizer leads to much improved results in MP, (ii) as expected, the MLP being fully-supervised, performs poorly compared to the semi-supervised approaches, and most importantly, (iii) MP significantly outperforms all other approaches. We believe that MP outperforms SQ-Loss as the loss function in the case of MP is better suited for classification. We also found that for larger values of f (e.g., at f = 1 ), the performances of MLP and MP did not differ significantly. But those are more representative of the supervised training scenarios which is not the focus here.
 Switchboard-I Phone Classification: Switchboard-I (SWB) is a collection of about 2,400 two-sided telephone conversations among 543 speakers [28]. SWB is often used for the training of large vocabulary speech recognizers. The corpus is annotated at the word-level. In addition, less reliable phone level annotations generated in an automatic manner by a speech recognizer with a non-zero error rate are also available [29]. The Switchboard Transcription Project (STP) [30] was undertaken to accurately annotate SWB at the phonetic and syllable levels. As a result of the arduous and costly nature of this transcription task, only 75 minutes (out of 320 hours) of speech segments selected from different SWB conversations were annotated at the phone level and about 150 minutes annotated at the syllable level. Having such annotations for all of SWB could be useful for speech processing in general, so this is an ideal real-world task for SSL. We make use of only the phonetic labels ignoring the syllable annotations. Our goal is to phonet-ically annotate SWB in STP style while treating STP as labeled data, and in the process show that our aforementioned parallelism efforts scale to extremely large datasets. We extracted features x i from the conversations by first windowing them using a Hamming window of size 25ms at 100Hz. We then extracted 13 perceptual linear prediction (PLP) coefficients from these windowed features and appended both deltas and double-deltas resulting in a 39 dimensional feature vector. As with TIMIT, we are interested in phone classification and we use a 7 frame context window to generate x , stepping successive context windows by 10ms as is standard in speech recognition.
 We randomly split the 75 minute phonetically annotated part of STP into three sets, one each for training, development and testing containing 70%, 10% and 20% of the data respectively (the size of the development set is considerably smaller than the size of the training set). This procedure was repeated 10 times (i.e. we generated 10 different training, development and test sets by random sam-pling). In each case, we trained a phone classifier using the training set, tuned the hyper-parameters on the development set and evaluated the performance on the test set. In the following, we refer to SWB that is not a part of STP as SWB-STP . We added the unlabeled SWB-STP data in stages. The percentage, s , included, 0%, 2%, 5%, 10%, 25%, 40%, 60%, and 100% of SWB-STP. We ran both MP and SQ-Loss in each case. When s = 100%, there were about 120 million nodes in the graph! Due to the large size m = 120 M of the dataset, it was not possible to generate the graph using the conventional brute-force search which is O ( m 2 ) . Nearest neighbor search is a well-researched problem with many approximate solutions [31]. Here we make use of the Approximate Nearest Neighbor (ANN) library (see http://www.cs.umd.edu/  X  mount/ANN/ ) [32]. It constructs a modified version of a kd-tree which is then used to query the NNs. The query process requires N ( x i ) is a function that returns the actual NN of x i while N ( x i ) returns the approximate NN. We constructed graphs using the STP data and s % of (unlabeled) SWB-STP data. For all the exper-iments here we used a symmetrized 10-NN graph and = 2 . 0 . The labeled and unlabeled points in the graph changed based on training, development and test sets used. In each case, we ran both the MP and SQ-Loss objectives. For each set, we ran a search over  X   X  { 1e X 8, 1e X 4, 0.01, 0.1 } and  X   X  { 1e X 8, 1e X 6, 1e X 4, 0.01, 0.1 } for both the approaches. The best value of the hyper-parameters were chosen based on the performance on the development set and the same value was used to measure the accuracy on the test set. The mean phone accuracy over the different test sets (and the standard deviations) are shown in figure 2(b) for the different values of s . It can be seen that MP outperforms SQ-Loss in all cases. Equally importantly, we see that the performance on the STP data improves with the addition of increasing amounts of unlabeled data.

