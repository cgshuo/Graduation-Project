 Relevance labels is the essential part of any learning to rank framework. The rapid development of crowdsourcing plat-forms led to a significant reduction of the cost of manual labeling. This makes it possible to collect very large sets of labeled documents to train a ranking algorithm. How-ever, relevance labels acquired via crowdsourcing are typi-cally coarse and noisy, so certain consensus models are used to measure the quality of labels and to reduce the noise. This noise is likely to affect a ranker trained on such labels, and, since none of the existing consensus models directly optimizes ranking quality, one has to apply some heuristics to utilize the output of a consensus model in a ranking al-gorithm, e.g., to use majority voting among workers to get consensus labels.

The major goal of this paper is to unify existing ap-proaches to consensus modeling and noise reduction within a learning to rank framework. Namely, we present a machine learning algorithm aimed at improving the performance of a ranker trained on a crowdsourced dataset by proper remap-ping of labels and reweighting of samples. In the experi-mental part, we use several characteristics of workers/labels extracted via various consensus models in order to learn the remapping and reweighting functions. Our experiments on a large-scale dataset demonstrate that we can significantly improve state-of-the-art machine-learning algorithms by in-corporating our framework.
 Learning to Rank; IR theory; Consensus Models
The fundamental problem faced by many of the of infor-mation retrieval systems is learning to rank . The aim of a ranking algorithm is to provide an ordered set of results, e.g., documents or recommendations, in response to a given query. In the last decade, various machine-learning tech-niques were applied to the construction of such rankers.
In the context of Web search, typically [ 1], ranking algo-rithms are trained with the use of supervised methods, i.e., methods employing a labeled set of query-document pairs. This makes labeling such a set an indispensable step in any learning to rank framework. In the classical setting, training and test datasets are manually labeled by professional asses-sors, who determine the relevance of each result retrieved in response to a given query. To achieve consistency of the la-bels, professional assessors are usually taught to follow very sophisticated and elaborated instructions. Although this ap-proach provides high-quality training datasets and leads to an accurate evaluation of ranking algorithms, it does not scale well and turns out to be comparatively expensive. Recent rapid take-up of crowdsourcing marketplaces, e.g., Amazon MTurk 1 , provides an alternative way of collecting relevance labels. On these marketplaces, employers post la-beling tasks to be completed by a large number of workers for a monetary payment. The human expertise in this scheme turns out to be much cheaper than in the scheme with hired professional assessors. However, labels collected via crowd-sourcing have a number of serious shortcomings: (1) crowd workers are usually not provided with detailed instructions like those compiled for professional assessors, since the ma-jority of them would either refuse or fail to follow compli-cated guidelines; (2) partly due to this, individual workers vary greatly in the quality of their assessments; (3) a large number of workers are spammers, answering randomly or using simple quality agnostic heuristics, see [22 ].
Label noise is likely to have a negative impact on machine learning algorithms. Traditionally, it is dealt with by various noise reduction techniques [ 6, 18 ]. For example, common ap-proaches to noise reduction include cleansing and weighting techniques. Noise cleansing techniques are similar to outlier detection and amount to filtering out samples which  X  X ook like X  mislabeled for some reason. With the weighting ap-proach none of the samples are completely discarded, while their impact on a machine learning algorithm is controlled by weights , representing our confidence in a particular la-bel. In both approaches, one has to use certain heuristics or assume some underlying model for the noise generation.
In the setting of crowdsourced labeling, one can modify the labeling process in order to gain some evidence for each label being correct. Namely, the employers often: (1) pro-vide facile labeling instructions, much simpler than in the case of professional assessors; (2) place  X  X oneypot X  tasks, i.e., tasks with a known true label; (3) assign each task to multi-ple workers in order to evaluate and aggregate their answers. http://www.mturk.com/
The presence of honeypots and multiple labels for each query-document pair in the dataset allows one to use certain crowd consensus models , e.g, [5 , 12 , 23 ]. These models infer a single consensus label for each task, providing more accu-rate labels than those generated by individual crowd work-ers. Consensus models make additional assumptions on the distributions of errors among labels and workers and derive certain quantities that estimate the probabilities of labels being correct. The simplest examples of consensus mod-els are  X  X ajority vote X  and  X  X verage score X , which assign the most frequent/average score to each query-document pair. In practice, crowd consensus models could be used to purify learning to rank datasets by substituting crowd labels with consensus labels or by discarding particular labels with low confidence in their quality. However, this approach is ad-hoc in its nature: the objective of a consensus model is accuracy of output labels, and optimizing the accuracy of labels, one does not necessarily optimize the quality of a ranker, trained on the dataset purified by the consensus model. In fact, our experiments in Section 6 demonstrate that a straightforward utilization of consensus labels within a learning to rank al-gorithm results in suboptimal rankers.

There is another aspect, which is usually not captured by the existing consensus models. Often, assessor instruc-tions are simplified (e.g., a 5-grade scheme is reduced to a 2-grade scheme) to easier attract non-professional asses-sors from crowdsourcing platforms. Unfortunately, while such simplification allows to hire and quickly instruct more workers, it also introduces a bias into their judgements, as they become much less precise and expressive. For instance, some workers are more conservative than the others, thus their positive labels might imply higher relevance than the positive labels of workers who assign them less reservedly. We argue that in such cases it becomes important to have an additional pre-processing mechanism for putting crowd labels on the same more fine-grained scale.

To sum up, with the current state of consensus modeling within a learning to rank problem, many questions remain unsettled: (1) Which consensus model out of many should we use for a particular dataset? (2) How to utilize sev-eral aspects of  X  X uality X  of crowd workers (e.g., experience, agreement rate with other workers, their parameters of the expertise inferred by the consensus models, etc) at once? (3) How to take into account variance in the workers X  interpre-tation of the assessment instructions and put their labels on the same scale?
To address these questions, we propose a learning frame-work that automatically assigns to each learning sample (1) its relevance value and (2) its weight, which captures the confidence in this value. These two quantities are modelled as functions of label features , which may include the out-puts of various consensus models, statistics on a given task, crowd label itself, etc. Our framework trains both functions (one for the relevance value and one for the weight). We as-sume that there is a background learning to rank algorithm which uses the assigned relevance values and the weights of samples. The proposed framework directly optimizes the ranking quality achieved by this background algorithm on the validation set. We refer to the two training steps as label remapping and sample (re)weighting .

With this paper we make the following contributions: 1. Utilize various approaches to consensus modeling and 2. Adopt the reweighting techniques to the problem of 3. Introduce the label remapping step which assigns a rel-
To evaluate our framework, we use two datasets. The first one is provided by Yahoo! within Learning to Rank Challenge 2 (LTRCD for short). LTRCD is one of common datasets for evaluation of learning to rank algorithms [3 ]. Since all samples in LTRCD are labeled by professional as-sessors, we were forced to adapt this dataset to our problem. We simulate errors by randomly corrupting some labels, see Section 6 for details. The second one (referred to as YD) is shared with us by a commercial search engine Yandex and precisely conforms to our problem statement. Experi-mental results demonstrate that our framework significantly outperforms various competitive baselines.

The rest of the paper is organized as follows. Section 2 reviews prior work related to our approach. The details on the YD dataset are provided in Section 3. Section 4 gives an overview of our framework and provides a rigorous formula-tion of the optimization problem we are solving. Section 5 describes the solution to this optimization problem in details and discusses possible extensions to a more general setup. In Section 6 we report the results of various experiments, showing that our approach significantly improves our base-lines. Finally, in Section 7 we discuss some features of our framework and make recommendations for the directions of future research.
Reweighting of samples in a training dataset is a well-established method, applied in various contexts in machine learning. It is one of the classical approaches to the machine learning in the presence of noise and outliers, see [ 18,  X  3].
Sample reweighting is also extensively used in transfer learning tasks , when one tries to use a labeled dataset from a source knowledge domain in order to predict/classify/rank instances in a target domain. The samples in the source and target datasets are often differently distributed, so reweight-ing techniques are used to reduce this difference. In [ 8], Gar-cke and Vanck study inductive transfer with a dataset shift. They suggest supervised (DITL) and unsupervised (KLITL) instance reweighting algorithms. The idea of the reweight-ing optimization step in our framework is slightly similar to the supervised algorithm DITL, since it also tunes weights in order to improve the quality on the target dataset. How-ever, as opposed to our approach, DITL does not address the problem of noise reduction and does not use any label fea-tures. In [ 4], Chen et al. use feature-and instance-transfer learning for cross-domain learning-to-rank. They propose a heuristic method for reweighting samples in the source set. Namely, the authors train a ranking function on a small por-tion of the target dataset and apply it on the source dataset. A query in the source dataset is assigned a weight propor-tional to the number of correctly ranked pairs of documents. This paper also does not deal with label noise and label fea-tures. Unlike the transfer learning task, in our case, samples in the source and target dataset are drawn from the same distribution and are represented by the same set of features. http://research.microsoft.com/en-us/um/beijing/ projects/letor/yahoodata.aspx
While our framework solves a completely different prob-lem, on the technical level, one of its optimization parts resembles the weights optimization method proposed by Ustinovskiy et.al. in [ 20 ]. This paper reweights clicks of users of a search engine in order to train a personalized ranker. Apart from tackling a different problem, our frame-work is also different in two important respects. Firstly, be-sides reweighting, our framework also includes a crucial label remapping step, which significantly advances the resulting quality. Secondly, the method from [20 ] has a serious short-coming: it is targeted solely at simple linear background ranking algorithms. Whereas our framework is extended to a much wider class of background algorithms, including en-semble methods and neural networks.

As we are going to use labels collected via crowdsourcing, we give an overview of the literature on models for crowd consensus. A usual setting in crowdsourcing is the following: each sample is labeled by multiple workers (and each worker labels several samples), these labels are called noisy labels, and the goal is to infer the best consensus label of each sam-ple, which is referred to as the consensus label. Two popu-lar consensus models are Dawid and Skene model (DS) [ 5] and Generative model of Labels, Abilities, and Difficulties (GLAD) [ 23]. Dawid and Skene used latent confusion matri-ces for workers to model their class-specific mistakes. And in GLAD, the probability that a sample is labled correctly depends on both the worker X  X  expertise and the sample X  X  difficulty. We describe these two models in more details in Section 3.

There has been a number of interesting extensions of these two models. [ 11 ] imposed additional priors on confusion ma-trices and studied Bayesian version of DS model, later [ 21] extended it to model communities of workers with similar confusion matrices, which is useful in the situation when each worker provides only a few labels. Works [ 24 , 25 ] extended the DS idea by introducing confusion vectors to encode samples difficulty, and proposed the minimax en-tropy principle for crowdsourcing. [16 ] describes a family of flexible consensus models (including DS and GLAD as special cases); models of this family can make use of addi-tional features of samples and workers to model commonality among workers and samples. Another work [15 ] addresses the problem of learning a classifier when consensus labels are unknown: each sample is represented by a set of fea-tures and has multiple noisy labels, and the paper presents an approach to the joint estimation of workers X  parameters, consensus labels within a classification problem. Unlike our approach, this method is unsupervised and relies on the fixed noise model for the workers. Similar set-up was used [13 ], where the authors aim at estimating class-conditional la-bel noise via unsupervised techniques. For other details on crowdsourcing see recent surveys, e.g., [17 , 14 ].
To the best of our knowledge, this paper is the first work that directly employs outputs of various consensus models within a learning to rank framework. In this framework, a noisy label is associated with a set of features, e.g., likeli-hoods of the noisy labels under various consensus models. In our empirical study, features for noisy labels are gener-ated by the original DS and GLAD models, but any other consensus models can be used similarly.
Prior to formulating the problem considered in this paper, we describe the experimental data in real-world YD dataset. The structure of this data and its features reveal the moti-vation for the particular problem we are solving and clarify the main ideas behind our approach.
 The first dataset consists of 132K query-document pairs. Each query-document pair is assigned three binary relevance labels from three crowd workers at a crowdsourcing platform YandexToloka 3 . The use of binary relevance labels allows to simplify assessment instructions and engage a larger num-ber of workers. With these query-document pairs and crowd labels a dataset X source consisting of 132K  X  3 = 396K sam-ples is formed. One sample in X source is a query-document pair together with one label assigned by one crowd worker. In particular, the same query-document pair may occur in X source with different labels. Besides these query-document pairs, there are 1900 honeypot tasks completed by the same workers (each task is completed by several workers). These are query-document pairs labeled by professional assessors with the same instructions. Usually, honeypots are used to detect and penalize spammer workers. Query-document pairs corresponding to honeypots do not get into X source
The second dataset T is collected in a standard manner for a learning to rank task. Namely, every query-document pair in the second dataset T consisting of 90K samples is judged once by one professional assessor and is endowed with a la-bel, corresponding to one of the relevance classes { Perfect, Excellent, Good, Fair, Bad } . This is the dataset intended for evaluation and for supervised tuning of our framework. Every query-document pair in both datasets T and X source is represented by a feature vector x  X  R N . These are standard ranking features , including text and link relevance, query characteristics, document quality, user behavior fea-tures, etc. Our framework is quite general and any ranking features could be utilized within it. Hence, particular choice of these features is out of the scope of the paper.
Besides ranking features, the samples in the crowdsourced dataset X source are endowed with a number of label features . These features comprise numerical information about the worker who assigned the label as well as about the task, and the label itself. Our framework is applicable to any set of label features, however we describe these features in detail, since their employment within a learning to rank algorithm is one of the main contributions of the paper. Intuitively, the purpose of label features is to provide evidence of the crowd label being correct.

To generate features for labels we utilize two classical con-sensus models. We describe these models assuming binary labeling tasks. For a sample in X source let us denote by w the worker who labeled this sample, by cl ( w )  X  { 0 , 1 } the crowd label itself.

It is assumed that the i th sample has a hidden consensus label t i , which is generated by the multinomial distribution with unknown parameters p = ( p 1 ,p 2 ): Pr( t i = c | p ) = p Each worker w has 2  X  2 confusion matrix  X  ( w ) whose rows describe distributions generating noisy labels for a given value of consensus label. Then a noisy label cl ( w ) i assigned by worker w to sample i is generated by the multinomial distri-bution with parameters  X  ( w ) ( t i ,  X  ): Pr( cl ( w )  X  ( w ) ( c,k ). Assuming that noisy labels assigned by different http://toloka.yandex.com/ workers are independent given the value of consensus label and that samples are independently identically distributed, we can define the joint likelihood of observed noisy labels and hidden consensus labels as the product over all samples: to find the maximum likelihood estimates for p and  X  and consensus labels t , we apply the Expectation-Maximisation (EM) algorithm. After this, we are able to evaluate the likelihood of each noisy label: This consensus model serves as the first source of label fea-tures.

Again, there is a hidden consensus label t i of sample i generated from the multinomial distribution with parame-ters p . Each worker w has parameter a w , which is a real number representing the worker X  X  expertise. And each sam-ple i has parameter d i ranging in (0 , +  X  ] and representing the sample X  X  difficulty. Then if worker w labels sample i , the probability that a noisy label cl ( w ) i is correct is modeled same set of assumptions on the labeling process as in Dawid and Skene and applying the EM algorithm, we obtain esti-mates for p , a , d and consensus labels t . Under this model the likelihood of each noisy label is computed as
Pr( cl ( w ) i | p , a , d , t ) = This is the source for the second set of features that we used for noisy labels.

Besides the outputs of these two models we use several simple statistics on tasks and workers. For a worker w , let n ( w ) be the number of completed tasks, n ( w ) of zero-labeled documents. The complete set of raw label features is listed in Table 1. Given any raw feature f (except cl ( w ) ) we form two label features: f i  X  I ( cl ( w ) = 1) and f M = 1 + 2  X  14 = 29 label features are constructed. Some important details on the dataset are provided in Table 2
In this section, we start with a general description of the problem we are solving. After this, we restrict the scope to a certain class of machine learning algorithms and provide the rigorous formulation within this specific class of algorithms.
As we discussed in the introduction, the subject of this paper lies on the intersection of (1) learning to rank and (2) crowd consensus modeling problems. The ultimate goal is to train, using the source crowdsourced dataset X source ranking function F of query-document features. The over-all performance of a ranking function F on the dataset T is evaluated according to some fixed ranking performance measure M (e.g., DCG, ERR). This performance measure 1 cl ( w ) the crowd label assigned by the worker 2-5  X  ( w ) confusion matrix of the worker in DS 6 a w worker X  X  parameters in GLAD model 7-8 Pr( cl ( w ) = t i ) probability the crowd label is correct 9-10 Pr( cl ( w ) = 1) probability of positive label under 11-12 Pr( cl ( w ) = 0) probability of negative label under 13 log( n ( w ) ) logarithm of the number of tasks, com-14 n ( w ) 0 /n ( w ) fraction of negative labels, assigned by 15 f ( w ) hp fraction of correctly-labeled honeypots is calculated with the use of the labels assigned by profes-sional assessors following a different (more complex) grading scheme. Slightly abusing the notation, we will write simply M ( F ) for the average of metrics M over all queries in T , with documents ranked according to the ranking model F . Remark. Of course, one can use datasets labeled on a crowdsourcing platform both for training and the evaluation of a ranking model. However, in real-life applications, it is crucial to have an evaluation methodology which is as precise and comprehensive as possible. In particular, we want to use a single evaluation scheme for all possible rankers. It is known [10 ] that evaluation based on the graded relevance is more realistic and accurate. Therefore, employment of high-quality fine-graded assessor labels is essential. This potential difference between X source and T is particularly evident in the experimental part, where we have only coarse binary relevance labels in X source as opposed to the standard 5-graded relevance labels in T .

The aim of our framework is to utilize the label features (not to be confused with the ranking features), while train-ing a ranker F , to directly maximize a selected quality per-formance measure M ( F ). The particular choice of label features is not essential for our learning framework, so we do not discuss any specialities of crowd consensus models here. Typically, before training a learning to rank algorithm with the use of crowd-labeled source set, one processes it in advance to reduce noise among the labels. This processing step may include filtration of samples with unreliable labels, ( 1) Source dataset processing, P  X  our focus (2) Background learning to rank algorithm, A (3) Evaluation of the ranker on the target dataset (4) Feedback for step (1) optimization  X  our focus Figure 1: General framework for learning to rank with a crowdsourced dataset. aggregation of labels based on majority voting among work-ers, reweighting of samples, etc. To distinguish the initial source dataset X source from the one actually used by the background learning to rank algorithm, we denote the pro-cessed dataset by X train .
 The outline of the global learning scheme is depicted in Figure 1. We focus on step (1), namely, given a fixed learn-ing to rank algorithm at step (2) and a fixed evaluation metric at step (3), e.g., DCG, we aim at direct maximiza-tion of the ranking metric by proper processing of the source dataset X source . More formally, let A be a learning to rank algorithm at the second step and let us denote the ranker F , trained on the dataset X train via the algorithm F = F A ( X train ). Finally, let P be a processing algorithm, with input X source and output X train . With these notations defined, the optimization problem we are solving is
We treat this problem as a supervised machine learning task. Namely, dataset T with its editorial labels is used to fit P by maximizing M F A ( X train ) on T . To make sense of this optimization problem we have to specify the learning to rank algorithm A and the class of possible processing algorithms { P } we are solving ( 1) within. We discuss them in the next section.
Let R N be the space of  X  X uery-document features X , mean-ing that every sample in X source and T is characterized by a vector x  X  R N . These are standard ranking features, includ-ing text and link relevance, query characteristics, document quality, etc.

In order to solve optimization problem ( 1) directly by gra-dient descent, we need to compute the partial derivatives of the output values of the ranking algorithm F A ( X with respect to the parameters of the processing algorithm P . These derivatives do not necessarily exist and even their finite-difference approximation might be computation-ally expensive, since it requires additional trainings of the algorithm F A . However, if there is a smooth closed-form ex-pression for the ranker F A ( X train ) in terms of the samples and features in the training set, the partial derivatives could be computed efficiently. We are aware of such closed-form expression only for a linear , pointwise machine-learning al-gorithm, so from now on we assume that:
At the first glance, the minimization problem (2 ) appears to be non-conventional, since each query-document pair ap-pears in the sum several times with presumably different labels. However, it, in fact, includes various standard base-line algorithms as special cases. For example, if we set each weight to 1 and each label l i to the corresponding noisy crowd label cl ( w ) , this minimization problem becomes equiv-alent to the  X  X verage label X  baseline, which predicts the aver-age of labels assigned by different workers to a given query-document pair. Setting some weights to zero is equivalent to discarding corresponding crowd labels, i.e. to data filtration. Similarly, one can implement the  X  X ajority vote X  baseline and various weighting techniques within this minimization problem.
 Remark It is well known (see [3 ]) that linear models are sig-nificantly outperformed by more sophisticated algorithms, e.g., neural networks, ensembles of trees, etc. In the next section, we show how our framework could be extended to cover these classes of learning models as well.

Let us now discuss two particular problems the process-ing algorithms P : X source 7 X  X train should solve. First, as we have discussed, the quality of crowd labeling signifi-cantly varies across different workers and tasks. Sometimes, it happens that our confidence in a particular crowd label in X source is low, e.g., the worker who labeled this sample makes many mistakes on honeypots, or the label contradicts the two labels from the other workers, etc. In this case, we want it to have less impact on the trained ranker F . In the WMSE algorithm this impact is controlled by weight w i . So the larger our confidence in a label is, the larger should be its corresponding weight. Second, some workers turn out to be more conservative than the others. For instance, imag-ine workers w 0 and w 00 , such that w 0 assigns a positive label cl ( w 0 ) only to  X  X erfect X  query-document pairs and worker w assigns a positive label to each query-document pair, unless it is completely irrelevant. Clearly, in this case, the positive label of the first worker should have a greater value , than of the second. The relevance value of a sample for a ranker F , minimizing WMSE, is reflected by l i .

Recall that by the initial assumptions on the source dataset X source , its every labeled sample is equipped with an M -dimensional vector y  X  R M of label features, see Ta-ble 1. Motivated by the above two issues, we consider the processing algorithms P assigning a weight w i and a rele-vance value l i to every sample i in X source , where w i are now both functions of the label features y i  X  R M (not to be mistaken with the ranking features x i  X  R N ). Namely, let us put w i and l i to be sigmoid transforms of y i : where  X  = (  X  1 ,..., X  M ) T and  X  = (  X  1 ,..., X  M ) T parameter vectors of weight w and label l functions and  X  ( x ) = 1 / (1 + e  X  x ) is a sigmoid transform, which ensures that all weights and labels fall into the unit interval [0,1]. The computations of weights w i and relevance values l i are referred to as reweighting and remapping steps. The class of the processing algorithms under the consideration forms a 2 M -dimensional family with vectors of parameters (  X  ,  X  ).
We stress out several important properties of this class of processing algorithms: 1. Sets of samples X source and X train are essentially the 2. The impact of an i -th sample on the trained ranker F 3. Labels cl ( w ) assigned to samples in X source by crowd
Now let us reformulate problem ( 1) in terms of an op-timization problem for parameters  X  and  X  of processing step P :
Since M is a locally constant function of the values of the ranker F , this is a non-convex and non-differentiable optimization problem. In the next section we adopt well-established smoothing techniques (see [ 1, 19 ]) to reduce ( 4) to a differentiable optimization problem and solve it using gradient descent.
We treat optimization problem ( 4) as a supervised learn-ing task. To solve it directly via gradient descent, we need to compute the gradients  X  M / X  X  j and  X  M / X  X  j . Via the chain rule one has: where s i = F ( x i ) = x i  X  b is a score assigned by a lin-ear ranker F to a sample i  X  X  . To compute the derivatives  X  M / X  X  i we, following the state-of-the-art approach to learn-ing to rank, transform the objective metric M into a smooth one by a smoothing procedure . We have experimented with LambdaRank smoothing by Burges et al. [1 ] and SoftRank by Taylor et al. [ 19]. Our experiments have shown that these techniques result in a similar quality, while LambdaRank is X source dataset labeled via crowdsourcing platform
X train output of processing step P , used by learning to a much faster algorithm, so, in what follows, we use only Lambda gradients. In order to alleviate notations, we will write simply  X  M / X  X  i , implying the LambdaRank gradient.
To find the derivatives  X  b / X  X  j and  X  b / X  X  j we need a bit of linear algebra. Let S be the number of samples in the source dataset X source and let X be the S  X  N matrix with the i -th row x i , representing query-document features of the i -th sample. Similarly, let Y be the S  X  M matrix with the i -th row y i , representing label features of the i -th sample in X source . Finally, let l to be the column vector of labels { l i =  X  ( y i  X   X  ) } in X train and W = diag( { w i } S i =1 the diagonal S  X  S matrix. Note that, by the definition, datasets X source and X train have the same feature matrix X . We summarize the notations in Table 3.

The minimizer of Equation ( 2) has the closed-form expres-sion F A ( x ) = x  X  b , where
Now set Z := ( X T WX +  X  I N ) and let b l := X  X  b be the column vector of values of the ranker F A on X train . Differ-entiating the equality X T Wl = Zb with respect to  X  j one gets:
X T  X  W
Finally, a little manipulation with Equation ( 7) yields: Computation of derivatives of b with respect to  X  from Equation 6 is a bit simpler, since only factor l depends on  X  : where  X  l / X  X  j = { y j  X  0 ( y i  X   X  ) } S i =1
Plugging these expressions into equation ( 5), we get the derivatives of the objective function M with respect to pa-rameters of the processing step  X  = (  X  1 ,..., X  M ) similarly  X  = (  X  1 ,..., X  M ) T ).

The algorithm for training the processing step P is summa-rized in pseudocode in Algorithm 1. The initial value  X  0 (0 ,..., 0) T correspond to the all equal weights (correspond-ing to W = 1 / 2 I N ) and the initial value  X  0 = (1 , 0 ,..., 0) distinguishes the label feature cl ( w ) .

Input: Crowdsourced dataset X source and target dataset T labeled by professional assessors
Parameters: number of iterations J , step size regularization , parameter of L 2 regularization  X 
Initialization: form matrices X , Y ; set  X  =  X  0 ,  X  =  X  0 , l = {  X  ( y i  X   X  0 ) } S i =1 , W = 1 / 2 I for j = 0 to J do end end
Algorithm 1: Steepest descend optimization of the pro-cessing algorithm P .
So far, the background algorithm considered in our ap-proach (Equation ( 2)) was assumed to be linear. This is a very strong and non-realistic assumption, since in the vast majority of regression/classification/ranking problems linear algorithms are outperformed by more sophisticated models, e.g., decision trees, ensembles of trees, neural networks, etc. We explain how to modify the learning scheme we have fixed in Section 4 in order to relax the linearity assumption and incorporate some of the more complicated models.

One of the main drawbacks of linear algorithms is their failure to leverage complex dependencies between individ-ual features. To overcome it, we suggest improving algo-rithm ( 2) by transforming the feature space in order to make it more tractable for a linear model. To be more precise, let us consider any algorithm A , which trains an ensemble of weak learners : where each weak learner f weak j is a function on the fea-ture space R N . Note that the class of models in Equa-tion (10 ) includes boosted decision trees [7 ], polynomial models, neural networks as its special cases. The model F ( x ) is trivially a linear transform of the T -dimensional vector ( f weak 1 ( x ) ,...f weak T ( x )).

Let us substitute the initial feature vector x  X  R N with the new one x new = ( x 1 ,...,x N ,f weak 1 ( x ) ,...f R
N + T . In what follows, this operation is referred to as fea-ture extension .
 Proposition 1. Let F A be the ranker in Equation (10 ) . Define ranker F WMSE (respectively F new WMSE ) as the solution to the WMSE minimization problem (2) over the initial fea-ture space R N (respectively, over the new extended feature space R N + T ) with L 2 -regularization parameter  X  = 0 . For a ranker F , let E ( F ) be the weighted error on the train dataset: Then one has the following bound: Proof. By its definition F new WMSE minimizes the error E ( F ) among all the linear models of the form x new  X  b Since the rankers F A and F WMSE both have this form, the required bound holds.

This proposition demonstrates that, in terms of E (  X  ), the linear ranker F new WMSE trained with the use of the new, ex-tended feature space is at least as good as F WMSE or F A Although in the general scheme (see Figure 1) we are inter-ested in a different quality measure on a different dataset, this proposition could be seen as an evidence of a possible benefit from the feature extension. In the experimental part we show that feature extension indeed significantly improves the quality of the trained ranker evaluated according to M on T .
We conduct two sets of experiments. The first series of experiments uses the Yahoo! Learning to Rank Challenge dataset (LTRCD). This dataset is labeled by professional assessors and has neither noisy crowd labels, nor any label features. Hence, LTRCD does not suit our setup as is, so we are forced to simulate label noise and derive some la-bel features. We use this simulated noise to demonstrate the actual weights and relevance values our algorithm is able to learn. There are no large-scale publicly available crowdsourced learning to rank datasets, so for the second set of experiments we use the proprietary dataset YD. This is the dataset described in Section 3 and it is used directly to evaluate various baselines and compare them with our processing framework. In both cases, we use discounted cu-mulative gain measure at positions k = 1 , 5 , 10, denoted by DCG@k [ 9], as the ranking quality metric. For both the datasets, graded relevance labels are mapped into the stan-dard numerical gains { 15 , 7 , 3 , 1 , 0 } .
In Section 4.2 , we motivated the introduction of the remapping and weighting steps in our framework by high-lighting two specific shortcomings of crowd labels: the vary-ing quality among the crowd workers and the inherent am-biguity of any labeling instructions. In the real world, both phenomena are difficult to observe as well as it is difficult to explicitly analyze their influence on the weights and the rel-evance values learned by our algorithm. For this reason, to illustrate the intuition behind our remapping and weighting steps, we design two simulated experiments with the known quality of the workers and interpretation of the instruction. With this setup, we demonstrate the particular label rele-vance values and weights our processing step learns. We use the training and the validating dataset from the Set1 (see [3 ] for details) in LTRCD as the bases for our datasets X source and T . Namely, samples and features in X source / T are the same as in the training/validation dataset in LTRCD; labels in T also coincide with the ones in the val-idation dataset, while the labels in X source are intentionally modified. Originally, samples in LTRCD are endowed with 5-graded relevance labels { 4 , 3 , 2 , 1 , 0 } (corresponding to the gains { 15 , 7 , 3 , 1 , 0 } in DCG@k respectively). In the simu-lated experiments, we treat each sample in X source as one task completed by one virtual worker.

In the first simulated experiment, we assume that each worker w assigns a binary crowd label to each sample. This label depends on two quantities: the actual qual-ity of the document, i.e., its label in the initial dataset l  X  { 4 , 3 , 2 , 1 , 0 } , and the worker X  X  rigor r ( w ) In this case, the worker assigns a crowd label: In simple words, the most conservative workers with r ( w ) 3 assign the positive label only to the perfect result, the less rigorous workers with r ( w ) = 2 assign positive labels to per-fect and good documents, etc. This labeling scheme models a variable interpretation of labeling instructions by crowd workers. Intuitively, one expects that the more conserva-tive the worker is, the higher actual relevance values her positive labels correspond to. To modify the initial dataset X source in LTRCD, we assign each worker with a uniformly random rigor r ( w ) and compute each crowd label according to Equation ( 11). Moreover, in this experiment, we endow each sample with 4 binary label features: The idea is that these features will help us to distinguish positive labels of different workers and remap them accord-ing to the worker X  X  rigor.

In the second simulated experiment, we model this vari-able quality among workers. Namely, each worker is assigned  X  X uality X . Further, we assume that each worker w makes a mistake with probability 1  X  q ( w ) . We expect that the higher the quality of the worker, the higher our confidence in her labels and, therefore, the larger weight to the samples should be assigned by our framework. In this simulated experiment, we again endow each sample with 4 binary label features re-Figure 2: The relevance values learned with our al-gorithm in Simulated Experiment 1. Figure 3: The weights learned with our algorithm in Simulated Experiment 2. flecting the quality of the worker: Simulated Experiment 1. For the first simulated exper-iment we learn only the relevance values, l i =  X  (  X   X  y see Equation ( 3), i.e., in Algorithm 1 only parameters  X  are tuned, while the vector  X  is fixed. The resulting rele-vance values for the samples with positive crowd labels are depicted in Figure 2. As we have expected, the relevance value grows monotonically with the growth of the rigor of a Simulated Experiment 2. In the second simulated ex-periment we, on the contrary, learn only the weights w i =  X  (  X   X  y i ) with  X  in Algorithm 1 fixed. Figure 3 demon-strates the resulting weights as the function of the worker X  X  quality q ( w ) . Again, the weights w i grow with the growth of worker X  X  quality. Surprisingly, the weight of workers with quality q ( w ) = 0 . 5 is negligible, implying that the use of the corresponding samples within the background machine learning algorithm does not improve the ranking quality.
This set of experiments is conducted on the dataset de-scribed in Section 3. For a ranker F and a dataset X , let us define DCG@k( F , X ) to be the average of the values of the DCG@k metrics over all queries in X , ranked according to the values of F .

To evaluate our algorithm along with a number of base-lines we perform 5-fold cross validation. Each time, the tar-get dataset T is randomly split into 3 parts T train , T validate and T test on the basis of the unique query id. The part T train is used to train Algorithm 1, specifically, to compute the LambdaRank-gradients  X  DCG@k( F , T train ) / X  X  i . The part T validate is used to tune various hyperparameters of the background machine learning algorithm A and the pro-13 . 03% ? 13 . 56% ? 9 . 53% ? 9 . 76% ? 12 . 46% ? 15 . 35 % 6 . 47% ? 6 . 49% ? 4 . 5% ? 4 . 99% ? 6 . 06% ? 7 . 30 % 5 . 16% ? 5 . 32% ? 3 . 64% ? 4 . 05% ? 4 . 80% ? 5 . 97 % Table 4: Comparsion of various baselines with our framework. cessing step, i.e., L 2 -regularization parameter  X  ; J and in our processing algorithm and the number of extended fea-tures T . The output of Algorithm 1 is the dataset X with samples corresponding to the elements in X source and weights/labels given by (3 ). Finally, the third part T is used to evaluate the ranker trained via background algo-rithm A on X train (our approach) and the baseline rankers.
Due to the proprietary nature of the data, we do not dis-close the actual values of DCG@k metrics on the test dataset T test . Instead, we report the relative improvement over the  X  X ajority vote X  baseline, described further. We denote this relative improvement by  X  X CG@k.

Since the specific problem we formulate in this paper is novel, there are no baselines intended specifically for our set-ting. For this reason, we adopt various common approaches to noise reduction and consensus modeling. All of them are used at the processing step (see Figure 1). As the back-ground algorithm for the baselines, we use the proprietary implementation of the Friedman X  X  gradient boosted ensem-ble of decision trees [ 7]. We experiment with the standard pointwise regression regime and the state-of-the-art listwise algorithm LambdaMART [ 2] both trained on X train .
We use the following processing algorithms as the base-lines: 1. MV, assigning the majority vote to every query-2. Av, assigning the average of 3 crowd labels to every 3-4. DS/GLAD, assigning the most likely label to each task
Besides these baselines, we experiment with the classi-cal noise reduction technique, reweighting by deviations [6,  X  3.3.2], denoted by RD. This reweighting method does not take into account any label features, instead, it assigns to the i -th sample the weight min(1 / X  2 , 1 /  X  2 i ), where  X  is the absolute error of the background learning algorithm on the initial dataset X source and  X   X  0 is a parameter.
One of the contributions of this paper is the introduction of the feature extension step (see Section 5.2 ). We first study the impact of the number of additional features T in the extended feature space R N + T . To construct the additional features, the ensemble of the gradient boosted decision trees on X source is trained. Each individual tree in this ensemble is treated as a single extended ranking feature. We run remapping and weighting over the extended feature space with T  X  { 0 , 100 , 200 , 400 } with T = 0 corresponding to no Figure 4: Performance of the feature extension step for different choices of T . feature extension. All the resulting models are evaluated on T validate , see Figure 4. In accordance with the evidence provided in Section 5.2 , the feature extension improves the quality of the linear background algorithm. However, with the growth of the number of the weak learners added, the background algorithm starts to overfit to the source dataset. Figure 4 suggests that the optimal number of trees is T = 300.
 Next, using the optimal parameter T = 300 tuned on T validate , we evaluate all the baselines together with our ap-proach on T test . We consider three versions of our frame-work: (1) the reweighting approach Rw which tunes only parameters  X  in Algorithm 1; (2) the remapping approach Rm which tunes only parameters  X  ; (3) their combination Rw+Rm which tunes both  X  and  X  . Relative improve-ments of all these methods over the majority vote baseline (MV) are shown in Table 4. The remapping approach Rm alone turns out to be significantly better then the reweight-ing approach Rw , and both are outperformed by their com-bination. Note that, according to all three evaluation mea-sures, Rw+Rm statistically significantly (with p &lt; 0 . 05) outperform the best performing baseline, namely, Lamb-daMART trained on DS consensus labels.

The performance results of the baselines suggest that DS consensus model suits better for our dataset than GLAD model. We confirm this observation by analyzing the con-tribution of various types of label features into the model. Let us divide all features into three groups: (1) the fea-tures based on DS model; (2) the features based on GLAD model; (3) the remaining basic features (corresponding to the features 1,13-15 in Table 1). Feature importance is an-alyzed through the feature ablation, i.e., we train the whole framework on certain subsets of the full feature set and eval-uate the resulting ranker according to  X  X CG@10 metric on T test . Similarly to our previous observations, on all the fea-ture sets remapping Rm outperforms reweighting Rw and for all the algorithms GLAD features are outperformed by DS features.
In this paper we address the problem of learning to rank with the use of noisy labels, acquired via crowdsourcing, and aggregated with different consensus labeling models. Unlike the existing approaches to consensus modeling and noise re-duction, the framework described in this paper leverages the label features (e.g., outputs of various consensus mod-els, features of a worker and a task, etc) directly optimizing the quality of the ranker it produces. Its success prompts one to collect as much information about the worker (asses-sor), the task and the label as possible, e.g., track the actions of the worker/assessor in the browser, the time spent at a given task, the ranking features of query-document pair, etc. By transforming this information into a large label features vector y , we are able to learn more refined weights and rel-evance labels. These, in turn, being fed to a background machine learning algorithm, will most likely lead to a better ranking function.

Also, we would like to make a remark about the role of the target dataset T in the framework. As we have dis-cussed above, the samples in the target set are labeled by professional assessors and, therefore, are expensive and rare. Nevertheless, our framework is supervised and requires these high-quality labels. Since commercial search engines have to update their rankings algorithm often, it might be expensive to renew the target dataset every time. Luckily, it suffices to tune the weighting and remapping functions only once. After it, they can be used directly within a background ma-chine leaning algorithm with the new datasets X source retrain the ranker F .

In the future we are planning to apply the remapping framework to other problems. One of the promising direc-tions is learning from the click-through data. Besides that, we intend to develop our algorithm on the technical level by extending it to deal with pairwise preferences.
