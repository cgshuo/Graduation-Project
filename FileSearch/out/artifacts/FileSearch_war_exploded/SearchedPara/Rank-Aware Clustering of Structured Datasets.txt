 In online applications such as Yahoo! Personals and Ya-hoo! Real Estate users de ne structured pro les in order to nd potentially interesting matches. Typically, pro les are evaluated against large datasets and produce thousands of matches. In addition to ltering, users also specify rank-ing in their pro le, and matches are returned in a ranked list. Top results in a list are typically homogeneous, which hinders data exploration. For example, a user looking for 1-or 2-bedroom apartments sorted by price will see a large number of cheap 1-bedrooms in undesirable neighborhoods before seeing a di erent apartment. An alternative to rank-ing is to group matches on common attribute values, e.g., cheap 1-bedrooms in good neighborhoods, 2-bedrooms with 2 baths, and choose groups in relationship with ranking. In this paper, we present a novel paradigm of rank-aware clus-tering ,anddemonstrat eitse ecti venes sonalarg edataset from Yahoo! Personals, a leading online dating site. Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Clustring; H.3.5 [On-line Infor-mation Services]: Web-based services; H.5.3 [Group and Or-ganization Interfaces]: Web-based interaction General Terms: Design Keywords: information ltering, information presentation, rank-aware clustering, structured datasets. In online applications with large structured datasets, e.g. Yahoo! Personals and Yahoo! Real Estate, there are often thousands of high-quality items, in this case, persons and apartments, that satisfy a user's information need. Users typically specify a structured target pro le in the form of attribute-value pairs, which is used to lter items. On dating sites, a target pro le may specify the age, height, income, education, political aliation, and religion of a potential
Research supported in part by National Institute of Health grant 5 U54 CA121852-03 match. In real estate applications, a pro le describes a user's dream home by its location, size, and number of bedrooms. The number of matches to a speci c pro le is often very high, making data exploration an interesting challenge.
Typically users also specify ranking criteria which are used to rank matches. In Yahoo! Personals potential matches may be ranked by decreasing income or increasing age, while in Yahoo! Real Estate houses may be ranked by increasing price or decreasing size. Ranking helps users navigate the set of results by limiting the number of items that they see at any one time, and making sure that the items users see rst are of high quality. However, ranking also brings the disad-vantage of match homogeneity : the user is often required to go through a large number of similar items before nding the next di erent item. This is illustrated in the following ctional example inspired by Yahoo! Personals.

Example 1.1. Mike is looking for a date with some col-lege education, 20 to 30 years old, and he wants to see matches sorted on decreasing income. When inspecting the results, Mike notices that the top ranks are dominated by women in their late-twenties with a Master's degree. It takes Mike a while to scroll down to the next set of matches which are di erent from the top-ranking ones. In doing so, he skips over some unexpected cases such as younger women with higher education and income levels, or women with high in-come who did not graduate from college. After additional data exploration, Mike realizes that there is a correlation between age, education ( ltering), and income (ranking.) Such correlations would have been obvious if data were pre-sented in labeled groups such as [20-24] year-olds with a bachelor's degree , [25-30] year-olds who make over 75K , etc. The user would then explore the interesting groups by looking at the ranked lists of results within the groups.
A key takeaway is that a user browsing a result set sequen-tially, item by item, is only able to understand trends in the data after seeing a signi cant number of items. Moreover, the diculty of sequential exploration increases with more sophisticated rankings. For example, the score of a match on a dating site could be proportional to its income and inversely proportional to the distance from the user's geo-graphic location. Helping users navigate their results more e ectively is even more important in this case, given that correlations between ltering and ranking are less obvious.
There are many families of clustering algorithms that pro-duce labeled groups from a dataset. In domains like Ya-hoo! Personals, where datasets are large and all items are describ ed by a large num ber of attributes, it is intuitiv e to use subsp ace clustering to nd clusters of items with mean-ingful descriptions.

Subspace clustering is an extension of traditional cluster-ing that seeks to nd clusters in di eren t subspaces of a dataset [7]. Clusters of items are high-quality regions iden-ti ed in multiple, possibly overlapping, subspaces. Man y subspace clustering algorithms use the density of a region as a qualit y measure. In the simplest case, densit y is the per-cen tage of data points that fall within a particular region, and the algorithm aims to nd all regions that have den-sity higher than a pre-de ned threshold. We now illustrate with an example how one of the rst subspace clustering algorithms, CLIQUE [1], uses densit y to iden tify clusters.
Example 1.2. Consider a ctional real estate example in Table 1: a datab ase of 300 rental apartments, listing the numb er of bedrooms, numb er of bathr ooms, size in f t monthly rental pric e, and the numb er of such apartments curr ently on the market. Mary is looking for an apartment that is at least 600-f t 2 in size and has at least one bedroom, and she wants the matches sorte d on pric e in incr easing or-der. All apartments in Table 1 match Mary 's pro le.
Assume a densit y threshold = 0 : 1. A densit y-based subspace clustering algorithm starts by dividing the range of values along eac h dimension (attribute) into cells, and by computing the densit y in eac h cell. For example, eac h distinct value of #beds , size , and #baths may corresp ond to a cell, and price may be brok en down into interv als (1500 ; 2000] ; (2000 ; 2500] ; (2500 ; 3000] ; and (3000 ; 3500]. Cells that do not pass the densit y threshold are pruned at this stage. The algorithm immediately prunes 600-f t 2 apart-men ts ( 5 300 &lt; ), 750-f t 2 apartmen ts ( 25 300 &lt; ), 1000-f t apartmen ts ( 10 300 &lt; ), and apartmen ts in the (1500 ; 2000] price range ( 5 300 &lt; .) Giv en Mary 's interest in cheap er apartmen ts (price is her ranking condition), it is problem-atic that the cheap est apartmen ts in the dataset, the 600-f t apartmen ts that cost $1800, are pruned.

Having iden ti ed dense cells, the algorithm pro ceeds to mer ge runs of neigh boring cells in eac h dimension.
Next, the algorithm progressiv ely explores clusters in higher dimensions by joining lower-dimensional ones. For example, the 1-dimensional cluster of 800-f t 2 apartmen ts (70 items) can be joined with the 1-dimensional cluster of apartmen ts in the (2000 ; 2500] price range (145 items.) The result of this join is a region with 60 800-f t 2 2-b edro oms at $2400 per mon th, whic h quali es as a cluster since it passes the densit y threshold. However, the region that results from joining the 950-f t 2 apartmen ts (105 items) with apartmen ts in the (2500 ; 3000] price range (40 items) does not qualify as a cluster (it con tains only 5 items) and is pruned, losing the poten tially interesting 3-b edro oms for a relativ ely low price ($2900.) Densit y decreases in higher dimensions and the algorithm stops when there are no more clusters to explore.
A lower densit y threshold would eviden tly guaran tee that some of the regions pruned using a higher threshold would be preserv ed. However, if the threshold is set too low, the algorithm would keep merging regions, ultimately iden tify-ing much larger clusters, and possibly one cluster con tain-ing the entire dataset. Moreo ver, not all regions that pass a typical clustering quality metric , e.g., densit y or entrop y, are equally interesting to the user. Indeed, given a scor-ing function, some items, and hence some clusters, are more desirable than others (e.g., Mary has little interest in the 2-bedro om apartmen ts that cost $3500, but would like to see the 1-b edro oms that cost $1800.) Even when the densit y of a region is high, as is the case with 2-b edro om apart-men ts for $3500, Mary would probably have less interest in them than in cheap er apartmen ts. Therefore, we prop ose to explore densit y measures that accoun t for item scores and ranks when assessing the qualit y of a cluster.
In an on-line data exploration scenario, a user speci es a pro le comp osed of ltering and ranking criteria. We as-sume that the user's ltering conditions result in a dataset D (and can thus tak e users out of the notation since we are interested in one user at a time.) Items in D are describ ed using attribute-v alue pairs, including a special attribute id whic h uniquely iden ti es eac h item. Attributes belong to a set A . Ranking is expressed by a scoring function S whic h assigns a score i:scor e to eac h item i 2 D . We denote by S ( D ) the set of all items from D augmen ted with i:scor e . Typically , items are presen ted to the user as a single rank ed list sorted by score. We rst argue that rank-una ware clus-tering algorithms (see Section 1.1) are inappropriate when users are interested in exploring rank ed datasets. Example 2.1. Consider user Mary from Example 1.2.
 Mary is inter este d in seeing apartments ranke d by pric e in incr easing order. Ann , another user who shar es Mary 's ltering conditions, may be inter este d in seeing the same apartments sorte d by size in decreasing order. Which clus-ters are best for which user depends on the user's ranking prefer ences. One reasonable option is to cluster apartments based on the scoring attribute. In particular, Ann may ap-preciate seeing the 950-f t 2 apartments which cost $ 2900 in the same cluster as the same-size apartments for $ 3500, while Mary may prefer to see 950-f t 2 apartments group ed to-gether with the same-pric ed 750-f t 2 apartments. A subsp ace clustering measur e that does not account for item scores would not distinguish betwe en these two users, and would ther efor e be inappr opriate for rank-awar e data explor ation.
The score of eac h item can be treated as an additional attribute and can thus be used for clustering. However, as we argue in the follo wing example, using scores as an ad-ditional clustering dimension still fails to e ectiv ely address data exploration for rank ed datasets.

Example 2.2. Consider again Example 1.2, wher e Mary wants to sort apartments by pric e. If item pric e is used as a clustering dimension, in the same way as other attributes, then Mary may see a high numb er of clusters, not all of which are of potential inter est to her: e.g. a cluster of ex-pensive 2-bedroom apartments may app ear alongside a clus-ter of cheap 2-bedrooms. If many clusters are disc over ed by the algorithm, the potential ly mor e inter esting ones may go unnotic ed. Worse yet, the algorithm may decide to mer ge to-gether intervals that are of high inter est to Mary with those of low inter est, resulting in a potential ly large heter ogene ous cluster with homo gene ous results dominating the top ranks.
Hence, we explore new clustering qualit y measures that use item scores and ranks to assess qualit y.

A region G is a set of items lab eled with a conjunction of predicates over attributes in A , whic h, if evaluated on the dataset D , results in all items in the region. The dimension-alit y of a region is the num ber of predicates that describ e it. Any subset of predicates that de ne a region G is a sub-region of G . A region quali es as a cluster if it satis es a clustering quality measur e . We use S ( G ; N ) to denote the N highest scoring items in S ( G ). N is a parameter in our for-malism that mo dels the user's attention span { the num ber of items the user is likely to explore sequen tially [6]. This parameter can be customized per user, or it can be set to re ect the preferences of an average user.

Our rst rank-a ware measure, Q topN , builds on the as-sumption that users are interested in clusters that con tain high-scoring items, and that they will only explore the best items in those clusters. Q topN states that a multi-dimensional region G is a cluster if it con tains enough items that are in the top-N of eac h of its one-dimensional sub-regions.
Q topN : jS ( G ; N ) \ S ( P 1 ; N ) \ : : : \ S ( P m ; N ) j
Q topN aims to disco ver attribute correlations among the high-scoring items in the dataset. We illustrate how this measure compares to densit y using Example 1.2. Recall that user Mary speci ed price as the ranking condition.
The join of the 700-f t 2 cluster with the (2000 ; 2500] price range cluster preserv es the lower-priced 1-b edro oms, since the top-N items in the join corresp ond to the high-scoring items in the (2000 ; 2500] cluster (one of the sub-regions) and to the high-scoring items in the 700-f t 2 cluster (its other sub-region.) On the other hand, the join of 2-bathro om apartmen ts with 950-f t 2 apartmen ts would not con tain any of the cheap est 2-bathro om apartmen ts in its top-N and would thus not qualify as a cluster.

The second measure, whic h we call Q SCORE &amp; RAN K , mo d-els the intuition that a region with exceptionally high-scoring items in high ranks may be just as interesting to the user as a region in whic h items have intermediate scores. We de ne this measure using nDCG (Normalized Discoun ted Cum ula-tive Gain) [4], with S ( [ k P k ; N ) as the ideal vector. Q
Consider the gain vector V = id;scor e S ( G ; N ). V is sorted by score in descending order. We deriv e the cumulative gain vector CG from V by assigning to eac h position in CG [ j ] the sum of scores of items in V up to and including j : CG [ j ] = k j V [ k ]. (All arra ys are 1-based.) To capture the intuition that the user is more likely to pay atten tion to items in higher ranks, we deriv e the disc ounte d cumu-lative gain vector DCG as DCG [ k ] = CG [ k ] =log b k . The constan t b con trols the rank-based discoun t: higher values of b corresp ond to lower discoun t. There is no discoun t for positions k b , where DCG [ k ] = CG [ k ]. To mak e dis-coun ted cum ulativ e gain comparable across regions, we in-troduce a normalization factor: eac h position in the DCG vector for a region G is normalized by the corresp onding value of DCG Ideal computed w.r.t. the ideal gain vector . Consider again Example 1.2 and Mary 's scoring function S
Mary , and let us tak e N = 5. Let us compute the nDCG for the 1.5-bathro om apartmen ts with the size of 950-f t 2 ideal gain vector consists of scores of the 5 best items that either have 1.5 bathro oms or are 950-f t 2 in size, namely , the 1.5-bath 700-f t 2 apartmen ts that cost $2100 per mon th ( i:scor e = 0 : 82.) With b = 2 we deriv e: DCG Ideal = [0 : 82 1 : 64 1 : 55 1 : 64 1 : 77].

Let us now compute the nDCG for the 950-f t 2 1.5-bath apartmen ts. The top-5 list of this region consists of ve 3-bedro om apartmen ts at $2900 ( i:scor e = 0 : 35.) We deriv e DCG = [0 : 35 0 : 7 0 : 66 0 : 7 0 : 75]. We now normalize eac h po-sition in DCG by the corresp onding position in DCG Ideal average the values, and arriv e at nD CG = 0 : 43.
We studied the prop erties of sev eral ranking functions on a dataset from Yaho o! Personals, a leading on-line dating service with millions of users. Users create a personal pro-le in whic h they describ e themselv es using 30 structured attributes, e.g., age, heigh t, occupation, education, income, etc. Users commonly store one or sev eral target pro les , ex-pressed in terms of the same attributes. Attributes in a tar-get pro le are designated as requir ed or desir able . Required attributes are used for ltering { exact matc hing against per-sonal pro les, and desirable attributes are used for ranking .
The ranking functions we consider use 6 attributes whic h induce a natural order on their values: age , height , body type , educ ation , income , and religious servic es (the frequency with whic h the user attends religious services.)
Our rst scoring function, called attribute-r ank , assigns equal weigh ts to eac h ranking attribute, and computes the score of an item as the sum of distances between the item and the ideal item along eac h attribute dimension. Here, an ideal item has the best possible value for eac h ranking at-tribute among items in the ltered dataset. The best value for eac h attribute is determined by the user's ranking con-ditions. Distances along eac h dimension are normalized by the di erence between extreme values for the corresp ond-ing attribute found in the ltered dataset. Note that this function is personalize d in two ways. First, the user speci es whic h attributes are included in the scoring function. Sec-ond, the value of eac h ranking attribute con tributes to the score based on how it compares to the best and worst values for that attribute, from among items that pass the ltering conditions of the target pro le.

The second function, geo-rank , scales the value returned by attribute-r ank by the geographic distance between the
We now give a qualitativ e intuition of the kinds of clus-ters that may be disco vered by rank-a ware clustering for the attribute-r ank ranking function. We use a pro le that is in-line with Example 1.1: age 2 [25,35], height 2 [160cm, 175cm], educ ation Bachelor's, ethnicity = Cauc asian, body type 2 f slim, slender, aver age, athletic, t g . These conditions return over 30,000 matc hes on Yaho o! Personals. We rank the matc hes on a com bination of income and educ ation , both from higher to lower. There are about 100 top-matc hes: women with post-graduate education who mak e more than $150K. Ab out two thirds of the top matc hes are over 29 years old, and so younger matc hes are not easily visible in a rank ed list.

The follo wing rank-a ware clusters deal directly with the correlation between income and age, and income and educa-tion: age 2 [25 ; 27] ^ income 2 [$35 K; $75 K ], age 2 [28 ; 33] ^ income 2 [$75 K; $150 K ], age 2 [28 ; 33] ^ income $150 K , age 2 [31 ; 35] ^ income $75 K , age 2 [28 ; 33] ^ education = post graduate , age 2 [31 ; 35] ^ education = post graduate .
Note that two clusters con tain di eren t sets of matc hes with age between 28 and 33. Note also that younger matc hes, age 25 to 27, are in a cluster with relativ ely lower income, and would not have been easily accessible in a single rank ed list. Other clusters may be disco vered that are not directly related to the ranking conditions, but for whic h a correla-tion exists among attributes at top ranks. So, there would be a cluster of matc hes who are politically very conserva-tive or conservative and who attend religious services mor e than onc e a week or weekly . Another cluster may con tain matc hes who are politically midd le of the road or liberal and who attend religious services no more often than monthly .
As we illustrated, the clustering outcome dep ends on the ranking attributes, and how they correlate with other at-tributes in the data. Let us now consider how the distri-bution of scores imp osed by the scoring function in uences the applicabilit y of rank-a ware clustering, and the choice of a clustering qualit y measure.
Since the clustering outcome dep ends on the com bination of a user's ltering conditions and the distribution of scores imp osed by his scoring function, it is not alw ays possible to nd a meaningful clustering. Intuitiv ely, rank-a ware clus-tering does not apply if ranking does not discriminate well between high-qualit y and low-qualit y results. So, selecting matc hes with income = 50K , and then ranking on income, is not helpful, since all matc hes tie for the same score. We now demonstrate the qualitativ e di erence between Q ular user, whom we call user 1 , as an example.

Ideally , a rank-a ware measure should be rich enough to capture the distribution of scores in the result. Q topN treats all items with N highest scores equally , and is appropriate for scoring functions where the best N items have the highest scores , but where ther e is no signi c ant variability in scores among the top-N . This is the case for the function attribute-rank for user 1 , as is demonstrated in Figure 1.

Con versely , Q SCORE &amp; RAN K is most meaningful if ther e is signi c ant variability in scores among the top-N . So, for N = 100 the best 10 items may have much higher scores than the follo wing 10 items etc. The function geo-rank for user 1 is one suc h function, as is sho wn in Figure 1. Based on the distribution of scores for user 1 we conclude that Q topN should be used with attribute-r ank , while Q SCORE &amp; RAN K more appropriate for the geo-rank scoring function.
Li et al[5] argue for nativ e supp ort of clustering and rank-ing operators in relational databases, and demonstrate how clustering can be implemen ted by means of a bitmap index over a summary grid with a particular focus on eciency .
In [3] the authors state the Many-A nswers Problem and sho w how correlations among attribute values in a struc-tured datasets can be used to automatically deriv e a suitable ranking function.T o this end, the authors dev elop a com-prehensiv e probabilistic information retriev al ranking mo del and presen t ecien t pro cessing techniques. A related Empty-Answers Problem is discussed in [2]. Here, the authors presen t an adaptation of inverse document frequency (IDF) that is used for automatic ranking of results.

Sun et al [8] recen tly presen ted RankClus, a framew ork that integrates ranking with clustering in a heterogeneous information net work suc h as DBLP . RankClus is based on a mixture mo del that uses mutual reinforcemen t between clustering and ranking. While we share a similar motiv ation, our application and solution are di eren t.
In this pap er, we formalized new clustering qualit y mea-sures that are appropriate for rank-a ware clustering of struc-tured datasets, and sho wed their e ectiv eness exp erimen-tally . We believ e this work is crucial for more e ectiv e ex-ploration of rank ed large structured datasets.
