 Designing effective ranking functions is a core problem for information retrieval and Web search since the ranking func -tions directly impact the relevance of the search results. T he problem has been the focus of much of the research at the intersection of Web search and machine learning, and learn-ing ranking functions from preference data in particular ha s recently attracted much interest. The objective of this pa-per is to empirically examine several objective functions t hat can be used for learning ranking functions from preference data. Specifically, we investigate the roles of ties in the learning process. By ties, we mean preference judgments that two documents have equal degree of relevance with re-spect to a query. This type of data has largely been ignored or not properly modeled in the past. In this paper, we ana-lyze the properties of ties and develop novel learning frame -works which combine ties and preference data using statis-tical paired comparison models to improve the performance of learned ranking functions. The resulting optimization problems explicitly incorporating ties and preference dat a are solved using gradient boosting methods. Experimen-tal studies are conducted using three publicly available da ta sets which demonstrate the effectiveness of the proposed new methods.
 H.3.3 [ Information Systems ]: Information Search and Retrieval X  Retrieval functions ; H.4.m [ Information Sys-tems ]: Miscellaneous X  Machine learning Algorithms, Experimentation, Theory Ranking function, machine learning, preference learning, paired comparison, ties, functional gradient descent, gra -dient boosting
The ranking problem has been widely explored in many fields including information retrieval and web search, reco m-mendation systems and sports such as chess playing. Rank-ing documents with respect to a given query is the most important problem for designing effective web search en-gines because the ranking functions directly influence the relevance of the search results. Several methodologies of d e-signing ranking functions have been explored in the past by information retrieval researchers, including the vector s pace model [24] and language modeling based approaches [19,25]. Those methods have been proven to be effective in both experiments and real world applications. Due to the large number of heterogeneous features employed in current rank-ing functions, recent methods for document ranking tend to rely heavily on discriminative machine learning technique s: training sets are generated and ranking functions are con-structed by fitting the training data. Some of these ranking algorithms, such as RankSVM [5, 16] and RankBoost [9], have shown encouraging improvements in performance.
One effective approach for learning ranking functions is based on learning from preference data and is called pref-erence learning. In the case of learning to rank web docu-ments, preference data are given in the form that one docu-ment is more relevant than another with respect to a given query. Such preference data can be generated from user clickthroughs, for example, and they also have the advan-tage of capturing user searching behaviors and preferences in a more timely manner [18]. Although several methods for learning ranking functions from preference data have been proposed [9, 16, 30], the existing methods have largely ig-nored one important type of preference relations, i.e., ties . A tie represents a relevance judgment indicating that two documents are of the same degree of relevance with respect to a query. In a sense, ties complement the preference data and they provide a different angle for learning a ranking function: preference data focus on the discriminative qual i-ties of documents with different degrees of relevance, while ties provide information on the common characteristics of documents with the same degree of relevance.

In this paper, we consider the problem of incorporating ties in the context of preference learning. We propose a novel framework to explore the relevance judgments of ties for learning ranking functions. Both of the preference data and the ties are naturally integrated in this framework by modeling the ties in a principled fashion. In particular, we explore statistical models for paired comparisons to deriv e the objective functions for our learning framework. Two approaches for paired comparisons are applied to model the ties and the preference data in this study, but other models can naturally be adapted to our framework as well. We then apply gradient descent in function spaces to optimize the proposed objective functions in order to learn the ranking functions [10,30].

Our experimental studies on three publicly available data sets show that the performance of the learned ranking func-tions can be significantly improved by taking ties into ac-count, so ties should be employed when they are available. More interestingly, we find that the performance improve-ment at the top-ranked results is more significant by includ-ing ties. This is because both the shared features of rel-evant documents and the discriminative qualities between relevant and irrelevant documents are captured by includ-ing ties. Thus incorporating ties are especially beneficial to web search engines since they tend to focus more on the top-ranked results. Another interesting observation is that ti es are more effective when multiple levels of relevance judge-ments are available.

The rest of this paper is organized as follows: In Section 2, related studies of learning to rank are briefly surveyed. The n we analyze the properties of ties in Section 3. The loss func-tions of our framework are proposed based on the statistical models for paired comparisons discussed in Section 4 and 5. In Section 6, we discuss functional gradient descent method s for optimizing the proposed objective functions. In Sectio n 7, we report and analyze the results of experimental studies on three publicly available data sets. Finally, we conclude this work and point out some directions for future research.
Many methods for designing ranking functions have been proposed in the past years. Vector space model [24] rep-resents queries and documents as vectors of features, so the degree of relevance can be calculated by the similar-ity between feature vectors of the documents and those of the queries. The Okapi BM25 ranking function is proposed based on the vector space model and applied in many in-formation retrieval systems [23]. Language modeling based methods consider the relevance of a document with respect to a query in a probabilistic framework and estimate the parameters in probability models that describe whether a document is relevant to a query [25].

In recent years, the ranking problem is frequently solved under the supervised machine learning framework [3, 7, 9, 16, 28 X 30]. This learning to rank approaches are capable of combining different kinds of features to train ranking functions. These approaches to learning ranking functions are usually based on the advanced techniques developed for other machine learning tasks. The problem of ranking is usually formulated as that of learning a ranking function from pair-wise preference data. The idea is to minimize the number of contradicting pairs in training data. For exam-ple, RankSVM [16] uses support vector machines to learn a ranking function from preference data. RankNet [3] ap-plies neural network and gradient descent to obtain a rank-ing function. RankBoost [9] applies the idea of boosting to construct an efficient ranking function from a set of weak ranking functions. The studies reported in [30] proposed a framework called GBRank using gradient descent in func-tion spaces [10, 11], which is able to deal with complicated features in the context of web search. Other methods for learning ranking functions are proposed in [4,12]
Another approach to learning a ranking function address the problem of optimizing the loss function directly relate d to the performance measures of information retrieval, such as precision, mean average precision and Normalized Dis-count Cumulative Gain [6, 17, 28, 29]. The idea of these methods is to obtain a ranking function that is optimal with respect to some information retrieval performance mea -sure. Since their objective functions are directly related to the performance measures, these methods show good per-formance in many practical situations. However, most of the performance measures are defined by absolute relevance judgements. As a result, these methods require the abso-lute relevant judgements, i.e., labeled data, in order to le arn ranking functions. Considering the fact that collecting ab so-lute relevant judgements is expensive and time-consuming, the application of these method can be limited.
Both ties and preference data arise in many scenarios of learning ranking functions for Web search engines. Pref-erence data are usually extracted from two sources: user clickthroughs and absolute relevant judgments.
 Clickthroughs. Commercial search engines can readily ob-Absolute relevant judgments. In absolute relevance
An important observation is that ties can also be obtained from these two sources. For clickthroughs, heuristic rules can also be developed for extracting ties. For example, a tie can be generated if the clicks of two documents frequently occur together. In the case of absolute relevant judgements , a straightforward approach is to consider pairs of document s with the same grade of relevance as ties. Since ties can be extracted through methods similar to the methods for pref-erence data generation, training data for preference learn ing should be expanded to include ties. More importantly, we will show that incorporating ties leads to improvements in the performance of the learned ranking functions.
Existing preference learning methods only consider prefer -ence data and ignore the extra constraints that are provided by ties. To illustrate the effect of ties, we consider a toy problem shown in Figure 1. Suppose that we need to rank four documents d 1 , d 2 , d 3 and d 4 in that order with respect to some given query. For the sake of clarify, we consider the simple case of ranking those documents by their pro-jection on a straight line. Assume we have the preference data: d 1 is preferred to d 3 and d 2 is preferred to d 4 preference relation between pairs ( d 1 , d 4 ) and ( d 2 not available. Such kind of situations usually occur when the preference data are extracted from user clickthroughs. The ranking function obtained may be represented by the thin line in the figure. We observe that the pair d 2 and d are ranked incorrectly. However, if we include ties between The tie between ( d 1 , d 2 ) requires that d 1 and d 2 have the same relevant scores (similarly for d 3 and d 4 ). As a result, the ranking function becomes the thick line in the figure. In this case, ties provide additional descriptions of the us er preference and the ranking function is refined by utilizing these information. From the above example, we can see that ties indeed provide new information about the rank-ing function and can enhance the performance of learned ranking functions.

We now examine the question of how to make use of ties in learning ranking functions. We will propose a novel frame-work that integrates both ties and preferences data in a sin-gle objective function. To this end, let D = { d 1 , d 2 , . . . , d be the set of feature vectors of all query-document pairs in consideration. 1 We denote ties and preferences as follows: Given the feature vectors of query-document pairs x and y , let x  X  y denote that x is preferred to y and x = y denote that x and y is preferred equally.

The basic ingredient of our framework is the statistical models for paired comparisons and they provide a princi-pled approach for modeling ties. Those models have been widely used in statistics, psychology and machine learning , two of the more well-known ones are Bradley-Terry model
A detailed discussion of the various types features extract ed for query-document pairs can be found in [30]. and Thurstone-Mosteller model [2,26]. Most of these models can be unified under the framework of general linear models as discussed below [8]. Given a nondecreasing function F : R 7 X  R satisfying F (+  X  ) = 1, F (  X  X  X  ) = 0 and F ( x ) = 1  X  F (  X  x ), the probability that document x is preferred to document y is expressed as: where h ( x ) and h ( y ) are the scores of document x and y given by the ranking function h , x is ranked higher than y if h ( x ) &gt; h ( y ). The original general linear models do not have the ability to express the case that two items are equally preferred. However, it can be extended to incorporate ties as follows: P ( x  X  y ) = F ( h ( x )  X  h ( y )  X   X  ) (2) The parameter  X  is a threshold that controls the probabil-ities for ties. When the parameter  X  = 0, the new model is identical to the original general linear models. In this study, we apply two special cases of general linear mod-els: the Bradley-Terry model and the Thurstone-Mosteller model.
The Bradley-Terry model is widely used for paired com-parisons, and its first use in learning ranking functions ap-pears in [3]. Under this model, the probability of preferenc e for a pair is given as follows: It can be observed that the Bradley-Terry model is a special case of the general linear model by letting F ( x ) = 1 1+exp(  X  x ) So the Bradley-Terry model can be extended to incorporate ties by substituting 1 1+exp(  X  x ) for F ( x ) in Equation (2) and (3) [22]: Similar as in the general linear models, the parameter  X  = e  X  is a threshold that controls the probabilities of ties. When the parameter  X  = 1, the new model above is identical to the original Bradley-Terry model in Equation (4).
The Thurstone-Mosteller model is another probabilistic model for paired comparisons. Unlike the Bradley-Terry model, the Thurstone-Mosteller model is based on the Gaus-sian distribution. It can be obtained by letting F ( x ) be the Gaussian cumulative distribution in the general linear mod -els.
 P ( x  X  y ) =  X ( h ( x )  X  h ( y )  X   X  ) (7) where  X ( x ) = 1  X  distribution function.

Unless otherwise stated, we use the terms Bradley-Terry model and Thurstone-Mosteller model to represent the cor-responding model with ties in the rest of this paper.
Assume that we have observed the preference data and y for all i = N +1 , . . . , N + M . Our goal is to learn a ranking function h from this set of data. Following the principle of empirical risk minimization [13], we need to measure how good a ranking function is with respect to the training data. The empirical risk is defined as follows: where L ( h, x i  X  y i ) and L ( h, x i = y i ) are the loss of the function h on the pairs x i  X  y i and x i = y i , respectively. Based on the discussions in Section 4, we can use the nega-tive log of the data likelihood as the loss: The probabilities P ( x i  X  y i ) and P ( x i = y i ) are determined by the paired comparison models in Section 4.
 Bradley-Terry Model. The loss function for the Bradley-Thurstone-Mosteller Model. Analogously, the loss
Substituting L BT and L TM defined above into Equation (8), we obtain the loss functions we need to minimize in order to learn the ranking functions. In Section 5, we define the empirical risk function  X  R [ h ]. The learning process is to obtain a function h  X  from a func-tion space H that minimizes the empirical risk  X  R [ h ]. For-mally, In order to obtain h  X  , we apply the gradient boosting algo-rithm [10,30,31]. The idea of gradient boosting is to approx -imate h  X  ( x ) by iteratively constructing a sequence of base Algorithm 1 Functional Gradient Boosting Input: A finite sample of query document pairs Output: h ( x ), which minimizes the empirical risk func-1: Initialize h 0 ( x ) = g 0 ( x ) 2: for t=1, 2, . . . T do 3: Compute g x ti =  X  X  X   X  R [ h t ( x i )], and g y ti =  X  X  X  4: Fit a base learner g t ( x ;  X  t ) based on training data 5: Update the approximation by h t ( x ) = h t  X  1 ( x ) + 6: end for 7: return h T ( x ) learners. So we need to approximate h  X  ( x ) as: g ( x ;  X  t )( t = 0 , . . . , T ) are called base learners and  X  resents the parameters of base learner g t ( x ), and g 0 the initial approximation of h  X  ( x ). The main idea is to per-form gradient descent in function spaces. At each iteration , a base learner g t is chosen to be the gradient of  X  R [ h ] with respect to h at the current iterate. More precisely, the next iterate h t ( x ) is given by However, we can not compute the value of  X  X  X   X  R [ h t ( x )] at all x in general, but we can compute its values at a finite g
Follow the techniques developed in [10,30], we apply the base learner for constructing an approximation g t ( x ;  X   X  X  X   X  R [ h t ( x )] from the following finite training data, ( x We summarize the above algorithm in Algorithm 1. In our experiments, the base learners are regression trees and we usually set the number of terminal nodes to a small num-ber around 10. There are two other parameters need to determined. They are the number of the iterations T and the shrinkage factor  X  which are usually found by cross-validation.
In this section, we apply the proposed algorithms to three publicly available real-world data sets and report the resu lts of the experimental studies.
We used the Letor 2 data collection [20]. This data collec-tion contains three data sets: the OHSUMED data set, the http://research.microsoft.com/users/tyliu/LETOR/ TREC2003 data set and the TREC2004 data set which we now briefly describe.

OHSUMED. The OHSUMED data set is a subset of the MEDLINE database, which is popular in the informa-tion retrieval community. This data set contains 106 querie s. The documents are manually labeled with absolute relevance judgements with respect to the queries. There are three lev-els of relevance judgments in the data set: definitely relevant , possibly relevant and not relevant . Each query-document pair is represented by a 25-dimensional feature vector that contains the most frequently used features in information r e-trieval, for example tfidf, BM25 score etc. The total number of query-document pairs is 16,140.

TREC2003. This data set is extracted from the topic distillation task of TREC2003 3 . The goal of the topic dis-tillation task is to find good websites about the query topic. There are 50 queries in this data set. For each query, the human assessors decide whether a web page is an relevant result for the query, so two levels of relevance are used: rel-evant and not relevant . The documents in the TREC2003 data set are crawled from the .gov websites, so the fea-tures extracted by link analysis are also used to represent the query-document pair in addition to the content features used in the OHSUMED data set. The total number of fea-tures is 44 and total number of query-document pairs is 49,171.

TREC2004. This data set is extracted from the data set of the topic distillation task of TREC2004, so it is very similar to the TREC2003 data set. This data set contains 75 queries and 74,170 documents with 44 features.
All these data sets are labeled in the form of absolute judgements, so it is necessary to convert them to preference data. Given a query q , let x and y be the feature vectors for the query-document pairs ( q, d x ) and ( q, d y ), respectively. If d x has higher grade than d y , the preference x  X  y is included, while if y has greater grade than x , the preference y  X  x is included. Similarly, in the cases of ties, a tie x = y is included if d x and d y have the same grade.
In order to evaluate the performance of the proposed al-gorithms, three evaluation measures are applied: Precisio n, Mean average precision and Normalized Discount Cumula-tive Gain [1, 15]. All these evaluation measures are widely used for comparing information retrieval systems.
Precision. Given the binary relevance judgment, the pre-cision of a ranked list is measured by the fraction of the retrieved documents that are relevant. In our experimental studies, precision at position n (P@n) is used to measure the quality of the top n results of the ranking list.
Mean Average Precision. The average precision of a query is the average of the precision scores after each rele-vant document retrieved. Formally, average precision (AP) is calculated by the following equation.
 where rel i is the indicator function whether the i-th doc-ument of the ranking list is relevant to the query. Mean http://trec.nist.gov/ Average Precision (MAP) is obtained by the mean of the average precision over a set of queries. Compared with P@n measure, the MAP score is sensitive to the entire ranking list and contains the aspects of recall as well as precision. Normalized Discount Cumulative Gain. P@n and MAP are defined based on binary judgements: relevant and irrelevant. In the case of multiple levels of judgements, a more sophisticated evaluation measure called Normalized Discount Cumulative Gain (NDCG) is used [15]. Unlike P@n and MAP, NDCG has the capability to deal with mul-tiple levels of relevance. The NDCG value of a ranking list is calculated by the following equation: where r i is the grade assigned to the i -th document of the ranking list. In our experiments, r i takes value of 0, 1 and 2 in OHSUMED data set for not, possibly and definitely relevant documents respectively. For data sets with binary judgments, such as TREC2003 and TREC2004 data set, r i is set to 1 if the document is relevant and 0 otherwise. The constant Z n is chosen so that the perfect ranking gives an NDCG value of 1.
We want to address the following questions: 1. Are ties effective for enhancing the performance of the 2. For what types of search problems will ties improve 3. How does the performance of our algorithms com-4. What is the convergence behaviors of our proposed
Each data set is partitioned on queries to perform 5 fold cross-validation . For each fold, there are three subsets of data: training, test and validation set. For each loss func-tion described in Section 5, we tune the algorithms on the validation set to determine the parameters. All our experi-ments are performed on a server with four 3.2G CPUs and 2GB main memory. The gradient boosting algorithm takes about 1 hour for training both the Bradley-Terry model and the Thurstone-Mosteller model over the TREC2004 data set, which is the largest data set in our experiments. For AdaRank, we use the version for optimizing the mean av-erage precision for comparison in this study. The results of RankSVM, RankBoost, AdaRank and FRank are reported in the Letor data set.
For the first question, we apply both Bradley-Terry model and Thurstone-Mosteller model to the three data sets and compare the performance of the models with and without ties. For comparisons of the different models, we report the performance measured by both the precision and NDCG at position 1, 3 and 5 as well as the mean average precision (MAP). We average these performance measures over 5 folds for each data set. The results on OHSUMED, TREC2003 and TREC2004 data set are shown in Table 1, 2 and 3, respectively.

We can observe from the tables that the Bradley-Terry model and Thurstone-Mosteller model with the ties (labeled by BT and TM) outperform the corresponding models with-out ties (labeled by BT-noties and TM-noties). These tables show that the performance of ranking is improved (some-times significantly) by learning ranking functions with tie s. We conduct significant tests on the improvements of BT and TM over BT-noties and TM-noties. The results show that the improvements in terms of NDCG@5 is statistically sig-nificant (p-value &lt; 0 . 05).

We report in Table 4 the performance improvements ob-tained by learning ranking functions with ties. The improve -ments over the OHSUMED data set are more significant than that over both TREC2003 and TREC2004 data sets in most cases. We think this is because ties are more effec-tive when more relevant judgment levels are available while TREC2003 and TREC2004 data sets are labeled by only two relevant judgment levels. To further explore this point, we construct a new data set OHSUMED-2L by combining the definite relevant and possibly relevant judgements as a sing le level and perform experiments over this new data set. We report the improvements in performance in column five of Table 4. Compared with learning from three levels of judge-ments (column 2), the improvements is reduced by learning from two levels. This observation indicates that ties can enhance the ranking function more significantly when mul-tiple relevant levels of judgements are applied. In the case of multiple relevant levels, it is usually difficult to assign documents to the right relevant levels with only preference data. Ties provide information about the distribution of each relevant levels. This information is able to constrain the learning process and thus enhance the ranking functions significantly.

Another observation from Table 4 is that the perfor-mance improvements on the top-ranked results, for example NDCG@1 and P@1, are more significant. The reason for the better performance on the top-ranked results could be that ties capture the common features of the relevant documents. To illustrate the effect of ties, we examined the top five re-sults for Query 79 and Query 84 of OHSUMED data set in Table 5. The results are produced by the Bradley-Terry model with ties and without ties, respectively. We also list the document id and the ground-truth grade of each doc-ument. Taking Query 79 for example, it can be observed that the Bradley-Terry models with ties ranks the Docu-ment 45619 in the forth position, while the model without Table 4: The performance improvements by including ties measured by NDCG of top ten positions n OHSUMED TREC2003 TREC2004 OHSUMED-2L 1 11.26% 4.45% 4.35% 4.58% 2 6.71% 6.15% 4.48% 3.61% 3 5.30% 2.80% 4.40% 4.67% 4 5.58% 4.90% 4.27% 4.33% 5 2.94% 2.99% 1.82% 2.45% 6 5.77% 4.60% 4.53% 2.75% 7 4.94% 2.50% 3.60% 1.68% 8 1.57% 2.86% 0.98% 0.66% 9 1.94% 6.32% 2.61% 0.36% 10 2.62% 2.32% 1.87% 0.33% Table 5: Top five results of example queries on OHSUMED data set ties ranks it out of the top five. The reason is that the feature vector of Document 45619 can not distinguish this document from the irrelevant documents. Consequently, the model BT-noties does not return it in the top results. Thus, the performance of the model BT-noties will be reduced since it is actually labeled by definite relevant (represented by 2 ). On the other hand, since the common features of the rele-vant documents are explicitly captured by ties, the similar ity between Document 45619 and other relevant documents is precisely addressed. As a result, Document 45619 is ranked at the forth position by the model Bradley-Terry model with ties. Similarly, the model with ties ranks Document 32155 in the fifth position for Query 84 as reported in Table 5(b).
It is interesting to investigate the effect of ties extracted from different levels of grades. To this end, we conduct ex-periments over the OHSUMED data set by extracting ties from relevant , possibly relevant and irrelevant documents re-spectively. We also extract ties from both the relevant and the possibly relevant documents. The performance mea-sured by NDCG@5 is reported in Figure 2. It can be ob-served that when ties are extracted from the relevant doc-uments or both the relevant and the possibly relevant doc-uments, the performance of the resulting model slightly re-duced. However, if the ties are extracted from the irrelevan t documents, the decrease in performance is much more sig-Figure 2: Performance vs. ties from different relevance levels
Figure 3: Learning curves of Bradley-Terry model nificant. One explanation is that the irrelevant documents are more diverse than the relevant document and thus ties become less effective in this case.
 The performance of Bradley-Terry model and the Thurstone-Mosteller model are comparable in most cases. We also compare the performance of our algorithms with RankSVM, RankBoost, FRank and AdaRank as reported in Table 1, 2 and 3. The Bradley-Terry model and Thurstone-Mosteller model with ties outperform both RankSVM and RankBoost over the OHSUMED data set. On TREC2003 data set, our algorithms are comparable or slightly bet-ter than RankSVM, but RankBoost is much worse while on TREC2004 data set our algorithms are comparable or slightly worse than RankBoost, but RankSVM is worse. What causes the dramatic difference of RankSVM and RankBoost on TREC2003 and TREC2004 data set remains to be investigated. Again, we want to emphasize that the focus of the paper is on investigation of the role of ties in learning ranking functions not comprehensive comparisons of existing methods.

In order the study the convergence behaviors of our algo-rithms, we plot the NDCG@5 over the training, test and val-idation set with respect the iteration number of the gradien t boosting algorithm in Figure 3. For each iteration, we aver-age the NDCG@5 scores of all five folds of the OHSUMED data set. It can be observed from Figure 3 that performance on training set monotonically increases as the number of it-erations grows, so the performance will gradually converge to a maximum point. Similar observations can be made also on the validation and test sets as well as over the TREC2003 and TREC2004 data sets.
In this paper, we demonstrated that incorporating can im-prove the performance of learned ranking functions based on the experimental studies over three publicly available dat a sets. Furthermore, we find that the improvements over the top-ranked results are more significant since the common features of relevant documents are explicitly captured by ties. This promising property of ties makes them effective for web search engines.

Future directions of research include theoretical analysi s of the effect of ties. We plan to explain the effectiveness of ties for the ranking problem in terms of the generaliza-tion bounds. Another direction of research is to develop new techniques for incorporate ties with other methods for lear n-ing to rank, such as RankSVM, RankBoost and RankNet. There several methods for online learning from preference data [14, 21], we hope to develop learning algorithms that both preference data and ties can be included to refine a ranking model incrementally. Also, since our framework is flexible to incorporate a large class of loss functions, it is interesting to compare and analysis other loss functions and determine what kind of characteristics the loss functio n should have for the ranking problems. [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern [2] R. Bradley and M.Terry. The rank analysis of [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [4] C. J. Burges, R. Ragno, and Q. V. Le. Learning to [5] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. [6] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. [7] C. Cortes, M. Mohri, and A. Rastogi.
 [8] H. A. David. The Method of Paired Comparisons . [9] Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. [10] J. Friedman. Greedy function approximation: a [11] J. Friedman. Stochastic gradient boosting.
 [12] G. Fung, R. Rosales, and B. Krishnapuram. Learning [13] T. Hastie, R. Tibshirani, and J. H. Friedman. The [14] R. Herbrich, T. Graepel, and K. Obermayer. Large [15] K. J  X arvelin and J. Kek  X al  X ainen. Cumulated gain-base d [16] T. Joachims. Optimizing search engines using [17] T. Joachims. A support vector method for multivariate [18] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [19] J. Lafferty and C. Zhai. Document language models, [20] T. Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: [21] F. Radlinski and T. Joachims. Active exploration for [22] P. Rao and L. Kupper. Ties in paired-comparison [23] S. Robertson and D. A. Hull. The TREC-9 filtering [24] G. Salton, A. Wong, and C. S. Yang. A vector space [25] F. Song and W. B. Croft. A general language model [26] L. Thurstone. A law of comparative judgement. [27] M.-F. Tsai, T.-Y. Liu, T. Qin, H.-H. Chen, and W.-Y. [28] J. Xu and H. Li. Adarank: a boosting algorithm for [29] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A [30] Z. Zheng, K. Chen, G. Sun, and H. Zha. A regression [31] Z. Zheng, H. Zha, T. Zhang, O. Chapelle, K. Chen,
