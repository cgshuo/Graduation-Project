 We describe our experiences in applying learning-to-rank techniques to improving the quality of search results of an online hotel reservation system. The search result quality factors we use are average booking position and distribution of margin in top-ranked results. (We expect that total revenue will increase with these factors.) Our application of the SVMRank technique improves booking position by up to 25% and margin distribution by up to 14%. H.3.5 [ Information Storage and Retrieval ]: Online Information Services  X  commercial services , web-based services . H.2.8 [ Database Management ]: Database Applications  X  data mining . Algorithms, Performance, Econom ics, Experimentation. Learning to rank, retail search engine, revenue maximization, multiple objectives, hotel reservations. Unlike traditional search engines, which are designed to return the results that are most relevant to a query, retail search engines are designed to maximize revenue. This goal is realized by improving revenue per conversion as well as conversion rate. Our goal is to identify results that are both relevant to a query and yield high margin. To this end, we apply a  X  X earning to rank X  (LTR) technique and adapt it to consider margin when producing the final ranked result set. The challenge is that the goals of query relevance and margin ma ximization are often mutually exclusive. Improving on one metric at the expense of the other may have a significant negative impact on total revenue. the Chicago and New York markets. We use an adapted version of SVMRank [5] as our LTR technique. Offline experimental results show that, with some tuning, SVMRank has the potential to improve both relevance and margin in search results. Liu describes three types of LTR techniques based on their types of training data [8]. Point-wise techniques require a relevance judgment for each result in a training set [3][7]. Pair-wise techniques require a relative relevance judgment for each pair of results [1][5]. List-wise techniques require relevance judgments for a list of results [2][9]. Among the many LTR techniques, we adopt the pair-wise SVMRank for several reasons. First, it is easy and cheap to collect preferences. (Building training data for either point-wise or list-wise techniques is much less pr actical.) Second, SVMRank is efficient in both learning and ranki ng, as it is a linear SVM. Other pair-wise LTR techniques, such as those based on neural networks, have a higher learning cost [1]. Third, SVMRank is guaranteed to find the global optimum, unlike other, non-convex techniques, such as ones base d on neural networks. Finally, SVMRank results in a linear scoring function, which is easy to understand and tune. Complex, nonlinear functions may also over-fit training data and perform poorly when deployed. A query is defined by a location and a date range and retrieves a ranked list of results. Each result is a hotel that matches the query criteria and is defined by features, including:  X  Price  X  The nightly room rate.  X  Star rating  X  The star rating.  X  Distance  X  The distance from the city center.  X  Margin  X  The profit earned by the seller of the hotel. We consider the first three featur es (among others) to be  X  X uyer preferences X  or  X  X uyer features X  because their values directly affect the result X  X  relevance to the user. Margin, in contrast, is a  X  X eller preference X  or  X  X eller feature X  because its value is of greatest concern to the seller. (Note that seller features are generally not exposed to users.) We assume that all features X  valu es are fixed. We thus do not consider the case where prices and margins can be tuned based on some special knowledge of the market. A result set is a ranked list of hotel information retrieved by a user known as an impression for the hotel. Each time the user books a rank of the booked hotel in a result set as the booking position of number of conversions to the numbe r of impressions for the hotel. conversions to the number of result sets. Margin (or revenue ) is the profit margin associated with the sum of the margins of the top N results in the result set. Total revenue is total margin of all bookings. Average margin is total revenue divided by the number of bookings.
 booking positions, excluding result sets with no bookings. Our goal is to maximize total revenue. To do this, our approach must increase one or both of conversion rate and average margin. We assume that conversion rate increases with the quality of the query results in terms of relevan ce. We also assume that users have a strong bias toward clic king (and, hence, booking) the top-ranked results. ( X  X osition bias X  is a well-known and accepted phenomenon (e.g., [4]) and also occurs in our data.) We measure the ability of our ranking function to rank relevant results highly by average booking position and measure the ability of our ranking function to rank high-margin results highly by margin distribution. Hence, if we are able to improve either average booking position or margin distribution (or both) without Although we do not quantify the ch ange in revenue, we believe that there is a direct relationship between total revenue and improvement in either booking position or margin distribution. For base case performance, we us e a standard ranking function, where each hotel j is scored as a weighted sum of feature values: n ) and w j is the weight of this feature. Initial weights are manually tuned by a system admi nistrator based on business goals (i.e., revenue maximization) and empirical evidence. Each result receives a score computed by this formula, and results are sorted in descending order of score. As will be shown, we achieve our goals to varying degrees by tuning these weights. Our training data consists of a ra ndom subset of Orbitz impression data from the last quarter of 2009 for the Chicago and New York markets  X  consisting of thousands of bookings. For each impression, we generate pair-wise preference rules , where a booked result is defined as being mo re relevant than others (i.e., unbooked results) in the result set. We apply SVMRank [5] to the impression data to yield we ights for the ranking function described above in Equation 1. We define as a positive rule any rule that relates the booked result to lower-ranked results. Simiarlly, we define as a negative rule any rule that relates the booked hotel to higher-ranked results. All SVMRank is to generate a mode l that minimizes the number of rule violations in the training set. Let rank i be the rank of the booked hotel in result set i . Let N the number of result sets in the impression data. For each experiment, we use the following metrics to measure result quality in terms of relevance:  X  Average booking position (ABP)  X  This is the average rank  X  Mean reciprocal rank (MRR)  X  We measure the average of Ideally, ABP is as low as possible, with a minimum value of 1. MRR is ideally as high as possibl e, with a maximum value of 1. ABP gives an unbiased view of result quality in the sense that it measures changes in booking position anywhere in a ranked list (unlike MRR). However, MRR captures the fact that users have a strong preference for higher-ranked results in practice. 10 (denoted m @5 and m @10). As stated above, ABP, MRR, and margin are assumed to be relate d to conversion rate and average margin. We report the percentage change in these metrics between baseline results (see Section 4.1) and experimental results. For consistency, we use positive percentage changes to indicate positive results. Hence, a positive percentage change in ABP actually refers to a decrease in rank value (i.e., closer to the top). We use 10-fold cross validation to tune parameters. With SVMRank, there are several parameters to tune (e.g., parameters that control for over-fitting). We use standard parameter exploration techniques [10] and pick the parameters that minimize the following expression: In this expression, pctimprove () represents the improvement for the respective metric for a set of parameters. We use both MRR and ABP to represent the dual goals of improving ranking quality near the top of the list of results and improving overall ranking quality. The margin @5 is included to find high-margin solutions and is given an equal contri bution as both ABP and MRR. SVMRank, by design, minimizes the number of rule violations (see Section 4.2) [6], which only indirectly satisfies our goals. Hence, we apply this additional obj ective. Similarly, we test a variety of numbers of positive and negative rules and pick the number that gives the best ove rall performance during training. After applying SVMRank to our da ta set to the impression data, we were able to improve average booking position and MRR as shown in Figure 1. ABP (MRR) improves by at least 25% (15%) in both markets. The fact that both ABP and MRR improve suggest that the improvement occurred over all ranking positions. Unfortunately, these improvements come at a price. Margin @ N decreases in both markets. Chicago X  X  margin @ 5 decreased by 12%, while New York X  X  decreased by 1%. This shows that users prefer less expensive hotels (the re was also a drop in  X  X rice@ N  X  on the order of 15%), which generally yield lower margins. The large improvement in ABP and MRR are good signs. However, the negative values in th e margin distributions in these results makes uncertain the effect of this model on total revenue, as discussed above. Hence, we s earch for a better solution with  X  one with no negative results. Due to the large improvement in buyer preferences with the basic SVMRank results, we decided to trade off ABP/MRR performance for improved margin @ N performance by varying the weight assigned to margin, f i M , using parameter  X  : We pick the  X  value that maximizes Equation 4 and show the results in Figure 2. We were able to improve margin distribution with a drop ranking quality (e.g., for Chicago, the percentage change in ABP drops from 28% to 23%). However, ranking quality is still positive compared with the base case. For Chicago, ranking quality is still good with 23% and 3% scores for ABP and MRR, respectively. Margin @5 and margin @10 are also both positive, but small at about 3% for both. We also tried improving margin @ N performance by a process we preference rules based on our objectives. Figure 2. Weighted sum of buyer and seller preferences. With rule deletion , we first create a set of input rules for SVMRank as described in Sections 4.4. We then delete a subset of these rules based on the following criterion: Let R be a rule between results r i and r j : r denote the margin value of the hotel associated with r i Hence, if  X  = 1, we keep all rules, and if  X  = 0, we only keep the rules that rank the results in descending order of margin. Note that  X  = 0 does not necessarily result in a ranking based on margin because of the contribution of the other features to the scoring. With rule addition, we probabilistically replicate rules that reinforce the seller X  X  preferences if the following criterion holds: Let R be a rule in the rule set between results r With probability 1 - X  : The goal of rule addition is to reinforce rules that satisfy the margin requirement and to weaken rules that violate it. Again, if  X  = 1, this technique reduces to the base case rule set. If weaken margin violations. For our experimental results, we tune  X  from Equation 5,  X  from above to values that maximize Equation 4. As shown in Figure 3, our results for rules engineering by deletion have mixed results. Fo r the Chicago market, ABP and MRR improve by 23% and 14%, respectively. Margin @5, however decreases by 3%, while margin @10 increases by 1%. The New York results are better overall. New York has ABP and MRR scores similar to those of Chicago at 24% and 10%, respectively, but its margin @5 and margin @10 are significantly better at 10% and 3%, respectively. As shown in Figure 4, the results for rule addition are more improvements to ABP and MRR are lower (there is no change in MRR for New York), the improveme nts in margin distribution are much greater than with rule deletion. Overall, our techniques improve on ABP more than MRR. This is likely a sign of how difficult it is to improve the rank position of results already ranked near the top of a list. Margin is also difficult to improve on, likely due to users X  preferences for more economical hotel rooms. Fortuna tely, the data strongly suggests that it is possible to improve rankings in a way that improves total revenue. Judging from the results above, the ordering of the ranking techniques from most to least effective is: We quantify the ordering of these three results by their scores with Equation 4. The fact that the rules engineering results performed both best and worst suggests the importance of picking the right set of preference rules for training SVMRank. Intuitively, to the learning process, rules addition ma kes more data available, while rules deletion does the reverse. This may explain their overall performances. Also, the s uperior performance of rules engineering suggests the importance of building a good model before fine-tune performance usi ng techniques like weighted sum. In practice, both rules engineeri ng and tuning the contribution of margin are important to retail search engine performance. Rules engineering will boost expected performance offline and tuning the weight of margin to rank score allows control over total revenue. Our goal is to share our experiences with learning how to rank to satisfy both buyer and seller objectives using SVMRank on real hotel booking data. Our business objective is to maximize revenue and our approach is to improve ranking quality and distribution of margin over the top-ranked results. We try three techniques: a weight ed sum score, rules engineering by rule deletion, and rules engineering by rule addition. Our results indicate that it is possible to improve both ranking quality and margin distribution. In Chicago, for example, we were able to improve average booking position by 19% and margin@5 by 3%. To this end, rules addition was the most approaches may be a better way of approaching this problem. For future work, we will consider other ways of creating and selecting rules for input into the learning system. We will also explore ways of creating a pairwise LTR technique that directly optimizes for the objective of total revenue. [1] Burges, C. J. C., Shaked, T., Renshaw, E., Lazier, A., Deeds, [2] Burges, C. J. C., Ragno, R., and Le, Q. V. 2007. Learning to [3] Crammer, K. and Singe r, Y. Pranking with Ranking. Advances [4] Agichtein, E, Brill, E., Dumais, S., and Ragno, R. 2006. [5] Joachims, T. 2002. Optimiz ing search engines using [6] Joachims, T. 2005. A Support Vector Method for [7] Li, P., Burges, C. J. C., and Wu, Q. 2008. Learning to Rank [8] Liu, T.-Y. 2009. Learning to Rank for Information [9] Moon, T., Smola, A., Chang, Y. and Zheng, Z. 2010. [10] Witten, I. and Franke, E. 2005. Data Mining: Practical 
