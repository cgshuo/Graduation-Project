 It is becoming increasingly common to construct databases from information automatically culled from many heteroge-neous sources. For example, a research publication database can be constructed by automatically extracting titles, au-thors, and conference information from online papers. A common difficulty in consolidating data from multiple sources is that records are referenced in a variety of ways (e.g. ab-breviations, aliases, and misspellings). Therefore, it can be difficult to construct a single, standard representation to present to the user. We refer to the task of constructing this representation as canonicalization . Despite its impor-tance, there is little existing work on canonicalization.
In this paper, we explore the use of edit distance mea-sures to construct a canonical representation that is  X  X en-tral X  in the sense that it is most similar to each of the dis-parate records. This approach reduces the impact of noisy records on the canonical representation. Furthermore, be-cause the user may prefer different styles of canonicaliza-tion, we show how different edit distance costs can result in different forms of canonicalization. For example, reduc-ing the cost of character deletions can result in represen-tations that favor abbreviated forms over expanded forms (e.g. KDD versus Conference on Knowledge Discovery and Data Mining ). We describe how to learn these costs from a small amount of manually annotated data using stochas-tic hill-climbing. Additionally, we investigate feature-based methods to learn ranking preferences over canonicalizations. These approaches can incorporate arbitrary textual evidence to select a canonical record. We evaluate our approach on a real-world publications database and show that our learning method results in a canonicalization solution that is robust to errors and easily customizable to user preferences. H.2 [ Information Systems ]: Database Management; H.2.8 [ Information Systems ]: Database Applications X  data min-ing Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. Algorithms Data mining, information extraction, data cleaning
Consider a research publication database such as Citeseer or Rexa 2 that contains records gathered from a variety of sources using automated extraction techniques. Because the data comes from multiple sources, it is inevitable that an attribute such as a conference name will be referenced in multiple ways. Since the data is also the result of extraction, it may also contain errors. In the presence of this noise and variability, the system must generate a single, canonical record to display to the user.

Record canonicalization is the problem of constructing one standard record representation from a set of duplicate records. In many databases, canonicalization is enforced with a set of rules that place limitations or guidelines for data entry. However, obeying these constraints is often te-dious and error-prone. Additionally, such rules are not ap-plicable when the database contains records extracted auto-matically from unstructured sources.

Simple solutions to the canonicalization problem are of-ten insufficient. For example, one can simply return the most common string for each field value. However, incom-plete records are often more common than complete records. For instance, this approach may canonicalize a record as  X  X . Smith X  when in fact the full name (John Smith) is much more desirable.

In addition to being robust to noise, the system must also be able to adapt to user preferences. For example, some users may prefer abbreviated forms (e.g., KDD ) instead of expanded forms (e.g., Conference on Knowledge Discovery and Data Mining ). The system must be able detect and react to such preferences.

In this paper, we first formalize the canonicalization prob-lem and then propose three solutions. The first uses string edit distance to determine which record is most central in a set of records. This approach can mitigate the noise con-tained in outlying records. To enable the system to adapt to user preferences, the second solution optimizes the edit distance parameters using human-labeled data. Finally, we www.citeseer.ist.psu.edu www.rexa.info describe a feature-based solution that can flexibly incorpo-rate textual evidence to select a canonical record. We again estimate the parameters of this method from labeled data. We show that this problem can be more naturally formu-lated as a ranking task, rather than a classification task, and modify the learning methods accordingly.

We perform several empirical comparisons of these ap-proaches on publication records culled from the Web. The results indicate that the feature-based approach significantly outperforms competing approaches as well as a number of simpler baselines. Furthermore, we show that the param-eters of the feature-based approach can be estimated from just a few training examples, and is quite robust to noise that is common in automatically generated databases.
While there has been little work explicitly addressing canon-icalization, the idea has been present in many application and research areas. In this section we review several of these applications as well as related work in the area of learning string edit distance costs.

Tejada et al. [13] devise a system to automatically ex-tract and consolidate information from multiple sources into a unified database. When a user queries this database, mul-tiple representations of an attribute are inevitable due to naming inconsistencies across the various sources from which they were drawn. Although object deduplication is the pri-mary goal of the that research, canonicalization arises when the system presents results to the user. Tejada et al. propose ranking the strings for each attribute based on the user X  X  confidence in the data source from which the string was ex-tracted.

One difficulty in this approach is that if the data is ex-tracted from a large number of sources, a non-trivial burden is placed on the users, who may not have the expertise to express preferences about each data source. Additionally, the database must store source-specific meta information for each string. Our canonicalization methods are adaptable to any database, regardless of whether the source of the in-formation is available. Additionally, we enable the user to express preferences independent of the data source.
Other work has focused on learning the parameters of string edit distance with encouraging results. Zhu and Unger [15] apply string edit distances to the task of merging data-base records. They observe that parameters cannot be opti-mized individually due to the complex interaction between various edit cost weights on the outcome. Additionally they note that greedy methods are too likely to converge prema-turely in local optima and that random restarts are unnec-essarily expensive. Instead they propose a genetic algorithm to learn the weights of each cost and find that it stabilizes after 100 generations. In lieu of genetic approaches we pro-pose learning the edit costs using either stochastic search, or an exhaustive search over a relatively small discrete space of possible parameter settings.

McCallum et al. [9] also use a discriminatively learned edit distance to perform record deduplication. They extend Zhu and Unger X  X  work by using a conditional random field to learn the costs of a variety of flexible edit distance oper-ations. However, they do not explore canonicalization.
Ristad and Yianilos [12] learn a probability distribution over atomic string edit operations (insertion, deletion, sub-stitution) and define a stochastic transducer that defines the probability of a string as either the Viterbi sequence of edit operations or the sum of all possible sequences of edit op-erations required to produce that string. The parameters of this generative model are learned using the expectation maximization (EM) algorithm.

Bilenko and Mooney [1] present a method to learn edit distance based similarity measures of each attribute between records in order to perform deduplication. They extend the work of Ristad and Yianilos [12] by accommodating affine gaps. In similar fashion, the weights are learned iteratively with EM.

Our learning methods differ from those outlined in Ristad and Yianilos [12] and Bilenko and Mooney [1] in that we are not concerned with learning a generative model. We pro-pose two methods to learn edit distance parameter settings using stochastic or exhaustive search. Additionally, we pro-pose two feature-based methods that combine the outputs of multiple parameter settings (i.e., multiple edit distance models) and other sources of textual evidence into a dis-criminative model of canonicalization.

Canonicalization has also been implicitly considered in deduplication research. For example, Milch et al. [11] and McCallum and Wellner [10] propose deduplication models containing variables for canonical attributes of a record. The variables are used to help deduplication, although the accu-racy of the resulting canonical records is not optimized or evaluated.

Recently, frameworks have been proposed to handle un-certainty in databases, particularly those containing auto-matically extracted records. One approach when consoli-dating extractions from various sources is to perform some type of weighted voting to determine which facts should be inserted in the database [8]. Another approach is to store the uncertainty of these extractions directly in the database. This can be accomplished by storing the top n most confident extraction results (along with correspond-ing probabilities or confidence measures) for each desired record. Gupta and Sarawagi [5] leverage confidence value outputs from the extraction models to improve query re-sults on databases containing uncertainty. Fundamentally the problem is canonicalization because the system is faced with a choice when presenting multiple query results with various confidence values to the user. It is analogous to our canonicalization task except that we do not have the luxury of confidence values. While the inclusion of such values is clearly beneficial, we propose methods that achieve canon-icalization in absence of such information (and often this information is strictly unavailable).
Let a record R be a set of fields, R = { F 1 ...F p } .Let field F i be an attribute-value pair a, v . Table 1 shows three example records.

Databases constructed from multiple sources often accu-mulate multiple versions of the same record. Record dedu-plication is the task of detecting these different versions. Table 1 shows three records that have been predicted to be duplicates. In fact, record (c) refers to a book chapter ver-sion, whereas (a) and (b) refer to conference proceedings.
Record deduplication is a difficult problem that has been well-studied. However, in this paper we are interested in a subsequent step: how to present the user one canonical representation of a record.

We define the canonicalization problem as follows: Given a set of duplicate records R = { R 1 ...R k } , create a canon-ical record R  X  that summarizes the information in R .We refer to the canonicalization operation as C ( R )
Note that it is not always clear what the optimal canoni-calized record should be. Indeed, different users may prefer different forms of canonicalization. For example, some users may prefer the abbreviated conference string IJCAI , while others may prefer the expanded string International Joint Conference on Artificial Intelligence . However, there are a few desiderata of a good canonicalization:
We now outline three classes of canonicalization solutions, in increasing order of ambition. We will then explore the first solution class in more depth.
The record selection approach to canonicalization selects an existing record as its output. For example, C ( R ) must select from the three records in Table 1. Record selection algorithms must ensure that the selected record contains no errors, and that is is representative of other records. Note that this approach is most prone to errors of incompleteness, since one record may not contain all the fields present in the duplicates. For example, selecting record (a) in Table 1 will omit the page numbers, but selecting record (b) will omit the full first name of the author.
The record merging approach to canonicalization constructs a canonical record by piecing together fields from different records.

While this approach can increase the completeness of canon-icalization, it does so at the risk of introducing errors. In the worst case, an error in record deduplication may merge together records that in fact refer to different objects. Con-structing one record containing fields from these non-duplicate records can result in a canonical record containing invalid information. For example, a record merging approach may return the record in Table 2: Table 2: Possible record merging canonicalization for the records in Table 1.

While this result is complete, it erroneously includes the editor field from record (c) , which is not truly a duplicate. To address this issue, it may be useful to consider measures of field compatibility, as in Wick et al. [14].
The record generation approach to canonicalization is an extension of the record merging approach that may also pro-pose field values that do not explicitly exist in any of the record duplicates. This allows the system to predict field values based on pattern analysis. For example, a record generation approach may return the following record: Table 3: Possible record generation canonicalization for the records in Table 1.

Here, the system has generated an expanded venue value from the abbreviated form, even though this expanded form does not appear among the duplicate records. This predic-tive operation can be accomplished either by learning sta-tistical patterns in the database, or by a pattern-matching approach.

While in this case record generation succeeded, in general positing field values that do not exist in any of the records can be quite dangerous and lead to unacceptable errors.
These three solution classes motivate a number of im-plementations and experiments. In this paper, we describe three record selection methods and perform experiments to measure their effectiveness.
The motivation for our approach is to minimize the effect of pre-processing errors on canonicalization. As we have de-scribed, errors from PDF-to-text conversions, misspellings, and incorrect deduplication can lead to poor canonicaliza-tion choices.

We make two assumptions about the behavior of pre-processing errors:
With these assumptions in mind, we propose selecting the record that has the greatest average similarity to every other document. We define the distance between two records as the string edit distance between them.

Let D : R i  X  R j  X  X  + be the edit distance between two records. Given a set of duplicate records R = { R 1 ...R we define the average edit distance of record R i as
The canonical record we return is the one with minimum average distance to every other string: We refer to C d ( R )asthe edit distance canonicalizer .
We now must decide on the form of D , the metric defining the distance between two strings. A natural choice is the Levenshtein distance: the number of character insertions, deletions, and replacements required to transform one string into another [6]. The recursive definition of the Levenshtein distance for strings s n and t m with length n and m is the following: where c r ( s n ,t m )isthe replacement cost for swapping char-acter s n with character t m , c i is the insertion cost ,and Algorithm 1 Exhaustive cost search 1: Input: 2: while More Costs do 4: if L ( c , S ) &lt; bestLoss then 5: bestLoss  X  X  ( c , S ) 7: end if 8: end while is the deletion cost . We can further define the replacement cost as That is, c = r is the cost of replacing one character with an-other, and c = r is the cost of copying a character from one string to the next. We refer to c = r as the substitution cost , and c = r as the copy cost .

The value of the edit distance costs greatly effects the out-put of the system. For example, if c i is small, then abbre-viated strings will have a small distance to their expanded version. Abbreviated strings will therefore have lower values of A ( R i ).

Rather than requiring the user to manually tune these costs, in the next section we propose ways of learning these costs automatically given labeled examples. Suppose the user provides a labeled training set where each set of duplicate records R i = { R 1 ...R k } is an-notated with label l i  X  X  1 ...k } , indicating which of the duplicates should be selected as the canonical record (i.e., R l i  X  R is the true canonical record). We wish to use S to learn the weights of D .

There has been a fair amount of work on methods to auto-matically learn edit distance costs, mostly applied to record de-duplication; (see Section 2). However, we are not aware of any work that learns edit distance costs for canonicaliza-tion.

We propose two simple but effective methods to learn edit distance costs from training data: exhaustive search and stochastic hill climbing .
The simplest method is to exhaustively enumerate set-tings of each cost and maximize the canonicalization perfor-mance on the training set.

Let L ( c , S ) be the loss function for an assignment to c .For example, L may be the proportion of records in S for which ( R ) returns a non-canonical record; i.e. C d ( R i ) = R
We wish to optimze c as follows:
Since we must discretize the cost settings to perform ex-haustive search, the input is the following: Algorithm 2 Stochastic cost search 1: Input: 2: for i&lt; NumIterations do 6: if L ( c , S ) &lt; bestLoss then 7: bestLoss  X  X  ( c , S ) 9: end if 10: i  X  i +1 11: end for
Search proceeds by cycling through each setting of c and returning the best found setting c  X  . The details are given in Algorithm 1. The method NextCosts generates the next cost setting as determined by the step size.
Computing L ( c , S ) requires computing C d ( R ) for all R S . This computational cost limits the number of settings we can enumerate using exhaustive search. Instead, we pro-pose a simple stochastic hill-climbing algorithm to optimize Equation 4. Given an initial setting for c , the algorithm pro-poses a modification to c and accepts the change if L ( c decreases. This can be understood as simulated anneal-ing without the temperature parameter. The details of this method are given in Algorithm 2.

The method SampleCostElement samples a cost uniformly from the cost vector. The method RandomUpdate uni-formly chooses between incrementing or decrementing c by step .
While the adaptive edit distance approach to record se-lection can be simple and effective, it is limited by the small number of tunable parameters (the four costs), which limits the expressivity of the model.

In this section, we propose two feature-based learning ap-proaches that enable the use of arbitrary features over the records, including the output of various edit distance mea-sures.
 Consider a set of duplicate records R = { R 1 ...R k } .Let  X  i = {  X  1 ( R i ) ... X  m ( R i ) } beavectorof binary feature func-tions  X  : R  X  X  0 , 1 } that compute evidence indicating whether R i should be selected as the canonical record. For example,  X  j ( R i ) may be 1 if record R i is the longest record in R .Let X = {  X  1 ... X  m } be a vector of real-valued weights associated with each feature.

We can compute a score for the event that R i is chosen as the canonical record by taking the dot product of the features and weights:
Below we describe two methods to estimate  X  from the training set S .
The first method is based on logistic regression (some-times called maximum entropy classification ). Here, we modify the traditional logistic regression loss function to rank , rather than classify , instances.

Let the binary random variable C i be 1 if and only if record R i is the true canonical record of R .Given X and F we can compute the probability of C i as follows: where the score for record R i is normalized by the scores for every other duplicate record.

Note that this formulation differs from traditional classifi-cation, which computes an independent binary classification decision for each record: where the parameters  X  1 represent the positive class, and  X  o represent the negative class.

By formulating canonicalization as a ranking task rather than a classification task, we can compute a loss function that is sensitive to competing records. This can be beneficial for at least two reasons. First, if no record is error-free, a classification loss function is forced to place a positive label on a partially incorrect example. Second, if a non-canonical record shares many features with the canonical record, a classification loss function will erroneously penalize those features. By focusing on the differences between examples, a ranking loss function overcomes these deficiencies.
We can estimate  X  from the training set S by minimizing the negative log-likelihood of the data given  X : Note that this is the sum of probabilities for each of the cor-rect canonical records for the current setting of  X . We also add a Gaussian prior over  X  with fixed mean and variance to mitigate over-fitting. We find the setting of  X  that min-imizes Equation 5 using limited-memory BFGS, a gradient ascent method with a second-order approximation [7].
MIRA (Margin Infused Relaxed Algorithm) is a relaxed, online maximum margin training algorithm [4]. It iteratively cycles through the training set and updates the parameter vector with two constraints: (1) the true canonical record must have a higher score than any other record by a given margin, and (2) the change to  X  should be minimal. This second constraint is to reduce fluctuations in  X . Using the same scoring function  X  as in the previous section, this opti-mization is solved through the following quadratic program:
In this case, the MIRA parameter update is a quadratic program with constraint size equal to the number of non-canonical records in the training example. This QP can be solved efficiently using the Hildreth and D X  X sopo method [2]. To improve the stability of this online method, we average the parameter vectors from each update at the end of train-ing, as in voted perceptron [3]. We collect 3,683 citations to 100 distinct papers from Rexa, an online publications search engine. 3 These citations were automatically extracted from the headers of research papers as well as from the reference section, and record deduplication was performed automatically by the Rexa sys-tem. The data therefore contains misspellings, PDF-to-text errors, abbreviations, and possibly deduplication errors. To construct a labeled data set, we collect the corresponding citations to these papers from the Digital Bibliography and Library Project (DBLP). 4 The DBLP citations are manually curated to ensure accuracy, so they provide a good source of canonical examples. In fact, as part of its pipeline, Rexa crawls the DBLP repository and performs record deduplica-tion to merge citations together.

For these experiments, we focus on constructing the canon-ical representation of the conference string for each paper. This is arguably the most difficult field to canonicalize be-cause of the prevalence of acronyms, abbreviations, and mis-spellings.

Using the DBLP data, we construct two versions of the dataset. In the first, the true canonicalization is the confer-ence title acronym. This simulates the use case in which the user desires abbreviated canonical forms. In the second ver-sion, the true canonicalization is the expanded conference title. This simulates the case when the user does not desire any abbreviations in the canonical form. We refer to the former version as the acronym dataset, and the latter as the expanded dataset.

Table 4 shows an example with labels from each of the datasets. We can see that the duplicate records contain a variety of abbreviated forms, as well as PDF-to-text conver-sion errors in the first and fourth duplicate records ( Artifici al , Conferenceonarti cial ) that make canonicalization diffi-cult.

Figure 1 shows the distribution of the number of dupli-cates for each record in the dataset. As we can see, most records have around 20 duplicates, but a few records have over 200 duplicates.

We perform 5-fold cross validation on the data, with each split containing 80 training examples and 20 testing exam-ples. To evaluate performance, we consider two measures:
Data is available at http://www.cs.umass.edu/  X  culotta/ data/canonicalization.html http://dblp.uni-trier.de Figure 1: Distribution of number of duplicates per record.
We evaluate eight different systems:
The features for the feature-based canonicalizers are as follows: ( 0); ( c d =1 ,c = r =0 ,c i =1 ,c = r = 0); ( c d = c = r 0) canonicalization: acronym and expanded forms. Table 5: Mean reciprocal rank, accuracy, and run-ning time on expanded dataset. The numbers in parentheses are the standard error over five cross-validation trials.
Tables 5 and 6 display results for the eight different meth-ods on the acronym and expanded datasets.

From these results, we can conclude that the feature-based canonicalizers consistently outperform the edit-distance canon-icalizers. We can see that for the expanded data logistic re-gression ( LR ) outperforms the fixed cost edit-distance ( ED-F ) by 27% MRR, and further outperforms the stochastic search edit distance ( ED-S ) by 11% MRR. The difference Table 6: Mean reciprocal rank, accuracy, and run-ning time on acronym dataset. The numbers in parentheses are the standard error over five cross-validation trials. between the two feature-based canonicalizers is small: logis-tic regression outperforms MIRA ( M ) by nearly 5% MRR on the expanded data, but MIRA outperforms logistic re-gression by .5% MRR on the acronym data. Similarly, the difference between the two cost learning methods is small ( ED-S versus ED-E ).

Furthermore, we can conclude that cost-learning greatly improves the performance of the edit-distance canonicaliz-ers, increasing MRR by nearly 16% on the expanded data and by 80% on the acronym data. The pronounced differ-ence on the acronym data can be attributed to the fact that the default setting used in ED-F has unit cost for inserting characters. This gives acronym records a large distance from non-acronym records, making it unlikely they will have the lowest average distance. However, the cost learning methods can discover settings that do not penalize insertions, thereby reducing the average edit-distance of acronyms.

None of the simpler baseline methods perform consistently well across the two datasets. Simply choosing the shortest, longest, or most common record is significantly worse than using one of the more complex record selection algorithms we propose.
We investigate the impact of features on the performance of the feature-based canonicalizers. Table 7 displays per-formance of the logistic regression method ( LR )withand without the textual features described in Section 6.2. These results show that using edit-distance features alone still out-performs the fixed-cost edit-distance canonicalizer ED-F by nearly 6% (.496 versus .438 from Table 5). However, the majority of the improvement of the feature-based classifiers appears to come from textual features. This result suggests Table 7: Mean reciprocal rank and accuracy of LR on the expanded dataset with and without textual features. The numbers in parentheses are the stan-dard error over five cross-validation trials. a simple approximation to improve the scalability of this approach, which we discuss in Section 6.7.
In a real-world application, it may be difficult to obtain labeled data from the user. We therefore perform exper-iments to evaluate how many labeled examples are needed to obtain accurate results. Figures 2 and 3 plot performance as the proportion of training data used increases. As we can see, using only 10% of the data (8 examples), performance is already quickly approaching its maximum.
We perform additional experiments to measure how ro-bust the methods are to the introduction of non-canonical records. For each training example R , we introduce noise as follows: Figures 4 and 5 show results as n varies from 0 to 20. We compare the four learning methods, as well as the Most Common baseline ( C ). These figures show that the feature-based methods are quite robust to noise, as their accu-racy drops only slightly as n increases. The Most Common baseline degrades significantly, which is unsurprising since as n increases it is very likely that it chooses the incor-rect record. The exhaustive cost-learning method also ap-pears relatively robust; however the stochastic cost-learning method degrades significantly.
In Table 5 and 6 we report the wall-clock running time of each method. Note that this includes the time to train each method. The logistic regression requires about one minute to train and evaluate on 80 training examples and 20 testing examples. Note that the long running times of the cost-learning methods is high because for each setting of costs, the average edit-distance for each record must be recomputed to calculate the loss function. This is in contrast to the feature-based methods, which uses fixed costs for the edit-distance features.

For databases containing many records with many dupli-cates, the computation of the average edit-distance may be-come burdensome. The edit distance computation has time complexity O ( m 2 ) where | R | = m . A ( R i ) requires iterat-ing over all records, and we must compute this m times. Since the edit distance canonicalizer is used as input to the feature-based canonicalizers, these have time complex-ity  X ( m 2 ). Thus, canonicalizing m records requires  X ( m 2 k time.
Figure 2: Learning curves for expanded dataset. Figure 3: Learning curves for acronym dataset.

We can alleviate the quadratic dependence on m by prun-ing elements of R i that are unlikely to be chosen as the canonical record. We propose the following method to prune records for the feature-based canonicalizers:
This method therefore prunes records from consideration prior to computing the edit-distance. We leave empirical evaluation of this approximation for future work.
Figure 4: Noise experiments for expanded dataset.
Figure 5: Noise experiments for acronym dataset.
Record canonicalization is an important and under-studied problem in databases populated with heterogeneous, imper-fect data. In this paper, we have formalized the canoni-calization problem and proposed three broad classes of so-lutions. We have implemented three instantiations of one solution class and empirically evaluated them on manually annotated data. These experiments show that it is possible to build a system to accurately learn canonicalization prefer-ences with only a few examples. Furthermore, this approach appears to be quite robust to the types of noise common in automatically extracted records.

In future research, we plan to explore record merging and record generation approaches to canonicalization. We also plan to explore the interaction of canonicalization with deduplication. These two tasks are inter-related, and it is likely that performing joint inference across them can im-prove performance. This work was supported in part by the Defense Advanced Research Projects Agency (DARPA), through the Depart-ment of the Interior, NBC, Acquisition Services Division, under contract #NBCHD030010, in part by U.S. Govern-ment contract #NBCH040171 through a subcontract with BBNT Solutions LLC, in part by The Central Intelligence Agency, the National Security Agency and National Sci-ence Foundation under NSF grant #IIS-0326249, in part by Microsoft Live Labs, and in part by the Defense Ad-vanced Research Projects Agency (DARPA) under contract #HR0011-06-C-0023.0 [1] M. Bilenko and R. J. Mooney. Learning to combine [2] Y. Censor and S. Zenios. Parallel optimization : [3] M. Collins. Discriminative training methods for [4] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, [5] R. Gupta and S. Sarawagi. Creating probabilistic [6] V. Levenshtein. Binary codes capable of correcting [7] D. C. Liu and J. Nocedal. On the limited memory [8] G. Mann and D. Yarowsky. Multi-field information [9] A. McCallum, K. Bellare, and F. Pereira. A [10] A. McCallum and B. Wellner. Conditional models of [11] B. Milch, B. Marthi, S. Russell, D. Sontag, D. L. Ong, [12] E. S. Ristad and P. N. Yianilos. Learning string edit [13] S. Tejada, C. A. Knoblock, and S. Minton. Learning [14] M. Wick, A. Culotta, and A. McCallum. Learning [15] J. J. Zhu and L. H. Unger. String edit analysis for
