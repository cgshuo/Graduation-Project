 Composed of several hundreds of processors, the Graphics Processing Unit (GPU) has become a very interesting plat-form for computationally demanding tasks on massive data. A special hierarchy of processors and fast memory units al-low very powerful and efficient parallelization but also de-mands novel parallel algorithms. Expectation Maximization (EM) is a widely used technique for maximum likelihood es-timation. In this paper, we propose an innovative EM clus-tering algorithm particularly suited for the GPU platform on NVIDIA X  X  Fermi architecture. The central idea of our algorithm is to allow the parallel threads exchanging their local information in an asynchronous way and thus updating their cluster representatives on demand by a technique called Asynchronous Model Updates (Async-EM). Async-EM en-ables our algorithm not only to accelerate convergence but also to reduce the overhead induced by memory bandwidth limitations and synchronization requirements. We demon-strate (1) how to reformulate the EM algorithm to be able to exchange information using Async-EM and (2) how to exploit the special memory and processor architecture of a modern GPU in order to share this information among threads in an optimal way. As a perspective Async-EM is not limited to EM but can be applied to a variety of algo-rithms.
 H.2.8 [ Database Applications ]: Data mining ; I.3.1 [ Hardware Architecture ]: Graphics processors Expectation Maximization; Graphics Processing Unit; CUDA; Fermi
Leading experts from the data mining community voted the EM algorithm to be among the top ten algorithms hav-ing most impact on data mining research [1]. The EM al-gorithm repeats two operations in a loop: The expectation operation (E) gradually assigns each sample to the clusters according to its current likelihood, and the maximization operation (M) updates the cluster representatives such that they are optimal for the currently assigned samples. Thus we can say that EM belongs to the paradigm of alternating least squares (ALS) algorithms: The E-step determines an optimal assignment provided that the cluster representatives are fixed, and the M-step determines an optimal set of clus-ter representatives under the condition that the sample-to-cluster assignment is fixed. The classical variant of the EM-algorithm (called Batch-EM) performs a complete iteration where the E-operation is executed for every sample. After this E-phase the cluster representatives are determined in a separate M-phase. An alternative algorithm called EM with incremental updates or short Incremental-EM performs the E-step for a single sample. If the sample changes its cluster assignment to a sufficiently large degree, the affected cluster representatives are immediately changed before considering the next sample. In a non-parallel environment, this modifi-cation of the algorithm does not cause much computational overhead. But since the new set of cluster representatives is usually better than the old set, this algorithm takes consid-erably fewer iterations until convergence [2].

However, in a massively parallel environment like a GPU with hundreds of processors, we have to consider that the update of a cluster representative causes an access to a cen-tralized data structure. This is undesirable for two reasons: (1) Global exchange of information is possible only through the device RAM memory which has a limited bandwidth and (2) the processes have to be synchronized when con-currently accessing such global information. This observa-tion leads us to the idea of Asynchronous Model Updates (Async-EM). Rather than updating the global cluster rep-resentatives upon every considerable membership change, the single processes should collect a certain number of these updates and update the global cluster representatives often enough to speed up the convergence but rarely enough to spare memory bandwidth and avoid synchronization over-head. We additionally exploit the hierarchical structure of processors (which are grouped into multiprocessors having more efficient access to the so-called shared memory ), and exchange more often the updates of the processes (called thread group ) that run on the same multiprocessor. In this paper, we propose a massively parallel variant of the EM algorithm suitable for GPU environments. Our main idea is to make incremental model updates using the expec-tations obtained in parallel from mini-batches of the whole data set and to merge all these updates into a global model update which is used to calculate the expectations in the next set of parallel mini-batches. To summarize the bene-fits: Clusters are modeled by the Gaussian probability density function. The conditional probability of an object x given a cluster C is provided by: with mean  X  = (  X  1 ,..., X  d ) T and d  X  d covariance matrix  X . Each cluster is defined by  X  ,  X  and the weight w which it has in the overall model. The model C consists of a number of k such clusters C 1 ,...,C k . The overall probability of a sample x to be generated by the k clusters of the model C is provided by:
The degree (probability) to with which an object x belongs to a cluster C = ( w, X ,  X ) is provided by the Bayes theorem:
In the maximization phase, the model parameters ( w, X ,  X ) of each cluster C are updated according to the formulas:
The objective function of the EM algorithm to maximize the log-likelihood of the data D with respect to the model parameters C , denoted by LL ( D, C ) is provided by:
Batch-EM performs a loop until convergence which de-termines first the cluster memberships P ( C | x ) of all objects x  X  D according to Eq. 3 (E-phase) and after that the model parameters ( w, X ,  X ) according to Eq. 4 (M-phase).
Incremental-EM performs a loop until convergence which does the following combined EM-operation for each object x  X  D : Determine the difference between the current clus-ter membership P ( i ) ( C | x ) and that in the previous iteration P ( i  X  1) ( C | x ) and if required update the model parameters ( w, X ,  X ) immediately according to Eq. 4.

The remainder of this paper is organized as follows: Sec-tion 2 summarizes the related work. Section 3 introduces the NVIDIA X  X  Fermi GPU Architecture. Section 4 elaborates our technique Async-EM and describes the GPU implemen-tation. Section 5 contains an experimental evaluation and Section 6 concludes the paper.
The algorithm Newscast EM [3] specified for the peer-to-peer scenario avoids a centralized M-step. Approximate decentralized updates improve parallelism and reduce com-munication costs. Randomly selected pairs of nodes ex-change their local parameter estimates and combine them by weighted averaging.
 In [4], the authors propose a technique for estimating GMMs for multimedia indexing in a peer-to-peer network. Each node stores a considerable amount of data. In a first step, each node estimates local parameters by running EM on its own data. The resulting models are then combined minimizing the Kullback Leibler Divergence which is imple-mented by transmitting the model parameters only.

From the area of sensor networks also some variants of distributed EM algorithms have been proposed [5 X 7]. The power supply of sensor nodes is often limited. Algorithms minimizing communication and computation costs are thus required. Most related to our work, the approaches [6, 7] exploit the fact that in EM local information can be col-lected independently and then be combined for obtaining the global parameter estimates, but with a different focus: While [6, 7] focus on reducing the communication costs in a sensor networks, we additionally focus on asynchronous model updates as a strategy to speed up convergence.
GPU clusters are organized like CPU clusters. A GPU cluster node has one or more powerful data parallel comput-ing GPUs, which are composed of hundreds of lightweight computing cores. Parallelization is possible by distributing the work over the computing cores for independent com-putation of expectation and later computing the model pa-rameter updates locally or in a centralized way similar to the P2P computing. The two major challenges we focus on are the synchronization of cores and the organization of the communication over the shared and main memory. Some approaches proposed implementations of the EM algorithm on GPUs, e.g. [8 X 10]. In all of these works, the E-Step and the M-step are separated and split into multiple kernel calls and all of them are GPU implementations of the classical Batch-EM algorithm. In our work, we have combined E-and M-steps into a single kernel call.

To summarize, we can distinguish between approaches closely following the classical EM paradigm sequentially it-erating E-and M-step until convergence and focusing on certain important goals in a distributed environment, such as reducing the communication cost in [6, 7] or adapting the algorithm to the SIMD-environment of the GPU [8 X 10]. To further reduce communication costs, the classical EM algo-rithm has been modified by approximate updates [3, 5] or merging of local models [4].

In this paper, we investigate, if and to which extent the strategy of asynchronous model updates accelerates the con-vergence of the EM algorithm. Furthermore, we propose Async-EM, an efficient algorithm exploiting the memory hi-erarchy of modern GPUs.
The Expectation phase of Async-EM is fully paralleliz-able, because each sample can be associated to each clus-ter independently. We demonstrate that the calculations in Maximization phase can be also effectively parallelized using reduction techniques. The parallel nature of Async-EM at the data-level motivates us to implement it on the NVIDIA Fermi Architecture. Detailed information about the Fermi Architecture can be found in NVIDIA X  X  web-site and in [11].
The Compute Unified Device Architecture (CUDA) has a heterogeneous execution model in which the CPU is called host, whereas the GPU is called device. The developer can use an extension of the standard programming language C, called CUDA-C, for application development. CUDA gives the developer the opportunity of utilizing the computational elements and the memory of the GPUs. CUDA has two levels of abstraction for parallel computation: Threads are organized into thread blocks and thread blocks are organized into a grid . The code that works for the whole grid is called kernel . The whole work is divided into manageable sizes by using thread blocks.
 Thread scheduling is handled by the GPU hardware. In Fermi architecture, the thread block scheduler called the Gi-gaThread engine assigns thread blocks (maximum 8 thread blocks) to a multi-threaded SIMD processor called Stream-ing Multiprocessor (SM). A simplified block diagram of the SM is shown in Figure 1. Each Fermi SIMD processor has two SIMD warp schedulers and 32 SIMD lanes called Streaming Processors (SP) or CUDA cores, 16 load-store units (LSUs) and 4 special function units (SFUs). Each floating/integer SIMD instruction can be dispatched to 16 SIMD lanes, therefore it takes 2 clock cycles per warp to complete a SIMD integer/float instruction. Fermi SM has two instruction dispatch units, which can dispatch at least 2 instructions per clock cycle, therefore the effective Instruc-tions Per Clock (IPC) is equal to 32 floating/integer oper-ations per clock per SM. Similarly, it takes 8 clock cycles to complete a special instruction per warp in a SFU. Each SIMD thread running on a SIMD processor has its own pro-gram counter and scheduled by the warp scheduler. The warp scheduler can keep track of up to 48 threads of SIMD instructions. Hence, at most 1536 CUDA threads can be scheduled on a multi-threaded SIMD processor. A SIMD instruction is executed for all CUDA threads in the same warp, therefore all threads in the same warp are implicitly synchronized. Branch instructions will be serialized, in case branch divergence occurs inside a warp. Not all threads are active during execution of divergent branches, which means idle clock cycles for some of the SIMD lanes and perfor-mance penalty. Therefore, branch divergence inside warps Figure 1: The SIMD warp scheduler has 48 inde-pendent threads of SIMD instructions with 48 PCs.
 Each Instruction Dispatch Unit can issue a SIMD in-struction to 16 SIMD Lanes or 16 Load/Store Units (LSUs). shall be avoided. We have seen that several levels and types of parallelism are possible using different levels and types of abstractions. Multiple instructions can be scheduled to dif-ferent SIMD multiprocessors using multiple data (MIMD). Multiple threads of SIMD instructions can run on SIMD pro-cessors. Independent instructions from the same warp can be scheduled to the SIMD lane pipelines which is called in-struction level parallelism. All of these different types of par-allelisms are called Single Instruction Multiple Level (SIMT) thread model by NVIDIA.

It is important to understand the Fermi memory model to effectively parallelize EM. The host can read or write to off-chip DRAM, but not to on-chip memories used only by the device. Each multi-threaded SIMD processor has its own on-chip Shared Memory /L1 Cache in total 64 KB used by all blocks assigned to the SIMD processor. The threads within a block can use shared memory together by taking care of synchronization issues. 32 SIMD lanes are fed by in total 32K four byte on-chip registers , which are not shared by the threads. Hence, if all 1536 threads are active (at 100% occu-pancy), each thread can have at most 21 registers. A thread can not have more than 63 registers (achieved at 33% occu-pancy). Other than this, a thread has its own not shared section on off-chip RAM which is called the local memory . Local memory contains the stack frame, spilling registers and arrays which do not fit into registers. The difference of this private off-chip RAM section from other RAM sec-tions is that it is cached into L1. Fermi does not have huge caches like CPUs or deep pipelines like vector architectures. Instead, it amortizes the memory latency mainly by using thread and instruction level parallelism extensively. Caches help to decrease memory demand to off chip RAM during local memory accesses or function calls and improve the ef-fective memory bandwidth in case of uncoalesced memory accesses.
The E-operation can be perfectly parallelized on GPUs as well as on any other parallel architecture by distributing the samples to different processors. However, when taking a closer look at Incremental-EM, its strict sequential philoso-phy becomes obvious: The cluster models ( w, X ,  X ) have to be updated before the cluster membership of the next ob-ject is determined. Therefore, Incremental-EM is in general not suitable for this kind of parallelization. To also have the advantage of faster convergence through more frequent model updates, our idea is to weaken the strict sequential policy and to allow parallel processes to perform their E-operations independently. But rather than waiting until the E-operations of all objects have been processed, we allow the processes to collect their private changes locally, to use them immediately, and to exchange them from time to time in an asynchronous way. When these local model updates are ex-changed, the different sets of cluster representatives have to be merged together, which we call the consolidation . The rate of the exchange and consolidation can be adapted to the bandwidth with which the processors are connected and to the conflict rate of synchronization. CUDA-based GPUs are composed of a set of streaming multiprocessors (SM) each of which consists of several processors. In analogy to this hierarchy, the threads are also grouped into thread-groups. All threads from one thread group are executed on the same SM, and can exchange data through the fast shared mem-ory (cf. Section 3). Threads of different thread groups can exchange information only through the much slower global memory. We will show how this hierarchy of processes, pro-cessors, and memory units can be exploited to share the cluster models in an optimal way. Figure 2 displays the algorithm Async-EM in Pseudocode.
First we show how the model parameters can be repre-sented in a way that facilitates consolidation. Since the vec-tor  X  = (  X  1 ,..., X  d ) T and the covariance matrix  X  = [  X  are defined by fractions, it is inefficient and numerically un-stable to directly update these parameters. It is easier to update the following parameters instead: Besides, Fermi Architecture uses a new standard implemen-tation Full IEEE 754-2008 32-bit and 64-bit precision , which improves the accuracy considerably compared to previous architectures. Our incremental method is relatively insensi-tive to numerical inaccuracies, because we do not use batch update in which numerical errors accumulate quickly with massive amounts of data. Obviously, the two original pa-rameters w = W/n and  X  i = X i /W can be easily derived from our new consolidated parameters. But also the more difficult  X  = [  X  i,j ] can be efficiently derived as we can see in the following:  X 
Since all parameters of a cluster model C = ( W,X i ,Q i ) are now defined in terms of a sum, the consolidation sim-ply reduces to summing up the different local parameter sets. The computation of the inverse and determinant of the covariance matrix  X  is facilitated by the Cholesky de-composition  X  = L  X  L T where L is a left triangular matrix. Cholesky decomposition is the least expensive method for the inversion of a symmetric and positive definite matrix. It can be empirically shown even for non-parallel versions of EM clustering [12] that doing Cholesky decomposition in an asynchronous way clearly improves the overall run-time provided that the two extremes (decomposing almost only once per iteration and decomposing almost at every update) are avoided. The performance curve yields an ex-tremely wide and stable minimum ( bathtub curve ) as long as these extremes are avoided. Therefore, our method performs Cholesky decomposition at every asynchronous update op-eration which naturally avoids the inefficient extremes. We show next how consolidation between different processors of a streaming multiprocessor and between different multipro-cessors works.
We decompose the data set into subsequences called super-chunks and each superchunk into subsequences called chunks . A chunk is processed by parallel threads in one common thread group (on the same SM). Each thread is responsible for one or more samples. First, all threads read the most current set of cluster models from the global memory to the shared memory, read the next sample to be clustered from the global memory to the registers and determine the grad-ual cluster memberships and the differences of these mem-berships to the previous iteration. After updating their local cluster models they might also process further samples.
When the chunk is finished the threads cooperatively con-solidate their cluster models by a technique called parallel sum reduction . Then the next chunk is processed, and this is repeated until no more chunk is associated to the respec-tive streaming multiprocessor. All streaming multiproces-sors have associated a set of chunks which they are working on. The cluster models are exchanged and consolidated in-side a SM after every chunk , and the fast shared memory is used for this parallel sum reduction.

After all threads inside the current thread block have pro-cessed their chunks (which form together a superchunk), a single thread from each thread block consolidates its own cluster model with the global model with atomic operations. For this consolidation, the global memory is used. Since the global memory has by a factor of 8 times lower memory bandwidth than the shared memory, the consolidation be-tween thread blocks should be executed much less frequently than the consolidation inside a thread block. Algorithm Async-EM runs on all of the CPUs in parallel
CPU-Thread 1 only part { 01-Randomly initialize global models W g cpu ,Xi g cpu ,Qi } 02-Synchronize CPU-Threads 03-counter:= 1; repeat 04-Initialize local CPU models W l cpu ,Xi l cpu ,Qi l cpu 05-Copy models to GPU memory W l gpu ,Xi l gpu ,Qi l gpu 06-Call Async-EM-Par. Kernel running on the GPU, 07-Copy local updates  X W l gpu , X Xi l gpu , X Qi l gpu 08-Synchronize CPU-Threads. 09-Increment counter.
 10-Merge local updates  X W l cpu , X Xi l cpu , X Qi l cpu from until counter = Async.EMIter.Limit ; end Algorithm Async-EM
Algorithm Async-EM-Par. GPU Kernel for each chunk  X  superchunk in parallel [CUDA Thr. Blk.] 02-Cholesky decomposition  X  = [  X  i,j ] at every 03-Compute P ( x | C ) , P ( x ) and P ( C | x ) (cf. Eq.1-3)
GPU-Thread 0 only part in each Thr. Blk. { 06-Atomic update  X W l gpu , X Xi l gpu , X Qi l gpu . } end Async-EM-Par. GPU Kernel Figure 2: Async-EM and parallel GPU Kernel Pseudocodes. Async.EMIter.Limit = 100, = 0.01, Stepsize.Cholesky = 1, spt = 4 (samples per thread) are the optimal algorithm parameters.

The parallel sum reduction, which is shown in Figure 3, is a technique in which the threads cooperatively con-solidate their cluster models automatically avoiding shared memory access conflicts. Additionally, the access pattern is selected in a way that it facilitates efficient access to the different shared memory banks (the so-called coalesced ac-cesses). The idea is that every thread knows an address where a fixed partner thread stores its local cluster models. The thread consolidates its model with that of the partner thread. In each step, the number of threads which are still alive decreases by half until only one thread is left and only one set C of k cluster models is known.
Figure 4 gives a comprehensive example and visualizes our algorithm for a data set of n = 2048 samples. The dataset is decomposed into two superchunks each of which is de-Figure 3: The Parallel Sum Reduction. Each of the thread is handling a single local parameter. Only half of the threads are active at each step. As an example, to consolidate the local statistics for 8 local parameters, 3 ( = log 2 8 ) steps are necessary. composed into 8 chunks. We consider here a grid which is composed of two thread blocks. It is possible that both thread blocks are running on the same SM consecutively or they are scheduled to different SMs by the GigaThread en-gine and run completely in parallel. On each of the thread blocks, a total of 128 threads run effectively in parallel as-signing samples to clusters and updating their local cluster models. After their completion, the 128 threads on each thread block consolidate their local updates using the par-allel sum reduction technique on Eq. 6-8 in the fast shared memory. Then, the 128 threads on each thread block are dedicated to the samples of the next chunk which are clus-tered according to the consolidated model. When all chunks of Superchunk #1 on all thread blocks have been processed, the thread blocks consolidate their local models using atomic operations to the global memory. When all superchunks have been processed in this way, we start again with Super-chunk #1. Convergence is achieved when there has been no update in both superchunks. It might also happen that convergence is achieved after the processing of Superchunk #1 (if Superchunk #2 has not caused any cluster update in the previous iteration).
 Figure 4: A Single Async-EM Iteration. All local pa-rameters fit into the shared memory. Global model parameters updated at the end of the kernel. Figure 5: Two GPUs Extension of the Single Async-EM Iteration. Each CPU thread is responsible for host-to-device (H2D), device-to-host (D2H) mem-ory transactions and kernel calls.
 We extend the Async-EM to two GPUs as shown in Figure 5. CPU Thread-1 is responsible to create the memory struc-tures shared by the two CPU threads. Each CPU Thread calls its own GPU kernel and manages its own GPU mem-ory transactions. The data samples in the Superchunk #1 (or Superchunk #2) are assigned and copied to GPU-1 (or GPU-2) and reside in the device during all of the Async-EM iterations. The master CPU thread (Thread-1) consol-idates the final global model parameters into the memory shared by the two CPU threads for the next Async-EM it-erations. Only consolidated model parameters, which have much smaller memory size compared to the superchunk data samples, are transferred to GPUs between Async-EM itera-tions.
 The local model parameters together with the Superchunk #1 (or Superchunk #2) do not fit to GPU memory for ex-tremely large amount of data samples with high number of dimensions. If this condition is detected and if there is no additional GPU is available, the superchunk is divided into smaller chunks, which can fit into the device mem-ory and transferred to the GPU part-by-part. Each smaller chunk transferred to the GPU separately and a kernel call is performed and the calculated model parameters are trans-ferred back to the CPU Thread. But, sequential execution of memory-transactions and kernel calls (H2D-Kernel-D2H) is expensive and transfer rate on the PCI-Express bus is the bottleneck. The CUDA Streams are a sequence of GPU op-erations which are executing in the issue-order and CUDA operations from different streams can run concurrently or interleaved in the GPU. In this way, memory accesses are hidden and up to 2.4 times speed-up is obtained compared to sequential execution.
 The scenario shown in Figure 4 is optimistic in the sense that the local model parameters ( W,X 1 ..D ,Q 1 ..D ) are copied to the shared memory on the device only once and reside there during all of the iterations of the Async-EM. Since the shared memory is limited in size, it is not feasible to store the whole local data on the shared memory for a large number of clusters or dimensions. If this condition is detected, the local parameters are copied from global device memory to shared device memory partially. For this purpose, a separate location for each thread block is reserved in the global device memory to store its own local parameters as shown in Figure 6. As an example, X 1 ..D (or diagonal Q 1 ..D ), which has originally 16 clusters, is not fitting to the shared memory. Therefore, in the first step, the local parameters of the first 8 clusters are cached to the shared memory. A second step is necessary to process the remaining 8 clusters. Finally, a single thread from each block consolidates the final local statistics into a single data structure with atomic operations both in the global device memory.
 Figure 6: Partial loading and updating of the local parameters in case of large number of clusters or dimensions (Q is diagonal).
Firstly, we compare convergence, modeling error and exe-cution time performances of the Batch-EM, Incremental-EM and Async-EM algorithms using synthetic data sets. We change the underlying characteristics of the data sets, e.g., model parameters and the overlapping of the clusters to ob-serve the impact on the performance values. Following this point, we compare GPU performance against the CPU per-formance of the gmdistribution.fit function of the MATLAB Statistics Toolbox on synthetic data. We demonstrate how well the performance of the Async-EM is scaling up with an additional GPU. The execution time performance of GPU reference implementations of Batch-EM are given in [8 X 10] for two types (small and big) of data sets. Therefore, we cre-ate randomly two synthetic multivariate distributions with a similar cluster constitution and compare per EM iteration execution time of the Async-EM.
After that, we run the reference implementation of [8] on our GPU to make a hardware independent comparison of Batch-EM and Async-EM algorithms. We selected [8], be-cause the source code is available and the authors demon-strated that [8] outperforms the other reference implementa-tions. Moreover, [8] can work with non-diagonal covariance matrices. But, the most important reason is that [8] is more suitable to work with massive data sets. For example, [9] uses in M-Step K  X  D  X  N matrix (K: number of clusters, D: number of dimensions and N: number of samples), which shall completely fit into GPU memory at once, which is not always feasible for the data sets used in this paper.
Finally, we report the convergence and the execution time performances of the Batch-EM and Async-EM GPU algo-rithms on two real data sets from the UCI machine learn-ing repository [13]: We use the Statlog (Shuttle) data set and the Forest Covertype data set; Covertype is one of the biggest data sets in UCI repository.

We report the results of our Async-EM implementation on one and two GPU configurations of NVidia GTX480 Fermi GPUs. The software is developed with CUDA C Toolkit ver-sion 4.1 and is running on a PC with Intel i7-920 @2.6Ghz, 12GB RAM, Win7-64bit operating system. The CUDA ex-periments and their results with corresponding kernel and compiler settings are given in section 5.4. Figure 7: The Batch-EM has a  X   X  1 and C  X  0 , which signs a super-linear convergence for non-overlapping clusters. Async-EM and Incremental-EM are converging sub-linearly where  X   X  C  X  1
Given the sequence of the target value evolution of an it-erative algorithm as X = { x 1 , x 2 , x 3 ,..., x  X  } which converges to x  X  , the convergence speed is defined as: lim i  X  X  X  ( x x ) / ( x i  X  x  X  )  X  = C where C is a positive constant, i is the iteration number and  X  is the convergence rate. An algo-rithm has a super-linear convergence if  X  = 1 and C = 0 and sub-linear convergence if  X  = 1 and C = 1. In case of 1 &lt;  X  , we have different kinds of super-linear converges, e.g. quadratic convergence if  X  = 2.

By taking the logarithm of both sides of the equality, we obtain: The convergence rate  X  and the positive constant C , in Eq.9, are calculated from the least squares fit of the like-lihood function as depicted in Figure 7. Here, the X-Axis values are determined by the equation log( x i  X  x  X  ), where x represents the likelihood value at i th iteration and x  X  the final converged likelihood value. Similarly Y-axis values are determined by the equation log ( x i +1  X  x  X  ). We observe in our experiments that the convergence of Batch-EM starts behaving sub-linearly, if we increase the overlapping of the clusters.

We analyze firstly the Log-likelihood of different EM al-gorithms for the randomly created distributions initialized exactly the same way. The Async-EM algorithm slightly outperforms Incremental-EM in terms of required number of samples to be observed before convergence. On the other hand, the Batch-EM methods requires considerably more iterations or to observe the same samples several times to converge. This observation is intuitive, because more fre-quent updates leads faster convergence, hence the Async-EM stops earlier. The Incremental-EM has an improved conver-gence rate as proposed and demonstrated in [14] compared to Batch-EM, with the guarantee of the convergence to a local optimum. The Async-EM provides additional stability by treating the mini-batches (with chunk size) as a single sample and has the guarantee of convergence similar to the Incremental-EM. The log-likelihood does not necessarily im-prove at every Async-EM step.

As the next step, we focus on how fast the desired high accuracies (or undesired low modeling-error ) for the model parameters can be reached. The EM Algorithms described in this paper are all unsupervised methods, which means that they are only targeted to optimize log-likelihood . The modeling-error metric shows quantitatively the squared er-ror of current and desired memberships of the data samples. P ( C desired | x ) can be calculated exactly for synthetic data by using the model parameters from which the input data distribution is created. P ( C real | x ) is obtained at the end of the clustering. We repeat our tests several times with differ-ent initializations and cluster constitutions. The Async-EM outperforms in majority of the cases both Incremental-EM and Batch-EM methods in terms of required number of it-erations to obtain accurate model parameters especially for clusters with much overlap. The Async-EM is more immune against being trapped in a local minima. We summarize that Async-EM combines the best of Batch-EM and Incremental-EM algorithms. The log-likelihood of Async-EM converges faster than the Incremental-EM. The estimated model pa-rameters reach accurate values in a stable manner but much faster than in Batch-EM.
The speed-up values are calculated with respect to gmdis-tribution.fit function of MATLAB Statistics Toolbox (ver.7.3), which is very efficiently coded and widely used. Figure 8 shows the speed-up of Async-EM CUDA implementation with respect to the gmdistribution.fit function for different data set sizes. The kernel time increases linearly propor-tional to number of samples as expected, because the com-putational complexity of EM is O ( NKD ) for a diagonal co-variance matrix. The speed-up obtained for 2 Mega (2 samples with K = D = 8 on a single GTX 480 GPU is ap-proximately 137 and the kernel execution time is nearly 51 msec. We observe that the Async-EM is scaling well with an additional GPU for the same problem size as plotted in Figure 9. The speed-up obtained for 2 Mega-samples with Figure 8: Speed-up of the Async-EM running on a single GPU w.r.t. MATLAB impl. of the Batch-EM on the CPU (blue Y-axis) and Kernel Execution Time plot (green Y-axis) K = D = 8 on two GPUs is approximately 272, which is nearly double of the single GPU performance. The kernel execution time is nearly half of the single GPU performance and is 26 msec.
 As the next step, we compare the execution time of our Async-EM single GPU implementation (marked by  X * X ) with other Batch-EM GPU implementations published previously. Our Async-EM implementation runs nearly 13 times faster than [9] for a relatively small data set as shown in Table 1. In addition to that, we obtain nearly 15 times speed-up for a larger data set compared to [10] as shown in Table 2. For both of these data sets, [8] performs comparable to our performance. Therefore, to make a better hardware inde-pendent comparison, we obtained the source code from the authors of [8]. We changed only the data reading module of the code and use the rest of the code as it is. We compile the code in Release mode and run it on our CPU and GPU platform with a big synthetic data set. The results are given in Table 3. We obtain nearly 720 times speed-up compared to single core CPU implementation. The speed-up com-pared to GPU implementation of the Batch-EM is nearly 2 times, which is very similar to GTX 260 result given in Table 2. Please note that, the execution times are given as per-kernel and total execution time of an algorithm depends on how many times the kernel is called. We observe that the Async-EM algorithm requires an order of magnitude less Figure 9: Two GPUs speed-up of the Async-EM w.r.t. MATLAB impl. of the Batch-EM on the CPU. The Async-EM is scaling well with an addi-tional GPU. kernel calls compared to Batch-EM GPU implementation with synthetic data sets.
 Table 1: Comparison of the Async-EM with other Batch-EM GPU implementations for small data set Table 2: Comparison of the Async-EM with other Batch-EM GPU implementations for large data set Table 3: Comparison of the Async-EM with Batch-EM CPU&amp;GPU implementations of [8] on the setup used in this paper Method, Ref Hw. N,K,D Time/Speed-up Batch EM, [8] i7-920 2 20 , 10 , 8 21 . 6 s/ 1 Batch EM, [8] GTX480 2 20 , 10 , 8 61 . 4 ms/ 352
Async-EM, * GTX480 2 20 , 10 , 8 30 . 0 ms/ 720
We use two real data sets from the UCI repository [13] to compare performance values of the MATLAB CPU Batch-EM and the GPU Async-EM implementations. We focus on the performance values: Mean negative log-likelihood per sample which is minimized in the EM, the number of the EM iterations until convergence and speed-up values obtained with a single and two GPUs. We run a preliminarily K-means clustering with 10% of the randomly selected data to initialize initial centroids. After that, the result of the K-means is used to initialize both of the EM algorithms. We obtain the statistical results given in Table 4 for the Statlog data set after 100 and for the much larger Covertype data set after 25 independent runs. The Async-EM has a slightly better log-Likelihood for Statlog and requires nearly 6 times less iterations to convergence. CPU uses the benefit of a bigger cache for a small data sets like Statlog, therefore the speed-up values are relatively lower compared to big data sets like Covertype. On the other hand, the Batch-EM has a slightly better log-Likelihood for Covertype data set. But, Batch-EM also needs nearly 4 times more iterations to convergence. The speed-up values are nearly 2.5 times higher than the smaller data set. In summary, we observe similar performance characteristics as on synthetic data.
All of the experiment results are given for the settings: 128 thread blocks with 128 threads per block,  X -maxregcout=63 X ,  X -use fast math=Yes X  compiler flags are set and L1/Shr.-Mem. is configured as (48/16) KB. Occupancy: Achieved occupancy is 35%. Theoretical occupancy is 42% limitted by registers per thread (50), which means that five Thread Covertype data sets Blocks can be assigned to each SM. Instruction Statis-tics: The issued and executed instructions are nearly the same with 0% of instruction serialization. The executed In-struction Per Clock (IPC) is equal to 1 . 21 (upper limit is 2). SM are active at least with one warp 96% of the time and workload is distributed fairly between SMs. Branch Statistics: There is no divergent branch. Branch and con-trol flow efficiencies are nearly equal to 100%, which means that our kernel will not suffer from serialized branch exe-cution. Issue Efficiency: Our kernel has at least 2 eligi-ble warps per clock cycle per SM as desired. The consecu-tive instructions do not have long execution dependency cy-cles, therefore maximum dependency IPC = 2. Achieved Flops: All floating point operations are single precision. Instruction mix is 50% ADD, 30% MUL, 15% Special and 5% Fused MUL/ADD. Achieved floating operations per sec-ond is nearly equal to 100 GFLOPS . Memory Statistics: There is nearly no-overhead in external/shared/local mem-ory operations, which means that the kernel operations are coalesced without shared memory bank conflicts and with high cache hit ratios (80% L1 hit and 50% L2 hit). Each memory request is performed in a single transaction.
We highlighted with our experiments that substantial speed-up values can be obtained with CUDA Async-EM implemen-tation over the multi-core Batch-EM CPU implementation. Moreover, state-of-the-art Batch-EM GPU implementations are outperformed by our Async-EM GPU implementation in terms of kernel execution time, required number of iterations to converge and to obtain accurate models parameters .
We applied distributed EM principles to the GPU Envi-ronment, which are widely used in P2P computing and sen-sors networks. We regarded the streaming multiprocessors (SMs) of the GPU like the nodes of a CPU cluster. Each SM is working independently on its local data which is stored in its shared memory. Similarly, the CPU nodes distributed inside a clusters are working on their own data in their local memory space. The global model parameters are updated by each SM independently on the GPU X  X  global device memory. Similarly, the nodes of a cluster update the global param-eters in the shared memory space located on a single node or on the peer node. We extended our idea by increasing the abstraction level in our architecture. We have added another layer on top and let multiple GPUs to communicate over the shared memory space (i.e. DDRAM on the main board) and assign a CPU thread to each GPU. In future work we connect multiple-GPU nodes together with an ad-ditional layer (e.g. Open MPI) on top of our abstraction layers and form a multi-GPU cluster. Furthermore, we aim at transferring these ideas to other alternating least squares algorithms, to enable e.g. high-performance non-negative matrix factorization or inference in Bayes nets. [1] X. Wu, V. Kumar, J. R. Quinlan, J. Ghosh, Q. Yang, [2] P. Liang and D. Klein,  X  X nline em for unsupervised [3] W. Kowalczyk and N. A. Vlassis,  X  X ewscast em, X  in [4] A. Nikseresht and M. Gelgon,  X  X ossip-based [5] T. Mensink, W. Zajdel, and B. Kr  X  ose,  X  X istributed em [6] R. D. Nowak,  X  X istributed em algorithms for density [7] D. Gu,  X  X istributed em algorithm for gaussian [8] A. D. Pangborn,  X  X calable data clustering using gpus. [9] N. S. L. P. Kumar, S. Satoor, and I. Buck,  X  X ast [10] A. Harp,  X  X m of gmms with gpu acceleration with [11] J. L. Hennessy and D. A. Patterson, Computer [12] C. Plant and C. B  X  ohm,  X  X nconco: interpretable [13] A. Frank and A. Asuncion,  X  X CI machine learning [14] R. Neal and G. E. Hinton,  X  X  view of the em
