 transaction database consists of a set of transactions (e.g., Table 1). A transaction is a set of items purchased by a customer at the same time. A transaction t contains an itemset X if every item in X is in t . The support for an itemset is defined as the ratio itemset whose support is no less than a certain user-specified minimum support threshold. An itemset of length k is called a k -itemset and a frequent itemset of length k a frequent k -itemset. 
In order to avoid generating large number of candidate itemsets and scanning the transaction database repeatedly to count supports for candidate itemsets, Han et al. proposed an efficient algorithm FP-Growth [4, 5], which is another innovative ap-proach of discovering frequent patterns in a transaction database. This algorithm cre-( FP-tree ) is an extended prefix-tree for storing compressed, crucial information about frequent patterns. The frequent pattern tree structure gives a good solution to store a whole transaction database. FP-Growth algorithm requires only two full I/O scans of the dataset to build a FP-tree in main memory and then recursively mines frequent patterns from this structure by building conditional FP-trees . However, this massive creation of conditional trees makes this algorithm not scalable to mine large datasets. 
In this paper, we introduce a new study for discovery of frequent patterns based on the FP-tree [4, 5]. Our approach is different from FP-Growth algorithm [4, 5] which needs to recursively generate conditional FP-trees such that a large amount of mem-ory space needs to be used. Our approach discovers frequent patterns by traversing the FP-tree without constructing any subtree, and applies merging techniques on the tree, which makes the FP-tree become smaller and smaller. By this way, our approach can dramatically condense the kernel memory space and reduce the search space without losing any frequent patterns. In this section, we introduce and describe our approach in details, and give a running example (Table 1) to illustrate our algorithm TFP (mining frequent patterns by Trav-ersing Frequent Pattern tree). TFP reduces FP-tree progressively and can be divided into three phases: The first phase is construction phase in which a FP-tree is con-structed. The second phase is frequent pattern generation phase which generates can-didate itemsets about a specific item from the constructed FP-tree or reduced FPtree and finds frequent patterns for the item. After generating frequent itemsets about the specific item from the (reduced) FP-tree, the last phase is merge phase which moves or merges subtrees in the (reduced) FP-tree. The last two phases need to be repeatedly executed until the reduced FP-tree only contains a node except the root node. 
In the first phase, our approach TFP constructs a FP-tree by scanning original trans-action database twice. The FP-tree construction method is similar to FP-Growth algo-rithm [4, 5]. The difference is that the root node of the FP-tree contains a header table. Each entry of the header table contains a frequent item and an empty link. After finish-ing the construction phase for Table 1, the constructed FP-tree is shown in Figure 1. 2.1 Frequent Pattern Generation Phase After constructing a FP-tree with a header table on the root, the next phase is to gen-erate candidate itemsets for a specific item and find frequent itemsets about this item. The first proposed method is called Candidate Generation 1 (CG1) algorithm which generates candidate itemsets for an item. CG1 traverses FP-tree to generate candidate frequent item I h with the highest support among all the frequent items, is firstly cho-sen as starting node to be traversed. This traversal will generate all the frequent item-sets about item I h . CG1 applies Depth First Search (DFS) algorithm to traverse the subtree starting from the node N h . from starting node N h to the current node N c and generates all the combinations about I and I c as candidate itemsets from this path. The count of each generated itemset is accumulated by adding the count of N c to it. date itemsets about I h can be computed and the frequent itemsets about item I h can be obtained. The algorithm for CG1 is shown in Algorithm CG1, in which CIS is used to store candidate itemsets with their support counts by the form: {Candidate Itemset: Support count}. next_child(N t ) is a function that systematically returns the child node node of the root, which contains item I h with the highest support currently. CG1 can output the set of candidate itemsets about I h and their support counts. is merged to the subtrees of the root. The merging algorithm will be described in Sec-tion 2.2. Our algorithm then chooses the next frequent item with highest support, and performs candidate generation method to traverse the subtree of the reduced FP-tree starting from the node containing this frequent item. After merging and removing the subtree rooted at the node containing the item with the highest support, our algorithm continues to choose the next frequent item with highest support until the reduced FP-tree only contains a node except the root node. 
CG1 traverses a subtree to generate candidate itemsets about a certain frequent item and compute supports directly from (reduced) FP-tree. The smaller the support of an item is, the fewer the candidates generated by traversing the subtree starting at this item is. However, there may be many combinations generated to search and com-pute their supports. In order to avoid the costly generation of a large number of com-binations, we propose another candidate generation method CG2 to generate fewer candidates for each traversal. 
CG2 also chooses the child node N h of the root, which contains frequent item I h first traversal, CG2 generates candidate 2-itemset I h I c and compute its support by ac-generated. 
For the second traversal, like CG1, CG2 keeps the passing path from N h to the cur-forming the combinations of the items between N h and N c of this path. The count of each generated itemset is accumulated by adding the count of N c to it. If the 2-itemset I
I c is not a frequent 2-itemset, the node N c can be ignored and CG2 does nothing for this node, that is, all the candidate itemsets about I h I c need not be generated. After the computed and the frequent k-itemsets about item I h can be obtained. The algorithm for CG2 is shown in Algorithm CG2, in which CIS2 is used to store candidate 2-itemsets with their support count by the form: {Candidate 2-Itemset: Support count}. CISK is used to store candidate k-itemsets (k  X  3) with their support count by the form: {Can-didate k-Itemset: Support count}. The other parameters are the same as CG1. 2.2 Merge Phase After generating frequent itemsets about an item, our approach merges the subtrees subtree rooted at N h is removed from the (reduced) FP-tree. After merging and remov-tion, we propose two merging algorithms Simple Merge (SM) and Compact Merge (CM) to merge these subtrees after finishing generating frequent itemsets for a certain item. 
After traversing the subtree rooted at N h and generating the frequent itmsets about the item contained in N h , SM directly moves the subtrees rooted at the children of node N h to the subtrees under the root. For each child node N m containing item I m of node N h , if the link of I m in header table is null, SM sets the link to point to N m . Oth-erwise, node N m is merged to node N m  X  which the link of I m in header table points to and the count of N m is added to the count of node N m  X , since the two nodes have the same item I m . The subtrees rooted at the children of node N m are moved to be the sub-trees under the node N m  X . 
Since SM moves the subtrees rooted at the children of node N h to become the sub-trees under the child nodes of the root, the reduced FP-tree is still similar to the pre-vious one. Therefore, many duplicate candidate itemsets can be generated to be counted when candidate generation algorithm is applied. We propose another merging algorithm CM to make the reduced FP-tree as small as possible. 
The same as SM, CM traverses each subtree rooted at the children of N h . For a sub-tree rooted at N m containing item I m , if the link of I m in header table is null, CM sets the link to point to N m . Otherwise, the count of N m is added to the count of node N m  X  which the link of I m points to, since the two nodes have the same item I m . CM contin-contains the same item with the visited node. If a child node of N m  X  contains the same item I c with the visited node N c , then the count of N c is added to the count of the child node and CM recursively performs the checking for the children of node N c . Other-wise, the subtree rooted at N c is moved to become a subtree under the node N m  X  and N becomes a child of N m  X . And then CM continues to traverse the other subtrees rooted at the children of N h . For our experiments, we evaluate the performance of our algorithms TFP(CG1+SM), TFP(CG1+CM), TFP(CG2+SM), TFP(CG2+CM) and FP-Growth algorithm [4, 5]. The source code of FP-Growth was downloaded from [8]. All our experiments were conducted on Intel X Pentium X 4 CPU 3.00GHz with 2.99 GHz, 1.99GB memory using C Programming Language running Microsoft windows XP environment. Syn-thetic datasets are generated from IBM Data Generator [7] with parameters as fol-lows: T-variable means average size of the transactions, I-variable means average size of the maximal potentially frequent itemsets and D-variable means the number of transactions. In the experiments, we generate two datasets: T5I2D100K, T5I2D500K, T10I4D100K and T10I4D500K to compare FP-Growth and Our algorithms. Figures 2 and 3 show the execution times for the five algorithms on dataset T5I2D100K and T10I4D100K, respectively, with minimum support from 0.1% to 1%. From Figures 2 and 3, we can see that TFP(CG1+SM) and TFP(CG1+CM) are much worse than the other algorithms since there are many combinations generated. It is costly to search and count the large number of combinations. FP-Growth need not generate candidate itemsets, but needs to recursively generate conditional FP-trees. Moreover, the nodes with the same item need to be linked together in the constructed FP-tree and conditional FP-trees. Because CG2 can generate fewer combinations to be computed for each traversal than CG1 and CM merges the subtrees much more compact than SM, TFP(CG2+CM) is much more efficient than the other algo-rithms. Therefore, the following experiments only compare the three algorithms TFP(CG2+SM), TFP(CG2+CM) and FP-Growth . Figures 4 and 5 show the execution times for the three algorithms on dataset T5I2D100K and T10I4D100K, respectively, with minimum support from 0.1% to 1%. In Figure 6, TFP(CG2+SM) slightly outperforms FP-Growth when minimum support is higher. However, FP-Growth slightly outperforms TFP(CG2+SM) as the minimum support decreases. In general, the performances of the two algorithms are similar. TFP(CG2+CM) also significantly outperforms the other two algorithms. 
The scale up analysis is shown in Figures 6 and 7. The execution times slightly in-creases as the number of transactions incr eases for the three algorithms. However, the shape of TFP(CG2+CM) in Figure 6 is more smooth and stable than the other two algorithms, when the number of transactions increases by 100K. From our experi-ments, we can conclude that TFP(CG2+CM) is the most efficient and stable among all the algorithms based on FP-tree structure. This paper studies frequent pattern mining algorithm based a frequent pattern tree and proposes four algorithms. Our approach only needs to construct a FP-tree and traverse constructing any other subtrees. Although the candidate itemsets need to be generated, there are only few candidates generated in each traversal for TFP(CG2+CM) algo-rithm. Through merging and removing subtrees and choosing the item with the high-est support, our approach can generate frequent itemsets for an item by only travers-ing a subtree. Since there is no extra trees constructed and the frequent patterns gener-TFP(CG2+CM) outperforms FP-Growth significantly. 
