 Blockmodelling is an important technique in social network analysis for discovering the latent structure in graphs. A blockmodel partitions the set of vertices in a graph into groups, where there are either many edges or few edges be-tween any two groups. For example, in the reply graph of a question and answer forum, blockmodelling can identify the group of experts by their many replies to questioners, and the group of questioners by their lack of replies among themselves but many replies from experts.

Non-negative matrix factorisation has been successfully applied to many problems, including blockmodelling. How-ever, these existing approaches can fail to discover the true latent structure when the graphs have strong background noise or are sparse, which is typical of most real graphs. In this paper, we propose a new non-negative matrix factorisa-tion approach that can discover blockmodels in sparse and noisy graphs. We use synthetic and real datasets to show that our approaches have much higher accuracy and compa-rable running times.
 H.2.8 [ Database Applications ]: Data Mining; E.1 [ Data Structures ]: Graphs and Networks; J.4 [ Computer Ap-plications ]: Social and Behavioral Sciences Blockmodel, Graphs, Non-negative Matrix Factorisation
Discovering latent community structures is an important aspect of understanding, predicting and modelling many This research was supported under Australian Research Council X  X  Discovery Projects funding scheme (project num-ber DP110102621).
Corresponding author: jeffrey.chan@unimelb.edu.au
Currently at NICTA ATP Research Laboratory, Australia Currently at Walter and Eliza Hall Institute, Australia types of graphs 1 [10][11]. A common and intuitive defini-tion of a community is a group of vertices that are densely connected among themselves and sparsely connected to ver-tices of other communities. However, there are many other graphs that have a latent structure that is different from this definition. For example, in the reply graph of a question and answer (Q&amp;A) forum, a group of experts can be identified by their many replies to questioners, and the group of ques-tioners by their lack of replies among themselves but many replies from experts [3]. If we use a community definition to decompose such a Q&amp;A forum, everyone will be incorrectly associated in the same community. This demonstrates that a more general approach to finding the inherent graph struc-ture is needed.

Blockmodelling is a powerful approach to decomposing graphs, and has been well studied in the social sciences [13]. Vertices are in the same position (group) if they have sim-ilar patterns of interactions to vertices of other positions. The Q&amp;A forum structure fits this definition. The inherent structure is revealed by the image diagram (e.g., Figure 1f), where each vertex represents a position and an edge repre-sents the aggregate interaction between positions, with in-significant interactions not displayed. The positions and the image diagram (and matrix representation of it) clearly sum-marise the overall social structure of the Q&amp;A behaviour, and together form a blockmodel . Blockmodels allow us to: (1) explore the membership of vertices and how they relate to each other, (2) understand and characterise the under-lying structure (e.g., is it a community or core-periphery structure) and (3) discover the important roles.

Blockmodelling is an NP-Hard problem [5], hence block-modelling algorithms only seek to find good local optima. Wang et al. [12] and Long et al. [9] have formulated the blockmodelling problem 2 as non-negative matrix tri-factor-isation. The problem becomes one of finding the optimal positions and image matrices that simultaneously minimise the difference between the original graph and the graph approximated by the blockmodel. Similarly, Reichardt et al. [11] have shown the similarity between the blockmod-elling problem and finding optimal spin configurations in material physics. While promising results were reported, there are three important, unresolved challenges.
 The first challenge is that many real graphs are sparse. Consider the well studied karate club network [13], which represents the associations between members after a schism
We use the terms graphs and networks interchangeably.
The authors of these works use the term  X  X ommunity X , but these works are effectively finding blockmodels. (a) Structure using [12]. (c) Structure us-ing [9].
Figure 1: Rearranged adjacency matrices of the Karate club association network (Figures 1a-1b) and word adja-cency graph (Figures 1c-1d). in the club. The known structure is a two community struc-ture (illustrated in Figure 1b, where an edge is represented by a black square and an absent edge by white space.). The blockmodel definition can represent a community struc-ture. Hence the two non-negative factorisation algorithms in [12][9] should be able to find these communities, but over 100 runs we regularly obtain the incorrect blockmodel structure illustrated in Figure 1a. The reason this occurs is because the factorisation algorithms penalise edge and non-edge mis-matches equally, where a non-edge is the absence of an edge between a vertex pair. In a sparse graph, the grouping of edges is more important than the grouping of non-edges, since performing the former is more difficult and therefore more likely that the group represents inherent structure and not structure by chance. Figure 1e shows the image matrix of the blockmodel discovered by [12]. This indicates that in the graph approximated by the blockmodel of [12], most of the vertex pairs will be predicted to have no edge between them, which is a good solution since many of the non-edges are matched, but the blockmodel of Figure 1b is in fact the more intuitive and valid one. Hence, an approach is needed that penalises the two types of mismatches differently.
The second challenge is that in some graphs the under-lying structure is obfuscated by background noise and un-expected edges. For example, consider Figures 1c and 1d, which shows a word adjacency network of common adjectives and nouns in the Charles Dickens novel  X  X avid Copperfield X  [10]. If we divide the nouns into one position and adjectives into another, the rearranged adjacency matrix looks almost uniformly random, suggesting for this dataset the adjacen-cies of the words follow structural patterns that are different from their classifications. A possible structure is illustrated in Figure 1d, which shows the adjectives and nouns that are adjacent to many of the other words that are grouped together. For this graph, the two factorisation algorithms preferred one position (although we initialise the algorithms to find two, see Figure 1c). As this example shows, it is im-portant for blockmodelling algorithms to be able to handle unexpected structure and be insensitive to noise.
 The third challenge is the scalability of the algorithms. Reichardt et al. [11] showed in their work that their ap-proach can find blockmodels for sparse graphs. However, their simulated annealing approach is slow (it did not fin-ish one run for a graph with 1000 vertices after five days). Therefore, it is important for any blockmodel algorithm to discover blockmodels in a reasonable amount of time.
In this paper, we introduce several objectives and opti-misation algorithms to tackle these challenges (we call our framework FactorBlock ). Our approach is able to discover the true blockmodel structure in a) sparse graphs; b) noisy graphs; and c) produces results of higher accuracy and com-parable running times to the state of the art. Our approach is based on placing more importance in approximating edges well when the graph is sparse. We evaluate existing and proposed algorithms using both real and sparse graphs, and show that our proposed approaches are more accurate in both synthetic and real datasets as well as having compara-ble running times to the fastest existing algorithms. Non-Negative Matrix Factorisation: Seung et al. [8] were the first to popularise non-negative matrix factorisa-tion in machine learning. Subsequent work introduced coor-dinate descent and projected gradient descent [2] to improve the optimisation obtained.

Long et al. [9] and Wang et al. [12] introduced the idea of non-negative matrix tri-factorisation for finding blockmodels for graphs, and produced different multiplicative optimisa-tion approaches. Zhang et al. [14] introduced a coordinate descent algorithm to find overlapping position blockmodels. These algorithms perform well when the dataset structure is clear, but are not able to handle sparse and noisy graphs well, as we show in Section 6.
 Blockmodelling: In [3], Chan et al. proposed an informa-tion theoretic approach of finding blockmodels in evolving graphs. Airold et al. [1] introduced a mixed membership probabilistic model, where vertices can belong to multiple positions. Karrer et al. [7] proposed a generative probabilis-tic model that takes the difference in the degree distribu-tions of vertices into account. However, their formulation is specifically targeting heavy tailed graphs, while our model is general and can easily incorporate different types of graphs.
Reichardt et al. [11] proposed a null model formulation that sums the difference between the adjacency matrix and the blockmodel approximation of it. A simulated annealing approach was proposed to optimise this. The objective of Reichardt is most similar to ours, but our formulation allows the computation of the gradient and hence we can use faster gradient descent approaches for optimisation.
In this section, we summarise the key ideas of blockmod-elling (please see [13] for more details). A graph G ( V,E ) consists of a set of vertices V and a set of edges E , E : V  X  V , which can be represened by an adjacency matrix A . We ini-tially consider unweighted, directed graphs in this paper, as the definition of blockmodels are traditionally defined in terms of this type of graphs [13].

A blockmodel is a form of dimension reduction and decom-poses a graph ( A ) into a set of vertex partitions (called po-sitions ), represented by a membership matrix C  X  [0 , 1] and an image matrix M  X  [0 , 1] k  X  k , which describes the likelihood of an edge between a vertex from one position to a vertex of another position ( k is the number of positions). Each entry in M is called a block. The blockmodel decom-position approximates A as CMC T .

As the decomposition is an approximation, we estimate the error as the sum of squared differences and the aim is to find a blockmodel ( C  X  0 and M  X  0 ) that minimises:
Equation 1 has the same form as non-negative matrix tri-factorisation [4]. In blockmodelling, there are different defi-nitions of vertex equivalence and in this paper, we solve the popular structural equivalence [13], which states that two vertices belong to the same position if they have similar sets of out and in neighbours. In terms of matrix structure, this means the densities of the entries of the image matrix are ideally close to 0 or 1.
We introduce two new constraints and weightings to ad-dress the sparsity and noise challenges.
 Reconsider Equation 1, which we rewrite as:
X where  X  10 and  X  01 are the penalty weightings for an edge mismatch ( A i,j = 1 and ( CMC T ) i,j &lt; 1) and a non-edge mismatch ( A i,j = 0 and ( CMC T ) i,j &gt; 0) respectively. In Equation 1,  X  10 =  X  01 = 1 (equal weighting).  X  10 and  X  01 determine the importance we place on avoid-ing each of the two types of errors. Real graphs tend to be sparse, hence it is much easier to match an actual non-edge with the blockmodel approximation. Reconsidering the karate club example, according to Equation 1, blockmodel A (Figure 1a) and blockmodel B (Figure 1b) have similar objective values, as the graph is sparse and therefore the non-edges are (mostly) matched by both blockmodels and hence both objective values are similar. But we know that the true blockmodel is more similar to blockmodel B, and to make that blockmodel more desirable in terms of the ob-jective, we need the matching of an existing edge ( A i,j to be given more weight than the matching of an existing non-edge ( A i,j = 0) or  X  10 &gt;  X  01 .

One possible scheme is to use the density of the graph as a background model:  X  10 = A i,j  X  m n 2 ,  X  01 = m n 2 where m is the number of edges. With this weightings, in the example  X  10 (  X  01 ) is almost 1 (0), hence the emphasis is to match the edges. The optimal configuration for this is to place most of the edges into the two diagonal blocks, which is exactly what we seek. We can rewrite Equation 1 with the changed edge mismatching weighting as: where R  X  [0 , 1] n  X  n , R i,j = m n 2 ,  X  i,j  X  V and  X  represents the element-wise multiplication operator. This weighting scheme can also be considered from the perspective of a null model, similiar to the modularity objective for community finding [10]. For example, our weighting scheme is the same as assuming a Erdos-Renyi null model [10].

In noisy graphs, it can be difficult to distinguish between the true structure of the graph and the background noise. We can differiate better by encouraging blocks to be either dense or sparse (according to the null model). If the density of the block is below what the null model predicts it should be, then we encourage the block to be sparse and hence does not attract edge assignments to it. Conversely, if the density is above what the null model predicts, then we encourage the block to be dense and attract more edges to it.

Recall that in the ideal case, the densities of the image matrix entries should either be 0 or 1 and we denote this as the ideal image matrix , M ideal  X  X  0 , 1 } k  X  k . Then we seek an M that is as close to M ideal as possible, i.e., it minimises the distance || M ideal  X  M || 2 F . M ideal is a function of M , and can
However, this is not differentiable and we cannot use faster gradient descent approaches. Hence we propose a sigmoid function to approximate it: M ideal = 1 related to the growth rate, and  X  and  X  related to when the function approaches 1. Combining Equation 3 with this constraint term and letting U = A  X  R , we aim to minimise the following objective: subject to C  X  0 and 0  X  M  X  1 . The first term in Equa-tion 4 tries to minimise the approximation of the blockmod-elling tri-factorisation, while the second, constraint term tries to find an M that is as close as possible to the ideal for the particular equivalence required.  X   X  [0 , 1] is a parameter that controls the amount of influence from the constraint.
The matrix  X  ,  X   X  [0 , 1] k  X  k , controls the boundary be-tween what is considered as a sparse or dense block. When there is no null model, i.e., U = 1 , then the default is for  X  = 0 . 5 for all its entries. When a null model is used, the edge mismatch errors  X  10 and  X  01 determine the density boundary. When we use our weighting scheme, then  X  = m n 2  X  controls the slope of the function, and after performing parameter evaluation, we found a setting of 100-1000 works well, as the sigmoid function is almost a step function and hence approximates the exact M ideal formulation well.
We have three different objectives that we evaluate in Sec-tion 6. We call Equation 1 as Euclidean , and Equation 3, 4 and, 4 with U = 1 as adjusted , constrained adjusted and constrained Euclidean respectively.
Both non-negative matrix factorisation and blockmodelling of three or more positions are NP-Hard problems [5]. Hence, we developed alternating optimisation approaches to find good M and C solutions to Equation 4, which can also be used to solve the other objectives.

We introduce three approaches for optimising C , one where we assume hard memberships (i.e., each vertex must belong to one and only one position), and a gradient descent and a coordinate descent algorithm for soft, overlapping mem-berships. In addition, we introduce two approaches, one based on gradient descent and the other on coordinate de-scent for optimising M . Unless stated otherwise, the deriva-tions for the algorithms and their gradients are available at http://people.eng.unimelb.edu.au/jeffreyc .
When the position memberships are hard, then we are solving a discrete, NP-Hard problem. The objective function needs to be recomputed and reassessed after each member-ship change. Recomputing Equation 4 is expensive (com-plexity O ( n 2 k 2 )), hence we introduce an incremental ap-proach that only updates the necessary entries.

Let the current position membership and image matrices be denoted by C ( t ) and M ( t ) . Without loss of generality, let the vertex v i be reassigned from its current position P g new position P h , and let the corresponding matrices (of size n  X  k ) to perform these atomic operations be denoted by  X  and  X  + i , h respectively.  X   X  i , g =  X  1 for ( i,g ), and 0 otherwise, and  X  + i , h = 1 for ( i,h ) and 0 otherwise. Note that C C  X  Then for vertex v i , the position that results in a minimal sum of the D ( t +1) terms in Equations 5 to 7 is its best position assignment. Computing Equations 5 to 7 has a complex-ity of O ( k ) and since we precompute D ( t ) , PI ( t ) the complexity of position evaluation is also O ( k ). PI and IP ( t +1) can be similarly updated using rules similar to Equations 5 and 6. We call this approach HardIncr . Projected Gradient Descent: A simple approach to solve Equation 4 is to use projected gradient descent [2]. The idea is to compute the gradient of C and M , then use a line search to find the appropriate step size (  X  ) that results in the minimal objective value. To ensure that both M are C are non-negative, they are projected to the non-negative quadrant. We call this approach SoftGrad .
 Coordinate Descent: Another well known optimisation approach is coordinate descent. We optimise C by optimis-ing a sequence of sub-problems based on a set of conjugate bases. It has been shown that optimising for each of the conjugate bases sequentially will converge to a stationary point. We follow [6][14] and use the unit bases of C , de-noted E  X  [0 , 1] n  X  k , where E i,j = 1 for ( i,j ), otherwise 0. Let  X  denote the step size for each conjugate basis and C ( 0 ) = C +  X  E i,j . Each sub-problem can be written as min  X  L i,j (  X  ), where L Following a similar approach taken by [14], we solve Equa-tion 8 for  X  , which results in a polynomial of order 4. Since Table 1: Summary of the algorithms and objectives. Names in italic represent our proposed algorithms.
 we use a similar approach to finding the roots as [14], please refer to [14] for details. We call this approach SoftCoord .
Optimising the image matrix involves optimising the sig-moid M ideal term. There is no closed form solution to a sigmoid function, hence we do not use the traditional mul-tiplicative method and instead use the gradient and coor-dinate descent approaches combined with a line search to optimise for M .
 Projected Gradient Descent: We apply the same pro-jected gradient descent approach as for optimising C . Please see the supplement for the gradient with respect to M . We call this method ImageGrad .
 Coordinate Descent: Similar to optimsing for C , we now solve the following problem: min where M 0 = M +  X  E i,j ( E  X  [0 , 1] k  X  k ) and M ideal this method ImageCoord .
In this section, we compare the accuracy and running times of the different objectives and algorithms. We use both real and generated datasets to evaluate our algorithms.
In Section 5, we introduced a number of objectives and po-sition and image matrix optimisation approaches. As there are many possible combinations, we selected 12 of them for reporting (see Table 1) and compared them against five existing algorithms. We divided the algorithms into two groups. The first group uses our incremental hard position approach (hardIncr) and evaluates the Euclidean objective and its variants. We also compare our proposed approaches against Reichardt X  X  simulated annealing approach (reic-H) and Long et al. [9] (RGC-H 3 ) The second group are based on soft memberships. The existing algorithms of [9], [12] and [14] are proposed for soft membership (RGC-S, ANMF-S and BNMTF-S, respectively).
We generate our synthetic datasets with the aim of eval-uating how sparsity and noise affect the running time and accuracy of the different objectives and algorithms.
Our approach is the reverse of the blockmodelling prob-lem. We first generate the memberships ( C ) and the im-age matrix ( M ). Then we generate the graph ( A ) using A = CMC T . We generate C by first generating the posi-tion sizes from a uniform distribution (note that other dis-tributions can be used). Then we determine the position membership of vertices by drawing from a hyper-geometrical distribution, where the probability of each position is its rel-ative size. We generated M such that they replicate three common graph structures: community , core periphery and hierarchy . To vary the sparsity, we keep C the same, but change the densities of the dense blocks in M . To vary the noise, we use an approach similar to [7], which adds uni-formally random background noise to M , with parameter  X  controlling the amount of noise.

We also evaluate the algorithms using six real networks 4 that have known vertex labels. These are graphs that are commonly used to evaluate blockmodelling algorithms. Their statistics are displayed in Figure 2.
Normalised mutual information (NMI) has been used to evaluate blockmodelling algorithms [9][14], hence we also use NMI as the basis of our evaluation. We use the clock run-ning times to evaluate the scalability of the algorithms. All algorithms are implemented in Matlab 2012b. Experiments were performed on an Intel Core 2 2.53GHz laptop with 4GB of memory. For existing methods that have parameters, like reic-H, we used the default parameter settings of the original implementation. Due to space constraints, our evaluation of the parmeter sensitivity of the sigmoid function and the ef-fect of graph size are available in the supplement. In this section, we present the results of our evaluation. For the synthetic datasets, we generated 5 graphs of 100 vertices per parameter setting, then ran each algorithm 10 times. For each run, we generated 10 random initialisations, obtained a blockmodel from each initialisation, and choose the blockmodel with the lowest objective value as the one to compare against. For each algorithm, we averaged the NMI and running times across the runs.
We coupled our incremental position approach with their image matrix update, which we found to be faster and more accurate then their default approach.
Available at http://www-personal.umich.edu/~mejn/ netdata/
To evaluate the effect of sparsity, we varied the density of the dense blocks from 1.0 to 0.1. We obtained similar results for the three graph types, hence we only report the results for the community structure.

Figure 2 shows the results of our evaluation. Note that each column of figures use the same legend. We divided the evaluation into three groups. The first group, Figures 2a, 2b, 2f and 2g, compare the different objectives with our proposed algorithms. It shows that coor-H-Ad and coor-H-AdCn has the highest average NMI across most of the density range. In particular, the results for algorithms that use the adjusted Euclidean objective or variants are gener-ally higher than those that only use the Euclidean objective as we decrease the density of the graphs.

All the algorithms find it difficult to find the true block-model when the density is 0.1. At that density, there are only a few edges in each diagonal block, which makes it almost like noise and hence difficult to optimise. The run-ning times (Figure 2f) of the projected gradient descent al-gorithms are generally faster than coordinate descent ones (Figure 2g), so the results suggest to use coordinate descent if accuracy is more important, but projected gradient de-scent if speed is more important.

Next, we compare the two best algorithms (coor-H-Ad and grad-H-Ad) against the existing algorithms, RGC-H [9] and reic-H [11] for hard position blockmodelling, illustrated in Figures 2c and 2h. The results show that our algorithms are clearly much more accurate than the other two, and of com-parable speed to RGC-H. Finally, we evaluate how the soft approaches perform for these datasets. We follow [9], [12] and [14] and use k = max k C i,k to determine the position of vertex v i ( k ). Figures 2d and 2i show the results. As can be seen, BNMTF-S [14] has the highest accuracy, but also the slowest running time. In addition, all the NMI results are clearly much lower than the results of the algorithms that use hardIncr, suggesting that these soft position algorithms might be inadequate for finding hard positions.
To evaluate how each of the algorithms perform as the amount of background noise increased, we vary  X  from 0 (no noise) to 0.9 (almost uniformly random edge distribution). We divide the analysis into three groups again, but due to space limitations, we only report the best results for hard positions (Figures 2e and 2j). The results for soft positions algorithms have the same trends as the sparsity results. Of the algorithms proposed, coor-H-Ad is the most accurate for most of the background noise range, although coor-H-AdCn is actually the most accurate at a noise proportion of 0.9 (see supplement). The figures again show the two best algorithms coor-H-Ad and grad-H-Ad are much more accurate than the existing algorithms over a large range of noise levels, and have comparable running times.
Our results for the real datasets are given in Table 3. Note that we stopped the evaluation of reic-H and BNMTF-S on the Political blogs dataset after 72 hours, which we believe is an unreasonable length of time to evaluate a graph with around 1500 vertices. Apart from the Baboon dataset, the results show that the adjusted Euclidean algorithms have the highest NMI while having comparable running speeds to the other hard position results. Our FactorBlock approaches have an improvement of 21.47% (Monastery dataset) or 0.3 or higher in absolute NMI for the Karate, Political blogs, Football and Political books datasets. In the Karate and Football datasets, the sigmoid constraints achieved the high-est NMI, suggesting that the constraint term makes it easier for the algorithm to avoid sub-optimal minima. In addition, these tests show that the soft position algorithms have diffi-culties finding good blockmodels, and hence are a potential area of future research.
In conclusion, we have described the important problem of blockmodelling and shown why the current state of the art factorisation methods cannot discover blockmodels accu-rately in sparse and noisy graphs. We proposed our frame-work FactorBlock, which enables new approaches based on optimising a weighted and constrained objective with gra-dient and coordinate descent methods. In our evaluation, we showed that the weighting for sparseness and the sig-moid constraint term enabled our approaches to have much higher accuracy for sparse and noisy graphs, while having comparable running time to the state of the art methods.
