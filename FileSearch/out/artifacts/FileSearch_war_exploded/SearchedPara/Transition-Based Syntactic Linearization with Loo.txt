 Word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words (White, 2004; Zhang and Clark, 2015; De Gispert et al., 2014; Bohnet et al., 2010; Filip-pova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and yields a sentence together with its dependency parse tree that conforms to input syntactic constraints. The system is flexible with respect to input constraints, performing abstract word ordering when no con-straints are given, but gives increasingly confined outputs when more POS and dependency relations are specified. It has been applied to syntactic lin-earization (Song et al., 2014) and machine transla-tion (Zhang et al., 2014).

One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search (White and Rajkumar, 2009) for de-coding. In practice, the system can take seconds to order a bag of words in order to obtain reasonable output quality. Recently, Liu et al. (2015) proposed a transition-based model to address this issue, which uses a sequence of state transitions to build the out-put. The system of Liu et al. (2015) achieves signifi-cant speed improvements without sacrificing accura-cies when working with unlabeled dependency trees. With labeled dependency trees as input constraints, however, the system of Liu et al. (2015) gives much lower accuracies compared with Zhang (2013).
While the low accuracy can be attributed to heavy pruning, we show that it can be mitigated by modi-fying the feature structure of the standard transition-based framework, which scores the output transi-tion sequence by summing the scores of each tran-sition action. Transition actions are treated as an atomic output component in each feature instance. This works effectively for most structured prediction tasks, including parsing (Zhang and Clark, 2011a). For word ordering, however, transition actions are significantly more complex and sparse compared with parsing, which limits the power of the tradi-tional feature model.

We instead break down complex actions into smaller components, merging some components into configuration features which reduces sparsity in the output action and allows flexible lookahead fea-tures to be defined according to the next action to be applied. On the other hand, this change in the feature structure prevents legitimate actions to be scored simultaneously for each configuration state, thereby reducing decoding efficiency. Experiments show that our method is slightly slower compared with Liu et al. (2015), but achieves significantly bet-ter accuracies. It gives the best results for all stan-dard benchmarks, being over thirty times faster than Zhang (2013). The new feature structures can be ap-plied to other transition-based systems also. Liu et al. (2015) uses a transition-based model for word ordering, building output sentences using a se-quence of state transitions. Instead of scoring out-put syntax trees directly, it scores the transition ac-tion sequence for structural disambiguation. Liu et al. X  X  transition system extends from transition-based parsers (Nivre and Scholz, 2004; Chen and Man-ning, 2014), where a state consists of a stack to hold partially built outputs. Transition-based parsers use a queue to maintain input word sequences. How-ever, for word ordering, the input is a set without order. Accordingly, Liu et al. uses a set to maintain the input. The transition actions are:  X  S HIFT -Word -POS, which removes Word from  X  L EFT A RC -LABEL, which removes the second  X  R IGHT A RC -LABEL, which removes the top Using the state transition system, the bag of words { John, loves, Mary } can be ordered by (S HIFT -John -NNP, S HIFT -loves -VBZ, L EFT A RC -SBJ, S HIFT -Mary -NNP, R IGHT A RC -OBJ).

Liu et al. (2015) use a discriminative perceptron model with beam search (Zhang and Clark, 2011a), designing decoding algorithms that accommodate flexible constraints. The features include word( w ), pos( p ) and dependency label( l ) information of words on the stack ( S 0 , S 1 , ... from the top). For example, the word on top of stack is S 0 w and the POS of the stack top is S 0 p . The full set of feature templates can be found in Table 2 of Liu et al. (2015), repro-duced here in Table 1. These templates are called configuration features . When instantiated, they are combined with each legal output action to score the action. Therefore, actions are atomic in feature in-stances.

Formally, given a configuration C , the score of a possible action a is calculated as: where ~  X  is the model parameter vector of the model and ~  X ( C,a ) denotes a sparse feature vector that con-sists of features with configuration and action com-ponents i.e ~  X ( C,a ) is sparse. ~  X  has to be loaded for each a .

For efficiency considerations and following transition-based models, Liu et al. (2015) scores all possible actions given a configuration simultane-ously. This is effectively the same as formulating the score into Here A is the full set of actions and ~  X ( C ) is fixed, and ~  X  a for all a can be loaded simultaneously. In a hash-based parameter model, it significantly im-proves the time efficiency. 3.1 Two limitations of the baseline model There are two major limitations in the feature struc-ture of Liu et al. (2015). First, the S HIFT actions, which consist of the word to shift and its POS, are highly sparse. Since the action is combined with all configuration features, there will be no active feature for disambiguating the shift actions for OOV words. This issue does not exist in transition-based parsers because words are not a part of their transition ac-tions. Second, input constraints are not leveraged by the feature model. Although the dependency rela-tions of the word to shift can be given as inputs, they are used only as constraints to the decoder, but not as features to guide the shift action. Such lookahead information on the to-be-shifted word can be highly useful for disambiguation.

For example, consider the bag of words { John , loves , Mary } . Without constraints, both  X  John loves Mary  X  and  X  Mary loves John  X  are valid word order-ing results. However, given the constraint ( John , SBJ, loves ), the correct answer is reduced to the former. The first action to build the two examples are (S HIFT -John -NNP) and (S HIFT -Mary -NNP), re-spectively. According to Liu et al. X  X  feature model, there is no feature to disambiguate the first S HIFT action if both  X  John  X  and  X  Mary  X  are OOV words. The system has to maintain both hypotheses and rely on the search algorithm to disambiguate them after the dependency arcs ( John , SBJ, loves ) and ( Mary , OBJ, loves ) are built. However, given the syntac-tic constraint that  X  John  X  is the subject, the disam-biguation can be done right when performing the first S HIFT action. This requires the dependency arc label to be extracted for the word to shift e.g.( John, Mary ), which is a lookahead feature. In addition, the OOV word  X  John  X  must be excluded from the feature instance, which implies that the S HIFT -John -NNP action must be simplified.
As a second example, information about depen-dents can also be useful for disambiguating S HIFT actions. In the above case, the fact that the subject has not been shifted onto the stack can be a useful indicator for not shifting the verb  X  loves  X  onto the stack in the beginning. Inspired by the above, we exploit a range of lookahead features from syntactic constraints. 3.2 New feature structure for SHIFT actions We modify the feature structure of Liu et al. (2015) by breaking down the S HIFT -Word -POS action into three components, namely S HIFT , Word and POS , using only the action type S HIFT as the output ac-tion component in feature instances, while combin-ing Word and POS with other configuration features to form a set of lookahead features.
 For example, consider the configuration feature S w , which captures the word on the top of the stack. Under the feature structure of Liu et al., it is combined with each possible action to form features for scoring the action. As a result, for scoring the action S HIFT -Lw -Lp , S 0 w is instantiated into S 0 w -S HIFT -Lw -Lp , where Lw is the word to shift and Lp is its POS. Under our new feature structure, the action component is reduced to S HIFT only, while Lw and Lp should be used in lookahead features. Now a effectively equivalent configuration feature to Liu et al. X  X  S 0 w is S 0 w -Lw -Lp , with the looka-head Lw and Lp . It gives S 0 w -Lw -Lp -S HIFT when combined with the action S HIFT .

This new feature structure reformulates the S HIFT action features only. The L EFT A RC / R IGHT A RC ac-tions remain L EFT A RC / R IGHT A RC -LABEL since they are not sparse. Note that the change is in the action features rather than the actions themselves. Given the bag of words { John , loves , Mary } , the ac-tion S HIFT -John -NNP is still different from the ac-tion S HIFT -Mary -NNP. However, the action compo-nent of the features becomes S HIFT only, and the words John / Mary must be used as lookahead con-figuration features for their disambiguation.
The new feature structure can reduce feature spar-sity by allowing lookahead features without word information. For example, a configuration feature S w -Lp , which contains only the stack top word and the POS of the lookahead word, can still fire even if the word to shift is OOV, thereby disambiguating OOV words of different POS. In addition, the looka-head Lw and Lp do not have to be combined with every other configuration feature, as with Liu et al. (2015), thereby allowing more flexible feature com-bination and a leaner model. 3.3 The new features The new feature structure includes two types of fea-tures. The first is the same feature set as Liu et al. (2015), but with the S HIFT action component not having Word and POS information. We call this type of features as base features . The second is a set of lookahead features , which are shown in Table 2. Here L cls represents set of arc labels on child nodes (of the word L to shift) that have been shifted on to the stack, L clns represents set of labels on child nodes that have not been shifted, L sls the label set of shifted sibling nodes, L slns the label set of un-shifted sibling nodes, L cps the POS set of shifted child nodes, L cpns the POS set of unshifted child nodes, L sps the POS set of shifted sibling nodes and L spns the POS set of unshifted sibling nodes. L ps is a binary feature indicating if the parent has been shifted. L lp represents label on the parent, L pp POS of parent and L wp the parent word form. We define similar lookahead features for S 0 . These features are instantiated only for S HIFT actions.

The new feature structure prevents all possible ac-tions from being scored simultaneously, because the lookahead Word and POS are now in configuration features, rather than output actions, making it neces-sary to score the shifting of different words or POS separately. This leads to reduced search speed. Nev-ertheless, our experiments show that they give a de-sirable tradeoff between efficiency and accuracy.
Note that the new features are much less than a full Cartesian product of lookahead features and the original features. This is a result of manual feature engineering, which allows similar accuracies to be achieved using a much smaller model, thereby in-creasing the time efficiency. Table 4: Final results. W09  X  Wann et al. (2009), Z11  X  Zhang and Clark (2011b) Following previous work we conduct experiments on the Penn TreeBank (PTB), using Wall Street Journal sections 2-21 for training, 22 for develop-ment and 23 for testing. Gold-standard dependency trees are derived from bracketed sentences using Penn2Malt, and base noun phrases are treated as a single word. The BLEU score is used to evaluate the performance of linearization.

Table 4 shows a difference in scores between transition-based linearization system of Liu et al. (2015) (L15) and best-first system of Zhang (2013) (Z13). L15 performs better for word ordering with unlabeled dependency arcs, but poorly for the task of labeled syntactic linearization.

Table 3 shows a series of development experi-ments comparing our system with Z13 and L15. We vary the amount of input syntactic constraints by randomly sampling from POS and dependency labels of the development set. Our system gives consistently higher accuracies when compared with both Z13 and L15. Compared to L15, the increase in scores for unconstrained word ordering is due to the introduction of reduced feature sparsity. The im-provements on tree linearization tasks involving par-tial to full dependency constraints are also due to lookahead features that leverage tree information to reduce ambiguity early. Though slower than L15, our system is over 30 times faster compared to Z13.
We compare final test scores with previous meth-ods in the literature in Table 4. Our system im-proves upon the previous best scores by 8.7 BLEU points for the task of unlabeled syntactic lineariza-tion. For the task of labeled syntactic linearization, we achieve the score of 91.8 BLEU points, the high-est results reported so far.
 Table 5 contains examples of fully constrained output . In the first example  X  will  X  is the ROOT node with two child nodes  X  also  X  and  X  compete  X . Looka-head feature for child dependency labels L cls ,L clns on the node  X  will  X  can help order the segment  X  also will compete  X  correctly in our system. Without such features, the system of L15 yields an output that starts with  X  The spinoff with Fujitsu  X  which is locally fluent, but leaving the words  X  also  X  and  X  will  X  diffi-cult to handle. In the second example,  X  Dr. Talcott  X  is OOV. Hence system of L15 is not able to score it and thus order it correctly. Our system makes use of both POS and dependency label of  X  Dr. Talcott  X  to order it correctly. We identified a feature sparsity issue in state-of-the-art transition-based word ordering, proposing a so-lution by redefining the feature structure and intro-ducing lookahead features. The new method gives the best accuracies on a set of benchmarks, which show that transition-based methods are a fast and accurate choice for syntactic linearization. Future work include the testing of this model in a lineariza-tion shared task (Belz et al., 2011) and investigating the integration of large scale training data (Zhang et al., 2012; Liu and Zhang, 2015).

We release our source code under GPL at https://github.com/SUTDNLP/ZGen/ releases/tag/v0.2 .
 We thank Yijia Liu for helpful discussions and for sharing the Latex templates and the anonymous re-viewers for their constructive comments. This work was supported by the Singapore Ministry of Educa-tion (MOE) AcRF Tier 2 grant T2MOE201301.
