 ZHIYUAN LIU, YUZHOU ZHANG, and EDWARD Y. CHANG , Google Inc.
 Latent Dirichlet Allocation (LDA) was first proposed by Blei et al. [2003] to model documents. Each document is modeled as a mixture of K latent topics, where each topic, k , is a multinomial distribution  X  k over a W -word vocabulary. For any document d , its topic mixture  X  j is a probability distribution drawn from a Dirichlet prior with parameter  X  . For each i th word x ij in d j , a topic z ij = k is drawn from  X  j ,and x ij is drawn from  X  k . The generative process for LDA is thus given by where Dir (  X  ) denotes the Dirichlet distribution. The graphical model for LDA is illus-trated in Figure 1, where the observed variables, that is, words x ij and hyper parame-ters  X  and  X  , are shaded.

Using Gibbs sampling to learn LDA, the computation complexity is K multiplied by the total number of word occurrences in the training corpus. Prior work has explored two main parallelization approaches for speeding up LDA: (1) paralleliz-ing on loosely coupled distributed computers, and (2) parallelizing on tightly-coupled multicore CPUs or GPUs (Graphics Processing Units). Representative loosely coupled distributed algorithms are Dirichlet Compound Multinomial LDA (DCM-LDA) [Mimno and McCallum 2007], Approximate Distributed LDA (AD-LDA) [Newman et al. 2007], and Asynchronous Distributed LDA (AS-LDA) [Asuncion et al. 2008], which perform Gibbs sampling on computers that do not share memory. This distributed approach may suffer high inter-computer communication cost, which limits achievable speedup. The tightly coupled approach uses multicore CPUs or GPUs with shared memory (e.g., the work of Yan et al. [2009]). Such a shared-memory approach reduces interprocess communication time. However, once the processors and memory have been configured, the architecture is inflexible when faced with changes in computation demands and the need to schedule simultaneous tasks with mixed resource requirements. (We discuss related work in greater detail in Section 2.)
In this work, we improve the scalability of the distributed approach by reducing intercomputer communication time. Our algorithm, which we name PLDA+, employs four interdependent strategies. (1) Data placement . Data placement aims to separate CPU-bound tasks and (2) Pipeline processing . To ensure that a CPU-bound processor is not blocked by com-(3) Word bundling . In order to ensure that communication time can be effectively (4) Priority-based scheduling . Data placement and word bundling are static allocation
The preceding four strategies must work together to improve speedup. For instance, without word bundling, pipeline processing is futile because of short computation units. Without distributing the metadata of word bundles, communication bottlenecks at the master processor could cap scalability. By lengthening the computation units via word bundling while shortening communication units via data placement, we can achieve more effective pipeline processing. Finally, a priority-based scheduler helps smooth out unexpected runtime imbalances in workload.

The rest of the article is organized as follows: We first present LDA and related distributed algorithms in Section 2. In Section 2.3 we present PLDA, an MPI imple-mentation of Approximate Distributed LDA (AD-LDA). In Section 3 we analyze the bottleneck of PLDA and depict PLDA+. Section 4 demonstrates that the speedup of PLDA+ on large-scale document collections significantly outperforms PLDA. Section 5 offers our concluding remarks. For the convenience of readers, we summarize the no-tations used in this article in Table I. Similar to most previous work [Griffiths and Steyvers 2004], we use symmetric Dirich-let priors in LDA for simplicity. Given the observed words x , the task of inference for LDA is to compute the posterior distribution of the latent topic assignments z , the topic mixtures of documents  X  ,andthetopics  X  . Griffiths and Steyvers [2004] proposed using Gibbs sampling, a Markov chain Monte Carlo (MCMC) method, to perform inference for LDA. By assuming a Dirichlet prior  X  on  X  ,  X  can be integrated (hence removed from the equation) using the Dirichlet-multinomial conjugacy. MCMC is widely used as an inference method for latent topic models, for instance, Author-Topic Model [Rosen-Zvi et al. 2010], Pachinko Allocation [Li and McCallum 2006], and Special Words with Background Model [Chemudugunta et al. 2007]. Moreover, since the memory requirement of VEM is not nearly as scalable as that of MCMC [Newman et al. 2009], most existing distributed methods for LDA use Gibbs sampling for inference, for example, DCM-LDA, AD-LDA, and AS-LDA. In this article we focus on Gibbs sampling for approximate inference. In Gibbs sampling, it is usual to integrate out the mixtures  X  and topics  X  and just sample the latent variables z . The process is called collapsing . When performing Gibbs sampling for LDA, we maintain two matrices: a word-topic count matrix C w ord in which each element C w k is the number of words w assigned to topic k , and a document-topic count matrix C doc in which each element C kj is the number of topics k assigned to document d j . Moreover, we maintain a topic count vector C topic in which each element C k is the number of topic k assignments in document collection. Given the current state of all but one variable z , the conditional probability of z ij is where  X  ij means that the corresponding word is excluded in the counts. Whenever z ij is assigned with a new topic drawn from Eq. (2), C w ord , C doc ,and C topic are updated. After enough sampling iterations to burn in the Markov chain,  X  and  X  are estimated. Various approaches have been explored for speeding up LDA. Relevant parallel meth-ods for LDA include the following.  X  X imno and McCallum [2007] proposed Dirichlet Compound Multinomial LDA (DCM-
LDA), where the datasets are distributed to processors, Gibbs sampling is performed on each processor independently without any communication between processors, and finally a global clustering of the topics is performed.  X  X ewman et al. [2007] proposed Approximate Distributed LDA (AD-LDA), where each processor performs a local Gibbs sampling iteration followed by a global update using a reduce-scatter operation. Since the Gibbs sampling on each processor is performed with the local word-topic matrix, which is only updated at the end of each iteration, this method is called approximate distributed LDA.  X  X n Asuncion et al. [2008], a purely asynchronous distributed LDA was proposed, where no global synchronization step like in Newman et al. [2007] is required. Each processor performs a local Gibbs sampling step followed by a step of communicating with other random processors. In this article we label this method as AS-LDA.  X  X an et al. [2009] proposed parallel algorithms of Gibbs sampling and VEM for LDA on GPUs. A GPU has massively built-in parallel processors with shared memory. Besides these parallelization techniques, the following optimizations can reduce LDA model learning computation cost.  X  X omes et al. [2008] presented an enhancement of the VEM algorithm using a bounded amount of memory.  X  X orteous et al. [2008] proposed a method to accelerate the computation of Eq. (2).
The acceleration is achieved by no approximations but using the property that the topic probability vectors for document d j ,  X  j , are sparse in most cases. We previously implemented PLDA [Wang et al. 2009], an MPI implementation of AD-LDA [Newman et al. 2007]. PLDA has been successfully applied in real-world applica-tions such as communication recommendation [Chen et al. 2009]. AD-LDA distributes D training documents over P processors, with D p = D / P documents on each pro-only on processor p . The document-topic count matrix, C doc , is likewise distributed and we represent the processor-specific document-topic count matrices as C doc | p .Each processor maintains its own copy of the word-topic count matrix, C w ord . Moreover, we use C w ord | p to temporarily store word-topic counts accumulated from local documents X  topic assignments on each processor. In each Gibbs sampling iteration, each processor p updates z | p by sampling every z ij | p  X  z | p from the approximate posterior distribution each processor recomputes word-topic counts for its local documents C w ord | p and uses an AllReduce operation to reduce and broadcast the new C w ord to all processors. One can refer to Wang et al. [2009] for the MPI implementation details of AD-LDA.

We have also implemented AD-LDA on MapReduce [Dean and Ghemawat 2004; Chu et al. 2006] as reported in Wang et al. [2009]. Using MapReduce, many operations can be carried out by combining three basic phases: mapping, shuffling, and reducing. We used MapReduce to implement AllReduce . However, before and after each iteration of the MapReduce-based AD-LDA, a disk IO is required to fetch and update the word-topic matrix at the master processor. In addition, local data must also be written onto disks. The benefit of forcing IOs between iterations is tolerating faults. However, using MPI, a fault recovery scheme can be more efficiently implemented via lazy IOs after the completion of each iteration. The primary reason for conducing IOs is because MapReduce cannot ensure two consecutive iterations of sampling the same set of data being scheduled on the same processor. Thus, documents and metadata (document-topic counts) must be fetched into memory at the beginning of each iteration even in the absence of a fault. Certainly, these shortcomings of MapReduce can be improved. But MPI seemed to be a more attractive choice at the time when this research was conducted. To further speed up LDA, the PLDA+ algorithm employs four interdependent strategies to reduce inter-computer communication cost: data placement, pipeline processing, word bundling, and priority-based scheduling. As presented in the previous section, in PLDA, D documents are distributed over P processors with approximately D / P documents on each processor. This is shown with a D / P -by-W matrix in Figure 2(a), where W indicates the vocabulary of document collection. The word-topic count matrix is also distributed, with each processor keeping a local copy, which is the W -by-K matrix in Figure 2(a).

In PLDA, after each iteration of Gibbs sampling, local word-topic counts on each processor are globally synchronized. This synchronization process is expensive partly because a large amount of data is sent and partly because the synchronization starts only when the slowest processor has completed its work. To avoid unnecessary delays, AS-LDA [Asuncion et al. 2008] does not perform global synchronization like PLDA. In AS-LDA a processor only synchronizes word-topic counts with another finished processor. However, since word-topic counts can be outdated, the sampling process can take a larger number of iterations than that PLDA does to converge. Figure 3(a) and Figure 3(b) illustrate the spread patterns of the updated topic distribution for a word from one processor to the others for PLDA and AS-LDA. PLDA has to synchronize all word updates after a full Gibbs sampling iteration, whereas AS-LDA performs updates only with a small subset of processors. The memory requirements for both PLDA and AS-LDA are O( KW ), since the whole word-topic matrix is maintained on all processors.
Although they apply different strategies for model combination, existing distributed methods share two characteristics.  X  X he methods have to maintain all word-topic counts in memory for each processor.  X  X he methods have to send and receive the whole word-topic matrix between proces-sors for updates.
 For the former characteristic, suppose we want to estimate a  X  with W words and K topics from a large-scale dataset. When either W or K is large to a certain extent, the memory requirement will exceed that available on a typical processor. For the latter characteristic, the communication bottleneck caps the potential for speeding up the algorithm. A study of high-performance computing [Graham et al. 2005] shows that floating-point instructions historically improve at 59% per year, but inter-processor bandwidth improves 26% per year, and inter-processor latency reduces only 15% per year. The communication bottleneck will only exacerbate over additional years. Let us first introduce pipeline-based Gibbs sampling. The pipeline technique has been used in many applications to enhance throughput, such as the instruction pipeline in modern CPUs [Shen and Lipasti 2005] and in graphics processors [Blinn 1991]. Al-though pipeline does not decrease the time for a job to be processed, it can efficiently improve throughput by overlapping communication with computation. Figure 4 illus-trates the pipeline-based Gibbs sampling for four words, w 1 ,w 2 ,w 3 ,and w 4 . Figure 4(a) demonstrates the case when t s  X  t f + t u , and Figure 4(b) the case when t s &lt; t f + t u , where t s , t f ,and t u denote the time for Gibbs sampling, fetching the topic distribution, and updating the topic distribution, respectively.
 In Figure 4(a), PLDA+ begins by fetching the topic distribution for w 1 . Then it begins Gibbs sampling on w 1 , and at the same time, it fetches the topic distribution for w 2 . After it has finished Gibbs sampling for w 1 , PLDA+ updates the topic distribution for w 1 on P w processors. When t s  X  t f + t u , PLDA+ can begin Gibbs sampling on w 2 immediately after it has completed sampling for w 1 . The total ideal time for PLDA+ to process W words will be Wt s + t f + t u . Figure 4(b) shows a suboptimal scenario where the communication time cannot be entirely masked. PLDA+ is not able to begin Gibbs sampling for w 3 until w 2 has been updated and w 3 fetched. The example shows that in order to successfully mask communication, we must schedule tasks to ensure as much as possible that t s  X  t f + t u .

To make the pipeline strategy effective or t s  X  t f + t u , PLDA+ divides processors into two types: one maintains documents and the document-topic count matrix to perform Gibbs sampling ( P d processors), while the other stores and maintains the word-topic count matrix ( P w processors). The structure is shown in Figure 2(b). During each iteration of Gibbs sampling, a P d processor assigns a new topic to a word in a typical three-stage process. (1) Fetch the word X  X  topic distribution from a P w processor. (2) Perform Gibbs sampling and assign a new topic to the word. (3) Update the P w processors maintaining the word.

The corresponding spread pattern for PLDA+ is illustrated in Figure 3(c), which avoids both the global synchronization of PLDA and the large number of iterations required by AS-LDA for convergence.

One key property that PLDA+ takes advantage of is that each round of Gibbs sam-pling can be performed in any word order. Since LDA models a document as a bag of words and ignores word order, we can perform Gibbs sampling according to any word order as if we reordered words in bags. When a word that occurs multiple times in the documents of a P d processor, all instances of that word can be processed together. Moreover, for words that occur infrequently, we bundle them with words that occur more frequently to ensure that t s is sufficiently long. In fact, if we know t f + t u , we can decide how many word occurrences to process in each Gibbs sampling batch to ensure that t s  X  ( t f + t u ) is minimized.

To perform Gibbs sampling word by word, PLDA+ builds word indexes to documents on each P d processor. We then organize words in a circular queue as shown in Figure 5. Gibbs sampling is performed by going around the circular queue. To avoid concurrent access to the same words, we schedule different processors to begin at different posi-tions of the queue. For example, Figure 5 shows four P d processors, P d 0 , P d 1 , P d 2 ,and P d 3 start their first word from scheduling algorithm works, PLDA+ must also distribute the word-topic matrix in a circular fashion on P w processors. This static allocation scheme enjoys two benefits. First, the workload among P w processors can be relatively balanced. Second, avoiding two P d nodes from concurrently updating the same word can roughly maintain serial-izability of the word-topic matrix on P w nodes. Please note that the distributed scheme of PLDA+ ensures stronger serializability than PLDA because a P d node of PLDA+ can obtain the word-topic matrix updates of other P d nodes in the same Gibbs sampling iteration. The detailed description of word placement are presented in Section 3.3.1.
Although word placement can be performed in an optimal way, scheduling must deal with runtime dynamics. First, some processors may run faster than others, and this may build up bottlenecks at some of the P w processors. Second, when multiple requests are pending, the scheduler must be able to set priorities based on request deadlines. The details of PLDA+ X  X  priority-based scheduling scheme are described in Section 3.4.3. The task of the P w processors is to process, fetch, and update queries from P d processors. PLDA+ distributes the word-topic matrix to P w processors according to the words con-tained in the matrix. After placement, each P w processor keeps approximately W / | P w | words with their topic distributions. 3.3.1. Word Placement over P w Processors. The goal of word placement is to ensure spatial load balancing. We would like to make sure that all processors receive about the same number of requests in a round of Gibbs sampling.

For bookkeeping, we maintain two data structures. First, we use m i to record the number of P d processors on which a word w i resides, which is also the weight of the word. For W words, we maintain a vector m = ( m 1 ,..., m W ). The second data structure keeps track of each P w processor X  X  workload, or the sum of weights of all words on that processor. The workload vector is denoted as l = ( l 1 ,..., l | P w | ).

A simple placement method is to place words independently and uniformly at random on P w processors. This method is referred to as random word placement . Unfortunately, this placement method may cause frequent load imbalances. To balance workload, we use the weighted round-robin method for word placement. We first sort words in descending order by their weights. We then pick the word with the largest weight from and then update the workload of p w . This placement process is repeated until all words have been placed. Weighted round-robin has been empirically shown to achieve balanced load with high probability [Berenbrink et al. 2008]. 3.3.2. Processing Requests from P d Processors. After placing words with their topic dis-tributions on P w processors, the P w processors begin to process requests from the P d by receiving initial word-topic counts from all P d processors. Then the P w processor p w begins to process requests from P requests.
  X  fetch ( w i , p w, pd ): A request for fetching the topic distribution of a word w by a P d processor pd . For each request, the P w processor p w returns the topic distribution
C w ord w | p w of the word w , which will be used as C  X  ij w k in Eq. (2) for Gibbs sampling.  X  update ( w, u , p w ): A request for updating the topic distribution of a word w using the update information u on pd . The P w processor updates the topic distribution of the word w using u .  X  fetch ( p w, pd ): A request for fetching the overall topic counts on a P w processor p w by a P d processor pd . The P w Processor p w sums up the topic distributions for all words they are summed up and used as C  X  ij k in Eq. (2) for Gibbs sampling.
 Each P w processor handles all requests related to the words it is responsible for main-taining. To ensure that requests are served in a timely manner, we employed a priority scheme sorted by request deadlines. According to its local word processing order, a P d processor needs communication completion for its fetch requests at various time units. When the P d processor sends its requests to P w processors, deadlines are set in the request header. A P w processor serves waiting requests based on their deadlines. The algorithm for P d processors executes according to the following steps. (1) At the beginning, it allocates documents over P d processors and then builds an (2) It groups the words in the vocabulary into bundles for performing Gibbs sampling (3) It schedules word bundles to minimize communication bottlenecks. (4) Finally, it performs pipeline-based Gibbs sampling iteratively until the termination
In the following, we present the four steps in detail. 3.4.1. Document Allocation and Building an Inverted Index. Before performing Gibbs sam-pling, we first have to distribute D documents to P d processors. The goal of document allocation is to achieve good CPU load balance among P d processors. PLDA may suffer imbalanced load since it has a global synchronization phase at the end of each Gibbs sampling iteration, which may force fast processors to wait for the slowest processor. In contrast, Gibbs sampling in PLDA+ is performed with no synchronization require-ment. In other words, a fast processor can start its next round of sampling without having to wait for a slow processor. However, we also do not want some processors to be substantially slow and miss too many cycles of Gibbs sampling. This will result in the similar shortcoming that AS-LDA suffers: taking a larger number of iterations to converge. Thus, we would like to allocate documents to processors in a balanced fashion. This is achieved by employing random document allocation .Each P d proces-sor gets approximate D / | P d | documents. The time complexity of this allocation step is O ( D ).

After documents have been distributed, we build an inverted index for the documents of each P d processor. Using this inverted index, each time a P d processor fetches the topic distribution of a word w , it performs Gibbs sampling for all instances of w on that processor. After sampling, the processor sends back the updated topic distribution to the corresponding P w processor. The clear benefit is that for multiple occurrences of a word on a processor, we only need to perform two communications, one fetch and one update, substantially reducing communication cost. The index structure for each word w is in which w occurs in document d 1 for 2 times and there are 2 entries. In implementation, 3.4.2. Word Bundle. Bundling words is to prevent the duration of Gibbs samplings from being too short to mask communication. Use an extreme example: a word takes place only once on a processor. Performing Gibbs sampling on that word takes a much shorter time than the time required to fetch and update the topic distribution of that word. The remedy is intuitive: combining a few words into a bundle so that the communication time can be masked by the longer duration of Gibbs sampling time. The trick here is that we have to make sure the target P w processor is the same for all words in a bundle so that each time only one communication IO is required for fetching topic distributions for all words in a bundle.
 For a P d processor, we start bundling words according to their target P w processors. For all words with the same target P w processor, we first sort them in descending order of occurrence times and build a word list. We then iteratively pick a high-frequency word from the head of the list and several low-frequency words from the tail of the list and group them into a word bundle. After building word bundles, each time we will send a request to fetch topic distributions for all words in a bundle. For exam-ple, when learning topics from NIPS dataset consisting of 12-year NIPS papers, we combine { curve, collapse, compiler, conjunctive, ... } as a bundle, in which curve is a high-frequency word and the rest are low-frequency words in this dataset. 3.4.3. Building the Request Scheduler. It is crucial to design an effective scheduler to determine the next word bundle to send requests for topic distributions during Gibbs sampling. We employ a simple pseudorandom scheduling scheme.

In this scheme, words in the vocabulary are stored in a circular queue. During Gibbs sampling, words are selected from this queue in a clockwise or counterclockwise order. Each P d processor enters this circular queue with a different offset to avoid concurrent access to the same P w processor. The starting point of each P d process at each Gibbs sampling iteration is different. This randomness avoids forming the same bottlenecks from one iteration to another. Since circular scheduling is a static scheduling scheme, a bottleneck can still be formed at some P w processors when multiple requests arrive at the same time. Consequently, some P d processors may need to wait for a response before Gibbs sampling can start. We remedy this shortcoming by registering a deadline for each request, as described in Section 3.3.2. Requests on a P w processor are processed according to their deadlines. A request will be discarded if its deadline has been missed. Due to the stochastic nature of Gibbs sampling, occasionally missing a round of Gibbs sampling does not affect overall performance. Our pseudorandom scheduling policy ensures the probability of same words being skipped repeatedly is negligibly low. 3.4.4. Pipeline-Based Gibbs Sampling. Finally, we perform pipeline-based Gibbs sam-pling. As shown in Eq. (2), to compute and assign a new topic for a given word x ij = w document d j is maintained by a P d processor. While the up-to-date topic distribution C w is maintained by a P w processor, the global topic count C topic should be collected over all P w processors. Therefore, before assigning a new topic for a word w in a docu-ment, a P d processor has to request C w ord w and C topic from P w processors. After fetching C w and C topic ,the P d processor computes and assigns new topics for occurrences of the word w . Then the P d processor returns the updated topic distribution for the word w to the responsible P w processor.

For a P d processor pd , the pipeline scheme is performed according to the following steps. (1) Fetch overall topic counts for Gibbs sampling. (2) Select F word bundles and put them in the thread pool tp to fetch topic distributions (3) For each word in Q pd , pick its topic distribution to perform Gibbs sampling. (4) After Gibbs sampling, put the updated topic distributions in the thread pool tp to (5) Select a new word bundle and put it in tp . (6) If the update condition is met, fetch new overall topic counts. (7) If the termination condition is not met, go to step (3) to start Gibbs sampling for
In step (1), pd fetches the overall topic distributions C topic . In this step, pd just sends the requests fetch ( p w, pd ) to each P w processor. The requests are returned with C summing overall topic counts from each P w processor, C topic = p w C topic | p w .
Since the thread pool tp can send requests and process the returned results in paral-lel, in step (2) it puts a number of requests to fetch topic distributions simultaneously in case some requests are delayed. Since the requests are sent at the same time, they are assigned with the same deadline. Once a response is returned, it will start Gibbs sampling immediately. Here, we mention the number of prefetch requests as F .In PLDA+, F should be properly set to make sure the waiting queue Q pd always has re-turned topic distributions of words waiting for Gibbs sampling. If not, it will stop to wait for the incoming member of Q pd , which is a part of the communication time cost of PLDA+. To make best use of threads in the thread pool, F should be larger than the number of threads in the pool.

It is expensive for P w processors to process the request for overall topic counts be-cause the operation has to access the topic distributions for each word on each P w processor. Fortunately, as indicated by the results of AD-LDA [Newman et al. 2009], topic assignments in Gibbs sampling are not sensitive to the values of the overall topic counts. We thus reduce the frequency of fetching overall topic counts to improve the efficiency of P w processors. Therefore, in step (6), we do not fetch overall topic counts frequently. In experiments, we will show that, by fetching new overall topic counts only after performing one pass of Gibbs sampling for all words, PLDA+ can obtain the same learning quality as LDA and PLDA.

The pipeline scheme is depicted in Figure 6, where the process of fetching C topic is not shown for simplicity. 3.4.5. Fault Tolerance. In PLDA+, we provide a fault-recovery solution similar to PLDA. We perform checkpointing only for z | pd on P d processors. This is because: (1) on the P d P w side, C w ord p w can also be recovered from z | pd . The recovery code is at the beginning of PLDA+: if there is a checkpoint on the disk, load it; otherwise perform random initialization. In this section, we analyze parameters that may influence the performance of PLDA+. We also analyze the complexity of PLDA+ and compare it with PLDA.
 3.5.1. Parameters. Given the total number of processors P , the first parameter is the proportion of the number of P w processors to P d processors,  X  =| P w | / | P d | . The larger the value of  X  , the more the average time for Gibbs sampling on P d processors will increase as fewer processors are used to perform CPU-bound tasks. At the same time, the average time for communication will decrease since more processors serve as P w to process requests. We have to balance the number of P w and P d processors to (1) minimize both computation and communication time, and (2) ensure that communication time is short enough to be masked by computation time. This parameter can be determined once we know the average time for Gibbs sampling and communication of the word-topic matrix. Suppose the total time for Gibbs sampling of the whole dataset is T s , the communication time for transferring the topic distributions of all words from one processor to another processor is T t .For P d processors, the sampling time will be T / | P and thus transfer time will be T t / | P w | . To make sure the sampling process is able to overlap the fetching and updating process, we have to make sure Suppose T s = W  X  t s where  X  t s is the average sampling time for all instances of a word, word, we get where  X  t f ,  X  t u ,and  X  t s can be obtained by performing PLDA+ on a small dataset and then empirically set an appropriate  X  value. Under the computing environment for our experiments, we empirically set  X  = 0 . 6.

The second parameter is the number of threads in the thread pool R , which caps the number of parallel requests. Since the thread pool is used to prevent sampling from being blocked by busy P w processors, R is determined by the network environment. R can be empirically tuned during Gibbs sampling. That is, when the waiting time for the prior iteration is long, the thread pool size is increased.

The third parameter is the number of requests F for prefetching the topic distribu-tions before performing Gibbs sampling on P d processors. This parameter depends on R , and in experiments we set F = 2 R .

The last parameter is the maximum interval inter max for fetching the overall topic counts from all P w processors during Gibbs sampling of P d processors. This parameter influences the quality of PLDA+. In experiments, we can achieve LDA models with similar quality to PLDA and LDA by setting inter max = W .

It should be noted that the optimal values of the parameters of PLDA+ are highly related to the distributed environment, including network bandwidth and processor speed. 3.5.2. Complexity. Table II summarizes the complexity of P d processors and P w proces-sors in both time and space. For comparison, we also list the complexity of LDA and PLDA in this table. We assume P =| P w |+| P d | when comparing PLDA+ with PLDA. In this table, I indicates the iteration number for Gibbs sampling, and c is a constant that converts bandwidth to flops.

The preprocessing of LDA distributes documents to P processors with time complex-ity D / | P | . Compared to PLDA, the preprocessing of PLDA+ requires three additional operations including (1) building an inverted document file for all documents on each P d words according to their frequencies, and (3) sending topic counts from P d processors to P w processors to initialize the word-topic matrix on P w with time O ( WK / | P w | ). In practice LDA is set to run with hundreds of iterations, and thus the preprocessing time for PLDA+ is insignificant compared to the training time.
 Finally, let us consider the speedup efficiency of PLDA+. Suppose  X  =| P w | / | P d | for PLDA+, without considering preprocessing, the ideal achievable speedup is where S denotes the running time for LDA on a single processor, S / P is the ideal time cost using P processors, and S / | P d | is the ideal time achieved by PLDA+ with communication completely masked by Gibbs sampling. We compared the performance of PLDA+ with PLDA (AD-LDA based) through empir-ical study. Our study focused on comparing both training quality and scalability. Since the speedups of AS-LDA are just  X  X ompetitive X  to those reported for AD-LDA as shown in Asuncion et al. [2008, 2010], we chose not to compare with AS-LDA. We used the three datasets shown in Table III. The NIPS dataset consists of scientific articles from NIPS conferences. The NIPS dataset is relatively small, and we used it to investigate the influence of missed deadlines on training quality. Two Wikipedia datasets were collected from English Wikipedia articles using the March 2008 snapshot from en.wikipedia.org . By setting the size of the vocabulary to 20 , 000 and 200 , 000, re-spectively, the two Wikipedia datasets are named Wiki-20T and Wiki-200T. Compared to Wiki-20T, more infrequent words are added in vocabulary in Wiki-200T. However, even for those words ranked around 200 , 000, they have occurred in more than 24 articles in Wikipedia, which is sufficient to learn and infer their topics using LDA. These two large datasets were used for testing the scalability of PLDA+. In experi-ments, we implemented PLDA+ using a synchronous Remote Procedure Call (RPC) mechanism. The experiments were run on a distributed computing environment with 2 , 048 processors, each with a 2 GHz CPU, 3GB of memory, and a disk allocation of 100GB. Similar to Newman et al. [2007], we use test set perplexity to measure the quality of LDA models learned by various distributed methods of LDA. Perplexity is a common way of evaluating language models in natural language processing, computed as where x test denotes the test set, and N test is the size of the test set. A lower perplexity value indicates a better quality. For every test document in the test set, we randomly designated half the words for fold-in, and the remaining words were used for testing. The document mixture  X  j was learned using the fold-in part, and the log probability of the test words was computed using this mixture. This arrangement ensures that the test words were not used in estimating model parameters. The perplexity computation follows the standard method in Griffiths and Steyvers [2004], which averages over multiple chains when making predictions using LDA models learned by Gibbs sam-pling. Using perplexity on the NIPS dataset, we find the quality and convergence rate of PLDA+ are comparable to single-processor LDA and PLDA. Since the conclusion is straightforward and similar to Newman et al. [2007], we do not present the evaluation results on perplexity in detail.
 As described in Section 3.4.3, PLDA+ discards a request when its deadline is missed. Here we investigate the impact of missed deadlines on training quality using the NIPS dataset. We define missing ratio  X  as the average number of missed requests divided by the total number of requests, which ranges [0 . 0 , 1 . 0). By randomly dropping  X  requests in each iteration, we simulated discarding different amounts of requests in each iteration. We compared the quality of learned topic models under different  X  values. In experiments we set P = 50. Figure 7 shows the perplexities with different  X  values versus the number of sampling iterations when K = 10. When the missing ratio is less than 60%, the perplexities remain reasonable. At interaction 400, the perplexities of  X   X  X  between 20% and 60% are about the same, whereas no deadline misses can achieve a 2% better perplexity. Qualitatively, a 2% perplexity drop does not show any discernible degradation in training results. Figure 8 shows the perplexities of converged topic models with various numbers of topics versus different  X  settings, at the end of iteration 400. A larger K setting suffers more severe perplexity degradation. Nevertheless,  X  = 60% seems to be a limiting threshold that PLDA+ can endure. In reality, our experiments indicate that the missing ratio is typically lower than 1%, far from the limiting threshold. Though the missing ratio depends highly on the workload and the computation environment, the result of this experiment is encouraging that PLDA+ can operate well even when  X  is high.
 The primary motivation for developing distributed algorithms for LDA is to achieve a good speedup. In this section, we report the speedup of PLDA+ compared to PLDA. We used Wiki-20T and Wiki-200T for speedup experiments. By setting the number of topics K = 1 , 000, we ran PLDA+ and PLDA on Wiki-20T using P = 64 , 128 , 256 , 512 and 1 , 024 processors, and on Wiki-200T using P = 64 , 128 , 256 , 512 , 1 , 024 and 2 , 048 set to  X  = 0 . 6 according to the unit sampling time and transfer time. The number of threads in a thread pool was set as R = 50, which was determined based on the experiment results. As analyzed in Section 3.5.2, the ideal speedup efficiency of PLDA+
Figure 9 compares speedup performance on Wiki-20T. The speedup was computed rel-ative to the time per iteration when using P = 64 processors, because it was impossible to run the algorithms on a smaller number of processors due to memory limitations. We assumed the speedup on P = 64 to be 64, and then extrapolated on that basis. From the figure, we observe that when P increases, PLDA+ simply achieves much better speedup than PLDA, thanks to the much reduced communication bottleneck of PLDA+. Figure 10 compares the ratio of communication time over computation time on Wiki-20T. When P = 1 , 024, the communication time of PLDA is 13 . 38 seconds, which is about the same as its computation time, much longer than that of PLDA+ X  X  3 . 68 seconds.

From the results, we conclude that: (1) When the number of processors grows large enough (e.g., P = 512), PLDA+ begins to achieve better speedup than PLDA; (2) in fact, if we take the waiting time for synchronization in PLDA into consideration, the speedup of PLDA could have been even worse. For example, in a busy distributed computing environment, when P = 128, PLDA may take about 70 seconds for communication in which only about 10 seconds are used for transmitting word-topic matrices and most of the time is used to wait for computation to complete.

On the larger Wiki-200T dataset, as shown in Figure 11, the speedup of PLDA starts to flatten out at P = 512, whereas PLDA+ continues to gain in speed. 1 For this dataset, we also list the sampling and communication time ratio of PLDA and PLDA+ in Figure 12. PLDA+ keeps the communication time to consistently low values from P = 64 to P = 2 , 048. When P = 2 , 048, PLDA+ took only about 20 minutes to finish 100 iterations while PLDA took about 160 minutes. Though eventually Amdahl X  X  law would kick in to cap speedup, it is evident that the reduced overhead of PLDA+ permits it to achieve much better speedup for training on large-scale datasets using more processors.

The preceding comparison did not take preprocessing into consideration because the preprocessing time of PLDA+ is insignificant compared to the training time as analyzed in Section 3.5.2. For example, the preprocessing time for the experiment setting of P = 2 , 048 on Wiki-200T is 35 seconds. For training, hundreds of iterations are required, with each iteration taking about 13 seconds. In this article, we presented PLDA+, which employs data placement, pipeline process-ing, word bundling, and priority-based scheduling strategies to substantially reduce inter-computer communication time. Extensive experiments on large-scale datasets demonstrated that PLDA+ can achieve much better speedup than previous attempts on a distributed environment.

