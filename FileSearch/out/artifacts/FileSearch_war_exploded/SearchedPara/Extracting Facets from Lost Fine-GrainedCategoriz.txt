 The dataspace abstraction describes data integr ation architectures that deal with large heterogeneous data, which are partially unstructured, possibly sparse and characterized by high dimensionality [6]. Differently from traditional data inte-gration architectures, where data from lo cal sources are consistently integrated in a global view after their schemas are aligned, pay-as-you-go data integration is needed in dataspaces: data are more an d more integrated along time, as more effective data access features are required [11].

Data integration methodologies inspired by dataspace principles are widely adopted for industry-scale Web data integration because of the amount and het-erogeneity of source instances to be integrated [5]. Several examples of Web data integration systems can be found in the eCommerce domain. Price Comparison Engines (PCEs) integrate a very large number of heterogeneous product offers (i.e., dataspace instances) from many different e-marketplaces providing search and browsing features over the integrated information. Through PCE front-ends, end-users compare different eMarketplaces product offerings in terms of price and/or product features. Many PCEs such as Google Shopping 1 , PriceGrab-ber 2 and Amazon 3 have been developed, which differ in terms of coverage and effectiveness of search and browsing features.

Category-based and facet-based browsing are two exampl es of data access fea-tures that many PCEs aim to deliver to their users. These features require the creation and maintenance of categorizations respectively based on taxonomies , i.e., hierarchies of product categories such as  X  X obile Phones X  or  X  X ines X , and facets , i.e., sets of mutually exclusive coordinate terms that belong to a same concept(e.g., X  X rape:Barolo,...,Cabernet X ,..., X  X ype:RedWine,...,White Wine X ) [19,21]. A global taxonomy is used to annotate all the instances in the dataspace with a coarse-grained categorization, which helps end-users to rapidly recall the  X  X amily X  of instances they ar e interested in. Facets can be used within a specific (global) taxonomy category, to annotate instances in the dataspace with a fine-grained categorization, which helps end-user to rapidly recall in-stances with specific characteristics (e.g.,  X  X rape: Barolo X ,  X  X ype: Red Wine X ). Facet-based categorizations can be also u seful in relation to Search Engine Opti-mization because facets indexed by sea rch engines can bring more traffic to the PCEs X  websites.

Unfortunately, facet creation and maint enance is an extremely time and effort consuming task in PCEs, which is left to manual work of domain experts. The definition of meaningful facets at large scale requires a deep understanding of the salient characteristics of dataspace instances (e.g., wines are characterized by grape, type, provenance, and so on) for a large number of diverse product categories. As a result, while a global taxonomy is used in most of PCEs 4 ,many PCEs provide only few generic facets (e.g.,  X  X rice Range X  and  X  X erchant X ) and others provide a richer set of facets but only for a limited amount of popular product categories.

In this paper we propose an approach to automatic facet extraction in datas-paces, which is aimed to support domain experts in creating and maintaining significant facets associated with global categories. Our approach leverages the information already present in the dataspace, namely (i) taxonomies used to classify instances in the data sources and (ii) mappings established from source taxonomies to the global taxonomy of the dataspace, to suggest meaningful facets for a given global category. Unlike the global taxonomy, which has to cover instances from very diverse domains, source taxonomies are often special-ized in certain domains (e. g.,  X  X ines X ). Domain experts map specific categories in source taxonomies (e.g.,  X  X arolo X ) to generic categories in the global tax-onomy (e.g.,  X  X ines X ). The idea behind our approach consists in reusing the fine-grained categories that occur in sev eral source taxono mies mapped to the global taxonomy (e.g.,  X  X arolo X , X  X abernet X ), to extract a set of relevant facets for a given global category (e.g.,  X  X rape: Barolo, ..., Cabernet X , ...,  X  X ype: Red Wine, ..., White Wine X ).

Our approach incorporates an automatic facet extraction algorithm that consists of three steps: extraction of potential facet values (e.g.,  X  X abernet X ); clustering of facet values into sets of mutually exclusive terms (e.g.,  X  X or-deaux X ,  X  X abernet X ,  X  X hianti X ); labeling of clusters with meaningful labels (e.g.,  X  X rape X ). The algorithm is based on structural analysis of source taxonomies and on Taxonomy Layer Distance , a novel metric introduced to evaluate the distance between mutually exclusive cate gories in different taxonomies. Experi-ments conducted to evaluate the approach show that our algorithm is able to ex-tract meaningful facets that can then be refined by domain experts. In addition, since our approach extracts facets from source categorizations, the annotation of the dataspace instances with the extracted facets is straightforward, supporting facet-based browsing.

The paper is organized as follows. The problem of facet extraction is defined and explained in Section 2. Our approach to facet extraction is described in Section 3 and the evaluation is discusse d in Section 4. Related work is presented in Section 5. Section 6 draws conclusions and discusses future work. A facet can be defined as  X  X  clearly defined, mutu ally exclusive, and collectively exhaustive aspect, property, or characteristic of a class or specific subject X  [19]. As input to our problem, we assume that there exists a global taxonomy used in the dataspace and a set of mappings from leaf categories in source taxonomies to leaf global categories. We assume that the mappings have many-to-one car-dinality, i.e., many categories in each source taxonomy are mapped one global category. Observe that mappings of this kind can be easily extracted in any dataspace where instances are categorized using one source taxonomy category and one global taxonomy category.

In the following paragraph we summarize the terminology used in the rest of the paper, along with a precise description of the problem of Facet Extraction. Source Taxonomy: a source taxonomy consists of a partially ordered set S of Global Taxonomy: a global taxonomy consist of a partially ordered set G of Leaf-to-leaf Category Mapping: a leaf-to-leaf (leaf for brevity) category Facet: a facet F g for a global category g is a finite set of values v 1 , ..., v n (e.g., Facet Extraction Problem: given a global leaf category g , a set of mappings As introduced in Section 1, the Facet Ex traction problem is common in PCEs. The dataspace of a PCE consists of offers (i.e., the dataspace instances) coming from many eMarketplaces (i.e., the data sources). The conceptual architecture of a PCE is sketched in Figure 1. Each eM arketplace categorizes offers using a own source taxonomy. Source instances are integrated within the dataspace by specifying mappings from a large population of (often domain specific) source taxonomies to a global taxonomy. Mappings are defined and maintained by do-main experts with the aid of (semi) automatic algorithms. To size the problem, we provide some figures about the dataspace of one of the most popular PCE on the Italian marke t. TrovaPrezzi 5 integrates many times per day 7 millions prod-uct offers from about 3900 eMarketplaces. Over more than 10 years of activity, more than 1 million of leaf mappings have been specified from source categories to more than 500 global categories. The semi-automatic approach to facet extraction proposed in this paper is sketched in Figure 2 and is aimed to support domain experts who are in charge of maintaining classifications and mappings within a dataspace. Domain experts trigger the facet extraction process for a specified global category using a Web interface. An automatic facet extraction algorithm suggests a set of facets to domain experts, who inspect, validate and refine the result of the automatic extraction algorithm, deciding which facets will be part of the dataspace.
The automatic facet extraction algorithm at the core of the proposed approach is inspired by the following principle: the specialized taxo nomies used in data sources contain information that can be a nalyzed to extract a set of significant facets for a global taxonomy category. The facet extraction algorithm extracts the set of facets F g for a global category g using a three-phase process: 1. Value Extraction: A set of normalized facet values is produced by case 2. Value Clustering: Facet values are clustered t ogether into facets according 3. Facet Labeling: Facets are labelled using external knowledge sources.
The third phase of the facet extraction process is aimed to suggest labels to domain experts X  who can accept the sugge stion or change the label. We assume that all the taxonomies are lexicalized in a same language. However, our approach does not depend on a particular language: frequency and structural-based princi-ples are used to select and cluster facet valu es; state-of-the-art Natural Language Processing techniques available in nearly any language are used for facet value normalization; external knowledge sources like the one we used for facet labeling are now available in several languages.
 3.1 Value Extraction During this phase we identify the set of facet values that are frequently used for categorization at the sources. In order to identify such values for a global category g we rely on existing mappings to g . For each source taxonomy S we form the set N g S of values occurring as names of source categories mapped to g and all their ancestors in the respectiv e source taxonomy. The level of detail of source taxonomies can be different in each source taxonomy, thus ancestors X  names are included in N g S to consider every possible significant value. The set N g S for a global category g and a source taxonomy S is defined as N g S = { s | X  g  X  sor  X  g  X  s ,withs  X  Sands is a descendant of s } .

The set V g S of normalized values is obtained by applying case lowerization, special characters removal and stemming to N g S . As far as stemming is concerned, we use Hunspell Stemmer 6 to normalize values X  terms with respect to their sin-gular form. Hunspell stemmer is based on language dependent stemming rules that are available for most of languages. Normalized values are then unioned together to form the set V g of facet values for a global category g . In this phase, duplicated values are removed. The set V g of unique values for a global category g over all the n source taxonomies is V g = n i =1 V g S i .
 After normalization and unioning, a simple ranking function is applied to V g . Unique values are ranked according to their frequency over the all sets V
S i . Intuitively, the more a value occurs as source category name mapped to the global category g , the higher rank it will get. Based on this ranking we reduce V g to the set V g k of the top k frequent values. The rationale behind this choice is to keep only those values that are more commonly used across many independent and heterogeneous sources and thus are likely to be more relevant for the fine-grained classification of dataspace instances. The set V g k of the top frequent values produced by this phase represents the input for the next phase. In addition, we keep track of the (possibly) many source categories to which each value v  X  V g k correspond. In this way, the annotation of the dataspace instances with facet values extracted by the algorithm is straightforward.
 Example. Given the two taxonomies A and B in Figure 2, for the global category  X  X ines X , N wines A = {  X  X everages X ,  X  X ines X ,  X  X uscany X ,  X  X hianti X ,  X  X icily X ,  X  X ero d X  X vola X ,  X  X ermentino X  } and N wines B = {  X  X oot X ,  X  X ood And Drinks X ,  X  X ines X ,  X  X hite Wines X ,  X  X erdicchio X ,  X  X ed Wines X ,  X  X abernet X ,  X  X ombardy X ,  X  X icily X ,  X  X hianti X  } . After normalization, values from N wines A and N
B form the set of unique values V And Drink X ,  X  X ine X ,  X  X ombardy X ,  X  X abernet X ,  X  X uscany X ,  X  X hianti X ,  X  X icily X ,  X  X ermentino X ,  X  X ero d X  X vola X ,  X  X hite Wine X ,  X  X erdicchio X ,  X  X ed Wine X  } . 3.2 Value Clustering Values in V g k are clustered to form the set of facets F g .Weaimatcluster-ing together all values that are more likely to be coordinate values of a same characteristic. As an example, suppose that we get V wines k = {  X  X abernet X ,  X  X hi-anti X ,  X  X ombardy X ,  X  X icily X  } . An ideal clustering should be F g 1 = {  X  X abernet X ,  X  X hianti X  } and F g 2 = {  X  X ombardy X ,  X  X icily X  } because each facet refers to a same characteristic (i.e., the wine X  X  grape variety and the origin Italian region). In order to discover the set F g of facets over V g k we make use of the DB-SCAN density-based clustering algorithm [4]. DBSCAN clusters together values within a maximum distance threshold and satisfying a cluster density crite-rion and discards as noise values that are distant from any resulting cluster. DBSCAN algorithm requires in input the minimum cardinality of expected clus-ters minPoints and the maximum distance threshold .Weset minPoints to 2 and empirically find the best value for (see Section 4). Finally, DBSCAN does not require a number of expected clusters as input. We use DBSCAN for several reasons. We deal with heterogeneous taxonomies, thus we cannot make any assumption about the shape of clusters (i.e. facets) and we must employ clustering techniques that incorporate the notion of noise. Otherwise, clustering algorithms requiring the expected number of clusters as input are not suitable (e.g. KMeans) since the number of facets to detect is not known in advance.
In order to use the DBSCAN clustering algorithm it is crucial to provide an effective distance metric between th e values. We propose a distance metric that considers near those values that refer to a same characteristic of instances, according to a taxonomic structural criterion. We now formally define the pro-posed distance metric, starting from the principle that it aims at capturing: source categories mutual exclusivity.
 Source Category Mutual Exclusivity Principle. We recall from Section 2 that a facet is  X  X  clearly defined, mutually exclusive, and collectively exhaustive aspect, property, or characteristic of a class or specific subject X  [19]. The Source Category Mutual Exclusivity principle (SCME) states that the more two values refer to mutually exclusive categories, the more they should be grouped together into the same facet. Given two source categories s 1 and s 2 , their occurrence as siblings indicates that s 1 and s 2 are mutually exclusive (e.g.,  X  X ombardy X  and  X  X icily X  in Figure 3). SCME is a structural principle: it takes source taxonomies structure into account by considering reciprocal relationships among categories. Taxonomy Layer Distance. We propose a distance metric that captures the SCME principle by considering sibling relationships between source categories, or more generally, the co-occurrence of categories on a same taxonomy layer. Given a taxonomy S ,ataxonomy layer l S of S is the set of all categories that are at the same distance from the taxonomy root. For example, the set {  X  X ombardy X ,  X  X uscany X ,  X  X icily X  } is a layer for taxonomy A in Figure 3. At large scale, categories occurring on same taxonomy layers are likely to be mutually exclusive since they usually represent partitions of the set of entities categorized under the considered taxonomy. Considering co -occurrences on the same taxonomy layer represents a good way to capture the SCME principle: the more two values v 1 and v 2 co-occur at the same layer across all source taxonomies, the more they should be clustered together and thus the less they are distant from each other.
We compute the Taxonomy Layer Distance (TLD) between two values v 1 and v 2 by counting their co-occurrences on the same taxonomy layer and scaling it by their nominal occurrences across all source taxonomies. Computing TLD is equivalent to computing the Jaccard Di stance between the two sets of taxonomy layers where two values v 1 and v 2 occur, respectively. Given a value v and a source taxonomy S we define the set L S v of layers containing v in S as L S v = { l S | v  X  l S } (a category can occur in more than one layer). The overall set L v of layers containing v is computed by unioning all layers across all n source taxonomies as L v = n i =1 L S i v . Then we compute the Jaccard Distance between L v 1 and L v 2 : Example. Given the two source taxonomies from Figure 3 and values  X  X abernet X  and  X  X hianti X , we first compute layers containing  X  X abernet X  and  X  X hianti X : l l l The set of layers containing  X  X abernet X  and  X  X hianti X  are: L L Then, we compute the distance between  X  X hianti X  and  X  X abernet X  as: TLD( X  X abernet X ,  X  X hianti X ) = 1  X  3.3 Facet Labeling During the labeling phase a semantically meaningful label is attached to each facet F g discovered in the value clustering phase. Ideally, each label should shortly describe the characteristic to which the values of a facet are likely to refer. For example, if we consider two facets F g 1 = {  X  X abernet X ,  X  X hianti X  } and F g 2 = {  X  X taly X ,  X  X rance X  } , meaningful labels can be  X  X rape Variety X  and  X  X ountry X , respectively. Conceptually, labeling each facet means responding to the following question:  X  X o which concept of reality do values of facet F g refer? X . In order to answer this question, we reconcile each facet value to entities from the Freebase 7 multilingual knowledge base. Given a facet F g , we submit each facet value of F g as a keyword query to the Freebase Search Web service 8 . This API performs keyword search over Freebase entities and returns a list of entities ranked by relevance. We select the entity type of the top k ranked entities returned by the keyword search API. We pick as label the most frequent type returned by the Freebase Search API for all facet values in F g . The core idea of our proposed approach to facet extraction is that we group facet values according to a structural criterion (i.e., TLD). We focus on evaluating the facet value clustering phase. Our goal is to show that TLD effectively captures the SCME principle and supports domain experts in facets definition. To the best of our knowledge there are no distance metrics for taxonomies that explicitly aim at capturing the SCME principle. However, structural similarity metrics that consider path distance between categories within a taxonomy are good candidates to compare our work to. Intuitively, the more two source categories co-occur in the same source taxonomy path (i.e. they are similar to some degree according to structural similarity metrics) from the root to a leaf, the less they are mutually exclusive and the more they should be clustered into different facets (i.e., the clustering algorithm should consider them distant from each other).
We compare TLD with two known struct ural concept similarity metrics, namely Leacock and Chodorow [9] (LC) and Wu and Palmer [22] (WP) metrics. Both LC and WP achieve high effectiveness results in determining the similarity of concepts within the WordNet taxonomy [16]. LC measures the similarity be-tween two taxonomy categories by considering the shortest path between them and scaling it by the depth of the taxonomy. Similarly, WP measures the sim-ilarity between two categories by considering the distance from their nearest common ancestor and the distance of the nearest common ancestor from the taxonomy root. We adapted LC and WP to the case of multiple taxonomies. More specifically, given two values v 1 and v 2 we evaluate their LC and WP sim-ilarities for each source taxonomy where v 1 and v 2 co-occur and we take the mean similarity as the final distance value. 4.1 Gold Standard We created a gold standard from the real w orld TrovaPrezzi It alian PCE datas-pace. We chose ten TrovaPrezzi global ca tegories and ran the Values Extraction phase over them. We presented the set of top k frequent values to TrovaPrezzi domain experts, who found that relevant facet values generally appear among the top 100 ranked values. Thus we choose k = 100 as cardinality of the set of extracted facet values. Facet values were manually grouped together by a do-main expert from TrovaPrezzi mapping te am and facets were then validated by other domain experts in order to ensure t heir correctness. As we expected, some of the values were discarded by domain experts as they could be added to any existing facet.

Gold Standards X  global categories cover different domains and a relevant por-tion of the overall dataspace of the PCE, that is 688 source taxonomies and 22594 leaf mappings. For each source taxonomy an average of about 33 mappings have been specified. Moreover, for 322 source taxonomies mappings to more than one global category have been specified. Notice that all the data upon which we created the gold standard are lexicalized in Italian. Our approach to facet extraction is language independent, thus results that we present in following sections are comparable to others obtained considering different languages. For sake of clarity, we provide examples translated to English. 4.2 Evaluation Metrics We evaluate our facet extraction approac h from two different perspectives: facet value effectiveness and value clusterin g effectiveness. This kind of evaluation campaign has been previous ly used to evaluate several facet extraction algo-rithms [8,3]. We introduce the notation we will use in the rest of the section. Given a global category g , we denote with V g the set of discovered facet values (i.e. values that have not been classified as noise by the algorithm). We denote with V g  X  the set of gold standard facet values (i.e., values not classified as noise by domain experts). Lastly, we denote with F g  X  the set of manually discovered facets (i.e., the gold standard for the global category g ), which is compared to the set F g of automatically discovered facets.
 Value Effectiveness. In our proposed approach noisy values are discarded. In order to evaluate the ability of our technique to filter noisy values out we compare sets V g and V g  X  , using Precision ( P ), Recall ( R ) and F-Measure ( F 1 ). All these metrics do not take clusteri ng effectiveness into account.
 Value Clustering Effectiveness. We evaluate clustering effectiveness using several standard clustering quality metrics, that are Purity ( P  X  ), Normalized Mutual Information ( NMI  X  ), Entropy ( E  X  ), and F-Measure for clustering ( F  X  ). One remark about the usage of these evaluation metrics is that the set of facet values clustered by our approach is different f rom the set of facet values grouped by humans (i.e., V g = V g  X  ). We may fail in including meaningful values into some clusters, or we may mistakenly include noisy values into some facets. Clustering quality metrics cannot handle these cases. Thus, we modify facets in F g by (1) removing all noisy values and by (2) adding to F g as single value facets all gold standard values that have been automatically classified as noise. These adjustment ensures that V g = V g  X  and thus clustering quality metrics can be used properly. With this adjustment, facet value effectiveness is not considered. Overall Quality. In order to evaluate the overall effectiveness of our approach, we aggregate facet value precision P , facet value recall R and clustering F-measure F  X  into an overall quality measure. The PRF  X  measure combines P , R and F  X  by means of an armonic mean: 4.3 Experimental Results We conducted several experiments, comparing clustering performance of TLD, LC and WP metrics. We recall from Sect ion 3.2 that the DBSCAN algorithm used for clustering is configured with a maximum distance threshold .Optimal values of depend on the used distance metric, and influence clustering perfor-mance. The tuning of can be driven by two orthogonal factors: overall quality (i.e., PRF  X  ) and the number of discovered clusters. High values of (i.e., quality oriented configuration) can lead to better overall quality, but fewer discovered clusters (i.e., the clustering algorithm will tend to group values into one single cluster). Lower values of (i.e., cluster number oriented configuration) can lead to lower quality, but more discovered clusters. We found that quality oriented and cluster number oriented configuratio ns generally coincide except for LC. In the following section we refer to quality oriented configuration of LC as LC q while we indicate with LC n the corresponding cluster number oriented configu-ration. Since optimal configurations for WP and TLD coincide we omit pedices for them. Moreover, due to space limitation we include only the mean value of metrics computed across all gold standard categories.

Table 1 presents results of our experiments. TLD is more effective in finding relevant facet values and discarding noisy ones, as indicated by an higher F 1 . The ability of effectively discarding noisy values substantially reduces domain experts X  effort in validat ing discovered facets. LC q and WP obtain almost per-fect value recall, but substantially lower p recision. Thus, they do not effectively support domain experts. Moreover, TLD a chieves best performance according to quite all clustering effectiveness met rics, with the exceptions of purity and entropy for LC n . Clusters discovered by LC n contain more homogeneous values, in the sense that they have been manually classified as belonging to the same gold standard group. However, LC n achieves better purity and entropy at the cost of discarding most of the values as noise, thus sacrificing overall quality.
The difference between TLD and state-of -the-art metrics is even more evident if we consider the number of detected cl usters for each gold standard category g (Table 2). WP and LC q fail in properly partitioning the overall set V g k of facet values, thus failing in detecting groups (i.e. they detect only one or two clusters). They are too inclusive and thus they group facet values at a granularity level that is too high to be suitable for effectively supporting domain experts in bootstrapping a faceted classification system within the dataspace. From the other side, LC n discards too much values to be effective.

In addition to standard evaluation metrics, we provide a more intuitive insight of results of the facet extraction proce ss, using TLD for facet value clustering compare to state-of-the-ar t metrics. Table 3 depicts an example of facets dis-covered for the global category  X  X ines X  by TLD, WP, LC q ,andLC n compared to manually defined ones. Validating and refining groups discovered by TLD re-quires much less domain experts X  effort than LC q ,LC n and WP, thus sensibly reducing the cost of bootstrapping faceted classification.

Table 3 highlights a difficulty of TLD in grouping together different lexi-calizations of same values (e.g.,  X  X ed Wine X  and  X  X ed X ). One naive approach to overcome this difficulty is to normalize source category names by removing terms belonging to the global category for which the facets are extracted (e.g., the term  X  X ine X  when extracting facets for global category  X  X ines X ). However, this naive solution cannot be generalized to every global category. For example, if we remove from the source category  X  X og Food X  all the terms belonging to the gold standard global category  X  X ogs and Cats Food X  we end up with an empty, inconsistent facet value. Moreov er, also the more conservative approach of removing global category terms only if they all occur in the source category cannot be generalized. For example, if we consider the gold standard category  X  X usical Instruments X , using the more conservative approach we will not nor-malize source categories  X  X ind Instruments X  and  X  X inds X .

We implemented and evaluated both the previously described naive solutions (we omit them due to space limitation), and found that they both decrease the effectiveness of our approach. We believe that effectively solving the problem of different lexicalizations requires Natural Language Processing language specific techniques. NLP techniques can be used t o discriminate between global category terms that refer to nouns, verbs, etc. and thus can be safely removed from source categories without creating inconsistenc ies or change category names X  semantics. Introducing this kind of NLP language specific techniques comes at the cost of sacrificing the language independence of our approach. However, this represents an interesting extension of our approach. Many different approaches to the problem of extracting facets from structured and unstructured Web resources have b een proposed (see [21] for a recent sur-vey). Facets are usually extracted from a document collection (e.g., [2,18,13,20]), from search engine query results (e.g., [23,8,3,7]) or from the combination of doc-uments and search engine query logs (e.g., [15,14,10]).

Document collection based approaches tac kle the problem of extracting faceted taxonomies across a document collectio n. Faceted taxonomies represent a hier-archy topics to which document refer to. External structured resources such as WordNet [2,18], Wikipedia [2,20] or its Linked Data version DBPedia [13] are exploited to enrich the extracted set of facets. Our approach is different from document collection based ones becaus e: (1) we extract facets and facet values in stead of a hierarchy of topics and (2) we analyze taxonomy structure in order to provide sets of mutually exclusive facet values.

The focus of query result based approaches is on classification of documents returned by a keyword query search. Fa cets for browsing results of a query are extracted from Wikipedia documents [23] analyzing, among other things, Wikipedia categories and reciprocal links between documents. In more gen-eral approaches facets are extracted by analyzing raw HTML pages in order to identify potential faceted classificat ions within them using unsupervised [3] or supervised [8] machine learning techniques. Facets are also extracted by im-ages annotated with a folksonomy [7] and external resources are exploited for value disambiguation and hyponym det ection. State of art query result based approaches deal with the specific problem of integrating and ranking heteroge-neous facets that are already present in documents. Our approach takes in input source taxonomies and mappings between them and the global taxonomy.
The focus of query logs approaches is on the usage of user query statistics to identify facet values that are useful/relevant. Query logs are analyzed in order to select relevant facet values with respect to closed, fixed [15] or open, not defined a-priori [14,10] set of facets. Query log based approaches are strictly dependent on end-user queries: they do not consider currently available dataspace instances. Our approach analyzes classifications at the sources, and thus provide a more comprehensive fine-grained dataspace instances classification.

All the previously described approaches are complementary to ours. We ex-tract facets from a different source than previous approaches: taxonomies used to categorize instances within a dataspace. We expect the f acet extraction process to benefit from the integration of state-of-the-art facet extraction techniques. Taxonomy structure analysis has b een employed in the field of Ontology Matching [17]. In this field, several similarity metrics between ontology (and also taxonomy) concepts have been proposed and/or adapted from other do-mains [1,9,22,12]. However, experimen tal results provided in this paper prove that our proposed distance metric (i.e. TLD) is more effective in capturing the mutual exclusiveness of concepts across multiple heterogeneous taxonomies. In this paper we proposed a semi-automatic, language independent approach to the problem of facets extraction from heterogeneous taxonomies within datas-paces. We proposed a novel metric designed ad-hoc to capture source categories mutual exclusivity across taxonomies. We used the proposed metric as a cluster-ing distance metric for grouping together mutual exclusive facet values. Exper-imental results show that our approach outperforms state-of-the-art taxonomy concepts similarity metrics in capturing category mutual exclusiveness. Our ap-proach provides valuable aid and reduces domain experts X  effort in bootstrapping dataspace fine-grained classifications.

We plan to extend our approach along different directions. Advanced NLP techniques can be used to improve the facet labelling phase and to normalize source categories considering different lexicalizations. Finally, the effective inte-gration of the proposed approach with evidence coming from the consideration of different additional input (e.g., user queries) as proposed in related work is currently under investigation.

