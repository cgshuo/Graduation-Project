 Green's function for the Laplace operator represen ts the propagation of in uence of point sources and is the foun-dation for solving man y physics problems. On a graph of pairwise similarities, the Green's function is the inverse of the com binatorial Laplacian; we resolv e the zero-mo de dif-cult y by sho wing its physical origin as the consequence of the Von Neumann boundary condition. We prop ose to use Green's function to propagate lab el information for both semi-sup ervised and unsup ervised learning. We also deriv e this learning framew ork from the kernel regularization us-ing Repro ducing Kernel Hilb ert Space theory at strong reg-ularization limit. Green's function pro vides a well-de ned distance metric on a generic weigh ted graph, either as the e ectiv e distance on the net work of electric resistors, or the average comm ute time in random walks. We sho w that for unsup ervised learning this approac h is iden tical to Ratio Cut and Normalized Cut spectral clustering algorithms. Exp er-imen ts on newsgroups and six UCI datasets illustrate the e ectiv eness of this approac h. Finally , we prop ose a novel item-based recommender system using Green's function and sho w its e ectiv eness.
 I.2 [ Arti cial Intelligence ]: Learning; I.5.3 [ Pattern Recog-nition ]: Clustering Algorithms, Exp erimen tation, Performance Green's Function, Semi-sup ervised, Lab el Propagation In semi-sup ervised learning, we have a large amoun t of un-lab eled data, but only a very small fraction of them are lab eled. This situation is common due to ever increasing amoun t of data being accum ulated, and most of them are unlab eled or partially lab eled because lab eling data requires human skills and extensiv e lab or. The learning task is to classify the unlab eled data based on lab eled data. There are man y di eren t approac hes to this problem [7, 43]. (a) The classi cation-based approac h, in whic h a classi er is rst trained on the small lab eled data and is gradually impro ved by incorp orating unlab eled data. Earlier metho ds mostly follo w this approac h [4]. (b) The clustering-based approac h, in whic h a clustering algorithm is used on the whole data (lab eled and unlab eled), with the lab eled data serv e as penalt y or regularization or prior information. Re-cen t metho ds mostly follo w this approac h, suc h as spectral clustering based metho ds [22, 8]. (c) Special mec hanisms, suc h as Gaussian pro cess [26], graph mincut [3], entrop y minimization [16], etc.
 In this pap er, we focus on the lab el information propagation point of view. Giv en a dataset with pairwise similarities ( W ), the semi-sup ervised learning can be view ed as lab el propagation from lab eled data to unlab eled data. In its simplest form, the lab el propagation is like a random walk on a similarit y-graph W [38]. Using di usion kernel [25, 37, 24], the semi-sup ervised learning is like a di usiv e pro-cess of the lab eled information. The harmonic function ap-proac h [44] emphasizes the harmonic nature of the di usiv e function; and consistency lab eling approac h [42], emphasizes the spread of lab el information in an iterativ e way. Our work is inspired by these prior works, esp ecially by the work of Zhou et al [42].
 In physics, di usion is a pro cess of particle random walk driv en by a heat gradien t, whic h emphasizes the local and random nature of the pro cess. We believ e, however, lab el information propagation is more like the eld resp onse to the presence of point charges, whic h emphasizes the global and coheren t nature of in uence propagation. This resp onse function is the Green's function of the Laplace operator. In this pap er, we formalize the above ideas into a concrete learning algorithm as outlined in x 2. We introduce Green's function as the kernel for the Laplace operator in ( x 3). We resolv e the zero-mo de problem of the com binatorial Lapla-cian by sho wing its physics origin as the Von Neumann boundary condition ( x 4).
 We further justify the Green's function approac h in x 5 by sho wing that Green's function is a well-de ned similarit y metric on a graph, utilizing a well-established (but not widely kno wn) remark able results on the e ectiv e resistance on an electric resistor net work, whic h also can be deriv ed from ran-dom walk persp ectiv e. In x 6, we deriv e the Green's learn-ing framew ork indep enden tly from the kernel regularization theory of repro ducing kernel Hilb ert space at strong regu-larization limit.
 In x 7, we discuss the unsup ervised learning asp ects of Green's function approac h and sho w the Green's function approac h is equiv alen t to Ratio Cut and Normalized Cut spectral clus-tering algorithms. In x 8, we explore the relations of Green's function approac h with the harmonic function approac h. In x 9, we presen t the exp erimen tal results on Internet news groups and six UCI datasets. In x 10 we prop ose a novel item-based recommender system using Green's function ap-proac h. Finally x 11 summarize our most imp ortan t con tri-bution. Com binatorial Laplacian Giv en a mesh/graph with edge weigh ts W , the combinatorial Laplacian is de ned to be where the diagonal matrix con tains row sums of W : D = diag ( W e ) , e = (1 1) T .
 Green's Functions We de ne Green's function for a generic graph as the inverse of L = D W with zero-mo de discarded. (The complete discussion of zero-mo de and its physical origin is one of the main con tributions of this pap er and is discussed in x 4). We construct the Green's function using eigen vectors of L : where 0 = 1 2 n are the eigen values. We assume the graph is connected (otherwise we deal with eac h connected comp onen t one at a time). The rst eigen vector is a constan t vector v 1 = e =n 1 = 2 with zero eigen value and multiplicit y one. This zero-mo de is discarded (see x 4). The Green's function is then the positiv e de nite part of L where ( D W ) + indicates that zero eigen-mo de is discarded. Green's function can also be de ned on the generalized eigen-vectors of the Laplacian matrix: where 0 = 1 2 n are the eigen values and the zero-mo de is u 1 = e =n 1 = 2 . We have (see Eq.19 for deriv ation). In practice, we truncate the ex-pansion at K terms and store the K 1 vectors. G is com-puted on the y. So the storage requiremen t is O ( Kn ). Semi-sup ervised Learning Supp ose we have lab eled data f x i g ` i =1 ; f y i g ` lab eled data f x i g n i = ` +1 . The algorithm is a simple in u-ence propagation from lab eled data points to unlab eled data points, and can be written as for 2-class problems ( y i = 1), or y jk = 1 if k = arg max k for K -class problems where Y = ( y 1 ; ; y K ) ; Y ik = 1 if the x is a lab eled as class k , Y ik = 0 otherwise. This algorithm is deriv ed from Eq.(14) and more formally in x 6. Unsup ervised Learning In semi-sup ervised learning, the in uence propagates only once, and propagates only from lab eled data to unlab eled data. In unsup ervised learning, we let in uence propagate from any points to any other points; and rep eat multiple times until con vergence, h This ensures lab els are consisten t with in uence propaga-tion. Giv en an initial guess of the lab eling for parts or all of the data, we run the above algorithm until con vergence. This algorithm is deriv ed in x 7.
 We often use vector/matrix notation and write Eq.(5) for 2-class semi-sup ervised learning as Eq.(6) for multi-class semi-sup ervised learning as and Eq.(7) for multi-class unsup ervised learning as where arg max is a row-b y-ro w operation and interpreted as in Eq.(7). For example, A = arg max B is done by going through all rows of B , and for eac h row of B , we select the largest elemen t and set the corresp onding elemen t in A as 1 and 0 for the rest of the row. We give an introduction to Laplace operator and Green's function.
 The Laplace operator describ es the most fundamen tal spatial variations in nature, whic h determines all ma jor physical phenomenon: heat ow, wave propagation, quan tum physics, etc. For example, given the electric charge distribution ( r ) (a source) and prop er boundary condition, the equation r 2 f ( r ) = 4 ( r ) gov-erns the scalar electric eld f ( r ), whic h determines the static and induced charge distributions.
 Green's function plays essen tial role in solving partial di er-ential equations by transforming them to integral equations. Giv en a linear di eren tial operator L and sour ce function s ( r ), the di eren tial equation can be solv ed by The kernel G ( r ; r 0 ) of the integral operator is the Green's function, whic h captures the eld resp onse at r duo to a single source at r 0 represen ted by ( r r 0 ): In 3D with open boundary condition, the Green's function for the Laplace operator is the well-kno wn Colum b's inverse law: G ( r ; r 0 ) = G ( r r 0 ) = 1 = jj r r 0 jj . Semi-sup ervised Learning via Green's Function Supp ose we have lab eled data f x i g ` i =1 ; f y i g ` beled data f x i g n i = ` +1 . Our assumption is that lab eled data points are the \electric charges", ( r ) = Their in uence on unlab eled data point at r is given by Eq.(12), or, In nature, there are two types charges, the positiv e and neg-ativ e charges. This corresp ond to 2-class problems, whic h gives Eq.(5) or Eq.(8) in vector-matrix form.
 We can generalize this to K types of charges. Di eren t type of charges propagate with the same Green's function. The nal charge type of the destination point dep ends on the comp etition among di eren t charge types, same as in the positiv e-negativ e charge case. This generalization corre-sponds to the K -class classi cation problems. The purp ose of the detailed discussion of the Laplacian op-erator is to sho w the physical origin of the zero-mo de of the com binatorial Laplacian L = D W and why we should discard it in constructing the Green's function of L as in x 2. On a discretized space speci ed with the weigh ts W of a graph, the Laplacian operator becomes r 2 f ! cL f where L is a matrix, f is a vector de ned on the nodes of the graph, and c is a constan t dep ending on the discretization. For a 1D regular grid, where a = x is the spacing between gridp oints. Now the matrix L and c can be inferred from this equation. c = 1 =a 2 . Under discretization, Green's function G ( r ; r becomes a matrix G r ; r 0 . Eq.(13) implies i.e., Green's function G ( r ; r 0 ) is the inverse of L . We will see below there are two speci c forms of L , the combinatorial Laplacian and the physic al Laplacian. Physical problems are determined by (1) the di eren tial equation and (2) the boundary condition. The Laplacian matrix L extracted from the Laplacian operator dep ends on the boundary condition.
 Consider a semi-sup ervised learning problem on a graph, whic h consists of the interior domain V and the boundary @V . Nodes on the boundary are lab eled (say y i = 1 ; i 2 @V ). Nodes on the interior domain V are unlab eled. The problem is to determine the lab els on unlab eled data V . Tw o common boundary conditions are: (1) Diric hlet bound-ary condition: f ( r i ) = 0 ; 8 r i 2 @V . (2) Von Neumann boundary condition: @f ( r ) =@ r = 0 ; 8 r i 2 @V . The dis-cretized Laplacian operator di ers for di eren t boundary conditions. Let weigh ts of the graph can be decomp osed as As a main con tribution of this pap er, we pro vide the follo w-ing new results: Theorem 1 . (1) Under the Von Neumann boundary condi-tion, the resulting matrix represen ting the Laplace operator is the combinatorial Laplacian where the diagonal matrix D uu = diag ( W uu e ), i.e., ( D the sum of i -th row in W uu . (2) Under the Diric hlet bound-ary condition, the resulting matrix represen ting the Laplace operator is the physic al Laplacian where diagonal matrix D ul = diag ( W ul e ), i.e., ( D ul sum of i -th row in W ul .
 The pro of is skipp ed due to lack of space. An imp ortan t consequence of Theorem 1(a) is the presence of the zero-mo de in the com binatorial Laplacian L = D W . When the deriv ativ es are speci ed on the boundary , the function value could di er by an overall additiv e constan t. More formally , the solution of the problem is not unique: for any solution f ( x ), f ( x ) + const , is also a solution. This gives rise to the zero-mo de e of L : L e = 0. Thus the zero-mo de of L being a constan t vector is not acciden tal: Corollary 1 . A consequence of using Von Neumann bound-ary condition in deriving the com binatorial Laplacian L = D W is that the zero-mo de must be a constan t vector. We note that the operator e L = I D 1 = 2 W D 1 = 2 is not a physical operator and its zero mo de ( d 1 ; ; d n ) T is not a constan t vector. As a consequence, ( e L ) 1 + is not a kernel (see footnote in x 6.1), while L 1 + is a kernel (see x 5.3). Due to this zero-mo de, strictly speaking, the Green's func-tion of the com binatorial Laplacian L = D W does not exist. To our kno wledge, previous work using Green's func-tion skipp ed this zero mo de without giving a justi cation or even men tioning it.
 By clarifying the situation, we see that the overall constan t due to the zero-mo de does not a ect the nal results in in uence propagation and thus we can discard this zero-mo de in computing L 1 .
 Another consequence of Theorem 1 is that the physical Lapla-cian matrix of Theorem 1(b) sho w sho w up in the harmonic function approac h [44] (see x 8). There we do not use von Neumann boundary condition; instead we xed the bound-ary to the kno wn lab els, whic h is equiv alen t to Diric hlet boundary condition. For this reason, by Theorem 1, we ex-pect the physical Laplacian matrix. We summarize this as Corollary 2 . In semi-sup ervised learning setting, we view data points with kno wn lab els as boundary points. This is equiv alen t to Diric hlet boundary condition and the results of the Laplacian operator approac h will involve the physical Laplacian, rather than the com binatorial Laplacian. Green's Function using generalized Laplacian The standard approac h to Green's function is to use the eigen vectors of Eq.(1). However, we sho w here that the gen-eralized eigen vectors de ned in Eq.(3) is equally suitable to de ne the inverse of ( D W ) + . We rewrite the general-ized eigen value problem of Eq.(3) as a standard eigen value problem From this, we have Since D 1 = 2 z = u k , we obtain Eq.(4). Green's function has a rich con ten t. In this section, we point out that the Green's function relates closely to a well-established (but not widely kno wn) distance metric on a generic weigh ted graph. 1 There are two equiv alen t ways to de ne this distance: (1) the e e ctive resistanc e distance of a net work of electric resistors (2) average num ber of random walks between two nodes on a graph. We view a generic weigh ted graph as a net works of electric resistors, where the edge connecting nodes i; j is a resistor with resistance r ij . The graph edge weigh t (the pairwise similarit y) between nodes i; j is w ij = 1 =r ij . (Tw o nodes not connected by a resistor are view ed as equiv alen tly connected by a resistor with r ij = 1 or w ij = 0).
 The most common task on a resistor net work is to calculate the e ectiv e resistance between di eren t nodes. The e ec-tive resistance R ij between nodes i; j is equal to 1/(total curren t between i and j ) when i is connected to voltage 1 and j is connected to voltage 0.
 Let G = ( D W ) 1 + be the Green's function on the graph. A remark able result established in 1970s[13, 23] is where e i is a vector of all 0's except an \1" at i -th entry [recall that the distance in a metric space is d 2 ( x i ; x ( x i x j ) T M ( x i x j )]. Clearly , we can view R ij as a distance metric on a graph. Random walks on a graph is a well studied sub ject [12, 13]. Giv en a graph with nonnegativ e edge weigh ts W , one can do random walk on the graph, with transition probabilit y t ij = p ( i ! j ) = w ij =d i , or T = D 1 W .
 Consider the average num ber of hops that a walker com-mutes from i to j and comes bac k to i . This is called aver-age commute time . It is sho wn in [6, 32] that this quan tity is prop ortional to R ij . Thus R ij is a distance metric from random walk point of view. This sho ws the critical role of the Green's function.
In these graphs, the edge weigh t measures the similarit y between the two end-no des.
 By de nition, Green's function is the kernel of the integral operator as in Eq.(12). Here we list some prop erties. First, G is clearly a semi-p ositiv e de nite function. Second, any function f 2 &lt; n can be expanded in the basis of G , i.e, ( u 2 ; ; u n ) plus a constan t e = Third, for a kernel function K , K ij measures the similarit y between two objects i; j . Indeed, G ij can be view ed as an e ectiv e similarit y between nodes i and j from the e ectiv e resistance in x 5.1 or the average comm ute time in x 5.2. We can see this in the follo wing way. In statistics [28] given pairwise similarit y S = ( s ij ), a standard way to con vert to distance is d ij = s ii + s jj 2 s ij . &gt;From Eq.(20), we have s ij = G ij + const . Ignore the additiv e constan t, G is the similarit y metric that underlie the e ectiv e resistor distance metric. We therefore conclude that Green's function is a well-de ned similarit y function among pairs of nodes. Thus the Green's function is a bona de kernel. In this section, we deriv e the Green's function learning algo-rithm of x 2 from the theory of Repro ducing Kernel Hilb ert Space (RKHS) at strong regularization limit.
 Supp ose we have lab eled data f x i g n i =1 with lab els f y We wish to learn the mapping function f ( x ) suc h as f ( x i ) jj 2 is minimized. In statistics, we often add a penalt y (regularization) term to ensure smo othness of certain quan-tities suc h as deriv ativ es. RKHS uses kernel as the regu-larization term. Let the kernel has the spectral expansion, K = nd the function f ( ) that minimizes RKHS theory is equiv alen t to the uniform con vergence the-ory of Vapnik [39]. When the loss function [ y i f ( x i )] replaced by the hinge function [1 y i f ( x i )] + , the dual space solution gives SVM.
 Solution of RKHS for the quadratic loss of is well-kno wn: At the large limit, we get the solution Now, setting the kernel as the Green's function: K = G , using only the leading order and ignoring the prop ortional constan t, this gives the learning algorithm Eq.(5) or Eq.(8) of x 2. Standard kernel mac hines are used for sup ervised learning with y i = 1. For semi-sup ervised learning we set y = 0 for those unlab eled data. The above generic results is valid for any type of kernels. Here we focus on the kernel being the Green's function of Laplacian operator. Regularization approac h is gener-ally used to con trol smo othness of certain deriv ativ es. The Laplacian operator, a scalar quan tity constructed from sec-ond order deriv ativ es is sometimes used as the regularization term [ ? , 42]. Our focus here is to sho w that di eren t Lapla-cian regularization's at strong regularization limit can be captured by Green's functions. This leads to a simple and ecien t algorithm.
 We write the second term in RKHS explicitly , Since f T ( D W ) f = 1 2 The solution to this variational problem can be easily de-rived. We write J ( f ) = jj y f jj 2 + f T K 1 f ; where K ( D W ) + . Setting @J=@ f = 2 f 2 y + 2 K 1 f = 0 ; we obtain f = ( I + K 1 ) 1 y = ( I + ( D W ) + ) 1 y : Note that ( I + K 1 ) 1 = K ( K + I ) 1 ; this reco vers Eq.(22). Zhou et al [42] prop ose the consistency framew ork that min-imizes the functional
J [ f ] = and obtain where f W is de ned in Eq.(18). They set (1 + ) 1 = 0 : 99 (or = 0 : 01 ) in practice. n the strong regularization limit, ! 0, Eq.26 becomes (ig-noring a prop ortional constan t) f = ( I f W ) 1 y . Discard-ing the zero mo de ( x 4), f = ( I f W ) 1 + y ; this is a Green's function form ulation of ( I f W ) 1 + , whic h, strictly speaking however, is not a kernel 2 . In this section, we sho w how the semi-sup ervised learning in x 6 can be extend to unsup ervised learning. This is based on the follo wing three observ ations/p ersp ectiv es. (A) A key feature in the RKHS based formalism for the semi-sup ervised learning in x 6 is that the Green's function learning is carried out in the parameter region where the regularization term is dominan t i.e., the regression term is only a small perturbation to the regularization term whic h determines the unsup ervised learning. (B) In the semi-sup ervised learning using Green's function, on the lab eled data points, the class lab els are re-calculated using Eq.(6) or Eq.(8). It is imp ortan t to note that the re-calculated lab els may di er from the original ones. When
Strictly speaking ( I f W ) 1 + is not a kernel, because not all functions in &lt; n can be expanded by the eigen vectors of ( I f W ) + plus a constan t, due to the fact that the excluded zero-mo de of ( I f W ) is z 1 = ( d 1 ; ; d n ) T whic h is not a constan t vector (see Eq.18).
 this happ ens, we consider the newly computed lab els as the correct lab el, because they are consisten tly computed in the same way as the lab els on the unlab eled data points. Thus this mec hanism allo ws us to correct mistak es on partially observ ed lab els. (C) Because of (A) and (B), we may consider the partially observ ed lab els as temp orary information, and the entire la-bels should be determined in a self-consisten t manner. In this way, we can do the unsup ervised learning by (1) tem-porarily assign lab el information to some or all data points and (2) iterate the lab el propagation a num ber of times un-til they are self-consisten t. This is the rational for the un-sup ervised learning using Green's function, as explained in Eqs.(7,10). In this section, we capture the essence of the unsup ervised learning by sho wing they are iden tical to the ratio cut and normalized cut spectral clustering. This sho ws the inheren t consistency of the Green's function learning framew ork. For simplicit y, we write the in uence propagates as Prop osition 3 . In the unsup ervised learning using Eq.(27), the con verged solution is the optimal solution to the opti-mization problem max H Tr[ H T GH ] ; with prop er orthogo-nalit y condition.
 Pro of . It is a well-kno wn in matrix computation theory that the solution for max Tr[ H T GH ] can be obtained by the subspace iteration algorithm[15] using Eq.(27) sub ject to the appropriate orthogonalit y condition. u { A main results of this pap er is to sho w that Green's function frame is equiv alen t to spectral clustering, the Ratio Cut[18] and the Normalized Cut[36]. It is kno wn that Ratio Cut can be form ulated as the optimization problem Similarly the Normalized Cut can be form ulated as Both optimization problems can be solv ed using the Green's function in uence propagation approac h with prop er orthog-onalit y. Therefore, our Green's function learning is equiv a-lent to spectral clustering.
 In recen t years spectral clustering using the Laplacian of the graph emerges as solid approac h for data clustering. Here we focus on the Ratio Cut [18] and the Normalized Cut [36] clustering objectiv e functions. We are interested in the multi-w ay clustering objectiv e functions, where ( C k ) = j C k j for Ratio Cut, and ( C k ) = for Normalized Cut, and s ( C p ; C q ) = The multi-w ay clustering relaxation is studied in [17]. Using cluster indicators H = ( h 1 h k ), the Ratio-Cut becomes the minimization problem The Normalized Cut becomes the minimization problem We now sho w that the Ratio Cut optimization problem of Eq.(31) is equiv alen t to the optimization problem of Eq.(28) and the Normalized Cut optimization problem of Eq.(32) is equiv alen t to the optimization problem of Eq.(29). Theorem 4 . The iteration the algorithm of Eq.(27) with orthogonalit y H T H = I con verges to an optimal solution of the multi-w ay Ratio Cut. The iteration the algorithm of Eq.(27) with orthogonalit y H T DH = I con verges to an optimal solution of the multi-w ay Normalized Cut. Pro of . Clearly , the null space of ( D W ) does not con-tribute to H T ( D W ) H . This can be seen by doing eigen expansion of L = D W and the zero-eigen value mo des drop out. Only the positiv e de nite part of ( D W ) con tribute. Thus we have This is Tr[ H T G (1) 1 H ]. The orthogonalit y of the eigen-vectors con tained in G (1) is consisten t with the H T H = I orthogonalit y. This pro ves the case for Ratio Cut. For Nor-malized Cut, the pro of is iden tical. The D -orthogonalit y of the eigen vectors con tained in G (2) is consisten t with the H
T DH = I orthogonalit y. This pro ves the case for normal-ized Cut. u { There exists a very large amoun t work on semi-sup ervised learning and we have men tioned sev eral of them in x 1. Among them, the closest to our Green's function approac h are (1) the consistency framew ork of Zhou et al [42], whic h is dis-cussed in x 6 and (2) the harmonic function approac h of Zhu et al. [44], whic h is discussed below. In x 9, we do extensiv e exp erimen ts on sev en datasets and compare our approac h to these two metho ds. \Harmonic function" refers to that fact the solution to r 0 has the prop erty that f ( r i ) equals to the average of f ( r ) at r 's neigh bors. In their approac h for semi-sup ervised learn-ing, f i is xed to the lab el for lab eled point i ; Separating lab eled and unlab eled nodes, the Laplace equation Lf = 0 becomes
Focus on f u part, we obtain Note the presence of the physical Laplacian as discussed in Corollary 2 ( x 4.1). Zhu et al. [44] obtain whic h di ers sligh tly from Eq.(34): D ul is missing. We apply Green's function (GF) approac h to 7 datasets. We compare to prior metho ds whic h are closest to GF approac h: (a) the consisten t framew ork (CF) approac h [42] and (b) the harmonic function (HF) approac h [44].
 Datasets . The sev en datasets include Internet newsgroups and 6 datasets from UCI rep ository [11]. We use a 5-newsgroup subset of the standard 20-newsgroup dataset consisting of the follo wing 5 categories: comp.graphics, rec.motorcycles, rec.sp ort.baseball, sci.space, and talk.p olitics.mideast. These 7 datasets are represen tativ e of real applications. Their data size, data dimension and num ber of classes are summarized in Table 1. Semi-sup ervised Learning We rst study semi-sup ervised learning. We randomly se-lection 10% of data points and treat them as lab eled data. The rest in the dataset are unlab eled data. We run the three metho ds. To get good statistics, we rerun these test 10 times so the lab eled datasets are di eren t from eac h run. Final results are the averages over these 10 runs. They are listed in Table 2. On one dataset, housing , all three metho ds are compatible. On other 6 datasets, our Green's function (GF) approac h generally outp erforms HF and CF metho ds, sometime very signi can tly.
 Table 2: Classi cation accuracy (in percen tage) with 10% of data lab eled. CF: Consisten t Framew ork.
 Semi-sup ervised Learning with Noisy Lab els For eac h dataset, 10% data points are randomly selected and are given correct lab els. Another 5% data points are randomly selected and are given incorrect lab els, to emu-late the noises. All 3 learning metho ds are applied to the 7 datasets. Results for the average of 10 runs of random samples are listed in Table 2. In general, accuracies in Ta-ble 3 are sligh tly worse than those in Table 1, as exp ected. In the Line with "GF-MP", we presen t the results of doing multiple propagations. The results are generally impro ved from the single propagation.
 Table 3: Classi cation accuracy (in percen tage) for 7 datasets with 10% correctly lab eled and another 5% incorrectly lab eled.
 E ects of Dimension Reduction In the learning form ula Eq.(26) for CF, we apply the di-mension reduction technique by expressing the function ( I (see [10]). We applied this dimension reduction (DR) ver-sion of CF and the results are sho wn in Table 4. The rst 2 columns are for the 10% lab eled case as in Table 2. The second 2 columns are for the noise lab el case as in Table 3. Dimension reduction signi can tly and consisten tly impro ves the performance.
 % Incorrectly Lab eled 0% 0% 5% 5% Unsup ervised Learning We study the unsup ervised learning using Green's function approac h. We use the results of K -means as initial starts, and run the in uence propagation of Eq.(27). The results are given in Table 5. For 4 out of 7 datasets, the accuracy values are impro ved.
 Recommendation systems, as the personalize d information navigation and ltering techniques used to iden tify a set of items that will be of interest to certain users, have been ac-tive and enjo ying a gro wing amoun t of atten tions with the explosiv e gro wth of world-wide-w eb and the emergence of e-commerce [1, 35]. They have been widely used to recom-mend pro ducts suc h as books, movies and musics to cus-tomers [27, 29] and to lter news stories [31, 2]. Various ap-proac hes for recommender systems have been dev elop ed by utilizing demographic, con ten t, or historical information [5, 35, 9, 14]. Among these metho ds, memory-b ased collabora-tive ltering has been widely used in practice [41, 27, 20]. Memory-based metho ds for collab orativ e ltering aim at predicting user ratings for given items based on a collec-tion of rating examples by averaging ratings between pairs of similar users or items. These metho ds can be further di-vided into user-based collab orativ e ltering [21, 20, 31] and item-based collab orativ e ltering [9, 40, 34]. To estimate an unkno wn rating of a given item by a test user, user-based metho ds rst measure the similarities between the test user and other users, then the unkno wn rating is ap-pro ximated by averaging the weigh ted kno wn ratings of the given item by similar users. Similarly , item-based metho ds rst measure the similarities between the test item and other items, the unkno wn rating is then calculated by averaging the kno wn ratings of similar items by the test user. Despite their success, however, there are still some ma jor limitations with these recommendation metho ds suc h as data sparsit y, recommendation reliabilit y and scalabilit y [33, 35]. In this section, we prop ose a novel item-based recommen-dation scheme via lab el propagation using Green's function. Man y researc hers have sho wn that item-based recommenda-tion algorithms can recommend a set of items more quic kly, with the recommendation results comparable to user-based metho ds [40]. We tak e a novel view by treating the item-based recommendation as the lab el information propaga-tion. Giv en item similarit y matrix W , the item recommen-dation can be view ed as lab el propagation from lab eled data (i.e., items with ratings) to unlab eled data. In a typical recommendation system, there is a set of users U = f u 1 ; u 2 ; ; u M g and a set of items I = f i 1 ; i And we can construct an M N user-item matrix R , with its ( p; q )-th entry R pq equal to the rating of the user u to the item i q . If user u p has not rated for item i q R pq = 0. Note that these ratings may either ordinal (as in Movielens [30]) or con tinuous (as in Jester [14]). We use u to denote the p -th row of R , whic h is called the user vector of u p , and i q to denote the q -th column of R , whic h is called the item vector of i q .
 De nition 1 (Item graph). An item graph is an undi-rected weighte d graph G = ( V ; E ) , wher e (1) V = I is the node set ( I is the item set, which means that each item is regarded as a node on the graph G ); (2) E is the edge set. Asso ciate d with each edge e pq 2E is a weight w pq subje ct to w pq 0 ; w pq = w qp .
 Typical similarit y calculation metho ds include the cosine similarity , conditional probability , and exponential cosine sim-ilarity [9, 40]. In our study , we use the cosine similarity to build the item graph. The recommendation on item graph can be view ed as a lab el propagation problem. Let y T = ( y 1 ; ; y n ) be the rating for a user. Giv en an incomplete rating the question is to predict those missing values. Usually , the num ber of missing values are far greater than the num ber of kno wn values. In the Green's function learning framew ork, we set and compute the complete rating as the linear in uence propagation of Eq.(12) where G is the Green's function built from the item graph. Giv en a user-item matrix R , with its ( p; q )-th entry R equal to the rating of the user u p to the item i q . Let R con tains the incomplete rating. Item-based recommendation is One can see this is an extremely simple algorithm. In this section we exp erimen tally evaluate the performance of our recommendation algorithm using Green's function and compare it with that of the traditional recommenda-tion algorithms.
 Dataset We use the Mo vielens [30] in our exp erimen ts. The movielens dataset is collected from a web-based researc h rec-ommender system MovieL ens , whic h is debut in Fall 1997. The dataset now con tains the records of over 43000 users who have rated over 3500 di eren t movies, with the ratings being integer values from 1 to 5. In our exp erimen ts, we use a subset of the 1 million movielens dataset, whic h con tains 10,000 records including the ratings of 943 users to 1682 movies. We randomly divided this dataset into a training set (including 90,570 records) and a testing set (including 9,430 records), and the two sets have no intersections. Evaluation Measures . We use the follo wing three di er-ent measures in our exp erimen ts and we exp ect these mea-sures would give us enough insigh ts: Mean Absolute Error (MAE) : Giv en the target ratings R and the predicted ratings R 0 , MAE is the average devia-tion of the prediction from the target, i.e., where n is the num ber of predicted ratings.
 Mean Zero-one Error (MZOE) : MZOE is de ned by i.e., it calculates the fraction of incorrect predictions. Order Consistency (OC) : Sometimes we do not need to compute very accurate ratings, but just want the predicted preferences (or, equiv alen tly, the order of the unrated items) to be accurate. OC is used to measure how iden tical the predicted order is to the true order [40]. Assuming there are d items, a is the vector that these d items are sorted in an decreasing order according to their predicted ranking scores, b is the vector that these d items are sorted in an decreasing order according to their true ratings. For these d items, we have C 2 d = d ! = (2!( d 2)!) ways to randomly select a pair of di eren t items. A is the set of item pair whose order in a are the same as in b , then order consistency (OC) is de ned as: wher e jAj represents the cardinality of A .
 Result Analysis . We compare the performance of our Green's function approac h with the traditional item-based metho ds and user-based metho ds using di eren t similarit y measures, namely the cosine (Cos), conditional probability (CP) and exponential cosine (ExCos) similarit y measures. We also compare with Item Rating Smo othness Maximiza-tion (ISRM) recen tly dev elop ed in [40]. ISRM metho d ex-plores the geometric information of the item data and com-putes the rating smo othness over the whole item graph via combinatorial graph Laplacian . It then predicts the ratings of a user to his unrated items by minimizing the rating smo othness.
 Table 6 sho ws the performance comparisons of various meth-ods. We observ e that our Green's function metho d has the lowest MAE and MOE errors among all the recommendation metho ds. The comparison also sho ws that item-based rec-ommendation metho ds perform better than the user-based metho ds. Figure 1 presen ts the order consistency (OC) val-ues of Green's function metho d, item-based metho ds and the ISRM metho d and it sho ws that our Green's function metho d also have the best OC values. The exp erimen tal results illustrate the e ectiv eness of our Green's function approac h.
 Table 6: Performance Comparisons of Various Rec-ommendation Metho ds.
 Figure 1: Order Consistency Performance Compar-isons. In this pap er, we prop ose to use Green's function as a mec h-anism of lab el information propagation. Theoretically , (1) we sho w that the zero-mo de of the com binatorial Laplace matrix is originated from the von Neumann boundary con-dition, and thus its zero-mo de must be a constan t vector, whic h therefore should be discarded. (2) We deriv e the Green's function learning framew ork from the kernel reg-ularization using Repro ducing Kernel Hilb ert Space theory at strong regularization limit. (3) We clarify that in semi-sup ervised learning, setting data points with kno wn lab els as boundary point is equiv alen t to using Diric hlet boundary condition and the results of the Laplacian operator approac h will involve the physical Laplacian, rather than the com-binatorial Laplacian. Overall, our results clarify the exact mec hanisms of the often vague concept of lab el propagation. We also sho w that the Green's function approac h is closely related to the well-established distance metric on a graph, i.e., the e ectiv e resistor distance (via an analogy to a net-work of electric resistors) and the average comm ute time via random walks. These give more concrete understanding of Green's function approac h and will help to deriv e more ecien t appro ximations and e ectiv e varian ts.
 We also performed extensiv e exp erimen ts on 7 datasets and the exp erimen tal results indicate the Green's function ap-proac h outp erform other approac hes. Finally , we prop ose a novel item-based recommender system using Green's func-tion.
 We thank Marco Saerens and Hongyuan Zha for pointing out the relationship to random walk and resistor distance. C. Ding and H.D. Simon are supp orted by the US Dept of Energy , Oce of Science, the LBNL LDRD funding, under Con tract No. DE-A C02-05CH11231. T. Li is partially sup-ported by a IBM Facult y Researc h Aw ard, NSF CAREER Aw ard IIS-0546280 and NIH/NIGMS S06 GM008205.
