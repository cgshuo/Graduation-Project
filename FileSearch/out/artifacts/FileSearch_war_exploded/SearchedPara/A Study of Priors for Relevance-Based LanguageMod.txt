 Probabilistic modelling of recommender systems naturally introduces the concept of prior probability into the recom-mendation task. Relevance-Based Language Models, a prin-cipled probabilistic query expansion technique in Informa-tion Retrieval, has been recently adapted to the item rec-ommendation task with success. In this paper, we study the effect of the item and user prior probabilities under that framework. We adapt two priors from the document re-trieval field and then we propose other two new probabilis-tic priors. Evidence gathered from experimentation indi-cates that a linear prior for the neighbour and a probabilis-tic prior based on Dirichlet smoothing for the items improve the quality of the item recommendation ranking.
 H.3.3 [ Information Search and Retrieval ]: Information filtering Algorithms, Experimentation Recommender Systems; Collaborative Filtering; Relevance-Based Language Models; prior probability
Recommender systems aim to find relevant pieces of infor-mation which may be of interest to the users. Since this ob-jective can be modelled as a personalised item ranking task, the use of techniques from the Information Retrieval (IR) field is becoming more and more popular. An effective ap-proach to recommendation is Collaborative Filtering (CF). This family of algorithms exploits the past interaction be-tween users and items to generate personalised suggestions. pseudo-relevance feedback technique for text retrieval. Their goal is to expand the user X  X  query with new relevant terms to improve the retrieval performance. In order to achieve so, an initial retrieval is computed and relevance over the first top results is assumed (this set of documents is called pseudo-relevant set). Recently, RM have been adapted to CF showing high accuracy figures [8]. Under this scenario, users X  profiles play the role of both documents and queries and the items are equivalent to the terms. In this way, instead of expanding queries with new terms, we can use RM to expand users X  profiles with new relevant items. The pseudo-relevant set in the CF task is the neighbourhood of the target user. These neighbourhoods can be determined using, for instance, the traditional k -NN algorithm.
There exist two estimates of Relevance Models that were proposed for recommendation. In this paper, we focus on RM2 since it was reported to provide the best results. This algorithm computes a Relevance Model for each user u cal-culates the relevance of each item i under it, p ( i | R u ): where V u refers to the set of neighbours of the user u . The conditional probability of an item given a user, p ( i | u ), is computed by smoothing the maximum likelihood estimate. The original papers used Jelinek-Mercer smoothing [8]; how-ever, in this paper, we employ Absolute Discounting smooth-ing which consists in subtracting a constant  X  from each rat-ing. The rationale behind this decision is that this smooth-ing method models the user bias yielding better recommen-dations. The reason is that the amount of smoothing applied from the background collection p ( i |C ) is inversely propor-tional to the average rating of the user [10]: Finally, we need to specify how to compute the user prior p ( v ) and the item prior p ( i ). In the original paper uni-form distributions were used [8]. Our proposed priors are described next. The recommendation formula that results from applying Relevance Models to the CF task (see Eq. 1) involves the use of a user prior for each of the neighbours, p ( v ), and an item prior for each of the candidate items to be recommended, p ( i ). Next, we introduce the user priors; from these, the deviation of the item priors is straightforward.

We will use the following priors to compute the prior prob-ability of a neighbour in the RM2 algorithm.

This prior is drawn from a uniform distribution. That is to say, every user in the population has the same prior probability. We use this prior as our baseline. The linear document length prior was previously used in Information Retrieval [5, 3]. Its adaptation to recommen-Dataset Users Items Ratings Density MovieLens 100k 943 1682 100,000 6.305% R3-Yahoo! 15,400 1,000 365,703 2.375% LibraryThing 7,279 37,232 749,401 0.277%
We performed five-fold cross-validation using the splits provided by MovieLens 100k to train the prior parameters and choose the best priors. Then, we applied these priors and their parameters to the R3-Yahoo! and LibraryThing collections. Since LibraryThing does not include a default split, we randomly selected 80% of the ratings of each user as training subset and the rest as test subset. Thus, the splits of the MovileLens collection were used for tuning parameters whilst the splits of other datasets were employed solely for evaluation purposes, that is, the training split was used as seed data for the recommender system and the testing split was used for assessing the performance.

We followed the TestItems approach described in [1] for estimating the ranking quality of the recommendations. For each user in the test subset, we compute recommendations including all the items in the test subset. Although this methodology underestimates the true value of the precision-oriented metrics (because it considers that non-rated items are irrelevant), it provides more reliable results.
We decided to use nDCG (Normalised Discounted Cumu-lative Gain) at a cut-off value of 10 for assessing the quality of the top ranking. We employed the standard formulation [11] using the ratings in test as graded relevance judgements. For computing the neighbourhoods, we utilised k -NN with Pearson X  X  correlation coefficient as similarity metric. We fixed the value of k to 400 neighbours. We used the opti-mal smoothing method (Absolute Discounting with  X  = 0 . 1) reported in [10].

Regardless the fact that we tested all possible combina-tions of priors (and their parameters), we decided to present the results in two steps for the sake of the discussion. First, we introduce the results of tuning only the user prior and, second, we show the results of tuning the item prior using the best user prior. We denote that we use the prior X for users and the prior Y for items with the following notation X-Y.
Taking a uniform distribution for the item prior, we tested all the priors proposed in Sec. 3. Figure 1 illustrates the results, in terms of nDCG@10, of this experiment.

We can observe that using a uniform distribution is the worst scenario: any of the others user priors improves the ranking quality. In spite of the fact that the Probabilistic priors improve the nDCG figures, the Linear prior is the one that provides the best results implying that we should rely on neighbours with a high number of ratings. This finding suggests that users with a tiny rating profile are not very reliable for computing recommendations since we have few data about them.

An advantage of this finding is that the Linear prior is a parameter-free technique. This allows to improve the per-formance of a RM2 recommender without introducing more complexity into the model. This is expected because in this extreme case since the ef-fect of this prior in the denominator would demote items that were rated by power users. In contrast, if we lower  X  too much, we are obtaining a uniform prior.

The Probabilistic prior based on Absolute Discounting smoothing produces worse results than the other Probabilis-tic priors because of the counterintuitiveness in its formula-tion. Although this method normalises the bias of the users (see Sec. 2), when we amplify the effect of the power users we are also decreasing the importance of popular items be-cause we discount a constant  X  from each rating (i.e., the more ratings an item has, the more discount we apply).
Finally, the Probabilistic prior based on Dirichlet presents the best figures, with  X  = 700 as the optimal value of the parameter. Dirichlet smoothing has been thoroughly stud-ied in Information Retrieval [7] concluding that it demotes large documents. In this case, as the item prior is in the denominator, we are promoting, in a controlled way, items with a larger number of ratings. Moreover, it shows a quite stable performance with respect to the parameter  X  .
To assess if the previous findings generalise to other col-lections, we apply those priors to the R3-Yahoo! and the LibraryThing datasets. We also report the results of the standard user-based Collaborative Filtering algorithm (us-ing the 50 nearest neighbours according to Pearson X  X  cor-relation) denoted by UB and a standard matrix factorisa-tion algorithm (using 350 latent factors) denoted by SVD. We present the nDCG@10 values of these recommendation methods in Table 2.

From this data, it can be seen that the use of the Linear prior for modelling neighbour prior probabilities and Probal-istic prior (with Dirichlet smoothing) for items improves the ranking precision in the three collections. Nevertheless, the results obtained for the R3-Yahoo! dataset are not statisti-cally significant according to the Wilcoxon test ( p &lt; 0 . 01).
In this paper, we have studied the effect that the user and item priors play in the RM2 algorithm, the most effec-tive Relevance-Based Language Model estimate for Collab-orative Filtering recommendation. This study has identified the Linear prior as the optimal one for modelling neighbour-hoods. Additionally, the Probabilistic prior based on Dirich-let smoothing is a good choice for computing the item prior probability. We have found that the correct modelling of these prior probabilities can significantly improve the rank-ing accuracy in two out of three collections.

