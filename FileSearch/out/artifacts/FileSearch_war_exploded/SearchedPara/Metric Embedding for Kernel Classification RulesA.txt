 Bharath K. Sriperumbudur BHARATHSV @ UCSD . EDU Omer A. Lang OLANG @ UCSD . EDU Gert R. G. Lanckriet GERT @ ECE . UCSD . EDU Parzen window methods, also called smoothing kernel rules are widely used in nonparametric density estimation and function estimation, and are popularly known as ker-nel density and kernel regression estimates, respectively. In this paper, we consider these rules for classification. To this end, let us consider the binary classification problem of classifying x  X  R D , given an i.i.d. training sample { ( X i , Y i ) } n i =1 drawn from some unknown distribution D , The kernel classification rule (Devroye et al., 1996, Chap-ter 10) is given by g n ( x ) = where K : R D  X  R is a kernel function , which is usually nonnegative and monotone decreasing along rays starting from the origin. The number h &gt; 0 is called the smooth-ing factor , or bandwidth , of the kernel function, which pro-vides some form of distance weighting. We warn the reader not to confuse the kernel function, K , with the reproduc-kernel Hilbert space (RKHS), which we will denote with K . 1 When K ( x ) = 1 {k x k na  X   X ve kernel ), the rule is similar to the k -nearest neighbor ( k -NN) rule except that k is different for each X i in the training set. The k -NN rule classifies each unlabeled ex-ample by the majority label among its k -nearest neighbors in the training set, whereas the kernel rule with the na  X   X ve kernel classifies each unlabeled example by the majority label among its neighbors that lie within a radius of h . De-vroye and Krzy  X  zak (1989) proved that for regular kernels (see Devroye et al., (1996, Definition 10.1)), if the smooth-ing parameter h  X  0 such that nh D  X  X  X  as n  X  X  X  , then the kernel classification rule is universally consistent . But, for a particular n , asymptotic results provide little guid-ance in the selection of h . On the other hand, selecting the wrong value of h may lead to very poor error rates. In fact, the crux of every nonparametric estimation problem is the choice of an appropriate smoothing factor. This is one of the questions that we address in this paper by proposing an algorithm to learn an optimal h .
 The second question that we address is learning an opti-mal distance metric. For x  X  R D , K is usually a func-tion of k x k 2 . Some popular kernels include the Gaus-sian kernel, K ( x ) = e  X  X  x k 2 2 ; the Cauchy kernel, K ( x ) = 1 / (1 + k x k D +1 2 ) ; and the Epanechnikov kernel K ( x ) = shown that the finite-sample risk of the k -NN rule may be reduced, for large values of n , by using a weighted Eu-clidean metric, even though the infinite sample risk is inde-pendent of the metric used. This has been experimentally confirmed by Xing et al. (2003); Shalev-Shwartz et al. (2004); Goldberger et al. (2005); Globerson and Roweis (2006); Weinberger et al. (2006). They all assume the met-ric to be  X  ( x, y ) = for x, y  X  R D , where  X  = L T L is the weighting matrix, and optimize over  X  to improve the performance of the k -NN rule. Since the kernel rule is similar to the k -NN rule, one would expect that its performance can be improved by making K a function of k L x k 2 . Another way to interpret transformed data lie in a Euclidean metric space.
 Some applications call for natural distance measures that reflect the underlying structure of the data at hand. For ex-ample, when computing the distance between two images, tangent distance would be more appropriate than the Eu-clidean distance. Similarly, while computing the distance between points that lie on a low-dimensional manifold in R D , geodesic distance is a more natural distance measure than the Euclidean distance. Most of the time, since the true distance metric is either unknown or difficult to com-pute, Euclidean or weighted Euclidean distance is used as a surrogate. In the absence of prior knowledge, the data may be used to select a suitable metric, which can lead to better classification performance. In addition, instead of x  X  R D , suppose x  X  ( X ,  X  ) , where X is a metric space with  X  as its metric. One would like to extend the kernel classification rule to such X . In this paper, we address these issues by learning a transformation that embeds the data from X into a Euclidean metric space while improving the performance of the kernel classification rule.
 The rest of the paper is organized as follows. In  X  2, we formulate the multi-class kernel classification rule and pro-pose learning a transformation,  X  , (that embeds the training data into a Euclidean space) and the bandwidth parame-ter, h , by minimizing an upper bound on the resubstitution estimate of the error probability. To achieve this, in  X  3, we restrict  X  to an RKHS and derive a representation for it by invoking the generalized representer theorem. Since the resulting optimization problem is non-convex, in  X  4, we approximate it with a semidefinite program when K is a na  X   X ve kernel. We present experimental results in  X  5, wherein we show on benchmark datasets that the proposed algorithm performs better than k -NN and state-of-the-art metric learning algorithms developed for the k -NN rule. some unknown distribution D where X i  X  ( X ,  X  ) and Y i [ L ] , with L being the number of classes. The multi-class kernel classification rule is given by where K : X  X  R + and K x some nonnegative function,  X  , with  X  (0) = 1 . The prob-ability of error associated with the above rule is L ( g n Pr ( X,Y )  X  X  ( g n ( X ) 6 = Y ) where Y is the true label associ-ated with X . Since D is unknown, L ( g n ) cannot be com-puted directly but can only be estimated from the training set. The resubstitution estimate , 3 b L ( g n ) , which counts the number of errors committed on the training set by the clas-sification rule, is given by b L ( g n ) := 1 n As aforementioned, when X = R D ,  X  is usually cho-sen to be k . k 2 . Previous works in distance metric learn-ing learn a linear transformation L : R D  X  R d lead-ing to the distance metric,  X  L ( X i , X j ) := k L X L X j k 2 = the variance-covariance structure of the data. In this work, our goal is to jointly learn h and a measurable function,  X   X  C := {  X  : X  X  R d } , so that the resubstitu-tion estimate of the error probability is minimized with  X  ( X i , X j ) := k  X  ( X i )  X   X  ( X j ) k 2 . Once h and  X  are known, the kernel classification rule is completely speci-fied by Eq. (2).
 Devroye et al., (1996, Section 25.6) show that kernel rules of the form in Eq. (1) picked by minimizing b L ( g n ) with smoothing factor h &gt; 0 are generally inconsistent if X is nonatomic. The same argument can be extended to the multi-class rule given by Eq. (2). To learn  X  , simply mini-mizing b L ( g n ) without any smoothness conditions on  X  can  X  can be constructed as follows. Let n l be the number of points that belong to l th class. Suppose n 1 = n 2  X  X  X  = n L . Then for any h  X  1 , choosing  X  ( X ) = Y when X = X i and  X  ( X ) = 0 when X /  X  { X i } n i =1 clearly yields zero resubstitution error. However, such a choice of  X  leads to a kernel rule that always assigns the unseen data to class 1, leading to very poor performance. Therefore, to avoid overfitting to the training set, the function class C should satisfy some smoothness properties so that highly non-smooth functions like the one we defined above are not chosen while minimizing b L ( g n ) . To this end, we intro-duce a penalty functional,  X  : C  X  R + , which penalizes non-smooth functions in C so that they are not selected. 4 Therefore, our goal is to learn  X  and h by minimizing the regularized error functional given as where  X   X  C , h &gt; 0 and the regularization parameter,  X  0 &gt; 0 . g n in Eq. (3) is given by Eq. (2), with  X  replaced by  X  . Minimizing L reg (  X , h ) is equivalent to minimizing the number of training instances for which g n ( X ) 6 = Y , over the function class, {  X  :  X [  X  ]  X  s } , for some appropriately chosen s .
 Consider g n ( x ) defined in Eq. (2). Suppose Y i = k for some X i . Then g n ( X i ) = k if and only if where the superscript  X  is used to indicate the dependence of K on  X  . 5 Since the right hand side of Eq. (4) involves the max function which is not differentiable, we use the inequality max { a 1 , . . . , a m }  X  maximize bound given by P use j 6 = i just to make sure that  X  ( X i ) is not the only point within its neighborhood of radius h . Define  X  ij := 2  X  Y i ,Y j  X  1 where  X  represents the Kronecker delta. Then, the problem of learning  X  and h by minimizing L reg (  X , h ) in Eq. (3) reduces to solving the following optimization problem, where  X  = n X  0 and  X  i (  X , h ) given by is an upper bound on 1 { g above non-convex optimization problem is NP-hard. The gradient optimization is difficult because the gradients are zero almost everywhere. In addition to the computational hardness, the problem in Eq. (5) is not theoretically solv-able unless some assumptions about C are made. In the following section, we assume C to be an RKHS with the reproducing kernel K and provide a representation for the optimum  X  that minimizes Eq. (5). We remind the reader that K is a smoothing kernel which is not required to be a positive definite function but takes on positive values, while K is a reproducing kernel which is positive definite and can take negative values. Many machine learning algorithms like SVMs, regulariza-tion networks, and logistic regression can be derived within the framework of regularization in RKHS by choosing an appropriate empirical risk functional with the penalizer be-ing the squared RKHS norm (see Evgeniou et al. (2000)). In Eq. (5), we have extended this framework to kernel classification rules, wherein we compute the  X   X  C and h &gt; 0 that minimize an upper bound on the resubstitution estimate of the error probability. To this end, we choose C to be an RKHS with the penalty functional being the squared RKHS norm, 8 i.e.,  X [  X  ] = k  X  k 2 C . By fixing h , the objective function on  X  only through {k  X  ( X i )  X   X  ( X j ) k 2 } n i,j =1 P  X  min The following result provides a representation for the min-imizer of Eq. (6), and is proved in Appendix A. We remind the reader that  X  is a vector-valued mapping from X to R d . Theorem 1 (Multi-output regularization) . Suppose C = {  X  : X  X  R d } = H 1  X  . . .  X H d where H i is an RKHS with reproducing kernel K i : X X X  X  R and  X  = (  X  1 , . . . ,  X  with H i 3  X  i : X  X  R . Then each minimizer  X   X  C of Eq. (6) admits a representation of the form where c ij  X  R and Remark 2. (a) By Eq. (7),  X  is completely determined by { c ij : i  X  [ n ] , j  X  [ d ] } . Therefore, the problem of learning  X  reduces to learning n  X  d scalars, { c ij : (b)  X  h in Eq. (6) depends on  X  through k  X  ( . )  X   X  ( . ) k 2 . Therefore, for any z, w  X  X , we have k  X  ( z )  X   X  ( w ) k 2 2 = P ( c 1 m , . . . , c nm ) T , k z m := ( K m ( z, X 1 ) , . . . , K  X  m := c m c T m ,  X  m  X  [ d ] and tr ( . ) represents the trace. (c) The regularizer, k  X  k 2 C in Eq. (6) is given by k  X  k 2 P P (d) Since  X  appears in the form of  X   X  and k  X  k 2 C Eq. (6), learning  X  is equivalent to learning {  X  m  X  0 : rank (  X  m ) = 1 , 1 T  X  m 1 = 0 } d m =1 .
 In the above remark, we have shown that  X  h and k  X  k C in Eq. (6) depend only on the entries in d kernel matrices (as-sociated with d kernel functions) and n  X  d scalars, { c ij In addition, we also reduced the representation of  X  from { c are convex quadratic functions of { c m } d m =1 , while they are linear functions of {  X  m } d m =1 . Depending on the nature of K , one representation would be more useful than the other. Corollary 3. Suppose K 1 = . . . = K d = K . Then, for any z, w  X  X ,  X  2  X  ( z, w ) is the Mahalanobis distance between Proof. By Remark 2, we have  X  2  X  ( z, w ) = k  X  ( z )  X   X  ( w ) k 2 2 = K 1 = . . . = K d = K , we have k z 1 = . . . = k z d = k z . Therefore,  X  2  X  ( z, w ) = ( k z  X  k w ) T  X  ( k z  X  k w ) where  X  := The above result reduces the problem of learning  X  to learning a matrix,  X   X  0 , such that rank (  X  )  X  d and 1 T  X 1 = 0 . We now study the above result for linear ker-nels. The following corollary shows that applying a lin-ear kernel is equivalent to assuming the underlying distance metric in X to be the Mahalanobis distance.
 Corollary 4 (Linear kernel) . Let X = R D and z, w  X  X . If K ( z, w ) =  X  z, w  X  2 = z T w , then  X  ( z ) = L z  X  R d and Proof. By Remark 2 and Corollary 3, we have  X  m ( z ) = P In the following section, we use these results to derive an algorithm that jointly learns  X  and h by solving Eq. (5). Having addressed the theoretical issue of making assump-tions about C to solve Eq. (5), we return to address the com-putational issue pointed out in  X  2. The program in Eq. (5) is NP-hard because of the nature of {  X  i } n i =1 . This issue can be alleviated by minimizing a convex upper bound of  X  , instead of  X  i . Some of the convex upper bounds for the function  X  ( x ) = 1 { x&gt; 0 } are  X ( x ) = max(0 , 1 + x ) := [1 + x ] + ,  X ( x ) = log (1 + e x ) etc. Replacing  X  i by  X  Eq. (5) results in the following program, where  X  + i (  X , h ) := P ally hard to solve depending on the choice of the smooth-ing kernel, K . Even if we choose K such that  X  + and  X   X  are jointly convex in  X  and h for some representation of  X  (see Remark 2), Eq. (8) is still non-convex as the argument if  X ( x ) = [1 + x ] + , then Eq. (8) is a d.c. (difference of convex functions) program (Horst &amp; Thoai, 1999), which is NP-hard to solve. So, even for the nicest of cases, one has to resort to local optimization methods or computation-ally intensive global optimization methods. Nevertheless, if one does not worry about this disadvantage, then solv-ing Eq. (8) yields  X  (in terms of { c m } d m =1 or {  X  m depending on the chosen representation) and h that can be used in Eq. (2) to classify unseen data. However, in the following, we show that Eq. (8) can be turned into a con-vex program for the na  X   X ve kernel. As mentioned in  X  1, this choice of kernel leads to a classification rule that is similar in principle to the k -NN rule. 4.1. Na  X   X ve kernel: Semidefinite relaxation The na  X   X ve kernel, K x the points,  X  ( x ) , that lie within a ball of radius h centered at  X  ( x 0 ) have a weighting factor of 1 , while the remain-ing points have zero weight. Using this in Eq. (8), we have  X  i (  X , h )  X   X  P ference between number of points with label different from Y that lie within the ball of radius of h centered at  X  ( X and the number of points with the same label as X i (exclud-ing X i ) that lie within the same ball. If this difference is positive, then the classification rule in Eq. (2) makes an er-ror in classifying X i . Therefore,  X  and h should be chosen such that this misclassification rate is minimized. Suppose that {  X  ( X i ) } n i =1 is given. Then, h determines the misclas-sification rate like k in k -NN. It can be seen that the kernel classification rule and k -NN rule are similar when K is a na  X   X ve kernel. In the case of k -NN, the number of nearest neighbors are fixed for any point, whereas with the kernel rule, it varies for every point. On the other hand, the ra-dius of the ball containing the nearest neighbors of a point varies with every point in the k -NN setting while it is the same for every point in the kernel rule.  X  i (  X , h )  X   X  more amenable form by the following algebra. Us-ing P  X  i (  X , h ) = 1  X  n n neglected the set { j :  X  ij =  X  1;  X   X  ( X i , X j ) = h } in the above calculation for simplicity. Using  X ( x ) = [1 + x ] + , the first half of the objective function in Eq. (8) reduces to P relaxation one more time to the step function, we get P as an upper bound on the first half of the objective function in Eq. (8). Since  X  2  X  is a quadratic function of { c m } d m =1 , it can be shown that representing  X  in terms of { c m } d m =1 results in a d.c. program, whereas its repre-sentation in terms of {  X  m } d m =1 results in a semidefinite program (SDP) (except for the rank constraints), since  X  is linear in {  X  m } d m =1 . Assuming for simplicity that K 1 = . . . = K d = K and neglecting the constraint rank (  X  )  X  d , we obtain the following SDP, min s.t.  X   X  0 , 1 T  X 1 = 0 ,  X  h &gt; 0 , (9) details, refer to Remark 2 and Corollary 3. Since one does not usually know the optimal embedding dimension, d , the  X  representation is advantageous as it is independent of d (as we neglected the rank constraint) and depends only on n . On the other hand, it is a disadvantage as the algorithm does not scale well to large datasets.
 Although the program in Eq. (9) is convex, solving it by general purpose solvers that use interior point methods scales as O ( n 6 ) , which is prohibitive. Instead, following the ideas of Weinberger et al. (2006), we used a first order Algorithm 1 Gradient Projection Algorithm 1: Set t = 0 . Choose  X  0  X  X  and  X  h 0 &gt; 0 . 2: repeat 3: A t = { i : 4: B t = { ( i, j ) : 1 +  X  ij tr ( M ij  X  t ) &gt;  X  ij  X  5: N t = B t \ A t 7:  X  h t +1 = max(  X ,  X  h t +  X  t 8: t = t + 1 9: until convergence 10: return  X  t ,  X  h t gradient method (which scales as O ( n 2 ) per iteration) and an alternating projections method (which scales as O ( n 3 ) per iteration). At each iteration, we take a small step in the direction of the negative gradient of the objective function, followed by a projection onto the set N = {  X  :  X   X  0 , 1 T  X 1 = 0 } and {  X  h &gt; 0 } . The projection onto N is performed by an alternating projections method which in-volves projecting a symmetric matrix alternately between the convex sets, A = {  X  :  X   X  0 } and B = {  X  : 1 T  X 1 = 0 } . Since A X  X  6 =  X  , this alternating projec-tions method is guaranteed to find a point in A X  X  . Given any A 0  X  A , the alternating projections algorithm com-putes B m = P B ( A m )  X  X  , A m +1 = P A ( B m )  X  X  , m = 0 , 1 , 2 , . . . , where P A and P B are the projection on A and B , respectively. In summary, the update rule can be given as B values of B m . 10 A pseudocode of the gradient projection algorithm to solve Eq. (9) is shown in Algorithm 1. Having computed  X  and  X  h that minimize Eq. (9), a test point, x  X  X , can be classified by using the kernel rule in Eq. (2), where K X ( k x  X  k X i ) T  X  ( k x  X  k X i ) . Therefore,  X  and h completely specify the classification rule. In this section, we compare the performance of our method (referred to as kernel classification rule (KCR)) to several metric learning algorithms on a supervised classification task in terms of the training and test errors. The training phase of KCR involves solving the SDP in Eq. (9) to learn optimal  X  and h from the data, which are then used in Eq. (2) to classify the test data. Note that the SDP in Eq. (9) is obtained by using the na  X   X ve kernel for K in Eq. (2). For other smoothing kernels, one has to solve the program in Eq. (8) to learn optimal  X  and h . Therefore, the results re-ported in this section under KCR refer to those obtained by using the na  X   X ve kernel.
 The algorithms used in the comparative evaluation are:  X  The k -NN rule with the Euclidean distance metric.  X  The LMNN (large margin nearest neighbor) method  X  The Kernel -NN rule, which uses the empirical kernel  X  The KMLCC (kernel version of metric learning by  X  The KLMCA (kernel version of large margin compo-Four benchmark datasets from the UCI machine learning repository were considered for experimentation. Since the proposed method and KMLCC solve an SDP that scales poorly with n , we did not consider large problem sizes the average performance over 20 random splits of the data with 50% for training, 20% for validation and 30% for test-for the kernel based methods, i.e., Kernel -NN, KMLCC, KLMCA and KCR. The parameters  X  and  X  (only  X  for Kernel -NN) were set with cross-validation by searching over  X   X  { 2 i } 4  X  4 and  X   X  { 10 i } 3  X  3 . While testing, KCR uses the rule in Eq. (2), whereas the k -NN rule was used KCR almost always performs as well as or significantly better than all other methods. However, on the timing front (which we do not report here), KLMCA, which solves a non-convex program for n  X  d variables, is much faster than KMLCC and KCR, which solve SDPs involving n 2 vari-ables. The role of empirical kernel maps is not clear as there is no consistent behavior between the performance accuracy achieved with k -NN and Kernel -NN.
 KMLCC, KLMCA, and KCR learn the Mahalanobis dis-tance metric in R n which makes it difficult to visualize the class separability achieved by these methods. To vi-sually appreciate their behavior, we generated a synthetic two dimensional dataset of 3 classes with each class being sampled from a Gaussian distribution with different mean and covariance. Figure 1(a) shows this dataset where the three classes are shown in different colors. Using this as training data, distance metrics were learned using KMLCC, KLMCA and KCR. If  X  is the learned metric, then the two dimensional projection of x  X  R n is obtained as b x = L x where L = ( and  X  1  X   X  2 &gt;  X  X  X   X  n . Figure 1(b-d) show the two di-mensional projections of the training set using KMLCC, KLMCA and KCR. The projected points were classified using k -NN if  X  was obtained from KMLCC/KLMCA and using Eq. (2) if  X  was obtained from KCR. The misclas-sified points are shown in bold. Since the classification is done on the training points, one would expect better error rate and separability between the classes. To understand the generalization performance, a new data sample shown as the training set. The learned  X  was used to obtain the two dimensional projections of the new data sample which KLMCA and KCR generate completely different projec-tions but have similar error rates. We briefly review some relevant work and point out simi-larities and differences with our method. In our work, we have addressed the problem of extending kernel classifica-tion rules to arbitrary metric spaces by learning an embed-ding function that embeds data into Euclidean space while minimizing an upper bound on the resubstitution estimate of the error probability. The method that is closest in spirit (kernel rules) to ours is the recent work by Weinberger and Tesauro (2007) who learn a Mahalanobis distance metric for kernel regression estimates by minimizing the leave-one-out quadratic regression error of the training set. With the problem being non-convex, they resort to gradient de-scent techniques. Except for this work, we are not aware of any method related to kernel rules in the context of distance metric learning or learning the bandwidth of the kernel. There has been lot of work in the area of distance metric learning for k -NN classification, some of which are briefly discussed in  X  5. The central idea in all these methods is that similarly labeled examples should cluster together and be far away from differently labeled examples. Shalev-Shwartz et al. (2004) proposed an online algorithm for learning a Mahalanobis distance metric with the constraint that any training example is closer to all the examples that share its label than to any other example of different label. In addition, examples from different classes are constrained to be separated by a large margin. Though Shalev-Shwartz et al. (2004) do not solve this as a batch optimization prob-lem, it can be shown that it reduces to an SDP (after rank relaxation) and is in fact the same as Eq. (9) except for the outer [ . ] + function and the constraint 1 T  X 1 = 0 . In this paper, two questions related to the smoothing ker-nel based classification rule have been addressed. One is related to learning the bandwidth of the smoothing kernel, while the other is to extending the classification rule to ar-bitrary domains. We jointly addressed them by learning a function in a reproducing kernel Hilbert space while mini-mizing an upper bound on the resubstitution estimate of the error probability of the kernel rule. For a particular choice of the smoothing kernel, called the na  X   X ve kernel, we showed that the resulting rule is related to the k -NN rule. Because of this relation, the kernel rule was compared to k -NN and its state-of-the-art distance metric learning algorithms on a supervised classification task and was shown to have com-parable performance to these methods. In the future, we would like to develop some theoretical guarantees for the proposed method along with extending it to large-scale ap-plications.
 We need the following result to prove Theorem 1.
 Lemma 5. Let H = { f : X  X  R } be an RKHS with K : X  X X  X  R as its reproducing kernel. Let  X  : R n 2  X  R be an arbitrary function. Then each minimizer f  X  X  of admits a representation of the form f = where { c i } n i =1  X  R and Proof. The proof follows the generalized representer the-orem (Sch  X  olkopf et al., 2001, Theorem 4). Since f  X  H , f ( x ) =  X  f, K ( ., x )  X  H . Therefore, the argu-ments of  X  in Eq. (10) are of the form { X  f, K ( ., x K ( ., x j )  X  H } n i,j =1 . We decompose f = f k + f  X  so that f k  X  span  X  f  X  , K ( ., x i )  X  K ( ., x j )  X  H = 0 ,  X  i, j  X  [ n ] . So, f = P i,j =1  X  ij ( K ( ., x i )  X  K ( ., x j )) + f  X  where {  X  R . Therefore, f ( x i )  X  f ( x j ) =  X  f, K ( ., x i )  X  K ( ., x  X  f k , K ( ., x i )  X  K ( ., x j )  X  H = the penalty functional,  X  f, f  X  H . For all f  X  ,  X  f, f  X  || f Thus for any fixed  X  ij  X  R , Eq. (10) is minimized for f  X  = 0 . Therefore, the minimizer of Eq. (10) has the form f = rameterized by n 2 parameters of {  X  ij } n i,j =1 . By simple algebra, f reduces to f = P We are now ready to prove Theorem 1.
 Proof of Theorem 1. The arguments of  X  h in k  X  ( X i )  X   X  ( X j ) k 2 2 = P izer in Eq. (6) reduces to k  X  k 2 C = fore, applying Lemma 5 to each  X  m , m  X  [ d ] proves the result.
 We thank the reviewers for their comments which greatly improved the paper. We wish to acknowledge support from the National Science Foundation (grant DMS-MSPA 0625409), the Fair Isaac Corporation and the University of California MICRO program.

