 MASAO UTIYAMA, EIICHIRO SUMITA, and AKIHIRO TAMURA , National Institute of Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders.

To address this problem, there has been a lot of research done into word reorder-ing: lexical reordering model [Tillman 2004], which is one of the distortion models, reordering constraints [Zens et al. 2004], pre-ordering [Xia and McCord 2004], hierarchical phrase-based SMT [Chiang 2007], and syntax-based SMT [Yamada and Knight 2001].

In general, source language syntax is useful for handling long distance word reorder-ing. However, obtaining syntax requires a syntactic parser, which is not available for many languages. Phrase-based SMT [Koehn et al. 2007] is a widely used SMT method that does not use a parser.

Phrase-based SMT mainly 1 estimates word reordering using distortion models. Therefore, distortion models are one of the most important components for phrase-based SMT. There are methods other than distortion models for improving word reordering for phrase-based SMT, such as pre-ordering or reordering constraints. However, these methods also use distortion models when translating by phrase-based SMT. Therefore, distortion models do not compete against these methods and are com-monly used with them. If a distortion model improves, it will improve the translation quality of phrase-based SMT and will benefit the methods using distortion models.
In decoding by phrase-based SMT, a distortion model estimates the source word position to be translated next (SP) given the last translated source word position (CP). In order to estimate the SP given the CP, many elements need to be considered: the word at the CP, the word at an SP candidate (SPC), the words surrounding the CP and an SPC (context), the relative word order among the SPCs, and the words between the CP and an SPC. In this article, these elements are called rich context . The major challenge of distortion modeling is consideration of all of the rich context. Previous distortion models could not consider all of the rich context simultaneously. This is because the learning strategy for existing methods was that the models learned probabilities in all of the training data. This meant that the models did not learn pref-erence relations among SPCs in each sentence of the training data. Consequently, it is hard to consider all of the rich context simultaneously using this learning strat-egy. The MSD lexical reordering model [Tillman 2004] and a discriminative distortion model [Green et al. 2010] could not simultaneously consider both the word specified at the CP and the word specified at an SPC, or consider relative word order. There is a distortion model that used the word at the CP and the word at an SPC [Al-Onaizan and Papineni 2006], but this model did not use context, relative word order, or words between the CP and an SPC. All of these elements are important, and the reasons for their importance will be detailed in Section 2.

In this article 3 , we propose a new distortion model consisting of one probabilistic model and which does not require a parser for phrase-based SMT. In contrast to the learning strategy of existing methods, our learning strategy is that the model learns preference relations among SPCs in each sentence of the training data. This leaning strategy enables consideration of all of the rich context simultaneously. Our proposed model, the sequence model , can simultaneously consider all of the rich context by iden-tifying the label sequence that specifies the span from the CP to the SP. It enables our model to learn the effect of relative word order among the SPCs as well as learn the effect of distances from the training data. Experiments confirmed the effectiveness of our method for Japanese-English, Chinese-English, and German-English translation using NTCIR-9 Patent Machine Translation Task data [Goto et al. 2011], NIST 2008 Open MT task data, and WMT 2008 Europarl data [Callison-Burch et al. 2008]. A Moses-style phrase-based SMT [Koehn et al. 2007] generates target hypotheses se-quentially from left to right. Therefore, the role of the distortion model is to estimate the source phrase position to be translated next whose target side phrase will be lo-cated immediately to the right of the already generated hypotheses. An example is shown in Figure 1. In Figure 1, we assume that only the kare wa (English:  X  X e X ) has been translated. The target word to be generated next will be  X  X ought X , and the source word to be selected next will be its corresponding Japanese word katta . Thus, a distor-tion model should estimate phrases including katta as a source phrase position to be translated next.

To explain the distortion model task in more detail, we need to redefine two terms more precisely, the current position (CP) and subsequent position (SP) in the source sentence. CP is the source sentence position corresponding to the rightmost aligned target word in the generated target word sequence. SP is the source sentence position corresponding to the leftmost aligned target word in the target phrase to be generated next. The task of the distortion model is to estimate the SP for each CP. 5
It is difficult to estimate the SP. Figure 2 shows examples of sentences that are sim-ilar yet have different SPs, with the superscript numbers indicating the word position in the source sentence.

In Figure 2(a), the SP is 8. However, in 2(b), the word ( kare )attheCPisthesameas 2(a), but the SP is different (the SP is 10). From these example sentences, we see that distance is not the essential factor in deciding an SP. We can also see that the word at the CP alone is not enough to estimate the SP. Thus, it is not only the word at the CP, but also the word at an SP candidate (SPC) that should be considered simultaneously.
In Figures 2(c) and 2(d), the word ( kare )attheCPisthesameand karita (borrowed) and katta (bought) are at the SPCs. Karita is the word at the SP for 2(c), while katta , the relative word order between words. Thus, we can see that considering relative word order, not just looking at what the word at the SP is, is important for estimating the SP. 6
In Figures 2(d) and 2(e), kare (he) is at the CP for both, and the word order between katta and karita are the same. However, the word at the SP for 2(d) and the word at the SP for 2(e) are different, which shows us that selecting a nearby word is not always correct. The difference is caused by the words surrounding the SPCs (context), the CP context, and the words between the CP and the SPC. Thus, these should all be considered when estimating the SP.

In order to estimate the SP, the following should be considered simultaneously: the word at the CP, the word at an SPC, the relative word order among the SPCs, the words surrounding the CP and an SPC (context), and the words between the CP and an SPC. In other words, rich context should be considered simultaneously.
Returning back to the distribution models, there are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses [Koehn et al. 2007], whose costs are linearly proportional to the reordering distance, al-ways gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model [Tillman 2004; Koehn et al. 2005; Galley and Manning 2008] only calculates probabilities for the three types of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and an SPC. Thus, these models are not sufficient for long-distance word re-ordering.

Xiong et al. [2006] proposed distortion models that used context to predict the orien-tations { left, right } of the SP for their CYK-style decoder. Zens and Ney [2006] proposed distortion models that used context to predict four classes discontinuous } . Green et al. [2010] extended the distortion models to use finer classes. Green et al. X  X  [2010] model (the outbound model) estimates how far the SP should be from the CP using the word at the CP and its context. 7 Feng et al. [2013] also predicted those finer classes using a CRF model. These models do not simultaneously consider both the word specified at the CP and the word specified at an SPC, nor do they con-sider relative word order.

Al-Onaizan and Papineni [2006] proposed a distortion model that used the word at the CP and the word at an SPC. However, their model did not use context, relative word order, or words between the CP and an SPC.

There is a method that adjusts the linear distortion cost using the word at the CP and its context [Ni et al. 2009]. This model does not simultaneously consider both the word specified at the CP and the word specified at an SPC.

In contrast, our distortion model, the sequence model, addresses the aforementioned issues and utilizes all of the rich context. In this section, we first define our distortion model and explain our learning strategy. Then, we describe two models: the pair model and the sequence model . The pair model is our base model and the sequence model is our main proposed model. Our distortion model is defined as the model calculating the distortion probability. In this article, distortion probability is defined as which is the probability of j being the SP, where i is a CP, j is an SPC, S is a source sentence, and X is the random variable of the SP. We train this model as a discriminative model that discriminates the SP from SPCs. subject to The model parameters are learned to maximize the distortion probability of the SP among all of the SPCs J in each source sentence. This learning strategy is a type of preference relation learning [Evgeniou and Pontil 2002]. In this learning, the distor-tion probability of the actual SP will be relatively higher than those of all the other SPCs J .
 This learning strategy is different from that of Al-Onaizan and Papineni [2006] and Green et al. [2010]. Green et al. [2010], for example, trained their outbound model subject to c  X  C P ( Y = c | i , S ) = 1, where C is a set of nine distortion classes Y is the random variable of the correct distortion class that the correct distortion is classified into. Distortion is defined as j  X  i  X  1. Namely, the model probabilities that they learned were the probabilities of distortion classes in all of the training data, not the relative preferences among the SPCs in each source sentence. The pair model , which is our base model, utilizes the word at the CP, the word at an SPC, and the context of the CP and the SPC simultaneously to estimate the SP. This can be done using our distortion model definition and the learning strategy described in the previous section.

In this work, we use the maximum entropy method [Berger et al. 1996] as a dis-criminative machine learning method. The reason for this is that a model based on the maximum entropy method can calculate probabilities. However, if we use scores as an approximation of the distortion probabilities, various discriminative machine learning methods can be applied to build the distortion model.

Let s be a source word and s n 1 = s 1 s 2 ... s n be a source sentence. We add a beginning of sentence (BOS) marker to the head of the source sentence and an end of sentence (EOS) marker to the end, so the source sentence S is expressed as s s + 1 = EOS). Our distortion model calculates the distortion probability for an SPC j  X  X  j | 1  X  j  X  n + 1  X  j = i } for each CP i  X  X  i | 0  X  i where w is a weight parameter vector, and each element of f (  X  which returns 1 when its feature is matched and if else, returns 0. Z factor, o is an orientation of i to j ,and d is a distance class.

Table I shows the feature templates used to produce features. A feature is defined as an instance of a feature template. Using example (a) from Figure 2 will show some instances of each variable, where i = 2and j = 8: o = s = katta , t of o , s i , s j is o = 1, s i = kare , s j = katta and a feature of o , s wa , s j = katta .

In Equation (2), i , j ,and S are used by the feature functions. Thus, Equation (2) can utilize features consisting of both s i , which is the word specified at i ,and s is the word specified at j , or both the context of i and the context of j simultaneously. Distance is considered using the distance class d . Distortion is represented by distance and orientation. The pair model considers distortion using six joint classes of d and o . The pair model does not consider relative word order among the SPCs nor all the words between the CP and an SPC. Our main proposed model, the sequence model ,whichis described in this section, considers rich context, including relative word order among the SPCs and including all the words between the CP and an SPC.

In Figures 2(c) and 2(d), karita (borrowed) and katta (bought) both occur in the source sentences. The pair model considers the effect of distances using only the dis-tance class d . If these positions are in the same distance class, the pair model cannot consider the differences in distances. In this case, these are conflict instances during training and it is difficult to distinguish the SP for translation. However, this problem can be solved if the model can consider the relative word order.

The sequence model considers the relative word order. It does this by discriminating the label sequence corresponding to the SP from the label sequences corresponding to each SPC in each sentence. Since each label sequence corresponds to one SPC, if we can identify the label sequence that corresponds to the SP, then we can obtain the SP. The label sequences specify the spans from the CP to each SPC using three kinds of labels that indicate the type of word positions in the spans. The three kinds of labels,  X  X , I, and S, X  are shown in Table II. Figure 3 shows examples of the label sequences for Figure 2(c). The label sequences are represented by boxes and the elements of the sequences are labels. The SPC is used as the label sequence ID for each label sequence.
The label sequence can handle relative word order. Looking at Figure 3, the label sequence ID of 10 knows that karita exists to the left of the SPC of 10. This is because karita 6 carries a label I, while katta 10 carries a label S, and a position with label I is defined as relatively closer to the CP than a position with label S. By utilizing the label sequence and corresponding words, the model can reflect the effect of karita existing between the CP and the SPC of 10 on the probability.

Karita (borrowed) and katta (bought) in Figures 2(c) and 2(d) are not conflict in-training for the pair model. The reason is because it is necessary to make the prob-ability of the SPC of 10 smaller than that of the SPC of 6. The pair model tries to make the weight parameters for features with respect to katta smaller than those for features with respect to karita for 2(c), but it also tries to make the weight parameters for features with respect to karita smaller than those for features with respect to katta for 2(d). Since they have the same features, this causes a conflict. In contrast, the se-quence model can give negative weight parameters for the features with respect to the word at the position of 6 with label I, instead of making the weight parameters for the features with respect to the word at the position of 10 with label S smaller than those of 6 with label S.

We use a sequence discrimination technique based on CRF [Lafferty et al. 2001] to identify the label sequence that corresponds to the SP. 9 tween our task and the CRF task. One difference is that CRF identifies label sequences that consist of labels from all of the label candidates, whereas we constrain the label sequences to sequences where the label at the CP is C, the label at an SPC is S, and the labels between the CP and the SPC are I. The other difference is that CRF is designed for discriminating label sequences corresponding to the same object sequence, whereas we do not assign labels to words outside the spans from the CP to each SPC. However, when we assume that another label such as E has been assigned to the words outside the spans and there are no features involving label E, CRF with our label constraints can be applied to our task. In this article, the method designed to discriminate label sequences corresponding to the different word sequence lengths is called partial CRF . The sequence model based on partial CRF is derived by extending the pair model. We introduce the label l and add two extensions to the pair model to identify the label sequences corresponding to the SP. One of the extensions uses labels and the other uses sequence. For the extension using labels, we suppose that label sequences specify the spans from the CP to each SPC using the labels in Table II. We conjoin all the feature templates in Table I with an additional feature template l a feature template of o , s i + 1 , s j , l i , l j is derived by conjoining o , s l , l j . The other extension uses sequence. In the pair model, the position pair of ( i , j ) is used to derive features. In contrast, to discriminate label sequences in the sequence model, the position pairs of ( i , k ) , k  X  X  k | i &lt; k k &lt; j  X  j &lt; k  X  i } are used to derive features. Note that in the feature templates in Table I, i and j are used to specify two positions. When features are used for the sequence model, a value of k is used as one of the two positions. For example, for the position pairs of ( i , k ) , the value of s k is used as the value of s used as the value of l j in the feature template of o , s for each k . This is conducted by interpreting the parameters of f ( when the feature templates are used to derive features in the following Equations (3) and (4).

The distortion probability for an SPC j being the SP given a CP i and a source sen-tence S is calculated as where and Since j is used as the label sequence ID, discriminating X non-SPs.

The first term in exp (  X  ) in Equation (3) considers all of the word pairs located at i and other positions in the sequence, and also their context. The second term in exp ( in Equation (3) considers all of the word pairs located at j and other positions in the sequence, and also their context.

By designing our model to discriminate among different length label sequences, our model can naturally handle the effect of distances. Many features are derived from a long label sequence because it will contain many labels between the CP and the SPC. On the other hand, fewer features are derived from a short label sequence because a short label sequence will contain fewer labels between the CP and the SPC. The bias from these differences provides important clues for learning the effect of distances. In order to train our discriminative distortion model, supervised training data built from a parallel corpus and word alignments between corresponding source words and target words is necessary. Figure 4 shows examples of this training data. We create the training data by selecting the target words aligned to the source words sequentially from left to right (target side arrows), then deciding on the order of the source words in the target word order (source side arrows). The source sentence and the source side arrows are the training data. Japanese to English (JE), Chinese to English (CE), and German to English (GE) trans-lation experiments. 11 We used the patent data from the NTCIR-9 Patent Machine Translation Task [Goto et al. 2011] for JE and CE translation. There were 2,000 sentences for the test data and 2,000 sentences for the development data. The reference data is single reference. The translation model was trained using sentences of 40 words or less from the train-ing data. So approximately 2.05 million sentence pairs consisting of approximately 54 million Japanese tokens whose lexicon size was 134k and 50 million English tokens whose lexicon size was 213k were used for JE. Approximately 0.49 million sentence pairs consisting of 14.9 million Chinese tokens whose lexicon size was 169k and 16.3 million English tokens whose lexicon size was 240k were used for CE.

We also used the newswire data from the NIST 2008 Open MT task translation. There were 1,357 sentences for the test data. The reference data is multi-reference (4 references). We used the NIST 2006 test set consisting of 1,664 test sen-tences as the development data. The translation model was trained using sentences of 40 words or less from the training data. So approximately 2.19 million sentence pairs consisting of 18.4 million Chinese tokens whose lexicon size was 907k and 20.7 million English tokens whose lexicon size was 932k were used.

We used the Europarl data from the WMT 2008 [Callison-Burch et al. 2008] trans-lation task for GE translation. There were 2,000 sentences for the test data. The refer-ence data is single reference. We used the WMT 2007 test set consisting of 2,000 test sentences as the development data. The translation model was trained using sentences of 40 words or less from the training data. So approximately 1.00 million sentence pairs consisting of 20.4 million German tokens whose lexicon size was 226k and 21.4 million English tokens whose lexicon size was 87k were used. MeCab 14 was used for the Japanese morphological analysis. We adjusted the tokeniza-tion of the alphanumeric characters in Japanese to be the same as for the English. The Stanford segmenter 15 and tagger 16 were used for Chinese segmentation and POS tagging and for German POS tagging. GIZA++ and grow-diag-final-and heuristics were used to obtain word alignments. In order to reduce word alignment errors, we removed articles { a, an, the } in English, particles { ga , wo , wa das, des, dem, den, ein, eine, eines, einer, einem, einen word alignments because these function words do not correspond to any words in the other languages (JE and CE) or articles do not always correspond like content words or prepositional words (GE). After word alignment, we restored the removed words and shifted the word alignment positions to the original word positions. We used 5-gram language models with modified Kneser-Ney discounting [Chen and Goodman 1998] us-ing SRILM [Stolcke et al. 2011]. The language models were trained using the English side of each set of bilingual training data.

We used an in-house standard phrase-based SMT system compatible with the Moses decoder [Koehn et al. 2007]. The phrase table and the lexical distortion model were built using the Moses tool kit. The SMT weighting parameters were tuned by MERT [Och 2003] using the development data. The tuning was based on the BLEU score [Papineni et al. 2002]. To stabilize the MERT results, we tuned the parameters three times by MERT using the first half of the development data and we selected the SMT weighting parameter set that performed the best on the second half of the development data based on the BLEU scores from the three SMT weighting parameter sets.
We compared systems that used a common SMT feature set from standard SMT features and different distortion model features. The common SMT feature set consists of four translation model features, phrase penalty, word penalty, and a language model feature. The compared different distortion model features are as follows.  X  The linear distortion cost model feature (L INEAR )  X  The linear distortion cost model feature and the six MSD bidirectional lexical dis-tortion model [Koehn et al. 2005] features (L INEAR +L EX  X  The outbound and inbound distortion model features discriminating nine distortion classes [Green et al. 2010] (9-CLASS )  X  The proposed pair model feature (P AIR )  X  The proposed sequence model feature (S EQUENCE ) Our distortion model was trained as follows: We used 0.2 million sentence pairs and their word alignments from the data used to build the translation model as the training data for our distortion models. The features that were selected and used were the ones that had been counted 17 , using the feature templates in Table I, at least four times for all of the ( i , j ) position pairs in the training sentences. We conjoined the features with three types of label pairs l i = C, l j = I , l to produce features for S EQUENCE . The L-BFGS method [Liu and Nocedal 1989] was used to estimate the weight parameters of maximum entropy models. The Gaussian prior [Chen and Rosenfeld 1999] was used for smoothing. 18 For 9-CLASS , we used the same training data as for our distortion models. We used the following feature templates to produce features for the outbound model: s for s i . These feature templates correspond to the components of the feature templates of our distortion models. In addition to these features, we used a feature consisting The relative source sentence position is discretized into five bins, one for each quintile of the sentence. For the inbound model 19 , i of the feature templates was changed to maximum entropy method with Gaussian prior smoothing was used to estimate the model parameters.

The MSD bidirectional lexical distortion model was built using all of the data used to build the translation model. We evaluated translation quality based on the case-insensitive automatic evaluation score BLEU-4 [Papineni et al. 2002] and RIBES v1.01 [Isozaki et al. 2010a]. RIBES is an automatic evaluation measure based on word order correlation coefficients between reference sentences and translation outputs. We used distortion limits of 10, 20, 30, and unlimited (  X  ), which limited the number of words for word reordering to a max-imum number for JE and CE. We used distortion limits of 6, 10, and 20 for GE. Our main results are presented in Tables III to VI. The values given are case-insensitive scores. Bold numbers indicate no significant difference from the best result in each language pair and in each evaluation measure using the bootstrap resampling test at a significance level  X  = 0.01 [Koehn 2004].
The proposed S EQUENCE outperformed the baselines for Japanese to English, Chi-nese to English, and German to English translation for both BLEU and RIBES. demonstrates the effectiveness of the proposed S EQUENCE . thought to be better than the compared methods for local word ordering since BLEU is sensitive to local word order. The proposed method is also thought to be better than the compared methods for global word ordering since RIBES is sensitive to global word or-der. The BLEU and RIBES scores of the proposed S EQUENCE were higher than those of the proposed P AIR . This confirms its effectiveness in considering relative word or-der and words between the CP and an SPC. The proposed P AIR for both BLEU and RIBES in most cases 22 , confirming that considering both the word specified at the CP and the word specified at the SPC simultaneously was more effec-tive than that of 9-CLASS .
 For translating between languages with widely different word orders such as Japanese and English, a small distortion limit is undesirable because there are cases where correct translations cannot be produced with a small distortion limit, since the distortion limit prunes the search space that does not fit within the constraint. There-fore, a large distortion limit is required to translate correctly. For JE translation, our S
EQUENCE achieved significantly better results at distortion limits of 20 and 30 than tems of L INEAR ,L INEAR +L EX ,and9-CLASS did not achieve this. This indicates that S
EQUENCE could treat long distance reordering candidates more appropriately than the compared methods.
 We also tested hierarchical phrase-based SMT [Chiang 2007] (H implementation [Hoang et al. 2009]. The common data was used to train H used unlimited max-chart-span for the system setting. Results are given in Table VII. Our S EQUENCE outperformed H IER for JE and achieved better than or comparable to H
IER for CE and GE. Since phrase-based SMT generally has a faster decoding speed than hierarchical phrase-based SMT, there is merit in achieving better or comparable scores.

To investigate how well S EQUENCE learns the effect of distance, we checked the average distortion probabilities for large distortions of j each distortion for S EQUENCE , and another is this for P probabilities for the actual distortions in the training data that were obtained from the word alignments used to build the translation model. The probability for a distortion for C ORPUS was calculated by the number of the distortion divided by the total number of distortions in the training data.

Figure 5 shows that when a distance class feature used in the model was the same (e.g., distortions from 5 to 20 had the same distance class feature), P average distortion probabilities that were almost the same. In contrast, the average distortion probabilities for S EQUENCE decreased when the lengths of the distortions increased even if the distance class feature was the same, and this behavior was the same as that of C ORPUS . This confirms that the proposed S effect of distances appropriately from the training data.
To investigate the effect of using the words surrounding the SPCs and the CP (con-text), we conducted experiments without using the words surrounding the SPCs and the CP for P AIR and S EQUENCE . The models without using the surrounding words were trained using only the features that did not contain context. Table VIII shows the results for Japanese-English translation. 24 Both the BLEU and RIBES scores for S
EQUENCE without using the words surrounding the SPCs and the CP (context) were lower than those for S EQUENCE using the words surrounding SPCs and the CP (con-text). There was a 1.5 point difference in the BLEU scores for S confirms that using the words surrounding the SPCs and the CP (context) was very effective.

To investigate the effect of using part of speech tags, we conducted experiments without using part of speech tags for P AIR and S EQUENCE part of speech tags were trained using only the features that did not contain part of speech tags. The results of this experiment for Japanese-English translation are shown in Table IX. Both the BLEU and RIBES scores for S EQUENCE speech tags were slightly lower than those using part of speech tags. There was a 0.5 point difference in the BLEU scores for S EQUENCE . This result confirms that using part of speech tags was slightly effective for S EQUENCE .

To investigate the training data sparsity tolerance, we reduced the training data for the sequence model to 100,000, 50,000, and 20,000 sentences for Japanese-English translation. 25 Figure 6 show the results for P AIR and S limit of 20 for P AIR and the best distortion limit of 30 for S used. To avoid effects from differences in the SMT weighting parameters, the same SMT weighting parameters used in Table III were used for each method. S using only 20,000 training sentences achieved a BLEU score of 32.22 and a RIBES score of 71.33. Although the scores are lower than the scores of S distortion limit of 30 in Table III, the scores were still higher than those of L L
INEAR +L EX ,and9-CLASS for JE in Table III. This indicates that the sequence model model considers not only the word at the CP and the word at an SPC but also rich context, and rich context would be effective even on a smaller set of training data. To investigate the effect of distortion limits for P AIR and S English translation more precisely, we conducted experiments using the same SMT weighting parameters to avoid the effects of differences in SMT weighting parame-ters. For all of the distortion limits of P AIR and S EQUENCE weighting parameters that were used for S EQUENCE with a distortion limit of 30 in Table III, which achieved the best scores in Table III. The results of this are given in Table X.

In Table III, the BLEU score for S EQUENCE with an unlimited distortion was lower than that with a distortion limit of 30. However, Table X shows that S an unlimited distortion achieved almost the same BLEU score as that achieved by S
EQUENCE with a distortion limit of 30. This indicates that the difference in BLUE scores for S EQUENCE between a distortion limit of 30 and an unlimited distortion in Table III was mainly caused by the difference in SMT weighting parameters. How-ever, although the RIBES score for S EQUENCE with an unlimited distortion in Table X was higher than that in Table III, the RIBES score for S ited distortion was still lower than that with a distortion limit of 30 in Table X. The RIBES score for S EQUENCE with a distortion limit of 30 was also lower than that with a distortion limit of 20 in Table X. This indicates that S handle long distance reordering over 20 or 30 words. For such long distance reorder-ing, incorporation with methods that consider sentence-level consistency, such as ITG constraint [Zens et al. 2004], would be useful. In this section, we will discuss related work other than that discussed in Section 2. phrase-based SMT [Cherry 2013]. However, since the training for this method depends on the SMT weight parameter tuning, the sparse features can only learn from the de-velopment data for the SMT weight parameter tuning and cannot utilize a large supply of word aligned training data. Thus, they viewed the sparse features as complemen-tary to existing distortion models. In contrast, our model utilizes a large supply of word aligned training data for training, and it can be built independently of the SMT weight parameter tuning. In addition, SMT sparse features do not calculate the probability of an SPC, whereas our model does. Since Cherry X  X  [2013] sparse features learn from the development data and our model learns from the training data with word alignments, if they are used together, then the SMT system can utilize both the development data and the training data with word alignments to learn reorderings.

There are also reordering models that use a parser: a linguistically annotated ITG [Xiong et al. 2008], a model predicting the orientation of an argument with respect to its verb using a parser [Xiong et al. 2012], and an MSD reordering model using a CCG parser [Mehay and Brew 2012]. However, none of these methods consider reordering distances. Structural information such as syntactic structures and predicate-argument structures are useful for reordering, but orientations do not handle distances. A distortion model considering distances of distortions is also useful for methods pre-dicting orientations using a parser when a phrase-based SMT is used, which means that our distortion model does not compete against methods predicting orientations using a parser, but would assist them if used together.

There are word reordering constraint methods that use ITG for phrase-based SMT sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and is used with other distortion models. Our distortion model straint methods are thought to be complementary.
 There are pre-ordering methods using a supervised parser [Dyer and Resnik 2010; Ge 2010; Genzel 2010; Isozaki et al. 2010b; Wang et al. 2007; Xia and McCord 2004] and methods that do not require a supervised parser [DeNero and Uszkoreit 2011; Neubig et al. 2012; Visweswariah et al. 2011]. These methods are not distortion mod-els, and a distortion model would be useful for their methods when a phrase-based SMT is used for translation.
 There are also tree-based SMT methods [Chiang 2007, 2010; Galley et al. 2004; Huang et al. 2006; Liu et al. 2006, 2009; Shen et al. 2008; Yamada and Knight 2001]. In many cases, tree-based SMT methods do not use distortion models that consider reordering distance apart from translation rules, because using distortion scores that consider the distances for decoders which do not generate hypotheses from left to right is not trivial. Our distortion model might contribute to tree-based SMT methods if it could be applied to these methods. Investigating the effects will be for future work. This article described our distortion models for phrase-based SMT. Our sequence model consists of only one probabilistic model, but it can consider rich context. In con-trast to the learning strategy of existing methods, our learning strategy is that the model learns preference relations among SPCs in each sentence of the training data. This leaning strategy enables consideration of all of the rich context simultaneously. Experiments indicated that our models achieved better performances as measured by both BLEU and RIBES for Japanese-English, Chinese-English, and German-English translation, and that the sequence model could learn the effect of distances appropri-ately. Since our models do not require a parser, they can be applied to many languages. Future work includes application to other language pairs, incorporation into ITG con-straint methods and other reordering methods, and application to tree-based SMT methods.

