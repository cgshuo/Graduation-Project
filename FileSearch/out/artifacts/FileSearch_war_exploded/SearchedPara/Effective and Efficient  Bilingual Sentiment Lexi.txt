 Bilingual sentiment lexicon is fundamental resource for cross-language sentiment analysis but its compilation remains a major bottleneck in computational linguistics. Traditional word alignment algorithm faces with the status of large alignment space, which may introduce redundant computations as well as alignment errors. In this paper, we use collocation alignment to extract bilingual sentiment lexicon overcoming the drawbacks of word alignment. The idea of collocation alignment is inspired by the strong cohesion between feature words and opinion words in sentiment corpus. Experimental results show that our approach not only decreases the computing time dramatically but also improves the precision of extracted bilingual word pairs due to the smaller alignment space. I.2.7 [ Artificial Intelligence ]: Natural Language Processing Algorithms, Experimentation, Performance bilingual sentiment lexicon, word alignment , collocation alignment language sentiment analysis has received a lot of attention recently. In cross-language sentiment analysis, bilingual sentiment lexicon is crucial resource, especially in unsupervised learning. However, it is hard to maintain the universal bilingual sentiment lexicons to cover all language pairs as opinion expressions vary significantly from language to language. Besides, constructing bilingual sentiment lexicon manually involves a lot of time and labor, thus it is necessary and challenging to extract bilingual sentiment lexicon automatically using statistical information of word features from a given set of corpora. corpora are based on EM (Expectation Maximization) algorithm. However, traditional word alignment based on EM algorithm suffers from two drawbacks: On one hand, the computational complexity is high. For a parallel sentence pair, if the sentence in source language has I words and sentence in target language has J words, it is necessary to compute ( I +1) J alignments (Och and Ney, 2003). On the other hand, the precision is low. In word alignment, only one alignment is correct per sentence pair, so the other alignments are redundant. The more redundant alignments computed, the more alignment errors introduced. and low precision, we substitute collocation alignment for word alignment aiming to improve the performance through two aspects: efficiency and effectiveness. In our work, a collocation denotes a word cluster as  X  X eature word|Opinion word X . A customer review usually contains a set of feature words and a set of opinion words, furthermore, there are strong associations between such feature words and opinion words, for example, feature word  X  X rice X  and opinion word  X  X xpensive X  are always co -occurred. Thus, we consider that the tight association of two kinds of words can offer heuristic information for collocation alignment. To be more specific, replacing word by collocation, the alignment space becomes smaller, so does the computing time. While reducing the time does not cause the loss of performance, instead, the performance enhances a lot because the cohesions between feature words and opinion words are strong and redundant alignments are less. steps: The first step is to mine collocations from both source language and target language. The second step is to align the collocations by statistical technique given the parallel corpora. The third step is to extract bilingual word pairs from bilingual collocation pairs. follows: (1) We propose an approach of bilingual sentiment lexicon (2) Comparing with traditional word alignment algorithm, (3) Fine-grained bilingual sentiment lexicon can be presented methods to extract bilingual lexicons of words from a parallel corpus, relying on the distribution of the words in the set of parallel sentences (or other units). Brown (1993) then extended their method and established a sound probabilistic model series, relying on different parameters describing how words within parallel sentences are aligned to each other. 1998) have tried to exploit comparable corpora for bilingual lexicon extraction. The main work in bilingual lexicon extraction from comparable corpora is based on lexical context analysis and relies on the simple observation that a word and its translation tend to appear in the same lexical contexts. components or attributes. In the work of Hu and Liu (2004) , frequent nouns and noun phrases are treated as product feature candidates. In our work, we extract only nouns as features. and expression levels. Hatzivassiloglou and McKeown (1997) proposed the first method for determining adjective polarities or orientations (positive, negative, and neutral). Wiebe (2000) proposed an approach to finding subjective adjectives using the results of word clustering according to their distributional similarity. Without loss of universality, we extract only adjective as opinion words. Though we only take all adjectives as opinion words, our approach can be extended to verbs, adverbs and adjective-noun phrases. opinion, sentiment or attitude of reviewers. Although some review units may express general opinions toward a target, most review units are regarding to specific features of the target (Su et al. 2008) . Figure 1: Collocation Alignment Using the set of Feature Seen from horizontal view, the left side denotes space of source language and the right side denotes space of target language. Seen from vertical view, the upper part denotes set of feature words and the lower part denotes set of opinion words. There are tight associations between feature words and opinion words, such as  X  X taff X  is associated with  X  X olite X  in English and  X   X  X  X  (staff) X  is associated with  X   X  X  X  (polite)  X  in Chinese. Suppose that we have already extracted collocations  X  X taff|polite X  and  X   X  X  X  |  X  X  X   X , if  X  X taff X  is aligned with  X   X  X  X   X , then  X  X olite X  is inclined to align with  X   X  X  X   X . It is the tight association between feature word and opinion word that helps us to reduce the alignment space and also the alignment errors.
 O={ o 1 , o 2 , ..., o n }. Then a weighted bipartite graph G from F and O can be built, denoted as G ( F , O , C ). Here, C =[ c weight matrix containing all the collocation weights between set F and O . The weight can be calculated with different weighting schemes. In this paper, we set c ij by Mutual Information score. The main idea of collocation mining is described in section 3.2. extract collocations from both source language and target language. Collocation is a typical lingual phenomenon of co-occurrence of words, and the typicality depends on the possibility of collocation. Any collocation is possible, but some are more appropriate than the others. In this paper, we adopt Mutual Information to compute the possibility of collocation, which me asures the influence degree of one word to the other in a collocation. The formula of Mutual Information is defined as: of word x and word y in the same sentence , p ( x ) denotes the possibility of occurrence of word x and p ( y ) denotes the possibility of occurrence of word y in the corpus. The collocation mining algorithm is described in Algorithm 1. Equation 1 collocations alignment and collocation alignment. Here, we take English as source language and Chinese as target language. As seen in Figure 2, the length (number of words) of English sentence is 9 and the length of Chinese sentence is 8. In this case, EM alignment model has to compute 9 9 different alignments. However, as seen in Figure 3, when word alignment is replaced by collocation alignment, the number of alignments is reduced to 2 . opinion words. When predicting the polarity of a review is positive (thumbs up) or negative (thumbs down), opinion words are more informative and discriminative than the other words. Though verbs and some other auxiliary words are neglected during collocation alignment, it has little effect on the sentiment analysis task. Furthermore, if high-precision aligned feature words and opinion words are acquired, they can be used as anchors to extract the other word pairs located between them. For example, if the word pairs  X  X taffs - X  X  X   X  and  X  X riendly - X  X   X  are already extracted, it X  X  easy to extract another word pair  X  X ery - X   X  between them. Figure 3: Bilingual Word Pairs Generated from Collocation follows. First, let X  X  define our problem: S = s 1 l = s 1 s 2 ... s l T = t 1 m = t 1 t 2 ... t m C = c 1 m = c 1 c 2 ... c m , c i  X  {0,..., l } translation possibility of sentence pair S and T is the sum of all possible alignments, which can be defined as: get a distribution of p ( t|s ) that maximizes the possibility of training data. In other words, given the bilingual parallel corpora, we want to derive a distribution of p ( t|s ) which can maximize the translation possibility. It is a constrained extremum problem, subject to the regularization of p ( t|s ). will converge to the global maximum regardless of the initial assignment. We initialize the translation possibility of the corresponding collocation pair with an average, subject to follows: ( | , ) P C T S with t in all possible alignments C . have a set of parameters p ( t|s ), we can compute the probability of all collocation alignments P(T,C|S) . If th e P(T,C|S) is derived, we can compute the new p ( t|s ). The M step can be divided into two tiny steps: The first step is to compute the sum of all possibilities sum according to s . bilingual collocation pairs. In a collocation, the first word denotes the feature word and the second word denotes the opinion word, thus we can get two bilingual word pairs from each collocation pair according to the sequential order. Figure 3 shows the result of collocation alignment and there are two bilingual collocation pairs, and we can derive four bilingual word pairs. However, there is some noise in bilingual parallel corp us, thus many extracted collocation pairs are incorrect. In order to enhance the precision of extracted word pairs, we further refine the collocation pairs. The refinement method is a bootstrapping like way. The refinement algorithm is described in Algorithm 2. is set to 1000 empirically. parallel corpora. The bilingual sentiment corpora are collected from http://www.booking.com/, and the parallel sentences are acquired from human assisted sentence alignment. In the experiment, the training data contains 10000 bilingual parallel sentence pairs. The English part-of -speech is tagged by Stanford POS tagger (http://nlp.stanford.edu/software/tagger.shtml) and the Chinese part-of -speech is tagged by ICTCLAS (http://ictclas.org/). performance of the proposed method, which is measured by counting the number of correctly aligned word pair s. We manually evaluate every aligned word pair. To justify the reliability of this labeling process, we ask three annotators to assess the accuracy. effectiveness. comparison experiments on the same computer and its configuration is: Dell Optiplex360; Intel(R) Pentium(R) Dual E2200@2.20GHZ; 1G memory. alignment approach is compared with two baseline methods: sentence pair is computed word by word instead of collocation by collocation. Besides, the sentence only contains nouns and adjectives for fairness. collocation alignment, a collocation is not composed of a feature word and an opinion word but composed of random two words. EM model with the same programming environment and technique to ensure the running time is comparable. Table 1 shows the running time of word alignment and collocation alignment in an iteration, and the running time is measured in seconds. 
Table 1: Comparison of running time of word alignment and during E step, so it is the most time-consuming part. The execution of M step depends upon the result of E step, and the more alignments derived from E step, the more time consumed during M step. In our experiment, we only selected bilingual parallel sentence pairs that are less than 10 words. Without long sentence pair filtering, the discrepancy of running time will expand more evidently. The reason why word alignment costs more time is due to its exponential computing complexity. The number of possible alignments is ( I +1) J , where I and J denote the length of sentence in source language and target language respectively. However, after collocation extraction, the sentence composed of words is regrouped into sentence composed of collocations. Accordingly, the computing complexity can be reduced dramatically because the number of collocations is approximately half of the number of words in a sentence. our approach with two baseline methods. Table 2 shows accuracy of top N percent extracted word pairs, where N increases from 25 to 75 with step width 25. Table 2: Comparison of accuracy of collocation alignment and pairs is poor, we discard the last 25% and build bilingual lexicon using the first 75% word pairs. As seen in Table 2, the specific collocation alignment based on feature word and opinion word performs best. In terms of the top 75% word pairs, our approach outperforms the baseline1 and baseline2 methods with a performance improvement of 15% and 5.7%. The reason why collocation alignment outperforms word alignment significantly is because that collocation alignment benefits from smaller alignment space. The alignment space of word alignment is quite large and there are exponential redundant possibilities. The more redundant possibilities the more errors will be introduced because only one alignment is correct. It can also be seen that random collocation alignment outperforms word alignment. This is because the random collocations are ranked by Mutual Information score before alignment in spite of random combination. The results show that the Mutual Information measure and collocation alignment are effective but not all collocations are helpful. It also proves that building a relationship between feature word and opinion word is appropriate for sentiment corpus and appropriate for bilingual lexicon generation based on sentiment corpus. automatically construct a bilingual sentiment lexicon. Consequently, we evaluate the accuracy of feature word and opinion word respectively. Table 3 shows the accuracy of bilingual sentiment lexicon extracted by our method and baseline methods. Results in Table 3 are generated from the top 75% word pairs as well. Table 3: Comparison of accuracy of collocation alignment and Feature word 0.706 0.766 0.805 Opinion w ord 0.608 0.779 0.847 the baseline1 and baseline2 methods with a performance improvement of 23.9% and 6.8% in terms of the opinion words. In bilingual sentiment lexicon generation, the advantage of specific collocation alignment over the baseline methods amplifies more. Additionally, it is easy to derive fine-grained bilingual sentiment lexicon because every collocation is a word cluster as  X  X eature word | opinion word X  and we can regard the secon d word as the description of the first word in a word cluster. For a given set of feature words, collecting all opinion words that combined with such feature word can generate the fine-grained bilingual sentiment lexicon. low precision on word alignment, we propose an approach based on collocation alignment. In our work, a collocation is a combination of a feature word and an opinion word. In customer reviews, opinion words are used to express sentiment or attitude and they are regarding to specific features of the target, thus there are tight cohesion between feature word and opinion word. We consider that the cohesion can be helpful to reduce the alignment space. Once the alignment space is diminished, the redundant alignments and computing time become less too. Moreover, less redundant alignments introduce less error during EM iteration. Our method can be divided into three steps: The first step is to extract collocations from both source language and target language. The second step is to align the collocations by statistical technique. The third step is to get bilingual word pairs from bilingual collocation pairs. Experiment al results show that our approach can not only decrease the computing time dramatically but also improve the precision of extracted bilingual word pairs. Except coarse bilingual sentiment lexicon, a fine-grained bilingual sentiment lexicon can be generated simultaneously through collocation alignment, which will be beneficial to fine-grained sentiment analysis. This work was mainly supported by two funds, i.e., the National Natural Science Foundation of China (60933005 &amp; 61100083 ). [1] Franz Josef Och, Hermann Ney. 2003. A Systematic [2] Gale, W A, Kenneth W.Church. 1993. A program for [3] Guang Qiu, Bing Liu et al. 2011. Opinion word expansion [4] Hatzivassiloglou, Vasileios et al. 1997. Predicting the [5] Hu Mingqing and Bing Liu. 2004. Mining and summarizing [6] Pascale Fung and Kenneth Church. 1994. Kvec: A new [7] Pascale Fung. 1998. A statistical view on bilingual lexicon [8] P.F.Brown, S.A.Della Pietra et al. 1993. The mathematics of [9] Qi Su, Xinying Xu et al. 2008. Hidden sentiment association [10] Wiebe, Janyce. 2000. Learning subjective adjective from 
