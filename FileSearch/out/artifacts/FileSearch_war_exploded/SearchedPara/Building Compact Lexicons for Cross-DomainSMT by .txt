 Pankaj Singh ( A statistical machine translation (SMT) model typically relies on the availabil-ity of a large parallel corpus, often collected from multiple sources and spanning different domains. While a domain-specific corpus might share some of its lexical characteristics with the cross-domain corpus, it often differs in its language usage and vocabulary. A cross-domain SMT model might, therefore, fail to reliably translate an in-domain text. While it is possible to train an in-domain transla-tion model, domain-specific parallel corpus is either non-existent or scarce and expensive to generate. The problem of domain adaptation deals with augment-ing a cross-domain translation model to reliably translate an in-domain text and poses an interesting research challenge [ 8 ].
 Although in-domain parallel text might be difficult to obtain, in-domain bilin-gual lexicons are often readily available or could be manually curated. Typically, these are restricted to words or short phrases specific to the domain of interest. A medical domain bilingual lexicon, for instance, consists of technical and popu-lar medical terminology covering the anatomy of body, certain diseases, medicines etc.

In addition to these however, a domain corpus, due to its specific language structure, is often replete with redundant phrases. Consider for instance, the phrase  X ... be given marketing authorisation  X , appearing 218 times in the EMEA medical corpus [ 23 ]. These, if extracted and translated in a bilingual lexicon, might aid in-domain translation [ 21 , 26 ]. In fact, repetition in a domain corpus could be further exploited by observing that certain phrases, which might themselves be infrequent, tend to have  X  X onsensus X  when generalized to higher-level pat-terns. Table 1 illustrates two patterns and corresponding sample phrases extracted from the EMEA medical domain corpus. These patterns are typically n-grams of tokens, domain-specific categories or higher-level phrase classes (noun phrase, verb phrase etc. ).
 terns to be manually translated. Moreover, in the absence of a parallel in-domain corpus, translation of these patterns requires manual effort, which poses other challenges. Specifically, syntactically well-formed patterns like treatment X  might be easier for humans to translate than others like condition has X  . Chen et. al. [ 3 ] present this and other quality criteria that every pattern must satisfy to be worth being translated in order to aid cross-domain SMT applications. We will refer to such patterns as quality patterns work, we generalize the search space of patterns as well as the quality criteria that a pattern must meet.
 significantly overlap in their spans in the corpus. Is translating each such ity pattern really necessary? We expect the human effort for translating pat-terns to have a budget constraint and therefore, a compact desirable. For example, it is desirable to extract a set of patterns (for bilin-gual lexicon), such that, the set maximally covers the corpus. We argue that some formulations of this problem are natural instances of submodular max-imization. A set function f ( . ) is said to be submodular if for any element v and sets A  X  B  X  V \{ v } , where V represents the ground set of elements, f( A  X  X  v } )  X  f( A )  X  f( B  X  X  v } )  X  f( B ). This is called the diminishing returns property and states, informally, that adding an element to a smaller set increases the function value more than adding that element to a larger set. Submodular functions naturally model notions of coverage and diversity, and therefore, a number of subset selection problems can be modeled as forms of submodular optimization [ 7 , 11 ].
 We illustrate the relevance of the submodular coverage function to pattern-subset selection in Fig. 1 . We plot the corpus coverage (in terms of number of words) with increasing number of patterns in the set, for pattern lengths varying from 3 to 9. In each case, while the coverage improves with increasing number of patterns, the gain in coverage progressively diminishes with growth in the size of the subset.
 Our contribution is a framework to curate a high quality bilingual lexicon based on three key ideas. Our first two ideas generalize the approach of Chen et al. [ 3 ]. 1. Language of patterns: A pattern could either be lexical, comprised of words alone, or it could be a combination of words and higher-level categories. 2. Quality criteria for a pattern: The quality (or cost) of every instance of a pattern is a function of several features including its frequency in the corpus and whether or not it is syntactically well-formed. The quality (or cost) of a pattern is then a simple (modular) aggregation of the instance costs. 3. Quality criteria for a set of patterns: We define the  X  X oodness X  of a set of patterns based on element-wise non-decomposable submodular costs. We incorporate these patterns along with their translations, as entries in a bilin-gual lexicon and study 1 its effect on the translation accuracy for the domain adaptation of a baseline SMT model. While significantly improving over the baseline, we also show significant improvement over the modular setting of Chen et al. Extraction of Bilingual Multi-word Expressions (BMWE): SMT sys-tems often use word-to-word alignment approaches for inferring translation prob-abilities from bilingual data [ 17 , 25 ]. However, in some cases it might not be possi-ble to perform word-to-word alignment between two phrases that are translations of each other [ 10 ]. This has motivated a body of work [ 10 , 18 , 21 ] on automatic extraction of multi-word expressions from bilingual corpora. Ren pose multiple techniques to integrate BMWE X  X  into a phrase-based SMT system and show improvement over the baseline translation system. Recently, Liu [ 12 ] proposed an approach to mine quality phrases from large text corpora. They use a phrasal segmentation-based approach for phrase mining and combine that with several phrase quality assessment metric in a scalable framework. While our approach is inspired by these works, we differ from them in that we aim to extract generalized patterns comprising words and categories. Also, we do not assume availability of a parallel corpus in the target domain.
 Domain Adaptation: Typically, the application domain of a translation sys-tem might be different from the domain of the system X  X  training data. In-domain parallel corpus might either be non-existent or scarce, but, in-domain mono-lingual corpus is usually available. The problem of domain adaptation therefore been in focus and there has been work [ 8 , 16 , 26 ] to build in-domain translation lexicons and combine them with out-of-domain parallel corpus to achieve in-domain translation. Koehn and Schroeder [ 8 ] use limited in-domain parallel corpus to train a language model and a translation model and present techniques to integrate them with corresponding models trained on an out-of-domain corpus. Wu et al. [ 26 ] manually create an in-domain lexicon where the lexicon entries are restricted to words. They propose an algorithm to combine an out-of-domain bilingual corpus, an in-domain bilingual lexicon, and monolingual in-domain corpora in a unified framework for in-domain translation. Pattern Mining: The other body of work most related to our approach comes from the area of pattern mining. While most earlier work [ 22 ] dealt with identi-fying consecutive word sequences, Joshi et al. [ 6 ] present an efficient approach to mine significant non-consecutive word sequences, where, significance by the support measure. Contrary to mining patterns that satisfy pre-specified criterion, there has also been work on interactive pattern mining [ 1 , 2 , 27 ] that uses human feedback to identify a set of interesting patterns. Chen proposed an English-Chinese medical summary translation system that adapts a baseline SMT model with significant patterns (of lexical as well as medical type tokens) learned from an English medical summary corpus. The quality of a pattern is assessed based on its frequency in the corpus and its linguistic com-pleteness. While being closest to our work, we differ from them and the other aforementioned works in two ways. Firstly, we realize that the quality crite-rion for a set of patterns is not always a modular function of quality of the constituent patterns in the set. We define several quality criteria based on both element-wise decomposable (modular) costs and element-wise non-decomposable (non-modular) costs and combine them in a mathematical formalism for the task of significant pattern mining. Secondly, domain-specific classes often rely on the availability of corresponding term lexicons. Our framework also makes use of general phrase classes such as noun phrases (NP), verb phrases (VP), thereby extracting generic patterns whose instances themselves might not be frequent in a corpus (Refer to Fig. 1 ). Moreover, the use of phrase classes allows for the induction of new instances in a class (type) lexicon. The task of lexicon curation finds applications in several NLP tasks including machine translation. We present a formulation of the problem and a solution framework that one could invoke based on underlying application requirements. The lexicon is composed of quality patterns extracted from a domain corpus and for the specific task of machine translation with low resource constraint, we then acquire translations of these patterns to create an in-domain 3.1 Formal Problem Definition We are given a domain corpus C and optionally a set of  X  X ypes X  represent a domain type , like disease in medical domain, a phrases or a complex type involving a combination of these. The problem of lexicon curation is to extract from C ,aset H of quality patterns, as per a quality function Q C ( h ) for the quality of a pattern h quality function Q C ( H ) for the quality of the set H . 3.2 Solution Framework We define and describe below the components of our solution framework. 1. Context Free Grammar G : A context free grammar (CFG) allows us to encode our types and is comprised of a set V of non-terminals, a set  X  of terminals, a start symbol S  X  V and a set P of productions  X  where  X   X  V and  X   X  ( V  X   X  )  X  . Our choice of CFG as a formalism to represent the types is motivated from the fact that the grammar can be directly consumed by a high-level grammar formalism like Grammati-cal Framework (GF) [ 20 ], which is type theoretic, multilingual, and modu-lar and suits our downstream translation usecase. We define a grammar G , where, the set V of non-terminals corresponds to the set of types type T i  X  X  could be available as a lexicon list, comprising entries (undis-ambiguated), T i = { t i 1 ,t i 2 ,...,t ik } , where, each entry t of lexical tokens alone (in case of simple types) or a combination of lexi-cal and type tokens (in case of complex types). Alternatively, a type could 2. Pattern Extractor: A pattern extractor is a program that uses the context 3. Quality Q C ( h ) of a Pattern: Quality of a pattern h is defined as a function (a) Pattern consensus: | S h | , the number of spans covered by the pattern h ; (b) Informativeness: For a set C of corpora, (c) Syntactic well-formedness: A span covered by a pattern is said to be (d) Lexical rule-based consensus: Spans covered by a pattern should conform (e) Semantic rule-based consensus: Enforces a semantic constraint among (f) Model-based quality criteria: A trained classifier could be used to classify 4. Quality Q C ( H ) of a Patterns Set H : Quality of a set H of patterns, given a corpus C , is defined as a function Q C ( H )from2 Q ( H ) is either modular ( e.g. | H | , h  X  H | cover ( h ) | X  5. Pattern Selection: The problem of lexicon curation can now be posed as the problem of selecting an optimal subset H of H Q . Clearly, H is optimal quality set when H = H Q , however, in practice, selection of an optimal H often involves an optimization of conflicting requirements on the quality and the cost of the subset. Formally, Or where, c and d are thresholds on the cost and the quality of H respectively. It is known that this optimization has an efficient solution under the assumption that the cost function Q 1 C ( H ) be modular and the quality function Q submodular [ 5 ]. 3.3 Our Approach We implemented our framework to curate high quality compact lexicons for the cross-domain SMT task. Given a source language corpus, we pose the problem of curating an optimal set of high quality patterns, solve it using a greedy algorithm and use a human-in-the-loop approach to get their translation. More precisely, we follow the steps described below: Context Free Grammar: We use Stanford parser to create a lexicon list of type Noun phrases (NP) present in the corpus. Refer to Sect. 3.2 for details. Pattern Extraction and Filtering: We use our grammar to index corpus and mine patterns. Our mining approach is inspired from Joshi first mine patterns for each sentence using their dynamic programming-based approach and then aggregate patterns across all sentences. Subsequently, we filter out bad patterns (we refer to this as pattern filtering), where, the quality of a pattern is judged based on two modular quality criteria X  X ggregated frequency of its instances and their syntactic well-formedness.
 Pattern Selection: We formulate this as a subset selection problem (Refer to the formulations ( 1 ) and ( 2 )). Although formulation ( 1 ) has a better approxi-mation guarantee, both formulations performed equally well in our evaluation. Both formulations can be efficiently solved if the cost function Q ular and the quality function Q 2 C ( H ) is submodular [ 9 ]. In our experiments, we use as Q 1 C ( H ) the modular cardinality constraint | H submodular token coverage of corpus: | X  h  X  H cover ( h ) submodular and monotone function, that is, A B then Q 2 C this problem can be solved greedily with theoretical guarantee of 1 This is the best approximation result we can achieve efficiently [ 15 ]. Further, we use an accelerated version of this algorithm [ 13 ] which at every iteration lazily evaluates the function value to get the best item to add in the output set. Pattern Translation: After mining a high quality set of patterns, we ask humans to provide translations of these patterns and thus create a lexicon . We leveraged Matecat [ 4 ] and MyMemory 3 to help human translators while using our interactive system for gathering translations. 4.1 Experimental Setup We study the effect of curating a domain-specific bilingual lexicon using our approach on domain adaptation of pre-built cross-domain statistical machine translation (SMT) models 4 trained for three language pairs on the Europarl corpus [ 26 ]. We experimented with adapting the pre-built SMT model on three domain specific datasets [ 23 , 24 ], each pertaining to a different domain: (i) JRC (legal), (ii) EMEA (medical) and (iii) KDE (technical). Each domain has spe-cific language usage that differs from that of a large cross-domain corpus ( Europarl) typically used to train an SMT model. Moreover, the availability of parallel corpora for training SMT models is very limited in these domains. While each of these is a parallel corpus, we made use of the aligned target language corpus only for evaluation. The remaining source language data was used for mining patterns. We experimented with these datasets for three language pairs: English-French (en-fr), English-Spanish (en-es), and English-German (en-de). In Table 2 , we present the number of sentences in each of these parallel corpora. For each dataset, we create a test set (TEST) for evaluation, by randomly sampling 3000 unique sentence pairs from the corpus. A different random sam-ple of up to 100 , 000 source language sentences is used as the set for mining the patterns (MINE). Pattern extraction is performed using the sentences in the source language from the MINE set and for the set of quality patterns mined, we manually obtain their corresponding translations, while being guided by the aligned target language sentences from the MINE set. We evaluate the transla-tion quality on the TEST set using the standard BLEU metric [ 19 ]. Our baseline corresponds to the pre-built SMT model. For domain adaptation, we incorporate the bilingual lexicon (curated using the MINE set) into the baseline model using the XML markup feature 5 available in the Moses tool.
 The entire process of sampling TEST and MINE sets for each corpus and language pair is repeated thrice and the baseline and domain-adapted numbers are reported after averaging across the three runs. In the following sections, we present several intermediate results and ablation tests before presenting the final BLUE score comparisons. Owing to space issues we present select plots for select datasets and language pairs here. 4.2 Effect of Syntactic Completeness-Based Consensus on Pattern Extraction The pattern extraction step extracts all frequent patterns (frequency threshold = 2) of up to a certain length. This results in a large number of patterns, not all of which are syntactically well-formed. We filter out patterns whose instances do not conform to a phrasal structure as per the Stanford parser (inferred via the Grammar discussed earlier), thus leaving behind between 6 % to 9 % patterns for further processing for lexicon curation ( 4.3 Effect of Varying the Lexicon Size Pattern selection (Eq. 1 ) allows to constrain the cardinality of the final set of quality patterns. The manual translation of these patterns requires human effort that is proportional to the number of patterns in this set. On the other hand, cardinality of this set might also affect the corpus coverage and thereby the translation accuracy. We study this effect by setting the cardinality of this set to various values: 25, 75, 125, 200, 250, 1000, 1750 and 2500.
 Coverage on MINE versus TEST: In Fig. 3 , we present the corpus coverage (in terms of number of words) on the MINE and TEST data sets with varying number of patterns and for different pattern lengths. Patterns mined using the MINE split seem to generalize well and the coverage on both MINE and TEST increases as we increase the number of patterns. This observation holds true for other datasets and language pairs as well and the coefficient of correlation between MINE and TEST coverage is consistently above 0 . 99. Coverage and Translation Accuracy on TEST: The patterns in the lexicon are translated and added to an in-domain bilingual lexicon. Figure 4 shows that as we increase the size of the lexicon, the TEST coverage improves and we see corresponding improvement in the translation accuracy. 4.4 Comparison of Different Approaches to Pattern-Set Extraction We compare different approaches to extracting a good set of patterns from a source language corpus and translating them for cross-domain SMT application. Figure 5 shows accuracy of these models for varying number of patterns in the lexicon.
 Effect of Bilingual Lexicon in Domain Adaptation: The task of domain adaptation involves using a translation model trained on a large out-of-domain parallel corpus and adapting it to reliably translate an in-domain corpus. The pre-built SMT models trained on the out-of-domain Europarl corpus serve as baselines and are used to evaluate translations on the in-domain TEST splits (Refer to B0 in the figure). Next, we adapt the model for in-domain transla-tion by incorporating our curated in-domain bilingual lexicons into the baseline model. The improvement in translation accuracy (Refer to B2) is quite evident. The lexicons capture significant in-domain patterns and provide their reliable translation, thereby, further aiding the baseline model already trained to trans-late common cross-domain phrases.
 Effect of Submodular Optimization: Would we have got the same improve-ment in translation accuracy had we curated a bilingual lexicon from a random subset of patterns? We curated a bilingual lexicon using our set of quality pat-terns (B2) and another using a random subset of frequent patterns (B6) and compared their impact on translation accuracy. The translation model incorpo-rating bilingual lexicon curated from random subset of frequent patterns does improve upon the baseline. However, the one with our high quality bilingual lex-icon, obtained after submodular optimization, does much better in generating a high quality translation. Effect of Pattern Generalization: Since our patterns comprise words and phrase classes, phrases in a corpus that might otherwise be infrequent, turn out to be frequent when folded into patterns. In order to ascertain that this indeed positively affects our curated lexicon and the final translation model, we compared this with a lexicon curated from frequent lexical-only phrases (B4 and B5) in the corpus. The final set of patterns was obtained, in one case, by extracting the top-k frequent phrases (B5: modular criterion) and in the other case, by using the submodular quality criterion (Refer to B4). As can be seen in Fig. 5 , the modular frequency-based criterion does much better on generalized patterns than on phrasal (lexical only) patterns and together with submodular optimization results in a much better bilingual lexicon.
 Comparison with Other Work. The system proposed by Chen et al. comes closest to our work. We used publicly available domain lexicons to anno-tate our corpora with domain types and used their clustering-based approach to extract a set of significant patterns. The bilingual lexicon was created by sampling and manually translating one representative pattern from each cluster (Refer to B1). Next, we applied our submodular optimization-based approach on the same annotated corpora (Refer to B3). We observe that the pattern-set obtained using our quality criteria does better, even with patterns composed of domain types instead of the more general phrase classes. We presented a novel framework for extraction of a high quality bilingual lexicon for domain specific translation. We defined several quality criteria that could be modeled as modular or submodular functions over the set of patterns mined from a domain specific corpus. The problem of pattern selection is then formulated as an optimization of these criteria and solved to produce a good set of repre-sentative in-domain patterns. Experimental results justify that a cross-domain SMT model indeed benefits from the availability of this high quality in-domain bilingual lexicon and does better in translating domain specific text.
