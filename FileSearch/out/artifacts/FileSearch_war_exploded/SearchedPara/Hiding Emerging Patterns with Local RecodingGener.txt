 Organizations often publish and share their data to support business collaboration. In the context of marketing and sales, companies can leverage on the customer pools of each other for cross-selling so that the involv ed parties can gain sales volume increase. Due to the equally important need of privacy protection, customers often expect their data to be anonymized before sharing [27] and studies on privacy-preserving data pub-lishing have bloomed [12]. Furthermore, trade secrets embedded in data are valuable to organizations [6] and needed to be properly p rotected. For instan ce, patterns like recent increase in the sales volume of a product line for a certain customer group (emerging marketing trends) can be an example. Leaking of related intelligence could cause com-pany loss in gaining the first-mover advantage. Even though companies understand that data sharing is unavoidable to support colla borative activities like cross-selling, they may face a great hindrance to data sharing if the emerging sales opportunities of their own business cannot be hidden.

Among others, emerging patterns [19] embedded in data carry sensitive information, that data owners may prefer to hide. In fact , previous studies have revealed that emerg-ing patterns are highly discriminative when used as features for classification [31,11,8], and thus carry salient features of the data. Th e hiding, however, is technically challeng-ing as collaborative data analysis is still often expected to facilitate collaboration. That is, some statistical properties of the data to-b e-shared are preserved as far as possible. In particular, frequent itemset mining has already been well-supported in most com-mercial data-mining packages. Therefore, i n this paper, we study how to hide emerging patterns while preserving frequent itemsets.

To hide emerging patterns, we adopt recoding generalization methods. In particular, we adopt local recoding which is (intuitively) a value-grouping generalization process, given an attribute generalization hierarchy . To ensure that the generalized data neither (i) reveal sensitive information nor (ii) produce a highly distorted mining result, we propose metrics for quantifying the two competing objectives. With the metrics, we present an iterative, bottom-up optimization framework. Compared with hiding frequent itemsets [25], hiding emerging patterns is more technically challenging. In particular, the a priori anti-monotone property does not hold in emerging patterns. Thus, the search space of emerging patterns is huge. Worst still, a local recoding may hide an emerging pattern while generating new emerging patterns. To the best of our knowledge, there has not been work on hiding emerging patterns. Studies on data sanitization can be dated back to the earlier work on statistical disclo-sure control [1]. Recent development in pri vacy preserving data mining [26] has con-tributed to some advances in privacy measure and data sanitization method. For example, to avoid personal identity to be recovered from an anonymized demographic dataset, a number of privacy measures were proposed in the literature, e.g., k -anonymity [27] and -diversity [22]. Other privacy measures include k m -anonymity [28] and ( h , k , p )-coherence [33]. Given a particular measure, re coding generalization [19,14,18,26,9,32,20] and perturbation [2,10,17] are two commonl y adopted data sanitization approaches. Re-coding generalization is often preferred over the perturbation approach as the dataset sanitized by recoding generalization is still semantically consistent with the original one, even though it is  X  X lurred X . While this study aims at hiding emerging patterns instead of personal identities, the concepts like equivalence classes and recoding generalization are adopted in the proposed methodology.

To control the distortion of the data caused by the sanitization, attempts have been made to preserve as much information of the original dataset as possible to, say, pre-serve the subsequent classification accuracy [15] and clustering structure [13]. In addi-tion, there has been some recent work studyi ng the tradeoff betw een privacy and utility [21] in the context of privacy-preserving data publishing. In this work, we try to pre-serve the frequent itemsets of the data as far as possible.

Recently, there has been work [25,24] on hiding patterns like frequent itemsets where users specify a subset of frequent itemsets, namely sensitive frequent itemsets, that are not supposed to be disclosed to the public. In our study, we focus on hiding emerging patterns, which makes a unique contribution to the area of pattern hiding. Emerging patterns ( EP ) are features that are distinctive from one class to another and has been found to be effective for building highly accu rate classifiers [16,31,11,8]. Mining EP s from large databases is technically intriguing as the total number of EP s, in the worst case, is exponential to the total number of attributes in transactions, and there has not been a corresponding notion of the apriori anti-monotone property of frequent itemsets in
EP s so that the search space can be pruned. Previous work on EP s mainly focuses only on the mining efficiency, e.g., using a border-based approach [4], a constraint-based approach [34], or focusing only on jumping EP s [3]. So far, there exists no related work on emerging pattern hiding. In the following, we present the definitions, notations used and the problem statement. of distinct items in D .A transaction t has a set of nominal attributes A = { a 1 , a 2 , ..., a about these notations. (i) While we assume transactional data with nominal attributes, data of a continuous domain can be cast into nominal data, for example by defining ranges. (ii) One may consider a relation as a set of transactions of a fixed arity. An itemset X is a (proper) subset of I . Supp D ( X ) denotes the support of an itemset old  X  , X is said to be a  X  -frequent itemset if Supp D ( X )  X   X  . The growth rate of an itemset is the ratio of its support in one dataset to that in the other.
 Definition 1. [7] Given two datasets, namely D 1 and D 2 ,the growth rate of an itemset X , denoted as GR ( X , D 1 , D 2 ), from D 1 to D 2 is defined as GR ( X , D 1 , D 2 )=  X  Definition 2. [7] Given a growth rate threshold  X  and two datasets D 1 and D 2 ,an itemset X is a  X  -emerging pattern (  X  -EP )from D 1 to D 2 if GR ( X , D 1 , D 2 )  X   X  . Intuitively, given two datasets, emerging patterns ( EP s) [7] are the itemsets whose sup-port increases significantly from one dataset to another. The formal definition of EP sis presented in Defintion 2. An emerging pattern with a growth rate  X  (i.e., itemset that appears in one dataset but not the other) is called a jumping emerging pattern .
For ease of presentation, we may skip  X  ,  X  , D 1 and D 2 of EP s when they are not essential to our discussions.
 Example 1. Figure 1 (a) shows a simplified hypothetical dataset D of the Adult dataset [30]. It contains some census information of the United States. More description of the dataset can be found in Section 7. We opt to present some nominal attributes of Adult for discussions. Each record (or transaction) represents a person. Consider two subsets D 1 containing married people and D 2 containing those who do not. From Figure 1 (a), we find the following emerging patterns, among many others.  X  The pattern ( MSE , manager ) has a support of 75% in D 1 and 20% in D 2 . Therefore,  X  High-school graduate ( HS ) has a support of 0% in D 1 but 20% in D 2 . Hence, its Next, we state the formal problem statement of this paper below (Figure 1 (b)). Problem statement. Given two datasets ( D 1 , D 2 ),  X  and  X  , we want to sanitize ( D 1 , D 2 )to( D 1 , D 2 ) such that no  X  -EPs from D 1 to D 2 can be mined while the distortion between  X  -frequent itemsets of ( D 1 , D 2 ) and those of ( D 1 , D 2 ) is minimized. Our algorithm for hiding emerging pattern is based on local recoding generalization ( multi-local-recode in Figure 3). In this section, we give an overview of recoding generalization, or recoding for simplicity.
 Recoding. As discussed in Section 1, recoding has been proposed for anonymization. The idea of recoding is to modify values in to more general ones such that more tuples will share the same values and cannot be distinguished individually. Thus, anonymiza-tion can be achieved. Here, we recode values in emerging patterns with some non-emerging values. Thus, the recoded patterns become less emerging.
 Multidimensional local recoding. In this work, we adopt the notion of multidimen-sional local recoding [9,32,20], from the context of k -anonymity. It recodes values at  X  X ell level X . It relies on equivalence classes. An equivalence class of attributes A is a set of tuples T ,where  X  A ( T ) is a singleton. That is, the tuples in T havethesamevalue in attributes A . In a recoding, the tuples in an equivalence class (of a set of attributes) and those in another equivalence class are recoded into the lowest common ancestors along the hierarchies. One subtle point is that this recoding does not require the entire equivalence class to be recoded, as long as anonymity can be achieved. Hence, both original and generalized values ma y co-exist in the recoded dataset.
 Example 2. Let us revisit the dataset in Figure 1 (a) and the emerging pattern ( MSE , manager ). The emerging pattern is related to the attributes of education background (
Edu. ) and occupation ( Occup. ). Regarding ( Edu. , Occup. ), the equivalence classes in D 2 are {{ 5, 8, 9 } , { 6 } , { 7 }} , where the numbers are the IDs. In multidimensional lcoal recoding, we may recode the Edu. attribute of the subset of { 2, 3, 4 } and { 5, 8, 9 } . For example, we may recode { 2, 3, 4 } with { 8, 9 } . and we may recode BA and MSE into degree holder Deg . The growth rate of ( Deg. , manager ) in the recoded dataset is 75%/40% = 1.875. Hence, ( Deg. , manager ) is not  X  -emerging when  X  = 3. In addition, after such a recoding, all BA , MSE and Deg. appear in the recoded dataset. Other notions of recoding, including single-dimensional global recoding [5,14,18,26] and multidimensional global recoding [19], generalize values in a relatively coarse gran-ularity and very often result in over-generalization. of a local recoding. util gain will guide the process for hiding emerging patterns local-recoding , in Figure 3. A recoding is effective if (i) the distortion of frequent itemsets is small and (ii) the reduction in the growth rate of emerging patterns is large. Metric for the distortion of frequent itemsets. For presentation clarity, we will present our proposed metric for global recoding followed by its adaption for local recoding. (A) Distortion metric for single-dimensional global recoding. Single-dimensional global recoding performs recoding on the domain of an attribute in a dataset. It recodes a value of the domain to another (generalized) value. That is, if a particular value is recoded, the attribute of all the tuples containing that particular value will be recoded. No frequent itemsets disappear but may appear in a gener alized form after a recoding (Figure 2 (a)).
Inspired by the distortion metric proposed in [20], we propose a metric for measur-ing the recoding distance ( RDist ) between the original and generalized form of a tuple. Then, we define a metric called value distance ( VD ) which measures the distance be-tween the original and generalized form of a single attribute value. We will use VD as a building block for the definition of distortion ( RD ). Since a recoding always assumes an attribute hierarchy, we may skip the hierarchy H when it is clear from the context. Definition 3. Recoding Distance (RDist): Consider a recoding G which generalizes a set of non-generalized values V to a single generalized value v g ,where V is the set of values under v g in an attribute hierarchy. The recoding distance of GRDist ( G ) is | V | . Definition 4. Value Distance ( VD ): Let h be the height of an attribute hierarchy H , where level h and 0 is the most generalized and specific level, respectively. Consider a value v at level p which is generalized to a value v at level q .Let G i denotes the recoding that generalizes an attribute from level i  X  1 to i ,where 0 &lt;i  X  h .The value Value distance is unfavorable to recoding (i) many values into one single generalized value; and (ii) a value into a generalized value that is close to the top of the hierarchy. This gives a measure for the distortion of a value due to a recoding. Next, we extend VD to measure the distortion of a tuple and frequent itemsets due to recoding. Definition 5. Tuple Distance (TD): Suppose a tuple f =( v 1 ,v 2 ,...,v n ) is gener-alized to f =( v 1 ,v 2 ,...,v n ). The tuple distance between f and f is defined as: TD ( f, f )= n i =1 VD ( v i ,v i ) .
 Definition 6. Recoding Distortion (RD): Let F = { f 1 , f 2 ...f n } be a set of  X  -frequent itemsets in D and F = { f 1 , f 2 ...f m } be the set of  X  -frequent itemsets in D ,where m  X  n . The corresponding frequent itemset of f i due to global recoding is denoted as f j = G ( f i ). The recoding distance between F and F is defined as: RD ( F, F )= i =1 TD ( f i ,G ( f i )) .
 Example 3. Following up Example 2, we compute the (global) recoding distortion of generalizing ( MSE , manager )to( Deg. , manager ). Figure 2 shows the attribute hier-archy of Edu .The recoding distortion RD ( { ( MSE , manager ) } , { ( Deg. , manager ) } ), RD , can be computed as follows: RD = TD (( MSE , manager ) , ( Deg ., manager )) = (B) Distortion metric for local recoding. Since single-dimensional global recoding may often lead to over-generalization, we adopted local recoding. We remark that there are two unique challenges in computing recoding distance for local recoding (Figure 2 (b)). (B.i) An itemset in F having no correspondence in F . Local recoding allows part of the tuples that share the same attribute val ues to be generalized. Such recoding may generalize some supporting tuples of a frequent itemset which makes the itemset (in the original or generalized form) not frequent anymore. To address this, we measure the distortion of the disappeared frequent itemset to the most general form. The reason is that the frequent itemset can be trivially recovered when the entire dataset is generalized to the most general form.
 Specifically, given a f in F , if we cannot find a corresponding frequent itemset in F , we first create an itemset, f max , which contains the most generalized value of each value in f . Then, RD of f is the recoding distance between f and f max .
 Example 4. Reconsider the dataset in Figure 1 (a). Suppose we recode the Edu. at-tribute of Records 1 and 2 to Deg .When  X  is 40%, { BA } and { MSE } were frequent itemsets (not minimal for illustration purposes) before recoding and there is no frequent itemset after recoding. (B.ii) An itemset in F having more than one corresponding itemset in F . As discussed, local recoding may generalize a frequent itemset f in F into more than one correspon-dence in F , denoted as F f . In this case, we calculate the tuple distance of each of the corresponding itemsets in F f and take the minimum tuple distance as the tuple distance of f . This is because the itemset with the minim um tuple distortion has been revealed in F , even when there may be more distorted itemsets. With the above, we have the following recoding distance for local recoding: Definition 7. Recoding Distance for Local Recoding ( RD local ): Let F = { f 1 , f 2 ... f } be a set of  X  -frequent itemsets in D and F = { f  X  -frequent itemsets in D . The corresponding frequent itemset(s) of f i due to local recoding is denoted as F f = G ( f i ). The recoding distance between F and F is:  X  q is a parameter that specifies the relative importance of the itemset distortion and missing itemsets, due to G , and TD local ( f i , f max ) is for normalizing RD local . Example 5. Following up Example 4, when  X  is 30%, the frequent itemset { ( BA ) } cor-responds to the frequent itemsets { ( BA ), ( Deg ) } in the recoded datasets. Metric for the change in growth rate. The second component of our heuristics con-cerns the growth rate of the emerging patterns. Intuitively, we aim at a recoding that significantly reduces the growth rate of the emerging patterns in order to hide them. Given an emerging pattern e and the result of a local recoding e , the reduction in growth rate due to the recoding can be easily defined as the growth rate of e minus thegrowthrateof e . Then, the growth rate reduction of E due to a local recoding G , denoted as RG local ( G , E ), can be defined as the total reduction in the growth rate of e in E divided by the total growth rate of e in E .
 Putting all these together. Based on the derivations above, the utility gain due to a local recoding G for a set of emerging patterns E is defined as: The two parameters  X  p and  X  q ,where  X  p ,  X  q  X  [0,1], are specified by users. In this section, we present the overall algorithm hide-eps (shown in Figure 3) for hiding emerging patterns with a minimal distortion in frequent itemsets.
 Overview of hide-eps . The main ideas of hide-eps can be described as follows. First, we determine the emerging patterns to be hidden (Line 02) and the frequent item-sets (incrementally) to be preserved (Line 04). We refer the details of Lines 02 and 04 to previous works [29,34], since our focus is on hiding emerging patterns. For each se-lected emerging pattern (Line 05), we carry out a local recoding local-recoding-sa (Line 06, more details soon). This process (Lines 03-08) is repeated until there is no more emerging pattern to hide (Line 03). To avoid sub-optima, we present hide-eps in the style of simulated annealing search (Lines 01, 07 and 18).
 Next, we discuss the details of the major steps of the algorithm.
 Mining emerging patterns ( mine-eps , Lines 02 and 08). During recoding, we in-voke mine-eps [34] to determine if all the emerging patterns have been hidden (Line 08). To the best of our knowledge, there does not exist any incremental algorithm for mining emerging patterns. As verified by our experiments, mine-eps is a bottleneck of runtime of hide-eps . However, it should be remarked that the emerging patterns may often be altered slightly by most local recodings, in practice. To address this perfor-mance issue, in Section 7, we tested another version of hide-eps ,where mine-eps is invoked only when all previously mined emerging patterns have been hidden.
 Incremental mining of frequent itemsets ( incr-mine-fis , Line 04). A local re-coding may alter the existing frequent itemsets. Figure 2 (b) (ii) shows an example. Since a local recoding changes only part of D 1 and D 2 , we need not mine the dataset from scratch but do it incrementally using algorithms like [29].
 Selecting emerging patterns for recoding ( next-overlapping-ep , Line 05). Given a set of emerging patterns E , next-overlapping-ep determines the emerging pat-tern e in E such that it overlaps with the remaini ng emerging patterns the most. The intuition is that reducing the growth rate of e may indirectly reduce the growth rate of the overlapping emerging patterns as well. We verify with some experiments that this approach consistently outperforms a number of other strategies (see Section 7). Determining the next local recoding ( local-recoding-sa , Lines 06, 10-21). As-sume that c e is the equivalence class of the emerging pattern e . We first compute the equivalence classes of the attributes of e to generalize with c k (Line 12). We apply the utility gain defined in Sectio n 5 to determine the goodness of local recodings (Line 16). Since there can be many equivalence classes, this is another bottleneck of runtime. We speed up that step using (i) a hashtable (Lines 01 and 16-17) to cache the utility gain values computed, and (ii) two filters on equivalence classes (Lines 14 and 15). The first filter is that we ignore the equivalence classes that would result in missing frequent item-sets, which is obviously undesirable. This can be computed by the change in support of itemsets in F due to a local recoding. Second, we discard a local recoding that would yield new single-attribute emerging patterns. This can be computed by determining the growth rate of the equivalence classes with the attributes of e . We did not compute possible new multi-attribute emerging patterns because of its daunting complexity.
With the utility gain of equivalence classes, we use a simulated annealing search ( get-next-step-sa , Line 18), as a black box, to get the next local recoding. Analysis. Given that A E is the set of attributes of the emerging pattern E ,and D and H are the overall domain size and the maximum height of the hierarchy of all possible A E  X  X , respectively. In the worst case, there can be O ( local recoding allows tuple-wise recoding and thus in the worst case, | D 1 | + | D 2 | recod-ing operations can be carried out. Thus, the search space of finding the optimal recoding is O ( ( | D 1 | + | D 2 | )  X D X H ). This work proposes a heuristic search for this problem. While the loop (Lines 03-08) may repeat many times in the worst case, the number of iterations needed was found small in practice. As discussed, mine-eps and the compu-tation of util gain are the bottlenecks of runtime. The runtime of the former is exper-imentally evaluated in [34]. The time complexity for the latter is O ( | A e | X D X H X | F | ), where e  X  E , | A e | X D X H is the number of possible equivalence classes and for each class, O ( | E | )and O ( | F | ) are used to compute RG local and RD local , respectively. To verify the effectiveness and efficiency of our proposed algorithms, we conducted several experiments on Adult dataset [30] using the attribute hierarchies from [14].
We implemented our algorithm in JAVA SDK 1.6 1 . We have run our experiments on aPCwithaQuad CPU at 2.4 GHz and 3.25 GB RAM . The PC is running Windows XP operating system. We have used system calls to invoke the implementations from [34] and [23] to determine emerging patterns and frequent itemsets, respectively.
The simplified Adult dataset contains 8 attributes. We removed the records with missing values. The records in the dataset were divided into two classes -people who have more than $50k/year (7508 records) and people who do not (22654 records). The effect of the parameters  X  p and  X  q . The first experiment is to verify the effects on the parameters  X  p and  X  q on the heuristic algorithm. In this experiment, we do not apply any filter (i.e., Lines 14-15 in hide-eps )and SA search (Line 18) in order to observe the effects on the parameters clearly. Instead, we used a Greedy search. The performance was presented in  X  X istortion on the frequent itemsets / the number of missing frequent itemsets X , unless otherwise specified .When  X  and  X  were set to 40% and 5, respec-tively, the frequent itemsets obtained are: { ( Husband , Married-civ-spouse , Male ), ( Married-civ-spouse , White ), ( Married-civ-spouse , United-States ), ( Male , Private , White ), ( Male , Private , United-States ), ( Male , White , United-States ), ( Private , White , United-States ) } .

When we recode all attributes to All in the frequent itemsets, we obtain the maxi-mum distortion of the frequent itemsets of Adult 623.1.

To illustrate the possible effect of recodings on the resulting frequent itemsets, we list the frequent itemsets after we applied Greedy ,where  X  p and  X  q were both set to 0.8: { ( Relationship , United-States ), ( Married , White , United-States ), ( Male , Private , White ), ( Male , Private , United-States ), ( Male , White , United-States ), Private , White , United-States ) } . The distortion obtained is 21.5 (out of 623.1).

Next, we varied  X  p and  X  q and measured the performance of Greedy . The perfor-mance is shown in Table 1 (LHS). The average runtime is 58 mins and 16 out of 58 mins is spent on mining EP s. The average number of local recodings is 14.

We make four observations from Table 1 (LHS). Firstly, when  X  p is set to 0, the algo-rithm concerns only distortion (regardless the corresponding reduction in growth rate) during local recodings. In such a case, the distortion on frequent itemsets of various  X  q  X  X  is in general large. The reason is that when  X  p is 0, the heuristics does not effectively re-duce the growth rate and the search takes more recodings that are not relevant to hiding emerging patterns. Secondly, when  X  p is 1, the algorithm concerns only the reduction in growth rate. Note that 3 out of 7 frequent itemsets have disappeared. Thirdly, when we set  X  q to 1, we do not concern the missing frequent itemsets. Hence, more frequent itemsets were lost. Similarly, when we set  X  q to 0, we concern only the frequent item-sets that do not disappear. Since there is one missing frequent itemset during recodings, overlooking this led to more distortion. Fourthly, we found that a significant runitme (42 mins) was spent on calculating the utility gain of equivalence classes. The reason is that no filters had been applied yet.

In all, we found that on Adult , Greedy yields frequent itemsets with a distortion 21.5 (out of 623.1) and 1 missing frequent itemset when both  X  p and  X  q are moderate. The effect of the determine-new-singleton-eps filter. This filter is used to avoid recoding equivalence classes that would yield new single-attribute EP s (Line 15 of hide-eps ). The performance of Greedy with this filter is shown in Table 1 (RHS).
We observe from Table 1 (RHS) that there a re similar trends on the performance with various  X  p and  X  q . The distortion is sometimes smaller but the missing frequent itemsets may sometimes be more. However, the average runtime of this experiment is 26 mins (compared to 58 previously). Specifically, the time for computing utility gain has been reduced from 42 to 15 mins. The number of recodings reduces from 14 to 9. The runtime improvement is due to (i) the smaller number of equivalence classes for computing the utility gain and (ii) fewer (if any) new EP s generated during hide-eps . The effect of the determine-missing-FIS filter. From the previous experiments, we note that there are missing frequent itemsets in most cases. Here, we test the ef-fectiveness of determine-missing-FIS filter (Line 14). The performance is shown in Table 2 (LHS). The average runtime for computing the utility gain increased from 15 to 21 mins and the number of recodings increased from 9 to 14. At first glance, the distortion might have increased. However, there is no missing frequent itemset for all  X   X  X  and  X  q  X  X . This improvement comes at the expense of a slight increase in runtime. The effect of calling mine-eps when E is empty. In the last experiment, 15 out of 36 mins was spent on mining EP s. In this experiment, we attempt to improve the runtime by hiding all existing EP s first before calling mine-eps , as opposed to calling mine-eps after each recoding. The performance is sh own in Table 2 (RHS). From the result, we found that there is a slight increase in distortion. However, the time for mining EP sis reduced from 15 to 8 mins. The average runtime for computing the utility gain increased from 21 to 23 mins and the number o f iterations remains unchanged.
 The effect of hiding the EP with the minimum overlapping. To justify the decision of hiding the maximum overlapping EP in Line 05 of hide-eps , we conducted an experiment which first hides the EP with the minimum overlapping. The result is shown in Table 3 (LHS). We observed that the distortion is slightly larger than that of the maximum overlapping. However, the average runtime for computing the utility gain increased from 23 to 39 mins and the time for mining EP increased from 8 to 23 mins. Simulated annealing search. After demonstrating the effects of various settings with Greedy , we applied SA on the algorithm (Line 18 of hide-eps ). We set a low tem-perature ( T =10) of SA with a high cooling rate (  X  =0.4). Hence, SA initially has some chances to avoid local sub-op tima and then converges to Greedy quickly. To explore the search space more, each SA was allowed to restart fifty times. The results are shown in Table 3 (RHS). SA introduces some randomness in the performance. Compared to the best versions (Table 2), SA often produces better results, at the expense of runtime. We presented a heuristic local-recoding algorithm for hiding emerging patterns of a dataset while preserving its frequent itemsets as far as possible. We tested our algorithm with a benchmark dataset and showed its effectiveness.

