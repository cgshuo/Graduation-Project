 Ariel Kleiner akleiner@cs.berkeley.edu Ameet Talwalkar ameet@cs.berkeley.edu Purnamrita Sarkar psarkar@cs.berkeley.edu Michael I. Jordan jordan@cs.berkeley.edu Assessing the quality of estimates based upon finite data is a task of fundamental importance in data anal-ysis. For example, when estimating a vector of model parameters given a training dataset, it is useful to be able to quantify the uncertainty in that estimate (e.g., via a confidence region), its bias, and its risk. Such quality assessments provide far more information than a simple point estimate itself and can be used to im-prove human interpretation of inferential outputs, per-form hypothesis testing, do bias correction, make more efficient use of available resources (e.g., by ceasing to process data when the confidence region is sufficiently small), perform active learning, and do feature selec-tion, among many more potential uses.
 Accurate assessment of estimate quality has been a longstanding concern in statistics. A great deal of classical work in this vein has proceeded via asymp-totic analysis, which relies on deep study of particu-lar classes of estimators in particular settings (Politis et al., 1999). While this approach ensures asymptotic correctness and allows analytic computation, its appli-cability is limited to cases in which the relevant asymp-totic analysis is tractable and has actually been per-formed. In contrast, recent decades have seen greater focus on more automatic methods, which generally re-quire significantly less analysis, at the expense of do-ing more computation. The bootstrap (Efron, 1979; Efron &amp; Tibshirani, 1993) is perhaps the best known and most widely used among these methods, due to its simplicity, generic applicability, and automatic na-ture. Efforts to ameliorate statistical shortcomings of the bootstrap in turn led to the development of re-lated methods such as the m out of n bootstrap and subsampling (Bickel et al., 1997; Politis et al., 1999). Despite the computational demands of this more au-tomatic methodology, advancements have been driven primarily by a desire to remedy shortcomings in statis-tical correctness. However, with the advent of increas-ingly large datasets and diverse sets of often complex and exploratory queries, computational considerations and automation (i.e., lack of reliance on deep analysis of the specific estimator and setting of interest) are in-creasingly important. Even as the amount of available data grows, the number of parameters to be estimated and the number of potential sources of bias often also grow, leading to a need to be able to tractably assess estimator quality in the setting of large data. Thus, unlike previous work on estimator quality as-sessment, here we directly address computational costs and scalability in addition to statistical correctness and automation. Indeed, existing methods have sig-nificant drawbacks with respect to one or more of these desiderata. The bootstrap, despite its strong statistical properties, has high X  X ven prohibitive X  computational costs; thus, its usefulness is severely blunted by the large datasets increasingly encountered in practice. We also find that its relatives, such as the m out of n bootstrap and subsampling, can have lesser computational costs, as expected, but are gen-erally not robust to specification of hyperparameters (such as the number of subsampled data points) and are also somewhat less automatic due to their need to explicitly utilize theoretically derived estimator con-vergence rates.
 Motivated by the need for an automatic, accurate means of assessing estimator quality that is scalable to large datasets, we present a new procedure, the Bag of Little Bootstraps (BLB), which functions by combining the results of bootstrapping multiple small subsets of a larger original dataset. BLB has a sig-nificantly more favorable computational profile than the bootstrap, as it only requires repeated computa-tion of the estimator under consideration on quantities of data that can be much smaller than the original dataset. Hence, BLB is well suited to implementation on modern distributed and parallel computing archi-tectures. Our procedure maintains the bootstrap X  X  au-tomatic and generic applicability, favorable statistical properties, and simplicity of implementation. Finally, as we show empirically, BLB is consistently more ro-bust than alternatives such as the m out of n bootstrap and subsampling.
 We next formalize our statistical setting and notation in Section 2, discuss relevant prior work in Section 3, and present BLB in full detail in Section 4. Subse-quently, we present an empirical and theoretical study of statistical correctness in Section 5, an exploration of scalability including large-scale experiments on a distributed computing platform in Section 6, practical methods for automatically selecting hyperparameters in Section 7, and assessments on real data in Section 8. We assume that we observe a sample X 1 ,...,X n  X  X  drawn i.i.d. from some true (unknown) underlying distribution P  X  P . Based only on this observed data, we obtain an estimate  X   X  n =  X  ( P n )  X   X , where P n = n  X  1 P n i =1  X  X i is the empirical distribution of X 1 ,...,X n . The true (unknown) population value to be estimated is  X  ( P ). For example,  X   X  n might estimate a measure of correlation, the parameters of a regres-sor, or the prediction accuracy of a trained classifi-cation model. Noting that  X   X  n is a random quantity because it is based on n random observations, we de-fine Q n ( P )  X  X  as the true underlying distribution of  X   X  , which is determined by both P and the form of the mapping  X  . Our end goal is the computation of some metric  X  ( Q n ( P )) : Q  X   X , for  X  a vector space, which informatively summarizes Q n ( P ). For instance,  X  might compute a confidence region, a standard error, or a bias. In practice, we do not have direct knowledge of P or Q n ( P ), and so we must estimate  X  ( Q n ( P )) it-self based only on the observed data and knowledge of the form of the mapping  X  . The bootstrap (Efron, 1979; Efron &amp; Tibshirani, 1993) provides an automatic and widely applicable means of quantifying estimator quality: it simply uses the data-driven plugin approximation  X  ( Q n ( P ))  X   X  ( Q n ( P n While  X  ( Q n ( P n )) cannot be computed exactly in most cases, it is generally amenable to straightforward Monte Carlo approximation as follows: repeatedly resample n points i.i.d. from P n , compute the esti-mate on each resample, form the empirical distribu-tion Q n of the computed estimates, and approximate  X  ( Q n ( P ))  X   X  ( Q n ). Though conceptually simple and powerful, this procedure requires repeated estimator computation on resamples having size comparable to that of the original dataset. Therefore, if the original dataset is large, then this computation can be costly. While the literature does contain some discussion of techniques for improving the computational efficiency of the bootstrap, that work is largely devoted to reducing the number of Monte Carlo resamples re-quired (Efron, 1988; Efron &amp; Tibshirani, 1993). These techniques in general introduce significant additional complexity of implementation and do not obviate the need for repeated estimator computation on resamples having size comparable to that of the original dataset. Bootstrap variants such as the m out of n boot-strap (Bickel et al., 1997) and subsampling (Politis et al., 1999) were introduced to achieve statistical con-sistency in edge cases in which the bootstrap fails, though they also have the potential to yield compu-tational benefits. The m out of n bootstrap (and sub-sampling) functions as follows, for m &lt; n : repeatedly resample m points i.i.d. from P n (subsample m points without replacement from X 1 ,...,X n ), compute the estimate on each resample (subsample), form the em-pirical distribution Q n,m of the computed estimates, approximate  X  ( Q m ( P ))  X   X  ( Q n,m ), and apply an an-alytical correction to in turn approximate  X  ( Q n ( P )). This final analytical correction uses prior knowledge of the convergence rate of  X   X  n as n increases and is neces-sary because each estimate is computed based on only m rather than n points.
 These procedures have a more favorable computational profile than the bootstrap, as they only require re-peated estimator computation on smaller sets of data. However, they require knowledge and explicit use of the convergence rate of  X   X  n , and, as we show in our em-pirical investigation below, their success is sensitive to the choice of m . While schemes have been proposed for automatic selection of an optimal value of m (Bickel &amp; Sakov, 2008), they require significantly greater compu-tation which would eliminate any computational gains. It is also worth noting that some work on the m out of n bootstrap has explicitly sought to reduce computa-tional costs using two different values of m in conjunc-tion with extrapolation (Bickel &amp; Yahav, 1988; Bickel &amp; Sakov, 2002). However, these approaches explicitly use series expansions of the cdf values of the estima-tor X  X  sampling distribution and hence are less auto-matically usable; they also require execution of the m out of n bootstrap for multiple different values of m . The Bag of Little Bootstraps (Algorithm 1) functions by averaging the results of bootstrapping multiple small subsets of X 1 ,...,X n . More formally, given a subset size b &lt; n , BLB samples s subsets of size b from the original n data points, uniformly at random (one can also impose the constraint that the subsets be disjoint). Let I 1 ,..., I s  X  { 1 ,...,n } be the cor-responding index multisets (note that |I j | = b,  X  j ), bution corresponding to subset j . BLB X  X  estimate of  X  ( Q n ( P )) is then given by s  X  1 P s j =1  X  ( Q n ( P though the terms  X  ( Q n ( P ( j ) n,b )) cannot be computed an-alytically in general, they are computed numerically by the inner loop in Algorithm 1 via Monte Carlo approx-imation in the manner of the bootstrap: for each term j , we repeatedly resample n points i.i.d. from P ( j ) compute the estimate on each resample, form the em-pirical distribution Q  X  n,j of the computed estimates, and approximate  X  ( Q n ( P ( j ) n,b ))  X   X  ( Q  X  n,j ). To realize the substantial computational benefits af-forded by BLB, we utilize the following crucial fact: each BLB resample, despite having nominal size n , Algorithm 1 Bag of Little Bootstraps (BLB)
Input: Data X 1 ,...,X n
Output: An estimate of  X  ( Q n ( P )) for j  X  1 to s do end for //Average values of  X  ( Q n ( P ( j ) n,b )) computed //for different data subsets contains at most b distinct data points. In particular, to generate each resample, it suffices to draw a vector of counts from an n -trial uniform multinomial distri-bution over b objects. We can then represent each resample by simply maintaining the at most b distinct points present within it, accompanied by correspond-ing sampled counts (i.e., each resample requires only storage space in O ( b )). In turn, if the estimator can work directly with this weighted data representation, then its computational requirements X  X ith respect to both time and storage space X  X cale only in b , rather than n . Fortunately, this property does indeed hold for many if not most commonly used estimators, including M-estimators such as linear and kernel regression, lo-gistic regression, and Support Vector Machines, among many others.
 As a result, BLB only requires repeated computation on small subsets of the original dataset and avoids the bootstrap X  X  problematic need for repeated computa-tion of estimates on resamples having size comparable to that of the original dataset. A simple and standard calculation (Efron &amp; Tibshirani, 1993) shows that each bootstrap resample contains approximately 0 . 632 n dis-tinct points, which is large if n is large. In contrast, as discussed above, each BLB resample contains at most b distinct points, and b can be chosen to be much smaller than n or 0 . 632 n . For example, we might take b = n  X  where  X   X  [0 . 5 , 1]. More concretely, if n = 1 , 000 , 000, then each bootstrap resample would contain approxi-mately 632 , 000 distinct points, whereas with b = n 0 . 6 each BLB subsample and resample would contain at most 3 , 981 distinct points. If each data point occu-pies 1 MB of storage space, then the original dataset would occupy 1 TB, a bootstrap resample would oc-cupy approximately 632 GB, and each BLB subsample or resample would occupy at most 4 GB.
 BLB thus has a significantly more favorable compu-tational profile than the bootstrap. As seen in sub-sequent sections, our procedure typically requires less total computation to reach comparably high accuracy (fairly modest values of s and r suffice); is significantly more amenable to implementation on distributed and parallel computing architectures which are often used to process large datasets; maintains the favorable sta-tistical properties of the bootstrap; and is more robust than the m out of n bootstrap and subsampling to the choice of subset size. We show via both a simulation study and theoreti-cal analysis that BLB shares the favorable statistical performance of the bootstrap while also being consis-tently more robust than the m out of n bootstrap and subsampling to the choice of b . Here we present a rep-resentative summary of our investigation of statistical correctness; see Kleiner et al. (2012) for more detail. We investigate the empirical performance character-istics of BLB and compare to existing methods via experiments on different simulated datasets and es-timation tasks. Use of simulated data is necessary here because it allows knowledge of Q n ( P ) and hence  X  ( Q n ( P )) (i.e., ground truth). We consider two dif-ferent settings: regression and classification. For both settings, the data have the form X i = (  X  X i ,Y i )  X  P , i.i.d. for i = 1 ,...,n , where  X  X i  X  R d ; Y i  X  R for re-gression, whereas Y i  X  X  0 , 1 } for classification. We use n = 20 , 000 for the plots shown, and d is set to 100 for regression and 10 for classification. In each case,  X   X  n estimates a parameter vector in R d (via either least squares or logistic regression with Newton X  X  method, all implemented in MATLAB) for a linear or gener-alized linear model of the mapping between  X  X i and Y . We define  X  as computing a set of marginal 95% confidence intervals, one for each element of the esti-mated parameter vector (averaging across  X   X  X  consists of averaging element-wise interval boundaries). To evaluate the various quality assessment procedures on a given estimation task and true underlying data distribution P , we first compute the ground truth  X  ( Q n ( P )) based on 2 , 000 realizations of datasets of size n from P . Then, for an independent dataset real-ization of size n from the true underlying distribution, we run each quality assessment procedure and record the estimate of  X  ( Q n ( P )) produced after each iteration (e.g., after each bootstrap resample or BLB subsam-ple is processed), as well as the cumulative time re-quired to produce that estimate. Every such estimate is evaluated based on the average (across dimensions) relative absolute deviation of its component-wise con-fidence intervals X  widths from the corresponding true widths: given an estimated confidence interval width c and a true width c o , the relative deviation of c from c o is defined as | c  X  c o | /c o . We repeat this process on five independent dataset realizations of size n and av-erage the resulting relative errors and corresponding times across these five datasets to obtain a trajectory of relative error versus time for each quality assessment procedure (the trajectories X  variances are small relative to the relevant differences between their means, so the variances are not shown in our plots). To maintain consistency of notation, we henceforth refer to the m out of n bootstrap as the b out of n bootstrap. For BLB, the b out of n bootstrap, and subsampling, we use r = 100 in all runs of BLB.
 Figure 1 shows a set of representative results for the classification setting, where P generates the com-ponents of  X  X i i.i.d. from StudentT(3) and Y i  X  Bernoulli((1 + exp(  X   X  X T i 1 ))  X  1 ); we use this represen-tative empirical setting in subsequent sections as well. As seen in the figure, BLB (left plot) succeeds in con-verging to low relative error more quickly than the bootstrap for b &gt; n 0 . 5 , while converging to somewhat higher relative error for b = n 0 . 5 . We are more robust than the b out of n bootstrap (middle plot), which fails to converge to low relative error for b  X  n 0 . 6 . In fact, even for b = n 0 . 5 , BLB X  X  performance is substantially superior to that of the b out of n bootstrap. For the aforementioned case in which BLB does not match the relative error of the bootstrap, additional empirical re-sults (right plot) and our theoretical analysis indicate that this discrepancy in relative error diminishes as n increases. Identical evaluation of subsampling (plots not shown) shows that it performs strictly worse than the b out of n bootstrap. Qualitatively similar results also hold in both the classification and regression set-tings (with the latter generally showing better perfor-mance) when P generates  X  X i from Normal, Gamma, or StudentT distributions, and when P uses a non-linear noisy mapping between  X  X i and Y i (so that we estimate a misspecified model).
 These experiments illustrate the statistical correctness of BLB, as well as its improved robustness to the choice of b . The results are borne out by our theoretical anal-ysis, which shows that BLB has statistical properties that are identical to those of the bootstrap, under the same conditions that have been used in prior analysis of the bootstrap. In particular, BLB is asymptotically consistent for a broad class of estimators and quality measures (see Kleiner et al. (2012) for proof): Theorem 1. Suppose that  X  is Hadamard differen-tiable at P tangentially to some subspace, with P and P ( j ) n,b viewed as maps from some Donsker class F to R such that F  X  is measurable for every  X  &gt; 0 , where F  X  = { f  X  g : f,g  X  F , X  P ( f  X  g ) &lt;  X  } and  X  P ( f ) = P ( f  X  Pf ) 2 1 / 2 . Additionally, assume that  X  is continuous in the space of distributions Q with respect to a metric that metrizes weak conver-gence. Then, up to centering and rescaling of  X   X  n s for any sequence b  X   X  and for any fixed s . Fur-thermore, the result holds for sequences s  X   X  if E |  X  ( Q n ( P ( j ) n,b )) | &lt;  X  .
 BLB is also higher-order correct: despite the fact that the procedure only applies the estimator in question to subsets of the full observed dataset, it shares the fast convergence rates of the bootstrap, which permit convergence of the procedure X  X  output at rate O (1 /n ) rather than the rate of O (1 / totic approximations. To achieve these fast conver-gence rates, some natural conditions on BLB X  X  hyper-parameters are required, for example that b =  X ( and that s be sufficiently large with respect to the vari-ability in the observed data. Quite interestingly, these conditions permit b to be significantly smaller than n , with b/n  X  0 as n  X  X  X  . See Kleiner et al. (2012) for our theoretical results on higher-order correctness. The experiments of the preceding section, though pri-marily intended to investigate statistical performance, also provide some insight into computational perfor-mance: as seen in Figure 1, when computing serially, BLB generally requires less time, and hence less total computation, than the bootstrap to attain comparably high accuracy. Those results only hint at BLB X  X  supe-rior ability to scale computationally to large datasets, which we now demonstrate in full in the following dis-cussion and via large-scale experiments.
 Modern massive datasets often exceed both the pro-cessing and storage capabilities of individual proces-sors or compute nodes, thus necessitating the use of parallel and distributed computing architectures. In-deed, in the large data setting, computing a single full-data point estimate often requires simultaneous dis-tributed computation across multiple compute nodes, among which the observed dataset is partitioned. As a result, the scalability of a quality assessment method is closely tied to its ability to effectively utilize such computing resources.
 Due to the large size of bootstrap resamples (recall that approximately 63% of data points appear at least once in each resample), the following is the most nat-ural avenue for applying the bootstrap to large-scale data using distributed computing: given data on a cluster of compute nodes, parallelize the estimate com-putation on each resample across the cluster, and com-pute on one resample at a time. This approach, while at least potentially feasible, remains quite problem-atic. Each estimate computation requires the use of an entire cluster of compute nodes, and the bootstrap repeatedly incurs the associated overhead, such as the cost of repeatedly communicating intermediate data among nodes. Additionally, many cluster computing systems in widespread use (e.g., Hadoop MapReduce) store data only on disk, rather than in memory, due to physical size constraints (if the data size exceeds the amount of available memory) or architectural con-straints (e.g., the need for fault tolerance). In that case, the bootstrap incurs the extreme costs associ-ated with repeatedly reading a very large dataset from disk; though disk read costs may be acceptable when (slowly) computing only a single point estimate, they easily become prohibitive when computing many esti-mates on one hundred or more resamples.
 In contrast, BLB permits computation on multiple (or even all) subsamples and resamples simultaneously in parallel, allowing for straightforward and effective dis-tributed and parallel implementations which enable ef-fective scalability and large computational gains. Be-cause BLB subsamples and resamples can be signifi-cantly smaller than the original dataset, they can be transferred to, stored by, and processed on individ-ual (or very small sets of) compute nodes. For ex-ample, we can naturally leverage modern hierarchical distributed architectures by distributing subsamples to different compute nodes and subsequently using intra-node parallelism to compute across different resamples generated from the same subsample. Note that gener-ation and distribution of the subsamples requires only a single pass over the full dataset (i.e., only a single read of the full dataset from disk, if it is stored only on disk), after which all required data (i.e., the subsam-ples) can be stored in memory. Beyond this significant architectural benefit, we also achieve implementation and algorithmic benefits: we do not need to parallelize the estimator computation internally, as BLB uses the available parallelism to compute on multiple resamples simultaneously, and exposing the estimator to only b rather than n distinct points significantly reduces the computational cost of estimation, particularly if the estimator computation scales super-linearly.
 We now empirically substantiate the preceding discus-sion via large-scale experiments performed on Amazon EC2. We use the representative experimental setup of Figure 1, but now with d = 3 , 000, n = 6 , 000 , 000, Y i  X  Bernoulli((1 + exp(  X  gistic regression implemented using L-BFGS. The size of a full observed dataset in this setting is thus ap-proximately 150 GB. We compare the performance of BLB and the bootstrap (now omitting the m out of n bootstrap and subsampling due to the shortcomings il-lustrated in Section 5), both implemented as described above (we parallelize the estimator computation for the bootstrap by simply distributing gradient compu-tations via MapReduce) using the Spark cluster com-puting framework (Spark, 2012), which provides the ability to either read data from disk or cache it in memory (provided that sufficient memory is available) for faster repeated access. For BLB, we use r = 50, s = 5, and b = n 0 . 7 . Due to the larger data size and use of a distributed architecture, we now imple-ment the bootstrap using Poisson resampling, and we compute ground truth using 200 independent dataset realizations. In the left plot of Figure 2, we show results obtained using a cluster of 10 worker nodes, each having 6 GB of memory and 8 compute cores; thus, the total mem-ory of the cluster is 60 GB, and the full dataset (150 GB) can only be stored on disk. As expected, the time required by the bootstrap to produce even a low-accuracy output is prohibitively high, while BLB provides a high-accuracy output quite quickly, in less than the time required to process even a single boot-strap resample. In the right plot of Figure 2, we show results obtained using a cluster of 20 worker nodes, each having 12 GB of memory and 4 compute cores; thus, the total memory of the cluster is 240 GB, and we cache the full dataset in memory for fast repeated access. Unsurprisingly, the bootstrap X  X  performance improves significantly with respect to the previous ex-periment. However, the performance of BLB (which also improves), remains substantially better than that of the bootstrap.
 Thus, relative to the bootstrap, BLB both allows more natural and effective use of parallel and distributed computational resources and decreases the total com-putational cost of assessing estimator quality. Finally, it is worth noting that even if only a single compute node is available, BLB allows the following somewhat counterintuitive possibility: even if it is prohibitive to actually compute a point estimate for the full observed data using a single compute node (because the full dataset is large), it may still be possible to efficiently assess such a point estimate X  X  quality using only a sin-gle compute node by processing one subsample (and the associated resamples) at a time.
 Like existing resampling-based procedures such as the bootstrap, BLB requires the specification of hyperpa-rameters controlling the number of subsamples and re-samples processed. Setting such hyperparameters to be sufficiently large is necessary to ensure good sta-tistical performance; however, setting them to be un-necessarily large results in wasted computation. Prior work on the bootstrap and related procedures gener-ally assumes that a procedure X  X  user will simply se-lect a priori a large, constant number of resamples to be processed (with the exception of Tibshirani (1985), who does not provide a general solution to this issue). However, this approach reduces the level of automa-tion of these methods and can be quite inefficient in the large data setting.
 Thus, we now examine the dependence of BLB X  X  per-formance on the choice of r and s , with the goal of bet-ter understanding their influence and providing guid-ance and adaptive (i.e., more automatic) methods for their selection. The left plot of Figure 3 illustrates the influence of r and s , giving the relative errors achieved by BLB with b = n 0 . 7 for different r,s pairs in the rep-resentative empirical setting described in Section 5. In particular, note that for all but the smallest values of r and s , it is possible to choose these values indepen-dently such that BLB achieves low relative error; in this case, choosing s  X  3 ,r  X  50 is sufficient. While these results are useful and provide some guid-ance, we expect the minimal sufficient values of r and s to change based on the identity of  X  (e.g., we expect a confidence interval to be harder to compute and hence to require larger r than a standard error) and the prop-erties of the underlying data. Thus, to help avoid the need to choose r and s conservatively, we now provide a means for adaptive hyperparameter selection, which we validate empirically.
 Concretely, to select r adaptively in the inner loop of Algorithm 1, we propose, for each subsample j , to con-tinue to process resamples and update  X   X  n,j until it has ceased to change significantly. Noting that the values  X   X  n,k used to compute  X  a subsample, for most forms of  X  the series of computed  X  n,j values will be well behaved and will converge (in many cases at rate O (1 / constant) to a constant target value as more resamples are processed. Therefore, it suffices to process resam-ples (i.e., to increase r ) until we are satisfied that  X  has ceased to fluctuate significantly; we propose us-ing Algorithm 2 to assess this convergence. The same scheme can be used to select s adaptively by process-ing more subsamples (i.e., increasing s ) until BLB X  X  Algorithm 2 Convergence Assessment
Input: A series z (1) ,z (2) ,...,z ( t )  X  R d
Output: true iff the input series is deemed to have ceased to fluctuate beyond the target relative error else false output value s  X  1 P s j =1  X   X  n,j has stabilized; in this case, one can simultaneously also choose r adaptively and independently for each subsample.
 The middle plot of Figure 3 shows the results of ap-plying such adaptive hyperparameter selection. For selection of r we use = 0 . 05 and w = 20, and for selection of s we use = 0 . 05 and w = 3. As seen in the plot, the adaptive hyperparameter selection allows BLB to cease computing shortly after it has converged (to low relative error), limiting the amount of unneces-sary computation that is performed. Though selected a priori, and w are more intuitively interpretable and less dependent on the details of  X  and the underlying data generating distribution than r and s . Indeed, the aforementioned specific values of and w yield results of comparably good quality when also used for a vari-ety of other synthetic and real data generation settings (see Section 8 below), as well as for different forms of  X  (see the righthand table in Figure 3, which shows that smaller values of r are selected when  X  is easier to compute). Thus, our scheme significantly helps to alleviate the burden of hyperparameter selection. Automatic selection of a value of b in a computation-ally efficient manner is more difficult due to the inabil-ity to reuse computations performed for different val-ues of b . One could consider similarly increasing b from some small value until the output of BLB stabilizes; devising a means of doing so efficiently is the subject of future work. Nonetheless, based on our fairly ex-tensive empirical investigation, it seems that b = n 0 . 7 is a reasonable and effective choice in many situations. We now present the results of applying BLB to sev-eral different real datasets. In this setting, given the absence of ground truth, it is not possible to objec-tively evaluate estimator quality assessment methods X  statistical correctness. As a result, we are reduced to comparing the outputs of different methods to each other; we also now report the average (across dimen-sions) absolute confidence interval width produced by each procedure, rather than relative error.
 Figure 4 shows results for BLB, the bootstrap, and the b out of n bootstrap on the UCI connect4 dataset (logistic regression, d = 42, n = 67 , 557). We select the BLB hyperparameters r and s using the adaptive method described in the previous section. Notably, the outputs of BLB for all values of b considered, and the output of the bootstrap, are tightly clustered around the same value; additionally, as expected, BLB con-verges more quickly than the bootstrap. However, the values produced by the b out of n bootstrap vary sig-nificantly as b changes, thus further highlighting this procedure X  X  lack of robustness. We have obtained qual-itatively similar results on six additional UCI datasets (ct-slice, magic, millionsong, parkinsons, poker, shut-tle) with different estimators (linear regression and lo-gistic regression) and a range of different values of n and d ; see Kleiner et al. (2012) for additional plots. We have presented a new procedure, BLB, which pro-vides a powerful new alternative for automatic, accu-rate assessment of estimator quality that is well suited to large-scale data and modern parallel and distributed computing architectures.

