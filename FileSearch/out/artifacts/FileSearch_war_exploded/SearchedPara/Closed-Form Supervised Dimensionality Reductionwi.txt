 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 USA Francisco Pereira fpereira@princeton.edu CSBMB, Green Hall, Princeton University, Princeton, NJ 08540 USA Geoffrey J. Gordon ggordon@cs.cmu.edu Dimensionality reduction (DR) is a popular data-processing technique that serves the following two main purposes: it helps to provide a meaningful in-terpretation and visualization of the data, and it also helps to prevent overfitting when the number of di-mensions greatly exceeds the number of samples, thus working as a form of regularization.
 When our goal is prediction rather than an (unsu-pervised) exploratory data analysis, supervised dimen-sionality reduction (SDR) that combines DR with si-multaneously learning a predictor can significantly out-perform a simple combination of unsupervised DR with a subsequent learning of a predictor on the result-ing low-dimensional representation (Pereira &amp; Gor-don, 2006; Sajama and Alon Orlitsky, 2005). The problem of supervised dimensionality reduction can be viewed as finding a predictive structure , such as a low-dimensional representation, which captures the in-formation about the class label contained in the high-dimensional feature vector while ignoring the  X  X oise X . However, existing SDR approaches are often restricted to specific settings, and can be viewed as jointly learning a particular mapping (most commonly, a lin-ear one) from the feature space to a low-dimensional hidden-variable space, together with a particular pre-dictor that maps the hidden variables to the class label. For example, SVDM (Pereira &amp; Gordon, 2006) learns a linear mapping from observed to hidden variables, effectively assuming Gaussian features when minimiz-ing sum-squared reconstruction loss; on the prediction side, it focuses on SVM-like binary classification using hinge loss. SDR-MM method of (Sajama and Alon Orlitsky, 2005) treats various types of features (e.g., binary and real-valued) but is limited to discrete clas-sification problems, i.e. is not suitable for regression. Recent work on distance metric learning (Weinberger et al., 2005; Weinberger &amp; Tesauro, 2007) treats both classification and regression settings, but is limited, like SVDM, to Gaussian features and linear mappings when learning Mahalanobis distances.
 This paper approaches SDR in a more general frame-work that views both features and class labels as exponential-family random variables, and allows to mix-and-match data-and label-appropriate general-ized linear models , thus handling both classification and regression, with both discrete and real-valued ing based on minimization of conditional probability of class given the hidden variables, while using as a regu-larizer the conditional probability of the features given the low-dimensional hidden-variable  X  X redictive X  rep-resentation.
 The main advantage of our approach, besides being more general, is using simple closed-form update rules when performing its alternate minimization procedure. This method yields a short Matlab code, fast perfor-mance, and is guaranteed to converge. The conver-gence property, as well as closed form update rules, re-sult from using appropriate auxiliary functions bound-ing each part of the objective function (i.e., reconstruc-tion and prediction losses). We exploit the additive property of auxiliary functions in order to combine bounds on multiple loss functions.
 We perform a variety of experiments, both on sim-ulated and real-life problems. Results on simulated datasets convincingly demonstrate that our SDR ap-proach can discover underlying low-dimensional struc-ture in high-dimensional noisy data, while outperform-ing SVM and SVDM, often by far, and practically al-ways beating the unsupervised DR followed by learn-ing a predictor. On real-life datasets, SDR approaches continue to beat the unsupervised DR by far, while often matching or somewhat outperforming SVM and SVDM. Let X be an N  X  D data matrix with entries denoted X nd where N is the number of i.i.d. samples, and n -th sample is a D -dimensional row vector denoted x n . Let Y be an N  X  K matrix of class labels for K sep-arate prediction problems (generally, we will consider K &gt; 1), where j -th column, 1  X  j  X  K , provides a set of class labels for the j -th prediction problem. Our supervised dimensionality approach relies on the assumption that each data point x n , n = 1 , ..., N , is a noisy version of some  X  X rue X  data point  X  n which lives in a low-dimensional space, and that this hidden representation of the noisy data is actually predictive about the class label.
 We will also assume, following ePCA (Collins et al., 2001), ( GL ) 2 M (Gordon, 2002), logistic PCA (Schein et al., 2003) and related extensions of PCA, that noise in the features follows exponential-family distributions with natural parameters  X  n , with different members of and that the noise is applied independently to each coordinate of x n . Namely, it is assumed that N  X  D parameter matrix  X  can be represented by a linear model in an L -dimensional ( L &lt; D ) space: where the rows of the L  X  D matrix V correspond to the basis vectors, the columns of the N  X  L matrix U correspond to the coordinates of the  X  X rue points X   X  , n = 1 , ...N in the L -dimensional space spanned by those basis vectors, and  X  X is the bias vector (cor-responding to the empirical mean in PCA). However, to simplify the notation, we will include  X  X as the ( L + 1) X  X  row of the matrix V (i.e., V L +1 =  X  X ), and add the ( L + 1) X  X  column of 1 X  X  to U , so that we can write  X  X as a product of two matrices,  X  X ( UV ) nd = Given the natural parameter  X  nd , an exponential-family noise distribution is defined for each X nd by log P ( X nd |  X  X where G x ( X  nd ) is the cumulant or log-partition func-tion that ensures that P ( X nd |  X  nd ) sums (or inte-grates) to 1 over the domain of X nd . This function uniquely defines a particular member of the exponen-tial family, e.g., Gaussian, multinomial, Poisson, etc. We can now view each row U n as a  X  X ompressed X  rep-resentation of the corresponding data sample x n that will be used to predict the class labels. We will again assume a noisy linear model for each class label Y k (column-vector in Y ) where the natural parameter is represented by linear combination with linear coefficients w = ( w 1 , ..., w L ) and K-dimensional bias vector  X  Y . As for  X  X will simplify the notation by including  X  Y as the ( L + 1) X  X  row of the matrix W , and write  X  Y ( UW ) nk = of exponential-family noise P ( Y nk |  X  Y dle both classification and regression problems. For example, in case of binary classification, we can model Y nk as a Bernoulli variable with parameter p nk and the corresponding natural parameter  X  nk = log( p nk 1  X  p and use logistic function  X  ( x ) = 1 1+ e  X  x to write the Bernoulli distribution for Y nk as In case of regression, Y nk will be a Gaussian variable with the expectation parameter coinciding with the natural parameter  X  Y In other words, we will use a generalized linear model (GLM) E ( X d ) = f  X  1 d ( UV d ) for each feature X d ( d -th column in X , 1  X  d  X  D ), and yet another set of GLMs E ( Y k ) = f  X  1 k ( UW ) for each class label Y k ( k -th column in Y , 1  X  k  X  K ), with possibly different link functions f d and f k (e.g., the logit link function f (  X  ) = identity link function f (  X  ) =  X  is used for real-valued regression with Gaussian output).
 SDR-GLM optimization problem . We formulate SDR as joint optimization of two objective functions corresponding to the reconstruction accuracy (data likelihood) and the prediction accuracy (class likeli-hood): where  X  X = UV ,  X  Y = UW , and the likelihoods above correspond to particular members of exponen-tial family. Then the SDR optimization problem is where the data likelihood can be viewed as a regu-larization added on top of the class likelihood maxi-mization objective, with the regularization constant  X  Comparison with SVDM . Note that the idea of combining loss functions for SDR was also proposed before in SVDM (Pereira &amp; Gordon, 2006), where, similarly to SVD, quadratic loss || X  X  UV || 2 2 was used for data reconstruction part of the objective, while the hinge loss was used for the prediction part (using U as the new data matrix). Herein, we generalize SVDM X  X  sum-squared reconstruction loss to log-likelihoods of exponential family, similarly to ePCA(Collins et al., 2001) and G 2 L 2 M (Gordon, 2002), replace hinge loss by the loglikelihoods corresponding to exponential-family class variables, and provide closed-form update rules rather than perform optimization at each iter-ation, which results into a significant speed up when compared with SVDM. Since the above problem (eq. 3) is not jointly convex in all parameters, finding a globally optimal solution is nontrivial. Instead, we can employ the auxiliary func-tion approach commonly used in EM-style algorithms, and using auxiliary function of a particular form, de-rive closed-form iterative update rules that are guar-anteed to converge to a local minimum. We show that an auxiliary function for the objective in eq. 3 can be easily derived for an arbitrary pair of L X and L Y provided that we know their corresponding auxiliary functions.
 Auxiliary functions. Given a function L (  X  ) to be maximized, a function Q (  X   X ,  X  ) is called an auxiliary function for L (  X  ) if L (  X  ) = Q (  X ,  X  ) and L (  X   X  )  X  Q ( for all  X   X  . It is easy to see that L (  X  ) is non-decreasing under the update i.e., L (  X  t +1 )  X L (  X  t ), and thus an iterative application of such updates is guaranteed to converge to a local maximum of L under mild conditions on L and Q . We will make use of the following properties of auxil-iary functions: Lemma 1 Let Q 1 (  X   X ,  X  ) and Q 2 (  X   X ,  X  ) be auxiliary func-tions for L 1 (  X  ) and L 2 (  X  ) , respectively. Then is an auxiliary function for L (  X  ) =  X  1 L 1 (  X  )+  X  2 where  X  i &gt; 0 for i = 1 , 2 .
 Proof. Q (  X   X ,  X  ) =  X  1 Q 1 (  X   X ,  X  )+  X  2 Q 2 (  X   X ,  X  )  X   X   X 
L 2 (  X   X  ) = L (  X   X  ), and Q (  X ,  X  ) =  X  1 Q 1 (  X ,  X  ) +  X  Q 2 (  X ,  X  ) =  X  1 L 1 (  X  ) +  X  2 L 2 (  X  ) = L (  X  ). Also, it is obvious that a function is an auxiliary for L (  X  ). This observations allows us to combine vari-ous dimensionality reduction approaches with appro-priate predictive loss functions, given appropriate aux-iliary functions for both (next section discusses two such combinations). When optimization of an auxil-iary functions yields an analytical solution (e.g., for quadratic functions), it is easy to obtain closed-form update rules for alternating minimization. Recall that natural parameters for X nd and Y nk variables are represented by  X  X and  X  Y and Q Y (  X   X  Y ,  X  Y ) be the auxiliary functions for the corresponding loglikelihoods in eq. 2, then
Q (  X   X  X ,  X  X ,  X   X  Y ,  X  Y ) = is an auxiliary function for the combined log-likelihood in eq. 3 when we fix two out of three parameters and optimize over the remaining one. The update rules for  X  U  X   X  U nl  X   X  U nl Note that we get simpler expressions for V and W since they appear only in Q X or only in Q Y parts of eq. 5, respectively.
 Auxiliary functions for Bernoulli and Gaussian log-likelihoods . For a Bernoulli variable s with nat-ural (log odds) parameter  X  we use the variational bound on log-likelihood L (  X  ) = log P ( s |  X  ) proposed by (Jaakkola &amp; Jordan, 1997) and used later by (Schein et al., 2003) where T = tanh (  X / 2)  X  . Note that the auxiliary function is quadratic in  X  and taking its derivatives leads to simple closed-form iterative update rules for U , V and W . For multinomial variables, one can use a recent extension of the above bound to multinomial logistic regression proposed by (Bouchard, 2007).
 For a Gaussian variable s with natural parameter  X  that coincides with the mean parameter (identity link function) we do not really have to construct an aux-iliary function, since we can simply use the negative squared loss proportional to the Gaussian loglikelihood as an auxiliary function itself, i.e.
 where c = (2  X  )  X  1 is a constant, assuming a fixed stan-dard deviation that will not be a part of our estimation procedure here, similarly to the approach of (Collins et al., 2001) and related work; c can be ignored since it will be subsumed by the parameter  X  .
 Using the above auxiliary functions, we can combine them into joint auxiliary functions as in eq. 5 for var-ious combinations of Bernoulli and Gaussian variables X nd and Y nk . Namely, assuming all X nd are Bernoulli, we get (Schein et al., 2003): while assuming all X nd are Gaussian, we get Note that Q Y (  X   X  Y Gaussian Y nk is obtained by replacing X with Y , V with W , and d with k in eq. 8 and 9, respectively. Due to lack of space, we omit the derivation of the it-erative update rules for the four versions of SDR-GLM that we experimented with (for more detail, see (Rish et al., 2008)): Gaussian-SDR , that assumes Gaus-sian X nd and Bernoulli Y nk , Bernoulli-SDR in case of Bernoulli X nd and Bernoulli Y nk , Gaussian-SDR-Regression and Bernoulli-SDR-Regression in case of Gaussian Y nk (appropriate for real-valued label, i.e. for the regression problem).
 Prediction step . Once we learn the parameters U , V and W , and thus  X  X and  X  Y , we can use them on test data in order to predict labels for previously unseen data. Given the test data X test , we only need to find the corresponding low-dimensional representa-tion U test by performing the corresponding iterative updates only with respect to U , keeping V and W fixed. Once U test is found, we predict the class labels as Y test = U test W . We evaluated our SDR-GLM methods on both sim-ulated and real-life datasets, in both classifica-tion and regression settings. We varied the low-dimensionality parameter L from 2 to 10, and eval-uated a range of regularization parameters  X  = 5.1. Classification Problems In the classification setting, we compared Bernoulli-SDR and Gaussian-SDR versus linear SVM 5 and ver-sus unsupervised dimensionality reduction followed by SVM and by logistic regression, which we refer to as SVM-UDR and Logistic-UDR , respectively. In both cases, unsupervised dimensionality reduction is per-formed first using the data-appropriate DR method (i.e., PCA for real-valued data and Logistic-PCA for binary data; this is equivalent to removing the predic-tion loss in eq. 3); then SVM or logistic regression are performed on the low-dimensional data representation U . For datasets with real-valued features, we also com-pared the above methods to SVDM (Pereira &amp; Gor-don, 2006). We performed k -fold cross-validation with k = 10 on the datasets with less than 1000 dimensions, and with k = 5 otherwise.
 Simulated data. In order to test our methods first on the data with controllable low-dimensional structure, we simulated high-dimensional datasets that indeed were noisy versions of some underlying easily-separable low-dimensional data. Particularly, a set of N = 100 samples from two classes was generated randomly in two-dimensional space so that the samples were lin-early separable with a large margin.
 Next, we simulated two sets of exponential-family ran-dom variables X nd , a Bernoulli set and a Gaussian set, using the coordinates of the above points for natu-ral parameters  X  nd , where the number of samples was N = 100 and the dimensionality of a  X  X oisy X  dataset varied from D = 100 to D = 1000. We then com-bined the data with the labels generated in the low-dimensional space and provided them as an input to our algorithms.
 Simulated data: Bernoulli noise . Figures 1a summa-rize the results for Bernoulli-noise dataset. Supervised DR methods (Bernoulli-SDR and Gaussian-SDR) ver-sus SVM and versus unsupervised DR followed by learning a predictor (Logistic-UDR and SVM-UDR); the reduced dimensionality parameters is set to L = 2 and the regularization constant is  X  = 0 . 0001 (the choice of those parameters is discussed below). We can see that Bernoulli-SDR significantly outper-forms all other methods, including SVM, and both Bernoulli-SDR and Gaussian-SDR also outperform the unsupervised DR followed by either logistic regres-sion or SVM. Apparently, Bernoulli-SDR is able to reconstruct correctly the underlying separable two-dimensional dataset and is robust to noise, as its error remains zero for up to 700 dimensions, and only in-creases slightly up to 0 . 05 for 1000 dimensions. On the other hand, SVM has zero-error prediction only in the lowest-dimensional case ( D = 100), and is much more sensitive to noise when the dimensionality of the data increases, making incorrect predictions in up to 14% to 21% cases when the dimensionality increases above D = 300. Apparently, SVM was not able to ex-ploit the underlying separable low-dimensional struc-ture disturbed by high-dimensional noise, while su-pervised dimensionality reduction easily detected this structure.
 Also, using the Bernoulli model instead of Gaussian when features are binary is clearly beneficial, and thus, as noted previously, proper extensions of PCA to exponential-family data must be used (Collins et al., 2001; Schein et al., 2003). However, previous work on logistic PCA by (Schein et al., 2003) demonstrated advantages of using the Bernoulli vs Gaussian assump-tion only for reconstruction of the original data, while this paper investigates the impact of such assumptions on the generalization error in supervised learning case. This is less obvious, since a good fit to the training data does not necessarily imply a good generalization ability, as shown by our experiments with unsuper-vised dimensionality reduction followed by learning a classifier.
 Regarding the choice of the regularization parameter  X  , we experimented with a range of values from 0 . 0001 to 10, and concluded that the smallest value was the most beneficial for both SDR algorithms; this is in-tuitive since it effectively puts most of the weight on the predictive loss. There is a clear trend (Figure 1b) in error decrease with the parameter decrease, where sufficiently low values of  X   X  0 . 1 yield quite similar low errors, but increasing  X  further, especially above 1, leads to a drastic increase in the classification error, especially in higher-dimensional cases. Note, however, that such tendency is not present in the other datasets we experimented with, where the effect of regulariza-tion constant can be non-monotonic, and thus under-scores the importance of using cross-validation or other Regarding the choice of the reduced-dimensionality pa-rameter L , for low values of  X  , we did not observe any significant variation in the results with increasing this parameter up to L = 10, e.g. the results for different L were practically identical when  X   X  0 . 1, while for higher values variance was more significant. Simulated data: Gaussian noise . Figure 1c shows the results for the Gaussian noise: supervised dimension-ality reduction provides a clear advantage over unsu-pervised ones, although the performance of SDR ver-sus SVM is somewhat less impressive, as Gaussian-SDR is comparable to SVM. However, Gaussian-SDR seems to outperform considerably another supervised dimensionality method, SVDM, proposed by (Pereira &amp; Gordon, 2006). SVDM was used with its regular-ization constant set to 100 since it provided the best SVDM performance among the same values of  X  as be-fore. Interestingly, the performance of SVDM does not show any monotonic dependence on this parameter. Real-life datasets. First, we considered several datasets with binary features , such as a 41-dimensional Sensor network dataset where the data represent con-nectivity (0 or 1) between all pairs of sensor nodes in a network of 41 light sensors (see Table 1). Note that Bernoulli-SDR with L = 10 and regularization parameter  X  = 0 . 1 outperformed SVM, Gaussian-SDR with L = 8 , 10 and same  X  = 0 . 1 matched SVM per-formance, while both the unsupervised dimensionality reduction methods followed by SVM and logistic re-gression -SVM-UDR and Logistic-UDR , respectively -performed worse. (Best results for each method are shown in the boldface.) Also, using the Bernoulli model for binary data instead of Gaussian seems to pay off: Bernoulli-SDR performs somewhat better than Gaussian-SDR. It is interesting to note that for really low dimensionality L = 2, all of the above methods have same error of 0.2, while increasing the dimen-sionality allows for much better performance, although this effect is non-monotonic.
 Another dataset related to network performance management, of somewhat larger dimensionality ( N = 169, D = 168), contains pairwise end-to-end network latency (ping round-trip times) collected by the PlanetLab measurement project (http://www.pdos.lcs.mit.edu/  X  simstrib/pl app) dis-cretized by applying a threshold as follows: above the average latency is considered  X  X ad X  (1) while the below-average latency is considered  X  X ood X  (0). We selected the first column (latencies of all 169 nodes to-wards the node 1) as the label, and predicted them given the remaining data. The results are shown in Table 2. The regularization parameters  X  selected by cross-validation were different here for different SDR methods: for Bernoulli-SDR,  X  = 100 turned out to be the best, while Gaussian-SDR performed better with  X  = 1. Overall, the results for different methods and varying dimensions L were surprisingly similar to each other, with both SDR methods achieving the lowest er-ror of 0 . 07 for L = 4 and L = 8, respectively, that was also achieved by SVM-UDR at L = 10, slightly out-performing SVM (0 . 10 error). Very similar results (not shown here due to lack of space) were also obtained on the Advertisement dataset from UCI ML repository. We experimented next with several extremely high-dimensional datasets from biological experiments that had real-valued features . The first dataset, called here Proteomics data , containing mass-spectrometry data for D = 38573 proteins (features), showing their  X  X x-pression levels X  in N = 50 female subjects (examples), 25 of which were pregnant (class 1), and the others were not (class 0). The results are shown in Table 3, comparing Gaussian-SDR with  X  = 0 . 001 (that yields lowest average SVDM error (among all dimensions) on this dataset) versus SVM, Logistic-UDR, SVM-UDR (both using the Gaussian assumption, i.e. PCA, for dimensionality reduction), and SVDM with its best-performing parameter 0 . 01. Despite its very high di-mensionality, this dataset turned out to be easy: both SVM and Gaussian-SDR achieved an error of 0 . 02, and Gaussian-SDR used only L = 4 dimensions to achieve it. On the contrary, unsupervised DR followed by pre-dictor learning (Logistic-UDR and SVM-UDR), suf-fered from really high errors at low dimensions, and only managed to achieve same low error at L = 10. SVDM was able to reach its lowest error (same 0 . 02) a bit earlier ( L = 6), although for L = 2 it incurred a huge error of 0 . 42, while Gaussian-SDR had 0 . 04 error at that same level of reduced dimensionality. Another truly high-dimensional dataset we used con-tained the fMRI recordings of subject X  X  brain activ-ity (measured using changes in the brain oxygenation levels) while the subject was observing on a screen words of two types, representing tools or buildings (see (Pereira &amp; Gordon, 2006) for details). The task was to learn a mind-reading classifier that would predict, given the fMRI data, what type of the word the subject was looking at. The features here correspond to fMRI voxels ( D = 14043 voxels were selected after some pre-processing of the data, as described in (Pereira &amp; Gor-don, 2006)), and there are N = 84 samples (i.e., word instances presented to a subject). This dataset was clearly more challenging then the previous one (both SVM X  X  and SDR X  X  errors were around 0 . 2).
 The results for all methods are shown in Table 4; for Gaussian-SDR we used  X  = 0 . 0001, while SVDM was best at 0 . 001 (as before, we used the average error over all dimensions to select best  X  ). For those values of  X  parameter, Gaussian-SDR matches SVM X  X  errors of 0 . 21 using just five dimensions ( L = 5), while SVDM reaches same error at L = 15 dimensions. Again, learning a predictor on  X  X ompressed X  data obtained via unsupervised dimensionality reduction was consis-tently worse than both supervised methods. 5.2. Regression Problems Finally, we did some preliminary experiments with the regression version of our SDR approach that makes Gaussian assumption about the label (re-sponse variable) Y and thus uses sum-squared predic-tive L Y ( UW ) loss in equation 3, comparing it with the state-of-art sparse-regression technique called the Elastic Net , which was shown to improve over both Lasso and ridge regression using a convex combination of the L 1-and L 2-norm regularization terms (Zou &amp; Hastie, 2005).
 We used the fMRI data from the 2007 Pitts-burgh Brain Activity Interpretation Competition (PBAIC)(Pittsburgh EBC Group, 2007), where the fMRI data were recorded while subjects were play-ing a videogame, and the task was to predict several real-valued response variables, such as level of anxi-ety the subject was experiencing etc. We used the data for the two runs (games) by the first subject, and for the Instructions response variable, learning from run 1 and predicting on run 2. The dataset con-tained N = 704 samples (measurements over time) and approximately D = 33 , 000 features (voxels). Fig-ure 2 compares the performance of Elastic Net and Gaussian-SDR-Regression, where the L parameter de-notes the number of active variables (voxels selected) by the sparse EN regression. We can see that SDR regression is comparable with the state-of-the art EN (or even slightly better) when the number of hidden dimensions is not too low. This paper proposes a family of SDR algorithms that use generalized linear models to handle various types of features and labels, thus generalizing previous ap-proaches in a unifying framework. Our SDR-GLM ap-proach is fast and simple: it uses closed-form update rules at each iteration of alternating minimization pro-cedure, and is always guaranteed to converge. Experi-ments on a variety of datasets show that this approach is clearly promising, although more empirical investi-gation is needed in case of SDR-regression.
 Although we only tested our approach with Gaussian and Bernoulli variables, it can be easily extended to multinomial variables (and thus to multiclass classi-fication) using a recent extension of the variational bound proposed in (Jaakkola &amp; Jordan, 1997) to multinomial logistic regression (soft-max)(Bouchard, 2007). Deriving closed-form update rules for other members of the exponential family (e.g., Poisson) re-mains a direction of future work. Another possible extension is to obtain closed-form SDR update rules for alternative DR methods, such as non-negative ma-trix factorization (NMF)(Lee &amp; Seung, 2000), simply plugging in NMF X  X  auxiliary function instead of (un-constrained) PCA-like quadratic-loss.
 Other potential applications of our approach in-clude dimensionality reduction on mixed-type data, weighted dimensionality reduction schemes (e.g., as-signing different weight to reconstruction error of dif-ferent coordinates in PCA and similar DR techniques), multitask learning, as well as semi-supervised learning, including only the reconstruction loss part of the ob-jective for unlabeled date, while keeping both recon-struction and prediction losses for the labeled ones. Finally, a more principled selection of the regulariza-tion constant  X  (e.g., using Bayesian approaches) is another open research direction.

