 Hong Chang hongch@cs.ust.hk Dit-Y an Yeung dyyeung@cs.ust.hk Hong Kong Man y mac hine learning and pattern recognition algo-rithms rely on a distance metric. Some commonly used metho ds are nearest neigh bor classi X ers, radial basis function net works and supp ort vector mac hines for classi X cation tasks and the k -means algorithm for clus-tering tasks. The performance of these metho ds often dep ends critically on the choice of an appropriate met-ric. Instead of choosing the metric man ually , a promis-ing approac h is to learn the metric from data automat-ically . This idea can be dated bac k to some early work on optimizing the metric for k -nearest neigh bor den-sity estimation (Fukunaga &amp; Hostetler, 1973). More recen t researc h along this line con tinued to dev elop various locally adaptiv e metrics for nearest neigh bor classi X ers, e.g., (Domeniconi et al., 2002; Friedman, 1994; Hastie &amp; Tibshirani, 1996; Lowe, 1995; Peng et al., 2002). Besides nearest neigh bor classi X ers, there are other metho ds that also perform metric learning based on nearest neigh bors, e.g., radial basis function net works and varian ts (Poggio &amp; Girosi, 1990). While class lab el information is available for metric learning in classi X cation tasks, suc h information is gen-erally una vailable in con ventional clustering tasks. To adapt the metric appropriately to impro ve the clus-tering results, some additional bac kground kno wledge or sup ervisory information should be made available. This learning paradigm between the sup ervised and unsup ervised learning extremes is referred to as semi-supervise d clustering , as con trasted to another type of semi-sup ervised learning tasks called semi-sup ervise d classi X c ation whic h solv es the classi X cation problem with the aid of additional unlab eled data.
 One type of sup ervisory information is in the form of limited lab eled data. 1 Based on suc h information, Sinkk onen and Kaski (2002) prop osed a local metric learning metho d to impro ve clustering and visualiza-tion results. Basu et al. (2002) explored using lab eled data to generate initial seed clusters for the k -means clustering algorithm. Also, Zhang et al. (2003) pro-posed a parametric distance metric learning metho d for both classi X cation and clustering tasks. Another type of sup ervisory information is in the form of pairwise similarit y or dissimilarit y constrain ts. This type of sup ervisory information is weak er than the  X rst type, in that pairwise constrain ts can be de-rived from lab eled data but not vice versa. Wagsta X  and Cardie (2000) and Wagsta X  et al. (2001) pro-posed using suc h pairwise constrain ts to impro ve clus-tering results. Klein et al. (2002) introduced spa-tial generalizations to pairwise constrain ts, so that the pairwise constrain ts can also have in X  X ence on the neigh boring data points. However, both metho ds do not incorp orate metric learning into the cluster-ing algorithms. Xing et al. (2003) prop osed using pairwise side information in a novel way to learn a global Mahalanobis metric before performing cluster-ing with constrain ts. Both Klein et al.'s and Xing et al.'s metho ds generally outp erform Wagsta X  et al.'s metho d in the exp erimen ts rep orted. Instead of us-ing an iterativ e algorithm as in (Xing et al., 2003), Bar-Hillel et al. (2003) devised a more e X cien t, non-iterativ e algorithm called relev ant comp onen t analy-sis (RCA) for learning a global Mahalanobis metric. However, their metho d can only incorp orate similarit y constrain ts. Shen tal et al. (2004) extended the work of (Bar-Hillel et al., 2003) by incorp orating both pair-wise similarit y and dissimilarit y constrain ts into the exp ectation-maximization (EM) algorithm for mo del-based clustering based on Gaussian mixture mo dels. Kw ok and Tsang (2003) established the relationship between metric learning and kernel matrix adaptation. To summarize, we can categorize metric learning meth-ods according to two di X eren t dimensions. The  X rst dimension is concerned with whether ( supervise d ) clas-si X cation or ( unsup ervise d ) clustering is performed. Most metho ds were prop osed for classi X cation tasks, but some recen t metho ds extended metric learning to clustering tasks under the semi-sup ervised learning paradigm. Sup ervisory information may be in the form of class lab el information or pairwise (dis)similarit y information. The second dimension categorizes met-ric learning metho ds into glob al and local ones. Pro-vided that su X cien t data are available, local metric learning is generally preferred as it is more  X  X xible in allo wing di X eren t local metrics at di X eren t locations of the input space. In this pap er, we prop ose a new metric learning metho d for semi-sup ervised clustering with pairwise similarit y side information. While our metho d is local in the sense that it performs met-ric learning through locally linear transformation, it also achiev es global consistency through interaction between adjacen t local neigh borho ods.
 The rest of this pap er is organized as follo ws. In Sec-tion 2, we presen t our metric learning metho d based on locally linear transformation. We also form ulate the learning problem as an optimization problem. In Section 3, we presen t two metho ds for solving this op-timization problem. Exp erimen tal results on both toy and real data are presen ted in Section 4, comparing our metho d with some previous metho ds. Finally , some concluding remarks are given in the last section. 2.1. Basic Ideas Let us denote a set of n data points in a d -dimensional input space by X = f x 1 ; x 2 ; : : : ; x n g . As in (Bar-Hillel et al., 2003), we only consider pairwise similarit y con-strain ts whic h are given in the form of a set S 0 of simi-lar point pairs. Intuitiv ely, we want to transform the n data points to a new space in whic h the points in eac h similar pair will get closer to eac h other. To preserv e the top ological relationships between data points, we move not only the points involved in the similar pairs but also other points. For computational e X ciency , we resort to linear transformation. One promising ap-proac h is to apply locally linear transformation so that the overall transformation of all points in X is linear lo-cally but nonlinear globally , generalizing previous met-ric learning metho ds based on applying linear trans-formation globally (Bar-Hillel et al., 2003; Xing et al., 2003). We call this new metric learning metho d locally line ar metric adaptation (LLMA). However, caution should be tak en when applying linear transformation to reduce the distance between similar points, as a degenerate transformation will simply map all points to the same location so that all inter-p oint distances vanish (and hence become the smallest possible). Ob-viously this degenerate case is undesirable and should be avoided. 2.2. Metric Adaptation as an Optimization We now pro ceed to devise the metric learning algo-rithm more formally . We  X rst generate the transitiv e and re X  X ctiv e closure S from S 0 . For eac h point pair ( x r ; x s ) 2 S , we apply a linear transformation to the matrix A r and d -dimensional vector c r . If a data point is involved in more than one point pair, we consider the transformation for eac h pair separately . The same lin-ear transformation is also applied to every data point x i in the neigh borho od set N r of x r . In other words, every data point x i 2 N r is transformed to where b r = ( I  X  A r ) x r + c r is the translation vector for all points x i 's in N r .
 However, a data point x i may belong to multiple neigh borho od sets corresp onding to di X eren t point pairs in S . Thus, the new location y i of x i is the overall transformation e X ected by possibly all similar point pairs (and hence neigh borho od sets): where  X  ri = 1 if x i 2 N r and 0 otherwise.
 Let m denote the num ber of point pairs in S . Thus a total of m di X eren t transformations have to be es-timated from the point pairs in S , requiring O ( md 2 ) transformation parameters in f A r g and f b r g . When m is small compared with the dimensionalit y d , we cannot estimate the O ( md 2 ) transformation parame-ters accurately . One way to get around this problem is to focus on a more restrictiv e set of linear transforma-tions. The simplest case is to allo w only translation, whic h can be describ ed by md parameters. Obviously , translating all data points in a neigh borho od set by the same amoun t leads to no change in the inter-p oint dis-tances. Although some data points may fall into mul-tiple neigh borho od sets and hence this phenomenon does not hold, we want to incorp orate an extra de-gree of freedom by changing the neigh borho od sets to Gaussian neigh borho od functions. More speci X cally , we set A r to the iden tity matrix I and express the new location y i of x i as where  X  ri is a Gaussian function de X ned as with  X  r being the covariance matrix. For simplicit y, we use a hyperspherical Gaussian function, meaning that the covariance matrix is diagonal with all diag-onal entries being ! 2 . Thus  X  ri can be rewritten as  X  ri = exp  X   X k x i  X  x r k 2 = (2 ! 2  X  ) : Note that (1) can be expressed as where B = [ b 1 ; b 2 ; : : : ; b m ] is a d  X  m matrix and vector. For data points that are far away from all points involved in S (and hence the cen ters of the neigh borho ods), all  X  ri 's are close to 0 and hence those points essen tially do not move (since y i  X  x i ). We now form ulate the optimization problem for  X nd-ing the transformation parameters. The optimization criterion is de X ned as where d S is the sum of squared Euclidean distances for all similar pairs in the transformed space and P , a penalt y term used to constrain the degree of transformation, is de X ned as where q ij = k y i  X  y j k and d ij = k x i  X  x j k repre-sen t the inter-p oint Euclidean distances in the trans-formed and original spaces, resp ectiv ely. N  X  ( d ij ) is again in the form of a Gaussian function, as N  X  ( d ij ) = exp  X   X  d 2 ij = X  2  X  ; with parameter  X  specify-ing the spread of the Gaussian windo w. The regular-ization parameter  X  &gt; 0 in (3) determines the rela-tive signi X cance of the penalt y term in the objectiv e function for the optimization problem. Note that the optimization criterion in (3) is analogous to objectiv e functions commonly used in energy minimization mo d-els suc h as deformable mo dels (Cheung et al., 2002), with the penalt y term P playing the role of an internal energy term. 2.3. Iterativ e Metric Adaptation Pro cedure The optimization problem form ulated above is solv ed in an iterativ e manner, resulting in an iterativ e met-ric adaptation pro cedure. To increase the local speci- X cit y gradually over time to allo w global nonlinearit y in the transformation, the Gaussian windo w parame-ters ! and  X  determining the neigh borho od size and the weigh ts in the penalt y term, resp ectiv ely, should decrease over time. We apply a simple metho d of decreasing the windo w parameters: ! ( t ) =  X  q ( t ) =  X  ( t ) =  X  ! ( t ) , for iteration t = 1 ; 2 ; : : : , where q average inter-p oint Euclidean distance in the trans-formed space over all point pairs in X (i.e., q ( t ) = stan t parameters.
 At iteration t , given the data point locations f y ( t ) and the windo w parameters ! ( t ) and  X  ( t ) , the overall optimization criterion in (3) is rewritten as the opti-mization criterion for iteration t : J = X Note that y r , y s and q ij dep end on f b r g and f y ( t ) However, for simplicit y, the dep endency is not explic-itly sho wn on the righ t-hand side of (5). We seek to minimize J ( t ) by  X nding the optimal values of f b r g as f b r g , whic h are then used to compute the location There are two stopping criteria in our iterativ e algo-rithm. The  X rst criterion is based on the ratio  X  ( t ) of the average inter-p oint distance over point pairs in S to that over all point pairs in X (i.e., q ( t ) ). The pro-cedure will stop when  X  ( t ) becomes smaller than some presp eci X ed threshold  X  . Another stopping criterion is simply to set a maxim um num ber of iterations T . The metric learning pro cedure will stop when either stopping criterion is satis X ed.
 We summarize our LLMA algorithm as follo ws: 1. y (1) i = x i , 1  X  i  X  n ; t = 1. 2. If  X  ( t ) &lt;  X  or t = T , then exit. 5. Compute the optimal b ( t ) r , 1  X  r  X  m , by mini-6. Update all data points as 7. t = t + 1; go to Step 2.
 In the algorithm, Step 5 is the key step whic h solv es the optimization problem for eac h iteration based on the criterion in (5). In the next section, we presen t two metho ds for solving this optimization problem. We now pro ceed to solv e the optimization problem in Step 5 of the LLMA algorithm sho wn above. Tw o dif-feren t optimization metho ds are discussed in the fol-lowing two subsections. 3.1. Gradien t Metho d While the  X rst term of J ( t ) in (5) is quadratic in f b the second term is of a more complex form. So we can-not  X nd a closed-form solution for the optimal values of f b r g simply by solving r b obtain an appro ximate closed-form solution where and s ij = 1 if ( x i ; x j ) 2 S and 0 otherwise. U + 2 de-notes the pseudo-in verse of U 2 . 3.2. Iterativ e Ma jorization Let us de X ne two d  X  n matrices Y = [ y 1 ; y 2 ; : : : ; y and Z = [ z 1 ; z 2 ; : : : ; z n ] for n data points before and after transformation, resp ectiv ely. 2 From (2), we have where  X  = [  X  1 ;  X  2 ; : : : ;  X  n ] is an m  X  n matrix. The optimization problem is then equiv alen t to minimiza-tion of J ( L ) with resp ect to L .
 The optimization criterion J ( L ) can be rewritten as: We can omit the second term since it does not dep end on L . The equiv alen t optimization criterion is where Since this form is the same as that for multidimen-sional scaling for discriminan t analysis (W ebb, 1995), we can solv e the optimization problem by iter ative ma-jorization , whic h can be seen as an EM-lik e algorithm for problems with no missing data. We de X ne and with Then the optimization problem consists of the follo w-ing steps: 3 1. Initialize L (0) ; u = 0. 2. u = u + 1; and compute 3. If con verged, then stop; otherwise go to Step 2. 3.3. Other Metho ds Recall that the penalt y term P in (3) serv es to con-strain the degree of transformation, partly to avoid the occurrence of a degenerate transformation and partly to preserv e the local top ological relationships between data points. Besides de X ning the penalt y term as in (4), there also exist other ways to achiev e this goal. One possibilit y is to preserv e the locally linear rela-tionships between nearest neigh bors, as in a nonlinear dimensionalit y reduction metho d called locally line ar emb edding (LLE) (Ro weis &amp; Saul, 2000). Due to page limit, details of this metho d are omitted here. To assess the e X cacy of LLMA, we perform extensiv e exp erimen ts on toy data as well as real data from the UCI Mac hine Learning Rep ository . 4 4.1. Illustrativ e Examples Figure 1 demonstrates the power of our LLMA metho d by comparing it with the RCA metho d (Bar-Hillel et al., 2003) on three toy data sets. 5 RCA, as a metric learning metho d, changes the feature space by a global linear transformation whic h assigns large weigh ts to relev ant dimensions and low weigh ts to ir-relev ant dimensions. The relev ant dimensions are es-timated based on connected comp onen ts comp osed of similar patterns. For eac h data set, we randomly select 10 similar pairs to form S 0 . While RCA can perform well on the  X rst data set, its performance is signi X -can tly worse than LLMA on the second and third data sets whic h are much more di X cult cases. On the other hand, LLMA can give satisfactory results for all three cases. More details about these exp erimen ts will be given in Section 4.3. 4.2. Clustering Algorithms and Performance In order to assess the e X cacy of LLMA for semi-sup ervised clustering, we compare the clustering re-sults based on k -means with and without metric learn-ing. Besides RCA metho d, we also rep eat the ex-perimen ts using the constrained k -means algorithm (W agsta X  et al., 2001). Constrained k -means algo-rithm is based on default Euclidean metric sub ject to the constrain ts that patterns in a pair ( x r ; x s ) 2 S are alw ays assigned to the same cluster. More speci X cally , the follo wing four clustering algorithms are compared: 1. k -means without metric learning 2. Constrained k -means without metric learning 3. k -means with RCA for metric learning 4. k -means with LLMA for metric learning The Rand index (Rand, 1971) is used to measure the clustering qualit y in our exp erimen ts. It re X  X cts the agreemen t of the clustering result with the ground truth. Let n s be the num ber of point pairs that are as-signed to the same cluster (i.e., matc hed pairs) in both the resultan t partition and the ground truth, and n d be the num ber of point pairs that are assigned to di X eren t clusters (i.e., mismatc hed pairs) in both the resultan t partition and the ground truth. The Rand index is de X ned as the ratio of ( n s + n d ) to the total num ber of point pairs, i.e., n ( n  X  1) = 2. When there are more than two clusters, however, the standard Rand index will favor assigning data points to di X eren t clusters. We mo dify the Rand index as in (Xing et al., 2003) so that matc hed pairs and mismatc hed pairs are assigned weigh ts to give them equal chance of occurrence (0.5). To see how di X eren t algorithms vary their performance with the bac kground kno wledge pro vided, we use 20 randomly generated S 0 sets for eac h data set. More-over, we compute the average Rand index over 20 ran-dom runs of (constrained) k -means for eac h S 0 set. The results for all four algorithms are then sho wn as box-plots using MA TLAB. 4.3. Semi-Sup ervised Clustering on Toy and In the LLMA algorithm, there are a few parameters that need to be set before running the exp erimen ts. These parameters are quite easy to set based on their physical meanings. The two parameters,  X  and  X  , for the deca y functions of the Gaussian windo ws are set to [0 : 1 ; 3] and (0 ; 1), resp ectiv ely. The regularization pa-rameter  X  adjusting the tradeo X  between local trans-formation and geometry preserv ation is set to [1 ; 5]. For the stopping criteria, we set  X  to [0 : 1 ; 0 : 2] and T to 5 (i.e., very few iterations of the LLMA algorithm are run). All data sets are normalized before applying the four algorithms. Gradien t metho d is used to ob-tain the exp erimen tal results sho wn, whic h are similar to those obtained using iterativ e ma jorization. Figure 2 sho ws the clustering results for the three toy data sets as illustrated in Section 4.1. Obviously , all the three data sets cannot be clustered well using the standard and constrained k -means algorithms. Even RCA can give good result only on the  X rst data set. On the other hand, LLMA can handle all these cases and perform particularly well on the second and third data sets whic h cannot be handled satisfactorily by the other metho ds.
 We further conduct exp erimen ts on nine UCI data sets. The num ber of data points n , the num ber of features d , the num ber of classes c , and the num ber of ran-domly selected similar pairs jS 0 j are sho wn under eac h sub X gure in Figure 3. From the clustering results, we can see that LLMA outp erforms the other metho ds for most of these data sets. As for the iris, Boston housing and balance data sets, RCA can impro ve the clustering results most.
 To summarize, these exp erimen tal results on both toy and real data sets demonstrate the e X ectiv eness of our LLMA metho d. In this pap er, we have prop osed a new metric learning metho d called LLMA for semi-sup ervised clustering. Unlik e previous metho ds whic h can only perform lin-ear transformation globally , LLMA performs nonlinear transformation globally but linear transformation lo-cally . This generalization mak es it more powerful for solving some di X cult clustering tasks as demonstrated through the toy data sets. To solv e the optimization problem as one step in the LLMA algorithm, we have presen ted two metho ds and hin ted some other possi-bilities, suc h as a spectral metho d like that used in LLE. We have also compared our metho d with some previous metho ds using real data sets.
 Note that in LLMA, the original input space and the transformed space are explicitly related via a mapping, as Y = L X  , where  X  is a nonlinear function with re-spect to X . Although it is not necessary for clustering problems, it is possible for new data points added to the input space to be mapp ed onto the transformed space. This possibilit y will be explored as we extend our LLMA metho d to other applications.
 Curren tly, our metho d can only utilize similarit y con-strain ts. A natural question to ask is whether we can extend LLMA by incorp orating dissimilarit y con-strain ts. In principle this is possible, but the optimiza-tion criterion has to be mo di X ed in order to incorp o-rate the new constrain ts. One challenge to face is to main tain the form of the objectiv e function so that the optimization problem remains tractable.
 Moreo ver, we have only considered a restrictiv e form of locally linear transformation, namely , translation. A poten tial direction to pursue is to generalize it to more general linear transformation types. Other possible re-searc h directions include impro ving the curren t LLMA algorithm suc h as performing globally linear transfor-mation  X rst and then LLMA only when necessary .
