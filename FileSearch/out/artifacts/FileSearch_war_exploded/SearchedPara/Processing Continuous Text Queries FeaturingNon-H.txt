 In this work we are interested in the scalable processing of content filtering queries over text item streams. In particular, we are aiming to generalize state of the art solutions with non-homogeneous scor-ing functions combining query-independent item importance with query-dependent content relevance. While such complex ranking functions are widely used in web search engines this is to our knowl-edge the first scientific work studying their usage in a continuous query scenario. Our main contribution consists in the definition and the evaluation of new efficient in-memory data structures for index-ing continuous top-k queries based on an original two-dimensional representation of text queries. We are exploring locally-optimal score bounds and heuristics that efficiently prune the search space of candidate top-k query results which have to be updated at the arrival of new stream items. Finally, we experimentally evalu-ate memory/matching time trade-offs of these index structures. In particular we experimentally illustrate their linear scaling behavior with respect to the number of indexed queries.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering, Selection process continuous top-k query processing, non-homogeneous scoring func-tions, text streams
Web 2.0 technologies have transformed the Web from a publishing-only environment into a vibrant information place where yester-day X  X  end users become nowadays content generators themselves. The vast amounts of user generated content available in various social media (Facebook, Twitter, blogs, discussion forums) in con-junction with traditional information producers (e.g., newspapers, television, radio) poses new challenges in achieving an effective, near real-time information awareness . Given the vast diversity and burstiness of the information published on Web 2.0 each minute [8], there is a vital need for efficient continuous information stream fil-tering systems capable of serving the information needs of millions of users.

In order to retrieve information items of interest from dif-ferent streams, users issue keyword queries. Such queries can be directly evaluated by a dedicated search engine aggregating news ( news.google.com, news.search.yahoo.com, search.msn.com/news ), blog messages ( blogsearch.google.com ) and microblogging events ( twitter.com/#!/search-home ). An alter-native way to observe information streams is to submit queries to alert services ( www.google.com/alerts, alerts.yahoo.com ) which continuously notify users about newly published items matching their filtering criteria. Since, the number of matching items can rapidly become too high, filtering is usually enhanced with ade-quate ranking functions. Ranking consists in estimating for each item its (query-independent) importance and/or (query-dependent) content relevance . Standard relevance scores are cosine similarity and Okapi BM25 [13]. Item importance can be estimated by mea-sures like information novelty [5], source authority [3, 9, 14, 15] or user attention [19, 11]. Moreover, to take into account information freshness, newer items are usually considered as more important than older ones. This functionality is supported either by using time decay functions [2] or by sliding window techniques.
The functionality of commercial web alerts systems is reminis-cent of continuous top-k textual query evaluation [16]. For ex-ample, the Google Alerts service periodically evaluates separately every submitted query on the Google News engines according to a predefined refreshing policy, retrieves the top-k results at that moment and notifies users for newly published items. However, transforming a continuous query to a series of periodically exe-cuted snapshot queries incurs serious limitations. For large num-bers of user queries and high item arrival rates (as in social media) it is practically impossible to keep the alert generation process syn-chronized with the incoming information streams. For this reason, commercial systems usually decrease the frequency of snapshot query evaluation and thus important newly published items may be missed by a user. On the other hand, snapshot queries on a local web archive may support more expressive filtering conditions and ranking operations than those currently supported in a continuous scenario.

In this paper we introduce a new class of continuous top-k textual queries featuring dynamic non-homogeneous total ranking functions which combine information freshness, query dependent text similarity and item importance scores. To the best of our knowledge, existing continuous top-k textual query processing sys-tems [16, 6, 17, 7] are based on variations of the Threshold Algo-rithm [4] and a one-dimensional ordering of queries which can only be defined by using monotonic and homogeneous ranking func-tions. In this respect, the main contributions of this paper are the following:
The rest of the paper is organized as follows. Section 2 gives a detailed problem statement. Section 3 presents our proposed in-verted query indexing and pruning techniques. Then, Section 4 describes a number of index implementations that apply these tech-niques. Related work is presented in Section 5 and Section 6 pro-vides the experimental evaluation. Finally, Section 7 gives a brief summary of our current and future work.
This section presents the notion of continuous top-k queries combining query-dependent and query-independent scores for ranking items from textual data streams.
We follow the traditional Vector Space Model approach [13] for the definition of items, queries and content similarity.
 terms V defines a | V | -dimensional space R | V | where each di-mension corresponds to a term t in V . Each vector ~ X  in represents either a query q or a text item i over V . We denote by  X  q,t and  X  i,t the weight (value) of term (dimension) t in query (vector) q and item (vector) i respectively.

Term weights can be assigned to queries and items using any weighting scheme, such as tf-idf or Okapi BM25. Without loss of generality, we also assume that query and item weights are both normalized, i.e. the sum of the weights for each query and each item is equal to 1 . We also say that a query q or an item i is relevant (irrelevant) to some term t if the corresponding weight  X  respectively, is greater than (equal to) 0 . This definition allows us to use a simplified set semantics for terms and items and to write t  X  q or t  X  i to denote that t is relevant to q or i respectively.
Our goal is to rank items with respect to queries by combin-ing query-dependent similarity scores with query-independent item scores [20] :
D EFINITION 2 ( TOTAL SCORES ). Let Q be a possibly infinite set of queries and I be a possibly infinite set of items. We define three ranking functions: So far, we have considered a possibly infinite set of items I . Now, suppose that this set is published in form of a stream and let I (  X  ) be the set of items published until time instant  X  : I (  X  )  X  I (  X  0 )  X  I for all time instants  X  &lt;  X  0 . Let Q be a finite set of queries and S total : Q  X  I (  X  )  X  [0 , 1] be a ranking function which computes for each item i  X  I (  X  ) and query q  X  Q a static total score S total ( i,q ) 1 .

Considering freshness of information is an important aspect of ranking items deriving from a stream. As new items are contin-uously inserted into the stream, it is crucial to appreciate more re-cently published pieces of information as more important than older ones. The two strategies most commonly employed over streams in order to take account of aging of information are sliding win-dows and time decay functions . While sliding windows has been a methodology widely used for ranking tuple based streams, more recent approaches on ranking streams of textual information pro-pose using continuously decaying scores, an approach which we also follow in this work.
 D EFINITION 3 ( DECAY FUNCTION ). A function decay : R  X  R  X  R is a decay function applied on a score s  X  R after some time interval  X   X   X  R iff decay ( s, 0) = s and decay ( s,  X   X  ) is monotonically decreasing for increasing  X   X  : D EFINITION 4 ( DECAYED SCORE ). The decayed score S decay ( i,q, X  ) of item i with respect to q at some time instant  X  is then defined as the decay of the total score S total ( i,q ) with respect to the age  X   X   X  i of item i : Constant  X  i denotes the publication date of item i .
Observe that S total ( i,q ) is undefined until the publication of item i in stream I (  X  ) and remains constant after its initial definition.
Continuous top-k queries and results over a stream are defined as follows.
 sult R ( q, X ,k ) of some (top-k ) query q  X  Q at some time instant  X  with respect to some decayed ranking function S decay is an ordered subset of maximally k items i  X  I (  X  ) such that i shares at least one term with query q and there exists no item i  X  ( I (  X  )  X  R ( q, X ,k )) where S decay ( i 0 ,q, X  ) &gt; S
Based on the previous definition we can state the following gen-eral continuous top-k query evaluation problem: Given a set of queries Q , a decayed ranking function S decay an item stream I (  X  ) , maintain for each query q  X  Q its top-k result R ( q, X ,k ) at any time instant  X  .

At a processing level this general problem is solved by a system which 1) adds each new stream item i in all relevant top-k results R ( q, X  i ,k ) and 2) maintains the results by removing and replacing items on top-k results, when necessary due to changes on scores over time. These two problems can be defined separately.

The first problem can be solved by a transactional system where each item arrival triggers a transaction maintaining the top-k lists of all relevant queries in Q . Updates are atomic actions isolated from each other.
 note by S min ( q, X  ) the score of the last item in R ( q,i,k ) at time instant  X  . Given a set of queries Q and an item i arriving at some time instant  X  i , update the top-k result for all queries q  X  Q where S total ( i,q ) &gt; S min ( q, X  i ) .

Observe that decay does not need to be applied on the score of the newly arriving item, as the time interval since its publication is of zero length. We will denote by U ( i ) the set of all queries whose top-k result will be updated by item i at the arrival time instant  X 
It is easy to show that the complexity of the query filtering prob-lem depends on the number of queries | Q | and the ranking function S total . A trivial solution to this problem is to compute at the arrival of each new item i at some time instant  X  i its score S total compare it to the minimal score S min ( q, X  i ) of all queries which share at least one term with the item. However, this solution ob-viously does not scale in the size of the items, i.e. number of terms it contains, and in the number of queries relevant to these terms. A better solution then is to define appropriate index struc-tures that prune the search space and avoid searching all queries q  X  U ( i ) for potential updates. In this article we are concerned by the increased complexity of the filtering problem introduced by non-homogeneous ranking functions (see Section 2.1). This is to our knowledge the first scientific work in this direction.
The second sub-problem derives from the application of decay on the scores, which adds a temporal dimension to the query fil-tering results: as time goes by, all scores change due to decay and even without the arrival of new items, top-k results can potentially change. We will call this the result maintenance problem . Given a decayed ranking function S decay ( i,q, X  ) , maintain for each query q  X  Q its top-k result R ( q, X ,k ) for any time instant  X  .
The complexity of result maintenance problem obviously de-pends on the decay function. We can essentially distinguish be-tween two kinds of decay functions: 1. Order-preserving functions, which guarantee that the relative 2. Non order-preserving decay functions, which do not fulfill
More formally, order preserving decay guarantees that for all time instants  X  ,  X   X , all items i , i  X  and all queries q , S decay ( i 0 ,q, X  0 ) . In this case, the continuous top-k query prob-lem is limited to query filtering, as top-k results can only be altered by the arrival of new items. In the second non order-preserving case the top-k results might change independently to the arrival of new items, making the result maintenance problem much more dif-ficult. It is worth noticing that most of the recent works on streams of news or blogs consider linear [19] and exponential decay func-tions [3, 14], which are order-preserving , rather than the opposite as in the case of Sigmoid functions[9]. Note also that sliding win-dow could be defined as a non order-preserving decay function. In this work we do not consider the problem of result maintenance generated by non-order preserving decay functions.
 tions. Ranking functions employed by web search engines [20] are in general more complex than those considered by exist-ing continuous top-k query evaluation algorithms [16, 6, 17, 7]. They essentially combine different query-dependent and query-independent scores which go beyond the monotonic and homoge-neous text similarity scores like cosine similarity. More precisely, a ranking function sim : R | V |  X  R | V |  X  R over some vector space R | V | is homogeneous of degree i if sim ( n  X  x,n  X  y ) = n for all non-zero n  X  R and vectors x,y  X  R | V | . It is easy to see that this property holds for functions like cosine similarity, but not for our definition of total score with non-zero  X  . Homogeneity and monotonicity are two necessary conditions for existing threshold-based algorithms for defining a total order over items (snapshot query setting) or queries (continuous query setting). Opposite to these works, we are interested in this paper in continuous queries featuring non-homogeneous ranking functions. As we will see in the following section, this extension raises new challenges to the query filtering problem.
In this section we present a new approach for processing large collections of continuous top-k queries over item streams. From a processing point of view the query filtering problem consists in identifying for each new item i all queries U ( i ) which must add item i to their top-k result. The main optimization goal is to re-duce this search space and to compute a smallest possible num-ber of candidate queries containing U ( i ) . Our solution works for non-homogeneous ranking functions which makes it more general than other continuous top-k query filtering solutions that are re-stricted to monotonic and homogeneous functions for computing item scores [17, 6, 7]. This generalization is achieved by replacing a total ordering of queries with a two-dimensional query represen-tation as explained in Section 3.1. Based on this search space rep-resentation, we then introduce a number of linear constraints (Sec-tion 3.1) spatially characterizing different sets of candidate queries. Later, in Section 4, we present and compare a different index im-plementations for efficiently evaluating these conditions.
For the sake of simplicity, we first abstract the notion of time and decay by considering that all scores are computed for a fixed time instant  X  0 (all variables referring to time instants or time periods disappear from the corresponding definitions). Later, in Section 3.2 we will show how decay can be added to this scenario.

For each term t in the vocabulary of terms V , we define a set of points in a two-dimensional space called the inverted query index of t and denoted P ( t ) : query index P ( t ) of a term t over a set of queries Q is the set: where S min ( q ) = S min ( q, X  0 ) .

Observe that each inverted query index P ( t ) contains all queries that are relevant for term t . Equivalently, for finding all candidate queries of an item i it is sufficient to explore the inverted query indexes P ( t ) of all terms t  X  i .

For example, let q be a query containing two terms, t and t  X . As we can see in Figure 1, q is stored in the corresponding indexes P ( t ) and P ( t 0 ) and positioned in the coordinates defined by the term weights and minimum score at that time instant. The right part of the image zooms into P ( t ) index. All queries q index contain term t with a positive weight.

Starting from this search space representation we define three linear upper bound conditions filtering for each new item i and for any term t  X  i a subset of P ( t ) containing candidate queries whose top-k result is potentially updated by i . For performance reasons, apart from knowledge on the item, each condition should depend only on information available locally for all queries q in the inverted query index P ( t ) of a term t , i.e. the query term weight  X  the minimal top-k list score S min ( q ) . This locality reduces the precision of search space pruning conditions, but as other inverted file data structures it facilitates implementations that scale in the number of queries and the number of terms.
 Local Upper Bound ( LUB ). In the following, we focus on the problem of bounding the query score S query ( q,i ) . Obviously, it is impossible to compute the precise value of S query ( q,i ) for any query q without aggregating information from all query indexes P ( t ) of terms t in i . However, we can estimate for each term t a value M ( q,i,t ) which is an upper bound for the term-query weight product sum of the other terms in i as shown in the following equa-tion: S
Let  X  i, t = max {  X  i,t 0 | t 0  X  V  X  t 0 6 = t } be the maximum item weight of all terms t  X  in i different from t . Observe that  X  all items containing exactly one term (which is rather an exception). Then we can prove that M ( q,i,t )  X   X  i, t  X  (1  X   X  q,t ) which leads to the following upper bound condition for S query ( q,i ) :
By replacing S query ( q,i ) in equation 1, we obtain the following first upper bound condition over P ( t ) denoted LUB : LUB : S min ( q ) &lt;  X   X S item ( i ) +  X   X  (  X  q,t  X   X  i,t
Condition LUB defines a subset of queries C LUB ( i,t ) of candi-date queries in P ( t ) as shown in figure 1. The only parameters are the item score S item ( i ) and both item weights  X  i,t and  X 
T HEOREM 1 (C ORRECTNESS OF LUB ). For all items i and all queries q  X  U ( i ) , q appears in the candidate set C all terms t shared between q and i .

Proof: We prove Equation (5) by using properties (1)  X  i, t From Theorem 1 directly follows that condition LUB is safe :
T HEOREM 2 (L OCAL O PTIMALITY OF LUB ). If item i con-tains only one term t then C LUB ( i,t ) = U ( i ) . Otherwise, for each query q  X  C LUB ( i,t ) we can define a query q 0 which is indistin-guishable from q in P ( t ) (  X  q 0 ,t =  X  q,t and S min ( q such that q 0  X  X  ( i ) .

Proof: If item i contains only one term t , then M ( q,i,t ) = 0 and LUB holds for all q  X  U ( i ) . Otherwise, let q  X  be a query with two terms { t, t } where t is the term different from t with the maximum item weight  X  i, t . It is easy to show that q 0 complete proof can be found in the extended version of this article.
Theorem 1 states that each query q  X  X  ( i ) appears as a candidate in C LUB ( i,t ) of all terms shared between q and i . This introduces some redundancy in the query matching algorithm. In the following we define two additional conditions which can each be safely added to C LUB ( i,t ) by conjunction to restrict the number of candidates for each term (however it is not possible to add both conditions without obtaining false negatives). The main idea is to exploit extra knowledge about the maximal query length and the query score distribution for still guaranteeing that each query candidate appears in the candidate set of at least one term.
 Higher Than Average ( HTA ). The first condition exploits the fact that for any query q  X  X  ( i ) , there exists at least one item/query weight product greater or equal to the average per term query score. This condition takes account of the maximal number of terms a query can share with some item . Since queries are in general shorter than items, this bound, denoted by  X  , can be defined by the maximum query length. Then we define the following constraint on the query score for queries q  X  X  ( i ) and for at least one term t : and the corresponding condition:
Condition HTA defines a subset of queries C HTA ( i,t ) of candi-date queries in P ( t ) as shown in figure 1.

T HEOREM 3 (C ORRECTNESS OF HTA ). For all queries q , all items i and all ranking functions S total ( q,i ) , if q  X  U ( i ) then there exists at least one term t shared by q and i , such that q  X  X  HTA ( i,t ) .

Proof: It is easy to show that there exists a term t where  X   X  i, t  X   X  q,t  X   X  i,t for all terms t . Then, by definition of  X  , we can show that condition (6) holds: Maximum Query Weight ( MQW ). Since item weights are normalized, we can show that there exists at least one term t where the following condition is true: This leads to the corresponding upper bound condition and candi-date set C MQW ( i,t ) :
T HEOREM 4 (C ORRECTNESS OF MQW ). For all queries q , all items i and all ranking functions S total ( q,i ) , if q  X  U ( i ) then there exists at least one term t shared by q and i , such that q  X  C MQW ( i,t ) .

Proof: Let t be the term shared by q and i with the maximum query weight  X  q, t . Then, by the fact that all item weights are nor-malized, we can proof condition (7) :
T HEOREM 5 ( GLOBAL CORRECTNESS ). For all items i and all queries q  X  X  ( i ) , 1. there exists at least one term t shared by q and i such that 2. there exists at least one term t  X  shared by q and i such that
Proof: Conditions (1) and (2) directly follow from theorems 1, 3 and 4 respectively.

Terms t and t  X  are not necessarily identical. Equivalently, the intersection of all candidate queries obtained by LUB and HTA over all terms contain all queries to be updated :
Finally, we also can show by a simple counter-example that it is not possible to combine all three conditions without  X  X oosing X  queries to be updated.

Figure 1 shows two additional constraints corresponding to the light blue and light green areas. The left blue rectangle contains all queries q where S min ( q ) &lt;  X   X S item ( i ) for item i . The green area adds all queries q where S min ( q ) &lt;  X   X S item ( i ) +  X   X   X  Both conditions are sufficient but not necessary for a query to be updated.
The query indexing techniques and upper bound conditions de-scribed in Section 3.1 did not take account of the decay func-tion. Recall that we only consider order-preserving decay functions where the decayed score S decay ( i,q, X  ) of all items i in the top-k result R ( q, X ,k ) continuously drops for all queries q , promoting newly arriving items over older ones. A direct interpretation of de-cay in our query representation consists in continuously moving the points in each index P ( t ) towards lower values (towards the left) parallel to the minimum score axis ( x -axis). Recall that decay does not change the item order which allows us to apply the decay func-tion directly on the minimum score value of each query without considering its top-k result.

To avoid continuously updating P ( t ) we apply the backwards decay technique proposed in [2]. This solution computes, stores and compares all scores with respect to some fixed reference time instant  X  0 used as a landmark. In particular, all query indexes P ( t ) are maintained with respect to a constant time instant  X  0 queries in these indexes are only updated when their minimal scores change. The basic idea to achieve this is the following: Suppose that a new item i is published at time instant  X  i . The total score of a query q at  X  i is S total ( i,q ) . In order to use this score with the corresponding query indexes, we have to calculate the inverse decayed score decay  X  1 ( S total ( i,q ) , X  i  X   X  0 )) , which corresponds to the hypothetical score value that should have been assigned to the query-item pair at time instant  X  0 : decay ( decay  X  1 ( S total ( i,q ) , X  i  X   X  0 ) , X  i  X   X 
Observe that the inverse decay function, always increases score values. This also implies that the minimum scores of items will always increase in time, since these, too, are computed with re-spect to the landmark  X  0 . Projecting all scores and constraints on a given time instant  X  0 allows us to immediately compare and decide whether an arriving item updates or not a given query. We will see in Section 4 the practical impact of this solution on the underlying data structures.
In this section we propose a number of in-memory index struc-tures based on the two-dimensional representation and the linear constraints presented in Section 3. All indexes are designed by taking into account particular data characteristics and a common processing scheme. First, in order to be able to manage large term vocabularies, we follow a traditional inverted file approach map-ping each term to a corresponding inverted grid of queries (we use the term grid opposed to the term list in the one-dimensional case). Second, each query q is encoded in the inverted grid of each terms t  X  q by a couple ( S min ( q ) , X  q,t ) . Since  X  q,t is constant, queries (data points) only move horizontally on the S min ( q ) -dimension. This simplifies the update process and favors the decomposition of the two-dimensional space into a list of horizontal grid lines for optimizing update cost. Observe, also, that each query minimal score update has to be done in all inverted grids of all remaining query terms. This obviously increases the importance of using data structures optimizing updates. Finally, as argued in Section 3.2, decay is computed with respect to a fixed landmark and does not cause any updates. However, this also leads to a monotonic un-bounded increase of the indexing space  X  the minimal score of a query monotonically increases in time  X  which leads to the need of more dynamic data structures and incremental memory allocation. We will discuss this problem separately for each implementation.
Following the previous observations, the query filtering process can be summarized as follows. On item arrival, the corresponding inverted grids are retrieved for all item terms through the inverted file (hashtable). Then, for each grid, we start scanning its grid lines from the left to the right until to the corresponding linear upper bound lines defined by the item term weight and the item score. For each candidate query q visited in this way, the system computes its total score and checks if its top-k list is updated by the new item. If this is the case, the minimal score of q increases and q is moved to its new position of all inverted grids corresponding to the query terms. Observe that, for the same item, a query can be visited several times, but it cannot be moved twice. An alternative solution would be to compute first the union of all candidate queries over all terms and check for updates afterwards. This might change memory usage and processing time, but not the principal role of the index structures.

In the following we will describe four solutions for implement-ing inverted grids. The differences between these solutions lie in the choice of the data structures for implementing grid lines. They can be compared following the standard evaluation criteria for data indexes: evaluation cost (including scanning, filtering), memory size and index update cost. All data structures are illustrated in Figure 2 and described in more detail below. Observe that all sub-figures contain the same queries and have been split into the same number of grid lines of constant height.
 Rectangular Grid. R ECT G RID is a two-dimensional array of equally sized cells as shown in Figure 2. Each cell is implemented as an unsorted set of queries called a bucket . For each item term we can immediately compute all cells (buckets) that intersect with the constraint lines and which contain the right-most candidate queries. Moving updated queries is efficient in R ECT G RID since the bucket corresponding to the new position of an updated query can be cal-culated in constant time. Observe also that the change of the min-imal scores might not necessarily change the position of a query in the grid. Concerning decay, since all scores are computed with respect to a fixed landmark time instant, the minimum (backward) scores of updated queries monotonically increase in time. Since this score increase is unbounded, it is not possible to define a static maximal array size. A solution consists in periodically recomput-ing all scores with respect to a new time instant, which might be-come very costly (lazy evaluation solutions might decrease this cost but are also more complex to be implemented). A better solution is to introduce more dynamic data structures as shown below. Sorted Buckets. S ORT B UCK introduces dynamic memory al-location by implementing grid lines as sorted lists of fixed sized buckets which contain at least one query. Buckets are ordered by their position in the grid (Figure 2). This data structure is obvi-ously less efficient for updates since it is not possible to identify the bucket corresponding to a particular cell in constant time. In order to increase efficiency, the sorted lists are implemented as a Red-Black tree structure (the variation of B+ Trees optimized for main memory usage) obtaining a logarithmic update cost.
 Notice that not all queries in the intersecting buckets of R G
RID and S ORT B UCK are candidate queries and must be filtered individually. The cost influence of this precision loss (marked in red in figure 2) over the global matching cost might become impor-tant under certain distributions of term weights and minimal scores. Whereas decreasing the grid width increases this matching preci-sion, it also increases the number of index updates.
 Sorted Queries. In order to reduce matching precision loss, S
ORT Q UER directly maintains a list of queries sorted by their mini-mum scores (Figure 2). Query filtering is straightforward. Updates are done in a similar way as for S ORT B UCK and can be achieved in logarithmic time over the number of queries (using a tree struc-tured ordered list implementation). An advantage of S ORT is that a query X  X  position only needs to be updated if its score gets higher than the minimal score of the next query. Observe, also, that this solution avoids the garbage collection cost generated by empty buckets.
 of fixed width. Since we cannot make any assumption on the dis-tribution of minimum score values (distribution depends on de-cay, term frequency, etc.), we obtain inhomogeneous collections of dense and sparse buckets. As discussed before, such biased distributions might have an important effect on matching precise-ness. D ENS B UCK adjusts bucket widths dynamically in order to maintain a minimum and maximum constant number of queries per bucket. Initially, all lines contain a single bucket of infinite width. After some time, these buckets will be split into sorted lists of buckets partitioning the whole inverted index space. This list is maintained dynamically similar to the nodes in a B-Tree struc-ture (split and merge). Updates are performed in logarithmic time over the number of buckets. This solution better distributes queries within the buckets and it is equivalent to the previous solutions in terms of indexing and search complexity. Decreasing bucket den-sity reduces search cost by improving precision but also leads to higher query update cost, as smaller buckets require more frequent split and merge operations. On the other hand, increasing density leads to the opposite effect (higher search cost against lower update cost). Our experiments in Section 6 confirm this trade-off between search cost and update cost and a low overall performance of the D
ENS B UCK index compared to the other index structures. rithms most closely related to our work are the Incremental Thresh-old [16, 17] and COL-Filter [6]. They both rely on monotonic and homogeneous ranking functions and use sliding window seman-tics without decay. Incremental Threshold is essentially a variation of the Threshold Algorithm (TA) [4] and uses two inverted lists: (a) the first for coherently indexing the N most recently published items (sliding window) and (b) the second for indexing all regis-tered top-k queries. This algorithm has been proven to be quite ex-pensive in maintaining a valid view on the sliding window because of frequent index updates. As experimentally demonstrated in [6], the Incremental Threshold X  X  cost of retrieving candidate queries is most of the times worse than a na X ve solution, where all queries containing the item terms are scanned without any stopping con-dition. To overcome this limitation, COL-Filter defines a single inverted list of queries ordered by an appropriate one-dimensional ranking criteria that guarantees an effective early stopping condi-tion.

Item importance has only been considered recently for query fil-tering [7]. Item importance is estimated by applying a particular weighting scheme taking into account the frequency of terms in the queries. The total score of an item with respect to a given query then depends on its importance and its similarity. Whereas this reminds our definition of non-homogeneous total score, the way of how item importance is defined in [7] makes the total scoring function again homogeneous for applying COL-Filter. Our work can accommodate the need for content-independent item scores re-sulting in non-homogeneous ranking functions, while its scalability gains even for the homogeneous case as presented in Section 6. retrieving candidate queries through our constraints is a spatial fil-tering problem in a two-dimensional space. Any incoming item de-fines a polygon for which all contained points of candidate queries need to be retrieved. Since, query updates affect the minimum score per result list they essentially redefine the position of a candidate query point by moving it towards to the right while keeping the same vertical position. Spatial indexes, such as Rectangular Grids, R-Trees, QuadTrees and k-d Trees, have be used in a similarly dy-namic setting for retrieving moving objects continuously contained in a target area [10]. However, these works rely on the strong as-sumption that the target search areas are known a priory. This is not the case of our spatial filtering where the polygons of interest are also continuously redefined based on the incoming items. Other spatial indexes proposed for moving objects either assume fixed velocity of the points [18], or compromise on the system X  X  accu-racy [1]. Unlike these works in our setting (a) points frequently change positions and (b) points move only in a particular direction. For these reasons we have designed four indexes that take into ac-count the peculiarities of points move and vertically partition in advance our two-dimensional space for all indexes.
 but not least several pruning techniques for the inverse problem of top-k snapshot queries have been proposed in [12] using a rank-ing function for documents equivalent to ours. As queries arrive, the top-k documents are retrieved using knowledge on the weights of their terms as well as their PageRank [13] score which is the equivalent of item importance in our work. In particular, reliable pruning technique proposed in [12] splits the collection of docu-ments into two groups based on whether their term weight is higher than a threshold. Each of these groups is then sorted by the PageR-ank score. Results for arriving queries are then retrieved using a condition similar to LUB . Adapting this technique to our setting consists in split the queries into two groups depending on the query term weights and then sort them by the queries X  minimum score. The S ORT Q UER index can be seen as generalization of such a so-lution for more than two groups of queries.
In this section we present an experimental evaluation over the in-dexes proposed in Section 4. We evaluate their performance under three different settings. We will start with the most simple sce-nario which considers only query dependent scores without decay ( homogeneous score without decay ). We will show the trade-off be-tween matching time and memory cost over different index tuning parameters and compare our solution to the current state of the art solution (COL-Filter). We will then explore the effect of introduc-ing decay on the matching performance ( homogeneous score with decay ). Our experiments terminate with the most general setting with decayed scores combining query similarity and query inde-pendent item scores ( non-homogeneous score with decay ). Setup. In order to run our experiments we use a real-world data collection and generate a corresponding set of queries. To the best of our knowledge there is no test-bed publicly available, including a stream of time-stamped items and user defined queries over the same vocabulary and period. We ran our experiments over a dataset of 13 , 000 RSS items extracted from the RoSeS testbed [8] which contains items from 8 , 155 RSS feeds, including press, blog, forum feeds etc., collected from March 2010 to October 2010. Before running experiments we applied standard stemming, stop-word re-moval and HTML tag removal on the item contents. Observe that the size of the dataset (number of items) is not a scaling parameter in our setting which assumes a stream processing scheme where each item is processed in one pass and discarded afterwards. We then generated a synthetic query workload based on the vocabu-lary of our dataset. Using set of terms semantics for the items, we generated the queries by computing term co-occurrence and find-ing the most frequent combinations of 2 to 4 terms appearing in the dataset, aiming at evaluating index performance over a workload with many query relevant items. The final query workload was cre-ated by uniformly selecting the most frequent combinations of each query size.

For both, queries and items, we used the standard tf-idf weight-ing scheme with normalized weights. All experiments started by a warm-up matching period of 3 , 000 items in order to initialize the top-k results and minimum scores. The time measurements repre-sent the average matching time required per item, over the remain-ing 10 , 000 items. The average matching time takes account of the time necessary for filtering all candidate queries (query filtering), updating the top-k results (query update) and updating the index according to the change of the minimal scores (index update). The k parameter, determining the size of the top-k results was set to 1 . Higher values increase the number of query updates by decreasing the S min ( q ) without any other particular effect on the index.
All algorithms were implemented in Java 6. Experiments were carried out on an Intel Core 2 Quad Q6600 @2.4 GHz, with 32-bit Windows 7 operating system, using 1GB of Java heap space. Figure 3: Trade-off between matching time and memory cost set of experiments we suppose a static environment (no decay), considering only query dependent scoring functions (  X  is set to 0). This simplification makes our problem statement compatible with existing continuous top-k query processing scenarios. In particular, under this scenario we can compare our work to COL-Filter [6], which is the state of the art solution and achieves better perfor-mance than the Incremental Threshold algorithm [16, 17].
Besides comparing our solution with COL-Filter, we tune the four indexes proposed in Section 4 and evaluate the influence of their configuration parameters on matching time performance and memory cost. These configurations parameters are: (i) the number of lines (horizontal partitioning) for all four indexes (ii) the number of columns (vertical partitioning) for R ECT G RID and S ORT and (iii) the bucket density for D ENS B UCK .

As mentioned in section 4, increasing the degree of partitioning decreases the candidate filtering error (red zone in Figure 2), but at the same time this also increases update cost (updated queries move more frequently between buckets) and memory requirements (empty buckets, data structure overhead). In our first experiment (Figure 3), we capture this trade-off between matching time and memory cost. First we must notice that for all four indexes increas-ing the number of divisions (lines/columns) naturally led to better matching time requirements, but after some point this performance was worsening. This is explained by the fact that after a certain par-titioning threshold, the performance gain from the higher candidate filtering precision cannot absorb the performance loss due to empty buckets and/or internal data management overload. Figure 3 there-fore, represents for each index the skyline of the optimal matching time and memory requirements pairs, after varying the tuning pa-rameters. Differently formulated, each point ( x,y ) reflects the best average per item matching time y the index can achieve using not more than x MB of memory. Finally, note that COL-Filter has no particular configuration parameters which might influence memory usage and is therefore represented by a single point.

R ECT G RID requires more memory than all other indexes to achieve the same performance and COL-Filter uses less memory than all other indexes however, with non-optimal time performance (single cross on the left). The additional space required by R G
RID can be explained by the static memory overhead of the under-lying array structure which is which independent to the actual num-ber of indexed queries and non-empty buckets. S ORT B UCK more clever dynamic usage of space, depending only on the buckets created. The goal of D ENS B UCK was to obtain a better control over the number of queries per bucket (bucket density) which improves memory cost. However, as shown in the figure, the time perfor-mance results are disappointing compared to the other solutions. In this index, changing bucket density introduces a trade-off between filtering accuracy (search cost) and the number of time consuming bucket split and merge operations (update cost). In both cases, the additional time requirements lead to poor overall performance.
Finally, we can see S ORT Q UER index, which directly gener-ates lists of queries achieves the best matching performance with low memory. For this same reason, COL-Filter has slightly less memory requirements than even S ORT Q UER (10-15%). Recall that COL-Filter does not consider two dimensions, but only stores sorted lists to maintain the equivalent of a grid in our solutions. Fi-nally, we can see that all our index structures (except D can achieve an average matching time performance gain of 50% (with respect to COL-Filter) under different memory constraints.
For the rest of the experiments we have initialized all index pa-rameters with the optimal values leading to the best matching time performance independently to their memory usage (as shown in figure 3, adding more memory has no performance benefit after a certain threshold). Table 1 summarizes these optimal values for each index. R ECT G RID 20 1800 -47.14 S ORT B UCK 20 4000 -22.57 S ORT Q UER 20 --14.09 D ENS B UCK 3 -16 13.98 COL-Filter ---11.22
Figure 4 shows the scaling performance of our four indexes, as well as that of COL-Filter over the number of queries stored in the system. We can observe that the average per item matching time of all indexes increases linearly with the number of indexed queries. Since each subset of queries is an unbiased sample with the same term distribution as the original set, this curve also reflects the scal-ing behavior with respect to the candidate queries. We can see that the matching performance of S ORT B UCK and S ORT Q UER more or less equivalent whereas S ORT B UCK requires 60% more memory than S ORT Q UER . Grouping queries in buckets reduces the number of position updates required after minimum score changes on the queries, but it also leads to a loss of preciseness in query can-didate filtering. Finally, COL-Filter performance decreases with a higher linear rate and requires up to 125% more average matching time than R ECT G RID and up to 70% more than S ORT B UCK S ORT Q UER . D ENS B UCK clearly has the worst performance. ing set of experiments we evaluate the behavior of the indexes when applying linear and exponential decay. As explained in Section 4, all indexes are maintained with respect to a reference time instant  X  0 by using a backward decay function. This has as a side effect Figure 6: Time requirements over faster scores X  decay (exponential decay) that the minimal score of a query is monotonically increasing in time. In our query index representation, this results in a time de-pendent unbound query point distribution on the minimal score.
Due to this dynamic change of the indexed space R ECT G RID which is based on a statically bound array structure, cannot be com-bined with our backward decay solution. We also exclude from the tests COL-Filter, which is based on sliding windows for reflecting information freshness. As before, in this experiment, the value  X  is set to zero and only query dependent scores are considered.
Generally, faster decay means that the item scores (including the minimal score) in the top-k list of a query decrease more rapidly with respect to a new item and arriving items are more likely to update relevant queries. Equivalently, when decay is applied to our constraints model, the three constraint lines continuously move toward the right and more queries have to be checked for updates. Figure 5 depicts the time requirements as scores decay faster, using a linear decay function. The x -axis in the diagram represents the continuous average score decrease per day. The chosen interval of values ( [0 . 001 , 0 . 01] ) corresponds to 5 up to 20 updates on average per query per day. The behavior of the three indexes is linear on the decay rate / number of more updates on queries caused by the items. This indicates that even though more queries are scanned due to decay, the average cost in time per update over the whole set of queries remains more or less constant.

In Figure 6 we can see the equivalent diagram for exponential decay. The x -axis of the diagram represents the time required (in hours) in order to divide scores by 2 (half-life period). As before, values were chosen so as to have on average between 5 and 20 up-dates per query per day. For exponential decay, too we also observe that the behavior of the systems is proportional to the number of queries updated. Here, as well, no changes are observed in the rel-ative position of the indexes performance, with D ENS B UCK ing twice the time of S ORT B UCK and S ORT Q UER having almost the same time performance as S ORT B UCK .
 previous experiments we assumed homogeneous ranking function (  X  = 0 ). In this final set of experiments we observe the behavior of the three index implementation over different values for  X  with linear and exponential decay. Item scores are generated as random values in [0 , 1] with uniform distribution. Decay was fixed to an average of 10 updates per query per day with homogeneous score  X  = 0 (the final number of updates depends on  X  ).

Figure 7 shows the performance of the three indexes over dif-ferent values of  X  while applying linear decay. The time behavior shown, directly reflects the average number of candidate queries, but also of the ones updated per item. We can observe that for all indexes, switching from the homogeneous total score (  X  = 0 ) to the non-homogeneous score (  X  &gt; 0 ) immediately increases the number of candidate queries per item. After this first peak, in-creasing  X  has the opposite effect, i.e. the number of candidate queries continuously decreases. The initial peak can be explained by properties of the scoring function employed. Generally there are many query-item pairs for which a low query-dependent score is assigned and which is not sufficient to lead to an update in the case of  X  = 0 . However, when item score is considered, a random number for the item score (in [0 , X  ] ) is added to the total score, in-creasing the expected total scores and the probability of queries to be updated. The fact that after the first peak, the number of query updates decreases can be explained by the properties of the linear decay function: all scores (higher and lower) decrease by the same amount after a given time period, forcing lower scores to go faster to a zero minimum score and thus, be updated by any relevant item (we have a higher number of updates when the average total score is low). From the way we have selected the item score, item scores are generally higher than similarity scores, so higher values of  X  (weight of item score) lead to higher values of total scores, and as explained, in a higher number of updates. Similar observations can be made on Figure 8 where exponential decay is applied.
 Conclusions. Our experiments show that depending on the amount of available memory, we can choose between S ORT Q which achieves good performance with low memory and R G
RID for a 10-20% performance gain with a high memory cost (S
ORT B UCK is situated between these two indexes). Compared to COL-Filter (which only works for homogeneous ranking func-tions), S ORT Q UER is up to 50% faster using only 15% more mem-ory. We have also observed that partitioning the space in more homogeneously sized partition (in terms of number of queries), as it is done by D ENS B UCK , does not improve per item, neither time performance nor memory cost. All three indexes supporting decay scale linearly with the number of queries. Matching time increases proportionally to the number of top-k list updates and is not af-fected neither by the decay rate nor by the linear combination of the query-independent and the query-dependent scores (  X  ). Conse-quently, once the desired update rate for the users X  queries has been fixed, the total scoring and the decay function, which affect imme-diately the results retrieved by the users, can be freely chosen with-out affecting the performance of the system. A final observation is that grouping of queries in buckets does not lead to a better per-formance. This observation is also confirmed by the experiments conducted in [6] comparing COL-Filter and POL-Filter.
This article presents a new optimization technique for process-ing large collections of continuous top-k text queries featuring non-homogeneous ranking functions. Whereas non-homogeneous rank-ing functions are widely used in standard search engines, to the best of our knowledge they are not exploited by existing systems for evaluating continuous top-k textual queries. As shown by our experiments, the index structures presented in this paper general-ize state of the art solutions for the homogeneous case with similar performance (COL-Filter) while they scale linearly in the number of queries matching an item, independently to the parameters of the ranking function.

A first direction for future work is to further optimize our spa-tial filtering of candidate queries with probabilistic guarantees. As shown in Figure 9, it is possible to estimate for a given term the probability of a candidate query to be updated by taking into ac-count of the query position in the two-dimensional space. These probability distributions are of course different for each term. Fur-thermore, we are planing to explore the locality property of our spatial constraints in modern parallel data processing paradigms (MapReduce) and Cloud architectures in order to efficiently im-plement filtering of candidate queries. Moreover, we intend to study the application of our model for pruning the search space of top-k queries over a large collection of text documents as men-tioned in the related work [12]. We are finally interested in combin-ing collaborative filtering and content-based querying over textual streams. The main idea is to consider total ranking scores com-bining user feedback ( X  X  like X  button) with content-based user pro-files. This raises the particular problem of introducing dynamic item scores into our model. [1] R. Cheng, D. V. Kalashnikov, and S. Prabhakar. Querying [2] G. Cormode, V. Shkapenyuk, D. Srivastava, and B. Xu. [3] G. M. Del Corso, A. Gull X , and F. Romani. Ranking a stream [4] R. Fagin. Combining fuzzy information: an overview.
 [5] E. Gabrilovich, S. Dumais, and E. Horvitz. Newsjunkie: [6] P. Haghani, S. Michel, and K. Aberer. The gist of everything [7] P. Haghani, S. Michel, and K. Aberer. Efficient monitoring of [8] Z. Hmedeh, N. Vouzoukidou, N. Travers, V. Christophides, [9] Y. Hu, M. Li, Z. Li, and W.-y. Ma. Discovering authoritative [10] D. Kalashnikov, S. Prabhakar, and S. Hambrusch. Main [11] J. Liu, P. Dolan, and E. R. Pedersen. Personalized news [12] X. Long and T. Suel. Optimized query execution in large [13] C. D. Manning, P. Raghavan, and H. Schtze. Introduction to [14] X. Mao and W. Chen. A method for ranking news sources, [15] Y. Miao, C. Li, L. Yang, L. Zhao, and M. Gu. Evaluating [16] K. Mouratidis and H. Pang. An incremental threshold [17] K. Mouratidis and H. Pang. Efficient evaluation of [18] S.  X altenis, C. S. Jensen, S. T. Leutenegger, and M. A. [19] C. Wang, M. Zhang, L. Ru, and S. Ma. Automatic online [20] F. Zhang, S. Shi, H. Yan, and J.-R. Wen. Revisiting globally
