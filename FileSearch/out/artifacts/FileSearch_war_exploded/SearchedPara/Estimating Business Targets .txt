 Determining and setting maximal revenue expectations or other business performance targets--whether it is for regional company divisions or individual customers--can have profound financial implications. Operational techniques are changed, staffing levels are altered and management attention is re-focused--all in the name of expectations. In practice these expectations are often derived in an ad hoe manner. To address this unsupervised task, we combine nearest neighbor methods and classical statistical methods and derive a new solution to the classical econometric task of frontier analysis. We apply our methodology to two real world business problems in Verizon, a major telecommunications provider in the United States, more specifically in the print yellow page division Verizon Information Services: (1) identifying under marketed customers for targeted upselling campaigns and focused sales attention, and (2) benchmarking regional directory divisions to incent performance improvements. Our analysis uncovers some commercially useful aspects of these domains and by conservative estimates can increase revenue by several million dollars in each domain. H.2.8[Database Management]: Database Applications--data mining; 1.5.3[Pattem Recognition]: Statistical Nearest neighbor, frontier analysis, maximal value estimation What is the maximal revenue a business can earn? What is the most a customer is willing to spend? These are difficult but important business questions that obviously vary from division to division, and customer to customer even within the requires prior specific permission and/or a fee. KDD 01 San Francisco CA USA 
Copyright ACM 2001 1-58113-391-x/01/08...$5.00 surface that exceeds, or "envelopes" each target. Seiford and 
Thrall (1990) [11] provide a recent description of the state of the art in DEA. The estimated target is set to the maximum (or minimum) from the observed targets. This method has been criticized for its sensitivity to errors or outliers since it assumes that all observed targets define the possible space. Referred to as frontier analysis by the econometricians, they term. This sets the target above its observed performance, and also estimates a function of the variables that can update the frontier when the variables change. As Bauer (1990) [2] points out, the major problem with this approach is the requirement of a model for the error term and for g. Popular choices have been linear models, the Cobb-Douglas function and the translog. All are known to be merely local approximations to a more global function. Our approach expands this methodology and is described next. When target estimation is necessary for several entities we argue that finding a smaller region of closely related entities provides more precise data for estimation. Two possibilities in the knowledge discovery area exist for finding closely related entities: nearest neighbor ([4], [1]) methods and clustering methods [7]. Although both of these techniques use similarity metrics to determine groups, the characteristics of the resulting groups are vastly different. In nearest neighbor, the closest entities are found for each entity whereas in clustering, groups are formed that reflect the general similarities among all entities. Although clustering is computationally more efficient than nearest neighbor, when clusters are formed two entities that are the closest to each other could theoretically fall into different clusters. In fact as the dimensionality of the problem space increases this is more likely to occur. This will not occur with nearest neighbors. For this reason we opt to determine nearest neighbors for each entity. In this section we describe our two-step process: (1) finding neighborhoods with similar entities using nearest neighbor and (2) estimating frontiers for each entity from its neighborhood. Assuming we have a dataset of i observations, let xi denote the ith observation and y~ denote the variable containing its target value. We define a neighborhood n i to be the neighborhood contains those entities that have the minimum distance from x~. Each observation will have a specific neighborhood containing its most similar observations. Defining the similarity or distance function between observations is a crucial part of finding the neighborhoods. Since our domains are defined by both continuous and nominal variables, we use a separate function for each type of variable and combine the results. For continuous variables we use a weighted Minkowski distance [5] and for nominal variables we use a weighted distance typically used by is a statistical distribution taking on only non-negative values. 
Some of the distributions chosen in the econometrics literature [11] are the exponential, Gamma or half-Normal. We chose an exponential distribution [6]. performance, so that Yi(1) is the largest target measure in n~. 
For our chosen distribution, two natural estimators are the maximum likelihood estimator, or a linear unbiased estimator. 
The maximum likelihood estimator of qT~ is just Y~tl~ itself, so that the target frontier would be set at the largest in that neighborhood. From a business viewpoint, this has the unpalatable consequence of requiring no increase, as its performance is also constructed to be at the frontier. To solve this problem, we propose the following unbiased estimator of the exponential parameter )l~ 
Set the estimator of the frontier (b i to for each observation xb depending on whether the maximum or minimum target is desired. 
In this unsupervised task, a method for comparing various distance metrics is necessary. Consider the ratio of y~ to the estimated target $~ defined by 
If we are considering the maximal frontier, E(xi) will range from 0 to 1, illustrating the proportion of the target attained. 
If we are considering the minimal frontier, E(xi) &gt; 1, and will show the number of times the observation's target is larger than the minimum. The distribution of E(x~) will vary depending on the estimation method and the distribution of the target. 
To illustrate target estimation techniques, we describe two application domains related to yellow page directories produced by Verizon Information Services. Each of these 750 directories contains thousands of ads placed by businesses large and small, local and national, to attract customers. Our first case study looks into revenue generated from advertisers. 
The revenue generated from even a single type of heading (e.g. doctors) varies greatly across directory books. One explanation is the competitiveness in the area. However, when these characteristics are accounted for by considering Figure 1 shows the distribution for E(x) for advertisers. In an effort to better understand the characteristics of advertisers with low spending compared to their neighbors, we used a, decision tree to predict the difference between the revenue ofxt and its estimated maximum revenue (i.e. ~,-y~). Figure 2 shows the partial structure of the decision tree for a random sample training/test sample. The contact type for the advertiser is the most important factor. Although its not shown, a close competitive split for the root is the annual revenue from the advertiser. Other important factors related to the directory distribution size and market size. Directory characteristics such as advertiser number and ad price, and market characteristics such as business count and household income are also important. If the right most node containing over 1100 observations is expanded ad price, median home value, and market heading penetration can more finely breakdown advertisers. It appears that low-efficiency advertisers are most numerous in competitive environments that require special sales reps and have large numbers of businesses. Further, efficiency differences develop in later years of an advertiser's tenure (few first year advertisers have small efficiencies), showing the importance of effective relationship management. book, book features) and information regarding the market the book covers (e.g. the national competitive index, number of households, and average household income). In addition, each observation contains the revenue generated during the time period. For analysis purposes, the revenue variable and variations are dropped during modeling. 
As in the previous domain, we preformed some common measures to standardize data, (e.g. all continuous variables are transformed with natural log) and applied the distance metrics described previously. We separated the data into two sets: the training set containing about 80% and the holdout set containing the remaining 20%. We calculated the four nearest neighbors for each directory xi (including xi) in the training set and defined this as the neighborhood n~. Initially we weighted the variables equally. 
Since Verizon has been producing printed yellow page directories for several years in very competitive environments achieving numerous awards, we assume that the efficiency of the businesses to be reasonable. If the efficiency of any directory is substantially low, we would expect competitive forces and revenue expectations to drive changes to make the business more efficient, otherwise the division is a lost expense. For these reasons we expect to see division efficiencies (i.e. E(x)) distributed normally hopefully skewed towards higher efficiencies. However we were somewhat disappointed with the results since according to our assumption of reasonably efficient businesses, too many directories had inefficient revenues (e.g. E(x) &lt; 20%). We contributed this to our variable weighting scheme. Following our assumption of reasonably efficient businesses, we weighted variables according to their contribution towards predicting revenue (i.e. Yi). This was done to mitigate complaints from regional directory managers stating they were being treated unfairly by being grouped with higher revenue divisions. We calculated regression weights for the continuous and nominal variables to predict revenue using only the training set and recalculated the neighborhoods. books. System books (Figure 4b) show increasing returns to scale [5] (i.e. E(x) increases as distribution increases) while neighborhood books have a fairly distinct maximum in the middle size ranges. It seems probable that such books are [6] too large, they compete with the system book in that franchise. We have presented a general data mining methodology for estimating business targets by extending frontier analysis. Our first case study examined the methodology when applied to Dentist advertisers for yellow page directories and identified existing advertisers that are under-marketed. [8] Increasing sales focus on these customers could increase the potential revenue for dentist advertisers by several million according to our conservative estimates. We expect similar gains when other headings within directories are analyzed. In [9] our second case study, we provide a methodology for estimating optimal revenue performance targets for directory divisions. Since nearest neighbors contain divisions with for benchmarking or other performance measures. Our conservative estimate of potential revenue increase for directory books is a minimum of several million dollars. [1] Aha, D., Kibler, D. &amp; Albert M. (1991). Instance-based [2] [3] [4] Duda, R. &amp; Hart, P. (1973). Pattern Classification and 
