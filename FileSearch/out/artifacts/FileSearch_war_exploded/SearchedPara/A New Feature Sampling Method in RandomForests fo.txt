 High-dimensional data has become common in today X  X  applications. State-of-the-art machine learning methods can work well for data sets of moderate size but they suffer when scaling for high-dimensional data. It is well-known that in a high-dimensional data set only a small portion of the predictor features are relevant to the response feature, the irrelevant features may even degrade the performance of the model. This requires methods for selecting good subsets of features for learning efficient prediction models.
 Random forests (RF) [ 1 ][ 2 ], an ensemble learning machine composed of L = { ( X dictor variables) and Y is the target (also called response feature), a regression problem and Y X  X  1 , 2 , ..c } for a classification problem ( and
M are the number of training samples and features, respectively. A stan-dard version of RF independently and uniformly resamples observations from the training data L to draw a bootstrap data set L  X  from which a decision tree is grown. Repeating this process K times produces a series of bootstrap data sets Given an input X = x , the predicted value by the whole RF is obtained by aggregating the results given by individual trees. Let  X  f of unknown value y of input x  X  R M by k th tree, we have tively.
 RFs have shown to be a state-of-the-art tool in machine learning. RF model can be used for both feature selection and prediction, and it can perform well in both classification and regression problems. However, the performance of random forests suffers when applied to high-dimensional data, i.e., data with thousands to millions of features. The main cause is that in the process of growing a tree from the bagged sample data, the subspace of features randomly sampled from the thousands of features in the training data to split a node of the tree is often dominated by less important features. The tree grown from such randomly sampled subspace features will have low accuracy in prediction, hence affects the final prediction of the random forests.
 In this paper, we propose a new approach for feature weighting subspace selection to improve the accuracy of prediction for RF, meanwhile maintain-ing the diversity and the randomness of the forest. Given a training data set L , we first use a feature permutation technique [ 3 ][ 4 ] to measure the impor-tance of features and produce raw feature importance scores. Then we apply p -value assessment on finding the cut-off between informative and less informa-tive features. For all informative features, the Spearman rank test is then used find the subset of highly informative features. The separation forms three sub sets of features. When sampling the feature subspace for learning, features from these three groups of highly informative, informative and less-informative fea-tures are taken into account for splitting the data at a node. Since the subspace always contains highly informative features, it can guarantee a better split at a node, therefore assuring a qualified tree. This sampling method always provides enough highly informative features for the subspace feature at any levels of the decision tree. By using taking into account features from all three subsets, the diversity and the randomness of the forests in the Breiman X  X  framework [ 1 ]are maintained.
 random forests algorithm, called ssRF, for dealing with both classification and regression problems. With the ssRF model, the quantile regression is employed to predict both point prediction and range prediction in regression problems. Our experimental results have shown that with the proposed feature sampling method, our random forests ssRF model outperformed existing random forests in reduction of prediction errors, even though a small feature subspace size of log high-dimensional data. 2.1 Importance Measure of Features from a Random Forest The feature importance measure obtained from the random forest is described as follows [ 5 ], [ 6 ]. At each node t in a decision tree, a split on feature determined by the decrease in node impurity  X R ( X j ,t ). For a regression tree, the node impurity R ( t )=  X  2 ( t ) p ( t ), where p ( t the impurity reduction that an sample chosen at random from the underlying  X  impurity in node t after splitting into t L and t R is tively.
 R ( t ). Suppose there are S categorical values in node t ( s  X  S proportion of the samples from the s th category in node t is defined as The chosen split of feature X j for each node t is the one that maximizes Let
IS T ,wehave Let
IS in a random forest, defined as It is worth noting that a random forest uses in-bag samples (i.e. the set of the bagged samples used in building the trees) to produce importance scores IS . This is the main difference between this importance score and an out-of-bag measure, which requires so much computational time using OOB-permutation follows: Having the raw importance scores VI j determined by Equation ( 4 ) we can eval-uate the contributions of the features in predicting the response feature. 2.2 A New Feature Sampling Method for Subspace Selection We first compute importance scores for all features according to Equation ( 4 ). Denote the feature set as L X = { X j } , j =1 , 2 , ..., M all values in each feature to get a corresponding shadow feature set, denoted as L = { A j } M 1 . The shadow features do not have prediction power to the response ran RF R times on the extended data set {L X  X  X  A ,Y } to get importance scores VI and VI r A j , and the samples for comparison denoted as V  X  1 , ..R } .
 The unequal variance Welch X  X  two-sample t-test [ 8 ] is then used to compare the importance score of each feature with the maximum importance scores of generated shadows. The non-parametric statistical test is required because the importance scores across the replicates are not normal distribution. Having com-puted the t statistic, we can compute the p -value for the features and perform hypothesis test on VI X j &gt; V tant, it consistently scores higher than the shadow over multiple permutations. Therefore, any feature whose importance score is smaller than the maximum considered important.
 The smaller the p -value of a feature, the more correlated the predictor feature to the response feature, and the more powerful the feature in prediction. Given a statistical significance level, we can identify informative features from low-threshold  X  , for instance  X  =0 . 05. Any feature whose p added to the low-informative feature subset denoted as X l with the Y values is assessed otherwise.
 the relationship between X j and Y  X  R 1 in regression problems. The value correlation. Spearman rank correlation coefficient performs well in cases when the conditional distribution is not normal, each pair ( X (
R where X, Y are the average values of important feature X j Y , respectively. Given all  X  values in the remaining features the mean of all  X  values as the threshold  X  , { X \ X ing categorical features are added to the informative feature subset, denoted as X m .
 hypothesis that the feature is independent of the response feature. All features whose p -value is smaller than 0 . 05 from the results of the remaining features are added to X m otherwise.
 features from three separated groups. For a given subspace size, we can choose proportions between highly informative, informative and less-informative fea-tures depending on the size of the three groups. That is mtry (
M mtry respectively. These are merged to form the feature subspace for splitting nodes of trees. The new feature subspace sampling method is now used to grow decision trees for building RFs. In regression problem, we propose to use quantile regression to notations as in [ 1 ], let  X  k be the random parameter vector that determines the for the forests generated from L . In each regression tree a positive weight w i ( x i , X  k ) for each case x i  X  X  .Let in
T . The cases x i  X  l ( x,  X  k ,t ) are assigned the same weight This means the prediction for a regression problem is simply the average and for the classification problem is the category received by a majority votes by all the cases not in L k are assigned zero weight.
 For a single tree prediction, given X = x , the prediction value is The new random forests algorithm ssRF is summarized as follows. from the less informative ones to obtain three feature subsets X l as described in Section 2.2 . 2. Sample the training set L with replacement to generate bagged samples L ,k =1 , 2 , .., K . 3. For each L k , grow a regression tree T k as follows: (a) At each node, select a subspace of mtry ( mtry &gt; 1) features randomly (b) Each tree is grown nondeterministically, without pruning until the min-(c) Compute the weights w i ( x,  X  k )ofeach X i by individual tree 4. Compute the weights w i ( x ) assigned by RF which is the average of weights by all trees: 5. Given an input X = x , use Equation ( 2 ) to predict the new sample for the 4.1 Data Sets We conducted experiments to test our proposed system on high-dimensional data sets for both classification and regression problems. Table 1 lists the real data sets used to evaluate the performance of random forests models. The Fbis data set was compiled from the archive of the Foreign Broadcast Information Service and the La1s , La2s data sets were taken from the archive of the Los Angeles Times for TREC-5 1 .
 were used to train the model. The level of the 1 , 440-th river was predicted in our experiments, the target values were converted from [0 The LOG1P data set was used in [ 10 ]. The Stock data set was described in [ 11 ] to make a stock price prediction. This data set has about 8 in the predictor features. The original Y value is between 880 and 82 target feature values were converted to [0; 1] using linear scale. Regarding the sets for training was separately from the testing. 4.2 Experimental Setting Evaluation Measure: We used Breiman X  X  method of measurement as described In which, for the regression problem the mean of square residuals ( sure was computed, for the classification problem the test error measure was used.
 The latest RF [ 12 ], QRF [ 13 ], cRF (cForest) [ 14 ] and GRRF R-packages [ 15 ] in CRAN 3 were used in R environment to conduct these experiments. For the GRRF model, we used a value of 0 . 1 for the coefficient  X  has shown competitive prediction performance in [ 16 ]. The novel SRF model [ 17 ] using the stratified sampling method was intended to solve the classifica-tion problem. The QRF and eQRF [ 18 ] models were developed for solving only regression problems. The ssRF model with the new subspace sampling method is a new implementation. In that implementation, we called the corresponding R/C++ functions in R environment.
 From each training data set we built 10 random forest models and the average of MSRs and the test errors of the models were computed; each of the RF models had 200 and 500 trees, respectively. The number of the minimum node size n min was 5 for regression and 1 for classification problems. The number of features-candidates was set with the default setting to mtry The parameters R , mtry and  X  for pre-computation of feature partition used in ssRF were 30, data set LOG1P , only 5% of the samples was used to train the eQRF and ssRF models for feature partition and subspace selection, since the computational time required for all the samples is too long.
 To address the missing values in the data set, we separate all samples con-taining missing values and create an extra  X  X issing X  group for them. We then values occur in the response feature, those samples are routinely omitted. After All experiments were conducted on the six 64-bit Linux machines, each one equipped with Intel R Xeon R CPU E5620 2.40 GHz, 16 cores, 4 MB cache, and 32 GB main memory. The ssRF and eQRF models were implemented as multi-thread processes, while other models were run as single-thread processes. 4.3 Results on Real Data Sets The performance of RF models is evaluated when the number of trees and fea-show the regression errors of the random forest models varied with the number of
K trees used with mtry = log of curves when the number of random features mtry in the subspace increases cates the size of a subspace of features mtry = log 2 ( M was suggested by Breiman [ 1 ] for the case when applying RF to low-dimensional data sets. Table 2 shows the test errors on the classification data sets against the number of trees and features. The RF, QRF and eQRF models were unable to build their models on the data sets Stock and Rivers containing missing val-ues. The imputation function in randomForest R-package was used to recover missing values on the two data sets. The eQRF model was not considered in this experiment because its prediction accuracy is last in this ranking on imputed data sets. The cRF model was processed well on data set containing missing values, however this model crashed when applied to the large-size data sets. The results of RF models when applied to imputed data sets are denoted as RF.i, QRF.i in the plots, respectively. We can see that ssRF always provided good results and achieved lower pre-diction error in Figure 1 and Table 2 when varying K and mtry data sets. In some cases where the ssRF model did not obtain the best results compared with SRF on the data sets Fbis and La1s , the differences from the best results were minor. These results demonstrated that, at lower levels of the higher levels of the tree. The other random forests models increase prediction errors while the ssRF model always produces better results. This was because the selected subspace of features contains enough highly informative features at any levels of the decision tree. The effect of the new sampling method is clearly demonstrated in this result.
 In Figures 1 (c), (d) and the right panel of Table 2 , the RF and QRF mod-els require larger number of features to achieve the lower prediction error. This means the RF and QRF models could achieve better prediction performance only if they are provided with a much larger feature subspace. For solving the tings of RF and QRF R-packages were set to mtry = M/ 3 and that, the ssRF model does not need many features in the subspace to achieve good prediction performance. For application on high-dimensional data, when the ssRF model uses a subspace of features of size mtry = the achieved results can be satisfactory. In general, when the feature subspace of the same size as the one suggested by Breiman is used, the ssRF model gives lower prediction error with a less computational time than those reported by Breiman. This achievement is considered to be one of the contributions in this work.
 dimensional data set LOG1P by the eQRF and ssRF models. The green and red Figure 2 (a) shows the point and 90% range predictions of the eQRF model, we can see that the point prediction is more scattered than that of the ssRF model in the results. Significant improvement in the prediction results of the ssRF model can be observed in Figure 2 (b). We can see that, the predicted points are closer to the diagonal line which indicates that the predicted values were close indicates that a large number of predictions were within the predicted ranges. These results clearly demonstrate the advantages of the ssRF model over very recently proposed eQRF model. We have presented a new approach for feature subspace selection for efficient node splitting when building decision trees in random forests. Based on that, a new random forest algorithm, ssRF, has been developed for prediction high-dimensional data. The quantile regression is employed to obtain predictions in the regression problem, which makes the RF more robust towards outliers. With the new subspace feature selection, the small subspace size reported by Breiman can be used in our algorithm to get lower prediction error. With ssRF, the performance for both classification and regression problems (the point and range prediction) is preserved and improved. Experimental results have demonstrated the improvement of our ssRF in reduction of prediction errors in comparison with existing recent proposed random forests including eQRF, GRRF and SRF, and especially it performed well on large high-dimensional data.
