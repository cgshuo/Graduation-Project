 1. Introduction
Since the early days of the World Wide Web users have had a choice of competing information retrieval (IR) systems to satisfy their information needs. Two types of systems were prevalent: search engines, with automated methods to collect documents on the Web and full-text search, and directories, with documents collected and categorized by human experts. After years of competition, a small number of search engines with advanced algorithms dominate information seeking on the Web ( Sullivan, 2006 ). This dominance might suggest that the question of which IR system was best for the Web had been settled. Despite advertisements asking,  X  X  X o you Yahoo?  X  ( Kaser, 2003 ), users have settled on using  X  X  X o Google  X  as the verb for Web search ( Quint, 2002 ).

Recently sites have begun to employ new methods to make Web surfing a social experience. Users of social bookmarking sites like Del.icio.us ( http://del.icio.us/ ) can add Web documents to a collection and  X  X  X ag  X  them with key words. The documents, tags, relationships, and other user-supplied information are compiled into what is called a  X  X  X olksonomy  X  ( Gordon-Murnane, 2006 ). Users are able to browse or search the folksonomies in order to find documents of interest. Most have mechanisms to share items with others and browse other users X  commonly used tags.

The word  X  X  X olksonomy  X  is derived from taxonomy. Taxonomies can be found on the Web in the form of directories such as Yahoo ( http://dir.yahoo.com ) serve as taxonomies cataloging links to millions of docu-ments. Taxonomies are usually controlled by experts and are fairly static, tending to use official terminology rather than vernacular phrases. Folksonomies, in contrast, are distributed systems of classification, created by individual users ( Guy &amp; Tonkin, 2006 ). Folksonomies can be broad, with many users contributing some (often overlapping) tags to items, or narrow, with few or one users tagging each item with unique keywords ( Vander Wal, 2005 ).
 Most academic papers on the use and effectiveness of folksonomies have been descriptive ( Al-Khalifa &amp;
Davis, 2006; Chudnov, Barnett, Prasad, &amp; Wilcox, 2005; Dye, 2006; Fichter, 2006 ). To place folksonomies in context with familiar topics of study like search engines and subject directories, a study was done to exam-ine the effectiveness or performance of systems employing folksonomies compared to more traditional Web IR systems.

User strategies for information seeking on the Web can be put into two categories: browsing and searching ( Bodoff, 2006 ). Although it would be very interesting to study the effectiveness of folksonomies versus tradi-tional hierarchical taxonomies when users browse a catalog of Web documents, studying search performance is more straightforward. Traditionally IR performance is measured in terms of speed, precision, and recall, and these measures can be extended to Web IR systems ( Kobayashi &amp; Takeda, 2000 , p. 149). Precision and recall are the primary measures of interest. Precision is defined as the number of relevant results retrieved by a particular IR system divided by the total number of items retrieved. Recall is traditionally found by divid-ing the number of relevant documents retrieved by an IR system by the number of relevant documents in the collection as a whole. Web IR systems have very large collections and the actual number of relevant items for a given query is unknown, so it is impossible to calculate absolute recall. A relative recall measure can be defined as the number of relevant items returned by one IR system divided by the total number of relevant items returned by all IR systems under study for the same search ( Hawking, Craswell, Bailey, &amp; Griffihs, 2001 , p. 34).

It is important to note that Web sites that employ folksonomies, even those included in this study, are not necessarily designed to have search as the primary goal. Studying folksonomies in this way is valuable, how-ever, because these systems are designed to organize information and because search is an important and com-monly used IR function on the Web.

There is no single widely accepted definition of folksonomy, so it is important to explain how the term is used in this study. A very strict definition of folksonomy might only include the tags and their relationships, ruling out any system that included document titles and descriptions, rating and recommendation systems, etc. One folksonomy might rely heavily on social network connections between users while another ignores them.
For the purposes of this study, a broad definition is used: 1. The collection is built from user contributions. 2. Users participate in distributed classification or evaluation. 3. There is a social networking aspect to the addition, classification, or evaluation of items.
Item two is an important point when considering sites like Reddit ( http://www.reddit.com ) and Digg ( http://www.digg.com ). These systems allow users to contribute document titles, categories,  X  X  X p  X  or  X  X  X own  X  votes to affect the ranking of items in the collection, and other information. A more strict definition that requires explicit tagging would leave such sites out of consideration.
 2. Review of related literature
Folksonomy performance has not been extensively investigated so this study relied on previous literature to determine how users search the Web and how to best evaluate IR performance. 2.1. How users search the Web
Any study of Web IR should support normal search behavior and involve the kinds of queries users gen-erally enter. Real-world search logs have been studied extensively. In a 1999 study, Silverstein, Henzinger,
Marais, and Moricz examined query logs from AltaVista that included more than one billion queries and 285 million user sessions. The study had three key findings: 1. Users generally enter short queries. 2. Users don X  X  usually modify their queries. 3. Users don X  X  usually look at more than the first 10 results.
 Jansen, Spink, and Saracevic (2000) and Spink, Dietmar, Jansen, and Saracevic (2001) studied queries from
Excite users and had very similar findings. In addition, they found that relevance feedback and Boolean oper-ators were rarely used and were as likely to be employed incorrectly as correctly.

Jansen and Spink (2006) compared the results of nine different large-scale search engine query log studies from 1997 through 2002. They found that in U.S. search engines around 50% of sessions involved just one query. Query length was fairly uniform with between 20% and 29% of queries containing just one term. Stud-eral Web searches in the number of terms per query and the number of results pages viewed. 2.2. Measuring IR performance on the Web
Studies of IR performance can examine systems with a defined database or systems that retrieve informa-tion from the Internet as a whole, with the latter more relevant to the topic at hand. Performance studies gen-erally use some measure of relevance to compare search engines. Greisdorf and Spink (2001) gave a good overview of the various ways in which relevance can be measured.
 Web search engines have been studied for more than a decade. In one relatively early study, Leighton and Srivastava (1999) compared the relevance of the first 20 results from five search engines for 15 queries.
Although earlier studies of search engine effectiveness exist, the authors went to lengths to describe and use a consistent, controlled methodology. Queries came from university library reference desk questions and an earlier study and were submitted in natural language, making no use of Boolean or other operators. The researchers were prevented from knowing which engine a particular result came from when judging relevance and performance differences were tested for statistical significance. Result documents were placed into catego-ries based on Mizzaro X  X  (1997) framework for relevance. Overall relevance was measured by  X  X  X irst 20 pre-cision  X  with an added factor to account for the effectiveness of ranking. The study found differences in relevance scores based on which relevance category was used, and found the best search engines performed significantly better than the worst. Table 1 presents additional relevant details about this and other compara-ble studies.

A 1999 study by Gordon and Pathak looked at eight search engines and calculated recall and precision measures. The researchers found that  X  X  X hootout  X  studies that pit search engines against each other often only considered the first 10 to 20 results, fewer than many traditional IR studies. They developed a framework of seven features thought to contribute toward the usefulness of such a shootout: 1. Searchers with genuine information needs should be the source of the searches. 2. In addition to queries, information needs should be captured fully with as much context as possible. 3. The number of searches performed must be large enough to allow meaningful evaluations of effectiveness. 4. Most major search engines should be included. 5. Special features of each search engine should be used, even if that means the actual queries submitted to different engines will not be identical. 6. The participants that created the information need should make relevance judgments. 7. The experiments must be designed and conducted properly, using accepted IR measurements and statistical tests to accurately measure differences (p. 146 X 147). The researchers, following a procedure suggested by
Hull (1993) , calculated the precision and relative recall of each engine for the first 15 documents, first 16, etc. up to the first 20 and averaged the measurements to generate the average at document cut-off value 15 X 20 or DCV(15 X 20). They found statistically significant differences in precision and recall at all document cut-off numbers studied. A later study by Hawking et al. (2001) studied effectiveness using queries culled from Web server logs. They generally agreed with Gordon and Pathak X  X  (1999) list of seven features, but found the requirement that those providing the information need evaluate the results too restrictive and thought it reasonable to present the same query to each engine. The authors also proposed an eighth desirable feature: 8. The searches should cover information needs with different topics and with different types of desired results (p. 35).

They presented four different types of information needs based on the desired results ( Table 2 ). The study found search engine performance results surprisingly consistent with Gordon and Pathak (1999) . For most engines, the precision decreased slowly as the number of results considered increased.
 Further evaluations of search engine IR performance have continued. In a study published in 2004, Can,
Nuray, and Sevdik devised and tested an automatic Web search engine evaluation method (called AWSEEM) against human relevance judgments. The number of search engines retrieving a document was used as a mea-sure of relevance, like an earlier study by Mowshowitz and Kawaguchi (2002) , but AWSEEM also took into account the intersection of the retrieved documents X  content. The study found a strong, statistically significant correlation between AWSEEM and human results when looking at the top five results or more for each engine.
 2.3. The overlap of search results between systems
Previous studies that have examined the overlap of results returned by different search engines inspired the design of this study as well. Gordon and Pathak (1999) examined overlap in addition to precision and recall and found that approximately 93% of relevant results appeared in the result set on just one search engine. This percentage was fairly stable even at higher document cut-off values. Overlap was higher for results judged to be relevant than for all results.

Spink, Jansen, Blakely, and Koshman (2006) conducted a large-scale study of overlap in search results between four major Web search engines and one metasearch engine. Two large sets of queries were randomly selected from user-entered queries and submitted. They found that the majority of the results returned on the first results page were unique to one search engine, with only 1.1% of results shared across all engines.
Overlap has even been used as a measure of relevance in itself. Can, Nuray, and Sevdik (2004) and Mow-showitz and Kawaguchi (2002) both used the appearance of a URL in the results of multiple search engines as a measure of relevance. 2.4. Query factors and performance
The studies already described varied in their use of logical operators and other search engine-specific fea-tures but these factors have been studied in depth. In a study by Lucas (2002) , participants created queries on eight topics for a search engine of their choice. The best-performing query was compared to all others to see how the use of operators related to performance. The number of terms in the query and the percentage of terms matching between queries on the same topic had the highest relevance correlation. Perhaps most inter-esting, users did not often consider which operators were supported by the engine of their choice, resulting in worse performance.

Eastman and Jansen (2003) sampled queries from a search engine that allowed query operators. They cre-ated a new set of duplicate queries with the operators removed and submitted both sets to three search engines.
The use of logical operators in queries did not significantly improve IR performance overall, although results varied by engine. 2.5. Other measures of performance
Precision and recall are not the only measure of IR performance in the literature. One issue to consider in comparing IR systems on the Web is the size of their indexes. Hawking et al. (2001) found no positive corre-lation between index size and performance. In a later study, Hawking and Robertson (2003) found that increased index size could improve performance but that study did not examine live search engines indexing the Web as a whole.

Search result ranking can be used to evaluate IR performance. In Vaughan X  X  (2004) study, for example, the correlation between engine ranking and human ranking was used to calculate the  X  X  X uality of result ranking.  X 
Rather than calculating recall,  X  X  X bility to retrieve top ranked pages  X  was calculated by comparing the result set with the set of the top 75% sites as ranked by human judges.

Performance has also been measured without explicit relevance judgments. Beg (2005) , for example, defined a measure of user satisfaction called the search quality measure (SQM). In this measure, participants did not directly judge relevance. Instead, a number of implicit factors were observed including the order in which the participant clicked on results, the time spent examining documents, and other behaviors. 3. Methodology 3.1. Research design
In order to better understand the effectiveness of folksonomies at information retrieval, a shootout-style study was conducted between three different kinds of Web IR system: search engines, directories, and folkso-nomies. The precision and recall performance of the systems was measured and compared and the overlap of results between different IR systems was examined.

Based on the arguments presented in Hawking et al. (2001) and Gordon and Pathak (1999) , participants were asked to generate the queries themselves using their own information needs and then judge the relevance of the search results. Just 20 results were collected from each IR system for each query because many users studies have found strong correlations between different judges ( Voorhees, 2000 ), so including additional judges did not seem necessary.

Relevance judgments were made on a binary, yes-or-no basis similar to the methods used in Hawking et al. (2001) and Can et al. (2004) . Greisdorf and Spink (2001) found that when the frequency of relevance judg-ments was plotted on a scale from not relevant to relevant, the highest frequencies were found at the ends. Binary judgments capture the ends of the scale and require less participant effort.

Table 1 shows relevant information about four of the comparable studies mentioned in the literature review along with the present study. The number of participants, queries, and IR systems tested for the present study were within the range of the earlier studies. The binary relevance scale and the precision and recall measures were also comparable. 3.1.1. Information needs and queries
To generate a range of information needs, participants were randomly prompted to create queries that fell under the topics listed in Table 2 . Topic 1 and topic 2 each address one of the information need types from
Hawking et al. (2001) , with the rest requiring a selection of relevant documents. The fourth type, the need for an exhaustive collection, would be very difficult to test on the Web and was not studied. 3.1.2. Participants
Participants were drawn from students in the School of Library and Information Science (SLIS) and Infor-mation Architecture Knowledge Management (IAKM) graduate programs at Kent State University.

Although this population might not completely reflect all Internet users, most studies in the literature have used academics to provide queries and judge relevance. 3.1.3. IR systems in this study
Studies in the literature review looked at as few as five and as many as 20 search engines. This study exam-ined just eight search systems in order to keep the time required to participants low. Examples of search engines, directories and folksonomies were needed.

Many Web sites employ tagging and allow users to search their folksonomies. Some, such as Flickr ( http:// www.flickr.com/ ) and YouTube ( http://www.youtube.com ), are restricted to one domain and would not be easy to compare to general search engines or directories. Social bookmarking sites, which allow users to make note of, share, and search Web documents, were thought to be most comparable to general search engines and directories. One difficulty in choosing comparable systems is the fact that Web IR systems vary widely in collection size.
Google indexes billions of pages, for example, whereas the Open Directory Project ( http://www.dmoz.com/ ) catalog is in the millions. Collection sizes were hard to come by for social bookmarking sites, although there are methods to make estimates ( Agarwal, 2006 ). Because size estimates were not available for all systems con-sidered, the impact on IR performance was not examined.

Only social bookmarking systems that allowed searches constrained their own collections were used. Furl ( http://www.furl.net ), for example, includes a search interface powered by the search engine Looksmart, but users may constrain their search to Furl X  X  folksonomy. Some folksonomies also limit the number of items retrieved by the search interface. These factors eliminated some folksonomies originally considered for study, such as StumbleUpon ( http://www.stumbleupon.com ). Furl, Del.icio.us and Reddit X  X  search interfaces func-tioned much like a traditional search engine, allowing automatic retrieval and parsing of 20 results.
The search systems were chosen for their popularity and large user base, ability to be reliably parsed by an automatic testing interface, and comparability to search engines examined in previous studies. Taking these factors into account, Google, Microsoft Live (formerly MSN), AltaVista, Yahoo (directory only, sans search engine results), the Open Directory Project, Del.icio.us, Furl, and Reddit were chosen for this study. 3.2. Data collection and procedures
Participants were able to access the test interface at their convenience. Each participant was asked to create and rate the results for three queries. Fig. 1 shows the search page for the first query. The query prompts from
Table 2 were assigned randomly to each search and put in the text of the search page. Participants were asked to describe their information need in the text area and then type a query into the  X  X  X earch  X  field. The test interface submitted the query to each of the IR systems in the study and retrieved the results pages. The raw HTML code of the result pages was parsed using rules tailored for each system. The document title,
URL, description, and rank was retrieved for the results from each search and stored in the database. If a description was not available, the test interface attempted to follow the document X  X  URL and retrieve a description from the document itself. The content of the  X  X  X escription  X  meta tag was used if available. If not, the content of the first paragraph tag was used. Despite these efforts no description could be found for some documents and was left blank.

The search interface collected up to 20 results from each IR system and then randomized their order and identified duplicated results via a case-insensitive comparison of URLs. Results were then presented like com-mon Web search result pages ( Fig. 2 ), with the document title displayed as a link to the document followed by the description and URL. Participants could click on the links to see each document in a new browser window, but were not required to explicitly view each document. Results from different IR systems were presented in the same way so that participants did not know which system returned a particular result.

Participants were asked to check a box to the right of each result if they thought the document was relevant to their search and the relevance judgments were saved to a database. The  X  X  X ubmit  X  button was placed at the very bottom of the results page, so participants had to see all the results before moving on. Participants were prompted to submit a query and rate the results three times, although one user submitted an additional query (most likely after hitting the back button). 3.3. Measures of effectiveness
Precision, relative recall, and retrieval rate (the number of documents returned compared to the maximum possible) were used to measure the effectiveness of the various IR systems. The measures were calculated at a cut-off of 20 and then at the average for cut-offs 1 X 5. The first measure was the maximum number of results collected by the interface while the second both weighed early relevant results more strongly than latter ones and smoothed out effects particular to one cut-off value ( Gordon &amp; Pathak, 1999 , p. 154).
When calculating relative recall at lower cut-offs, the number of relevant items retrieved by one IR system up to that cut-off was divided by the number of relevant documents retrieved for all engines at the maximum ber of relevant documents. Cases where no documents were judged to be relevant are excluded from recall measurements. Calculations of precision do not take into account searches that did not retrieve any documents. 4. Results
A total of 34 participants completed the experiment, completing 103 searches. Each search was submitted to 8 different systems and the top 20 results were collected. For many searches, one or more of the search sys-tems returned less than 20 documents  X  the total number actually returned was 9266. About 22% of the results (2021 documents) were returned more than once for the same query, usually when different systems had over-lapping results. 4.1. Statistical analysis
A comparison of the queries entered by the participants with those in other shootout studies and Web query log studies can give some sense of this study X  X  external validity ( Table 3 ). The queries entered by the partic-ipants had more words and used more operators than generally found in query logs, but fell within the range found in other shootout studies.

The documents listed on the results page were not explicitly ranked but most search engines attempt to return results with the most relevant documents near the top. Eisenberg and Barry (1988) found significant differences in relevance judgments when ordering randomly, from high to low, and from low to high. Their recommendation to present results randomly was followed and very little bias was found in the participants X  relevance judgments due to display rank. When considering the relevance of results at each position within each search, Spearman X  X  rho for document position was 0.076 (significant at the 0.01 level), meaning that for any given search the ordering effect was minor.

Many tests for statistical significance rely on an assumption of a normal distribution and equal variances sures, the null hypothesis that the distributions were symmetric and normal was rejected. The Levene test for homogeneity of variance also yielded significant statistics so equal variances could not be assumed. Testing at different document cut-off values or when cases were grouped by IR system yielded similar results. When data 2000 , p. 415). The Kruskal X  X allis test can be used instead of a normal analysis of variance by considering ranks instead of the values from the observations ( Hollander &amp; Wolfe, 1973 , p. 194). This study largely followed the literature by testing significance with Kruskal X  X allis tests, grouping using Tukey X  X  Honestly
Significant Difference (HSD), and using Spearman X  X  Rank for correlation analysis. 4.2. Information retrieval effectiveness 4.2.1. Precision and recall at DCV 20
Precision, recall, and retrieval rate were calculated and significant differences were found ( p &lt; 0.01) among the IR systems and system types at all document cut-off values examined.

Precision results for each engine can be seen in Table 4 . Google, Yahoo, AltaVista and Live have been com-peting against each other for years, so it is not surprising that they performed fairly well. Del.icio.us returned results that were almost as likely to be relevant as those from Live, which shows that a folksonomy can be as precise as a major search engine. Unlike the other systems, Del.icio.us tended to have increased precision as the DCV increased. This could be because of a less effective ranking algorithm or the lack of a system for users to rate documents as in Furl and Reddit. Reddit X  X  very low precision score may be due to the lack of tags and reliance on user-submitted titles. Table 5 shows the results for recall. AltaVista had the highest relative recall with Google and Live following. The rest of the search systems fell much lower. The traditional search engines, with automatic collection gathering, obviously had the advantage.

Tukey X  X  HSD (alpha = 0.05) were calculated to determine the subsets of IR systems that were statistically indistinguishable from each other by precision ( Table 4 ). Three overlapping groups were found with the folksonomies are represented in each group. Del.icio.us actually fell into the highest performance group with search engines and directories. Grouping the search systems on recall performance ( Table 5 ) showed more clear distinctions. All the directories and folksonomies fell within the same group, search engines filling out the two higher performance groups.

When searches were grouped by IR system type, the search engines had both the highest precision and similar. The folksonomies were much more likely to retrieve documents than the directories, perhaps because folksonomies impose less control over submissions. This increase in quantity saw a corresponding decrease in quality. 4.2.2. Precision and recall at DCV(1 X 5)
Fig. 3 shows how the precision of each IR system varied as more documents were considered. Most of the systems X  precision scores fell as the cut-off increased, with AltaVista, Google, Live and Yahoo showing par-ticularly pronounced drops early on. This decline indicates that documents were ranked effectively. Del.icio.us, oddly enough, tended to have higher precision as the DCV increased. The rest of the systems showed an early increase in precision after the first result followed by a steady decline.

In Fig. 4 , a similar chart is shown for recall. The gap between the search engines and all other systems is striking. It is also interesting to note that the search engines continued to increase recall performance at a relatively stable rate at the higher cut-off values. The directories and folksonomies had much lower retrieval rates, so it stands to reason that they would not see recall gains at high document cut-off values where few searches retrieved additional results.
 Only the search engines reliably retrieved all 20 documents, so a lower DCV was used for further analysis.
Table 7 shows the results at DCV 1 X 5. AltaVista led both precision and recall while Reddit had the poorest performance in both measures. Compared with the values at DCV 20, all the IR systems except Del.icio.us had a higher precision score, with higher-ranked documents more likely to be relevant. Unsurprisingly, recall scores were lower for all IR systems. A Tukey X  X  HSD analysis showed very similar groupings to the values at DCV 20, with precision groups showing a great deal of overlap.

Table 8 shows the breakdown by IR system type. Search engine searches again had the best performance in all three measures. Directory searches had the next best precision and recall, with folksonomy searches taking second place in terms of retrieval rate. 4.3. Performance for different information needs 4.3.1. Specific information needs
An IR system might be particularly well suited to a particular information need. Because folksonomies have social collection and classification methods it was thought they might have an advantage for entertain-ment and news searches. Search engines may miss novel items between spider visits and directories may be slow to add new items, but users can find and tag items as quickly as news can spread. Conversely it was thought folksonomies might perform poorly at factual or exact site searches because they lack both the struc-ture of a directory and the breadth and full-text processing of search engines. Table 9 shows the results by search system type and information need. Differences between groups were significant ( p &lt; 0.01).
The folksonomies did outperform directories for news searches in both precision and recall but fell well behind the search engines. The directories retrieved very few documents for news searches, none of them rel-evant. For entertainment searches the folksonomies lost out to the other IR systems but multiple comparisons (Tukey HSD, alpha = 0.05) showed that the performance difference between folksonomies and directories was not significant.
 As expected the folksonomies performed inferior to the other IR systems for factual and exact site queries.
These were the worst performing information needs for the folksonomies. Comparing the scores for the dif-ferent groups for factual and exact site information needs (Tukey HSD, alpha = 0.05) showed that the differ-ence between folksonomies and search engines was significant for both precision and recall. Although the folksonomies consistently scored worse than the directories, the difference in scores was not significant. 4.3.2. Categories of information needs
When searches were subdivided by information need category and IR system type ( Table 10 ), the precision, recall and retrieval performance varied significantly ( p &lt; 0.01) between groups. The search engines had the highest precision and recall scores for all information need categories, with particularly good recall perfor-mance for specific item searches. The directories and folksonomies saw their best precision performance in searches for a selection of items. The folksonomies seemed to be least suited to searches for a specific item. The precision scores of the folksonomies and directories fell into the same group when compared by a
Tukey HSD. The same was true for recall scores. So although the directories did outperform the folksonomies in precision and recall in each category of information need, there is little confidence in this difference. 4.4. Overlap and relevance of common results In many cases a particular document appeared in the results of multiple IR systems in a single search.
Reviewing the literature has revealed that documents returned by more than one search engine are more likely to be relevant. Table 11 shows that this is clearly the case in this study as well. The percentage of results deemed relevant by participants for a document returned by two IR systems was almost double the relevance vidual IR system or system type.

Nearly 90%, of documents only appeared in the results of one IR system. Table 12 compares the results of the current study with previous studies discussed in the literature review. Although the similar Gordon and
Pathak study (1999) saw less overlap, the current study falls comfortably between those results and the results of Spink et al. (2006) . Overlap rates went up when only considering results that were judged to be relevant, much like Gordon and Pathak X  X  (1999) results. These similarities point to the validity of comparing results from the directories and folksonomies in the same ways previous studies have compared just search engines.
A positive effect was found when a document was returned by more than one type of search system. Doc-uments that appeared in the results of just one type were only about half as likely to be relevant (17.5%) as those that appeared in two types (34.7%), and those that appeared in all three were even more likely to be relevant (42.3%). A breakdown of these results by specific IR system type is shown in Table 13 . Documents that were returned by all three types were most likely to be relevant, followed closely by those returned by directories and search engines and those returned by folksonomies and search engines. This is very interesting because it suggests that meta-searching a folksonomy could significantly improve search engine results.
Although documents that appeared in both directory and search engine results scored even better, the differ-ence between that set and the folksonomy/search engine set was not statistically significant. On the other hand, incorporating a folksonomy into an existing directory might not significantly improve performance. 4.5. Query characteristics affecting performance
An examination of the characteristics and content of the participant X  X  queries uncovered some reasons why the folksonomies so often performed inferior to the other IR systems. Only a few query characteristics corre-query the lower the retrieval rate ( 0.141). The presence of operators had a significant negative correlation with retrieval rate ( 0.110) especially when excluding Boolean operators ( 0.151) and correlated significantly with recall ( 0.103) as well. When just considering folksonomy searches the word count correlation was no Some of the folksonomies X  poor performance may have been due to poor support of query operators.
A few example queries illustrate other causes for poor performance. In one search the participant entered the query  X  X  X howtimes 45248 borat,  X  including a zip code, explaining,  X  X  X  want information on the showtimes for the movie Borat.  X  Google, Live and AltaVista performed very well with this query, with precisions of 100%, 100%, and 62.5% and recall values of 22.2%, 33.3%, and 55.6%, respectively (DCV 20). None of the folksonomies returned any results at all. Removing the zip code from the query produced results in Del.icio.us and Furl and searches for  X  X  X howtimes  X  or  X  X  X orat  X  produced thousands. The folksonomies seem to have trea-ted all words in the query as absolutely required. Search engines had the advantages of more liberal policies and much larger collections to pick from. What are the chances of another user in a social bookmarking site drilling their way down to a relevant page and then tagging it with the zip code?
The query  X  X  X ouis Hennepin  X  was likewise successfully executed by the search engines but had no results in the folksonomies. The participant X  X  information need was biography, bibliography, and criticism of Hennepin for a class paper. Had another Del.icio.us, Furl, or Reddit user needed to write a similar paper, the participant could have benefited from that user X  X  research. In cases like this, however, where the subject of study is spe-cialized, the searcher may be the first user using the site for this subject. Folksonomies were thought to be well suited to timely searches because users could easily add new items of interest to the collection. Conversely, for older topics or items falling outside of mass interest, folksonomies might perform poorly. As these systems grow larger and gain more users, though, this could become less of an issue. 5. Discussion 5.1. Conclusions
This study demonstrated that folksonomies from social bookmarking sites could be effective tools for IR on the Web and can be studied in that same way that search engines have been studied in the past. The folkso-nomies X  results overlapped with the results from search engines and directories at rates similar to previous IR studies with documents returned by more IR systems more likely to be judged relevant. A document that was returned by both a folksonomy and a search engine was more likely to be relevant than a document that only appeared in the results of search engines.

Significant performance differences were found among the various IR systems and the IR system types. In general the folksonomies had lower precision than directories and search engines, but Del.icio.us did perform better than Open Directory and about as well as Live at DCV 20. In recall, there were few statistically signif-icant differences between the directories and the folksonomies.

The social methods used by folksonomies may be helpful for some information needs when compared to expert-controlled directories, but in general the search engines with their automated collection methods were more effective. Folksonomies performed better for news searches than the directories but the search engines had much higher performance in all categories. The folksonomies did particularly poorly with searches for an exact site and searches with a short, factual answer.

This study discovered a number of possible causes for the differences in performance among the various IR systems and IR system types. The use of query operators was found to have a small but significant negative correlation with recall and retrieval rate. The use of operators correlated more strongly with poor retrieval rates for folksonomies. The folksonomies seemed to handle queries differently that the other IR systems, in some cases requiring all terms in the search string be present in order to return a result.

Despite the fact that the search engines had consistently higher performance, folksonomies show a great deal of promise. First, this study showed that search results from folksonomies could be used to improve the IR performance of search engines. Second, this study demonstrated a number of ways in which existing folksonomies might be able to improve IR performance, for example by better handling query operators and not requiring all terms. Finally, folksonomies are a relatively new technology that may improve as more users are added and techniques are fine-tuned. 5.2. Suggestions for future research
The current study is one of the first studies to examine folksonomies on the Web empirically and barely scratches the surface of this interesting subject. Having completed this study, a number of ways to improve the methodology have presented themselves. Additional information, such as whether or not participants fol-lowed links before making relevance judgments, would be very interesting. This study used a relatively small number of IR systems and more participants and searches would also be beneficial.

The folksonomies differed in precision and recall, and it is likely that there are specific characteristics of folksonomies that would illuminate these differences. Methods of collection building, socialization, tagging, and ranking may differ from one folksonomy to the next. The poor precision and recall performance of Reddit suggests that there might be a very important difference between systems that employ tagging and those that search user-submitted document titles and rankings.

Although this study examined the use of query operators, query length, and similar factors, the methods by which folksonomies can increase the effectiveness of their internal searching functions deserve further study. If a social bookmarking system allows users to assign categories, tag with keywords, change the title, contribute a description, or input comments for an item, which of these fields should be given the most weight? If one document has been given a highly positive average rating by users, while another has a lower rating but better matches the query text, how should they be ranked in the results? Concepts that are well-known in the IR lit-erature like Boolean logic, wildcards, stemming, correcting for spelling errors, and use of synonym rings are not consistently applied (or even applied at all) in folksonomies.

This study did not address any information needs that could only be satisfied by an exhaustive list of doc-uments. Although this is a difficult proposition on a large, uncontrolled collection like the Web, there are pos-sible ways to address these information needs. One possibility would be to set up an artificial scenario where new documents were made available on the Internet referencing a nonsense word or name that does not cur-rently return any results in any of the IR systems under consideration. It would be important to find ways to ensure that such an artificial situation matches real user information needs and IR tasks.

Further studies with IR systems that cover different domains are also needed. With the growth of blogs and systems dedicated to finding blogs and articles posted to blogs, it would be interesting to perform a similar study with search systems such as Google Blog Search and Technorati. A shootout-style study pitting an expert-built classification system against Flickr or YouTube for multimedia would be very interesting. Search is just one small aspect of the use of social classification, collaborative tagging, and folksonomies.
Studies could be done comparing navigation path length, task completion rate and time, and other measures when browsing a conventional, hierarchical directory as opposed to a tag cloud with only  X  X  X imilar-to  X  or  X  X  X ee-also  X  relationships. It would also be interesting to study the many other ways in which users might use folkso-nomies and social bookmarking systems, for example browsing the newest items, looking for random items out of curiosity or for entertainment, organizing their own often-used resources, or socializing with other users.
 Acknowledgement
The researcher would like to thank Professor David Robins for all his input and advise during the course of this study. Professors Yin Zhang and Marcia Zeng also provided valuable advice and direction.
 References
