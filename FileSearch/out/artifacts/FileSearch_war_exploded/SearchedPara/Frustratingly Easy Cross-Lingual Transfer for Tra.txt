 Cross-lingual learning techniques enable to transfer useful supervision information from well-resourced to under-resourced languages, helping the develop-ment of NLP tools for a large number of languages. In this work, we present a simple method for trans-ferring dependency parsers between languages.
Two main strategies have been considered to transfer syntactic annotations: (a) direct model transfer and (b) annotation transfer. The first ap-proach assumes a common representation between the source and target languages (e.g. at the level of PoS tags), which enables to train a model on source data and to use it to parse target sentences. The per-formance of  X  X ure X  delexicalized dependency trans-fer can be significantly improved using additional techniques such as self-training (Zeman and Resnik, 2008), smart data selection (S X gaard, 2011), relex-icalization and/or multi-source model transfer (Co-hen et al., 2011; Naseem et al., 2012; T  X  ackstr  X  om et al., 2013). The second approach (transfer of anno-tations) requires parallel sentences, in which word alignments are used to infer target syntactic struc-tures from source dependencies. The main difficulty here is to cope with cases of non-isomorphism be-tween the source and target structures as well as with the noise in source annotations and in alignments. Turning source trees into target trees indeed may require to filter poor alignments and to apply vari-ous heuristic transformation rules, such as the ones introduced in Hwa et al. (2005), later improved in Tiedemann (2014).

In this study, we consider a simple, yet effective approach to transfer annotations, which entirely dispenses from the transfer rules of Hwa et al. (2005), the sharp filtering of partially annotated trees (Tiedemann, 2014), the inclusion of fake root dependencies for unattached words (Spreyer and Kuhn, 2009), or the multi-step process of Rasooli and Collins (2015). Our proposal is, in fact, quite as straightforward (apart from the use of parallel texts) as the delexicalized transfer method of McDonald et al. (2013) while achieving performances that surpass this state-of-the-art method by a wide margin, and competing with recent algorithmically costly methods: it globally outperforms the scores of (Ma and Xia, 2014) and even achieves the same performance as (2015) for 1 language out of 5. It can thus be used as a fair and simple baseline when evaluating new transfer methodologies.
Our method relies on the observation (Section 2) that transition-based dependency parsers using the dynamic oracle strategy can be trained from partially annotated trees (in which some words may not have a governor) using exactly the same algorithm that is used to train from fully annotated tree . As explained in Section 3, this observation allows us to design a simple transfer strategy that, first, (partially) projects syntactic annotations from a source language onto a target language via unambiguous word alignments and, second, learns a dependency parser from these partially annotated target data. We then apply this strategy for six language pairs. 2.1 Training with a Dynamic Oracle We consider a transition-based dependency parser based on the arc-eager algorithm (Nivre, 2003): this parser builds a dependency tree incrementally by performing a sequence of actions . At each step of the parsing process, a classifier scores each possible action and the highest scoring one is applied.
Training relies on the dynamic oracle of Goldberg and Nivre (2012): for each sentence, a parse tree is built incrementally; at each step, if the predicted action creates an erroneous dependency (or, equiva-lently, prevents the creation of a gold dependency), a weight vector is updated, according to the percep-tron rule. The set of all  X  X orrect X  actions is built con-sidering the (potentially wrong) predicted tree and the gold action is defined as the correct action with the highest model score.

It is crucial to notice that the training algorithm is an error-correction learning procedure that solely depends on its ability to detect when an action choice will result in an error: when no error is detected, the construction of the parse tree continues according to the model prediction. Consequently, this training procedure can also be used, unchanged, to train a dependency parser from partially annotated data: when no supervision information is available (no reference dependency is known), all actions are considered as correct; in this case, the predicted action is one of the correct actions, the weight vector is not updated, and the training process goes on.
This observation can be readily generalized to de-For the experiments in Section 3, we use a beam-search version of the parser trained with an early-update strategy (Collins and Roark, 2004). 2.2 Experiments on Artificial Datasets We first carry out a control experiment on datasets in which dependencies have been artificially removed to show that learning from partially annotated data is possible. We compare the performance achieved by a parser trained on n % of the sentences of the train set with the performance of a parser trained on the whole train set, but in which only n % of the depen-dencies of each sentence are known. In both condi-tions, the total number of dependencies considered during training is roughly the same. Figure 1 plots the parsing performance for German, evaluated by the UAS, with respect to the percentage of depen-dencies that were kept. To avoid any bias, the re-ported scores have been averaged over 10 runs. Sim-ilar results are observed for 5 other languages of the ald et al., 2013).

Overall, these results show that learning a parser from partially annotated data is possible. Two other conclusions can also be drawn. First, it appears that the number of training examples can be reduced without significantly hurting the performance: re-moving half the training sentences only reduces the UAS by 1.2 absolute. Second, for a similar num-ber of annotations (i.e. number of dependencies known), better results are achieved when more sen-tences are annotated, even if this annotation is only partial: in Figure 1, the UAS of a parser trained on partially annotated sentences is higher than the UAS of a parser trained from a subset of the training set.
Indeed, in a partial structure, information on un-known dependencies can be inferred from neigh-bouring dependencies because of the projectivity constraints. Therefore, the set of gold actions is sometimes smaller than the set of possible actions and an update can happen even if the dependency is unknown. For instance, when training a German dependency parser, 35,382 updates are performed when only 60% of the dependencies are known, to be compared with the 31,339 updates that take place when training on 60% of the fully annotated sen-tences. In this section, we show how learning from partially annotated data can be used for cross-lingual depen-dency transfer. A partial projection strategy is first applied to infer partially annotated data for a target language from a full-parsed source data. The target annotations are then used to learn an effective pars-ing model for the target language. 3.1 Partial Projection of Dependencies Using sentence-aligned bitexts associating an au-tomatically parsed text in a resource-rich language with its translation in target language, dependencies can readily be projected via alignment links, yield-ing  X  X heap X , albeit noisy, supervision data. The main difficulties with the projection arise with many-to-many links and un-aligned tokens. Hwa et al. (2005) have proposed several specific heuristics to deal with the different kinds of alignments and project a full dependency tree. However, this solution comes at the expense of deleting words or creating fake de-pendencies in the target sentence, which may intro-duce unreliable annotations in the target data.
In this work, we advocate another approach and show that it is simpler and more effective to ignore unattached words and many-to-many alignments: we claim that training a parser from a corpus of high-quality annotated (albeit partially) data will result in better parsing performances than a parser trained from fully-annotated but noisy data.

In practice, parallel sentences are aligned in both directions with Giza++ (Och and Ney, 2003) and these alignments are merged with the intersection heuristic. This heuristic only selects 1:1 align-ment links that occur in the two directional align-ments and, intuitively, contains only reliable align-ment points, as they have been predicted by two in-dependent models. Note that we do not try to model the reliability of dependency and/or alignment links, making our approach easy to implement and param-eter free.

We additionally consider three simple heuristics to filter the transferred annotations and improve their precision: we first remove from the training set tar-get sentences containing non-projective dependen-of the words are attached. The latter case indeed cor-responds to parallel sentences with few alignment links that are often not perfect translation of each other. Finally, following Rasooli and Collins (2015), we ignore all alignment links that associate words with different PoS tags. As shown in Figure 2, for each pair of aligned sentences, only the dependen-cies for which both the head and the dependent are each aligned to exactly one word (PoS-consistent)
This approach finally produces an automatically annotated corpus for the target language that con-tains mostly accurate annotations, even if the depen-dency structure is incomplete. 3.2 Datasets and Experimental Setup All our experiments are carried out on six lan-Project: German, English, Spanish, French, Italian and Swedish. We considered as parallel corpora a subset of the Europarl corpus (Koehn, 2005) that have exactly the same English sentences, collect-ing 1 , 231 , 216 parallel sentences for the 6 language pairs.

For training the target partial data, we used our own implementation of the arc-eager dependency parser with a dynamic oracle, using the features de-scribed in (Zhang and Nivre, 2011), with a beam size of 8. The beam-search strategy is used for training (20 iterations) and decoding. 3.3 Dependency Transfer Experiments For each language pair, the source dataset (Europarl) is PoS-tagged and parsed using the transition-based version of the MateParser (Bohnet and Nivre, 2012), trained on the UDT corpus with a beam size of onto the target side of the corpus and filtered using the method described above. As reported in Table 2, after filtering, the number of sentences in the train set varies between 15 , 191 for German and 52 , 554 for Swedish and the percentage of tokens receiv-ing a dependency varies from 88.15% for French to 90.84% for German.

Our parser is then trained on the resulting partially annotated dataset and its performance evaluated on the target UDT test set by the Unlabeled Attachment Score, UAS (excluding punctuation). Gold PoS were used for evaluating in order to make results of our method comparable with state-of-art methods.
The proposed method is compared to three trans-fer method baselines: the relexicalisation procedure of McDonald et al. (2011), the method of Ma and Xia (2014) for transferring cross-lingual knowledge using entropy regularization, and the recent density-driven approach of Rasooli and Collins (2015) ex-ploiting partially annotated data. The results are first compared for cross-lingual transfer from English ring from multiple sources. Note, however, that a di-rect comparison with these results is not completely fair as systems were not trained with the same ex-act conditions (less features, lower beam size, etc). As a baseline for comparing parsers, we also report the scores achieved by Rasooli and Collins (2015) and by our method on fully projected sentences ( X  X n-100% X ). 3.4 Results Table 1 reports the results of the various transfer methods. Our method achieves significantly better results than the relexicalisation procedure of Mc-Donald et al. (2011) (up to +8.33 in Spanish) and outperforms the method of Ma and Xia (2014) for 3 languages (from +0.91 (fr) to +2.86 (sv)) and equal-izes it for one (it). Finally, for Swedish, it achieves performance that are on a par with that of Rasooli and Collins (2015).

It therefore appears that, while being much sim-pler, the proposed approach achieves results very competitive with state-of-the-art methods at a much cheaper computational cost: our results have been obtained by training a single parser with a beam size of 8, while Ma and Xia (2014) use a parser with ex-act inference, the training and inference complexity of which is O ( n 4 ) and the method of Rasooli and Collins (2015) requires the costly training of 4 dif-ferent parsers each using a beam size of 64. Results of Table 1 also show that Rasooli and Collins (2015) achieves better scores than our method when training on fully projected trees. This can be explained by the differing training conditions show the benefits of considering partial dependency trees and not only sentences for which a complete parse tree is transferred: a parser trained with partial dependencies improves the UAS up to 4.8 points. In this paper, we have proposed and evaluated a very simple procedure to train a dependency parser with projected partial annotations. In fact, our training algorithm is virtually unchanged with respect to the fully supervised case. Yet, it has proved extremely effective when combined with an appropriate selec-
Further improvements could be obtained using additional tricks, such as better data selection strate-gies or constrained parsing. Besides that, it is worth noting that our method is not only on a par with the method of Rasooli and Collins (2015) but could also be combined with it. Indeed, their first step can be substituted by our method. Since the latter outper-forms the former, the combination of the two should improve their best final scores.

In our future work, we intend to study how this training strategy behaves for other transition-based systems or, more generally, for other NLP scenarios using partially annotated data.
 This work has been partly funded by a DGA-RAPID pyrus) and the French Direction g  X  en  X  erale de l X  X rmement . We thank the reviewers for their accu-rate comments and suggestions.

