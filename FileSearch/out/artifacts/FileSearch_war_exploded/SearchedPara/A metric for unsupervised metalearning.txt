 Department of Computer Science, Brigham Young University, Provo, UT, USA 1. Introduction
Two main forms of learning are generally considered: supervised and unsupervised. In supervised learning, the task consists in discovering a mapping from a set of features, or independent variables, representing data items, to some target outcome, or dependent variable. The task is deemed supervised because the training data available to the learner includes both the data items and their associated, teacher-given target values. Classi fi cation and regression are typical examples of supervised learning. In unsupervised learning, the tasks consists in discovering a grouping of data items based on the set of features used to represent them. The task is deemed unsupervised because the training data available to the learner is restricted to the data items themselves. No outcome is speci fi ed; the learner must rely on some notion of similarity among data items to induce a meaningful grouping thereof. Clustering and segmentation are typical examples of unsupervised learning.

A mapping, in the supervised sense, can easily be viewed as a grouping of data items based on the values of the target outcome, i.e., one group per target value. Because a teacher provides the value of the outcome for eachtraining data item, supervisedlearning may thus theoretically be used to induce arbitrary groupings. On the other hand, given a similarity measure, unsupervised learning can only induce a single grouping. In practice, however, supervised learning also induces a single grouping, namely that speci fi ed by the teacher-labeled training data items. The main difference is that, whereas supervised learning requires, and is constrained by, something external to the training data items, unsupervised learning relies solely on the information available in the description of the training data items. Unsupervised learning has proven useful in a wide variety of applications, either as the technique of choice or as a complement to supervised learning.
 Our goal here is to raise unsupervised learning to the metalevel, i.e., unsupervised metalearning. Metalearning has been de fi ned as the study of  X  X ethods that exploit metaknowledge to obtain ef fi cient models and solutions by adapting machine learning and data mining processes X  [5]. Metalearning differs from base-level learning in the scope of adaptation. Whereas learning at the base level focuses on accumulating experience on a speci fi c learning task (e.g., credit rating, medical diagnosis, mine-rock discrimination, customer segmentation, etc.), learning at the metalevel seeks connections among learning algorithms and/or tasks. In other words, at the metalevel, the data items of interests, i.e., those to learn from, are learning algorithms and tasks. Metalearning increases our understanding of the nature of base-level learning algorithms and in turn our ability to apply machine learning more effectively and extensively.

While a signi fi cant amount of work has been done in supervised metalearning, especially mapping tasks to learning algorithms (e.g., see [2,6,24, 25,30]), little, if any, has been done in unsupervised metalearning. Yet, we argue that, as is the case at the base level, unsupervised learning may also prove useful as a complement to supervised learning at the metalevel. For example, clustering tasks could facilitate transfer learning, and clustering learning algorithms based on their behavior may reveal interesting similarities, which could be further exploited to improve algorithm selection. One of the prerequisites for unsupervised learning, however, is the existence of suitable similarity measures, or reciprocally, distance functions.

In this paper, we focus our attention on measures of similarity for algorithm behavior, as a precursor to clustering learning algorithms. Over the years, through theoretical advances and experience with increasingly many applications across a wide range of domains, we have gained some valuable insights about the relative performance of a number of algorithms over certain types of tasks. For example, Naive Bayes is known to be optimal if the features are conditionally independent given the class, and backpropagation often outperforms decision tree learning if the features are continuous. Yet, our understanding and interpretation of behaviors among these learning algorithms remain rather limited. This is mainly due to the fact that our efforts tend to focus on designing new algorithms or extensions to existing algorithms that address known limitations. Relatively few researchers have attempted to generalize across algorithms and tasks to capture knowledge about the interaction between the mechanism of learning and the concrete contexts in which that mechanism is applicable. And when they have, the focus has been on comparing behavior based on global measures of performance such as predictive accuracy or area under the ROC curve.

Here, we propose a fi ner-grained approach where behavior is analyzed at the data item-level using diversity measures. Unlike global measures, which provide only an idea of average performance over all data items, diversity measures capture local variations among data items. Historically, these measures have been almost exclusively studied in the context of ensemble learning, where one seeks to maximize measure, would seem suf fi cient to allow behavior-based clustering of learning algorithms. However, in order for clustering to be well-de fi ned, such distance measures should actually be distance functions, or metrics. We revisit the most popular diversity measures and show that only one of them, namely classi fi er output difference, gives rise to a distance function. We subsequently use that metric to cluster 21 learning algorithms based on their behavior on 129 datasets. We highlight some interesting relationships arising from that clustering, as well as contrast it with the clustering of the same 21 algorithms obtained by using a global measure based on accuracy.
 The paper is organized as follows. In Section 2, we review previous work relevant to our analysis. In Section 3, we present an overview of the most popular pairwise measures of diversity, analyze them in terms of their suitability as distance functions, and show that only one of them satis fi es all of the requirements for a metric. We use the thus selected metric in Section 4 to cluster a number of well-established learning algorithms, and contrast it to the clustering obtained by a global distance measure. Section 5 concludes the paper and points to further possibilities for unsupervised metalearning. 2. Related work
One attempt at unsupervised metalearning, focused on tasks rather than algorithms, is described in [29]. There, the authors show the result of clustering 57 learning tasks based on 21 (meta)features, i.e., statistical and information theoretic measures over the corresponding datasets, using Kohonen self-organizing maps. Overlaid on each cluster is the relative performance of 6 learning algorithms (IBk, C4.5, PART, NB, OneR and KD). Simple conclusions such as, cluster C contains tasks that seem to have these characteristics and are best solved by these learning algorithms, can then be drawn. Similarly, the work in [33] describes a method for clustering time series based on extracted characteristics from these or metafeatures, and clusters this transformed data. The results show that higher quality clusters may be obtained this way than with the raw data.

Perhaps closest to ours is the work of [16], where results on clustering 45 pairs of algorithms based on the characterization of each cluster is derived from metafeatures obtained from the tasks to which the learning algorithms were applied. In particular, the authors identify metafeatures predictive of whether learning algorithms are likely to exhibit very high or very low error correlation. However, the metafeature values are not clearly distinguishable across clusters, and 2 of the 4 clusters look rather mixed according to their error correlation distributions. Furthermore, the 10 algorithms under study represent only 3 model classes. The analysis presented here complements and signi fi cantly extends these, as well as others (e.g., see [19,23]).

There has been long-standing interest in the notion of diversity in machine learning, mostly due to work in ensemble learning and multiple classi fi er systems, where it has long been known that diversity is essential to improving accuracy (e.g., see [7 X 9,11,13,17,18,22]). Over the years, a number of measures of algorithm diversity have thus been proposed, for both pairwise comparisons and non-pairwise comparisons. Interestingly, several of the same measures of diversity used in machine learning, or their reciprocal known as measures of association, have also been used extensively in the social sciences, where one often seeks to characterize the degree of agreement among, for example, expert labelers or survey respondents (e.g., see [4,27]). Since we will use diversity for clustering, and thus as a measure of distance, we focus exclusively on pairwise measures here. Whereas others have often focused on comparing and analyzing relationships among various measures (e.g., see [10,32]), we analyze them individually with respect to their s uitability for clu stering, i.e., whether they induce metrics. 3. A metric for the space of learning algorithms
The most obvious, and often used, distance measure between two learning algorithms consists of the absolute value of their difference in (predictive) accuracy (e.g., see [35]). Let A 1 and A 2 be two learning algorithms and acc number of learning tasks. We would de fi ne their accuracy distance AD by: There are some issues with AD that suggest that an alternative distance measure may be preferable. Consider, for example, a situation in which both A 1 and A 2 come out with an accuracy of 50%, i.e., acc A 1 = acc A 2 =0 . 5 . In this case, AD ( A 1 ,A 2 )=0 and one would thus be tempted to conclude that A 1 and A 2 are similar. Yet, they may actually have drastically different behaviors, as A 1 could be correct on all of the instances that A 2 misclassi fi es, and vice-versa. This mismatch between AD and its intended interpretation is due, of course, to the fact that accuracy is a global measure of performance. Similar discrepancies would arise with any other global measure, such as area under the ROC curve.
Hence, we seek to fi nd a distance function based on more local measures of performance, which take into accountdifferences at the instancelevel rather than acrosswhole datasets. While these measureshave often been used in work on ensemble learning, where diversity is intended to be maximized, they have not, to the best of our knowledge, been used in clustering, where diversity is intended to be minimized, or conversely, similarity maximized. Table 1 summarizes the most popular diversity measures as reported in recent studies. We will show that all but one of these measures, although useful for diversity, fail to qualify as distance functions.

We dismiss the  X  statistic, or measure of interrater agreement, at the onset. Indeed, while it has been used in a few instances as a pairwise measure, it is more often considered a non-pairwise similarity measure in machine learning. Furthermore, there is not a single de fi nition of  X  , even though the probably most reported one originated in [12], which was designed speci fi cally as a non-pairwise measure. Some versions also lead to unexpected answers (e.g., smaller values for seemingly stronger agreement) while others produce indeterminate forms (i.e., 0 0 ) when the learners have diametrically opposed behaviors (e.g., one is always correct and the other is always wrong). We likewise dismiss Pearson X  X  correlation coef fi cient,  X  , since one can show that for any two learners, |  X  | | Q | [13,18], and hence  X  is in some sense subsumed by Q . Finally, the Q statistic, originally de fi ned in [36] as a means to capture the degree of association between two induced hypotheses with respect to the target hypothesis, cannot distinguish among different output distributions. For this reason, H was suggested in [13] as an alternative to Q . Hence, we also ignore Q in what follows.

We now turn to a brief description and discussion of the remaining measures. Since we do not typically have access to actual probab ilitie s, we use frequency-estimates to com pute diversity values. As above, let A 1 and A 2 be the two learning algorithms under study. Let h be the target classi fi cation hypothesis. Let h 1 and h 2 be the hypotheses induced by A 1 , respectively A 2 , on some training set derived from h . We adopt the notation of Table 2, adapted from [17].
 Note that N 00 = N 00  X  Double Fault ( DF ). DF , also known as compound diversity [14], is the proba bility that both h 1  X  Disagreement Measure ( DM ). DM , introduced in [28], is the probability that either h 1 or h 2 is  X  Hamann X  X  coef fi cient ( H ). H , suggested in [13] as an alternative to Q , captures the degree of  X  Error Correlation ( EC ). To the best of our knowledge, EC has been proposed and used only  X  Classi fi er Output Difference ( COD ). COD , introduced in [23], is the probability that h 1 and h 2
In practice, of course, the above measures are computed by running each algorithm on a number of datasets and averaging the results.

Now, recall that a distance measure, d , is a metric (or distance function) if and only if it satis fi es the following four properties (here X and Y are algorithms): 1. Non-negativity: d ( X, Y ) 0 2. Identity of indiscernibles: d ( X, Y )=0  X  X  X  X = Y 2 3. Symmetry: d ( X, Y )= d ( Y, X ) 4. Triangle inequality: d ( X, Y ) d ( X, Z )+ d ( Z, Y )
These properties, especially the second one, bring out a critical distinction between measuring distance as is typically done in ensemble learning and measuring distance as we intend to do in clustering algorithms. In ensemble learning, the focus is on diversity, where one tries to maximize the distance between algorithms, thus focusing on the high-end of the distance spectrum (i.e., away from 0). In clustering, the focus is on similarity, where one tries to minimize the distance between algorithms, thus focusing on the low-end of the distance spectrum (i.e., close to 0). As a result, the identity of indiscernibles, which has to do with close points, may easily be relaxed  X  X nd often is X  in the context of diversity, but is critical in the context of clustering.

It is easy to show that DF does not satisfy the identity of indiscernibles, and thus is not a metric. The same is true of H as shown in Theorem 1.
 Theorem 1. H does not give rise to a distance function.
 Proof 1. We use 1  X  H 2 rather than H because H ranges over [  X  1,1], with  X  1 meaning complete dis-agreement, 0 meaning an equal number of agreements and disagreements, and + 1 meaning complete agreement. The transformed quantity ranges over [0,1] and behaves more like a distance. We show that 2 does not satisfy the identity of indiscernibles in general. which, for binary classi fi cation is equivalent to X = Y since N 00 = N 00 the general case as demonstrated by in the following table, where both the predictions of X and Y ,and the target value are shown.
 i.e., they do not have the same behavior.

It is easy to show that EC 100% accurate, which makes it undesirable. On the other hand, EC function since it does not satisfy the identity of indiscernibles as shown in Theorem 2.
 Theorem 2. EC Proof 2. We use 1  X  EC distance. We show that 1  X  EC counterexample. Consider a simple dataset consisting of 5 instances, each labeled as demonstrated in the following table, where both the predictions of X and Y , and the target value are shown. It is clear that X and Y act exactly identically on each of the instances, i.e., they have the same behavior ( X = Y ). Yet, 1  X  EC a ( X, Y )=1  X  2 / 5=3 / 5 =0 .
 For binary classi fi cation, COD ( X, Y )= DM ( X, Y ) , since in that case N 00 for both algorithms to be wrong and have different predicted values. In the general case, however, when N indiscernibles.
 This leaves us with only COD . We now show that COD is a distance function. 3 Theorem 3. COD is a distance function.
 Proof 3. We need to show that COD satis fi es the four properties of metrics. 1. COD ( X, Y ) 0 2. COD ( X, Y )=0  X  X  X  X = Y 3. COD ( X, Y )= COD ( Y, X ) 4. COD ( X, Y ) COD ( X, Z )+ COD ( Z, Y )
We thus propose to use COD as an effective distance function for algorithm behavior. In addition to being a metric, COD has the following intuitively appealing characteristics.  X  It generalizes DM .  X  It is independent of the target, inasmuch as its computation relies only on the predictions of the two  X  It is rather intuitive as a notion of distance, with some analogy to the classical Hamming distance.  X  Whereas EC , and other measures, are restricted to errors only, COD captures a more complete
Finally, Theorem 4 establishesthat COD is strictly stronger than AD in the sense that if two algorithms are similar according to COD ,then they also have similar predictive performance. The converse is clearly not true as shown by the counterexample discussed earlier in this section to highlight the problem with AD .
 Theorem 4. COD ( X, Y )  X   X  AD ( X, Y )  X  Proof 4. Using our notation, we can write AD = | N 10  X  N 01 | 4. Clustering learning algorithms
Equipped with a distance metric, we can now apply clustering techniques over a set of learning algorithms. Since the data being clustered is the result of learning at the base level, our clustering is a form of unsupervised metalearning. Given a resulting clustering, we will be particularly interested in algorithms that are grouped together into tight clusters, as these will highlight algorithms whose behaviors are very close to each other.

We consider 21 learning algorithms from Weka [34], selected to be representative of various model classes. In all cases, the algorithms are considered with their default settings. The algorithms and their grouping into model classes based on Weka X  X  internal taxonomy are shown in Table 3.

For our distance-based clustering method, we use hierarchical agglomerative clustering (HAC), as it produces a complete sequence of nested clusterings, as follows. HAC starts by assigning each learning algorithm to its own cluster. Then, the two closest clusters are merged into a single new cluster. This pairwise merging process is repeated until a single cluster containing all of the learning algorithms is obtained.
 Although we have a distance de fi ned over algorithms, HAC also needs a distance over clusters. Several distance measures may be considered. The most popular ones are complete linkage, which uses the maximum distance between all pairs of objects across clusters, single linkage, which takes the minimum distance, and average linkage, which computes the average of all inter-cluster distances. We choose complete linkage here as it tends to create more compact, clique-like clusters. 5
To compute the distance between pairs of algorithms, we use 129 datasets from severalpopular sources:  X  72 data sets from the UCI Machine Learning Repository [3]  X  45 data sets from the Gene Expression Machine Learning Repository [31]  X  12 data sets from ASU X  X  Multi-clas s Protein Fold Recognition data 6
For each dataset, the predictions of all instances are obtained by 10-fold cross-validation. The COD value for every pair of algorithms is then obtained by averaging the COD values obtained on all of the datasets. This is shown in Fig. 1, where  X  is Kronecker X  X  delta function:  X  ( a, b )=1 if a = b ,and0 otherwise.
 For our implementation of HAC, we use the agnes function from the cluster package of R [26]. Figure 2 shows the dendrogram resulting from clustering our 21 learning algorithms. Before discussing it, we make a few remarks about how to interpret the dendrogram.

The height of the  X  X rm X  joining two clusters corresponds to the distance between them. As stated above, pairs of clusters are successively merged in increasing value of the distance between them, so that the dendrogram shows a complete nesting of clusterings. Each point along the y -axis may be used to obtain one of these clusterings, by drawing a horizontal line at the corresponding value; the clusters are then given by the algorithms  X  X anging X  from the vertical lines it intersects. For example, a COD value of 0.19 in Fig. 2 gives rise to a clustering of the algorithms into 9 clusters, namely, { BayesNet, DecTable, JRip, SimpleCart } , { LWL } , { FT,Logistic,SMO,MLP } , { NB, RBFN } , { IB1, IB3 } , { NNge } , { J48, PART, LADTree, NBTree, RandForest } , { Ridor } ,and { ZeroR } . Finally, agnes also computes a dimensionless quantity, known as the agglomerative coef fi cient, which is intended to capture the amount of structure extracted from the data. A value close to 0 suggests that the data cannot be split in natural to organize the data into well-separated clusters.

A detailed analysis of all of the implications of our clustering is beyond the scope of this paper. For example, we cannot offer a complete analysis of some of the interesting relationships it uncovers (e.g., see NB and RBFN below). Many of these give rise to complete papers in their own right. We are, however, highlighting some of these fi ndings, pointing to ongoing work, and making a few pertinent observations about the clustering, to illustrate how it may be used to con fi rm, complement and/or extend our knowledge about learning algorithms.

First we note that the agglomerative coef fi cient value of 0.61 suggests that HAC with COD is able to extract a signi fi cant amount of structure in the data. As one might expect, much of Weka X  X  taxonomy (see Table 3) is found in the natural clustering. For example, three of the four function-based learning algorithms (i.e., Logistic, MLP and SMO) land in the same cluster relatively low in the dendrogram, with the fourth one, RBFN, joining a little higher. Similarly, most decision tree learning algorithms tend to cluster together fi rst, as do both nearest-neighbor algorithms (i.e., IB1 and IB3). On the other hand, other clusters do not match the taxonomy so well. For example,  X  The rule-based learning algorithm PART clusters fi rst with the decision tree learner J48,  X  The decision tree learning algorithm FT clusters fi rst with the group of three function-based learners,  X  The rule-based learning algorithm ZeroR is clustered last, high in the dendrogram, and  X  The function-based learner RBFN clusters fi rst with the Bayes learner NB.

Despite being in different classes, PART and J48 do share signi fi cant similarity since PART also uses a divide-and-conquer approach in which, at each iteration, it builds a partial tree and extracts a rule from it. Similarly, FT bears resemblance to Logistic as it induces classi fi cation trees with logistic regression functions at the inner nodes and leaves. Also, ZeroR, despite being labeled as rule-based, simply extracts the majority class and uses it as its prediction on all new instances. Such simplistic behavior is unlikely to match any of the more sophisticated learning algorithms considered.

The last grouping seems less obvious at fi rst sight. Upon further examination of their inner workings and speci fi c Weka default implementations, one may better appreciate the similarities and differences between these two learning algorithms. This is the subject of a separate analysis [21]. However, we note that such an analysis was prompted by the results of the present clustering. In addition to NB being probability-based and RBFN being function-based, NB is a generative model while RBFN is a discriminative one. Yet, overall, RBFN behaves more like NB than any other algorithm. It is rather unlikely that traditional perceptions about NB and RBFN would have led to such a discovery and subsequent analysis.

To make the value of our COD metric for unsupervised metalearning clearer, we contrast the COD -based clustering with the clustering obtained by AD , the difference in predic tive accuracy . The se tting is the same as above, with AD replacing COD throughout. The resulting cluster ing is in Fig. 3.
The agglomerative coef fi cient value of 0.79 suggests that HAC with AD is also able to extract a signi fi cant amount of structure in the data. Note, however, that the dendrogram tends to be rather fl at, except for ZeroR, which, as expected, clusters last and higher up in the dendrogram. In fact, there is less than about 10% difference in average accuracy among the selected learning algorithms. This con fi rms the fi ndings of others that most learning algorithms, including simple ones, perform rather similarly in terms of accuracy across a wide variety of datasets (e.g., see [15]).

One of the challenges of research in data clustering is the assessment of the quality of the results. Two types of quality measures are available, namely external and internal measures. External measures are by far the most commonly used, since they are well understood and easy to compute. However, they rely on the availability of some ground trut h specifying what data point should belong to what cluster. There is clearly no such ground truth here, which leaves us with internal measures. Unfortunately, much less work has been done in this area, and it has even been argued that  X  X ood scores on an internal criterion do not necessarily translate into good effectiveness in an application. X  7 We thus focus our remarks on a more qualitative analysis and comparison, motivated by intended applications.

There are two natural areas where the clustering resulting from our unsupervised metalearning can be useful, namely ensemble learning 8 and algorithm selection. Given a learning task, ensemble learning consists of combining several learning algorithms in hopes that the combined system is more accurate than any of its constituents alone. Algorithm selection, on the other hand, is concerned with picking the algorithm most likely to produce the highest accuracy on the given learning task from a set of candidate algorithms.

The success of ensemble learning depends in large measure upon the amount of diversity among the algorithms it combines. In other words, one should choose to put together algorithms that belong to different clusters. Here, the distinction between the two clusterings is obvious. Indeed, the picture provided by AD is different from that provided by COD , and variations in algorithm behavior are not captured in the aggregate by AD . For example, the following pairs of algorithms, RandForest and MLP, Ridor and SimpleCart, and JRip and Ridor, are close together in the AD clustering, while the COD clustering makes clear that their behavior is not that similar. As a result, one may omit to include in the ensemble, algorithms that could improve it. Experienced machine learners would, of course, argue that they never use AD as a selection criterion for ensemble building, but rather an instance-level diversity measure, such as COD . If so, our COD -based clustering still provides them with a global view of the diversity among a large number of algorithms and a simple mechanism to build more effective ensembles.
An approach to algorithm selection, that has received increased interest in the last decade, is metalearn-ing, where one uses machine learning to build a model that maps learning tasks to learning algorithms. Since the number of learning algorithms available is large relative to the number of learning tasks (e.g., here we have 21 algorithms for 129 datasets), the ratio of instances to target values in the metalearning task is rather low, making accurate learning dif fi cult. Suppose now that some of these learning algorithms exhibit similar behavior, then trying to discriminate among them would be unnecessary and even unde-sirable during metalearning. Instead, one could take all of the instances whose target value is one of the algorithms in a group of similarly-behaved algorithms and relabel them with a single value corresponding for that group. In other words, the metalearner no longer maps learning tasks to learning algorithms but to groups of similarly-behaved learning algorithms. Hence, the metalearning task is greatly simpli fi ed and performance is not severely hindered, as choosing any algorithm from the predicted cluster would produce an accuracy not much different from that gotten by choosing another one. Clearly, the success of such an approach hinges on a relevant, high-quality clustering of learning algorithms. Again, as for the case of ensemble learning, COD -based clustering is more appropriate than AD -based clustering. According to Theorem 4, algorithms close to each other according to COD are also close together according to AD (e.g., see NB and RBFN). The converse is not true, however. Again, our COD -based clustering provides a natural means of designing a cluster selection system. 5. Conclusion
In this paper, we have argued the value of unsupervised metalearning and discussed the attendant necessity of suitable similarity, or distance, functions. We have capitalized on the well-known notion of diversity among learners used in ensemble learning, and proposed to use them as distance measures to cluster learning algorithms. We have revisited the most popular measures of diversity and showed that only one of them, Classi fi er Output Difference ( COD ) quali fi es as a metric. We have then used COD to produce a clustering of 21 learning algorithms, based on results from 129 datasets. We have shown how this clustering differs from a clustering based on accuracy, and how it can be used to highlight interesting, sometimes unexpected, similarities among algorithms.

Unsupervised metalearning, as described here, contributes to increasing our understanding of how learning algorithms behave. By focusing its attention on similarity, rather than diversity, it might also prove useful in reducing the complexity of the corresponding supervised metalearning task of algorithm selection. Indeed, rather than attempting to build a system that maps a dataset to one of N algorithms, where N is relatively large, the metalearner could induce a mapping from a dataset to one of C clusters of similar algorithms, where C is much smaller than N . Work in this area in ongoing [20]. References
