 University of Southern California University of Southern California way which is predictive of statistical machine translation performance. 1. Introduction machine translation (SMT) approaches. There were a number of research papers pre-sented from 2000 to 2005 at ACL, NAACL, HLT, COLING, WPT03, WPT05, and so forth, outlining techniques for attempting to increase word alignment quality. Despite this high level of interest, none of these techniques has been shown to result in a large other metric. We find this lack of correlation between previous word alignment quality metrics and BLEU counterintuitive, because we and other researchers have measured this correlation in the context of building SMT systems that have benefited from using the BLEU metric in improving performance in open evaluations such as the NIST evaluations. 1 develop a methodology for measuring alignment quality that is predictive of BLEU. We also show that alignment error rate (AER) is not correctly derived from F-Measure and is therefore unlikely to be useful as a metric. 2. Experimental Methodology 2.1 Data
To build an SMT system we require a bitext and a word alignment of that bitext, as well as language models built from target language data. In all of our experiments, we will hold the bitext and target language resources constant, and only vary how we construct the word alignment.
 using links between words showing translational correspondence. Links which must be present in a hypothesized alignment are called Sure links. Some of the alignment sets also have links which are not Sure links but are Possible links (Och and Ney 2003). Possible links which are not Sure 2 may be present but need not be present. out translation test set and measuring the BLEU score of our hypothesized translations against one or more reference translations. We also have an additional held-out transla-tion set, the development set, which is employed by the MT system to train the weights of its log-linear model to maximize BLEU (Och 2003). We work with data sets for three different language pairs, examining French to English, Arabic to English, and Romanian to English translation tasks.

Hansard data set, from which the word aligned data (presented in Och and Ney 2003) was also taken. The English side of the bitext is 67.4 million words. We used a separate
Canadian Hansard data set (released by ISI) as the source of the translation test set and development set. We evaluate two different tasks using this data, a medium task where 1/8 of the data (8.4 million English words) is used as the fixed bitext, and a large task where all of the data is used as the fixed bitext. The 484 sentences in the gold standard word alignments have 4,376 Sure links and 19,222 Possible links.
 translation evaluation. 3 The English side of the bitext is 99.3 million words. The trans-lation development set is the  X  X IST 2002 Dry Run, X  and the test set is the  X  X IST 2003 evaluation set. X  We have annotated gold standard alignments for 100 parallel sentences using Sure links, following the Blinker guidelines (Melamed 1998), which call for Sure links only (there were 2,154 Sure links). Here we also examine a medium task using 1/8 of the data (12.4 million English words) and a large task using all of the data. alignment at WPT03 (Mihalcea and Pederson 2003) and WPT05 (Martin, Mihalcea, translation development and test sets. The English side of the training corpus is 964,000 (there were 3,181 Sure links). 294 2.2 Measuring Translation Performance Changes Caused By Alignment
In phrased-based SMT (Koehn, Och, and Marcu 2003) the knowledge sources which vary with the word alignment are the phrase translation lexicon (which maps source phrases to target phrases using counts from the word alignment) and some of the word level translation parameters (sometimes called lexical smoothing). However, many knowledge sources do not vary with the final word alignment, such as rescoring with
IBM Model 1, n -gram language models, and the length penalty. In our experiments, we use a state-of-the-art phrase-based system, similar to Koehn, Och, and Marcu.
The weights of the different knowledge sources in the log-linear model used by our system are trained using Maximum BLEU (Och 2003), which we run for 25 iterations individually for each system. Two language models are used, one built using the target language training data and the other built using additional news data. 2.3 Generating Alignments of Varying Quality
We have observed in the past that generative models used for statistical word alignment create alignments of increasing quality as they are exposed to more data. The intuition behind this is simple; as more co-occurrences of source and target words are observed, the word alignments are better. If we wish to increase the quality of a word alignment, alignment, we divide the bitext into pieces and align the pieces independently of one another, finally concatenating the results together.
 plements both the IBM Models of Brown et al. (1993) and the HMM model (Vogel,
Ney, and Tillmann 1996). We use Model 1, HMM, and Model 4, in that order. The output of these models is an alignment of the bitext which projects one language to another. GIZA++ is run end-to-end twice. In one case we project the source language to the target language, and in the other we project the target language to the source language. The output of GIZA++ is then post-processed using the three  X  X ymmetriza-tion heuristics X  described in Och and Ney (2003). We evaluate our approaches using these heuristics because we would like to account for alignments generated in different fashions. These three heuristics were used as the baselines in virtually all recent work on automatic word alignment, and most of the best SMT systems use these techniques as well.
 of the bipartite graph created, which results in fully connected components indicating translational correspondence. 4 Each of the presented alignments are equivalent from a translational correspondence perspective and the first two will be mapped to the third in order to ensure consistency between the number of links an alignment has and the translational equivalences licensed by that alignment.
 3. Word Alignment Quality Metrics 3.1 Alignment Error Rate is Not a Useful Measure
We begin our study of metrics for word alignment quality by testing AER (Och and Ney 2003). AER requires a gold standard manually annotated set of Sure links and Possible links (referred to as S and P ). Given a hypothesized alignment consisting of the link set A , three measures are defined: set. We broke the data into separate pieces corresponding to 1/16, 1/8, 1/4, and 1/2 of the original data to generate degraded alignments, and we used 2, 4, and 8 times the original data to generate enhanced alignments. For the  X  X ractional X  alignments we report the average AER of the pieces. 5 would look like a line from the bottom left corner to the top right corner. As can be seen by looking at the graph, there is low correlation between 1 concise mathematical description of correlation is the coefficient of determination ( r 296 r = 0 . 16, which is low.
 formulation of AER, which to our knowledge has not been previously reported. Och and Ney (2003) state that AER is derived from F-Measure. But AER does not share a very important property of F-Measure, which is that unbalanced precision and recall are penalized, where S  X  P (i.e., when we make the Sure versus Possible distinction). We will show this using an example.

Ney X  X  Precision and Recall formulae together with the standard F-Measure formula (van Rijsbergen 1979). In the F-Measure formula (4) there is a parameter  X  which sets the trade-off between Precision and Recall. When an equal trade-off is desired,  X  is set to 0 . 5.
 alignment links, is the same, for instance, | A | = 100. Let |
P |
P  X  A | = 75 and | S  X  A | = 25. Precision is 0 . 75 and Recall is 0 . 25.
 precision and recall should be penalized. The first hypothesized alignment has an
F-Measure with Sure and Possible score of 0 . 50, whereas the second has a worse score, 0 . 375.
 tion (3)), we see that 1  X  AER for both of the hypothesized alignments is 0 . 5. Therefore
AER does not share the property of F-Measure (with  X  = 0 . 5) that unbalanced precision and recall are penalized. Because of this, it is possible to maximize AER by favoring precision over recall, which can be done by simply guessing very few alignment links.
Unfortunately, when S  X  P , this leads to strong biases, which makes AER not useful as a metric. fairly optimized by using a bias toward precision which was unlikely to improve the usefulness of the alignments. Possible problems with AER were discussed at WPT 2003 and WPT 2005.
 some predictive power for the data points generated using a single heuristic, but the overall correlation is still low, r 2 = 0 . 20. We need a measure that predicts BLEU without having a dependency on the way the alignments are generated. 3.2 Balanced F-Measure is Better, but Still Inadequate
We wondered whether the low correlation was caused by the Sure and Possible dis-tinction. We reannotated the first 110 sentences of the French test set using the Blinker guidelines (there were 2,292 Sure links). We define F-Measure without the Sure versus results. Correlation is higher: r 2 = 0 . 67.
 3.3 Varying the Trade-Off Between Precision and Recall Works Well
We then hypothesized that the trade-off between precision and recall is important. This results are:  X  = 0 . 1 for the original annotation annotated with Sure and Possible (see
Figure 4), and  X  = 0 . 4 for the first 110 sentences as annotated by us (see Figure 5). relevant r 2 scores were 0 . 80 and 0 . 85, respectively. With a good  X  setting, we are able 298 to predict the machine translation results reasonably well. For the original annotation, recall is very highly weighted, whereas for our annotation, recall is still more important than precision. 8 Our results also suggest that better correlation will be achieved when using Sure-only annotation than with Sure and Possible annotation. translation performance for this set.
 we have no additional data. For the large French/English corpus the best results are 300 annotation of 110 sentences with only Sure links (see Figure 7). Relevant r 0 . 62 and 0 . 64, respectively. Disappointingly, our measures are not able to fully explain MT performance for the large French/English task.
 was at  X  = 0 . 1, for which r 2 = 0 . 90 (see Figure 8). We can predict MT performance for factors that affect the performance of our MT system, which only appear in conjunction with the large French/English task.
 alignment task from the Workshop on Parallel Text at ACL 2005 (Martin, Mihalcea, and
Pedersen 2005). We only decreased alignment quality and used 5 data points for each symmetrization heuristic due to the small bitext. The best setting of  X  was  X  = 0 . 2, for which r 2 = 0 . 94, showing that F-Measure is again effective in predicting BLEU. 4. Conclusion
We have presented an empirical study of the use of simple evaluation metrics based on gold standard alignment of a small number of sentences to predict machine translation performance. Based on our experiments we can now draw the following conclusions: 1. When S  X  P , AER does not share the important property of F-Measure 2. Good correlation was obtained for the medium French and Arabic data 3. We have only partially explained the effect of alignment quality on BLEU 4. We recommend using the Blinker guidelines as a starting point for new machine translation and for other tasks. For an evaluation campaign the organizers should pick a specific task, such as improving phrasal SMT, and calculate an appropriate  X  to be used. Individual researchers working on the same phrasal SMT tasks as those reported here (or on very similar tasks) could use the values of  X  we calculated. presented only evaluations based on metrics like AER or balanced F-Measure, and explains the lack of correlation in the few works which presented both such a metric and final MT results. A good example of the former are our own results (Fraser and
Marcu 2005). The work presented there had the highest balanced F-Measure scores for the Romanian/English WPT05 shared task, but based on the findings here it is possible performance. Other work includes many papers working on alignment models where words are allowed to participate in a maximum of one link. These models generally have higher precision and lower recall than IBM Model 4 symmetrized using the  X  X e-fined X  or  X  X nion X  heuristics. Recall that in Section 3.1 we showed that AER is broken in a way that favors precision. It is therefore likely that the results reported in these papers are affected by the AER bias and that the corresponding improvements in AER score do not correlate with increases in phrasal SMT performance.
 identified final task such as machine translation. F-Measure with an appropriate setting of  X  will be useful during the development process of new alignment models, or as a maximization criterion for discriminative training of alignment models (Cherry and Lin 2003; Ayan, Dorr, and Monz 2005; Ittycheriah and Roukos 2005; Liu, Liu, and Lin 2005; Fraser and Marcu 2006; Lacoste-Julien et al. 2006; Moore, Yih, and Bode 2006). Acknowledgments References 302
