 Ron Appel appel@caltech.edu Caltech, Pasadena, CA 91125 USA Thomas Fuchs fuchs@caltech.edu Caltech, Pasadena, CA 91125 USA Piotr Doll  X ar pdollar@microsoft.com Microsoft Research, Redmond, WA 98052 USA Pietro Perona perona@caltech.edu Caltech, Pasadena, CA 91125 USA Boosting is one of the most popular learning tech-niques in use today, combining many weak learners to form a single strong one (Schapire, 1990; Freund, 1995; Freund &amp; Schapire, 1996). Shallow decision trees are commonly used as weak learners due to their simplicity and robustness in practice (Quinlan, 1996; Breiman, 1998; Ridgeway, 1999; Kotsiantis, 2007). This power-ful combination (of Boosting and decision trees) is the learning backbone behind many state-of-the-art meth-ods across a variety of domains such as computer vi-sion, behavior analysis, and document ranking to name a few (Doll  X ar et al., 2012; Burgos-Artizzu et al., 2012; Asadi &amp; Lin, 2013), with the added benefit of exhibit-ing fast speed at test time.
 Learning speed is important as well. In active or real-time learning situations such as for human-in-the-loop processes or when dealing with data streams, classifiers must learn quickly to be practical. This is our motiva-tion: fast training without sacrificing accuracy. To this end, we propose a principled approach. Our method offers a speedup of an order of magnitude over prior approaches while maintaining identical performance. The contributions of our work are the following: 1. Given the performance on a subset of data, we 2. Based on this bound, we propose an algorithm 3. We outline an algorithm for quickly Boosting de-In the following sections, we discuss related work, inspect the tree-boosting process, describe our algo-rithm, prove our bound, and conclude with experi-ments on several datasets, demonstrating our gains. Many variants of Boosting (Freund &amp; Schapire, 1996) have proven to be competitive in terms of prediction accuracy in a variety of applications (B  X uhlmann &amp; Hothorn, 2007), however, the slow training speed of boosted trees remains a practical drawback. Accord-ingly, a large body of literature is devoted to speeding up Boosting, mostly categorizable as: methods that subsample features or data points, and methods that speed up training of the trees themselves.
 In many situations, groups of features are highly cor-related. By carefully choosing exemplars, an entire set of features can be pruned based on the performance of its exemplar. (Doll  X ar et al., 2007) propose cluster-ing features based on their performances in previous stages of boosting. (K  X egl &amp; Busa-Fekete, 2009) parti-tion features into many subsets, deciding which ones to inspect at each stage using adversarial multi-armed bandits. (Paul et al., 2009) use random projections to reduce the dimensionality of the data, in essence merging correlated features.
 Other approaches subsample the data. In Weight-trimming (Friedman, 2000), all samples with weights smaller than a certain threshold are ignored. With Stochastic Boosting (Friedman, 2002), each weak learner is trained on a random subset of the data. For very large datasets or in the case of on-line learning, elaborate sampling methods have been proposed, e.g. Hoeffding trees (Domingos &amp; Hulten, 2000) and Fil-ter Boost (Domingo &amp; Watanabe, 2000; Bradley &amp; Schapire, 2007). To this end, probabilistic bounds can be computed on the error rates given the number of samples used (Mnih et al., 2008). More recently, Lam-inating (Dubout &amp; Fleuret, 2011) trades off number of features for number of samples considered as training progresses, enabling constant-time Boosting.
 Although all these methods can be made to work in practice, they provide no performance guarantees. A third line of work focuses on speeding up training of decision trees. Building upon the C4.5 tree-training algorithm of (Quinlan, 1993), using shallow (depth-D ) trees and quantizing features values into B N bins leads to an efficient O ( D  X  K  X  N ) implementation where K is the number of features, and N is the number of samples (Wu et al., 2008; Sharp, 2008).
 Orthogonal to all of the above methods is the use of parallelization; multiple cores or GPUs. Recently, (Svore &amp; Burges, 2011) distributed computation over cluster nodes for the application of ranking; however, reporting lower accuracies as a result. GPU imple-mentations of GentleBoost exist for object detection (Coates et al., 2009) and for medical imaging using Probabilistic Boosting Trees (PBT) (Birkbeck et al., 2011). Although these methods offer speedups in their own right, we focus on the single-core paradigm. Regardless of the subsampling heuristic used, once a subset of features or data points is obtained, weak learners are trained on that subset in its entirety. Con-sequently, each of the aforementioned strategies can be viewed as a two-stage process; in the first, a smaller set of features or data points is collected, and in the second, decision trees are trained on that entire subset. We propose a method for speeding up this second stage; thus, our approach can be used in conjunc-tion with all the prior work mentioned above for even greater speedup. Unlike the aforementioned methods, our approach provides a performance guarantee: the boosted tree offers identical performance to one with classical training. A boosted classifier (or regressor) having the form H ( x ) = P t  X  t h t ( x ) can be trained by greedily min-imizing a loss function L ; i.e. by optimizing scalar  X  t and weak learner h t ( x ) at each iteration t . Be-fore training begins, each data sample x i is assigned a non-negative weight w i (which is derived from L ). Af-ter each iteration, misclassified samples are weighted more heavily thereby increasing the severity of mis-classifying them in following iterations. Regardless of the type of Boosting used (i.e. AdaBoost, LogitBoost, L
Boost, etc.), each iteration requires training a new weak learner given the sample weights. We focus on the case when the weak learners are shallow trees. 3.1. Training Decision Trees A decision tree h TREE ( x ) is composed of a stump h j ( x ) at every non-leaf node j . Trees are commonly grown using a greedy procedure as described in (Breiman et al., 1984; Quinlan, 1993), recursively setting one stump at a time, starting at the root and working through to the lower nodes. Each stump produces a binary decision; it is given an input x  X  R K , and is parametrized with a polarity p  X  { X  1 } , a threshold  X   X  R , and a feature index k  X  X  1 , 2 ,...,K } : We note that even in the multi-class case, stumps are trained in a similar fashion, with the binary decision discriminating between subsets of the classes. How-ever, this is transparent to the training routine; thus, we only discuss binary stumps.
 In the context of classification, the goal in each stage of stump training is to find the optimal parameters that minimize  X  , the weighted classification error: In this paper, we focus on classification error; however, other types of split criteria (i.e. information gain, Gini impurity, or variance) can be derived in a similar form; please see supplementary materials for details. For binary stumps, we can rewrite the error as: In practice, this error is minimized by selecting the single best feature k  X  from all of the features: In current implementations of Boosting (Sharp, 2008), feature values can first be quantized into B bins by linearly distributing them in [1 ,B ] (outer bins corre-sponding to the min/max, or to a fixed number of standard deviations from the mean), or by any other quantization method. Not surprisingly, using too few bins reduces threshold precision and hence overall per-formance. We find that B = 256 is large enough to incur no loss in practice.
 Determining the optimal threshold  X   X  requires accu-mulating each sample X  X  weight into discrete bins cor-responding to that sample X  X  feature value x i [ k ]. This procedure turns out to still be quite costly: for each of the K features, the weight of each of the N samples has to be added to a bin, making the stump training operation O ( K  X  N )  X  the very bottleneck of training boosted decision trees.
 In the following sections, we examine the stump-training process in greater detail and develop an in-tuition for how we can reduce computational costs. 3.2. Progressively Increasing Subsets Let us assume that at the start of each Boosting itera-tion, the data samples are sorted in order of decreasing weight, i.e: w i  X  w j  X  i &lt; j . Consequently, we define Z m , the mass of the heaviest subset of m data points: Clearly, Z m is greater or equal to the sum of any m other sample weights. As we increase m , the m -subset includes more samples, and accordingly, its mass Z m increases (although at a diminishing rate).
 In Figure 1, we plot Z m /Z for progressively increas-ing m -subsets, averaged over multiple iterations. An interesting empirical observation can be made about Boosting: a large fraction of the overall weight is ac-counted for by just a few samples. Since training time is dependent on the number of samples and not on their cumulative weight, we should be able to leverage this fact and train using only a subset of the data. The more non-uniform the weight distribution, the more we can leverage; Boosting is an ideal situation where few samples are responsible for most of the weight. The same observation was made by (Friedman et al., 2000), giving rise to the idea of weight-trimming , which proceeds as follows. At the start of each Boosting it-eration, samples are sorted in order of weight (from largest to smallest). For that iteration, only the sam-ples belonging to the smallest m -subset such that Z m /Z  X   X  are used for training (where  X  is some prede-fined threshold); all the other samples are temporarily ignored  X  or trimmed . Friedman et al. claimed this to  X  X ramatically reduce computation for boosted models without sacrificing accuracy. X  In particular, they pre-scribed  X  = 90% to 99%  X  X ypically X  , but they left open the question of how to choose  X  from the statistics of the data (Friedman et al., 2000).
 Their work raises an interesting question: Is there a principled way to determine (and train on only) the smallest subset after which including more samples does not alter the final stump? Indeed, we can prove that a relatively small m -subset contains enough in-formation to set the optimal stump, and thereby save a lot of computation.
 3.3. Preliminary Errors Definition: given a feature k and an m -subset, the best preliminary error  X  ( k ) m is the lowest achievable training error if only the data points in that subset were considered. Equivalently, this is the reported er-ror if all samples not in that m -subset are trimmed . The emphasis on preliminary indicates that the subset does not contain all data points, i.e: m &lt; N , and the emphasis on best indicates that no choice of polarity p or threshold  X  can lead to a lower preliminary error using that feature.
 As previously described, given a feature k , a stump is trained by accumulating sample weights into bins. We can view this as a progression; initially, only a few samples are binned, and as training continues, more and more samples are accounted for; hence,  X  ( k ) m may be computed incrementally which is evident in the smoothness of the traces. Figure 2 shows the best preliminary error for each of the features in a typical experiment. By examining Figure 2, we can make four observations: 1. Most of the overall weight is accounted for by the 2. Feature errors increasingly stabilize as m  X  N 3. The best performing features at smaller m -subsets 4. The optimal feature is among the best features for From the full training run shown in Figure 2, we note (in retrospect) that using an m -subset with m  X  0 . 2 N would suffice in determining the optimal feature. But how can we know a priori which m is good enough? Averaging over many training iterations, in Figure 3, we plot the probability that the optimal feature is among the top-performing features when trained on only an m -subset of the data. This gives us an idea as to how small m can be while still correctly predicting the optimal feature.
 From Figure 3, we see that the optimal feature is not amongst the top performing features until a large enough m -subset is used  X  in this case, Z m /Z  X  75%. Although  X  X ptimality X  is not quite appropriate to use in the context of greedy stage-wise procedures (such as boosted trees), consistently choosing sub-optimal pa-rameters at each stump empirically leads to substan-tially poorer performance, and should be avoided. In the following section, we outline our approach which determines the optimal stump parameters using the smallest possible m -subset.
 Figure 3 suggests that the optimal feature can often be estimated at a fraction of the computational cost using the following heuristic:
Faulty Stump Training 1. Train each feature only using samples in the m -2. Prune all but the 10 best performing features. 3. For each of un-pruned feature, complete training 4. Finally, report the best performing feature (and This heuristic does not guarantee to return the opti-mal feature, since premature pruning can occur in step 2. However, if we were somehow able to bound the er-ror, we would be able to prune features that would provably underachieve (i.e. would no longer have any chance of being optimal in the end).
 Definition: a feature k is denoted underachieving if it is guaranteed to perform worse than the best-so-far feature k 0 on the entire training data.
 Proposition 1: for a feature k , the following bound holds (proof given in Section 4.2): given two subsets (where one is larger than the other), the product of subset mass and preliminary error is always greater for the larger subset: Let us assume that the best-so-far error  X  0 has been determined over a few of the features (and the param-eters that led to this error have been stored). Hence, this is an upper-bound for the error of the stump cur-rently being trained. For the next feature in the queue, even after a smaller ( m &lt; N )-subset, then: Therefore, if: Z m  X  ( k ) m  X  Z X  0 then feature k is under-achieving and can safely be pruned. Note that the lower the best-so-far error  X  0 , the harsher the bound; consequently, it is desirable to train a relatively low-error feature early on.
 Accordingly, we propose a new method based on com-paring feature performance on subsets of data, and consequently, pruning underachieving features:
Quick Stump Training 1. Train each feature only using data in a relatively 2. Sort the features based on their preliminary er-3. Continue training one feature at a time on pro-4. Finally, report the best performing feature (and 4.1. Subset Scheduling Deciding which schedule of m -subsets to use is a sub-tlety that requires further explanation. Although this choice does not effect the optimality of the trained stump, it may effect speedup. If the first  X  X elatively small X  m -subset (as prescribed in step 1) is too small, we may lose out on low-error features leading to less-harsh pruning. If it is too large, we may be doing unnecessary computation. Furthermore, since the cal-culation of preliminary error does incur some (albeit, low) computational cost, it is impractical to use every m when training on progressively larger subsets. To address this issue, we implement a simple sched-ule: The first m -subset is determined by the param-eter  X  kp such that Z m /Z  X   X  kp . M following subsets are equally spaced out between  X  kp and 1. Figure 4 shows a parameter sweep over  X  kp and M , from which we fix  X  kp = 90% and M = 20 and use this setting for all of our experiments. Using our quick stump training procedure, we deter-mine the optimal parameters without having to con-sider every sample for each feature. By pruning un-derachieving features, a lot of computation is saved. We now outline the full Boosting procedure using our quick training method:
Quickly Boosting Decision Trees 1. Initialize weights (sorted in decreasing order) . 2. Train decision tree h t (one node at a time) using 3. Perform standard Boosting steps: We note that sorting the weights in step 3(c) above is an O ( N ) operation. Given an initially sorted set, Boosting updates the sample weights based on whether the samples were correctly classified or not. All cor-rectly classified samples are weighted down, but they maintain their respective ordering. Similarly, all mis-classified samples are weighted up, also maintaining their respective ordering. Finally, these two sorted lists are merged in O ( N ).
 We now give a proof for the bound that our method is based on, and in the following section, we demonstrate its effectiveness in practice. 4.2. Proof of Proposition 1 As previously defined,  X  ( k ) m is the preliminary weighted classification error computed using the feature k on samples in the m -subset; hence: Proof:  X  ( k ) m is the best achievable preliminary error on the m -subset (and correspondingly, { p ( k ) m , X  ( k ) the best preliminary parameters); therefore:
Z Hence, switching the optimal parameters { p ( k ) m , X  for potentially sub-optimal ones { p ( k ) n , X  ( k ) n } (note the subtle change in indices): Further, by summing over a larger subset ( n  X  m ), the resulting sum can only increase: But the right-hand side is equivalent to Z n  X  ( k ) n ; hence: For similar proofs using information gain, Gini impu-rity, or variance minimization as split criteria, please see the supplementary material. In the previous section, we proposed an efficient stump training algorithm and showed that it has a lower expected computational cost than the traditional method. In this section, we describe experiments that are designed to assess whether the method is practical and whether it delivers significant training speedup. We train and test on three real-world datasets and empirically compare the speedups. 5.1. Datasets We trained Ada-Boosted ensembles of shallow decision trees of various depths, on the following three datasets: 1. CMU-MIT Faces dataset (Rowley et al., 1996); 2. INRIA Pedestrian dataset (Dalal &amp; Triggs, 2005); 3. MNIST Digits (LeCun &amp; Cortes, 1998); 6 . 0  X  10 4 5.2. Comparisons Quick Boosting can be used in conjunction with all previously mentioned heuristics to provide further gains in training speed. We report all computational costs in units proportional to Flops, since running time (in seconds) is dependent on compiler optimizations which are beyond the scope of this work.
 In Figure 5, we plot the computation cost versus train-ing loss and versus test error. We compare vanilla (no heuristics) AdaBoost, Weight-Trimming with  X  = 90% and 99% [see Section 3.2], LazyBoost 90% and 50% (only 90% or 50% randomly selected features are used to train each weak learner), and StochasticBoost (only a 50% random subset of the samples are used to train each weak learner). To these six heuristics, we apply our method to produce six  X  X uick X  versions.
 We further note that our goal in these experiments is not to tweak and enhance the performance of the clas-sifiers, but to compare the performance of the heuris-tics with and without our proposed method.
 5.3. Results and Discussion From Figure 5, we make several observations.  X  X uick X  versions require less computational costs (and produce identical classifiers) as their slow counterparts. From speed-up offered by our method; often around an or-der of magnitude. Quick-LazyBoost-50% and Quick-StochasticBoost-50% are the least computationally-intensive heuristics, and vanilla AdaBoost always achieves the smallest training loss and attains the low-est test error in two of the three datasets.
 The motivation behind this work was to speed up training such that: (i) for a fixed computational bud-get, the best possible classifier could be trained, and (ii) given a desired performance, a classifier could be trained with the least computational cost.
 For each dataset, we find the lowest-cost heuristic and set that computational cost as our budget. We then boost as many weak learners as our budget permits for each of the heuristics (with and without our method) and compare the test errors achieved, plotting the rela-tive gains in Figure 6. For most of the heuristics, there is a two to eight-fold reduction in test error, whereas for weight-trimming, we see less of a benefit. In fact, for the second dataset, Weight-Trimming-90% runs at the same cost with and without our speedup.
 Conversely, in Figure 7, we compare how much less computation is required to achieve the best test er-ror rate by using our method for each heuristic. Most heuristics see an eight to sixteen-fold reduction in com-putational cost, whereas for weight-trimming, there is still a speedup, albeit again, a much smaller one (be-tween one and two-fold).
 As discussed in Section 3.2, weight-trimming acts sim-ilarly to our proposed method in that it prunes fea-tures, although it does so naively -without adhering to a provable bound. This results in a speed-up (at times almost equivalent to our own), but also leads to classifiers that do not perform as well as those trained using the other heuristics. We presented a principled approach for speeding up training of boosted decision trees. Our approach is built on a novel bound on classification or regression error, guaranteeing that gains in speed do not come at a loss in classification performance.
 Experiments show that our method is able to reduce training cost by an order of magnitude or more, or given a computational budget, is able to train classi-fiers that reduce errors on average by two-fold or more. Our ideas may be applied concurrently with other techniques for speeding up Boosting (e.g. subsampling of large datasets) and do not limit the generality of the method, enabling the use of Boosting in applications where fast training of classifiers is key.
 This work was supported by NSERC 420456-2012, MURI-ONR N00014-10-1-0933, ARO/JPL-NASA Stennis NAS7.03001, and Moore Foundation.
