 Frequent itemset mining (FIM) is a core component in many data analysis tasks such as association analysis [1] and sequential-pattern mining [2]. Traditionally, FIM is applied to data that is certain and precise. As an example, a transaction in a market-basket dataset registers items that are purchased by a customer. Applying FIM on such a dataset allows one to identify items that are often purchased together. In this example, the presence/absence of an item in a trans-action is known with certainty. Existing FIM algorithms, such as the well-known Apriori algorithm [1] and other variants, were designed for mining  X  X ertain X  data.
Most of the previous studies on FIM assume a data model under which trans-actions capture doubtless facts about the items that are contained in each trans-action. However, in many applications, the existence of an item in a transaction is best captured by a probability. As an example, consider experiments that test certain drug-resistant properties of pathogens. Results of such tests can be represented by a transactional dataset: each pathogen is represented by a trans-action and the drugs it shows resistance to are listed as items in the transaction. Applying FIM on such a dataset allows us to discover multi-drug-resistant asso-ciations [3]. In practice, due to measurement and experimental errors, multiple measurements or experiments are conducted to obtain a higher confidence of the results. In such cases, the existence of an item or property in a transaction should be expressed in terms of a probability. For example, if Streptococcus Pneumo-niae (a pathogen) shows resistance to penicillin (an antibiotics drug) 90 times out of 100 experiments, the probability that the property  X  X enicillin-resistant X  exists in Streptococcus Pneumoniae is 90%. We call this kind of probability ex-istential probability . In this paper we study the problem of applying FIM on datasets under the existential uncertain data model , in which each item is asso-ciated with an existential probability that indicates the likelihood of its presence in a transaction. Table 1 shows an example of an existential uncertain dataset.
The problem of mining frequent itemsets under the existential uncertain data model was first studied in [4]. The Apriori algorithm was modified to mine uncer-tain data. The modified algorithm, called U-Apriori, was shown to be computa-tionally inefficient. A data trimming framework (LGS-Trimming) was proposed to reduce the computational and I/O costs of U-Apriori. As a summary, given an existential uncertain dataset D , LGS-Trimming creates a trimmed dataset D
T by removing items with low existential probabilities in D . The trimming framework works under the assumption that a non-trivial portion of the items in the dataset are associated with low existential probabilities (e.g., a pathogen may be highly resistant to a few drugs but not so for most of the others). Based on this assumption, the size of D T is significantly smaller than D and mining D
T instead of D has the following advantages:  X  The I/O cost of scanning D T is smaller.  X  Since many low-probability items have been removed, transactions in D T are
However, there are disadvantages of the trimming framework. First, there is the overhead of creating D T . Second, since D T is incomplete information, the set of frequent itemsets mined from it is only a subset of the complete set. A patch-up phase (and thus some overhead) is therefore needed to recover those missed frequent itemsets. As a result, if there are relatively few low-probability items in D ,then D T and D will be of similar sizes. The savings obtained by LGS-Trimming may not compensate for the overhead incurred. The performance of LGS-Trimming is thus sensitive to the percentage ( R ) of items with low exis-tential probabilities. Trimming can be counter-productive when R is very low. Third, a trimming threshold  X  t (to determine  X  X ow X  probability) is needed, which in some cases could be hard to set. A large  X  t implies a greater reduction of the size of D but a larger overhead in the patch-up phase to recover missed frequent itemsets. On the other hand, a small  X  t would trim D by little extent resulting in little savings. The performance of Trimming is thus sensitive to  X  t . In [4], it was assumed that the existential probabilities of items in a dataset follow a bimodal distribution . That is, most items X  can be classified as very-high-probability ones or very-low-probability ones. There were few items with moderate existential probabilities. In that case, it is easy to determine  X  t as there is a clearcut dis-tinction between high and low existential probabilities. It would be harder to select an appropriate  X  t if the distribution of existential probabilities is more uniform.

In this paper we propose an alternative method, called Decremental Pruning (DP), for mining frequent itemsets from existential uncertain data. As we will discuss in later sections, DP exploits the statistical properties of existential prob-abilities to gradually reduce the set of candidate itemsets. This leads to more efficient support counting and thus significant CPU cost savings. Comparing with LGS-Trimming, DP has two desirable properties: (1) it does not require a user-specified trimming threshold; (2) its performance is relatively less sensitive to R , the fraction of small-probability items in the dataset. DP is thus more applicable to a larger range of applications. Moreover, we will show that DP and LGS-Trimming are complementary to each other. They can be combined to achieve an even better performance.

The rest of this paper is organized as follows. Section 2 describes the mining problem and revisits the brute force U-Apriori algorithm. Section 3 presents the DP approach. Section 4 presents some experimental results and discusses some observations. We conclude the study in Section 5. In the existential uncertain data model, a dataset D consists of d transactions t ,...,t d .Atransaction t i contains a number of items. Each item x in t i is associated with a non-zero probability P t i ( x ), which indicates the likelihood that item x is present in transaction t i 1 .A Possible World model [5] can be applied to interpret an existential uncertain dataset. Basically, each probability P i ( x ) associated with an item x derives two possible worlds, say, W 1 and W 2 . In World W 1 ,item x is present in transaction t i ;InWorld W 2 ,item x is not in t i .Let P ( W j ) be the probability that World W j being the true world, then we have P ( W 1 )= P t i ( x )and P ( W 2 )=1  X  P t i ( x ). This idea can be extended to cover cases in which transaction t i contains other items. For example, let y be another item in t i with probability P t i ( y ). Assume that the observations of item x and item y are independently done, then there are four possible worlds. In particular, the probability of the world in which t i contains both items x and y is P t i ( x )  X  P t i ( y ). We can further generalize the idea to datasets that contain more than one transaction. Figure 1 illustrates the 16 possible worlds derived from the dataset shown in Table 1.
In traditional frequent itemset mining, the support count of an itemset X is defined as the number of transactions that contain X . For an uncertain dataset, such a support value is undefined since set containment is probabilistic. However, we note that each possible world derived from an uncertain dataset is certain, and therefore support counts are well-defined with respect to each world. For example, the support counts of itemset { a, b } in Worlds W 1 and W 6 (Figure 1) are 2 and 1, respectively. In [4], the notion of expected support was proposed as a frequency measure. Let W be the set of all possible worlds derivable from an uncertain dataset D . Given a world W j  X  W ,let P ( W j ) be the probability of World W j ; S ( X, W j ) be the support count of X with respect to W j ;and T i,j be the i th transaction in World W j . Assuming that items X  existential probabilities are determined through independent observations, then P ( W j ) and the expected support S e ( X )ofanitemset X are given by the following formulae 2 : Problem Statement. Given an existential uncertain dataset D and a user-specified support threshold  X  s , the problem of mining frequent itemsets is to return all itemsets X with expected support S e ( X )  X   X  s  X | D | .
 U-Apriori, a modified version of the Apriori algorithm, was presented in [4] as a baseline algorithm to solve the problem. The difference between Apriori and U-Apriori lies in the way supports are counted. Given a candidate itemset X and a transaction t i , Apriori tests whether X  X  t i . If so, the support count of X is incremented by 1. Under U-Apriori, the support count of X is incremented by the value x  X  X P t i ( x ) instead (see Equation 2). In this section we describe the Decremental Pruning (DP) technique, which ex-ploits the statistical properties of the existential probabilities of items to achieve candidate reduction during the mining process. The basic idea is to estimate upper bounds of candidate itemsets X  expected supports progressively after each dataset transaction is processed. If a candidate X  X  upper bound falls below the support threshold  X  s , the candidate is immediately pruned. To illustrate, let us consider a sample dataset shown in Table 2. Assume a support threshold  X  s =0 . 5, the minimum support count is min sup =4 candidate itemset { a, b } . To obtain the expected support of { a, b } , denoted as S ( { a, b } ), U-Apriori scans the entire dataset once and obtains S e ( { a, b } )=1 . 54, which is infrequent.

During the dataset scanning process, we observe that a candidate itemset X can be pruned before the entire dataset is scanned. The idea is to maintain a decremental counter  X  S e ( X, X ) for some non-empty X  X  X . The counter main-tains an upper bound of the expected support count of X , i.e., S e ( X ). This upper bound X  X  value is progressively updated as dataset transactions are pro-cessed. We use  X  S e ( X, X ,k )todenotethevalueof  X  S e ( X, X ) after transactions t ,...,t k have been processed.
 Definition 1. DecrementalCounter. For any non-empty X  X  X , k  X  0 ,  X  S From Equation 2, we have
Hence,  X  S e ( X, X ,k ) is an upper bound of S e ( X ). Essentially, we are assuming that the probabilities of all items x  X  X  X  X are 1 in transactions t k +1 ,...,t | D | in In our running example, suppose we have executed the first iteration of U-Apriori and have determined the expected supports of all 1-itemsets, in par-ticular, we know S e ( { a } )=2 . 6. At the beginning of the 2 nd iteration, we then process the fir st transaction t 1 and find that P t 1 ( b ) is 0.5 (instead of 1 as assumed when we calculated the upper bound), we know that we have over-cess t 2 . By similar argument, we know that we have overestimated the support  X  S ( { a, b } , { a } , 1)  X  0 . 18 = 1 . 92. At this point, the bound has dropped below the support threshold. The candidate { a, b } is thus infrequent and can be pruned.
Equation 3 summarizes the initialization and update of the decremental counter  X  S e ( X, X ,k ):  X  S e ( X, X ,k )=
From the example, we see that { a, b } can be pruned before the entire dataset is scanned. This candidate reduction potentially saves a lot of computational cost. However, there are 2 | X |  X  2 non-empty proper subsets of a candidate itemset X . The number of decremental counters i s thus huge. Maintaining a large num-ber of decremental counters involves too much overhead, and the DP method could be counter-productive. We propose two methods for reducing the number of decremental counters while mainta ining a good pruning effectiveness in the rest of this section.
 Aggregate by Singletons (AS). The AS method reduces the number of decre-mental counters to the number of frequen t singletons. First, only those decre-mental counters  X  S e ( X, X )where X is a frequent singleton are maintained. Second, given a frequent item x , the decremental counters  X  S e ( X, { x } ) for any itemset X that contains x are replaced by a singleton decremental counter d s ( x ). Let d s ( x, k )bethevalueof d s ( x )afterthefirst k data transactions have been processed. Equation 4 shows the initialization and update of d s ( x, k ). where max s ( k )= max { P t k ( x ) | x  X  t k ,x = x } returns the maximum existential probability among the items (except x ) in transaction t k . One can prove by induction that  X  S e ( X, { x } ,k )  X  d s ( x, k ) for any itemset X that contains item x . With the AS method, the aggregated counters can be organized in an array . During the mining process, if a counter X  X  value d s ( x, k ) drops below the support requirement, we know that any candidate itemset X that contains x must not be frequent and hence can be pruned. Also, we can remove item x from the dataset starting from transaction t k +1 . Therefore, AS not only achieves candidate reduction, it also shrinks dataset transactions. The latter allows more efficient subset testing during support counting.
 Common-Prefix Method (CP). The CP method aggregates the decremental counters of candidates with common pre fix. Here, we assume that items follow a certain ordering  X  , and the set of items of an itemset is listed according to  X  . First, only decremental counters of the form  X  S e ( X, X )where X is a proper prefix of X (denoted by X X ) are maintained. Second, given an itemset X , all counters  X  S e ( X, X ) such that X X are replaced by a prefix decremental counter d p ( X ). Let d p ( X ,k )bethevalueof d p ( X )afterthefirst k data trans-actions have been processed. Equation 5 shows the initialization and update of d ( X ,k ). items in X according to the item ordering  X  } .
 Again, by induction, we can prove that  X  S e ( X, X ,k )  X  d p ( X ,k ) for any X X . Hence when d p ( X ,k ) drops below the support requirement, we can conclude that any candidate itemset X such that X X must be infrequent and can thus be pruned. We remark that since most of the traditional frequent itemset mining algorithms apply a prefix-tree data structure to organize candidates [1][6][4], the way that CP aggregates the decremental counters facilitates its integration with the prefix-tree data structure.

Figure 2 shows the size-2 candidates of the dataset in Table 2 organized in a hash-tree data structure [1]. A hash-tree is essentially a prefix tree, where can-didates with the same prefix are organized under the same sub-tree. A prefix is thus associated with a node in the tree. A prefix decremental counter d p ( X )is stored in the parent node of the node that is associated with the prefix X .For example, d p ( b ) is stored in the root node since the prefix b is at level 1 of the tree (the second child node shown in Figure 2). [1] presented a recursive strategy for searching candidates that are contained in each transaction using a hash-tree structure. We illustrate the steps of processing a transaction t 1 from our running example (see Table 2) and explain how the counter d p ( b ) is updated in Figure 2.
From the figure, we see that d p ( b, 1) = 1 . 75 after t 1 is processed. Since d p ( b, 1) is an upper bound of the expected supports of { b, c } and { b, d } , and since d p ( b, 1) is smaller than the support requirement, we conclude that both { b, c } and { b, d } are infrequent and are thus pruned. With the hash-tree structure, we can virtu-ally prune the candidates by setting the pointer root.hash ( b )= NULL .Also,the counter d p ( b ) is removed from the root. As a result, the two candidates cannot be reached when subsequent transactions are processed. The computational cost of incrementing the expected support counts of the two candidates in subsequent transactions is saved.
 Item ordering. According to Equation 5, the initial value of a counter d p ( X ) is given by d p ( X , 0) = S e ( X ), i.e., the expected support of the prefix X .Since candidates are pruned if a prefix decremental counter drops below the support requirement, it makes sense to pick those prefixes X such that their initial val-ues are as small as possible. A heurist icwouldbetosettheitemordering  X  in increasing order of items X  supports. We adopt this strategy for the CP method. We conducted experiments comparing the performance of the DP methods against U-Apriori and LGS-Trimming. T he experiments were conducted on a 2.6GHz P4 machine with 512MB memor y running Linux Kernel 2.6.10. The algorithms were implemented using C.

We use the two-step dataset generation procedure documented in [4]. In the first step, the generator uses the IBM synthetic generator [1] to generate a dataset that contains frequent itemsets. We set the average number of items per trans-action ( T high ) to 20, the average length of frequent itemsets ( I )to6,andthe number of transactions ( D ) to 100K 3 . In the second step, the generator uses an uncertainty simulator to generate an existential probability for each item. The simulator first assigns each item in the dataset with a relatively high probability following a normal distribution with mean HB and standard deviation HD .To simulate items with low probabilities, the simulator inserts T low items into each transaction. The probabilities of these items follow a normal distribution with mean LB and standard deviation LD . The average number of items per transac-tion, denoted by T ,isequalto T high + T low .Aparameter R is used to control the percentage of items with low probabilities in the dataset (i.e. R = T low T
As an example, T 25 /R 20 /I 6 /D 100 K/HB 75 /HD 15 /LB 25 /LD 15 represents an uncertain dataset with 25 items per transaction on average. Out of the 25 items, 20 are assigned with high probabilities and 5 are assigned with low prob-abilities. The high (low) probabilities are generated following a normal distribu-tion with mean equal to 75% (25%) and standard deviation equal to 15% (15%). We call this dataset Synthetic-1 . 4.1 Pruning Power of the Decremental Methods In this section we investigate the pruning power of the decremental methods. The dataset we use is Synthetic-1 and we set  X  s =0 . 1% in the experiment. Figure 3a shows the percentage of candidates pruned by AS and CP in the second iteration after a cer tain fraction of the dataset transactions have been processed. For example, the figure shows that about 20% of the candidates are pruned by CP after 60% of the transactions are processed. From the figure, we observe that the pruning power of CP is higher than that of AS. In particular, CP prunes twice as many candidates as AS after the entire dataset is scanned.
Recall that the idea of AS and CP is to re place a group of decremental counters by either a singleton decrem ental counter (AS-counte r) or a prefix decremental counter (CP-counter). We say that an AS-or CP-counter d s/p ( X ) X  X overs X  a decremental counter  X  S e ( X, X )if  X  S e ( X, X ) is replaced by d s/p ( X ). Essen-tially, an AS-or CP-counter serves as an upper bound of a group of decremen-tal counters covered by it. In the 2 nd iteration, candidates are of size 2 and therefore all proper prefixes contain only one item. We note that in general, a CP-counter, say d p ( { a } ) covers fewer decrementa l counters than its AS coun-prefix is a stronger requirement than con tainment, the set of counters covered by d ( { a } ) is always a subset of d s ( { a } ). Therefore, each CP-counter  X  X overs X  fewer decremental counters tha n an AS-counter does. CP-counters are thus generally tighter upper bounds, leading to a more effective pruning.

Figure 3b shows the CPU cost in each iteration of the mining process. We see that in this experiment the costs of the 2 nd iteration dominates the others under all three algorithms. The pruning effectiveness of AS and CP in the 2 nd iteration (Figure 3a) thus reflects the CPU cost savings. For example, the 40% candidate reduction of CP translates into about 40s of CPU cost saving. Another observation is that although CP prunes twice as much as AS, the CPU cost saving of CP is not double of that of AS. This is because CP requires a more complex recursive strategy to maintain t he prefix decremental counters, which is comparatively more costly. 4.2 Varying Minimum Support Threshold Our next experiment compares the CPU costs of the DP methods against U-Apriori as the support threshold  X  s varies from 0.1% to 1.0%. Figure 4a shows the CPU costs and Figure 4b shows the percentage of savings over U-Apriori. For example, when  X  s = 1%, CP saves about 59% of CPU time compared with U-Apriori. From the figures, we see that CP performs slightly better than AS over a wide range of  X  s value. Also, the CPU costs of both CP and AS decrease as  X  s increases. This is because a larger  X  s implies fewer candidates and frequent itemsets, so the algorithms execute faster. Also, a larger  X  s implies the minimum support requirement is larg er. Hence, it is easier for the decremental counters to drop below the required value and more candidates can be pruned early. 4.3 Comparing with Data Trimming Recall that LGS-Trimming consists of th ree steps: (1) remove low-probability items from dataset D to obtain a trimmed dataset D T ;(2)mine D T ;(3)patch up and recover missed frequent itemsets. LGS-Trimming and DP methods are orthogonal and can be combined. (DP can be applied to mining D T and it also helps the patch-up step, which is essentially an additional iteration of candidate-generation and support-counting). In this section we compare U-Apriori, AS, CP, LGS-Trimming, and the combined method that integrates CP and LGS-Trimming. In particular, we study how the percentage of low-probability items ( R ) affects the algorithms X  performa nce. In the experiment, we use Synthetic-1 and set  X  s =0 . 1%. Figure 5a shows the CPU costs and Figure 5b shows the per-centage of savings over U-Apriori. From Figure 5b, we see that the performance of LGS-Trimming is very sensitive to R . Trimming outperforms AS and CP when R is large (e.g., 50%). This is because when there are numerous low-probability items, the trimmed dataset D T is very small, and mining D T is very efficient. On the other hand, if R is small, Trimming is less efficient than DP methods, and it could even be counter-productive for very small R .Thisisbecausefor small R , D T is large, so not much savings can be achieved by mining a trimmed dataset to compensate for the patch-up overhead.
In contrary, the performance of the DP methods are very stable over the range of R values. To understand this phenomenon let us consider Equation 4 for updating a AS-counter. The value of a AS-counter is determined by three terms: S e ( x ), P t k ( x )and max s ( k ). We note that varying R has small impact on the value of S e ( x ) because S e ( x ) is the expected support of item x ,whichis mainly determined by the high-probability entries of x in the dataset. Also, if transaction t k contains a small-probability entry for x ,then P t k ( x )issmallandso the decrement to the value d s ( x, k ) would be insignificant. Hence, the population of small-probability items (i.e., R ) has little effect in the decremental process. Finally, since max s ( k ) is determined by the maximum existential probability of the items (except x ) in transaction t k , low-probability items have little effect on the value of max s ( k ). As a result, the performance of AS is not sensitive to the population of low-probability items. A similar conclusion can be drawn for CP by considering Equation 5.

From the figures, we also observe that the combined algorithm strikes a good balance and gives consistently good performance. It X  X  performance is comparable to those of AS and CP when R is small, and it gives the best performance when R is large. In this paper we proposed a decrementa l pruning (DP) approach for efficient mining of frequent itemsets from existent ial uncertain data. E xperimental results showed that DP achieved significant candidate reduction and computational cost savings. Compared with LGS-Trimming, DP had the advantages of not requiring a trimming threshold and its performance was relatively stable over a wide range of low-probability-item population. In particular, it outperformed data trimming when the dataset contained few low-probability items. We argued that the Trimming approach and the DP approach were orthogonal to each other. We showed that the two approaches could be combined leading to a generally best overall performance.

