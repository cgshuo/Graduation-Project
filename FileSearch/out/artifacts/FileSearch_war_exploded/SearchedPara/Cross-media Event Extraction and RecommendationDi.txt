 Every day, a vast amount of unstructured data in dif-ferent modalities (e.g., texts, images and videos) is posted online for ready viewing. Complex event ex-traction and recommendation is critical for many in-formation distillation tasks, including tracking cur-rent events, providing alerts, and predicting possi-ble changes, as related to topics of ongoing con-cern. State-of-the-art Information Extraction (IE) technologies focus on extracting events from a sin-gle data modality and ignore cross-media fusion. More importantly, users are presented with extracted events in a passive way (e.g., in a temporally ordered event chronicle (Ge et al., 2015)). Such technologies do not leverage user behavior to identify the event properties of interest to them in selecting new sce-narios for presentation.

In this paper we present a novel event extraction and recommendation system that incorporates ad-vances in extracting events across multiple sources with data in diverse modalities and so yields a more comprehensive understanding of collective events, their importance, and their inter-connections. The novel capabilities of our system include:  X  Event Extraction, Summarization and Search : extract concepts, events and their arguments (par-ticipants) and implicit attributes, and semanti-cally meaningful visual patterns from multiple data modalities, and organize them into Event
Cubes (Tao et al., 2013) based on Online Analyt-ical Processing (OLAP). We developed a search interface to these cubes with a novel back-end event summarization component, for users to specify multiple dimensions in a query and re-ceive single-sentence summary responses.  X  Event Recommendation : recommend related events based on meta-paths derived from event arguments, and automatically adjust the ranking function by updating the weights of dimensions based on user browsing feedback. 2.1 System Architecture The overall architecture of our system is illustrated in Figure 1. We first extract textual event informa-tion, as well as visual concepts, events and patterns from the raw multimedia documents to construct event cubes. Based on the event cubes, the search interface (Figure 2) returns the document that best matches the user X  X  query as the first primary event for display. A query may consist of multiple dimen-sions. The recommendation interface displays mul-tiple dimensions and rich annotations of the primary event, and recommends similar and dissimilar events to the user. 2.2 Data Sets To demonstrate the capabilities of our system, we use two event types, Protest and Attack , as our case studies. We collected the following data sets:  X  Protest : 59 protest incidents that occurred be-tween January 2009 and December 2010, from 458 text documents, 28 images and 31 videos.  X  Attack : 52 attack incidents that occurred between
January 2014 and December 2015, from 812 text documents, 46 images and 6 videos. 3.1 Event Extraction We apply a state-of-the-art English IE system (Li et al., 2014) to jointly extract entities, relations and events from text documents. This system is based on structured perceptron incorporating multiple lev-els of linguistic features. However, some impor-tant event attributes are not expressed by explicit textual clues. To enrich the profile of each protest event, the system identifies two additional implicit attributes that derive from social movement theo-ries (Della Porta and Diani, 2009; Furusawa, 2006; Dalton, 2013):  X  Types of Protest , including demonstration , riot , strike , boycott , picket and individual protest .  X  Demands of Protest , indicating the hidden be-havioral intent of protesters, about what they de-sire to change or preserve, including pacifist , po-litical , economic , retraction , financial , religious , human rights , race , justice , environmental , sports rivalry and social .
 We annotated 92 news documents, 59 of which con-tained protests, to learn a set of heuristic rules for au-tomatically extracting these implicit attributes from the IE outputs of these documents. 3.2 Event Cube Construction and Search Event extraction helps in converting unstructured texts into structured arguments and attributes (di-mensions). However, a user may still need to  X  X rill down, X  searching back through many documents and changing query selections before finding an item of interest. We use EventCube (Tao et al., 2013) to effectively organize and efficiently search relevant events, and measure event similarity based on multi-ple dimensions. EventCube is a general online ana-lytical processing (OLAP) framework for importing any collection of multi-dimensional text documents and constructing text-rich data cube. EventCube dif-fers from traditional search engines in that it returns Top-Cells , where relevant documents are aggregated by dimension combinations.

We regard each event as a data point associated with multiple dimension values. After a user in-puts a multi-dimensional query in the search inter-face (Figure 2), we build inverted index upon the dimensions in Event Cube to return related events, which provides much more flexible matching com-pared to keyword search. After the search interface retrieves the most rele-vant event ( X  X rimary event X ), the user will be di-rected to the recommendation interface (Figure 3) and can start exploring various events. The user X  X  initial query is displayed at the top (gray bar) and is updated to capture new user selections. The number of events that match the user X  X  initial query and the number of documents associated with the primary event are displayed in the gray bar, at the far right side. In addition to text IE results, we apply the fol-lowing multi-media extraction and summarization techniques to illustrate and enrich event profiles. 4.1 Summarization From each set of relevant documents, we apply a state-of-the-art phrase mining method (Liu et al., 2016) to mine top-k representative phrases. Then we construct an affinity matrix of sentences and ap-ply spectral clustering to find several clustering cen-ters (i.e., representative sentences including the most important phrases) as the summary. The user is also provided two options to show the original doc-uments and the document containing the summary. 4.2 Visual Information Extraction For each event, we retrieve the most representa-tive video/image online using the key-phrases such as date and entities as queries. Videos and im-ages are often more impressive and efficient at conveying information. We first apply a pre-trained convolutional neural network (CNN) archi-tecture (Kuznetsova et al., 2012) to extract visual concepts from each video key frame based on the EventNet concept library (Ye et al., 2015). For example, the extracted visual concepts  X  crowd on street, riot, demonstration or protest, people march-ing  X  appear when the user X  X  mouse is over the video of the primary event (Figure 3). Then we adopt the approach described in (Li et al., 2015) which applies CNN and association rule mining technique to gen-erate visual patterns and extract semantically mean-ingful relations between visual and textual informa-tion to name the patterns. We rank and recommend events based on meta paths (Sun et al., 2011), by representing the whole data set as a heterogeneous network, that is composed of multi-typed and interconnected ob-jects (e.g., events, location, protesters, target of protesters). A meta path is a sequence of rela-tions defined between different object types. For protest events, we define six meta paths:  X  X vent-date-event X ,  X  X vent-location-event X ,  X  X vent-target of protest-event X ,  X  X vent-protesters-event X ,  X  X vent-type of protest-event X  and  X  X vent-demand of protest-event X ; and four meta paths for attack events:  X  X vent-date-event X ,  X  X vent-location-event X ,  X  X vent-attackers-event X  and  X  X vent-type of attack-event X .
The similarity between two events is the weighted sum of the six meta path similarities. The weights are assigned dynamically by the user X  X  activity:  X  When the user clicks on a certain image/video: as-sign 1 . 0 to all meta-path similarities.  X  When the user clicks on a certain dimension X : 1 . 0 for the similarity based on Event-X -Event, and 0 . 2 to other meta-paths.

To illustrate the meta paths, the dimension names of recommended events are highlighted if they share the same dimensions with the primary event. More-over, the system is switchable between recommend-ing the most similar and most dissimilar events with a toggle button that the user can click. In this paper we present a cross-media event ex-traction and recommendation system which effec-tively aggregates and summarizes related complex events, and makes recommendations based on user interests. The current system interface incorporates a medium-level human agency (the capacity of an entity to act) by allowing a human user to provide relevance feedback while driving the browsing in-terests by multi-dimensional recommendations. We plan to follow the cognitive fit theory (Vessey, 1991) and conduct a series of human utility evaluations to formally quantify the impact of each new compo-nent and each data modality on enhancing the speed and quality of aggregating and summarizing event-related knowledge, detecting conflicts and errors, and generating alerts.
 This work was supported by the U.S. ARL NS-CTA No. W911NF-09-2-0053, DARPA Multimedia Seedling grant, DARPA DEFT No. FA8750-13-2-0041 and NSF CAREER Award IIS-1523198. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Gov-ernment is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.
