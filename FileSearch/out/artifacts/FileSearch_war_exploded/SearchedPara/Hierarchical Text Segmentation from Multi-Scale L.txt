 Recovering structural organization from unformat-ted texts or transcripts is a fundamental problem in natural language processing, with applications to classroom lectures, meeting transcripts, and chat-room logs. In the unsupervised setting, a variety of successful systems have leveraged lexical cohe-sion (Halliday and Hasan, 1976)  X  the idea that topically-coherent segments display consistent lex-ical distributions (Hearst, 1994; Utiyama and Isa-hara, 2001; Eisenstein and Barzilay, 2008). How-ever, such systems almost invariably focus on linear segmentation, while it is widely believed that dis-course displays a hierarchical structure (Grosz and Sidner, 1986). This paper introduces the concept of multi-scale lexical cohesion , and leverages this idea in a Bayesian generative model for hierarchical topic segmentation.

The idea of multi-scale cohesion is illustrated by the following two examples, drawn from the Wikipedia entry for the city of Buenos Aires.
The two sections are both part of a high-level seg-ment on transportation. Words in bold are charac-teristic of the subsections (buses and trains, respec-tively), and do not occur elsewhere in the transporta-tion section; words in italics occur throughout the high-level section, but not elsewhere in the article. This paper shows how multi-scale cohesion can be captured in a Bayesian generative model and ex-ploited for unsupervised hierarchical topic segmen-tation.

Latent topic models (Blei et al., 2003) provide a powerful statistical apparatus with which to study discourse structure. A consistent theme is the treat-ment of individual words as draws from multinomial language models indexed by a hidden  X  X opic X  asso-ciated with the word. In latent Dirichlet allocation (LDA) and related models, the hidden topic for each word is unconstrained and unrelated to the hidden topic of neighboring words (given the parameters). In this paper, the latent topics are constrained to pro-duce a hierarchical segmentation structure, as shown in Figure 1.
These structural requirements simplify inference, allowing the language models to be analytically marginalized. The remaining hidden variables are the scale-level assignments for each word token. Given marginal distributions over these variables, it is possible to search the entire space of hierarchical segmentations in polynomial time, using a novel dy-namic program. Collapsed variational Bayesian in-ference is then used to update the marginals. This approach achieves high quality segmentation on multiple levels of the topic hierarchy.

Source code is available at http://people. csail.mit.edu/jacobe/naacl09.html . The use of lexical cohesion (Halliday and Hasan, 1976) in unsupervised topic segmentation dates back to Hearst X  X  seminal T EXT T ILING system (1994). Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). The application of Bayesian topic models to text segmentation was investigated first by Blei and Moreno (2001) and later by Purver et al. (2006), using HMM-like graphical models for linear segmentation. Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. All of these papers consider only linear topic seg-mentation; we introduce multi-scale lexical cohe-sion, which posits that the distribution of some words changes slowly with high-level topics, while others change rapidly with lower-level subtopics. This gives a principled mechanism to model hier-archical topic segmentation.

The literature on hierarchical topic segmentation is relatively sparse. Hsueh et al. (2006) describe a supervised approach that trains separate classifiers for topic and sub-topic segmentation; more relevant for the current work is the unsupervised method of Yaari (1997). As in T EXT T ILING , cohesion is measured using cosine similarity, and agglomerative clustering is used to induce a dendrogram over para-graphs; the dendrogram is transformed into a hier-archical segmentation using a heuristic algorithm. Such heuristic approaches are typically brittle, as they include a number of parameters that must be hand-tuned. These problems can be avoided by working in a Bayesian probabilistic framework.
We note two orthogonal but related approaches to extracting nonlinear discourse structures from text. Rhetorical structure theory posits a hierarchi-cal structure of discourse relations between spans of text (Mann and Thompson, 1988). This structure is richer than hierarchical topic segmentation, and the base level of analysis is typically more fine-grained  X  at the level of individual clauses. Unsupervised approaches based purely on cohesion are unlikely to succeed at this level of granularity.

Elsner and Charniak (2008) propose the task of conversation disentanglement from internet chat-room logs. Unlike hierarchical topic segmentation, conversational threads may be disjoint, with un-related threads interposed between two utterances from the same thread. Elsner and Charniak present a supervised approach to this problem, but the devel-opment of cohesion-based unsupervised methods is an interesting possibility for future work. Topic modeling is premised on a generative frame-work in which each word w t is drawn from a multi-nomial  X  y language model that generates w t . From a modeling standpoint, linear topic segmentation merely adds the constraint that y t  X  { y t  X  1 ,y t  X  1 + 1 } . Segmen-tations that draw boundaries so as to induce com-pact, low-entropy language models will achieve a high likelihood. Thus topic models situate lexical cohesion in a probabilistic setting.

For hierarchical segmentation, we take the hy-pothesis that lexical cohesion is a multi-scale phe-nomenon. This is represented with a pyramid of lan-guage models, shown in Figure 1. Each word may be drawn from any language model above it in the pyra-mid. Thus, the high-level language models will be required to explain words throughout large parts of the document, while the low-level language models will be required to explain only a local set of words. A hidden variable z t indicates which level is respon-sible for generating the word w t .

Ideally we would like to choose the segmentation  X y = argmax with the hidden language models  X  and scale-level assignments z . The language models can be inte-grated out analytically (Section 3.1). Given marginal likelihoods for the hidden variables z , the globally optimal segmentation  X y can be found using a dy-namic program (Section 4.1). Given a segmentation, we can estimate marginals for the hidden variables, using collapsed variational inference (Section 4.2). We iterate between these procedures in an EM-like coordinate-ascent algorithm (Section 4.4) until con-vergence. 3.1 Language models We begin the formal presentation of the model with some notation. Each word w t is modeled as a single draw from a multinomial language model  X  j . The language models in turn are drawn from symmetric Dirichlet distributions with parameter  X  . The num-ber of language models is written K ; the number of words is W ; the length of the document is T ; and the depth of the hierarchy is L .

For hierarchical segmentation, the vector y t indi-cates the segment index of t at each level of the topic hierarchy; the specific level of the hierarchy respon-sible for w t is given by the hidden variable z t . Thus, y ates w t .

With these pieces in place, we can write the ob-servation likelihood, where we have merely rearranged the product to group terms that are drawn from the same language model. As the goal is to obtain the hierarchical seg-mentation and not the language models, the search space can be reduced by marginalizing  X  . The derivation is facilitated by a notational convenience: x j represents the lexical counts induced by the set of words { w t : y ( z t ) p ( w | y , z , X  ) =
Here, p dcm indicates the Dirichlet compound multinomial distribution (Madsen et al., 2005), which is the closed form solution to the integral over language models. Also known as the multivariate Polya distribution, the probability density function can be computed exactly as a ratio of gamma func-tions. Here we use a symmetric Dirichlet prior  X  , though asymmetric priors can easily be applied.
Thus far we have treated the hidden variables z as observed. In fact we will compute approxi-mate marginal probabilities Q z Q under distribution Q z , we approximate,  X  x j ( i )  X  Q where x j ( i ) indicates the count for word type i gen-erated from segment j . In the outer sum, we con-sider all t for possibly drawn from segment j . The inner sum goes over all levels of the pyramid. The delta functions take the value one if the enclosed Boolean expression is true and zero otherwise, so we are adding the fractional counts  X  t` only when w 3.2 Prior on segmentations Maximizing the joint probability p ( w , y ) = p ( w | y ) p ( y ) leaves the term p ( y ) as a prior on seg-mentations. This prior can be used to favor segmen-tations with the desired granularity. Consider a prior of the form p ( y ) = tional convenience, we introduce a base level such point. At every level ` &gt; 0 , the prior is a Markov
The constraint y ( ` ) linear segmentation at each level. To enforce hierar-chical consistency, each y ( ` ) point only if t is also a segmentation point at the lower level `  X  1 . Zero probability is assigned to segmentations that violate these constraints.
To quantify the prior probability of legal segmen-tations, assume a set of parameters d ` , indicating the expected segment duration at each level. If t is a valid potential segmentation point at level ` ity of a segment transition is r ` = d `  X  1 /d ` , with d M  X  N segments in level `  X  1 , then the prior p hierarchical segmentation constraint is obeyed.
For the purposes of inference it will be prefer-able to have a prior that decomposes over levels and segments. In particular, we do not want to have to commit to a particular segmentation at level ` be-fore segmenting level ` + 1 . The above prior can be approximated by replacing M with its expecta-tion  X  M  X  d ranging from w u to w v (inclusive) will contribute log r This section describes the inference for the segmen-tation y , the approximate marginals Q Z , and the hy-perparameter  X  . 4.1 Dynamic programming for hierarchical While the model structure is reminiscent of a facto-rial hidden Markov model (HMM), there are impor-tant differences that prevent the direct application of HMM inference. Hidden Markov models assume that the parameters of the observation likelihood dis-tributions are available directly, while we marginal-ize them out. This has the effect of introducing de-pendencies throughout the state space: the segment assignment for each y t contributes to lexical counts which in turn affect the observation likelihoods for ture of segmentation, efficient inference of the opti-mal hierarchical segmentation (given the marginals Q Z ) is still possible.

Let B ( ` ) [ u,v ] represent the log-likelihood of grouping together all contiguous words w u ...w v  X  1 at level ` of the segmentation hierarchy. Using x t to indicate a vector of zeros with one at the position w , we can express B more formally:
The last two terms are from the prior p ( y ) , as ex-plained in Section 3.2. The value of B ( ` ) [ u,v computed for all u , all v &gt; u , and all ` .
Next, we compute the log-likelihood of the op-timal segmentation, which we write as A ( L ) [0 ,T ] . This matrix can be filled in recursively: A ( ` ) [ u,v ] = max
The first term adds in the log probability of the segment from t to v at level ` . The second term re-turns the best score for segmenting this same interval at a more detailed level of segmentation. The third term recursively segments the interval from u to t at the same level ` . The boundary case A ( ` ) [ u,u ] = 0 . 4.1.1 Computational complexity
The sizes of A and B are each O ( LT 2 ) . The ma-trix A can be constructed by iterating through the layers and then iterating: u from 1 to T ; v from u +1 to
T ; and t from u to v + 1 . Thus, the time cost for filling A is O ( LT 3 ) . For computing the observation likelihoods in B , the time complexity is O ( LT 2 W ) , where W is the size of the vocabulary  X  by keeping cumulative lexical counts, we can compute B [ u,v ] without iterating from u to v .

Eisenstein and Barzilay (2008) describe a dy-namic program for linear segmentation with a space complexity of O ( T ) and time complexities of O ( T 2 ) to compute the A matrix and O ( TW ) to fill the B matrix. 1 Thus, moving to hierarchical seg-mentation introduces a factor of TL to the complex-ity of inference. 4.1.2 Discussion
Intuitively, efficient inference is possible because the location of each segment boundary affects the likelihood of only the adjoining segments at the same level of the hierarchy, and their  X  X hildren X  at the lower levels of the hierarchy. Thus, the observa-tion likelihood at each level decomposes across the segments of the level. This is due to the left-to-right nature of segmentation  X  in general it is not possible to marginalize the language models and still perform efficient inference in HMMs. The prior (Section 3.2) was designed to decompose across segments  X  if, for example, p ( y ) explicitly referenced the total number of segments, inference would be more difficult.
A simpler inference procedure would be a greedy approach that makes a fixed decision about the top-level segmentation, and then applies recursion to achieve segmentation at the lower levels. The greedy approach will not be optimal if the best top-level segmentation leads to unsatisfactory results at the lower levels, or if the lower levels could help to disambiguate high-level segmentation. In contrast, the algorithm presented here maximizes the overall score across all levels of the segmentation hierarchy. 4.2 Scale-level marginals The hidden variable z t represents the level of the segmentation hierarchy from which the word w t is drawn. Given language models  X  , each w t can be thought of as a draw from a Bayesian mixture model, with z t as the index of the component that generates w t . However, as we are marginalizing the language models, standard mixture model infer-ence techniques do not apply. One possible solu-tion would be to instantiate the maximum a posteri-ori language models after segmenting, but we would prefer not to have to commit to specific language models. Collapsed Gibbs sampling (Griffiths and Steyvers, 2004) is another possibility, but sampling-based solutions may not be ideal from a performance standpoint.

Recent papers by Teh et al. (2007) and Sung et al. (2008) point to an appealing alternative: col-lapsed variational inference (called latent-state vari-ational Bayes by Sung et al.). Collapsed variational inference integrates over the parameters (in this case, the language models) and computes marginal distributions for the latent variables, Q z . However, due to the difficulty of computing the expectation of the normalizing term, these marginal probabili-ties are available only in approximation.

More formally, we wish to compute the approx-imate distribution Q z ( z ) = ing across all latent variables. As is typical in vari-ational approaches, we fit this distribution by opti-mizing a lower bound on the data marginal likeli-hood p ( w , z | y )  X  we condition on the segmentation y because we are treating it as fixed in this part of the inference. The lower bound can be optimized by iteratively setting, indicating the expectation under Q Due to the couplings across z , it is not possible to compute this expectation directly, so we use the first-order approximation described in (Sung et al., 2008). In this approximation, the value Q z  X  which we abbreviate as  X  t`  X  takes the form of the likelihood of the observation w t , given a mod-ified mixture model. The parameters of the mixture model are based on the priors and the counts of w and  X  for all t 0 6 = t :
The first term in equation 2 is the set of compo-nent weights  X  ` , which are fixed at 1 /L for all ` . The fraction represents the posterior estimate of the lan-guage models: standard Dirichlet-multinomial con-jugacy gives a sum of counts plus a Dirichlet prior (equation 3). Thus, the form of the update is ex-tremely similar to collapsed Gibbs sampling, except that we maintain the full distribution over z t rather than sampling a specific value. The derivation of this update is beyond the scope of this paper, but is sim-ilar to the mixture of Bernoullis model presented in Section 5 of (Sung et al., 2008).

Iterative updates of this form are applied until the procedure appears at step 5a of algorithm 1. 4.3 Hyperparameter estimation The inference procedure defined here includes two parameters:  X  , the symmetric Dirichlet prior on the language models; and d , the expected segment du-rations. The granularity of segmentation is consid-ered to be a user-defined characteristic, so there is no  X  X ight answer X  for how to set this parameter. We simply use the oracle segment durations, and pro-vide the same oracle to the baseline methods where possible. As discussed in Section 6, this parameter had little effect on system performance.

The  X  parameter controls the expected sparsity of the induced language models; it will be set automat-ically. Given a segmentation y and hidden-variable marginals  X  , we can maximize p (  X , w | y , X  ) = p Dirichlet compound multinomial has a tractable gra-dient, which can be computed using scaled counts,  X  x counts are taken for each segment j across the entire segmentation hierarchy. The likelihood p (  X  x |  X  ) then has the same form as equation 1, with the x ji terms replaced by  X  x ji . The gradient of the log-likelihood Algorithm 1 Complete segmentation inference is thus a sum across segments,
Here,  X  indicates the digamma function, which is the derivative of the log gamma function. The prior p (  X  ) takes the form of a Gamma distribution with parameters G (1 , 1) , which has the effect of dis-couraging large values of  X  . With these parame-ters, the gradient of the Gamma distribution with re-spect to  X  is negative one. To optimize  X  , we inter-pose an epoch of L-BFGS (Liu and Nocedal, 1989) optimization after maximizing  X  (Step 5b of algo-rithm 1). 4.4 Combined inference procedure The final inference procedure alternates between up-dating the marginals  X  , the Dirichlet prior  X  , and the MAP segmentation  X  y . Since the procedure makes hard decisions on  X  and the segmentations y , it can be thought of as a form of Viterbi expectation-maximization (EM). When a repeated segmentation is encountered, the procedure terminates. Initializa-tion involves constructing a segmentation  X  y in which each level is segmented uniformly, based on the ex-pected segment duration d ` . The hidden variable marginals  X  are initialized randomly. While there is no guarantee of finding the global maximum, lit-tle sensitivity to the random initialization of  X  was observed in preliminary experiments.

The dynamic program described in this section achieves polynomial time complexity, but O ( LT 3 ) can still be slow when T is the number of word to-kens in a large document such as a textbook. For this reason, we only permit segment boundaries to be placed at gold-standard sentence boundaries. The only change to the algorithm is that the tables A and B need contain only cells for each sentence rather than for each word token  X  hidden variable marginals are still computed for each word token. Implemented in Java, the algorithm runs in roughly five minutes for a document with 1000 sentences on a dual core 2.4 GHz machine. Corpora The dataset for evaluation is drawn from a medical textbook (Walker et al., 1990). 2 The text contains 17083 sentences, segmented hierarchically into twelve high-level parts, 150 chapters, and 520 sub-chapter sections. Evaluation is performed sep-arately on each of the twelve parts, with the task of correctly identifying the chapter and section bound-aries. Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries.

Practical applications of topic segmentation typ-ically relate to more informal documents such as blogs or speech transcripts (Hsueh et al., 2006), as formal texts such as books already contain segmen-tation markings provided by the author. The premise of this evaluation is that textbook corpora provide a reasonable proxy for performance on less structured data. However, further clarification of this point is an important direction for future research.
 Metrics All experiments are evaluated in terms of the commonly-used P k and WindowDiff met-rics (Pevzner and Hearst, 2002). Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are prop-erly segmented with respect to each other. Win-dowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while P k only asks whether the two sentences are in the same segment or not. This eval-uation uses source code provided by Malioutov and Barzilay (2006).
 Experimental system The joint hierarchical Bayesian model described in this paper is called H
IER B AYES . It performs a three-level hierarchical segmentation, in which the lowest level is for sub-chapter sections, the middle level is for chapters, and the top level spans the entire part. This top-level has the effect of limiting the influence of words that are common throughout the document.
 Baseline systems As noted in Section 2, there is little related work on unsupervised hierarchical seg-mentation. However, a straightforward baseline is a greedy approach: first segment at the top level, and then recursively feed each top-level segment to the segmenter again. Any linear segmenter can be plugged into this baseline as a  X  X lack box. X 
To isolate the contribution of joint inference, the greedy framework can be combined with a one-level version of the Bayesian segmentation algorithm de-scribed here. This is equivalent to B AYES S EG , which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). The hierarchical segmenter built by placing B AYES S EG in a greedy algorithm is called G REEDY -B AYES .

To identify the contribution of the Bayesian segmentation framework, we can plug in alter-native linear segmenters. Two frequently-cited systems are LCS EG (Galley et al., 2003) and T EXT S EG (Utiyama and Isahara, 2001). LC-S
EG optimizes a metric of lexical cohesion based on lexical chains. T EXT S EG employs a probabilis-tic segmentation objective that is similar to ours, but uses maximum a posteriori estimates of the lan-guage models, rather than marginalizing them out. Other key differences are that they set  X  = 1 , and use a minimum description length criterion to deter-mine segmentation granularity. Both of these base-lines were run using their default parametrization.
Finally, as a minimal baseline, U NIFORM pro-duces a hierarchical segmentation with the ground truth number of segments per level and uniform du-ration per segment at each level.
 Preprocessing The Porter (1980) stemming algo-rithm is applied to group equivalent lexical items. A set of stop-words is also removed, using the same list originally employed by several competitive sys-tems (Utiyama and Isahara, 2001). Table 1 presents performance results for the joint hierarchical segmenter and the three greedy base-lines. As shown in the table, the hierarchical system achieves the top overall performance on the harsher WindowDiff metric. In general, the greedy seg-menters each perform well at one of the two levels and poorly at the other level. The joint hierarchical inference of H IER B AYES enables it to achieve bal-anced performance at the two levels.

The G REEDY -B AYES system achieves a slightly better average P k than H IER B AYES , but has a very large gap between its P k and WindowDiff scores. The P k metric requires only that the system cor-rectly classify whether two points are in the same or different segments, while the WindowDiff metric insists that the exact number of interposing segments be identified correctly. Thus, the generation of spu-rious short segments may explain the gap between the metrics.

LCS EG and T EXT S EG use heuristics to deter-mine segmentation granularity; even though these methods did not score well in terms of segmentation accuracy, they were generally closer to the correct granularity. In the Bayesian methods, granularity is enforced by the Markov prior described in Sec-tion 3.2. This prior was particularly ineffective for G
REEDY -B AYES , which gave nearly the same num-ber of segments at both levels, despite the different settings of the expected duration parameter d .
The Dirichlet prior  X  was selected automatically, but informal experiments with manual settings sug-gest that this parameter exerts a stronger influence on segmentation granularity. Low settings reflect an expectation of sparse lexical counts and thus encour-age short segments, while high settings reflect an ex-pectation of evenly-distributed counts and thus lead to long segments. Further investigation is needed on how best to control segmentation granularity in a Bayesian setting. While it is widely agreed that language often dis-plays hierarchical topic structure (Grosz, 1977), there have been relatively few attempts to extract such structure automatically. This paper shows that the lexical features that have been successfully exploited in linear segmentation can also be used to extract a hierarchical segmentation, due to the phenomenon of multi-scale lexical cohesion. The Bayesian methodology offers a principled proba-bilistic formalization of multi-scale cohesion, yield-ing an accurate and fast segmentation algorithm with a minimal set of tunable parameters.

It is interesting to consider how multi-scale seg-mentation might be extended to finer-grain seg-ments, such as paragraphs. The lexical counts at the paragraph level will be sparse, so lexical cohesion alone is unlikely to be sufficient. Rather it may be necessary to model discourse connectors and lexical semantics explicitly. The development of more com-prehensive Bayesian models for discourse structure seems an exciting direction for future research. Acknowledgments Thanks to Michel Galley, Igor
