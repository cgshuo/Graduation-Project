 1.Introduction
This paper reports on an investigation of a measure of bias introduced by Mowshowitz and Kawaguchi (1999, 2002a, 2002b) and Kawaguchi and Mowshowitz (2001) . Bias is compared with the classical measures of retrieval performance in an effort to show what the bias measure can and cannot do. Several experiments designed to test the measure and to demonstrate its utility are discussed and analyzed. The experiments aim to resolve questions about performance differences between search engines, and the possible influence of subject areas and keywords on the measure. The findings suggest that (1) there are significant differences in the performance of search engines, (2) the measure is sensitive to the subject domain being searched, and (3) the search terms chosen in a given subject domain have little influence on bias.

The bias measure is designed to capture the degree to which the distribution of URLs, retrieved by a search engine in response to a query deviates from an ideal or fair distribution for that query. This ideal is approximated by the distribution produced by a collection of search engines. Like traditional measures of retrieval performance (i.e., recall and precision), bias is a function of a system X  X  response to a query, but it does not depend on a determination of relevance ( Meadow, 1973 ; Saracevic, 1975 ; Wishard, 1998 ). In-stead, the ideal distribution of items in a response set must be determined. Using the output of a collection of search engines to approximate the ideal makes the measurement of bias computationally feasible in real time.

Bias is a relative concept. A search engine is being weighed against its peers, not against an absolute norm derived from features of the universe. It might be desirable to adopt the latter approach, but it just is not feasible given the enormous size of the World Wide Web. Until there is a method of ascertaining, in real time, the distributions of items that might be retrieved in response to any given query, other ap-proaches X  X  X uch as the one taken here X  X  X o defining an  X  X  X deal X  X  distribution are needed.

Whether bias in a search engine is intentional or not X  X  X ee Mowshowitz and Kawaguchi (2002a) for a discussion of sources of bias X  X  X t is important for designers to know a system X  X  bias profile. If intentional, designers need to know how effective the engine is in realizing a particular bias (e.g., prominently listing items dealing with one particular subtopic or viewpoint in response to a query concerning a research topic).
If unintentional and the designer wishes to minimize bias, measurement is likewise essential to gauging per-formance.

Users would benefit by knowing whether or not the URLs retrieved on a given topic are representative of that subject area. This is very different from the question of relevance. Consider a search on the topic  X  X  X uthanasia X  X . The Websites retrieved by a particular search engine may all be judged relevant to the topic  X  X  X uthanasia X  X  by a user, but the selection may be biased in the sense that the Websites retrieved are uni-formly pro-euthanasia.

A cluster analysis of URLs retrieved in a search can be used to clarify the bias in results. This is one way of showing the structure of a response set of URLs obtained by one or more search engines for a given query ( Flake, Lawrence, Giles, &amp; Coetzee, 2002 ). A subset of URLs on the Web (and the Web itself ( Klein-berg, Kumar, Raghavan, Rajagopalan, &amp; Tomkins, 1999 )) can be interpreted as a directed graph whose nodes represent the URLs, and in which there is a directed edge from node x to y if the URL corresponding to x has a hyperlink pointing to the URL corresponding to y . In the case of a subset of URLs on the Web, hyperlinks to URLs that are not in the subset are ignored in the current analysis. With this interpretation clusters of URLs can be determined quite naturally as either weakly or strongly connected components of the directed graph ( Harary, 1969 ).

Fig. 1 shows the results of such an analysis of the results of a search on the keyword  X  X  X uthanasia X  X  con-ducted in late 1999. The example is meant to underscore the importance of bias, but it should be noted that clustering requires interpretation of the contents of Websites, which bias measurement itself does not do.
Although some of the search engines used and some of the URLs obtained are now defunct, the analysis illustrates a general method for interpreting bias values. Ten URLs retrieved by each of five search engines (AltaVista, Excite, Google, NorthernLight and Yahoo) were included in the analysis. Thirty-nine of these 50 URLs are distinct. As shown in the figure, the directed graph corresponding to the response set consists of two non-trivial, weakly connected components. Not shown in the figure are 28 isolated nodes.
The division into two clusters corresponds to the split between groups that respectively accept (e.g., soros.org/debate/Euthan.htm, bitsnet.com/choicebyright/euthnews.htm) and reject (e.g., euthanasia.com/, iaetf.org/, priestsforlife.org/euthanasia/euthanasia.html) the practice of euthanasia. The substantially larger cluster represents factions opposed to euthanasia, while the smaller one represents groups that accept the practice. This particular example made use of the collective results of several search engines, but the same analysis could be performed on the results of a single engine with a view to clarifying a bias value computed for a single engine.

Cluster analysis might reveal even tighter connections between URLs. A search producing a large num-ber of nodes might exhibit non-trivial, strongly connected components. The example shown here, with only 39 nodes, did not reveal any such strong components.

Notice of possible bias may be especially important for the na X   X  ve user who might be able to judge rel-evance, but would not be in a position to determine whether or not the retrieved Websites contain a dispro-portionate amount of material favoring one point of view over others. It is of course possible that a user would want a search engine to turn up only those Websites espousing a particular viewpoint, in which case a high bias value suggesting a weighted selection might be desired. Since there could be different interpre-tations of the views expressed in a Website, other users in this situation might be better served by the results of a search exhibiting a low bias value. 2.Bias,recallandprecision
The definition of bias used in this research is described briefly below. For more discussion of the defini-tion and its justification see Mowshowitz and Kawaguchi (2002a) ; for a detailed example of a bias compu-tation see Mowshowitz and Kawaguchi (2002b) .

The bias of search engine E is defined as the one minus the similarity between a vector representing the set of responses generated by E and a vector representing the response set of a collection C of search engines used to approximate an  X  X  X deal X  X  set of responses. In both cases the responses are elicited by a set of queries related to a given subject.

More precisely, suppose t queries q i  X  1 6 i 6 t  X  are to be processed by a given search engine. Let R note the response sequence of URLs generated for query q i collection C ; and let S i , j be the set of URLs included in R
S  X f u 1 ; u 2 ; ... ; u lij g where u k is the k th URL in the list retrieved by search engine E q and l i , j is the size of the list. For simplicity the subscript l
Now suppose the union of the tn sets S i , j consists of the set A  X  a the response sets listed in row major form. The number of times each URL occurs among the
R  X  1 6 k 6 nt  X  is tabulated by computing a tn  X  K matrix whose kl th element is 1 if S 0 otherwise. The sum P l of the l th column is the number of times URL a
R , ... , R tn . Without loss of generality, the URLs of A are given in non-increasing order of frequency. The plicity in describing the computation, E is assumed to be in C so that the response set of E is a subset of the response set of C .

The response vector for a particular search engine E is determined as follows. First the union V of re-sponse sets is computed. In this case there are t sets R i generated by E for query i  X  1 6 i 6 t  X  . Suppose B  X f b membershi pmatrix T i , j whose i , j th element is 1 if b the elements in column l  X  1 6 l 6 N  X  . The response vector for E is given by x  X  X  p
The components of x are ordered so as to correspond to those of X . For simplicity of presentation, we will use the notation X  X  X  X l  X  and x  X  X  x l  X  to represent the response vectors and assume that each is appropri-ately ordered with the requisite number K of components.
 The similarity s ( v , w ) of vectors v  X  X  v 1 ; ... ; v n well-known in information retrieval research, namely,
The bias of E with respect to the collection of engines C for queries q
Characterizing performance has become an important issue for researchers as well as search engine designers. Much of the research has focused on statistical analysis of Web coverage by search engines ( Gor-don &amp; Pathak, 1999 ; Lawrence &amp; Giles, 1998, 1999 ). Typically, search engines are compared according to the percentage of the indexable URLs on the Web that they actually cover ( Schwartz, 1998 ; Xie, Wang, &amp;
Goh, 1998 ). The research reported here aims to complement these studies by establishing procedures for assessing bias as a characteristic of search engine performance. In particular, measurement of bias is in-tended to complement the measurement of recall and precision, i.e., to help establish performance bench-marks for search engines.

The reference distribution (or norm), with which the performance of a particular search engine is to be compared, depends on the search engines in the collection used to define the norm. The selection of search engines comprising the norm should take account of changes in the search engine industry ( Sullivan, 2003a ). New companies may appear on the scene, existing ones may disappear or be absorbed by other companies, resulting in part from changes in business strategies.

Interdependencies among commercial search engines and lack of detailed information about the index-ing procedures and retrieval algorithms used by them should also be taken into account in defining the norm. Sullivan (2003c) has compiled a table showing that some search engines are  X  X  X owered X  X  by other en-gines. Google, for example, powers AOL, Yahoo and Netscape as well as Google itself. The meaning of the verb  X  X  X o power X  X  is not entirely clear, but it appears that there is considerable overlap in the sets of URLs retrieved by different engines powered by the same one. However, a good norm for bias measurement is one that approximates the universe of search responses seen by users. This suggests choosing the most popular engines. To approximate the distribution likely to be seen by a typical user, a sizable sample of commer-cially available search engines is used in the current study to define the norm for purposes of investigating properties of the bias measure.

Bias can be interpreted in very different ways. On the positive side, the skewing of results may mean that an engine picks up interesting items not found by the others; on the negative side, it may be that the engine simply fails to find the most interesting items retrieved by the majority. Bias values, like those computed for most performance measures, simply indicate the level of bias in the system; they cannot pinpoint a partic-ular source of bias. Further analysis, such as undertaken in determining the significance of recall and pre-cision is required to account for the computed values.
 Bias captures an aspect of a retrieval system that is not covered by the classical measures ( Becker &amp;
Hayes, 1963 ; Salton, 1968 ; Salton &amp; McGill, 1983 ) of recall and precision. The differences between bias and the classical measures stand out sharply in the extreme cases for recall and precision values. Two as-pects of bias measurement must be considered, namely, emphasis and prominence. The former can be ana-lyzed by treating the responses to a query as a set; the latter by interpreting the responses as a list of items in which order of presentation matters. There are four cases to examine.

Case 1. High recall, low precision . This case obtains when most of the relevant items in the given database have been retrieved but these items are overwhelmed by the inclusion of irrelevant ones. Taken as a set of items, the response could exhibit high or low bias depending on the norm. If most of the other engines mak-ing u pthe norm have lower recall values for the given query, bias would be high. If recall is also high for most of the other engines, bias would be low. Taking order of presentation into account, the bias value would be low if most of the relevant items are at the to pof the list; it would be high if the relevant ones were closer to the bottom of the list.

Case 2. High precision, low recall . This could occur when relatively few of the relevant items are retrieved from the database, but even fewer irrelevant ones are retrieved in response to the given query. Once again bias could be high or low when the responses are treated as a set. If most other engines retrieve a different subset of relevant items, bias could be high; it would be low if most of the engines retrieved the same subset of relevant items as the engine being measured. When order of presentation is considered, further variations in bias values become possible. That is to say, even if the engines defining the norm agree on the set of items, these items may be presented in different orders by the different engines.

Case 3. Low recall, low precision . When both recall and precision are low, relatively few relevant items have been retrieved from the database and of those retrieved most are irrelevant. Under these conditions bias could be high or low but is most likely to be in the mid-range and roughly the same for all the engines making up the norm. Low recall and precision may indicate a poorly constructed query and differences be-tween any two engines X  deviation from the norm (with or without taking account of presentation order) are likely to be small.

Case 4. High recall, high precision . If almost all the relevant items in the database are retrieved and very few irrelevant ones are included among the responses to a query, bias (treating the responses as a set) would be high unless most of the engines in the norm also score high on recall and precision. Taking account of presentation order complicates the picture. Even if most of the engines achieve high recall and precision, they may order the results differently, in which case the bias of a particular engine could be high or low.
Bias of engine E would be high if most of the other engines order their results in the same way but differ-ently from E  X  X  results; bias is likely to be low if no one presentation order is dominant.
The foregoing analysis shows how bias differs from recall and precision under various conditions. Even in the case of high recall and high precision, the bias value is illuminating. A high bias score for an engine that does well on the classical measures may indicate superior performance in retrieving useful items or it may reveal an idiosyncratic ordering of results relative to other engines. These two possible outcomes can be resolved by comparing the results produced by the different engines. Consistently high bias for different queries, coupled with high recall and precision that is not caused by idiosyncratic presentation, gives evi-dence of superior search engine performance. 3.Experimentsonthebiasmeasure
To facilitate empirical investigation, the authors have developed a specialized system that acts as a metasearch engine ( Glover, Lawrence, Birmingham, &amp; Giles, 1999 ; Liu, 1998 ) capable of automatically computing bias in search results. The system together with explanatory details are accessible at http:// wikiwiki.engr.ccny.cuny.edu/IntelSearch . A brief description of the system is also given in Mowshowitz and Kawaguchi (2002a) . At the time of writing this paper, the system could invoke 22 commercially avail-able search engines, namely, About, Ah-ha, AltaVista, AOLSearch, FastSearch, FindWhat, Galaxy, Goo-gle, InfoTiger, Jayde, LookSmart, Lycos, Msn, Netscape, OpenDirectory, Overture, Sprinks, Teoma,
TrueSearch, WiseNut, Xuppa, Yahoo. Almost all of these are among the listings of major search engines in Sullivan (2003b) and Search Engines.com (2003) .

The results of earlier experiments have been reported in Mowshowitz and Kawaguchi (2002b) . These experiments examined the influence on bias measurement of the following three variables: (i) subject domain; (ii) search engines; (iii) search terms employed to represent a given subject domain.

The subject area  X  X  X omputer science X  X , represented by the classification scheme adopted by the Communi-cations of the ACM for computing literature was used in these experiments. The research reported here is meant to check the validity and generalizability of these results in other subject domains.
The same variables are investigated further here. For consistency and simplicity, since choice of search terms is known to influence search results ( Spink, Jansen, Wolfram, &amp; Saracevic, 2002 ), domains and search terms have been chosen from tree-structured classification schemes. Two such schemes are used in the current experiments, namely,  X  X  X utline of the Law X  X  ( West Publishing, 2002 ), and the Library of Con-gress classification system ( http://www.loc.gov ). The experiments reported here use the same procedure as in Mowshowitz and Kawaguchi (2002b) .

Reliance on tree-structured classification schemes simplifies the identification of relatively disjoint sub-ject domains. Two domains (defined by terms in the classification) can be viewed as independent if their respective terms do not lie on the same path to the root of the classification tree. A term covers the nodes (subject areas) in the maximal subtree of which it is the root. For purposes of experimentation, at least three different domains should be chosen within each classification system. The choice of do-mains is dictated by independence and coverage (say at least half of the subject areas in the classifica-tions system).

Sixteen popular commercial search engines were selected to compute bias values and collectively to de-fine a norm in the current experiments. These search engines (i.e., About, Ah-ha, AltaVista, AOLSearch,
FastSearch, Google, LookSmart, Lycos, Msn, Netscape, Overture, Sprinks, Teoma, TrueSearch, WiseNut, and Yahoo) were chosen because they are either well-known or heavily used ( Sullivan, 2003b ). Popular commercial search engines are more likely to be well maintained and upgraded when necessary and to keep pace with the growing Web. In each search session (i.e., the processing of a set of search terms), the top 30
URLs returned by the 16 search engines for each of the terms were used to compute the bias values. Thus the norm for each bias calculation was based on 480 (not necessarily distinct) URLs.  X  X  X utline of the Law X  X  experiments . The  X  X  X utline of the Law X  X  classification scheme distinguishes seven main categories of law, namely, Persons, Property, Contracts, Torts, Crimes, Remedies, and Government.
These main categories are further subdivided. For example, the Persons category has five subdivisions, con-taining 86 terms in all. Five distinct categories have been chosen from  X  X  X utline of the Law X  X  and five terms randomly selected from each of the five categories. These are as follows:
Persons category: personal relations subdivision (adoption, husband and wife, labor relation, parent and child, master and servant).

Contracts category: particular classes of agreements subdivision (ailment, bonds, guaranty, joint adven-tures, pensions). Crimes category (adultery, kidnapping, perjury, robbery, suicide).

Remedies category: means and method of proof subdivision (acknowledgment, affidavit, oath, witness, evi-dence).

Government category: judicial powers and functions subdivision (security regulation, taxation, federal courts, judges, social security).

In the first experiment (examining bias across subject domains), 25 bias values were computed for each search engine, one value for each of five search terms per subject domain. Thus, 16 5  X  5 tables were con-structed whose rows correspond to search terms and whose columns correspond to subject domains. Table 1 shows the results of a (one-way) analysis of variance (one-way ANOVA) of bias values across the five subject areas. Two-way analysis of variance (two-way ANOVA) is unwarranted in this case since each set of five keywords corresponds to a given subject area, i.e., the keywords are subject-specific. The software system used for the statistical analysis in all the experiments reported here is Minitab13 (Minitab Inc.). The p -values (probability values) in the table measure the credibility of the null hypothesis, i.e., they indicate whether the sample could have been drawn from the population being tested given the assumption that the null hypothesis is true ( Wonnacott &amp; Wannacott, 1984 ). The null hypothesis is rejected if the computed p -value is less than or equal to the widely accepted figure of 5% (0.05) error.

The table shows that except for AltaVista , Fast , Google , Lycos ,and Yahoo the p -values are smaller than 0.05. With 0.05 as the rejection threshold, this means that the null hypothesis is rejected for all but these five search engines. Thus the computed bias values for 11 of the 16 search engines, namely, About , Ah-ha , AOL ,
LookSmart , MSN , Netscape , Sprinks , Overture , Teoma , TrueSearch , and WiseNut , exhibit sensitivity to the subject area being searched.

Table 2 shows the results of tests on the two other variables mentioned above, i.e.,  X  X  X earch engine X  X  and  X  X  X eyword selection X  X . Both of these tests used one-way analysis of variance. In the  X  X  X earch engine X  X  test, ex-actly one bias value was computed for each keyword. Similarly, in the  X  X  X eyword selection X  X  test exactly one value was computed for each search engine.

The  X  X  X earch engine X  X  test was designed to determine whether or not the bias measure discriminates be-tween search engines. Eighty bias values were computed for each subject domain, one value for each of five search terms per search engine. Thus, five 5  X  16 tables were constructed whose rows correspond to search terms and whose columns correspond to search engines.

The p -values in the  X  X  X earch engine X  X  row, all of which are 0.000, show that the null hypothesis must be rejected for each the subject areas shown. This means that the differences in bias values between search en-gines are statistically significant in each of the subject areas examined.

A two-way analysis of variance that includes as factors both engines and subjects was also applied to obtain stronger evidence against the null hypothesis (the Minitab result is shown in Table 3 ). This test was performed on a 5  X  16 design whose rows correspond to subjects and whose columns correspond to search engines. The cell corresponding to subject S and engine E in this design has five bias values, computed by engine E for each of the five keywords associated with subject S . The p -values for engine and subject factors are 0.000, which clearly indicates that the bias measures computed for search en-gines are statistically different and such difference of bias values also comes from the choice of subject areas.

The  X  X  X eyword selection X  X  test was designed to ascertain whether or not the bias measure discriminates between search terms associated with a given subject domain. Eighty bias values were computed for each subject domain, one value for each of 16 search engines per subject domain. Thus, five 16  X  5 tables were constructed whose rows correspond to search engines and whose columns correspond to search terms.
The p -values in the  X  X  X eyword selection X  X  row of Table 2 are all greater than 0.05 indicating that the null hypothesis is not rejected for any of the five. This means that for all of the five subject areas selected for this experiment from  X  X  X utline of the Law X  X , the distributions of the bias values computed for the 16 search engines exhibit no statistically significant differences, even though the search terms used to represent a given subject are different.

Rating search engines based on their  X  X ias performance X  may be especially useful for consumers. A crit-ical question to answer is which search engines tend to perform with relatively high or low bias. Fig. 2 (with x -axis representing collections of search terms and y -axis representing bias values between 0 and 1) illustrates the separation between search engines on bias values computed for the keywords in the legal subject area Crimes and Government. The successive bias values plotted in the figure for each search en-gine were computed using a growing set of search terms. This is indicated by the label  X  X  X umber of search terms X  X  on the x -axis. That is to say, the leftmost value is the bias of a search using the first search term, the second value is the bias computed for the first and second terms together, etc. One can see from inspection of the graphs that the bias measure does discriminate between search engines. For instance, Ah-ha X  X  results plotted as line segments in both graphs are consistently placed higher than those of Google.

Statistical analysis confirms this informal observation: for each of the Crimes and Government experi-ments, the differences between Ah-ha and Google are statistically significant. Furthermore, Fig. 3 shows the bias values computed using all the 25 terms applied to the search. Comparing the confidence intervals indi-cates that, limited to these term sets, the engines AOLSearch, FastSearch, Google, Netscape, Yahoo all ex-hibit a low bias profile, whereas engines Ah-ha, Msn, Sprinks, Teoma, and TrueSearch have a high bias profile that is independent of the legal category.

LC Subject  X  X  X hilosophy X  X  experiments . The second set of experiments analyzing the bias measure makes use of the  X  X  X ibrary of Congress Subject Headings in Philosophy X  X  ( http://www.loc.gov ). Subdo-mains and keywords for searches in the subject area  X  X  X hilosophy X  X  are selected from the headings used in the Library of Congress classification. Roughly speaking, the Library of Congress classification differ-entiates subject areas according to disciplines, i.e., humanities, social sciences, fine arts, natural sciences, and physical sciences. The scheme divides knowledge into 21 classes, with each class further broken down from the general to the specific subject. The class entitled  X  X  X eneral Philosophy X  X  was chosen, and the fol-lowing five subclasses (out of six possibilities) and search terms for each of these were selected for the experiment. Ancient philosophy : assyria babylonia, modern thought, plato, nature philosophy, hedonism
Medieval philosophy : arabic philosophy, aristotle influence, platonism, islamic philosophy, albertus magnus Renaissance philosophy : humanism, skepticism, montaigne, thomas more, galileo Modern philosophy : realism, comparative philosophy, conservatism, individualism, alienation General work : tradition, positivism, absurd, ideals, monism.

As in the  X  X  X utline of the Law X  X  experiments, five subject domains and five search terms for each domain have been used for the tests in the  X  X  X eneral Philosophy X  X  case, and the same 16 search engines were em-ployed. Thus the tables of computed bias values have the same dimensions and their respective rows and columns represent the same elements as in the first set of experiments; the Minitab13 (Minitab Inc.) system was used for the statistical analysis.

Table 4 shows the results of an analysis of variance of bias values across five subject areas. Only About  X  X  p -values is larger than 0.05, which means that except for About the computed bias values are dependent on the choice of subject areas.

The test for search engine differences showed the same pattern as in the  X  X  X utline of the Law X  X  case. All of the p -values in the  X  X  X earch engine X  X  row of Table 5 below are 0.000, which shows that the differences in bias values between search engines are statistically significant in each of the subject areas examined.
As in the previous set of experiments for the  X  X  X utline of the Law X  X  classification scheme, the result of a two-way analysis of variance indicates that bias differences are strongly related to differences in search en-gines and subject domains (Minitab results shown in Table 6 ).
 The influence of keywords on bias reinforces the result obtained for the  X  X  X utline of the Law X  X  scheme.
All of the p -values in the  X  X  X eyword selection X  X  row of Table 5 are greater than 0.05, which means that for any of the five philosophy subject areas selected for this experiment, the distributions of the bias values computed for the 16 search engines exhibit no statistically significant differences, even though the search terms used to represent a given subject are different.

Fig. 4 shows the bias values computed using all the 25 terms applied to the search. Although the distri-butions of the bias values are different, the relative order of their mean values is strikingly similar to the result obtained from the previous experiment using the  X  X  X utline of the Law X  X  classification scheme. AOL-Search, FastSearch, Google, Netscape, Yahoo all exhibit a low bias profile, whereas engines Ah-ha, Msn,
Sprinks, Teoma, and TrueSearch have high bias values. 4.Conclusion
These two sets of experiments and the earlier results of Mowshowitz and Kawaguchi (2002b) provide support for several important conclusions about bias measurement and search engine performance. First, the bias measure adopted by the authors is useful for comparing the performance of search engines. Some search engines tend to retrieve items that are found by others, and some search engines do not. This differ-ence in search performance can be determined with the aid of the bias measure as demonstrated.
Second, the distribution of bias values depends on the subject matter under consideration. The experi-ment reported in Mowshowitz and Kawaguchi (2002b) using the CACM classification scheme showed no statistically significant difference in bias values over the subdomains searched except for two of the fifteen search engines included in that analysis. But in the current experiments (using  X  X  X utline of the Law X  X  and the
Library of Congress classification) the majority of search engines differ significantly in bias from one subject to another, strongly suggesting that such variations across subjects are likely to be the norm.
In general, one search engine does not perform uniformly better than others in obtaining either popular or rare information on the Web. Moreover, within a given subject area (such as the ones examined, i.e., computer science, law, philosophy), choice of search terms relevant to that subject area does not account for much of the variance among the bias values. This is especially true when the items retrieved are the col-lective result of a series of searches with different but related search terms. Differentiating between engines on general performance calls for establishing a bias profile defined for a variety of well-chosen subjects.
Third, all of these observations point to the need for further research to characterize search engine per-formance, analyze its sensitivity to subject areas, and to determine the significance of bias under specified search conditions. Bias measurement is one of the critical tools needed to evaluate today X  X  search services; bias profiling as part of a broader benchmarking procedure would be a logical extension of the results re-ported here.

As argued above, assessing bias is an important problem since the Internet is already a major source of information for individuals and organizations, and its role as a source can be expected to increase in the future. Failure to take account of bias in the results of searches could be hazardous to Web users. The measures and procedures for assessing bias, described here, are intended as additions to the stock of tools for assessing the quality of information obtained on the new medium. In particular, practical measures can be implemented for use in detecting bias in Web search engines.
 References
