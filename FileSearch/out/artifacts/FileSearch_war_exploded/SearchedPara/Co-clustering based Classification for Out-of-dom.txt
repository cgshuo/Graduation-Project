 In many real world applications, labeled data are in short supply. It often happens that obtaining labeled data in a new domain is expensive and time consuming, while there may be plenty of labeled data from a related but different domain. Traditional machine learning is not able to cope well with learning across different domains. In this paper, we address this problem for a text-mining task, where the la-beled data are under one distribution in one domain known as in-domain data, while the unlabeled data are under a related but different domain known as out-of-domain data. Our general goal is to learn from the in-domain and apply the learned knowledge to out-of-domain. We propose a co-clustering based classification (CoCC) algorithm to tackle this problem. Co-clustering is used as a bridge to propa-gate the class structure and knowledge from the in-domain to the out-of-domain. We present theoretical and empirical analysis to show that our algorithm is able to produce high quality classification results, even when the distributions be-tween the two data are different. The experimental results show that our algorithm greatly improves the classification performance over the traditional learning algorithms. I.2.6 [ Learning ]: Induction Algorithms, Experimentation Classification, Co-clustering, Out-of-domain, Kullback-Leibler divergence
Document classification plays an important role in many text processing tasks, ranging from search engines to online KDD X 07, August 12-15, 2007, San Jose, California, USA. Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. advertisements. Traditional document classification algo-rithms rely on the availability of a large amount of labeled data. In practice, labeled data are often scarce, especially for learning tasks in new domains. When a task from a new domain comes, it may be the case that we have no labeled data at all. Labeling data for classification can be expen-sive and time consuming in general, but there may be plenty of labeled data from a related but different domain. This may be the case when the labeled data are out of date, but the new data are obtained from fast evolving information sources. Unfortunately, traditional machine learning fails to deal with this situation, since it requires that the labeled and unlabeled data be drawn from the same distribution. This raises a critical problem on how to learn from the labeled data from one domain, and then classify the documents from another domain accurately.

In this paper, we focus on the problem of classifying doc-uments across different domains. We have a labeled data set D i from one domain which is called in-domain ,andan-other data set D o from a related but different domain which is called out-of-domain . The latter is unlabeled and to be classified. D i and D o are drawn from different distributions, since they are from different domains. This may be the case when we consider two related Web directories, for example, when one directory contains documents about cars, and an-other about trucks. We assume that, the class labels in D and the labels to be predicted in D o are drawn from the same class-label set C . Furthermore, we assume that even though the two domains are different in distributions, they are similar in the sense that similar words describe simi-lar categories. In other words, the true probability of a class label given a word is very close in the two domains. This assumption is often true, as we will also demonstrate in our experiments, since D i and D o are related text do-mains, although some words in one domain may be missing in the other domain, which makes the estimated probability in the two domains to be quite different. Under such cir-cumstances, our objective is to accurately classify the out-of-domain documents in D o , by making use of the in-domain data D i and their labels.

We propose a novel co-clustering based classification al-gorithm to solve this problem, as briefly shown in Figure 1. First, the in-domain data D i provide the class structure, which defines the classification task, by propagating label information. Then, co-clustering [9] is extended for out-of-domain data D o to obtain out-of-domain document and word clusters, as the step 2 in Figure 1. Our key extension Figure 1: The model of our co-clustering based clas-sification algorithm to co-clustering is that class labels in D i can constrain the word clusters, which is shared among the two domains, as the step 1 in Figure 1. This allows each out-of-domain clus-ter to be mapped to a corresponding class label based on their correlation with the document categories in D i ,com-pleting our classification task. A key intuition of our work is that even though the two domains may be under different distributions, we are able to identify a common part between them. In our work, this common part is the common words. Class information and knowledge passes through common words from the in-domain to the out-of-domain. Moreover, the word clustering part of co-clustering can even enrich the common words by drawing together seemingly unrelated words.

In this paper, we define a unified information theoretic formulation for the above learning task. The objective func-tion for building the co-clustering based categorization is designed to minimize a loss function in mutual information between out-of-domain documents and words, and between words and class labels in the in-domain data set, simulta-neously. As a result, the class category knowledge provided by in-domain data D i is used as a constraint to enhance the classification of out-of-domain documents based on co-clustering.

To show that our co-clustering based classification algo-rithm works well, we carry out theoretical analysis to show that our algorithm increasingly optimizes the objective func-tion as our algorithm iterates to completion, by converging quickly to a locally optimal co-clustering result. We sup-plement the theoretical study with extensive experiments, where we demonstrate that our algorithm is effective in mak-ing the predictions for out-of-domain documents.

The rest of the paper is organized as follows. In Section 2, we discuss the related works. In Section 3, some prelim-inary concepts from information theory is introduced. The problem formulation is presented in Section 4. Section 5 pro-poses our co-clustering based classification algorithm. The empirical analysis is presented in Section 6. Section 7 con-cludes the whole paper and give some future works. Some detailed proofs of the theoretical conclusions are given in the Appendix.
In this section, we review several prior works mostly re-lated to our work, including traditional classification, multi-task and multi-domain learning, and semi-supervised clus-tering.
The traditional classification formulation assumes that the class labels are given for training data under the same distribution as the test data. Two schemes are generally considered, where one is supervised classification and the other is semi-supervised classification . Supervised classifica-tion focuses on the case where the labeled data are sufficient, and where the learning objective is to estimate a function that maps examples to class labels using the labeled training instances. Naive Bayes Classifiers [20] and Support Vector Machines [3] are known as two of the most effective methods for document classification.

Semi-supervised classification [28] addresses the problem that the labeled data are too few to build a good classifier. It makes use of a large amount of unlabeled data, together with a small amount of the labeled data to enhance the classifiers. Many semi-supervised learning techniques have been proposed, e.g., co-training [2], EM-based methods [23], cluster-based methods [27], transductive learning [15] etc.
Both supervised and semi-supervised classification assume that the distributions of the labeled and unlabeled data should be identical. However, in our problem, the labeled data are from in-domain, while the unlabeled data are from out-of-domain. The distributions of the labeled and un-labeled data are different from each other. This violates the basic assumption of traditional supervised and semi-supervised classification algorithms.
Another related learning research area is multi-task learn-ing, where the domain-specific information in related tasks (training and test data sets) are jointly trained in a way that can benefit each other [4]. A shared representation is exploited while the extra tasks can be used as an inductive bias during learning. By defining the common knowledge carefully, it is possible to allow the knowledge learned from each task to help the learning of other tasks.

In contrast to multi-task learning, our problem should be considered as single-task learning, since the class labels for in-domain and out-of-domain are from the same class label set. However, our classification problem crosses different domains. This problem can be referred to as multi-domain learning ,or cross-domain learning . [25] studied on cross-domain learning in neural network, while we focus on the cross-domain text classification. [7] studied a similar prob-lem where they investigated how to learn a general model from the in-domain and out-of-domain labeled data to train a statistical classifier for a natural language Mention Type Classification and Tagging problem. In contrast, in our work, we assume that the out-of-domain data are completely unlabeled.
Semi-supervised clustering [11] builds clusters under some additional constraints provided by a few labeled data, in the form of must-links (two examples must in the same cluster) and cannot-links (two examples cannot in the same cluster). It finds a balance between satisfying these constraints and optimizing the original clustering objective function. Several semi-supervised clustering algorithms have been proposed, including [1, 5, 12, 10].

Semi-supervised clustering provides a good method to make use of a few labeled data in clustering. However, the must-link and cannot-link constraints must be available for clus-tering to work. When the labeled data are few, the same-distribution requirement can be relaxed. This fact makes it feasible to extend semi-supervised clustering for different distribution data sets.

In this paper, we consider a co-clustering based classi-fication algorithm which extends the information theoretic co-clustering approach of [9], where constraints given by in-domain data is added to the word clusters [8] to provide a class structure and partial categorization knowledge. Our algorithm is essentially a classification algorithm using the co-clustering technique. It will be shown theoretically and empirically that our algorithm works well for classifying out-of-domain documents.

In addition to building clusters, we are interested in us-ing the class-label knowledge gained from in-domain data to help classify out-of-domain problems, which is not solvable by traditional semi-supervised clustering algorithms alone. As we will present later, our algorithm adds constraints on the word clusters to help assign labels to co-clustering re-sults. The class structures on word clusters are passed on from the in-domain data to the out-of-domain data, which makes classification possible.
In this section, we introduce some preliminary concepts from information theory that will be used frequently in this paper. For more details, please refer to [6]
Let X and Y be random variable sets with a joint distri-bution p ( X, Y ) and marginal distributions p ( X )and p ( Y ). The mutual information I ( X ; Y ) is defined as The mutual information is a measure of the dependency be-tween random variables. It is always non-negative, and it is zero if and only if the variables are statistically independent. Higher mutual information values indicate more certainty that one random variable depends on another.

The use of mutual information can also be motivated us-ing the Kullback-Leibler (KL) divergence or relative entropy measures, defined for two probability mass functions p ( x ) and q ( x ), KL-divergence can be considered as a kind of a distance between the two probability distributions, although it is not a real distance measure because it is not symmetric. Besides, KL-divergence is always non-negative [6].
Let D i be the set of in-domain data with labels, D o be the set of out-of-domain data without labels. D i and D o can also be considered as random variable sets that take the in-domain and out-of-domain instances as random variables, respectively. From the in-domain data D i , we are able to get a set of class labels C which is the class structure informa-tion. The labels (which are unknown and to be predicted) of out-of-domain data D o are also drawn from the same label set C .From D i and D o ,thewordset W can be obtained from the word occurrences in D i and D o .

In our approach, we take co-clustering as a bridge to prop-agate the knowledge from the in-domain to out-of-domain. Co-clustering on out-of-domain data aims to simultaneously cluster the out-of-domain documents D o and words W into |C| document clusters and k word clusters, respectively. Let  X  D o denote the out-of-domain document clustering, and  X  W denote the word clustering, where |  X  W| = k . The docu-ment cluster-partition function C D o and the word cluster-partition function C W can be defined as where  X  d represents the document cluster that d belongs to and  X  w represents the word cluster that w belongs to. Then, the co-clustering can be represented by ( C D o ,C W (  X  D o ,  X  W ).

In order to measure the quality of a co-clustering, we de-fine the loss for co-clustering in mutual information as This form of loss function is the same as that used in [9]. From Equation (5), we know that co-clustering aims to mini-mize the loss in mutual information between documents and words before and after clustering.
 Since our problem is to classify out-of-domain documents D , a key point is to add the knowledge about classes to the co-clustering process, which is extracted from in-domain data D i . In this paper, we use the relationship between word clusters and class labels to apply class label information to the co-clustering. We define the loss in mutual information for a word clustering as This form of loss function is the same as that used in [8]. Equation (6) indicates that a word clustering should min-imize the loss in mutual information between class labels C and words W before and after clustering, for in-domain data.

Integrating Equations (5) and (6), the loss function for co-clustering based classification can be obtained: where  X  is a trade-off parameter that balances the effect to word clusters from co-clustering (see Equation (5)) and word clustering (see Equation (6)). The objective is to find a co-clustering that minimizes the function value of Equation (7).

With Equation (7), we can now describe our process of classifying out-of-domain documents through co-clustering. Since I ( D o ; W )and I ( C ; W ) are fixed, minimizing Equation (7) equivalents maximizing I (  X  D o ;  X  W )and I ( C ;  X  W definition of mutual information in Section 3, in order to maximize I (  X  D o ;  X  W )and I ( C ;  X  W ),  X  D o should depend on and  X  W should depend on C . Under our problem assumption,  X  D o would depend on C , which indicates that the clusters in  X  D o should be rely on the classes in C .Wecanletthenum-ber of document clusters be the same as the number of class labels to enable a 1-1 mapping between them. That is, we let |  X  D o | = |C| , and build a mapping between  X  D o and on the dependence between each  X  d  X   X  D o and each c  X  X  . Then, using the co-clustering based classification approach that optimizes Equation (7), the documents in D o will be assigned to their corresponding classes according to cluster membership, which enables our co-clustering based classifi-cation approach.

In the rest of this section, we will rewrite the objective function in Equation (7) into another form that is repre-sented by KL-divergence. Before rewriting the objective function, let us first define some probability mass functions.
Definition 1. Let f ( D o , W ) denote the joint probability distribution of D o and W .Thatis  X  f (
D o , W ) denotes the joint probability distribution of D and W under co-clustering (  X  D o ,  X  W )that where d  X   X  d and w  X   X  w ,where  X  d is a document cluster, and  X  w is a word cluster, respectively.

Similarly, g ( C , W ) denotes the joint probability distribu-tion of C and W that  X  g ( C , W ) denotes the joint probability distribution of C and W under the word clustering  X  W that where w  X   X  w .

The marginal and conditional probability distributions for f ,  X  f , g and  X  g can be defined naturally. For example,
Lemma 1. For a fixed co-clustering (  X  D o ,  X  W ) ,wecanwrite the loss in mutual information as I ( D o ; W )  X  I (  X  D o ;  X  W )+  X   X  ( I ( C ; W )  X  I ( where D (  X || X  ) is defined in Equation (2).
 Proof.

Equation (13) shows that the loss in mutual information in the objective function equals to the sum of KL-divergence between f and  X  f and KL-divergence between g and  X  g .To minimize the objective function in Equation (7), we need only to minimize the KL-divergence between f and  X  f ,and the KL-divergence between g and  X  g .
We now describe the co-clustering based classification al-gorithm for classifying the out-of-domain data, which min-imizes the objective function in Equation (13). The objec-tive function given in Equation (13) is a multi-part function which is hard to be optimized. Therefore, we should find a way to make the optimization easier. Lemmas 2 and 3 show an alternative approach, which allows us to iteratively reduce the divergence values.
 Lemma 2.
 Proof.
 Note that Equation (21) is based on f ( d, w )= p (  X  d,  X  w ) p ( d |  X  d ) p ( w |  X  w )= p ( d ) The last equality follows by p ( d )= f ( d )and p ( X  w |  X   X  f ( w |  X  d ).
 Using the same argument, we can prove that
Lemma 2 tells us that minimizing D ( f ( W| d ) ||  X  corresponding to a single document d can reduce the global objective function value given in Equation (13). The same conclusion can be derived for minimizing D ( f ( D o | w ) corresponding to a single word w .
 Lemma 3.
 D ( g ( C , W ) ||  X  g ( C , W )) = The proof of Lemma 3 is omitted, and it can be derived using the similar argument to Lemma 2. From Lemma 3, wecanobtainthesimilarconclusionwiththatinLemma2.

According to Lemmas 2 and 3, our co-clustering based classification algorithm, called CoCC, is derived. This al-gorithms iteratively searches a co-clustering for the out-of-domain data, and then assigns class labels to the document clusters to complete the classification task.
 Algorithm 1 The Co-clustering based Classification (CoCC) Algorithm Input: A labeled in-domain data set D i ; an unlabeled out-of-domain data set D o ;aset C of all the class labels; a set W of all the word features; initial co-clustering ( C (0) the number of iterations T .
 Initialize the joint probability distribution f ,  X  f , g and  X  g based on Equations (8), (9), (10) and (11), respectively. For t  X  1 , 3 , 5 ,..., 2 T +1 1: Compute the document cluster: 2: Update the probability distribution  X  f ( t ) based on C 3: Compute the word cluster: 4: Update the probability distribution  X  g ( t +1) based on End For Output: the partition functions C ( T ) D o and C ( T ) W
As shown in Algorithm 1, in each iteration, the algorithm chooses the best document cluster  X  d for each d to minimize we discussed above, this can reduce the global objective function value in Equation (13). Then, in each iteration, the algorithm chooses the best word cluster  X  w to minimize the function D ( f ( D o | w ) ||  X  f ( D o |  X  w )) and D ( g ( simultaneously (see Equation (29)). This can reduce the global objective function value too. We will prove the mono-tonically decreasing property of the objective function in the following theorem:
Theorem 4. The algorithm CoCC in Algorithm 1 mono-tonically decreases the objective function in Lemma 1. D ( f ( D o , W ) ||  X  f ( t ) ( D o , W )) +  X   X  D ( g ( D ( f ( D o , W ) ||  X  f ( t +1) ( D o , W )) +  X   X  D ( g ( The detailed proof of Theorem 4 is given in the Appendix. Note that, although the algorithm is able to minimize the objective function value in Equation (13), it is only able to find a locally minimal one. Finding the global optimal co-clustering is NP-hard.

Corollary 5. Algorithm 1 converges in a finite number of iterations.

Proof. Since the total number of co-clusterings is finite, the corollary can be derived straightforward from Theorem 4.

Regarding the computational complexity, suppose the to-tal number of document-word co-occurrences in D o is N .For each iteration, updating C D o takes O ( |C| X  N ), while updat-ing C W takes O (( |C| + |  X  W| )  X  N ). The number of iterations is T . Therefore, the time complexity of our co-clustering based iments, it is shown that T = 10 is enough for convergence. Considering space complexity, our algorithm need to store all the document-word co-occurrences and their correspond-ing probabilities. Thus, the space complexity is O ( N ).
In order to evaluate the properties of our algorithm, we perform the experiments in this section. In the experiments, we focus on the binary classification. Moreover, the data sets are all balanced between the class labels. Note that the binary classifiers can be easily extended for multiple class.
We conducted experiments on three data sets, 20 News-groups [18], SRAA [21] and Reuters-21578 [19]. In order to make the data set satisfy our problem setting, we split the original data in a way to make the labeled and unlabeled data drawn from related but different domains, as follows.
The 20 Newsgroups [18] is a text collection of approxi-mately 20,000 newsgroup documents, partitioned across 20 different newsgroups nearly evenly. We generated six differ-ent data sets for evaluating cross-domain classification algo-rithms. For each data set, two top categories 1 are chosen,
Three top categories, misc , soc and alt are removed, be-cause they are too small.
 Table 1: The description of 20 Newsgroups data sets for cross-domain classification.
 Table 2: The description of SRAA data sets for cross-domain classification. one as positive and the other as negative. Then, we split the data based on sub-categories. Different sub-categories can be considered as different domains, while the task is defined as top category classification. The splitting strat-egy ensures the domains of labeled and unlabeled data re-lated, since they are under the same top categories. Besides, the domains are also ensured to be different, since they are drawn from different sub-categories. Table 1 shows how we generated the data sets in our experiments.
SRAA [21] is a Simulated/Real/Aviation/Auto UseNet data set for document classification. 73,218 UseNet articles are collected from four discussion groups about simulated autos ( sim-auto ), simulated aviation ( sim-aviation ), real autos ( real-auto ) and real aviation ( real-aviation ).
Consider the task that aims to predict labels of instances between real and simulated . Weusethedocumentsin real-auto and sim-auto as in-domain data, while real-aviation and sim-aviation as out-of-domain data. Then, the data set real vs simulated is generated as shown in Table 2. As a result, all the data in the in-domain data set are about autos, while all the data in the out-of-domain set are about aviation. The auto vs aviation data set is generated in the similar way as shown in Table 2.
Reuters-21578 [19] is one of the most famous test collec-tions for evaluation of automatic text categorization tech-niques. It contains 5 top categories. Among these cate-gories, orgs , people and places are three big ones. For the category places , we removed all the documents about the USA to make the three categories nearly even, because more than a half of the documents in the corpus are in the USA sub-categories. Reuters-21578 corpus also has hier-Table 3: Description of the data sets for cross-domain text classification, including errors given by SVM.  X  D i  X  D o  X  means training on D i and testing on D ; X  D o  X  X V X  means 10-fold cross-validation on D o . The performances are in test error rate. archical structure. We generated three data sets orgs vs people , orgs vs places and people vs places for cross-domain classification in a similar way as what we have done on the 20 Newsgroups and SRAA corpora. Since there are too many sub-categories, we can not list the detailed de-scription here.
Table 3 shows the description of all the data sets. The first three columns of the table show the statistical properties of the data sets. The first two data sets are from SRAA corpus. The next six are generated using 20 Newsgroups data set. The last three are from Reuters-21578 test collection. KL-divergence values calculated by D ( D i ||D o )onallthedata set are presented in the second column in the table, sorted in decreasing order from top down. It can be seen that the KL-divergence values for all the data sets are much larger than the identical-distribution case which has a KL value of nearly zero. The next column titled  X  X ocuments X  shows the size of the data sets and the vocabulary set used. Under the column titled  X  X VM X , we show two groups of classification results in two sub-columns. First,  X  D i  X  D o  X  denotes the test error rate obtained when a classifier is trained based on the in-domain data set D i and applied to the out-of-domain data set D o . The column titled  X  D o  X  X V X  denotes the best-case obtained by the corresponding classifier, where the best case is to conduct a 10-fold cross-validation on the out-of-domain data set D o using that classifier. Note in obtaining the best case for each classifier, the training part is labeled data from D o and the test part is also from D o , according to different folds, which gives the best possibly result for that classifier. It can be found that the test error rates, given by SVM, in thecaseof X  D i  X  D o  X  is much worse than those in the case of  X 
D o  X  X V X . This indicates that our data sets are not suitable for traditional supervised classification algorithms.
Figure 2 shows the document-word co-occurrence distri-bution on the auto vs aviation data set. In this figure, documents 1 to 8000 are from D i , while documents 8001 to 16000 are from D o . The documents are ordered first by their domains ( D i or D o ), and second by their categories (positive or negative). The words are sorted by n + ( w ) /n  X  ( w ), where n ( w )and n  X  ( w ) represent the number of word positions w appears in positive and negative documents, respectively. From Figure 2, it can be found that the distributions of in-domain and out-of-domain data are somewhat different, however the figure also shows large commonness exists be-tween the two domains. In our algorithm, the class infor-Figure 2: Document-word co-occurrence distribu-tion on the auto vs aviation data set mation and knowledge passes through these common infor-mation from the in-domain to the out-of-domain. Moreover, the word clustering part in the co-clustering can even enrich the common part to further propagate knowledge between different domains.
Since our co-clustering based classification algorithm (CoCC) is a classification algorithm essentially, we should compare CoCC with the existing classification methods to show the advantages of our algorithm. We take the supervised clas-sification algorithms to be the baseline methods. Naive Bayes Classifier (NBC) [20] and Support Vector Machines (SVM) [3] are introduced in the experiments. Transductive Support Vector Machines (TSVM) [15] and Spectral Graph Transducer (SGT) are also introduced as comparison semi-supervised learning methods. Data preprocessing has been applied to the raw data. First, we converted all the letters in the text to lower case, and stemmed the words using the Porter stemmer [24]. Be-sides, stop words were removed. We used a simple feature selection method, Document Frequency (DF) Thresholding [26], to cut down the number of features, and speed up the classification. Based on [26], DF thresholding, which has comparable performance with Information Gain (IG) or CHI, is suggested since it is simplest with lowest cost in computation. In our experiments, we set the DF threshold to 3.

TF-IDF is used for feature weighting when training Sup-port Vector Machines (SVM) [3, 15] and Spectral Graph Transducer (SGT) [16]. TF is used for feature weighting when training Naive Bayes Classifier (NBC) [20] and our co-clustering based classification (CoCC) algorithm.
SVM and TSVM are implemented by SVM light [14] with default parameters (linear kernel). For more details about SVM and TSVM, please refer to [3] and [15]. SGT is imple-mented by SGT light [13] with default parameters ( k =50, d =80and c = 100). For more details about SGT, please refer to [16].

The initialization of CoCC is important, since different initialization will lead to different local optimal co-clustering. In the experiments, we assign the initial document cluster-ing by NBC. NBC is trained using D i , and then predicts the labels of D o . Then, the documents in D o are assigned to the clusters based on their prediction labels. The ini-tial word clustering is derived by CLUTO [17] with default parameters.

Another important issue to be mentioned is that, in order to avoid infinity values for D ( f ( W| d ) ||  X  f ( W|  X  (28), and D ( f ( D o | w ) ||  X  f ( D o |  X  w )) and D ( g ( Equation (29), Laplacian smoothing [22] is applied to esti-mate the probabilities.

Finally, after co-clustering, we assign each document d to the class c by Equation (31) indicates that we always assign the document d to the class c which is most relevant to  X  d .Notethat, our objective function in Equation (13) ensures that C and  X  D o are highly dependent, and hence the assignment makes sense.
The performance of the proposed methods was evaluated by test error rate. Let C be the function which maps from document d to its true class label c = C ( d ), and F be the function which maps from document d to its prediction label c = F ( d ) given by the classifiers. Test error rate is defined as Table 4 presents the performance on each data set given by NBC, SVM, TSVM, SGT and our algorithm CoCC in test error rate. The implementation details of the algorithms have already been presented in the last subsection, and the parameter setting for CoCC will be given later.

From the table, we can see that CoCC always give the best performances. Besides, it seems that NBC is better for clas-sifying out-of-domain documents than SVM, although SVM is known as a stronger classifier than NBC. In our opinion, SVM is a relatively strong classifier for traditional classifica-tion problem, compared with NBC, but NBC is more general for out-of-domain data. TSVM and SGT give better perfor-mance than NBC and SVM on most data sets, since they utilize the information given by the unlabeled data in D o although their basic assumption is violated. However, they still fail sometimes, e.g. TSVM on the orgs vs places data set, and SGT on the orgs vs people data set.

Figure 3 presents the test error rates on different sizes of the labeled in-domain data . The labeled data are randomly chosen from D i in the auto vs aviation data set by differ-ent proportion. It can be seen that CoCC gives comparable performance even when there is only 10% of the in-domain data, while NBC gets quickly worse when the proportion of in-domain data is less than 20%.
 Table 4: Test error rate for each classifier on each data set Figure 3: Test error rate curve on different size of auto vs aviation data set
Since our algorithm CoCC is an iterative algorithm, an important issue for CoCC is the convergence property. The-orem 4 has already proven the convergence of CoCC the-oretically. Now, let us empirically show the convergence property of CoCC. Figure 4 shows the test error rate curves as functions for each iteration on three data sets, real vs simulated , rec vs sci and orgs vs places . From the fig-ure, it can be seen that CoCC always achieves almost conver-gence points within 5 iterations. This indicates that CoCC converges very fast. We believe that 10 iterations is enough for CoCC.
There are two parameters in our algorithm. One is the trade-off parameter  X  in Equation (7); the other is the num-Figure 4: Test error rate curves after each iteration Figure 6: Test error rate curve on different number of word clusters ber of word clusters. We perform the parameter tuning on the auto vs aviation data set. When tuning the param-eter  X  , we tried three different numbers of word clusters  X  16, 64 and 128. The error rate for each  X  from 0.003125 to 8 is given in Figure 5. According to the figure, we set  X  to 0.125 in our experiments. When tuning the number of word clusters, we tried different  X  s which are 1, 0.5 and 0.25. The error rate for each number of word clusters from 2 to 512 is given in Figure 6. According to the figure, we set the number of word clusters to 128 in our experiments. We test how the difference in the distribution between D i and D o influence the performance of CoCC. The KL-divergence and relative improvements by test error rate re-duction between CoCC and NBC, and between CoCC and SVM are calculated for each data set in Figure 7. The data sets have been sorted by KL-divergence in decreasing order from left to right. In this figure, when the KL-divergence is small, the relative improvement is not much significant. The improvement becomes great when KL-divergence values be-come large, in general. But, there are still some exceptional points.
In this paper, we presented a novel co-clustering-based classification algorithm (CoCC) to classify out-of-domain documents. The class structure passes through word clus-ters from the in-domain data to the out-of-domain data. Additional class-label information given by the in-domain data is extracted and used for labeling the word clusters for Figure 7: Test error rate reductions against NBC and SVM on all data sets sorted by KL divergence in descending order from left to right out-of-domain documents. We formulate the problem under an information-theoretic scheme, and designed an objective function to minimize the loss in mutual information before and after co-clustering based categorization. Our theory shows that CoCC can monotonically reduce the objective function value. The empirically results also support our theoretical analysis. In our experiment, it is shown that CoCC greatly outperforms traditional supervised and semi-supervised classification algorithms when classifying out-of-domain documents.

In CoCC, the number of word clusters are quite large (128 clusters in the experiments) to obtain good performance. Since the time complexity of CoCC depends on the num-ber of word clusters, it can inefficient. In the future, we will try to speed up the algorithm to make it more scalable for large data set. Moreover, the parameters in CoCC are tuned manually. In the future, we will investigate automatic approaches to tune the parameters. [1] S. Basu, A. Banerjee, and R. J. Mooney.
 [2] A. Blum and T. Mitchell. Combining labeled and [3] B. E. Boser, I. Guyon, and V. Vapnik. A training [4] R. Caruana. Multitask learning. Machine Learning , [5] D. Cohn, R. Caruana, and A. McCallum.
 [6] T.M.CoverandJ.A.Thomas. Elements of [7] H. Daum  X  e III and D. Marcu. Domain adaptation for [8] I. S. Dhillon, S. Mallela, and R. Kumar. Enhanced [9] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [10] J. Gao, P.-N. Tan, and H. Cheng. Semi-supervised [11] N. Grira, M. Crucianu, and N. Boujemaa.
 [12] X. Ji, W. Xu, and S. Zhu. Document clustering with [13] T. Joachims. SGT light . http://sgt.joachims.org/ . [14] T. Joachims. SVM light . [15] T. Joachims. Transductive inference for text [16] T. Joachims. Transductive learning via spectral graph [17] G. Karypis. Cluto  X  software for clustering [18] K. Lang. Newsweeder: Learning to filter netnews. In [19] D. D. Lewis. Reuters-21578 test collection. [20] D. D. Lewis. Representation and learning in [21] A. K. McCallum. Simulated/real/aviation/auto usenet [22] T. M. Mitchell. Machine Learning , chapter 6, page [23] K. Nigam, A. K. McCallum, S. Thrun, and [24] M. F. Porter. An algorithm for suffix stripping. [25] S. Swarup and S. R. Ray. Cross-domain knowledge [26] Y. Yang and J. O. Pedersen. A comparative study on [27] H.-J. Zeng, X.-H. Wang, Z. Chen, H. Lu, and W.-Y. [28] X. Zhu. Semi-supervised learning literature survey.
We split Theorem 4 into two lemmas, Lemma 6 and Lemma 7. Lemma 6 proves Theorem 4 for t =1 , 3 ,..., 2 T +1. Lemma 7 proves Theorem 4 for t =2 , 4 ,..., 2 T +2. Com-bine the two lemmas, Theorem 4 is proved.
 Lemma 6. Theorem 4 holds when t =1 , 3 ,..., 2 T +1 .
Proof. For t =1 , 3 ,..., 2 T +1, since C ( t ) W = C ( t need only to prove
D ( f ( D o , W ) ||  X  f ( t ) ( D o , W ))  X  D ( f ( D o
D ( f ( D o , W ) ||  X  f ( t ) ( D o , W )) =  X  = =  X  = D ( f ( D o , W ) ||  X  f ( t +1) ( D o , W )) (38) Note: Equation (33) is based on Lemma 2; Equation (34) is based on Equation (28); Equation (35) follows by re-arranging the sum and C ( t ) W = C ( t +1) W ; Equation (36) fol-lows by rearranging the sum and total probability theorem  X  f ) ( w |  X  w )=  X  f ( t +1) ( w |  X  w )and Equation (41) follows by the non-negativity of the Kullback-Leibler divergence. Note that X = Lemma 7. Theorem 4 holds when t =2 , 4 ,..., 2 T +2 . Proof. For t =2 , 4 ,..., 2 T +2, D ( f ( D o , W ) ||  X  f ( t ) ( D o , W )) +  X   X  D ( g ( =  X  Equation (44) is based on Equation (29). Using the same argument as Lemma 6, we can prove this lemma.

