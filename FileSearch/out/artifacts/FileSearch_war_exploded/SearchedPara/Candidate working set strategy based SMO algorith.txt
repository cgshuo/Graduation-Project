 1. Introduction
Support vector machine (SVM), which was developed from the statistic learning theory ( Cortes &amp; Vapnik, 1995; Vapnik, effectively improve SVMs performance is still an ongoing research issue. There are many approaches proposed lately, such as large optimization problem into lots of small-scale ones. SVMlight (Joachims, 1998a, 1998b) and Libsvm (Chen, Fan, &amp; or SMO, utilize the steepest feasible descent approach for the working set selection, and introduce the kernel caching and shrinking strategies to accelerate the training speed for SVM. But it costs much time in working set selection for decompo-the current working set, which contribute to minimize the objective function in the current step.

In the iterating process of training SVM, the samples, that violate the Karush-Kuhn-Tucker condition (KKT) most, could contribute to descending the objective function. For example, the maximal violating pair is selected as the working set iteration.

Based on the idea mentioned above, this paper proposes a new strategy for selecting the working set applied in SMO. The strategy can improve the efficiency of the kernel cache and reduce the computational cost related to the working set selec-can reduce the computing cost greatly, especially for the problems with large-scale samples.

This paper is organized as follows. In Section 2, we analyzed the training SVM process and the existing working set selec-tion methods were presented in Section 3. In Section 4, our new method is described in details. Experimental results and discussion are presented in Section 5. Finally, concluding remarks are given in Section 6. 2. SVM and its training process
Unlike traditional methods (such as neural networks) which minimize the empirical training error only, SVM aims at min-imizing an upper bound of the generalization error by maximizing the margin between the separating hyper-plane and the data (Cortes &amp; Vapnik, 1995; Vapnik, 1995).
 in a user-selected regularization parameter C &gt; 0. The mathematical descriptions are given bellow.
In the condition of satisfying: The classification problem is equivalent to solving the following quadratic programming problem: below. Factor C in (3) is a penalty parameter used to penalize those misclassified samples.

So the simple SVM (formula (3)) dual optimization problem can be rewritten as the following: fined by a 1  X  a 2 = c . Therefore, the solution range of a 1 , a 2 can be deduced as follows: In which a old 1 , a old 2 are the values of a 1 , a 2 in the last iterating step.
 Based on the above conditions, the objective function can be rewritten as follows: in which E i = f old ( x i ) y i is an training error; a new 2 are the values of a 2 in the current iterating step. In the condition of formula (5) and (6) , the solution of a 1 , a 2 is as follows:
Based on the above theory, Platt (1999) proposed SMO algorithm which is the best method solving the SVM large-scale QP problem until now. It decomposes the whole QP problem into a series of smallest-scale QP, which deals with only two La-step in SMO is how to determine one pair of optimizing Lagrange multipliers. 3. Existing working set selections
Joachims (1998a, 1998b) employed feasible direction strategy to select working set in SVMlight software. Chang and Lin multipliers determined by feasible direction method are the most KKT conditions violating pair. But Joachims methods for steps dominate the whole training process in the traditional SMO algorithm, and contributed less to the final results.
Motivated from SVMlight and SMO algorithm, Chang and Li (2001) proposed Libsvm algorithm, which employed steepest feasible descent strategy for selecting a working set. This strategy speeds up the SVM training processing to some extends, ciency of the kernel cache in SMO, some other researchers also proposed modified algorithm for SMO. Flake and Lawrence (2002) in his efficient SVM regression training algorithm proposed a new cache usage strategy which employed query, insert, ciency of the whole SMO algorithm fundamentally.
 Consideringthe conflictbetweenfeasibledirectionstrategyandkernelcacheunder certaincircumstance, Li JianminandLin Fuzong(2002) proposedanewstrategytopromotethehittingaccuracyoftheworstviolatingpairinthecache.Intheinnerloop, allthevariableswhosekerneloutputwiththefirstvariableselectedintheouterloopiscachedareallscanned.TheSMOwithLi X  X  posedanothernewstrategy,whichpreferentiallyselectthesampleswhosekerneloutputshasbeenheldintheCacheasawork-ing set. Although this method improves the efficiency of the kernel cache usage, the convergence of SVM training to the objective function optimal is slow without considering computing cost in the working set selection by this strategy.
Except that the methods above employ first-order information, Fan, Chen, and Lin (2005) and Glasmachers and Igel set. Simon (2007) presented the computational complexity of working set selection which gives us a better theoretical understanding of the working set selection.

In order to solve shortages of current working set selection methods, candidate working set strategy based SMO algorithm the subset in SVMlight. We needn X  X  to optimize this subset entirely because the left training steps for optimizing subsets algorithm can improve the efficiency of Cache. Moreover, it can reduce the computing cost for working set selections. 4. SMO algorithm based on candidate working set will make much more progress towards the minimum of objective function (Joachims, 1998a, 1998b). Feasible direction methods are employed by many researchers (Joachims, 1998a, 1998b; Fan et al., 2005 ).

This approach leads to the following optimization problem:
In SMO, we should recompute g t before selecting the working set by the feasible direction strategy. In order to reduce the computing cost, Chen et al. (2006) has proposed a formula for updating g t , optimizing the current two Lagrange multipliers in the working set.

These great violating samples could do much more contribution to the convergence of objective function. Then we pro-posed Theorem 1 and present its proof.
 samples in the working set.

Proof. At the k th iterating step, the main operations of the method consist of solving the sub-problem and updating tion searching ( Chen et al., 2006; Fan et al., 2005 ). Then (12) transforms to:
Subject to: the above formula (14), we know that the working set is two-element, so r f  X  a k  X  T B d B can be analyzed as follows: are included in the most violating samples, the objective function gets the steepest decline. h
Based on the above Theorem 1 , the most violating samples can be competent for the following working set. Accordingly, the cost of the working set selection and the computation for updating g t are saved in the next several iterations.
Firstly, q samples with the most violation of the KKT condition are selected by the feasible direction strategy as follows, where q should be set to even number by the user.
 The following subset I + and I were obtained according to the g t sequence.
 In which
I + contains the top q /2 elements sorted by g t = y t r f ( a )t in I , and I contains the bottom q /2 elements sorted by g = y t r f ( a ) t in J .
 the following gt:
Then we take the sample members whose kernel outputs Q t have been kept in Cache as I * , then we can find those great violating samples in the Cache.
 Now several great-violating-pairs set I C can be obtained by matching the samples in I  X  and I one by one.
These pairs have great violating KKT conditions although being inferior to the maximal violating pair. So I C is called the candidate working set (CWS) for selecting great-violating-pairs for the next iterations.
 set are selected as the working sets for the next iterations. Until the candidate working set has been used completely, we update the parameters and re-sort g t in the training set. So it can be found clearly that the virtue of candidate working of Cache.

The performance of strategy employing CWS proposed in this paper can be influenced by the scale of CWS. In a certain the scale of CWS.

The strategy employing CWS in SMO proposed in this paper is called CWS-SMO algorithm. No matter if Cache contains both of I  X  and I are not void sets on the condition of the steepest objective function declining. The kernel outputs for i and j could be replaced in Cache by least-recent-used (LRU) mechanism when they are not kept in Cache. So the Cache can keep updating in time.

Based on the above analysis and investigation of working set selection in theory, the detail CWS-SMO algorithm is given as the following: 4.1. CWS-SMO algorithm
Step1. According to (20) and (21), I + and I can be obtained from (18) and (19) . Then I  X  and I can be obtained from Cache according to (24) and (25).

Step2. Taking the minimal size of I  X  and I as p , then p great-violating pairs are selected by matching the sample members of I  X  and I , where the first pair is the maximal violating pair.
 working set. When maximal violating pair is optimizing, if Cache does not contain their kernel outputs, LRU strategy is used to update Cache.
 egy (Libsvm adopted here); otherwise, go to step 1.

It should be noted that when the objective function is converging to its optimal results, the degree of violating condition stop using CWS strategy. 5. Experiments and discussion 5.1. Data and experiment
In this section, we aim at comparing the proposed CWS strategy with the Traditional Working Set selection strategy violating pairs for all iterations. The same parameters are adopted to ensure that two codes differ only in the working set implementation. Dataset ijcnn1 and acoustic are adopted from UCI ML repository (http://archive.ics.uci.edu/ml/ ) for exper-are in Table 1 . The size of Cache is set to 20M or 40M as in Libsvm. 5.2. Size of candidate working set
The size of candidate working set is determined by the parameter q and Cache (see Section 4). Not only does q affect the iable according to the steps, that is not a fixed value. In experiments, we set q as following: where m M can reflect the optimizing convergence rate for current iterating rate, and ( m M ) 10 eps is the exit condition the capacity of Cache enlarge, the q pre-selected samples are more likely to be in the candidate working set. One advantage training ijcnnl-1 when Cache was set to 20M and 40M. Obviously, the size of CWS is bigger when Cache is 40M than that of 20M Cache as a whole. 5.3. Performance of our strategy Table 2 lists the experiment results on CWS-SMO and TWS-SMO. There are more iteration steps in CWS than TWS, but
CWS uses Cache more times, and the fewer kernels computing time, so CWS can reduce the whole running time. When Cache is set to 40M, the algorithm would use more Cache and shorter running time than that when Cache is set to 20M, although undergoing more iteration steps. In addition, with the size of training set increases, the performance of CWS strategy on training speed is more obvious.
 puting cost on selecting the working set, CWS can prepare working sets for the next several or more iterations, which can save much more computing cost for working set selection. This avoid from the need for working set selection before each iterating step happened in TWS. Therefore considering the trade-off between working set selection computing time and iter-ating steps, CWS strategy can reduce the running time more than TWS strategy.
 When applied to a large-scale dataset, CWS shows higher training speed than TWS. We compare the training times of
CWS and TWS on 5 subsets of Acoustics in Fig. 3 . With the same Cache, the performance of CWS is the same as TWS when it is performed on small-scale subsets. When applied in large-scale subsets, the performance of CWS is better than that of
TWS. The same results appear on the subsets of Ijcnn1. It can be concluded that CWS strategy can show high performance on a large-scale dataset. 6. Concluding remarks
Working set selection is the most important issue in SMO algorithm. In this paper, we propose a candidate working set experiments have shown that the CWS-SMO is more efficient than TWS-SMO, especially in the large-scale dataset. Therefore, the CWS-SMO can be widely applied in the text classification, information retrieval and so on, to speed up its efficiency. Acknowledgements
The work described in this paper was supported by a grant from National Science Foundation of China, Project No. 60574083, and the Natural Science Foundation of Jiangsu Province in China, Project No. BK2007195. References
