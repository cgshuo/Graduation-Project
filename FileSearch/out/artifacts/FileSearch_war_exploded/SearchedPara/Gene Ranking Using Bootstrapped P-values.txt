 Recent research has shown that it is possible to find genes involved in the pathogenesis of a particular condition on the basis of microarray experiments. Genes which are differen-tially expressed, for example between healthy and diseased tissues, are likely to be relevant to the disease under study. Some of the properties of microarray datasets make the task of finding these genes a challenging one. This paper pro-poses a gene-ranking algorithm whose main novelty is the use of bootstrapped P-values. We present an analysis of the algorithm, showing how it takes account of small-sample variability in observed values of the test statistic, in a way conventional statistical tests cannot. Experimental results show that our algorithm outperforms the widely-used two-sample t -test on challenging artificial data. Gene ranking is then performed on two well-known microarray datasets, with encouraging results. For example, a number of genes from one of the datasets, whose differential expression was subsequently confirmed by a more reliable biochemical anal-ysis, are found to be ranked higher by the bootstrapped al-gorithm than by the conventional t -test, suggesting that the proposed algorithm may be better able to exploit the limited data available to infer biologically useful information. microarrays, differential expression, t -test, bootstrap In recent years a number of seminal studies [7; 9; 12] have demonstrated the feasibility of using global expression anal-yses to better understand various diseases. Genes relevant to the pathology under investigation are expected to be up-or down-regulated between healthy and diseased tissues. An important task in microarray data analysis is therefore iden-tifying genes which are differentially expressed in this way. Statistical analysis of gene expression data relating to com-plex diseases is of course not really expected to yield results to whom correspondence should be addressed of the form  X  X ene X causes disease Y  X . A realistic goal is to narrow the field for further analysis, to give geneticists a short-list of genes which are worth investing hard-won funds into analysing.
 What makes it hard to find differentially expressed genes? Quite simply, experimental noise and biological variability. Experimental noise including errors in fabrication, hybridiza-tion, image analysis and so on, mean that the real-valued expression levels returned by a microarray experiment do not exactly reflect true mRNA levels. Biological variabil-ity refers to the natural variation we would expect to en-counter even under ideal experimental conditions. That is to say, even if we could sidestep experimental issues, magi-cally looking inside the cell and counting the RNA molecules of interest, we would still expect some variation in counts between cells in the same category.
 All this means we cannot simply look at expression levels of genes in diseased and healthy tissues and choose the ones which are most different, but must treat those values as ran-dom variables, and the task of gene selection as essentially statistical. A variety of two-sample statistical tests have been applied to microarray data, including conventional [13] and non-parametric [15; 16] tests. However, with typically many thousands of genes to choose from and perhaps a few dozen to be selected, this can be a little like looking for a needle in the proverbial haystack 1 .
 In this paper, taking a classical two-sample test as our start-ing point, we focus on accounting for small-sample variabil-ity in the observed value of the test statistic. Canonical tests do not explicitly address this issue, even when parametric assumptions hold: in light of the properties of microarray data we argue that the consequences of such variability may be considerable. We use the bootstrap [5] to take account of this variability. The method developed is based on the two-sample t -test, which is widely used in microarray anal-ysis [13], but we emphasise that our algorithm, and many of
It turns out that the scale of this mismatch means that it computationally entirely infeasible to actually consider every possible subset of genes as a candidate solution. Most research in this area (ours included) essentially looks at one gene at a time.
 the observations made here, generalise to other two-sample tests.
 Making a brief digression, we note that the task addressed here is subtly different from that of feature selection for gene expression based classifiers [10; 18]. Two-sample tests aim to find all genes which are significantly up-or down-regulated between tissue classes; feature selection algorithms try to find genes which best explain class labels. As an example, consider a hypothetical dataset where a single gene fully ex-plains the class labels, but a hundred genes are nonetheless consistently up-regulated in one set of tissues. A two-sample test will aim to identify all the up-regulated genes, while a feature selection algorithm should return the single explana-tory gene. The distinction is biologically important: all hun-dred genes may have pathological effects of interest to the investigator, despite the fact that a single gene captures the class information. Let us introduce some notation to state more clearly the questions we wish to answer. Consider microarray slides (or chips) belonging to two classes, say, healthy and diseased, with G gene expression levels measured on each slide. Re-cent work has shown that microarray data from higher or-ganisms are very close to log-normally distributed [11]; in or-der to justify the assumptions of the t -test we therefore work in a log space. The data consists of m G -dimensional vec-tors x i (collectively referred to as X ), and n G -dimensional vectors y i (collectively referred to as Y ). m and n are the number of slides in each class, and the vector elements are log expression levels. We now assume these data are drawn from two (possibly different) multivariate normal distribu-tions q and r respectively: Thus, each gene has a pair of true (but unknown) class means, one from each of the distributions q and r . Our task is to rank the genes according to how likely it is that these means are distinct. One way of accomplishing this is through the use of a test statistic, such as the t -statistic. In this pa-per we use the canonical form of the t -statistic throughout, assuming normally distributed data with equal but unknown variances in the two classes. We briefly present the essentials of the t -test below, emphasising the functional relationships between the data, statistic and P-value. A comprehensive account of the test can be found in most statistics textbooks, e.g. [4].
 Let t k represent the t -statistic for gene k . t k is of course just a function of the data for the k th gene ( X k and Y respectively). Let  X  X and  X  Y represent the sample means of X k and Y k respectively,  X  2 X and  X  2 Y the (unbiased) sample variances 2 , and T (  X  ) the t -statistic function. Then t
For typographic simplicity we have taken the liberty of dropping the subscript k from the means and variances in Equation 4. given by: The form of Equation 4 means that it is possible to analyt-ically obtain the distribution of the statistic, which in turn allows the probability of type I errors (false positives) to be calculated. This probability is called the P-value. Under the assumptions of the canonical test, t k has a non-central t -distribution [4], with degrees of freedom v = ( m + n  X  2) and non-centrality parameter  X  k . In the special case of the distribution under the null hypothesis,  X  k = 0 and t k has the familiar t -distribution with degrees of freedom v . The observed value of the statistic is thus mapped to a P-value ( p k ) by a function (which we shall call f ) which de-pends on the t -distribution. For the two-sided test being used, f is given by: where C v (  X  ) represents the cumulative distribution function (cdf) for a t -distribution with v degrees of freedom. The method proposed in this paper is motivated by the following observations about the t -test: In classical hypothesis testing settings, the number of data-points is relatively high, making the statistic t k and corre-sponding P-value p k good representatives of the k th feature. In contrast, the imbalance between slides and genes in mi-croarray experiments places a considerable burden on the ability of ranking algorithms to discriminate between rele-vant and irrelevant genes.
 one. As shown in Equation 5, the P-value is a statistic of the data; we use the bootstrap [5] to obtain an estimate of its value. The bootstrap is a widely-used resampling technique, by which an empirical estimate of the distribution of a statistic of interest can be obtained by repeatedly computing its value from datasets sampled with replacement from the original. Let E  X  [ F ( Z )] represent the bootstrap average of a function F of data Z : where the Z  X  b  X  X  are datasets obtained by resampling Z with replacement. In practice, B is set to a large finite value; in all our experiments B = 500.
 Following Equations 5 and 6, the bootstrap estimate of the P-value for gene k , p  X  k , is given by: where X  X  b k and Y  X  b k represent data for gene k from the b bootstrap iteration and T (  X  ) the t -statistic function. The bootstrap mean of t k may be obtained in a similar manner. We note that as the tail probability mass f (Equation 5) is a non-linear function, its bootstrap average will not, in general, be identical to either the observed P-value p k , or the tail probability mass corresponding to the bootstrap mean of t k , f ( E  X  [ t k ]).
 Statistical power analysis [2] is superficially similar to our method, in that it explicitly deals with the distribution of the test statistic t k . However, the aim of such analysis is quite different, namely to quantify the probability of type II error at a given significance level.
 Previous applications of resampling, and the bootstrap in particular, to testing, have included P-value adjustments and non-parametric tests [3], as well as multiplicity correc-tions [17] (also, in the context of microarray analysis [6]). Our algorithm is closest in spirit to bootstrap P-value ad-justments [1; 8], insofar as it treats the P-value itself as the statistic of interest.
 A useful approach to understanding bootstrapped P-values is to explicitly think of the P-value p (we drop the subscript k for clarity) and its bootstrap estimate p  X  as realisations of random variables. Let P represent the P-value, and g ( P ) its density function; P  X  represents the bootstrap estimate and h ( P  X  ) the corresponding density. Following [14] we ap-proximate g ( P ) by a beta distribution with parameters  X  (0 &lt;  X   X  1) and 1. Thus: Viewed in this way, conventional tests draw a conclusion, as to whether a gene is differentially expressed or not, on the basis of a single draw from the P-value density g ( P ); errors in the procedure can be thought of as arising due to the uncertainty in g ( P ) 3 . Under the null hypothesis,  X  = 1, and the P-value is uniformly distributed in the range [0 , 1]; consequently the probability of type I error at a given sig-nificance level  X  does not depend on any unknowns and is simply equal to  X  . Under the alternative hypothesis, the parameter  X  is unknown. The probability of type II error then depends on  X  and is given by:
A simple example of a less uncertain density: if g 0 was a delta function located at the true mean of P , a single draw would unerringly reveal which hypothesis was correct. where F g is the cdf corresponding to the P-value density g in Equation 8. In general, for fixed significance level  X  , this second error probability rises with the parameter  X  . The effect of taking account of variation in the t -statistic via the bootstrap, is that a draw from the density function of the bootstrapped P-value, h ( P  X  ), is more likely to be close to the true mean of P than a draw from the density of the standard P-value, g ( P ). Indeed, we find via simulation that the average deviation of the bootstrapped P-value P  X  around the true mean of P (i.e., E [ P  X   X  E [ P ]] 1 2 ) is lower than than the corresponding figure for the conventional P-value (i.e., the standard deviation of P , ( E [ P  X  E [ P ]]) We further illustrate the operation of the algorithm by mak-ing some simplifying assumptions, obtaining a qualitative picture of the interplay between the various quantities in-volved. We assume that the bootstrap distribution of the t -statistic is approximately Gaussian, with mean  X   X  t (defined as the bootstrap mean of t , E  X  [ t ]) and variance  X  2 . Under these assumptions, the bootstrap P-value p  X  can be thought of as being the integral of the product of the Gaussian with the tail probability mass f (Equation 5). The integral is then itself a function,  X  f  X  , of  X   X  t and  X  2 ; for a given vari-ance,  X  f  X  can be thought of as mapping a bootstrap mean t -statistic  X   X  t to an expected P-value: We simulate Equation 10 directly, by sampling from the Gaussian and computing mean P-values via the function f (Equation 5). Figure 1 shows the integral version of the bootstrapped P-value  X  f  X  (with variances 1 and 0.5) and f , as functions of the bootstrap mean t -statistic  X   X  t . The right side of the figure shows an illustrative example for a single (hypothetical) gene with the observed t -statistic t = 2 . 5 and the corresponding bootstrap mean  X   X  t = 2 . 25. The form of the function f makes it clear why variation around high absolute values of t has little effect on P-values, but for moderate values, fluctuations in t can profoundly effect the final P-value and ranking of the gene. Many genes of in-terest have moderate t -statistics and are highly sensitive to the exact observed value. For example, if the 3000 genes analysed from the colon cancer dataset [12] are arranged in descending order of absolute bootstrap mean t -statistics |  X  t | , only the 72 highest ranked genes have |  X   X  t | in excess of 6.
 The bootstrap distribution of the t -statistic on which the P-value of Equation 7 is based is purely empirical. Under the assumptions of a canonical t -test, however, the form of the distribution is known. We note that an alternative ap-proach to the one taken above would thus be to estimate the non-centrality parameter  X  via the bootstrap, using the estimated value to obtain the appropriate non-central t -distribution. Using suitable approximations, the integral of Equation 10 could then be evaluated to obtain a P-value for each gene. One advantage of the approach we have taken is that it generalises easily to the non-parametric case: the cdf C v used in Equation 5 need only be replaced by an em-pirical (for example, permutation-based) cdf. The P-value could then be obtained as before from Equation 7. Figure 2: Results of bootstrapped and conventional t -tests on artificial data. The score reported is the proportion of two hundred iterations in which the algorithm in question ranked the correct features in the top two places. The boot-strapped test is able to take account of small-sample vari-ability in the observed statistic and outperforms the conven-tional test at various noise levels. We assess the ability of the proposed algorithm to detect differentially expressed genes on artificially generated data. Six-dimensional data in two classes are generated from six pairs of univariate Gaussians, only two of which have dis-tinct means. The class variances are equal and are made to vary incrementally between 1 and 3, to simulate increasing noise levels; the number of samples in the two classes are 10 and 5. We choose a relatively small number of samples to mimic microarray data; to account for small-sample effects in the results we report average scores over 200 iterations. At each iteration, 15 samples are drawn from the Gaussians and passed to two ranking algorithms: a conventional t -test and the proposed algorithm, performance being assessed in terms of the proportion of runs in which the two highest ranked features are the correct ones. Repeatedly drawing data in this way provides an estimate of generalisation accu-racy under small-sample conditions: the proposed algorithm scores  X  6.5% higher than the t -test, averaged over a range of variances and all 200 runs. Results are shown in Figure 2. We select two well-known and widely analysed microarray datasets -the colorectal cancer data of Notterman et al. [12] and the leukaemia data of Golub et al. [7].
 The colorectal cancer dataset consists of 36 labeled slides with 6600 complementary DNAs (cDNAs) and expressed se-quence tags (ESTs) represented on each.
 The leukaemia data consists of a training set of bone marrow samples taken from patients suffering from Acute Myeloid Leukaemia (AML) and Acute Lymphoid Leukaemia (ALL), and a separate test set with bone marrow as well as periph-between the ranks is 25.5.
 eral blood samples. The training set contains data from 38 samples taken from patients with AML or ALL, and the test set 34 patient samples. Expression levels are given for 7129 genes/ESTs. For this work we use only the test dataset. We pre-process the gene expression data according to cur-rent practice [19], removing within-slide location by chang-ing from absolute to relative expression values. As noted previously, microarray data from higher organisms are very close to log-normally distributed [11]; we therefore transform the data into a log-space.
 Colorectal cancer data: Table 1 shows P-values and iden-tities of the five highest ranked genes identified by the boot-strap algorithm. These genes have extreme t -statistics, so as expected, their ranks under both algorithms are similar. Rankings returned by our algorithm and the t -test are no-ticeably different across the entire dataset: on an average, there are 65 places between the positions of the same genes; when the top 100 genes returned by the bootstrap algorithm are considered, the average displacement reduces to 15. The difference between the results of the tests thus lies in the positions assigned to genes with high, but not extreme, t -statistics. These genes may be of great practical importance: indeed, from a biological perspective the aim of microarray experiments (which are high-throughput but noisy) is es-sentially to guide further investigation. More accurate tran-script abundance analyses, for example quantitative real-time RT-PCR 4 can be used to confirm differential expres-sion. RT-PCR is too expensive to be used to assess every gene; thus one major objective of microarray data analysis is to identify a subset of genes for such assessment. Table 4.2 compares the ranks of genes whose differential expression was subsequently confirmed by RT-PCR [12]. In most cases we find the proposed algorithm ranks these genes higher than the t -test, suggesting that if used to select a subset for further assessment it is more likely to uncover relevant genes.
 Leukaemia data: Table 3 shows P-values and identities of the five highest ranked genes identified by the bootstrap al-gorithm on the leukaemia dataset. Once again, the extreme t -statistics of these top-ranked genes mean that their ranks under both algorithms are similar. For this dataset there are, on an average, 244 places between the ranks assigned to the same genes by the two algorithms; when only the top 100 genes returned by the bootstrap algorithm are consid-ered, the average displacement is 9. Table 4 shows a selec-tion of highly ranked genes whose ranks were higher under the bootstrapped test than the t -test. Some of these genes have been implicated in other studies too: for example, the Human myeloperoxidase gene has recently been found to be ranked much higher, compared to the t -test, by well-founded methods including information gain and a one-dimensional support vector machine [20]. Reverse transcription polymerase chain reaction In this paper we have proposed a novel gene ranking method based on bootstrapped P-values, and shown that it can suc-cessfully account for small-sample effects in the observed test statistic for a gene. While it is premature to draw defini-tive biological conclusions from our results, experiments on both artificial and real data suggest that our algorithm is better able to deal with the level of uncertainty inherent in microarray data than a classical two-sample test. In par-ticular, results concerning the comparative ranks of genes from the colon cancer dataset [12] whose differential expres-sion was confirmed using RT-PCR (Table 4.2) are encour-aging, and suggest that the proposed algorithm may be able to guide further investigation more accurately than the t -test. In essence, our method obtains a more accurate P-value at the cost of computational efficiency, but we feel that in this particular domain compute-time should not be an over-riding concern -with sometimes millions of dollars being spent on designing experiments and acquiring data, a few extra minutes or even hours of processing should be acceptable if better results can be obtained! Clearly, many questions remain to be addressed. Further theoretical analysis is required to fully understand the dis-tributional properties of the bootstrapped P-value. Our re-sults are promising but preliminary -a thorough empirical evaluation of the proposed algorithm, potentially using a different two-sample test, as well as further investigation of the biological impact of the results reported here, will be informative. Also, the extension of the method proposed to a fully non-parametric setting may prove useful in analysing data, for instance from lower organisms, which do not con-form to the assumptions made here.
 SNM gratefully acknowledges the support of the Biotech-nology and Biological Sciences Research Council (BBSRC); thanks also to Dr. Sayan Mukherjee. [1] R. Beran. Prepivoting Test Statistics: A Bootstrap [2] J. Cohen. Statistical power analysis for the behavioral [3] A. C. Davison and D. V. Hinckley. Bootstrap Meth-[4] M. H. DeGroot and M. J. Schervish. Probability and [5] B. Efron. Bootstrap methods: another look at the jack-[6] Y. Ge, S. Dudoit, and T. P. Speed. Resampling-based [7] T. R. Golub, D. K. Slonim, P. Tamayo, C. Huard, [8] P. H. Hall and M. A. Martin. On Bootstrap Resampling [9] I. Hedenfalk, D. Duggan, Y. Chen, M. Radmacher, [10] S. Hochreiter and K. Obermayer. Feature Selection and [11] D. C. Hoyle, M. Rattray, R. Jupp, and A. Brass. Mak-[12] D. Notterman, U. Alon, A. J. Sierk, and A. J. Levine. [13] W. Pan. A comparative review of statistical methods [14] T. Sellke, M. J. Bayarri, and J. O. Berger. Calibra-[15] O. G. Troyanskaya, M. E. Garber, P. O. Brown, D. Bot-[16] V. Tusher, R. Tibshirani, and G. Chu. Significance [17] P. H. Westfall and S. S. Young. Resampling-Based Mul-[18] J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, [19] Y. H. Yang, S. Dudoit, P. Luu, and T. P. Speed. [20] Yang Su, T. M. Murali, V. Pavlovic, M. Schaffer, and
