 so that there is an adv antage in  X  X ooling X  together data across man y related tasks. describing these products.
 a theoretical justification of the algorithm in connection with 1 -norm regularization. of single-task 1 -norm regularization.
 approach with other related multi-task learning methods and dra w our conclusions. We begin by introducing our notation. We let IR be the set of real numbers and IR subset of non-ne gative (positi ve) ones. Let T be the number of tasks and define IN For each task t 2 IN Based on this data, we wish to estimate T functions f well the data and are statistically predicti ve, see e.g. [11 ]. If we define the p -norm of vector w as k w k a define the ( r; p ) -norm of A as k A k We denote by S d the set of d d real symmetric matrices and by S d inite ones. If D is a d d matrix, we define trace( D ) := P d orthogonal matrices. Finally , D + denotes the pseudoin verse of a matrix D . 2.1 Pr oblem formulation The underlying assumption in this paper is that the functions f small set of featur es . Formally , our hypothesis is that the functions f where h assumption is that all the features but a few have zero coef ficients across all the tasks. For simplicity , we focus on linear features, that is, h we assume that the vectors u the vectors u w occasion.
 Let us denote by W the d T matrix whose columns are the vectors w with entries a our current work in Section 4.
 In the follo wing, we describe our approach to computing the feature vectors u a . We first consider the case that there is only one task (say task t ) and the features u learn the parameter vector a cal error P m of the 1 -norm of a or equi valently the unconstrained problem solutions, that is, man y components of the learned vector a increasing function of [14].
 ization error function 1 -norm of the vector b ( A ) = ( k a 1 k common features will be selected across them.
 of important each feature is and favor uniformity across the tasks for each feature. function E over U , that is, we consider the optimization problem We note that when the matrix U is not learned and we set U = I problem We shall return to problem (2.5) in Section 4 where we present an algorithm for solving it. nonsmooth which mak es it more dif ficult to optimize.
 problem. To this end, for every W 2 IR d T and D 2 S d Theor em 3.1. Problem (2.4) is equivalent to the problem optimal solution for (3.2), wher e Pr oof . Let W = U A and D = U Diag ( k a i k 2 Therefore, min which corresponds to the nonzero eigen values of D , and hence if Therefore, min case there is a matrix D for which P T of matrix A .
 we define the function f ( w; D ) = w &gt; D + w , if D 2 S d sup f w &gt; v + trace( ED ) : E 2 S d ; v 2 IR d ; 4 E + vv &gt; 0 g . (recall that w When we keep D fix ed, the minimization over w For a fix ed value of the vectors w The follo wing theorem characterizes the optimal solution of problem (4.1). Algorithm 1 ( Multi-T ask Featur e Learning )
Input: training sets f ( x Parameters: regularization parameter
Output: d d matrix D , d T regression matrix W = [ w
Initialization: set D = I d d while con vergence condition is not true do end while Theor em 4.1. Let C = W W &gt; . The optimal solution of problem (4.1) is and the optimal value equals (trace C 1 2 ) 2 .
 We first introduce the follo wing lemma which is useful in our analysis. Lemma 4.2. For any b = ( b and any minimizing sequence con ver ges to ^ Pr oof . From the Cauch y-Schw arz inequality we have that k b k ( P b P infimum is attained when b Pr oof of Theor em 4.1. We write D = U Diag ( ) U &gt; , with U 2 O d and 2 IR d mize over . For this purpose, we use Lemma 4.2 to obtain that inf f trace ( W &gt; U Diag ( ) 1 U &gt; W ) : 2 IR d ++ ; Ne xt we sho w that and a minimizing U is a system of eigen vectors of C . To see this, note that since u only on W . representations.
 Cor ollary 4.3. Problem (2.5) is equivalent to the problem and the optimal is given by D = Diag ( ) , where the vector = ( 1 ; : : : ; d ) is computed using equation (4.5). one-out cross validation.
 Synthetic Experiments. We created synthetic data sets by generating T = 200 task param-eters w y with standard deviation equal to 0 : 1 .
 number of features decreases with .
 low dimensionalities but increases as the number of irrele vant dimensions increases. features (middle) and attrib utes learned by the most important feature (right). more tasks leads to better estimates of the underlying features. telephone hot line (TE), amount of memory (RAM), screen size (SC), CPU speed (CPU), hard per task as the test data and 8 examples per task as the training data. patterns in people X  s decision process.
 37 : 1% compared to 29 : 5% in that paper . These results will be reported in future work. guaranteed to con verge to a local minimum.
 the regularization parameter .
 Ackno wledgments This work was supported by EPSRC Grants GR/T18707/01 and EP/D071542/1, and by the IST Programme of the European Commission, under the PASCAL Netw ork of Excellence IST -2002-506778.

