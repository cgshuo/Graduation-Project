 New York, NY 10115 Several recent developments such as the growth of the world wide web and the maturation of ge-nomic technologies, have brought new domains of application to machine learning research. Many ple, hyper-linked web-documents are often about similar topics, even if their textual contents are disparate when viewed as bags of words. In document categorization, the citations are important computational biology, knowledge about physical interactions between proteins can supplement ge-nomic data for developing good similarity measures for protein network inference. In such cases, a learning algorithm can greatly benefit by taking into account the global network organization of such inter-relationships rather than relying on input attributes alone.
 One simple but general type of relational information can be effectively represented in the form of a graph G =( V , E ) . The vertex set V represents a collection of input instances (which may E X  X  X V represents the pairwise relations over these input instances. In this paper, we restrict our about correlation between the vertex instances. In particular, we allow edges to be of two types  X   X  X ositive X  or  X  X egative X  depending on whether the associated adjacent vertices are positively or negatively correlated, respectively. On many problems, only positive edges may be available. linkage structure may be derived from data input attributes. In graph-based semi-supervised meth-serves as an estimate of the global geometric structure of the data. Many algorithmic frameworks positively correlated.
 Several methods have been proposed recently to incorporate relational information within learning grate relational information with input attributes in a non-parametric Bayesian framework based on Gaussian processes (GP) (Rasmussen &amp; Williams, 2006), which leads to a data-dependent covari-ance/kernel function. We highlight the following aspects of our approach: 1) We propose a novel pectation Propagation techniques under a Gaussian process prior. The covariance function of the RGP. RGP provides a novel Bayesian framework with a data-dependent covariance function for su-points. 2) When applied to semi-supervised learning tasks involving labelled and unlabelled data, et al., 2005) using a novel graph regularizer. Unlike many recently proposed graph-based Bayesian transductive by design, RGP delineates a decision boundary in the input space and provides proba-bilistic induction over unseen test points. Furthermore, by maximizing the joint evidence of known semi-supervised hyper-parameter tuning method can be very useful when there are very few, possi-high-quality generalization on unseen test examples as compared to standard GP classification that tasks comparing with competitive deterministic methods.
 The paper is organized as follows. In section 2 we develop relational Gaussian processes. Semi-supervised learning under this framework is discussed in section 3. Experimental results are pre-sented in section 4. We conclude this paper in section 5. attributes, denoted as a column vector x  X  X  X  R d . The key idea in Gaussian process models is f x and f z is fully determined by the coordinates of the data pair x and z , and is defined by any Mercer kernel function K ( x , z ) . Thus, the prior distribution over f =[ f x any collection of n points x 1 ... x where  X  is the n  X  n covariance matrix whose ij -th element is K ( x consider the scenario with undirected linkages over a set of instances. 2.1 Undirected Linkages Let the vertex set V in the relational graph be associated with n input instances x 1 ... x set of observed pairwise undirected linkages on these instances, denoted as E = {E is treated as a Bernoulli random variable, i.e. E instances x follows: to positive and negative edges have the same and opposite signs respectively. In the presence of uncertainty in observing E Gaussian noise that allows some tolerance for noisy observations. The Gaussian noise is of zero  X  P E where  X ( z )= z first and third quadrants where f x 1  X  X  E Remarks: One may consider other ways to define a likelihood function for the observed edges. For example, we could define P variable and then the probability of observing the graph G can be simply evaluated as: P ( G| f )= exp  X  1 2 f T  X  f where  X  is a graph-regularization matrix (e.g. graph Laplacian) and Z is a normalization factor that depends on the variable values f . Given that there are numerous graph this paper, we will use the likelihood function developed in (3). 2.2 Approximate Inference Combining the Gaussian process prior (1) with the likelihood function (3), we obtain the posterior distribution as follows, where f =[ f x serves as a yardstick for model selection.
 the correlation between examples but never change individual mean. To preserve computational a joint Gaussian centered at the true mean than resort to sampling methods. A family of inference techniques can be applied for the Gaussian approximation. Some popular methods include Laplace approximation, mean-field methods, variational methods and expectation propagation. It is inappro-propagation (EP) algorithm (Minka, 2001) can be applied here. In this paper, we employ the EP captures the posterior covariance structure allowing prediction of link presence.
 The key idea of our EP algorithm here is to approximate P ( f ) where ij runs over the edge set, f parameters { s Leibler divergence, ing up to the second order. At the equilibrium the EP algorithm returns a Gaussian approximation to the posterior distribution where A =( X   X  1 + X )  X  1 ,  X = augmented from  X  Gaussian approximation serves as approximate model evidence that can be explicitly written as The detailed updating formulations have to be omitted here to save space. The approximate evidence based procedure can be employed for hyperparameter tuning. Although the EP algorithm is known to and Winther (2005) proposed expectation consistent (EC) as a new framework for approximations EC algorithm as future work. 2.3 Data-dependent Covariance Function given by a modified covariance function defined in the following proposition.
 { f variance matrix whose elements are given by evaluating the kernel function  X  K ( x , z ): X X X  X  R for x , z  X  X  given by: where I is an n  X  n identity matrix, k x is the column vector [ K ( x 1 , x ) ,..., K ( x n  X  n covariance matrix of the vertex set V obtained by evaluating the base kernel K , and  X  is defined as in (6).
 A proof of this proposition involves some simple matrix algebra and is omitted for brevity. RGP is obtained by a Bayesian update of a standard GP using relational knowledge, which is closely related to the warped reproducing kernel Hilbert space approach (Sindhwani et al., 2005) using a novel graph regularizer  X  in place of the standard graph Laplacian. Alternatively, we could simply employ the standard graph Laplacian as an approximation of the matrix  X  . This efficient approach has been studied by (Sindhwani et al., 2007) for semi-supervised classification problems. 2.4 Linkage Prediction Given a RGP, the joint distribution of the random variables f pair x written as a zero-mean bivariate Gaussian N ( f as which can be simplified as after we learn from the observed linkages. We now apply the RGP framework for semi-supervised learning where a large collection of unla-apply RGP, we construct positive reciprocal relations between examples within K nearest neighbor-connected graph over labelled and unlabelled examples, where there is a path between each pair of procedure is also applicable to regression, multi-class classification and ranking. model, i.e. P ( y | f z )= X ( y f z level. Combining the probit likelihood with the RGP prior defined by the covariance function (8), we have the posterior distribution as follows, where f =[ f z evaluated accordingly (Seeger, 2003). The predictive distribution of the variable f z z  X  K ( z the Bernoulli distribution over the test label y To summarize, we first incorporate linkage information into a standard GP that leads to a RGP, and then perform standard inference with the RGP as the prior in supervised learning. Although we describe RGP in two separate steps, these procedures can be seamlessly merged within the Bayesian framework. As for model selection, it is advantageous to directly use the joint evidence particularly useful when labelled data is very scarce and possibly noisy. usefulness of this approach on three real world data sets. Throughout the experiments, we con-K ( x , z ) = exp  X   X  K ( x , z )  X  1 The label noise level  X  2 K nearest-neighbor graphs, K is fixed at the minimal integer required to have a connected graph. Figure 1: Results on the synthetic dataset. The 30 samples drawn from the Gaussian mixture are sity of Washington and the University of Wisconsin. The numbers of categorized Web pages and undirected linkages in the four university dataset are listed in the second column. The averaged AUC scores of label prediction on unlabelled cases are recorded along with standard deviation over 100 trials.
 4.1 Demonstration Suppose samples are distributed as a Gaussian mixture with two components samples from this distribution, shown as dots on the x axis of Figure 1(a). With K =3 , there are 56  X  X ositive X  edges over these 30 samples. We fixed  X  2 =1 for all the edges, and varied the samples. The pairs within the same cluster become positively correlated, whereas the pairs between on density distributions. Given two labelled samples, one per class, indicated by the diamond and that the decision boundary of the standard GPC should be around x =1 . We observed our decision boundary significantly shifts to the low-density region that respects the geometry of the data. 4.2 The Four University Dataset We considered a subset of the WebKB dataset for categoriza-universities, contains 4160 pages and 9998 hyperlinks interconnecting them. These pages have been other. The text content of each Web page was preprocessed as bag-of-words, a vector of  X  X erm fre-quency X  components scaled by  X  X nverse document frequency X , which was used as input attributes. The length of each document vector was normalized to unity. The hyperlinks were translated into 66249 undirected  X  X ositive X  linkages over the pages under the assumption that two pages are likely Figure 2: Test AUC results of the two semi-supervised learning tasks, PCMAC in (a) and USPS respectively at different percentages of labelled samples over 100 trials. The notched-boxes have with values beyond the ends of the whiskers, which are displayed as dots. where the entire linkage data was used to learn the RGP model and comparisons were made with GPC for predicting labels of unlabelled samples. We make comparisons with a discriminant kernel approach to semi-supervised learning  X  the Laplacian SVM (Sindhwani et al., 2005) using the lin-ear kernel and a graph Laplacian based regularizer. We recorded the average AUC for predicting by incorporating the linkage information in modelling. RGP is very competitive with LapSVM on would be interesting to utilize weighted linkages and to compare with other graph kernels. 4.3 Semi-supervised Learning We chose a binary classification problem in the 20 newsgroup dataset, 985 PC documents vs. 961 MAC documents. The documents were preprocessed, same as we did in the previous section, into vectors with 7510 elements. We randomly selected 1460 documents as training data, and tested on the remaining 486 documents. We varied the percentage of labelled data 100 times. We used the linear kernel in the RGP and GPC models. With K =4 , than GPC and LapSVM, especially when the fraction of labelled data is less than 5% . When the to use while our approach provides a Bayesian model selection by the model evidence. U.S. Postal Service dataset (USPS) of handwritten digits consists of 16  X  16 gray scale images. We test samples. With K =3 , we obtained 2769 edges over the 1214 training samples. We randomly picked up a subset of the training samples as labelled data and treated the remaining samples as unlabelled. We varied the percentage of labelled data from 0 . 1% to 10% gradually, and at each percentage repeated the selection of labelled data 100 times. In this experiment, we employed the in Figure 2(b) as a boxplot, along with the test results of GPC and LapSVM. When the percentage of labelled data is less than 5% , our algorithm achieved greatly better performance than GPC, and very competitive results compared with LapSVM (tuned with 50 labelled samples) though RGP used fewer labelled samples in model selection. Comparing with the performance of transductive SVM (TSVM) and the null category noise model for binary classification (NCNM) reported in (Lawrence &amp; Jordan, 2005), we are encouraged to see that our approach outperforms TSVM and NCNM on this experiment. We developed a Bayesian framework to learn from relational data based on Gaussian processes. The resulting relational Gaussian processes provide a unified data-dependent covariance function for many learning tasks. We applied this framework to semi-supervised learning and validated this approach on several real world data. While this paper has focused on modelling symmetric (undi-further proposed efficient algorithms. This is a promising direction.
 Acknowledgements
