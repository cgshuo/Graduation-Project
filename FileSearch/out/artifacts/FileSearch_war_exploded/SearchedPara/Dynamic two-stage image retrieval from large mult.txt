 1. Introduction
In content-based image retrieval (CBIR), images are represented by global or local features. Global features are capable of generalizing an entire image with a single vector, describing color, texture, or shape. Local features are computed at multiple points on an image and are capable of recognizing objects.

CBIR with global features is notoriously noisy for image queries of low generality , i.e. the fraction of relevant images in a collection. In contrast to text retrieval where documents matching no query keyword are not retrieved, CBIR methods typ-ically rank the whole collection via some distance measure. For example, a query image of a red tomato on white background would retrieve a red pie-chart on white paper. If the query image happens to have a low generality, early rank positions may be dominated by spurious results such as the pie-chart, which may even be ranked before tomato images on non-white back-grounds. Fig. 3 a and b demonstrate this particular problem.  X  Local-feature approaches provide a slightly better retrieval effectiveness than global features ( Aly, Welinder, Munich, &amp;
Perona, 2009 ). They represent images with multiple points in a feature space in contrast to single-point global feature rep-resentations. While local approaches provide more robust information, they are more expensive computationally due to the high dimensionality of their feature spaces and usually need nearest neighbors approximation to perform points-matching field. Thus, global features are more popular in CBIR systems as they are easier to handle and still provide basic retrieval mechanisms. In any case, CBIR with either local or global features does not scale up well to large databases efficiency-wise.
In small databases, a simple sequential scan may be acceptable, however, scaling up to millions or billion images efficient indexing algorithms are imperative ( Li, Chen, Zhang, Lin, &amp; Ma, 2006 ).

Nowadays, information collections are not only large, but they also consist of multimedia . Take as an example Wiki-pedia, where each topic or article consists of a textual part and may include non-textual media such as image, sound, and video. Furthermore, such collections may also provide several ways of accessing each medium. For example, a topic may be covered in several natural languages, and non-textual media may be annotated in a variety of metadata fields, such as filename, author, description, and comment, possibly also in several languages. Without a clear definition and rather confusingly, some have come to use the term multimodal interchangeably to multimedia for describing such re-trieval setups.

Conforming to one of the definitions of modality in the dictionary, i.e.  X  X  X  quality, attribute, or circumstance that denotes mode, mood, or manner X  X  , 1 we find it more appropriate in IR to define modality as a manner of retrieval. For example, a multi-general (but at the same time more granular) term which may also imply X  X ut not necessarily denote X  X ultimedia. Current search engines usually focus on limited numbers of modalities, e.g. English text queries on English articles or maybe on anno-tations of other media as well, not making use of all the available information. In an image retrieval system where users are assumed to target visual similarity, all media beyond image can be considered as secondary; nevertheless, modalities from sec-ondary media can still provide useful information for improving image retrieval.

In this paper, we experiment with a method for image retrieval from large multimedia databases, which targets to improve both the effectiveness and efficiency of traditional CBIR by exploring information from secondary media. In the setup considered, an information need is expressed by a query in the primary medium (i.e. an image example) accompanied by a query in a secondary medium (e.g. text). The core idea for improving effectiveness is to raise query generality before performing CBIR, by reducing collection size via filtering methods. In this respect, we perform retrie-val in a two-stage fashion: first use the secondary medium to rank the collection and then perform CBIR only on the top-K items. Using a  X  X heaper X  secondary medium, this improves also efficiency by cutting down on costly CBIR operations.

Best results re-ranking by visual content has been seen before, but mostly in different setups than the one we consider or ers used external information, e.g. an external set of diversified images ( Popescu et al., 2009 ) (also, they did not use image ( Popescu et al., 2009 ) who re-ranked the top-30% of retrieved items. Most of them used global features for images. Effective-ness results have been mixed; it worked for some, it did not for others, while some did not provide a comparative evaluation or system-study. Later, we will review the related literature in more detail.

In view of the related literature, our main contributions are the following. First, our threshold is calculated dynamically per query to optimize a predefined effectiveness measure, without using external information or training data. Second, we provide an extensive evaluation in relation to thresholding types, levels, and robustness. Third, we investigate the influ-ence of different effectiveness levels of the second visual stage on the whole two-stage procedure. Fourth, beyond using glo-bal features, we also investigate the performance of the proposed setup for local feature derivatives in the visual stage. Fifth, we provide a comprehensive review of related literature and discuss the conditions under which such setups can be applied effectively. Traditionally, the method that has been followed in order to deal effectively with multimodal databases is to search the modalities separately and fuse their results, e.g. with a linear combination of the retrieval scores of all modalities per item; the same can be done across media. Consequently, our sixth contribution is a theoretical and experimental com-parison of two-stage to fusion.

The rest of the paper is organized as follows. In Section 2 we discuss the assumptions, hypotheses, and require-ments behind two-stage image retrieval from multimedia databases. In Section 3 we perform an experiment on a standardized multimodal snapshot of Wikipedia. In Section 4 we switch the type of features used for the visual stage from global to locally-based and repeat a part of the main experiment. In Section 5 we compare the proposed two-stage setup to fusion. In Section 6 we review related work. Conclusions and directions for further research are sum-marized in Section 7 . 2. Two-stage image retrieval from multimedia databases
Mutlimedia databases consist of multiple media for each retrievable item; in the setup we consider these are image and annotations. On the one hand, the visual content of images corresponds to large amounts of information which can hardly be described by words. On the other hand, textual descriptions are key to retrieving relevant results for a query but at the same time capture little visual information ( van Leuken et al., 2009 ); past experiments have shown that users tend to annotate images based on the objects appearing in them rather than color or texture ( Mulhem &amp; Lim, 2002 ).

Traditionally, the method that has been followed in order to deal effectively with multimodal databases is to search the modalities separately and fuse their results, e.g. with a linear combination of the retrieval scores of all modalities per item. While fusion has been proved robust, we argue that it has a couple of important issues:
Appropriate weighing of modalities and score normalization/combination are not a trivial problems and may require training data.

It is not a theoretically sound method if results are assessed by visual similarity only; the influence of textual scores may worsen the visual quality of end-results.

The latter issue points to that there is a primary medium , i.e. the one targeted and assessed by users. Additionally, the total search time in fusion is the sum of the times taken for searching the participating modalities.

An approach that may tackle the issues of fusion would be to search in a two-stage fashion: first rank with a secondary medium, draw a rank-threshold, and then re-rank only the top items with the primary medium. The assumption on which such a two-stage setup is based on is the existence of a primary medium, and the success would largely depend on the rel-ative retrieval effectiveness of the two media involved. For example, if text retrieval always performs better than CBIR (irre-spective of query generality), then CBIR is redundant. If it is the other way around, only CBIR will be sufficient. Thus, the hypothesis is that CBIR can do better than text retrieval in small sets or sets of high query generality.
 In order to reduce collection size raising query generality, a ranking can be thresholded at an arbitrary rank or item score.
This improves the efficiency by cutting down on costly CBIR operations, but it may not improve too much the result quality: a too tight threshold would produce similar results to a text-only search making CBIR redundant, while a too loose threshold would produce results haunted by the red-tomato/red-pie-chart effect mentioned in the Introduction. Three factors deter-mine what the right threshold is: 1. the number of relevant items in the collection, 2. the quality of the ranking, and 3. the measure that the threshold targets to optimize ( Robertson &amp; Hull, 2000 ).

The first two factors are query-dependent, thus thresholds should be selected dynamically per query, not statically as most previously proposed methods in the literature (reviewed in Section 6 ).

The approach of Popescu et al. (2009) , who re-rank the top-30% retrieved items which can be considered dynamic, does not take into account the three aforementioned factors. While the number of retrieved results might be argued correlated to the number of relevant items (thus, seemingly taking into account the first factor), this correlation can be very weak at times, e.g. consider a high frequency query word (almost a stop-word) which would retrieve large parts of the collection. Further, such percentage thresholding seems remotely-connected to factors (2) and (3). Consequently, we will resort to the approach of Arampatzis, Kamps, and Robertson (2009) which, based on the distribution of item scores, is capable of estimating (1), as well as mapping scores to probabilities of relevance. Having the latter, (2) can be determined, and any measure defined in (3) can be optimized in a straightforward way. More on the method can be found in the last-cited study.

Targeting to enhance query generality, the most appropriate measure to optimize would be precision. However, since the smoothed precision estimated by the method of Arampatzis et al. (2009) monotonically declines with rank, it makes sense to set a precision threshold. The choice of precision threshold is dependent on the effectiveness of the CBIR stage: it can be seen as guaranteeing the minimum generality required by the CBIR method at hand for achieving good effectiveness. Not knowing the relation between CBIR effectiveness and minimum required generality, we will try a series of thresholds on precision, as well as, to optimize other cost-gain measures. Thus, while it may seem that we exchange the initial problem of where to set a static threshold with where to threshold precision or which measure to optimize, it will turn out that the latter problem is less sensitive to its available options, as we will see.

A possible drawback of the two-stage setup considered is that relevant images with empty or very noisy secondary media would be completely missed, since they will not be retrieved by the first stage. If there are any improvements compared to single-stage text-only or image-only setups, these will first show up on early precision since only the top results are re-ranked; mean average precision or other measures may improve as a side effect. Fusion does not have these problems. In any case, there are efficiency benefits from searching the most expensive medium only on a subset of the collection.
The requirement of such a two-stage CBIR at the user-side is that information needs are expressed by visual as well as textual descriptions. The community is already experimenting with such setups, e.g. the ImageCLEF 2010 Wikipedia Retrie-val task was performed on a multimodal collection with topics made of textual and image queries at the same time ( Popescu et al., 2010 ). Past experiments have shown that users seem to find it more intuitive to access images using natural language descriptions rather than low-level characteristics such as colors or textures ( Martinet, Chiaramella, &amp; Mulhem, in press;
Rodden &amp; Wood, 2003 ). Furthermore, multimodal or holistic query interfaces are showing up in experimental search engines allowing concurrent multimedia queries ( Zagoris et al., 2010 ). As a last resort, automatic image annotation methods ( Chang,
Goh, Sychay, &amp; Wu, 2003; Li &amp; Wang, 2008 ) may be employed for generating queries for secondary media in traditional im-age retrieval systems. 3. Experiments on Wikipedia
In this section, we report on experiments performed on a standardized multimodal snapshot of Wikipedia. It is worth not-ing that the collection is one of the largest benchmark image databases for today X  X  standards. It is also highly heterogeneous, containing color natural images, graphics, grayscale images, etc., in a variety of sizes. 3.1. Datasets, systems, and methods
The ImageCLEF 2010 Wikipedia test collection has image as its primary medium, consisting of 237,434 items, associated with noisy and incomplete user-supplied textual annotations and the Wikipedia articles containing the images. Associated annotations exist in any combination of English, German, French, or any other unidentified (non-marked) language. There are 70 test topics, each one consisting of a textual and a visual part: three title fields (one per language X  X nglish, German,
French), and one or more example images. The topics are assessed by visual similarity to the image examples. More details on the dataset can be found in Popescu et al. (2010) .

For text indexing and retrieval, we employ the Lemur Toolkit V4.11 and Indri V2.11 with the tf.idf retrieval model. use the default settings that come with these versions of the system except that we enable Krovetz stemming. We index only the English annotations, and use only the English query of the topics.

Compact Composite Descriptors (CCDs) ( Chatzichristofis, Arampatzis, &amp; Boutalis, 2010; Chatzichristofis, Zagoris, Boutalis, &amp; Papamarkos, 2010 ) are global image features, capturing more than one type of information at the same time in a very com-pact representation. We index the images with two CCDs: the Joint Composite Descriptor (JCD) and the Spatial Color Distri-bution (SpCD). The JCD is developed for color natural images and combines color and texture information ( Chatzichristofis,
Arampatzis, et al., 2010 ). In several benchmarking databases, JCD has been found more effective than MPEG-7 descriptors ( Chatzichristofis, Arampatzis, et al., 2010 ). The SpCD combines color and its spatial distribution; it is considered more suit-able for colored graphics since they consist of a relatively small number of colors and less texture regions than color natural images. It is recently introduced in Chatzichristofis, Boutalis, and Lux (2010) and found to perform better than JCD in a het-erogeneous image database ( Chatzichristofis &amp; Arampatzis, 2010 ).
 We evaluate on the top-1000 results with mean average precision (MAP), precision at 10 and 20, and bpref ( Buckley &amp;
Voorhees, 2004 ). The bpref measure is inversely related to the fraction of judged non-relevant documents that are retrieved before relevant documents ( Buckley &amp; Voorhees, 2004 ); adding additional unjudged documents to a retrieved set can have no effect on that sets bpref score, but can have significant influence on the other measures scores. We are using it as a mea-sure of ground-truth incompleteness that signals whether we retrieve un-judged items. 3.2. Thresholding and re-ranking
We investigate two types of thresholding: static and dynamic. In static thresholding, the same fixed pre-selected rank threshold K is applied to all topics. We experiment with levels of K at 25, 50, 100, 250, 500, and 1000. The results that are not re-ranked by image are retained as they are ranked by text, also in dynamic thresholding.

For dynamic thresholding, we use the Score-Distributional Threshold Optimization (SDTO) as described in Arampatzis et al. (2009) . For tf.idf scores, we used the technically truncated model of a normal-exponential mixture. The method normal-izes retrieval scores to probabilities of relevance (prels), enabling the optimization of K for any user-defined effectiveness measure. Per query, we search for the optimal K in [0,2500], where 0 or 1 results to no re-ranking. Thus, for estimation with the SDTO we truncate at the score corresponding to rank 2500 but use no truncation at high scores as tf.idf has no theoretical maximum. If there are 25 text results or less, we always re-rank by image; these are too few scores to apply the SDTO reli-of the SDTO is that it does not require training data; more details on the method can be found in the last-mentioned study.
We experiment with the SDTO by thresholding on prel as well as on precision. Thresholding on fixed prels happens to optimize linear utility measures ( Lewis, 1995 ), with corresponding rank thresholds:  X  h = 0.5000: It corresponds to 1 loss per relevant non-retrieved and 1 loss per non-relevant retrieved, i.e. the Error Rate,  X  h = 0.3333: It corresponds to 2 gain per relevant retrieved and 1 loss per non-relevant retrieved, i.e. the T9U measure
These prel thresholds may optimize other measures as well; for example, 0.5000 optimizes also the utility measure of 1 gain per relevant retrieved and 1 loss per non-relevant retrieved. Thus, irrespective of which measure prel thresholds optimize, we arbitrarily enrich the experimental set of levels with four more thresholds: 0.9900, 0.9500, 0.8000, and 0.1000.
Furthermore, having normalized scores to prels, we can estimate precision in any top-K set by simply adding the prels and dividing by K . The estimated precision can be seen as the generality in the sub-ranking. According to the hypothesis that the effectiveness of CBIR is positively correlated to query generality, we experiment with the following thresholding: max K : Prec@ K &gt; g , where for g is the minimum generality required by the CBIR at hand for good effectiveness. Having no clue on usable g values, we arbitrarily try levels of g at 0.9900, 0.9500, 0.8000, 0.5000, 0.3333, and 0.1000. 3.3. Fusion of image modalities
In the current setup, we index images with more than one descriptor. Moreover, most topics have more than one example query image. Thus, we briefly describe here an appropriate method for fusing the image modalities resulting from the multi-ple descriptors and example images.
 Let i be the index running over example images ( i =1,2, ... ) and j running over the visual descriptors ( j 2 {1,2}). Thus,
DESC ji is the score of a collection item against the i th example image for the j th descriptor. We normalize DESC
MinMax, taking the maximum score seen across example images per descriptor. Assuming that the descriptors capture orthogonal information, we add their scores per example image. Then, to take into account all example images, the natural combination is to assign to each collection image the maximum similarity seen from its comparisons to all example images; this can be interpreted as looking for images similar to any of the example images. Summarizing, the score s for a collection image against the topic is defined as: 3.4. Setting the baseline
In initial experiments, we investigated the effectiveness of each of the stages individually, trying to tune them for best results.

In the textual stage, we employ the tf.idf model since it has been found to work well with the SDTO ( Arampatzis, Rob-ertson, &amp; Kamps, 2009 ). The SDTO method fits a binary mixture of probability distributions on the score distribution (SD).
A previous study suggested that while long queries tend to lead to smoother SDs and improved fits, threshold predictions are better for short queries of high quality keywords ( Arampatzis et al., 2009 ). To be on the safe side, in initial experiments we tried to increase query length by enabling pseudo relevance feedback of the top-10 documents, but all our combinations of the parameter values for the number of feedback terms and initial query weight led to significant decreases in the effec-tiveness of text retrieval. We attribute this to the noisy nature of the annotations. Consequently, we do not run any two-stage experiments with pseudo relevance feedback at the first textual stage.

In the visual stage, first we tried the JCD alone, as the collection seems to contain more color natural images than graphics, and used only the first example image; this represents a simple but practically realistic setup. Then, incorporating all exam-ple images but still using only the JCD, we used Eq. (1) which in this case simplifies to s = max able example images and descriptors, we used Eq. (1) as given above. Table 1 presents the results.

The image-only runs perform far below the text-only run. This puts in perspective the quality of the currently effective global CBIR descriptors: their effectiveness in image retrieval is much worse than the effectiveness of the traditional tf.idf text retrieval model even on sparse and noisy annotations. Since the image-only runs would have provided very weak base-lines, we choose as a much stronger baseline for statistical significance testing the text-only run. This makes sense also from an efficiency point of view: if using a secondary text modality for image retrieval is more effective than current CBIR meth-ods, then there is no reason at all for using computationally costly CBIR methods.

Comparing the image-only runs to each other, we see that using more information X  X ither from more example images or more descriptors X  X mproves effectiveness. In order to investigate the impact of the effectiveness level of the second stage on the whole two-stage procedure, we will present two-stage results for both the best and the worst CBIR methods. 3.5. Experimental results
Table 2 presents two-stage image retrieval results against text-and image-only retrieval. It is easy to see that the dynamic thresholding methods improve retrieval effectiveness in most of the experiments. Especially, dynamical thresholding using h shows improvements for all values we tried. The greatest improvement (+23%) is observed in P@10 for h = 0.8. The table con-tains lots of numbers; while there may be consistent increases or decreases in some places, in the rest of this section we focus and summarize only the statistically significant differences.

Irrespective of measure and CBIR method, the best thresholds are roughly at: 25 or 50 for K , 0.95 for g , and 0.8 for h . The weakest thresholding method is the static K : there are very few improvements only in P@20 at tight cutoffs, but they are accompanied by a reduced MAP and bpref. Actually, static thresholds hurt MAP and/or bpref almost anywhere. Effectiveness degrades also in early precision for K = 1000. Dynamic thresholding is much more robust. Comparing the two CBIR methods at the second stage, the stronger method helps the dynamic methods considerably while static thresholding does not seem to receive much improvement.

Concerning the dynamic thresholding methods, the probability thresholds h correspond to tighter effective rank thresh-olds than these of the precision thresholds g , for g and h taking values in the range [0.1000,0.9900]. As a proxy for the effec-tive K we use the median threshold e K across all topics. This is expected since precision declines slower than prel.
Nevertheless, the fact that a wide range of prel thresholds results to a tight range of some score per query. This makes the end-effectiveness less sensitive to prel thresholds in comparison to precision thresh-olds, thus more robust against possibly unsuitable user-selected values. Furthermore, if we compare the dynamic methods at similar e K , e.g. g = 0.9900 to h = 0.9500 ( e K 50) and g = 0.8000 to h = 0.5000 ( slightly better. Fig. 1 depicts the evaluation measures against the top image results for the query of Fig. 3 a. We will explain in Section 4 what  X  X  X OP-SURF 10k X  X  and Fig. 3 f are.
As we mentioned in Section 2 , a possible drawback of the two-stage setup is that relevant images with empty or very noisy textual descriptions would be completely missed, since they will not be retrieved by the first stage, possibly hurting hardly reaches 0.35 at top-1000; since the average relevant items per topic is 252.3 we can conclude that the text-only stage is far from perfect and that the aforementioned drawback has some merit.

In summary, static thresholding improves initial precision at the cost of MAP and bpref, while dynamic thresholding on precision or prel does not have this drawback. The choice of a static or precision threshold influences greatly the effective-ness, and unsuitable choices (e.g. too loose) may lead to a degraded performance. Prel thresholds are much more robust in this respect. As expected, better CBIR at the second stage leads to overall improvements, nevertheless, the thresholding type seems more important: While the two CBIR methods we employ vary greatly in performance (the best has almost double the effectiveness of the other), static thresholding is not influenced much by this choice; we attribute this to its lack of respect for the number of relevant items and for the ranking quality. Dynamic methods benefit more from improved CBIR. Overall, prel thresholds perform best, for a wide range of values. 4. Two-stage with bag-of-visual-words
So far, the hypothesis for effective two-stage image retrieval was that CBIR can do better than text retrieval in small or high query generality collections; a hypothesis deemed valid by the experiments in Section 3 , at least for global features such as the CCDs. In this section, we investigate whether the hypothesis holds also for other types of features beyond global.
SURF local features are among the best-interest-point descriptors currently available. They have been shown to outper-form other well-known methods based on interest points, such as SIFT ( Lowe, 2004 ) and GLOH ( Mikolajczyk &amp; Schmid, &amp; Lew, 2010 ).

Bag-of-visual-words (BOVW) ( Cula &amp; Dana, 2001 ) is a representation of images which is built using a large set of local features. They are inspired by the bag-of-words models in text classification, where a document is represented by a set of distinct keywords. Analogously, in BOVW models, an image is represented by a set of distinct visual words derived from local features. BOVWs are fast becoming a widely used representation for content-based image retrieval, for mainly two reasons: their better retrieval effectiveness over global feature representations on near identical images, and much better efficiency than local feature representations. However, experimental results of reported work show that the commonly generated vi-sual words are still not as expressive as the text words ( Zhang, Tian, Hua, Huang, &amp; Li, 2009 ).

The most modern implementation of BOVW suitable for a wide range of CBIR applications is the TOP-SURF ( Thomee et al., 2010 ) descriptor. TOP-SURF combines interest points with visual words, resulting in a high performance compact descriptor.
The TOP-SURF descriptor, initially extracts SURF local features from the images and then groups these features into a desired weighting is applied in order to assign a score to all the visual words in the histogram. The TOP-SURF image descriptor is created by choosing the top scoring visual words.
 4.1. TOP-SURF vs. CCDs
We index the images with the TOP-SURF descriptor, employing two visual-word dictionaries: one with 10,000 and the other with 200,000 visual words. In order to investigate the impact of dictionary size on both the retrieval effectiveness and the matching time. For CBIR, we used the JCD, SpCD, and TOP-SURF, separately, as well as the late fusion setup of
JCD and SpCD as in Eq. (1) . The general setup is similar to the one described in Section 3 . For measuring efficiency, we report the average matching time per topic. The results are presented in Table 3 .

For all h , the CCDs perform similarly (JCD) or significantly better (SpCD and JCD/SpCD) than the text-only baseline, while the TOP-SURF descriptor shows significant drops in effectiveness irrespective of dictionary size. The differences in effective-ness of CCDs and TOP-SURF are larger in early precision than in MAP. We also observe that the TOP-SURF effectiveness de-grades with increased dictionary size. Furthermore, TOP-SURF are more sensitive to the choice of h :as h decreases (i.e. for larger K s), effectiveness deteriorates faster than this of the CCDs. Efficiency-wise, the experimental results show that although the TOP-SURF uses a speedy matching algorithm, it still cannot match the speed of the CCDs.

A possible explanation for the inferior effectiveness of BOVW in the proposed setup is the following. Although BOVW models have the ability to recognize objects and retrieve near-duplicate (to the query) images, this advantage over global features such as CCDs is diminished when visual diversity is enhanced by using a secondary medium, such as text, to pre-filter images. Thus, BOVW are not suitable for the proposed two-stage setup. In practice, applications like Google Gog-gles, where a user is querying an image in order to recognize a logo or a famous painting, BOVW models should be more effective. But in applications like Google Similar Images, where images are pre-filtered by text similarity, global features should be more suitable. 5. Two-stage vs. fusion of media
Fusion of different media into a single ranking is not trivial ( van Leuken et al., 2009 ). In this section, we provide an exper-imental comparison of fusion to two-stage retrieval. Although in Section 2 , we argued theoretically against fusion, in view also of the underlying assumption, hypothesis and drawbacks of two-stage retrieval, a comparison of the effectiveness of the two methods is in order.

Various techniques have been proposed to effectively fuse multimedia, e.g. simple linear weighting, principle component analysis ( van Zwol, 2005 ), or using a weighted schema for aggregating features based on item scores ( Wilkins, Ferguson, &amp; Smeaton, 2006 ). In Zhou, Depeursinge, and M X ller (2010) , traditional approaches such as maximum combination (comb-
MAX), sum combination (combSUM), and others, were employed. The results show that fused runs outperform the best sin-gle-medium runs. Several approaches for visual and textual information fusion that have been used in ImageCLEF over the past seven years are described in Mller et al. (2010) .In Atrey, Hossain, El-Saddik, and Kankanhalli (2010) , the authors provide an overview of the state of the art fusion strategies, which are used for combining multiple modalities in order to accomplish various multimedia analysis tasks. 5.1. Fusion of media
In Section 3.3 we elaborated on how to fuse image modalities. Here, we are extending this into fusing media, in a way suitable for the current setup. Starting from Eq. (1) and incorporating text as orthogonal medium, we add its contribution. Thus, the score s for a collection image against the topic is defined as:
The parameter w controls the relative contribution of the two media; for w = 1 retrieval is based only on text while for w =0 is based only on image. We report for five w values between 0 and 1. 5.2. An experiment The general setup is similar to the one described in Section 3 . For the two-stage experiment we threshold on prel with the
SDTO. This was found in Section 3.5 to be more effective and robust than thresholding on estimated precision. We report for five prel thresholds.

Table 4 presents the effectiveness of fusion and two-stage against text-and image-only runs. Irrespective of measure, the best parameter values are roughly at: 0.6666 X 0.8000 for fusion X  X  w , and 0.8000 for two-stage X  X  h . Both methods perform sig-nificantly better than text-only and far better than image-only. On the one hand, two-stage achieves better results than fu-sion, but it has more variability across topics: fusion passes the test at lower significance levels (i.e. higher confidence). On the other hand, effectiveness is less sensitive to the values of h than the values of w : two-stage provides significant improve-ments in all measures for a wide range of thresholds (i.e. 0.3333 X 0.9900), while fusion can significantly deteriorate effective-ness for unsuitable choices of w .

Both methods are significantly better that text-and image-only baselines. Indicatively, the largest improvements in MAP against the text-only baseline are +9.0% and +10.4% for fusion and two-stage respectively, while the corresponding improve-ments in P@10 are +15.0% and +22.9%.

While two-stage performs better than fusion in 3 out of 4 measures, improvements are statistically non-significant at the to the weighing parameter of the contributing media, while two-stage provides a much lower sensitivity to its thresholding parameter but has a higher variability. Nevertheless, two-stage has an obvious efficiency benefit over fusion: it cuts down greatly on costly image operations. Although we have not measured running times, only the 0.02 X 0.05% of the items (on average) had to be scored at the image stage. While there is some overhead for estimating thresholds, this offsets only a small part of the efficiency gains. 6. Related work only on visual re-ranking. Subset re-ranking by visual content has been seen before, but mostly in different setups than the one we consider or for different purposes, e.g. result clustering or diversity. It is worth mentioning that all the previously proposed methods we review below used global image features to re-rank images.

For example, Barthel (2008) proposed an image retrieval system using keyword-based retrieval of images via their anno-tations, followed by clustering of the top-150 results returned by Google Images according to their visual similarity. Using the clusters, retrieved images were arranged in such a way that visually similar images are positioned close to each other.
Although the method may have had a similar effect to ours, it was not evaluated against text-only or image-only baselines, and the impact of different values of K was not investigated. In van Leuken et al. (2009) , the authors retrieved the top-50 results by text and then clustered the images in order to obtain a diverse ranking based on cluster representatives. The clusters were evaluated against manually-clustered results, and it was found that the proposed clustering methods tend to reproduce manual clustering in the majority of cases. The approach we have taken does not target to increasing diversity.
Another similar approach was proposed in Popescu et al. (2009) , where the authors state that Web image retrieval by text queries is often noisy and employ image processing techniques in order to re-rank retrieved images. The re-ranking tech-nique was based on the visual similarity between image search results and on their dissimilarity to an external contrastive results and dissimilar to the external class. To determine the visual coherence of a class, they took the top 30% of retrieved images and computed the average number of neighbors to the external class. The effects of the re-ranking were analyzed via a user-study with 22 participants. Visual re-ranking seemed to be preferred over the plain keyword-based approach by a large majority of the users. Note that they did not use an image query but only a text one; in this respect, the setup we have considered differs in that image queries are central, and we do not require external information.

In Myoupo et al. (2009) , the authors proposed also a two-stage image retrieval system with external information require-ments: the first stage is text-based with automatic query expansion, whereas the second exploits the visual properties of the query to improve the results of the text search. In order to visually re-rank the top-1000 images, they employed a visual model (a set of images which depicts each topic) using Web images. To describe the visual content of the images, several methods using global or local features were employed. Experimental results demonstrated that visual re-ranking improves the retrieval performance significantly in MAP, P@10 and P@20. We have confirmed that visual re-ranking of top-ranked re-sults improves early precision, though with a simpler setup without using external information.
 Some other similar setups to the one we propose are these in Berber and Alpkocak (2009) and Maillot et al. (2006) .In
Berber and Alpkocak (2009) , the authors trained their system to perform automatic re-ranking on all results returned by text retrieval. The re-ranking method considered several aspects of both document and query (e.g. generality of the textual fea-tures, color amount from the visual features). Improved results were obtained only when the training set had been derived from the database which is searched. Our method re-ranks the results using only visual features; it does not require training and can be applied to any database. In Maillot et al. (2006) , the authors re-rank the top-K results retrieved by text using vi-sual information. The rank thresholds of 60 and 300 were tried and both resulted to a decrease in mean average precision compared to the text-only baseline, with the 300 performing worse. Our experiments have confirmed their result: static thresholds degrade MAP. They did not report early precision figures. 7. Conclusions and directions for further research
We have experimented with two-stage image retrieval from a large multimedia database, by first using a text modality to rank the collection and then perform content-based image retrieval only on the top-K items. In view of previous literature, the biggest novelty of our method is that re-ranking is not applied to a preset number of top-K results, but K is calculated dynamically per query to optimize a predefined effectiveness measure. Additionally, the proposed method does not require any external information or training data.

The choice between static or dynamic nature of rank-thresholds has turned out to make the difference between failure and success of the two-stage setup. We have found that two-stage retrieval with dynamic thresholding is more effective and robust than static thresholding, practically insensitive to a wide range of reasonable choices for the measure under opti-mization, and beats significantly the text-only and several image-only baselines.

Additionally, we have compared fusion to dynamic two-stage retrieval. We have found that also fusion is significantly better than text-and image-only baselines. While two-stage performs better than fusion in 3 out of 4 measures, improve-ments are statistically non-significant at the 0.05 level. Further, both methods are robust in different ways: fusion provides less variability across topics but it is sensitive to the weighing parameter of the contributing media, while two-stage provides a much lower sensitivity to its thresholding parameter but has a higher variability.

A two-stage approach, irrespective of thresholding type, has also an obvious efficiency benefit over fusion: it cuts down greatly on expensive image operations. Although we have not measured running times, only the 0.02 X 0.05% of the items (on average) had to be scored at the expensive image stage for effective retrieval from the collection at hand. While for the dy-namic method there is some overhead for estimating thresholds, this offsets only a small part of the efficiency gains.
We also investigated the performance of the visual codebook approach, specifically the TOP-SURF image descriptor, in the two-stagesetup.WefoundthatTOP-SURFislesseffectivethan globaldescriptorssuchaCCDs:betterthanimage-onlybutdoes not beat the text-only run. In efficiency, TOP-SURF is slower in matching speed than CCDs. Although the visual codebook meth-ods are currently trendy because of their ability to recognize objects and retrieve near-duplicate (to the query) images, this advantage over global features is diminished when visual diversity is enhanced by first using a secondary text modality to pre-filter images. Thus, visual codebook methods are less suitable than global descriptors for the proposed two-stage setup.
There are a few interesting directions to pursue in the future. First, the idea can be generalized to multi-stage retrieval for multimodal databases, where rankings for modalities are successively being thresholded and re-ranked according to a modality hierarchy. In this respect, a suitable application may be SenseCam ( Hodges, Williams, Berry, &amp; Izadi, 2006 ) X  X he sensor-augmented wearable still camera. SenseCam captures a digital record of the wearer X  X  day, by recording a series of images and other sensor data. The measurements taken from the sensors are used to augment and enhance the detection of concepts from visual features ( Byrne, Doherty, Snoek, Jones, &amp; Smeaton, 2010 ). In Doherty, Conaire, Blighe, Smeaton, and O X  X onnor (2008) , the authors propose a late fusion method in order to combine information from image with data from the available sensors X  X uch as Accelerometer, Temperature and Passive InfraRed (PIR) X  X n order to retrieve recorded events.
In applications involving many media and huge amounts of data, multistage retrieval could be a method alternative to fu-sion, providing an effective and more efficient solution.

Second, we have not implemented any query classification to determine fusion weights dynamically per query ( Yan, Yang, &amp; Hauptmann, 2004 ) but used global weighting. This was a conscious decision in order to compare fusion to two-stage in their own merits without any training information. Recall that two-stage was also performed globally, by using a single threshold value for all queries optimizing some pre-defined effectiveness measure. In this respect, one may consider to implement query classification and investigate the relationship between query classes and suitable effectiveness measures to optimize.

Last, all experiments were performed on a standardized multimedia snapshot of Wikipedia; as an encyclopedic domain, this may be deemed narrow. However, our focus was to improve image retrieval with the help of other media beyond image, and this collection is one of the largest benchmark image databases for today X  X  standards. It is also highly heterogeneous, containing color natural images, graphics, grayscale images, etc., in a variety of sizes, making it a challenging experimental setup. As more large multimedia testbeds become available, further experiments may reinforce our findings. References
