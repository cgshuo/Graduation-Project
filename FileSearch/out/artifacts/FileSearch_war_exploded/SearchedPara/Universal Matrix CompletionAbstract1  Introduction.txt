 Srinadh Bhojanapalli BSRINADH @ UTEXAS . EDU The University of Texas at Austin Microsoft Research, India In this paper, we study the problem of universal low-rank matrix completion. Low-rank matrix completion is an im-portant problem with several applications in areas such as recommendation systems, sketching, and quantum tomog-raphy (Recht et al., 2010; Cand ` es &amp; Recht, 2009; Gross et al., 2010). The goal in matrix completion is to re-cover a rank-r matrix, given a small number of entries of the matrix. That is, to find a matrix M given values { M ij , ( i, j ) 2  X  } , where  X  is the set of observed indices. Recently, several methods with provable guarantees have been proposed for solving the problem under the follow-ing two assumptions: a) M is incoherent, b)  X  is sampled uniformly and |  X  | cnr log n . Moreover,  X  needs to be re-sampled for each matrix M that is to be recovered, i.e., the same  X  cannot be re-used without worsening the guar-antees significantly.
 While the first assumption can be shown to be necessary for any matrix oblivious sampling, the second assumption is relatively restrictive and might not hold in several practi-cal settings. The main goal of this work is to develop a gen-eral result that can handle other sampling schemes as well. Moreover, we aim to develop a universal method where one fixed  X  would be enough to recover any low-rank matrix M . Such a universal recovery result is highly desirable in several signal processing applications, where the goal is to design one  X  that can recover any low-rank signal matrix M by observing M over  X  alone.
 To this end, we reduce the problem of recoverability using an index set  X  to the spectral gap (gap between largest and second largest singular values) of G , where G is a bipar-tite graph whose biadjacency matrix G 2 R n  X  n is given by: G ij =1 iff ( i, j ) 2  X  and G ij =0 otherwise. In particular, we show that if G has a large enough spectral gap and if the rank-r matrix M satisfies the standard in-coherence property, then the best rank-r approximation of P  X  ( M ) (see (1)) itself is enough to get a  X  X easonable X  ap-proximation to M (see Theorem 4.1 for details).
 Note that our approximation result is similar to Theorem 1.1 of (Keshavan et al., 2010), but our result holds for any  X  with large spectral gap unlike (Keshavan et al., 2010) that requires uniform sampling. On the other hand, we require explicit incoherence condition on singular vectors of M , while the result of (Keshavan et al., 2010) only requires a bound on M max = max ij M ij . The later assumption is strictly weaker assumption; the assumptions coincide for PSD matrices.
 Next, we show that by assuming certain stronger inco-herence properties, the number of samples required by the popular nuclear-norm minimization method (Cand ` es &amp; Recht, 2009) to recover back M depends only on n, r and the spectral gap of the d -regular bipartite graph G . In partic-ular, we require d 2 ( G )  X  r , where 2 ( G ) is the second largest singular value of G . Hence, if 2 ( G )= O ( i.e., if G is an expander, then |  X  | = nd = O ( nr 2 ) samples suffice for exact recovery.
 Our recovery results applies to any low rank matrix M that satisfies the stronger incoherence property, given that the fixed graph G has a large spectral gap. To the best of our knowledge, this is the first universal guarantee for matrix completion . Furthermore, using recent results by (Feige &amp; Ofek, 2005) we show that for the standard uniform sam-pling of  X  , only O ( nr 2 ) samples suffice for exact recovery of a rank-r matrix M that satisfies a stronger incoherence condition (see A 2 in Section 3).
 Next, we discuss the stronger incoherence property that we require for our universal recovery guarantees. In particular, we show that the standard incoherence condition alone can-not provide universal recovery with any graph G and hence a stronger incoherence property is required.
 Finally, we empirically demonstrate our observation that, instead of the number of samples, the spectral gap of G is what really governs recoverability of the true matrix. In particular, we construct a family of graphs based on the stochastic block model and show that the probability of success grows linearly with the spectral gap, irrespective of the number of samples.
 Notation : We denote matrices by capital letters (e.g. U ) and vectors by small letters (e.g. u ). U T denotes the trans-pose of the matrix U . U ij represents the ( i, j ) -th element of
U . U i represents the i -th column of U and U i represents the i -th row of U (but in column format). k u k represents the L 2 norm of u and k U k represents the spectral norm of U , i.e., k U k = max x : k x k X  1 k Ux k . k U k F represents the Frobenius norm and k U k 1 is the absolute maximum ele-ment of U . C = A.B represents the Hadamard product of A, B , i.e., C ij = A ij B ij . Similarly, ( u.v ) i = u i notes the all 1  X  X  vector. 1 ? represents a unit vector that is perpendicular to 1 and is determined appropriately by the context.
 Paper Organization : In the next section, we discuss some related works. Then, in Section 3, we define the problem of matrix completion and the bipartite graph G that we use. We present our main results in Section 4 and discuss the ad-ditional incoherence assumption in Section 5. In Section 6, we present observations from our empirical study. Finally in Section 7, we provide the proof of our exact recovery result. Matrix completion : In a seminal paper on matrix com-pletion, (Cand ` es &amp; Recht, 2009) showed that any n  X  n incoherent matrix of rank r can be recovered from Cn 1 . 2 r log( n ) uniform random samples using nuclear norm minimization. Later, assuming the matrix to be strongly incoherent , (Cand ` es &amp; Tao, 2010) improved the sample complexity for nuclear norm minimization method to O ( nr log 6 ( n )) . Subsequently, (Recht, 2009; Gross, 2011) generalized this result for any incoherent matrix using matrix Bernstein inequalities and presented signifi-cantly simpler proofs. Concurrently other algorithms were shown to recover incoherent matrices using O ( nr log( n )) (or worse) samples such as: SVD followed by descent on Grassmanian manifold (Keshavan et al., 2010), alternating minimization (Jain et al., 2012). We note that all the above mentioned results need to assume a rather restrictive sam-pling scheme, i.e., each entry is sampled uniformly at ran-dom and furthermore require a fresh set of samples for each new matrix. Moreover, the number of samples required is at least O ( nr log n ) .
 Other sampling schemes : Recently, there has been some results for different type of sampling schemes such as power-law distributions (Meka et al., 2009), but here again universal results are not known and furthermore the pro-posed algorithms are not robust to noise. Another line of work has been to devise sampling schemes dependent on the data matrix (Chen et al., 2013), (Kir  X  aly &amp; Tomioka, 2012). Naturally, these schemes cannot be universal as the sampling scheme itself is dependent on the data ma-trix. Furthermore, practicality of such schemes is not clear a priori.
 Universality: Universality is an important property in sig-nal processing or sketching applications, as the goal there is to have one fixed sampling operator that performs well for all the given signals. While, universality results are well known for several other sensing problems, such as sparse vector recovery (Candes &amp; Tao, 2005), one-bit compres-sive sensing (Gopi et al., 2013), similar results for low-rank matrix sensing are mostly restricted to RIP-type op-erators (Recht et al., 2010; Liu, 2011). Unfortunately, RIP-type of operators are typically dense, requiring knowledge of all elements of matrix to get observations, and also re-quire large storage/computational complexity. On the other hand sampling individual elements is a sparse operator and hence computationally efficient. Hence, for several sig-nal processing applications, universal matrix completion results are critical.
 In fact, several recent works have studied problems sim-ilar to that of universal matrix completion. For example, (Kir  X  aly &amp; Tomioka, 2012; Heiman et al., 2013) and (Lee &amp; Shraibman, 2013). However, there are critical differ-ences in our results/approaches that we now highlight. In (Kir  X  aly &amp; Tomioka, 2012) authors consider an algebraic approach to analyze sufficient conditions for matrix com-pletion. While they propose interesting deterministic suf-ficient conditions, the algorithm analyzed in the paper re-quires solving an NP hard problem. In contrast we analyze the nuclear norm minimization method which is known to have several efficient implementations. In (Heiman et al., 2013) and (Lee &amp; Shraibman, 2013) authors consider sam-pling based on expanders but only provide generalization error bounds rather than exact recovery guarantee. More-over, the recovered matrix using their algorithm need not have a low-rank. Let M 2 R n 1  X  n 2 be a rank-r matrix and let n 1 n 2 . Define n = max { n 1 ,n 2 } = n 1 . Let M = U  X  V T be the SVD of M and let 1 2  X  X  X  r be the singular values of
M . We observe a small number of entries of M indexed by a set  X  2 [ n 1 ]  X  [ n 2 ] . That is, we observe M ij  X  . Define the sampling operator P  X  : R n 1  X  n 2 ! R n 1  X  n as: Next, we define a bipartite graph associated with the sam-pling operator P  X  . That is, let G =( V, E ) be a bi-partite graph where V = { 1 , 2 ,...,n 1 } [ { 1 , 2 ,...,n and ( i, j ) 2 E iff ( i, j ) 2  X  . Let G 2 R n 1  X  n 2 be the biadjacency matrix of the bipartite graph G with G ij = 1 iff ( i, j ) 2  X  . Note that, P  X  ( M )= M.G , where . de-notes the Hadamard product.
 Now, the goal in universal matrix completion is to design a set  X  and a recovery algorithm, s.t., all rank-r matrices M can be recovered using only P  X  ( M ) . In the next sec-tion, we present two results for this problem. Our first re-sult gives an approximate solution to the matrix completion problem and our second result gives exact recovery guaran-tees.
 For our results, we require G , that is associated with  X  be a d -regular bipartite graph with large spectral gap. More concretely, we require the following two properties from the sampling graph G : Assumptions on G/  X  :  X  G1 Top singular vectors of G are all 1  X  X  vector.  X  G2 1 ( G )= d and 2 ( G )  X  C Note that as the graph is d -regular, hence |  X  | = nd . The eigenvalues of the adjacency matrix of the bipartite graph G are { i ( G ) , i ( G ) } ,i =1 ,..n . We state all the definitions in terms of singular values of G instead of the eigenvalues of the adjacency matrix. The above two prop-erties are satisfied by a class of expander graphs called Ra-manujan graphs; in fact, Ramanujan graphs are defined by using this spectral gap property: Definition 3.1 (Ramanujan graph (Hoory et al., 2006)) . Let ( G ) , 2 ( G ) ,..., n ( G ) be the singular values of G creasing order. Then, a d -regular bipartite graph G is a Ramanujan graph if 2 ( G )  X  2 Ramanujan graphs G are well-studied in literature and there exists several randomized/deterministic methods to gener-ate such graphs. We briefly discuss a couple of popular constructions in Section 4.2.
 Incoherence assumptions : Now, we present incoherence assumptions that we impose on M : A 1 || U i || 2  X  A 2 k d 0 = dn 2 /n 1 . Note that A 1 is the standard incoherence assumption required by most of the existing matrix com-pletion results. However, A 2 is a stricter assumption than A 1 and is similar to the stronger incoherence property in-troduced by (Cand ` es &amp; Tao, 2010). We discuss necessity of such assumption for universal matrix completion in Sec-tion 5. We now present our main results for the matrix completion problem. We assume that  X  is generated using a bipar-tite d -regular expander and satisfies G 1 and G 2 (see Sec-tion 3). Our first result shows that, if M satisfies A 1 , then the best rank-r approximation of P  X  ( M ) is  X  X lose X  to and hence serves as a good approximation for M that can also be used for initialization of other methods like alter-nating least squares. Our second results shows that if M satisfies both A 1 and A 2 , then using nuclear-norm min-imization based method, P  X  ( M ) can be used to recover back M exactly. 4.1. Matrix approximation Theorem 4.1. Let G be a d -regular bipartite graph satis-fying G 1 and G 2 . Let M be a rank-r matrix that satisfies assumption A 1 . Then,
That is, k n k r , where P k ( A ) is the best rank-k approximation of and can be obtained using top-k singular vectors of A . Now, if M is a PSD matrix then the above result is exactly same as the Theorem 1.1 of (Keshavan et al., 2010). For non-PSD matrices, our result requires a bound on norm of each row of singular vectors of M , while the result of (Ke-shavan et al., 2010) only requires a bound on the largest element of M , hence is similar to our requirement but is strictly weaker as well.
 On the other hand, our result holds for all M for a given if  X   X  X  associated graph G satisfies both G 1 and G 2 . More-over, if G is generated using an Erdos-Renyi graph then, after a standard trimming step, the above theorem directly implies(for PSD matrices) Theorem 1.1 of (Keshavan et al., 2010). Finally, we would like to stress that our proof is sig-nificantly simpler and is able to exploit the fact that Erd  X  os-R  X  enyi graphs have good spectral gap in a fairly straightfor-ward and intuitive manner.
 We now present a detailed proof of the above theorem. Proof. Let M = U  X  V T , U,V 2 R n  X  r . Note that, k Now, Let y.U i =  X  i 1 + i 1 i Hence, =  X 
Cn p where  X  1 follows from assumption G 2 and  X  2 follows from the Cauchy-Schwarz inequality. Now, where  X  1 follows from A 1 and  X  2 follows by using k y k 1 . Using similar argument as above, P r Theorem now follows by using (5), (6), and the above inequality. The proof of the second part is given in ap-pendix A. 4.2. Nuclear norm minimization We now present our result for exact recovery of the ma-trix M using P  X  ( M ) alone. For recovery, we use the stan-dard nuclear norm minimization algorithm, i.e., we obtain a matrix X by solving the following convex optimization problem: where k X k  X  denotes the nuclear norm of X ; nuclear norm of X is equal to the sum of its singular values.
 As mentioned in Section 2, nuclear norm minimization technique is a popular technique for the low-rank matrix completion problem and has been shown to provably re-cover the true matrix, assuming that  X  is sampled uni-formly at random and |  X  | cnr log n (Cand ` es &amp; Tao, 2010).
 Below, we provide a universal recovery result for the nuclear-norm minimization method as long as the samples  X  come from G that satisfies G 1 and G 2 .
 Theorem 4.2. Let M be an n 1  X  n 2 matrix of rank r sat-isfying assumptions (A1) and (A2) with d  X  1 generated from a d -regular graph G that satisfies the as-sumptions (G1) and (G2). Also, let d 36 C 2  X  2 |  X  | = nd 36 C 2  X  2 0 r 2 max { n 1 ,n 2 } . Then M is the unique optimum of problem (7) .
 Note that the above result requires only deterministic con-straints on the sampling operator P  X  and guarantees exact recovery for any matrix M that satisfies A 1 ,A 2 . As men-tioned earlier, A 2 is a stronger assumption than A 1 . But as we show in Section 5, universal recovery is not possible with assumption A 1 alone.
 We can use the above theorem to derive results for several interesting sampling schemes such as random d -regular graphs. Using Theorem 1 in (Friedman, 2003), the second singular value of a random d -regular graph is  X  2  X  , for every  X  &gt; 0 , with high probability. Hence, a random d -regular graph, with high probability, obeys G 1 and G 2 which implies the following exact recovery result: Corollary 4.3. Let M be an n 1  X  n 2 matrix of rank r sat-isfying assumptions (A1) and (A2) with d  X  1 generated from a random d -regular graph, then M is the unique optimum of program (7) when d 36  X  4  X  2 high probability.
 Note that the standard completion results such as (Cand ` es &amp; Recht, 2009), (Keshavan et al., 2010) generate  X  us-ing Erd  X  os-R  X  enyi graph, which are slightly different than the random d -regular graph we considered above. How-ever, (Feige &amp; Ofek, 2005) showed that the second largest singular value of the Erd  X  os-R  X  enyi graph, G ( n 1 ,n O ( p p = c/n 2 , i.e. n 2  X  p is a constant, trimming the graph (i.e., removing few nodes with high degree) gives a graph G s.t. ( G )= O ( to obtain the following result: Corollary 4.4. Let M be an n 1  X  n 2 matrix of rank r sat-isfying assumptions (A1) and (A2) with d  X  1 generated from a G ( n, p ) graph after trimming, then M is the unique optimum of program (7) when p 36 c X  2 0 r 2 with high probability.
 While the above two results exploit the fact that a random graph is almost a Ramanujan expander and hence our gen-eral recovery result can be applied, the graph construction is still randomized. Interestingly, (Lubotzky et al., 1988; Margulis, 1988; Morgenstern, 1994) proposed explicit de-terministic constructions of Ramanujan graphs when d 1 is a prime power. Moreover, (Marcus et al., 2013) showed that bipartite Ramanujan graphs exist for all n and d . How-ever, explicit construction for all n and d still remains an open problem. In this section, we discuss the two assumptions A 1 and A 2 that are mentioned in Section 3.
 Note that A 1 is a standard assumption that is used by most of the existing approaches (Cand ` es &amp; Recht, 2009), (Ke-shavan et al., 2010). Moreover, it is easy to show that for any matrix  X  X blivious X  sampling approach, this assumption is necessarily required for exact recovery. For example, if G ij =0 , i.e., ( i, j ) -th element is not observed then we can-not recover M = e i e T However, A 2 is a slightly non-standard assumption and in-tuitively it requires the singular vectors of M to satisfy RIP. Note that A 2 is similar to the strong incoherence property introduced by (Cand ` es &amp; Tao, 2010). Below we show the connection between strong incoherence property (SIP) as-sumed in (Cand ` es &amp; Tao, 2010) and assumption A 2 . Claim 5.1. Let M 2 R n 1  X  n 2 be a rank-r matrix. Let M = | h e i ,UU T e j i | h e i ,VV T e j i Then, M satisfies A 2 for all d r and d  X   X  1 p r . Note that the above claim holds with d =  X  1 p r, 8 d r . This bound is independent of d and hence weak; as d becomes close to n 1 , n 1 to 0 since U T U = I . We leave the task of obtaining a stronger bound as an open problem.
 In the context of universal recovery, a natural question is if any additional assumption is required or the standard A 1 assumption alone suffices. Here, we answer this question in negative. Specifically, we show that if M satisfies A 1 only, then universal recovery guarantee for even rank-2 matrices cannot be provided by using as many as n 1 n 2 / 4 observa-tions.
 Claim 5.2. Let  X  be a fixed set of indices and let P  X  be the sampling operator as defined in (1) . Let n 1 = n 2 = n and let |  X  | = n 2 / 4 . Then, there exists a rank-2 matrix cannot be recovered exactly from P  X  ( M ) .
 Now, another question is if we require a property as strong as
A 2 and if just a lower-bound on k U i k 2 , k V j k 2 is enough for universal recovery. The proof of the above claim (in appendix) shows that even if k U i k 2 , k V j k 2 are lower-bounded, then also exact recovery is not possible. In this section, we will present a few empirical results on both synthetic and real data sets. The goal of this section is to demonstrate effect of the spectral gap of the sampling graph G (associated with  X  ) on successful recovery of a matrix.
 First, we use synthetic data sets generated in the follow-ing manner. We first sample U, V 2 R 500  X  10 using stan-dard normal distribution. We then generate rank-10 ma-trix M , using M = UV T . As U, V are sampled from the normal distribution, hence w.h.p., M satisfies incoher-ence assumptions A 1 ,A 2 mentioned in Section 3. Next, we generate a sequence of sampling operators P  X  (and the associated graph G ) with varying (relative) spectral gap( 1 2 ( G ) / 1 ( G ) ) by using a stochastic block model. In the basic stochastic block model, the nodes can be thought of as being divided into two clusters. Now, each intra-cluster edge is sampled uniformly with probability p and an inter-cluster edge is sampled uniformly with prob-ability q . Note that, when p = q , then the spectral gap is largest and when p =1 ,q =0 , then spectral gap is smaller as there are two distinct clusters in that case (Nadakuditi &amp; Newman, 2012).
 Note that number of samples generated in this model de-pends only on the value of p + q . To generate  X  (i.e., G we first fix a value for p + q , hence fixing the number of samples, and then vary p, q , which gives graphs of different down. Figure 1(a) clearly demonstrates this trend. We use an Augmented Lagrangian Method (ALM) based method (Lin et al., 2010) to solve the nuclear norm mini-mization (7) problem. A trial is considered to be successful if the relative error (in Frobenius norm) is less than 0 . 01 We average over 50 such trials to determine the success ra-tio.
 Figure 1(a) plots the (relative) spectral gap (dotted lines) and the success ratio (solid lines) as p varies. Lines of dif-ferent colors indicate different number of samples ( p + q As expected, the spectral gap increases initially, as p varies of successful recovery also follows a similar trajectory and hence, is more or less independent of the number of sam-ples (given a particular spectral gap).
 Figure 1(b) shows fraction of successful recoveries as the spectral gap increases. Here again, lines of different colors indicate different number of samples ( p + q ). Clearly, suc-cess ratio is positively correlated with the spectral gap of the sampling operator and in fact exhibits a phase transition type of phenomenon. We expect the difference in success ratio for different p + q values to decrease with increasing problem size, i.e, dimensionality of M .
 Now, we conduct an experiment to show that spectral gap of
G helps in reducing effect of noise as well. To this end, we generated noisy input matrix ( M + Z ) , where Z is a random Gaussian matrix and let = || Z || F / || M || F .We consider two values of , i.e., =0 . 1 and 0 . 2 . Figure 2(a) plots the error(in Frobenius norm) in the recovered matrix against the relative spectral gap in the noisy setting. Solid lines represent =0 . 1 and dotted lines represent =0 . 2 . Clearly, larger spectral gap leads to smaller error in recov-ery. Moreover, the  X  X atrix completion denoising X  effect (Candes &amp; Plan, 2010) can also be observed. For example, when || Z || F =0 . 1 and p + q =0 . 4 , output error is less than 0.05, and when || Z || F =0 . 2 , error is less than Temperature prediction: Finally we take a real dataset of temperature values ( T ) for 365 days at 316 different lo-cations from (NCDC), which has been used to test ma-trix completion algorithms (Candes &amp; Plan, 2010) be-fore. Note that T is approximately rank-1 matrix with ( T ) / || T || F =0 . 98 ( i ( T ) are singular values of use the block model sampling scheme to sample entries from T , and let the output of (7) be  X  T . In figure 2(b) we plot the error ||  X  T T || / || T || for different values of spec-tral gap of G and for different number of samples ( p + q ) Note that || X T || / || T || 2 ( T ) / 1 ( T ) for any rank-1 matrix X , and we see that for large enough spectral gap we achieve this bound.
 Finally in figure 2(c) we compare running times of the r-closure algorithm proposed in (Kir  X  aly &amp; Tomioka, 2012) with the nuclear norm minimization algorithm. While it is noted that this algorithm has better error guarantees, it is combinatorial and takes exponential time to compute. In this section, we present the proof of our main result (Theorem 4.2). The main steps of our proof are similar to the proof given by (Recht, 2009). The main difference is that the bounds in the existing proof assume that  X  is independent of M and hence is not adversarial and holds with high probability. In contrast, for our proofs, bounds are deterministic and our proofs have to work under the as-sumption that M is adversarially selected for a given  X  . The key steps in the proof are: a) provide conditions that an optimal dual solution (or dual certificate) of problem (7) should satisfy, so that the true matrix M is the unique op-timum of (7), b) construct such a dual certificate and hence guarantee that M is the unique optimum of (7).
 We first introduce a few notations required by our proof. For simplicity, we assume that n 1 = n 2 = n . Note that, our proof easily generalizes to case when n 1 6 = n 2 . Define T space in V or column space in U . Hence, the projection operator P T is defined as follows: Hence any matrix in T can be written as UX T + YV T , for some X and Y such that Y and U are orthogonal to each other. Similarly, we can define the projection operator onto T Now, before presenting conditions on the dual certificate and construction of the dual certificate, we provide a few structural lemmas that show  X  X oodness X  of operators P T and P  X  . We would like to stress that the key differences of our proof from that of (Recht, 2009) is in fact proofs of these structural lemmas and also in the way we apply Lemma 7.3. We specifically show (using Lemma 7.3) that each matrix in the series used in the construction of dual certificate Y (discussed later in the section) is incoherent and has small infinity norm.
 The first lemma proves injectivity of operator P  X  on the subspace T : Lemma 7.1. Let M = U  X  V T satisfies A 1 ,A 2 and let the graph G that generates  X  satisfies G 1 ,G 2 (see Section 3). Then, for any matrix Z 2 T , Next, we provide a lemma that characterizes the  X  X iffer-ence X  between P  X  ( Z ) and Z , for any incoherent-type Z 2 T : Lemma 7.2. Let Z 2 T , i.e., Z = UX T + YV T and Y is orthogonal to U , and X and Y be incoherent, i.e., Let  X  satisfy the assumptions G 1 and G 2 , then: The next lemma is a stronger version of Lemma 7.1 for special incoherent-type matrices Z 2 T .
 Lemma 7.3. Let Z 2 T , i.e., Z = UX T + YV T and Y is orthogonal to U . Let X and Y be incoherent, i.e., Let  X  Z = Z n M,  X  that satisfy conditions given in Lemma 7.1:  X   X  Z = U  X  X T +  X  YV T and  X  X and  X  Y are incoher-The proof of the above three lemmas is provided in the ap-pendix.
 Conditions on the dual certificate : We now present the lemma that characterizes the conditions a dual certificate should satisfy so that M is the unique optimum of (7): Lemma 7.4. Let M,  X  satisfy A 1 ,A 2 and G 1 ,G 2 , respec-tively. Then, M is the unique optimum of (7) , if there exists a  X  P  X  ( Y )= Y  X  ||P T ( Y ) UV T || F  X   X  ||P T ? ( Y ) || &lt; 1 2 Having specified the conditions on dual certificate and also the key structural lemma, we are now ready to present the proof of Theorem 4.2.
 Proof of Theorem 4.2. We prove the theorem by con-structing a dual certificate Y that satisfies conditions in lemma 7.4 and hence guarantee that M is exactly recov-ered by (7). Our construction of Y is similar to the golfing scheme based construction given in (Gross, 2011; Recht, 2009). In particular, Y is obtained as the p -th term of the series given below: where W 0 = UV T . That is, Y = Y p where p = d Now, the first condition of Lemma 7.4 is satisfied trivially by construction as Y p is a sum of P  X  ( W i ) terms. Bounding ||P T ( Y ) UV T || F : By construction:
P T ( Y ) UV T = Now, note that each W k 2 T . Hence, using Lemma 7.1, where the last inequality follows by using assumption on and by using  X   X  1 / 6 . Hence, using (9), (10), we have: k P
T ( Y ) UV T k F = k W p k F  X  where the last inequality follows by using p = d Bounding k P W Moreover, let, Note that, for W 0 = UV T , c W 0 using Lemma 7.3: c W 1 ing Lemma 7.3 k -times, we get: Now, by using construction of Y , by triangle inequality, and by using the fact that P T ? is a contraction operator: k P where the last inequality follows by using Lemma 7.2. Now, using (12), (13), and Lemma 7.3, we have: Hence, proved. In this paper, we provided the first (to the best of our knowl-edge) universal recovery guarantee for matrix completion. The main observation of the paper is that the spectral gap of
G (that generates  X  ) is the key property that governs re-coverability of M using P  X  ( M ) alone. For example, if G is a Ramanujan expander (i.e., 2 ( G )= O ( have universal recovery guarantees for matrices with strong incoherence property.
 For uniformly sampled  X  , our main result implies exact recovery of constant rank matrices using O ( n ) entries, in contrast to the O ( n log n ) entries required by the existing analyses. One caveat is that we require stronger incoher-ence property to obtain the above given sample complex-ity. Our results also provide a recipe to determine if a given index set  X  is enough to recover a low-rank matrix. That is, given  X  and its associated graph G , we can measure the spectral gap of G and if it is large enough then our results guarantee exact recovery of strongly incoherent matrices. In Section 5, we showed that the standard incoherence as-sumption alone is not enough for universal recovery and a property similar to A 2 (see Section 3) is required. How-ever, it is an open problem to obtain precise information theoretic limits on d (see A 2 ) for universal recovery guar-antees. Another interesting research direction is to study the alternating minimization method under assumptions given in Section 3.
 Candes, Emmanuel J and Plan, Yaniv. Matrix completion with noise. Proceedings of the IEEE , 98(6):925 X 936, 2010.
 Cand ` es, Emmanuel J and Recht, Benjamin. Exact ma-trix completion via convex optimization. Foundations of Computational mathematics , 9(6):717 X 772, 2009. Candes, Emmanuel J and Tao, Terence. Decoding by linear programming. Information Theory, IEEE Transactions on , 51(12):4203 X 4215, 2005.
 Cand ` es, Emmanuel J and Tao, Terence. The power of convex relaxation: Near-optimal matrix completion. In-formation Theory, IEEE Transactions on , 56(5):2053 X  2080, 2010.
 Chen, Yudong, Bhojanapalli, Srinadh, Sanghavi, Sujay, and Ward, Rachel. Coherent matrix completion. arXiv preprint arXiv:1306.2979 , 2013.
 Feige, Uriel and Ofek, Eran. Spectral techniques applied to sparse random graphs. Random Structures &amp; Algo-rithms , 27(2):251 X 275, 2005.
 Friedman, Joel. A proof of alon X  X  second eigenvalue con-jecture. In Proceedings of the thirty-fifth annual ACM symposium on Theory of computing , pp. 720 X 724. ACM, 2003.
 Gopi, Sivakant, Netrapalli, Praneeth, Jain, Prateek, and
Nori, Aditya. One-bit compressed sensing: Provable support and vector recovery. In Proceedings of the 30th international conference on machine learning (ICML-13) , pp. 154 X 162, 2013.
 Gross, David. Recovering low-rank matrices from few co-efficients in any basis. Information Theory, IEEE Trans-actions on , 57(3):1548 X 1566, 2011.
 Gross, David, Liu, Yi-Kai, Flammia, Steven T, Becker,
Stephen, and Eisert, Jens. Quantum state tomography via compressed sensing. Physical review letters , 105(15): 150401, 2010.
 Heiman, Eyal, Schechtman, Gideon, and Shraibman, Adi.
Deterministic algorithms for matrix completion. Ran-dom Structures &amp; Algorithms , 2013.
 Hoory, Shlomo, Linial, Nathan, and Wigderson, Avi. Ex-pander graphs and their applications. Bulletin of the American Mathematical Society , 43(4):439 X 561, 2006. Jain, Prateek, Netrapalli, Praneeth, and Sanghavi, Su-jay. Low-rank matrix completion using alternating min-imization. arXiv preprint arXiv:1212.0467 , 2012. Keshavan, Raghunandan H, Montanari, Andrea, and Oh,
Sewoong. Matrix completion from a few entries. In-formation Theory, IEEE Transactions on , 56(6):2980 X  2998, 2010.
 Kir  X  aly, Franz J. and Tomioka, Ryota. A combinatorial alge-braic approach for the identifiability of low-rank matrix completion. In ICML , 2012.
 Lee, Troy and Shraibman, Adi. Matrix completion from any given set of observations. In Advances in Neural Information Processing Systems , pp. 1781 X 1787, 2013. Lin, Zhouchen, Chen, Minming, and Ma, Yi. The aug-mented lagrange multiplier method for exact recov-ery of corrupted low-rank matrices. arXiv preprint arXiv:1009.5055 , 2010.
 Liu, Yi-Kai. Universal low-rank matrix recovery from pauli measurements. arXiv preprint arXiv:1103.2816 , 2011. Lubotzky, Alexander, Phillips, Ralph, and Sarnak, Pe-ter. Ramanujan graphs. Combinatorica , 8(3):261 X 277, 1988.
 Marcus, Adam, Spielman, Daniel A, and Srivastava,
Nikhil. Interlacing families i: Bipartite ramanujan graphs of all degrees. arXiv preprint arXiv:1304.4132 , 2013.
 Margulis, Grigorii Aleksandrovich. Explicit group-theoretical constructions of combinatorial schemes and their application to the design of expanders and concen-trators. Problemy peredachi informatsii , 24(1):51 X 60, 1988.
 Meka, Raghu, Jain, Prateek, and Dhillon, Inderjit S. Ma-trix completion from power-law distributed samples. In
Advances in Neural Information Processing Systems , pp. 1258 X 1266, 2009.
 Morgenstern, Moshe. Existence and explicit constructions of q+ 1 regular ramanujan graphs for every prime power q. Journal of Combinatorial Theory Series B , 62(1):44 X  62, 1994.
 Nadakuditi, Raj Rao and Newman, Mark EJ. Graph spec-tra and the detectability of community structure in net-works. Physical review letters , 108(18):188701, 2012. NCDC. National climatic data center. URL http:// www.ncdc.noaa.gov/oa/ncdc.html .
 Recht, Benjamin. A simpler approach to matrix comple-tion. arXiv preprint arXiv:0910.0651 , 2009.
 Recht, Benjamin, Fazel, Maryam, and Parrilo, Pablo A.
Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review ,
