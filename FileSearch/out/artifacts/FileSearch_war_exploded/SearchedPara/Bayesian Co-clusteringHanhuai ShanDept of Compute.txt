
In recent years, co-clustering has emerged as a power-ful data mining tool that can analyze dyadic data connect-ing two entities. However, almost all existing co-clusteri ng techniques are partitional, and allow individual rows and columns of a data matrix to belong to only one cluster. Sev-eral current applications, such as recommendation systems and market basket analysis, can substantially benefit from a mixed membership of rows and columns. In this paper, we present Bayesian co-clustering (BCC) models, that al-low a mixed membership in row and column clusters. BCC maintains separate Dirichlet priors for rows and columns over the mixed membership and assumes each observation to be generated by an exponential family distribution cor-responding to its row and column clusters. We propose a fast variational algorithm for inference and parameter es-timation. The model is designed to naturally handle sparse matrices as the inference is done only based on the non-missing entries. In addition to finding a co-cluster structu re in observations, the model outputs a low dimensional co-embedding, and accurately predicts missing values in the original matrix. We demonstrate the efficacy of the model through experiments on both simulated and real data.
The application of data mining methods to real life prob-lems has led to an increasing realization that a considerabl e amount of real data is dyadic, capturing a relation between two entities of interest. For example, users rate movies in recommendation systems, customers purchase products in market-basket analysis, genes have expressions under ex-periments in computational biology, etc. Such dyadic data are represented as a matrix with rows and columns repre-senting each entity respectively. An important data mining task pertinent to dyadic data is to get a clustering of each entity, e.g., movie and user groups in recommendation sys-tems, product and consumer groups in market-basket anal-ysis, etc. Traditional clustering algorithms do not perfor m well on such problems because they are unable to utilize the relationship between the two entities. In comparison, co-clustering [13], i.e., simultaneous clustering of rows and columns of a data matrix, can achieve a much better per-formance in terms of discovering the structure of data [8] and predicting the missing values [1] by taking advantage of relationships between two entities.

Co-clustering has recently received significant attention in algorithm development and applications. [10], [8], and [12] applied co-clustering to text mining, bioinformatics and recommendation systems respectively. [3] proposed a generalized Bregman co-clustering algorithm by consider-ing co-clustering as a matrix approximation problem. While these techniques work reasonably on real data, one impor-partitional [16], i.e., a row/column belongs to only one row/column cluster. Such an assumption is often restrictiv e since objects in real world data typically belong to multipl e clusters possibly with varying degrees. For example, a user might be an action movie fan and also a cartoon movie fan. Similar situations arise in most other domains. Therefore, a mixed membership of rows and columns might be more ap-propriate, and at times essential for describing the struct ure of such data. It is also expected to substantially benefit the application of co-clustering in such domains.

In this paper, we propose Bayesian co-clustering (BCC) by viewing co-clustering as a generative mixture modeling problem. We assume each row and column to have a mixed membership respectively, from which we generate row and column clusters. Each entry of the data matrix is then gen-We introduce separate Dirichlet distributions as Bayesian priors over mixed memberships, effectively averaging the mixture model over all possible mixed memberships. Fur-ther, BCC can use any exponential family distribution [4] as the generative model for the co-clusters, which allows as real, binary, or discrete matrices. For inference and pa-rameter estimation, we propose an efficient variational EM-style algorithm that preserves dependencies among entries in the same row/column. The model is designed to natu-rally handle sparse matrices as the inference is done only based on the non-missing entries. Moreover, as a useful by-product, the model accomplishes co-embedding , i.e., si-multaneous dimensionality reduction of individual rows an d columns of the matrix, leading to a simple way to visualize the row/column objects. The efficacy of BCC is demon-strated by the experiments on simulated and real data.
The rest of paper is organized as follows: In Section 2, we present a brief review of generative mixture models. In Section 3, we propose the Bayesian co-clustering model. A variational approach for learning BCC is presented in Sec-tion 4. The experimental results are presented in Section 5, and a conclusion is given in Section 6.
In this section, we give a brief overview of existing gen-erative mixture models (GMMs) and co-clustering models based on GMMs as a background for BCC.
 Finite Mixture Models. Finite mixture models (FMMs) [9, 4] are one of the most widely studied models for discovering the latent cluster structure from observed data. The densit y function of a data point x in FMMs is given by: where  X  is the prior over k components, and  X  = {  X  ( [ z ] k 1  X  z = 1 ,...,k ) are the parameters of k component distributions p ( x |  X  ily distribution [6] with a form of p  X  (  X  )) p 0 ( x ) [4], where  X  is the natural parameter, cumulant function, and p sure.  X  determines a particular family, such as Gaussian, Poisson, etc., and  X  determines a particular distribution in that family. The parameters could be learned by maximum-likelihood estimation using an EM style algorithm [20, 17]. Latent Dirichlet Allocation. One key assumption of FMMs is that the prior  X  is fixed across all data points. La-tent Dirichlet allocation (LDA) [7] relaxes this assumptio n by assuming there is a separate mixing weight  X  for each data point, and  X  is sampled from a Dirichlet distribution Dir (  X  ) . For a sequence of tokens x = x k components has a density of the form p ( x |  X ,  X ) = Getting a closed form expression for the marginal density p ( x |  X ,  X ) is intractable. Variational inference [7] and Gibbs sampling [11] are two most popular approaches proposed to address the problem.
 Bayesian Naive Bayes. While LDA relaxes the assump-tion on the prior, it brings in a limitation on the conditiona l distribution: LDA can only handle discrete data as tokens. Bayesian naive Bayes (BNB) generalizes LDA to allow the model to work with arbitrarily exponential family distribu -tions [5]. Given a data point x = x function of the BNB model is as follows: p ( x |  X ,  X  ,F )= where F is the feature set, f missing entry in x , p tial family distribution for the component z BNB is able to deal with different types of data, and is de-signed to handle sparsity.
 Co-clustering based on GMMs. The existing literature has a few examples of generative models for co-clustering. Nowicki et al. [19] proposed a stochastic blockstructures model that builds a mixture model for stochastic relation-ships among objects and identifies the latent cluster via posterior inference. Kemp et al. [14] proposed an infinite al. [2] recently proposed a mixed membership stochastic blockmodel that relaxes the single-latent-role restricti on in stochastic blockstructures model. Such existing models have one or more of the following limitations: (a) The model only handles binary relationships; (b) The model deals with relation within one type of entity, such as a so-cial network among people; (c) There is no computationally efficient algorithm to do inference, and one has to rely on stochastic approximation based on sampling. The proposed BCC model has none of these limitations, and actually goes much further by leveraging the good ideas in such models.
Given an n co-clustering, we assume there are k i, [ i ] k 1 1 } and k 2 column clusters { z 2 = j, [ j ] co-clustering (BCC) assumes two Dirichlet distributions Dir (  X  from which the mixing weights  X  and each column v are generated. Row clusters for entries in row u and column clusters for entries in column v are sam-pled from discrete distributions Disc (  X  respectively. A row cluster i and a column cluster gether decide a co-cluster ( i,j ) , which has an exponential family distribution p the generative model for co-cluster ( i,j ) . For simplicity, we drop  X  from p whole data matrix is as follows (Figure 1): 1. For each row u , [ u ] n 1 2. For each column v , [ v ] n 2 3. To generate an entry in row u and column v ,
For this proposed model, the marginal probability of an entry x in the data matrix X is given by: p ( x |  X  1 , X  2 ,  X ) = product of all such marginal probabilities. That is because  X  for any row and  X  for all entries in this row/column. Therefore, the model introduces a coupling between observations in the same row/column, so they are not statistically independent. Not e that this is a crucial departure from most mixture models, which assume the joint probability of all data points to be simply a product of the marginal probabilities of each point .
The overall joint distribution over all observable and la-tent variables is given by = Y where  X  x tries are considered, z cluster and z ter for observation x ditionally independent given {  X  {  X 
Figure 2. Variational distribution q .  X  =
Y where the marginal probability = X Marginalizing over {  X  probability of observing the entire matrix X is: p ( X |  X  1 , X  2 ,  X ) = (2) Z
Y d X  tering models such as BNB and LDA are special cases of BCC. Further, BCC inherits all the advantages of BNB and LDA X  X bility to handle sparsity, applicability to diverse data types using any exponential family distribution, and flexible Bayesian priors using Dirichlet distributions. Given the data matrix X , the learning task for the BCC is to estimate the model parameters (  X   X  that the likelihood of observing the matrix X is maxi-mized. A general way is using the expectation maximiza-tion (EM) family of algorithms [9]. However, computa-tion of log p ( X |  X  we propose a variational inference method, which alter-nates between obtaining a tractable lower bound of true log-likelihood and choosing the model parameters to maximize the lower bound. To obtain a tractable lower bound, we con-sider an entire family of parameterized lower bounds with a set of free variational parameters, and pick the best lower bound by optimizing the lower bound with respect to the free variational parameters.
To get a tractable lower bound for log p ( X |  X  we introduce q ( z brevity) as an approximation of the latent variable distrib u-tion p ( z q ( z 1 , z 2 ,  X  1 ,  X  2 |  X  1 ,  X  2 ,  X  1 ,  X  2 ) = where  X  tional Dirichlet distribution parameters with k mensions respectively for rows and columns, and  X  {  X  crete distribution parameters with k rows and columns. Figure 2 shows the approximating dis-tribution q as a graphical model, where m number of non-missing entries in row u and v . As com-pared to the variational approximation used in BNB [5] and LDA [7], where the cluster assignment z for every single feature has a variational discrete distribution, in our app rox-imation there is only one variational discrete distributio n for an entire row/column. There are at least two advantages of our approach: (a) A single variational discrete distributi on for an entire row or column helps maintain the dependencies among all the entries in a row or column; (b) The inference is fast due to the smaller number of variational parameters over which optimization needs to be done.

By a direct application of Jensen X  X  inequality [18], we obtain a lower bound for log p ( X |  X  log p ( X |  X  1 , X  2 ,  X )  X  E q [log p ( X, z 1 , z 2 ,  X  We use L (  X  denote the lower bound. L could be expanded as: L (  X  1 ,  X  2 ,  X  1 ,  X  2 ;  X  1 , X  2 ,  X ) = E q [log p (  X  1 |  X  1 )]+ E q [log p (  X  2 |  X  2 )]+ E + E q [log p ( z 2 |  X  2 )]+ E q [ p ( X | z 1 , z 2 ,  X )]  X  E  X  E q [log q (  X  2 |  X  2 )]  X  E q [log q ( z 1 |  X  1 )]  X  E The expression for each type of term in L is listed in Ta-ble 1; the forms of E E algorithm maximizes the parameterized lower bounds with respect to the variational parameters (  X  the model parameters (  X  4.1.1 Inference In the inference step, given a choice of model parameters (  X  1 , X  2 ,  X ) bound L (  X  eters (  X  taking derivative of L and setting it to zero, the solution can be obtained by iterating over the following set of equations : where [ i ] k 1  X  v , and similarly for  X  function [7]. From a clustering perspective,  X  the degree of row u belonging to cluster i , for [ u ] [ i ]
We use simulated annealing [15] in the inference step to avoid bad local minima. In particular, instead of using (4) and (5) directly for updating  X  at each  X  X emperature X  t . At the beginning, t =  X  , so the probabilities of row u /column v belonging to all row/column clusters are almost equal. When t slowly de-creases, the peak of  X  ( t ) we reach t = 1 , where  X  (1) as in (4) and (5). We then stop decreasing the temperature and keep on updating  X  that, we go on to update  X  4.1.2 Parameter Estimation Since choosing parameters to maximize log p ( X |  X  directly is intractable, we use L (  X   X  the optimal lower bound, as the surrogate objective func-tion to be maximized, where (  X   X  timum values obtained in the inference step. To estimate the Dirichlet parameters (  X  Newton update as shown in [7, 5] for LDA and BNB. One potential issue with such an update is that an intermediate implementation, we avoid such a situation using an adaptive line search. In particular, the updating function for  X  where H (  X  at  X  1 respectively. By multiplying the second term by  X  , we are performing a line search to prevent  X  feasible range. At each updating step, we first assign  X  to be 1. If  X   X  factor of 0 . 5 until  X   X  is guaranteed to be improved since we do not change the update direction but only the scale. A similar strategy is performed on  X 
For estimating  X  , in principle, a closed form solution is possible for all exponential family distributions. We first consider a special case when the component distributions are univariate Gaussians. The update equations for  X  = {  X  Following [4], we note that any regular exponential family distribution can be expressed in terms of its expectation pa -rameter m as p ( x | m ) = exp(  X  d is the conjugate of the cumulant function  X  of the family and m = E [ X ] =  X   X  (  X  ) , where  X  is the natural parame-ter. Using the divergence perspective, the estimated mean M = {  X  ij , [ i ] k 1 1 , [ j ] k 2 1 } parameter is given by: and  X 
We propose an EM-style alternating maximization algo-rithm to find the optimal model parameters (  X   X  that maximize the lower bound on log p ( X |  X  particular, given an initial guess of (  X  (0) gorithm alternates between two steps till convergence: 1. E-step : Given (  X  ( t  X  1) tional parameters (  X  1 ,  X  = argmax lower bound function for log p ( X |  X  2. M-step : An improved estimate of the model parameters can now be obtained as follows: (  X  After ( t  X  1) iterations, the objective function becomes t The first inequality holds because from the variational E-step, L (  X  as in (11), and the second inequality holds because from the M-step, L (  X  ( t ) in (12). Therefore, the objective function is non-decreasi ng until convergence. [18].
In this section, we present extensive experimental results on simulated datasets and on real datasets.
Three 80  X  100 data matrices are generated with 4 row clusters and 5 column clusters, i.e., 20 co-clusters in tota l, such that each co-cluster generates a 20  X  20 submatrix. We use Gaussian, Bernoulli, and Poisson as the generative model for each data matrix respectively and each submatrix is generated from the generative model with a predefined parameter, which is set to be different for different subma-trices. After generating the data matrix, we randomly per-mute its rows and columns to yield the final dataset.
For each data matrix, we do semi-supervised initializa-tion by using 5% data in each co-cluster. The results in-clude two parts: parameter estimation and cluster assign-ment. We compare the estimated parameters with the true model parameters used to generate the data matrix. Fur-ther, we evaluate the cluster assignment in terms of cluster accuracy. Cluster accuracy (CA) for rows/columns is de-fined as: CA = 1 rows/columns, k is the number of row/column clusters and nc ber of rows/columns that fall into a same true cluster. Since the variational parameters  X  weights for rows and columns, we pick the component with the highest probability as its result cluster.

For each generative model, we run the algorithm three times and pick the estimated parameters with the highest log-likelihood. Log-likelihood measures the fit of the mode l to the data, so we are using the model that fits the data best among three runs. Note that no  X  X lass label X  is used while choosing the model. The comparison of true and estimated parameters after alignment for Gaussian case is in Figure 3. The color of each sub-block represents the parameter value for that co-cluster (darker is higher). The cluster accurac y is shown in Table 2, which is the average over three runs. From these results, we observe two things: (a) Our algo-rithm is applicable to different data types by choosing an ap -propriate generative model; (b) We are able to get an accu-rate parameter estimation and a high cluster accuracy, with semi-supervised initialization by using only 5% of data. Three real datasets are used in our experiments X  Movielens, Foodmart, and Jester: (a) Movieles: 1 Movie-lens is a movie recommendation dataset created by the Grouplens Research Project. It contains 100,000 ratings (1-5, 5 the best) in a sparse data matrix for 1682 movies rated by 943 users. We also construct a binarized dataset such that entries whose ratings are higher than 3 become 1 and others become 0 . (b) Jester: 2 Jester is a joke rating dataset. The original dataset contains 4.1 million continu -ous ratings (-10-+10, +10 the best) of 100 jokes from 73,421 users. We pick 1000 users who rate all 100 jokes and use this dense data matrix in our experiment. We also binarize the dataset such that the non-negative entries become 1 and the negative entries become 0 . (c) Foodmart: Foodmart data comes with Microsoft SQL server. It contains trans-action data for a fictitious retailer. There are 164,558 sale s records in a sparse data matrix for 7803 customers and 1559 products. Each record is the number of products bought by the customer. Again, we binarize the dataset such that en-tries whose number of products are below median are 0 and others are 1. Further, we remove rows and columns with less than 10 non-missing entries. For all three datasets, we use both the binarized and original data in our experiments. 5.2.1 Methodology For binarized data, we use bernoulli distribution as the gen -erative model. For original data, we use Discrete, Poisson, and Gaussian as generative models for Movielens, Food-mart and Jester respectively. For Foodmart data, there is one unit right shift of Poisson distribution since the value of non-missing entries starts from 1 instead of 0, so we sub-stract 1 from all non-missing entries to shift it back.
Starting from a random initialization, we train the model by alternating E-step and M-step on training set as de-scribed in Section 4 till convergence, so as to obtain model parameters (  X   X  ational lower bound on the log-likelihood. We then use the model parameters to do inference, that is, inferring the mixed membership for rows/columns. In particular, there are two steps in our evaluation: (a) Combine training and test data together and do inference (E-step) to obtain vari-ational parameters; (b) Use model parameters and varia-tional parameters to obtain the perplexity on the test set. In addition, we also report the perplexity on the training set. Recall that the perplexity [7] of a dataset X is defined as: perp ( X ) = exp(  X  log p ( X ) /N ) , where N is the number of non-missing entries. Perplexity monotonically decreas es with log-likelihood, implying that lower perplexity is bet-ter since higher log-likelihood on training set means that the model fits the data better, and a higher log-likelihood on the test set implies that the model can explain the data better. For example, in Movielens, a low perplexity on the test set means that the model captures the preference pat-tern for users such that the model X  X  predicted preferences on test movies for a user would be quite close to his actual preferences; on the contrary, a high perplexity indicates t hat the user X  X  preference on test movies would be quite differ-ent from model X  X  prediction. A similar argument works for Foodmart and Jester as well.

Let X sets respectively. We evaluate the model X  X  prediction per-formance as follows: We compute variational parame-ters (  X  them to compute perp ( X create  X  X Figure 6. Perplexity curves for Movielens, Foodmart and ters (  X   X  compute perp (  X  X If the model yields a lower perplexity on the true test set than on the modified one, i.e., perp ( X the model explains X prediction based on log-likelihood, the model will accu-rately predict X the perplexity to increase with increasing percentages of test data being modified. Ideally, such an increase will be monotonic, implying that the true test data X the most-likely according to the model and a higher per-our experiments, since X paring perp ( X
We compare BCC with BNB and LDA in terms of per-plexity and prediction performance. Each user/customer is treated as one data point in a row. The comparison with BNB is done on both binarized and original datasets. The comparison of BCC with LDA is done only on binarized datasets since LDA is not designed to handle real values. To apply LDA, we consider the features with feature value 1 as the tokens appearing in each data point, like the words in a document. For simplicity, we use  X  X ow cluster X  or  X  X luster X  to refer to the user/customer clusters, and use  X  X olumn clus -ter X  to refer to the movie, product and joke clusters for BCC on Movielens, Foodmart and Jester respectively. To ensure a fair comparison, we do not use simulated annealing for BCC in these experiments because there is no simulated an-nealing in BNB and LDA either. 5.2.2 Results In this section, we present three main experimental results : (a) Perplexity comparison among BCC, BNB and LDA; (b) The prediction performance comparison between BCC and LDA; (c) The visualization obtained from BCC.
 Perplexity Comparison. We compare the perplexity among BCC, BNB and LDA with varying number of row clusters from 5 to 25 in steps of 5, and a fixed number of column clusters for BCC to be 20, 10 and 5 for Movie-lens, Foodmart and Jester respectively. The results are re-ported as an average perplexity of 10-cross validation in Figures 4, 5 and Table 3.

Figure 7. Perplexity curves of BCC and LDA with in-Figure 4 compares the perplexity of BCC, BNB, and LDA on binarized Jester, and Figure 5 compares the per-plexity of BCC and BNB on original Movielens dataset, both with varying number of clusters. Note that due to the distinct differences of perplexity among three models, y-axes are not continuous and the unit scales are not all the same. Table 3 presents the perplexities on both binarized and original datasets with fixed 10 row clusters. From these results, there are two observations: (a) For BCC and LDA, the perplexities of BCC on both training and test sets are 2-3 orders of magnitude lower than that of LDA, and the icant with an extremely small p-value. The lower perplexity of BCC seems to indicate that BCC fits the data and explains the data substantially better than LDA. However, one must be careful in drawing such conclusions since BCC and LDA work on different variants of the data; we discuss this aspec t further at the end of the next subsection. (b) For BCC and BNB, although BNB sometimes has a lower perplexity than BCC on training sets, on test sets, the perplexities of BCC are lower than BNB in all cases. Again, the difference is sig-nificant based on the paired t -test. BNB X  X  high perplexities on test sets indicate over-fitting, especially on the origin al
Figure 8. Perplexity curves of BCC and LDA with in-Movielens data. In comparison, BCC behaves much better than BNB on test sets, possibly because of two reasons: (i) BCC uses much less number of variational parameters than BNB, so as to avoid overfitting; (ii) BCC is able to capture the co-cluster structure which is missing in BNB. Prediction Comparison. To evaluate the prediction perfor-mance, we test the perplexity on ( X on ( X tion 5.2.1, by modifying a certain percentage of data in X data, which is a reasonable simplification because in real recommendation systems, we usually only need to know whether the user likes (1) the movie/product/joke or not (0) to decide whether we should recommend it. To add noise to binarized data, we flip the entries of 1 to 0 and 0 to 1. We record the perplexities with the percentage of noise p increasing from 1% to 10% in steps of 1% and report the average perplexity of 10 cross validation at each step. The perplexity curves are shown in Figure 6.

At the starting point, with no noise, we have perplexity of data with the true test set X end, 10% of the entries in the test set have been modified. As shown in Figure 6, all three lines go up steadily with an increasing percentage of test data modified. This is a surprisingly good result, implying that our model is able to detect increasing noise and convey the message through increasing perplexities. The most accurate result, i.e., t he one with the lowest perplexity, is exactly the true test set a t the starting point. Therefore, BCC can be used to accurately predict missing values in a matrix.

We add noise at a finer step of modifying 0 . 1% and 0 . 01% test data each time, and compare the prediction per-formance of BCC with LDA. The results on binarized Jester and Movielens are presented in Figure 7 and 8. In both fig-ures, the first row is for adding noise at steps of 0 . 01% the second row is for adding noise at steps of 0 . 1% . The trends of the perplexity curves show the prediction perfor-mance. On Jester, we can see that the perplexity curves for BCC in both Figure 7(a) and 7(c) go up steadily at almost all times. However, the perplexity curves for LDA go up and down from time to time, especially in Figure 7(b), which means that sometimes LDA fits the data with more noise better than that with less noise, indicating a lower predic-tion accuracy compared with BCC. The difference is even more distinct on Movielens. When adding noise at steps of 0 . 01% , there is no clear trend in perplexity curves in Fig-ure 8(a) and 8(b), implying that neither BCC nor LDA is able to detect the noise at this resolution. However, when the step size increases to 0 . 1% , perplexity curve of BCC starts to go up as in Figure 8(c) but the perplexity curve of LDA goes down as in Figure 8(d). The decreasing perplex-ity with addition of noise indicates LDA does not have a good prediction performance on Movielens.
 While extensive results give supportive evidence to BCC X  X  better performance, we should be cautious of the conclusion we draw from the direct perplexity comparison between BCC and LDA. Given a binary dataset, BCC works on all non-missing entries, but LDA only works on the entries with value 1. Therefore, BCC and LDA actually work on different data, and hence their perplexities cannot be compared directly. However, the comparison gives us a rough idea of these two algorithms X  behavior, such as the distinct difference in perplexity ranges, similar perplex ity trends with increasing number of clusters. Moreover, the result of prediction shows that BCC indeed does much bet-ter than LDA, no matter which part of dataset they are using. Visualization. The co-clustering results give us a com-pressed representation of the original matrix. We can vi-sualize it to study the relationship between row and column clusters. Figure 9 is an example of user-movie co-clusters on Movielens. There are 10  X  20 sub-blocks, correspond-ing to 10 user clusters and 20 movie clusters. The shade of each sub-block is determined by the parameter value of the bernoulli distribution for each co-cluster. A darker su b-block indicates a larger parameter. Since the parameter of a bernoulli distribution implies the probability of genera t-ing an outcome 1 (rate 4 or 5), the darker the sub-block is, the more the corresponding movie cluster is preferred by the user cluster. Based on Figure 9, we can see that users in cluster 2 (U2) are a big fan of all kinds of movies, and users in U5 seem uninterested in all movies except those in movie cluster 13 (M13). Moreover, movies in M18 are very popular and preferred by most of the users. In comparison, movies in M4 seem to be far from best sellers. We can also tell that users in U1 prefer M18 the best and M8 the worst. U2 and U6 share several common favorite types of movies.
The variational parameters  X  rows, and  X  dimensional representation for all the row and column ob-jects. They can be considered as the result of a simultane-ous dimensionality reduction over row and column feature vectors. We call the low-dimensional vectors  X  a  X  X o-embedding X  since they are two inter-dependent low-dimensional representations of the row and column objects derived from the original data matrix. Co-embedding is a unique and novel by-product of our algorithm, which ac-complishes dimensionality reduction while preserving de-pendencies between rows and columns. None of partitional co-clustering algorithms is able to generate such an embed-ding, since they do not allow mixed membership to row and column clusters. To visualize the co-embedding, we apply ISOMAP [21] on  X 
The results of co-embedding for users and movies on binarized Movielens are shown in Figure 10(a) and 10(c). Each point in the figure denotes one user/movie. We mark three clusters with red, blue and green for users and movies respectively; other points are colored pink. By visualiza-tion, we can see how the users/movies are scattered in the space, where the clusters are located, and how far one clus-ter is from another, etc. Such information goes far beyond clusters of objects only. In addition, we choose several points from the co-embedding to look at their properties. In Figure 10(a) and 10(c), we mark four users and four movies, and extract their  X  X ignatures X . In general, we can use a va-riety of methods to generate signature. In our experiment, we do the following: For each user, we get the number of movies she rates  X 1 X  in movie cluster 1-20 respectively. Af-ter normalization, this 20-dim unit vector is used as the sig -nature for the user. Similarly, for each movie, we get the number of users giving it rate  X 1 X  in user cluster 1-10 re-spectively. The normalized 10-dim unit vector is used as the signature for the movie. The signatures are shown in Fig-ure 10(b) and 10(d) respectively. The numbers on the right are user/movie IDs corresponding to those marked points in co-embedding plots, showing where they are located. We can see that each signature is quite different from others in terms of the value on each component.
In this paper, we have proposed Bayesian co-clustering (BCC) which views co-clustering as a generative mixture modeling problem. BCC inherits the strengths and robust-ness of Bayesian modeling, is designed to work with sparse matrices, and can use any exponential family distribution a s the generative model, thereby making it suitable for a wide range of matrices. Unlike existing partitional co-cluster ing algorithms, BCC generates mixed memberships for rows and columns, which seem more appropriate for a variety of applications. A key advantage of the proposed variational approximation approach for BCC is that it is expected to be significantly faster than a stochastic approximation based on sampling, making it suitable for large matrices in real life applications. Finally, the co-embedding obtained fro m BCC can be effectively used for visualization, subsequent predictive modeling, and decision making.
 Acknowledgements: The research was supported by NASA grant NNX08AC36A and NSF grant IIS-0812183.

