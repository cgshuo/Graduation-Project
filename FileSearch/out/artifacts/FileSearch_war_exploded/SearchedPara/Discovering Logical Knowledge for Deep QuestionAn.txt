 Most open-domain question answering systems achieve bet-ter performances with large corpora, such as Web, by taking advantage of information redundancy. However, explicit an-swers are not always mentioned in the corpus, many answers are implicitly contained and can only be deducted by infer-ence. In this paper, we propose an approach to discover logical knowledge for deep question answering, which au-tomatically extracts knowledge in an unsupervised, domain-independent manner from background texts and reasons out implicit answers for the questions. Firstly, we use seman-tic role labeling to transform natural language expressions to predicates in first-order logic. Then we use association analysis to uncover the implicit relations among these pred-icates and build propositions for inference. Since our knowl-edge is drawn from different sources, we use Markov logic to merge multiple knowledge bases without resolving their inconsistencies. Our experiments show that these proposi-tions can improve the performance of question answering significantly.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval Question Answering, Semantic Role Labeling, Markov Logic Networks  X  Corresponding Author
Automated question answering (QA) has generated a grow-ing interest in the past few years[1, 24]. The pipeline of open-domain QA [24] is question analysis, retrieval, answer extraction, answer selection and validation. Most QA sys-tems can achieve better performances with large corpora, such as Web, by taking advantage of information redun-dancy [4], which can simplify the jobs of the other parts. As these methods cannot handle deep inference relations, they cannot deal with the questions when there are no obvious indicators for answers in available resources. Some systems use limited man-made rules which can only cover very little knowledge of domains. Other systems use lexical chain or eXtended WordNet (XWN) 1 [7] to construct the inference chain [13, 12]. However, sometimes lexical information is far from enough to reason out the correct answers.

In this paper, we propose an approach to discover logical knowledge for deep question answering, which automatically extracts knowledge in an unsupervised, domain-independent manner from background texts and reasons out implicit an-swers for the questions. Firstly, we use semantic role la-beling to transform natural language expressions to predi-cates in first-order logic. Due t hat these extracted predi-cates are grounded and too fine-grained to discover general rules, we generalize them into un-grounded predicates. Then we use association analysis to uncover the implicit relations among these predicates and build propositions for inference. Since our knowledge is drawn from different sources, we use Markov logic networks [5] to merge multiple knowledge bases without resolving their inconsistencies. Our experiments show that these propositions can improve the performance of question answering significantly.

The overall architecture of our system is shown in Figure 1.

The rest of the paper is organized as follows. We first present our method to learn deep logic rules from the back-ground corpus in 2. Then we describe how to transform questions and answers into logic representations section 3. The inference method is introduced in section 4. Section 5 http://xwn.hlt.utdallas.edu/ presents our experimental analysis. Section 6 presents the related works. In the end, section 7 summarizes the paper.
First of all, we focus on how to get useful implicit knowl-edge from the background corpus. Our system first extracts predicates from the background corpus. Here, a predicate is composed of a verb and its corresponding arguments, which is the same to the predicate defined in first-order logic. For example,  X  Transmit (sender, sent, sent-to) X  and  X  Have (somebody, something) X  X re both predicates. Then as-sociation analysis in data mining and statistical relevance in inductive logic programming are adopted to learn inference rules which construct the knowledge base. After building this implicit knowledge base we transfer questions into log-ical forms. Then supported by explicit knowledge from the background corpus, WordNet and rules discovered, we will validate which is the best answer to the questions.
In first-order logic, a predicate resembles a function that returns either True or False. However, the extracted pred-icates are grounded, so we need to generalize them to un-grounded representations. Consider the following sentences:  X  X ocrates is a philosopher X ,  X  X lato is a philosopher X . We know Socrates and Plato are both persons. So from these two sentences we will obtain a predicate Phil with a vari-able PERSON. The predicate  X  Phil ( p ) X  represents that the person p is a philosopher. [8] describe a method for automatic acquisition of the hy-ponymy lexical relation from unrestricted texts. However Hearst X  X  patterns cannot find all the word classes because of its limited patterns. We also need to recognize names of per-sons and locations. Fortunately, name entity recognition is quite precise for English texts and there are many state-of-the-art NER toolkits. Then all these names can be regarded as instances of variables  X  X omebody X  or  X  X omewhere X . Although there are structured resources such as Word-Net[10], yet many more text re sources are not structured including Web resources. Getting structural information in these corpora will greatly facilitate the question answer-ing system or information retrieval. [21] uses R E V ERB [6] Open Information Extraction tool to extract relation phrases and their corresponding arguments. However, due to the fixed patterns used, R E V ERB only captures 2-ary relations of verbs and there are lots of noise in these tuples. For a precise QA system we need more pure relations and more relations with three or more variables.

Semantic Role Labeling (SRL)[17] is a shallow semantic parsing task, in which for each predicate in a sentence, the goal is to identify all constituents that fill a semantic role, and to determine their roles (Agent, Patient, Instrument, etc.) and their adjuncts (Locative, Temporal, Manner etc.). [3].

Accordingly, we adopt the SRL method [17] to label the re-lated background corpus. Although this step may be rather slow because of a complete syntactic parser, fortunately, we run this step only once as an offline preprocessing procedure and we can use the incremental method for newer knowledge resources. So a little expensive cost of the SRL method is accepted by QA systems.

Semantic role labeling method labels each part of a sen-tence with semantic role tags and predicates are generated after filling each blank of this predicate. However, these predicates are over-specified and more than necessary, and we cannot get a higher level general relation extraction. These predicates are grounded predicates in which each vari-able is substituted by a specific constant, but we cannot in-duce general knowledge from these grounded predicates. So we have to find out more general predicates with variables instead of over-specific entities. We use the head of the cor-responding child tree as the argument. If it is a name entity we will use the full name as an argument. In this stage we ignore such semantic roles as  X  X M-TMP X ,  X  X M-LOC X ,  X  X M-MNR X ,  X  X M-MOD X  and  X  X M-DIS X , etc. After that we replace each argument with its word class mentioned in Section 2.1.
Through the above steps we acquire all extracted predi-cates. But predicates alone are not enough, and more deep knowledge is needed for inference. As we will use first-order logic to validate answers for questions, we should learn the inference rules from the background corpus. Besides that, a LFT formed WordNet -eXtend WordNet -is transferred into first-order logic form and supplies the common sense relations between words.

We assume that predicates contained in a multi-sentence context in one passage tend to have strong relations among them. The combinations of these predicates tend to be use-ful rules. One context in a large corpus is similar to market basket transaction in Data Mining [23].

Let P = { p 1 ,p 2 , ..., p d } be the set of all predicates in a context and T = { t 1 ,t 2 , ..., t N } be the set of all contexts. Each context t i contains a subset of items chosen from P .An inference rule is an implication expression of the form X Y ,where X and Y are disjoint predicate sets, i.e., X  X  Y = Intuitively if a rule occurs in more passages than others, this rule will be more likely to be a strongly supported and useful rule. Mathematically, the support count,  X  ( X ), for an predicate set X can be stated as follows: So we wish the probability is larger than a threshold T ,where N is the total number of passages. This probability formula is also the definition of support of an association rule in Association Analysis in Data Mining [23, pp. 242 X 243 ].

What is more, to ensure that the rules have a greater support, we also need to estimate relative probability. That is to say, we will test whether p ( Y | X ) p ( Y ) is satisfied. If so, X is said to be statistically relevant to Y [19]. We change the formula of statistical relevance to So r ( X  X  Y ) is larger and therefore is better. And Association Analysis in Data Mining [23, pp. 242 X 243 ] defines the confidence of an association rule as The formula r ( X  X  Y )isinthesameformof interest factor of objective measure in evaluation of association patterns.
Due to the highly skewed distribution of predicates, some common predicates appear much more frequently than oth-ers and they are always an important prerequisite in propo-sitions. The appearance of highly frequent predicates leads to a very low level of mutual information between the pre-condition and the result. So we use as the weight of one rule discovered.
Questions of different types need reasoning in different ways. For example,  X  X hy is Avelile suffering from AIDS? X  and X  X n which European cities has Annie Lennox performed? X  are completely different. For the former we need to know what fact induces the result  X  X uffering from AIDS X , while for the latter we need to find the correct name entity to fill the missing position of the question. Another logical rule is needed for the former while a particular constant as an argument of a predicate for the latter.

In the overview of QA4MRE at CLEF 2011 [16], it is reported that about half of the questions (64 out of 120) were FACTOID, 17 were CAUSE and 16 were WHICH-IS-TRUE. So we first classify these questions into three basic types.
 The first question type is to ask for one or more entities. Most of these questions contain WHAT, WHICH, WHO, WHOM, WHEN, WHERE, WHAT PLACE and so on.

For the second type, questions and answers are all com-plete sentences. Answers are candidate reasons or results of questions.

The last type is the true-or-false question. For the first type, after extracting the predicates of questions we need to substitute the WH Question Words with all candidate en-tities and validate these propositions supporting by related pieces in passages, deep knowledge and so on. For the sec-ond type, we will estimate the probabilities of the reasoning chains between the questions and answers. The last type is similar to the first type of questions except without a sub-stitute process.

After classification questions and answers are transferred to logical forms following the above method using SRL and so on. Then these logical forms can be combined into differ-ent facts and we will use the knowledge base to validate these facts. The facts with the highest probability is regarded as the correct inference chain between questions and answers.
For the question  X  X hy is Avelile suffering from AIDS? X  the logical form in an ideal situation is One of the candidate answers  X  X ecause her mother transmit-ted the virus to her X  is transferred to As this is the right answer, we wish that the fact can be strongly supported by the knowledge base and the inference engine.

For the question and answer  X  X n which European cities has Annie Lennox performed? X ,  X  X dinburgh and Oslo X , we wish to transfer them into and these two facts are supported by the knowledge base. Predicates always have different but associated arguments. For example, have ( Somebody,AIDS )and have ( Child,AIDS ) are different, but meanwhile Child is of course a kind of Somebody . Sothetwopredicatesaresimilartosomeex-tent. We define a factor to reflect the connection between them. If two arguments are synonymous in WordNet, two predicates are more likely to be the same predicates. For two predicates of hypernym relations in WordNet, the longer the path is between them, the more dissimilar they are. In the same way, for the different arguments, the longer inference chain is, the lower its rationality is.
 So we define the similarity of two arguments as
The similarity of two predicates and two rules is defined as here A p 1 , A p 2 are the argument set of each predicate and arg 1 , arg 2 playthesameroleinthesetwopredicates.Anal-ogously, P r 1 , P r 2 are the predicate set of each rule and p p are in the same position in these two rules.

For an inference chain r 1 ,r 2 , ..., r n the probability is de-fined as here Z is a constant and L is the length of this inference chain.
To sum up, we use four knowledge resources  X  lexical chain, background knowledge directly extracted from back-ground collections, relations from WordNet and deep logical knowledge extracted from background collections. Each re-source has a weight that measures to what extent it deter-mines which is the right answer for a question.

However, due to the noises and lack of negative train ex-amples, there may be some wrong or not exactly right in-ference rules. So we use a probabilistic logical inference en-gine to bridge the gap between the questions and answers. Markov logic networks (MLNs) [18] are one of the appropri-ate models. Markov logic allows multiple knowledge bases to be merged without resolving their inconsistencies, and ob-viates the need to exhaustively specify the conditions under which a formula can be applied. A Markov logic network is a probabilistic logic which applies the ideas of a Markov network to first-order logic, allowing uncertain inference.
We use Tuffy [14] as the inference engine of MLNs in our system.
Our experiments demonstrate that in comparison with the best performances of QA4MRE at CLEF2011 our system is also outstanding and the rules automatically extracted from the background collections improve our system significantly from our baseline.

We use the English corpus of QA4MRE task at CLEF2011 [16] to evaluate our system.

To evaluate the performance of our approach, we compare it with the following methods: c @1[15] was used in CLEF 2011 QA4MRE subtask, which takes into account the option of not answering certain ques-tions. The c@1 value of each question answering system is shown in Table 1.
 Table 1: c@1 value of question answering systems
By adding explicit background knowledge and deep logi-cal knowledge, our system substantially improve the perfor-mance of QA4MRE, compared with the lexical chain base-line, which does not represent explicit background knowl-edge base.
Question answering systems have traditionally depended on a variety of lexical resources to bridge surface differences between questions and potential answers. COGEX [11] is a logic prover designed for question answering. When involv-ing question answering in specific domain, only the logical expressions in eXtended WordNet are never enough.
Shen et al. [22] introduce the semantic role labeling tech-nology to improve question answering. All the roles labeled give extra information for question answering without fur-ther inference.

DIRT [9] utilizes entailment relations discovered inference rules from text by an unsupervised algorithm to improve question answering system. However they only focus on the entailment relations omitting other kinds of inference rela-tions.

Schoenmackers et al. [20] present HOLMES system shows that the logical rules can help improve the results of the web search. However in that paper all the rules are constructed by hands so this method is not practicable in real situations.
In this paper, we propose an approach to discover logical knowledge for deep question answering, which automatically extracts knowledge in an unsupervised, domain-independent manner from background texts and reasons the implicit an-swers for the questions. In the future, we wish to investi-gate a method to obtains more appropriate word classes and good-grained NER labels in order to construct more useful rules.
 This work was funded by N SFC (No. 61003091 and No. 61073069), 863 Program (No. 2011AA010604) and 973 Pro-gram (No. 2010CB327900). [1] S. Azzam and K. Humphreys. New directions in [2] L. Cao, X. Qiu, and X. Huang. Question answering for [3] X. Carreras and L. M` arquez. Introduction to the [4] C. Clarke, G. Cormack, and T. Lynam. Exploiting [5] P. Domingos and D. Lowd. Markov logic: An interface [6] A. Fader, S. Soderland, and O. Etzioni. Identifying [7] S. Harabagiu, G. Miller, and D. Moldovan. Wordnet 2 [8] M. A. Hearst. Automatic acquisition of hyponyms [9] D. LIN and P. PANTEL. Discovery of inference rules [10] G. Miller. Wordnet: a lexical database for english. [11] D. Moldovan, C. Clark, S. Harabagiu, and D. Hodges. [12] D. Moldovan and A. Novischi. Lexical chains for [13] D. I. Moldovan and V. Rus. Logic form transformation [14] F. Niu, C. R  X  e, A. Doan, and J. Shavlik. Tuffy: scaling [15] A. Pe  X  nas, P. Forner, R. Sutcliffe, A. Rodrigo, [16] A. Penas, E. H. Hovy, P. Forner, A. Rodrigo, R. F. E. [17] V. Punyakanok, D. Roth, and W. Yih. The importance [18] M. Richardson and P. Domingos. Markov logic [19] W. Salmon, R. Jeffrey, and J. Greeno. Statistical [20] S. Schoenmackers, O. Etzioni, and D. Weld. Scaling [21] S. Schoenmackers, O. Etzioni, D. S. Weld, and [22] D. Shen and M. Lapata. Using semantic roles to [23] P.-N. Tan, M. Steinbach, and V. Kumar. Introduction [24] E. Voorhees and D. Tice. Building a question
