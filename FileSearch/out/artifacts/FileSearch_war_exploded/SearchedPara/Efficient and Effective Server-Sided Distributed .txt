 application domains where the data are originally located at different sites. In order to create a central clustering, all clients have to trans-mit their data to a central server. Due to technical limitations and se-curity aspects, at the central site often only vague object descriptions are available. The server then has to carry out the clustering based on vague and uncertain data. In a recent paper, an approach for clus-tering uncertain data was proposed based on the concept of medoid clusterings. The idea of this approach is to create first several sample clusterings. Then based on suitable distance functions between clus-terings the most average clustering, i.e. the medoid clustering, was determined. In this paper, we extend this approach for partitioning clustering algorithms and propose to compute a centroid clustering based on these input sample clusterings. These centroid clusterings are new artificial clusterings which minimize the distance to all the sample clusterings.
 H.2.4 [Information Systems, Database Management, Systems]: Distributed Databases Algorithms, Management, Performance Distributed Clustering, Uncertain Data aims at partitioning the data set into distinct groups, called clusters, while maximizing the intra-cluster similarity and minimizing the in-ter-cluster similarity. Traditionally, clustering algorithms require full access to the data which is going to be analyzed. All data has to be located at the site where it is processed. Nowadays, large amounts of heterogeneous, complex data reside on different, independently working computers which are connected to each other via local or wide area networks. Examples comprise distributed mobile net-works, sensor networks or supermarket chains where check-out scanners, located at different stores, gather data unremittingly. If a central server wants to cluster these data, it naturally has to cope with uncertain object descriptions. In other distributed application areas, like sensor databases or location-based services, the uncertainty of the object descriptions at server site results from an application in-herent inaccuracy. For instance, in the area of moving objects it is often not possible to pinpoint an exact position of a moving object due to technical limitations of the GPS system. proaches presented in the literature. First, in Section 2.1, we intro-duce a general approach to model the data on the server. Then, in Section 2.2, we define several different possibilities to carry out the clustering based on the proposed server-sided object description. the local sites some suitable approximations are generated which are transmitted to the server. The server then carries out the server-sided clustering based on approximated information. These approximated information on the server can be described by spatial probability density functions which can be regarded as a generalization of the approach presented in [2] where one dimensional probability density functions, called pdfs, were introduced to model imprecise data. Let o  X  D  X  IR d be an object from a database. An uncertain server-sided object representation is a function o eralization of existing server-sided object description techniques used in the area of distributed clustering. ty-based distributed clustering algorithms were presented which are based on the density-based partitioning clustering algorithm DBSCAN. The idea of these approaches is to group feature vectors together to small clusters at client site. Each of these local mi-cro-clusters is represented by a feature vector and a covering-radius quite similar to an M-tree [3] directory node. This information is transmitted to the server. If we assume that V is the volume of the hyper-sphere belonging to the micro-cluster of object o , the serv-er-sided object representation o uncertain assigns to each feature vec-tor contained in the hyper-sphere a value of 1/ V and to each feature IR +  X   X  vector outside the hyper-sphere a value of 0. Note that all objects within such a micro-cluster have the same uncertain object represen-tation. feature vectors was introduced. In order to save transmission cost, only certain dimensions of a feature vector are transmitted to the server. For the dimensions which are not transmitted, the server can limit the possible values by an interval. Thus, the server can individ-ually generate for each feature vector a conservative approximating box. If we assume that V is the volume of the box belonging to object o , the server-sided object representation o uncertain feature vector contained in the box a value of 1/ V and to each feature vector outside the box a value of 0. In this case, the server-sided ob-ject descriptions are different for the different objects.
 also be regarded as distributed objects. In this case, technical prob-lems with the GPS system, or outdated positional information force the server to approximate moving objects by one-or two-dimension-al Gaussian probability density functions o uncertain proach for clustering such moving objects with uncertain positions was proposed. Furthermore, if we assume that the exact positions of the moving objects are available [10], the probability density func-tions o uncertain correspond to dirac-delta functions [1] which assign to the exact position a value of infinity and to all other positions a value of 0. Another approach presented in [9] suggests to group sev-eral moving objects together to moving micro-clusters. If we assume that A is the area of the moving micro cluster belonging to object o , the server-sided object representation o uncertain assigns to each fea-ture vector contained in this moving micro-cluster a value of 1/ A and to each feature vector outside this moving micro cluster a value of 0. monitoring values like wind speed, pressure or temperature. Due to continuous changes, a central database has at each time only approx-imated information of each of these attributes. In [2] it was suggested to model each of these values by appropriate density functions, which corresponds to a 1-dimensional server-sided object represen-tation according to Definition 1. 1, we can introduce different ways to carry out the server-sided clus-tering. All of the presented approaches are based on monte-carlo sampling where a single object is described by a set of sample feature vectors.
 tering distributed objects on the server is to assign to each object o an exact position according to its density-probability function o uncertain . Then arbitrary clustering algorithms can be carried out based on ordinary feature vectors. In [4, 5] this approach is used for the server-sided clustering.
 plings for each object and compute the distance between all these samples. Then, the distance between the objects is computed by the average value between these sample distances. If we assume that we have s samples for each object, the distance between two objects cor-responds to the average value of the s 2 many distances between the samples. In [6] this approach, called distance expectation value, is used for the server-sided clustering. single-sampling approach used in [4, 5] s different times. Thus, s in-dependent sample clusterings are created. Based on suitable distance functions between clusterings [7], the most average of these sample clusterings, called medoid clustering, is computed. The medoid clustering is the clustering having the smallest overall distance to all other clusterings. In [8], this approach is used for the server-sided clustering. It is shown that these medoid clusterings can efficiently be computed based on parallelization, and, furthermore, achieve higher qualities than the approach based on single sampling [4, 5] or the approach based on the distance expectation value [6]. 
Centroid Clustering. In this paper, we present an extension of these medoid clusterings. We propose to compute a central artificial clustering rather than a medoid clustering for clustering uncertain object representations on the server site. Based on s sample cluster-ings and a suitable distance measure between clusterings [7, 8], we compute a new artificial central clustering. First, we initialize the re-sulting clustering with an arbitrary sample clustering. Then, we iter-atively change this clustering in such a way that in each cycle the newly created result clustering has a smaller overall distance to all sample clusterings than the former result. Thereby, in each cycle, our approach compares each cluster of the old result clustering to exactly one cluster in each sample clustering. The clusters of the new result clustering are then created based on this matching information. We transform each old result cluster into a new cluster by minimizing the sum of all distances to the matched clusters in the sample clus-terings. By iteratively repeating this procedure we can generate an artificial clustering which minimizes the distance to all sample clus-terings. Both from an efficiency and effectivity point of view this ap-proach outperforms the medoid clustering approach [8], and the ap-proach based on the distance expectation value [6]. [1] Bracewell, R. The Impulse Symbol. Ch. 5 in The Fourier Trans-form and Its Applications, 3rd ed.: McGraw-Hill, 1999. [2] Cheng R., Kalashnikov D.V., Prabhakar S.: Evaluating proba-bilistic queries over imprecise data . SIGMOD X 03, pp. 551-562. [3] Ciaccia P., Patella M., Zezula P.: M-tree: An Efficient Access Method for Similarity Search in Metric Spaces . VLDB X 97, pp. 426-435. [4] Januzaj E., Kriegel H.-P., Pfeifle M.: Scalable Density-Based Distributed Clustering . PKDD'04, pp. 231-244. [5] Januzaj E., Kriegel H.-P., Pfeifle M.: Density-Based Distrib-uted Clustering . EDBT'04, pp.88-105. [6] Kriegel H.-P., Kunath P., Pfeifle M., Renz M.: Approximated Clustering of Distributed High Dimensional Data. PAKDD X 05. [7] Kriegel H.-P., Pfeifle M.: Measuring the Quality of Approxi-mated Clusterings . BTW X 05, pp. 415-424. [8] Kriegel H.-P., Pfeifle M.: Clustering Moving Objects via Medoid Clusterings. SSDBM X 05. [9] Li Y., Han J., Yang J.: Clustering Moving Objects . KDD X 04, pp. 617-622. [10] Yiu M. L., N. Mamoulis N.: Clustering Objects on a Spatial Network . SIGMOD X 04, pp. 443-454.
