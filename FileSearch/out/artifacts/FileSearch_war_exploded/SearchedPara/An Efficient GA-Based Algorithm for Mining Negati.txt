 The concept of discovering sequential patterns was firstly introduced in 1995 [1], and aimed at discovering frequent subsequences as patterns in a sequence database, given a user-specified minimum support threshold. Some popular algorithms in sequential pattern mining include AprioriAll [1], Generalized Sequential Patterns (GSP) [10] and PrefixSpan [8]. GSP and AprioriAll are both Apriori-like methods based on breadth-first search, while PrefixSpan is based on depth-first search. Some other methods, such as SPADE (Sequential PAttern Discovery using Equivalence classes)[12] and SPAM (Sequential PAttern Mining)[4], are also widely used in researches.

In contrast to traditional positive sequentia l patterns, negative sequential patterns focus on negative relationships between ite msets, in which, absent items are taken into consideration. We give a simple example to illustrate the difference: suppose p 1 = &lt; abc each item, a , b , c , d and e , stands for a claim item code in the customer claim database of an insurance company. By getting the pattern p 1 , we can tell that an insurant usually claims for a , b , c and d in a row. However, only with the pattern p 2 , we are able to find that given an insurant claim for item a and b , if he/she does NOT claim c ,thenhe/she would claim item e instead of d . This kind of patterns cannot be described or discovered by positive sequential pattern mining.

However, in trying to utilize traditional frequent pattern mining algorithms for min-ing negative patterns, two problems stand in the way. (1) Huge amounts of negative candidates will be generated by classic breath-first search methods. For example, given 10 distinct positive frequent items, there are only 1,000 (=10 3 ) 3-item positive can-didates, but there will be 8,000 (=20 3 ) 3-item negative candidates because we should count 10 negative items in it. (2) Take a 3-item data sequence &lt; abc &gt; , it can only negative case, data sequence &lt; abc &gt; not only supports the positive candidates as the  X  candidates even after effective pruning.

Based on Genetic Algorithm (GA) [5], we propose a new method for mining neg-ative patterns. GA is an evolvement method, which simulates biological evolution. A generation pass good genes on to a new generation by crossover and mutation, and the populations become better and better after many generations. We borrow the ideas of GA to focus on the space with good genes, because this always finds more frequent patterns first, resulting in good genes. It is therefore more effective than methods which treat all candidates equally, especially when a very low support threshold is set. It is equally possible to find long negative patterns at the beginning stage of process.
Our contributions are:  X  A GA-based algorithm is proposed to find negative sequential patterns efficiently. It  X  Extensive experimental results on 3 synthetic datasets and a real-world dataset This paper is organized as follows. Section 2 talks about related work. Section 3 briefly introduces negative sequential patterns and presents formal descriptions of them. Our GA-based algorithm is then described in Section 4. Section 5 shows experimental re-sults on some datasets. The paper is concluded in the last section. Most research on sequential patterns has focused on positive relationships. In recent years, some research has started to focu s on negative sequential pattern mining.
Zhao et al. [13] proposed a method to find negative sequential rules based on SPAM &lt;  X  A  X  X  B &gt; . Ouyang &amp; Huang [7] extended traditional sequential pattern definition (A,B) to include negative elements such as (  X  A,B) , (A,  X  B) and (  X  A,  X  B) . They put for-ward an algorithm which finds both frequent and infrequent sequences and then obtains negative sequential patterns from infrequent sequences. Nancy et al. [6] designed an al-gorithm PNSPM and applied the Apriori principle to prune redundant candidates. They extracted meaningful negative sequences using the interestingness measure; neverthe-less the above works defined some limited negative sequential patterns, which are not general enough. Sue-Chen et al. [11] presented more general definitions of negative se-quential patterns and proposed an algorithm called PNSP, which extended GSP to deal with mining negative patterns, but they generated negative candidates in the appending step, which then may produce a lot of unnecessary candidates.

Some existing researches have used GA for m ining the negative association rule and positive sequential pattern. Bilal and Erhan [2] proposed a method using GA to mine negative quantitative association rules. T hey generated uniform initial population, and used an adaptive mutation probability and an adjusted fitness function. [9] designed a GA to mine generalized sequential patterns, but it is based on SQL expressions. It is an instructive work since there are few resear ch works using GA for negative sequential pattern mining. 3.1 Definitions is an element. An element e i (1  X  i  X  k) consists of one or more items. For example, sequence &lt; ab(c,d)f &gt; consists of 4 elements and (c,d) is an element which includes two items. The length of a sequence is the number of ite ms in the sequence. A sequence with k items is called a k-sequence or k-item sequence .

Sequence is a general concept. We extend sequence definition to positive/negative sequence. A sequence s = &lt; e 1 e 2 ... e n &gt; is a positive sequence , when each element e when  X  i , e i (1  X  i  X  n ) is a negative element, which represents the absence of an element. are both negative sequences.
 e 2 ... e p n &gt; , if there exists 1
A sequence s r is a maximum positive subsequence of another sequence s p ,if s r is a subsequence of s p ,and s r includes all positive elements of s p . For example, &lt; ab f &gt; is maximum positive subsequence of &lt; ab  X  cf &gt; and &lt; ab  X  (c,d) f &gt; .
Definition 1: Negative Sequential Pattern . If the support value of a negative sequence is greater than the pre-defined support threshold min sup , and it also meets the following constraints, then we call it a negative sequential pattern. 1) Items in a single element should be all positive or all negative. The reason is that a positive item and negative item in the same element are unmeaning. For example, &lt; a (a,  X  b) c &gt; is not allowed since item a and item  X  b are in the same element. 2) Two or more continuous negative elements are not accepted in a negative se-quence. This constraint is also used by other researchers [11].
 3) For each negative item in a negative pattern, its positive item is required to be be frequent. It is helpful for us to focus on the frequent items.

In order to calculate the support value of a negative sequence against the data se-quences in a database, we need to clarify the sequence matching method and criteria. In other words, we should describe what ki nds of sequence a data sequence can support.
Definition 2: Negative Matching . A negative sequence s n = &lt; e 1 e 2 ... e k &gt;matches a data sequence s = &lt; d 1 d 2 ... d m &gt; ,iff: 1) s contains the max positive subsequence of s n such that:  X  e i  X  1  X  d p  X  e i +1  X  d r ,andfor  X  d q ,e i  X  d q dca &gt; , since the negative element c appears between the element b and a . 3.2 Ideas of GA-Based Method As introduced in Section 1, negative sequential pattern mining may encounter huge amounts of negative candidates even after effective pruning. It will take a long time to pass over the dataset many times to get the candidates X  support.

Based on GA, we obtain negative sequential patterns by crossover and mutation, without generating candidate s; high frequent patterns are then selected to be parents to generate offspring. It will pass the best genes on to the next generations and will always focus on the space with good genes. By going through many generations, it will obtain a new and relatively h igh-quality population.

A key issue is how to find all the negative patterns since the GA-based method cannot ensure locating all of them. We therefore use an incremental population, and add all negative patterns, which are generated by crossover and mutation during the evolution process, into population. A dynamic fitness function is proposed to control population evolution. Ultimately, we can secure almo st all the frequent patterns. The proportion can reach 90 % to 100 % in our experiments on two synthetic datasets. The general idea of the algorithm is shown as Fig. 1. We will describe the algorithm from how to encode a sequence, and then introduce population, selection, crossover, mutation, pruning, fitness function and so on. A detailed algorithm will then be introduced. 4.1 Encoding Sequence is mapped into the chromosome code in GA. Both crossover and mutation operations depend on the chromosome code. We need to define the chromosome to represent the problems of negative sequential pattern mining exactly. There are many different methods to encoding the chromosome, such as binary encoding, permutation encoding, value encoding and tree encoding [5]. The permutation encoding method sequence data, so we use it for sequence encoding.

Each sequence is mapped into a chromosome. Each element of the sequence is mapped into a gene in the chromosome, no matter whether the element has one item or more. Given a sequence &lt; e 1 e 2 ... e n &gt; , it is transformed to a chromosome which has n-genes. Each gene is composed of a tag and an element. The element includes one or more items, and the tag indicates that the element is positive or negative. For example, a negative sequence &lt; ab  X  (c,d) &gt; is mapped into a 3-gene chromosome, see Table 2. 4.2 Population and Selection In the classical GA method, the number of populations is fixed [5]. We using a fixed number of populations to produce the next ge neration, but the populations tended to contract into one high frequent pattern, and we can only obtain a small part of frequent patterns. To achieve as many sequential patterns as possible, we potentially needed a population to cover more individuals. We therefore adjusted the basic GA to suit negative sequential pattern mining in the following ways.
 Initial Population. All 1-item frequent positive patterns are obtained first. Based on the 1-item positive patterns, we transform all of them to their corresponding 1-item negative sequences, such as transfo rming the frequent positive sequence &lt; e &gt; to the negative sequence &lt;  X  e &gt; . We then take all positive and negative 1-item patterns as the initial population.
 Population Increase. We do not limit population to a fixed number. When we acquire new sequential patterns during the evolve ment, new patterns are put into the population for the next selection. If the population has already included the patterns, we ignore them. To improve the performance of this pro cess, a hash table is used to verify whether a pattern has already appeared in the population.
 Selection. The commonly used selection method is roulette wheel selection [3]. We have an increased population and the population number depends on the count of se-quential patterns; thus, we can not use roul ette wheel selection b ecause the selection will be too costly if the population number is huge. We select the top K individuals with high dynamic fitness (see Section 4.5), where K is a constant number showing how many individuals will be selected for the next generation. To improve the perfor-mance of this selection method, we sort all individuals in population in descending order by dynamic fitness value. In every generation, we only select the first K individuals. 4.3 Crossover and Mutation Crossover. Parents with different lengths are allowed to crossover with each other, and crossover may happen at different positions to get sequential patterns with varied lengths. For example, a crossover takes place at a different position, which is shown by  X  %  X  in Table 3. After crossover, it may acquire two children. Child 1 &lt; b  X  ce &gt; consists ofthefirstpartof parent 1 and the second part of parent 2 . Child 2 &lt; da &gt; consists of the second part of parent 1 and the first part of parent 2 . So we get two children with different lengths. If a crossover takes place both at the end/head of parent 1 and at the head/end of parent 2 ,asTable4shows, child 2 will be empty. In that case, we shall set child 2 by reverse. A Crossover Rate is also used to control the probability of cross over when parents generate their children.
 Mutation. Mutation is helpful in avoiding contraction of the population to a special frequent pattern. To introduce mutation int o sequence generation, we select a random position and then replace all genes after that position with 1-item patterns. For example, &lt;  X  e &gt; are 1-item patterns. M utation Rate is a percentage to indicate the probability of mutation when parents generate their children. 4.4 Pruning When a new generation is obtained after crossover and mutation, it is necessary to verify whether the new generation is valid in terms of the constraints for negative sequential patterns before passing over the whole dataset for their supports.
 max positive subsequence of c ,thatistosay, e i , e j ,...and e k are all positive elements, and other elements in c are negative. If c is not frequent, c must be infrequent and should be pruned. This method is simple but effective for pruning invalid candidates without cutting off possible va lid individuals by mistake. 4.5 Fitness Function In order to evaluate the individuals and decide which are the best for the next generation, a fitness function for individuals is implemented in GA. We use the fitness function shown in Eqn.(1): Fitness. The fitness function is composed of two parts. Support is the percentage that indicates how many proportion records are matched by the individual. If support is high, fitness will be high, so that the individual has good characteristics to pass down to next generation. min sup is a threshold percentage value for verifying whether a sequence is frequent. Dataset size is the record count of whole dataset.
 Dynamic Fitness. Because the characteristics of the individual have been transmitted to the next generation by crossover or mutation, the individual should exit after a few generations. The result will tend to contr act to one point if the individual doesn X  X  exit gradually. We therefore set a dynamic fitness df i t n e s s to every individual in the popu-lation, shown in Eqn.(2). Its initial value is equal to fitness , but decreases during the evolvement. It indicates th at the individuals in the population will gradually ceased to evolve. It is like a life value. When an individual X  X  dynamic fitness is low or close to 0( &lt; 0.01), we set it to 0 because we regard it as a wasted individual which cannot be selected for the next generation. Decay Rate. We set a decay rate to indicate the decr ease speed of individual X  X  fitness. The decay rate is a percentage value between 0% and 100%. If an individual is selected by the selection process, its dynamic fitn ess will decrease by the speed of the decay rate. If the decay rate is high, dynamic fitness will decrease quickly and individuals will quickly cease to evolve. Thus, we may get less frequent patterns through a high decay rate. If we want to obtain the maximum fre quent patterns, we can set a low decay rate, such as 5 % , but this will give rise to a longer running time. 4.6 Algorithm Description Our algorithm is composed of the following six steps.

Step 1: We obtain the initial population which includes all frequent 1-item positive and 1-item negative sequences. Step 2: Calculate all initial individuals X  fitness. Their dynamic f itness is set to their fitness . Step 3: We select the top K individuals with high dynamic fitness from the population. After selection, the dynamic fitness of the selected individuals is updated by Eqn.(2). Step 4: Crossover and mutation between the selected individuals to produce the next generation. Step 5: After obtaining the next generation, we first prune invalid indivi duals and then calculate the frequency and fitness of remained individuals in new gener ation. If the frequency of an individual is greater than min sup , we add it into the population, and set its fitness and dynamic fitness, but if the population has included this individual, we ignore it. Step 6: Go back to step 3 and iterate the above process until all individuals in the population are dead (i.e., their dynamic fitness has become cl ose to 0). The dead individuals are still in population, but they ceased to evolve. In t he end, we obtain the final result -whole population, which is composed of all dead individuals.
 The pseudocode of our algorithm is given as follows.
 Our algorithm was implemented with Java and tested with three synthetic sequence datasets generated by an IBM data generator [1] and a real-world dataset. We also im-plemented the PNSP algorithm [11] and Neg-GSP algorithm [14] with Java for perfor-mance comparison. All the experiments were conducted on a PC with Intel Core 2 CPU of 2.9GHz, 2GB memory and Windows XP Professional SP2.

Dataset 1( DS 1) is C8.T8.S4.I8.DB10k.N1k, which means the average number of elements in a sequence is 8, the average num ber of items in an element is 8, the average length of a maximal pattern consists of 4 e lements and each element is composed of 8 items average. The data set contains 10k sequences, the number of items is 1000. Dataset 2( DS 2) is C10.T2.5.S4.I2.5.DB100k.N10k.
 Dataset 3( DS 3) is C20.T4.S6.I8.DB10k.N2k.

Dataset 4( DS 4) is real application data for insura nce claims. The data set contains 479 sequences. The average number of elements in a sequence is 30. The minimum number of elements in a sequence is 1, and the maximum number is 171.

Experiments were done to compare the different Crossover Rate , M utation Rate and Decay Rate on two synthetic datasets, DS 1 and DS 2 . Each experiment was run 10 times and then the average value was got as the final result. We focused on comparing runtime, the number of patterns and the runtime per pattern, which indicates how long it takes to get one pattern. The total number of all patterns was determined by PNSP and Neg-GSP algorithm, and it was then easy to know the proportion of patterns we could get by using our algorithm. The Y axis (see the 3rd and 4th chart of Fig. 2) indicates the proportion of patterns.
 Crossover Rate. We compared different crossover rates from 60% to 100%. Fig. 2 shows the effect of different crossover rates on DS 1 and DS 2 . With low crossover rates, such as 60 % , we obtained almost the same proportion of patterns as with high crossover rates (see the 2nd and 5th charts in Fig. 2). The least runtime per pattern is achieved when the crossover rate is low, so 60 % is the best choice for the two datasets in our experiments.
 Mutation Rate. We compared different mutation rates from 0% to 20% on DS 1 and DS 2 (see Fig. 3). They show that the mutation rate will not have an outstanding effect, but if it is set to 0%, it will result in missing a lot of patterns. A Mutation rate of 5-10 % is a good choice because it can produce around 80% patterns for DS 1 and above 90% patterns for DS 2 . When the mutation rate is 5%, the average runtime per pattern is lower. We therefore set a mutation rate of 5% for the following experiments. Decay Rate. Decay rate is a variable that we used t o control evolution speed. If the decay rate is high, individuals will die quickly, so we can get only small proportion of patterns. If the decay rate is low, we can get more patterns, but a longer runtime is necessary (see Fig. 4). In order to get all negative sequential patterns, we always choose decay rate =5 % , which enables us to obtain around 90 % to 100 % patterns on the two datasets. Performance Comparison. We compared our algorithm with PNSP and Neg-GSP, which are two algorithms proposed recently for n egative sequential pattern mining. The tests are based on Crossover Rate =60 % , M utation Rate =5 % and Decay Rate =5 % . The results (see Fig. 5) on 4 different datasets show that the performance of the GA-based algorithm is better than PNSP and Neg-GSP when the support threshold is low. Our algorithm is not better than others when min sup is high, because most patterns are very short and the GA-based method cannot demonstrate its advantage.

When min sup is high, there are not as many patterns and the patterns are short, so it is very easy to find the patterns with existing methods. However, when min sup is low, the patterns are longer and the search space is much bigger, so it is time-consuming to find patterns using traditional methods. Using our GA-based algorithm, it is still can obtain the patterns quickly even though min sup is very low.
 Based on GA, we have proposed a method for negative sequential pattern mining. Ex-tensive experimental results on synthetic datasets and a real-world dataset show that the proposed method can find negative patterns efficiently, and it is better than existing algorithms when the support threshold min sup is low or when the patterns are long.
In our future work, we will focus on studying new measures including fitness func-tion, selection and crossover method to make our algorithm more efficient. There should also be some better methods for pruning. Other work will be to explore post mining to find interesting patterns from the discovered negative sequential patterns. As we have obtained many negative sequential patterns, the means of finding interesting and inter-pretable patterns from them is valuable in industry applications.

