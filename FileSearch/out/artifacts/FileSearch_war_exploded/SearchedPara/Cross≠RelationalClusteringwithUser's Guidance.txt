 Clustering is an essen tial data mining task with numerous applications. However, data in most real-life applications are high-dimensional in nature, and the related information often spreads across multiple relations. To ensure e X ectiv e and e X cien t high-dimensional, cross-relational clustering, we prop ose a new approac h, called CrossClus , which per-forms cross-relational clustering with user's guidance. We believe that user's guidance, even likely in very simple forms, could be essen tial for e X ectiv e high-dimensional clustering since a user knows well the application requiremen ts and data seman tics. CrossClus is carried out as follows: A user speci X es a clustering task and selects one or a small set of features pertinen t to the task. CrossClus extracts the set of highly relevant features in multiple relations connected via linkages de X ned in the database schema, evaluates their e X ectiv eness based on user's guidance, and identi X es inter-esting clusters that  X t user's needs. This metho d takes care of both qualit y in feature extraction and e X ciency in clus-tering. Our comprehensiv e experimen ts demonstrate the ef-fectiv eness and scalabilit y of this approac h.
 Categories and Subject Descriptors: H.2.8 [Database Managemen t]: Database applications { Data mining General Terms: Algorithms.
 Keyw ords: data mining, clustering, relational databases.
Clustering is a process of partitioning data objects into groups according to some similarit y measure [11, 16, 17, 19]. Most existing metho ds perform clustering within a single ta-ble, and many handle only a small number of dimensions. Recen t studies on high-dimensional clustering have devel-
The work was supp orted in part by the U.S. National Sci-ence Foundation NSF IIS-02-09199, and an IBM Facult y Award. Any opinions,  X ndings, and conclusions or recom-mendations expressed in this paper are those of the authors and do not necessarily re X  X ct views of the funding agencies. Cop yright 2005 ACM 1 X 59593 X 135 X  X /05/0008 ... $ 5.00. oped metho ds for subspace or projected clustering [1, 2, 6] that aim at  X nding clusters on subsets of dimensions in a high-dimensional space. However, many of such metho ds ignore user's expectation but only examine the dissimilari-ties among data objects. The clusters so generated could be lack of seman tic meaning and poor in qualit y.

Real-life applications often pose additional challenges since data can be not only high-dimensional but also with related information spread across multiple relations. Few studies handle clustering across multiple relations. Consider a task of clustering studen ts in a relational database based on their academic performance (as in the database shown in Figure 1). It may not be meaningful to cluster studen ts based on the attributes like phone number, ssn, and residence address, even if they reside in the same relation. However, some crucial information, such as studen t's registration, courses taken, and researc h publications, can be stored across mul-tiple relations. Thus it is necessary to explore clustering across multiple relations.

Based on the above observ ations, we introduce two new concepts: (1) user-guide d clustering , and (2) cross-r elational clustering . The necessit y to introduce these two notions is obvious. First, a user often knows well the application requiremen ts and data seman tics. Thus a user's guidance, even in a simple form such as clustering students based on their research areas , could be essen tial for e X ectiv e high-dimensional clustering. Second, crucial information is often stored across multiple relations, and it is di X cult for a user to specify all pertinen t information. For example, to cluster studen ts according to their researc h areas (see Figure 1), most crucial information may be stored in several relations, such as Group , Advise , Course , and Publication .
To handle such a clustering task, we prop ose a new metho d-ology , called cross -relational clus tering with user's guid-anc e , or simply , CrossClus . This metho dology explores how to specify and utilize user's guidance in clustering and how to perform cross-r elational clustering , i.e., partition the data objects into a set of clusters based on their similar-ity, utilizing information in multiple relations .
Clustering in multi-relational environmen ts has been stud-ied in [7, 14, 15], in which the similarit y between two objects are de X ned based on tuples joinable with them. However, there are two problems with such approac hes. First, al-though they provide similarit y de X nitions in multi-relational environmen ts, it is usually very expensiv e to compute such similarities because an object is often joinable with hun-dreds or thousands of tuples. Second, there are usually a large number of candidate features in a database (such as in Figure 1), generated from di X eren t attributes with di X eren t join paths. They cover di X eren t aspects of information (e.g., researc h, grades, address), but only a small portion of them are pertinen t to the user's goal. However, all features are used indiscriminately in above approac hes, which is unlik ely to generate desirable clustering results.

The above problem can be solved by semi-sup ervised clus-tering [12, 13, 21], in which a set of similar (or dissimilar) pairs of instances are provided by user. However, this re-quires a user to have good knowledge about the data and the clustering goal. Moreo ver, since di X eren t users have dif-feren t clustering goals that can only be expressed by train-ing data, and the complex information in multiple relations may overwhelm the user, it is very burdensome for a user to provide a high qualit y training set, which is critical to the clustering results.

In this paper we prop ose a di X eren t approac h. First, we remo ve the user's burden of providing training data. In order to express her clustering goal, a user poses a clustering query that consists of a target relation and one or a small set of pertinent attribute(s) . The target relation may join with each relation via di X eren t join paths, which leads to the generation of a huge number of \connected" features with diverse seman tic meaning (see Figure 1). Our clustering task is to perform comprehensiv e analysis across multiple relations to select the set of pertinen t features and deriv e high qualit y clustering that meets user's expectation. Example 1. In the CS Dept database in Figure 1, the target relation is Student , and the goal is to cluster studen ts according to their researc h areas. A user query for this goal could be \ CLUSTER Student WITH Group:ar ea ".
 Such a user guidance shows her preference in clustering. However, this guidance should not be treated as the speci- X cation of class label attribute as in classi X cation or semi-supervised clustering for the following reasons. First, a stu-dent may belong to multiple groups or may not be in any group if the studen t is new or prefers to work alone. Sec-ond, \group" may represen t only one factor of \researc h area". There are many other importan t factors, such as re-searc h publications, advisors, projects, and courses taken. Third, the guidance may serve only as a loose hint to the system since a nonexp ert user may only have partial knowl-edge about the data, and thus may not even select the most importan t attributes in the query . Nevertheless, such a guid-ance plays a key role in our system since it will help the an-alyzer  X nd other strongly correlated attributes across mul-tiple relations.

The crucial challenge in cross-relational clustering is how to select pertinen t features for clustering in the huge feature space. In a database with nontrivial schema, there are usu-ally a large number of candidate features. Since only a small number of them are pertinen t to the clustering goal, a pow-erful technique for feature searc h is needed to select good features for clustering. There have been many approac hes for feature selection [4, 8, 18]. But they are mainly designed for classi X cation or trend analysis instead of clustering. For example, two features with very di X eren t trends may actu-ally cluster objects in similar ways (details in Section 2).
To successfully perform user-guided clustering in multiple relations, CrossClus developed two new techniques: (1) e X ectiv e feature selection according to how features cluster tuples, and (2) e X cien t searc h for pertinen t features across interconnected relations.

In cross-relational clustering each feature clusters target tuples by indicating similarities between them. Thus we introduce similarity vector , a new notion for represen ting a feature. For a feature f , its similarit y vector V f contains the similarit y between each pair of target tuples indicated by f . V f is a vector of N 2 dimensions if there are N target tuples. The similarit y vector of a feature indicates how it clusters target tuples, and the similarit y between two such vectors indicates whether they cluster tuples in a similar way. It is a good measure for feature selection because it captures the essen tial information for clustering and provides a coheren t way to compare di X eren t types of features (categorical and numerical). However, it is very expensiv e to materialize the similarit y vectors and compute feature similarit y based on them. Thus we prop ose new metho ds that compute similar-ity between di X eren t types of features in linear time, without materializing the similarit y vectors.

With a good measure for selecting features, CrossClus needs to searc h for pertinen t features across many inter-connected relations. Because of the huge feature space, it is impossible to perform exhaustiv e searc h, and heuris-tic algorithms must be used. Two principles are followed by CrossClus in feature searc h: (1) start from the user-speci X ed attribute and gradually expand the searc h scope to other relations, and (2) the searc h must be con X ned in promising directions to avoid fruitless searc h. CrossClus  X nally selects a set of highly pertinen t and non-redundan t features for clustering. The k -medoids algorithm is selected for clustering target tuples due to its high scalabilit y and applicabilit y in non-Euclidean space.

Our experimen ts on both real and synthetic data sets show that CrossClus successfully identi X es pertinen t features for each clustering task, which validates the e X ectiv eness of our approac h for searc hing and selecting features. It also shows the high e X ciency and scalabilit y of CrossClus even for databases with complex schemas.

The remaining of the paper is organized as follows. The problem de X nition is introduced in Section 3. Section 4 describ es the approac h for feature searc h, and Section 5 presen ts the approac h for clustering tuples. Experimen tal results are presen ted in Section 6. We discuss the future extensions in Section 7 and conclude the study in Section 8.
Clustering has been extensiv ely studied for decades in dif-feren t disciplines including statistics, pattern recognition, database, and data mining. There are mainly two categories of clustering approac hes: (1) probabilit y-based approac hes [3], and (2) distance-based approac hes [16, 11]. Many other approac hes have been prop osed recen tly that focus on the e X ciency and scalabilit y issues (e.g., [19]).

Traditional clustering approac hes may not generate good clusters in high dimensional space. Subspace clustering [1, 2, 6] was prop osed to address this problem, in which each cluster is de X ned based on a subset of dimensions. How-ever, when there are a large number of features with very di X eren t seman tic meanings, subspace clustering may gen-erate many clusters with di X eren t seman tic meanings, and only a small set of them  X t the user's goal. In contrast, CrossClus explores the concept of user-guided clustering: It selects a set of pertinen t features according to user guid-ance and then performs clustering with them, which leads to more meaningful clustering results.

Clustering in multi-relational environmen ts has been stud-ied in [7, 14, 15]. When computing similarit y between two tuples, they consider not only the attributes of the two tu-ples, but also those of the tuples related to them (joinable via a few joins) [5, 7]. However, these approac hes su X er from poor e X ciency and scalabilit y, because in a large database an object is often joinable with thousands of tuples via a few joins, which makes it very expensiv e to compute similarit y between each pair of objects. Moreo ver, they use all features indiscriminately and may not generate good clustering re-sults that  X t the user's goal.

Semi-sup ervised clustering [12, 13, 21] also performs clus-tering under user's guidance. The user guidance is provided either as a set of similar (or dissimilar) pairs of instances [12, 21] or by relev ance feedbac k [13]. Such guidance is ei-ther used to reassign tuples [12] to clusters or to wrap the space to generate better models [21]. However, it is burden-some for each user to provide such training data, and it is often di X cult to judge whether two tuples belong to same cluster since all relev ant information in many relations needs to be shown to user. In contrast, CrossClus allows the user to express the clustering goal with a simple query contain-ing a pertinen t attribute, and will searc h for more pertinen t features across multiple relations.

Selecting the right features is crucial for cross-relational clustering. Although feature selection has been extensiv ely studied for both supervised learning [8] and unsup ervised learning [4, 18], the existing approac hes may not be appro-priate for cross-relational clustering. The widely used cri-teria for selecting numerical features include Pearson Cor-relation [9] and least square error [18]. However, such cri-teria focus more on trends of features, instead on how they cluster tuples. For example, consider the 80 tuples shown in Figure 2, with their values on two features. According to the above criteria, the two features have correlation of zero. If we cluster the tuples into 8 clusters according to each feature, feature 1 will create clusters f t 1 ;:::;t f t 71 ;:::;t 80 g , feature 2 will create f t 1 ;:::;t 5 ;t 76 ::: , f t 36 ;:::;t 40 ; t 41 ;:::;t 45 g . If two tuples belong to same cluster according to feature 1, they have half chance to be-long to same cluster according to feature 2, and vice versa. One can see that feature 1 and feature 2 actually cluster tu-ples in rather similar ways, although they are \uncorrelated" according to some correlation measures. For categorical fea-tures, the most popular criteria include information gain [17] or mutual information [8]. However, it is di X cult to de X ne information gain or mutual information for multi-relational features, because a tuple has multiple values on a feature.
CrossClus accepts user queries that contain a target re-lation, and one or a small set of pertinent attribute(s) . Con-sider the query \ CLUSTER Student WITH Group:ar ea " in Ex-ample 1 that aims to cluster studen ts according to their re-searc h areas. Student is the target relation R t , and Group:ar ea is the pertinen t attribute. Since the pertinen t attribute provides crucial but very limited information for cluster-ing, CrossClus searc hes for other pertinen t features, and groups the target tuples based on those features. Di X eren t relations are linked by joins. As in some other systems on re-lational databases (such as DISCO VER [10]), only joins be-tween keys and foreign-k eys are considered by CrossClus . Other joins are ignored because they do not represen t strong seman tic relationships between objects. Due to the nature of relational data, CrossClus uses a new de X nition for ob-jects and features, as describ ed below. A multi-relational feature f is de X ned by a join path R t ./ R 1 ./  X  X  X  ./ R k , an attribute R k :A of R k , and pos-sibly an aggregation operator ( e.g. , average, coun t, max). f is formally represen ted by [ f:joinpath; f:attr;f:aggr ]. A feature f is either a categoric al feature or a numeric al one , depending on whether R k :A is categorical or numerical. For a target tuple t , we use f ( t ) to represen t t 's value on f . We use T f ( t ) to represen t the set of tuples in R k that are joinable with t , via f:joinpath . Figure 3: A feature of areas of courses tak en by studen ts
If f is a categorical feature, f ( t ) represen ts the distri-bution of values among T f ( t ). Supp ose f:attr has l values v ;:::;v l . f ( t ) is an l -dimensional vector ( f ( t ) :p with f ( t ) :p i represen ting the prop ortion of tuples in T having value v i . For example, supp ose Student is the tar-get relation in the CS Dept database. Consider a feature f = [ Student ./ Register ./ OpenC ourse ./ Course , area , null 1 ] (where area represen ts the areas of the courses taken by a studen t). An example is shown in Figure 3. A stu-dent t 1 takes four courses in database and four in AI, thus f ( t 1 ) = (database:0.5, AI:0.5). In general, f ( t ) represen ts t 's relationship with each value of f:attr . The higher f ( t ) :p is, the stronger the relationship between t and v i is. This vector is usually sparse and only non-zero values are stored. Figure 4: A feature of studen ts' average grades
If f is numerical, then f has a certain aggregation operator (average, coun t, max, . . . ), and f ( t ) is the aggregated value of tuples in the set T f ( t ), as shown in Figure 4.
When searc hing for pertinen t features during clustering, both categorical and numerical features need to be repre-sented in a coheren t way, so that they can be compared and pertinen t features can be found. In clustering, the most im-portan t information carried by a feature f is how f clusters tuples, which is conveyed by the similarit y between tuples indicated by f . Thus in CrossClus a feature is mainly represen ted by the similarity vector , as de X ned below.
De X nition 1. ( Similarit y Vector ) Supp ose there are N target tuples t 1 ;:::;t N . The similarit y vector of feature f , V f , is an N 2 dimensional vector. V f similarit y between t i and t j indicated by f .

The similarit y vector of feature f in Figure 3 is shown in Figure 5. The two horizon tal axes are tuple indices, and the vertical axis is similarit y. The similarit y vector of a fea-ture represen ts how it clusters target tuples. It contains the most importan t information for feature selection in cluster-ing, and is used for identifying pertinen t features according to user guidance.
In this section we  X rst describ e the similarit y measure between features, then introduce our approac h for feature searc hing based on user's guidance.
Given the user guidance, CrossClus selects pertinen t fea-tures based on their relationships to the feature speci X ed by user. To achieve this goal, we should compare features to  X nd pertinen t features but avoid redundan t ones. Features should be compared based on how they cluster target tuples. If two features cluster tuples in a similar way, they should be considered to be related to each other; and vice versa.
Similarit y between two features are measured based on their similarit y vectors, which represen t the ways they clus-ter tuples. If two features cluster tuples very di X eren tly, their similarit y vectors are very di X eren t, and their similar-ity is low. If they cluster tuples in almost the same way, their similarit y is very high, which indicates that they contain redundan t information. In general, the similarit y between features gives us good hints about the relationship between them. Moreo ver, although di X eren t types of features contain di X eren t formats of information that is hard to compare di-rectly , we can de X ne similarit y between them in a coheren t way as long as we know their similarit y vectors.
In CrossClus a categorical feature does not simply par-tition the target tuples into disjoin t groups. Instead, a tar-get tuple may have multiple values on a feature, as de X ned in Section 3.2. Consider a categorical feature f that has l values v 1 ;:::;v l . A target tuple t 's value on f is a vec-joinable with t (via f:joinpath ) that have value v i on f:attr , which represen ts the probabilit y that t is related to v
The similarit y between two tuples t 1 and t 2 w.r.t. f is de X ned as the probabilit y that t i and t j is related to the same value on f . An example is shown in Figure 6.
De X nition 2. The similarit y between two tuples t 1 and t w.r.t. f is de X ned as
There are other measures for de X ning similarit y between two probabilistic distributions, such as KL-distance (or rela-tive entropy), or cosine similarit y between the two distribu-tion vectors. However, such measures focus on the similarit y of the two distributions, instead of the prop erties of the two objects. For example, supp ose we are clustering studen ts according to their researc h areas. If studen ts t 1 and t have probabilit y of 0.5 to take courses in DB area and 0.5 in AI area, then their similarit y is 1 according to KL-distance or cosine similarit y. But actually it is quite possible that one is in DB area and the other is in AI area. If t 1 has probabil-ity 1.0 in DB, and t 2 has 0.7 in DB and 0.3 in AI, then it is more likely they are in the same area although their similar-ity is not high according to KL-distance or cosine similarit y. We choose a simple but appropriate measure|the probabil-ity that t 1 and t 2 take courses in same area, which better represen ts the relationship between tuples w.r.t. a feature.
Di X eren t numerical features have very di X eren t ranges and variances, and we need to normalize them so that they have equal in X  X ences in clustering. For a feature h , similarit y between two tuples t 1 and t 2 w.r.t. h is de X ned as follows.
De X nition 3. Supp ose  X  h is the standard deviation of tu-ple values of numerical feature h . The similarit y between two tuples t 1 and t 2 w.r.t. h is de X ned as sim h ( t 1 ;t 2 ) = 1  X 
That is, we  X rst use Z score to normalize values of h , so that they have mean zero and variance one. Then the sim-ilarit y between tuples are calculated based on the Z scores. If the di X erence between two values is greater than  X  h , their similarit y is considered to be zero.
When clustering tuples, a feature is used to indicate the relationship between each pair of tuples. CrossClus repre-sents a feature f by the similarities between tuples indicated by f . Supp ose there are N target tuples t 1 ;:::;t N . f is represen ted by its similarit y vector V f (in which V f iN + j sim f ( t i ;t j )).

When measuring the similarit y between two features f and g , we compare their similarit y vectors V f and V g , which represen t how they cluster tuples. The similarit y between two features is de X ned as the cosine similarit y between their similarit y vectors. This de X nition can be applied on both categorical and numerical features.

De X nition 4. The similarit y between two features f and g is de X ned as 2 in which j V f j = sim ( f;g ) = 1 if and only if V f and V g di X er by a con-stant ratio, and sim ( f;g ) is small when most pairs of tu-ples that are similar according to f are dissimilar accord-ing to g . Figure 7 shows the values of two features f = eas of courses taken by each studen t) and g = [ Student ./ W orkIn , group , null] (researc h group of each studen t). Their similarit y vectors V f and V g are shown in Figure 8. To compute sim ( f;g ), one needs to compute the inner product of V f and V g , as shown in Figure 8.

The similarit y between features is expensiv e to compute, if directly following the de X nition. We cannot even a X ord storing N 2 dimensional vectors for many applications. An e X cien t algorithm is needed to compute sim ( f;g ) without materializing V f and V g .

We  X rst describ e the approac h for computing similarities between categorical features. The main idea is to convert the hard problem of computing inner product of two simi-larity vectors of N 2 dimensions, into an easier problem of computing similarities between feature values, which can be solved in linear time. Similarities between tuples are de X ned according to their relationships with the feature values, as in De X nition 2. Similarities between feature values can be de X ned similarly according to their relationships with tuples.
De X nition 5. The similarit y between value v k of feature f and value v q of feature g is de X ned as
The de X nitions of tuple similarit y and feature value sim-ilarit y are similar and symmetric to each other. Because of the symmetric de X nitions, we can convert the inner product of V f and V g from summation of tuple similarities into that of feature value similarities.
 Lemma 1.

With Lemma 1, to compute the similarit y between f and g , we only need to compute sim ( f:v k ;g:v q ) for each value v k of f and v q of g . We can compute them by scanning the tuples just once. For each tuple t , we get f ( t ) and g ( t ). For each value v k so that f ( t ) :v k &gt; 0, and each v g ( t ) :v q &gt; 0, we update sim ( f:v k ;g:v q ) by adding f ( t ) :v g ( t ) :v q . In this way sim ( f:v k ;g:v q ) (1  X  k  X  l , 1  X  q  X  m ) can be computed in one scan. This requires an entry in memory for each sim ( f:v k ;g:v q ). In realit y most categorical features have only several or tens of values. If f has too many values, CrossClus will make an indexed scan on f (scanning tuples having each value of f ), and it only needs to main tain an entry for each value of g . In general, V f  X  V be computed in linear time, with very limited extra space.
A di X eren t approac h is needed for computing similarities involving numerical features, because of the di X eren t de X -nitions of tuple similarities. Consider the numerical feature h in Figure 4, whose similarit y vector V h is shown in Fig-ure 9. For simplicit y we assume that t 1 ;:::;t N are sorted according to h .
As mentioned above, it is expensiv e to directly compute the inner product of two N 2 dimensional vectors V h and V Again we try to convert this problem into a problem that can be solved in linear time. From Section 4.1.2 one can see that only tuples whose values di X er by at most  X  (standard deviation) have non-zero similarit y with each other. When we scan tuples in order of h , only a subset of tuples need to be considered when scanning each tuple. The main idea of our algorithm is to decomp ose V h  X  V f into two parts, so that one part only depends on each tuple, and the other part contains some statistics of the previously scanned tuples, which can be main tained incremen tally. In this way V h  X  V can be computed by one scan.

Because of the symmetric de X nition of tuple similarit y, the minim um index j so that h ( t i )  X  h ( t j )  X   X  . As i in-creases when scanning tuples,  X  ( i ) either increases or stays the same. Thus we can  X nd all tuples with h ( t i )  X  h ( t by main taining two pointers on t i and t  X  ( i ) . We have which can be e X cien tly computed with Lemma 2.

Lemma 2. V h  X  V f can be compute d as follows
To compute V h  X  V f , we scan all tuples in order of h . When scanning each tuple t i , we get f ( t i ) :p k  X  (1  X  h ( t for each value v k of f . We also dynamically main tain the In this way we can compute the inner product according to Lemma 2. One can see that V h  X  V f can be computed by one scan on the tuples, also with very limited extra space.
The similarit y between two numerical features h and g is computed in a di X eren t way. Supp ose tuples are sorted according to their values on h . According to De X nition 3, when scanning tuple t  X  , only tuples t with j h ( t )  X  h ( t  X  a small subset of tuples. Therefore, we compute V h  X  V g the following way. Figure 10: Computing similarit y between h and g
As shown in Figure 10, we scan tuples in order of h . A searc h tree is dynamically main tained, which contains the indices of all tuples t with h ( t  X  )  X  h ( t ) &lt;  X  h indexed by tuple values on g . When scanning a tuple t  X  , it is inserted into the tree, and all tuples t with h ( t  X  )  X  h ( t ) &lt;  X  ; j g ( t )  X  g ( t  X  ) j &lt;  X  g can be easily found in the tree by an indexed searc h. V h  X  V g can be updated with the values of these tuples according to De X nitions 3 and 4. Because the  X  is usually quite small, V h  X  V g can be e X cien tly computed by one scan on the tuples.

Although similarit y between two features can usually be computed in linear time, it is still expensiv e to compute similarities between many pairs of features when the num-ber of target tuples is large. We observ e that sim ( f;g ) is actually the cosine of the angle between two vectors V f and V g , which lie in the Euclidean space. Thus we can utilize triangular inequality to further impro ve e X ciency .
According to the law of cosines, the distance between V f and V g can be computed by
For any three features f , g and h , since V f , V g and V are vectors in Euclidean space, according to triangular in-equalit y,
With triangular inequalit y, before computing the similar-ity between features f and g , a range can be determined for sim ( f;g ) using their similarities to other features. This helps save some computation in searc hing for features simi-lar to a certain feature.
When searc hing for pertinen t features, CrossClus needs to generate the values of target tuples on many di X eren t fea-tures. It is often very expensiv e to join all relations along the join path of a feature. Thus we need an e X cien t approac h for generating feature values. CrossClus uses a technique called Tuple ID Propagation [20], which propagates the IDs of target tuples to other relations, in order to  X nd tuples joinable with each target tuple.

Tuple ID propagation is a way to virtually join tuples in target relation R t with tuples in another relation R k via a certain join path p = R t ./ R 1 ./  X  X  X  ./ R k . Its main idea is to propagate the IDs of target tuples from R t to the relations along the join path one by one, so that for each relation R on the join path, each tuple t 0 of R i is associated with the IDs of all target tuples joinable with t 0 . After propagating IDs along a join path, the IDs are stored on each relation on the path and can be further propagated to other relations. Supp ose a feature f is de X ned by a join path p = R t ./ R 1 ./  X  X  X  ./ R k , and an attribute R k :A . To generate the values of each target tuple on f , we  X rst propagate IDs along path p . (If we have already propagated IDs along a pre X x of p , those IDs can be used.) The IDs associated with every tuple t 0 in R k represen t target tuples joinable with t this information we can easily compute the tuples in R k joinable with each target tuple, then compute the prop ortion of tuples having each value on R k :A among them, or any aggregation of these tuples. In this way the value on f for each target tuple can be computed.
Given a clustering task with user guidance, CrossClus searc hes for pertinen t features across multiple relations. There are two major challenges in feature searc h. (1) The numb er of possible features is huge in multi-r elational envir onment. The target relation R t can usually join with each relation R via many di X eren t join paths, and each attribute in R can be used as a feature. It is impossible to perform any kind of exhaustiv e searc h in this huge feature space. (2) Among the huge numb er of features, some are pertinent to the user query ( e.g. , a student's advisor is related to her research area), while many others are irrelevant ( e.g. , a stu-dent's classmates' personal information). It is a challenging task to identify pertinen t features while avoiding aimless searc h in irrelev ant regions in the feature space. To overcome the above challenges, CrossClus must con- X ne the searc h process in promising directions. It adopts a heuristic approac h, which starts searc h from the user-speci X ed feature, and then repeatedly searc hes for useful features in the neigh borhood of existing features. In this way it gradually expands the searc h scope to related rela-tions, but will not go deep in random directions.

The large number of possible features cover di X eren t as-pects of information. For example, some features are about a studen t's researc h area, such as her researc h group or confer-ences of publications. Some others are about her academic performance, such as her grade or publications. Our exper-imen ts show that features in the same aspect usually have high similarities between each other, while features in di X er-ent aspects usually have low similarities. A user is usually interested in clustering tuples based on a certain aspect of information, and indicates this by a query . An initial feature is created for the query , and more pertinen t features can be found by computing similarities between di X eren t features in di X eren t relations.

We also consider three prop erties of a feature f : (a) fan out : the average number of tuples joinable with each target tuple via f:joinpath , (b) coverage : prop ortion of target tu-ples that have values on f , and (c) the length of f:joinpath . A feature is not considered if it has too high fan out, too low coverage, or too long join path. Example 2. Supp ose a user wants to cluster studen ts ac-cording to researc h areas, and thus query with \ CLUSTER Student WITH Gr oup:ar ea " .

To create the initial feature for this query , CrossClus searc hes for the shortest join path from the target relation Student to relation Group , and creates a feature f using this path. If f is not quali X ed ( e.g. , f has too low coverage), CrossClus searc hes for the next shortest join path, and so on. As in Figure 1, an initial pertinen t feature [ Student ./ W orkIn ./ Gr oup , area , null] is created for this query .
Then CrossClus searc hes for pertinen t features among multiple relations. It considers the relational schema as a graph, with relations being nodes and joins being edges. CrossClus starts from the node of the initial feature, and explores the surrounding nodes of pertinen t features. It keeps adding in new pertinen t features, and expanding the searc h scope to all neigh bor nodes of those containing per-tinen t features. In this way it avoids aimless searc h in the huge feature space by con X ning the searc h in promising di-rections. On the other hand, this strategy allows it to grad-ually expand the searc h scope to many relations, so that most pertinen t features can be found and su X cien t informa-tion can be provided for clustering target tuples.
We simulate the procedure of feature searc hing, as shown in Figure 11. At  X rst CrossClus considers features in the following relations: Advise , Publish , Register , WorkIn , and Group . Supp ose the best feature [ Student ./ Adv ise , prof essor , null] (advisor of a studen t), which brings Professor relation into consideration in further searc h. CrossClus will searc h for more pertinen t features, until most tuples are su X cien tly covered.

Each feature is given a weight between 0 and 1, which is determined by its relationship to other pertinen t features. The weight of the initial pertinen t feature is 1. For each newly added feature f , f should have high weight if it is very similar to some highly pertinen t features, and vice versa. Therefore, f 's weight is based on the L features that f is most similar to. Supp ose f is most similar to f 0 1 ;:::;f
Figure 12 shows the algorithm of feature searc h. At  X rst the initial feature is the only pertinen t feature. At each step, CrossClus gets all candidate features and evaluates each candidate feature f c by the L pertinen t features most simi-lar to f c . Triangular inequality is used to infer the ranges of similarities between f c and some pertinen t features, which may exclude the possibilit y that f c is similar to some fea-tures. The candidate feature with the highest weight, f  X  added to the set of pertinen t features at each step, and new candidate features related to f  X  c are then added. We say a tuple is covered by a feature if it has non-empt y value on this feature. The algorithm stops when most target tuples have been covered by at least H features.

A set of pertinen t features are generated by feature searc h, but some of them may contain redundan t information. Cross-Clus further selects a set of non-redundan t features, so that the similarit y between any two features is no greater than sim max . This can be easily done because similarit y between each pair of pertinen t features has been computed.
Given a group of features f 1 ;:::;f h , CrossClus groups the target tuples into clusters that contain tuples similar to each other. We will  X rst introduce the similarit y measure between tuples, then introduce the clustering algorithm.
The similarit y between tuples w.r.t. each feature has been de X ned in Section 4.1.1 and 4.1.2, and such similarit y has been used for selecting features for clustering. The same de X nition is used for measuring tuple similarit y in clustering.
Consider two tuples t 1 and t 2 , which have values on h features f 1 ;:::;f h . Each feature f i has weight f i :weight . The similarit y between t 1 and t 2 is de X ned as the weighted sum of their similarit y on each feature. With the potentially large number of target tuples, an ef- X cien t and scalable clustering algorithm should be chosen. Since the features do not form a Euclidean space, we choose k -medoids [11, 19], a most widely used clustering algorithm that only requires distance measure between tuples. We choose Clarans [19], an e X cien t k -medoids algorithm for CrossClus . The main idea of Clarans is to consider the whole space of all possible clusterings as a graph, and use randomized searc h to  X nd good clustering in this graph. It starts with k initial medoids and constructs k clusters. In each step an existing medoid is replaced by a new medoid that is randomly picked. If the replacemen t leads to bet-ter clustering, the new medoid is kept. This procedure is repeated until the clusters remain stable.

Finally , CrossClus provides the clustering results to the user, together with information about each feature. From the features' join paths, attributes, and aggregation opera-tors, the user will know the meaning of this clustering, which helps her understand the clustering results.
We report comprehensiv e experimen ts on both synthetic and real databases. All tests were done on a 2.4GHz Pentium-4 PC with 1GB memory , running Windo ws XP. The follow-ing parameters are used in CrossClus : H = 10, L = 3 and " = 0 : 05 in Algorithm 1. sim max = 0 : 95. k = 20 in k -medoids algorithm. We compare CrossClus with three other approac hes: Baseline , Proclus , and RDBC .
Baseline clusters data by the initial feature only, and is very e X cien t because no feature searc h is performed.
Proclus [2] is the state-of-the-art subspace clustering ap-proac h that works on single tables. To apply Proclus , we  X rst convert relational data into single tables. For each clustering query , we perform a breadth- X rst searc h of depth three from the initial feature in the schema graph, and use every attribute in every relation as a feature (except features with too high fan out or too low coverage). This generates a table with 50 to 100 features for each query .

RDBC [14, 15] is a recen t clustering approac h for  X rst-order represen tations. It uses the similarit y measure in [5] that considers the related objects when computing the sim-ilarit y between two objects. RDBC can utilize di X eren t clustering algorithms, and we choose the same Clarans al-gorithm [19] as in CrossClus . When measuring similarities between two objects, only directly related objects are used, because it becomes too slow with larger depth.

All four algorithms use the same philosoph y in clustering ( k -medoids). Proclus and RDBC are slightly modi X ed to utilize the user guidance. In Proclus it is enforced that the initial feature is included in the feature set of every cluster. In RDBC , the objects related to each target tuple t via the initial feature are added to the set of related objects of t .
In this experimen t we test whether CrossClus can pro-duce reasonable clusters with user queries. Two real databases are used. The  X rst is the CS Dept database 4 (as shown in Figure 1), which is collected from web sources of Dept. of CS, UIUC. It has 10 relations and 4505 tuples. All relations contain real data, except the Register relation which is ran-domly generated. The second is the Movie database from UCI repository , whose schema is shown in Figure 13. This database has 7 relations and 61660 tuples.
A few clustering tasks are chosen for each database, and a clustering query is given for each task. To test the accuracy of a clustering algorithm, we need a standar d clustering for each task, which represen ts the correct partitioning of the target tuples. The standard clustering for a clustering task is created by a standar d feature set , which is the right set of pertinen t features for this task. The following principle is used to identify the standard feature set: For a clustering task, the standar d feature set is the set of features that contain information directly related to the task . For example, if the task is to cluster studen ts according to researc h area, the standard feature set consists of studen t's researc h groups, group areas, advisor, and conferences of publications. If the task is to cluster actors according to their eras, the standard feature set consists of the actor's birth year, death year, start and end year of career, and years of movies she participated. Indirectly related infor-mation, such as birth year of directors of movies she partic-ipated, is not considered.

The similarit y between two clusterings C and C 0 is mea-sured by how much the clusters in C and C 0 overlap with each other. Supp ose C has n clusters c 1 ;:::;c n , and C n 0 clusters c 0 1 ;:::;c 0 n 0 . The degree of C subsuming C de X ned as deg ( C  X  C 0 ) represen ts the average prop ortion that each cluster in C is contained in some cluster in C 0 . The similarit y between C and C 0 is de X ned as the average degree that one of them subsumes another. sim ( C;C 0 ) = 1 if and only if C and C 0 are identical.
For each clustering task, the standard clustering is gener-ated by applying the k -medoids algorithm on the standard feature set. The four algorithms are tested on this task. The clustering result of each algorithm is compared with the stan-dard clustering, and their similarity is used as the accuracy of this algorithm. Each experimen t is done 10 times and the average is reported.
 Task 1 : clustering professors according to researc h areas in CS Dept database. The query is \ CLUSTER P rof essor WITH Gr oup:ar ea " . The standard feature set is The feature set selected by CrossClus contains  X ve fea-tures, including the  X rst three standard features, plus [ P rof essor Cour se ./ OpenC our se , instr uctor , null] . The accuracies and runtime (in seconds) are shown in Table 1.
 There are only a small number of distinct values for Group:ar ea . CrossClus successfully identi X es clusters of professors work-ing in di X eren t areas, some of which are not speci X ed in Group:ar ea : and other clusters such as operating system, programming language, etc.
 Task 2 : clustering studen ts according to researc h areas in CS Dept database. The query is \ CLUSTER Student WITH Gr oup:ar ea " . The standard feature set is The feature set selected by CrossClus contains the  X rst three of the standard features. The fourth feature is not selected because of its high cardinalit y. Again CrossClus successfully identi X es clusters of studen ts in di X eren t areas. Task 3 : clustering directors according to the genres and styles of their movies in Movies database. The query is \ CLUSTER Director s WITH M ovies:categ ory " . The movies' styles are related to their categories (genres), coun tries, studios, and award information. The standard feature set is The feature set selected by CrossClus includes all above features, plus Movies.ye ar and Movies.pr ocess (b&amp;w, color, silent, etc). The results are shown in Table 3.

In the clusters generated by CrossClus , we easily  X nd some directors of famous movies, as follows.
 and other clusters such as romance, thrillers and musicals. Task 4 : clustering actors according to their eras in Movies database. The query is \ CLUSTER Actor s WITH M ovies:mov iey ear " The standard feature set is The feature set selected by CrossClus contains feature 1,2,5, and another feature [ Actor s ./ Casts ./ M ovies ./ Director s , year end , average] , which also indicates an actor's era. Some standard features are missed because they are considered redundan t. The results are shown in Table 4. 5
From the above experimen ts one can see that CrossClus successfully  X nds features pertinen t to the clustering tasks, which shows the e X ectiv eness of our approac h for feature se-lection. CrossClus generates reasonable clustering results and achieves signi X can tly higher accuracy and e X ciency than the other three approac hes. It shows that user guidance re-ally plays an importan t role in clustering. We  X rst test the scalabilit y of CrossClus , Proclus , and RDBC w.r.t. the sizes of databases. We use TPC-H databases of raw data sizes from 5MB to 25MB. The following query is used \ CLUSTER Customer WITH Order s:total price " . For each database, the CrossClus algorithm selects the following four features for clustering: average total price of orders, priorit y of orders, mktsegmen ts of customers, and regions of customers. Please notice that CrossClus  X nds categorical pertinen t features although the initial feature is numerical. We test the running time of CrossClus , Proclus , and RDBC as shown in Figure 14 (a) (in log scale), and that of CrossClus is shown in Figure 14 (b). It can be seen that CrossClus and Proclus are linearly scalable w.r.t. database size, and CrossClus is substan tially more e X -cient. RDBC becomes una X ordable for large datasets. We also test scalabilit y w.r.t. number of relations in databases. We use the data generator used in [20], which randomly gen-erates relational schemas and  X lls data according to some rules. The expected number of tuples in each relation is to 1000. The results are shown in Figure 15. It can be seen that CrossClus is scalable w.r.t. the number of relations.
When a database cannot  X t in memory , random access on disk-residen t data will be expensiv e. Here we discuss so-lutions for di X eren t comp onen ts of CrossClus when data is disk-residen t. When generating values for a tuple on a feature (Section 4.2), tuple ID propagation can be done in a similar way as joining relations in a relational database. Actually , if the data is stored in a relational database, an SQL query can be used to propagate tuple IDs along a join path. When computing the similarit y between two features f and g (Section 4.1), only one sequen tial scan is needed. Moreo ver, the similarities between multiple pair of features can be computed simultaneously by one scan. If f is a nu-merical feature, we may need to sort tuples according to f . After sorting, we can use one sequen tial scan to com-pute similarities between f and all other existing features. In general, CrossClus can select pertinen t features with a small number of scans on the dataset. For clustering target tuples (Section 5.2), CrossClus uses Clarans [19], a scal-able clustering algorithm. Therefore, in general CrossClus is adaptable to disk-residen t data in large databases.
In this paper we prop ose CrossClus , an e X cien t and ef-fectiv e approac h for cross-relational clustering. Unlik e previ-ous approac hes that work only on single tables, CrossClus works on multi-relational data, thus can be applied in many new  X elds. Because there exist numerous features with vari-ous seman tic meaning, CrossClus emplo ys a new concept: user-guide d clustering , which selects features and performs clustering based on user's guidance. We also prop ose a new similarit y measure for feature selection based on how they cluster tuples, which successfully identify pertinen t features in experimen ts. Our experimen ts show that CrossClus generates meaningful clusterings that matc h the users' ex-pectation and achieves high e X ciency and scalabilit y. We believe CrossClus represen ts a promising direction for clus-tering high-dimensional data in multiple relations.
