 Given the vast amount of information on the web, users have to use search engines to locate the useful web pages that are relevant to their interests and inquiries. The goal higher ranks for pages that are most important and relevant to the users X  query. There-fore, search engines need to distinguish the normal web pages from Spam pages so as to prevent misleading of the users [1]. measures for determining the relevancy of a page and a query. To measure the impor-Rank [2] that often are calculated from web link structure [3]. While these two impor-Search Engine Optimizers has develope d (SEO) recently. Malaga and Ross [4] guidelines by search engines and Web Spams (also known as Black hat SEOs) that violate the rules and transgress accepted norms. Web Spamming means boosting the rates of web pages undeservedly, without improving the true value of a page. 
The web Spamming methods cause crucial problems for search engines, e.g., they tremendously waste the resources for indexing illegitimate web pages, unduly de-crease the quality of retrieval process, damage search engines [5]. 
According to [3] Web Spam techniques ca n be categorized to the following types:  X 
Content Spam: If Spammers target textual similarity measures it is a content spam generation method. The content of pages is filled by popular words so that they are relevant to more popular users X  queries. In [6] the term  X  X eyword-stuffing X  is used to refer to this method.  X 
Link Spam: There is a general belief that pages with more incoming links are more popular and important than others. As mentioned, Search engines use some link-based measures like Page Rank to assess importance of a web page. Spamming methods that intend to influence these algorithms are named Link Spam. Spam-mers create so many pages that link to the target page to increase its popularity. Extensive researches have been presented to reduce the impact of Web Spam. Most of the proposed solutions such as [6, 7] considered Web Spam detection as a classifica-extracted from content of pages within the host and links among them. Previous expe-riences show that in Web Spam Detection, instances are not independent and data labels are unbalanced [8]. In this paper we present a new approach to handle biasness time that Hidden Markov Model (HMM) is used to do this. The proposed system starts by building a classifier based on Aggregating One Dependence Estimators (AODE) [9]. Hidden Markov Model helps us to consider the dependency of web hosts during the prediction process and boost the performance of AODE. A simple method to adopt HMM for this task is to find the most frequent sequences of visiting hosts. In quired sequences of hosts. 
The paper is organized as follows. In Section 2, we provide an overview of pre-apply HMM on these sequences. Finally, we conclude by summarizing key principals of our approach. Several automatic techniques for web spam detection have been presented in the lite-rature. Fetterly et al. [10] justified the statistical difference between machine generat-ed spam pages and normal web pages. They presented some features based on page also proposed several content based features and a decision tree, classifying Spam and Normal pages. Piskorski et al [11] studied on some linguistic features and discovered several discriminative features that are available publicly for others. Moreover in [12] features and language model based ones. They observed the semantic relation be-classification task. 
In addition to traditional learning models many papers used graph based methods to boost the performance of Web Spam Detection by considering topological depen-dency between the hosts. Link propagation as one the most popular methods in graph Their experiments show that link based metrics like TrustRank and Truncated Page-Rank can improve the performance of Web Spam classifiers. TrustRank separates an initial set of good pages. It starts with a seed of good pages and then follows the link nores the direct contribution of near nei ghbors according to a damping function. Ex-discriminative feature. Castillo et al [5] proposed stacked graphical learning for prop-agating labels across the web graph. In addition to content and link based features the average of probability of Spam for neighboring hosts is added to the feature vector of each host and thus is considered in decision process. 
Link refinement, elimination and regularization methods are other methodologies that exploit link structure to improve performance of basic classifiers. Elimination of Nepotistic links is one of the proposed method to reduce the impact of the link stuff-presented a graph regularization based algorithm, WITCH, that learns simultaneously from graph structure, content and link based features. 
There are also other works and experiences in this regard. For additional studies on the above mentioned topics, you can refer to the Survey on Web Spam Detection by Nikita Spirin and Jiawei Ha [8] , that is a good survey covering many papers and pro-posed systems to date. The following paper has tested and trained the proposed method by WebSpam-Uk2006 [18] which is a public Web Spam dataset. This collection contains 11402 hosts from the .uk domain. For each host 263 features have been extracted from links into three categories of Spam, Normal and Borderline. Here, we use the first two cat-egories to build a model that recognizes spam hosts from normal ones. 
In this research several classifiers such as decision trees, neural networks and sta-tistical classifiers were examined and compared against each other. We use F-measure results showed that AODE was superior to other competitive methods. AODE is a assumption for Na X ve Bayes. Thus it has less error rate and still is as fast as possible in training and test phases. In comparison with other methods like Bayesian networks, AODE benefits from the advantage of not performing model selection while it X  X  accu-works [9]. The proposal has taken advantage of AODE by using its implementation in Weka [19]. this paper. The paper approach is to use Wrapper Feature Selection method with which evaluates features by using a learning model [20] and chooses the most discri-minative and relevant features. After feature selection process, only 22 of 263 features feature subset has been found that results the best accuracy for AODE classifier. Ta-ble 1 shows the performance for different feature selection models and AODE clas-spam incorrectly". Tables 1 and 3-5 are result of cross validation with 10 folds. True positive rate 74% 77.2% 81.2% False positive rate 9% 10.4% 4.9% 
Reported results show that Wrapper feature selection improves the result by in-creasing true positive and reducing false positive ratio which has resulted in the selec-tion of AODE and the new feature space to setup the proposed system. In this Section, the research proposes a method to detect Web spam by using topolog-ical dependency between hosts. It started by representing the web spam detection as a graph based problem. In this presentation each host is a node in graph  X  X , X  X  X  X  X  of ber of links from host  X  to  X  . identically distributed. But in web spam problem, samples are topologically depen-dent [21, 22]. Therefore lots of latent information in the link structure between hosts are missed if we confine ourselves to use only the base classifier in the previous Sec-tion. The proposed system is based on smoothness assumption of semi-supervised share a label [23]. In web graph for each pair of nodes the paper declared a closeness factor: 
On the other hand Castillo et al [5] showed in their experiments that normal nodes tend to be linked by very few Spam nodes and they mainly link to other normal nodes study on WebSpam-Uk2006 dataset are shown. Probability of transition from a spam host. 
Considering the conditional dependency of Spam and Normal hosts to each other learning schema to build a pattern of dependency between nodes and handle imbal-ance between spam and normal labels. HMM is a probabilistic method to model sequence of data [24]. Indeed, to take HMM into use we need a sequence of connected hosts. This paper proposes Ant colo-ny optimization algorithm to extract sequence s of related hosts according to similarity measure (1). Fig. 1 presents the workflow of the proposed system. 4.1 Ant Colony Optimization In computer science, Ant Colony Optimization refers to a general propose method of solutions and exchange information by depositing pheromone on the ground in order to remark favorable path to the optimization problem [25]. To use ACO it is needed to represent the problem space as a graph and declare three fundamental components:  X  each step of walk. This mechanism uses a probability distribution based on heuris-tic function  X   X  X  X  and pheromone values  X   X  X  X  . The probability function is defined as: to host  X  that has not yet visited by ant  X  .  X 
Heuristic function: The heuristic function has been defined using the assumptions others.  X 
Pheromone update: According to [25] each artificial ant should update pheromone on edge  X  X , X  X  after each step of walk to communicate with other ants. These phe-romone's updates incrementally specify the best paths of connected hosts. 
This study combines offline and local pheromone updates [25] in one formula. Eq-posed equation, amount of pheromone on each edge decreases over time. Higher value of  X  gives a greater chance to other paths to be selected by edge selection me-nected hosts. 4.2 Hidden Markov Model dimensional feature vectors and AODE model that was presented in Section 3. Since AODE is a probabilistic model [9] that here predicts posterior class probabilities, this model is appropriate to be used as emission probability of HMM. 
In training phase, Baum Welch algorithm is run on generated sequences from ACO meters are recalculated in maximization step of Baum Welch except emission proba-bilities  X   X   X  X  X  X  that have already been estimated using an AODE classifier. extracts sequences of hosts that are linked to the  X  by ACO. Fig. 2 illustrates an ex-ample of host sequences with length three that are linked to the target web host. The hosts according to each sequence. follows:  X   X  X  X  X  X   X  X  X  X   X   X  X  X  X  X  X  X   X  X  X  X   X   X  X  X  X  X   X  X  X  X  X  X   X   X  X  X  X  X  X  X   X  X  X  X  X  X  and  X   X   X   X   X  X omal |  X   X : X   X  over all sequences are used. In Table 3, the performance of the new classification method is shown. 4.3 Multiple HMMs So far, we only use the output of ACO algorithm in the system, i.e., we only use the mones to better estimate the HMM parameters. We assume the pheromone values of each edge as a measure of conditional dependency between two hosts. Here, we introduce a technique for label smoothing by using multiple HMMs. experiments, we first use ACO with 100 artificial ants and then we extract sequences with length 2. Then a discretization with 10 equal depth bins is performed on phero-mone values. Afterwards, we train a Hidden Markov Model for each bin, so we have ten different HMMs. Prior probability of spam  X  X  X   X   X  X pam X  and transition proba-bility  X  X  X   X   X  X pam|Z  X   X  X pam X  for each bin are presented in Fig. 3. According to the reported parameters, the label of destination point is conditionally more dependent on the source point when there is more pheromone on the edge between them. Fur-thermore, it illustrates that probability of spam has an inverse relation with amount of pheromone. The result of the above experiment is convincing enough to make use of different HMM components to model relation between points with different dependency values (Pheromones). For sequences with length two implementation of such a system with higher we should present a technique that considers pheromones in edges in depth two we aim to use non-parametric models in HMM, we need to discretize the edge values so as to decrease the amount of sparsity. 
In this paper two approaches were examined for sequences with length three. In the pheromones of the edges. Afterwards the binning was performed on these weights. In the second approach, the binning is applied two times. First on the edges connected to the target host, and second time on the edges connected to the neighbors of the target table of 100 HMM component was created. Each sequence was assigned to one of the HMM components according to the amount of pheromone on their edges. For exam-ple if the first edge of sequences belonged to bin 3 and second edge belonged to bin 6 the system assigned the sequence to the HMM number 18 in row 3 and column 6 of the pheromone table.  X   X   X   X   X  X omal |  X   X : X   X  should be computed accordi ng to the appropriate HMM com-ponent. Table 4 shows the result of the experiments. 
As you can see in Table 4, the proposed system achieves an improvement by using several HMM components on sequences with length 2. The performance of system but then is reduced when first approach was used to model sequences with length 3. However it is raised again when the second approach is used. 
According to our experiments the general trend of using this method is a consider-able increase of 10 percent in detection rate of baseline classifier while F-measure has also improved from 0.825 to 0.859. Next Section is a comparison of the result of this study and other existing methods to show to what extent the application of HMMs is contributed to the improvement for Web spam detection. exploit dependency patterns between data points. This study proposed a system to detect web spam according to the content and link based features and dependency between points in web host graph. To our knowledge this is the first attempt to boost the performance of web spam detection using Hidden Markov Models. Table 5 shows a comparison between the presented method and other systems according to the F-Geng et al in [26] boosted the performance of classification task using under sampling method and reached F-measure 0.759. Castillo et al [5] as one of the most significant learning. Bencz X r et al [27] reported F-measure 0.738 following the same methodolo-gy as Castillo et al [5]. 
To compare the performance of the proposed system with the results of the partici-the test set provided by the organization. 
One disadvantage of the proposal system is the number of the needed HMMs in the second approach. For instance using the second approach needs to create 1000 HMM mate parameters of these HMMs. In near future, we plan to propose a HMM with parametric transition probabilities that can handle the weights of the edges. Moreover we intend to employ a new content based feature using language modeling tech-niques. Based on the ongoing researches and studies on the topic we strongly believe that it is possible to achieve better performance using these new features. 
