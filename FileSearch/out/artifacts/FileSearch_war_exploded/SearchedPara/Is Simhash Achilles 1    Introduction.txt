 Simhash [1], just like Achilles the Greek hero of the Trojan War, is the hero to solve the large-scale similarity search problem such as text retrieval [2], duplicate detection [3], etc. Simhash hashes similar objects to similar hash values such that searching over the obtained hash values is much more efficient [2,3]. A natural question is whether Simhash can be applied to any similarity search problems? Obviously, the answer is NO. In this paper, we will give a quantitative analysis on this problem. We will also give one feasible method for the case that the basic Simhash is infeasible.
The basic Simhash [1] randomly generates some hyperplanes and objects X  hash val-ues are determined by the sides of these planes that they locate on. Just like the heel is the deadly weakness of Achilles in spite of his overall strength, Simhash loses efficiency when the data is ill-distributed. This is mainly because when data are ill-distributed, a lot of points tend to lie on the same side of hyperplanes thus are hashed to the same hash value. However, we can handle this problem by incorporating both similarity between two objects and the location of the whole dataset.

Motivated by above analysis, we propose a novel analysis method of data distribu-tion based on Support Vector Data Description (SVDD)[4]. It involves two steps: (1) using SVDD to find a tighten sphere (called SVDD sphere) to cover most of the data points, and (2) calculating the intersection circle between the unit sphere and the ob-tained SVDD sphere. Then, we can quantitatively measure whether the basic Simhash is suitable or not. For those unfeasible cases, this paper proposes two methods, model-and feature-based Simhash. Specifically, m odel-based Simhash tries to generate some random hyperplanes crossing through the data area, which makes the obtained hyper-planes be more discriminative. An approximation of this method is designed for large-scale data, which is very simple for implemen tation. Differently, feature-based Simhash alters the data representation so that the data points can be well distributed thus the basic Simhash can be directly used.

We evaluate our method on a synthetic dataset and a real-world image dataset. We also design some evaluation criterions to quantitatively compare our method with the basic Simhash. Most results show that our method outperforms the basic Simhash. This paper is organized as follows: Section 2 presents our spacial analysis method. In Section 3, we present our spacial analys is based Simhash algorithm in details. Ex-perimental results and some analysis are shown in Section 4. Section 5 reviews some related works. In Section 6, we make a conclusion. To begin, we fix some notation. We assume each data point is described by a column vector x  X  R K . Since Simhash measures the cosine similarity between two points, we normalize each data point such that all the points are located on a unit sphere, i.e. x =1 .Wehaveadataset X = { x i } , i =1 ,...,N . 2.1 Support Vector Data Description Support Vector Data Description (SVDD) [4] c oncerns the characterization of a data set but includes almost no superfluous space. Inspired by the Support Vector Machine (SVM), SVDD describes a data set in terms of a sphere which tries to enclose all the data. The sphere is characterized by the center a and the radius R&gt; 0 . SVDD obtains the optimal description via solving the following optimization problem: where  X  i is the slack variable and control parameter C is set to be a constant value in pervious which controls the trade-of between the volume and the errors.

The intuitive interpretation of above problem is that (1) we hope that all the data points can be involved in a tightened sphere, and (2) by introducing the slack variables, SVDD can tolerate the outliers in the data set.
 2.2 Accurate Spatial Data Analysis Given a data set, we can obtain a SVDD sphere as described in Section 2.1. The relation between the unit sphere and the SVDD sphere is illustrated in Figure 1(a). Since all the points are located on the unit sphere, the volume of the SVDD sphere is no larger than the unit sphere. Obviously, such two spheres must intersect and the intersected region is the location of the given data. For description simplicity, as shown in Figure 1(b), we assume our data is 2 dimensional and the center of the unit sphere o =(0 , 0) .Giventhe center a and the radius R of the SVDD sphere 1 , it is convenient to obtain the radius r of the intersection circle by solving the following equation: and the center of the intersection circle is Therefore, the intersection circle is: and the cone vertex angle is 2  X  =2arcsin( r ) .

In short, we can summarize one guideline: the larger the distance from the center of the intersection circle to the center of the unit sphere (the distance equals to c 2 ), the worse the data distribution is. In this section, we firstly briefly introduc e Simhash and figure out the Achilles hill of it. Then, based on the analysis in Section 2, we will describe our approach. 3.1 Simhash Charikar X  X  Simhash [1] is an effective met hod for similarity search. A notable imple-mentation is random hyperplane based Simhash [1,3]. Specifically, it randomly gener-ates some hyperplanes and hash objects accord ing to the sides of these hyperplanes that they lie on. Given an object x and a random hyperplane r = { r 1 ,r 2 ,...,r n } , the hash function can be specified as: The Achilles X  X  Heel of Simhash. The simplicity and effectiv eness of simhash leads to its great success in many applications in we b search and data mining area. However, just like the heel is the deadly weakness of Achilles in spite of his overall strength, a shortcoming of Simhash makes it inefficien t and even infeasible in some cases espe-cially when the data is seriously distributed. For example, as described in Figure 1(a), if most of the points are located in a small region of the unit sphere, Simhash is very inefficient. In fact, Simhash would generate very similar fingerprints for most of points such that further processing is quite time consuming. In short, Simhash prefers the well-distributed data. 3.2 SDA-Simhash In this section, we will present our improved Simhash algorithm which is based on the spacial analysis. Inspired by the guideline described in Section 2.2, we hope to:  X  Make the random hyperplanes cross through the intersection circle;  X  Minimize the distance from the center of the intersection circle to the center of the In brief, we have two ways to extend the basic Simhash. One (called model-based Simhash ) is to generate the random hyperplanes with the constraint that they should cross through the intersection circle. Another (called feature-based simhash )istoal-ter the original representation such that the distance from the center of the intersection circle to the center of the unit sphere, i.e., c 2 , is minimum.
 Model-Based Simhash. Model-based Simhash makes the hyperplanes cross through the intersection circle. This is carried out by adding a constraint to the generation pro-cess. Specifically, after the spacial an alysis, we can obtain a SVDD sphere ( a ,R ), an intersection circle ( c ,r ) and the cone vertex angle 2  X  . We hope to choose a random hyperplane that crosses through the intersection circle. Some feasible methods include coordinate-by-coordinate strategy [5] and a simple rejection method. Details are omit-ted here for the space limitation.
 Approximated Model-based Simhash. Although the model-based Simhash described above can follow the guideline, the rejection method is inefficient for high-dimensional data. This part presents an approximation version of the model-based Simhash for high-dimensional data.

For description simplicity, we assume there do not exist outliers, i.e. the parameter C in Equation 1 is set to a very large value. As shown in Figure 2, some hyperplanes cross through the shell while others do not. There fore, we divide all the random hyperplanes into two groups:
We measures the percentage of the points lying on the two different sides of a hyper-plane r as follows: where I { X } equals to 1 when the condition is true otherwise 0, and |X| the size of the On the other hand, if the numbers of points lying on the two sides of a hyperplane are equal,  X ( r , X )=0 . Obviously, we can find that  X ( r , X ) &gt; 0 ,  X  r  X  X  + , while  X ( r , X )=0 ,  X  r  X  X   X  .

Furthermore, we assume all the points in the data set X are normally distributed on the shell (determined by a SVDD sphere). Then, we observe that if the  X (  X  ) value of a hyperplane equals to 0, such a hyperplane splits the shell into two parts of equal size. The smaller the  X (  X  ) value is, the closer the hyperplane is to the boundary of the shell. This offers a good inspiration for speed up the model-based Simhash.

If we choose a candidate set of hyperplanes uniformly distributed on the unit sphere with size N , sort this set of hyperplanes in terms of the  X (  X  ) values and select 2  X N  X  hyperplanes with smallest  X (  X  ) values, we can ensure to a certain extent that the se-lected hyperplanes should cross througth the shell. The size of the candidate set can be determined according to Theorem 1.
 Theorem 1. Given the cone vertex angle 2  X  , we need to generate N candidate random hyperplanes such that, with the confidence interval ( 2  X   X   X   X , 2  X   X  +  X  ) , X  &gt; 0 and the confidence level  X  , we can ensure that there are 2  X N  X  hyperplanes cross across the shell. The N can be determined by the following inequality: Bernoulli distributed random variables with parameters x and y respectively. Proof. For description simplicity, a random hyperplane r crossing through the shell is considered as an event A . The probability that A occurs ( A =1 )is p =Pr[ h ( x )  X  h ( y )] = 2 Pr[sign( r T x ) &gt; 0 , sign( r T y ) &lt; 0] = 2  X / X  . Assume random variables A ,A 2 ,...,A N  X  X  0 , 1 } are i.i.d. According to Chernoff-Hoeffding theorem, we have Furthermore, On the other hand, according to the definitions of confidence level and interval, we have Put it all together and finally we can get method is simple, so it is quite easy to implement. We should emphasize that, as the method shown in [5], the generation of hype rplanes normally distributed on the unit sphere is very efficient.
 Algorithm 1. Approximated Model-Based Simhash Feature-Based Simhash. In this part, we present an alternative way to follow the guideline. Different from the model-based Simhash, we directly convert the data rep-resentation. As we know, if a K -dimensional variable x follows the standard normal distribution N (0 , I) , x / x 2 is normally distributed on the unit sphere. This implies that if we can convert a given distribution t o the standard normal, we can re-locate the data uniformly on the unit sphere. For example, if a data set X is normally distributed and the features are independent, we can standardizing it via scaling transform and translation transform. Specifically, we firstly calculate the mean E j and the variance V j of the j -th component. Secondly, we st andardize each component of x as follows: Then, we normalize each object x such that its L2-norm equals to 1, i.e. x 2 =1 . Finally, the basic Simhash is directly applied to process the normalized data. To reflect different properties of our SDA-Simhash, we evaluate our method on a toy case and a real-world data set. To begin, we will firstly describe the evaluation criterions and then show the experimental results. 4.1 Evaluation Criterions To demonstrate the effectiveness of our method, we compare SDA-Simhash with the basic Simhash.

Simhash involves two periods, pre-processing and query. In the pre-processing pe-riod, Simhash generates fingerprints for data points by concatenating the outputs of Equation 5 with M randomly generated hyperplanes { r 1 ,..., r M } . Then the algorithm permutes the fingerprints and sort them lexicographically to form T sorted orders. In the query period, we firstly perform an approximation search in M -dimensional Hamming space to generate a candidate set. The candidate set is defined as follows: where q is a query fingerprint. Then, we need to compute the actual similarity between the query object and objects in V . Finally, the items with the largest similarity are re-turned as results. Obviously, if the candidate set is very large, the further processing is quite inefficient. Therefore, the size of candidate set |V| is a good measurement of algorithms X  efficiency.

Alternatively, we can alter the data repres entation to relocate the data uniformly on the unit sphere such that the basic Simhash can be directly applied. As the analysis in Section 2, we calculate the distance from the intersection circle to the center of the unit sphere c 2 . 4.2 A Toy Case To better understand the properties of our method, we utilize a toy case to prove the cor-rectness of our method. We will generate some synthetic points in R 3 . The visualization of the results can help us intuitively understand our method.

We generate the synthetic data in two steps : (1) choosing an area (shell) on the 3-dimensional sphere, and (2) r andomly generating 200 points uniformly distributed on the shell. The synthetic data are represented as the red points in Figure 3(a). We can see that the data is ill-balanced distributed.

Figure 3(a)(b) present the influence of the control parameter C to our spatial data analysis method. As described in Section 2.1, we compute the SVDD sphere with dif-ferent values of C .When C is set to 0.5 and 0.01, the corresponding SVDD spheres are displayed in Figure 3(a)(b) respectively. It shows that the radius of the SVDD sphere decreases with the decrease C , which consists with the analysis in Section 2.1. The data after standardization is displayed in Figure 3(c). Obviously, the data is well distributed on the unit sphere thus the basic Simhash can be directly applied. 4.3 Real-World Data In this section, we evaluate our method on a real-world image data set. This image set is collected as follows: (1) crawling a large number of images from the web, (2) resizing the images such that all the images are in the same size, and (3) removing all the duplicated images. In short, we get an image set with 253,083 distinct images with the same size.
 Data Representation. We use two common features to represent each data, gray-scale histogram and Local Binary Pattern (LBP) [6]. Gray-scale histogram is a representation of the gray-scale distribution in the images. Given an image, we convert it to 256-level gray scale, statistic its gray-scale histogram and finally normalize the histogram such that the sum of all the components equals to 1. Local Binary Pattern feature has been widely used in various applications, such as image retrieval, texture classification, etc [6]. The most important property of LBP is its robustness to monotonic illumination changes and rotation invariance. In our experiments, we use 8-neighborhood. Therefore each image is represented as the histogram of 59 labels, which is a 59-dimensional vector. Please see [6] for more details.
 Experimental Results. Considering the dimension of images is relatively high, we utilize the approximated model-based Simhash. We compare our method with the basic Simhash [1] in terms of the size of candidate set |V| . We generate a 64-bit fingerprint ( M =64) for each image. To obtain the set V , we set the threshold k =1 when using LBP feature and k =3 when using gray-scale histogram feature. The reason why we use different thresholds is that we hope the intermediate set V to be large enough for comparison. In our experiments, each time, we randomly select 1,000 images from the data set as queries. Then we calculate the mean and the variance of |V| . To reduce the influence of the random errors, this process is repeated 10 times.
The results of approximated model-based Simhash are presented in Figure 4. It shows that for both gray-scale histogram and LBP, our algorithm can effectively de-crease E[ |V| ] and D[ |V| ] . This illustrates that in the further processing, our method is more efficient thus is more discriminative.

From Figure 4(a), we find when the size of candidate hyperplane set is large (640 here), the basic Simhash outperforms our method. This is mainly because of the hyper-plane selection process. Specifically, we select the random hyperplanes with the small-est  X (  X  ) . According to the definition of the  X (  X  ) value, random hyperplanes with smaller  X (  X  ) values tend to cross through the center of the shell. So when the N is very large, our method prefers to choosing those hyperplanes that just cross through the center of the shell, which makes the selected hyperplanes tend to be similar.

Figure 5 reports the performance of the feature-based Simhash. The left side of Fig-ure 5 shows our feature-based Simhash can effectively decrease E[ |V| ] . But unfortu-nately, as presented in the right side of the Figure 5, the feature-based Simhash totally fails when using gray-scale histogram feat ure. We examine the data set carefully and find that most images have a large area of white background and the original value dis-tribution is not normal or near normal. In this case, some other techniques should be considered to convert the given distribution to a standard normal.

A natural question is whether the combination of the model-based and the feature-based Simhash can improve the final search performance. Here, we combine such two models and test the combination on the imag e set. Specifically, we firstly process the original data by the standardization trans formation. Then, our approximated model-based Simhash is carried out.

The results are shown in Figure 6. Figure 6(b) shows that when using LBP feature, the combination significantly outperforms the basic Simhash. However, affected by the feature-based simhash, the combination of model-and feature-based Simhashs com-pletely fail (as presented in Figure 6(a)). As analysis above, Normal standardization in feature-based Simhash needs to be replaced by some other standard ization techniques.
Considering the result in Figure 6, we make a quantitative analysis of different al-gorithms when using LBP feature. The result is listed in Table 1. It shows that model-and feature-based Simhash are complemen tary and the combination can improve the performance.

Most results illustrate that our model-based Simhash can significantly decrease E[ |V| ] and D[ |V| ] , which demonstrates that our method is mo re effective, robust and stable than the basic Simhash. Many works have proposed on fast similarity search. For example, KD-tree [7] returns accurate results. But when the dimen sion of the feature space is high ( &gt; 10 ), it becomes quite inefficient and even slower than brute-force approach [8]. For high-dimensional data, a notable method is Locality Sensitive Hashing (LSH) [9]. Just like Simhash, LSH also performs a random linear projection to map similar objects to similar hash codes. However, in practice, LSH suffers from the efficiency problem, since LSH may tend to generate long codes [10]. Another notable work is Spectral Hashing [11], which has been proven significantly improvement on LSH. However, the assumption made by spectral hashing of input vectors with a known probability distribution restricts its application. Some other works involve reducing the storage of LSH/Simhash [12,13], kernelized Simhash [14] and so on. In this paper, we have presented a novel method for quantitatively measuring whether the basic Simhash can be directly applied to perform similarity search in a given data set. Our method is mainly based on the analysis on the location of the whole data set via Support Vector Data Description (SVDD). Based on the analysis, we propose two methods to improve the search performance of the basic Simhash algorithm. Most re-sults show that our method outperforms the basic Simhash significantly.
 Acknowledgments. We would like to thank the anonymous reviewers for their insight-ful comments. This work is supported by the National Natural Science Foundation of China under Grant No. 60873174.

