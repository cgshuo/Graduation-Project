 Abstract Several recent Information Extraction (IE) systems have been restricted to the identification facts which are described within a single sentence. It is not clear what effect this has on the difficulty of the extraction task or how the performance of systems which consider only single sentences should be compared with those which consider multiple sentences. This paper compares three IE evaluation corpora, from the Message Understanding Conferences, and finds that a significant proportion of the facts mentioned therein are not described within a single sentence. Therefore systems which are evaluated only on facts described within single sentences are being tested against a limited portion of the relevant information in the text and it is difficult to compare their performance with other systems. Further analysis dem-onstrates that anaphora resolution and world knowledge are required to combine information described across multiple sentences. This result has implications for the development and evaluation of IE systems.
 Keywords Information Extraction  X  Evaluation  X  Message understanding conferences 1 Introduction Information Extraction (IE) is the process of identifying specific pieces of infor-mation in text, for example, the movements of company executives or the victims of terrorist attacks. IE is a complex task; information may be spread across a document. Several sentences or paragraphs of a text may have to be examined to identify a fact. For example, the following two sentences describe a management succession event (i.e. a change in corporate management personnel): Mark Stevenson  X 
Pace American Group Inc. said it notified two top executives it intends to dismiss them because an internal investigation found evidence of  X  X  X elf-dealing X  X  and  X  X  X ndisclosed financial relationships. X  X  The executives are Don H.
Pace, cofounder, president and chief executive officer; and Greg S. Kaplan, senior vice president and chief financial officer.
 The fact that the executives are leaving and the name of the organisation are listed in the first sentence while the names of the executives and their posts are listed in the second sentence. The succession events can only be fully understood from a com-bination of the information contained in both sentences.

Combining the required information from multiple sentences is not a simple task executives X  X  and  X  X  X he executives X  X  in the above example. Additional difficulties occur because the same entity may be referred to in different ways. For example,  X  X  X nternational Business Machines Ltd. X  X  may be referred to by an abbreviation ( X  X  X BM X  X ), nickname ( X  X  X ig Blue X  X ) or an anaphoric expression such as  X  X  X t X  X  or  X  X  X he company X  X . These complications make it difficult to identify the correspondences between different portions of the text describing this event.

Traditionally IE systems have consisted of several components with some anal-ysing each sentence and others being responsible for combining the information discovered (Grishman, 2003 ). These systems were often designed for a specific extraction task and could only be modified by experts. In an effort to overcome this brittleness machine learning methods have been applied to port systems to new domains and extraction tasks with minimal manual intervention. However, many of these systems consider each sentence in isolation and only extract facts which are described within a single sentence, examples include Chieu and Ng ( 2002 ), Culotta and Sorensen ( 2004 ), Sekine ( 2006 ), Soderland ( 1999 ), Stevenson ( 2005 ), Yangarber, Grishman, Tapanainen and Huttunen ( 2000 ), Zelenko, Aone and Richerdella ( 2003 ). For the remainder of this paper we shall refer to these systems as Single Sentence Approaches (SSA). Conversely, IE systems that have the ability to identify information described in more than one sentence are Multiple Sentence Approaches (MSA).

The development of SSA systems is now a well established methodology in IE research. However, since SSA systems analyse each sentence in isolation and do not attempt to combine items from different sentences, they are limited to identifying information described within a single sentence but are unable to recognise facts expressed across multiple sentences. In the above example these systems could employed by Pace America Group. Of course, this relation could be identified using techniques for combining information across sentences but this is rarely applied; none of the cited examples of SSA systems use anaphora resolution to help identify relations between items mentioned in different sentences.

A possible reason for developing SSA systems may be the assumption that the majority of facts described in the text are expressed within a single sentence and there is little to be gained from the extra processing required to combine informa-tion. In fact, SSA systems only report results across the facts they consider i.e. those expressed within a single sentence. Conversely MSA systems consider a wider set of facts and report results across those. The facts considered by SSA systems are a subset of those examined by MSA systems but the proportion is not known, making it difficult to compare their performance.

This paper describes an analysis of three corpora commonly used to evaluate IE systems which demonstrates that a significant proportion (up to 60%) of the facts in those documents cannot be identified by SSA systems. The fact that documents contains facts described across multiple sentences is not surprising in itself but it might not be expected that such a large proportion fall into this category. This result demonstrates that SSA systems do not properly consider a large proportion of facts in text and this has implications for the evaluation and development of IE systems. Results of SSA systems which have been previously reported cannot be considered to be directly comparable with those for MSA approaches and should be reinter-preted. In addition, IE system designers who use SSA techniques cannot expect their systems to identify all facts within texts and effort must be spent on the development of techniques for extracting facts described across multiple sentences.

The remainder of this paper is organised as follows. Section 2 describes data from the Message Understanding Conferences and Sect. 3 the process that is applied to it to determine the proportion of facts they contain which are described in a single sentence. Section 4 describes the results of this analysis. Section 5 discusses the ways in which facts described across multiple sentences could be identified and describes an experiment which estimates the amount of additional facts which could be found if anaphora resolution was applied. Section 6 summarises related work while Sect. 7 discusses the implications which can be drawn from this analysis. 2 MUC templates The data used for the experiments described in this paper are taken from various Message Understanding Conferences (MUCs). These were a series of seven con-ferences run between 1987 and 1998 which were intended to evaluate the accuracy of IE systems. The evaluation regime gradually evolved over the course of the con-ference series but always followed the same general format. An IE task was defined and participants provided with sample documents describing information pertinent to the task along with completed templates demonstrating what should be extracted from them. Participants are given a period of time to develop their systems to carry out the extraction task. At the end of this time each system is evaluated by running it over the evaluation documents and its results compared against manually completed templates which the participants do not have access to. A conference is then held to discuss the results and their implications.
 The experiments described here make use of the evaluation data from three of the MUC conferences: (1) MUC4, for which the extraction task was concerned with reports of terrorist incidents in Latin America, (2) MUC6, dealt with management succession events and (3) MUC7, rocket launches. These corpora are commonly used to evaluate IE systems. The aim of these evaluation was to develop systems which could fill answer templates with information extracted from text. The tem-plates consisted of three basic elements: String Slots which are filled using strings extracted directly from the text; Text Conversion Slots and Set Fill Slots which contain values that have to be inferred for the document. Figure 1 shows a filled template from the MUC4 evaluation. Slots 9 and 10 are examples of string slots. These often list alternative expressions which refer to the same entity, such as  X  X  X ARABUNDO MARTI NATIONAL LIBERATION FRONT X  X  and  X  X  X MLN X  X  for slot 10. Slots 4 and 5 are text conversion slots which are completed using a set of pre-defined values for each slot. Slots 14 and 21 are set fill slots. These are completed by deriving the number of items falling into a particular class and enumerating that list. After MUC4 a more complex nested template structure was adopted for sub-sequent evaluations. This new structure effectively retained the use of three basic slot types. 3 Fact matching Our goal is to identify the proportion of facts in the MUC corpora which are de-scribed within a sentence and can therefore be extracted by a SSA IE system. One way to estimate this is to examine the template X  X  string slots, which are taken directly from the text, and compute the proportion of events for which all of the string slots occur within the same sentence. The rationale behind this approach is that, since each sentence is examined separately, items much occur together for these ap-proaches to identify the connection between them. This procedure will provide an upper bound on the number of facts which are described within one sentence; if the string slots cannot be found together in a sentence then that event must be described across multiple sentences but, on the other hand, if they do occur in the same sentence then that does not necessarily mean the event is described therein. This approach only considers string slots in the template and this is because it is straightforward to identify where they are mentioned in text but difficult to complete automatically for other fields.
 The matching process was applied to the evaluation corpora used for three of the MUC exercises as follows: the text was initially split into sentences. 1 The set of possible fillers were then extracted from the answer key templates and converted into a regular expression for pattern matching. 2 Each fact was then compared against each sentence in the document it was derived from and the sentence for which the most fields matched stored.

As part of this matching process the facts in each MUC template are transformed into a common representation which includes the most important information contained in the string slots and makes the process of comparing each fact against the text more straightforward. The common representation consists of a set of fields each of which has at least one associated filler. For example, the following event consists of three fields which have, respectively, three, two and one possible fillers, separated by  X  X  |  X  X :
PERP(SHINING PATH MEMBERS | SHINING PATH | PHYSTGT:ID(STATE ENERGY COMPANY | ENERGY COMPANY) INCIDENT:INSTRUMENTID(BOMB)
Matches between facts in this form and sentences are identified as belonging to one of three categories: Full , Partial or NoMatch . Each of these possibilities may be described as follows:
Full A fact fully matches a sentence if it mentions a filler for each field. For example, there is a Full match between the fact just shown and the following sentence since one of the possible fillers for each of the fields is mentioned in the
Partial A Partial match occurs when one of the fillers for at least two of the fields are mentioned. The following example demonstrates a Partial match, fillers for the
PERP and HUMTGT fields are mentioned but not the INCIDENT: INSTRUMENTID field.
 PERP(URBAN GUERRILLA COMMANDOS) HUMTGT(JUAN CARLOS MERIOS | EMPLOYEE | INCIDENT: INSTRUMENTID (BULLET)
NoMatch A fact does not match a sentence if the conditions for a Full or Partial match are not met. This occurs when there is no sentence in the corpus which contains more than one of the fact X  X  fields and, in other words, the fillers of the fields which make up the fact are spread across separate sentences in the text.
Although the fact X  X  fields will appear in the text (since they are string slots) they must appear in the same sentence to be identified by SSA systems. The fact used as an example for the Partial match category would not match this sentence,
To identify the proportion of facts which could be identified by an SSA system, each fact is compared against all the sentences in the corpus to identify the pro-portion which fully match at least one sentence. When this process has been com-pleted the remaining facts are once again compared against the corpus to discover which of those partially match at least one sentence. We now go on to describe the results of experiments in which this process is applied to corpora from the MUC evaluations. 4 Experiments A key decision in these experiments is the choice information in the templates to search for in the documents. The aim is to capture as much of the template X  X  salient information as possible. The matching process (Sect. 3 ) is limited to considering string slots from templates. Some templates include string slots which are generally not instantiated because the information does not occur in the text and these are ignored for simplicity. In addition, some of the string slots contain similar infor-mation. For example, the MUC6 templates contain two slots which list ways of referring to the same person ( PER_NAME and PER_ALIAS ). These slots are com-bined and their possible fillers concatenated.

It is worth noting that this process of selecting the key information within templates and combining together field X  X  values makes it more likely that the information will match the text (either fully or partially). Selectivity means that fewer pieces of information need to co-occur within a sentence while combining slots requires that only one of them need to occur within a sentence to count as a match.
The aim of this work is not to make any claim about what constitutes a fact. In the context of this work a  X  X  X act X  X  is considered to be any piece of information which can be identified within a text. The intention is to make use of the core information contained in a standard data set which is commonly used as a benchmark for IE systems. In two of these corpora (MUC4 and MUC7) the core information could be considered to be the description of events while in the MUC6 corpus the facts used for these experiments are more like descriptions of individuals. 4.1 Latin American terrorism The templates used for MUC4, such as the one shown in Fig. 1 , contained information describing terrorist incidents in Latin America (Sundheim, 1991 ). They included six string slots: (slot 6) INCIDENT: INSTRUMENT ID , the device used to carry out the act of terrorism; (9) PERP: INDIVIDUAL ID , person responsible for a terrorist incident; (10) PERP: ORGANIZATION ID , organisation responsible for a terrorist incident; (12) PYHS TGT: ID , any inanimate object that was the target of a terrorist act; (18) HUG TGT: NAME , any person who was the target, or became the victim of, an attack and (19) HUM TGT: DESCRIPTION , the title or role of a human target of a terrorist act or the general description of a unnamed human target. These slots contain some of the core information for each fact and were used to provide the fact definition for the experiment using the MUC4 corpus. Two pairs of slots contained similar information and were com-bined: slots 9 and 10 both describe the perpetrator of a terrorist act while slots 18 and 19 provide information about people who were the terrorist X  X  targets. Con-sequently for these experiments the information examined consists of four fields which contain information about the identity of the perpetrator of a terrorist act ( Perp ), the target which may be either human ( Humtgt ) or physical ( Phystgt ) and the instrument used, such as a bomb ( Instrument ). The template shown in Fig. 1 would be represented as follows: Instrument(BOMB) Perp(URBAN GUERRILLA GROUP | FMLN | Humtgt(ROBERTO GARCIA ALVARADO | ATTORNEY GENERAL)
Phystgt(ARMORED VEHICLE) 4.2 Management succession The MUC6 corpus concerns management succession events. For example the sentence  X  X  X aniel Glass was named president of EMI Records Group, a unit of London X  X  Thorn EMI PLC. X  X  describes an executive ( X  X  X aniel Glass X  X ) taking up a position ( X  X  X resident X  X ) within an organisation ( X  X  X MI Records Group X  X ). The core information in the MUC6 templates is stored in a sub-template which lists (1) the person who is moving, (2) the organisation they are joining/leaving, (3) their post (job title) and (4) whether they are joining or leaving the organisation. The last piece information were taken from the templates to form the facts for this corpus. 3 The fact shown above would be represented as follows: Person( X  X aniel Glass X  X  X  X lass X ) Org( X  X horn EMI PLC X  X  X  X MI X ) Post( X  X resident X ) Alternative field fillers are identified by concatenating together fields in the MUC6 template which list various descriptions for entities. 4.3 Rocket launches The MUC7 task concerned information about rocket launches described in newswire reports. An example sentence from this corpus containing information of interest is the following:  X  X  X n the early hours of Feb. 15, a new Chinese rocket took off from its launch pad in western Xinjiang province with a 205 million dollar satellite on board. X  X 
The core information in the MUC7 templates is stored in a sub-template which consists of fields containing details about the space vehicle, where it was launched and the payload being carried. A number of slots were concatenated to identify three keys pieces of information from the templates: VEHICLE , LAUNCH_SITE and PAYLOAD . The template containing information about the rocket launch described in the above sentence would be represented as follows: VEHICLE( X  X  new Chinese rocket X  X  X  X ocket X ) PAYLOAD( X  X  205 million dollar satellite X  X  X  X atellite X  X   X  X estern Satellite X )
LAUNCH_SITE( X  X injiang X  X  X  X hina X ) 4.4 Results: Fact matches Table 1 shows the result of the fact matching process described in Sect. 3 when applied to each of these corpora. The column marked  X  X  X ll X  X  indicates the number of centage. The columns marked  X  X 2 X  X ,  X  X 3 X  X  and  X  X 4 X  X  show the proportion of facts con-sisting of two, three and four fields falling into each category. (The facts derived from the MUC6 and MUC7 corpora contain up to three fields and consequently there are no facts listed in the column marked  X  X 4 X  X  for these corpora.)
It can be seen that the proportion of facts falling into the full match category is around 60% for all three corpora. This suggests that a SSA IE system could, at best, identify only hope to fully identify three fifth of the facts in these texts. Therefore it seems that the coverage of SSA systems is severely limited on these three corpora and that the approach is not sufficient to identify the information contained in these texts.

In each of the corpora around 40% of the facts fall into the partial and Nomatch categories. These facts cannot be fully identified by a SSA system. The distribution of facts across the partial and Nomatch categories is similar for the MUC4 and MUC7 corpora but differs for MUC6. In the MUC4 and MUC7 corpora both the partial and Nomatch categories contain around 20% of the facts. However, in the MUC6 corpus 39.8% of the facts fell into the partial match category and only 0.7% were Nomatch X  X s. It is impossible for a SSA system to identify facts which fall into the Nomatch category, suggesting that these approaches may be more successful on the MUC6 corpus than the other two used in these experiments.

One reason for the low proportion of facts falling into the Nomatch category in the MUC6 corpus may be the relative simplicity of the facts derived from these text compared with the other two corpora. Management succession events in this corpus are often described within a comprehensive sentence, for example  X  X  X VC Network Inc., as expected, named Barry Diller its chairman and chief executive officer. X  X  Sentences which summarise the facts of interest occur less frequently in the other corpora. In addition, the MUC6 corpus contains a larger proportion of facts con-sisting of more than two fields than the other two corpora. Facts with two fields can either match the text fully or not at all while those with more fields can also par-ticipate in both Partial and Full matches. In MUC6 98.8% 558 565 of facts have at least three fields while this figure is just 32% 384 1200 for the MUC4 corpus and 37.6% 59 157 for MUC7. 4.5 Results: Field Matches Table 2 shows an analysis of matches for individual fact fields. The pairs of figures in the main body of the table refer to the number of instances of the relevant field which are mentioned in the sentence matched by an event, identified by finding the sentence which matches the greatest number of fields for a particular fact, and the total number of instances of that field. The column headed  X  X  X ull match X  X  lists the facts which fully match the text and, as would be expected, all fields are matched. The columns marked  X  X  X artial match X  X  and  X  X  X oMatch X  X  lists the facts which fall into those categories. The  X  X  X ll matches X  X  column shows the proportion of facts falling into either the Full or Partial Match categories and the total number of fields in the corpus. This figure is also expressed as a percentage.

It can be seen that there are differences between the percentage of matches both across the three corpora and for the various fields within each corpus. The highest proportion of matches is seen in the MUC6 corpus and it is likely that this is due to the fact that a higher proportion of facts in this corpus fall into the  X  X  X artial match X  X  category compared with the other corpora and the relative simplicity of the facts in this corpus. The facts contained within the MUC4 corpus have the most complex structure, in terms of number of potential fields, and this may explain why the lowest matches are recorded for those texts.

Within each of the corpora it can be seen that there is some variation between individual fields in terms of the proportion of facts which match. In the MUC4 corpus the lowest results are observed for the Instrument field; less than half of the instances of this field participate in facts which match the text. Better performance is recorded for the Perp and Humtgt fields with around two thirds of instances par-ticipating in facts which match the text.

A reason for this difference is that fillers of the Perp and Humtgt or Phystgt fields often appear together in a sentence which summarises the incident and the filler of the Instrument field, which lists the weapon used, appears later in the text in a sentence which provides further detail. An example can be seen in the following pair of sentences from a MUC4 document which refer to an incident in which the Humtgt is  X  X  X ARIA ELENA DIAZ PEREZ X  X , Perp  X  X 10 PAID ASSASSINS X  X  and the Instrument  X  X  X UBMACHINE GUN X  X .
 MARIA ELENA DIAZ PEREZ, THIRD JUDGE OF PUBLIC ORDER, AND TWO OF HER BODYGUARDS FROM THE DAS [ADMINIS-TRATIVE DEPARTMENT OF SECURITY], WERE ASSASSINATED IN MEDELLIN TODAY BY A GROUP OF 10 PAID ASSASSINS IN TWO CARS. ... A TOTAL OF 55 9-MM SUBMACHINE GUN ROUNDS HIT THE LEFT SIDE OF THE CAR.
 Results from the MUC6 corpus show that the Post and Person facts participate in matches more frequently than the Org field. This difference can also be explained management succession events are commonly introduced near the start of the newswire story and these descriptions almost invariably contain all three fact fields. For example, one story starts with the following sentence:  X  X  X ashington Post Co. said Katharine Graham stepped down after 20 years as chairman, and will be succeeded by her son, Donald E. Graham, the company X  X  chief executive officer. X  X  Later in the story further succession events may be mentioned but many of these use an anaphoric expression (e.g.  X  X  X he company X  X ) rather than explicitly mention the name of the organisation in the event. For example, this sentence appears later in the same story:  X  X  X lan G. Spoon, 42, will succeed Mr. Graham as president of the company. X  X 
There is less difference between the percentage of the individual fields par-ticipating in a match in the MUC7 corpus. The documents which form this corpus tend to be less regular than the MUC4 and MUC6 documents (in which infor-mation of interest is often summarised at the start of the document and elabo-rated later). In these texts the facts to be identified tend to be distributed through the document and it is common to find sentences which contain two of the fact fields with another described separately. For example this pair of sentences from a MUC7 document shows the description of a rocket launch in which the VEHICLE ( X  X  X ndeavour X  X ) and LAUNCH_SITE ( X  X  X ennedy Space Center X  X ) are mentioned in the first sentence and the PAYLOAD ( X  X  X  $10 million NASA satellite X  X ) in the later one.  X  X  X he shuttle Endeavour and a crew of six are to blast off Thursday at 4:18 a.m. EST from NASA X  X  Kennedy Space Center. Midway through the mission, the crew plans to deploy a $10 million NASA satellite for nearly 48 h of opera-tions... X  X  In another example the VEHICLE ( X  X  X riane 5 X  X ) and LAUNCH_SITE ( X  X  X ourou, French Guiana X  X ) are mentioned in the first sentence and PAYLOAD ( X  X  X our Euro-pean Space Agency Cluster satellites X  X ) in the second.

Kourou, French Guiana, June 4 (Bloomberg) X  X riane 5, a new and more powerful rocket developed by the pan-European Arianespace group, exploded within seconds of blastoff in a major setback to the world X  X  leading commercial satellite launcher. The unmanned rocket, the most powerful yet built specifi-cally for commercial payloads, was carrying four European Space Agency
Cluster satellites, part of a $500 million project to study the interaction of the sun and the earth.
 In summary, the style in which the documents are written has an effect on the facts which can be extracted from them using a SSA system. In some corpora, such as the ones used for MUC4 and MUC6, many of the facts are summarised in a single sentence at the start of the document. For these texts it would be feasible to extract certain pieces of information by examining single sentence contexts. For example, a SSA system could extract many of the relations between Person and Post in the MUC6 text, although it would be unable to identify many of the Person and Org relations. In other texts the information of interest is distributed in the documents in a less regular way. For example, MUC7 documents do not generally start with a summary of the rocket launches mentioned in the document and this information is normally distributed across the text. This suggests that SSA approaches may be more feasible for some extraction tasks than others and that the structure of documents from which information is being extracted is important.
 4.6 Alternative analysis It has already been mentioned that the approach described here estimates an upper bound on the proportion of facts which are described within single sentences. Stevenson ( 2004 ) reports an alternative approach which places a more accurate bound on this figure, but required additional data and could only be applied to the MUC6 corpus. This approach made use of an alternative version of the MUC6 corpus, produced by Soderland ( 1999 ), in which only facts described within a single sentence were annotated. This set of facts was compared with the ones extracted from the MUC6 templates (which include all facts mentioned in the documents). Each fact derived from the templates was identified as being either a full match, partial match or nomatch, with these categories being analogous to the definitions used here: a full match was said to occur when a fact derived from the MUC6 template was also listed in Soderland X  X  version of the corpus, a partial match when at least two of the fields match for facts in both corpora and Nomatch when a fact in the MUC6 corpus is not mentioned in Soderland X  X  version. This approach is more accurate than the one used here because a match (full or partial) occurs when a sentence genuinely mentions a fact, not just when the string slots occur together. These experiments used the same fields to define a fact as used here (post, organi-sation and person). Stevenson ( 2004 ) reported that 40.6% of the facts fell into the Full match category, 39.1% were partial matches and the remaining 20.3% No-matches.

The number of facts categorised as full matches is substantially less than the one reported here (59.5%). We do not have access to corpora annotated with events at the sentence level which are necessary to carry out this analysis for the MUC4 and MUC7 corpora so it is not possible to generate comparable results for these data sets. It may also be problematic to try to infer too much about how these results may effect other corpora given that the fact structure is less complex in MUC6. However, the difference in these results suggests that the true proportions may be substantially lower than the figures reported in this paper. 5 Combining facts across sentences The experiments described so far show that it is not possible to identify a substantial proportion of facts within a document by only examining each sentence in isolation. This naturally raises the question of how these facts can be identified. Analysis of the documents used for these experiments show that various linguistic devices are used to connect the parts of a fact description across sentences.

The most straightforward of these is when an anaphoric expression is used to refer to one of the fact X  X  fields. For example, this pair of sentences appear in the MUC6 corpus:  X  X  X all Street was hoping for stronger outside management to help Figgie. Instead, the company named a director, 66-year-old Walter M. Vannoy, who has been on the board since 1981. X  X  The second sentence describes the promotion of Walter M. Vannoy to the position of director in a company called Figgie. However, the name of the company is not mentioned directly but is referred to by an anaphoric expression. In these cases the fact could be considered to be described entirely within one sentence but with some fields being referred to indirectly. We refer to these cases as single sentence facts containing anaphoric references.
In more complex cases the fact description is genuinely spread across more than one sentence with the various parts of the description being linked by anaphoric expressions or alternative descriptions. For example, Sect. 4.5 shows two sentences from the MUC7 corpus describing the launch of the  X  X  X riane 5 X  X  rocket. The first sentence mentions the vehicle and launch site while the second contains details of its payload. The two sentences are connected through the phrase  X  X  X nmanned rocket X  X  but neither sentence contains all three fields which form this fact, even if anaphoric expressions are resolved. Another example, this one from the MUC6 corpus, is shown in Sect. 1 where the name of the organisation ( X  X  X ace America Group Inc. X  X ) is mentioned in the first sentence and the name of two executives leaving that company and their positions in the second. Although the sentences are connected by the coreference chain connecting  X  X  X he executives X  X  and  X  X  X wo top executives X  X  neither contains all three of the fields which form the fact, either directly or indirectly via coreference. These cases are referred to as connected multiple sentence facts. For cases such as these some inference will be required to combine together all the parts of the fact description.

In the cases discussed so far the various parts of the fact description are connected via some referential relationship in the text. However, in other cases there may be no direct connection between the sentences describing the fact. These facts can only be identified using a deeper understanding of the text such as discourse analysis or the application of world knowledge. For example, the two sentences from a MUC4 document shown on Section 4.5 describe an assassination. The main description of the incident is listed in the first sentence and the instrument ( X  X  X UBMACHINE GUN X  X ) in the second. These pieces of information can only be combined to form a complete fact with knowledge that the main topic of this document is the assassi-nation and the second sentence provides detail about it. There is no direct con-nection between the two sentences. (Note that the noun phrase  X  X  X HE TWO CARS X  X  in the first sentence is not the antecedent of  X  X  X HE CAR X  X  in the second sentence.) This is an example of a situation in which the various parts of the fact are described in text without being directly connected. There are other cases in which information is not mentioned in the text but has to be inferred using world knowledge. For example, the following two sentences are taken from the MUC6 corpus:  X  X  X avid J. Bronczek, vice president and general manager of Federal Express Canada Ltd., was named senior vice president, Europe, Africa and Mediterranean, at this air-express concern. Mr. Bronczek succeeds Kenneth Newell, 55, who was named to the new post of senior vice president, retail service operations. X  X  This text describes two movements of position for Kenneth Newell: leaving the position of vice president and moving to the position of senior vice president. However, the fact that Newell is leaving a position can only recognised with knowledge that when one executive replaces another then that executive must leave their current position. Cases such as these are referred to as unconnected multiple sentence facts.

Single sentence facts containing anaphoric expressions are likely to be the most straight forward to identify automatically since they do not require the combination of information in separate sentences. However, multiple sentence facts, both con-nected and unconnected, require inference and, possibly, the application of world knowledge to be recognised. The remainder of this Section describes an experiment which quantifies the proportion of single sentence facts containing anaphoric expressions in the MUC6 corpus.
 5.1 Experiment In order to estimate the proportion of single sentence facts containing anaphoric expressions we require a corpus for which the facts have been identified (such as those used for the experiments in Sect. 4 ) and some method for resolving anaphoric expressions in those texts. Any automatic system for anaphora resolution will make errors so we prefer to make use of manual annotation. Fortunately, portions of the MUC6 and MUC7 corpora were manually annotated with coreference chains as part of the evaluation and are ideal for this purpose. 4 However, only a small portion of each corpus was annotated with this information (presumably because of the cost of annotation). The MUC6 corpus contains 20 texts which are annotated with core-ference information and have facts associated with them. These texts contain a total of 97 facts. The MUC7 corpus does not contain any appropriate documents since none of those which are annotated with coreference information contain any facts. 5 The 20 texts from the MUC6 corpus were used for the experiments described in this Section and are referred to as the  X  X  X oreference corpus X  X .

Texts in this corpus are annotated with coreference chains. Each coreferential expression is labelled with a unique identifier and the identifier of its immediate antecedent. Figure 2 shows two sentences from the MUC6 corpus annotated with coreference information (slightly simplified for clarity). 6 The second sentence is a single sentence fact containing an anaphoric expression. It describes the fact that Alan G. Spoon will become president of Washington Post Co., although the name of the company is referred to indirectly using a coreferential expression ( X  X  X he com-pany X  X ). The annotation shows that the referent of this expression is one which has been labelled with the identifier 11 ( X  X  X he company X  X  in the first sentence) and that its reference is the one labelled 2 :  X  X  X ashington Post Co. X  X 
This data was used to carry out an experiment to determine the proportion of single sentence facts containing anaphoric expressions in the coreference corpus. The experiment was based on the matching process described in Sect. 3 . However, rather than requiring fillers of the fields which constitute a fact to appear together within a sentence, we also consider them to co-occur if a possible filler is one of the possible antecedents of an expression which occurs within the sentence. The filler can occur anywhere in the text before the expression and does not need to be the immediate antecedent.

For example, using the matching process described in Sect. 3 (which does not include the antecedents of anaphoric expressions in possible matches) a fact with the following fields person(Alan G. Spoon) post(president) and org(Washington Post Co.) would match the second sentence in Fig. 2 only partially. However, when the antecedents of coreferential expressions are also allowed to participate in matches this fact would fully match that sentence.
This procedure, like the one described in Sect. 3 , places an upper bound on the number of facts which could be matched. Annotation of the MUC data represents perfect anaphora resolution and it is unlikely that this result could be repeated in an actual system.
 The experiments included two levels of anaphora resolution: all and pronominal. In the first the antecedents of all anaphoric expressions are examined to identify matches. When anaphora resolution is restricted to anaphora only the antecedents of pronominal anaphora expressions participate in matches. Pronominal anaphora is examined in isolation because it is the most common form of anaphora (Mitkov, 2003 , p. 268) and this experiment is designed to determine how much can be gained when it is used alone. These two approaches are compared with the case when no anaphora resolution is used (referred to as  X  X  X one X  X ) which is identical to the matching process outlined in Sect. 3 . 5.2 Results The results of this experiment are shown in Table 3 . For each level of anaphora resolution (all, pronominal and none) the proportion of facts falling into the full, partial and nomatch categories is shown. Each of the 97 facts in the coreference corpus consisted of three fields so the results are not broken down by number of field (unlike those reported in Table 1).

A first observation is that the proportion of facts falling into the full match category when no coreference resolution is carried out is around 54%. This figure is lower that the one recorded when all texts in the MUC6 corpus were included in the analysis (see Table 1 ). This shows that there is variation in the proportion of facts which are expressed within a single sentence and may also indicate that the events contains in these particular corpora are more distributed than the rest of the MUC6 corpus.

When all anaphoric expressions are resolved almost 20% more facts are fully matched. However, over a quarter of the facts are still only partially matched and these must be multiple sentence facts (either connected or unconnected). The application of pronominal anaphora resolution allows 6% more facts to be fully matched than when no anaphora resolution was applied. This demonstrates that, while the resolution of pronominal anaphora is useful, there is a significant benefit from resolving as wide a range of anaphoric expressions as possible.

The results of a field by field analysis for these experiments is shown in Table 4 , which uses a similar format to Table 2 . It can be seen that matches for the Post and Person fields remain consistent for the various levels of anaphora resolution; mat-ches for the Post field vary by a little more than 5% and Person not at all. However, a far larger variation, over 20%, is observed for the Org field. This is consistent with the analysis in Sect. 4.5 which showed that the Org field of the fact was often linked with the other fields through a coreferential expression.

These results show that the use of anaphora resolution leads to a substantial increase in the proportion of facts which can be identified by SSA systems. The antecedents of a full range of anaphoric expressions need to be identified to realise this benefit. However, field analysis shows that this may benefit some pieces of information more than others and care should be taken to ensure that anaphora resolution will be of benefit for a particular extraction task. For example, an IE system which aims to identify relations between Person and Post in the MUC6 corpus will not gain substantially from the use of anaphora resolution but this would be highly beneficial for the Person  X  Org relation.

Even when anaphoric expressions are resolved a significant proportion of the facts in the MUC6 corpus could not be fully identified by a SSA system. These facts require inference across the information contained in various sentences to be identified, possibly using discourse analysis and world knowledge. An IE system which aims for comprehensive identification of facts in text must then make use of these techniques and cannot rely on simpler approaches.

Unfortunately the results reported in this Section are limited by the fact that there is only a small amount of data which is suitable for these experiments available. However, they do indicate that anaphora resolution will help in the process of fact identification and that the resolution must be carried out over as wide a range of anaphoric expressions as possible.
 6 Related work Hirschman ( 1992 ) carried out an analysis of the difficulty of the MUC4 evaluation set. She categorised each document as requiring a single template or multiple templates to be filled. In addition the information which filled these templates was classed as being found in either a single sentence or multiple sentences. It was discovered that document requiring the filling of more than one template were easier when the information for each template was contained within a single sentence than when it was spread across multiple sentences. However, an unexpected result was that documents which require a single template to be filled where the information was contained within a single sentence were actually more difficult than those where the information was spread across multiple sentences. Hirschman attributed this to the fact that these documents were mainly comprised of irrelevant information and that the process of identifying this overshadowed the difficulty of combining infor-mation across sentences. It was also found that the performance of different systems across documents was very consistent which implied that some texts are more dif-ficult to understand than others.
 Bagga and Biermann ( 1997 ) developed techniques for comparing the difficulty of IE tasks by assigning a  X  X  X omain number X  X  which represented the complexity of the facts being extracted. They found that the MUC6 evaluation task was easier than the one used for the fifth MUC (international joint ventures) but harder than the one used for MUC4. However, Bagga and Biermann X  X  technique did not take into account the distribution of the facts in text.

The results presented by Hirschman and the analysis presented here shows that the description of different parts of facts can be distributed through a text. Huttunen, Yangarber, and Grishman ( 2002 ) also demonstrated that facts which are described in this way are more difficult to identify. 7 Summary and implications The experiments described here show that a substantial proportion of facts in three commonly used IE evaluation corpora are not expressed within a single sentence and, therefore, cannot be identified by SSA systems. These experiments used a variety of domains and text types including newswire text, broadcast news and transcribed speech. Further experiments using anaphora resolution show that, while identify all facts contained within documents. The exact proportion of facts which cannot be expressed within a single sentence is perhaps not particularly significant in itself, and will depend upon the particular documents and facts being extracted from them. The procedures outlined here place upper bounds on the proportion of facts which are expressed within a single sentence and the true maximum performance for SSA systems may be even lower.

These results have implications for the evaluation of IE algorithms. Many recent systems have been evaluated in terms of their ability to extract facts which are expressed within single sentences, for example (Chieu &amp; Ng, 2002 ; Soderland, 1999 ; Stevenson &amp; Greenwood, 2005 ; Yangarber et al., 2000 ; Zelenko et al., 2003 ), and the analysis reported here demonstrates that the results for these approaches are likely to be significantly lower if those systems attempted to extract all facts. Results from SSA systems should be reinterpreted in light of this information.

These results should also be taken into account in the implementation of IE systems. Approaches which fail to consider facts whose description is spread across several sentences are unable to fully identify up to 60% of the facts in the three corpora analysed for these experiments. So, applications which require a compre-hensive set of facts to be extracted from a document, particularly if those facts are not simple binary relations, must ensure that their systems can identify those ex-pressed across multiple sentences. In addition, IE systems which aim to identify all facts in text must make use of relatively deep analysis of text, including modelling of the discourse and use of world knowledge.

The experiments reported here also show that some facts (for example the relation between Person and Post in the MUC6 corpus) are more likely to be described within a single sentence than others. SSA systems may be an appropriate technique for identifying these facts. However, it is important for text to be analysed to determine whether the facts of interest are stated this way before an approach which considers each sentence in isolation is chosen.
 References
