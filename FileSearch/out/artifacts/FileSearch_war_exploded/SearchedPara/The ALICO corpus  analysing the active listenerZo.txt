 Zofia Malisz 1,2  X  Marcin W X odarczak 3  X  Hendrik Buschmeier 4  X  Joanna Skubisz 5  X  Stefan Kopp 4  X  Petra Wagner 6 Abstract The Active Listening Corpus (ALICO) is a multimodal data set of spontaneous dyadic conversations in German with diverse speech and gestural annotations of both dialogue partners. The annotations consist of short feedback expression transcriptions with corresponding communicative function interpreta-tions as well as segmentations of interpausal units, words, rhythmic prominence intervals and vowel-to-vowel intervals. Additionally, ALICO contains head gesture annotations of both interlocutors. The corpus contributes to research on spontaneous human X  X uman interaction, on functional relations between modalities, and timing variability in dialogue. It also provides data that differentiates between distracted and attentive listeners. We describe the main characteristics of the corpus and briefly present the most important results obtained from analyses in recent years. Keywords Active listening Multimodal feedback Backchannels Head gestures Attention Multimodal corpus Multimodal corpora are a crucial part of scientific research investigating human X  human interaction. Recent developments in data collection of spontaneous communication emphasise the mutual influence of verbal and non-verbal behaviour between dialogue partners (Oertel et al. 2013 ). In particular, the listener X  X  role during interaction has attracted attention in both fundamental research and technical implementations (Sidner et al. 2004 ; Kopp et al. 2008 ; Truong et al. 2011 ; Heylen et al. 2011 ; de Kok and Heylen 2011 ; Buschmeier and Kopp 2012 ). Recent efforts in the collection and analysis of listener data can be found in de Kok and Heylen ( 2011 ) and Heldner et al. ( 2013 ).
 In the present paper, we report on the design and main results obtained from the Active Listening Corpus (ALICO), collected at Bielefeld University. ALICO is a multimodal corpus of German dialogues built to study spoken and gestural behaviour in face-to-face communication, with a special focus put on the listener. The corpus consists of 50 dialogues in which the communicative context X  interaction with a storytelling partner X  X as set up to facilitate spontaneous, active listening behaviour. Short feedback expressions (henceforth SFEs) were extracted from the corpus (cf. Schegloff 1982 ; Ward and Tsukahara 2000 ; Edlund et al. 2010 ) and classified using an inventory of communicative feedback functions Buschmeier et al. 2011 . Short feedback expressions of the listeners have been annotated and analysed for 40 out of 50 ALICO dialogues so far. The corpus also includes head gesture annotations for both the listener (in 40 dialogues) and the storyteller (in 9 dialogues), along with gesture type tags with categories such as nod , shake ,or tilt .
Additionally, the ALICO conversational sessions feature a task in which the listener X  X  attention was manipulated experimentally. Previous studies reported that the listener X  X  attentional state had an influence on the quality of speaker X  X  narration and the number of feedback occurrences in dialogue. Bavelas et al. ( 2000 ) carried out a study in which the listener was distracted by an ancillary task during a conversational session. Both Bavelas et al. ( 2000 ), and the results of a similar study by Kuhlen and Brennan ( 2010 ), demonstrated that the preoccupied listener produced less context-specific feedback, suggesting that listener distractedness affected the behaviour of the interlocutor and interfered with the speaker X  X  speech. At the same time, the effect of distractedness on the verbal and non-verbal behaviour in the listener herself has so far received little attention. We adapted the task used by Bavelas et al. ( 2000 ) in half of the ALICO conversational sessions to allow an investigation into how active listening behaviour changes when the attention level is varied in dialogue. In several analyses of the ALICO data (Buschmeier et al. 2011 ; Malisz et al. 2012 ; W X odarczak et al. 2012 ), we managed to reveal some of the communicative strategies listeners used when distracted.
The corpus was also built for the purpose of studying temporal relations across modalities, within and between interlocutors. The rhythmic annotation layer in the speaker X  X  speech (vocalic beat intervals and rhythmic prominence intervals) has been annotated in 20 dialogues and has served as input for coupled oscillator models providing an important testbed for hypotheses concerning interpersonal entrainment in dialogue (Wagner et al. 2013 ). Inden et al. ( 2013 ) reported on the first evaluations of entrained timing behaviour in two modalities implemented in an artificial agent.

The unique features of ALICO enable a targeted study of active listening with varying listener attention levels in the context of spontaneous interaction, thereby contributing to a better understanding of human discourse. The data is particularly well suited for studying temporal interactions between multimodal phenomena, effects of distractedness on communication and basic mechanisms of interaction, especially related to providing feedback and establishing common ground. In addition to informing basic research on human X  X uman dialogue, the corpus can be useful for fields such as interaction quality monitoring, where detecting distracted users could assist the early prevention of communication problems. More generally, the design of ALICO resulted in conversations rich in feedback behaviour that could improve existing models of feedback implemented in dialogue systems and help build more human-like and sociable conversational agents. Indeed, analysis outcomes have already proven useful in these domains (Inden et al. 2013 ).

The findings so far indicate a link between feedback form and its pragmatic function in both visual and verbal domains. Temporal and functional interdepen-dencies between the two domains further suggest complex patterns of mutual influence underlying multimodal interaction. For instance, while complex head gestures are more likely to be accompanied by verbal feedback, simple head movements exhibit tighter synchronisation with the verbal counterpart, and visual-only feedback is shorter than bimodal. In addition, distractedness was found to significantly influence both the amount and the type of feedback given by listeners. Specifically, distracted listeners produce less feedback overall and a decrease in communicating understanding of the interlocutor X  X  message emerged as a consistent marker of distractedness across the modalities. Annotation and analysis of the remaining material is underway.

In the present paper, we describe the main corpus characteristics and summarise the most important results obtained from analyses done so far, demonstrating its utility to studies of spontaneous multimodal communication. In Sect. 2 we define the multimodal feedback phenomena that constitute the core of the ALICO corpus. Section 3 provides an overview of the corpus architecture. Sections 4 and 5 discuss annotations of verbal and non-verbal modalities in the data set. Section 6 summarises work to date on multimodal aspects of feedback in ALICO, followed by conclusions and plans for future work in Sect. 7 .

A schematic guide to previous studies on ALICO and to the relevant discussion sections in this work is provided in Fig. 1 .
 Although the active speaker usually fulfils the more dynamic role in dialogue, the listener contributes to successful grounding by giving verbal and non-verbal feedback. Short vocalisations like  X  X hm X ,  X  X kay X ,  X  X  X  constitute the majority of listener turns and form an integral part of face-to-face communication. Such verbal feedback expresses the ability and willingness to interact and understand, as well as conveys listener emotions and attitudes. The duties of grounding in dialogue fulfilled by verbal feedback (Clark 1996 ; Clark and Schaefer 1989 ) are shared with head gestures and other non-vocal behaviour. Additionally, head movements, by co-occurrence with mutual gaze (Peters et al. 2005 ) and correlation with other active listening displays, emphasise the degree of listener involvement in conversation. By means of head gestures, the listener can also encourage the speaker to stay active during his or her speech at turn relevance places (Wagner et al. 2014 ; Heldner et al. 2013 ). Both kinds of feedback are discussed in more detail in the following sections. 2.1 Spoken feedback in active listeners Short feedback expressions, as understood in the present paper, are located at the intersection of several other concepts found in literature defining dialogue phenomena, most significantly backchannels (Yngve 1970 ), continuers Schegloff 1982 , discourse markers (Grosz and Sidner 1986 ), or accompaniment signals (Kendon 1967 ). Unfortunately, distinctions between them are vague at best. In what was perhaps the most stringent attempt so far, Ward and Tsukahara ( 2000 ) give an overview of definitional criteria commonly adopted in literature, which include: responsive character to the content of the interlocutor X  X  utterance, optionality, brevity, expressing attitudinal meaning, non-turn-taking character and facilitating communication flow.

More generally, Schegloff ( 1982 ) groups theories of human interaction into two broad categories: those which describe feedback phenomena in terms of providing  X  X  X vidence of attention, interest, and/or understanding on the listener X  X  part X  X  (e.g. Kendon 1967 ; Clark and Schaefer 1989 ) and those which treat them as a means of active listening and a mechanism of interspeaker coordination (e.g. Duncan and Fiske 1977 ).
 In accordance with grounding theory Clark and Schaefer 1989 , we assume that SFEs are tightly linked to coordinative processes underlying successful commu-nication, a position that motivated analyses of rhythmic feedback coordination in this data (Wagner et al. 2013 ). At the same time, we explicitly manipulate attention levels in the recorded sessions by introducing distractors in order to describe the differences in feedback behaviour as a function of attention.

In the present study, we also take the middle ground on another distinction made in determining the concept of feedback, namely between semantic/functional (Allwood et al. 1992 ; Schegloff 1982 ) and formal definitions relating to surface realisation (e.g. brevity, lexical form, position within the turn, Ward and Tsukahara 2000 ; Edlund et al. 2010 ).

Hence, we adopt a predominantly functional definition of SFEs as indicating uptake of interlocutor X  X  utterance, while at the same time incorporating certain surface criteria. In particular, following Koiso et al. ( 1998 ), we require that, as the name implies, SFEs be short. Our approach thus stands in contrast to the original definition of backchannels by Yngve ( 1970 ), which also included longer stretches of speech, for instance, completions by the listener. Admittedly, this is a method-ological compromise since longer feedback utterances do not yield themselves to the intended analyses for reasons of incomparable prosodic structure and timing properties (Wagner et al. 2013 ; Heldner et al. 2013 ; W X odarczak et al. 2015 ). At the same time, surface criteria such as lexical type alone are not sufficient since, e.g. some occurrences of  X  X a X  function as answers or discourse markers rather than as feedback.

More importantly, in accordance with formal semantic accounts, we posit that the feedback category comprises a wide range of pragmatic meanings from merely conveying perception of interlocutor X  X  communicative behaviour to higher-level evaluation of the message (acceptance, surprise, etc., Allwood et al. 1992 ). However, as we show in Sect. 6.1 , adopting a formal taxonomy leads to interpretative ambiguities when confronted with real data, suggesting that semantic definitions be augmented to allow optional interpretations related to sequential organisation of discourse (Gravano et al. 2007 ) and affective stance (Kopp et al. 2008 ; Buschmeier and Kopp 2012 ).
 2.2 Head gesture feedback in active listeners Functions of communicative head movements include marking contrast between topics, referring to objects by pointing with the head in space (McClave 2000 ; Heylen 2006 ) or eliciting feedback from dialogue partners (McClave 2000 ; Goodwin 1981 ). These functions are most frequently expressed by rotating the head vertically or laterally. Here, we constrain our functional definition of head movements in a similar fashion as the SFE definition: we take that establishing common ground, displaying attention and facilitating dialogue coordination by providing feedback are the most prevalent reasons to use head gestures in communication, particularly in listeners (W X odarczak et al. 2012 ; Heldner et al. 2013 ).

Head movement forms are relatively free, since they are much less convention-alised than speech forms but more constrained than manual gestures. This freedom complicates the mapping of head gesture form and function. However, we observe that most annotation schemes developed so far describe simplified models of head movement forms that already carry a functional load. The load is due to the frequent re-occurrence and a certain coherence of specific head movement shapes in human dialogue, from which a group of prototypes emerges: nods, shakes, turns, etc. (see also Table 3 ). These prototypes are typically later analysed and matched with more detailed meanings.

Recent analyses of multimodal corpora report that head nodding predominates among head gestures in free conversation (56 % in a multimodal corpus of Japanese in Ishi et al. 2014 ). When the listener role is assigned to the participants, 81.5 % of head movements are nods, as previous analyses of the corpus featured in the present study revealed (W X odarczak et al. 2012 ).

Several authors have attempted to further subclassify nodding into more finely grained feedback functions. However, surveys of the literature (Wagner et al. 2014 ) reveal that the functional interpretation of a simple distinction such as single versus multiple cycle nodding, varies from language to language (Hadar et al. 1985 for English, Cerrato 2007 for Swedish, W X odarczak et al. 2012 for German, Ishi et al. 2014 for Japanese) and depending on the dialogue task and analysis.

Some general functional distinctions between gestures based on criteria such as the number of nodding cycles are possible. For example Hadar et al. ( 1985 ) differentiated between linear and cyclic head movement forms and categorised between communicative and non-communicative head movement frequency ranges, equivalent to e.g. single and multiple nodding bouts. The approach in Hadar et al. ( 1985 ) attempted at objectivising the description of head movement forms by using kinematic measurements. Modern motion tracking systems further enable the operationalisation of higher order derivatives of movement (velocity, jerk) and precise points of maximum extension. An attempt to map head movement prototypes to kinematic parameters derived from motion tracking was made in Kousidis et al. ( 2012 ).

Qualitative descriptors such as jerkiness (Poggi et al. 2010 ) or fluidity (Hartmann et al. 2006 ) were also proposed, usually in order to find correlations between movement features and contextual or attitudinal information. Poggi et al. ( 2010 ) suggest that simple vertical movements of the head share basic semantic cores that depend on speakership roles: in speaking turns, head nods communicate levels of  X  X  X mportance X  X , while in listening turns, the main message conveyed is feedback (Poggi et al. 2010 use the term  X  X  X cceptance X  X ).

In the present work, we use the time-aligned annotations of head movement prototypes (Table 3 ) and report on several analyses relating the SFE function and the function of the co-occurring head movements. We also look at the temporal relations between the two modalities that might point to semantic coherence. ALICO consists of 50 same-sex conversations between 25 dyads of German native speakers (34 female and 16 male) recorded audio-visually. All participants were students at Bielefeld University and, apart from four dialogue partners, did not know each other before the study. Participants were randomly assigned to dialogue pairs and rewarded for their effort with credit points or a payment of 4 euros. None of the participants reported any hearing impairments. The total length of the recorded material is 5 h 12 min. Dialogues have a mean length of 6 min and 36 s (Min = 2:00 min, Max = 14:48 min, SD = 2:50 min). Table 13 in the Appendix summarises the data file types, sizes and annotation progress at the time of publication.
 The sessions were recorded in a studio at Bielefeld University (MIntLab; Kousidis et al. 2012 ). Dialogue partners were placed approximately three metres apart (to minimise cross-talk) in a comfortable setting. Participants wore high quality headset microphones (Sennheiser HSP 2 and Sennheiser ME 80) while another condenser microphone captured the whole scene. Camcorders (Sony VX 2000 E) recorded the interactions from three camera perspectives: medium shots showing the storyteller and the listener X  X nabling future fine-grained analysis of their head gestures X  X nd a long shot showing the whole scene. Figure 2 shows the setting and one of the dyads from all three perspectives.

Face-to-face dialogue forms the core of the corpus. The conversational scenario engaged one dialogue partner, the storyteller , in telling two holiday stories. The other participant, the listener , was instructed to listen actively to the stories told by the storyteller, make remarks and ask questions, if they pleased. Participants were assigned to their roles randomly and received their instructions separately.
Furthermore, similarly to Bavelas et al. ( 2000 ), the listener was engaged in an ancillary task during one of the stories (the task order was counterbalanced across dyads): he or she were to press a button on a hidden remote control (see Fig. 2 ) every time they heard a word beginning with a letter  X  X  X , which is the second most common word-initial letter in German and often corresponds to perceptually salient sibilant sounds. A fourth audio channel was used to record the  X  X licks X  synthesised by a computer when listeners pressed the button on the remote control. The listeners were also required to retell the stories after the study and to report on the number of  X  X  X  words to ensure they perform both tasks. The storyteller was made aware that the listener is going to search for something in the stories but no further information about the details of the listener X  X  tasks was disclosed. Annotation of the interlocutors X  speech was performed in Praat (Boersma and Weenink 2013 ) and post-processed using TextGridTools (Buschmeier and W X odar-czak 2013 ), independently from head gesture annotation. As speech annotation differs for listener and speaker role in the corpus (see Table 1 for an overview of the respective annotation tiers), they are discussed in turn below. 4.1 The listener We annotated short feedback expressions produced by the listener and interpreted the corresponding communicative feedback functions in 40 dialogues thus far, i.e. in 20 sessions involving the distraction task and 20 sessions with no distractions. We carried out listener SFE segmentation automatically on the basis of signal intensity in Praat and we subsequently checked and adjusted the segmentation manually. Another annotator then transcribed the pre-segmented SFEs according to German orthographic conventions. Longer non-SFE listener turns were marked but not transcribed.

Three independent annotators assigned feedback functions to listener SFEs in each dialogue. 1 Each annotator carefully took the communicative context into account. A feedback function scheme was developed and first described in Buschmeier et al. ( 2011 ), largely based on Allwood et al. ( 1992 ). The inventory involves core feedback function categories that signal perception of the speaker X  X  message (category P1 ), understanding (category P2 ) of what is being said, acceptance/agreement (category P3 ) with the speaker X  X  message. The levels can be treated as a hierarchy with an increasing value judgement of grounding  X  X epth X  (Allwood et al. 1992 ; Clark 1996 ; W X odarczak et al. 2010 ). The negation of the respective functions was marked as N1  X  N3 . Due to very low frequency of negative feedback, we discuss only positive feedback in the present paper.

The annotators had an option to extend listener feedback function categories with three modifiers. Modifiers C and E referred to feedback expressions occurring at the beginning or the end of a discourse segment initiated by the listener (Gravano et al. 2007 ). Modifier A in turn referred to the listener X  X  emotions or attitudes co-occurring with core functions, leading to categories such as P3A (Kopp et al. 2008 ; Buschmeier and Kopp 2012 ). If an SFE functioned as a clear emotional or attitudinal display, the annotators were allowed to use the category A as a single, core category. Therefore, the attitudinal/emotional category is the only one in the ALICO scheme that can fulfil both a modifying and a core function. Table 2 summarises the resulting inventory of feedback functions.

Automatically derived majority labels determined the final feedback function interpretation. The annotators discussed the remaining disagreements, i.e. cases which could not be settled by majority labels (10 % of all tagged feedback expressions) and resolved them manually. We discuss the procedure and its implications for interpretation of feedback functions in greater detail in Sect. 6.1 . 4.2 The storyteller Twenty sessions not involving a distraction task contain storyteller X  X  speech annotations. In addition, annotators delimited the following rhythmic phenomena in storyteller speech: vowel-to-vowel intervals, rhythmic prominence intervals and minor intonational phrases (Breen et al. 2012 ). First, vowel onsets were extracted semi-automatically from the data. Algorithms in Praat (Barbosa 2006 ) were used first, after which two annotators checked the resulting segmentation for accuracy by inspecting the spectrogram, formants and pitch curve in Praat. Rhythmic promi-nences e.g. perceptually salient syllables were judged perceptually. Whenever an annotator perceived a  X  X eat X  on a given syllable, the syllable was marked as prominent, regardless of lexical or stress placement rules (Breen et al. 2012 ). Similarly, every time a perceptually discernible gap occurred in the storyteller X  X  speech, a phrase boundary was marked. The resulting minimum pause length of 60 ms is comparable to pauses between so called  X  X nterpausal units X  used in other studies (e.g. Ben  X  us  X  et al. 2011 ).

Apart from manual rhythmic segmentation, the storyteller X  X  speech was automatically segmented by forced alignment using the WebMAUS tool (Kisler et al. 2012 ). WebMAUS produces a fairly accurately aligned and multi-layered annotation on small linguistic units, e.g. in segmented data. The output of the aligner provides tiers with word and phoneme segmentation along with SAMPA transcription. All automatically aligned tiers were checked and corrected manually. The corpus contains gestural annotation of both dialogue partners (see Table 1 ). Annotators performed the segmentation and labelling in ELAN (Wittenburg et al. 2006 ) by close inspection of the muted video, stepping through the video frame-by-frame. Uninterrupted, communicative head movements were segmented as minimal annotation units. Movements resulting from inertia, slow body posture shifts, ticks, etc. were excluded from the annotation. Thus obtained head gesture units (HGUs) contain perceptually coherent, communicative head movement sequences, without perceivable gaps.

Each constituent gesture in an HGU was marked for head gesture type. The full inventory of gesture types is presented in Table 3 . We illustrate prototypical movements along particular axes in Fig. 3 , using mathematical conventions for 3D spatial coordinates and following bio-mechanical and physiological studies on head movements (Yoganandan et al. 2009 ).

The annotators identified constituent gestures in each HGU and also marked the number of gesture cycles and, where applicable, the direction of the gesture (left or right, from the perspective of the annotator). For example, the label nod-2 ? tilt-1-right describes a sequence consisting of two different movement types with two-and one cycle, respectively, where the head is tilted to the right side of the screen.
The resulting head gesture labels describe single , simple ,or complex gestural units. Single units refer to one head movement with one cycle, whereas complex HGUs denote multiple head movement types with different number of cycles. Simple head movement types consist of one movement type and at least two cycles (see Fig. 4 ). Where applicable, the annotated HGU labels also provide information about the following features: complexity (the number of subsequent gesture types in the phrase) and cycle frequencies of their component gestures. 5.1 The listener We annotated listener head gestures in 40 dialogues so far, i.e. in 20 sessions involving the distraction task and 20 sessions with no distractions. We found that listener head gesture type categories were limited to a subset of the inventory presented in Table 3 , namely to nod , shake , tilt , turn , jerk , protrusion and retraction (W X odarczak et al. 2012 ). Table 4 presents types of head gestures found for listeners in the corpus. Two annotators segmented, labelled and checked the listener HGUs for errors. 5.2 The storyteller Co-speech head gestures produced by the storyteller are much more varied than those of the listener, which necessitated addition of several other categories, such as slide , shift and bobble . Consequently, we used the full inventory in Table 3 ,as described and evaluated on a similar German spontaneous dialogue corpus by Kousidis et al. ( 2013 ). The inter-annotator agreement values found for the full inventory in Kousidis et al. ( 2013 ) equalled 77 % for event segmentation, 74 % for labelling and 79 % for duration. The annotation of storyteller X  X  head gestures has been completed in 9 conversations in the no-distraction subset so far, as the density and complexity of gestural phenomena is much greater in the storyteller than in the listener. 6.1 Verbal The listeners produced a total number of 1505 verbal feedback signals. The mean ratio of time spent producing feedback signals relative to other listener-produced turns, e.g. questions and remarks, normalised by their respective mean duration per dialogue equals 65 % (Min  X  32 % ; Max  X  100 % ), suggesting that the corpus contains a high density of spoken feedback phenomena. The mean feedback rate is 10 signals per minute, mean dialogue turn rate is 5 turns per minute, with a significantly higher turn rate in the attentive listener (6 turns/min) than in the distracted listener (4 turns/min, two-sample Wilcoxon rank sum test: p \ 0 : 01).
As the reader will recall from Sect. 4.1 , three independent annotators assigned a pragmatic function from the inventory in Table 2 to each feedback expression in the listener X  X  speech. Below we describe the observed disagreement patterns and the resolution procedure. We also consider their import on feedback function semantics.
We assess pairwise inter-annotator agreement on core verbal feedback function categories using Cohen X  X  kappa ( j ). 2 The values range from j  X  0 : 25 to j  X  0 : 43 with a mean of j  X  0 : 33 (SD  X  0 : 056; see Table 5 ). This degree of agreement is commonly regarded as  X  X air X  to  X  X oderate X  Landis and Koch 1977 , and is usually considered to be insufficient for annotation of linguistic data (cf. Reidsma and Carletta 2008 ). It should be borne in mind, however, that feedback functions are not simple surface phenomena. Instead, specific dialogue acts are an expression of listener X  X  non-observable internal states and intentions (Kopp et al. 2008 ; Buschmeier and Kopp 2012 ), which makes the annotation task difficult. Impor-tantly, comparable j -values have been reported in existing work on feedback function annotation by na X   X  ve annotators using the DIT ?? scheme (Geertzen et al. 2008 ). While expert annotators in that study reached much higher agreement, the evaluation was based on a selection of dialogues from task-oriented corpora, which are likely to constitute a much narrower and semantically simpler domain. This is especially true of human-computer dialogues which made up the majority of the test material. By contrast, storytellers in our study constructed extended narratives pertaining to their personal experiences, which in turn elicited complex emphatic responses from listeners. Needless to say, these went far beyond the type of feedback required when interacting with a question answering railway information system or discussing a route on a map (Garrod and Anderson 1987 ).

More recently, Pre  X  vot et al. ( 2015 ), reported a mean pair-wise j of 0.6 for discrimination between basic feedback functions ( contact , acknowledgement , evaluation ). However, their tagset included functions related to question answering, feedback elicitation as well as other label indicating that an item had been erroneously classified as feedback at an earlier automatic processing stage. Arguably, these categories are much easier to identify, thus inflating the obtained j values. For sub-classification of higher-order evaluative functions ( approval , expectation , amusement , confirmation / doubt ), they obtained an average agreement of 0.3, which is identical to our result. Admittedly, however, the latter sub-classification task is more difficult than ours and would roughly correspond to interpreting various meanings subsumed by categories P3 and A in the ALICO inventory.

Additionally, the presented j values were calculated using core categories obtained by stripping off any of the optional modifiers that actually formed the decision taken by the annotator. A crucial factor is the relatively subjective modifier A . It is possible that instead of committing to a core category, this modifier was attached to a core function category on occasion, especially if an interpretative overlap existed, e.g. in case of A versus P3 . Indeed, there were 164 cases in which one annotator used A as a modifier while another used it as a core category. This certainly points to a trade-off built into the inventory where interpretative flexibility given to the annotators impacts precision.

Figure 5 presents the disagreement patterns between core feedback function categories in greater detail as a confusion matrix. The matrix reveals a substantial degree of disagreement. Only for categories P2 and A did two annotators most frequently choose the same label. By contrast, if one annotator chose P1 , a second annotator most frequently chose P2 . The same holds for P3 . Given that feedback functions form a hierarchy of grounding strength, it seems that annotators tend to be drawn towards the middle category. Confusion matrices for individual annotator pairs can be found in  X  X  Appendix  X  X  (Fig. 11 ).

We tabulate the proportion of agreements on a given feedback function relative to all judgements involving this function, N ( X , X )/ N ( X , ?) in Table 6 .This measure is equivalent to the conditional probability of an annotator choosing a particular category, given that another annotator chose the same category, P (( X , X )|( X , ?)). We compare the observed proportions against a baseline proba-bility of two annotators selecting a given function category by chance, P ( X , X ), given its empirical probability based on frequency in the data, P ( X ). For all four core feedback function categories, if one annotator chose a particular category, the second annotator was more likely to choose any of the other categories. Here, P1 has the biggest likelihood (0.4) of another annotator choosing P1 as well, followed by P2 (0.39), A (0.36), and P3 (0.29). All of these values, however, are much higher than the probability of two annotators choosing the same category by chance.
To resolve the disagreements, the final SFE function labels used in subsequent analyses were determined algorithmically by calculating majority labels from individual annotations. If at least two out of three annotators agreed upon one of the core categories P1 , P2 , P3 ,or A , this category was chosen. If at least one of the annotators used a modifier ( C , E ,or A ) in addition to a core category, the modifier was carried over to the final annotation label. This ensured that surface features represented by the modifiers (e.g. position relative to the full turn, an attitudinal component) as well as subtler semantic distinctions, especially related to emotional content of utterances, were preserved. Complete disagreements on core categories, i.e. cases in which all three annotators disagreed, were discussed and resolved manually.

This resolution method results in three classes of inter-annotator agreement-quality with (1) all three annotators agreeing, (2) two annotators agreeing, and (3) all annotators disagreeing on the assigned feedback function. We present the percentages of cases in each of these categories, additionally split on feedback function category as well as category type (complex vs. core) in Fig. 6 . While disagreement was resolved predominantly by majority voting (57 % of cases for both complex and core categories), full agreement between annotators is more frequent (33 vs. 26 %) and full disagreement is less frequent (10 vs. 17 %) when modifiers are stripped from listener X  X  feedback function labels. For this reason, core categories were used in all subsequent analyses on ALICO. Concerning individual functions, all three annotators unanimously assigned P1 and P2 somewhat more frequently than P3 and A , suggesting that lower-level functions may be less ambiguous than higher-level ones. P3 in particular, showed a higher proportion of complete disagreements, which required manual intervention (12 % for P3 against 8 % in both P1 and P2 ).

In Fig. 7 , we further explore cases where the final label had to be resolved by majority voting. Since in these cases, two out of three annotators agreed on the assigned feedback function, the figure presents percentage-wise distribution of dissenting annotations given a majority category. The results are in line with the hierarchical dependencies between feedback functions implicit in our inventory. In particular, disagreements most likely involved neighbouring categories: P2 for the P1 and P3 majority labels and both P1 and P3 for the middle P2 majority label.
The observed disagreement is also consistent with the existence of entailment relations between preconditions of communicative functions (Bunt 2007 ; W X odar-czak et al. 2010 ). In our case,  X  X  X cceptance X  X , P3 , logically implies  X  X  X nderstanding X  X , P2 , which in turn implies  X  X  X erception X  X , P1 . Figure 7 reflects a similar tendency in that the most common minority category is logically entailed by the majority vote: P3 ) P2 and, to a lesser extent, P2 ) P1 . Annotators are thus more likely to assign feedback functions with fewer rather than more preconditions, possibly because they either do not consider all higher-level function preconditions to be fully satisfied or because they choose a conservative strategy by favouring safe decisions.

We conclude the overview of listener X  X  verbal feedback in ALICO by considering the relation of particular feedback expressions to their function interpretation. We present the most frequent German SFEs found in the corpus and their corresponding core feedback functions in Table 7 . Each SFE lexical type listed, subsumes several variations that we interpret to reflect the same lexical form. For example, the lexical type  X  X a X  encompasses forms such as  X  X ja X ,  X  X a ja X ,  X  X h ja X  or  X  X a ja ja X , where the constituent  X  X a X  is repeated in quick succession. Similarly, the type  X  X lar X  ( sure ) includes also  X  X a klar X  or  X  X a klar X . Values listed under  X  X ch X , apart from the exclamation usually transcribed as /ax/, likewise represent all phonetic forms ending with an open, low vowel, e.g. transcribed as  X  X h X  or  X  X ah X  but not subsumed under the bisyllabic  X  X ha X  category, etc. We classify other SFEs found in the corpus into three semantic cores for the purposes of Table 7 , namely certainty (e.g.  X  X enau X ,  X  X timmt X ,  X  X ichtig X ), excitement/enthusiasm (e.g.  X  X ow X ,  X  X lasse X ,  X  X oll X ) and surprise/incredulity (e.g.  X  X cht? X ,  X  X rass X ,  X  X oah X ). Table 14 in the Appendix provides a full list and glossary of these expressions.

From Table 7 it can be readily appreciated that some feedback expressions are used with certain functions more frequently than with others (on a related point see also Gravano et al. 2007 ). The four most frequent German SFEs are  X  X a X ,  X  X  X ,  X  X hm X  and  X  X kay X , with  X  X a X  apparently exhibiting the most unspecific character in terms of feedback functions it can be associated with. The SFEs  X  X  X  and  X  X hm X  in turn are used infrequently as expressions of agreement/acceptance and affective displays while occurring relatively often in what can be generalised as a backchanneling capacity ( P1 and P2 ). We find fairly good correlates of understanding ( P2 ) in feedback forms such as  X  X kay X ,  X  X ch so X  and other short exclamations in Table 7 . Note that these SFEs are also potentially good  X  X ews markers X  as defined by Gardner ( 2001 ), where new information introduced by the partner into the discourse is signalled as received. The few specific lexical tokens listed in Table 7 and the Appendix under excitement/enthusiasm , certainty , surprise/ incredulity predictably cluster within the categories P3 (especially words express-ing certainty ) and A (especially enthusiastic, surprised and disapproving attitudes). Malisz et al. ( 2012 ) also investigated the relation between the most common SFEs,  X  X  X ,  X  X hm X  and  X  X a X  and their function in a subset of 28 dialogues. The analysis revealed that  X  X a X  best classifies higher-level functions compared to P1 . The opposite was true of  X  X hm X  and no statistically significant relation was found for  X  X . X  Additionally, feedback functions were linked to several prosodic features in Malisz et al. ( 2012 ): feedback expressions communicating lower-levels of feedback were characterised by shorter durations, lower intensity, less pitch variation in their middle part and more pitch variation towards the end of the expression. 6.2 Visual Non-verbal behaviour is an important source of information in the study of active listening. Head gestures, produced by the listener conjointly with or separately from spoken feedback expressions, significantly contribute to feedback in dialogue (Wagner et al. 2014 ). In fact, the number of visual feedback expressions found in ALICO exceeds the number of verbal expressions: the ratio of head gesture units ( N  X  2598) to SFEs ( N  X  1505, including bimodals) equals 1.7 in 40 dialogues.
In the present section, we concentrate on a subset of 20 ALICO dialogues with complete gestural annotations for listeners not engaged in a distraction task. We describe head gesture units with respect to their modality, that is whether they overlap with SFEs or constitute  X  X ure X  visual feedback without co-occurring speech (see Fig. 8 for an illustration). We also differentiate among HGU structures: simple, where only one gesture movement type occurs versus complex. We additionally look at HGU cyclicity, where single repetitions of the same movement are juxtaposed with those of N [ 1 repetitions (Fig. 4 ). The dependencies of complex HGUs with overlapping speech are discussed in greater detail in the following section.

Table 8 lists the relative and absolute frequencies of specific head movement categories for this subset. We found a total of 1003 HGUs, with more instances of pure visual feedback (62 %, N  X  619) compared to bimodal feedback (38 %, N  X  384). Nodding constitutes the most frequent movement category in attentive gestural feedback (78.4 %, N  X  786). In contrast, the next two most frequent head movement types occur fewer than forty times each (for jerk N  X  38, for tilt N  X  33). In single head movements, listeners produced twice as many multiple as single HGUs (72 vs. 28 %). In accordance with previous outcomes (W X odarczak et al. 2012 ), we found that the multiple nod is used more often than single nod labels (78 vs. 22 %). In general, labels nod-2 ( N  X  248), nod-3 ( N  X  154) and nod-4 ( N  X  70) constitute the majority (60 %) of all nod labels, with nod-2 being the most frequent nodding gesture (31.5 %). The higher relative prevalence of polycyclical head movements in listeners, especially multiple nods, was also evidenced in other studies and languages, e.g. in English (Hadar et al. 1985 ), in Swedish (Allwood et al. 2007 ) and in Japanese (Ishi et al. 2014 ).

The investigation of visual feedback in ALICO also shows a strong tendency for listeners to produce simple head movements ( N  X  901) relative to complex head gestures ( N  X  102), that is, listeners generally disprefer stringing several head movement types within one head gesture unit.

We also provide an analogous summary of visual feedback frequencies for distracted listeners in Table 9 . The table indicates that distracted listeners, while also generally preferring simple gestures, produce complex gestures relatively more frequently than attentive listeners (20 vs. 10 %). Additionally, a greater proportion of these complex gestures is realised in the purely visual modality rather than as part of a bimodal feedback expression. We discuss further details of an analysis of multimodal feedback in distracted listeners in Sect. 6.4 . 6.3 Bimodal Given the availability of verbal and visual annotations, ALICO facilitates studying mutual influences between the two communicative modalities. Above all, we investigate the possibility that those listener head movements that overlap with spoken feedback are semantically and temporally constrained by the verbal expression (see also Wagner et al. 2014 for a discussion of speech-gesture interdependencies).
We again turn to attentive listeners in ALICO who produced 384 bimodal expressions, that is 38 % of the head gesture units in 20 dialogues. The majority of complex HGUs, that is those containing at least two distinct head gesture types one after another, occur while in overlap with a verbal SFE. This relation is presented in Fig. 9 .A v 2 -test was applied to investigate whether the relationship between modality and HGU complexity is independent. The test revealed a statistically significant relationship ( v 2  X  22 : 7, p \ 0 : 001) between the internal complexity of an HGU and whether it overlaps with an SFE. In other words, more complex gestures, e.g. beyond the most frequent multiple nod class, are more likely to accompany verbal feedback, possibly achieving means to express relatively more specific and complementary meanings in concert.

As in W X odarczak et al. ( 2012 ), we first assume that the feedback function assigned to listener X  X  verbal expression in bimodal feedback defines the function of the overlapping head movement. That is, e.g. a double nod overlapping with a short feedback expression bearing the majority label P1 ( X  X  X erception X  X ) likewise expresses perception.

Table 10 presents the frequency of listener X  X  HGU types across feedback function categories assigned to SFEs the head gestures overlap with. The results show that the most frequent and  X  X eneric X  gestural response, i.e. the nod, happens almost equally often in overlap with P1 as P2 , less so with P3 . The jerk , similarly to results in W X odarczak et al. ( 2012 ), seems to correlate with verbal feedback expressing  X  X nderstanding X , likewise complex HGU gestures overlapping with SFEs. The label tilt is characteristic for higher level functions, increasing in frequency from P2 through to A . Summarising, nodding almost single-handedly performs in the backchanneling function P1 , while other frequent head gestures such as jerk and tilt correlate with higher feedback categories.

The assumption that the function of head gestures is identical with the overlapping SFE functional category is most likely only partially viable, since communicative gestures can add complementary information to the co-occurring verbal expression, as well as carry redundant meaning (Goldin-Meadow et al. 1993 ; Bergmann and Kopp 2006 ), not to mention attitudinal and affective nuancing.
Consequently, Skubisz ( 2014 ) asked if communicative feedback functions of head gestures could be identified independently from speech. She applied the feedback function inventory from Buschmeier et al. ( 2011 ) in the non-verbal modality. Ten one-minute long clips from different ALICO dialogues involving an attentive listener were used in a rating task. Clips with at least 10 HGUs and the smallest number of listener X  X  verbal interruptions were selected. The resulting excerpts contained 159 listener X  X  HGUs and exhibited similar distributions of visual (69 %) and bimodal feedback (31 %), single (40 %), multiple (60 %) and complex gestures as the ones found in the whole ALICO corpus (compare Table 8 ). For instance, nods amounted to 80.5 % of all gestures in the test sample vs. 78 % in ALICO. For jerk the respective proportions were 7 and 3 %, and for tilt equalled 3.2 % in both datasets, yielding a comparable set of gestures types to be rated.
Participants assigned three positive feedback functions, namely, perception ( P1 ), understanding ( P2 ), and agreement ( P3 ) to the head movement gestures of the listener by inspecting the video clips carefully. Modifier categories listed in Table 2 were not available to the raters in this study. The video material was presented without the sound track to avoid the impact of verbal feedback expressions on the annotators X  judgement.

Agreement among 11 annotators was first estimated by Fleiss X  j for 11 raters. The resulting value shows low agreement among all the raters: j  X  0 : 22. Skubisz ( 2014 ) further analysed the level of agreement in this task by calculating percentage agreement on feedback function ratings per head movement type. Table 12 presents majority based categories determined by a vote of at least six raters and their relative frequencies per gesture type. Head gesture types that appeared at least eight times in the watched and rated clips of the listener are shown in Table 11 . The results revealed that the majority of raters could not agree on the function of ca. 11 % of all HGUs. The majority agreement has shown that nods were mostly assigned to the P1 and P2 -function. It seems that only jerk elicited a fair amount of classifications (27 %) into the  X  X cceptance X  category ( P3 ) with most instances agreed upon for P2 .

The results of the first study on independent feedback function assignment to head gestures that we are aware of indicate difficulties in judging the meaning of the movement, at least with the scheme proposed in Table 2 . The scheme was originally developed for spoken feedback. A restricted number of categories was used in the HGU rating task, the inter-rater agreement values are nonetheless low. It was possible however, to establish majority labels for specific head gesture types where up to 90 % of the most frequent tokens were classified. The patterns observed follow in part the conclusions on the functional interpretation of bimodals, e.g. especially in case of the jerk label, classified as  X  X nderstanding X .

Linguistic constraints on movement may also manifest themselves in the location of the head gesture onset relative to the verbal expression and the relative duration of the two components. Studies have shown that the onset of communicative manual gestures tends to precede their lexical affiliate by a few hundred milliseconds (Kendon 1980 ; Schegloff 1984 ; Nobe 2000 ; de Ruiter 2000 ). In case of multimodal feedback, the SFE corresponds to the lexical affiliate that the head movement overlaps with. In W X odarczak et al. ( 2012 ), the median distance from the head movement onset to the overlapping verbal expression onset, i.e. the lead of the head gesture, equalled 220 ms.
In the present study, based on a superset of the data analysed in W X odarczak et al. ( 2012 ), the gesture onset predominantly precedes listener X  X  verbal expression by a median of 190 ms (min. lead =-1920 ms, max. lag = 400 ms; cf. Dittmann and Llewellyn 1968 ). We plot the distribution of head movement onset in bimodal feedback in Fig. 10 for the dialogue sessions with no listener distraction. The HGU onset is presented relative to the onset of the verbal element located at t  X  0 s, with positive values signifying gesture lag and negative values signifying gesture lead. We differentiate the data according to whether the bimodal expression in question involves a simple or a complex HGU. The figure suggests that simple head gestures are more tightly synchronised with their verbal affiliate (median HGU lead = -0.18 s) than complex gestures (median HGU lead = -0.26 s). The difference in the complex head gesture lead compared to the simple head gesture lead is statistically significant (Wilcoxon rank sum test, p \ 0 : 01).

Morrel-Samuels and Krauss ( 1992 ) found that the degree by which gesture precedes speech, as well as the duration of the gesture, appeared to be a function of how familiar the lexical affiliate was to the speaker. SFEs in German generally involve a lexically and phonologically restricted set of forms (see Table 7 ) that are used repeatedly (but probably not randomly) in the course of a dialogue. These forms are therefore readily available in terms of lexical access. Head movements expressing feedback on the other hand, predominantly involve simple bi-phasic movements such as nods, jerks and tilts. Given the two facts, the short search times for feedback representations (especially in backchanneling) and the uncomplicated motoric programmes of most communicative head gestures, a close temporal coordination between head gestures and their verbal affiliates can be expected. The shorter lead for simple gestures found in the present study confirms this mechanism, while the fronting of complex overlapping gestures suggests that coordination of bimodal expressions might be planned.

We further look at the dependence of listener X  X  HGU duration on the number of movement cycles it contains, and the movement complexity it exhibits. We add a factor of modality to test for the possible effect of the overlapping verbal expression on head gesture unit duration. In order to account for all these factors, a Linear Mixed Model was formulated with the abovementioned independent variables, as well as log(HGU duration) as the dependent variable. The exact head movement label and dialogue ID were entered as random factors. We report the model coefficients and p values here that resulted from the best model fit, the quality was determined by log-likelihood.

The model coefficients show that the duration of head gesture feedback significantly depends on movement cyclicity. The patterns are predictable: multiple gestures (e.g. nod-10 ) are longer than complex gestures (e.g. nod-2 ? jerk-1 )( b  X  0 : 48, p \ 0 : 001), while single gestures (e.g. nod-1 ) are shorter than complex gestures ( b  X  0 : 627, p \ 0 : 001). The interaction between modality and cyclicity was not significant, meaning that there is no interdependent impact of the two factors on head gesture feedback duration. The listener however, is significantly influenced by feedback modality in that visual gestures are produced in less time than bimodal gestures ( b  X  0 : 11, p \ 0 : 001).
Finally, Inden et al. ( 2013 ) reported on timing of multimodal feedback in ALICO based on a study implemented in an artificial agent. The results indicate that listeners distribute head gestures uniformly across the interlocutor X  X  utterances, while the probability of verbal and bimodal feedback increases sharply towards the end of the storyteller X  X  turn and into the following pause. While the latter hypothesis is well established, the former was not strongly attested in the literature: the specific nature of the conversational situation in ALICO, specifically concentrated on active listening, provided a sufficiently constrained setting, and revealed the function of the visual modality in this discourse context. 6.4 Feedback in distracted listeners One of the fundamental design decisions behind ALICO involved introducing an ancillary task with a view to studying variation in feedback produced by distracted listeners. Thus, studies of ALICO complement earlier results which demonstrated the effect a distracted listener has on the speaker (Bavelas et al. 2000 ; Kuhlen and Brennan 2010 ) but omitted changes in the listener X  X  behaviour itself.

Analyses so far show that distracted listeners produce less feedback overall. In particular, a decreased rate of feedback communicating understanding (i.e. P2 ) was consistently associated with distractedness (Buschmeier et al. 2011 ; W X odarczak et al. 2012 ), which can be interpreted either in terms of listeners X  inability to retrieve semantic content of the message in conditions of attention deficits or an intentional strategy aimed at avoiding confusion caused by explicitly feigning understanding. Notably, distracted listeners continue to produce reactive feedback , possibly based on shallow processing of the utterance surface features, as well as communicate acceptance of the interlocutor X  X  message, thereby conveying implied understanding.

Subsequently, changes in prosodic realisation of SFEs in distracted and non-distracted listeners were investigated in Malisz et al. ( 2012 ). Significant differences were found in the intensity and pitch domains: feedback expressions produced by distracted listeners were on average quieter and showed less variability in F 0 . At the same time, SFEs in the distracted condition varied more in intensity. In addition, some of the features were sensitive to segmental realisation of specific feedback expression.
Regarding the interaction between modalities and feedback functions in the corpus, W X odarczak et al. ( 2012 ) found that in HGUs overlapping with verbal feedback expressions nod s, especially multiple ones, predominate. However, the tilt was found to be more characteristic of higher feedback categories in general, while the jerk was found to express understanding. A significant variation shown in the use of the jerk , between distracted and attentive listeners W X odarczak et al. 2012 is in accordance with the previous result in Buschmeier et al. ( 2011 ). Hitherto ALICO provided two converging sources of evidence confirming the hypothesis that communicating understanding is a marker of attentiveness.

Furthermore, the ratio of non-verbal to verbal feedback significantly increases in the distracted condition, suggesting that distracted listeners choose a more basic modality of expressing feedback, i.e. with head gestures, rather than verbally (W X odarczak et al. 2012 ). Information in Table 9 also suggests that complex head gesturing is avoided when providing feedback concurrent with an SFE, unlike in the attentive listener. The Active Listening Corpus offers an opportunity to study multimodal and cognitive phenomena that characterise listeners in spontaneous dialogue and to observe mutual influences between dialogue partners. It includes extensive and detailed annotations of dialogue partners X  verbal and head movement behaviour: from segmental transcription and local timing phenomena to entire talkspurts and gestural units. In addition, corpus design based on a storytelling scenario makes ALICO a particularly rich resource for both fundamental and applied research on communicative feedback across modalities. An inventory of feedback functions, based on existing standards, was used to assign pragmatic functions to short feedback expressions in a multi-annotator setup. Subsequent analyses revealed a number of regularities between feedback functions and their realisations, pertaining, among others, to their lexical form, prosodic and temporal properties, as well as the structure of accompanying head gestures. Finally, the inclusion of a distraction task allowed to identify decreased attention effects on surface feedback features.

The results outlined in the previous sections highlight ALICO X  X  relevance for both fundamental and applied research. On the one hand, they provide an insight into the inner workings of such fundamental mechanisms in interpersonal communication as grounding, communicating agreement and expressing involve-ment. On the other hand, they pave the way towards more human-like and sociable dialogue systems. Above all, however, they underscore the importance of studying communicative feedback and, more generally, listener-specific behaviours, in spontaneous, naturalistic conversation.

Work on additional tiers containing lexical, morphological information, turn segmentations and further prosodic labels is ongoing and the annotations are being continuously updated. A corpus extension is planned with recordings using motion capture and gaze tracking available in the MIntLab (Kousidis et al. 2012 ).

Due to legal restrictions on dissemination of multimodal data, we are unable to make the audio and video recordings accessible to third parties. However, secondary ALICO data, such as transcriptions, annotations or extracted prosodic profiles of feedback expressions are available on request. They provide information on temporal and prosodic organisation and feedback signal frequency in both verbal and non-verbal domains, as well as on interdependencies between modalities. Since this information relies exclusively on surface forms, annotation reliability poses no significant problem even without access to the signal itself. While reliability is more of an issue for the functional classification, the annotations are available as well.
 See Fig. 11 and Tables 13 , 14 . References
