
New approaches to managing flood events are increasingly of more relevance due to recent widespread floods and the presumed changes in the climate. These approaches fall under the integrated flood management (IFM) banner and focus not only on flood prevention, but on flood resilience. This pap er introduces an application (FLORETO) for IFM that utilizes the data mining approach, in a web based three tier system, devoted to the capacity building of stakeholders as a micro-scale resilience strategy o f IFM. The intelligent models, which constitute the business logic in FLORETO, are used to match the input parameters or design criteria, describing properties prone to flooding, to technically justif iable flood mitigation measures. Datasets from the German city of Kellinghusen were collected and intelligent models were built. Satisfactory results have been obtained, which shows the promise of this data mini ng approach and opens the door for its application for IFM in other regions. Over the past years the world has witnessed a growi ng probability and extent of floods, posing a risk to health and wellbeing and causing huge damage to properties and personal belongings. This problem gains even more importance since the climate change favours extreme weather situations and flooding becomes increasingly likely. The recent flood events (Europ e, 2006, New Orleans, 200, Asia 2004) showed that the existing conventional flood protection systems do n ot guarantee sufficient protection level of people and properties. Those conventional strategies, such as dikes and walls, that manage the risk of flooding by mitigating the probability of flooding, are very co st intensive and the financial resources usually can n ot be provided ad-hoc. Also, improving (raising) dikes causes restrictions of the natural dynamics of a ri ver system and spoils the landscape qualities. New strategies to cope with flooding have to be develop ed and implemented to adapt the communities to climate change in an adequate way. 
The governmental institutions worldwide are trying to redefine their flood policies. In Europe, the pr oposal for the  X  X irective of the European Parliament and o f the council on the assessment and management of floods X  {SEC(2006) 66} was released. This Directive extends the objectives of the Water Framework 
Directive (WDF), promoting coherent and interdisciplinary cross border, catchments based approach in flood management. Driven by the EU initiative for integrated flood management, many countries adapted or released new water policies to cope with floods. For example, in Germany those regulations are brought forward within the Flood Control Act (FCA). [1] 
These new integrative strategies present a new approach to manage the risk of flooding. Instead of mitigating probability of flooding, the preference is given to strategies that aim at mitigating their im pact. 
Those strategies are called flood resilience strate gies or non-structural flood mitigation (table 1) and are a substantial part of the Integrated Flood Management (IFM). IFM covers overall activities for flood mitigation related to Flood Probability Assessment, Flood Impact Assessment, Non-Structural Flood Mitigation and Flood Risk Management Planning. 
The term resilience originates from ecology and can be defined as the ability of a system to resist the perturbation or the speed at which the system recov ers after being disturbed. The underlying idea behind f lood resilience management is to foster the ability of a reas to recover after they had been prone to flood. It impl ies that in a flood resilience policy flood damages hav e to be minimized and normal life has to return as soon as possible after flood. [2].

This paradigm shift in flood management from  X  X ighting flood X  to  X  X iving with floods X  triggers a demand for more interaction among the experts of different fields such as spatial planning, social s ciences, structural engineering or geology, but also for the active involvement of the public in integrated floo d management. The public is not a passive receptor an y more, it becomes an important stakeholder of flood management and has to be actively involved in decis ion making as decisions made influence the way of livin g in flood prone areas. But also, the citizens in flo od prone areas can reduce the flood risk of their prop erties by protecting them in an adequate way. 
Capacity building of stakeholders is a resilience strategy devoted to promote the importance of the concept of  X  X iving with floods X  on the micro scale level, focusing on individual flood mitigation meas ures and increasing public awareness of flood hazard so that the people are able to adjust their homes by applyi ng appropriate flood mitigation measures. 
The mitigation measures applied to the existing buildings with the aim to reduce flood damage potential can be summarized under the term retrofitting. Retrofitting can be defined as [a combination of adjustments or addition to features of existing structures, that are intended to eliminate or reduce the possibility of flood damage [3]. By appl ying flood retrofitting measures, the resilience of the buildings can be improved either by preventing floo d water entering the building or by applying waterpro of materials and elevating the inventory above the expected flood level. The main resilience strategie s for the existing buildings are dry proofing and wet proofing [4], often combined with using of easily removable inventory in lower parts of the building. The main flood retrofitting strategies are depicted in table 2. 
In the case of dry proofing, floodwater is kept out of the building. Those techniques are generally more c ost-intensive than the wet proofing ones and always car ry a certain failure risk, as the stability of the build ing can be jeopardized through the increased water pressure . 
By applying wet proofing measures floodwater is not prevented from entering the building, but the building is adapted to flooding in a way that poten tial damage is reduced by applying water resistant build ing material or adapting the occupancy of the building (e.g. basement is not used for living). 
Those strategies have some shortcomings when applied solely. Most of the deficiencies are relate d to the limitations of the current construction technol ogies, design and materials, that are, to lesser or greate r extent, subjected to the impact from flood. An overview of the main characteristics of flood retrofitting strategies is depicted in table 3. Bef ore any decision is made, a selection of technically justif iable and logical solutions is to be performed based on t he following general design criteria: a) building type (description of fabric and inventory) b) distance to the water bodies c) availability of resources, d) logistical parameters, e) parameters describing the flood event, f) stability issue g) (owner description) 
Additionally, an important issue of a capacity building system is how to reach stakeholders and in which form they should get required knowledge on flood mitigation issues. 
Having recognized this importance of the active involvement of the stakeholders in flood management , many environmental agencies or governmental institutions publish material or offer consultancy to inform citizens about possible mitigation measures for their homes. Taking advantage of the internet as a powerful communication media, a considerable part o f provided information is available via internet and is usually presented in the form of pdf files or html text.A list of examined web sites is given in www.tuhh.de/wb/floreto . strategy measure main characteristics techniques available Dryproofing 
Wetproofing Web sites usually provide the public with the information whom to contact or who carries the responsibility for certain flood related issues. So me of the websites go a step forward and try to use animation tools such as simulations of a building getting flooded. Those animations are supported by, in some cases, detailed explanations of the mitigat ion measures that can be applied. But a capacity building system that would integrate both, hazard awareness of stakeholders and appropriate mitigation measures for each single cas e on the microscale level, is not available 
FLORETO is a system for capacity building of stakeholders in form of a web based advisory tool o n a microscale level tailored to the user X  X  own prope rty data that at the same time: -Enables determination of suitable retrofitting measure(s) which improve flood resilience of single households and consequently of the whole area. As a final output of the system, the user gets different flood retrofitting scenarios for his own property, based on the cost benefit analysis (CBA). -Improves flood hazard awareness of the stakeholders by providing them with the information related to flood management and helps them understanding the main terms and concepts relevant for flood protection and mitigation. -Facilitates decision making on the microscale lev el and enables continuous exchange of information between the experts and the stakeholders by giving the stakeholders enough information to make their decisions and at the same time retrieving the feed back of the preferable solutions and use it as an i nput for the spatial planning and further flood manageme nt decision making process. 
FLORETO targets the public (especially the population potentially affected by flood) as well a s the experts and decision makers e.g. local authorit ies. 
Development of FLORETO was initiated within the EU funded FLOWS project ( www.flows.nu ), that focuses on finding innovative flood management solutions for an adequate response of the communities to the climate change, considering both , technical and social aspects. 
The system has a client/server architecture consisting of three well-defined and separate processes (three-tier design), each running on a different platform, as depicted in figure 1 (link: http://floreto.wb.tu-harburg.de available from October 2006, contact address: natasa.manojlovic@tuhh.de) 
The three-tier design has many advantages, the chief one being the modularity of such a system, enabling modification and replacement of one tier without affecting the other ones. The tiers discuss in more detail are as follows: 1) User interface (UI), which runs on the user's computer with the browser (the client) 2) The functional (business logic) module . This is the brain of FLORETO, it matches users inputs representing the design criteria of their propertie s to the food mitigation measures that protect them in t he event of a flood. The selected technically justifia ble measures, are processed and evaluated to yield a th e cost benefit analysis (CBA). As benefit, the damage reduction in case of the selected combination of measures is defined. Costs include all expenses for applying the selected measure (material, labour and maintenance costs). 3) A database management system (DBMS) that stores the data required by the middle tier. This t ier runs on a second server called the database server. The database used for FloReTo design is an open source database system-MySQL. 
The database is optimized for different input modules, for users and for the experts (data as res ults for polls, interviews etc) The heart of the cost benefit analysis (CBA) for FLORETO is the matching of the input parameters describing the stakeholders X  property to the measur es that must be implemented in order to protect the properties in the event of a flood. 
The input parameters consist of vectors of categorical and/or numerical attributes, which constitute the design criteria describing the prope rty and the flood water parameters of the specific location. The input parameters are further represen ted as X with the complete set of all possible design criteria represented as  X  X , and the cardinality X being the number of attributes of the input parameters. For the example of the final Kellinghus en location dataset used for the experiments in this 
The class parameter is made up of technically justifiable mitigation measures that are needed to protect the property in the event of a flood. The s et possible measures. 
Matching from the design criteria or input parameters to measures is realized with a mapping function M defined below: 
There are many possible matching functions M , the challenge and task of this paper is to find a mitigation measures for the stakeholders. Different alternatives for realizing optimal M are now explored. 
The mapping of input parameters to the technically justifiable mitigation measures is a knowledge intensive process. Two alternatives to realize this where considered. The first approach is based on eliciting and elucidating the knowledge of flood management experts into rules that are then incorporated into an expert system. The second approach is a data mining approach using computational intelligence models that learn from data to derive the rules or patterns needed for the matching function. 
Expert systems are used to solve knowledge intensive technical problems The first successful deployment of expert systems happened about four decades ago with the development at Stanford of DENDRAL by Lederberg, Feigenbaum, Buchanan et al.; and MYCIN developed by Shortliffe [5]. Since then they have been widely and successfully deploye d in various domains ranging from medical diagnosis t o chemical spectroscopy, to oil exploration and configuration of computer components etc [6]. 
Irrespective of the complexity of the implementation, each expert system consists of 3 major parts i.e. the user interface, the knowledge base and the inference engine. 
The FLORETO framework readily maps to these three components of an expert system. The data laye r of FLORETO maps to the physical storage of the rules set or the knowledge base of an expert system . The Business Logic layer of FLORETO maps to the inference engine which matches users input parameters to the appropriate measures. The representation used in the knowledge base also belongs to the business logic layer of FLORETO. The equivalent of an expert system X  X  user interface in FLORETO is provided by the web client used for interaction with end users to collect input paramet ers, display the flood mitigation measures and other recommendations from the system. 
Despite this obviously appropriate and applicable mapping of the matching function of FLORETO to the basic components of an expert system and despit e the success of expert systems for solving knowledge intensive technical problems, some major shortcomings limit the realization of FLORETO X  X  matching function as an expert system. 3.1.1. The Knowledge Acquisition Bottleneck. This is a well studied and well documented bottleneck of expert systems. It consists of the knowledge elicitation problem and the knowledge representatio n problem [7], [8], [9]. The Knowledge Elicitation Problem: To build the knowledge base for expert systems, first genuine experts need to be identified and then they must be available and willing to share their expertise with the knowledge engineer. This is not always possible. When they are willing they are often unable to easi ly and accurately and explicitly express their implici t knowledge as rules for a machine. Sometimes experts have conflicting opinions about aspects of their knowledge. All of these define the challenge of eliciting knowledge from experts. The process is often very laborious and time-consuming, for example MYCIN with only 400 rules required 100 man hours for its development. Knowledge Representation Problem: The knowledge elicited from the experts has to be translated into a form that can be stored in and understood by the machine. The transcribed knowledge must then be validated for inconsistencie s. This process is inherently uncertain due to leak of knowledge in the transcription from experts to machine form. There is no guarantee that the knowledge elicited is exactly the same as that transferred into the system. Many expert systems store knowledge as production rules using predicate logic, but experts do not always think in rigid cri sp logic. So to minimize knowledge leak the style of t he expert needs to be adapted to the knowledge representation method or the knowledge representation must be selected to accommodate the expert X  X  style of eliciting knowledge. Different methods used for knowledge representation have included rules in crisp and fuzzy logic, frames, semantic networks, belief networks, decision trees, exemplars and objects. 3.1.2. The Robust learning limitation Bottleneck. An expert system is only as good as its knowledge base. It has no way of learning new concepts by its elf. In order to update its knowledge base, the same experts, or where they are not available new expert s, must be approached; the latest knowledge of the experts must be elicited again and transcribed into the machine. Every time confronted with the knowledge elicitation bottleneck. This is a considerable disadvantage in a domain where the initial knowledg e is incomplete or the knowledge turnover is high. Th e novelty of the FLORETO platform addressing problems in the infant but rapidly developing IFM domain implies that the knowledge turnover will be rather high. Finding the right experts, with knowle dge on the design criteria for properties in a particul ar area and the right measures to protect these properties, is not guaranteed for every adaptation of FLORETO to that particular area. Hence the expert system or rule based approach is not the right one for FLORETO, considering that also in the absence of experts it might still be possible to collect histo rical data of flood events. 
The disadvantages, described above, of using an expert system for the FLORETO platform can be mitigated by using computational intelligence (CI) models in a data mining process to construct the optimal mapping function for matching the input parameters describing the design criteria of the properties to the damage mitigating measures in the event of a flood. This approach incorporates the knowledge of experts in the constructed CI model bu t overcomes the knowledge acquisition bottleneck by supplementing any available expert knowledge with more objective knowledge extracted from databases. This process of knowledge discovery from databases [10] with the use of CI models also overcomes the robust learning problem, because the models are abl e to autonomously learn new concepts and patterns when presented with new datasets containing the design criteria of the properties and measures that were used in previous flood events. 
The learning theory [11] presupposes that by training a CI model with a representative sample of generalize to unseen members of the input space optimally at predicting appropriate measures: 
This assumption in expression (2) is at the heart o f building intelligent models with training data in t he data mining process and later validating them with test sets which were previously unseen by the model s. The $performance of the trained models on the test set is then extrapolated to real life scenarios whi ch are also input parameters that were previously unseen b y the models. 
A CI model encompasses algorithms and learning methods from the fields of artificial neural networ ks (ANN), fuzzy logic computing, evolutionary computing and machine learning [12]. 
Scores of CI models have been developed and figure 2 lists some of these, especially ANN and machine learning models which are more widely used for such mapping as in equation (1). Deciding on which model to use is a major technical challenge (i.e. the model selection problem) which is still b eing studied in the machine learning and the computation al intelligence community. One approach is to pre sele ct different models based on the experience and expertise at hand and to train each of these models with the training dataset. All trained pre-selected models are then applied on the validation dataset a nd the model with the most satisfactory performance here is chosen for deployment in the real life application scenario. The performance measure could be the ability to correctly predict the class or in this particular IFM domain, the measures of the validati on set. Other constraints which can influence the mode l selection are the clarity or comprehensibility of t he results from the models, the compactness of the knowledge representation in the models etc. 
The data mining process involves many steps [13] such as understanding the domain and specification of the problem, preparation of data, application of computational intelligence models on the data, and analysis of the results. The domain and the specification of the matching problem have already been described above, now we proceed with the process of acquiring the data. 
Data acquisition represents one of the most delicate tasks of decision making in microscale flo od management. Required data, that are referred to as design criteria, is usually not available beforehan d and has to be acquired either by on-site acquisitio n done by the experts or by interviewing the affected citizens. Acquiring data in such a manner requires considerable effort and soft skills to overcome any subjective perception of flood data. The main problems of this data acquisition are: 1. difficulty to reach people-some people reject 2. reliability of acquired data 3. legal aspects-the way data is collected must not 
Figure 3 Flooding of Kellinghusen, February, 
Data from the German city of Kellinghusen has been collected and used for the case study in this paper wherein data mining approach was used to develop a tool for flood mitigation strategies on t he micro-scale level. Kellinghusen is located in the state of Schleswig-Holstein, Germany (population 8.024 -30. June 2005). Due to its proximity to the river St X r and the given hydraulic and hydrologic conditions, the urba n fabric of the city has always been prone to floodin g. Recent flood events in 1995, 1997, and especially i n February 2002, showed the vulnerability of the existing flood defense strategies. 
The flood event of 2002, when the river St X r overtopped its banks reaching the water level of 3, 66 m above the sea level, statistically corresponds to the event that occurs once in 30 years (HQ30). It sever ely affected more than 100 properties, mostly in the ol d city quarters. The impact of river flooding was followed by the groundwater seepage through unprotected basements, resulting in further increas e in damage. After the flood event, a thorough study was performed by the Department of River and Coastal Engineering, TUHH, with the aim to assess the structure of the urban fabric and direct damage to it. This dataset is referred to as the initial dataset of Kellinghusen. The analysis of data mining test resu lts is given in table 5. It is limited only to the prop erties affected by the HQ 30 that does not fully cover the impact in case of any higher flood event (e.g. HQ 100). Considering the area (i.e. the properties) th at would be affected in case of HQ100 is an important planning step for development of an appropriate flo od mitigation strategy on microscale level. Nr. Description Type 1 Basement-wall-External-Base Categorical 2 Basement-wall-Internal-Base Categorical 3 Basement-wall-Internal-Coating Categorical 4 Basement-wall-Internal-Wall-5 Basement-Floor-Floor base Categorical 6 Basement-Floor-Covering Categorical 7 Basement-doors Categorical 8 Basement-Inventory-Movable-9 Services-Sewerage-System-in-10 Services-Electrical-Appliances-11 Services-Heating-System Categorical 12 Flood-Water-Parameters-Flood-13 CLASS_ATTRIBUTE-Measures Categorical 
Therefore, within the case study presented in this paper, the initial dataset was extended to building s potentially affected by flooding with the objective to acquire enough data to generalize matching of desig n criteria to the technically justifiable solutions f or flood mitigation. Further, this matching function i s to be implemented in the FLORETO Platform. 
For the scope of the study presented in this paper, only residential area (without industry and public infrastructure) was considered and design criteria relevant for decision making (matching) in the case of Kellinghusen, are selected (table 4). 
Based on the hydraulic and hydrologic analysis, flood water would mostly affect the lower parts of the buildings. In a few cases, the ground floor is affe cted. In those cases, the method of shielding is consider ed and integrated into the CLASS_ATTRIBUTE-Measures (table 2). As depicted in table 3, the basement is described with detailed description of walls, floors and ceiling, which implicitly takes i nto account the pathways of flooding and stability issu e. Services and fittings, building openings, as well a s the heating system are considered for refining the mitigation strategies. 
A description of the experiments performed with intelligent models to solve the matching problems follows first with a description of the datasets, t he computational intelligence models used, the test methodology and the results. 4.2.1. The Dataset. Two Kellinghusen datasets were collected for the experiments. 
The original Kellinghusen dataset (HQ30) consisted of 102 examples each with 15 attributes describing the parameters of the estate and one cla ss attribute representing a label for the measures nee ded to protect the estate in the event of a flood. All but the 15 th non-class attribute were categorical attributes. The initial results (discussed in section 4.3) from this dataset was unpromising, hence new and more refined data had to be collected  X  see section (4.1). 
The final Kellinghusen dataset consisted of 210 examples with 12 non-classes and 1 class attribute with 23 distinct values. The description of the fin al Kellinghusen dataset is presented in table 4 above. 4.2.2. The Models. Four models were used to learn what underlying patterns exist within the datasets to predict the appropriate flood mitigating measures f or the specific property defined by the design criteri a in the non class attributes. The methods used are: a multilayer perceptron neural network with back propagation learning [14] [15], the nested generali zed exemplar method [16] [17] [18] [19], Bayesian or belief networks [20] [21], and an implementation of the C4.5 decision tree [22] [23] [24]. Multilayer Perceptron with Back Propagation Learning (MLP): Artificial neural networks mimic the information processing capabilities at the cent er of intelligence in humans and other animals  X  the nervous system. The learning in the interconnected neurons (or perceptrons) is stored in the synaptic weights between these neurons. It is proven that a MLP can approximate any function by converging its weights using such methodologies as back propagation learning. Also a MLP provides robust learning even in a noisy domain. Nested Generalized Exemplars Method (NGE): It belongs to the nearest neighbor class of instance based learning method that classifies new examples by using a distance measure to find the similarity between data points in memory. The new example to be classified is assigned the class of the nearest example previously stored in memory. In the case of NGE we not only apply the class of the nearest exemplar in memory but also create generalizations as axis parallel hyper rectangles. These generalized exemplars are then used for the classification. Due to these generalizations The NGE method is attractive where a more compact representation is needed in comparison to the widely used k-nearest neighbour (widely used instance based learning method). Also the dimensions of the hyper rectangle can be map to logic rules thereby improving the comprehensibility of the NGE method. Bayesian or Belief Networks (BayesNet): Bayesian networks or Bayesian belief networks are based on the Bayesian learning theory, which uses the well known Bayes theorem (3) to assign input parameters to the class (or measure) with the maximum posterio r probability. The probability distribution is constructed from th e dataset using the Bayesian network structure, which is a network of nodes that constitute a directed acycl ic graph. Each node, given its parents, is assumed to be independent of its ancestors i.e. conditionally independent. Additionally each node contains conditional probability tables. Because of the larg e numbers of possible network structures and conditional probability tables different search methods have been developed for construction an optimal network from the dataset. The default K2 hi ll climbing search algorithms in WEKA were used for this experiment. Decision Tree (J48): J48 is Weka X  X  implementation of Ross Quinlan X  X  C4.5 algorithm which is an improvement on ID3 the basic decision tree algorithm. A decision tree uses a graphical tree to represent knowledge. All leaves (end nodes) of the tree correspond to classes (mitigation measures in this experiment) whereas other nodes correspond to values of the non-class attributes. A path from roo t to leaf corresponds to a classification rule. At every node, in a decision tree, a test is performed to fi nd the leading attribute based on possible information gai n. The node is then split according to the values of t he leading attribute. This process of node-splitting continues until stopping criterions, well defined f or the specific implementation of the decision tree, a re met. C4.5 X  X  improvement to ID3 includes the abilities to deal with numeric attributes, handle missing values, the improved leading attribute selection method and tree compactness due to post pruning. 4.2.3. The Methodology. The models used and the experiments were conducted using WEKA [25], an open platform for machine learning. Two approaches for training and validation were used with each of the methods on both datasets  X  the split and cross validation methods of validation. SPLIT: Here the dataset is split into a training set and a test set. After training the test set is used to validate the model by comparing the output from the data to the known class of the test set example. Th e accuracy of such comparison is the predictive accuracy of the model K-Fold Cross Validation: The dataset is split into k approximately equal parts. K-1 parts of these are u sed for training and the final part is used for testing . This is repeated k times until each of the k sets has be en used for testing exactly once. The predictive accur acy of the model is taken as the mean from all k tests.
The results of our experiments are shown in tables 4 and 5 below. 
The results from the initial Kellinghusen dataset showed there weren X  X  enough samples in the dataset for good generalization properties. One reason for this unsatisfactory result could be that enough dat a representative of important measures was not collected. Another reason could be due to insignificant attributes. NGE 45.24 48.39 46.81 32.04 MLP 42.86 45.16 44.01 34.95 BayesNet 42.86 48.39 45.62 38.83 J48 40.48 45.16 42.82 38.83 
Armed with this experience from the initial experiment in addition to utilizing the know how of experts (see section 4.1), new data was collected w ith the results in table 6 below. The final Kellinghusen dataset showed greatly improved generalization properties. Considering the underlying distribution of the class attributes and that there are 23 distinct class attribute values, this result is satisfactory for an initial deployment of the matching function of FLORETO. The matching function for the FLORETO Platform, to help stakeholders match parameters of design criteria to technically justifiable flood mitigation measures, has been described and realize d in a data mining process using different computational intelligence models. Different datase ts from Kellinghusen were acquired. The CI Models were applied with satisfactory generalization resul ts discovered on the final Kellinghusen dataset. The models with the better performance the decision tre e (J48) and the NGE method have the added advantage of using representations that can be easily transformed into rules that are easier to comprehen d by humans. This is not possible with the MLP model for example. The realization of such comprehensible rules is of additional benefit for the stakeholder building function of FLORETO, because the rules serve as justification for the selected measures an d this justification helps to improve human interacti on with the system and their confidence in the results . 
The results obtained although satisfactory still leave room for improvement. This can be realized by collecting more data, refining the design criteria (attributes) which is achieved by identifying and eliminating irrelevant attributes. Also measures ca n be refined to be more specific. 
With improvement of the Kellinghusen results, further development of this work will involve adapting the business logic of FLORETO to cover other regions by similarly collecting data and mini ng it considering to adjust the design criteria and th e measures to the new locations. 
