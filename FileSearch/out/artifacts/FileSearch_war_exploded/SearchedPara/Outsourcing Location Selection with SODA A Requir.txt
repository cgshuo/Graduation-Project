 Information systems (IS) outsourcing h as received extensive academic atten-tion over the last twenty years with research providing insights into why firms outsource, what they outsource, whic h decision process they apply, how they implement their decision, and how the outcomes are handled [7]. A large stake of outsourcing in practice originates from application development. Outsourcing non-critical development activities in application development can be considered as selective sourcing of application systems. Some software components are de-veloped by internally located software teams and others are delivered by third parties from near-shore or far-shore development centers.

However, the information technology (IT) artifact as research object has ex-perienced little academic attention in the context of outsourcing decisions. Most research into outsourcing decision takes an organizational perspective, driven by major reference theories, such as tra nsaction cost economics or the resource-based view [7]. While the identified outsourcing determinants are certainly valid in a software outsourcing context, they ar e insufficient to capture the whole story since the software outsourcing question is a multi-dimensional decision problem [7]. A decision model for software outsourcing must additionally incorporate decision rationales derived from basic software engineering (SE) principles [14].
We therefore have set our focus on smal l and medium-sized enterprises (SME) in which internal resource deficits (skill-wise, personnel-wise, or performance-wise) outweigh other factors as some sort of overriding contingency when taking a decision whether to outsource or not [8]. This is substantiated by a more recent study among German SME [13]. T his study reveals that SME mainly outsource software development striving for increased flexibility and better skill access rather than mere cost savings. Miti gating these risks req uires a structured approach in the shape of a risk management process [25], for example, or simply guiding principles for a sourcing decision that enable systemic thinking [6]. This is particularly challenging for smaller firms who are characterized by less struc-tural formalism [4]. Their decisions typically evolve in some sort of collaborative ad-hoc manner. Moreover, their skills are likely to be insufficient for establishing outsourcing relationships. Also, depending on the degree and the object to be outsourced, SME might need external support that can contribute the required outsourcing expertise [2]. However, a l earning curve effect can presumably be expected provided that SME frequently engage in outsourcing.

In order to address the challenges in research and practice, we have devel-oped a decision support methodology and a tool that provide a complementary view on the outsourcing decision through the introduction of a SE perspective. Our research objects are software requir ements. Our method supports the pro-cess of structuring requirements into modular clusters and deciding which of these clusters qualify for being outsourced. It builds upon a graph representa-tion of requirements and applies spectral clustering along with graph algorithms known from social network analysis. The method is meant to provide SME with outsourcing decision support by structuring the decision problem and allowing decision makers structural analyses to classify work packages for insourcing and outsourcing. The applicability of the methodology and, hence, of the tool shall be ensured if the following requirements can be fulfilled: 1. Good clustering quality: The clust ers generated by our method have to be 2. Good scalability / Low setup costs: The developed methodology must scale 3. Perceived enhancement: The devel oped methodology must generate per-Our work follows the design science research methodology [20]. In the subse-quent section we step into relevant theor ies, concepts and existing approaches our method draws from. Afterwards, in Section 3 we describe our decision sup-port method and its prototypical instantiation. By means of the prototype, the decision support methodology is evaluated in Section 4 before we conclude with an overall summary in the last section. Our method draws from mainly three resea rch streams that comprise information systems outsourcing, requirements engineering, and graph theory. We elaborate on the different streams in the following subsections and highlight the relevant aspects that have impact on our proposed research. 2.1 Reference Theories in IS Outsourcing Decision Making The software outsourcing question which is in our focus is a multi-dimensional decision problem [7]. The majority of studies have applied transaction cost eco-nomics (TCE) to explain outsourcing behavior through conditions of market efficiency [7]. However, internal resources are increasingly considered as a deci-sive factor as resources are heterogeneously distributed among firms and might constitute a competitive advantage, i.e. the resource-based view (RBV) adds a strategic perspective [7]. As an exten sion of the RBV, the knowledge-based view (KBV) stresses the importance of kn owledge as differentiating resource, particularly in knowledge-intensive industries. Hence, we base our research on the proposed theoretical foundations and consider a set of derived implications in our method and tool.

Since TCE imply that the outsourcing firm has to carefully assess the speci-ficity of any outsourcing candidate, we conclude that splitting software compo-nents with interdependent tasks is related with high communication effort and has therefore to be reduced to a minimum. Furthermore, the implications of RBV and KBV reveal that using external development locations for the devel-opment of organization-specific knowledge is inefficient and that outsourcing of components with high competitive rel evance for a company X  X  position on the market is inappropriate. 2.2 Requirements Engineering Classification of Requirements. The prevailing distinction of requirements is into functional and non-functional requirements (NFRs). Functional require-ments comprise statements on services the system must be capable to perform. Apart from the services a system offers, functional requirements define reactions to specific events or system behavior in certain situations. NFRs, on the other hand, describe constraints that include time constraints, process constraints, or standards, for example. They usually relate to the system as a whole rather than individual functional requirements or services. Put differently, a functional requirement describes  X  X hat X  the software does in contrast to an NFR that describes  X  X ow good X  a software does something.

Requirements which do not immediately pertain to the system are irrelevant for our method as its inherent model aims to semantically capture the software product, not its context. However, even system-related NFRs have to be treated carefully. NFRs constrain the way required functionality may be realized. In other words, functional requirements open up the solution space of a software system, NFRs confine it. This is due to the interdependencies that exist between requirements. While interdependencies between functional requirements rarely lead to conflicts, interdependencies between NFRs often induce unsolvable in-consistencies [22]. High security and high performance are typical examples for NFRs that stand in a conflicting relationship.

A more recent distinction of requiremen ts is represented by the classifica-tion into aspectual and non-aspectual requirements that has been brought for-ward through the discipline of aspect-ori ented requirement engineering (AORE). AORE advances the idea of aspect-orien ted programming by forming a holistic approach that embraces not only the implementation phase but also early SE phases [18]. Aspect-orientation is con cerned with aspects. Aspects are cross-cutting concerns which exist throughout the SE process. At requirements-level,  X  X n aspect [...] is a broadly scoped property, represented by a single requirement or a coherent set of requirements [...] X  [1 8, p. 388]. At the stage of requirements engineering (RE), these cro ss-cutting concerns are ref erred to as early aspects because they are not necessarily transfor med into what deserves the label  X  X s-pect X  from a technological viewpoint, i.e . they are candidate aspects. The benefit of early aspects is to deepen the understanding of the problem domain and its semantic interdependencies [18]. Aspects establish intercomponent dependencies as their cross-cutting nature makes it nece ssary that components provide services to or require services from other components. Typical examples for functional aspects are capabilities which are required throughout a software system, such as persistency, collaboration, synchronization or locking mechanisms. Requirements Interdependencies. By focusing on requirements, we argue that it is crucial for software clustering and architectural design to understand a system from a semantic perspective, independent of syntactic dependencies. Hence, our method attempts to capture relevant types of semantic interdepen-dencies that exist between requirements.

Generally, such interdependencies can be classified into vertical and horizontal ones as illustrated by Figure 1. Vertical dependencies stem from transitions be-tween RE phases that require a change of the level of abstraction. These vertical relations are mostly hierarchical in that lower level requirements refine what a higher level requirement specifies. A simple example for a refinement is a feature edit that is decomposed into copy, paste , and delete. Because these relation-ships represent transitions between di fferent degrees of abstraction of the same requirement, they are not of interest for the method we present in this paper. Instead, the focus is on horizontal dependencies that constitute system cohesion and impact on how requirements are semantically related and how they can be structured, correspondingly.

We refer to a model of seven fundamental interdependency types [5] that syn-thesizes different views from the literature. In that definition, similar to refers to a semantic match of varying degree and is significant for structuring require-ments as it is an expression of cohesion between two requirements. Requires describes the condition that  X  X the] fulfi llment of one requirement depends on the fulfillment of another requirement X  [5, p. 103]. Beside conditional and functional dependencies, requires also describes temporal dependencies in the form that a requirement needs to be implemented be fore another one can be implemented. In summary, requires is highly relevant as it alters the semantics of interlinked requirements and thus impacts on their cohesion with varying intensity. Our method focuses on the relations requires and similar to which are considered the main drivers of semantic cohesion. 2.3 Graph Theory Graph Representation. Graph theory essentially relies on the existence of pairwise relations between objects. Requ irements and their interdependencies exactly represent such objects with pai rwise relations in between. Hence, graph theory is ideally suited for the formal re presentation of requirements and their relations. The formal graph representati on provides the necessary degree of struc-ture for the computable part of the decision problem, the clustering of require-ments and corresponding metrics. Hence, we represent requirements by the use of typed and weighted graphs where types describe different sorts of interde-pendencies and weights express varying degrees of cohesion. To work with these graphs programmatically, we use a weighted adjacency matrix.
 Graph Partitioning. Clustering vertices into disjoint subsets is known as graph partitioning in graph theory. The properties quality and time of graph partitioning heuristics were found to stand in a conflicting relationship. By ten-dency, algorithms that take longer produc e better results than more time-efficient algorithms. Whereas this is a trade-off in time-critical settings, such as parallel computing, time is not that much of an issue in the context of our decision prob-lem. As a logical consequence, high partitioning quality is the primary intent. Partitioning quality, in turn, can be defined in various ways. For graph parti-tioning in general, it is usually determ ined by the cut size. Given two disjoint partitions V1 and V2, the cut describes all edges that have one end in V1 and the other in V2. Its size is the sum of edges or, given they are weighted, the sum of their weights. Typically, partitioning algorithms try to minimize the cut size.
Graph-partitioning algorithms can be classified into geometric and coordinate-free algorithms [10]. In the context of graphs, geometric algorithms require a graph to be embedded into the metric space which is not given in the present case. Requirements do not possess any coordinates. This fact rules out geomet-ric algorithms leaving non-geometric heu ristics. These coordinate-free methods focus on the combinatorial structure of the graph [10] which, from the perspec-tive of this paper, replicates the seman tic cohesion of requirements. We focus on the set of algorithms from the field of recursive spectral bisection (RSB). By tendency, RSB algorithms outperform traditional approaches, are easy to imple-ment, and efficient to solve through standa rd linear algebra operations [17]. They leverage the algebraic properties of a graph X  X  matrix representation, particularly that of its Laplacian matrix.
 Structural Analysis of Graphs. Structural analysis of graphs uses metrics to derive conclusions about the characteristics of the whole graph, subgraphs, or individual vertices. Whereas algorithms process the graph structure in order to solve a certain problem, structural analysis generates quantitative measures in order to describe it and the construct it has been derived from. This is particu-larly useful for the given problem as it allows to objectively identify important and less important requirements on the basis of their relations and weights. To do so, we make use of the centrality concept that originally stems from the anal-ysis of communication networks and tries to identify nodes that are important to the communication within the network. 2.4 Related Work Holistic research into the question of wh at parts of a software system, that is yet to be developed, qualify for outsourcin g is scarce. Hence, our review of related work follows our method X  X  three-step approach: graph-based representation of requirements, clustering of requirements a nd structural analysis of requirements. Graph-Based Representation of Requirements. Approaches towards the graph-based representation of requirements mainly include graph-based trace-ability and the representation of NFRs. The latter mostly deals with decision making but is not in our focus. The field of graph-based traceability attempts to leverage the benefits of a graph-based visualization for improving the ability to explore and analyze requirements and their interrelationships, e.g., a graph-based model that uses labeled edges to represent requirements. The model contains a weight measure for edges that express the semantic match between different re-quirements [11,12]. Schwarz et al. [23] present a more recent and comprehensive approach to apply a formal graph representation in order to improve traceability. They used typed edges to distinguish between different sorts of interdependen-cies. Li et al. [15] presented a graph-based approach for change impact analysis as part of requirements traceability. They used different types of associations that are based on an older version of the model of fundamental interdepen-dency types [5]. Only the approach by Yaung [26] could be identified as using a graph-based model for the purpose of requirements clustering. His approach also focuses on functional requirements withou t distinguishing between relationship types. A relation in his model expresses a certain degree of cohesion. The degree is included as an edge weight.
 Requirements Clustering. The majority of requirements clustering approaches use clustering for the purpose of system modularization which is commonly viewed as decomposition into strongly cohesive and loosely coupled groups of require-ments. However, the individual approaches to arrive at this state differ substan-tially. Li et al. [16] aimed at requirements encapsulation which, in essence, is the modularization of requirements and the definition of interfaces for these modules. They defined a set of seven requirements a ttributes that pertain to semantics and structure. They did not use explicit relations between requirements.

Requirements are clustered based on the overall similarity of their attributes, hence multi-dimensional similarity. Their approach requires a detailed require-ments specification and extensive manual work in order to define the set of requirements and their attributes. The approach of Yaung [26] is presumably closest to our objective of clustering a g raph structure into cohesive groups of nodes. His approach is, however, simpler in that he did not distinguish between different types of relations. Furthermore, the algorithm he proposed requires the specification of a cohesion threshold by a n expert. If the cohesion of two require-ments is above that threshold, they ar e assigned into the same cluster. Hence, the result of the algorithm is highly dependent on how that parameter has been defined. Nonetheless, it is an early example of the applicability of a graph-based approach.

Finally, some additional attempts to cluster a requirements similarity matrix for the purpose of software modularization have been identified [1]. It is, however, difficult to draw a conclusion on how the algorithms competitively perform. Structural Analysis of Requirements. Structural metrics in the context of graphs give insights into the characteristics of a graph (global measures) or certain vertices (point/local measures). Of interest are centrality measures that reflect the importance of a vertex in a network. The analysis of such networks has become popular in social sciences as social network analysis (SNA). Within graph-based traceability, social network analysis (SNA) has been applied but with emphasis on important persons within the traceability network [12].
Only one approach could be identified that applies SNA techniques to as-sess the importance of individual requirements. Fitsilis et al. [9] attempted to conduct a prioritization through structural metrics. To do so, they created a requirements interdependency matrix which was then used to determine the individual centrality measures. They de monstrated the results for betweenness centrality, closeness centr ality, and different types of degree centrality (in, out, total). Results indicated that different centrality metrics yield different values, i.e. rankings varied to a certain degree depending on the centrality measure chosen. While their example illustrates the applicability of this approach, it also demonstrates that this field is understudied. It requires standard measures along with reference values to describe the characteristics of requirements throughout the SE process [9]. 3.1 Method Overview Figure 2 sketches the conceptual steps that constitute the SODA (Software Out-sourcing Decision Aid) method. The first step relates to the creation of the re-quirements model. Software requirements are transformed into a graph-based model which is based on specific syntactic rules. The graph-based model is then forwarded to the second step that attempts to identify cohesive groups of re-quirements through the application of a clustering algorithm. The third step takes the clusters that have been found and conducts a structural analysis in order to determine each cluster X  X  outsourcing suitability.
 3.2 Representing Requirements The requirements model (RM) is meant to r eflect the combinato rial structure of a software project X  X  requirements. Only horizontal relations ought to be included. Dahlstedt X  X  and Persson X  X  model of fundamental interdependency types [5] has been narrowed down to the interdependency types requires and similar to .Our graph-based requirements model is defined as where V represents the set of requirements, E represents the multiset of directed edges that refer to the interdependencies between the requirements in V . w E is a weight function that assigns a weight to each edge in E reflecting an interdepen-dency X  X  strength. We suggest three degrees of dependency strengths. How these three degrees are transformed into quantitative values ultimately depends on the specific implementation of SODA. The only restriction imposed here postulates that W may not contain negative elements. G RM is a labeled graph. Hence, t E is a map that assigns a type, i.e. label, to each edge in E from the set of types T . T is defined as T = { similar to, requires } . It can be easily extended by sim-ply adding additional interdependency types to T . Through this strategy, new perspectives can be included into the model. This allows clustering the model based on selected interdependency types to investigate differences between de-compositions under selected decomposition criteria. Put differently, the model can be considered as a layered model with each layer being spanned by a specific interdependency type. The general model aggregates all types in a single graph. A typed model thus represents an excerpt of the general model. The model does not allow loops. For the given set of interdependency types, loops generate no informative value. Figure 3 gives an impression of how the requirements model is presented in our prototype. The size of each requirement is derived from its centrality.

An important aspect that shall be briefly mentioned pertains to the population of our requirements model. From a cost perspective, SODA X  X  main cost driver is the identification of interdependencies between requirements. Unless (semi-)automatic approaches are at hand to support the identification activity, pairwise comparisons must be conducted at the cost of ( n  X  ( n  X  1)) / 2where n is the number of requirements. 3.3 Structuring Requirements The objective of the clustering step is to find cohesive groups of requirements. In the case of software outsourcing neither the number of clusters nor a cluster X  X  size is known a priori. A major aspect is that semantically cohesive groups of requirements are not necessa rily equally sized. Also, the number of clusters is to be unconstrained. Setting it would improperly bias the algorithm and prevent it from finding a partition that is optimal from SODA X  X  objective viewpoint. Hence, in the present case, the algorit hm needs to be non-p arameterized and free of externally imposed constraints. Its sole input represents the structure that is to be divided.

We selected an algorithm developed by Newman [19]. It is particularly applica-ble for the research problem as it is non-parameterized and attempts to maximize the modularity of a given network. The objective of Newman X  X  algorithm [19] can be referred to as community struct ure detection. A community structure denotes the appearance of  X  X ...] densely connected groups of vertices, with only sparser connections between groups X  [19, p. 8577]. An algorithm that detects community structures respects that the number and size of communities are de-termined by the network rather than a supervisor. It also admits the fact that there might not be any suitable division of the network. This criterion is decisive for SODA and could not be identified in a ny of the other spectral algorithms as they usually force partitions into a pr escribed size tolerance. Newman [19], however, loosened this constraint through a redefinition of the Laplacian matrix. Another advantage is that his algorithm automatically determines the number of clusters through the inclusion of a control measure: modularity. Newman X  X  approach [19] is thus tailored to the objective of identifying modular communi-ties which are unbalanced in terms of size. We combine his global partitioning algorithm with a variant of the Kernighan-Lin algorithm he proposes as well. It conducts a local optimization of the modularity measure through movements of individual vertices between pairs of clust ers. Figure 4 shows our prototype after clustering the set of requirements. 3.4 Structural Analysis of Requirements Global: Modularity. The objective of SODA X  X  third step is to take the re-quirements structure that has been algorithmically determined in step two and analyze it from a global (whole graph), regional (cluster), and local perspective (vertex) in order to guide a human decision maker in his or her outsourcing decision. A global metric can be derived from Newman X  X  clustering algorithm [19] in order to characterize the modularity Q P of a given network partition P . This allows putting it into relation to other partitions of the same network or reference values that might stem from other projects, for example. Modularity is quantitatively assessed through a pairwise comparison of nodes. If two nodes fall within the same group, their contribution to modularity is the weight of the edges between them minus the expected weight in an equivalent network with randomly placed edges [19].
Through this approach, global modularity is dependent on the partition of the network. If a user decides to manually change the partition that has been proposed by the algorithm, he or she can observe the corresponding impact on modularity. Altering the partition proposed by the algorithm cannot improve modularity as measured here since the algorithm determines the partition that maximizes Q . Hence, Q max shall denote the achievable modularity for a specific requirements model. It is computed us ing the partition generated by SODA X  X  clustering algorithm. Any manual movement of nodes impairs Q P . Nonetheless, there might be valid reasons to alter a generated partition as an individual might base his or her decision on additional information which is not included in the model.

The value range of Q P stretches from  X  1 to +1. Q P ranging around zero indicates a random distribution of edges with no identifiable community struc-ture. For networks that exhibit a perceivable community structure, Q P typically ranges between 0 . 3and0 . 7. Networks with negative Q P have weakly cohesive groups that have fewer intra-linkages than one would expect in a random graph. As indicated, Q max sets an upper bound on the scale from  X  1to+1foragiven requirements model. It is a first indicator of how interdependent a project X  X  re-quirements are from a global viewpoint. Regional and point metrics then allow creating a more differentiat ed picture of the situation.
 Regional: Cluster Coupling and Cohesion. Cluster coupling and cohesion represent regional or group metrics. The former is an indicator of how strongly related the responsibilities of a subset of requirements are. The latter expresses how strongly a subset is connected to or re quires external requirements. Whereas Q max characterizes the entire model, coupling and cohesion describe groups of requirements, specifically the cluster s that have been generated in SODA X  X  par-titioning algorithm. Coupling and cohesion hence provide a deeper insight into the outsourcing suitability of each cluster. Briand et al. [3] formulated properties for functions that measure coupling and cohesion to which our proposed metrics adhere.

For the expression of a cluster X  X  degree of coupling, we propose a simple count-based measure that cumulates the weight of all inter-linkages of that cluster to other clusters. It is deliberately not a normalized measure and thus has a value range from 0 to positive infinity. The underlying notion is that coupling is independent of both the size of the cluster and the project and only determined by the weight of linkages to other clusters.

Unlike coupling, cohesion is not an additive metric. Instead, Briand et al. [3] stress that cohesion is a normalized metric that is forced into a specific interval. Hence, we propose a function that puts the weight of all intra-linkages into relation to the weight of all intra-and inter-linkages of a specific cluster. The value range is thus constrained to the interval from 0 to 1. Cohesion of below 0.5 indicates that a cluster of requirements has more external than internal interdependencies. Hence, it is desirable to have a cohesion that lies significantly over 0.5 and approaches 1. It is unlikely to achieve maximum cohesion for all clusters due to the near decomposability property of complex systems [24]. Local: Requirements Centrality. For SODA, the importance of a require-ment is essentially dependent on its position in the combinatorial structure of the model. This position can be characterized by the weight of its interdepen-dencies to other requirements as well as the importance of these neighboring requirements. In that sense, importance i s reciprocal. Centrality is thus driven by differences in degree. The group of cen trality measures that reflects these aspects is feedback centrality. We have adopted the eigenvector centrality in which larger components of a graph are by tendency weighted more strongly than smaller ones. As previously stressed, there is a lack of reference values re-garding the application of network analysis measures for requirements analysis. Hence, SODA does not define a threshold that allows a clear statement whether a specific requirement might critical, i.e. too important to be outsourced. Rule-Based Recommendations. SODA does not aim at deciding which re-quirements can be outsourced and whi ch cannot. Its role is meant to be sup-portive in the sense that decision makers get an understanding of the project X  X  semantic structure, relevant interdependencies, and the role of individual re-quirements. In the end, it is up to the deci sion maker to derive a final conclusion his or her decision can be grounded on. As a logical conclusion, SODA is a means to explore the decision problem. Through the definition of threshold values for eigenvector centrality, coupling, and cohes ion it allows for a rule-based identifica-tion of requirements clusters that represent outsourcing candidates. The quality of these identified outsourcing candidates ultimately depends on the ability of the decision maker to set these thresholds and judge the situation. It is to a large extent subject to his or her experience. The method which is subsequently applied is a mixture of experimental and descriptive approaches. The experimental simulation addresses the formal core of our decision support method and provides evidence for fulfilling the solution criteria (clustering quality, scalability, and enhancement) as introduced in Sec-tion 1. The simulation was performed using data from four student projects. The projects have been conducted within the context of a lecture for master students in information systems. Students were meant to develop a computer game based on prescribed requiremen ts. The project teams had to use a col-laboration platform and were incentivized to exploit its functionality, so all of them maintained requirements interdep endencies. These were recorded based on the students X  judgment of what are adequate semantic relations. This data set was particularly suitable to investigate how a varying combinatorial structure of semantically identical sets of requireme nts determined the prototype X  X  output.
Hence, our focus was on any observable correlations between the number of interdependencies and the method X  X  output figures. We could observe that more interdependencies led to a more coarse-grained partition of the graph. Achievable modularity ( Q max ) turned out lower as a logical result of the algorithm finding only larger and hence fewer community structures. The question that came up here was how clustering quality is affect ed by that constellation. To investigate this, we conducted another test and applied the Rand index to measure the quality of the clustering algorithm [21]. The results are shown in Table 1 below. The ground truth was defined based on our perception of what would be a sensible partition.

The differences in the individually generated partitions that can be observed mainly consist in more fine-grained partitions than this is the case for the ground truth. Simply merging individual clusters gives a good approximation of the ground truth which is why the Rand index reports a good clustering quality for all projects. Project C which has by far the most interdependencies also exhibits the coarsest granularity with only six clusters in its partition. Whether this is desirable is speculative. What can be said is that group C put the most effort into interdependency maintenance. The benefit of this investment is not clear. Judged from the viewpoint of scalability, this is encouraging with respect to the effort necessary to set up the prototype. It suggests that even in a business environment, the required effort might lie within manageable boundaries be-cause fewer interdependencies apparently allow for results of satisfying quality. In contrast, project D featured 46 require ments with significantly less interde-pendencies than C. Beside the surprisingly good clustering quality as indicated by the Rand index, an important finding is that clustering quality is not nec-essarily better in a model that has a larger set of interdependencies. This is particularly promising given the previously mentioned problem of costly inter-dependency identification. Thus, we can infer from our experimental simulation that the postulated requirements of good clustering quality and scalability (cf. Section 1) are being fulfilled by our method under laboratory conditions.
The third requirement of perceived e nhancement could not be validated in these laboratory conditions. We believe that SODA X  X  major weaknesses are lo-cated in the last step that pertains to the structural analysis of requirements clus-ters and the subsequent recommendation of outsourcing candidates. Although the rule-based approach assures consistency, its expressiveness is limited due to the fact of scarce research. Moreover , SODA is highly dependent on human in-put as the quality of its suggestions is essentially induced by the quality of the underlying model and the threshold values that have been defined by the user. With regard to the model, we can conclude that the inferencing capabilities are driven by the quality of the underlying interdependencies. Interdependencies are the most critical element within the decision support method developed here. They predominantly determine the combinatorial structure of the model. Hence, the results SODA produces are very sensitive to the input it receives. Another problematic aspect is the lack of reference values for outsourcing decisions. As a logical conclusion, the quality of SODA X  X  recommendations is similarly sensitive to the threshold values as it is to the model itself.

It shall be stressed that SODA does not propose a software architecture. It is grounded on the argumentation that it is beneficial to understand a software system from a semantic perspective. Cl ustering of requirements can lead to a modular design [16] but design is subject to more factors than just semantic relations. To investigate its potential outside of the laboratory, we plan to con-duct a qualitative case study that builds upon requirements data from a business setting and benefits from expert judgments, especially targeting at evaluating to what extent SODA does perceivably enhance the outsourcing decision process in a SME. We have developed SODA, a decision support method and tool that support decision makers in analyzing the outsourcing suitability of requirements from a SE perspective and leverage the locatio n dependent sourcing decision prob-lem. SODA builds on existing research in that it represents a cross-discipline artifact that draws from the research streams of information systems outsourc-ing, requirements engineer ing, and graph theory. SOD A constitutes a three-step approach, including graph-based modeling of requirements and their semantic interdependencies, model clustering, and structural analysis of clusters and re-quirements through a set of structural m etrics. Based on these metrics, it per-forms a rule-based recommendation of outsourcing candidates.

The evaluation result reflects the novelty, robustness and scalability of the approach. Overall, it indicates that SODA is applicable for the identification of outsourcing candidates. This, however, n eeds to be interpreted in a differentiated manner. Its strengths are located in the representation and clustering of require-ments. Its main weakness pertains to the set of structural metrics. Because this field is notoriously understudied, there is little knowledge SODA can draw from which induces a substantial degree of uncertainty. Furthermore, a qualitative case study will shed more light on the practical usefulness of our approach.
