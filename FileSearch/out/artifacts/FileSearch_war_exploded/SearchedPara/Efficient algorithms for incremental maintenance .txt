 1. Introduction cient algorithms proposed to mine frequent sequential patterns [5,23,17,36,2,9,28].
Recently, mining compact frequent patterns has become an active research topic in data mining community followfromtheexponentialgrowth [29,8] .First,userswillbeoverwhelmedbytheoutputandhavetroubletogaininsightsfrom nential number of frequent patterns. For example, let a sequence database contain only two sequences: { h a h a a 2 a 30 i } and the min _ sup value be 1. There will be 2 50 h a a 2 i :2, ... , h a 1 a 2 ... a 50 i :1}. In the complete set of frequent sequences, only two of them { h a non-redundant, and all the other frequent sequences and their supports can be derived from the two sequences. only the set of maximal patterns. In the above example, only h a ports of frequent sequences such as h a 1 a 2 a 29 i and h a [33,27] ,i.e.thosecontainingnosupersequencewiththesameoccurrencefrequency.Closedpatternminingisalosslessmethod. algorithms more efficient. As a real example, in a web log sequence database CSLOGS the same machine. Thus, it is better to mine only closed sequential patterns. existing sequences. Sometimes old sequences may be deleted, and some items of existing sequences may be updated or base of many sequences. Frequent sequences mined from the database are candidate blocks where Copy X  X aste bugs may occur. When a new software version is released, some sequences may be modified, inserted and deleted. some extremely long patterns, they also fail to mine and store the complete set of frequent patterns. other hand, deleted sequences (or items) may make previous frequent patterns become infrequent. How can we perform deletions efficiently and at the same time keep the states of left patterns up-to-date? of 4 to more than an order of magnitude.
 struction algorithm is presented. In Section 5, two algorithms, IMCS in Section 8. 2. Preliminary concepts and problem statement tern maintenance problem and analyze the problem complexity. 2.1. Preliminary concepts s example, the length of h ABABD i is 5. A sequence a = h a there exist integers 1 6 j 1 &lt; j 2 &lt; &lt; j l 6 m , such that a in D is the number of sequences (in D ) containing a , denoted as freq divided by the number of sequences in D , (i.e. freq D  X  a  X  to describe the algorithms and use relative support to present the experimental results. A sequence a is called a frequent sequential pattern in D , if sup ( a ) and a @ b .
 patterns h BC i :3 and h BD i :3.

Given two sequences a = h a 1 a 2 a l i and b = h b 1 b 2
For example, h ACA i}h CAB i X h ACACAB i . For a sequence s = h s s  X  s 0 i (1 6 i 6 m ). c  X h s 0 n m s 0 n 1 s 0 n i (0 6 m &lt; n ) is called a suffix of s if s taining s , i.e., s 0 w s and b w s and there does not exist a prefix c of s 0 such that b that / -projected database in D is equal to D .
 h DAB i , not h DABCAB i , since h DABCAB i A h DAB i and h DAB i is the last appearance of b j in the first instance of b in s , and LF in h DBAADB i are the first D , the second A and the second B , respectively. Note that in this example, LF 1&lt; j 6 m , it is the piece of sequence between the end of the first instance of h b
A , so the 1 st semi-maximum period of b in s is / . LF 2 maximum period of b in s is h B i . 2.2. Problem statement  X  INSERT : insert new sequences to the database.  X  APPEND : append items to some existing sequences.  X  DELETE : delete items from the tail of some sequences.  X  DELETE S : delete some sequences from the database completely.  X  MODIFY : change some items of sequences to other items.
 Thus, we only need to consider APPEND and DELETE in this paper.
 the sequence database in Fig. 3 , we get the updated database in Fig. 1 .
An incremental changed database (denoted as b D ) is defined as: (1) for APPEND, s 0  X  s } d  X g ; (2) for DELETE, b D  X f s j s 2 D ; 9 s 0 ; d  X  s 0 2 D running example, whether for APPEND or for DELETE, the incremental changed database
In the above example, for both APPEND and DELETE cases, the unchanged database has the same three sequences { h BCDA i , h AEA i , h BDCA i }. The notations used in this paper are summarized in Table 1 .
Example 2. After the sequences in the incremental sequence database (Fig. 2 ) are appended to the sequences in the new closed patterns h BCA i and h BDA i occur in the final results.
 the knowledge about D .
 problem in P.
 Theorem 1. The APPEND problem is NP-hard.
 patterns is NP-hard. Thus, it can be concluded that APPEND is an NP-hard problem too. h Theorem 2. The DELETE problem is in P.
 eliminating non-closed patterns in C which takes at most O ( j FC j needs O (max( j FC j * l 2 * j D 0 j * l s , j FC j 2 * l Thus, the DELETE problem is in P. h 3. Related work egories: non-incremental sequence mining algorithms [4,5,23,36,2,33,27,14] and incremental ones [22,18,20,24,10] . 3.1. Non-incremental sequence mining algorithms tern-growth methods [23] and vertical format based methods [36,2] . 3.1.1. Apriori-like methods base are made. In the first pass, GSP finds the set of the frequent items L set L k 1 , which contains the frequent length-( k 1) sequences discovered in the previous pass. A candidate set C ated by joining L k 1 with itself. A sequence s 0  X h s 0 1 determine the set of frequent sequences in C k , i.e., L k kind of methods need many scans of the whole database. 3.1.2. Pattern-growth methods important algorithms belonging to this kind.
 a projection position. For example, the pseudo-projection of D sive candidate-generation-and-test cost is saved.

PLWAP [14] algorithm stores a sequence database in a Pre-order Linked WAP tree (PLWAP tree). PLWAP tree is a pre-end with that link.
 PrefixSpan and CloSpan is that when traversing the search space, CloSpan uses backward sub-pattern and backward the size of the projected database of a previously found sequence s 0 is equal to that of current sequence s and s ple, when CloSpan deals with the projected database of h D i , it finds that the size of D sequences with h D i as a prefix must not be closed. So it does not recursively deal with D compensated by the search space it skips.
 ing. The BI-Directional Extension scheme decides whether a pattern p = h i closed. For example, like CloSpan, BIDE can also skip the recursive search of D work is motivated by the BackScan strategy. The BackScan pruning method can prune the search space more aggres-database is dense or the min _ sup value is low. 3.1.3. Vertical format based methods ations very straightforward. The representative methods are SPADE [36] and SPAM [2]. since id-lists can be several times larger than the initial database.

Similar to SPADE, SPAM [2] is also an algorithm based on a vertical data format. The difference is that it uses a of each sequence-extended and itemset-extended child is computed. If the child is frequent, SPAM repeats search recursively. Due to the fast bitmap computation, SPAM is much faster than SPADE. However, SPAM is less space-effi-cient than SPADE. SPADE is expected to roughly outperform SPAM in terms of space requirements by a factor from 5 to 20 [2].
 patterns incrementally.
 3.2. Incremental mining algorithms and finds L D 1 which is the set of frequent 1-sequences embedded in D . Then it uses L information need to be maintained, so support counting is more efficient. MFS+ [18] and IncSP [20] are two other incremental algorithms based on GSP. MFS+ supports INSERT and DELETE the multiple scans of the database, the huge candidate sets and the time-consuming counting methods. efficient.
 patterns with common prefixes can share some nodes.
 sequential pattern mining introduced in this work can solve these problems well. 4. Closed sequence tree 4.1. The structure of closed sequence tree which are defined as follows. The sequence corresponding to a node n is denoted by s Closed node :
For a node n in a CSTree for a sequence database D ,if s n Stub node :
For a node n in a CSTree for a sequence database D ,itisa stub node if there exist an integer j (1 appears in each of the j th semi-maximum periods of s n in all the sequences in D which contain s nodes are not even enumerated. Thus, the costs of enumerating and storing these kinds of nodes are eliminated. Bridge node :
For a node n in a CSTree for a sequence database D ,if s n bridge node.
 Non -zero infrequent node : satisfied: (i) l =1 ^ 0 &lt; sup D ( s n )&lt; r ; (ii) l P 2 ^ 0 &lt; sup
In other words, n is a non-zero infrequent node: (i) if n is at depth 1, and s min _ sup value; or ii) if n  X  X  depth is greater than 1, n  X  X  parent node p and the sibling node of p up-to-date CSTree.
 the experiment section.
 nance task is to efficiently update the CSTree to reflect the change of the original database. and the node A at depth 2 (under A ) has become a closed node. 4.2. The CSTree construction algorithm accepts four parameters: current node n , the sequence corresponding to n ( i . e ., s (initially no child) of the CSTree.
 quent sequences and corresponding projected databases, based on the property that if a sequence a = h a two CSTrees for the original sequence database and the updated sequence database in Fig. 5 respectively with min _ sup =2.
 base D h i i is constructed. At the same time, we can get the supports of these items. Here, D nodes under root in Fig. 6 are added.

Algorithm 1. ConstructCSTree( n , s , D s , r ) 1: if n = root then 2: for each item i 2 I do 3: construct h i i -projected database ( D s ) h i i ; 4: if  X  sup D s  X h i i X  &gt; 0  X  then 5: create a node t , and t . item i , t : sup ( sup D s  X h i i X  ; 6: add t as a child of n ; 7: call ConstructCSTree( t , h i i ,( D s ) h i i , r ); 8: else 9: if n is a stub node then 10: n . state STUB ; 11: else if 0&lt; n . sup &lt; r then 12: n . state NON-ZERO-INFREQUENT ; 13: else 14: if n is a closed node then 15: n . state CLOSED ; 16: else 17: n . state BRIDGE ; 18: for each child x of n  X  X  parent s.t. x . sup P r do / * here let i = x . item * / 19: construct h i i -projected database ( D s ) h i i ; 20: if  X  sup D s  X h i i X  &gt; 0  X  then 21: create a node t , and t . item i , t : sup ( sup D s  X h i i X  ; 22: add t as a child of n ; 23: call ConstructCSTree( t ; s }h i i ;  X  D s  X  h i i ; r ); node, since neither a forward nor backward item exists. Then for each frequent sibling x of node A ,( D structed. The set of frequent siblings of node A is { A , B , C , D } in this example. ( D { h D i , h D i }, U , U and { h E i , h AD i , h AD i }, respectively. Since the supports of two items A , D in D A and D are inserted.
 base. But in this algorithm, it is not enumerated due to the good property of stub nodes.
Now the algorithm proceeds to node D at depth 2 (under A ) with s = h AD i . It is a closed node. Then ( D ( D h AD i ) h D i are constructed. ( D h AD i ) h A i ={ h D i , h D i } and ( D the nodes in Fig. 6 are added. 4.3. Properties of closed sequence tree and state update process (in Section 5).
 node never becomes a stub node. It may change to a bridge node or stay to be a closed node. node after APPEND. Let s n be h a 1 a 2 a l i and H be the set of sequences in D that contain s imum period of s n in s . There are two cases: maximum period of s n in s .  X  For s 2 X  X  D e D  X \ H  X  , because s n v s and the j th semi-maximum period of s can infer that e appears in the j th semi-maximum period of s Thus, n is a stub node in the CSTree for D . It contradicts the assumption. depth 2 (under D) is still closed. h a closed node or a bridge node.

Proof. Let the original stub node be n and the set of sequences containing s C at depth 1 is increased, it is still a stub node. h change to a closed node. It never becomes a stub node.

Proof. We first prove that the node never becomes a stub node. Let the original bridge node be n , s integer m (1 6 m 6 l ) and an item e 0 which occurs in each of the m th semi-maximum periods of s contain s n . Since item e 0 must occur in the first instances of s same after APPEND, we can infer that item e 0 occurs in each of the m th semi-maximum periods of s n is a stub node in the CSTree for D which conflicts with the condition. must be some sequence b = h b 1 b 2 b j 1 e b j b l i which has the same support as s node either, so n is still a bridge node after update.
 The support of node A at depth 3 (under D in the first branch) is increased, and is changed to a closed node. h
Fig. 8 summarizes the state switching ways of different kinds of nodes after APPEND. New nodes of any type may be and bridge nodes never become stub nodes. With support unchanged, bridge nodes and stub nodes keep their states. bridge) node does not change, it is still in the updated CSTree for D 0 . exists an ancestor node p of n which has become a stub node. Let s an integer j (1 6 j 6 k ) such that e occurs in each of the j th semi-maximum periods of s original closed (or bridge) node. h change to a bridge node or a stub node, or be deleted.

Proof. Let the original closed node be n . The support of any sequence a that contains s closed. The left part of this theorem is easy to verify by examples. We leave it to interested readers. h database D 0 . It never becomes a closed node or a bridge node.
 kept in the tree after update. There must exist an integer j (1 imum periods of s n in the sequences in D that contain s n periods of s n in the sequences in D 0 that contain s n . Thus, n keeps to be a stub node in D 0 . The proof for the latter part of this theorem can be derived from Theorems 3 and 5. h an original bridge node does not change, it does not become a stub node. and an item e that occurs in each of the j th semi-maximum periods of s support of s n does not change, item e must also occur in each of the j th semi-maximum periods of s node, if its support is not decreased. all frequent patterns and the set of all closed patterns in D by F when the number of frequent patterns and the number of closed patterns are equal. zero infrequent nodes N n of the CSTree is equal to  X  P p 2 X  B [ FC  X  nodes of p which satisfy the constraints of non-infrequent nodes. N depth. We will show the number of nodes in CSTrees experimentally in Section 6. 5. The incremental algorithms In this section, we introduce two incremental closed sequential pattern mining algorithms, IMCS to use only the information in the incremental changed database states of a lot of nodes are not changed, so the two algorithms can update the CSTree efficiently. 5.1. The IMCS A algorithm incremental database D and the min _ sup value. IMCS A first calls the subroutine UpdateCSTree the corresponding node of s in the CSTree is stored in the key X  X  value field. Before calling IMCS Below we explain the algorithm line by line.

Algorithm 2. IMCS A ( root , D , D , r ) 1: get b D and D 0 from D and D ; 2: call UpdateCSTree A  X  root ; / ; b D ; D 0 ; r  X  ; 3: call ChangeNodeState A ( root );
ProcedureUpdateCSTree A  X  n ; s ; b D s ; D 0 ; r  X  1: if n = root then 2: let Items be the set of all items that appear in b D s 3: else 5: let p i be the child node of n with item i (if it has one, otherwise p 6: for each item i 2 Items do 7: construct h i i -projected database  X  b D s  X  h i i , and update sup 8: if sup D 0  X  s p i  X  &gt; 0 ^ p i  X  null then 9: create a new node t , t : item ( i ; t : sup ( sup D 0 10: for each item i 2 IExt ={ x . item j x is a new frequent sibling node of n g[ 11: construct s p i -projected database D 0 s 12: if sup D 0  X  s p i  X  &gt; 0 ^ p i  X  null then 13: create a new node t , t : item ( i ; t : sup ( sup 14: for each item i 2 IExt [ Items do 15: if  X  sup D 0  X  s p i  X  P r ^ sup D  X  s p i  X  &lt; r  X _ X  p 16: call ConstructCSTree( p i ; s p i ; D 0 s 17: else if (sup D 0  X  s p i  X  P r  X ^ X  p i : state  X  STUB  X  ^  X j X  18: call UpdateCSTree A ( p i ; s p i ;  X  b D s  X  h i i
Procedure ChangeNodeState A ( n ) 1: if n is a new frequent node or n . state = STUB or n is a stub node in D but not in D 0 then 2: return; 3: if n  X  root then 4: if ( n . state = CLOSED ) _ ( n . state = BRIDGE ^ sup 5: insert n to H , update n and the nodes having the same ID-sum as n ; 6: for each frequent child node t of n do 7: call ChangeNodeState A ( t ); 5.1.1. Update the CSTree
In UpdateCSTree A (), lines 1 X 4 get the child nodes of n whose supports can be updated using only quent or non-zero infrequent) into the tree. To calculate the support change of p database in b D . For a given sequence a  X  a 1 } a 2 in b instance of s p i occurs before a 2 , then a does not contribute to the support increase of p support increase of p i .
 according to Theorem 4 , the node may change its state. Thus, it needs an extension. UpdateCSTree ing ConstructCSTree(). When calling ConstructCSTree(), we need to pass it  X  s projections will be discussed in the following section. Here we assume that the full projection of s
If s p i  X  X  support is greater than 0 and it has no corresponding node in the tree, a new node is added (lines 12 X 13). closed sequences are up-to-date already, so the closure checking for these nodes in ChangeNodeState unnecessary and the related cost is saved.

Finally, if there are some sequences in the projected database  X  dure to update the CSTree for frequent non-stub nodes in the original tree (lines 17 X 18). 5.1.2. Full projection optimization base comes, the buffered full projections are updated incrementally.

When we need to compute the full projection for node n (sometimes we also need to compute the full projections for by other new frequent nodes or changed stub nodes in the subtrees rooted at them. by other nodes under them. 5.1.3. Change the states of nodes the node X  X  state must have been determined in UpdateCSTree as n ,if s n @ s t ^ sup D 0 ( s n ) = sup D 0 ( s t ), then n . state BRIDGE , and if s 6 X 7 call the procedure recursively for the frequent children of n . 5.1.4. A running example base in the same figure. Initially, the call to UpdateCSTree b
At the same time, it updates the supports of the corresponding nodes at depth 1. For example node.
 pletes, a stub node D at depth 3 is added to the leftmost branch.

Then, the algorithm recurses on node D at depth 2 (under A) with according to Theorem 4 . Then the algorithm makes a recursive call on node A at depth 3 (under D) with at depth 1. After the recursive calls on the two nodes are finished, the CSTree is shown in Fig. 11 . 5.2. The IMCS D Algorithm The IMCS D algorithm is shown in Algorithm 3 , and it takes the same four arguments as IMCS and remove subtrees under these nodes. Finally, ChangeNodeState bridge nodes which do not become stub nodes after update. 5.2.1. Update the CSTree projected database in b D s , and update the support of the corresponding node p If the support of a node is zero after update, the node should not be in the CSTree anymore. IMCS dropped (lines 7 X 8).

For each child node t of p i which is not a leaf node, let x be the uncle node with the same item as t .If s update, we can safely delete node t (lines 9 X 13). If the number of sequences in the projected database  X  the same item must be frequent in D 0 , since s x is a subsequence of s their descendants) of p i that we need to delete anymore due to support decrease of the sibling nodes of p benefits we can obtain here.  X  First, we do not need to call the procedure for node p i  X  Second, according to Theorem 7 , we can safely insert the closed nodes under p which is empty at the start of IMCS D (lines 14 X 15). below) of p i and ancestors of p i can be saved (Theorem 6 ), since p
Tree D () recursively for each frequent non-stub node with j X 
Algorithm 3. IMCS D ( root , D , D , r ) 1: get b D and D 0 from D and D ; 2: call UpdateCSTree D  X  root ; / ; b D ; r ); 3: call StubCheck( root , D 0 ); 4: call ChangeNodeState D ( root );
Procedure UpdateCSTree D ( n ; s ; b D s ; r ) 1: let Items be the set { x . item j x is a child node of n } and p 2: for each item i 2 Items do 3: construct h i i -projected database  X  b D s  X  h i i , and update sup 4: for each item i 2 Items do 5: if (sup D 0  X  s p i  X  X  0) then 6: delete p i from the child nodes of n , and destroy the subtree rooted at p 7: else if (0 &lt; sup D 0  X  s p i  X  &lt; r  X ^ X  sup D  X  s 8: destroy the nodes below p i and p i . state NON-ZERO-INFREQUENT ; 9: if ( p i is not a leaf node) then 10: for each child node t of p i do 11: let x be t 0 s corresponding uncle node which has the same item as t ; 12: if (sup D 0 ( s x )&lt; r ) then 13: delete t from the child nodes of p i ; 14: if ( j X  b D s  X  h i i j X  0  X  then 15: insert the closed nodes below p i into H ; 16: if  X  p i : state  X  CLOSED  X ^ X  sup D  X  s p i  X  X  sup D 0 17: insert p i into H ; 18: if  X  sup D 0  X  s p i  X  P r  X ^ X  p i : state  X  STUB  X  ^  X j X  19: call UpdateCSTree D ( p i ; s p i ;  X  b D s  X  h i i
Procedure StubCheck ( n ;  X  D 0  X  s 1: if n is not a Type I/II/III node then 2: return; 3: if n is a Type I node or a Type II node then 4: check if n has become a stub node after DELETE; 5: if n is a stub node then 6: n . state STUB and destroy the descendent nodes of n ; 7: for each child node t of n which is of Type II/III do 8: make the full projection  X  D 0  X  s 9: StubCheck( t ;  X  D 0  X  s
Procedure ChangeNodeState D ( n ) 1: if n  X  root then 2: if ( n . state = CLOSED ^ sup D 0 ( s n ) &lt; sup D ( s 3: insert n to H , update n and the nodes having the same ID-sum as n ; 4: for each frequent child node t of n do 5: if n . state  X  STUB then 6: call ChangeNodeState D ( t ); 5.2.2. STUB checking node in the CSTree for D 0 (Theorems 7 and 9) and its descendant nodes should be destroyed. (or bridge node) in the CSTree for D ; (2) r 6 sup D 0 ( s We need to check whether possible stub nodes have changed their states after calling UpdateCSTree duce the STUB checking method, we distinguish three types of nodes:  X  Type I: the node that is a possible stub node but has no possible stub nodes as descendants.  X  Type II: the node that is a possible stub node and has one or more possible stub nodes as descendants.  X  Type III: the node that is not a possible stub node but has one or more possible stub nodes as descendants. The type of a node can be collected and recorded in UpdateCSTree the procedure recursively for each child of n that has some potential stub nodes as descendants. eCSTree D () is that it can achieve more computation sharing. After UpdateCSTree full projections, is impossible.
 duced in Section 5.1.2 to accelerate full projection computation. 5.2.3. Change the states of nodes 5.2.4. A running example Fig. 7 .

Initially, IMCS D makes a call to UpdateCSTree D ( root ; / ; nodes A,B,C are all decreased by 1.
 node B is removed from the tree. Then the algorithm makes a recursive call on node A at depth 2 (under A) with b
D
In this call, the algorithm does not find any change on child node D and j continues until all the branches of root are finished.
 parent node is a stub node in the updated CSTree. The final CSTree is shown in Fig. 6 . 5.3. Algorithm analysis
Let the number of nodes in the updated CSTree for D 0 be N 0 , the maximal sequence length in D 0 be l tree be l 0 . UpdateCSTree A () takes at most O  X  N 0 j D includes all closed nodes, stub nodes and bridge nodes in the tree. ChangeNodeStates is no less than l 0 and j U j &lt; N , IMCS A takes at most O  X  N
O  X  N j D j l 2 s  X  time where N is the number of nodes in the CSTree for D and l 6. Experimental results In this section, we perform a thorough evaluation of IMCS with 512MB memory, running Windows Server 2003. 6.1. Datasets Synthetic datasets: items and the dataset has 20K distinct items. MSNBC dataset: respectively.
 CSLOGS dataset: with average sequence length = 13.94. Its maximal sequence length is 429. dataset. Similar to the method used in [10], given a dataset D sequences in D 1 are randomly selected. Then v percent of items from the tail of the selected sequences in D and h are called vertical ratio and horizontal ratio , respectively. After the deletion, we get the dataset D between D 1 and D 2 is the incremental database, which simulates the incremental update on D use D 2 and D 1 as the original database and the updated database, respectively. For IMCS two datasets. 6.2. Comparison of IMCS A with other algorithms tial patterns from scratch, and incremental algorithms are faster than non-incremental ones. IMCS low support, there are too many non-closed patterns generated, IMCS In comparison with the closed sequence mining algorithms BIDE and CloSpan, IMCS using a fast hashing technique.
 318.65 s when min _ sup is 0.02%, and the standard error is a small value 0.87. save the patterns in a compact structure. ures that IMCS A still outperforms the other algorithms by a large margin. counting techniques used by other methods. sets, and IMCS A is always the clear winner over the other algorithms.
 exceeds 10%, BIDE outperforms IMCS A . The explanations are as follows. The main cost IMCS nodes extensions which need full projections. The main extra overhead IMCS mine the dataset from scratch. The same phenomenon is also observed for IncSpan. IncSpan uses more time than IMCS varied from 0.04 to 0.8 when h is fixed at 2%. All the algorithms show very little variation.
Tables 3 and 4 show the number of changed patterns after update when h and v are varied. The second column is the larger update. In order to further explore the joint influence of the horizontal and vertical ratios on IMCS can see that the running time of IMCS A increases smoothly with the increase of the two ratios. be inserted to the hash index without closure checking. This leads to the high efficiency of ChangeNodeState pruning operations can not be compensated by the subspace they can prune. closed. IMCS A takes only 1.55 s to mine the dataset.
 equal to the number of all frequent patterns. terns. When the min _ sup value is high, IMCS A consumes a little more memory than IncSpan. It is because IMCS maintain more information (such as node states and ID-sums) for frequent patterns than IncSpan and the number of ture to store the patterns.
 boundary.
 quences are varied, the trend of count ratios is similar to that shown in Fig. 35 . Thus, they are not shown here.  X  X  X MCS A All X  is about 4 for all support values. 6.3. Comparison of IMCS D with other algorithms Since IncSpan cannot deal with DELETE, we only compare IMCS state update methods that contribute to the high efficiency of IMCS 9N10) respectively with min _ sup fixed at 0.05%. We can observe that the running time of IMCS of distinct items is varied (D10C10T2.5N5-15). It shows that IMCS an order of magnitude.
 0.9 when h is fixed at 5%. It can be observed that IMCS D set is involved. Fig. 44 shows the running time of IMCS D spends less than 1.88 s under all the parameter settings. Overall, when the vertical ratio is low ( plains the decline of the running time when the two ratios are both high. can see that IMCS D has similar memory consumption to IMCS running time of all the algorithms increases linearly with respect to the database size. 50 is similar to that of Figs. 29 and 30 . The reasons are similar, so they are not duplicated here. 7. Discussion some research directions.

First, sometimes users may want to change dynamically the min _ sup value. crease case, the states of original frequent nodes will not change too. So it is much simpler than IMCS of the method is omitted here.
 an S-extension node according to the difference between the node and its parent node.
The definition of stub node needs to be changed to accommodate itemset sequences. Let b = h b (1 to itemset sequence here. The j th I-extension period of b in s is defined as: (1) if 1 &lt; j tween the end of the first instance of h b 1 b 2 b j 1 i in s and the first event after LF of b in s is h ( B , C ) i . The 2 nd I-extension period of b in s is h ( A , D ) i . Let S j th semi-maximum period of b in s for I-extension is defined as: ( c there exist an integer j (1 6 j 6 l ) and an item e which appears in each of the j th semi-maximum periods of s (or S-extension) in all the sequences in D which contain s not described here.
 8. Conclusions base and old sequences (or items) are deleted. Two efficient algorithms, IMCS other algorithms by a large margin.

References
