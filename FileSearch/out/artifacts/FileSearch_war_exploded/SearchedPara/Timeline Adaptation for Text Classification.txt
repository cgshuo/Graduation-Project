 In this paper, we address the text classification problem that a period of time created test data is different from the train-ing data, and present a method for text classification based on temporal adaptation. We first applied lexical chains for the training data to collect terms with semantic relatedness, and created sets (we call these Sem sets). Semantically re-lated terms in the documents are replaced to their represen-tative term. For the results, we identified short terms that are salient for a specific period of time. Finally, we trained SVM classifiers by applying a temporal weighting function to each selected short terms within the training data, and classified test data. Temporal weighting function is weighted each short term in the training data according to the tem-poral distance between training and test data. The results using MedLine data showed that the method was compa-rable to the current state-of-the-art biased-SVM method, especially the method is effective when testing on data far from the training data.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval; I.2.7 [ Computing Methodologies ]: Natural Language Processing Text Classification, Temporal Analysis
As the volume of online documents has drastically in-creased, the analysis of temporal aspects for text classifi-cation is a practical problem attracting more and more at-tention. One attempt is concept or topic drift dealing with temporal effects in classification [9]. The earliest known ap-proach is the work of Klinkenberg and Joachims [5]. They presented a method to handle concept changes with SVM. He and Parker proposed a method to find bursts, periods of elevated occurrence of events as a dynamic phenomenon in-stead of focusing on arrival rates [3]. Most of these focused just on identifying the increase of a new context, and not relating these contexts to their chronological time. Mour  X  ao et al. have shown evidence that time is an important fac-tor that should be taken into consideration in classification techniques and algorithms [7]. Adaptive document classifi-cation is one of the attempts considering temporal aspects. Rocha et al. proposed a heuristic that is able to determine temporal contexts efficiently. The temporal contexts were used to sample the training documents for the classification process [8]. The results showed that the method attained at the same accuracy of SVM classifier. However, most of the attempts use only surface information of terms (words) and ignore their senses. The meaning of a word depends on the domain in which it is used: the same word can be used dif-ferently in different domains. It is also often the case that a word which is polysemous in general is not polysemous in a restricted subject domain such as MedLine. However, even in MedLine texts, one encounter quite a large number of pol-ysemous words if the texts are collected from a long period. Because polysemous words are usually also high-frequency words, their treatment is crucial in actual applications such as text classification.

In this paper, we address the text classification problem that a period of time created test data is different from the training data, and present a text classification method based on temporal adaptation. We first applied lexical chains for the training data to disambiguate senses and create sets (Sem sets), each of which consisting semantically related terms. Semantically related terms in the documents are re-placed to their representative term. We regarded represen-tative term in each Sem set to the highest frequency in the training data. Next, for the results, we identified terms be-ing salient for a specific period of time. We call these short terms. This is done by using a term weighting method,  X  2 statistics. Finally, we trained SVM classifiers by applying a temporal weighting function to each short terms within the training data, and classified test data. A Temporal Weight-ing Function(TWF) is weighted each short term in the train-ing data according to the temporal distance between training and test data. It is based on a method of Salles et al. [10]. The difference is that we applied the function to only short terms while a method of Salles weights all terms in the train-ing documents. Because it is often the case that terms such as  X  X irus X  in Figure 1 that is salient for a specific period of time (1990) and a term such as X  X ose X  X hich is important for a long period are both included in the training documents. Figure 1:  X  X irus X  and  X  X ose X  from the MedLine data In this case, these terms included in the training documents are equally weighted, and it affects classification accuracy.
The method, Term-Based Temporal Adaptation(TBTA) for text classification consists of three steps: collection of semantically related terms, short term extraction, and text classification based on temporal weighting.
The first computational model for lexical chains was pre-sented in the work of Morris and Hirst [6]. Barzilay and Elhadad applied lexical chain algorithm to text summariza-tion [1]. We applied their technique to disambiguate term senses and collect semantically related terms. Like Barzi-lay and Elhadad X  X  method, the procedure for constructing lexical chains consists of three steps: (1) Select a set of can-didate senses of terms, (2) For each candidate term(sense), find an appropriate chain relying on a relatedness criterion among members of the chains, and (3) If it is found, insert the sense in the chain and update it accordingly. In the first step (1), we choose as candidate terms simple nouns in the training data. Relatedness of senses in (2) is determined in terms of the distance between their occurrences and the shape of the path connecting them in the WordNet. We used two kinds of relations, relation between a term and its repetition, and semantic relation between two terms con-nected by a WordNet synonymy relation. In order to collect semantically related terms, we need to identify the strongest chains among all those that are produced by the algorithm. Like Barzilay and Elhadad X  X  method, we scored chains based on (i) the number of occurrences of members of the chain and (ii) homogeneity index, i.e. , 1 -the number of distinct occurrences divided by the length. Once strong chains have been selected by scoring chains, each term in the training document within the member of a Sem set is replaced to its representative term (we call it replacement procedure). We regarded representative term in each Sem set to the highest frequency in the training data.
For the results of replacement procedure, we extracted short terms from the training data. Short terms are selected based on the use of  X  2 statistics defined by Eq. (1). We collected all the documents within a year, and make a set for each year Y i .Intheexperiments, Y i in Eq. (1) is between the years of 1982 and 2012. Using the two-way con-tingency table of a term t and the year Y i , a is the number of documents of the year Y i containing the term t , b is the number of documents of other years (not Y i ) containing t , c is the number of documents of the year Y i not containing the term t ,and d is the number of documents from other years not containing t . N is the total number of documents. Short term extraction is applied to the sets of documents with different years. Thus the larger value of  X  2 ( t, Y dicates the term t is more salient in the specific year Y We extracted terms whose  X  2 value is larger than a certain threshold value. We regarded these as salient terms in a specific period of time.
TWF we used is based on a method of Salles [10]. We ap-plied TWF to the extracted short term in the training data while Salles et al. applied it to the training documents, i.e. , each term in the document is equally weighted. The function uses the temporal distance between training and test documents creation times. The weights refer to the relatedness of term-category relationships. It is defined as dominance ( t, c )= N tc of documents in category c that contain term t . When the dominance dominance ( t, c ) is larger than a certain threshold value  X  1 , the term is judged to have a high degree of exclusiv-ity with some category. Let S t = {  X   X  p n  X  p r | X  rp n be a set of temporal distances that occur on the stability pe-riods of term t . Here, p n be the time of creation concerning to a training document. Stability periods of term t , referred to as S t,r is the largest continuous period of time, starting from the reference time point p r in which the test docu-ment was created. For instance, if S t,r is { 2000,2001,2002 and p r = 2001, then S t = { -1,0,1 } . Finally, the function is determined considering the stability period of each term as a random variable. As shown in Table 1, we consider the random variable D  X  related to the occurrences of  X  ,which represents the distribution of each temporal distance  X  i all terms t . D  X  is lognormally distributed if InD  X  is nor-mally distributed. We considered a mixture of two Gaus-sians F ( x )= F 1 ( x )+ F 2 ( x )tofit D  X  . Here, F i ( x ) denotes a 3-parameter Gaussian function, F i ( x )= a i e the parameter a i is the height of the curve X  X  peak, b i is the position of the center of the peak, and c i controls the width of the curve. These parameters are estimated by using a Maximum Likelihood method.

We used TWF for text classification. Each document in the training data is represented by terms including short terms. For each short term t in the training documents, t is weighted by F (  X  )where  X  denotes temporal distance between training and test data. We trained SVM classifiers, and classified test documents.
We investigated different values for  X  ,andset  X  = 50% in the experiment
We chose the MedLine data for evaluation. It consists of 357,964 documents, with documents from 1982 to 2012 clas-sified into 7 distinct classes, AIDS, Bioethics, Cancer, Com-plementary Medicine, History of Medicine, Space Life Sci-ences, and Toxicology. We obtained a vocabulary of 66,464 unique noun words after eliminating words that occurred only once, stemming by a part-of-speech tagger [11], and stop word removal. We divided all the documents into two sets: one is used to estimate the number of extracted terms weighted by  X  2 statistics and 3 parameters Gaussian func-tion. For the results of lexical chains, we extracted short terms by using  X  2 statistics. We applied  X  2 statistics to the data with and without lexical chains to examine the ef-fect of lexical chains. The numbers of short terms extracted from the former and the latter were 3,000 and 4,000, respec-tively. We assumed that the labeled training data is only a data from one year as our goal is to achieve classification accuracy from the labeled training data with only a limited period of time as high as that from those with a whole period of time. We calculated dominance ( t, c ) by using the 1997 labeled training data. The estimated parameters used in a Gaussian function are shown in Table 2. We divided another data consisting of 178,982 documents into three equal sizes for each year in order to employ three cross validation. We obtained the average accuracy of the individual classifiers across the three folds. The number of training/test data in each category is shown in Table 3. We used the SVM-Light package for training and testing [4] 2 . We used a linear kernel and set all parameters to their default values.

We compared our method with three baselines: (1) SVM, (2) a method developed by Salles et al. [10], and (3) biased-SVM. We chose Salles et al. method because it is very similar to our framework as the method applies TWF to each document. Biased-SVM is the state-of-the-art SVM method, and often used for comparison [2]. For biased-SVM, we used training data and classified test documents directly. We empirically selected values of two parameters,  X  c  X  (trade-off between training error and margin) and  X  j  X , i.e. , cost (cost-factor, by which training errors on positive documents) that optimized the F-score obtained by classi-fication of test documents. The results, the micro-averaged http://svmlight.joachims.org F-score across the three folds are shown in Table 4. The top of Table 4 shows the results obtained by using lexical chains and the bottom refers to the results without lexical chains.  X   X   X  in Table 4 shows that our method, TBTA is statistical significance t-test compared with the  X  marked methods. In contrast, bold font in Table 4 indicates that the result ob-tained by biased-SVM is statistically significant compared with the result obtained by TBTA.
 As can be seen clearly from Table 4 that micro-averaged F-scores obtained by the use of lexical chains were better than those without lexical chains in all of the methods. Ta-ble 5 shows examples of a set of semantically similar terms obtained by the lexical chain technique in each category.  X  X et X  in Table 5 refers to the total number of sets. Tables 4 and 5 indicate that lexical chains are effective for text clas-sification. Table 4 also shows that the overall performance obtained by TBTA were better than those obtained by other three methods, while some results such as  X  X IDS X  X nd X  X an-cer X  obtained by biased-SVM were better than TBTA. How-ever, in biased-SVM, it is necessary to run SVM many times, as we searched  X  c  X  X nd X  j  X . In contrast, TBTA does not re-quire such parameter tuning. Table 5: A set of terms extracted by lexical chains
We note that we used TWF to improve classification ac-curacy, especially when the test data is far from the train-ing data. Figures 2 and 3 illustrate macro-averaged F-score against temporal distances between training and test data in each method. Both training and test data are the docu-ments from 1982 to 2012. For instance,  X 10 X  of the x -axis in Figures 2 and 3 indicates that the test documents are cre-ated 10 years later than the training documents. We can see from both Figures 2 and 3 that TBTA was the best in all of the temporal distances. There are no significant differences among four methods when the test and training data are the same year. The performance of these methods including our method drops when the creation time point of test data is far from the training data. However, the performance of TBTA was still better to those obtained by other methods. This demonstrates that the algorithm which applies TWF to each term is more effective than the method applying it to each document in the training data.
We have developed an approach for text classification based on temporal adaptation. We applied lexical chains, and TWF to each short terms within the training data. The results using MedLine data showed that micro-averaged F-score obtained by our method was 0.891 and it was better than those obtained by SVM (0.844), Salles et al. method (0.850), and biased-SVM method (0.855). Moreover, we found that the TBTA is more effective than other methods, especially when a period of the time created test data is far from the training data. Future work will include: (i) com-parison to other feature extraction techniques, e.g. , burst models [3], and (ii) applying the method to other data such as ACM-DL and News articles for quantitative evaluation. [1] R. Barzilay and M. Elhadad. Using Lexical Chain for [2] C. Elkan and K. Noto. Learning Classifiers from Only [3] D. He and D. S. Parker. Topic Dynamics: An [4] T. Joachims. SVM Light Support Vector Machine. In [5] R. Klinkenberg and T. Joachims. Detecting Concept [6] J. Morris and H. Hirst. Lexical Cohesion Computed by [7] F. Mour  X  ao, L. Rocha, R. Araujo, T. Couto, [8] L.Rocha,F.Mour  X  ao, A. Pereira, M. A. Gon  X  calves, [9] G. J. Ross, N. M. Adams, D. K. Tasoulis, and D. J. [10] T. Salles, L. Rocha, G. L. Pappa, F. Mour  X  ao, W. M. [11] H. Schmid. Improvements in Part-of-Speech Tagging
