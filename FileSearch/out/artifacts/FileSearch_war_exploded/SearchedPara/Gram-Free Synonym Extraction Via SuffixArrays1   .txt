 This paper considers a problem of extracting synonymous strings of a query given by users. Synonyms, or paraphrases, are words or phrases that have the same meaning but different surface strings.  X  X DD X  and  X  X ard Drive X  in docu-ments related to computers and  X  X BS X  and  X  X essage Boards X  in Web pages are examples of synonyms. They appear ubiqu itously in different types of documents because, often, the same concept can be described by two or more expressions, and different writers may select different words or phrases to describe the same concept. Therefore, being able to find su ch synonyms significantly improves the usability of various systems. Our goal is to develop the algorithm that can find synonymous strings to the user input. Applications of such an algorithm include augmenting queries with synonyms in IR or text mining systems, assisting input systems by suggesting expressions similar to users X  input, etc. One main prob-lem in such tasks is that we do not know what kind of queries will be posted by users. For example, consider a system that calculates similarities between all the pairs of words or noun phrases in a corpus and provide a list of synonyms of a given query based on such similarity. Such systems can not return any output for queries that are neither words nor noun phrases, such as prepositional phrases like  X  X n the other hand. X  Of course, this problem can be solved if we have simi-larities between every substrings in the c orpus. However, considering all strings (or n-grams) in the corpus as synonym candidates greatly increases the number of synonym candidate pairs, which makes computation of similarities between them very expensive in time and space.

We avoid this problem by abandoning extraction of synonym candidates in ad-vance. Instead, we provide an algorithm to retrieve synonyms of user X  X  queries on the fly . This goal is achieved by utilizing suffix arrays . Suffix arrays are efficient data structures that can index all substrings of a given string. By using them, the system can extract dynamica lly contexts to calculate sim ilarities between strings. By extracting contexts for the query string, and subsequently by extracting strings that are surrounded by the contexts, synonym candidates can be retrieved in rea-sonable time. As a result, the system that allow many types of queries, such as  X  X n the other hand X ,  X  X e propose that X ,  X :-) X ,  X  X -mail: X , etc., is realized.
Our task is to extract synonymous expressions regardless of whether strings have similar surfaces ( e.g., having many characters in common,) or not. In such a situation, similarity calculation is typically done by using contexts .Thestrategy is based on the assumption that  X  X imilar words appear in similar contexts. X  Some previous systems used contexts based on syntactic structures like dependencies [1] or verb-object relations [2][3], but we do not use this type of contexts for the simplicity of modeling and language independency, as well as the fact that our goal is to develop a system that accepts any kinds of queries ( i.e., independent from grammatical categories), although incorporating such kinds of contexts into our suffix-array based algorithm is an interesting issue for future work. There also exist studies on the use of other resources such as dictionaries or bilingual corpuses [4] [5], but we assume no such outside resources to make our system available to various kinds of topics and documents. Another type of contexts is surrounding strings (i.e., strings that appear near the query). [6] reported that surrounding strings (which they call proximity ) are effective features for syn-onym extraction, and combining them with other features including syntactic features stabilized the performance. Using long surrounding strings [7] can spec-ify paraphrases with high precision but low recall, while using short surrounding strings [8] extracts many non-synonyms (i.e., low precision), which make systems to require other clues such as compara ble texts for accura te paraphrase detec-tion. We use such surrounding strings i.e., the preceding and following strings, as contexts in our system. In addition, the contexts in our system can be any length to achieve a good pr ecision-recall balance.

The remainder of this paper is organi zed as follows. Section 2 introduces notations used in this paper and makes a brief explanation of suffix arrays. Section 3 describes our algorithm and Section 4 and 5 reports the experimental results. In Section 6, we concludes t his paper and discuss future work. The input to the system is a corpus S and a query q . Corpus S is assumed to be one string. For a set of documents, S is a result of concatenating those documents into one string. The system finds synonyms of q from S .
In this paper, context of the string s is defined as the strings adjacent to s , i.e., s.x  X  S or x.s  X  S where . represents the concatenation operation and x  X  S means x appears in corpus S .If s.x  X  S , x is called right context of s .Onthe other hand, if x.s  X  S , x is called left context of s .

Suffix arrays [9] are data structures that represent all the suffixes of a given string. It is a sorted array of all suffixes of the string. By using the suffix array constructed on the corpus S , all the positions of s in S can be obtained quickly (in O ( logN ) time, where N is the length of S ) for any s . They require 4 N bytes 1 of additional space to store indexes and even more space for construction. We assume that both the corpus and the suffix array are on memory. 2
The algorithm uses two suffix arrays: A and A r . The former is constructed from S , and the latter is constructed from rev ( S ), where rev ( x ) is a reverse operation on string x . Right contexts are retrieved by querying A for q and left contexts are retrieved by querying A r for rev ( q ).

We define two operations nextGrams ( A ,x )and freq ( A ,x ). The former re-turns the set of strings in A whose prefix is x and whose length is one larger than x , and the latter returns the number of appearance of x in A .Wealsowrite them as nextGrams ( x )and freq ( x )if A is obvious from contexts.

We use a sorted list cands and a fixed-size sorted list results . 3 Cands retains strings that are to be processed by the algorithm, and results retains a current top-n list of output strings. Elements in cands are ranked according to a priority function priority ( x ), and elements in results are ranked according to a score function sc ( x ). We define priority ( x ) to be smaller if x has larger priority ( i.e., more important), and sc ( x ) to be larger if x is more important ( i.e., relevant as synonyms). Note that elements in both lists are sorted in the ascending order. getFirst operations therefore return t he most important element for cands and the least important element for results . This means that getFirst operation of results returns the N The algorithm is divided mainly into two steps: context retrieval and candidate retrieval. The context retrieval step finds top-N 1 (ranked by the score defined below) list of left contexts and right contexts. 4 After that, the candidate retrieval step extracts top-N 2 list of candidates for synonyms. 3.1 STEP-1: Context Retrieval The context retrieval step harvests the contexts, i.e. , strings adjacent to the query. For example, a left context list for the word example might include the string in the following .Wesetparameter N 1 that indicates how many contexts are harvested. We only explain how to extract right contexts, but left contexts can be extracted in a similar manner.

Figure 1 shows the context retrieval algorithm for right contexts. Note that  X  X  indicate a length-zero string. We also write removing context string c from string s as cut ( s, c ). Starting from a set cands = {  X  X  } , the search proceeds by expanding the length of strings in cands .(Forexample,element bye may be added to cands when by is in cands .) Note that this strategy causes search spaces very large because string lengths possibly increase to the end of the corpus. Our idea to avoid this problem is to cut off unn ecessary search spaces by terminating string search if the score of current string and their children ( X .e., the strings generated by adding suffixes to the current strings,) must be lower than the current N 1 -thscore.Wecallsuchtermination pruning of search spaces.

The score for context strings are defined as follows, by analogy with tf-idf scoring functions. where | S | is a size of corpus S . We also define the score for pruning as Note that sc c ( x ) can be used as the upper bound of sc c ( x.y ) for any y be-sc Threshold Values. We introduce the parameter F 1 to reduce execution time of our algorithm. F 1 is set not to include contexts that appears too frequently in the corpus. If a context freq ( c )isover F 1 , c is not added to results . 3.2 STEP-2: Candidate Retrieval After context strings are obtained, the algorithm extracts strings adjacent to the contexts. We refer to the strings as synonym candidates ,orsimply candidates . We set the parameter N 2 that indicates the number of candidates to be retrieved. The algorithm proceeds in the following way.
 Stage-1: obtain the N 2 -best candidates by using le ft contexts only, according Stage-2: obtain the N 2 -best candidates by using right contexts only, according Stage-3: rerank all obtained candidates according to score function sc ( c )and
Roughly speaking, in stage-1 and 2, the algorithm searches for top-N 2 syn-onym candidates by using the score which is relatively simple but useful for pruning of search spaces. After that, in stage-3, the algorithm re-ranks these top-N 2 results by using a more complex scoring function. Here, we only explain stage-1, because stage-3 is straightforward and stage-2 can be performed in a similarmannertostage-1.

Figure 2 shows the algorithm. Here, C l represents a left context set. Note that elements in cands are pair ( c, x ), where the list is sorted according to priority ( x ). priority ( x ) is defined to rank strings with the highest score come to the first. The algorithm first makes a set D by expanding the current best candidate x . After that, for each y  X  D ,if sc l ( y ) is larger than the current N 2 -th score, y is newly added to cands .

The score sc l ( x ) is defined as the number of c  X  C l for which c.x  X  S , i.e., how many types of left contexts appearing adjacent to x . The good point of this This means that if the sc l ( x ) is lower than the current n -th best score, there is no need for searching for x.y for any y .

On the other hand, the score in stage-3 is defined as where C r represents a set of right conte xts extracted in step-2, and freq exp ( x ) is the frequency of x expected from the context frequency and the number of x appearing in the whole-corpus, defined as where | S | is the size of corpus S .
 List Cleaning. Obtained lists of contexts often contain redundant elements be-cause it contains strings of any length. For example,  X  X ave to do X  and  X  X ave to do it X  can be in the same list. To remove such redundancy, list cleaning is per-formed on each context list. If the n -th element is a substring of the m -th element or m -th element is a substring of the n -th element for m&lt;n ,the n -th element is removed from the list. Not only it reduces the execution time by reducing the number of contexts, but also we observed that it generally improves the quality of extracted results mainly because it prohibits similar contexts from appearing repeatedly in the same list. List cleaning is also performed on candidates lists. We observed that it also improved the quality of candidate lists. Note that list cleaning operations make the size of resulting lists smaller than N 1 or N 2 . We applied our algorithm to the web documents crawled from the web-site of University of Tokyo. 56 The size of corpus was about 800 Mbytes and parameters were set to F 1 = N 1 = N 2 = 1000. The system was run on an AMD Opteron 248 (2.2GHz) machine with 13Gbytes memory. Figure 3 shows some example results of synonym extraction. 7 Both results were obtained in a few seconds. We observed that phrases like  X  X atural Language Processing X  were correctly associated with the one word string  X  X LP X  without any preprocessing like NP chunkers. In addition, phrases like  X  X e propose X  that are not in one phrase structure category (like NP or VP), which are difficult to chunk, were able to be processed thanks to the property of our method that takes into account every-length string.
We also ran the system with a part of the Reuter corpus (487 MBytes) on the same machine with the same parameter settings. Figure 4 shows the extraction results for some example queries. We observed that the results for acronym query  X  X .S. X  correctly included  X  X n ited States X , and the results for contraction query  X  X oesn X  X  X  correctly included  X  X oes not X . We used the JAL (Japan Airlines) pilot r eports which had been de-identified for data security and anonymity. The re ports were written in Japanese except for some technical terms written in English. The size of concatenated documents was 7.4 Mbytes. The system was run on a machine with an Intel Core Solo U1300 (1.06GHz) processor and 2GBytes memory. 8
Many expressions that have their synonymous variants are found in this cor-pus, such as loan terms that can be written in both Japanese and English, and long words/phrases that have their abbreviated forms (e.g.,  X  X DG X  is an abbre-viation of  X  X anding X ), etc.

In order to evaluate the performance of the system, we used a thesaurus for this corpus that are manually developed and independent of this research. The thesaurus consists of ( t, S ( t )) pairs where t is a term and S ( t )isasetofsynonyms of t ,suchas(CAPT, { Captain } )and(T/O, { Takeoff } ), etc. In the experiment, t is used as a query to the system, and S ( t ) is used as a collect answer to evaluate the synonym list produced by the system. The number of queries was 404 and the average number of synonyms was 1 . 92.

The system performance was evaluated using average precision [10]. We pro-vided each query in the test set to the system, which in turn returns a list of synonym candidates c 1 ,c 2 , ..., c n ranked on the basis of their similarity to the query. Given this list and synonym set S = { s 1 ,s 2 , ..., } , the average precision of the result list is calculated as where precision ( k ) is the accuracy (i.e., ratio of c orrect answers to all answers) of the top k candidates, and r k represents whether the k -th document is relevant (1) or not (0). (In other words, r k =1if c k  X  S ,and r k =0otherwise.)
We investigated the execution time of the algorithm and average precision values for various parameter values. N 1 parameter was set to 1000. Table 1 shows the results. We observed that setting F 1 threshold value contributed to improvement both of execution time and average precision. Among them, larger parameter values contributed to improvement of output quality, at the expense of execution time. The best result was obtained when F 1 = N 2 = 1000. The execution time for that s etting was 1.96 seconds.

To analyze quality of outputs of our method, we compared them with the output by the standard vector space model algorithm (VSMs) with cosine simi-larity measure. Two major features for synonym extraction, namely, surrounding words (or proximity )[6],and dependency relations [3], were used for the vector space model. We defined three types of weighting schemes for vector values: term frequency (TF), tf-idf values (TF-IDF), and logarithm of TF (logTF). 9 The window size for proximity was set to 3.

We compared our algorithm with VSMs on the task of candidate sorting , where the task is to make a ranked list of c  X  T ,where T is a set of words in the thesaurus, according to the similarity to the query q . Ranked lists were made from outputs of our method by filtering out elements in output candidate list if they were not T . 10 Note that this task is slightly different from the synonym extraction because candidates outside of T is ignored, and therefore the aver-age precision values are higher than the values in Table 1. Parameters of our algorithm was set to F 1 = 1000 ,N 2 = 1000.

Table 2 shows the result. Among VSMs, we observed that proximity features were effective for synonym extraction a nd the performance was improved by using dependency features. This result agreed with the results reported in [6].
Among three weighting schemes, logTF weighting performed much better than other two schemes. We think that it is because logTF emphasizes the number of types of context words than their frequency, and the number of types of context words shared by the query is important for synonym extraction.
 The performance of our method was slightly inferior to the best results among VSMs. We think that it was partly because our context features can not handle relations among strings separated by various strings, and partly because our method does not retrieve all the possibl e strings which cause some answers in T to be not contained in resulting candidate lists. We proposed a method to extract synonymous expressions of a given query on the fly by using suffix arrays. Experimental results on 7M bytes corpus showed that our method was able to extract synonym s in 0.7  X  7.0 seconds. However, quali-tative performance of our method was slightly worse than standard vector space model methods. It suggests that our method still leaves room for improvement by, for example, extracting synonymous expressions of context strings them-selves and using them as new contexts for synonym extraction. Future work also includes exploring possibility of use of other kinds of features like dependency structures in our suffix-array based re trieving method. Use of more efficient suffix-array implementations like compressed suffix arrays is also an important issue for future work.

