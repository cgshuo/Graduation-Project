 Nearest neighbor proximity search in large graphs is an impor-tant analysis primitive with a variety of applications in graph data from different domains. We propose a novel proximity measure for weighted graphs called Effective Importance which incorporates multiple paths between nodes and captures the inherent structural clusters within a network. We develop effective bounds on the EI value using a modified small subnetwork around a query node, en-abling scalable exact nearest neighbor (NN) search at query time . Our NN search does not require heavy offline analysis or holistic knowledge of the graph, making our method suitable for very large dynamically changing networks or composite network overlays.
We employ our NN search algorithm on social, information and biological networks and demonstrate the effectiveness and scala-bility of the approach. For million-node networks, our method retrieves the exact top 20 neighbors using less than 0 . 2% of the network edges in a fraction of a second on a conventional desk-top machine. We also evaluate the effectiveness of our proximity measure and NN search for three applications, namely (i) finding good local clusters, (ii) network sparsification and (iii) prediction of node attributes in information networks. The EI measure and NN search method outperform recent counterparts from the literature in all applications.
 H.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Infor-mation Search and Retrieval Effective Importance, Graph Search, Nearest Neighbors, Random Walks, Sparsification
The common ground for graph data of different genres and from diverse domains is the need for flexible and scalable analysis meth-ods. One important analysis task is search for nearest neighbors of a node based on a proximity measure that reflects the overall graph structure. Given a large network and a query node of interest, how can we quickly compute its exact k Nearest Neighbor ( k NN) nodes without relying on precomputed network summaries? The latter re-quirement is important as real networks change frequently and/or include relations of different types whose contribution to proximity may vary per application (query).

A network proximity measure is a key ingredient for robust near-est neighbors search. A good proximity measure should reflect the network structure, beyond the immediate neighbors. Nodes within a network community are intuitively closer than nodes that do not cluster together. Consider, for example, an uniformly weighted net-work of two clusters that are connected by a single link. Although the nodes adjacent to this link are direct neighbors, they belong to different communities and are expected to be less similar (close) than direct neighbors within the community.

A robust proximity measure employed for social, information and biological networks should also allow for edge weights since in many applications links have different strengths affecting how close/similar a given neighbor is. For example, interactions in gene networks have levels of certainty and magnitude captured by their method of detection. Similarly, co-authorship links in a sci-entific collaboration network are  X  X tronger X  for collaborators with multiple joint manuscripts or when they have collaborated on a manuscript of high impact. A proximity measure and NN search methods should not be agnostic to the strength of links in the net-works.

We define a proximity measure for weighted undirected net-works, called Effective Importance (EI) which captures the local community structure of a node. EI exhibits advantageous theoret-ical properties that allow efficient local computation of top neigh-bors of a target node in an online fashion and without exploring the whole graph.

A scalable NN search provides an important tool for exploration and analysis of networked data. It allows for characterization of the neighborhood of a node and enables diverse applications, such as local community identification, anomaly node detection, network sparsification, link prediction and collaborative filtering. For ex-ample, in the case of community identification, one is interested in the underlying network clusters, formed due to network connec-tions. One can approach this problem by first obtaining the near-est neighbors of nodes in the network and then merging nodes of significantly overlapping neighbor sets. Such application scenar-ios demand a scalable and flexible NN search solution suitable for large and dynamic networks.

Existing work on proximity in networks adopts either indexing of the network to facilitate online queries [14, 19], or proximity ap-proximations that work well in practice but lack theoretical accu-racy guarantees [7, 36]. Indexing approaches are not feasible when the underlying network changes or when search is performed in a composite network, constructed according to query-specific prior-itization. Approximations may be undesirable when accuracy is a priority. In addition, previous approaches are not flexible enough to handle prioritization of layers at query time in composite networks.
We propose a scalable solution for nearest neighbor search that explores a small subnetwork around the query node, sufficient to retrieve the exact k nearest neighbors. Instead of relying on of-fline indexing, we adopt scalable online pruning of infeasible NN candidate based on bounds of their proximity to the query. The unique advantage of our online pruning, enabled by local bounds computed from scratch, lies in its direct applicability to nearest neighbors search in composite, query-centric mixtures of networks and large and dynamic social (Facebook, Twitter) or information (Wikipedia, the Internet) networks undergoing frequent updates. Our contributions in this paper are as follows: Novelty. We introduce the Effective Importance (EI) network prox-imity measure that captures the local community structure, and show that it can be computed efficiently with guarantees. Scalability. We propose a scalable and accurate online algorithm for NN search based on EI in large networks. Our algorithm re-trieves the top neighbors 100 times faster than an exhaustive tech-nique while using less than 0 . 2% of all network edges. Applicability. We demonstrate that our EI proximity measure and scalable NN search have high utility and outperform alternatives from the literature in several applications, including local partition-ing, graph sparsification and attribute prediction.
The problem of NN search in networks has been previously ad-dressed in the context of different network types and in terms of different proximity measures. Shortest paths have been proposed for NN search on road networks [17, 29]. This measure, how-ever, disregards the multiplicity of (sub-optimal) paths as well as the degrees of connecting neighbors along the shortest path. An-other proximity measure that captures the effect of multiple paths is the maximum flow between two nodes [11]. However, it does not penalize longer paths and can be sensitive to small perturba-tions, since it depends on the capacity of the bottle-neck between the source and destination nodes.

Network proximity has also been modeled by effective conduc-tance (EC) in an electrical circuit corresponding to the network graph [13, 21, 33]. EC does not account for large degree nodes, connected to multiple small degree nodes [13]. Drawbacks of EC, are addressed by either introducing a universal sink [13, 33] or by considering a cycle-free version of the effective conductance [21]. Different from such topology augmentations, our EI measure han-dles large degree nodes without the need of additional parameters. The stationary probability of random walks with restarts (RWR) has also been used as a proximity measure [34, 35], employing pre-computation of a low-rank approximation of the adjacency ma-trix (or Laplacian) of the graph.The latter methods are not applica-ble to dynamic graphs as the low-rank approximation procedure is too expensive to perform online. In addition, the RWR stationary probability is not well-suited for a proximity measure due to its bias towards high degree nodes. Our EI measure is related to the RWR probability, but addresses the problem of high degree neighbors and captures the community structure.

The use of random walk average commute time as proximity measure was proposed by Sarkar et al. [30, 31]. GRanch [30] adopts a truncated version of the commute time, in which walks are restricted to a threshold length, and discovers closest neigh-bors according to this measure. In a later work, the authors pro-pose a better running time algorithm for truncated commute time, based on sampling [31]. Our EI NN search algorithm, in contrast, does not require expensive dynamic programming, and the top-k guarantees we provide are deterministic as opposed to probabilis-tic. Moreover, our method examines a query node X  X  locality adap-tively based on decreasing proximity as opposed to defining a fixed hop-based truncation of the random walks adopted by GRanch .
Another proximity measure for general metric spaces (complete graphs) based on a cover tree [8] has been considered for applica-tions such as storytelling in entity networks [16]. Scalability in this context is achieved by avoiding consideration of all-to-all relation-ships via precomputing a cover tree. Kumar and colleagues [22] propose another proximity measure and corresponding algorithms for the case of bipartite graphs. Different from all the above, we consider general domain-specific graph structures (as opposed to complete graphs or bipartite graphs), and do not impose a prelimi-nary sparsification step (such as the cover tree approach) to manage complexity, but instead achieve scalability via local online expan-sion around the query node.
 Scalable RWR computation has been addressed in the World Wide Web and data mining communities [12, 14, 15, 19, 36]. There are three major optimization directions for RWR from a single target: off-line index construction that facilitates on-line proxim-ity queries [19], inexact evaluation [10] or a combination of the two [14]. Indexing approaches are not suitable for dynamic net-works possibly featuring distinct edge types, since the index, would need to be precomputed for every mixture of edge types and after every change in the structure. In addition, the above methods are tailored towards approximating the actual shape of the stationary distribution of RWR, while we aim at computing the top neighbors with guarantees.

Our proximity measure is related to a recent graph partitioning method proposed by Andersen et al [4]. The method approximates a random walk with restarts and performs a sweep on the degree-normalized RWR vector to obtain a small-conductance cut. The degree-normalized stationary probabilities were originally used by Lovasz et al to prove a mixing rate result for Markov Chains [24]. This normalized stationary probability renders nodes from the same network cluster closer than nodes from different clusters. We call this quantity Effective Importance (EI) and adopt it as a proximity measure. We establish new theoretical properties of EI that enable our fast online and index-free NN search algorithm.
In this section we define our proximity measure and establish its theoretical properties. We leverage these properties to construct efficient locally-computed bounds to the proximity of candidate neighbors to a query node. The bounds are then employed for a scalable online NN algorithm.
 A network is modeled as a weighted undirected graph G ( V,E,W ) , where V is the set of nodes, E is the set of edges, and W is a mapping of the edges to real weights W : ( i,j )  X  w ij , ( i,j )  X  E . The volume of a node w i is the sum of the weights of its adjacent edges w i = P ( i,j )  X  E w ij . The transition probabil-ity of a random walk from node i to node j is defined as w Markov chain corresponding to the graph random walk is ergodic then its stationary behavior is governed by a unique distribution of state visit probabilities, called stationary distribution . The sta-tionary probability for RWR of node j , if restarting to node r with probability  X  , is denoted as  X   X  r ( j ) . It can be expressed according Figure 1: A small network example with target node 1 (a). Av-erage conductance of cuts based on Effective Importance (EI), random walks with restarts (RWR) and Shortest paths (SP) in a protein interaction network YeastNet[25] (b). to the balance condition as:  X  r ( i ) = We will omit the  X  superscript when the context allows it.
Next, we introduce our proximity measure Effective importance (EI) and discuss (i) theoretical results that support EI X  X  quality and (ii) present empirical evidence for its superiority over alternatives.
D EFINITION 1. The Effective importance (EI) of a node is its volume-normalized RWR stationary probability: q  X   X  ( i ) /w i .

For zero restart probability (  X  = 0) on undirected graphs, EI is a constant 1 / P j  X  V w j over all nodes [23]. For increasing  X  , the EI of the restart node X  X  neighbors that keep the walker in the restart node X  X  proximity increases. Such close neighbors receive more visits per adjacent edge than nodes further away or nodes that let the walker  X  X scape" to other parts of the network. EI reflects the community structure around the query node, by associating high proximity values to nodes that are in the same cluster as the query node. No explicit cluster detection is performed in order to achieve the above goal. Alternative proximity measures based on shortest paths (SP) or RWR [34, 35]) are not suitable for capturing the com-munity structure, because they either disregard multiplicity of paths (SP) or are sensitive to very high degree nodes (RWR). In the rest of this section we discuss the superiority of EI in capturing the effect of network clusters, compared to alternative proximity measures.
We first illustrate the effect of using EI as a proximity measure in a small example graph in Fig. 1(a) where node 1 is chosen as a query node. Edges between nodes designate similarity and for the sake of clarity they are unweighted (i.e. all edge weights equal 1 ) in this example. For this example network, using shortest paths as proximity from node 1 will render all of its direct neighbors equally close. Alternatively, RWR ranks nodes 5 and 6 as the nearest neigh-bors of 1 , due to their high degree compared to the other neighbors of 1 . If we consider the clustering structure of the graph, however, node 6 should not be the closest neighbor, because 2 , 3 and 4 form a better (exclusive) cluster with 1 than 6 does. In order to separate nodes { 1 , 6 } from the rest of the network we need to cut 11 edges, compared to only 3 edges in order to separate { 1 , 2 , 3 , 4 } . Node 6 is similar (connected) to a number of nodes, which are neither similar to node 1 , nor to 1  X  X  neighborhood and intuitively, it should not be 1  X  X  closest neighbor.

If proximity is measured according to EI, however, nodes 3 , 2 and 4 are closer to 1 as they connect and are similar exclusively to 1 and among themselves. EI measures the number of visits per adjacent edge and thus ranks high nodes that connect well with the query and contains the random walk in its vicinity.

The existing literature on local partitioning also provides im-plicit support for the utility of EI to render well-clustered neigh-bors around a query node close [3, 4]. All above methods seek to obtain a good-quality local cut around a target node by performing a sweep based on normalized RWR probabilities (our EI measure). Particularly Andersen and colleagues [4] show that for any set of nodes C of conductance  X  , one can produce a cut of conductance O ( q  X  log P in C . While the above methods use implicitly the EI as a ranking quantity (they do not study explicitly the notion of EI as a proxim-ity measure), their goal is local partitioning. In this work, we prove useful local properties of EI that enable its efficient computation and its applicability for NN search in large graphs.

Apart from the above theoretical support for EI X  X  utility in cap-turing local clusters, we also evaluate it in a real world biological network Fig. 1(b) as well as in several more networks. To quantify the difference of proximity rankings, we evaluate the  X  X ightness X  of the clusters induced by the subgraph containing the closest neigh-bors. We consider the top EI, RWR and SP neighbors of randomly chosen nodes and measure their separation from the rest of the net-work in terms of cut conductance [9] (see Experiments for exact definition and more networks). The smaller the conductance the better the separation of the nodes on both sides of the cut. EI top neighbors exhibit a consistently lower average conductance (com-prise better clusters) than RWR and SP for varying number of top neighbors k 1(b). RWR X  X  worse performance can be explained by its preference for high degree nodes, while SP suffers from its in-ability to account for multiplicity of paths.

The ability of EI to rank high well-clustered neighbors of the query coupled with the theoretical properties we derive in the fol-lowing sections make it a useful graph measure for large dynamic networks and network overlays.
When  X  &gt; 0 , there is a bias in the Effective Importance, causing neighbors of the restart node to receive more visits per adjacent edge. As a result, every node in the network, excluding the restart node r , has at least one neighbor of higher EI.

L EMMA 1. (Unbalanced EI) For a random walk with  X  &gt; 0 and restart node r :  X  i 6 = r,  X  j { ( j,i )  X  E,q r ( i )  X  (1  X   X  ) q The first three equations follow from the definition of EI, and the balance equation. The inequality follows from the algebraic in-equality ab + cd  X  max ( b,d )( a + c ) ,a,b,c,d  X  0 . Finally, we use the definition of the volume of a node.

Lemma 1 is central to deriving a bound to all nodes in the net-work based on a subgraph around the query.For the rest of our dis-cussion, we assume a partitioning of the nodes in the graph in three sets: (i) a set of known (active) nodes K ; (ii) a set of fringe nodes F , directly connected to nodes in K ; and (iii) the set of all other nodes U . The restart node r is a member of K and there is no direct edge between K and U . Our NN algorithm will operate on the set K , using lower and upper bounds to all network nodes, also com-puted within K . The smaller the active set K , the lower the online search running time.

If the EI values of nodes in F are available, we can established an upper bound on the EI of all nodes in U as follows.

T HEOREM 1. (Bound on EI in U)
P ROOF . Choose  X  u to be the node in U with largest q r from Lemma 1 and using the fact that u  X  U  X  u 6 = r it follows that  X   X  f { (  X  f,  X  u )  X  E, (1  X   X  ) q r (  X  f ) &gt; = q of  X  u as the node of maximum EI in U , it follows that  X  node  X  f is also not in K since there are no edges between U and K . The only possibility is that  X  f  X  F as the three sets are mu-tually exclusive. Then we get the following chain of inequalities: q ( u )  X  q r ( X  u )  X  (1  X   X  ) q r (  X  f )  X  (1  X   X  ) max f  X  F every u  X  U .

Note that according to Theorem 1, there are no restrictions on how the set K S F is chosen as long as it contains the restart node. This theorem provides a powerful mechanism for pruning irrelevant candidates residing in U without the need of considering them as part of the active graph used at query time. If we find a way of obtaining the largest actual EI within the fringe set F , we can use (1  X   X  ) -fraction of it as an upper bound to all unobserved nodes in U . Obtaining the actual EI in F , however, requires computing the stationary distribution of the whole graph. Instead, we will use this theorem together with upper bounds to the EI in F derived based on analyzing the subgraph induced by K S F .

Next, we define our lower and upper bound constructions for the values of EI within the active set K . First we introduce some additional notation. The stationary probability of RWR can be ex-pressed as an infinite sum of tour probabilities. A tour t in the network is a sequence of traversed nodes t : v 1  X  v 2  X   X  X  X  v denoted also as t : v 1  X  v n . Each tour is associated with a length l ( t ) = n  X  1 and probability of traversal P ( t ) = Q n  X  1 Jeh et al. [18, 19] introduce the inverse p-distance and show that it is equivalent to the stationary probability of a RWR.
 T HEOREM 2. (Lower bound) For a modified network graph G lb in which every outgoing edge ( f,x )  X  E,f  X  F is replaced with a self-edge ( f,f ) , such that w ff = P ( f,x )  X  E we have  X   X  lb ( i )  X   X   X  ( i ) ,  X  i  X  K
P ROOF . For a node k  X  K , denote the total probability of all tours in G from r to k that stay in K and do not include a node in F S U as T K = P t us denote the total probability of paths from r to k that include at least one node in F S U as T FU . As the above two sets of paths are mutually exclusive and span the whole space of paths from r denote the total probability of paths from r to k in the perturbed network G lb as T K lb . There a is one-to-one correspondence from all paths contributing to T K in G to all paths in G lb that contribute to T lb . The latter is true as we have not removed any edges within K , so every path within K in G exists and has the same probability in G lb . We obtain the following equality: T K = T K lb introduced notation, and the above (in)equalities we have:  X  ( k ) = P
Our lower bound construction transforms all fringe nodes into sinks, as their incoming edges are kept but their outgoing edges are redirected to themselves. The random walker in this augmented network cannot progress to the nodes in U and hence  X  U lb result, we can compute the values of  X  K lb in the sub-graph contain-ing only nodes in K S F by solving for the stationary distribution of this smaller graph. The lower bound values  X  K lb approach the actual values  X  K as | K | approaches | V | . Refining the lower bound estimates is possible at the price of solving for the stationary dis-tribution of larger sub-networks. This  X  X ay-as-you-go X  property of the construction allows for flexibility in defining the set K . In our experiments, the lower bound is very tight with respect to the actual value for small sizes of K . The bound for EI is obtained from the stationary distribution bound as q lb ( i ) =  X  lb ( i ) w
In order to provide guarantees for the top k neighbors of a given node, we also need to bound each node X  X  EI from above. Our up-per bound construction is inspired by a push-based approximation algorithm for RWR [7, 19] and later modified for performing effi-cient local graph partitioning [4]. We extend the idea of push and approximation vectors to construct our upper bound proof and the corresponding algorithm to compute it. While the authors use in [4] use this idea to construct an upper-bounding procedure.

The method in [4] computes an -approximation of the RWR distribution  X   X  ~r defined as another distribution  X   X  ~r  X  ~s restarting according to the restart vector ~r  X  ~s , where 0  X  ~s ( i )  X  w i . In the notation above, ~r is a vector containing 1 in the position of the restart node r and 0 elsewhere and ~s is a vector containing small non-negative values. Andersen and colleagues fix the desired value of and compute the approximation in a size-unconstrained subgraph of the original graph. We turn the problem around:  X  X f the approximate vector is computed in a fixed subgraph, what is the maximal node-wise deviation ?" .

Our upper bound is computed by transferring mass of decay-ing magnitude along the edges in our active set of nodes K of the network until the mass left to transfer from every node becomes negligibly small. In this process we maintain two vectors: the ap-proximation vector ~p and the push vector ~s , both of length | K | . We construct an upper bound on the RWR distribution vector based on the vectors ~p and ~s .

The input to upper bound procedure (Algorithm 1 ) includes the restart vector ~r , the explored subnetwork K S F , and the restart probability  X  . The push algorithm is performed on a  X  X azy X  ver-sion of the adjacency matrix, in which every node has a self edge of weight equal to the volume of the node. As a result, at every node a random walker may (i) move to the restart node with probability  X  , (ii) stay in the same node with probability (1  X   X  ) / 2 or (iii) fol-low a random outgoing edge with probability (1  X   X  ) / 2 . Andersen et al. [4] show that performing a  X  X azy X  RWR with restart probabil-ity  X  lazy is equivalent to computing a non-lazy random walk with restart probability 2  X  lazy / (1 +  X  lazy ) . As our goal is to bound the distribution for a given input restart probability, we first determine the corresponding  X  X azy X  restart probability (line 3) . Next, the ap-Algorithm 1 Upper Bound proximation ~p and push ~s vectors are initialized (lines 4,5) . Push steps are performed until ~p and ~s converge.

Every push operation increases the approximation value of the current node (line 8) , increases the push values of all neighbors (lines 9-11) and finally decreases the push value of the current node (line 12) . We compute the deviation of the approximation vector from the actual stationary distribution, based on the maximal ratio of remaining push value per unit mass (line 14) . An upper bound to the stationary probabilities is obtained by adding the approxi-mation value and the maximal deviation (line 15) . Similar to the lower bound, the upper bound to the EI is obtained by normalizing the importance upper bound computed with the push algorithm by every node X  X  volume q ub ( i ) =  X  ub ( i ) w
L EMMA 2. Let ~p (  X  ) and ~s (  X  ) be the resulting vectors after ap-plying a single push operation from Algorithm 1 on the previous ~p P ROOF . The push operation we perform is the same as in [4]. The only difference is that we work with generally weighted graphs, however, the same proof applies here as well.

Notice, that after performing a push operation on all nodes in K , the corresponding decreases. Formally, the initial pair ~p = a valid trivial approximation of the stationary distribution, how-ever its corresponding approximation deviation is not practical moved from the push vector and added to the approximation vector. As a result, the norm of ~p increases monotonically while the one of ~s decreases monotonically by the same amount. When this transfer of mass becomes close to zero, we terminate the push operations. T HEOREM 3. (Upper bound) The vector  X   X  ub ( i ) , computed by Algorithm 1 provides a node-wise upper bound to  X   X  ( i ) .
P ROOF . From Lemma 2 and the fact that the initial assignment of ~p is a ( 1 w erations of push operations the obtained ~p =  X   X  ~r  X  ~s -approximation of the  X   X  ~r . The corresponding should domi-From [4], we have that  X   X  ~r  X  ~s is an -approximation of  X  P set of nodes in the network. If we choose the set S as a singleton node in K S F and by subtracting P i  X  S w i on both sides, we operations ~p ( i ) =  X   X  ~r  X  ~s ( i ) then the vector ~p ( i ) + w node-wise upper bound to the elements in  X   X  .
The computational complexity for the evaluation of both our bounds is O ( c | E | K ) , where | E | K is the number of edges adja-cent to nodes in K . The c term depends on the mixing rate of the Markov Chain, corresponding to the adjacency matrix of the nodes in K . In our experiments c is typically a constant in the order of 100 . The size and density of the active graph induced on K is the dominant component of the complexity. Therefore, being able to determine the top neighbors using a small active subgraph is cru-cial for the small online query time.
We employ our lower and upper bound constructions for pruning nodes that are not among the nearest neighbors. We can also use the same node-wise bounds to determine the exact ranking among the top k neighbors if the application demands this.

Figure 2 shows an example of our pruning criterion, for NN searching assuming that the active set K is available. Lower and upper bounds to the EI in K are computed and feasibility inter-vals are formed for each node. (shown as vertical error bars in Fig. 2). Nodes are sorted by decreasing lower bound and the first 8 nodes comprise the known part of the network K , nodes 9 -16 comprise the fringe set F and nodes 17 and on belong to the un-known part of the network. Since the nodes in U are unknown, together with the edges among them, we can only bound them from above according to Theorem 1. Note that in order to ap-ply Theorem 1, we need the actual maximum EI in the fringe set F . Instead of computing the exact RWR stationary distribution on the whole graph, we can use their upper bounds obtained as: (1  X   X  ) max f  X  F q ub ( f )  X  (1  X   X  ) max f  X  F q ( f )  X  q
If the query in the example is 4 -NN, we can guarantee that the first 4 nodes are the actual top-4 neighbors as all of their lower bounds dominate the upper bounds of the rest of the nodes in K S F and also dominate the upper bounds of nodes in U . Note that this guarantee holds regardless of how many nodes comprise U and without exploring more nodes than the ones already in K S F . The actual ordering of the top 4 , is not certain in this case as some of their feasibility intervals overlap and their actual order may pos-sibly be different than the one shown. We cannot provide guaran-tees for the 8 -NN, since the feasibility interval of the eight top can-didate in K overlaps with those of nodes in F and U . In this case, we can expand K by including more nodes with full information about their neighbors. Subsequent expansions of K would refine the bounds estimation and shrink the feasibility intervals making guarantees possible for the nearest neighbors set.
The refinement of the feasibility intervals, discussed in the previ-ous section, comes at the cost of re-computing the lower and upper bounds for a larger instantiation of K . We would like to obtain the minimal set K that allows us to answer a specific query. An ex-haustive search procedure for an optimal K would have to evaluate Algorithm 2 Online k NN Search all subsets of connected nodes that contain the query. For our ap-plications on large and composite networks, any attempt to find an optimal subgraph would add an impractical overhead. We define a greedy procedure that uses the previous bound estimates to direct the expansion. We add a fixed number of nodes from the current F to K that have the highest lower bound estimates.

Our online k NN search is outlined in Algorithm 2. The input consists of the query node r , the restart probability, the number of top neighbors k and the network. The set K is initialized with the query node and its immediate neighbors (line 3) . Next, we com-pute the lower and upper bounds (line 4) of the EI of nodes in the subgraph G K S F , according to the constructions and algorithm in Section 3.2. The upper bound for all unexplored nodes (part of U ) is computed based on Theorem 1 (line 5) . A series of expansion and refinement steps is performed until the top-k list can be guaranteed using the feasibility intervals of candidate nodes (lines 6-9) .
Every extension step (line 7) adds k more nodes to K from the set of fringe nodes F . The refinement step (line 8) involves pre-computing the lower and upper bound vectors from scratch in the new instantiation of the the set K as well as the upper bound for all unobserved nodes in U (this is similar to lines 4 and 5).
The size of the set K , sufficient for determining the exact k NN, depends on the network structure around the query. Particularly, structures that result in close EI values of the k -th and ( k + 1) -th neighbors, demand a large number of expansions due to persistent overlap between their feasibility intervals.

Our nearest neighbor search can be easily relaxed to overcome such situations when very fast response is demanded and a small uncertainty of the top-k set is tolerable. In order to adapt our algo-rithm to m -tolerant k NN search, we can terminate at line 6 when less than m candidates are left to prune. The result set contains at most k + m nodes including the actual top-k neighbors. Eval-uation of the computational savings from such a relaxation in our experimental section shows that significant online time is spent in pruning the last few candidates.
Next, we evaluate the scalability and applicability of our NN search algorithm based on Effective Importance and their utility for several applications.
We experiment with several real world datasets from the social and information network domains (Flickr,DBLP) and from Biology (BioGRID,YeastNet) in order to demonstrate the wide applicability of our method and its scalability across various network genres.
The DBLP co-author network includes collaboration links be-tween scientific authors based on joint papers ( | V | X  700 k, | E | X  4 . 5 m ). This network is created from the public DBLP 1 http://dblp.uni-trier.de/xml/dblp.xml adding an edge between two authors if they have joint publications. The weight of and edges corresponds to the number of papers co-authored, and hence the prolific collaborations are represented by adjacent nodes that are very close to each other.

Another large scale network we use for evaluation contains 3 million nodes and 14 million friendship edges from the Flickr so-cial graph (termed FRIENDS ). This network corresponds to the largest connected component in the dataset provided by Mislove et al. [26]. The nodes represent users and the edges correspond to bi-nary (unweighted) friendship connections. The Flickr dataset [26] also contains information about 11 million photos and favorite pho-tos bookmarked by users. Using this data we construct a Flickr taste similarity graph in which nodes are users and edges represent similarity of taste, based on shared photo bookmarks. To score the similarity of user tastes we use the Dice set similarity coefficient. where u 1 and u 2 are two users and B u 1 and B u 2 are their cor-responding sets of bookmarked photos. We further threshold the similarities, keeping only values greater than 0 . 01 . For overlay ex-periments on Flickr we use overlapping users that are both in the largest connected component in the friendship network and in the photograph taste similarity network termed FAV in the experiments.
Other experimental networks come from genomic research. Bi-oGRID 2 is a functional yeast interaction network in which nodes represent gene products and edges represent the strength of inter-action between genes. The network contains 35 , 630 edges and 4913 genes. Another biological network is YeastNet , a functional gene network overlay of 10 data sources available due to McGary et al. [25]. Each layer corresponds to interactions detected using a different experimental methodology. The overlay contains 5400 genes and more than 250 , 000 interactions in all 10 layers.
Scale-free synthetic networks in our experiments are constructed according to Barabasi X  X  preferential attachment (PA) generative model [6]. We also use Erd X s-R X nyi random networks.
We study the performance of our k NN in terms of running time and pruning power. All experiments are performed on a single ma-chine with 2 GB of main memory and 3 GHz dual-core processor. Our proposed algorithms are implemented in C++.

A natural control for scalability comparison when reporting query time is the evaluation of kNN on the full networks, i.e. with-out using bounding and pruning. Traces for this baseline are named Full in the figures. Another machine-independent performance metric is the fraction of edges k NN explores to evaluate a query, i.e. the pruning power of our lower and upper bounds.
 Synthetic networks. We use synthetic networks to evaluate the scalability with nodes and edge density. Fig. 3(a) and 3(b) present the scalability of k NN for increasing number of nodes, while keep-ing the average degree in a synthetic network fixed to 6 . The ex-pected growth behavior of scale-free graphs is in line with this ex-periment, since such graphs are typically characterized by a large number of small-degree nodes and a small number of high-degree ones. The size of the active subnetwork K , sufficient to answer the k NN query, remains constant for increasing network sizes. We observe this both in the pruning traces 3(a) that decrease linearly on a log-log scale and from the running time which remains con-stant 3(b). In comparison, the exact stationary distribution (denoted Full 3(b)) for a 100 thousand nodes network takes close to 9 min-www.biogrid.com Figure 3: Average performance (over 1000 queries) for increas-ing number of nodes and fixed average degree of 6 (a), (b); and for increasing number of edges and fixed number of 10 k nodes (c), (d)(  X  = 0 . 3 ). utes to compute. Our algorithm answers k NN queries in less than a second for k up to 30 , which makes it ideal for online analysis.
Next, we evaluate the scalability of NN search for a single syn-thetic network as it becomes denser (Fig. 3(c), 3(d)). We fix the number of nodes to 10 k and increase the total number of edges. The average size of the active edge set K is 6% for 80 k edges, but increases to more than half of all edges when the average de-gree reaches 20 . Note that 10 k nodes and 200 k edges corresponds to a dense scenario in which computing the exact EI in the whole network takes 50 s (trace Full in 3(d)).
 NN search in real-world networks. Our performance evaluation on real-world networks is presented in Fig. 4. We achieve sub-second search time in DBLP for values of k up to 30 , while us-ing less than 0 . 2% of the network edges. The Full (no pruning) EI in the whole network takes more than 150 s to compute. The top neighbor search for the Flickr friendship graph takes 7 seconds on average, while pruning more than 99 . 4% of the network edges for  X   X  0 . 3 . The increased complexity, compared to the DBLP graph is due to the higher density and bigger size of Flickr. The k NN search expands the known graph to 40% and higher in the BioGRID network due to its smaller size. The actual number of used edges is about 10 k , allowing for sub-second evaluation. For all three networks we tolerate at most 3 additional candidate nodes that are not pruned ( m = 3 ), which eliminates corner cases of very close k -th and ( k + 1) -st neighbors.

Restart values  X   X  0 . 3 in Fig. 4 allow for practical (close to 1 s) search performance. If  X  is too low, the random walker explores almost the whole network. If  X  is too high (exceeding 0 . 6 ), the walker is restrained to the immediate neighbors of the query and does not capture the deeper community structure. A value of  X  should reflect the balance between these two extremes.
 NN on composition of networks. Composite network overlays model the connections of a node in multiple networks in which it participates. We consider k NN search according to user prioritiza-tion of the networks in the overlay. We define the composite k NN query as the triple &lt; r, k, ~  X  &gt; , where r and k are the query node and the number of desired NN and ~  X  is a vector that speci-fies the user-defined weight of each layer. We assume there exists a Figure 4: Average pruning power (a), (c), (e) and online run-ning time (b), (d), (f) for DBLP, Flickr and BioGRID. Run-ning time for computing the measure on the whole network for Flickr is projected based on DBLP size due to the inability to fit the whole network in the main memory (2GB). one-to-one mapping between nodes that represent the same entity in different layers.

One Naive approach to performing composite k NN search is to (i) apply the online k NN procedure (Algorithm 2 ) to each layer sep-arately, (ii) discover the corresponding relevant subnetworks K each network layer G n and then (iii) compose a network, induced by { S n K n } with links, weighted according to the mixture vec-tor ~  X  . However, the overlap of relevant nodes in each layer may be small for uncorrelated layers. For example, friends might not always have the same taste in music. Moreover, the expansion in each layer should be driven according to its weight in ~  X  . High-priority layers should be expanded more aggressively than low-priority ones. In order to select a small relevant subnetwork in the overlay, we expand with priority-aware best expansion candidates, taking into account feasibility intervals computed according to all layers at the previous expansion iteration. As a result, we maintain a small active set K .

Next, we measure the performance of our k NN in composite net-work overlays. Fig. 5(a) shows a comparison of the pruning power of the aforementioned Naive composition search (trace kNN-Naive ) and our tier-optimized k NN for overlay of two synthetic networks ( | V | = 10 k, | E | = 80 k,k = 20 ). We choose a mixture vector ~  X  of norm 1 and increase the weight of one of the networks from 0 to 0 . 5 while decreasing the weight of the other correspondingly. The k NN algorithm explores two times fewer edges than its Naive counterpart and is twice as fast. For the same overlay, computing the exact EI ( Full in 5(b)) is 100 times slower. Figure 5: Average performance under different mixing condi-tions of a two-network overlay (  X  = 0 . 3 ). (a) Number of edges Figure 6: Performance for increasing number of overlaid net-works (  X  = 0 . 3 ).

We also measure the performance for increasing number of equally-weighted networks in an overlay (Fig. 6(a) and 6(b)). We generate the networks by computing random permutation of the nodes in single power-law network and reconnect the permuted nodes using the original edges. In this respect, the presented results are pessimistic, as the separate networks have unrelated (orthogo-nal) edge sets. The number of nodes is fixed to 10 k and edges of each separate overlay network are 80 k (trace k NN-80 k ) and 200 k (trace k NN-200 k ). For 80 k edge overlays, our online k NN completes in a second, while the respective full computation (trace Full-80 k ) is 1000 times slower on average. When mixing denser networks ( 200 k edges in 10 k node networks), the search time in-creases to ten seconds for 7 -network overlay. Even when adding 10 dense networks in an overlay, thus forcing the resulting network to have 2 million edges (20% of all possible edges), the k NN search takes on average less than 100 seconds.

Search time for real-world composite network overlays is re-ported for Flickr (Fig. 7(a)) overlaying the friendship and book-mark similarity graphs; and YeastNet (Fig. 7(b)) combining the graphs from multiple sources. The Flickr composite search time (trace FAV +FRIEND ) is similar to that of the FRIEND layer on its own, for k up to 20 , while further neighbors become harder to compute. The similar bookmarks (trace FAV ) layer is sparser (sim-ilarity edges of more than 1% are considered) and hence the lower running time. For the gene overlay YeastNet 7(b), we iteratively add layers (from different evidence sources) and compute neigh-bors for k up to 40 . Regardless of the relatively small number of nodes ( 5 k ), the density of the composite network results in search times up to 5 seconds, while computing the actual EI for the whole network takes more than 80 seconds.

Scalability of relaxed NN. We study the effect of relaxing the k NN search to m -tolerant k NN search for values of m up to 15 (Fig. 8(a) and 8(b)). Significant savings both in running time and pruning are observed for small m = 5 as compared to exact evalu-ation. This is due to typically few nodes of very similar EI situated around the k -th position. On average, 30% more online time is re-quired to separate the feasibility intervals of these few nodes, via (a) Flickr (Full:&gt;1000s) Figure 7: Search time for (a) Flickr friends and favorite photos(  X  = 0 . 4 , m = 5 ) and (b) YeastNet (  X  = 0 . 3 , m = 0 ). (a) Number of edges
Figure 8: Performance for increasing tolerance m (  X  = 0 . 2) . 15% increased expansion of edges. For large networks and over-lays, this small tolerance can enable an order of magnitude perfor-mance improvement.
Next, we focus on three applications of the EI measure and our fast NN search algorithm: (i) finding local partitions around a node of interest (ii) sparsification of dense networks for faster global par-titioning and (iii) node label prediction.
 Finding local partitions. Given a query node, how can we find a good subgraph including this node such that we minimize the cut edges? This problem is also known as the local partitioning problem. As we demonstrated earlier(see Sec. 3.1 and Fig. 1(b)), the top neighbors based on EI correspond to good local partitions as measured by the conductance of their induced subgraphs in a biological network. Here, we formally define the conductance of a cut and show results for three more real networks.
 The conductance of a cut is defined as: where Q is the set of top neighbors of a query node q . The conduc-tance measures the fraction of cut edges to all edges on the smaller size of the cut in terms of volume. It is a well-established measure of the optimality of a two-way clustering [9, 4]. The smaller the conductance the better the separation of the nodes on both sides of the cut.

Fig. 9 presents a comparative analysis of employing EI, RWR and Shortest paths (SP) as proximity measures in social, co-authorship and gene interaction networks [26, 1, 25]. We mea-sure the conductance around a random target node for sets Q com-prised of the closest k neighbors. We report the average conduc-tance for 50 randomly chosen target nodes in each of the networks. EI achieves consistently better conductance than RWR and SP for increasing position of the cut k . RWR X  X  worse performance can be explained by its preference for high degree nodes. The good performance of SP on DBLP (9(c)) is due to the natural clusters Figure 9: Conductance of cuts based on Effective Importance, Random walks with restarts and Shortest paths. Closest EI neighbors form better clusters around the query as compared to the closest RWR or SP neighbors for Flickr[26] (a), Bi-oGRID[1] (b) and DBLP co-authors (c). Lower conductance corresponds to better separation of the top k neighbors from the rest of the network. (c) Preferential Attachment Figure 10: Increase of partition conductance ( X  sp  X   X  o for varying sparsification ratio s = | E sp | / | E | . Comparison of the Global (GS) and Local (LS) and EI nearest neighbors sparsification quality. corresponding to academic labs with high degree among the lab members and not many connections to other labs. The SP measure suffers from instability due to multiple tied nodes in unweighted networks (we order such nodes arbitrarily) and its inability to ac-count for multiplicity of paths.
 Graph sparsification. As a second application of our measure, we consider the problem of how to sparsify the edges of a graph, keeping the nodes intact in a way that the global clustering structure is preserved. Obtaining a good sparsified graph allows for more efficient global clustering. The problem of sparsification can also be thought of as a way to summarize the graph by retaining only essential edges that hold clusters together.

Graph sparsification have been considered with a variety of ob-jectives including preserving graph cut sizes, node distances and dense subgraphs [2]. Recently, Satuluri and colleagues [32] fo-cused on the specific goal of preserving the global graph clusters and speeding up various global clustering algorithms by applying them on the sparsified graphs with small compromise of cluster quality. The latter work considers local similarity measures for linked vertices in order to select edges to keep or remove. In what follows, we sparsify the graph based on the structure among top EI and compare the quality of the obtained sparse version with that of the methods proposed in [32].

In order to sparsify a target graph, we take into account the near-est EI neighbors of nodes and the edges that interconnect them, following on our earlier observation that closest EI neighbors com-prise  X  X ood X  local clusters. We obtained the top neighbors of a node using our scalable local expansion procedure. The number of considered neighbors varies according to a node X  X  degree k where d i is the number of neighbors of node i and s  X  [0 , 1] is the target sparsification ratio defined as the fraction of remaining edges. An edge is a connector , if it connects any pair of top neigh-bors or a target node with a top neighbor. We obtain an edge score sc ij = f ij  X  w ij , where f ij is the number of times the edge ( i,j ) was a connector when exploring the top neighbors of all graph nodes and w ij is the edge weight. The sparsified version of the graph includes the top s  X  X  E | edges ranked by their sc score.
We compare the sparsification quality produced by our EI nearest neighbors approach with the global and local sparsification (GS and LS) methods introduced in [32]. We extend the original GS and LS methods to work with weighted graphs by computing a weighted version of the Jaccard similarity as where N j is the set of neighbors of node j . We i) sparsify the graph using one of the competing techniques, ii) compute a global 2-partitioning of the nodes using Metis [20] and iii) report the in-crease of average partition conductance ( X  sp  X   X  o ) /  X  is the avg. conductance of the sparse clusters in the original graph and  X  o is the avg. conductance when Metis is run directly on the original (not sparsified) graph. A sparsification method has high quality if its conductance increase remains low for sparser graph instances, i.e. decreasing s = | E sp | / | E o | .

As seen in Fig. 10, our EI sparsification scheme enables par-titions with better quality than alternatives across both real and synthetic datasets and for all levels of sparsification s . For dense weighted graphs such as FAV (photo favoring similarity among Flickr users), the EI sparsification not only enables faster global partitioning, but also improves the partition quality by 40% com-pared to working with the original graph (Fig. 10(a)). In BioGRID (Fig. 10(b)), EI outperforms LS and GS for aggressive sparsifica-tion ( s &lt; 0 . 4 ), and behaves similar to LS for higher values of s . The quality of EI sparsified graph partitions is also retained in scale free and random networks Fig. 10(d),10(c) (the average behavior of multiple instantiations of size 5000 nodes is presented). We ob-serve similar behavior for higher number of target partitions. In the rest of our real data set EI X  X  performance is similar to that of LS.
In a sparsification-aided partitioning, the majority of time is spent in running the global partitioning procedure (Metis in our case). The EI sparsification step, although more demanding than LS, is much faster than the actual partitioning, thus enabling speedup compared to directly partitioning the original graph. In addition, as we demonstrate, it allows improvement in the partition quality for weighted graphs.

Attribute prediction in partially annotated graphs. Yet another application of our proximity measure and NN search is for the prob-lem of annotating missing attributes (labels) in partially annotated graphs. Assume that in an information network (a private wiki, the web graph, etc.) with attributes corresponding to the category of an information resource (nodes), some of the annotations are missing. It is natural to expect that nodes of similar category form well con-nected clusters and hence label propagation ideas can be employed Figure 11: Average fraction of top RWR and EI neighbors sharing category with a target page in Wikipedia. to recover the missing attributes [5, 28, 27]. Following this intu-ition, we examine the utility of the EI nearest neighbor search for annotation prediction.

We consider a subnetwork of Wikipedia involving 10k pages with edges corresponding to hyperlinks among them. Each page is annotated with labels from the category hierarchy within Wikipedia. We compute the fraction of top neighbors who share a category with a target node and report the average of this quan-tity when ranking neighbors by EI and random walks with restarts (RWR) in Fig. 11. Since, EI is better in capturing local clusters it naturally ranks neighbors of similar attributes closer than random walks with restarts. As a result, one can annotate missing labels in real networks using a majority label vote scheme among top EI neighbors of a target node that misses with a missing label.
In this paper, we addressed the important problem of nearest neighbor proximity search in large graphs. Our approach  X  X ills X  an important unsaturated niche of the solution space X  X amely that of fully online and yet very scalable methods. We proposed a novel proximity measure called Effective Importance that captures the community structure around a query node and exhibits favorable theoretical properties allowing the computation of efficient bounds on the measure at query time. Our proposed NN search solution is scalable and accurate (computing exact nearest neighbors) and does not require holistic knowledge of the graph, and hence is tailored to dynamic and composite networks across multiple genres.
Through extensive experiments on real world and synthetic net-works, we demonstrated up to 100 X running time improvement of our NN search, compared to an exhaustive counterpart. We also evaluating the quality of our method for graph sparsification, local partitioning and missing attribute prediction. In all these applica-tions, our method outperforms the state-of-the-art alternatives and simpler baselines.
Research was sponsored partially by NSF grant IIS-1219254 and the Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053 (NS-CTA). The content of the infor-mation does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.
