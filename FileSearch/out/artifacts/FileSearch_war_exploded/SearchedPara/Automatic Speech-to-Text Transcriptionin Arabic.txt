 LORI LAMEL, ABDELKHALEK MESSAOUDI, and JEAN-LUC GAUVAIN LIMSI-CNRS 1. INTRODUCTION This article summarizes research aimed at speech-to-text transcription (STT) of Arabic broadcast data. Much of this work has been carried out in the context of the DARPA EARS and GALE programs for which speech recognition and machine translation are key supporting technologies. 1 The Arabic language poses challenges somewhat different from the other languages for which we have developed automatic speech recognition systems (mostly Indo-European Germanic or Romance) [Gauvain and Lamel 2000; Gauvain et al. 2002; Lamel and Gauvain 2008]. Modern Standard Arabic is learned in school, used in most newspapers, and considered to be the official language in most Arabic-speaking countries. In contrast many people speak in dialects for which there is only a spoken form and no recognized written form. Arabic is a strongly consonantal language with nominally only three vowels, each of which has a long and short form. Arabic is a highly inflected language with many differ-ent word forms for a given root, produced by appending articles ( X  X he, and, to, from,with,... X ) tothewordbeginningandpossessives( X  X urs,theirs,... X ) on the word end. Written texts are by and large non-vowelized, meaning that the short vowels and gemination marks are not indicated. There are typically sev-eral possible (generally semantically linked) vowelizations for a given written word, which are spoken. The word-final vowel varies as a function of the word context, and this final vowel is often not pronounced. Thus one of the chal-lenges faced when explicitly modeling vowels in Arabic is to obtain vowelized resources, or to develop efficient ways to use non-vowelized data. It is often necessary to understand the meaning of the text in order to know how to vow-elize or pronounce it correctly. To addres s this problem the Buckwalter Arabic Morphological Analyzer [Buckwalter 2004] is used to propose possible multiple vowelized word forms, and a speech recognizer is used to automatically select the most appropriate one.
 vocalized data [Messaoudi et al. 2004], which enabled explicit modeling of the Arabic short vowels. It was shown that even when producing a non-vocalized transcript, explicitly modeling short vowels improves recognition performance [Afify et al. 2005] over a grapheme-based approach where only characters in the non-vocalized written form are modeled [Billa et al. 2002]. However, since only very limited vocalized resources wer e available, research was carried out to reduce the reliance on such data. Two main directions were pursued. One di-rection aimed to reduce the supervision needed for acoustic model training, and another investigated how to efficiently combine vocalized and non-vocalized texts when constructing language models. As summarized in Section 4, it was demonstrated that by building a very large vocalized vocabulary of more than 1.2 million words, and by using a language model including a vocalized com-ponent, the word error rate (WER) 2 could be significantly reduced [Messaoudi et al. 2006]. ward in Arabic from vocalized texts, there are frequent variants arising in the pronunciation of the definite article  X  X l X  (the) depending on the word context which causes the following consonant to be doubled. The tanwin, a grammati-cal mark specifying that a noun is nondefinite, causes word final short vowels to be doubled (phonetically realized by adding an  X  X  X  after the vowel X  X his also referred to as nunation). Studies that address explicitly representing the gem-ination and tanwin in an attempt to improve the acoustic and lexical models are reported in Section 6.
 the challenges of dealing with the huge lexical variety. For the Arabic lan-guage, the combination of compounding, agglutination, and inflection generate a large number of surface forms for a given root form. Morphological decom-position [Kirchhoff et al. 2002; Vergyri et al. 2004; Xiang et al. 2006] has been proposed to address this problem, thereby increasing the lexical coverage and reducing errors that are due to words that are unknown to the system. Our studies on morphological decomposition are described in Section 8. Prior to presenting work specifically directed at processing the Arabic language, an overview of the speech transcription system is given in the next section. 2. RECOGNITION SYSTEM OVERVIEW Radio and television broadcast data are challenging to transcribe since they are heterogeneous, containing segments of various acoustic and linguistic na-tures. The signal may be of studio quality or may have been transmitted over a telephone or other noisy channel (i.e., corrupted by additive noise and non-linear distortions), or can contain speech over music or pure music segments. The speech is produced by a wide variety of speakers with different speaking styles: news anchors and talk show hosts, reporters in remote locations, inter-views with politicians and common people, unknown speakers, new dialects, non-native speakers, etc. Speech from the same speaker may occur in differ-ent parts of the broadcast, and with different background noise conditions. In recent years the focus of research has moved from broadcast news data (pri-marily prepared speech in studio conditions) to the transcription of what is referred to as  X  X roadcast conversational X  speech (talk shows, debates, and in-teractive programs). This type of data requires the explicit modeling of spon-taneous speech effects, which are much more common than in broadcast news, and also the ability to deal with speech from a variety of Arabic dialects. The acoustic and language modeling must accurately account for this varied data. main components: an audio partitioner and a word recognizer. Data partition-ing is based on an audio stream mixture model [Gauvain et al. 1998, 2002] and serves to divide the continuous stream of acoustic data into homogeneous seg-ments, associating cluster, gender, and labels with each non-overlapping seg-ment. For each speech segment, the word recognizer determines the sequence of words in the segment, associating start and end times and an optional confidence measure with each word. The recognizer makes use of continuous density HMMs for acoustic modeling and n -gram statistics for language mod-eling [Young and Bloothooft 2000; Chou and Juang 2003; Gauvain and Lamel 2000; Lamel and Gauvain 2003]. Each context-dependent phone model is a tied-state left-to-right CD-HMM with Gaussian mixture observation densities where the tied states are obtained by means of a decision tree.
 generates a word lattice which is exp anded with a four-gram LM. Then the posterior probabilities of the lattice edges are estimated using the forward-backward algorithm and the four-gram lattice is converted to a confusion network with posterior probabilities by iteratively merging lattice vertices and splitting lattice edges until a linear graph is obtained. This last step gives comparable results to the edge clustering algorithm proposed in Mangu et al. [1999]. The words with the highest posterior in each confusion set are hypothesized.
 for cluster-based acoustic model adaptation. This is done via one pass (less than 1xRT) cross-word trigram decoding with gender-specific sets of position-dependent triphones (typically 5K tied states) and a trigram language model. The trigram lattices are rescored with a four-gram language model. These hy-potheses are used to carry out unsupervised acoustic model adaptation for each segment cluster using the MLLR technique [Leggetter and Woodland 1995] with one regression class. Then a second lattice is generated for each segment using a bigram LM and position-dependent triphones with 11,500 tied states (32 Gaussians per state). The word graph generated in this second decoding pass is rescored after carrying out unsupervised MLLR acoustic model adap-tation using a variable number of regression classes. 3. PRONUNCIATION LEXICON Letter-to-sound conversion is quite straightforward when starting from vow-elized texts. A grapheme-to-phoneme conversion tool was developed based on a set of 37 phonemes and three nonlinguistic units (silence/noise, hesitation, and breath). The phonemes include the 28 Arabic consonants (including the emphatic consonants and the hamza), three foreign consonants (/p,v,g/), and six vowels (short and long /i/, /a/, /u/). In a fully expressed vowelized pronun-ciation lexicon, each vowelized orthographic form of a word is treated as a distinct lexical entry. The example ent ries for the word  X  X itaAb X  are shown in the top part of Figure 1. As reported in Messaoudi et al. [2004], initial speech-to-text transcription studies were carri ed out using vocalized word lists. Some example entries of a vocalized lexicon are given in the left part of Figure 1. In 50 hours of manually transcribed vocalized data, there were only 57K distinct lexical forms. The out-of-vocabulary (OOV) rate on an independent set of 12 hours of test data is approximately 15%, which is very high [Messaoudi et al. 2004]. 3 sentation is to use the nonvowelized orthographic form as the entry, allowing multiple pronunciations, each being associated with a particular written form. Each entry can be thought of as a word class, containing all observed (or even all possible) vowelized forms of the word. This representation is illustrated in the right side of Figure 1, where the left column contains the nonvocalized orthographic form or word class, and the right column associates each vocal-ized word with a pronunciation. The pronunciation is on the left of the equal sign and the vowelized written form is on the right. This latter representation was used to create a word lexicon, where a pronunciation graph is associated with each word so as to allow for alternate pronunciations [Messaoudi et al. 2005]. Since multiple vowelized forms are associated with each non-vowelized word entry, the Buckwalter Arabic Morphological Analyzer was used to pro-pose possible forms that were then manually verified. 4 The morphological analyzer was also applied to words in the vowelized training data in order to propose forms that did not occur in the training data. A subset of the words (about 1K), mostly proper names and technical terms, were manually vow-elized. Using this latter representation, the 57K word vocalized entries are replaced by 33K word classes. The generalization enabled by this represen-tation almost halves the OOV rate of the test data (to approximately 8%). In Section 6 an expanded phone set is explored, and in Section 7 pronunciation variants are introduced to bette r represent some Arabic dialects. 4. LARGE VOCALIZED LANGUAGE MODELS The previous section described a method based on word classes which allows the transcription system to explicitly use information about the short vowels in Arabic while being able to generalize to other word forms so as to make use of nonvocalized audio and textual resources. To address the large lexical vari-ety of Arabic, a much larger recognition vocabulary is needed. In Messaoudi et al. [2006] the lexicon was extended to 200K word-classes (with more than 1 million vocalized words). Both vocalized and nonvocalized audio and textual resources are used for language modeling by constructing separate language models and interpolating them. More precisely, a 1.2 million word vocalized word language model was built by interpolating the nonvocalized LM trained on texts (390M words from LDC Arabic Gigaword corpus [Graff 2007] 204M words collected from Internet news so urces) and a vocalized LM trained on 1.1M words of vocalized manual transcri ptions of data from several broadcast news sources [Messaoudi et al. 2005]. There are a total of 85K different vo-calized forms corresponding 50K distin ct nonvocalized forms. As described in the previous section, the vocalized vocabulary has been obtained by semi-automatically generating all possible vocalized forms for the 200K nonvocal-ized word vocabulary.
 ing way ( v i and w i are respectively the vocalized and nonvocalized forms of i th word): where P a is the vocalized LM trained only on the vocalized part of the acoustic data, P v is trained on all the acoustic data after Viterbi alignment, and P t is the standard nonvocalized LM trained on all of the data described above. Adding the automatically vowelized transcripts to the data used to estimate the vo-calized LM did not improve performance. Independent of whether a vocalized or nonvocalized language model is used, the decoder outputs a nonvocalized transcription. When using the vocalized LM, the posterior probabilities of the vocalized forms corresponding to the s ame nonvocalized word are summed to compute the word posterior probabilities. This is the same as what is done for consensus decoding [Mangu et al. 1999] with alternate pronunciations. Using pronunciation probabilities is quite important given the large number of pos-sible pronunciation per word class. On average there about eight forms per lexical entry, and using probabilities can give a relative the word error rate reduction of almost 10%. 5. TRAINING MODELS WITH GENERIC VOWELS Generally speaking, extending the pronunciation dictionary to include entries for additional training data entails some manual intervention or verification. For Arabic, the difficulty lies in determining the vocalized forms, after which grapheme-to-phoneme conversion is relatively straightforward. In the case of a large quantity of training data with nonvocalized transcripts, there can be too many words without vocalizations to add these manually or even semi-automatically. One possibility that we considered was to generate all possi-ble vocalized forms, allowing all three short vowels or no vowel after every consonant. This idea was quickly rejected since there are too many possible vocalized forms. For example, with wor ds with four consonants generate 512 possible pronunciations, and words with eight consonants have 8,192 possible pronunciations.
 to replace the three short vowels. This does not pose any problem since even though short vowels are represented internally in the system, the Arabic recog-nizer outputs the nonvocalized word form. Using a generic vowel offers two main advantages. First, the manual work in dealing with words that are not handled by the Buckwalter morphological analyzer (typically proper names, technical words, words in Arabic dialects) is reduced. With this approach these can be automatically processed. Second, the number of vocalizations, and hence pronunciations, per word is greatly reduced (one vowel instead of three).
 vowel from the nonvocalized word form. Some rules concern the word initial Alif (support of the Hamza), which can be stable or unstable. For the former case a pronunciation is generated with a glottal attack (denoted / X /) followed by a generic vowel (denoted /@/). These rules also cover word initial letter sequences [wAl, wbAl, wkAl, fAl, fbAl, fkAl] which often correspond to a com-posed prefix ending in  X  X l X . Different pronunciations are generated to repre-sent both situations. For example, the possible pronunciations for wAl are: w@l w X  X l wAl. In word final position, short vowels can be followed by an  X  X  X  (tanwin), so two forms are proposed: the generic vowel alone and the generic vowel followed by an  X  X  X . Similar rules handle the pronunciation of words end-ing in  X  X A X  and a final letter  X  X  X  (which symbolizes the ta marbouta). Within a word, a generic vowel is added after each consonant with the exception of the semivowels  X  X  X  and  X  X  X  which can be realized as respective semivowels or can serve as a support for the long vowels /U/ and /I/. A word internal Alif can represent the long vowel or a glottal attack.
 sented with consonants, long vowels, and the generic vowel. Since vowels may also be absent (written with a Sukoun), additional pronunciations are added by removing one generic vowel at a time. For example, the rules generate the following two generic vowel forms for the word  X  X tb X : which after allowing each generic vowel to be deleted produces: account when generating pronunciations with generic vowels. This decision was made to limit the number of pronunciations even though the gemination is explicitly represented for most words in the lexicon. In the current system, words with generic vowels are not included in the recognition word list, and are only used during training.
 generic vowels by mapping all short vowels in the vocalized lexicon to a generic vowel. Acoustic models were retraine d by first mapping all short vowels to a single generic vowel (@), and training context dependent models with the stan-dard consonant set and the single generic vowel. A pronunciation lexicon was then created that used the standard pronunciations with short vowels for the vocalized words and automatically generated pronunciations with the generic vowel for the nonvocalized words. We then segmented all of the audio data using this lexicon with a combined set of acoustic models formed by merging the CD models with short vowels and those with a generic vowel. Note that the basic idea was to use the generic vowel only in training, but not during recognition so a number of CD models are never used. In the future we may consider also extending the recognition lexicon in an analogous manner. In order to assess the feasibility of this, several model sets were built and tested in decoding using only a generic vowel.
 the first pass of the evaluation system described below) are given in Table I with the standard phone set including three short vowels, and with models trained with only one generic short vowel. Both model sets have 5K tied states (64 Gaussians per state) and covering 5K phone contexts. It can be seen that there is only a slight degradation in performance for both the broadcast news (bnat06) and broadcast conversation (bcat06) data types, when using a generic vowel. Therefore it was decided that the generic vowels provide an effective means to facilitate training on nonvocalized data. 6. MODELING GEMINATES AND TANWIN The original phone set for Arabic described in Section 3 contained 37 symbols. When pronunciations were produced with this phone set, all consonants with a gemination mark were simply doubled. While this may be a reasonable ap-proximation for some sounds, such as fricatives, if is clearly not well adapted to plosives where gemination does not result in multiple bursts.
 Al$  X  ir(aAEiyap) X . An aligned approximate phone transcription is shown on the bottom. There are two geminates in this example. The first is the  X  X  X  (em-phatic  X  X  X ) around time 783.85 and the other a geminate  X  X  X  ( X  X h X ) is centered at time 784.4. These segments have a duration that is about 50% longer than their nongeminate counterparts.
 phones. The frequencies of the consonants in single and geminate form were counted in a 100-hour corpus of manually transcribed and vocalized Arabic broadcast news data [Messaoudi et al. 2005]. The right part of Figure 2 lists the solar and lunar consonants, along with the percentage of occurrences as geminates. It can be observed that the solar consonants generally have a higher proportion of geminates than the lunar ones. Figure 3 shows how the geminates are represented in the original pronunciation dictionary (top) and the new dictionary with specific geminate symbols.
 opment data used in the GALE community, comparing models trained using the original phone set and the extended one which includes geminates. It can be seen that modeling geminates improves performance for both the broad-cast news (bnat06) and broadcast conversation (bcat06) data types, and that a further gain is obtained by combining the two models [Fiscus 1997]. Increas-ing the phone set also has the added advantage of increasing the number of context-dependent phones that are modeled.
 forms. These can be realized as a vowel-n sequence or a nasalized vowel. In order to better capture this variability three additional phones were added to the phone set to represent the three tanwin phones (in, an, un) with a single unit. Acoustic models were built using this new phone set, and tested on the development data sets. These models obtained word error rates comparable to that of the non-tanwin models, and when used in system combination [Fiscus 1997] gave a gain of 0.4% absolute over either model set alone. Given the large variability in the realization of tanwin, these results are not surprising. 7. PRONUNCIATION VARIANTS FOR DIALECTAL SPEECH An analysis of the errors made by the STT system showed that many of the errors involve the insertion or deletion of a prefix or a suffix, such as the con-fusion of ktAb and wktAb or ktAbh and ktAb. The article  X  X l X  is found in 37% of the prefix errors, and contributes a n absolute error of 1%. In examining the errors a number of dialectal pronunciation variants were observed, that were not represented in the lexicon. Figure 4 shows two spectrograms of the word  X  X ljalsa X  (meeting). The final short vowel in the example on the left is an /a/. The right example is the same word, but the final vowel is not produced in the same manner. Arabic speakers consider this to be an /i/, whereas it appears more like an /e/ in the spectrogram.
 absolute WER reduction of 0.3% on broad cast news data and 0.6% on broadcast conversation data which contains more dialectal speech.
 8. MORPHOLOGICAL DECOMPOSITION As for other morphologically-rich languages such as Estonian, Finnish, German, Korean, and Turkish [Carki et al. 2000; Whittaker and Woodland 2000; Adda-Decker 2003], one of the challenges of Arabic speech recognition is to deal with the huge lexical variety. For Arabic the combination of compound-ing, agglutination, and inflection generate a large number of surface forms for a given root form. Morphological decomposition [Kirchhoff et al. 2002; Vergyri et al. 2004; Xiang et al. 2006] has been proposed to deal with this character-istic, resulting in increased lexical coverage, thereby reducing errors that are due to words that are unknown to the system.
 found in texts of the language. This view is a bit simplistic as it assumes that the texts have already been normalized, which in turn entails a variety of more or less important decisions [Adda et al. 1997; Adda-Decker and Lamel 2000]. For morphologically rich languages there has been growing interest in using sub-word units to reduce the needed vocabulary size for a given lexi-cal coverage. There are two main approaches to morphological decomposition: those based on the use of explicit linguistic knowledge and rules (for example, Schmid [1994], Vergyri et al. [2004], and Xiang et al. [2006]) and unsupervised methods (for example, Harris [1955], Goldsmith [2001], Adda-Decker [2003], and Creutz and Lagus [2005]). Since the Arabic language has a relatively lim-ited number of affixes, and rules can capture the manner in which they are applied, in this work the rules as implemented in the Buckwalter morphologi-cal analyzer are used [Buckwalter 2004; Ghaoui et al. 2005].
 described, followed by a description of the audio and text training corpora used in the recognition experiments. 8.1 Methodology Three variant methods for morphological decomposition were investigated. For all three the basis for decomposition is derived from the results of the Buck-walter morphological analysis [Buckwalter 2004]. In Buckwalter, the following affixes are decomposed (the Buckwalter transliteration codes are used here):  X 12prefixeswith X  X l X :AlwAlfAlb Al wbAl fbAl ll wll fll kAl wkAl fkAl  X 11prefixeswithout X  X l X :wfbwbfblwlflkwkfk  X 6 negation prefixes: mA wmA fmA lA wlA flA  X 3 prefixes future tense: s ws fs  X  X uffixes (possessive pronouns): y, ny, nA, h, hm, hmA, hn, k, kmA, km, kn. In total there are 32 prefixes, six for negation, three for the future formed, 12 formed with the definite article, and 11 others without  X  X l X . The suffixes in Arabic are personal pronouns, the objective form serves as a direct object of a verb, and the possessive form serves as the complement of a noun.
 in the training texts that were identifiable by the Buckwalter morphological analyzer. Of the 1,137K distinct words in the training texts, 880K can be de-composed with the rules. About half of the remaining words are simple words, and the remainder have several possible decompositions (29%) or have a root that is not in the recognition dictionary (12%). Decomposition of a 200K lexicon results in a lexicon with 79K entries a nd reduces the out-of-vocabulary rate from 4.4% to 2%. If the decomposition rules are applied to the entire 1.1M words, it is reduced to 270K forms (stems, affixes and decomposed words). During decomposition, each affix that is split from the word root is marked by adding a  X + X  (to the end of prefixes and the start of suffices) to signify that it should be recomposed with the following or preceding word in the recognizer hypothesis.
 words that were recognized by Buckwalter. Of these 880K were decomposed, and 256K remained unchanged. After decomposition, the word list was re-duced to 270K forms (stems, affixes and decomposed words). Following what has been done by others, in the second version (v2), the most frequent 65K words were never decomposed. This had the effect of blocking the decomposi-tion of 35K words, which when added to the word list increased its size to 300K words.
 the word begins with a solar consonant (the solar consonants in the Buckwalter codearet,v,d,g,r,z,s,$,S,D,T,Z,l,n.). Thereasontoforbidthedecom-position of  X  X l X  preceding words starting with a solar consonant is because the  X  X  X  is assimilated with the following consonant and it is difficult to isolate a portion of the signal that clearly corresponds to the  X  X l X . This problem is il-lustrated by the spectrogram in Figure 5 which is the same excerpt  X (kaAn)ati AlT  X  aA } irap Al$  X  ir(aAEiyap) X  in Buckwalter code shown in Figure 2. The letters in parenthesis at the start and end provide the context. For the portion of interest  X  X ti AlT  X  aA } irap Al$  X  ir X  the first  X  X  X  was underlyingly a Sukoun (a mark which inhibits the pronunciation of a vowel). However, preceding the  X  X l X  it is realized as an  X  X  X  (which is reduced to more or less a schwa) and the  X  X l X  causes the following consonant to be realized as a geminate  X  X T X . This ex-ample shows a second gemination  X  X S X  corresponding to the second  X  X l X . These type of phenomena are extremely difficult to model when the  X  X l X  is allowed to be decomposed from the word, and explains why the  X  X l X  was involved in so many of the errors in the first version. This restriction blocks decomposition of the prefix  X  X l X  preceding a solar letter if it is a simple prefix. If the prefix  X  X l X  is preceded by other prefixes, the other prefixes are split off and the  X  X l X  is kept with the stem.
 which has three prefixes  X  X +b+Al+slAm X  into  X  X bAl+ slAm, X  whereas the ver-sion three decomposition gives  X  X b+ AlslAm. X  found to be in dialectal Arabic. Adding seven dialect prefixes to the Buckwalter prefix table allows over 85% of these words to be decomposed.
 els, in order to build a complete system using the morphological decomposition, the affixes needed to be added to the pronunciation dictionary, and acoustic models trained with the decomposed lexical units. The pronunciation lexicon was extended to include all possible pronunciations of the affixes. One particu-lar problem is handling the article  X  X l X  when it is followed by a solar consonant, since in this case the  X  X  X  assimilated with the consonant. This phenomenon is taken into account within words by assigning a gemination mark to the con-sonant. To represent this in the decomposed prefix  X  X l X , contextual pronun-ciations are included for all solar consonants. For the acoustic models, the decomposition rules were applied to the transcripts of the audio training data, and several iterations segmentation and model estimation were carried out. 8.2 Experimental Results The training and test data are all from the Gale program, and distributed by LDC 5 . The audio training data used in this work are comprised of 1,200 hours of manually transcribed broadcast data (1200h train). Roughly 60% of the data are classed as broadcast news (BN), that is typically well-prepared speech from announcers and reporters speaking Modern Standard Arabic, and 40% is classified as broadcast conversation (BC), which tends to be more casual in style and has a higher proportion of dialectal Arabic. Results are reported on the Gale development and evaluation data sets from 2006 and 2007 (bnat06, bnad06, bcat06, bcad06, eval06, dev07, eval07), with each set containing two to three hours of audio data.
 sources and transcriptions of audio dat a. The written texts comprise more that 1.1 billion words from a variety of news sources, predominantly newspapers and news wires in Arabic. The transcriptions of audio data contain over 11M words: 6.3M words from BN and 4.8M words of BC, and an additional 3.8M words of Web transcripts of Aljazeera BC data.
 which is associated with multiple vocalized forms, which in turn are associated with one or more phone pronunciations [Messaoudi et al. 2006]. The pronun-ciations make use of 71 symbols, including 31 simple consonants, 30 geminate consonants, three long and three short vowels, plus three pseudo phones for nonlinguistic events (breath, filler, silence). There are on average 8.6 pronun-ciations/word.
 model adaptation are given in Table III. The acoustic models were trained on approximately 1,200 hours of manually transcribed speech data distributed by LDC. The three versions of decomposition were applied to the training tran-scripts, and three sets of word-position dependent acoustic models were esti-mated, specific to each versions. The WER of the reference word based system with MLE training was 20.9%. With the first decomposition method that sim-ply splits all affixes, the WER is increased by 2% absolute. By forbidding the decomposition of the most frequent 65K words (v2) most of these errors are avoided. Applying the third version of decomposition rules prevents the de-composition of the prefix  X  X l X  preceding 11K solar words. After applying these to the full 1.1M word list, the recognition vocabulary contains 320K entries (stems, affixes and decomposed words). The WER is reduced by 1.3% com-pared to the v2 decomposition, but there is only a small gain relative to the word-based system.
 ent stages of system development. A complete system was developed based on the prior results and on complementary work on using multi-layer percep-trons to provide discriminative acoustic feature extraction [Zhu et al. 2005; Stolcke et al. 2006; Fousek 2007; Gr  X  ezl and Fousek 2008; Fousek et al. 2008a, 2008b] and neural net language models to cope with the data sparseness prob-lem in estimating n -gram probabilities [Schwenk and Gauvain 2005; Schwenk 2007]. Standard techniques used in state-of-the-art speech transcriptions such as speaker adaptive training (SAT) [Anastasakos et al. 1996] and Maximum Mutual Information (MMI) training, Constrained Maximum Likelihood Lin-ear Regression (CMLLR) and MLLR [Leggetter and Woodland 1995] adapta-tion are all used.
 hours of manually transcribed data), developed for the word-based system, that is the training transcriptions use a word representation. Results are given for all Gale development sets with neural net language models estimated on the texts that have been morphologically decomposed and for the baseline word based NN LM [Schwenk 2007]. Comparing the first two entries, it can be seen that the baseline word-based and morphologically decomposed language models give quite comparable results. The results obtained by combining the two models using Rover [Fiscus 1997] are given in the third row of this table. Compared to the baseline system the average word error reduction across all test sets is approximately 0.6%. The final entry in the table shows the results of a four-way Rover obtained using the 290K word-based LM and the 290K LM with morphological decomposition each with two acoustic model sets, one using standard cepstral features and the other MLP based features [Fousek et al. 2008a,b]. The word error rate is reduced on all test sets by more than 1% compared to the two-way combination. 9. CONCLUSIONS This article described the incremental improvements to a system for the au-tomatic transcription of broadcast data in Arabic, highlighting techniques de-veloped to deal with specificities of the Arabic language. One of the challenges is training with incomplete information since most Arabic texts are written without diacritics, yet the diacritics provide useful information for pronuncia-tion modeling and higher level processing. After initial studies which focused on Modern Standard Arabic broadcast news data using a completely vocal-ized representation, different methods were explored to reduce the reliance on vocalized data and to handled more varied data. Many vocalized word forms can be derived using the Buckwalter morphological analyzer and modifications thereof. However it is necessary to also be able to generate pronunciations for words that Buckwalter is not able to process. Rules to generate pronuncia-tions with a generic vowel have been proposed, and this method has been used to significantly facilitate training on n onvocalized data. Concerning pronunci-ation modeling, explicit rules were developed to handle frequent dialectal vari-ants, as well as systematic variations in the language. The explicit modeling of gemination and the introduction of pronunciation variants led to significant improvements in speech-to-text transcription performance.
 The authors gratefully acknowledge the participation of Petr Fousek and Hol-ger Schwenk to some parts of this work.

