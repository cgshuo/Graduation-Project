 Web search engines consistently collect information about users in-teraction with the system: they record the query they issued , the URL of presented and selected documents along with their ran king. This information is very valuable: It is a poll over millions of users on the most various topics and it has been used in many ways to mine users interests and preferences. Query logs have the po tential to partially alleviate the search engines from thousand of s earches by providing a way to predict answers for a subset of queries a nd users without knowing the content of a document. Even if the p re-dicted result is at rank one, this analysis might be of intere st: If there is enough confidence on a user X  X  click, we might redirec t the user directly to the page whose link would be clicked. In this paper, we present three different models for predicting user click s, rang-ing from most specific ones (using only past user history for t he query) to very general ones (aggregating data over all users for a given query). The former model has a very high precision at lo w recall values, while the latter can achieve high recalls. We show that it is possible to combine the different models to predic t with high accuracy (over 90%) a high subset of query sessions (24% of all the sessions).
 Key References: [10, 12] H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Theory, Algorithms, Experimentation Web Retrieval, Query log analysis, User modelling, re-findi ng, re-peat queries
Social search is quickly gaining acceptance and is increasi ngly seen as a promising way of harnessing the common knowledge of million of users to help each other and search more efficien tly. Users are increasingly understood as the driving force of th e inter-net and many initiatives are aimed at empowering them. Besid es explicit user provided information, there exists very larg e sources of implicit user information in the internet logs that recor d user acts. In particular, search engines keep records of their in teraction with users in click-through logs , which record some sort of user id (through login or cookies), the queries issued by the user , the results returned by the engine and the resulting user clicks .
User clicks on hyperlinks are a soft source of endorsement, s ince users tend to click on documents they are interested in. For t his reason click-through logs are the source of growing attenti on in the search community. In this work we will analyse query logs fro m a novel perspective, concentrating on their click predicting ability .
In particular we are interested in predicting which documen t a user will click on immediately after the issuing a query. We a re interested in models that can accurately predict these clic ks, but more importantly we are interested in models that can predic t the confidence of the prediction. This is important because we wish to be conservative: we are happy not to make a prediction, but if we make a prediction we want to be confident that this predicti on is correct. One can imagine a number of applications for thes e models, if sufficient accuracy can be achieved. For example, in a result page we could highlight the page that we believe is the one the user will choose, or directly take the user to that page.
We do not pretend that it is possible to predict with high accu racy the target page of a user for most queries. If a user types  X  X ta lian cooking X  into a search engine, it is hard to tell if the user wa nts a restaurant, a book, a history page, the page on cooking the us er saw last week and really liked, etc. However, we believe that we c an achieve high accuracy for some queries . In particular, there are two types of queries that we hope to predict with high accuracy.
The first type of prediction we are interested arises when use rs re-find information. Users commonly follow known paths when searching [3] for information they already found. More spec ifically, they will tend to re-issue a query when searching for a docume nt they found thanks to a search engine. Our work was motivated b y the research on click-through logs carried out by Teevan et. al. [10, 11] that analyses this user behaviour. They studied click-t hrough logs and found that 40% of all queries lead to a click on a resul t formerly clicked by the same user during a previous session. Fur-thermore, 71% of the queries leading to repeated clicks by a u ser were identical (same string), and 87% of identical query re-writes led to repeated clicks. Only 14% of the queries leading to a re -peated click also led to a click on a new (previously unclicke d) document. Furthermore, on average 28% of documents clicked by a user received more than one click by that user that year. On t he other hand, only 7% of clicks were clicked by multiple users. With this study, Teevan et. al. clearly showed the important role that re-finding plays in web search today. They went on to build mod els which predict whether a user query is a re-find or not.

In this paper, we go one step forward and we build models which predict, not only if a re-find is taking place, but also what is the user X  X  target page, and which is the confidence of our predict ion. We refer to such type of models as user-centric or user models. Furthermore, we generalise the concept of re-finding to take into account not only queries issued by the user in the past, but al so queries issued by similar users ; we refer to these as user-group models . Finally, we can extend our models to deal with simple navigational queries that many other users have issued in the past and agreed on (i.e.  X  X BC X ,  X  X LM X , etc.). We refer to these mod els as global .

We note here the difference between this type of predictive m od-els and personalisation models. Personalisation models attempt to produce better result rankings; this is done by building a mo del of the user, and then biasing the general ranking function with the user model to improve relevance. In our work we are trying to achie ve something else: predicting with high confidence the target p age of a user involved in a re-finding activity. In fact, personalis ation is a complementary problem to this one, and indeed personalisa tion models could be used simultaneously to the prediction model s de-scribed here.

This paper contains a number of novel ideas: Furthermore it evaluates the different models as well as a ba seline in real click-through data from a commercial search engine. Our re-sults show that predictive models can be used effectively to greatly improve the search experience on the web.
In this section we will give our formal definition of the probl em of predicting user clicks and present a na X ve baseline model . Fur-thermore we will propose some evaluation measures for this p rob-lem. To our knowledge, the problem definition and the measure s are novel.
Assume that we recorded the queries and clicks of users over a period of time. We call a query session s := ( u,q,t,D ) denoting a user u , a query q , a time t , and a sequence of clicked documents D := ` d 1 ,...,d | D |  X  . For a given session s , we use q u , etc. to denote the different elements in s (the query, the user, etc.). We note the sets of users, sessions and queries U , S and Q . We note S u the set of all the sessions by the user u , and S subset of these with the same query q . We can count the different clicks issued by users on specific documents and queries. The basic counts from which the rest are derived are called click counts :
To simplify notation, we have dropped the dependency of the different quantities with respect to the time t . It will be implicit for the remaining of the paper that any computation or predic tion made at time t can made using of (but only of) the sessions in the past { s | t s &lt; t } .

Furthermore, we will compute click counts in two alternativ e ways. We will either count all clicks or single clicks. All clicks are straightforward counts over the click-through data. When c ount-ing single clicks, we eliminate all the sessions for which th e user clicked more than once (i.e. | D | &gt; 1 ). Clicking on a single docu-ment may be a stronger indication of user satisfaction than c licking on several. If the user did not try anything else, she was prob ably satisfied with the first answer. On the other hand it is possibl e that after observing a single document she abandoned altogether or she re-issued a new query.

Our objective is to define models which makes predictions of clicks. We will study this from the point of a Bernoulli proce ss: a sequence of binary events. After a user u issues a query assume that she is presented with a single document d . This event, noted  X  uqd , has two possible outcomes: the user either clicks on the document (  X  uqd = 1 ) or not (  X  uqd = 0 ). This process is repeated many times. We will build different estimators of Pr(  X  uqd using different assumptions.

A prediction  X  uq  X  X  W,  X  X  is a choice of a document for a given user and query at a given time. We want to allow ourselves to be conservative so we allow null predictions (  X  ), particularly for the cases were there is not enough confidence in any particular pr edic-tion. A prediction model will always follow the two steps: where  X  qud is some measure of confidence on our prediction Different estimators of Pr(  X  uqd = 1) and of  X  will lead to different predictions.

There are various ways to deal with equality, i.e. when there is more than one document with same value of Pr(  X  uqd One could select one document randomly, or to make a null pre-diction since there is no preference and we are targeting one click sessions. We chose the latter in our experiments since intro ducing randomness in our model did not match one of our requirements , controlling the confidence of the model. To measure performances, we define a precision-recall metri c. In order to measure different aspects of our prediction, we c ompute the expectation over the users and queries. Doing so allows u s to give, for example, an equal importance to each user or, in the op-posite, an importance proportional to the number of times th e user asked a query. We consider prediction to be correct if the pre dicted target page for session s (noted  X  s ) was clicked by the user in that session (that is, if  X  s  X  D s ). where: We experimented with many of these combinations, but it did n ot change the relative benefit of the models. Therefore we repor t on the simplest: each session has the same weight:
Figure 1a shows the probability that a user will click on a doc -ument after a query as a function of the number of times the use r has already clicked on the same document for the same query in the past. This is equivalent to a model where Pr (  X  uqd = 1)  X  v If we count all clicks, we see that this probability is more th an half for documents that have been clicked at least once, and g rows quickly to over 0.95. The behaviour is very similarly for single clicks , but the starting point is much higher (0.8 instead of 0.55). This confirms the hypothesis that single clicks are more info rma-tive. Figure 1b shows the histogram of the number of previous clicks on that document for every user click.

The statistics in 1 give us an idea for a baseline model: predi ct the most clicked page if it has been clicked above a certain nu mber of times. This can be done with:
This will be very precise for queries that the user repeats of ten to re-find pages. It will be very unprecise for queries that th e user issues rarely, or queries for which the user is exploring for new information rather than re-finding. Furthermore, it will no t be de-fined for any queries that the user has not previously typed. W e refer to this type of model as a user-centric because it depends on the document, the query and the user.

In fact the performance of this estimator can be estimated di -rectly from Figure 1. For example, if we set the threshold to we would make a prediction for roughly 14% of the queries a use r types, and we would be correct roughly 95% of the times we make a prediction.

As we can see, this baseline can reach very high accuracy, but this comes at the price of being very conservative (i.e. not m aking any predictions for 86% of the queries).
We wish to improve over the baseline models proposed above in a number of ways. First, we do not want to have to choose a priori a value for the threshold  X  : this value should depend on the click distribution of each query. Very consistent queries should require little evidence, whereas noisy queries should require more . To take this into account we will use Bayesian estimators, which int egrate naturally the notion of confidence in the absence of the infini te data. Second, we wish to extend our predictions to queries that the user never typed; for this, we will extend the notion of re-finding .
We wish to model the likelihood p (  X  uqd = 1 |L ) considering our data as coming from a Bernoulli process of clicks and no-clicks, where L is any available past information that can modify our knowledge on  X  uqd . In the Bayesian framework this likelihood is the result of integrating all the possible Bernoulli mode ls (that is over all possible values of the probability  X  of success), weighted by their likelihood given past information L :
The advantage of using Bayesian estimators is that they will nat-urally be pulled towards their prior likelihood p (  X  ) in the absence of sufficient data. Choosing the prior likelihoods appropri ately, the estimators will be naturally conservative in the absence of strong evidence 1 . This will allows us to trust their estimation directly, not only for prediction but also to determine our confidence. In o ther words, when using Bayesian estimators we will set:
This way we can set  X  to some global fixed value ( 0 . 95 ple), and the implications of this will be handled by the esti mator automatically for every model.

A natural choice of prior for the Bernoulli is its conjugate p rior, the beta distribution  X  (  X  ; a,b ) with parameters a,b &gt; 0 parameters correspond to pseudo-counts or fictitious a priori obser-vations of events, in our case clicks on the document or on oth er likelihood of a click on the document, and the magnitude determine the strength of the prior over the observations.
Without any information, i.e. when L is empty, the prior is gov-erned by the a priori parameters a and b , and is given by:
As we gather more and more information, we need to update our prior on  X  . As the beta distribution is a conjugate prior of the Bernoulli process, the posterior distribution of the param eters when some data is available is a simple count of positive (clicks) and negative (non clicks) events in the Bernoulli process that w e add to the fictitious observations. Using the Bayes law, the post erior distribution of a beta distribution, knowing observations coming from a Bernoulli process, is given by: be achieved by smoothing the estimators of frequency (maxim um likelihood estimators) so they are robust to small data sets ; we have chosen the Bayesian framework instead because we are more fa -miliar with it. Eq. (2) can be solved analytically (see [6] for a full develop ment): where S denotes the set of events considered, S d denotes the subset of positive events and S \ S d the subset of negative ones. In our case, S are the sessions considered in the model and S = { s | d d, s  X  S } the subset of these which lead to a click on d Eq. (2) into Eq. (1), we can write the solution to (1) analytic ally: There are several choices to construct such sets. We will exp lore two options for S and S d , namely user-query models and global-query models. In the first case we build a model for each query of each user u independently, using only data coming for that user, whereas in the second model we mix all the users together into one group: user-centric model: Pr u (  X  uqd = 1) is obtained from (4) using global model Pr g (  X  uqd = 1) is obtained from (4) using In order to fix the values of the hyperparameters a and b standard optimisation technique [7] described in Appendix A. We found values for a and b of 1 and 0.3 for the user-centric model, and 29.7 and 6.8 for the global one. In both cases, there is a favou rable a priori on the document. This was expected since the paramet ers are computed for documents in the set of those already clicke d by the user(s). Note also that the a priori parameters for the gl obal model are much higher than those of the user model. This impli es that the user centric model will be much more influenced by new observations.
In the previous section, we explored how we can confidently pr e-dict a document for a user-query pair. We can use this informa tion to build groups of related users. When a user-query pair does not have enough related past information to predict confidently a doc-ument, this information can still be used to find an already ex isting group of users that is close to the user. We construct such gro ups such that to each group corresponds one and only one document to be predicted with high confidence.

To build such groups, we would like to group users that would click on the same document for the same query. Since this info rma-tion is not available, we use the user model defined in the prev ious section, and group users that would be, for a given query, predicted the same document. Let us define the user group G qd as the set of users that would be predicted the document d for the query
This definition is deterministic at each time t since predictions are deterministic. We can define click counts for a group as fo llows:
These clicks take only into account the users that are highly re-lated to a group of users who click on d for query q with a high confidence. In other words, it ignores clicks from users who d o not prefer the document d clicked. With these counts we can compute our third type of predictive model: group model P gqd  X  (  X  uqd = 1) is obtained by multiplying the The interest of group models is that they pool clicks from similar users, allowing the confidence to grow much faster. If a user c licks on a document once, but there are many similar users who also clicked on it, we can be confident that this document is a targe t.
Each one of the three models proposed has different characte ris-tics. For example, global models can be very accurate for pop ular navigational queries whereas user models can be very accura te for user specific re-find queries. Since the models seem compleme n-tary, it makes sense to combine them into a single model.
We are currently working on probabilistic models that can co m-bine all the methods, but this work is not completed. Instead , we propose here a simple ad-hoc method to learn the combina-tion of the models. The method is based on one RankBoost algo-rithm [5] described in Appendix B. We call this model the Rank-Boost model. Other models for combination can be used, but Rank-Boost had the advantage of working well with ranking models l ike ours.
In this section, we report experiments with the three models : user-centric, community and global. We report results of in dividual models as well as the combination model.
We made use of click-through logs from the Yahoo search engin e over a period of 57 days. In order to associate users with a que ries, we eliminated from the log all the users for which we did not ha ve a unique ID (i.e. users not logged into the search engine).
In order to establish user sessions we used a timeout of thirt y minutes: every click related to the same query and user withi n thirty minutes was considered to be within the same session. This th resh-old was suggested by [11]. We verified this threshold with our data: in Figure 2 we plot the distributions of time differences bet ween two consecutive clicks of the same user for the same query (fo r all users and clicks in the 57 days). We see that the clicks decrea se ex-ponentially between 10 and 100. Average query length comput ed by removing non alphanumeric characters and one letter word s was of 2.9 words which is also comparable to values reported in th e lit-erature [12, 11].

Queries were not normalised lexically or semantically in an y way. This could be an issue, since a small scale experiments a bout refinding has shown that within a single hour, only 72% of user s remembered exactly a query. This is something we may try to im -prove in future experiments. However it is difficult to norma lise queries semantically with introducing much noise (i.e. by c onfus-ing two different user intents). A consequence of this choic e is that the model favours high precision to high recalls.
 Figure 2: Percentage of events within a given time frame. The 30 minutes limit is the dashed straight line.

Another open issue is the fact that our query logs did not cont ain information about URLs that were not selected by users, and con-sequently of the rank of the URLs clicked. This has a direct ef fect on performance analysis, because we cannot know whether a UR L was not clicked upon because the user saw refused it or becaus e it was not even presented to the user. Moreover, as stated in [ 11], the simple change in presentation order (27% of reclicks are at a different rank as they were for the first click) decrease the n umber of re-clicks. We ignore these facts in our work.
In Figure 3, we plotted the precision of the different models with respect to the recall measure described before for all click counts. The curves were obtained by using different threshold value s on  X  qud . The left hand side graph (a) is a zoom of the right hand side graph (b) for low values of recall. In this graph we can see the per-formance of the user-centric methods, which cannot predict beyond 13% of recall. First we note that the baseline method (Count) per-forms worse than the other methods in general, but neverthel ess it has some predictive power: for 1% of sessions they can predic t the target page with 90% accuracy! The maximum likelihood (MaxL k) method performs similarly to the count method, but is not abl e to distinguish the cases where there was only a few  X  one or two  X  sessions from the cases where there are more. Consequently, this model is not able to reach high precision values for lower rec all val-ues. Probabilistic models (User, Group and Global) are much better than the baseline for very low recall regions: they can build confi-dence faster than the counting method. However they are simi lar to the baseline for high recall levels (beyond 10%). It seems th at our naive method of assigning group membership is not sufficient to increase the precision. We plan to investigate this further in the fu-ture. Global methods reach much higher recall values (up to 5 0%) as expected, but surprisingly they seem almost as accurate a t the low recall (high rejection) end. The probabilistic global m odel out-performs the baseline for all values of recall. It also outpe rforms the probabilistic user-centric models for all recalls beyo nd 0.06. This is impressive and unexpected. Finally, the RankBoost m ethod successfully combines the different methods producing mod el that outperforms all others for all ranges of recall. Furthermor e, the combination model is almost 5% better than the global one con sis-tently for all values of recall. recall, whereas global methods reach 50%.
 Figure 4: Number of sessions (for predicted or not predicted results) with respect to the rank.

These are very encouraging results. It means that we can hope to predict the target page of a user 50% of the time with an accura cy of 75%, or alternatively 5% of the time with an accuracy of 98% . Such high accuracy can open up the way to new forms of search interfaces which will pro-actively highlight or open the pr edicted target pages.

In the following we will analyse further two aspects of our mo d-els to gain insights on how this accuracy is being achieved by the different models. The first question we pose is: how many of th ese predictions were already ranked 1 by the search engine witho ut any history information? To some degree, predicting correctly a click on rank 6 is more useful to the user than predicting correctly a click one at rank 1. In Figure 4, we plotted the percentage of predic tions we made and the percentage of clicks given a rank. For instanc e, 9% of the good predictions where documents at a rank superior or equal to 2. This should be compared with the click behaviour o f users, who click at rank 2 or more 60% of the time. We can also remark that the more specific strategy, the higher the ranks o f the predicted documents, and therefore the more useful the pred iction. We note however than even if all our predictions were at rank= 1, our models would still be very useful because they do not only pick a document, they also decide when to predict and when not to. This confidence feature is useful to design search interfaces tha t do not solely rank documents; for example, if we wish to highlight t he predicted document, open it directly on a browser window, et c.
The second analysis is about the relationship between single click and all click ways of computing the counts. In Figure 5, we plotted the distribution of the number of clicks per session for our d ifferent methods. We can observe that all models predict mostly one cl ick queries (more than 80% of the cases). The combination method makes more predictions for multiple click sessions. It show s that combining the informations from the different models can be very advantageous, since it allows to predicts clicks in multipl e clicks sessions.
Recently there has been some work on user refinding behaviour , which can be roughly divided between qualitative studies and quan-titative studies. Qualitative studies aim at analysing how the user is searching some information he/she has already seen. As us ers have to be recruited for the experiments, these studies are t ypically conducted on a small number of users and queries for which de-tailed information is collected, including the navigation behaviour of the user (going back, using the history of the browser, etc .). On the opposite, quantitative studies use only basic informat ion (clicks, query string, time stamp, etc.) collected by the search engi ne. They aim at providing statistics on how to characterise clicks of users trying to re-find information. As they use server side log dat a, they don X  X  need to actively involve the user into the experim ents, and hence can be conducted with a great amount of users and/or queries. Our work belongs to this latter category. Figure 5: Average number of clicks in sessions in general and predicted sessions.

Within qualitative studies, we can cite [3, 1], authors stud y the key strategies for information re-access and underline the fact that search engines are widely used for this task, although this  X  strategy has problems as it is difficult to remember the exact search te rms used to find the material in the first place X . While we did not st udy the query reformulation between queries issued by the same u ser while searching for the same information, we found out that t he already available information is enough to ease refinding.
Wedig et al. [12] studied the amount of data that is available for personalisation, and found out that even when users are not l ogged it is possible to use cookies: In this case, the amount of info rmation is enough to personalise the search results. They also prese nted some results on the amount of information provided by a click , and could be useful in an extension of our work to predict user int erests in general in order to build other groups of users.

Using web logs, Tauscher and Greenberg [8] have found out tha t people tend to revisit a considerable number of web pages: Th ey found a 58% probability that the next page visited was previo usly seen. Noticeably, they also found out that users visit a few w eb pages frequently. This matches our findings, since people te nd to use bookmarks queries for the few web sites they visit freque ntly, hence providing one possible explication for the good resul ts of our predicting models.

Dumais et al. [4] proposed a system for facilitating informa tion re-use that provide meta-information about the searched re sults. The Re:Search search engine [9] was designed to help people r eturn to information in the dynamic environment of the Web by maint ain-ing consistency in the search results it returns across time through a mechanism of cache of past queries and merge between past cli cked results and current results returned by an external search e ngine.
As mentioned in the introduction, the work of Teevan et al. [1 1] inspired this paper. In their work, they focussed on the anal ysis of navigational repeated queries. They also designed a Suppor t Vec-tor Model to predict whether or not a result would be clicked, and whether or not a previously clicked document would be clicke d. The difference with our work is that they did not focus on mode ls where misclassification error can be controlled, and did onl y con-sider previous clicks from the same user.
 Web search queries can be categorised into different catego ries [2]. In this article, we have shown that it is possible to use past c licks to predict with high accuracy (over 90%) a high subset of quer y sessions (24% of all the sessions), that shows that we are abl e to predict most of the navigational queries with high accuracy .
Finally, we can improve our model by taking into account mul-tiple queries. To investigate if this is worthwhile, we plot ted in Figure 6 the number of distinct queries per document. We can s ee that the far majority of documents are clicked for a unique qu ery, but that there is still a great number of documents that were c licked for more than one query. This shows that there is some informa tion that can be exploited there.

We are currently working on using complete (including not clicked results displayed to the user) query logs for a larger time fr ame (one year) for a subset of users. We don X  X  expect our findings to be changed, although it would be interesting to take into accou nt the time in our models.

Another important direction would be to cope with informati onal queries. The difficulty lies in the variability of the user cl icks and of the queries. It would be interesting to look at query chain s (re-formulations) for this purpose.
In order to set the a priori hyperparameters, we used data fro m the first part of the logs. For some configurations of n = |S| p = |S\S d | , we can count the number of times o ( n,p ) the user clicked on the document d link and the number of time O ( n,p ) configuration occurred. We can then compare the observed num ber of clicks o ( n,p ) to the expected one E ( # clicked p )) . As we can suppose that the events (clicking on document afte r having clicked on it n times out of n + p sessions) are independent, the expected number of clicks is simply E ( # clicked / X  ( a + n,b + p )) = O ( n,p )  X  E (  X  ( a + n,b + p )) We then optimised the mean square error between the observat ion and the expectation to set the a and b parameters: ( a,b ) = argmin where there is o ( n,p ) sessions out of O ( n,p ) where a user, who had already clicked n times out of N sessions on a given document, clicks on it again. We minimised the function using a Newton-type algorithm [7], and found priors of 1 and 0.3 for the user model , and 29.7 and 6.8 for the global one.

We did not choose to optimise the maximum likelihood as we had many cases where either all the users clicked on the docum ent or none of them did, yielding infinite values in the optimisat ion criteria.

We adopted the RankBoost algorithm [5], a boosting algorith m for combining models. RankBoost is a machine learning algor ithm that searches an optimal combination of several weak or unce rtain classifiers, which are in our cases the different models for U RL prediction. Each of the above described models can compute a score for a document d , a query q and a user u . The corresponding scores are the input to the RankBoost algorithm.

Thanks to the fact that the decisions are binary, we used the s im-plest version of the RankBoost algorithm (RankBoost.B): At each iteration i of the algorithm, a base model m ( i ) is chosen, along with a threshold score and a weight  X  i . This gives rise to a thresh-old function f i which equals 1 if p learnt threshold and 0 otherwise. As user level models canno t com-pute always a score, a predefined and learnt value q i is used for p for a given model).
In our experiments, we performed 1000 iterations of the algo -rithm which are sufficient for stabilisation of the performa nce over the training set.

The result of learning is a set of step functions f i , one for each base model. The final probability of an URL d for a query given by
We furthermore required that each function f i is decreasing with respect to the score v i , in order to avoid over-fitting as suggested by the authors. The consequence is that  X  i &gt; 0 for any that v rb is an increasing function of the v i : The learnt function can be seen as a combination of monotonic increasing step functi ons of p
