 Privacy-preserving data mining has concentrated on obtain-ing valid results when the input data is private. An extreme example is Secure Multiparty Computation-based methods, where only the results are revealed. However, this still leaves a potential privacy breach: Do the results themselves violate privacy? This paper explores this issue, developing a frame-work under which this question can be addressed. Metrics are proposed, along with analysis that those metrics are con-sistent in the face of apparent problems.
 H.2.8 [ Database Management ]: Database Applications X  Data mining ; H.2.7 [ Database Management ]: Database Administration X  Security, integrity, and protection Security Privacy, Inference
There hasbeengrowing interest inprivacy-preservingdata mining, with attendant questions on the real effectiveness of the techniques. For example, there are discussions about the effectiveness of adding noise to data: while adding noise to a single attribute can be effective [3, 2], the adversary could have much higher ability to recover individual val-ues for multiple correlated attributes [12]. An alternative encryption based approach was proposed in [14]: nobody learns anything they didn X  X  already know, except the result-ing data mining model. While [14] only discussed the case
This material is based upon work supported by the Na-tional Science Foundation under Grant No. 0312357.
The following example shows that for the  X  X edical diag-nosis X  scenario above, it is reasonable to expect that pub-lishing the classifier will not cause a privacy violation. Indi-viduals can use the classifier to predict their own likelihood of disease, but the adversary (insurer) does not gain any additional ability to estimate the likelihood of the disease.
To simplify the problem, we assume that the classifier is a  X  X lack-box X : the adversary may probe (use the classifier), but cannot see inside. An individual can use the classifier without any risk of disclosing either their private data or their private result. 1 This represents a best-case scenario: If this classifier violates privacy, then no approach (short of limiting the adversary X  X  access to the classifier) will provide privacy protection.

Formally, suppose X =( P,U ) T is distributed as N (0 ,  X ) with where  X  1 &lt;r&lt; 1 is the correlation between P and U .As-sume that for n independent samples ( x 1 ,x 2 ,...,x n )from N (0 ,  X ), the sensitive data S =( s 1 ,s 2 ,...,s n )canbedis-covered by a classifier C 0 that compares the public data p i and the unknown data u i : We now study whether publishing the classifier C 0 violates privacy, or equivalently, whether the adversary can get a better estimate of any s i by probing C 0 .

Given the public data p i for an individual i ,theadver-sary could try to probe the classifier C 0 to get an estimate of s i as follows. It is reasonable to assume that the adver-sary has knowledge of the (marginal) distribution that the u are sampled from; we can even assume that the adversary knows the joint distribution that ( p i ,u i ) T are sampled from, or equivalently  X  or r . (We will see soon that though the adversary seems to know a lot, he doesn X  X  know anything more about the s i  X  this makes our example more surpris-ing). Thus for each individual or for each p i , the adversary could sample  X  u i from the conditional distribution of ( U | P ), he then can use the pairs ( p i ,  X  u i ) T to probe C 0 and get an
This is feasible, for examples see [9].
To understand the privacy implications of data mining re-sults, we first need to understand how data mining results can be used (and misused). As described previously, we as-sume data is either P ublic, U nknown, or S ensitive. We now discuss additional background leading toward a model for understanding the impact of data mining results on privacy.
We assume an adversary with access to P ublic data, and polynomial-time computational power. The adversary may havesomeadditionalknowledge, possiblyincluding U nknown and S ensitive data for some individuals. We want to ana-lyze the effect of giving the adversary access to a classifier C ; specifically if it will improve the ability of the adversary to accurately deduce S ensitive data values for individuals that it doesn X  X  already have such data for.
Iftheclassifier model C iscompletely open (e.g., a decision tree, or weights in a neural network), the model description may reveal sensitive information. This is highly dependent on the model.

Instead, we model C as a  X  X lack box X : The adversary can request that an instance be classified, and obtain the class, but can obtain no other information on the classifier. This is a reasonable model: We are providing the adversary with access to C ,not C itself. For example, for the pro-posed CAPPSII airline screening module, making the clas-sifier available would give terrorists information on how to defeat it. However, using cryptographic techniques we can provide privacy for all parties involved: Nothing is revealed but the class of an instance[9]. (The party holding the clas-sifier need not even learn attribute values.)
Here, we will only consider the data mining results in the form of classification models. We leave the study of other data mining results as future work.
While itisnice toshow thatan adversary gains noprivacy-violating information, in many cases we will not be able to say this. Privacy is not absolute; most privacy laws provide for cost/benefit tradeoffs when using private information. For example, many privacy laws include provisions for use of private information  X  X n the public interest X  X 6]. To trade-off the benefit vs. the cost of privacy loss, we need a metric for privacy loss.

One possible way to define such a metric for classifier ac-curacy is using the Bayesian classification error. Suppose for data ( x 1 ,x 2 ,...,x n ), we have classification problems in which we try to classify x i  X  X  into m classes which we labeled as { 0 , 1 ,...,m  X  1 } . For any classifier C : we define the classifier accuracy for C as:
Does this protect the individual? The problem is that some individuals will be classified correctly: If the adversary can predict those individuals with a higher certainty than the accuracy, then the privacy loss for those individuals is worse than expected. Tightening such bounds requires that
First, assume attributes P and U are independent, or more generally, though P and U are dependent, C only con-tains the marginal information of P .Insuchcases,classifier C wouldn X  X  be much help to the adversary: as C contains no valuable information of U ,weexpectthat C wouldn X  X  be much more accurate than random guess, and as a result, we expect that the adversary is unable to improve his estimate about S by using C , or formally, the Bayes error for all clas-sifiers using P only should be the same as the Bayes error for all classifiers using ( P,C ( P )).

However, it is expected that C contains information on the joint distribution of P and U (or equivalently the condi-tional information of ( U | P )), otherwise C would be uninter-esting (no better than a random guess.) The adversary can thus combine C or C ( P ) with already known information of P to create an inference channel for S , and the prediction accuracy of the newly learned classifier violates privacy.
Formally, given C and t samples from P,S , letting be the Bayes error for classifiers using P only and using P,C ( P ) respectively; also, letting we have the following definition:
Definition 1. For 0 &lt;p&lt; 1, we call the classifier C ( t,p )-(  X  ,p )-privacy violating if  X   X  ( C )  X   X   X   X  p .
The important thing to notice about the above definition is that we measure the privacy violation with respect to number of available samples t .Anadversarywithmany training instances will probably learn a better classifier than one with few training instances.

In this case, the release of the C 1 has created a privacy threat. The main difference between this example and the one given in the Section 1 is that we put a limitation on the number of available examples to the adversary.
We now give a formal analysis of such an inference in the case of Gaussian mixtures. Although we gave our defini-tions for a classifier C , in the case of the Gaussian mixtures, the sensible way to model C is the conditional distribution of some particular attribute based on the other attributes. Note that C can also be viewed as a  X  X lack box X .

Suppose X =( P,U ) T is distributed as a n -dimensional 2-point mixture (1  X  ) N (0 ,  X ) + N (  X ,  X ), where For a set of t realizations X =( x 1 ,x 2 ,...,x t )(here x i = ( p ,u i ) T ), t sensitive data S =( s 1 ,s 2 ,...,s t ) are generated according to the rule: Assume: distribution of S in detailsincehealready knewthemarginal distribution of P . Furthermore, he can use this conditional distribution tosample u i basedoneach p i , the resulting data s =( p i ,  X  u i ) T is distributed as (1  X  ) N (0 ,  X ) + N (  X ,  X ); though s i  X  X  are not the data on our hand, but in essence the adversary has successfully constructed an independent copy of our data. In fact, the best classifier for either case is the Bayesian rule, which classifies s i  X  X  to 1 or 0 according to here we use f ( x ;  X ,  X ) to denote the density function of N (  X ,  X ). Thus there won X  X  be any difference if the adversary know any u i  X  X  of our data set, or just know the conditional distribution of ( U | P ). This suggests that when S is highly correlated with U , revealing any good method to predict U may be problematic.
For most distributions it is difficult to analytically evalu-ate the impact of a classifier on creating an inference chan-nel. An alternative heuristic method to test the impact of a classifier is described in Algorithm 1. We now give experi-ments demonstrating the use, and results, of this approach. Algorithm 1 Testing a classifier for inference channels 1: Assume that S depends on only P,U , and the adversary 2: Build a classifier C 1 on t samples ( p i ,s i ). 3: To evaluate the impact of releasing C , build a classifier 4: If the accuracy of the classifier C 2 is significantly higher We tested this approach on several of the UCI datasets[4]. We assumed that the class variable of each data set is pri-vate, treat one attribute as unknown, and simulate the effect of access to a classifier for the unknown. For each nominal valued attribute of each data set, we ran six experiments. In the first experiment, a classifier was built without using the attribute in question. We then build a classifier with the unknown attribute correctly revealed with probability 0.6, 0.7, 0.8, 0.9, and 1.0. For example, for each instance, if 0.8 is used, the attribute value is kept the same with probability 0.8, otherwise it is randomly assigned to an incorrect value. The other attributes are unchanged.

In each experiment, we used C4.5 with default options given in the Weka package [17]. Before running the exper-iments, we filtered the instances with unknown attributes from the training data set. Ten-fold cross validation was used in reporting each result.

Most of the experiments look like the one shown in Fig-ure 1 (the credit-g dataset). Giving an adversary the ability to predict unknown attributes does not significantly alter classification accuracy (at most 2%). In such situations, ac-cess to the public data may be enough to build a good clas-sifier for the secret attribute; disclosing the unknown values to the adversary (e.g., by providing a  X  X lack box X  classifier to predict unknowns) does not really increase the accuracy of the inference channel.

In a few data sets (credit-a, kr-vs-kp, primary-tumor, splice, and vote) the effect of providing a classifier on some real values are obscured, protecting privacy while preserving statistics on the collection. Recently data mining techniques on such altered data have been developed for constructing decision trees[3, 2] and association rules[15, 7]. While [2] touched on the impact of results on privacy, the emphasis was on ability to recover the altered data values rather than inherent privacy problems with the results.

The second approach is based on secure multiparty com-putation: privacy-preserving distributed data mining[14, 5, 11, 16, 13]. The ideas in this paper compliment this line of work. Privacy-preserving data mining tries to guarantee that nothing is revealed during the data mining process. In our case, we want to make sure that even a limited access to the data mining result does not cause a privacy threat.
The inference problem due to query results has also been addressed in a very different context: Multi-level secure databases. A survey of this work can be found in [1]. This does not address the privacy threat due to the data mining result, and does not directly apply to our problem.
Increasesinthepowerandubiquityofcomputingresources pose a constant threat to individual privacy. Tools from privacy-preserving data mining and secure multi-party com-putation make it possible to process the data without with disclosure, but do not address the privacy implication of the results. We have defined this problem and explored ways that data mining results can be used to compromise privacy. We gave definitions to model the effect of the data mining results on privacy, analyzed our definitions for a Mixture of Gaussians for two class problems, and gave a heuristic example that can be applied to more general scenarios.
Wehavelooked at othersituations, such as a classifier that takessensitive dataas input(can sampling theclassifier with known output reveal correct values for input?) and privacy compromise from participating in training data. We are working to formalize analysis processes for these situations. We plan to test our definitions in many different contexts. Possible plans include a software tool that automatically as-sesses the privacy threat due to the data mining result based on the related training instances and the private data. We also want to augment existing privacy-preserving algorithms so that the output of data mining is guaranteed to satisfy the privacy definitions, or the algorithm terminates without generating results. Finally, we want to be able extend the formal analysis to more complex data models using tools from statistical learning theory. [1] N. R. Adam and J. C. Wortmann. Security-control [2] D. Agrawal and C. C. Aggarwal. On the design and [3] R. Agrawal and R. Srikant. Privacy-preserving data
