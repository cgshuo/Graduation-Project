 Matteo Cristani  X  Elisa Burato Abstract Moral dilemmas are one of the major issues of current research in ethical reasoning. In particular, it is well known that admitting moral dilemmas in Standard Deontic dilemmas means performing preferential reasoning, we argue that one simple approach to rings between two actions, agents have intrinsic preferences based on classification issues, ferential reasoning is performed by using a second-level choice approach. Decision theory has dealt with the problem of making decisions in presence of conflicting decision criteria, tice, the choice of preferences in presence of conflicting criteria can be seen as a form of humans are quite clever in solving moral dilemmas, and the usage they make of preferential soning in a combinatorial fashion and provide an algorithm for making decisions on moral algorithm and prove that this approach can be applied in practice. 1 Introduction about ethical reasoning [ 12 ]. In particular many authors dealt with the so called Moral Dilemmas [ 10 ].

The prototype dilemmas that are presented in literature (Sophie X  X  Choice, The Father and moral dilemmas: prohibition dilemmas and obligation dilemmas. The first ones are those determined by the fact that an agent is required to choose only an action among two that are both mandatory, being therefore forced not to do one action that was a duty. The second case is determined by two actions that are both forbidden, and the agent cannot avoid to perform one of them, being therefore forced to violate a rule.
 There are two major classes of moral dilemmas: the Ethical Judgement ,andthe Proper Moral Dilemma .

The Ethical judgement is a question posed with respect to a given action act and an agent ag :is act legitimate, by the moral rules to which ag is committed? This seems not to be problematic, since we know that ag commits herself to a given set of actions, and does not commit to any other set disjoint with the first. However, the majority of judgements are complicated by two aspects: (1) though a given action act was ethically acceptable, it may be the case that some of the consequences of doing act would be not; (2) the evaluation of act may be probabilistic, namely we may have a critical judgement of act saying that act is  X  X ikely to be dangerous X , or  X  X robably immoral X , where the uncertainty depends upon either conditions. A complex ethical reasoning process can therefore take place in order to decide about a given ethical judgement. Moreover, an agent may exhibit an incoherent attitude. For she may be convinced that doing something socially dangerous would be not morally worth. damage, or should I not perform it, because is socially dangerous, discharging therefore a duty of mine?
The Proper Moral Dilemma is a question posed with respect to two given actions act 1 and act 2 andanagent ag : what should ag do, between act 1 and act 2 ? The Moral Dilemma questionissaidtobe genuine if and only if none of the two actions prevails upon the other one, namely, when both actions are ethically equivalent. We can immediately extend the above defined dilemma to the case in which involved actions are more than two.
There can be three types of this dilemmas:  X  Obligation dilemma . All the feasible actions are mandatory. The agent cannot do more  X  Prohibition dilemma . All the feasible actions are forbidden. The agent has to do one  X  Incoherence dilemma .Allthefeasibleactionsareincoherent,baseduponthecommitments
The above defined problem is particularly interesting in a multiple agent context, like has been limited to studies that attempt to understand how agents should behave in order to achieve a common goal. The problem of analysing the behavior of those agents that do not act in a cooperative way has been neglected. We are focusing, in this paper, upon uncooperative environment, in which agents either compete in achieving their own goals, or collaborate , but not cooperate. 1
Obviously human agents have different behaviors in the practise of real societies. For instance, rules may not be legal, but just ethical, and individuals can have the attitude of doing only things that are individually advantageous and socially useful.
A widely accepted tool for ethical reasoning is Deontic Logic. Though this approach has been considered as valid by many reputable scholars in the community, it exhibits known difficulties with respect to Moral Dilemmas.

In particular, if we consider a classical Deontic Logic, SDL, namely neither a para-consistent nor a more complex model, with the basic operators ( X  X ught to be X ), ( X  X s necessary X ) and ( X  X s possible X ), we have two fundamental principles: the Principle of Deontic Consistency (PC) and the Principle of Deontic Logic (PD).
 ( PC ) establishes that the same action cannot be both obligatory and forbidden. Dilemmas actually do not conflict with PC. Dilemmas involve a situation in which an agent ought to do A, ought to do B, but cannot do both A and B. But if we add a principle of deontic logic, then we obtain a conflict with ( PC ). We now introduce the Principle of Deontic Logic. How could a moral dilemma be formulated in deontic logic? We essentially have From ( 3 )wederive( 4 ) and further we derive ( 5 ). As an instance of ( PD )wederive( 6 ), which, used along with ( 5 ) derives ( 7 ). Finally we derive, from ( 2 )and( 7 )wederive( 8 )andfinallyfrom( 1 )and( 8 )wederive( 9 ). ( 9 ) contradicts ( PC ). And from PC and ( 1 ), we can conclude In recent investigations, many scholars have tried to deal with the technical difficulties of inserting moral dilemmas into deontic logic. For a rather exhaustive survey see [ 7 ]. However, to the best of our knowledge, no attempt has yet been carried out to analyse the combinatorial and algorithmic questions posed by moral dilemmas in multiple agents systems. 2
The basic conflict in ethical reasoning is generated when an action that is mandatory is not advantageous or neutral from an agent X  X  viewpoint. This contrast, between individually advantageous and legally mandatory, can produce a basic incoherence in agents attitude, when an agent thinks that negative effects on her in terms of individual advantage should be avoided first, and that every legally mandatory action has to be performed. In this case we speak of incoherence dilemmas.

In order to provide a specific family of interesting problems, we need a formalised MAS and provide a solution for it in two simplified cases, and in the general case as well.
The type of reasoning an agent has to perform in an Ethical MAS is complicated by two major facts:  X  Agents can conflict in the way they look at different actions, and this generates a third  X  Conflicts among agents may produce not only dilemmas but also difficulties in decision
Three recent papers advanced ideas upon the models of agency which are produced by a a non-monotonic multi-modal logic arising from the combination of agency, intention and obligation where the problem of moral dilemma as a form of conflict between intention and obligation is introduced and solved by a form of conflict minimization.

Another one which is important in the economy of this investigation is [ 15 ], where authors deal with the problem of implementing the notion of dilemma as a specific computational problem that arises in a well-known logic model, the Prospective Logic.

Finally we consider a goof starting point the paper of Holbo [ 14 ], that is a traditional philosophical investigation with just few possible inspirations for computational purposes.
This paper is organised as follows: Sect. 2 presents Terminology and basic definitions for classifications based upon binary or ternary evaluation methods and in Sect. 4 we show an extension to four valued evaluation criteria and analyse how to map it in the three and two valued ones. In Sect. 5 we show how to solve some basic problems posed in Sect. 2 with simple algorithmic methods. In Sect. 6 we present the algorithm that automates the process of choosing an actions which results to be the approximatively the best among the feasible actions and in Sect. 7 we show the properties the above mentioned algorithm exhibits. In Sect. 8 there are some simple examples that show how the process of choosing action evolves takes some conclusions and sketches further work. 2 Ethical rules and multi-agent systems In this section we formalise the basic notion henceforth used in the paper. This approach is commitment in multiple agent systems. The fundamental notions introduced here constitute an attempt at addressing the issues modelled in [ 5 ].
 evaluates actions for every agent.

For every agent ag ,  X  ( ag ) denotes the actions that ag can perform, called the feasible actions , and for every act  X  X  ( ag ) , e ( act , ag ) evaluates act for ag . r and r 2 can move right and left, whilst only r 3 can move forward and backward. 3 From r r and r 3 , instead, consider all their feasible actions advantageous.

We interpret the above described structure so that if an action is feasible for an agent, then that agent can perform it; if not the agent cannot perform the action. The evaluation function associates to each action for every agent a n -dimensional vector of values in the evaluation semantics, where n is the number of the relevant contexts the agent regards to. In the rest of this paper we use the word context to identify a way with respect to which an the context space .
 Example 2 Consider the agent system of Example 1 . We may have four contexts: the context in certain goals that are out of the interest of robot r 2 ( C ).

In this paper, we consider two basic context spaces. The binary and the ternary one. The binary context space is interpreted as defined by the obedience to the law and the interest in personal advantage . The ternary space is interpreted as defined by the same contexts of the binary one along with a context for social usefulness .

The evaluations contexts are ordered by each agent by two preference relations among agent with respect to her obligations, whilst the negative attitude is the ordering N which establishes preference about her prohibitions. Given the positive and the negative attitudes of an agent, we can introduce the definition of an agent commitment .
 ag apair P , N of total strong orderings of evaluation contexts. Example 3 Again, let us consider the EMAS of Example 1 and the context space of Example 2 . The positive commitment is L &gt; S &gt; C &gt; I , the negative commitment is S &gt; C &gt; L &gt; I .

A commitment P , N is said to be coherent if and only if P and N are equal to each other. The EMAS of Example 3 is incoherent.
 We consider the following problems in EMASs.
 Problem 1 ( Ethical Judgement) Given an agent ag , and an action act , assuming to know the for ag .
 Problem 2 ( Proper Moral Dilemma) Given an agent ag , and a set of actions Acti ons , assu-evaluated in the same way by ag but she has to perform only one action.
 andanagent ag , establish the minimal set of admissible attitudes for ag that are compatible with the performance of the actions of Acti ons .
 Example 4 In the case of Example 4 , with respect to agent r 1 deciding about action f is an ethical judgement. The pair formed by f and b is a moral dilemma for r 1 . The above problems are said to be solvable when they have a solution.

The third problem includes the first, since in order to establish which attitudes are compa-ethical judgements.
 We can claim the following propositions.
 Theorem 1 Ethical judgements of agents with coherent commitments are solvable. Since a has coherent commitments, then N = P . Suppose by contradiction that for an action act , ag would not be able to decide about ethical validity of act . Then, there would be a context c , such that  X  agent ag evaluates act as mandatory in c ; and there would be a context c , such that  X  agent ag evaluates act as forbidden in c .
 Moreover, in the positive commitment order, c and c should be such that c &gt; c , whilst in Therefore the claim is proved.
 Conversely, not all moral dilemmas are solvable by agents with coherent commitments. Obviously, agents with coherent commitments never face incoherence dilemmas. 3 Modelling agents X  commitments in EMAS with binary and ternary evaluation context spaces The binary evaluation space is very simple to analyse. In this space the only two types of incoherence that can be formulated are the ones established by an agent that considers an action simultaneously illegal and personally useful or legally mandatory but personally negative. The first type of incoherence is encountered by those agents who are committed to the law before than to personal advantages from a positive viewpoint and the way around from a negative viewpoint. The second type is encountered by those agents who exhibit inverse commitments wrt the first case.

Both these incoherence situations are strictly unsolvable as claimed in the following proposition whose proof is straightforward and therefore left to the reader. Proposition 1 Ethical judgement of incoherently committed agents of an EMAS under eva-luation on a binary context space is always unsolvable.
 utility, individual advantage. In particular:
Thedirectconsequenceoftheaboveproposedapproachisthatthepossible basic attitudes , classified cases. The combinatorial cases are 27 as in below. irrational. In fact, no one would perform an action that is not personally advantageous, not and it is not useful.

Moreover, some attitudes are trivially correct for all the possible cases. If something is positive or neutral for each of the three above defined cases, then it is worth performing. If something is permitted or enforced by the law, it is socially useful or indifferent, and personally advantageous or indifferent, than it is an action that everyone would perform. The remainder 12 cases are depending to each other in a complex way, represented in Fig. 1 .
 includes x then it includes y as well.
 We now introduce some fundamental notions that we employ henceforth.

The possible positive and negative orderings of contexts are, most obviously, 6. Therefore, in principle, there are 36 commitments, of which 6 are coherent. 4 Modelling agents X  commitments in EMAS with quaternary evaluation context space nings of the possible values are:  X  P the action is mandatory ;  X  pP the action is likely mandatory ;  X  pN the action is likely forbidden ;  X  N the action is forbidden .
 The difference with respect to a ternary evaluation context space lies on the fact that one value ( indifferent (  X  )) is split into two values pP and pN indicating that an indifferent action could be considered by some agent not equally a mid point between an obligation and a prohibition. 4
Often, in the real world, agents may presume that an action act is a legal duty being not legal prohibition but she is not sure because she is not informed of the contrary. 5
The difficulty in analysing a quaternary evaluation space is that devising a total order of the values is not a straightforward task this makes more complicated the comparison between contexts. To model the behavior of an agent when choosing the best action to perform, we have to introduce the agents X  standpoints, namely how agents view their obligations and prohibitions in the context spaces.

In the next subsection we formalise the notion of agent X  X  standpoint and show how this helps us in introducing a mappings from the quaternary evaluation context space to a ternary one. 4.1 Agents X  standpoints In our formalisation we suppose that agents could consider in different ways the notion of considers a likely forbidden action as an indifferent evaluated one. We say that agents have different standpoints based upon how they consider the value likely mandatory and likely forbidden.
 Then an agent can be typed as: We model standpoint as a map from quaternary semantics to a ternary one. Then given an before entering in the EMAS.

Introducing standpoints, the four values ordering becomes total because the evaluation
The mapping of the quaternary evaluation context space in a binary one is a very simple one.Thereisonlyonepossiblemap:itstatesthatanagenthastoevaluatemandatory/forbidden all actions that seem to be so. Figure 6 shows the map and the total ordering. 4.2 Actions evaluation in three contexts by quaternary evaluation context and 4 values semantics space. In Table 1 we show how the actions are considered by all the agent X  X  standpoints. which are:
It is easy to see that PPP is an invariant and legitimate action and that NNN is an invariant illegitimate action.

The remaining actions are very interesting in our study because are actions which are not always performed by all the agents. The execution of these actions depends on the agent X  X  commitment and above all on the agent X  X  standpoints.

For example, examine the action which is evaluated in pP pP pN . An individualist agent sees that action as PP  X  and she considers it as a legitimate action; by the other side, a morally rigid agent sees that action as a  X  X  X  N action and she considers it an illegitimate action. 5 Methodological issues We now introduce the notion of degree of incoherence . In order to provide such definitions we need to introduce obligation of degree k .

An action act is an obligation of degree k for an agent ag with a commitment P , N if for the ( n  X  k + 1 ) th context by P .
An action act is a prohibition of degree k for an agent ag with a commitment P , N if and only if ag evaluates act not negative for the first n  X  k contexts and evaluates act negative for the ( n  X  k + 1 ) th context by N .
 with n contexts, we say that an action act has a degree of incoherence k with respect to an agent ag if and only if ag evaluates act by a commitment P , N where P and N agree about act for the first n  X  k contexts.

If P and N agree in all the contexts, then we say that ag is coherent with the commitment of ag .If P and N are equal, then every action is coherent with the commitment.
An interesting approximation for Moral Dilemma is based on the notion of degree of commitment of an agent ag .Thesetof least positive incoherent actions (LPIA) is formed by those actions whose positive degree of incoherence is minimum within the set S .Thesetof least negative incoherent actions (LNIA) is formed by those actions whose negative degree of incoherence is minimum within the set S . Given an incoherence dilemma, D , formed by a set of actions not coherent with the commitment of an agent ag can be solved by ag in an approximate way if and only if the set of LPIA and LNIA in D is a singletons.
We introduce the notion of degree of incoherence of an action act . We say that an action act is a k -incoherent action for the agent ag with a commitment P , N if the evaluation of act ordered by P and the evaluation of act ordered by N are the same for the first ( n  X  k ) contexts and they differs in the ( n  X  k + 1 ) th context.

Forcoherentcommittedagentsallthefeasibleactionsare0-incoherentbecausethepositive and the negative commitment ordering are the same.
A dilemma is said to be approximately solvable if and only if we can find a unique solution to the dilemma that is not a coherent action with respect to the agent X  X  commitment. Straightforwardly, we can claim the following proposition.
 Proposition 2 Incoherence dilemmas with singleton LPIA set are approximately solvable. Proposition 3 Incoherence dilemmas with singleton LNIA set are approximately solvable. Less obviously, LPIAs and LNIAs can be used to solve Obligation dilemmas as well as we omit them here.
 Theorem 2 Obligation dilemmas with singleton LPIA set are approximately solvable. Proof An agent ag is in Obligation dilemma when all her feasible actions are mandatory. She has to perform only one action then she has to choose one among them. Ideally the best action for ag is the action which has the maximum number of positive evaluated contexts in her positive attitude, namely it is the obligation she considers most important. Suppose ag has the commitment order P , N . ag preference relation between contexts is given by the by ag is the action with the less positive incoherent action as defined above. By definition the set LPIA collects the feasible actions that are the less positive incoherent. Because of LPIA ={ x } is a singleton, the obligation dilemma is resolved by act .
 Theorem 3 Prohibition dilemmas with singleton LNIA set are approximately solvable. Proof An agent ag is in Prohibition dilemma when all her feasible actions are forbidden. She has to perform only one action then she has to choose one among them. Ideally the best action for ag is the action which has the maximum number of negative evaluated contexts in her negative attitude, namely it is the prohibition she considers least worse to perform. Suppose ag has the commitment order P , N . ag preference relation between contexts is to perform by ag is the action with the less negative incoherent action as defined above. By Because of LNI A ={ x } is a singleton, the prohibition dilemma is resolved by act .
In the next section we presents the algorithm an agent adopts to choose the action to perform and which follows the guidelines given by our formalisation. 6 Choosing algorithm Here we present the algorithm the agents execute to choose the actions they have to perform.
Starting from all the feasible actions of an agent, the algorithm chooses the action that has the maximum obligation degree and which is minimally incoherent. We introduce in Definition 6.1 the notion of positive incoherence degree to formalise the concept of being the maximum positively committed .
 k  X  Q with respect to an agent ag if and only if ag evaluates act by a commitment P , N is evaluated as indifferent in the k th context then k = k + 0 . 5else k = k .
When in calculating the positive incoherence degree an indifferent value is found, we add 0 . 5 to the current degree because we assume it is preferable to be indifferent in performing an action instead of being contrary to it.

The choice of the action to perform is done by continuously calculate the positive incohe-The minimal incoherence degree assures that every agent chooses the action she does not consider as a  X  X trange prohibition X . By the mediation of the positive incoherence degree and the incoherence degree, the algorithm returns the action which is the best action to perform described above.

The key methods of the algorithm are: positiveDegree(evaluated action) : it checks the positive incoherence degree selectActs(feasible actions, contexts X  number) : for each actions in the positiveCommittOrderEvaluation(action) : it orders the evaluation of the negativeCommittOrderEvaluation(action) : it orders the evaluation of the findIncoherence(pos action, neg action, standpoint) : it calculates 7 Computational analysis of approximate solution of moral dilemmas In this section, we provide some interesting results belonging to our formalisation. We show that chooseAction is sound and complete and that the algorithm works in polynomial time with respect to the number of evaluation contexts.

We need to define the concept of approximatively the best action in order to prove the above cited results.
 Definition 7.1 An action a is said to be the approximatively the best one with respect to a set of feasible actions S iff 1. positive-incoherence-degree of a best  X  positive-incoherence-degree of a and 2. incoherence-degree of a best  X  incoherence-degree of a .
 In the next theorem we claim the soundness of chooseAction and prove it.
 Theorem 4 ( Soundness of chooseAction ) If chooseAction selects the action a from Proof Consider a set of feasible actions S , and suppose that by applying chooseAction that every actions selected by chooseAction is being namely approximatively the best one.

By definition, an action a best is approximatively the best for the agent Ag when  X  a  X   X  (
Ag ) 1. positive-incoherence-degree of a best  X  positive-incoherence-degree of a and 2. incoherence-degree of a best  X  incoherence-degree of a The first property is assured by the method selectActs that scans the feasible actions and records those ones with the highest positive incoherence degree.

The second property is guaranteed by invoking the method collectIncoherenceDegree , which creates equivalence classes based on the inco-with the minimum incoherence degree.
The proof of completeness for for chooseAction is slightly more difficult. First of all we need to claim the following properties whose proof is a straightforward consequence of the definitions above and is therefore omitted.
 Lemma 1 If the action a best is approximately the best action for a genuine moral dilemma, then a best has the maximum positive incoherence degree and the minimum incoherence degree among all the feasible actions.
 Theorem 5 ( Completeness of chooseAction ) Let ag be an agent in the EMAS M ,if a  X  X  ( ag ) is approximatively the best action in  X  ( ag ) then chooseAction returns it. Proof Suppose that chooseAction returns the action a and that approximatively the best Lemma 1 ,  X  a  X  X  ( Ag )w eha v e : 1 positive-incoherence-degree of a best  X  positive-incoherence-degree of a and 2 incoherence-degree of a best  X  incoherence-degree of a 3 positive-incoherence-degree of a  X  positive-incoherence-degree of a and 4 incoherence-degree of a  X  incoherence-degree of a Thus, by 1 and 2 , the positive-incoherence-degree of a best  X  positive-incoherence-degree of a and incoherence-degree of a best  X  incoherence-degree of a . Moreover, by 3 and 4 ,the positive-incoherence-degree of a  X  positive-incoherence-degree of a best and that incoherence-degree of a  X  incoherence-degree of a best .Then a best = a . Having that the algorithm for choosing action is sound, we have to prove that it is also complete. The following theorem claims the completeness of the algorithm. Theorem 6 chooseAction returns the action a  X  FeasibleActions iff the genuine moral dilemma in input is approximately solvable. Proof It follows by Theorems 4 and 5 .
 The complexity of chooseAction is analysed by the following theorem.
 Theorem 7 chooseAction solves every approximately solvable Genuine Moral number of the feasible actions.
 Proof Consider a set of feasible actions S such that | S |= m . Suppose that approximatively the best action in S would be a .

Suppose being in the worst case: a is approximatively the best action with respect to the last contexts in P ,where P , N is the agent X  X  commitment order.

Let n be the number of the evaluation contexts. In order to select a , chooseAction has then the repeat ... until cycle is executed n times. In each execution, for each actions in S the algorithm checks the positive incoherence degree by making n comparisons and the incoherence degree by making n comparisons. Summing up: n times 2  X  n comparisons for m actions.
 If the actions in the set of feasible one are all evaluated as the same, the genuine moral the resolution is very simple as stated by Corollary 1 .
 Definition 7.2 ( Purely incoherent dilemma (PID)) A genuine moral dilemma is a PID iff there exists no action having the least k  X  X  1 ... n } contexts evaluations equal to other actions.
 Corollary 1 Every PID is approximately solvable.
 Proof It is a direct consequence of Theorem 6 .
 In the next section we make some examples of genuine moral dilemmas and we show how chooseAction resolves them. 8 Examples We make some examples to show how the algorithm works by monitoring the set of the least one evaluation context.
 incoherent values.
 Example 5 Suppose that the agent Ag considers four contexts in which she evaluates her feasible actions and the semantics she wants to evaluate them is three values. Suppose that Ag could perform the actions in Acts ={ P  X  NP , PPN  X  , PPNP , PP  X  X  X } . 6  X  FeasibleActions = Acts  X  &gt; + such that c 1 &gt; + c 4 &gt; + c 3 &gt; + c 2  X  &gt;  X  such that c 3 &gt;  X  c 1 &gt;  X  c 2 &gt;  X  c 4 Regarding to the attitude commitments the actions are evaluated in:  X  positive: Acts +={ PPN  X  , P  X  NP , PPNP , P  X  X  X  P }  X  negative: Acts  X ={ NP  X  P , NPP  X  , NPPP ,  X  PP  X  X  Here is the sequence of steps and controls the algorithm makes to choose what action Ag is worth performing. 1. for each action in F ={ P  X  NP , PPN  X  , PPNP , PP  X  X  X } find the positive incohe-2. Then k P  X  NP = 2, k PPN  X  = 1 . 5, k PPNP = 2and k (  X  PP  X  ) = 1 . 5. The maximum is 3. F ={ P  X  NP , PPNP } ; 4. for each action in F ={ P  X  NP , PPNP } find the incoherence degree j by making 5. j P  X  NP = 3and j PPNP = 3then h [ 3 ]=[ P  X  NP , PPNP ] and F ={ P  X  6. | F | = 1and P  X  NP = PPNP ; 7. for each action in F ={ P  X  NP , PPNP } find the the positive commitment degree 8. k  X  NP = 1and k PNP = 1. The maximum is 1 then; 9. F ={ P  X  NP , PPNP } ; 10. for each action in F ={ P  X  NP , PPNP } find the incoherence degree j considering 11. j  X  NP = 2and j PNP = 2then h [ 2 ]=[ X  NP , PNP ] and F ={ P  X  NP , PPNP } ; 12. | F | = 1and P  X  NP = PPNP ; 13. for each action in F ={ P  X  NP , PPNP } find the the positive commitment degree k 14. k  X  N = 1and k PN = 1. The maximum is 1 then; 15. F ={ P  X  NP , PPNP } ; 16. for each action in F ={ P  X  NP , PPNP } find the incoherence degree j considering 17. j  X  N = 0and j PN = 0then h [ 0 ]=[ X  N , PN ] and F ={ P  X  NP , PPNP } ; 18. | F | = 1and P  X  NP = PPNP ; 19. for each action in F ={ P  X  NP , PPNP } find the the positive commitment degree 20. k  X  = 0and k P = 1. The maximum is 1 then; 21. F ={ PPNP } ; 22. | F |= 1thenreturn a  X  F .
 one evaluation contexts. The example shows that the algorithm choose the action that is the least negative to perform, then the action with the least degree of prohibition. Example 6 Suppose that the agent Ag considers three contexts in which she evaluates her that Ag could perform the actions in Acts ={ X  NN ,  X  X  X  N , NN  X  X  . 7  X  FeasibleActions = Acts  X  &gt; + such that c 1 &gt; + c 2 &gt; + c 3  X  &gt;  X  such that c 2 &gt;  X  c 1 &gt;  X  c 3 Regarding the commitment orders the actions are evaluated:  X  positive: Acts +={ X  NN ,  X  X  X  N , NN  X  X   X  negative: Acts  X ={ N  X  N ,  X  X  X  N , NN  X  X  Here is the sequence of steps and controls the algorithm makes to choose what action Ag is worth performing. 1. for each action in F ={ X  NN ,  X  X  X  N , NN  X  X  find the positive incoherence degree k 2. Then k  X  NN = 0 . 5, k  X  X  X  N = 0 . 5and k NN  X  = 0. The maximum is 0 . 5 then; 3. F ={ X  NN ,  X  X  X  N } ; 4. for each action in F find the incoherence degree j by making the comparisons shown in 5. j  X  NN = 0and j  X  X  X  N = 0then h [ 0 ]=[ X  NN ,  X  X  X  N ] and F ={ X  NN ,  X  X  X  N } ; 6. for each action in F ={ X  NN ,  X  X  X  N } find the positive incoherence degree k considering 7. Then k NN = 0and k  X  N = 0 . 5. The maximum is 0 . 5 then; 8. F ={ X  X  X  N } ; 9. | F |= 1thenreturn a  X  F . 9 Discussion Two branches of current research in Knowledge Representation have shown interest in the problem of collaboration of agents within a formal framework: Multiple Agent Systems studies and Legal Reasoning. Some major problems of the two branches are quite similar from a theoretical viewpoint. In MAS one fundamental question that arises is how to give account to the notion of collaboration. Collaborating means trying to achieve goals (common structures we may have individuals who accept social rules; in a more sophisticated envi-ronment people can also make an effort in the direction of projecting individual advantages not correspond to a society, whilst the second one is the case of society without structured society benefits, society rules). An example of such an approach is in the paper.
Though individuals may sincerely would be interested in collaborating, they may not be able to solve a family of problems known as Moral Dilemmas . In the case of moral dilemmas, agents may be forced to perform actions they do not evaluate positively or may be incapable of doing their duties.

In legal reasoning several interesting approaches have been tried that are based upon different families of formal logics (Deontic Logic SDL, Paraconsistent Legal Reasoning Framework, where, for instance, we do not assume the ex falso quodlibet principle). However, a Deontic Logic that fully accommodates dilemmas is still not completed. 10 Conclusion In this paper, we dealt with the problem of choosing an action among many feasible ones. There are many problem in doing so because of the evaluation of the feasible actions may be the same for all of them or because every action is mandatory for some aspects and forbidden for others.
 We defined the above mentioned problem as Ethical Judgement and Proper Moral Dilemma, and generally we call them moral dilemmas.
 The approach we used in formalising moral dilemmas is about a legal reasoning fashion. We supposed that there are many agents in the system and each of them has her own viewpoint of the world they lives. Agent agency is regulated by the commitment the agents assume. This is the basic formalism upon which the framework is built. The commitment of an agent in performing mandatory or forbidden actions. We named the total orderings of the agent commitments, the positive and the negative attitude of the agent.

We dealt with the problem of representing the attitudes of agents involved in a multi-agent system with ethical rules. Such a system, called Ethical Multiple Agent System, is We introduced the notion of coherence for commitments and specify a notion of degree of incoherence, that allows agents to solve moral dilemmas in an approximate way.
The idea we followed to approximate moral dilemmas is to perform those actions which are the maximum positively committed to and the least incoherent ones in order to let the best reward to the agent by bounding her incoherence feeling in performing it.
To the best of our knowledge, this is the first attempt in literature to study the behaviors of agents in a legal reasoning framework by accommodating in the framework itself moral dilemmas. Such an approach would be particularly useful when these MAS formalise inter-working systems, like Internet Wiki tools, or on-line collaboration systems. References Author Biographies
