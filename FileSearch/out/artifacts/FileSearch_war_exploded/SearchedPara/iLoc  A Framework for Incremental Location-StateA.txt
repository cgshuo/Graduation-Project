 Much research focuses on predicting a person X  X  geo-spatial traversal patterns using a history of recorded geo-coordinates. In this paper, we focus on the problem of predicting location-state transitions. Location-states for a user refer to a set of anchoring points/regions in space, and the prediction task produces a sequence of predicted location states for a given query time window. If this problem can be solved accu-rately and efficiently, it may lead to new location based ser-vices (LBS) that can smartly recommend information to a user based on his current and future location states. The proposed iLoc ( I ncremental Loc ation-State Acquisition and Prediction) framework solves the prediction problem by uti-lizing the sensor information provided by a user X  X  mobile device. It incrementally learns the location states by con-stantly monitoring the signal environment of the mobile de-vice. Further, the framework tightly integrates the learning and prediction modules, allowing iLoc to update location-states continuously and predict future location-states at the same time. Our extensive experiments show that the qual-ity of the location-states learned by iLoc are better than the state-of-the-art. We also show that when other learners failed to produce reasonable predictions, iLoc provides good forecasts. As for the efficiency, iLoc processes the data in a single pass, which fits well to many data stream processing models.
 H.2.8 [ Database Management ]: Database Applications X  Data ming Spatial databases and GIS Algorithms, Experimentation, Performance, Reliability
The increasingly sophisticated set of sensors built into mo-bile devices have recently attracted tremendous amount of R&amp;D attention. The capabilities of a modern mobile phone far surpass those necessary for making simple voice calls. For example, today X  X  smartphone includes WLAN networking, a GPS receiver, an accelerometer, a high resolution camera, and many other sensors. Commercial and research applica-tions are already exploiting this rich set of sensing informa-tion from mobile devices, and common across many of these applications is the use of location information. Given the importance of location information, much effort has been put into accurate and efficient location sensing.
Interestingly, current approaches typically separate the lo-cation information from the state information. For example, many social network services directly utilize geo-coordinates gathered from mobile devices, but state informa-tion such as  X  X t home X  or  X  X t work X  is either not collected, or not fully connected to the location information. Mining location-state information may enable more semantically en-riched information retrieval (e.g., retrieve today X  X  call entrie s when I was at office). And utilizing the state information for predicting users X  intentions may empower a new set of smart content serving applications. For example, if a service wants to push lunch coupons for a particular restaurant to a set of users based only on their current geo-coordinate, it cannot differentiate if the users intend to have lunch or are simply passing by on their way to work. By accurately predicting a user is leaving for lunch, the relevancy of the lunch coupon increases greatly.

In this paper, we propose a novel framework called iLoc ( I ncremental Loc ation-State Acquisition and Prediction). This framework provides two components critical to a new generation of location-based services, namely the efficient acquisition of location-state information and a predictive model to forecast future location-state transitions. More specifically, the primary contributions of this paper include:  X  An unsupervised incremental location-state learning and predication framework (Section 4).  X  A set of effective, yet practical, implementations of var-ious learning and prediction components (Section 5, 6).  X  An extensive empirical evaluation to validate different learning and prediction components, based on over two months of real sensing data from actual users (Section 7).
In the following discussion, we use an example dataset to better illustrate related approaches for location-state de-termination and prediction. Table 1, shows data collected using a mobile device X  X  WiFi scanner sampling the environ-ment every 2 minutes. Each scan generates a set of observa-tions, also called WiFi fingerprints. The first 2 records ( tid1 , tid2 ) are observed when the user is at home, and the last 2 records ( tid30 , tid31 ) are at the office. The middle record ( tid15 ) represents a WiFi access point observed during the transition from home to work.

The problem of mining location states from signal inputs can be viewed as an unsupervised learning task, where many clustering algorithms can be applied. However, we notice that the data is highly unstructured; at each sampling point multiple signal sources can be observed, and the number of potential signal sources can be in the thousands. There-fore, the location state discovery problem is very similar to the topic mining problem in the text mining domain. The leading method for this task is Latent Dirichlet Allocation (LDA) [3,5,13]. LDA is a generative model that uses a mix-ture of multinomials with Dirichlet priors. In our example, an observed signal source in a record is generated from a mixture of latent state variables, and each has a multino-mial distribution. Although LDA has shown very good per-formance in text mining domains, there are three problems when applied to our use case. First, LDA does not con-sider time dependencies. Each record/document is assumed to be independent or interchangeable. Therefore, an LDA model cannot be directly used in predicting future location states. Second, LDA does not have the semantic concept of stationary states, which represent the places where a user spends most of his time. For instance, the first two and last two records in Table 1 can be considered from two sta-tionary states, but not the middle one. However, in a LDA model, the WiFi source in the third record will appear in both of the stationary states, which leads to poor cluster-ing performance. Third, it is difficult to incrementally up-date the LDA model, especially when deciding the number of clusters and estimating the parameters. Although other non-model based clustering algorithms, such as K-means or hierarchical agglomerative clustering (HAC), can also be ap-plied, they have been shown less accurate than model-based methods [8]. Furthermore, these non-model based methods suffer from similar problems as LDA.

Since our data records have time dependencies, sequence mining techniques can be applied. The most popular mod-eling method is hidden Markov model (HMM) [7,10]. HMM is also a generative model, such that each observation is gen-erated by a latent state variable. The latent state at a time point also has a dependency on its previous latent state. The parameters in the HMM can be estimated by using Baum-Welch algorithm, which is a generalized expectation-maximization (GEM) algorithm. Although modeling state transition is part of HMM, the states do not physically cor-respond to the actual location states. This leads to poor performance in predicting future movements. For HMM, the typical predication task is to predict the best set of latent state sequences for a given observation sequence, and the Viterbi algorithm is designed for this task. This algorithm finds the most likely sequence of hidden states given a se-quence of observed events. This is not sufficient for our prob-lem, as the observation sequence is not available at the point of prediction. For example, only given the first 2 records in our sample data set, we would like to predict the transition between the two stationary location states. Without pro-viding future observations, HMM is very unlikely to predict the transition. However, we adopt the Viterbi algorithm in our prediction process. By further incorporating time and movement information, we are able to accurately predict the transition behaviors.

Many high-end mobile devices are equipped with GPS re-ceivers. Unfortunately, power issues typically restrict the use of the phone X  X  GPS as a reliable sensor. For instance, continuous usage of the built-in GPS receiver will drain the phone X  X  battery in less than a few hours. In addition, GPS receivers may not work in indoor situations. Recent work has shown that one can use WiFi or GSM signal fingerprints for localization [4, 9, 12], which may overcome some of the energy problems with GPS. These methods have been shown to accurately estimate actual geo-coordinates. While we do not depend on accurate geo-coordinates, when available, our framework can use the information to improve the clustering quality.
 In [6], the authors propose an algorithm called Beacon-Print that uses WiFi and GSM radio fingerprints to auto-matically learn the places a user visits, and detects when the user returns to those places. This is similar to the algorithm used in this paper for movement detection. However, unlike our approach, Beacon-Print stops short in using the move-ment information to incrementally cluster the signal finger-prints into stationary states, and does not combine time and stationary states to predict the future movements of a user.
In this section, we provide the definitions to various terms used in this paper. Table 2 summarizes the major symbols.
Location-state: It corresponds to two possible states  X  a stationary state z or a transitional state s . Stationary states Z represent a set of repeatable geographical region that a user frequently  X  X tays X  in. Transitional states S represent transitional behaviors among the stationary states.
Time Tick ( t i ): We partition the time dimension into fixed intervals (e.g., 2 minutes). A time tick is the smallest unit of time. Therefore, we use time instance and time tick interchangeably in the reminder of the paper.
Sensor stream: A sensor stream ( X ) represents a con-tinuous set of measures from a sensor. At a time instance ( t ), a sensor measure can be viewed as a data point (  X  ) in a feature space ( F ). Based on different sampling schemes, a sensor stream can also be represented as a set of data points with time stamps (i.e., {  X  1 . . .  X  i . . . } ). In this paper, we focus on two types of sensor streams:  X  gsm and  X  W iF i representing GSM-and WiFi-related sensor streams respec-tively.
This paper focuses on the following two problems:
Obviously, these two problems are related. If we can solve the first problem accurately and efficiently, we can use the resulting models for the second problem. Also, we note that our prediction problem focuses on a future time window with unknown observations. Although our solution can be ap-plied to predicting location states for historical observations, forecasting future location states is a more challenging and practically useful problem to solve.
Figure 1 shows the overall framework of iLoc . As shown in the figure, the three major components of the framework are: annotation, learning, and prediction. While this paper-focuses on the learning and prediction components of the framework, we first briefly describe all three components.
The learning component consists of two sub-components: a movement detector and an incremental state learner. The movement detector determines movement (i.e., stationary, moving) and attaches movement information to the input data stream. Detection happens by comparing a set of cur-rent data points with previous observations, and then deter-mining if significant changes have occurred. The incremental state learner maintains the stationary and transitional mod-els by utilizing the outputs from movement detector. Sec-tion 5 provides further detail on these two sub-components.
The prediction component uses the output from the move-ment detector, along with the the location-state models, to predict a user X  X  future location states. The problem for-malization and detailed implementations are presented in Section 6.

The annotation component connects the user X  X  tagged states to the learned states in the learning component X  X  station-ary model, providing additional semantic information to the generated output states (e.g., X  X t home X  X r X  X t work X ). While not the focus of this paper, we briefly describe how the tag-ging information can be effectively obtained. Tags can be collected passively or actively, or even a combination of the two methods. In passive collection, a server can listen to the content pushed from a mobile device (e.g., micro-blogging), and based on this content, the location tagging information can be derived. In active collection, the user is prompted to provide the needed information, such as prompting the user to confirm different location tags when the system be-lieves the user is in a stationary state. As we will show in Section 7, our learner can quickly and accurately derive the frequent location states. The system needs to ask only a very few questions to the user, which makes the active approach feasible and less annoying to the user.
In this section, we discuss the implementation of the iLoc framework X  X  learning component.
A movement detector is essentially a classifier C that has two output classes (i.e., move , stay ). Given a sensor input  X  at a time instance t , it attaches a movement decision (i.e., C (  X  t )) to  X  t .

In general, building such a classifier requires a set of la-beled training examples that reflect a user X  X  movement be-havior. One way to obtain such a training set is to regu-larly prompt the user to classify his movements, and then attach sensor inputs to these movement tags. A classifier can then be trained to classify the movements. However, in practice it is difficult to optimize the rate at which a user is prompted. If prompted frequently, the user may be-come annoyed and stop providing movement information. Prompting infrequently may not provide enough informa-tion to cover a user X  X  day to day life.

For the purpose of discovering location states, we found that signal fingerprints work well. Instead of using them for the purpose of localization [4,9,12], we use them to de-tect movements. The basic idea is that if a user is station-ary, the corresponding signal fingerprints should remain rel-atively similar to each other. To further develop this idea, we first partition the time into a set of fixed length time intervals w (e.g., 10 minutes). We maintain the two most recent time windows (a sliding window) for an input stream  X . If a similarity function M X  [0 , 1] is defined on these two sets, the classifier can then be built by imposing a threshold value  X  on the outcome of the similarity function M . If the similarity value is bigger than the threshold, we classify the current input  X  t as not moving , and moving otherwise. The overall classifier C (  X  t ) can then be defined as:
Since we focus on the location-states, the sensor inputs should correspond to some fixed location regions. GSM and WiFi data streams are good candidates for this purpose. For GSM and WiFi, we can define the similarity function using cosine similarity, commonly used by many informa-tion retrieval systems [11]. For the two time windows, we group the observations into 2 vectors ( v 1 , v 2). The cosine
In Section 7, we demonstrate that this simple approach successfully captures a large amount of a user X  X  daily move-ments, and hence provides a reliable input to the subsequent mining components.
The goal of the stationary state learner is to efficiently and accurately determine the stationary states, which can then be used for the purpose of modeling state transitional behavior. Figure 2 outlines the algorithm. (Note that, in this section we focus on discovering and maintaining the stationary states Z = { z 1 . . . z k } , and postpone discussion of the transitional behavior until the following section.) The algorithm begins by initializing the stationary states Z = {} . At a time instance i , the movement detector decides if the stream input  X  i belongs to a stationary state. If so, the learner checks if the input belongs to an existing state or is a new state. If it is found in state z i , it updates the cluster representation; otherwise, a new cluster z new is added to the stationary states.

The key component to this algorithm is the classification process at line 6. Essentially, it computes the posterior dis-tribution of P r ( z |  X  i ). Each input can be decomposed into a set of feature-value pairs, also called attribute-value pairs. For example, a WiFi signal input  X  can have a set of k WiFi access points or SSIDs {  X  1 . . .  X  k } , where each of the SSID can be treated as a binary feature, and the input represents a set of visible SSIDs (i.e.,  X  i = {  X  1 . . .  X  k } ). The posterior distribution can be estimated as:
The conditional independent assumption P r (  X  1 . . .  X  k Q i =1 P r ( a i | z ) is similar to the Naive Bayes (NB). It is a rea-sonable assumption since sensor features (e.g., WiFi SSIDs), very often, are independently installed. Given the posterior distribution of P r ( z |  X  i ), we can take the best class/state for the current input. We can then use an accuracy threshold  X  to establish the quality of the classification. If the best state has the posterior probability less than the threshold, we do not assign the state label, and a new cluster is created (line 11). The model parameters P r (  X  i | z ) and P r ( z ) can be easily updated at line 8.

The above approach works when each P r (  X  i | z ) 6 = 0, oth-erwise the likelihood probability for the input vector P r (  X  becomes 0. We handle this case in two different ways. In the first scenario, if a large portion (e.g., 70%) of the terms in a input vector are missing from a cluster z , the likelihood probability for that cluster becomes 0. Furthermore, if like-lihood probabilities for all the clusters are 0, a new cluster will be generated. In the second scenario, only a small por-tion of the terms in the input vector are missing given a cluster z , each of the missing term  X  i is assigned with a small likelihood probability 1 to the cluster. This way, the overall likelihood will remain positive.
In this paper, we simply assign 1 |  X  | , where |  X  | is the size of stream inputs observed so far.
 1: Z  X  X  X  , S  X  X  X  7: if z 6 = NULL then 8: z.update parameters (  X  i ) 10: else 11: Z .newCluster (  X  i ) 12: RETURN ( Z , S )
While mining stationary states, we also incrementally learn and update a transitional state model, as shown in Figure 2, line 4 and line 9. The goal of this model is to predict fu-ture transitional states given past observations. To achieve this goal, we must first determine the temporal representa-tion. In Section 5.3.1, we introduce a process for extracting time features. Based on these features, we propose an incre-mental learning algorithm in Section 5.3.2 that models the transitional behavior of a user.
A temporal feature has a fixed number of domain values that represent some semantic concepts of time. For exam-ple, a feature can have { morning, af ternoon, evening } as its domain values. Table 3 lists an example set of tempo-ral features that are used in this paper. Notice that Each temporal feature has a unique ID, and each value of a fea-ture also has a unique ID. In this paper, we use very general features, but domain specific features can be easily added.
At a given time instance t , it is possible to have multi-ple matching temporal feature-value pairs. In this case, we want to select only those that are discriminative or impor-tant. We use a precedence graph to encode the importance relationship. The graph defines a partial-order for all the temporal feature-value pairs. The extraction process then works as follows: map the time instance to all possible tem-poral feature-value pairs, then use the precedence graph to eliminate those values that are dominated.

The following example illustrates the process of extracting time features. Suppose the time instance  X 8 A.M. on Jan. 9 th  X  is a normal workday morning. This instance matches two feature-value pairs:  X  X 1:Morning X  X nd X  X 4:Non-vacation X . According to our precedence graph (Figure 3), X  X 1:Morning X  is more discriminative than  X  X 4:Non-vacation X , so it is used as the the final time feature. Table 4 shows additional ex-amples of this process.

In the end of the extraction process, only the set of im-portant feature-value pairs {  X  1 . . .  X  k } are left to represent the time instance t . In our experiments, we empirically eval-uate different combinations of time features, and show that even with a few simple features, we can achieve very good prediction accuracy.
There are two types of transitions: self-transition and transition between two different stationary states. In a tran-sitional model, the state space S consists of all the possible pairs of stationary states (i.e., S = { ( z i , z j ) } ). Figure 4 out-lines the algorithm for updating the transitional states. The algorithm takes inputs of the current transitional model ( S ) and a pair of stationary states: the previous stationary state, z from , and the current stationary state, z current . The algo-rithm starts by evaluating the significance of the current sta-tionary state z current . The probability P r ( z current known as support [2]. If the support of the stationary state is bigger than a predefined minimum support, it is regarded as significant. (We evaluate different minimum support set-tings in Section 7.) If the stationary state is significant, and if the subject is moving at the point of recording, we do not know the destination yet. We cache the information of the from-state z from and time information at line 5. Once the subject reaches a stationary state, if the movement cache z from is not empty, we form a moving transition at line 8 (i.e., involving two stationary states). Otherwise, we update the self-transition (line 11) since we know it must transit from the same state. Given the time and transitional in-formation, to reliably update the parameters P r ( s i +1 line 12, we use the temporal features extractor F and past observations:
P r ( s i +1 | s i ) = P r ( s | s i ,  X , F ( t i )) = P r ( s | s
In this equation,  X  represents the time duration of re-maining in transitional state s i . It is updated at line 3. A set of k temporal feature-value pairs {  X  1 . . .  X  k } is obtained as illustrated in the previous section. Since the temporal feature-value pairs cannot dominate each other, they can be assumed to be conditional independent. We thus simplify the computation as:
We now need to know the distribution of P r ( s | s i ,  X ,  X  We first estimate the probability P r ( s = s i | s i ,  X  known as survival probability , which is the probability the subject stays in the same transitional state s i at next time tick. Given a duration value from  X  , we use a nor-mal distribution to model it: P r ( s = s i | s i ,  X ,  X  j
To further simplify the computation, we assume the uni-form conditional prior P r (  X  | s i ,  X  ) for the temporal features. Table 4: Example: Temporal feature extraction 1:  X   X   X  + t 4: if movement = move then 6: else 10: else 12: S .update parameters ( s,  X , t ) 13:  X   X  0 R  X  X  X  N (  X ,  X  ) dx from historical data, as illustrated in the following example. Assume the subject is currently in a stationary state z at time t i . From the past data, for a time feature value of  X  (e.g.,  X  X 1:Morning X ), we know that the subject stay in z for 20 minutes on average (i.e.,  X  = 20) with standard deviation of 5 minutes (i.e.,  X  = 5). If the subject has already stayed in z for 25 minutes, the probability of remaining in z reduces to 16%, which also means that the probability of transiting to other states is 84%.

Therefore, using the survival probability, we also know the total probability of going to other transitional states P r ( s 6 = s i | s i ,  X ,  X  j ). Depending on the meaning of s probability of transiting to other transitional states will be computed differently. In the first situation, s i represents moving from one stationary state z 1 to another station-ary state z 2. Since the destination is specified, P r ( s 6 = s | s i ,  X ,  X  j ) represents the probability of reaching z 2 (i.e., s = ( z 2 , z 2)). In the second situation, if s i represents staying tal probability mass of transiting to other transitional states: P transit to s k , the duration at s i becomes irrelevant, and hence P r ( s = s k | s i ,  X ,  X  j ) equals to P r ( s = s can be modeled by a multinomial distribution. The parame-ters of the distribution can be maintained on-line when new observations arrived.
Given a time window that consists of x number of time ticks. At a time tick t , the prediction task can be formalized observations s 0 . . . s t and future observations s t +1 rectly estimating this probability distribution is not feasible since the space for storing the joint distribution is large. In this paper, we make the first-order Markov chain assump-tion, such that the probability of observing state s at time tick t is only conditional dependent on the state at t  X  1.
When the number of the observations for s i is small, we use uniform distribution instead. The prediction task can then be simplified as: The first-order Markov assumption is reasonable since, in practice, our next location is highly associated with the cur-rent location, but less so for the previous locations. Since in Section 5.3.2, we have already derived an efficient algo-rithm for computing P r ( s i +1 | s i ), the prediction task be-comes feasible. We separate the prediction task into two sub-problems. The first problem is to predict the most prob-able transitional state s at current time t . The second prob-lem is to predict the most probable state sequence for the time window x . If we can solve the first problem, we can use its solution for the second problem.

For the first problem, there are two possible scenarios. In the first scenario, at the point of prediction, the movement detector does not detect any movements, so the stationary learner will predict one of the stationary states z i . There-fore, the transitional state s t represents a self-transition s t = ( z i , z i ). In the second scenario, the movement de-tector reports the subject is on the move; at query time, we will not know s t , we only know the last self-transitional state. In this case, the prediction starts from the last self-transitional state. The time interval from the last station-ary state is discretized into a set of time ticks (e.g., every 10 minutes). Since we follow the first-order Markov assump-tion, we can apply the well-known Viterbi algorithm [10] to compute the most probable transitional state at query time t . The Viterbi algorithm is a dynamic programming algo-rithm for computing Viterbi path, which corresponds to a state sequence with maximum probability. Let us assume at query time t there are m time ticks since the last sta-tionary state, and the best state at time t has probability: P = max( P r ( s m )). We can then define the states at each state s 1, we set P r ( s 1 = z 1 ) = 1 and P r ( s 1 6 = z it starts from a known stationary state z 1 .

For the second problem, instead of returning the most probable transitional state, we return the Viterbi path. Given the query end time t , we divide the time from the last known stationary state into m time ticks, and output the state se-quence with the maximum probability. In Figure 5, we il-lustrate a temporal range query. In this example, we have four time ticks. Each node in this graph represents a tran-sitional state s . If s is a self-transitional state, we repre-sent its stationary state only as z ; otherwise, s is a pair of stationary states. In each node, we also keep the previous best node/state and the probability of reaching the current node. Therefore, at the end of the query time tick, we have the transitional state ( z 3  X  z 2). If we backtrack using the previous best node, we can obtain the best sequence to be ject will transit to the stationary state z 3, and then start transiting to the stationary state z 2.
In this section, we experimentally evaluate the effective-ness and efficiency of the proposed algorithms. In Section 7.1, we focus on the data collection and evaluation techniques. The prototype of our system has been implemented using Python. The run time reported in this paper are measured from a P4-2 GHz PC with Linux OS. For comparisons, the LDA and HMM models are generated utilizing the various statistical tools from Matlab.
 Figure 5: An example of Temporal Range Query
In this paper, we focus on GSM and WiFi sensor data, because these two types of data have clear location sense and are widely available. To evaluate the effectiveness of our approach, we have been continuously monitoring 3 users for a period of 66 days (12/25/2008 to 02/28/2009).

The data collection client is implemented using Python for S60, and runs on Nokia S60 series mobile phones. The client periodically uploads the data to a server. The overall service is also known as Nokia Simple Context service [1]. Figure 6 shows a screen capture of the client running on a Nokia N95 mobile phone. The client is highly customizable, with the capabilities of monitoring multiple sensors (GSM, WiFi, GPS, Bluetooth, and etc.) at the same time. One interesting feature of the client is an adaptive GPS, which uses the same movement detection algorithm described in this paper to control the GPS sensing. It can smartly turn on the GPS when the device is on the move, and switch it off otherwise. Although we only use the GPS data to geo-locate the WiFi and GSM signal sources, it is possible to further utilize the GPS data in the state detection algorithms, but we leave this for future work.

Figure 7 shows the server side interface of the last a few records of a user. Once the data is stored on the server, various APIs can be used to retrieve the data. Note that, although our models are built on a PC, the code can also be directly run on the mobile client in a real-time mode; how-ever, we use a standard PC to benchmark the performance against other alternatives.

During the measurement time period, thousands of dis-tinct WiFi access points and GSM cell IDs have been ob-served. Based on one user X  X  data, if we geo-locate some of the GSM and WiFi signal sources (i.e., Figure 8 and Fig-ure 9), we notice that they cover a very large spatial region. We also notice that it is not sufficient to use only individ-ual signal sources (e.g., a GSM cell ID) to represent a sta-tionary state, because many of these signal sources do not correspond to the stationary states (e.g., discovered during commutes), and many of them represents the same station-ary states (e.g., multiple WiFi access points in one scan).
Table 5 summarizes the data sets recorded over a period of two months. The table shows the total number of records and the number of distinct signal sources for each user. No-tice that the number of records for WiFi is less than GSM, this is because a WiFi scan is more power consuming than a GSM scan, and hence WiFi scans are done at slower rate than GSM scans. For our users, WiFi scanning rate is set at Figure 6: Client Figure 7: Server Figure 8: Cells Figure 9: SSIDs 2 to 5 minutes, and GSM scanning rate is set at 30 seconds to 2 minutes. In the reminder of this section, we focus on User1  X  X  data to understand how well each component works. We use all users X  data to summarize the overall performance.
In table 6, we show the default parameter settings used by iLoc . For different components, the parameters have differ-ent sensitivity. For instance, the threshold settings in move-ment detection and stationary state learner are less sensi-tive than the parameters used by transitional state learner. Therefore, in the later part of section, we evaluate perfor-mance sensitivity to the parameters. Figure 10: Movement ra-tio (days) To show the effectiveness of the movement detector, in Figure 10, we show the daily movement ratio for User1 . We can see that movements are not limited only to the week-ends, but week days as well. After consulting the user, we confirm the movement detector can correctly capture large movements especially for commuting to different locations. In Table 7, we compute the average movement ratio per day for all users. We notice that the WiFi input stream can capture more movements than the GSM stream. This is because that a WiFi access point normally has smaller spa-tial coverage than a GSM cell, and hence more sensitive to the movements. However, the difference is not very large, which indicates GSM cells can also be used for movement detection.
In Figure 11, we show the total number of the station-ary states discovered by User1 in different days. It clearly demonstrates that our stationary state learner can incre-mentally discover new stationary states as data arrives. We also notice that the number of WiFi states are larger than GSM states. This is also expected since many places nowa-days have free WiFi access, but many of these access points might be under the same GSM cell. In Table 8, we summa-rize the total number of the stationary states discovered by all 3 users. Figure 12: CDF of top states (GSM and WiFi)
The next question is that how these stationary states dis-tributed. In Figure 12, we shows the cumulative probabil-ity of top-1 to top-k stationary states for User1 in differ-ent days. Both GSM and WiFi has a big stationary state that consistently has more than 50% of the probability mass. After going through the state manually, we can clearly con-clude it corresponds to the Home state. When we take top-5 for WiFi and top-6 for GSM, the cumulative probability is consistently more than 95% for each day. These states cor-respond clearly to Work , and other frequently visited places. Table 9: Num. clusters est. for HMM and LDA
Since HMM and LDA learners are not incremental learn-ers, to compare with them, we give the full advantage to Figure 13: Model build time (GSM) these learners, such that we train HMM and LDA with the full set of data. Another difficult question is to decide the number of clusters for HMM and LDA. There are different heuristics proposed in the literature. In this paper, we can provide very good estimations by utilizing the results from iLoc . The number of clusters for HMM and LDA is based on the top stationary states that cover 95% of the station-ary time of a user. The intuition is that, if HMM and LDA can outperform iLoc , it should discover a similar set of sta-tionary states. We go through the process for each users, in Table 9, we show the number of clusters for different users. Lastly, to apply HMM and LDA effectively, we also need to determine the number of iterations. For HMM, we set the maximum number of iterations to be 100. For LDA, we use Gibbs sampler approach to training the LDA model [5]. Gibbs sampling approach has been shown to be more ef-ficient than variational EM [3] and producing comparable results. The number of the iterations for the Gibbs sampler is set to be 500.

Figure 13 and Figure 14 show the model building time for each user. Even with 100 iterations, HMM takes the longest time to train. We also show that our incremental leaner is about 2 to 10 times faster than LDA with Gibbs sampling. The performance gain is mainly due to iLoc is an one-pass algorithm, and others require multiple iterations.
Next, we want to examine the quality of clusters from dif-ferent learners. Since for User1 we have decided to use top-5 clusters (WiFi stream) for HMM and LDA, in Table 10, we show the top-3 WiFi access points from each cluster algo-rithm. To the user, these access points can be easily asso-ciated with some semantic meaning. In Table 11, we ask the user to look at the top-10 access points and tag these cluster. For iLoc , the user is able to identify several interest-ing places, including home, work and a friend X  X  home. Also, iLoc correctly clusters the WiFi access points near his usual lunch and Gym locations. Table 11 also shows the prior probabilities of each cluster, which represents the degree of importance of each cluster. After consulting with the user, he agrees with the assignment. However, HMM and LDA cannot clearly separate these states. For example, Home appears in a few clusters. If use these clusters as station-ary states, it will severely affect the transitional model, and lead to poor prediction performance as we will show in the next section. We also go through the same exercise for other users, similar clustering behaviors are observed.
To fully test out the transitional model and the prediction framework, we form a continuous set of online prediction queries. In this online setting, we sequentially re-play the inputs from a data stream. At each sampling point, we apply Figure 15: Accuracy Qt1 by days for User1 (GSM) Figure 17: Accuracy Qt2 by days for User1 (GSM) the movement detection algorithm, and two types of queries are formed.
 The first query type is called non-moving query (i.e, Qt1 ), such that if a user is not moving, we form a time range query of 60 minutes. Our predictor, will predict at every 10 minutes intervals until the user starts to move again or the 60 minutes prediction window elapses. The accuracy will be computed at each time tick by comparing the predicted transitional state s against the stationary state z 0 , such that it is correct if s represents the self-transition s = ( z The second query type is called moving query (i.e., Qt2 ), such that if the user is on the move, starting from the most recent stationary state z 0 , we form a time range until the user reaches another stationary state z 1 . During this time range, we predict at every 10 minutes interval. The ac-curacy will be measured at each time tick by comparing the predicted transitional state s against the correct transition between two stationary states z 0  X  z 1 (i.e., if s = ( z Table 12 shows the query load for both types of queries.
In fact, these two query types cover many use cases. A non-moving query essentially asks how long a user will be staying in the same stationary state, and a moving query asks if a user is about to transit and to which stationary state. If iLoc can consistently provide accurate predication for both queries, it can be utilized for many real-life appli-cations (e.g., Ad-serving).

Although our models are fully incremental models, we do not apply the prediction tests on the initial days (i.e., from 12/25/2008 to 12/31/2008). We only use these a few days to bootstrap the models. This way, we start with a reasonable prediction model. The predictions are performed continu-ously on exactly two months of data (i.e., from 01/01/2009 to 2/28/2009).

For the predictions, if a predictor refuses to make a pre-diction, we consider the predictor does not have sufficient
Table 13: HMM State Transition Matrix (WiFi) information to make the prediction, and it is not part of the query load. This is reasonable since the goal is to have very accurate predictions rather than very broad ones. To gain some insights to the prediction process, we first take a detailed look at daily prediction for User1 . If there X  X  no predictions made for a day, we treat it as correct choice (i.e., 100% correct for the day).

In Figure 15 and Figure 16, we show the average predic-tion accuracy on each day for User1 with the query type Qt1 . We noticed that predictions based on GSM stream is not as good as WiFi stream. The main reason is that GSM cells cover larger spatial regions. For Qt1 , it mainly tests the self-transitions for different stationary states. The sta-tionary states from GSM stream can sometimes mix-up real stationary states if they are nearby. As the result, the self-transition model for GSM is not as good as the one from WiFi. However, even in this situation, for GSM, iLoc is still able to achieve 62% overall accuracy. For WiFi stream, it performs very well, and achieves 73% overall accuracy.
Similarly, in Figure 17 and Figure 18, we show the av-erage prediction accuracy on each day for User1 for query type Qt2 . We observe that for some days, the accuracy is not so good (i.e., close to 0). After consulting with the users, it turns out that these days mostly are weekends and vaca-tions. Predicting these days are challenging due to irregular movements. However, being able to differentiate these days from normal week days can improve the prediction accuracy. Another observation is that GSM predictor is again worse than WiFi predictor. Therefore, it is important that the stationary learner can discover the stationary states accu-rately. Overall, iLoc predictor based on WiFi stream is able to achieve 84% prediction accuracy, and GSM stream gets 25%. To clearly show the impact of the query time window, we only plot the query time windows at 10, 30, and 60 min-utes (i.e., tick1, tick3, and tick6). Since the lines are largely overlapped for both types of queries, it means both predic-tors (GSM and WiFi) can make reasonable predictions for the time range up to an hour. This is sufficient to many ap-plication scenarios, for which predicting a user X  X  behavior for the next hour is critical for the recommendation purposes.
After having some understandings on how prediction works for one user, in Table 14, we show the overall accuracy mea-sures for all users under different approaches, and different parameter settings. The first row shows the accuracies of iLoc with the default parameters (i.e., Table 6). If a method has better performance for a query setting, the result is high-lighted. This way, we can easily compare the performance of different approaches.

Table 14 has three parts. In the first part, we compare the performance of iLoc against HMM and LDA. Both HMM and LDA are not suitable for our prediction task. LDA only provides clustering information, which can be used as stationary states. HMM requires observations to infer the hidden states, whereas for us, at the query time instance, we do not have future observations yet, but we would like to in-fer the state sequence. Although HMM provides transitional probability distribution for different states, these transitions are mostly for self-transitions. If we would run Viterbi algo-rithm, it will always end up with the same state. To illus-trate this, in Table 13, we show the transition matrix from a HMM model. From the matrix itself, we can observe some transitions other than self-transition. For instance, state1 is able to transit to state3 with 63% of probability. However, given the semantic meaning of the states shown in Table 11, we notice that they are essentially the same state. Similar conclusions can be drawn for other transitions. Therefore, to have a fair comparison, we only use the cluster/stationary states from LDA and HMM, and plug them into our transi-tional model and prediction framework. As we can observe from the results, especially for the query type Qt2 , instead of unable to predict transitional behavior, we have some predictions for HMM and LDA. However, the accuracy is still far behind the iLoc with default parameters (i.e., iLoc -default in the table). This can also be observed from the last column of the table, which shows the average accuracy for the corresponding method. We would like to note that since the full set of data are used to train HMM and LDA, any incremental learning methods for HMM and LDA cannot outperform the results shown in this paper. Furthermore, being able to easily plug-in different clustering methods, we have demonstrated the flexibility and openness of our overall framework.

The second part of the Table 14 shows the accuracy of iLoc when we vary the min-support parameter. We have tested 4 settings (0%, 1% default, 2% and 5%), which correspond to iLoc -0, iLoc -default, iLoc -2, and iLoc -5. The higher the min-support, less number of stationary states are considered in the transitional model, and generally increases the predic-tion accuracy with the price of not being able to predict some infrequent movements. To further study the tradeoffs, in Figure 19 and Figure 20, we show the number of queries for different users with different data streams. We notice that as min-support increases, the number of the queries that can be answered monotonically decreases. At min-support 0%, iLoc can answer maximal amount of queries both for Qt1 and Qt2 , but with the worst accuracy. For Qt1 , with min-support at 1%, iLoc is still able to answer a large por-tion of queries with good gain in accuracy. For Qt2 , at 1% min-support, the number of queries dropped can be large for some settings. However, the accuracy gain is dramatic. As we keep increasing the min-support (i.e., iLoc -2, and iLoc -5), the number of queries dropped also increases, but the accuracy gain is only moderate (6% to 9% overall improve-ments). Therefore, to reach the right balance, we choose to use 1% min-support as our default.

The third part of the Table 14 shows the effect of the temporal features. The iLoc -none uses no temporal fea-tures, it performs badly for Qt2 queries. This is expected since it does not differentiate movements from time dimen-sion. When we add the morning, afternoon and night feature (i.e., iLoc-man), the performance improves drastically, and further enhanced with the full feature set (i.e, iLoc -default).
Lastly, we would like to demonstrate the efficiency of the prediction algorithm. In Figure 21, we show the average speed for executing a prediction query. Most of the queries complete within 5 millisecond. With this speed, we are very confident that iLoc can scale nicely as a web service.
In this paper we presented our approach for learning and refining the location state models. The iLoc framework is not only able to produce up-to-date location models, but is also able to predict future location states. We have con-ducted an extensive set of experiments to demonstrate the effectiveness and the efficiency of our solution.
There are several future research directions that we would like to pursue. Firstly, we would like to incorporate more sensor inputs to our framework. For instance, we are con-sidering to use accelerometer sensor inputs to differentiate finer grain movements. Secondly, we would like to incorpo-rate signal strength to help us identify smaller stationary states. Last but not least, we would like to conduct larger trials to test out our framework.
