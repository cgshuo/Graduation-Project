 Most of the existing active learning algorithms assume all the category labels as independent or consider them in a  X  X lat X  X tructure. However, in reality, there are many applica-tions in which the set of possible labels are often organized in a hierarchical structure. In this paper, we consider the prob-lem of active learning when the categories are represented as a tree. Our goal is to exploit the structure information of the label tree in active learning to select the most informa-tive samples to be labeled. We propose an algorithm that estimates the semantic space, embedding the category hier-archy. In this space, each category label is represented as a prototype and the uncertainty is measured using a variance-based fashion. We also demonstrate notable performance improvement with the proposed approach on synthetic and real datasets.
 H.2.8 [ Information Systems ]: Database Application X  data mining Algorithms, Experimentation, Theory Active Learning, Hierarchical Classification, Label Tree Em-bedding
Obtaining labels is an expensive or time-consuming pro-cess, especially for large scale multi-class classification prob-lems. Active learning is proposed to make the learning task more efficient [12], by intelligently choosing specific unla-beled instances to be labeled by a user/oracle, in terms of labeling cost. With an active learning method, generally new instances are selected to maximize the model uncer-tainty (usually measured by entropy). In many multi-class Fi gure 1: An illustrative example of the limitation of entropy as value of information measure in hierarchical classification problem. classification problems, such as document and web catego-rization, the set of possible labels are often organized in a hierarchical structure, i.e., a label tree. However, most of existing active learning algorithms consider all the category labels as independent [13, 2]. There are a few approaches for modeling the label relationship with a  X  X lat X  structure [7, 11], which aim to utilize the relationship among the classes to help in selecting the informative instances. It is shown that exploiting the relation among the class labels in the  X  X lat X  structure can boost the performance of active learn-ing. However, it is still insufficient to measure the value of the informativeness of the samples by ignoring the category hierarchy. For example, in Fig.1, the labels are organized in a tree structure and two unlabeled instances have dif-ferent posterior category distribution estimation. Although both instances have the same value of uncertainty if only measured by entropy without considering the hierarchy, the uncertainty of two instances should be different if the label tree structure is considered. This is because the label pre-diction for the first (left) instance cannot confidently classify the instance at the first level (both the posterior probabili-ties are 0.5), while the classifier is able to predict the label instances at the first level for the second (right) one (the posterior probabilities are 0.8 and 0.2 respectively).
To address this problem, in this paper, we propose a novel active learning algorithm that is able to exploit the hierarchi-cal structure of the categories to efficiently predict the most informative sample to query. We propose an embedding-based method and aim to re-discover a continuous semantic space underlying the hierarchical structure. All the labels, both in the leaf nodes and in the intermediate nodes, are embedded into the latent semantic space. Then the vari-ance is computed by considering each label as a point in the new space, and the uncertainty is measured by this variance. We show empirically that active learning employing the pro-posed uncertainty measure results in notable improvement upon the learning rate (and performance) of the baseline met hods. The remainder of the paper is organized as fol-lows. We describe the related work in Section 2. The em-bedding method and uncertainty measure are discussed in Sections 3. Experiments are reported in Section 4, and we conclude the paper in Section 5.
In the active learning scenario [9, 12], unlabeled data are available and at each iteration an algorithm is able to choose an instance for a user/oracle to label. The objective is of learning the appropriate concept with certain accuracy while incurring the lowest cost. In most of real-world learning problems, the pool-based active learning framework is used, in which there is a large pool U of unlabeled data sampled from a distribution P ( x ). In each step, the learner is allowed to query one unlabeled data x  X  U from the pool and get its label. The simplest and commonly used query strategy is uncertainty sampling, in which an active learner queries the instances about which it is least certain on how to la-bel them. Suppose that we are going to build a predictive model p y = P ( y | x )( y  X  X  1 , 2 , ..., m } ): given the data x from the input space X , we can predict the conditional probabil-ity for label y . The key to measure how useful labeling a sample x is to measure the value of the information gained by requesting the unknown label y for each unlabeled sam-ple x  X  U . A generally used measure of information is to measure the uncertainty of an unknown label by entropy of the posterior class distribution, which is defined as: where p  X  = P ( y =  X  | x ) and y ranges over all possible labels. There are some other measures proposed. Nader et al. find that using variance to measure uncertainty has very similar performance entropy in [10]. They show that selecting the unlabeled data that maximizes the entropy is equivalent to selecting the unlabeled data that maximizes the variance in some condition.

Most of the active learning approaches focus on binary classification. For multi-classification problem, each cate-gory is handled independently by a binary active learning algorithm in the traditional methods [13, 6]. These ap-proaches largely ignore the relationship among multiple la-bels. Most of recent few approaches exploit the relationship of the labels using a  X  X lat X  label structure. Jain et al. in [7] presented an uncertainty measure that generalizes margin-based uncertainty to the multi-class case for active learning. In [11], Guojun et al. proposed a two-dimensional frame-work which considers the sample dimension and the inherent label correlation. Although modeling the labeling relation-ship of the labels using a  X  X lat X  label structure can boost the performance of active learning, it is arguable that exploit-ing label tree structure in the active learning schemas can further push the performance.
In this section, we propose to measure the uncertainty with the variance of the category prediction, and derive a method to embed the hierarchical tree of labels into a la-tent sematic space. All the category labels, both in the leaf nodes and in the intermediate nodes as well as the training data, are first embedded into this space. Then the variance is computed by considering each label as a point in the space, and this variance is utilized as a measure of uncertainty.
We assume that our input consists of instances, repre-sented as a set of vectors x 1 , x 2 , ..., x N  X  X of dimension-ality d . In addition, these instances are accompanied by single topic labels y  X  X  1 , 2 , ..., m } that lie in the label tree T with m total topics. A label tree [1] is a tree T = ( V, E ) with nodes V and edges E . Each node v  X  V is associated with a set of class label l ( v )  X  { 1 , 2 , ..., m } . It is required that the set labels { 1 , 2 , ..., m } has a one-to-one mapping to the set of leaf nodes, and each non-root node X  X  label set should be a subset of its parent X  X  label set. A cost matrix C  X  R m  X  m is defined, where C  X , X   X  0 is the distance of the labels between class  X  and  X  , and C  X , X  = 0. The class dis-tance matrix could be obtained from side-information from a category tree. In this paper, the distance between two labels is defined as the length of the shortest path between corresponding two nodes in the tree.

Given a label tree T = ( V, E ), and the labeled data ( x like to embed both the labels and the data into a space, with the following criteria: 1), The embedded labels should be able to characterize the tree structure of these labels; and 2), the embedded labels should be representative of the data points of the accompanied category. Suppose the label y =  X  , let e  X  = [0 , ..., 1 , ..., 0] be the vector with a single 1 in the  X  -th position and the others 0. This vector can be embedded with a linear transformation P :
In the semantic space, each label y =  X  is represented as a prototype z  X  after the embedding. Better results in this sematic space can be expected when the prototypes of sim-ilar categories are closer than those of dissimilar categories. We consider the distance between two embedded labels  X  and  X  in the semantic space, which is defined as || z  X   X  z It should both reflect the distance defined by the tree struc-ture and the distance of the corresponding data with labels  X  and  X  . We use t  X , X  C  X , X  as an estimate of dissimilarity and aim to place the prototypes such that the distance || z  X  reflects the cost specified in t  X , X  C  X , X  . More formally, we set the distance error of the two in Eq.3 as: where C  X , X  , as we mentioned above, is the geographic dis-tance. t  X , X  is the distance between the center of the data in the class  X  and class  X  defined as: where N  X  is the number of data points within category  X  . Based on the embedding rule mentioned above, we want to minimize the error between the semantic distance || z  X   X  z and the estimate of dissimilarity t  X , X  C  X , X  for all the classes. Thus, the final objective can be written as Eq.5 and P can be obtained by minimizing the objective function: wh ere  X ,  X   X  X  1 , 2 , ..., m } . If the label distance C squared Euclidean distance, minimizing Eq.5 is actually the Multidimensional Scaling problem [5], and can be solved via eigenvector decomposition. Define matrix B = 1 2 HC 0 H , wit h C 0  X , X  = t  X , X  C  X , X  and centering matrix H defined as H = I  X  n  X  1 11 T , where 1 is a vector of n ones. Let the eigenvector composition of B = V X V T . The optimal P that minimizes Eq.5 can be obtained as P = V X  1 2 .
After embedding each class label  X  into a point z  X  in semantic space, and suppose we have a probability mass p  X  = p ( y =  X  | x ) at the point z  X  given instance x , the vari-ance can be defined as the trace of the covariance matrix of all the label points, which is similar to A-optimality for the information matrix [12]: where d y = m P an estimation point of view, the larger the variance of an instance is, the more difficult it is to estimate the true label of this instance. By encoding the structure of the labels into the location of the label points, the variance measure is able to exploit this information and achieves better sampling efficiency.

If all the labels are independent(not hierarchical struc-ture), they can be embedded uniformly into a high dimen-sional Euclidean space by setting the projection matrix P = I . Suppose y  X  { 1 , 2 , ..., m } , then we can embed them into a m -dimensional space. In particular, we can embed label y =  X  to e  X  , where e  X  is a vector with its  X  -th element 1 and the rest of element zero. Then the variance Eq.6 can be derived as: This shows that the proposed methods is actually a gener-ation of the basic variance-based measure. The proposed approach for uncertainty measure is depicted in Alg.1. Al gorithm 1 Variance-based Uncertainty Measure Embed-ding Label Tree 1: Input: The label tree T = ( V, E ), the labeled data 2: Output: uncertainty measure Var(y , x) given x 3: Take label tree T = ( G, E ), all the labels y  X  4: Obtain the projection matrix P by minimizing Eq. 5 5: Project each e  X  into z  X  using Eq. 2. 6: Compute each p  X  = p ( y =  X  | x ) at the point z  X  given 7: Compute the uncertainty using Eq.6. 8: Return Var(y , x).

I n each step, the uncertainty measure is computed for all the unlabeled training data, and the unlabeled training data with the largest uncertainty measure is given to the human labeler for labeling.
In order to demonstrate the effectiveness of our proposed approach, we evaluate it on both synthetic and real-world datasets. To make a prediction of the label given an input x , the hierarchical SVM [3, 4] is employed. The hierarchi-cal SVM algorithm traverses the tree from the root until it reaches a leaf node, and at each node, follows the child that has the largest classifier score. The label at the final leaf node is outputed as the classification label. In all the experi-ments, the proposed hierarchical active learning is compared with two baseline methods: the entropy-based uncertainty sampling, and random sampling. In the paper, we report predication accuracy and tree-loss [3] with the approaches on different datasets.
First we use synthetic data to clearly illustrate our active learning approach. We use a simple 2-level label tree shown in Fig.2a. The four leaf classes are assumed to be drawn from four independent Gaussian distributions, respectively, and 1000 sample points are drawn from each of four Gaussian distributions. The mean of the Gaussian of the four labels:  X  X 1 X ,  X  X 2 X ,  X  X 1 X  and  X  X 2 X  is set to [0,0,0,0],[0,0,1,1,],[0.5,0.5,-1,-1], [0.5,0.5,-2,-2] respectively as shown in Fig.2b, and the standard derivation of each Gaussian is set to 0.5. We ran-domly divide the data in each leaf node into three parts: 200 for initial training, 400 samples for active learning and 400 for testing. In each step one sample is selected to be labeled, and we then tracked the classification accuracies after each step. We plot the test accuracies as well as the tree-loss as the various methods learn each additional sample selected in every active learning step, and the comparisons between different selection approaches are shown in Fig.3. Hierar-chical active learning maintains the best performance under both measures compared to the entropy-based method and random sampling.
We also performed experiments on the widely used bench-mark set called RCV1-v2/LYRL2004 [8]. Here, the docu-ments have been tokenized, stopped and stemmed to 47,236 unique tokens(features) and represented as L2-normalized log tf-idf vectors. The associated taxonomy of labels, which are the topics of the documents, has 101 nodes organized in a forest of 4 trees. We divided this dataset with 3000 data points into 3 subsets: 200 for initial training, 500 for train-ing and 2300 for testing. We repeat the sampling step and measure the average of the accuracy and tree-loss at each active learning step. In Fig.4, we plot the average accura-(a ) Accuracy comparison with 3 algorithms
Figure 3: Experimental results on the synthetic dataset (a ) Accuracy comparison with 3 algorithms cies and tree-loss on the three competing methods at each learning step. We found that for this dataset, the proposed approach achieves better performances when the number of the labeled data is small. When the amount of labeled data is large, all the three methods achieve similar performance. These shows that the hierarchical active learning does im-prove the learning rate and classification performance com-pared to the baseline methods.
We have proposed an embedding-based uncertainty mea-sure for hierarchical active learning. Both the label tree and the accompanied training data are embedded into a semantic space in which the uncertainty is computed. The proposed uncertainty measure is able to exploit the topology structure of the category labels to efficiently predict the most informa-tive sample to query. Experimental results on both synthetic and real-world datasets demonstrated that the proposed ac-tive learning method for hierarchical classification task can improve upon the learning rate and performance compared to the baseline methods. The results indicate that utiliz-ing the hierarchical structure of the category labels is very helpful to active learning. In future, we will consider some problems where the category labels are organized with more complicated structures. This work is supported in part by NSF award numbers CCF-0621443, OCI-0724599, CCF-0833131, CNS-0830927, IIS-0905205, OCI-0956311, CCF-0938000, CCF-1043085, CC F-1029166, and OCI-1144061, and in part by DOE grants DE-FG02-08ER25848, DE-SC0001283, DE-SC0005309, DE-SC0005340, and DE-SC0007456. The authors thank Dr. Weikeng Liao for helpful comments. [1] S. Bengio, J. Weston, and D. Grangier. Label [2] K. Brinker. On active learning in multi-label [3] L. Cai and T. Hofmann. Hierarchical document [4] S. Dumais and H. Chen. Hierarchical classification of [5] T. F.Cox and M. A. A. Cox. Multidimensional Scaling . [6] J. Huang, S. Ertekin, Y. Song, H. Zha, and C. L. [7] P. Jain and A. Kapoor. Active learning for large [8] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A [9] D. J. MacKay. Information-based objective functions [10] E. M. N. Ebrahimi. Measuring informativeness of data [11] G.-J. Qi, X.-S. Hua, Y. Rui, J. Tang, and H.-J. Zhang. [12] B. Settles. Active learning literature survey. Technical [13] R. Yan, J. Yang, and A. Hauptmann. Automatically
