 With the development of Web2.0, users are becoming more and more deeply involved in the Internet, not only as readers, but also as authors. This develop-ment has made the quantity of text corpora on the Internet increase rapidly. As a result, it becomes more and more challenging to organize corpora efficiently, through this, users can find what they need conveniently. Dimensionality reduc-tion is a reasonable way to model large amount of data and get short descriptions for texts which is useful for certain basic tasks such as classification or relevance judgments.
 texts. Among them, a series of Latent Dirichlet Allocation (LDA) based topic models initiated by Blei[1] have been developed. LDA uses topics as latent vari-ables for text description. It has been extended in several different ways and has achieved success in some applications. For example, supervised LDA [2] assumes that there is a label generated from each document X  X  topic distribution. Labeled-LDA [3], TagLDA[4] and Multi-Multinomial LDA (MM-LDA)[5] have been used to model multi-labels text. Labeled-LDA constrains the topic distribution by user X  X  labels as supervised information, while the tag set and the word set are assumed to be independently sampled from the document in TagLDA/MM-LDA. The Author-Topic Model (ATM)[6][7] models the interests and topics based re-lations of authors. The Author Interest Topic Model (AITM)[8] allows a number of possible latent variables to be associated with authors X  interests, where the model assumes that each author has only one interest for one document. readers) have the same word distributions for a same topic. That is to say, when a topic is found, the word distribution of the topic is same for all users, no matter who they are.
 is a big probability they will prefer to use different words. That is to say, for a same topic, there may be several different word distributions for different kind of people. For example, when two people talk about a mobile phone, one may first think about its capability of communication, while the other may first talk about its convenience, depending on their backgrounds and interests. When they talk about the convenience of the mobile phone, one may first use  X  X ortable X , another may first use  X  X arry-on X , because of different wording preferences they may have. It is the same when different people read a document on the same topic, they will be concerned with different words in the document or use different words to tag the documents, depending on their backgrounds and/or wording preferences. is not only dependent on the topic, but also dependent on users. We assume there is a latent user characteristics for different groups for people, which makes different groups of users have different word distribution even for a same topic. in one LDA based model. Due to existing user characteristics, the word distri-bution on a similar topic for different users will be different. A topic model that does not concern these differences can be thought of as an average model of a large collection of different users with different characteristics. By making use of user-specific differences of topics, we aim to not only achieve better topic mod-eling for documents, but also extracting more information of both writers and readers(taggers) of the documents .
 characteristics LDA (UC-LDA). The difference between our model and previous is that our model assumes that words of a document are not only rested with document X  X  topic distribution, which is the same as that in LDA, but also con-trolled by the user X  X  characteristics distribution. Experimental results shown that our model (UC-LDA) outperforms LDA, ATM and AITM significantly in text modeling task. In addition, it can discover some interesting results about user X  X  characteristics which cannot be given by previous ones. Also, we applied the idea of user-characteristics (UC) to TagLDA (UC-TagLDA), from the experimental results, we can find UC is not just specific to a certain topic model, it can be applied to a wide range of topic models.
 model works with the example of UC-LDA and UC-TagLDA. The experimental results are shown in Section 3. Section 4 draws some conclusions.
 2.1 Motivation Topic models like LDA only concern the generative process of the documents. For each document in the LDA model, the topic distribution  X  d is a multinomial distribution randomly sampled from a Dirichlet distribution, for each word in document d , the topic assignment z ( w ) di is chosen from this topic distribution for the i th word, and then a word w di is generated from a topic-specific multinomial distribution  X  z is a great probability that they will prefer to use different words. A topic model that does not concern these differences can be thought as an average model of a large group of different users with different characteristics. We now add the user X  X  characteristics to the generative process based on the following assump-tions: When a word is chosen for a topic by a user, it not only rests with the topic, but also rests with that user X  X  characteristics. Meanwhile, different types of characteristics generate different users.
 characteristics 2.2 Description of User-Characteristics LDA(UC-LDA) UC-LDA can be thought as a combination of the above assumptions and LDA. It gives a way to model users and documents at the same time.
 tation of UC-LDA in Figure 1(b). In UC-LDA, each word is not only influenced by its topic assignment, but also by characteristics of the user who is relative to this word. Our notation is summarized in Table 1.
 as follows: 1. Draw H multinomial  X  s from Dirichlet prior  X  ; 2. Draw K  X  H multinomial  X  zs from Dirichlet prior  X  to represent the tag 3. Draw a multinomial  X  from Dirichlet prior  X  ; 4. For each document d , draw a multinomial  X  d from a Dirichlet prior  X  ; 2.3 Inference As a topic model can not be exactly inferred, we use Gibbs sampling to get an approximate inference of our model. For each iteration, we need to sample the topic of each word, and also need to sample the characteristics of the user who gives that tag.
 And, n ( z,s,w ) is the number of tokens of word w is assigned to topic z with document d is assigned to topic z , and n ( s,u ) is the number of occurrence of user u is assigned to characteristic s . 2.4 UC-TagLDA User characteristics can also be combined with other topic models. We describe the combination of User characteristics with TagLDA in this section.
 tagged system, different user may use different tags to tag a same content since they will concern different aspects of the content or they may use different words to describe the same content. In this way, we add the users characteristics to the generative process based on the following assumptions: when a tag is chosen for a topic by a user, it not only rests with that topic, but also rests with the user X  X  characteristics. Meanwhile, different types of characteristics generate different users. process of both words and tags. For document d , the i th word w di is generated in the same way as in LDA, while topic assignment z ( t ) dj for the j th tag is chosen from the document X  X  topic distribution  X  d , and that tag t dj is also generated from a topic-specific multinomial distribution  X  z tag t dj is not only influenced by tag distribution  X  z user X  X  characteristics assignment s ( u ) dj which also influence the generation of user u for posterior inference.
 3.1 Dataset UC-LDA We use two dataset of papers for experiments about UC-LDA, the abstract from CiteseerX and the full paper from NIPS. The CiteseerX dataset (1.2GB xml file) is downloaded from CiteSeerX OAI collection and it has 456,353 abstracts from 650,478 authors. The NIPS dataset (35MB mat file) [9] has 2,484 papers from 2,865 authors.
 the testing data. For CiteseerX dataset, there are 407,812 training documents (598,745 authors) and 48,541 testing documents (60,1973 new authors). For NIPS dataset, there are 2,207 documents (2,290 authors) and 277 documents (575 new authors) left for testing.
 UC-TagLDA We use the data from del.icio.us provided by DAI Labor[10] for UC-TagLDA. Del.icio.us is a social bookmarking system in which users can tag each of their bookmarks freely. Each record of the data consists of three parts, including user, url and tags.
 data sparseness, we have removed the URLs that have been tagged by fewer than 20 users, and also removed those users who tagged fewer than 50 URLs. After preprocessing, 1121 users and 2476 URLs remained. We then crawled the web pages of these URLs. After preprocessing to remove the irrelevant content and the web page stop words , there were about 143 words left for each page. this dataset with 10-crossfold validations. 3.2 Perplexity UC-LDA In the comparison experiments for UC-LDA, we compute the per-plexity of words comparing among LDA, ATM, AITM and UC-LDA on 10, 20, 50, 100, 150, 200, 500 and 1000 topics. where 1000 topics only set for NIPS dataset. As CiteseerX dataset is much larger than NIPS dataset and the limita-tion of memory, it is hard to do experiments with exactly the same number of topics on both dataset. For AITM, we also set different interest number (1, 5, 10, 20, 50, 100, 150, 200), and choose the best result (lowest perplexity) on each topic number for comparative purposes.
 Table 2 and Table 3.
 150 topics. And our model (with 1 characteristics) and LDA has nearly the same perplexity on 200, 500, 1000 topics, better performance. As the increasing of number of topic, LDA gets its best performance, but the seperation of topics in LDA and UC-LDA are not same. The best perplexity of UC-LDA is got at 5 characteristics and 150 topics, while LDA does not get best perplexity at 5  X  150 = 750 topics. The best performance of UC-LDA improves about 9 . 9% comparing with best LDA.
 1 characteristic.
 mance of UC-LDA improves about 12 . 2% comparing with best LDA. UC-LDA brings bigger improvement of perplexity on bigger data.
 UC-TagLDA In UC-TagLDA, we have a latent variable for the user X  X  char-acteristics in addition to the latent variable for topics common in both of our model and TagLDA. For topics, we tested six different values. For user X  X  char-acteristics, we tested three different values. The experimental results are shown in Table 4 and Table 5.
 and 10 characteristics with different topics number. The best perplexity of UC-TagLDA improves about 1 . 4% comparing with best TagLDA. Perplexities on tags shown in Table 5 bring us to the same conclusion. Our model outperforms TagLDA and the best perplexity of our model improves about 18 . 8% comparing with best TagLDA.
 therefore compare our model to ATM.
 together and assume that all URL tags were co-created by all users who tagged the URL. The same parameters are set as the previous experiments.
 nificantly outperforms the ATM model on social tagged data.
 3.3 Word distributions over different characteristics The user X  X  characteristics can perform in different forms. Some characteristics may represent different aspects of a topic, and some characteristics may repre-sent different wording preferences. Table 6 and Table 7 respectively show the word/tag rank of UC-LDA(on Citeseerx dataset) and UC-TagLDA.
 characteristics. And we can find that although the top-10 words are nearly in the same set, they are in different order, which demonstrats different wording preferences. For example, in topic-7 (CiteSeerX), top 9 words are the same but in different position in charac-0 and charac-4.  X  X edical X  is the top one word in charac-0, and the second word in charac-4,  X  X atients X  is the second word in charac-0, and the 7th word in charac-4.  X  X eural X  is the second word in charac-0, and the 5th word in charac-4. Obviously, these different topics cause by user wording preference can not found by LDA models. And we can also find the different perspective of the same topic. For example, topic-4 (NIPS) is about physiological, where charac-9 is interested in the physiological property, while charac-15 concerns more technical details.
 concerned with the usage of blog, while the charac-5 users may be paying more attention to the search technology. For topic-47, charac-0 and charac-3 are both interested in the hardware product, because they have used almost the same most used words, but in different order when they talk about a same topic, demonstrating different wording preferences.
 3.4 Application on recommendation(UC-TagLDA only) Both UC-TagLDA and TagLDA can be used for tag recommendation. As [5] shows TagLDA has better performance compared to K-means on tag recommen-dation, we designed two groups of experiments to compare their recommendation performance.
 TagLDA, for UC-TagLDA, the number of user X  X  characteristics is also set to two different values, 5 and 30.
 pared them to the user X  X  tags.
 data, and the remaining was used as test data.
 data from DAI-Labor dataset.
 if the scale of training data will bring different effects. To evaluate the influences on the amount of training data, we randomly chose 1/2, 1/4 and 1/8 of the whole training data to train the model, and evaluated on the same set of testing data. The experimental data is shown in Table 9. In the experiment, the topic number was also set to be 50, and the evaluation is on the top 10 tags. page for each model, and calculated the average ranks of the user X  X  real tags in the models. The ranking results are shown in Table 10, and from the results, it is easily to see that our model significantly outperforms TagLDA. This paper proposes a new idea of topic model to address the problem of user characteristics. User X  X  relevant words are assumed to be not only gen-erated from latent topics as in a normal topic model, but also influenced by the user X  X  characteristics. Experimental results show that the model with user X  X  characteristics(UC-LDA &amp; UC-TagLDA) outperforms the previous models(LDA &amp; TagLDA) and other similar topic models(ATM &amp; AITM) on text modeling. characteristics. We have found two varieties of user characteristics from our model, one concerns different views of a topic, and the other is different wording preference. The work presented in this paper is supported by the National Science Foun-dation of china (No. 61273365) and National High Technology Research and Development Program of China (No. 2012AA011104).

