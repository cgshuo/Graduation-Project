 Graph classification concerns the learning of discriminative mod-els, from structured training data, to classify previously unseen graph samples into specific categories, where the main challenge is to explore structural information in the training data to build classi-fiers. One of the most common graph classification approaches is to use sub-graph features to convert graphs into instance-feature rep-resentations, so generic learning algorithms can be applied to derive learning models. Finding good sub-graph features is regarded as an important task for this type of learning approaches, despite that there is no comprehensive understanding on (1) how effective sub-graph features can be used for graph classification? (2) how many sub-graph features are sufficient for good classification results? (3) does the length of the sub-graph features play major roles for clas-sification? and (4) whether some random sub-graphs can be used for graph representation and classification?
Motivated by the above concerns, we carry out empirical studies on four real-world graph classification tasks, by using three types of sub-graph features, including frequent sub-graphs, frequent sub-graph selected by using information gain, and random sub-graphs, and by using two types of learning algorithms including Support Vector Machines and Nearest Neighbour. Our experiments show that (1) the discriminative power of sub-graphs varies by their sizes; (2) random sub-graphs have a reasonably good performance; (3) number of sub-graphs is important to ensure good performance; and (4) increasing number of sub-graphs reduces the difference be-tween classifiers built from different sub-graphs. Our studies pro-vide a practical guidance for designing effective sub-graph based graph classification methods.
 H.2.8 [ Database Applications ]: Data mining Algorithms, Experimentation, Performance Graph classification, sub-graph, random feature Figure 1: An example of sub-graph pattern representation. Sub-graphs g 1 , g 2 , and g 3 are used to convert graphs G 1 feature vectors.
The advancement of data acquisition and analysis technology has resulted in many applications involving complex structured data. Different from traditional data which are represented as feature vectors, the new structured data are often represented as graphs to denote the content of the data entries and their structure relation-ships. This has resulted in an increasing interest in graph classifi-cation, which tries to learn classification model from a number of labeled graphs to separate previously unseen graphs into different categories. Finding effective representations for graph data is re-garded as a major challenge for graph classification. One possible solution is using substructure patterns ( i.e. sub-graphs) as features to represent each graph as a feature vector. As a result, each graph in a training graph set can be converted into an instance in a generic training instance set. The graph classification can be solved by ap-plying generic learning algorithms to the converted training data. Fig. 1 shows an example of sub-graph pattern representation. Sub-graph based approaches are useful in many graph related tasks, in-cluding discriminating different groups of graphs, classifying and clustering graphs and building graph indices in vector spaces. An inherent advantage of embedding graphs into vector space is that it makes existing algorithmic tools developed for feature based object representations available for graph structured data.

In this paper, we focus on sub-graph mining based graph classifi-cation. Frequent Sub-graph Mining (FSM) has been an established sub-graph mining problem, mainly because sub-graph patterns are meaningful tokens to effectively map graphs into vector space for classification. Given a graph dataset, D = { G 0 ,G 1 ,...,G sume Sup g denotes the number of graphs (in D ) which include g as a sub-graph, the objective of FSM is to find all frequent sub-graphs whose number of occurrences is above a specified threshold min sup ( s.t. Sup g  X  min sup ) from the given graph dataset D . Given a set of frequent sub-graphs g 1 ,g 2 ,...,g d , a graph G be represented as a feature vector X = [ x 1 ,x 2 ,...,x d x = 1 if g i  X  G x ; otherwise, x i = 0 [7]. Existing research has Figure 2: The runtime of frequent sub-graph pattern mining with respect to the increasing number of edges of sub-graphs. demonstrated that such vectorization helps build efficient indices to support fast graph search [8]. In addition, this representation can also help preserve some basic structure information for graph data analysis.

Several reasons motivate the proposed empirical studies on fre-quent sub-graphs for graph classification. Firstly, the mining of the sub-graphs involves graph matching operation which is computa-tionally demanding. Indeed, just testing whether a graph is a sub-graph of another graph is an NP-complete problem [6]. In Fig. 2, we report the runtime of sub-graph mining with respect to an in-creasing number of edges of the sub-graphs for a given graph set. The results show that it is very time-consuming to match patterns in graph datasets, especially for sub-graphs with a large number of edges. Secondly, we are interested in finding discriminative power of sub-graphs. In other words, we intend to understand how much the classification accuracy can be affected by different properties of the selected sub-graphs, such as different number of nodes, differ-ent number of edges, and the size of the selected sub-graph set. To achieve the goal, a typical feature selection measure, information gain (IG) [3], is used to test the discriminative power of sub-graphs. Due to the inherent structure nature of sub-graphs, the IG scores of sub-graphs are neither monotonic nor anti-monotonic with respect to the size of the sub-graphs [7]. In addition, the internal structure correlation between sub-graphs can also impact on the classifica-tion result. For instance, sub-graphs with similar structures tend to have similar feature scores, whereas using sub-graphs with high correlations for classification should be avoided because it will in-troduce redundant features with high dependency which may dete-riorate the classification accuracy. A comprehensive understanding of the role of sub-graph features for classification can help select better sub-graphs to represent graph datasets. Thirdly, most ex-isting graph classification methods are computationally expensive, so we are wondering whether there is any less expensive way to achieve graph classification. More specifically, if we use some ran-dom sub-graphs to represent a graph datasets, how good the clas-sification accuracy will be, compared to the approaches which use sophisticated sub-graph mining algorithms?
In this paper, we present an empirical comparison of graph clas-sification based on different sets of sub-graph features, including frequent sub-graphs, random sub-graphs, and sub-graphs selected by using information gain measure. The comparisons are carried out by using different number of sub-graphs, and sub-graphs with different edges. Experiments are performed on four real-world graph datasets from three domains, by using two popular learning algorithms including support vector machines and nearest neigh-bour classifiers. The studies show that using random sub-graph fea-tures has a reasonably good performance compared to its expensive peers which use frequent sub-graphs. Experiments also show that sub-graphs with many edges are not necessarily good candidates for graph classification.

The remainder of the paper is organized as follows. Section 2 formulates the problem and defines some important notations. Ex-perimental studies are reported in Section 3 and we conclude the paper in Section 4.
In this section, we introduce several basic concepts related to graph representation, frequent sub-graph mining, and graph classi-fication.

Definition 1. (Graph): L V and L E are finite label sets which denote nodes and edges, respectively. A graph g = ( V,E, X , X  ) , where: V denotes a finite set of nodes. E  X  V  X  V is the set of edges.  X  : V  X  L V is the node labeling function, and  X  : E  X  L E is the edge labeling function.

The number of nodes of a graph g is denoted by | g | , while D represents the set of all graphs over the label alphabets L L
Definition 2. (Sub-graph): Let g 1 = ( V 1 ,E 1 , X  1 , X  1 ( V 2 ,E 2 , X  2 , X  2 ) be two graphs, Graph g 1 is a sub-graph of g noted by g 1  X  g 2 , if V 1  X  V 2 . E 1  X  E 2 .  X  1 ( v ) =  X  v  X  V 1 , and  X  1 ( e ) =  X  2 ( e ) for all e  X  E 1 .

Given a user specified minimum support threshold min sup and a graph database D , a frequent sub-graph is a sub-graph whose support is at least min sup ( i.e. Sup D  X  min sup ) and the frequent sub-graph mining problem is to find all frequent sub-graphs in D .
Definition 3. (Graph Isomophism): There are two graphs g 1 ( V 1 ,E 1 , X  1 , X  1 ) and g 2 = ( V 2 ,E 2 , X  2 , X  2 ) , a graph isomorphism is a bijective function f : V 1  X  V 2 satisfying
Two graphs are called isomorphic if there exists an graph iso-morphism between them.

Definition 4. (Sub-graph Isomorphism): Given two graphs g ( V 1 ,E 1 , X  1 , X  1 ) and g 2 = ( V 2 ,E 2 , X  2 , X  2 ) , an injective function f : V 1  X  V 2 from g 1 to g 2 is a sub-graph isomorphism if there exists a sub-graph g  X  g 2 such that f is a graph isomorphism be-tween g 1 and g .
 Definition 5. (Graph Embedding): Let D be a set of graphs. A graph embedding is a function  X  : D  X  R n mapping graphs to n -dimensional vectors, i.e. ,  X  ( g ) = ( x 1 ,x 2 ,...,x
After mapping graph data into the vector space, graph classifica-tion is almost the same as learning and classifying generic instances represented in the feature space.
In this section we first discuss the benchmark data and the exper-imental settings, and then report detailed experimental results and analysis.
We carry out our experimental studies on four real-world graph datasets, which are collected from three different domains: Chem-ical compound analysis, Protein structures, and Citation networks. Chemical Compound: The activity of chemical compound molecules can be predicted by their special 3-dimensional structures. The chemical compound analysis intends to use the structure informa-tion to analyze the activity/functionality of the chemical compounds. In our experiments, we use a series of binary-label graph datasets from the PubChem website 1 , which provides real-world datasets on the biological activities of small molecules, containing the bioas-say records for anti-cancer screen tests with different cancer cell lines collecting from National Cancer Institute (NCI). Each dataset belongs to a certain type of cancer screen with active or inactive response ( i.e. class labels) [7]. Because each NCI bioassay dataset contains very few active graphs, we use under-sampling to down-sample inactive graphs to form a relatively balanced dataset for performance evaluation. The number of vertices in most of those compounds ranges from 10 to 200. We use 2 graph datasets in the experiments and their detailed information is reported in Table 1. Protein Structure: Proteins are organic compounds made of amino acids sequences joined together by peptide bonds. A large number of proteins have been sequenced over years, and the structures of thousands of proteins have been resolved so far. The well known role of proteins in the cell is as enzymes which catalyze chemi-cal reactions [4]. The D&amp;D dataset we used contains 1178 protein structures which can be divided into two classes: 691 enzymes and 487 non-enzymes [1]. Each protein is represented by a graph, in which the nodes are amino acids and two nodes are connected by an edge if they are less than 6 Angstroms apart. These proteins, with an average size of 285 vertices and 716 edges, are larger and strongly connected than molecules from the NCI screening. DBLP Citation Network: The DBLP dataset contains bibliogra-phy on Computer Science 2 . Each record in DBLP is associated with a number of attributes such as paper ID, authors, years, ti-tle, abstract and reference ID [5]. We build a binary-label graph dataset by using papers published in a list of conferences (as shown in Table 2). The classification task is to predict whether a paper belongs to the field of DBDM (database and data mining) or CVPR (computer vision and pattern recognition), by using references and title of each paper. In our experiments, each paper in DBLP is represented as a graph, where each node denotes a Paper ID or a keyword and each edge denotes the citation relationship between http://pubchem.ncbi.nlm.nih.gov http://arnetminer.org/citation papers or keywords appeared in the paper title. More specifically, we denote that (1) each paper ID is a node; (2) if a paper A cites an-other paper B, there is an edge between A and B; (3) each keyword in the title is also a node; (4) each paper ID node is connected to the keyword nodes of the paper; and (5) for each paper, its keyword nodes are fully connected with each other. An example of DBLP graph data is shown in Figure 3.
 Figure 3: Graph representation for a paper (ID17890) in DBLP. The rectangles are paper ID nodes and diamonds are keyword nodes. The paper ID17890 cites (connects) paper ID17883 and ID18068, and ID17890 has keywords Patch , Motion , and Invariance in its title. Paper ID18068 has keyword Edge and Detection , and paper ID17883 X  X  title includes keywords Vision and Edge . For each paper, the keywords in the title are linked with each other.
To understand the relationship between sub-graph features and the classification accuracy, we carry out sub-graph feature selection by using different sizes of sub-graphs (with respect to the number of edges), which are from one to nine (sub-graphs with more than nine edges are much less frequent). In Table 3, we report the num-ber of sub-graph features with respect to different sizes ( i.e. number of edges) in the benchmark datasets. Meanwhile, we also vary the number of select sub-graph features ( i.e. the size of the  X  X elected feature set X ) to include 50, 1000, and 2000 sub-graphs respectively, in all experiments.
 Table 3: Number of sub-graphs with respect to different sizes ( i.e. number of edges) To select sub-graph features, we use frequency and Information Gain (IG) based feature selection criteria, respectively. For fre-quency based criterion, we simply select sub-graphs with the high-est frequency. For IG based approach, we calculate Information Gain (IG) between each sub-graph and the class label, and select the ones with the highest IG scores as sub-graph features. It is worth noting that IG is commonly used for sub-graph feature selec-tion and most significant patterns likely fall into the high-quantile of frequency [7].

In addition to frequency and Information Gain based sub-graph features, we also employ a random feature selection approach. In our experiments, we first collect all one-edge sub-graphs from the training set. After that, multiple-edge sub-graphs are generated by randomly combining one-edge sub-graphs. If a multiple-edge sub-graph appears more than once in training set, this sub-graph will be considered as a possible random sub-graph feature. This process is equivalent to random sub-graphs collecting from the whole struc-ture space. Because our experiments focus on sub-graph features with different edges, we generate a sub-graph with X edges by us-ing X one-edge sub-graphs selected in a random manner ( X = 1 , 2 ,  X  X  X  , 9 ). This process is much more efficient and more feasi-ble than generating all possible features from training set (which is computationally infeasible).

Among all four benchmark datasets, DBLP is a special graph dataset. The node space in DBLP is very sparse, because there are thousands of nodes with different labels (as shown in Table 2) (the nodes in DBLP represent paper ID and paper keywords). As a result, the most discriminative features in DBLP are one-node sub-graphs. So in our experiments, we also include sub-graphs contain-ing only one-node (0-edge features mean one-node sub-graphs).
In our experiments, classification accuracy is used to measure the performance of the algorithm. We use 10-fold cross-validation to evaluate and compare the algorithm performance. Each graph dataset is evenly partitioned into 10 parts. Only one part is used as test set and the other nine parts are used for sub-graph mining, sub-graph feature selection, and classifier generation. To reduce vari-ability, the validation was collected from the average of 10 times cross-validation. To train classifiers from graph data, we use Sup-port Vector Machines (Lib-SVM based MATLAB package 3 ) and Nearest Neighbor algorithm (NN) [2]. All experiments are con-ducted on machines with 4GB RAM and Intel Core T M i5 CPUs of 3.10 GHz.
In this section we report the graph classification accuracy with respect to four major factors: (1) the sub-graph feature sizes; (2) the size of the sub-graph feature set; (3) different learning algorithms; and (4) different benchmark datasets. In Figures 4 and 5, we report the algorithm performance with respect to different sizes of sub-graph features, by using Support Vector Machines (Figures 4) and Nearest Neighbour (Figure 5) learning algorithms. In each figure, each row corresponds to one benchmark dataset.

The results in Figures 4 and 5 suggest the following major find-ings.
 The discriminative power of sub-graphs varies by their sizes: The results show that the size of sub-graph features has significant impact on its discriminative power. For NCI data, features with 4 to 7 edges have good classification performance. For D&amp;D protein data, features with 1 to 3 edges are better than others. For DBLP data, features with one-node are more discriminative. The variance of the discriminative power of sub-graphs is mainly attributed to the domain characteristics of the graph data:  X  NCI graphs have very similar structures between each other. Ex-amples include benzene rings, which can be observed in many graphs. Each NCI dataset directly focuses on a special type of compounds (or structures). So most NCI data are very similar in structures.  X  Although D&amp;D graphs only have less than 44 node labels, these graphs have very complex structures, and the distribution of the data is more scattered than NCI data. Because there are up to thou-sands of enzymes found in the biosphere. The large structure dif-ferences result in less graphs sharing the same sub-graphs with rel-atively large size. As a result, the most discriminative features of D&amp;D graphs have 1 to 3 edges on average.  X  DBLP data is the most sparse graph datasets. There are thousands unique paper IDs and keywords in the DBLP (each paper ID and keyword represent one node). Because of the large number of node http://www.csie.ntu.edu.tw/ v cjlin/libsvm labels, it is hard to find shared sub-graphs with more than 4 edges from any two DBLP graphs. The most frequent and the most dis-criminative features we found are, in fact, one-node, and sub-graph features with more than 3-edge may only appear a few times. As a result, when using features with 3 or more edges to build classi-fiers, the classification accuracy is about 50% (which is equivalent to random predictions) (as shown in Figure 4 (j), (k), and (i)).
Overall, the experiments suggest that the actual discriminative power of sub-graph varies, depending on their sizes and the domain of the graph data. Sub-graphs with many edges are not necessary to be considered for graph classification.
 Random sub-graphs have a reasonably good performance: com-pared to sub-graphs discovered from expensive sub-graph mining and selection criteria, such as frequency based sub-graphs and in-formation gain based sub-graphs, the classification accuracy of ran-dom sub-graph is reasonably good. Our experiments show that none of the feature selection approaches can significantly outper-form all other methods (including random sub-graph) on all four benchmark graph datasets. Although the accuracy of the informa-tion gain based method is often higher than others, in most cases, the difference of accuracy between random feature selection and frequent feature selection with IG is less than 5%.

Overall, our experiments suggest that random sub-graphs are useful for graph classification. The classifiers built from random sub-graphs are not significantly inferior to other peers. Number of sub-graphs is important to ensure good performance: When comparing three figures in each row (the left, middle, and the right panel represent results corresponding to 50, 1000, and 2000 sub-graph features, respectively), it is clear that increasing the num-ber of sub-graph features often results in an improved accuracy. For example, when increasing the number of sub-graphs from 50, to 1000, and to 2000, the average classification accuracy of the clas-sifiers trained by using three types of sub-graph features (Random, Frquent, and IG) for 9-edge features on NCI485346 graph dataset will increase from 63%, 67% to 69%. Similar trends can also be observed from other benchmark datasets. For each specific selected sub-graph feature subset ( i.e. a feature set with 1000 sub-graphs), the accuracies will vary significantly, depending on the size of the sub-graphs in the set. A larger improvement can be observed for sub-graph feature subset containing more features. For example, when using 2000 sub-graphs, the difference between the maximum and the minimum classification accuracies is significantly larger than the difference of using 50 sub-graphs (as shown in Figure 4 (d) vs. (f)).

Overall, our experiments suggest that the number of sub-graph is important for achieving good classification accuracy. High accu-racy can be expected if a good number of sub-graphs are combined with good sizes of sub-graphs.
 Increasing number of sub-graphs reduces the difference be-tween classifiers built from different sub-graphs: The results from all benchmark datasets show that when increasing the num-ber of sub-graphs, the overall prediction accuracy will also increase (for example, Figure 4 (a), (b) , and (c)). For a small number of sub-graphs, e.g. the feature size is 50, the accuracies of the clas-sifiers trained from different sub-graphs are largely different, and random sub-graphs are inferior to other approaches (as shown in 4 (a) , (d) , (g) , and (j) where the differences of accuracy among three methods are from 0% to 21.5%). When the size of feature set increases to 2000, the accuracies of all three classifiers are very close to each other. In some cases, the three curves are found to be exactly the same, which suggests that the performance of using ran-dom sub-graphs will steadily improve when an increasing number features, and Information Gain based sub-graph features, respectively. of sub-graphs is used to train the classifier. Indeed, when a large number of sub-graph are used, the difference between different fea-ture set will be reduced. In an extreme case, if all sub-graphs are used to train classifiers, there is no difference between feature sets, which will therefore result in the same classification performance.
Overall, our experiments suggest that The difference between classifiers trained by using different sub-graphs is mostly notice-able when the number of sub-graphs is small. Such difference will be reduced when the number of sub-graphs increases, and may van-ish when a relatively large number of sub-graphs, say more than 1000, are used.
 Performance with respect to different classifiers shows similar trend: In Figure 5, we report the results of using Nearest Neigh-bour (NN) classifier on the benchmark data. Due to space limita-tions, we only select one NCI graph dataset. Compared with Sup-port Vector Machines, we can find that the overall accuracies of NN are slightly lower than SVM. However, the overall trends, such as the increasing/decreasing of the classification accuracies with re-spect to the size of sub-graphs (or the number of sub-graphs), are very similar to the results from SVM. The observations we made from SVM are mostly valid for NN as well.

Overall, our experiments suggest that There is minor difference between classifiers trained by using different learning methods, and the relationship between classifiers and sub-graphs remains mostly the same for different learning algorithms.
Existing research has commonly agreed that finding discrimina-tive sub-graphs to represent graphs is one of the main challenges for graph classification. Yet there is no comprehensive study about (1) the genuine relationship between sub-graphs and the classification accuracy, and (2) the actual difference between sub-graphs discov-ered from expensive mining process (such as frequent sub-graph mining) with sub-graphs from simple approaches (such as random sub-graphs). In this paper, we empirically validated the relation-ship between sub-graphs and graph classifiers by varying (1) the sub-graph feature sizes; (2) the size of the sub-graph feature set; (3) different learning algorithms; and (4) different benchmark datasets. We characterized sub-graphs discovered from different approaches (including random sub-graphs, frequent subgraphs, and frequent sub-graphs selected from Information Gain) by their size ( i.e. num-ber of edges) and by their number ( i.e. number of sub-graphs in a set), and validated the performance of classifiers trained from these sub-graphs on four benchmark graph datasets from three domains. Our study drew a number of important findings, which provide a clear view about the relations between sub-graphs and graph clas-sifications.
This research is sponsored by an Australian Research Council (ARC) Future Fellowship under grant No. FT100100971. [1] P. D. Dobson and A. J. Doig. Distinguishing enzyme [2] G. Gutin, A. Yeo, and A. Zverovich. Traveling salesman [3] J.R.Quinlan. Induction of decision trees. Machine Learning , [4] R. Kaspar and H. Bunke. Cluster ensembles based on vector [5] L. Tang, H. Liu, J. Zhang, and Z. Nazeri. Community [6] R. M. H. Ting and J. Bailey. Mining Minimal Contrast [7] X. Yan, H. Cheng, J. Han, and P. S. Yu. Mining significant [8] X. Yan, P. S. Yu, and J. Han. Graph indexing: A frequent
