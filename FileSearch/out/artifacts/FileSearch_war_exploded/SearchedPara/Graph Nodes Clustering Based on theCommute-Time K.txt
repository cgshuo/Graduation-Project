 This work presents a general methodology for clustering the nodes of a weighted, undirected, graph. Graph nodes clustering is an important issue that has been the subject of much recent work; see for inst ance [4], [5], [7], [11], [17] and [19].
On the other hand, kernel-based algorithms are characterized by two proper-ties: they allow (i) to compute implicitly similarities in a high-dimensional space where the data are more likely to be well-separated and (ii) to compute similari-ties between structured objects that ca nnot be naturally represented by a simple set of features. In this paper we propo se a new kernel matrix on a weighted, undirected, graph, which defines similarities between the nodes. These similar-ities take both direct and indirect links into account; they therefore take the indirect paths between the nodes into co nsideration. Two nodes are considered as similar if there are many short paths connecting them.
 Based on this kernel matrix, nodes are clustered thanks to a kernel clustering. The kernel clustering algorithms proposed in this paper differ from existing ones ([2], [9], [10], [20], [22] and [23]) by the fact that a prototype vector is explicitly defined for each cluster. This is more natural since it allows to mimic the iterative update rules reminiscent from k-means and fuzzy k-means in the sample space, instead of the feature space. In addition to be very similar to the original feature-based algorithms, this sample-based method can easily be extended to variable-metric or multi-prototype kernel k-means, in the same way as the original k-means and fuzzy k-means [6]. In addition to this, the resulting algorithm is very simple and natural.

The performances are evaluated on the problem of clustering newsgroups documents, and compared to the popular spherical k-means algorithm, which is especially designed for document clust ering [3], as well as a classic spectral clustering method [12]. The collection of documents is viewed as a graph and the basic problem is to cluster the documents in order to eventually retrieve the newsgroups. The results indicate that the introduced algorithms perform well in comparison with the spherical k-m eans and the spectral clustering, with significant improvement.

The paper is organized as follows. Sectio n 2 introduces the s igmoid commute-time kernel ( K CT ) on a graph that will be used as similarity measure for clus-tering the nodes. Section 3 derives our version of the kernel k-means and kernel fuzzy k-means, while Section 4 shows the results obtained on the newsgroups database. Section 5 is the conclusion. Let us consider that we are given a weighted, undirected, graph, G , with symmet-the adjacency matrix A of the graph are defined in a standard way as a ij = w ij if node i is connected to node j and 0 otherwise. Based on the adjacency matrix, the Laplacian matrix L of the graph is defined by L = D  X  A ,where D = Diag ( a i. ) is the degree matrix, with diagonal entries d ii =[ D ] ii = a i. = n j =1 a ij .We suppose that the graph has a single connected component; that is, any node can be reached from any other node of the graph. In this case, L has rank n  X  1, where n is the number of nodes. Moreover, it can be shown that L is symmetric and positive semidefinite (see for instance [8]).

The  X  X ommute time X  kernel [14] , [8] takes its name from the average com-mute time , n ( i, j ), which is defined as the average number of steps a random walker, starting in node i = j , will take before entering a node j for the first time, and go back to i . Indeed, we associate a Markov chain to the graph in the following obvious manner. A state is associated to every node ( n in total), One can show [14], [8] that, in this case, the average commute time can be the graph is represented by a basis vector, e i (the i -thcolumnoftheidentity matrix I ), in the Euclidean space n and V G = a .. is the volume of the graph. L + is the Moore-Penrose pseudoinverse of the Laplacian matrix of the graph and is positive semidefinite. Thus, n ( i, j )isa Mahalanobis distance between the nodes of the graph and is referred to as the  X  X ommute time distance X  or the  X  X esistance distance X  beca use of a close analogy with the effective resistance in electrical networks [8].

One can further show that L + is the matrix containing the inner products of the node vectors in the Euclidean space where these node vectors are exactly separated by commute time distances. In other words, the entries of L + can be viewed as similarities between nodes and L + can be considered as a kernel matrix: The sigmoid commute time kernel K CT is obtained by applying a sigmoid transform [15] on K . In other words, each element of the kernel matrix is given by the formula where l + ij =[ L + ] ij and  X  is a normalizing factor, corresponding to the standard deviation of the elements of L + . The parameter a will be set to a constant value determined by informal preliminary tests. The sigmoid function aims to normalize the range of the similarities in the interval [0 , 1] [15]. Notice, however, that the resulting matrix is not necessarily positive semi-definite so that, strictly speaking, it is not a kernel matrix. We now introduce our kernel, prototype-b ased, version of th e k-means and fuzzy k-means clustering algorithms. 3.1 Kernel k-Means The goal is to design an iterative algorithm aiming to minimize a cost function which, in the case of a standard k-means, can be defined, in the feature space, as the total within-cluster inertia: where the first sum is taken on the m clusters, while the second sum is taken on the nodes i belonging to cluster k , i  X  C k . In Equation (3), x i is the feature vector corresponding to node i , g k is a prototype vector of cluster k in the feature space and || x i  X  g k || is the Euclidean distance between the node vector and the cluster prototype it belongs to. The number of clusters, m ,isprovided a priori by the user.

We denote by X the data matrix containing the transposed node vectors as rows, that is, X =[ x 1 , x 2 ,..., x n ] T . Let us now define the following change of parameter: corresponding to the  X  X ernel trick X  (see [16]). It aims to express the prototype vectors, g k , as a linear combination of the node vectors, x i (the columns of X
T ). The h k will be called the prototype vectors in the n -dimensional sample space . Now, recompute the within-c lass inertia in terms of the h k and the inner products: where K = XX T , k ii =[ K ] ii = x T i x i , k i = Xx i = col i ( K ).
The k-means iteratively minimizes J by proceeding in two steps, (1) re-allocation of the node vectors while keeping the prototype vectors fixed, and (2) re-computation of the prototype vectors, h k , while maintaining the cluster labels of the nodes fixed. Clearly, the re-allocation step minimizing J is where l i contains the cluster label of node i .

For the computation of the prototype vector, by taking the gradient of J with respect to h k and setting the result equal to 0 ,weobtain Kh k = 1 n n k i  X  C k e i where n k is the number of nodes belonging to cluster k .Bylooking carefully, we immediately observe from the left-hand side of the equation that Kh k is a linear combination of the k i , while the right-hand side is also a linear combination of the k i . Therefore, one solution to this linear system of equations is simply the following:
In other words, h k contains 1 /n k if i  X  C k and 0 otherwise. This two-step procedure (equations (6) and (7 ))is iterated until convergence. 3.2 Kernel Fuzzy k-Means We now apply the same procedure for deriving a kernel fuzzy k-means. This time, the cost function is where the u ik define the degree of membership of node i to cluster C k . The pa-rameter q&gt; 1 is controlling the degree of fuzzyness of the membership functions. As for the kernel k-means, we perform the change of parameter (4), leading to the following update formula for the membership function. and the re-computation of the prototype vectors is simply, 4.1 Data Set In order to test the performances of the K CT k-means and the K CT fuzzy k-means, both algorithms will be assessed on a real data set and compared to classical clustering algorithms. The idea is to assess both algorithms on graph data set where only the informatio n on relation between nodes is given. The tested graphs are extracted from the newsgroups data set (Available from http://people.csail.mit.edu/jrennie /20Newsgroups/); it is composed of 20,000 unstructured documents, taken from 20 discussion groups (newsgroups) of the Usernet diffusion list. As the data set is composed of documents, the clustering performances of both methods will be co mpared to the spheri cal k-means [3], which is a reference in text mining; and to Ng X  X  spectral clustering [12], which presents some similarities with our approach.

For our experiment, 9 subsets including different topics are extracted from the original database, as listed in figure 1. More precisely, for each subset, 200 documents are sampled from different newsgroups. Thus, the three first subsets (G-2cl-A, G-2cl-B, G-2cl-C) contain 400 documents sampled from two news-groups topics, the next three subsets (G-3cl-A, G-3cl-B, G-3cl-C) contain 600 documents sampled from three topics and the last three subsets (G-5cl-A, G-5cl-B, G-5cl-C) contain 1000 documents sampled from five topics. The selected topics can be related such as politics/mideast and politics/guns in subset G-5cl-A. Both the classification rate (obtained by comparing the clustering to the real newsgroups and performing an optimal assignment) and the adjusted Rand index (with values scaled in [0 , 1]) will be reported. 4.2 Graph Definition The newsgroups data set can be seen as a large bipartite graph between docu-ments and terms. Each document node is connected to terms nodes contained in the document, each edge being weighted by the tf.idf factor [18]. After some preprocessing steps (see below) aimin g to reduce the number of terms, a graph involving only documents is computed from this bipartite graph in the following way: the link between two documents is given by the sum of all document-term-document paths connecting them and passing through the terms they have in common. In other words, if W represents the term-do cument matrix contain-ing the tf.idf factors, the adjacency matrix of th e resulting document-document graph is provided by A = W T W . 4.3 Preprocessing Steps In order to reduce the high dimensionality of the feature space (terms), the following standard preprocessing steps are performed on the data set before the clustering experiment. 1. Stopwords without useful information are eliminated. 2. Porter X  X  stemming algorithm [13] is applied so that each word is reduced to 3. Words that occur too few times ( &lt; 3) or in too few documents ( &lt; 2) are 4. The mutual information between terms and documents is computed. For a 5. The term-document matrix W is constructed with the remaining words and 6. Each row of the term-document matrix W is normalized to 1.

Finally, the adjacency matrix of the documents graph A is given by the document-document matrix W T W . Based on A , K CT is computed by Equation (2). For example, the subset G-2cl-A is composed of 400 documents, and 2898 terms with stopwords already eliminated. After preprocessing, only 1490 terms are kept. Thus, the clustering algorithm will be run on a 400  X  400 document-document matrix, instead of a 1490  X  400 term-document matrix for a standard feature-based algorithm. 4.4 Experimental Settings Suppose we have a graph of n nodes to be partitioned into m clusters. First, the prototype vectors h i ( i =1 , ..., m ) are initialized by randomly selecting m columns of the identity matrix I . Then, each algorithm is run 30 times (30 runs), and the classification rate as well as the adjusted Rand index, aver-aged on the 30 runs, are computed. The K CT k-means, K CT fuzzy k-means and Ng X  X  spectral clustering are ru n on the document-document matrix A , while the spherical k-means is run on the term-document matrix W after preprocessing.

Each run consists in 50 trials: the clustering algorithm is launched 50 times and the best solution among the 50 trials, having the minimal within-class inertia, is sent back as the solution.

Two parameters need to be tuned. The first one is the parameter a for comput-ing the sigmoid transform of the K CT (see Equation (2)). The second one is the parameter q which controls the degree of fuzzyness for the kernel fuzzy k-means (see Equation (9)). Based on preliminary informal experiment, the parameters a and q were set to 7 and 1.2 respectively, for all experiments. 4.5 Experimental Results and Discussion The results (the classification rate as well as the adjusted Rand index, each averaged on 30 runs) of the four clustering algorithms ( K CT k-means, K CT fuzzy k-means, spherical k-means and Ng X  X  sp ectral clustering) on the nine document subsets are reported in Table 1.

We observe that the K CT k-means and the K CT fuzzy k-means outperform the spherical k-means on the nine subsets. Ng X  X  spectral clustering presents good results on the 2-classes and 3-classes data sets, but degrades when the number of clusters increases. Moreover, the K CT fuzzy k-means provides slightly better results than the two other methods. This can be partly explained by the fact that boundaries between the different topics: a discussion within a specific newsgroup can also be related to other domains. A close examination of the data set shows that several discussions can even be out of subject or are simply empty of useful information. We introduced a new method allowing to cluster the nodes of a weighted graph by exploiting the links between them. I t is based on a recently introduced kernel on a graph, the commute-time kernel, co mbined with a kernel clustering. The obtained results are promising since the proposed methodology outperforms the standard spherical k-means as well as spectral clustering on a difficult graph clustering problem. Further work will be devoted to (1) additional experiments on other text databases, and (2) developing kernel versions of the Gaussian mixture, the entropy-based fuzzy clusteri ng, Ward X  X  hierarchical clustering, and assessing their performances.

