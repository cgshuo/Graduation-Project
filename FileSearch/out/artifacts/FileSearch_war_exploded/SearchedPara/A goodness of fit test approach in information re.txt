 Kostas Fragos  X  Yannis Maistros
Abstract In many probabilistic modeling approaches to Information Retrieval we are in-terested in estimating how well a document model  X  X its X  the user X  X  information need (query assessing the assumptions about the underlying distribution of a data set. Supposing that the query terms are randomly distributed in the various documents of the collection, we actually want to know whether the occurrences of the query terms are more frequently distributed by chance in a particular document. This can be quantified by the so-called goodness of fit tests.
In this paper, we present a new document ranking technique based on Chi-square goodness of fit tests. Given the null hypothesis that there is no association between the query terms q and the document d irrespective of any chance occurrences, we perform a Chi-square goodness of fit test for assessing this hypothesis and calculate the corresponding Chi-square values.
Our retrieval formula is based on ranking the documents in the collection according to these calculated Chi-square values. The method was evaluated over the entire test collection of
TREC data, on disks 4 and 5, using the topics of TREC-7 and TREC-8 (50 topics each) conferences. It performs well, outperforming steadily the classical OKAPI term frequency weighting formula but below that of KL-Divergence from language modeling approach. De-spite this, we believe that the technique is an important non-parametric way of thinking of of-fit statistical tests X  framework, modeling the data in various ways estimating or assigning any arbitrary theoretical distribution in terms.
 Keywords goodness of fit tests . Information Retrieval Introduction
Information Retrieval is the area of natural language processing where statistics have been successfully applied. Two statistical models of ranked retrieval developed in the early 70s are still in use today: Salton X  X  vector space model (Salton, 1971) and Robertson and Sparck-
Jones X  probabilistic model (Robertson and Jones, 1976). For nearly 30 years, the statistical language model has been the workhorse of statistical speech recognition and many of the statistical techniques that were first successfully applied in speech recognition systems, like
Shannon X  X  noisy channel model, n-gram models and Hidden Markov Models are used today in all sorts of applications. It has been recently shown that statistical language models can be used to model Information Retrieval as well. A statistical language model is a probabilistic mechanism for generating text and has its origin back from the date of Shannon (Shannon, 1948) in the  X  X ource channel perspective X  who considered how well simple n-gram models did at predicting natural text.

Although for several years a significant interest has been expressed in language model-ing methods for predicting and generating text in a variety of natural language processing tasks, in Information Retrieval, language modeling ideas were used rather to the opposite direction. In the classic probabilistic models Robertson and Sparck Jones (1976), OKAPI system (Robertson et al., 1981), the so-called naive-Bayesian networks (Turtle and Croft, 1991), the Inquery system (Broglio et al., 1995), probability distributions over documents control this probability because, in most cases, the only evidence is the query terms and the construction of an accurate distribution model is impossible. Despite this drawback, these models have been successfully applied in many retrieval tasks with considerable success.
To overcome the above mentioned obstacle, (Ponte and Croft, 1998) used a smoothed ver-sion of the unigram language model and proposed a method to assign likelihood scores to a query from the documents. This method is known as language modeling approach and can be thought of as using a language model as a  X  X oisy channel X  or  X  X ranslation model X  that maps documents to queries. The past two years show a remarkably large number of publi-cations in which statistical language models are used to compute the ranking of documents given a query. Miller et al. (1999a,b), use Hidden Markov Models for ranking, including the use of bi-grams to model two word phrases and a method for performing blind feed-back. Berger and Lafferty (1999) developed a model that includes statistical translation to model synonyms, that is, words having the same sense and words related to other semantic relations.

An interesting way for deriving nonparametric probabilistic models of information re-trieval in the language model approach is the  X  X ivergence from randomness X  framework (Amati and Rijsbergen, 2001). Within this framework, term-weighting models are derived by measuring the divergence of the actual term distribution from that obtained under a random process.

In this paper, we adopt a different approach to ranking documents for a particular query based on Chi-square goodness of fit tests. More specifically, to rank a document d we count the frequencies of the query terms in that document and compare them to the frequencies expected by chance. If the difference between the observed and expected values is large, we can pretend that the document may be biased towards the query and give it a higher ranking.
Performing a Chi-square goodness of fit test for assessing the validity about the randomness of occurrence of query terms, we finally rank the documents in the collection according to corresponding Chi-square values.

A first attempt to use the Chi-square test method in Information Retrieval is referred to by the work of (Oakes et al., 2001) for document classification. In this work, Chi-square test is used to produce a ranked list of  X  X eywords X  deemed to be the most and least characteristic of the texts in a category. The list of most characteristic  X  X eywords X  is then used to rank the documents for the specific category.

We present here a short introduction to the goodness of fit statistical tests and to our document ranking retrieval method. A short description of the models to which our system is compared is also included. Finally, we provide experimental results and conclude with a discussion on our technique and suggestions for further work. 1. Chi-square goodness of fit statistical tests
The aim of the goodness of fit tests in statistics is to verify the hypothesis that experimental data comes from a random variable whose distribution is well known. This is a very important and experimental distribution follow the same functional law. In other words, the problem is reduced to the choice of one of these two alternative hypotheses. The null hypothesis H that the sample follows the underlying theoretical distribution and the alternative hypothesis
H 1 that does not. A test is considered powerful if the probability of accepting H is wrong is low.
 The most important and known test is Pearson X  X  Chi-squared test (D X  X gostino and
Stephens, 1986). It has been introduced to study discrete distributions X  adaptation, but it can be useful also in the case of continuous distributions if the data will be properly grouped into classes. A simple and effective way of studying continuous variables is by counting the number of observations falling into each class. The number associated with each category is called the frequency and the collection of frequencies over all classes gives the frequency distribution of that variable.

The Chi-square goodness of fit test statistic is defined as where O i is the observed frequency for class i and E i is the expected frequency for class i .
The expected frequency is calculated by E i = Np i where p and N is the sample size.

The test statistic follows, approximately, a Chi-square distribution with ( k freedom where k is the number of classes and c the number of parameters for the distribution plus one. For example, for a 2-parameter normal distribution, c or Chi-square distribution tables we compute the probability p for the calculated Chi-square value from Eq. (1) and then we reject if p is too low (typically if beneath a significance level a ) or retain H 0 otherwise. The Chi-square is a continuous random variable, so the point-wise probability is zero. The probability computed is the probability that the Chi-square value is applied if the counting of the theoretical frequencies in each class are less than 5.
Let see an example to make clearer the above idea. Suppose we are interested in testing the null hypothesis that a random sample of 100 people has been drawn from a population where men and women are even. In this case the observed number of men and women would be compared to the expected frequencies (those arisen from our theoretical assumption about the data of equal frequencies in the population) of 50 men and 50 women. If the observed frequencies were 45 men in the sample and 55 women, then
There is one degree of freedom in the comparison (since either difference between ob-served and expected frequencies, once known, infers the other). Consulting Chi-square dis-a p -value not higher than 0.05 are generally deemed  X  X ignificant X  and are widely considered unlikely attributed to chance. This probability is higher than conventional criteria for statis-tical significance, so normally we would accept the null hypothesis that the number of men in the population is the same as the number of women. 2. The document ranking retrieval method
The essence of the proposed method is to compare the observed frequencies of the particular query terms in the document d with the expected ones by chance and quantify a discrepancy between the two values using the Chi-square test. This discrepancy can be used as a ranking criterion for Information Retrieval.

An assumption about the distribution of the occurrence of the terms, according to which the query terms are supposed to be distributed in the documents following discrete uniform distribution with equal probability p = 1 / C , where C the total number of terms in collection, leads to a simple ranking retrieval formula. Of course, another assumption about the random-ness (for example the binomial model) may lead, within the same framework of Chi-square tests, to a different document retrieval formula.

In the retrieval framework of Chi-square tests, the null hypothesis H of the query terms are distributed by chance in the documents (Theoretical assumption) and the opposite H 1 is that the occurrences do not follow the chance distribution. We reason that rejecting the null hypothesis is an evidence for a bias or a relatedness between query and document. The null hypothesis is rejected when the  X  2 value calculated from Eq. (1) is higher than the value obtained from Chi-square distribution,  X  2
Chi-square percent point function with k  X  c degrees of freedom and a significance level of  X  . K is the number of classes and c the number of parameters for the distribution plus one. The greater the calculated  X  2 value is the stronger the evidence will be to reject the null hypothesis and thus yield to a relatedness between the query and document. Therefore, for the ranking purposes of Information Retrieval we use the relatedness between query and document and we rank all the documents in the collection according to their corresponding  X  2 values. The documents with the higher values are ranked first in the returned document ranking list.

Since, we are not actually interested in testing the rejection or not of the null hypothesis conditions of the test. So, there is no need to calculate the distribution. The calculated  X  2 value from Eq. (1) is sufficient for the ranking purpose.
For the documents X  text data we are dealing with, we assume that there are as many classes as the query terms, and we try to estimate for each query term the expected and observed frequencies. Denoting the frequency of a query term k i in the document d as tf the frequency of the term in the collection C as ct f i = the terms in the document and in the collection as D = k respectively, the expected and observed frequencies will be given by the following formulas:
Our document scoring retrieval formula, Eq. (4), results from the substitution of Eqs. (2) and(3)in1.
 the sum refers to all the terms of the query q .

The difference O k i -E k i can be considered as a retrieval score obtained for each query term given the document d . The score is squared and scaled by dividing by the expected value E for that query term. These normalized difference scores are then summed to get an overall retrieval score.

The main advantage of this retrieval method is that it is a simple non parametric way of scoring the documents in the collection. In other parametric approaches, like in Language
Modeling approach for example, the derived models need to be supported by various data driven methodologies, such as parameter learning with training or data smoothing support functions. A non-parametric model is derived in a purely theoretic way as a combination of different probability distributions. 3. Description of the TFIDF retrieval Schemas and KL-divergence We proceed with a short description of the models which were used for comparison with the
Chi-square retrieval method in the evaluation section. 3.1. TFIDF schemas and OKAPI term weighting formula
The tf  X  id f retrieval schemas are known as the classical vector space model, firstly pro-posed by Salton in 1971 (Salton, 1971). According to this model each term k d is associated with a positive non-binary weight w ij that expresses the importance of the term to identify the semantics of this document. Further, the index terms in the query are also weighted. If we represent the document d j as the vector ( then we can use the cosine of the angle between the two vectors as a similarity measure between query and document.
We now turn to the weighting scheme of the vector space model. One could just count the number of occurrences of a term in a document (term frequency) and the number of documents in which this term appears (document frequency), and combine them into a single weighting formula. Very soon researchers realized that using raw term frequencies is not optimal whereas a logarithmic scaling of these frequencies offers a better weighting metric.
Recently, various weighting schemas have been proposed and very often these methods are based on researchers X  experience with systems and large scale experimentation. To compare our system with a classical tf  X  id f retrieval schema in the evaluation section of this work we use one of the most successful and widely used term weighting method, the well known term frequency OKAPI ( TF ) BM25 formula and the cosine similarity to score the documents in the collection (Robertson et al., 1995). A reasonable baseline performance can usually be obtained by using the BM25 TF with appropriate parameter settings in tf (Zhai, 2001). This term weighting formula is given by Eq. (6). where tf is the term X  X  frequency in document, qt f the term X  X  frequency in the query, N the total number of documents in the collection, df the number of documents that contain the term, dl the document X  X  length (in bytes), a v dl is the average document length and finally, k between 1.0-2.0, b (usually 0.75), k 3 between 0-1000 are constants. 3.2. KL-Divergence
The KL-Divergence is a flexible retrieval model that extends language modeling approach to a retrieval process that involves the estimation of a document language model and a language query model and compares them with the Kullback-Leibler divergence. Given two language models p(x) and q(x), the Kullback-Leibler divergence or relative entropy between the two probability mass functions is defined by the formula 7
Although this is not a true distance (it is not symmetric and does not hold the triangle inequality) we think it of as a good measure of the  X  X istance X  between two distributions and hence as a ranking criterion in retrieval. In KL-divergence, supposing that the query q is generated by a generative model p ( q |  X  q ) and the document d by p ( denote the parameters of the unigram language model, then, the relevance value of d with respect to q can be measured by the following negative KL-divergence function 8:
The second term on the right-hand side of the formula 8 is a query-dependent constant, or more specifically the entropy of the query model  X  q , and can be ignored for the purpose of ranking documents. In the same formula we see that the relevance value of d with respect to q depends on the estimation of the query model p ( w |  X  q
In this work, for the estimation of the language model p ( technique which makes performs a linear interpolation of the maximum likelihood estimate p ( the influence of each model (Jelinek and Mercer, 1980), (Zhai and Lafferty, 2001). Where c ( w ; d ) represents the count of term w in document d , w c ( terms in the document d and V is an estimated vocabulary size (e.g., the total number of the distinct words in the collection). The final smoothing model is summarized in Eq. (9).
The simplest method to compute the query model is to compute the probability mass using the maximum likelihood estimator p ml ( w |  X  q ) = c ( w, text. A more interesting important way to compute the query model in KL-Divergence is to exploit feedback documents using an interpolation of the maximum likelihood estimate p ml ( w |  X  q ) with a feedback model p ( w |  X  F ) estimated on feedback documents.
In general, incorporating feedback documents is a popular technique to expand query terms in Information Retrieval, but the application of this technique in the models that are to be compared here is out of the scope of this paper. 4. Evaluation
As it is well known the performance of a particular retrieval algorithm is depends on the selected test data and may be vary from collection to collection. To have a better illustration of the retrieval capabilities of the proposed retrieval method, we evaluated it on the whole test collection of TREC data from both disks 4 and 5, using the uniform assumption about the model of randomness. For this collection we used the topics of TREC-7 and TREC-8 (50 topics each) for ad hoc retrieval (Voorhess and Harman, 2001). Disks 4 and 5 consist of about 556,000 documents (about 2.1 Gbytes of data) from the Congressional Record, Federal Register, Financial Times, Foreign Broadcast Information Service and LA Times collections.
From these collections weW did not index Congressional Record (about 28000 documents) as they were removed from TREC-7 and TREC-8.

In all these texts we did not apply any tokenization and also did not use any removal list of stop words, but we indexed deliberately all the words in the document collection, since we thus avoided being biased biases by any artificial choice of stop words.
 Some statistics of the test collection are given in Table 1
In topics of TREC-7 and TREC-8 each of the 50 topics consists of three fields: title from one to three words, description from one to two sentences and narrative a paragraph with instructions for accepting or rejecting a document. From these fields we used only title in the experiments as we believe that this is closer to the user queries in real applications.
We tested the proposed  X  2 -GOF method and compared it with OKAPI BM25 weighting retrieval formula and KL-Divergence from language modeling approach using topics TRE-7 (topics 351-400) and TREC-8 (topics 401-450). For the three retrieval methods, we mea-sured the average precision in the top 1000 ranked returned documents, as it reflects the overall ranking accuracy well, as well as the average (interpolated) precision at 11 recall points.
 The results are shown in Tables 2 and 3 for topics TREC-7 and TREC-8 respectively.
Although in  X  2 -GOF retrieval formula only raw frequencies are used, the model performs well outperforming steadily OKAPI BM25 weighting retrieval formula. However, in both cases of TREC-7 and TREC-8 KL-Divergence has the best performance but this method is parametric and needs the smoothing of parameters over the entire collection.

The basic information used in the  X  2 -GOF retrieval formula to rank a document d is the frequency tf i of a query term k i in the document d , the frequency ct f collection C , the total frequency D of all the terms in the document and the total frequency C of all the terms in the collection. It is important to note the difference between the retrieval formula and the classical retrieval formulas from the vector space model. In the family of the so-called tf.idf weighting schemes the retrieval formula is closely related to the use of the document frequency , which is the number of documents in the collection that query term k i occurs in and it can be interpreted as a measure of the informativeness of the term.
The experimental results show that the absence of this quantity from the formula does not affect its retrieval performance.

The simplicity is one of the main advantages of the  X  2 -GOF raw frequencies the calculated  X  2 -GOF value improves retrieval performance and allows retrieval of documents that approximate the query conditions. Moreover, Chi-square allows us to decide whether there exist a statistically significant relationship between a query and a document. If the calculated value is large we conclude that there is a strong relationship between query and document and the value itself is sufficient to sort the documents according to their degree of similarity to the query.
 Performances of the compared algorithms in these experiments seem to be different.
To consider more formally the extent to which these results provide persuasive evidence in performance difference we could perform a paired t-test. The paired t-test is used to determine whether the means of paired samples from two populations are equal. In our case, if we treat the obtained precision values at the 11 recall points for each model as paired samples, then the computed paired t-test gives the probability that the average precisions are different. The smaller the probability is, the more significant the difference between the average precisions.
Performing a paired t-test for the models  X  2 -GOF and OKAPI weighting formula, the returned p-value for topics 351-400 is 0.0326 and for topics 401-450 is 0.00010608. Thus, we conclude that the average precision obtained for models  X  2 -GOF and OKAPI weighting formula are different with 96.74% and 99.98% confidence for topics 351-400 and 401-450 respectively.
Similarly, comparing the models  X  2 -GOF and KL-divergence we find a p-value 0.0004 for topics 351-400 and 0.0018 for topics 401 X 450.

As it was aforementioned, the proposed  X  2 -GOF retrieval model offers the possibility to try simple alternative retrieval formulas using different processes as the basic model of randomness. In  X  X ivergence from randomness X  framework (Amati and Rijsbergen, 2001), it is proposed to define those processes with urn models and random drawings as models of randomness. Following this proposal, we shifted our assumption about the model of randomness into to the binomial model. According to this model the occurrence of a single term i in a document d is considered as a Bernoulli process with probability p
N the number of documents in the collection. If ct f i the collection frequency of the term i , then we make the assumption that the term i should distribute over the N documents according to the binomial law. Therefore the probability of tf given by where p = 1 / N and q = ( N  X  1) / N .

The expected number of occurrences of the term in a document is is constant for all the documents in the collection. Using tf ct f i / N as the expected frequency, we obtain from Eq. (1) the document retrieval formula for the new assumption about the model of randomness the sum is over all the terms of the query q .

We compared the performance of this new retrieval formula (binomial assumption) with that of our initial retrieval formula (uniform assumption), in a second experiment using the fbis document collection from disk 5 of TREC test data.

The statistics of the fbis collection and the evaluation results of the second experiment are shown in Tables 4, 5 and 6.

From these results we draw the conclusion that the model of the uniform assumption performs slightly better than the model of the binomial assumption. 5. Discussion and suggestions for further work In this work we presented a method to apply goodness of fit statistical tests in Information
Retrieval. According to this method we formulate a  X  X ull hypothesis X  that terms of a query are distributed in the various documents randomly and there is no association between a document d and the query terms beyond chance occurrences. Using a Chi-square goodness of fit test for assessing this hypothesis we quantify a discrepancy between expected and ob-served frequencies by the calculated  X  2 value. This discrepancy characterizes the relatedness between query and documents and it is used as ranking criterion for Information Retrieval.
The goodness of fit test approach has the advantage that is not parametric and, let enables us to model the data in various ways estimating or assigning any arbitrary theoretical distribution in the terms and then exploiting the powerfulness of goodness of fit statistical tests. Some different theoretical assumptions about the randomness of the data like normality, the Weibull distribution etc., might be proven a good choice to test the performance in a future work.
Moreover, other available successfully applied in statistics goodness of fit statistical tests, such as the Kolmogorov-Smirnov and Anderson-Darling tests (D X  X gostino and Stephens, 1986) might be present a good alternative of the Chi-square test.
 References
