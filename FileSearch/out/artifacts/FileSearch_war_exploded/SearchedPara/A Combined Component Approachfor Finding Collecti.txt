 In this paper, we propose a new method to discover collection-adapted ranking functions based on Genetic Programming (GP). Our Combined Component Approach (CCA) is based on the combination of several term-weighting components (i.e., term frequency, collection frequency, normalization) extracted from well-known ranking functions. In contrast to related work, the GP terminals in our CCA are not based on simple statistical information of a document collection, but on meaningful, effective, and proven components. Experi-mental results show that our approach was able to outper-form standard TF-IDF, BM25 and another GP-based ap-proach in two different collections.
 CCA obtained improvements in mean average precision up to 40.87% for the TREC-8 collection, and 24.85% for the WBR99 collection (a large Brazilian Web collection), over the baseline functions. The CCA evolution process also was able to reduce the overtraining, commonly found in machine learning methods, especially genetic programming, and to converge faster than the other GP-based approach used for comparison.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval; I.2.6 [Artificial Intelligence]: Learning General Terms: Algorithms, Measurement, Experimenta-tion.
 Keywords: Information Retrieval, Ranking Functions, Term-weighting, Genetic Programming, Machine Learning.
The growth in volume of the Web and other textual repos-itories, such as digital libraries, throughout the last decade, Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. has made the information retrieval task difficult, costly, and in many cases, very complex for the end user. In this con-text, search engines became valuable tools to help users find content relevant to their information needs. Naturally, re-search on information retrieval models that can effectively rank search results according to document relevance has be-come a fundamental subject.

Information Retrieval models have come a long way. Al-though the most popular is still undoubtedly the vector space model proposed by Salton [19], many new or com-plementary alternatives have been proposed, such as the Probabilistic Model [16]. From all these models, document ranking formulas can be derived for document searching.
Thus, many alternatives exist on how to compose a rank-ing function. Most of them have a common characteris-tic: they attempt to be very general in nature, i.e., they were designed to be applied in any type of collection. The work of Zobel and Moffat [26], for example, presented more than one million possibilities to compute a similarity func-tion. However, after all the experiments, they concluded that no weighting scheme is consistently good in all collec-tions. That is, a ranking function can have success in one domain but fail in another. Further, they comment that it would be prohibitive to discover the best weighting scheme simply by an exhaustive exploration of the similarity space.
In this work, we discover specialized ranking strategies for specific collections. Our method is able to consider the im-portant and unique characteristics of each collection so that the discovered function is more effective than any general solution. To accomplish this, we use Genetic Programming (GP), a machine learning technique inspired by Darwinian evolutionary processes, to discover specific ranking functions for each document collection. GP has been successful in many IR problems [7 X 10, 13, 22]. GP was chosen due to its ability to find any arbitrary function, even when deal-ing with very large search spac es. However, differently from other GP-based approaches, which use only basic statistical information from terms and documents, our strategy uses rich, meaningful, and proven effective components present in well-known ranking formulas, such as Okapi BM25 [18] and Pivoted TF-IDF [21]. Our assumption is that by provid-ing these human-discovered formula components as building blocks, the GP process can take advantage of all the human knowledge that has been applied to produce them. As a con-sequence, it will be able to better explore the search space.
To validate our GP approach we performed experiments with the TREC-8 and WBR99 collections. Results indicate that the use of meaningful components in a GP-based frame-work leads to effective ranking functions that significantly outperform the baselines (standard TF-IDF, BM25 and an-other GP-based approach [9]). Our Combined Component Approach (CCA) ranking functions also converged to good results faster than the GP approach used as baseline, and the overtraining also was reduced.

This paper is organized as follows. In Section 2, we pro-vide background information on term-weighting components and genetic programming. In Section 3, we present our Combined Component Approach for similarity calculation. The collections used and experimental results are detailed in Section 4. In Section 5, we describe related work. Fi-nally, Section 6 concludes the paper and gives suggestions for future work.
In this section we present the term-weighting components used in our approach and a brief review of some concepts of Genetic Programming.
In [20], Salton and Buckley present a specification for the main function of a term-weighting system. Generally, a typ-ical term-weighting formula is defined as being composed of two component triples: tfc q ,cfc q ,nc q , which represents the weight of a term in a user query q ,and tfc d ,cfc d ,nc which represents the weight of a term in a document d .The term frequency component ( tfc ) represents how many times a term occurs in a document or query. The collection fre-quency component ( cfc ) considers the number of documents in which a term appears. Low frequencies indicate that a term is unusual and thus more important to distinguish doc-uments. Finally, the normalization component ( nc )triesto compensate for the differences existing among the document lengths.

Typical term-weighting formulas combine these three com-ponents. For instance, as in [20], we can define w td = tfc cf c d  X  nc d ,and w tq = tfc q  X  cf c q  X  nc q ,where w td weight of term t in document d and w tq is the weight of term t in query q . A common definition for some of these components is tfc d as the raw term frequency of term t in document d , cf c d as the inverse document frequency ( idf ) of term t (usually defined as cf c d =log( N/n t ), where N is the total number of documents and n t is the number of doc-uments where term t occurs), and nc d as the inverse of the size of document d . This is usually called a TF-IDF (term frequency X  X nverse document frequency) weighting scheme.
We can express a ranking function based on such a term-weighting system as follows: where sim ( q, d ) is the similarity measure between a query q and a document d .

Ten years after Salton and Buckley X  X  proposal, the work of Zobel and Moffat [26] explored this taxonomy further by adding eight different types of weighting functions. Their approach leads to more than 1,500,000 combinations for calculating the similarity between documents and queries, demonstrating that the space of possibilities for customizing and refining ranking functions is extremely large. This has stimulated the application of effective search space explo-ration techniques, such as GP [11], for discovering collection-adapted similarity functions.
Genetic Programming (GP), an inductive learning tech-nique introduced by Koza in [11] as an extension to Genetic Algorithms (GA), is a problem-solving system inspired by the idea of Natural Selection . The search space of a prob-lem, i.e., the space of all possible solutions to the problem, is investigated using a set of optimization techniques that imitate the theory of evolution, combining natural selection and genetic operations to provide a way to search for the fittest solution.

The evolution process starts with an initial population composed by a set of individuals . Generally, the initial pop-ulation is generated randomly. Each individual denotes a solution to the examined problem and is represented by a tree. To each individual is associated a fitness value. This value is determined by an evaluation function, also known as fitness function . The fitness value indicates goodness of an individual and it is used to eliminate from the populations all  X  X nfit X  individuals, selecting only those that are closest to the desired goal. The individuals will evolve generation by generation through genetic operations such as reproduction , crossover ,and mutation . The reproduction operator simply breeds a new individual. The mutation operator simulates the deviations that take place in the reproduction process. Finally, the crossover operator generates new individuals by the composite of some characteristics present in two other individuals (the parents ).

Thus, for each generation, after the genetic operations are applied, a new population replaces the current one. The fitness value is measured for each new individual, and the process is repeated over many generations until the termi-nation criterion has been satisfied. This criterion can be a preestablished maximum number of generations or some additional problem-specific success predicate to be reached (e.g., an intended value of fitness for a specific individual).
Our Combined Component Approach (CCA) is a GP-based approach for discovering good ranking formulas. Our goal is to discover new ranking functions more adapted to the specificities of a particular collection. As mentioned before, differently from other GP approaches, our idea consists in examining important information retrieval ranking formulas from systems such as [1,4,18] and extracting from their rank-ing schemes components such as those described in Section 2.1. These components can be entire formulas or some parts of a ranking formula. Once they are identified, we can use them as building blocks of our GP approach and combine them to generate new ranking functions.
As in Fan et al. [7], we use a tree data structure to repre-sent a term weighting formula. This tree-based representa-tion allows for easy parsing, implementation and interpreta-tion. Figure 1 illustrates an example individual representing a TF-IDF formula. The leaf nodes in such trees are called terminals , and represent the basic information units that will be composed to create the final formula. Terminals are combined through functions which are represented in the in-ternal nodes. In previous works [7 X 10, 22], terminals always reflect basic statistics directly derived from the collection, such as term frequency or document size. Our work differs from this approach, as explained below.
 Figure 2 displays another individual representing the TF-IDF weighting scheme. In this case, idf information is itself a terminal and not the result of a combination of termi-nals. In previous approaches, this information is a subtree that must be explicitly discovered by the GP evolutionary process. Thus, our CCA approach takes advantage of using information previously known to be effective in other rank-ing formulas, such as idf , pivoted normalization and others, allowing for a more effectively oriented exploration of the search space.
 Figure 1: A sample tree for a TF-IDF individual based on statistical information Figure 2: A sample tree for a TF-IDF individual based on our Combined Component Approach The GP framework is basically an iterative process with two phases: training and validation. For each phase, we select a set of queries and documents from the collection, which we call the training set and the validation set .

The framework starts with the creation of an initial ran-dom population of individuals that evolves generation by generation using genetic operations (reproduction, crossover, and mutation). The process continues until a stopping cri-terion is met. In the training phase, each time a new gener-ation is created, the fitness function is applied to each new individual, to select only the fittest. Since each individual represents a weighting scheme, applying this fitness func-tion corresponds to ranking the set of training documents according to the set of training queries, using the individ-ual X  X  weighting scheme. The obtained fitness value is simply a quality assessment of the generated ranking.

After the last generation is created, to avoid selecting in-dividuals that work well in the training set but do not gen-eralize for different queries/documents (the overfitting prob-lem), the validation phase is applied. In this case, the fitness function is used but on the validation set of queries and doc-uments. Only the individuals that perform the best in this phase are selected as the final solutions. This process is describedinListing1.
 An individual is represented by terminals and functions, or-ganized in a tree structure, as shown in Figure 2. Terminals contain information extracted from the main ranking for-mulas published on IR literature. Table 1 describes all the terminals to be used. Besides these, we also use constant values in the range [0..100]. As functions, we use addition (+), multiplication (  X  ), division ( / ) and logarithm (log). Based on Koza [11], we use the genetic operators of repro-duction, crossover and mutation as detailed in Section 2.2. Since our individuals represent term-weighting schemes to be used in a document ranking function, our fitness func-tion must measure the quality of the ranking generated by a given individual. We experimented with two different func-tions: (1) non-interpolated average precision over all rel-evant documents (PAVG) as described in [5, 24], and (2) function FFP4 as defined in [5], a utility function based on the idea that the utility of a relevant document decreases with its ranking order.
 As mentioned before, we use a validation set to help in choos-ing good solutions that are not over-specialized for the train-ing queries, i.e. that are able to generalize for unseen queries. In [12], the choice of the best individual is accomplished by considering the average performance of an individual in both the training and validation sets minus the standard devia-tion value. We call this method AVG  X  . The individual with the highest value of AVG  X  will be selected as the best.
Despite being a balanced approach, our experiments have shown that this method may not lead to the best perfor-mance in some runs. We therefore propose a similar method, which also considers the dispersal between training and val-idation values, but uses the sum of these values in place of the average. We call this method SUM  X  . More formally, let t be the training performance of an individual i ,let v i be the validation performance of this individual, and let  X  i the corresponding standard deviation. The best individual is selected by: The experiments described in this paper in Section 4 were performed with both AVG  X  and SUM  X  methods.
In this section we describe our experiments and present the results obtained.
In our experiments, we use the TREC-8 [24] and WBR99 collections to evaluate our approach. The TREC-8 collec-tion has roughly 528K documents, and 737K distinct terms. Our experiments were performed using 50 topics numbered from 401 to 450. For each topic, there is a set of relevant documents that can be used for testing a new ranking for-mula. The queries were automatically generated using the title, description and narrative of each topic. We divide the queries into three groups: topics 401-420 were used for the training phase, topics 421-430 were used for the validation phase, and topics 431-450 were used for the test phase.
The WBR99 collection 1 , which has been previously used in works such as [15], contains a database of Web pages, a set of queries and a set of relevant documents associated with each query. The relevant documents were generated through a manual evaluation of ranked documents retrieved by a query processor based on the vector space model. The col-lection has almost 6M Web pages, crawled from the Brazil-ian web (the .br domain), and almost 2.7M distinct terms. It represents a considerably connected snapshot of the Brazil-ian Web community, which is probably as diverse in content and link structure as the entire Web. Thus, we believe it makes a realistic testbed for our experiments. Queries 1-20
Available at http://www.linguateca.pt/Repositorio/WBR-99/ were used for the training phase, queries 21-30 for valida-tion, and queries 31-50, except query 35, for the test phase. All the results reported in Section 4.4 for both collections are based on the test queries.
Almost all the parameters and configurations were tuned based on Fan et al. X  X  work [9]. An initial population of 200 individuals was created randomly using the ramped half-and-half method . We experimented with PAVG and FFP4 fit-ness functions, as described in Section 3.1, for the TREC-8 and WBR99 collections. PAVG was more effective for the TREC-8 collection, whereas FFP4 was the best choice for WBR99. Thus, we report only the results obtained with PAVG for TREC-8 and with FFP4 for WBR99. Due to the stability of the results after 30 generations, we defined this value as the termination c riterion. We used crossover, reproduction and mutation rates of 90%, 5%, and 5%, re-spectively. The random seed used was 1234567890. At the end of each generation, the validation phase was run for the top 20 best individuals discovered on the training phase of that generation.

The terminals used were those defined by our CCA ap-proach in Table 1 plus real constant numbers generated ran-domly. We used the addition (+), multiplication (*), divi-sion (/) and protected logarithm 2 (log) functions to combine terminals and subtrees of an individual.

The maximum depth of the generated trees ranged from 3 to 12. We experimented with each value of maximum depth to analyze its influence on the quality of the discovered ranking functions.
We compared the retrieval results of our approach with three others: (i) Okapi BM25, as described in [17] (using pa-rameters k 1 =1.2, k 2 =0, k 3 =1000, b =0.75), (ii) vector space model with standard TF-IDF weighting scheme, and (iii) the GP approach proposed by Fan et al. in [9]. Due to space restrictions, we report only the two best methods per collection  X  BM25 and FAN-GP for TREC-8, and TF-IDF and FAN-GP for WBR99. TF-IDF outperformed BM25 for the WBR99 collection, probably due to the way that the relevance information was obtained. Despite the fact that Fan et al. X  X  original work had not varied the tree depth, we allowed the depth to vary between 3 and 12, as in our ap-proach, in order to ensure a fair comparison. The remaining GP parameters were set as defined in that work, except the mutation factor, which was set to 5%. Although Fan et al. ran the experiments without mutation, our experiments showed that this operation leads to better results. We chose the best ranking function discovered by Fan et al. X  X  approach (henceforth named FAN-GP) to use as a baseline. This se-lection of the best ranking function was done as described in Section 3.1, due to the AVG  X  and SUM  X  selection meth-ods leading to ranking functions at least as good as the best validation individuals.

To evaluate the performance of our approach against the baselines, we used, as in [24], the (non-interpolated) average precision measure over all relevant documents, the precision
The protected logarithm function, used to prevent numeric overflow, returns zero if its ar gument is zero and otherwise returns the natural logarithm of the absolute value of its argument [11]. at 9 document cutoff values, and R-precision. We also plot all retrieval results in precision-recall curves. In this section we present the results of our experiments. The best results were obtained by ranking functions discov-ered using the SUM  X  method to select the best individuals, and maximum tree depths of 5 and 8 for the TREC-8 and WBR99 collections, respectively. We report the best in-dividuals for TREC-8 considering their global performance. For WBR99, we considered the best performance overall and in the top of the ranking, an important aspect for a Web col-lection 3 . The best ranking functions discovered by CCA for the TREC-8 and WBR99 collections are shown in Figures 3 and Figure 4, respectively. Figure 3: CCA discovered ranking function for the TREC-8 collection Figure 4: CCA discovered ranking function for the WBR99 collection
Each discovered ranking function had one terminal related to term frequency component for the queries  X  t 19 or t 20  X  and at least three different terminals related to collection frequency  X  t 06 , t 07 , t 08 , t 09 , t 10 ,or t 11 .Figure3shows parts of the BM25 ranking function, t 05 , t 18 ,and t 19 ure 4 presents general TF-IDF components such as t 01 , t t ,and t 12 . This shows that CCA was able to reuse good components of the best baseline function for each collection and to combine them with others to generate better ranking functions.

Figure 5 displays the evolution process after 30 genera-tions. For each generation, we evaluate the best 20 indi-viduals sorted according to their performance, calculated through the fitness function. As we can see, CCA rank-ing functions converge faster than FAN-GP. The figures also show that the CCA curves beha ve very similarly. Despite the fact that training, validation, and test sets present differ-ent fitness values, validation and test curves tend to follow the training behavior. In the FAN-GP curve, test and val-idation curves do not follow the training behavior, which suggests overfitting. The gap between the training curve and the others also indicates overfitting  X  the greater the gap, the higher the overfitting. As we can see, the CCA curves are also closer than FAN-GP. In fact, contrary to TREC-8, the best global individual in WBR99 was not the best on the top of the ranking. Figure 6 shows the 11-point average precision figures for CCA and the baselines on TREC-8 and WBR99 collections. We notice that CCA yields better precision values than TF-IDF, BM25 and FAN-GP throughout almost all recall levels.
Table 2 presents detailed average precision figures at dif-ferent ranking levels. As we can see, CCA yields better pre-cision than the BM25 and FAN-GP approaches for TREC-8 and better than TF-IDF and FAN-GP for the WBR99 col-lection. For the TREC-8 collection, CCA reached an average precision of 16.40%, corresponding to gains of 40.87% over BM25 and 14.00% over FAN-GP. For the WBR99 collection, CCA achieved an average precision of 16.68%, corresponding to gains of 21.67% over TF-IDF and 24.85% over FAN-GP. We also observe that FAN-GP did not surpass the TF-IDF results for the WBR99 collection. All the CCA performance improvements related to average precision were statistically significant at p&lt; 0 . 1 for TREC-8 and p&lt; 0 . 05 for WBR99. Table 2 shows the confidence levels (1-p ) obtained by the pair-wise t-tests.
 We also observe in Table 2 that, for the top 5 documents, CCA improves BM25 and FAN-GP by 18.52% on TREC-8, and improves TF-IDF and FAN-GP by 59.10% and 2.94% on WBR99. Our CCA approach also attains improvements in R-precision of more than 26% over BM25 and 15% over FAN-GP for TREC-8, and more than 8% over TF-IDF and 12% over FAN-GP for WBR99.

Figure 7 presents the average precision for each query. For the TREC-8 collection, we observe that our CCA results im-prove the performance over FAN-GP in some topics where it had no gain over BM25, such as topics 438, 443, 445, 449, and 450. Figure 7 also shows that CCA X  X  ranking functions only present worse performance than BM25 and FAN-GP in topics where mean average precision is already very low, such as 432, 435, 437, 440, and 448. For the WBR99 collec-tion, CCA obtains better results than the baselines for the queries 31, 32, 33, 34, 44, 47, 48, 49, and 50. CCA stands next to the best results for the queries 39, 40, and 46. Our approach does not surpass FAN-GP for the queries 37, 39, 41, 42, 43, and 45.

Considering the experiment with the depth that produced our best individual for each collection, more than 30 different individuals discovered by CCA surpassed the baselines for TREC-8 and WBR99, which indicates that CCA is able to find several good ranking functions.
Different approaches to discover ranking functions based on machine learning techniques, such as genetic algorithms and genetic programming, have been proposed in the lit-erature. Fan et al. [7] proposed a new approach to au-tomatically generate term weighting strategies for differ-ent contexts, based on genetic programming (GP). They argue that each specific context demands a different term weighting strategy, that is, a ranking function should be adapted to different document collections and users. In their works [8 X 10] they have demonstrated that GP has been ef-fective at improving the perform ance of information retrieval tasks. In [5], they also study the effect of different utility functions, i.e., functions that privilege the retrieval of rel-evant documents on the top of the ranking, when used as fitness functions. In all these works, terminals are based on basic statistical information of the collection, documents and queries, such as term frequencies ( tf ), total number of documents, length in bytes of a document, number of unique terms in a document, etc. In [8], Fan et al. compare FAN-GP to a learning approach based on neural networks. Their results showed that FAN-GP overcome substantially the neural network strategy. I n [10], FAN-GP exploits struc-tural information of Web documents. Their approach was compared to one based on Support Vector Machines (SVM). FAN-GP manages to improve SVM for both ad hoc and routing tasks in retrieval.

The work in [22] also presents a GP approach, based on statistical information of the collection, documents, and queries. Additionally, and unlike the works of Fan et al., this work adds the baseline functions, such as those in [18,20,21] as individuals in the initial population. According to the author, this addition guarantees that the worst possible per-formance during the training phase is at least as good as the best baseline of the functions added. The fact that Trotman has chosen to represent individuals (including the baselines) with simple statistical information means that he could not guarantee the integrity of the baseline components, since these could be very distorted or completely destroyed by the evolutionary process. In fact, the best individuals re-ported by Trotman did not contain components of the base-lines. CCA instead guarantees the preservation of these good building blocks and the knowledge they carry, hav-ing (indirectly) outperformed Trotman X  X  approach. When applied to a specific collection (FBIS), Trotman was able to achieve an improvement of 32.90% in MAP. This, how-ever, was described as an atypical result. The average gain for all tested collections in Trotman X  X  best run was around 8%, with negative gains for some collections. Our results surpassed BM25 by 40.87% for TREC-8, suggesting a sub-stantial improvement over Trotman X  X  approach. Finally, dif-ferent from CCA, in which we advocate a collection-based approach, Trotman has the goal of finding ranking functions good for all colections.

In [13], Oren explores several tf -idf functions generated by a GP approach. His work uses statistical information of the collection, idf , and two instances of tf as terminals. However, his improvements over a basic tf -idf strategy were not significant.

In contrast to all of these aforementioned works, our ap-proach uses parts of well-known, significant, and proven ef-fective ranking formulas as terminals, for representing term-weighting components, instead of simple statistical informa-tion. As mentioned before, our hypothesis is that providing richer components for GP to work with will allow the discov-ery of better final ranking formulas. In fact, our approach proved to be more stable and consistent in generating effec-tive ranking functions than previous work, which used only basic statistical information.

Other works that are also somewhat related to ours have focused on the combination of results from different ranking formulas. In [3], Bartell et al. show that retrieval effec-tiveness can be improved significantly by a combination of the results of a number of different retrieval algorithms (or experts ), since these emphasize different aspects of the doc-uments to be retrieved.
 Pathak et al. [14] introduces a method of utilizing Genetic Algorithms in Information Retrieval tasks. Specifically, it shows how GA can be used to adapt several matching func-tions that are used to match documents descriptions with query descriptions. Vogt et al. [23] propose a linear com-bination model for fusion of ranking results. This model combines the result lists of multiple IR systems by scoring each document using a weighted sum of the scores from each of the component systems. The work in [6] also uses Genetic Algorithms to combine different expert matching functions. The weights associated with combinations are evolved us-ing GA. Our approach differs from these, since we do not combine ranking formulas or result lists, but components or portions of ranking functions fro m different retrieval systems to generate a completely new ranking formula.
In this paper we have introduced a new approach based on the combination of term weighting components, extracted from well-known information retrieval ranking formulas, us-ing genetic programming. We show that our Combined Com-ponent Approach (CCA) improves the retrieval performance compared to standard TF-IDF, BM25 and other GP-based approaches [9, 22] that use only basic statistical informa-tion derived from collections and documents. We used the TREC-8 and WBR99 collections to validate our approach.
Using the TREC-8 collection, our experiments showed that CCA leads to significant improvements in retrieval ef-fectiveness. Mean average precision was improved by 40.87% and 14.00% over BM25 and FAN-GP respectively. R-precision and precision at the top of the ranking improvements were also observed. For the WBR99 collection, we obtained im-provements of 21.67% and 24.85% in mean average precision over TF-IDF and FAN-GP, respectively.
 Examining the CCA evolution process, we observed that CCA ranking functions converged faster than FAN-GP. Our approach also reduced overfitting. Results obtained by CCA lead us to conclude that the use of meaningful terminals instead of simple statistical information improves the quality of the process of discovering ranking functions with GP.
For future work, we will investigate ranking formulas from other IR models such as the Set-based Model [15] to extract new terminals. We will utilize the structural information within documents to compare our approach to others, such as [10], for Web search. We also will investigate the influence of the mutation factor, tree depth, and population length on the discovery of good ranking functions, considering the use of meaningful terminals and statistical information. Finally, but not less important, we also intend to examine closely the discovered best ranking functions to understand better how they work and the reasons for their effectiveness.
Special thanks go to Prof. Edward Fox for his very help-ful comments. This work is partially supported by projects GERINDO (CNPq/CT-INFO 552.087 /02-5), 5S-VQ (CNPq /CT-INFO 55.1013/2005-2), FCT project ref. POSC/EIA/ 58194/2004, GRICES/CNPq bilateral cooperation project  X  X DAPTINF X , by grant 10023 -UFMG/RTR/PRPQ/RE-CEM-DOUTORES/04 (sub-grant 32 -PROGRAMACAO GENETICA), and by an individual grant from CNPq to Marcos Andr  X  eGon  X  calves.
