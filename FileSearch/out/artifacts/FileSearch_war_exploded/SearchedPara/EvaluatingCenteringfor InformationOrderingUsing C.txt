 University of Cambridge University of Essex
In this article we discuss several metrics of coherence defined using centering theory and a general methodology applied on several corpora. Our main result is that the simplest metric for the development of both text-to-text and concept-to-text generation systems. 1. Introduction
Information ordering (Barzilay and Lee 2004), that is, deciding in which sequence to present a set of preselected information-bearing items, has received much attention in recent work in automatic text generation. This is because text generation systems need to organize the content in a way that makes the output text coherent , that is, easy to read and understand. The easiest way to exemplify coherence is by arbitrarily reordering the sentences of a comprehensible text. This process very often gives rise to documents that do not make sense although the information content is the same before and after the reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000).
 relate subsequent clauses in the text, is an important aspect of textual organization.
Since the early 1980s, when it was first introduced, centering theory has been an influential framework for modelling entity coherence. Seminal papers on centering such as Brennan, Friedman [Walker], and Pollard (1987, page 160) and Grosz, Joshi, and
Weinstein (1995, page 215) suggest that centering may provide solutions for information ordering.
 generation exploits constraints on entity coherence to organize information (Mellish et al. 1998; Kibble and Power 2000, 2004; O X  X onnell et al. 2001; Cheng 2002; Lapata 2003; Barzilay and Lee 2004; Barzilay and Lapata 2005, among others). Although these approaches often make use of heuristics related to centering, the features of entity coherence they employ are usually defined informally. Additionally, centering-related features are combined with other coherence-inducing factors in ways that are based mainly on intuition, leaving many equally plausible options unexplored.
 is centering for information ordering in text generation? (ii) Which aspects of centering are most useful for this purpose? These are the issues we investigate in this paper, which presents the first systematic evaluation of centering for information ordering. To do this, we define centering-based metrics of coherence which are compatible with several extant information ordering approaches. An important insight of our work is that centering can give rise to many such metrics of coherence. Hence, a general methodology for identifying which of these metrics represent the most promising candidates for information ordering is required.
 demonstrate the portability and generality of our evaluation methods by experimenting with several corpora. Our main result is that the simplest metric (which relies exclusively on NOCB transitions) sets a baseline that cannot be outperformed by other metrics that make use of additional centering-related features. Thus, we provide substantial insight into the role of centering as an information ordering constraint and offer researchers working on text generation a simple, yet robust, baseline to use against their own information ordering approaches during system development.
 approach in relation to other work on text generation. After a brief introduction to centering in Section 3, Section 4 demonstrates how we derived centering data structures from existing corpora. Section 5 discusses how centering can be used to define various metrics of coherence suitable for information ordering. Then, Section 6 outlines a corpus-based methodology for choosing among these metrics. Section 7 reports on the results of our experiments and Section 8 discusses their implications.
We conclude the paper with directions for future work and a summary of our main contributions. 1 2. Information Ordering
Information ordering has been investigated by substantial recent work in text-to-text generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and
Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al. 2007, among others) as well as concept-to-text generation (particularly Kan and McKeown [2002] and Dimitromanolaki and Androutsopoulos 2003). 2 We added to this work by presenting approaches to information ordering based on a genetic algorithm (Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and Koller 2004) which can be applied to both concept-to-text and text-to-text generation.
These approaches use a metric of coherence defined using features derived from 30 article.
 1998; Kibble and Power 2000, 2004; Cheng 2002). With the exception of Kibble and
Power X  X  work, the features of entity coherence used in these metrics are informally specified by combining these features with other coherence-inducing factors such as rhetorical relations (Mann and Thompson 1987). However, as acknowledged in most of this work, these are preliminary computational investigations of the complex interactions between different types of coherence which leave many other equally plausible combinations unexplored.
 devising more complicated metrics. To address this question, we define metrics which are purely centering-based, placing any attempt to specify a more elaborate model of coherence beyond the scope of this article. This strategy is similar to most work on centering for text interpretation in which additional constraints on coherence are not taken into account (the papers in Walker, Joshi, and Prince [1998] are characteristic examples). This simplification makes it possible to assess for the first time how useful the employed centering features are for information ordering.
 relation. However, ELABORATION has been characterized as  X  X he weakest of all rhetorical relations X  (Scott and de Souza 1990, page 60). Knott et al. (2001) identified several theoretical problems all related to ELABORATION and suggested that this relation be replaced by a theory of entity coherence for text generation. Our work builds on this suggestion by investigating how appropriate centering is as a theory of entity coherence for information ordering.
 organize information for text generation. McKeown X  X   X  X onstraints on immediate focus X  (which are based on the model of entity coherence that was introduced by Sidner [1979] and precedes centering) are embedded within the schema-driven approach to generation which is rather domain-specific (Reiter and Dale 2000). By contrast, our metrics are general and portable across domains and can be applied within information ordering approaches which are applicable to both concept-to-text and text-to-text generation. 3. Centering Overview
This section provides an overview of centering, focusing on the aspects which are most closely related to our work. Poesio et al. (2004) and Walker, Joshi, and Prince (1998) discuss centering and its relation to other theories of coherence in more detail. ranked list of forward looking centers (i.e., discourse entities) denoted as CF(U members of CF(U n ) must be realized by the NPs in U n (Brennan, Friedman [Walker], and Pollard 1987). The first member of CF(U n ) is called the preferred center CP(U n ).

CB(U n ) is defined as the most highly ranked member of CF(U to CF(U n ). CF lists prior to CF(U n  X  1 ) are not taken into account for the computation of CB(U n ). The original formulations of centering by Brennan, Friedman [Walker], and
Pollard (1987) and Grosz, Joshi, and Weinstein (1995) lay emphasis on the uniqueness and the locality of the CB and will serve as the foundations of our work.
 adjacent utterances (Table 1). This definition of transitions is based on Brennan, Friedman [Walker], and Pollard (1987) and has been popular with subsequent work. There exist several variations, however, the most important of which comes from Grosz,
Joshi, and Weinstein (1995), who define only one SHIFT transition. is known as Rule 2. Rule 2 states that CONTINUE is preferred to RETAIN , which is preferred to SMOOTH -SHIFT , which is preferred to ROUGH -SHIFT . Although the
Rule was introduced within an algorithm for anaphora resolution, Brennan, Friedman [Walker], and Pollard (1987, page 160) consider it to be relevant to text generation too. Grosz, Joshi, and Weinstein (1995, page 215) also take Rule 2 to suggest that text generation systems should attempt to avoid unfavorable transitions such as is that CF(U n ) should contain at least one member of CF(U as the principle of CONTINUITY (Karamanis and Manurung 2002). Although Grosz,
Joshi, and Weinstein and Brennan, Friedman [Walker], and Pollard do not discuss the effect of violating CONTINUITY , Kibble and Power (2000, Figure 1) define the additional transition NOCB to account for this case. Different types of NOCB transitions are introduced by Passoneau (1998) and Poesio et al. (2004), among others. Other (Miltsakaki and Kukich 2004).
 (see Table 1). To improve the way centering resolves pronominal anaphora, Strube and Hahn (1999) introduced a fourth principle called CHEAPNESS and defined it as
CB(U n ) = CP(U n  X  1 ). They also redefined Rule 2 to favor transition pairs which satisfy 32 over every other centering principle in Strube and Hahn X  X  model.
 the introduction of the various principles, parameters such as  X  X tterance, X   X  X anking, X  and  X  X ealization X  can also be specified in several ways giving rise to different instantiations of centering (Poesio et al. 2004). The following section discusses how these parameters were defined in the corpora we deploy. 4. Experimental Data
We made use of the data of Dimitromanolaki and Androutsopoulos (2003), the GNOME corpus (Poesio et al. 2004), and the two corpora that Barzilay and Lapata (2005) experimented with. In this section, we discuss how the centering representations we utilize were derived from each corpus. 4.1 The MPIRO-CF Corpus
Dimitromanolaki and Androutsopoulos (2003, henceforth D&amp;A) derived facts from the database of the MPIRO concept-to-text generation system (Isard et al. 2003), realized them as sentences, and organized them in sets. Each set consisted of six facts which were ordered by a domain expert. The orderings produced by this expert were shown to be very close to those produced by two other archeologists (Karamanis and Mellish 2005b).
 to us by D&amp;A. We computed a CF list for each fact in each ordering by applying the instantiation of centering introduced by Kibble and Power (2000, 2004) for concept-to-text generation. That is, we took each database fact to correspond to an  X  X tterance X  and specified the  X  X ealization X  parameter using the arguments of each fact as the members of the corresponding CF list. Table 2 shows the CF lists, the CBs, the centering transitions, and the violations of CHEAPNESS for the following example from
MPIRO-CF:
MPIRO facts consist of two arguments, the first of which was specified as the CP following the definition of  X  X F ranking X  in O X  X onnell et al. (2001). second argument can often be an entity such as en914 that is realized by a canned phrase of significant syntactic complexity ( a warrior performing splachnoscopy before leaving for the battle ). Moreover, the deployed definition of  X  X ealization X  is similar to what Grosz,
Joshi, and Weinstein (1995) call  X  X irect realization, X  which ignores potential bridging relations (Clark 1977) between the members of two subsequent CF lists. These relations are typically not taken into account for information ordering and were not considered in any of the deployed corpora. 4.2 The GNOME-LAB Corpus
We also made use of the GNOME corpus (Poesio et al. 2004), which contains object descriptions (museum labels) reliably annotated with features relevant to centering. The motivation for this study was to examine whether the phenomena observed in
MPIRO-CF (which is arguably somewhat artificial) also manifest in texts from the same genre written by humans without the constraints imposed by a text generation system.
 20 such texts in GNOME, which were published in a book and a museum Web site (and were thus taken to be coherent). The following example is a characteristic text from this subcorpus (referred to here as GNOME-LAB):
The GNOME corpus provides us with reliable annotation of discourse units (i.e., clauses and sentences) that can be used for the computation of  X  X tterance X  and of
NPs which introduce entities to the CF list. Each feature was marked up by at least two annotators and agreement was checked using the  X  statistic on part of the corpus.
 computed the CF lists from the units that seemed to correspond more closely to MPIRO facts. So instead of using sentence for the definition of  X  X tterance, X  we followed most work on centering for English and computed CF lists from GNOME X  X  finite units . 34 text spans with the indexes (a) to (d) in Example (2) are examples of such units. Units such as (2a) are as simple as the MPIRO-generated sentence (1a), whereas others appear to be of similar syntactic complexity to (1d). On the other hand, the second sentence in
Example (2) consists of two finite units, namely (b) and (c), and appears to correspond to higher degrees of aggregation than is typically seen in an MPIRO fact. The texts in GNOME-LAB consist of 8.35 finite units on average.
 violations of CHEAPNESS for Example (2). Note that the same entity (i.e., de374 )isused to denote the referent of the NP Item 144 in (2a) and its in (2b), which is annotated as coreferring with Item 144 . All annotated NPs introduce referents to the CF list (which often contains more entities than in MPIRO), but only direct realization is used for the computation of the list. This means that, similarly to the MPIRO domain, bridging relations between, for example, it in (2c) and the terminals in (2d), are not taken into account.
 linear order, which is a robust way of estimating  X  X F ranking X  in English (Poesio et al. 2004). In this instantiation, the CP corresponds to the referent of the first NP within the unit that is annotated as a subject or as the post-copular NP in a there -clause. 4.3 The NEWS and ACCS Corpora
Barzilay and Lapata (2005) presented a probabilistic approach for information ordering which is particularly suitable for text-to-text generation and is based on a new representation called the entity grid . A collection of 200 articles from the North American News Corpus (NEWS) and 200 narratives of accidents from the National Transportation
Safety Board database (ACCS) was used for training and evaluation. Example (3) presents a characteristic text from the NEWS corpus:
Barzilay and Lapata automatically annotated their corpora for the grammatical function of the NPs in each sentence (denoted in the example by the subscripts S, O, and
X for subject, object, and other, respectively) as well as their coreferential relations (which do not include bridging references). More specifically, they used a parser (Collins 1997) to determine the constituent structure of the sentences from which the grammatical function for each NP was derived. 6 Coreferential NPs such as Microsoft
Corp. and the company in (3a) were identified using the system of Ng and Cardie (2002).
 referents across sentences in the text using the aforementioned symbols for their grammatical role and the symbol  X   X   X  for a referent that does not occur in a sentence.
Table 4 illustrates a fragment of the grid for the sentences in Example (3). considerably more elaborate than centering. To derive an appropriate instantiation of centering for our investigation, we compute a CF list for each grid row using the referents with the symbols S, O, and X. These referents are ranked according to their grammatical function and their position in the text. This definition of  X  X F ranking X  is similar to the one we use in GNOME-LAB. For instance, department is ranked higher than microsoft in CF(3a) because the Justice Department is mentioned before Microsoft
Corp. in the text. The derived sequence of CF lists is used to compute the additional centering data structures shown in Table 5.

As we explain in the next section, our centering-based metrics of coherence can be 36 deployed directly on unseen texts, so we treated all texts in NEWS and ACCS as test data. 8 5. Computing Centering-Based Metrics of Coherence
Following our previous work (Karamanis and Manurung 2002; Althaus, Karamanis, and Koller 2004), the input to information ordering is an unordered set of information-bearing items represented as CF lists. A set of candidate orderings is produced by creating different permutations of these lists. A metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output. 9 on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5, is a candidate ordering. Table 6 summarizes the NOCB s, the violations of COHERENCE , according to M.NOCB, the metric used by Karamanis and Manurung (2002) and
Althaus, Karamanis, and Koller (2004), is 2. Another ordering with fewer NOCB s(should such an ordering exist) will be preferred over this candidate as the selected output of information ordering if M.NOCB is used to guide this process. M.NOCB relies only on every other centering feature, M.NOCB is the simplest possible centering-based metric and will be used as the baseline in our experiments.
 important centering feature for anaphora resolution. We are interested in assessing how suitable M.CHEAP, a metric which utilizes CHEAPNESS , is for information ordering. according to M.CHEAP is 2. 11 If another candidate ordering with fewer violations of
Friedman [Walker], and Pollard (1987). The first score to be computed by M.BFP is the sum of CONTINUE transitions, which is 2 for the candidate ordering according to
Table 6. If this ordering is found to score higher than every other candidate ordering for the number of CONTINUE s, it is selected as the output. If another ordering is found to have the same number of CONTINUE s, the sum of RETAIN s is examined, and so forth for the other two types of centering transitions. 12 system, sums up the NOCB s as well as the violations of CHEAPNESS , COHERENCE , and SALIENCE , preferring the ordering with the lowest total cost. In addition to the violations of CONTINUITY and CHEAPNESS , the candidate ordering also violates lower score (if any) will be preferred by this metric. Although Kibble and Power (2004) introduced a weighted version of M.KP, the exact weighting of centering X  X  principles remains an open question, as argued by Kibble (2001). This is why we decided to experiment with M.KP instead of its weighted variant.
 section as the most appropriate starting point for experimentation. We would like to emphasize, however, that these are not the only possible options. Indeed, similarly to the various ways in which centering X  X  parameters can be specified, there exist many other ways of using centering to define metrics of entity coherence for information ordering. These possibilities arise from the numerous other definitions of centering X  X  transitions and the various ways in which transitions and principles can be combined.
These are explored in more detail in Karamanis (2003, Chapter 3), which also provides a formal definition of the metrics discussed previously. 6. Evaluation Methodology
Because using naturally occurring discourse in psycholinguistic studies to investigate coherence effects is almost infeasible, computational corpus-based experiments are 38 often the most viable alternative (Poesio et al. 2004; Barzilay and Lee 2004). Corpus-based evaluation can be usefully employed during system development and may be later supplemented by less extended evaluation based on human judgments as suggested by Lapata (2006).
 framework. This methodology is based on the premise that the original sentence order (OSO, Barzilay and Lee 2004) observed in a corpus text is more coherent than any other ordering. If a metric takes an alternative ordering to be more coherent than the OSO, it has to be penalized.
 rate which is computed according to the formula: Better(M,OSO) + Equal(M,OSO)/2.
Better(M,OSO) stands for the percentage of orderings that score better than the OSO according to a metric M, and Equal(M,OSO) is the percentage of orderings that score equal to the OSO. 13 This measure provides an indication of how likely a metric is to lead to an ordering different from the OSO. When comparing several metrics with each other, the one with the lowest classification error rate is the most appropriate for ordering the sentences that the OSO consists of. In other words, the smaller the classification error rate, the better a metric is expected to perform for information ordering. The average classification error rate is used to summarize the performance of each metric in a corpus.
 classify each alternative ordering as scoring better, equal, or worse than the OSO according to M. When the number of CF lists in the OSO is fairly small, it is feasible to search through all possible orderings. For OSOs consisting of more than 10 CF lists, the classification error rate for the entire population of orderings can be reliably estimated using a random sample of one million permutations (Karamanis 2003,
Chapter 5). 7. Results
Table 7 shows the average performance of each metric in the corpora employed in our experiments. The smallest X  X hat is, best X  X core in each corpus is printed in boldface. The table indicates that the baseline M.NOCB performs best in three out of four corpora.
M.CHEAP, M.KP, and M.BFP in each corpus are reported in Table 8. The exact number of texts for which the classification error rate of M.NOCB is lower than its competitor for each comparison is reported in the columns headed by  X  X ower. X  For instance, M.NOCB has a lower classification error rate than M.CHEAP for 110 (out of 122) texts from
MPIRO-CF. M.CHEAP achieves a lower classification error rate for just 12 texts, and there do not exist any ties, that is, cases in which the classification error rate of the two metrics is the same.
 of texts in each corpus, rounded to the third decimal place, is also reported. respect to the exemplified comparison of M.NOCB against M.CHEAP in MPIRO-CF, the p value is lower than 0.001 after rounding. This in turn means that M.NOCB than M.CHEAP. In other words, M.NOCB outperforms M.CHEAP significantly in this corpus.
 12 cases. 15 In the remaining two comparisons, the difference in performance between
M.NOCB and M.BFP is not significant (p &gt; 0.05). However, this does not constitute evidence against M.NOCB, the simplest of the investigated metrics. In fact, because
M.BFP fails to outperform the baseline, the latter may be considered as the most promising solution for information ordering in these cases too by applying Occam X  X  razor. Thus, M.NOCB is shown to be the best performing metric across all four corpora. 40 8. Discussion
Our experiments show that M.NOCB is the most suitable metric for information ordering among the metrics we experimented with. Despite the differences between our corpora (in genre, average length, syntactic complexity, number of referents in the CF list, etc.), M.NOCB proves robust across all four of them. It is also the most appropriate metric to use in both application areas we relate our corpora to, namely concept-to-text (MPIRO-CF and GNOME-LAB) as well as text-to-text (NEWS and ACCS) generation.
These results indicate that when purely centering-based metrics are used, simply avoiding NOCB s is more relevant to information ordering than the combinations of additional centering features that the other metrics make use of.
 including the corpus-based investigation of centering by Poesio et al. (2004); discuss the implications of our findings for text generation; and summarize our contributions. 8.1Recent Evaluation Studies in Information Ordering
There has been significant recent work on the corpus-based evaluation for information ordering. In this section, we discuss the methodological differences between our work and the studies which are most closely related to it.
 which computes the probability of generating the OSO and every alternative ordering. Then, all orderings are ranked according to this probability and the rank given to the
OSO is retrieved. Several evaluation measures are discussed, the most important of which is the average OSO rank , that is, the average rank of the OSOs in their corpora.
This measure does not take into account that the OSOs differ in length. However, this information is necessary to estimate reliably the performance of an information ordering approach, as we discuss in Karamanis and Mellish (2005a) in more detail.
 measure called ranking accuracy which expresses the percentage of alternative orderings that are ranked lower than the OSO. In Karamanis X  X  (2003) terms, ranking accuracy equals 100%  X  Better ( M , OSO ), assuming that no equally ranking orderings exist. often sampled out of several millions. On the other hand, Barzilay and Lee (2004) enumerate exhaustively each possible ordering, which might become impractical as the search space grows factorially. We overcame these problems by using a large random sample for the texts which consist of more than 10 sentences as suggested in Karamanis (2003, Chapter 5). Equally important is the emphasis we placed on the use of statistical tests, which were not deployed by either Barzilay and Lee or Barzilay and Lapata. orderings on the basis of their distance from observed sentence orderings in a corpus.
A measure of rank correlation (called Kendall X  X   X  ), which was subsequently shown to correlate reliably with human ratings and reading times (Lapata 2006), was used to estimate the distance between orderings. orderings, we measure how likely a metric is to lead to an ordering different than the
OSO. Taking into account more than one OSO for information ordering is the main strength of Lapata X  X  method, but to do this one needs to ask several humans to order the same set of sentences (Madnani et al. 2007). Karamanis and Mellish (2005b) conducted an experiment in the MPIRO domain using Lapata X  X  methodology which supplements the work reported in this article. However, such an approach is less practical for much larger collections of texts such as NEWS and ACCS. This is presumably the reason why
Barzilay and Lapata (2005) use ranking accuracy instead of  X  in their evaluation. 8.2 Previous Corpus-Based Evaluations of Centering
Our work investigates how the coherence score of the OSO compares to the scores of alternative orderings of the sentences that the OSO consists of. As Kibble (2001, page 582) noticed, this question is crucial from an information ordering viewpoint, but was not taken into account by any previous corpus-based study of centering. Grosz,
Joshi, and Weinstein (1995, page 215) also suggested that Rule 2 should be tested by examining  X  X lternative multi-utterance sequences that differentially realize the same content. X  We are the first to have pursued this research objective in the evaluation of centering for information ordering.
 every instantiation of centering they tested and concluded that centering is inadequate as a coherence model. 17 However, the frequency of NOCB s does not necessarily provide adequate indication of how appropriate NOCB s (and centering in general) are for information ordering. Although over 50% of the transitions in GNOME-LAB are NOCB s, the average classification error rate of approximately 20% for M.NOCB suggests that the
OSO tends to be in greater agreement with the preference to avoid NOCB s than 80% of the alternative orderings. Thus, it appears that the observed ordering in the corpus does optimize with respect to the number of potential NOCB s to a great extent. 8.3 A Simple and Robust Baseline for Text Generation
How likely is M.NOCB to come up with the attested ordering in the corpus (the OSO) if it is actually used to guide an algorithm that orders the CF lists in our corpora? performance of M.NOCB varies across the corpora from about 15.5% (ACCS) to 30.9% (NEWS). We attribute this variation to the aforesaid differences between the corpora.
Notice, however, that these differences affect all metrics in a similar way, not allowing for another metric to significantly outperform M.NOCB.
 approximately one out of six alternative orderings on average are taken to be more coherent than the OSO. Given the average number of sentences per text in this corpus 42 (11.5), this means that several millions of alternative orderings are often taken to be more coherent than the gold standard.
 best sentence ordering method in ACCS. This corresponds to an average classification error rate of 12.7% (assuming that there are no equally scoring orderings in their evaluation; see Section 8.1). This is equal to an improvement of just 2.8% over the performance of our baseline metric (15.5%) using a coherence model which is substantially more elaborate than centering. However, it is in NEWS (for which
M.NOCB returns its worst performance of 30.9%) that this model shows its real strength, approximating an average classification error rate of 9.6%, which corresponds to an improvement of 21.3% over our baseline. We believe that the experiments reported in this article put the studies of our colleagues in better perspective by providing a reliable baseline to compare their metrics against. 8.4 Moving Beyond Centering-Based Metrics
Following McKeown (1985), Kibble and Power argue in favor of an integrated approach for concept-to-text generation in which the same centering features are used at different stages in the generation pipeline. However, our study suggests that features such as ordering. The poor performance of these features can be explained by the fact that they were originally introduced to account for pronoun resolution rather than information ordering. CONTINUITY , on the other hand, captures a fundamental intuition about entity coherence which constitutes part of several other discourse theories. relatively high classification error rates for M.NOCB, which needs to be supplemented with other coherence-inducing factors in order to be used in practice. This verifies the premises of researchers such as Kibble and Power who a priori use features derived from centering in combination with other factors in the definition of their metrics. Our work should be quite helpful for that effort too, suggesting that M.NOCB is a better starting point for defining such metrics than M.CHEAP or M.KP. 9. Conclusion
In conclusion, our analysis sheds more light on two previously unaddressed questions in the corpus-based evaluation of centering: (i) which aspects of centering are most relevant to information ordering and (ii) to what extent centering on its own can be transitions (M.NOCB) sets a baseline that cannot be outperformed by other coherence metrics which make use of additional centering features. Although this metric does not perform well enough to be used on its own, it constitutes a simple, yet robust, baseline against which more elaborate information ordering approaches can be tested during system development in both text-to-text and concept-to-text generation.
 of possible centering-based metrics one may investigate whether a different metric can outperform M.NOCB in any corpus or application domain. M.NOCB can also serve as the starting point for the definition of more informed metrics which will incorporate additional coherence-inducing factors. Finally, given that we used the instantiation of centering which seemed to correspond more closely to the targeted application domains, the extent to which computing the CF list in a different way may affect the performance of the metrics is another question to explore in future work.
 Acknowledgments References 44
