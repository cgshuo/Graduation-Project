 The discovery of functional dependencies in a dataset is of great importance for database redesign, anomaly detection and data cleansing applications. However, as the nature of the problem is exponential in the number of attributes none of the existing approaches can be applied on large datasets. We present a new algorithm Dfd for discovering all functional dependencies in a dataset following a depth-first traversal strategy of the attribute lattice that combines aggressive pruning and efficient result verification. Our ap-proach is able to scale far beyond existing algorithms for up to 7.5 million tuples, and is up to three orders of magnitude faster than existing approaches on smaller datasets. H.2.0 [ Database Management ]: General functional dependency, algorithms, lattice traversal
Functional Dependencies ( Fd s) are among the most rele-vant dependencies in relational databases. Despite their rel-evance for normalization of relations, Fd constraints gained importance for data quality and data cleaning applications. In particular, they can resemble integrity constraints [5] or denial constraints [4], which can be used to detect and repair data violations.

The discovery of functional dependencies is a popular topic in the domain of data profiling [15]. Foundations regarding the general complexity of the problem, which is in  X (2 for m columns, were proven by Mannila et al. [14] in 1992. Based on those principles, several major papers concern-ing FD discovery were published between 1999 and 2002. However, none of the approaches is applicable to realistic datasets with tens of attributes and millions of tuples. While further effort was put into the application area of Fd s [4, 7], by extending the Fd concept to more specialized constraints, such as approximate Fd s and conditional Fd s, research on the efficiency of Fd discovery approaches has been neglected.
Recent advances in the related domain of unique column combinations (composite keys) [10,17], however, showed that it is possible to achieve superior runtime behavior by ag-gressive pruning of the search space. We adapt the insights gained from that line of research and create a new efficient approach for the discovery of Fd s in a relational table. Next, we formally define relevant concepts and the computational challenge of Fd discovery and outline the contributions of this paper.
 Functional Dependencies. Given a relational database schemata R = { C 1 ,C 2 ,...C n } with a relational instance r  X  dom ( C 1 )  X  dom ( C 2 )  X  X  X  X  X  dom ( C n ), a functional de-pendency states that an attribute C k  X  R is determined by some other attributes X  X  R . This can be formally defined as follows:
Definition 1.1. Functional Dependency ( Fd ). Given a relational database schema R with a relational instance r . Let C k  X  R and X  X  R , then X  X  C k is a functional dependency over r iff X is called the left-hand side ( Lhs ) and C k is the right-hand side ( Rhs ) of the dependency.
 Accordingly we define a non-functional dependency ( non -Fd ) as follows: Definition 1.2. Non-Functional Dependency (Non-Fd ) Let C k  X  R and X  X  R , then X 6 X  C k is a non-Fd over r iff
Naturally, for any C k  X  X the trivial Fd X  X  C k holds and can be ignored during the discovery process. Further-more, the set of all Fd s can be generated based on the set of all minimal Fd s, which are defined as follows: Definition 1.3. Minimal FD. With C k  X  R and X,X 0  X  R the Fd X  X  C k is minimal iff  X  X 0  X  X : X 0 6 X  C k . The maximality of a non-Fd can be defined accordingly: Definition 1.4. Maximal non-FD. With C k  X  R and X,X 0  X  R the Fd X 6 X  C k is a maximal non-Fd iff
In general, the task of discovering all Fd s can be reduced to the task of discovering all minimal Fd s and all maxi-mal non-dependencies. While for any Fd X  X  C k , fur-ther non-minimal Fd s exist by appending the Lhs with fur-ther columns, for any non-Fd the Rhs is also non-dependent from all subsets of the non-dependency X  X  Lhs . In this pa-per, we show that similar to the key discovery problem [17], it is possible to leverage the complement relationship be-tween the set of minimal Fd s and the set of maximal non-dependencies.
 Contributions. We make the following contributions: 1. With Dfd we present a new Fd discovery approach for 2. Extensive experiments on multiple real world datasets The next section discusses and compares related work. In Section 3 we present our approach Dfd . In Section 4 we evaluate our approach and conclude in Section 5.
Functional dependency discovery approaches can be clas-sified into two main groups. The first group consists of algo-rithms that traverse the search space in a breadth-first, lev-elwise manner. The strategies of this group can be further divided into bottom-up, top-down, and hybrid approaches. The second class is represented by algorithms that traverse the search space depth-first.

The group of breadth-first levelwise algorithms consists of Tane [11], FUN [16], FDMine [19], and DepMiner [13]. They all use the Apriori algorithm for candidate genera-tion [2]. The key characteristic of the breadth-first bottom-up approach is that it works levelwise, whereas each level k contains only Lhs candidates for Fd s with a cardinality of k attributes. The results of each level are used to generate the left-hand side candidates of size k + 1. Such breadth-first approaches work well on datasets that contain Fd s on lower levels, where all minimal Fd s can be detected early and all of their supersets can be pruned (see Def. 1.3). However, if some minimal Fd s have high cardinalities, i.e., involve many LHS attributes, the algorithm has to check the can-didates of all levels until the highest level with a minimal Fd is reached. Furthermore, if some columns are never part of a minimal Fd , the algorithms might need to generate all supersets until an Fd is discovered. While we use some tech-niques that have already been proposed by Tane, we present a new traversal approach that avoids the verification of un-necessary column combinations by flexibly traversing both bottom-up and top-down in a depth-first manner.

Tane, FDMine, and FUN all rely on quite similar con-cepts. DepMiner follows a different approach than the classi-cal breadth-first approaches: It calculates covers of so-called agree sets to determine minimal dependencies in the un-derlying dataset. DepMiner generates the candidates in a breadth-first levelwise manner, which still leads to an ex-ponential runtime behavior depending on the number of columns. Furthermore, the calculation of agree sets has a quadratic time complexity depending on the number of tu-ples in the dataset. Despite the improved strategies to gen-erate the agree sets by Lopes et al. [13], this preprocessing step still has a significant impact on the runtime, because nowadays datasets with millions of tuples are common. Our approach uses the partition refinement approach proposed in [11], which scales linearly in the number of tuples.
The second group of approaches are algorithms that search the powerset lattice of the attributes in a depth-first manner. For a given Rhs column, they start with an individual col-umn as Lhs and append more columns until an Fd for the selected Rhs holds. Depth-first traversal algorithms have the advantage that they can identify the first dependency within n steps for n attributes. Although this approach suf-fers from the fact that the identified Fd is not necessarily minimal, it has the advantage that for any discovered non-Fd Lhs on the path to the discovered Fd all of its subsets can be pruned, because they also constitute non-Fd Lhs s. This category is represented by FastFDs [18], a tuple-oriented al-gorithm. It features many similarities with DepMiner [13]. Candidates for Lhs s are selected based on difference sets, which are similar to the agree sets of DepMiner. A differ-ence set of two tuples contains all attributes of the rela-tional schema in which the values of the tuples are distinct. Therefore, difference sets are the complements of agree sets. Similar to DepMiner [13] the computational complexity of this preprocessing step is quadratic in the number of tuples. Additionally, the number of possible covers grows exponen-tially with the number of attributes in the difference sets. Therefore, also the determination of minimal covers can be a costly processing step.

Flach et al. propose FDep, which uses machine learning techniques to infer functional dependencies based on the tu-ples of the underlying dataset [8]. Besides a na  X   X ve brute-force approach, the authors provide a top-down, bottom-up, and a bi-directional version of FDep. However, Wyss et al. have shown that FDep only performs well on small datasets [18]. FDep has the same drawbacks as FastFDs [18] and Dep-Miner [13], because it is based on pairwise tuple comparison.
In Section 4, we present a detailed evaluation that shows the effective pruning strategy of our algorithm Dfd comparative study with the most popular approaches Tane and FastFDs. After 2002 there were no papers that further improved the performance of Fd discovery. Instead, the re-search focus shifted from functional dependencies to approx-imate [9, 12] and conditional functional dependencies [3, 6]. DFD is a novel algorithm for the D iscovery of F unctional D ependencies. It recombines components from known ap-proaches for the discovery of Fd s and the discovery of unique column combinations. First, we describe our search strat-egy for efficient identification of relevant column combina-tions and then we describe data structures that ensure fast pruning and fast verification of Fd candidates.
For each relation, a powerset lattice can be constructed that represents all column combinations, as shown in Fig-ure 1. To identify all existing Fd s one has to traverse that lattice, pruning as many combinations as possible. Tane, for example, does this verification levelwise, which means for each k = 2 , 3 ,.., | R | it checks all combinations of size k at the same time beginning with k = 2. For each col-umn combination it checks which k  X  1 subsets functionally determine the remaining column. Whenever an Fd is dis-covered, Tane ignores all possible Fd s with a Lhs superset of the current Fd . For large k the generation of all possible candidates is a bottleneck. Especially for datasets where a small number of minimal Fd s exist at relatively high levels of the lattice, the breadth-first traversal of Tane results in a significant overhead for candidate generation.

We radically change the traversal of the lattice based on insights from the discovery of unique column combinations. Although, the problem of discovering all minimal unique column combinations is easier to solve than the problem of Fd discovery, we show that some advances in that field can greatly contribute to the Fd discovery problem. In particu-lar, our approach is inspired by the algorithms [10] and [17].
In general both concepts, unique column combinations and Fd s, are strongly related: functional dependencies can be seen as a generalization of the key concept for a relation. A unique column combination K  X  R does not contain any duplicate tuples [1]. Minimal unique column combinations are valid Lhs s of minimal Fd s that functionally determine values in all remaining columns R \ K . Thus, there are at least as many minimal Fd s in a dataset as there are mini-mal unique column combinations. In fact, unique discovery is a subproblem of determining all Fd s of a dataset. One major difference between unique column combinations and Fd s is that for discovering uniques we need to consider only column combinations as a whole. In contrast, Fd s consist of
Lhs and a Rhs column combination. Current algorithms manage this issue by considering all possible Lhs s and Rhs when checking a column combination. For breadth-first ap-proaches like Tane this is straight forward, because in any case only Lhs s of a certain size are considered at each level. However, traversing the lattice in a depth-first manner re-quires to identify decidable paths. If one node provides mul-tiple dependencies to check, the decision for the next traver-sal step might not be intuitively clear and result in multiple stacks of paths that have to be traced later.

To disentangle this situation, we decompose the lattice of all column combinations into multiple lattices, where each corresponds to one possible Rhs of the relation. Having a relation R = ( A,B,C,D ) for example, we create for the Rhs A the lattice that contains the subsets of the powerset of { B,C,D } to identify all Lhs s that functionally determine the column A . For our example R we create four lattices for the four possible Rhs s. In each lattice we can apply pruning for supersets of valid Fd Lhs s and subsets of non-Fd Lhs Iterating one by one over the Rhs attributes, Dfd is not only easier to understand, it also allows us to discard any redundant pruning data structures after each iteration step, as we show in the following.
Algorithm 1 illustrates the main loop of Dfd . Its input is the set of columns and their partitions. We describe the structure of these partitions in Section 3.7. They directly represent the data of the single columns of the underlying dataset. In lines 1 to 4, Dfd checks each attribute partition for uniqueness. Finding a unique attribute enables Dfd to infer a set of minimal Fd s without traversing the lattice. Each remaining attribute is then considered as a possible Rhs and findLHSs is called to create the corresponding pow-erset lattice and identify possible Lhs s.
 Algorithm 1 : The main loop of DFD.
 Data : All attributes A  X  R , relational instance r
Result : the set of minimal non-trivial functional foreach A  X  R do foreach RHS  X  R do return F r
Algorithm 2 shows how Dfd determines all minimal Fd s for the current Rhs attribute by classifying all possible In line 1, Dfd picks randomly a seed as an initial candidate. Initially the individual columns except the cur-rent Rhs candidate constitute the seeds. Subsequently, Dfd starts to traverse the search space beginning with the chosen Lhs and classifies each visited combination/node according to the following categories:
Dependency and non-dependency represent Lhs nodes that are neither minimal nor maximal, respectively. A candidate minimal dependency/maximal non-dependency is a column combination that still can be the Lhs of a minimal Fd or maximal non-Fd . In case the current column combination was already traversed at an earlier stage of the process, examines only those column combinations again that are classified as candidates (line 6). In lines 8 and 11, checks whether the category of a candidate can be changed or not. It could be that after revisiting a candidate mini-mal dependency all of the subsets of that node have been classified as non-dependency, making the candidate a min-imal dependency. That is why Dfd maintains the visited candidate nodes in a stack trace. The trace allows Dfd to backtrack its way through the lattice, revisiting nodes that can be eventually classified at a later stage.

In case the node was not yet visited, Dfd examines if the node is a proper superset/subset of a previously discovered dependency/non-dependency and updates its category ac-cordingly. Otherwise, it has to perform the costly partition calculation in order to identify which kind of candidate the combination represents (line 17). If the partition calculation results into a dependency, the node is classified as a candi-date minimal dependency, otherwise as a candidate maximal non-dependency.

After categorizing the current node, Dfd chooses the next node that is visited during the lattice traversal in line 18. Algorithm 2 : findLHSs () Data : RHS attribute A , relational instance r
Result : the set of minimal non-trivial Lhs s seeds  X  R \{ A } ; while !isEmpty(seeds) do return minDeps The general idea of choosing the next node is to move down-wards in the lattice if the current column combination rep-resents a dependency Lhs and upwards otherwise. The de-tailed process determining the next node is shown in Sec-tion 3.3. When all reachable nodes from the current trace have been visited, Dfd picks the next seed from the list of seeds that were initially calculated. Whenever the list of seeds is empty, the algorithm calls generateNextSeeds() to generate possibly remaining candidates.
Algorithm 3 shows how Dfd picks the next node based on its stack-trace and the currently considered column com-bination. The first important step is to determine if the current combination is a candidate for a minimal depen-dency or maximal non-dependency. If not, the algorithm instantly backtracks by choosing the latest available node on the trace.

If the current node is still a candidate for a minimal de-pendency, Dfd needs to determine if it is actually minimal. Therefore, in line 2 Dfd retrieves all unchecked subsets of the current node and removes all subsets that can be pruned according to Def. 1.3. If the set of unchecked subsets is empty, then the current node is a Lhs for a valid minimal Fd (lines 5 and 6). This is because there are no subsets left that still can be a Lhs for a valid Fd with fewer attributes than the current node.

However, if the set of unchecked subsets is not empty, Dfd picks an unchecked subset as the next node in line 8, and adds the previous node to the trace.

In case of the current node being a candidate for a max-imal non-dependency, Dfd retrieves the set of unchecked supersets and removes the prunable supersets according to Def. 1.4. Those nodes are potential Lhs s for maximal non-Algorithm 3 : pickNextNode() Data : node
Result : nextNode if isCandidateMinimalDep(node) then else if isCandidateMaximalNonDep(node) then else dependencies that contain more attributes than the current node.

Example 3.1. Given the schema R = { A,B,C,D,E } , assume the current right-hand side attribute is E and the current examined node is { A,C } . DFD categorized { A,C } as a candidate for a maximal non-dependency. To determine whether the left-hand side is actually a valid maximal non-dependency for the current right-hand side, DFD needs to make sure that there are no supersets of { A,C } that are also non-dependencies. The unchecked supersets of the current node are { A,B,C } and { A,C,D } .

Assume { C,D }  X  E is a valid functional dependency that was already found by Dfd . In this case, the right-hand side { A,C,D } can be excluded from the set of potential next nodes, because it already can be classified as a dependency.
The lattice traversal terminates whenever there is no reach-able column combination left. As previously mentioned, the aggressive pruning approach can lead to unreachable but not finally categorized candidate nodes. To cope with that prob-lem, we again make use of insights from unique discovery al-gorithms. This time we leverage the complement operation that was introduced for the Gordian algorithm [17]. Sisma-nis et al. show that having the set of all maximal non-unique column combinations, it is possible to generate all minimal unique column combinations through a complementation ap-proach. Heise et al. prove the reversibility of that operation and its feature to identify missing combinations if one of the sets is not complete [10]. Thus, to prove the correctness of Dfd we need to show that the correctness proof given by Heise et al. can be generalized to Fd s. The only characteris-tics relevant for the proof are the minimality of uniques and maximality of non-uniques. For a Rhs column A , the set of Algorithm 4 : generateNextSeeds ()
Data : minimal dependencies for the current RHS maximal non-dependencies for the current RHS maxNonDeps
Result : new seeds seeds  X  X } newSeeds  X  X } foreach maxNonDep  X  maxNonDeps do seeds  X  seeds \ minDeps ; return seeds ; all Lhs s for minimal Fd s that determine A and the set of all maximal column combinations that constitute a non-Fd for A have the same characteristics as the set of all mini-mal unique and maximal non-unique column combinations, respectively.

Algorithm 4 describes how to identify the untouched nodes of the Lhs lattice graph. Whenever the lattice traversal for the current right-hand side column terminates in Algo-rithm 2, Dfd has to determine whether all minimal func-tional dependencies for the current Rhs have been found. Algorithm 4 is able to detect missing Lhs s for the current Rhs . Furthermore, it provides new seeds that enable Dfd to classify missing nodes.
 Algorithm 4 allocates two data structures, seeds and new-Seeds that maintain found seeds and the new set of seeds for the current iteration. In line 3, Dfd iterates over the whole set of maximal non-dependencies. For each of those non-Fd Lhs combinations, Dfd determines the complement. Be-cause we use bitsets to store the set of columns in a column combination, this is a very cheap operation.

In the first iteration of the outer loop, seeds in line 5 is still empty. In that case, Dfd creates a column combination bitset called emptyColumns , which has the same size as the number of columns provided by the underlying dataset with-out the Rhs column. All of its bits have been set to zero, which means that the combination contains no columns yet.
Subsequently, Dfd adds a new potential Lhs to the set of seeds for each set bit in the complement of the current max-imal non-dependency combination. Dfd finally removes the already determined Lhs of minimal Fd from the set of new seeds in newSeeds . The remaining set contains unclassified seed nodes that can be used for the next traversal itera-tion. In general those nodes are very close to the actual of minimal Fd s and maximal non-dependencies so that the remaining search space is effectively narrowed down.
Example 3.2 illustrates why the approach of Sismanis et al. [17] for detecting missing keys from maximal non-keys, also works for minimal dependencies and maximal non-depen-dencies.

Example 3.2. Given the schema R = { A,B,C,D,E,F } , assume the current right-hand side is F and the lattice traver-sal terminated. Dfd was not able to determine any minimal dependency, but it identified { A,B,C } 6 X  F as a maximal non-dependency.

Using that result, lines 5 to 9 of Algorithm 4 generate the seeds D and E based on { A,B,C } 0 = {{ D } , { E }} as can-didates for dependencies. This is because both of those at-tributes are not part of the maximal non-dependency, which implies that they have to be part of a minimal dependency: if they were not a component of a minimal dependency, they would have to be a part of the present maximal non-dependency Lhs s. However, in that case { A,B,C } 6 X  F would not be a maximal non-dependency anymore. There-fore, D and E can be used as seeds for the missing minimal dependency.

If we now extend the example with a second maximal non-dependency { A,C,D } 6 X  F , we can explain what happens in lines 9 to 17 of Algorithm 4. We know already that seeds = { D,E } from the previous step. Dfd now sim-ply merges the dependencies of the first step with the com-plements of the remaining maximal non-dependency in the same manner as in the first part of the algorithm. Be-cause { A,C,D } 0 = { B,E } , Dfd generates all cross combi-nations {{ B,D } , { D,E } , { B,E } , { E }} as new dependency candidates. After the minimization step in line 13, only {{ B,D } , { E }} remain as seeds for Lhs s.
The pruning of supersets of dependency Lhs and subsets of non-dependency Lhs are a crucial factor for the efficiency of
Dfd , because it enables Dfd to classify column combi-nations without calculating their partition, based only on the available dependencies and non-dependencies. There-fore, we need to provide a data structure that enables us to quickly identify classified supersets and subsets.

Similar to [10], we use two hashmaps to manage sub-set/ superset relationships. For the case of dependency Lhs s, we use a hash map whose keys are represented by the individual column indices. Those columns map to sets of de-pendency Lhs s that contain the respective column. When-ever Dfd discovers a dependency with a Lhs K , that de-pendency is added to the value sets of all columns that are contained in K . Since it is redundant to store supersets of
Lhs of valid Fd s, we minimize the sets of dependencies afterwards by removing previously stored supersets of K . The same is valid for non-Fd s, albeit it is necessary to re-move redundant subsets of non-Fd Lhs after adding a new combination.

However, as shown in [14], the search space of the com-plete set of minimal Fd s and consequently for maximal non-Fd s is exponential, and the set of stored dependencies and non-dependencies increases very fast during the traversal of the powerset lattice. That is why we rebalance those data structures after adding new combinations by creating sub-lists for column pairs. The threshold for rebalancing the dependency and non-dependency sets can be adapted specif-ically to certain data sources. In general, the minimization of dependency sets or respectively the maximization of non-dependency sets in combination with the rebalancing allows Dfd to reduce the lookup times. Whenever Dfd performs a check whether a column combination can be pruned, it only needs to compare the current column combination to the as-sociated combinations that have been stored for the columns of the current combination in the respective hashmap. In our experiments, we chose to set the maximum value set size to 100,000, which triggers the rebalancing process for some of the tested datasets.
Similar to Tane, we use stripped partitions for Dfd validate dependencies [11]. Stripped partitions have also been referred to as position list indices [10].

Definition 3.1. Partition of an attribute set. Given a relational database schema R = { C 1 ,...C n } with a database instance r  X  dom ( C 1 )  X  X  X  X  X  dom ( C n ) , let X  X  R . Then the partition  X  associated with X is defined as  X  X = { [ a ] a  X  r } .

A stripped partition c  X  X for a column combination X con-tains a list of equivalence classes where each class contains the row/position ids of one specific value projection in X and all classes with only one element have been removed. Furthermore, || c  X  X || is the total number of duplicate value combinations and | c  X  X | the number of equivalence classes.
Definition 3.2. Validity check of FDs. Given a re-lational database schema R = { C 1 ,...C n } with a database instance r  X  dom ( C 1 )  X  X  X  X  X  dom ( C n ) , let X,Y  X  R and let |  X  | be the number of equivalence classes in  X  . Then X  X  Y  X  X   X  X | =  X  X  X  X  Y } .
 The calculation of  X  X  X  X  Y } from  X  X and  X  Y is called parti-tion intersection. This means, in order to determine whether a Lhs X is valid for a right-hand side C k in the form of X  X  C k , Dfd needs to calculate not only the stripped par-tition of the Lhs , but also the respective partition of the Lhs intersected with the partitions of the Rhs .

Computing the intersection of two stripped partitions scales linearly in the number of tuples of a relation. We use probe tables to intersect partitions of two attribute sets c  X  , as also applied for Tane and Ducc . A probe table is a data structure that maps each tuple index t  X  r of the first input partition to its corresponding equivalence class index c  X  c  X  X . In a second path on the tuples in c  X  Y , a second map is created that maps pairs of class indexes to sets of tuple ids. Each tuple id from c  X  Y is probed against the previously created map. Whenever the lookup succeeds a map-entry is created with the classes c  X  c  X  X and c 0  X  c  X  X as the key and the tuple id that matched both classes as value. If the key ( c,c 0 ) already exists the new tuple is just appended to the list of values of ( c,c 0 ). In a final step, we strip the second map from all class pairs that map only to one tuple id.
Because we use the same concept as Tane for determining the validness of Fd s, Dfd can easily be modified to discover approximate FDs. Approximate or also called partial Fd s are dependencies where some tuples violate the dependency. For example data corruption might be a reason why specific column combinations are not discovered as dependencies. Therefore it might be reasonable to consider a dependency also as valid, when it holds only for 99 . 99% of tuples.
In general, when calculating composed partitions, most of the time Dfd re-uses previously computed partitions over and over again. Nevertheless, the number of partitions might still grow exponentially and result in high memory consump-tion.

Therefore, we provide a method to deallocate partitions that are not needed any longer. Since we cannot be ab-solutely sure which partitions are not necessary anymore, we keep track of the recently used partitions and the us-age counts. Whenever the number of partitions exceeds a certain threshold we determine the median usage count of the currently allocated partitions. We then delete all non-atomic partitions that have a usage count below the median value, giving least recently used partitions an advantage.
The difficulty is to determine the threshold for the start of that process. The reason is that the structure of datasets, such as the distinctiveness count of the columns, the col-umn and the row count itself, and the number of Fd s lead to a different number of partitions that we need to calculate. One possibility is to trigger the partition removal operation whenever a static threshold is exceeded. However, that is not suitable since the number of partitions between different datasets for a constant number of rows and columns differs by orders of magnitude. It can be more practical to con-nect the partition removal process to the currently allocated memory by the partitions. In our experimental section, we show the effect of static partition removal on some dataset. We label the Dfd version with partition removal as Dfd Mem.
To show the effective pruning and overall efficiency of our approach we performed multiple experiments. We analyzed the effect of different parameters, such as number of columns and number of rows, on the runtime behavior of Dfd . We provide a detailed comparison of Dfd with the most popular Fd mining algorithms Tane and FastFDs, which represent different lines of Fd discovery strategy.
For the experiments we used a Dell PowerEdge R620 server: If not stated otherwise, all tests are under a maximum heap size of 64GB and a maximal running time of 3 hours.
Table 1 shows our datasets, selected based on the type, the column, and row count. The Uniprot dataset is the public Universal Protein Resource database(UniProt, www.uniprot. org ) that contains protein sequences and functional infor-mation. NCVoter is a collection of North Carolina X  X  Voter Registration statistics 1 . Lineitem is one relation of the TPC-H database benchmark, emulating the common structure of a table of shopping transactions. For different experiments we created modified versions of the real-world datasets by truncating them after the first 20 columns or the respective number of rows.
In this section we analyze the effectiveness of the pruning strategies used by Tane, FastFDs, and Dfd . We investigate the impact of the number of columns, the number of rows. Subsequently, we inspect the memory requirements of each approach.
As mentioned in Section 3.5, functional dependency dis-covery is a problem with an exponential time complexity depending on the number of columns. The complete search space for a dataset with n columns has a cardinality of 2 Adding one column to a dataset doubles the number of nodes to examine. Therefore, a brute force approach that does not use pruning strategies is infeasible.

Figure 2 shows the scaling for the number of columns on the NCVoter and Uniprot datasets. We had to restrict the number of rows in order to have at least some datapoints for FastFDs and Tane. On both datasets, Dfd outperforms existing work significantly. While being up to three orders of magnitude faster than FastFDs, Dfd was also able to process 10 more columns than FastFDs. Because of the cal-culation of the difference sets, FastFDs has high initial costs even for only two columns. Although Tane initially was only slightly slower than Dfd it ran into memory problems from the 23rd column on, hitting the 64 GB memory restriction. We can observe a similar behavior for the NCVoter dataset. However, this time FastFDs was able to process two columns more than Dfd , because Dfd hit the memory limit from the 32nd column on. In general we can observe a plain corre-lation of Dfd  X  X  runtime and the result size, (number of de-pendencies) illustrated via the grey bars. This supports our claim that Dfd significantly prunes the search space through the random walk strategy. To further clarify our point we compared the number of generated partitions through Dfd and Tane in Figure 3. Here we see clearly that Dfd performs by magnitudes fewer Fd verifications than Tane.

Figure 4 shows the development of the memory require-ments of Tane, FastFDs, and Dfd depending on the number of columns in the dataset. On both datasets, Tane is the first algorithm that exceeds the default heap size of the Java Vir-tual Machine. In both cases, Tane X  X  memory requirement increases dramatically with the number of columns. Dfd manages to process eight more columns than Tane on the NCVoter and 11 more on the Uniprot dataset. Unlike Tane, Dfd did not fail because of the memory limit on the second benchmark.

Since FastFDs is a tuple oriented approach, its memory re-quirement is strongly dependent on the value distributions of http://www.ncsbe.gov/ncsbe/data-statistics. Figure 3: Execution time and number of partitions for Tane and DFD n the first 100,000 rows of the Uniprot dataset. the dataset. For Uniprot, the distribution favored FastFDs, which managed to process two columns more than Dfd . Figure 4: Memory requirements depending on the col-umn count for Tane, FastFDs, and DFD on NCVoter and Uniprot. (  X  -Time Limit  X  -Memory Limit)
Since the goal for the design of Dfd was a scalable algo-rithm for the discovery of Fd s, we also compared the run-times of Tane, FastFDs, and DFD on the complete datasets. Figure 5 shows how Tane, FastFDs, and Dfd performed on the whole datasets of NCVoter and Uniprot. As expected, FastFDs was not able to process the datasets and already failed at the calculation of the difference sets.

On the NCVoter dataset, Tane and Dfd perform equally fast for up to 11 columns. However, because of the memory limit, Dfd is able to process two columns more than Tane. Dfd fails at a column count of 14, because of the given time constraint. A similar result can be observed regarding the Uniprot dataset. Up to 12 columns, Tane and DFD have a similar runtime. Afterwards, Tane X  X  runtime increases faster than Dfd and hits the memory limit early for 22 columns. Dfd manages to determine the set of minimal Fd s for up to 31 columns.
Mannila et al. state that Tane X  X  time complexity is only linearly dependent on the number of rows [11]. This intu-ition holds for the partition verifications of Tane and Dfd However, FastFD X  X  preprocessing step, the generation of all difference sets has a quadratic time complexity [18].
Figure 6a shows that for a very small amount of rows, all approaches are suitable. FastFDs is not able to process the dataset for 250,000 rows. Note that we also had to re-Time Limit  X  -Memory Limit) Figure 5: Execution time for Tane, FastFDs, and DFD on the complete NCVoter and Uniprot dataset. (  X  -Time Limit  X  -Memory Limit) strict the number of columns on both datasets to the first 20 columns, in order to produce any numbers. Here, the limit-ing factor was not the timeout as seen in the measurements in Section 4.2.1, but the heap space. Profiling showed that the generation of the search tree for the calculation of the difference set X  X  minimal covers is responsible for the memory consumption.

Although Dfd is only a slightly faster than Tane for up to 100,000 rows, it is able to process 250,000 rows within the given memory limits, which Tane cannot. Even the maximal dataset of 500,000 rows is not an issue for Dfd . As expected, Dfd scales linearly on the number of rows.

The results of the Uniprot dataset in Figure 6b show a similar behavior for all approaches. Again, FastFDs fails to process 250,000 rows, because it exceeds the memory limit when calculating the minimal covers. Dfd is by multiple factors faster than Tane for 250,000 rows and nearly an order of magnitude faster for 500,000 rows. The reason is that Tane needs to calculate more than 11 times the number of partitions compared to Dfd .

Figure 7 displays the growth of the memory requirement dependent on the number of rows in the dataset. We used the same datasets as in Section 4.2.2.

Unlike in the previous experiment where we tested the memory growth depending on the number of columns, FastFDs performs worse than both Tane and Dfd . The reason is the preprocessing step, which calculates the difference sets based on the tuples of the dataset. Since the number of possible Figure 6: Execution time for Tane, FastFDs, and DFD on the first 20 columns of the NCVoter and Uniprot dataset. (  X  -Time Limit  X  -Memory Limit) difference sets increases quadratically with the number of rows, the number of difference sets that need to be covered in the later stage of FastFDs increases in the same manner. However, this forces FastFDs to cover a much larger num-ber of paths in the search tree that determines the minimal covers that eventually result in the minimal dependencies. Profiling of FastFDs showed that storing that search tree led to the violation of the given memory limit.

Tane performed slightly better than FastFDs for the NCVoter dataset. However, both algorithms failed when processing 250,000 rows. On the Uniprot dataset Tane was able to Figure 7: Memory requirements depending on the row count for Tane, FastFDs, and DFD on NCVoter and Uniprot. (  X  -Memory Limit) calculate the complete result for the largest dataset, albeit almost exhausting the given memory pool.

In contrast, Dfd easily processes both datasets, regard-less of the number of rows. Dfd requires around 30% of the memory that Tane needs in order to process NCVoter with 100,000 rows. The advantage is even more noticeable for 500,000 tuples on Uniprot. Dfd solely requires 5% of Tane X  X  memory pool. That is because of Dfd  X  X  aggressive pruning strategy, which significantly reduces the partition computation.

Table 2 shows a comparison of the runtimes on the lineitem dataset. We generated the lineitem datasets with different scale factors (SF): 0 . 1, 0 . 3, and 1 . 0.

FastFDs was only able to calculate a results in the given memory and time constraints for 2 and 3 columns in the dataset with a scale factor of 0.1. Even for those two data points, Dfd was two orders of magnitude faster. This shows that FastFDs scales exceptionally poor with an increasing number of rows. Profiling showed that it was not even able to reach the minimal cover calculation step, because it was still occupied with computing the difference sets. Dfd and Tane perform much better. Both algorithms are able to determine the complete result for the whole 0 . 1 dataset. For a scale factor of 0 . 3, Tane has to terminate early, because of the memory limit for column counts beyond 13. In contrast, Dfd processes the dataset for the whole range of columns. At a scale-factor of 1 . 0 Tane already fails because of memory issues at a column count of 11. Dfd is able to process the dataset for up to 14 columns. However, unlike Tane, it fails later due to the time limit.

For some data points, especially for SF = 0 . 1, Tane per-forms better than Dfd . The reason is that for the lineitem dataset, Tane actually often calculates fewer partitions than Dfd , e.g., only 9,417 partitions for the complete 0 . 1 dataset while Dfd created 19,224 partitions. As lineitem is a gen-erated dataset, the cardinality of the vast amount of the dependencies is very similar and small, being at maximum size 5 and equally distributed on only a few levels of the powerset lattice. In such a scenario a level-wise breadth-first approach is more efficient than a depth-first approach if the dataset is small enough.
In Section 3.7, we introduced a partition removal concept that enables Dfd to remove partitions from the heap if a certain amount of created partitions is exceeded. Figure 2b shows results where Dfd performed worse than FastFDs be-cause the memory limit was already exceeded for 32 columns. Since Dfd stored 40,732 partitions to calculate the complete set of minimal Fd s for 31 columns, we set the maximum par-tition threshold to 10,000 for our experiment. Figure 8: Execution time for Tane, FastFDs, DFD, and DFD-Mem on the first 100,000 rows of the NCVoter dataset. (  X  -Time Limit  X  -Memory Limit)
Figure 8 shows how the memory aware Dfd , Dfd -Mem performs compared to the other three algorithms. The thresh-old of 10,000 partitions is exceeded the first time for a col-umn count of 26. Up to 30 columns Dfd and Dfd -Mem are equally as fast. However, at 31 columns the negative impact of the memory limit is noticeable for the runtime of Dfd . That is because Dfd spends a lot of time garbage collecting whereas Dfd -Mem does not exceed the memory limit at all. As expected, Dfd -Mem actually performs only slightly worse than Dfd . Considering that we reduced the amount of stored partitions to less than 25%, the partition removal heuristic seems to be promising. Table 3 illustrates that the heuristic for choosing which partitions can be re-moved, works quite well. In fact, Dfd -Mem rarely needs to recalculate partitions.
In this paper, we presented Dfd  X  a new algorithm for discovering functional dependencies. Dfd benefits from ag-gressive pruning through a random-walk depth-first traver-sal and an efficient result verification that enables it to ignore many false positives. We presented extensive experiments that illustrate the superiority of Dfd over the popular al-gorithms Tane and FastFDs on large real world datasets, outperforming them most of the time by orders of magni-tude. In general, a random walk approach for choosing Lhs candidates works well. However, we can imagine that incor-porating some heuristics might further improve the runtime, despite considering the number of distinct values of a column combination as a heuristic had no influence. [1] Z. Abedjan and F. Naumann. Advancing the discovery [2] R. Agrawal and R. Srikant. Fast algorithms for mining [3] P. Bohannon, W. Fan, F. Geerts, X. Jia, and [4] X. Chu, I. F. Ilyas, and P. Papotti. Discovering denial [5] X. Chu, I. F. Ilyas, and P. Papotti. Holistic data [6] W. Fan, F. Geerts, J. Li, and M. Xiong. Discovering [7] W. Fan, J. Li, S. Ma, N. Tang, and W. Yu. Interaction [8] P. A. Flach and I. Savnik. Database dependency [9] L. Golab, H. Karloff, F. Korn, and D. Srivastava. Data [10] A. Heise, J.-A. Quian  X e-Ruiz, Z. Abedjan, A. Jentzsch, [11] Y. Huhtala, J. K  X  arkk  X  ainen, P. Porkka, and [12] I. F. Ilyas, V. Markl, P. J. Haas, P. Brown, and [13] S. Lopes, J.-M. Petit, and L. Lakhal. Efficient [14] H. Mannila and K.-J. R  X  aih  X  a. On the complexity of [15] F. Naumann. Data profiling revisited. SIGMOD Rec. , [16] N. Novelli and R. Cicchetti. FUN: An efficient [17] Y. Sismanis, P. Brown, P. J. Haas, and B. Reinwald. [18] C. Wyss, C. Giannella, and E. Robertson. FastFDs: A [19] H. Yao, H. J. Hamilton, and C. J. Butz. FD Mine:
