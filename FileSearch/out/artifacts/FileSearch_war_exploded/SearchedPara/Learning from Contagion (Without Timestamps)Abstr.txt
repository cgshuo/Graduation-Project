 Computer and Information Science, University of Pennsylvania We present new techniques for learning from contagion in networks. Our motivation is settings in which we are able to select or observe an initial  X  X eed X  set of infected vertices, and only at some later time, observe the set of subsequent infections resulting from some underlying stochastic con-tagion process on the network. Much classical work on network diffusion processes focuses on characterizing how contagions spread across a known network. Our aim, on the other hand, is to develop a more comprehensive the-ory for inferring unknown network structure from observed contagion behavior.
 As an example, consider epidemiological studies, where an initial set of infections are observed in a population, and after a while, new infections develop. One might hope to learn the underlying social interactions within the popula-tion so as to better prevent future contagion. In direct mar-keting, a marketer might advertise to a group of potential customers. After some time, certain members of the popu-lation adopt a product. 1 Inferring edges in the network can help optimize future campaigns.
 Only recently has attention been given to the problem of inferring network structure from contagion/diffusion pro-cesses (see Section 1.1). These recent works have assumed that, rather than observing only the initial and final infec-tion sets, the learner is given temporal information describ-ing the exact order in which infections take place on the network. This is a reasonable assumption when consid-ering, for example, contagion in online social networks, where timestamped content is readily available. However, for many other natural contagion processes, accurate tran-scripts of how the contagion evolved are often unavailable; instead one only observes snapshots of infection states. For the underlying contagion process, we examine the well-studied independent cascade model (Kempe et al., 2003; Goldenberg et al., 2001a;b). We consider both the case where the learner has active seed queries , or the weaker model of only passive seed observations . Our main results provide efficient algorithms for learning tree-like structures for both active and passive seed selection, along with im-possibility results showing that some of our sample-size dependencies are necessary. We then investigate the im-portant and natural extension of these results to cyclical graphs. We show that a natural extension of the algorithms we propose can be utilized to learn the structure (exactly or with low error) so long as the underlying network is suf-ficiently sparse. When the network is too dense, the pres-ence of many short cycles in a network presents challenges in our models that bear a high-level similarity to those of problems such as inference in Bayesian networks  X  the short cycles create many paths via which vertices can influ-ence each other, conflating the sources for large-scale con-tagion and making learning difficult. However, we hope that some of the analytical and algorithmic tools we de-velop may be useful for even richer classes of networks. 1.1. Related Work Our work mainly concerns a well-studied model for con-tagion, the independent cascade model (Kempe et al., 2003; Goldenberg et al., 2001a;b) Only recently has there been work on learning network structure from contagion (Rodriguez &amp; lkopf, 2012; Myers &amp; Leskovec, 2010; Rodriguez et al., 2011; Abrahao et al., 2012; Gomez-Rodriguez et al., 2010; Du et al., 2012; Netrapalli &amp; Sang-havi, 2012), primarily (with slight variations) in the inde-pendent cascade model.
 As previously mentioned, a key assumption in these works is that the learner observes a sequence of cascade processes with timestamps, or the exact order in which vertices be-came infected. Gathering such information about the in-fection times is not always possible. For example, when monitoring the effectiveness of a viral marketing campaign, a firm might survey a population some time into the cam-paign in order to assess its effectiveness, but it is unlikely that this would reveal information on the precise order re-spondents adopted the product. In contrast, we assume that we have access to data ( S i ,A i ) , where S i is some initial seed set of infections, and A i are infections eventually re-sulting from the contagion process when the vertices in S were initially infected.
 This weaker assumption vastly changes the types of tech-niques that must be employed in order to infer network structure. In particular, the learner can no longer directly attribute the infection of some vertex to some previously infected vertex by watching the contagion unfold. To the best of our knowledge (Gripon &amp; Rabbat, 2013) is the only work that addresses the problem of learning the network structure without such transcripts. There the learner is al-lowed access to all triples { a,b,c } where { a,b,c } are path-connected vertices.
 Furthermore, for the learning objective, unlike (Rodriguez &amp; lkopf, 2012; Myers &amp; Leskovec, 2010; Rodriguez et al., 2011) we do not seek to maximize the likelihood of the ob-served data, rather, similar to (Abrahao et al., 2012), our goal is to (exactly or approximately) reconstruct the under-lying network.
 As mentioned in the introduction, we study two ways of seeding, or initiating, the infection: the active model, in which the learner can choose the seeds, and the passive model in which the seed sets are randomly sampled from a distribution. Both the active (see (Abrahao et al., 2012; Myers &amp; Leskovec, 2010)) and the passive model (see (Ne-trapalli &amp; Sanghavi, 2012; Gomez-Rodriguez et al., 2010)) have been studied in prior work.
 Finally, we highlight that our results are largely combina-torial in nature, whereas the previous literature uses con-vex programming (Myers &amp; Leskovec, 2010; Rodriguez et al., 2011; Du et al., 2012; Netrapalli &amp; Sanghavi, 2012), or submodularity (Rodriguez &amp; lkopf, 2012; Gomez-Rodriguez et al., 2010) to approximate an optimal network. There are a number of other papers that are remotely re-lated to our work: (Vert &amp; Yamanishi, 2005) examine the problem of network inference in the context of supervised learning, where part of the graph is revealed to the learner and the goal is to learn edges that connect  X  X imilar X  ver-tices to each other. (Lippert et al., 2009) propose an unsu-pervised kernel-based method instead. The broad setting we consider is the following: There is an unknown network structure G ( V,E ) in which an initial seed set S  X  V becomes infected; the contagion subse-quently spreads according to the rules of some underlying (possibly stochastic) contagion model with parameters  X  , and generates the final infected set A ( S )  X  V observed at some later time. The network structure, contagion model, and seed infection set define a distribution D = D ( S,G, X  ) on 2 V , and a draw A  X  D specifies the subset of V that becomes infected after the contagion has run its course. Our results examine the stochastic independent cascade (Kempe et al., 2003; Goldenberg et al., 2001a;b) model of contagion. 2 In the independent cascade model each edge ( u,v ) has an associated infection probability p ( u,v ) dynamics of the contagion are as follows: when a vertex u first becomes infected, it infects each of its neighbors, say v  X  N ( u ) , with probability p ( u,v ) . We say the edge ( u,v ) is flipped once this probability p ( u,v ) trial is conducted. Re-gardless of whether v is infected as a result, the edge ( u,v ) can never be flipped again. An alternate way of viewing the model is that we first flip each edge in G with the appro-priate probability, and delete edges for which the trial fails, resulting in a subgraph of G . Then any vertex in the same connected component of this subgraph as some seed vertex becomes infected.
 In the active seed selection model, a learner is permit-ted to choose an adaptive sequence of seed sets S 1 ,...,S each of which generates a resulting set of infections A i A ( S i )  X  X  ( S i ,G, X  ) . In the passive seed selection model, the learner is not permitted to choose the seed sets, but rather they are chosen randomly from a distribution P . In the passive model, we will need to refer to the dis-tribution which is induced by first drawing S  X  P then A  X  X  ( S,G, X  ) . For this we use the shorthand D ( P ,G, X  ) . Broadly speaking, the active seed model is more appropri-ate for settings such as viral marketing, where the learner can repeatedly target different populations for infections via promotions, advertising, or give-aways, while the more challenging passive seed model is better suited for settings in which Nature determines the initial infections, as in the spread of actual disease such as annual flu cycles. Note that in either case we assume the underlying contagion model is known, but the actual parameters of the model (e.g. the probabilities p ( u,v ) ) are not.
 The main problem we consider is that of exactly detect-ing the underlying network structure. 3 Doing so efficiently will in general require that the true or target graph belong to some restricted class G . By  X  X fficient X  we mean the stan-dard polynomial dependence on | V | , and on other param-eters that we shall discuss at the appropriate place. Within this framework, we consider classes of graphs that are re-stricted to be trees or  X  X ear X  trees in the sense of having few or limited cycles. We also demonstrate empirically that similar algorithms provide us with low error rates when learning sparse network structures. Our first result gives an efficient algorithm for exactly learning the structure of an arbitrary undirected tree in the active seed model. The sample size required 4 (and run-ning time) will depend inversely on 1 /  X  , where we define  X  = min u,v min { 1  X  p ( u,v ) ,p ( u,v ) } . This dependence is clearly necessary for any algorithm  X  if the p ( u,v ) can be arbitrarily small, infections will never be observed, and ex-act learning of the structure is impossible.
 Theorem 1. In the active seed selection model and inde-pendent cascade contagion, any tree structure is exactly learnable from samples { ( S i ,A i ( S i )) } M i =1 , where M is polynomial in 1  X  , | V | and log( 1  X  ) .
 We present an algorithm that will only select seed sets of size one (singleton seeds). The algorithm generates M = m | V | observations by selecting each vertex u as the seed m times ( m will be determined by the analysis). 5 We then define R u ( v ) = { i | v  X  A i  X  S i = { u }} . Thus R u ( v ) denotes the rounds in which v was infected given that u was the seed vertex. We shall show with high prob-ability, the containment relationships between the R uniquely determines the tree structure. The proof of Theo-rem 1 follows as a consequence. Let N ( u ) denote the set of vertices adjacent to vertex u . Lemma 1. Suppose v 6 X  N ( u ) and m  X  1  X  2 log(1 / X  ) . Then with probability at least 1  X   X  there exists a v 0 where R u ( v ) ( R u ( v 0 ) .
 Proof. If v is not a neighbor of u , then there exists some neighbor of u , v 0 , along the path from u to v . Given that the underlying network is a tree, every time v is infected, v must also be infected, establishing that R u ( v )  X  R u ( v For the strict inclusion, let v 00 be the immediate neighbor of v 0 that isn X  X  u (but is possibly v ). Given that the un-derlying network is a tree, every time u is selected as the seed vertex, the probability that the contagion reaches v probability at least  X  2 , v 0 is infected but not v . The prob-ability that the infection reaches both v 0 and v for all m rounds in which u is the seed vertex is therefore at most (1  X   X  2 ) m  X  exp(  X   X  2 m )  X   X  for m = 1  X  2 log(1 / X  ) , establishing the lemma.
 Lemma 2. Suppose v,v 0  X  N ( u ) and m  X  1  X  2 log(2 / X  ) . Then neither R u ( v )  X  R u ( v 0 ) nor R u ( v 0 )  X  R u probability at least (1  X   X  ) .
 Proof. Given that the underlying network is a tree, if u is the seed vertex, the probability that the contagion reaches ments yield that m  X  1  X  2 log(1 / X  ) suffices for this to oc-cur on at least one round, thereby ensuring that R u ( v ) 6 X  R u ( v 0 ) . The symmetric argument, swapping v and v 0 , and a union bound yields the lemma.
 We say that v is maximally infected by u if there does not exist a v 0 such that R u ( v )  X  R u ( v 0 ) . If m = O 1  X  2 log( | V | / X  ) , then with probability at least (1  X   X  ) , for all u,v pairs, v is maximally infected by u if and only if v  X  N ( u ) . This suggests a clear procedure for detecting the neighborhood of each vertex u . Repeating this for all vertices, we can exactly learn the tree with high probability. While the algorithm for learning in the active model re-quired non-trivial book keeping, the learner in that setting is greatly aided by the fact that all infections observed in a given round must have originated from a particular seed. In the passive model, this is no longer the case  X  the affects of seeding a particular vertex may never be seen in isolation. Thus attributing any infection in A i to any particular vertex in S i is more difficult.
 As for the seed distribution P , we obviously cannot hope to exactly learn for arbitrary distributions, since this would include pathological cases such as when S i = { V } with probability one. Therefore in what follows we assume that P belongs to the family of product distributions where each u  X  S i becomes initially infected independently with prob-ability q u for unknown 0 &lt; q u &lt; 1 6 . We will once again let  X  be the smallest value among the p u,v , q u , 1  X  p 1  X  q u . 4.1. Characterizing Lifts Our algorithm will work by observing the relation that a particular vertex X  X  infection has on likelihood of infection at other locations in the tree. To this end, we will define the lift u has on v as: In other words, this lift is the increase in the probability that v is infected from conditioning on u  X  X  presence in the (passive) seed set. Before describing the algorithm, we will record some useful facts about lifts.
 First notice that the outcome of the independent cascade model is determined by the outcomes of | V | + | E | Bernoulli random variables. | V | coins are flipped to determine which vertices belong to the seed set S i , after which the | E | edges of G are flipped to determine the outcome A i of the conta-gion process. We say that an edge ( u,v ) is  X  X ctive X  if the random variable corresponding to it is equal to 1 , and thus contagion is allowed to pass freely through that edge. Thus, if ( u,v ) is active, either both u and v are infected, or neither is infected. We define the active component containing u to be the subgraph of T which contains u and any vertex v for which all edges in the path u  X  v are active. Once again, observe that all vertices in the active component containing u are either infected or uninfected.
 Our first lemma gives a decomposition for L ( v | u ) =  X  ( u,v )  X  ( u,v ) in terms of two quantities  X , X  . The first of the product is over the edge set of the path u  X  v . In other words,  X  ( u,v ) is simply the probability that the u  X  v path is active. We say that s  X  w is an infecting tributary for u  X  v if s  X  S i , w  X  u  X  v and the path s  X  w is active (we consider the trivial path s  X  s when both s  X  u  X  v and s  X  S i to be an infecting tributary for u  X  v as well). Let E ( u,v ) be the event that there is no infecting tributary for u  X  v and  X  ( u,v ) = P ( E ( u,v )) .
 Lemma 3. For any u,v , L ( v | u ) =  X  ( u,v )  X  ( u,v ) . Proof. Fix u and v . Let C ( u,v ) (for  X  X onnected X ) be the event that the u  X  v path is active. Let D ( u,v ) (for  X  X is-joint X ) be the event the u  X  v path is not active (equivalently, u and v belong to disjoint active components). We first derive an expression for P ( v  X  A | u  X  S ) :
This first two equalities above follow from the law of total probability, and the fact that the event u  X  S is independent of C ( u,v ) . The next equality follows by observing that v  X  A with certainty if u  X  S and the u  X  v path is active. The final line follows from the conditional independence of the events u  X  S and v  X  A given D ( u,v ) . Writing and applying the above, we conclude that:
Conditioned on u  X  v being active, the probability that v 6 X  A is precisely the probability that there are no infecting tributaries for u  X  v , completing the proof.
 The previous lemma gives several insights into L (  X  |  X  ) . For example, we see that the function is symmetric in its arguments. Also note that lifts are monotonic along paths: for any u  X  v path and w along that path, u provides a better lift to w than to v . A useful special case is recorded in the next lemma.
 Lemma 4. Let u  X  v be a path of length greater than 1 , and let ( w,w 0 ) be any edge along that path. L ( v | u )  X  (1  X   X ) L ( w | w 0 ) .
 Proof. Appealing to Lemma 3, we see that L ( v | u ) =  X  ( u,v )  X  ( u,v ) . Recall that for every edge e , p e Since u  X  v contains at least one edge not equal to ( w,w by definition of  X  , L ( v | u )  X  (1  X   X )  X  ( w,w 0 )  X  ( u,v ) . Recalling the definition of  X  , we see that E ( u,v ) im-plies E ( w,w 0 ) . If there are no infecting tributaries for u  X  v there cannot be any infecting tributaries for w  X  w . Thus  X  ( u,v )  X   X  ( w,w 0 ) , and L ( v | u )  X  (1  X   X )  X  ( w,w 0 )  X  ( w,w 0 ) . Applying Lemma 3 once more com-pletes the proof. 4.2. Algorithm Armed with our understanding of lifts, we can now give an algorithm for learning an unknown tree T under passive in-fections. The central observation of the previous section is that, when w is a vertex along a u  X  v path, then u must provide a better lift to w than to v . We might have hoped that a vertex u gives all its highest lifts to its neighbors. Unfortunately, this is not true. u could, for example, be at the head of a path u  X  v for which every edge e  X  u  X  v has p e very close to 1 . Thus the presence of an infection at u has a large effect on the infection rate of all the vertices along u  X  v . In contrast, some of u  X  X  immediate neighbors w might already be prone to infection, or may have small p ( u,v ) , and therefore conditioning on u being seeded does not change the observed infection-rate of w by much. Ulti-mately, we will have to exploit the monotonicity of L (  X | X  ) along paths, established in the previous section.
 In what follows, we assume that the algorithm has observed M iid samples ( S i ,A i ) , from which it has derived esti-mates  X  L ( v | u ) for each pair of vertices u,v . The exact nature of this sampling will be covered subsequently. For the sake of intuition, suppose that these estimates are per-fect. The algorithm works by growing a forest of connected components of T . Given such a component C , a u  X  C and a v 6 X  C , if u and v are not neighbors then there is an edge ( x,y ) 6 = ( u,v ) where the u  X  v path crosses out of the component C (i.e. x  X  C , but y 6 X  C ). Lemma 4 tells us that L ( v | u )  X  L ( y | x ) . Thus, by taking ( u  X  ,v  X  ) = arg max u  X  C,v 6 X  C L ( v | u ) , the algorithm is guaranteed to find an edge of the network. We present the pseudocode for the algorithm below.
 Algorithm 1 Algorithm for exactly learning trees under passive seeds and independent cascade. 1: Input: Estimates  X  L (  X | X  ) , vertex set V 2: % Begin with singleton components 3: % and the empty edge-set . 4: Set Components = {{ v }| v  X  V } . 5: Set  X  E =  X  6: % Iterate until we have a single 7: while |C|6 = 1 do 8: for C i  X  X  do 9: % Discover an edge . 11: Add ( u  X  ,v  X  ) to E . 12: end for 13: Set Components equal to connected components 14: end while 15: Return ( V,  X  E ) 4.3. Analysis It is not difficult to show that this algorithm requires O | V | 2 log | V | computation time. The following theo-rem establishes the correctness of the algorithm given suf-ficiently good estimates. First, let us define notation for the smallest lift between two neighboring vertices: L min Theorem 2. Suppose for every u,v , |  X  L ( v | u )  X  L ( v | u ) | &lt;  X  L min 2 . Then Algorithm 1 returns  X  E = E in time O | V | 2 log | V | .
 Proof. Fix an arbitrary subgraph C i of the true network ( V,E ) . To prove correctness, it suffices to show that for this choice of C i line 10 returns ( u  X  ,v  X  ) only if ( u E . Suppose for the sake of contradiction that this is not the case. Then the true u  X   X  v  X  path contains an edge ( w,w 0 ) where w  X  C i and w 0 6 X  C i . Lemma 4 tells us that L ( w | w 0 )  X  L ( v | u )  X   X  L ( w | w 0 )  X   X  L min assumption, |  X  L ( v | u )  X  L ( v | u ) | &lt;  X  L min 2  X  L ( w | w 0 )  X   X  L ( v | u ) &gt; 0 . Since w  X  C i , w 0 could not have been the argmax of line 10, establishing the contradiction.
 Finally, we provide an upper bound on the number of sam-ples M needed in order to satisfy the hypothesis of The-orem 2 with high probability. Let  X  n A ( v ) = P M i =1 A ) ,  X  n S,A ( u,v ) = P M i =1 1 ( u  X  S i ,v  X  A i ) , and  X  n P i =1 1 ( u  X  S i ) . Taking  X  n A ( v ) /M lets us state the following theorem.
 Theorem 3. M = O 1 L 2 for any u,v , | L ( v | u )  X   X  L ( v | u ) | &lt;  X  L min bility at least 1  X   X  . Furthermore, as a consequence of this fact and Theorem 2, Algorithm 1 returns  X  E = E in time O | V | 2 log | V | with probability at least 1  X   X  . Proof. Fix a u,v and &gt; 0 . There exists a C &gt; 0 such that taking M = C 2 log(16 | V | 2 / X  ) , guarantees with probability at least 1  X   X  2 | V | 2 : (1) (1  X  ) P ( u  X  S ) &lt; 1 M ) P ( u  X  S ) , (2) | 1 M  X  n S,A ( u,v )  X  P ( u  X  A,v  X  S ) |  X  , and (3) | 1 M  X  n A ( v )  X  P ( v  X  A ) |  X  . This follows by ap-plying a multiplicative Chernoff bound for (1), Hoeffding X  X  inequality for (2,3) and taking a union bound.
 Assume that &lt; 1 / 2 . With probability at least 1  X   X 
The first inequality is a consequence of the concentration inequalities previously mentioned. The second inequality holds from assuming &lt; 1 / 2 and the fact that P ( u  X  S ) = q u &gt;  X  . The third follows because P ( v  X  A,u  X  S ) /P ( u  X  S ) = P ( v  X  A | u  X  A ) &lt; 1 . Taking = 1 / 12 X  2 L min gives  X  L ( v | u )  X  L ( v | u ) +  X  L min probability at least 1  X   X  2 | V | 2 . The symmetric argument lets us conclude the same for  X  L ( v | u )  X  L ( v | u )  X   X  L A union bound concludes the proof. 4.4. Necessary Dependence on Minimum Lift The only dependence of our algorithm whose necessity is not self-evident is that of the inverse minimum lift 1 L the sampling complexity bound of Theorem 3. We now show that this dependence is in fact unavoidable in the worst case: there are trees in which L min can be arbitrarily small, and any algorithm for exact learning must sample approximately 1 L The lower bound relies on the following construction. Let V consist of 4  X  X pecial X  vertices { a,b,z 0 ,z 1 } and 2 n  X  1 additional vertices V = { z,x (0) 1 ,...,x (0) n  X  1 ,x (1) We describe two trees. The tree T n a is given by first con-necting z 0 to a and z 1 to b . T n b , on the other hand, has z 0 connected to b and z 1 connected to a . Everything else about both T n a and T n b will be the same. In particular, z will connect to each of x (0) i , z 1 to each of x (1) i and both z and z 1 will be connected to z . Finally, let the infection probability be p for any edge, and seed probability be q for any vertex be  X  .
 Now suppose that T n is drawn from { T n a ,T n b } with equal probability. In order to exactly learn E , the learner must determine whether the edges ( a,z 0 ) , ( b,z 1 ) or the edges ( b,z 0 ) , ( a,z 1 ) exist.
 The basic idea behind this construction is that the lift that either z 0 or z 1 provide to either a or b can be made arbi-trarily small by making n large, since then z 0 and z 1 are infected with near certainty under a product distribution on seed sets. However, detecting whether we are in T a n T n requires distinguishing lifts between these four vertices. This leads to the following result, whose proof is technical and deferred to the Appendix.
 Theorem 4. For any , there exists an n such that L min ( T n ) &lt; , and any algorithm must sample at least (32 X  L min )  X  1 observations in order to reconstruct the edge set of T n with probability at least 7 / 8 . The algorithm described in Section 4 specifically leverages the fact that the underlying network is a tree. It computes all pairwise lifts in the network, then greedily selects the top | V |  X  1 edges, as long as those edges do not create a cycle. Therefore, this algorithm will certainly fail to learn a network that is not a tree. However, even for non-trees, observe that the lifts L ( v | u ) themselves remain well-defined. Moreover, we should continue to expect that lifts reveal relevant information about the structure of a net-work. A large lift L ( v | u ) implies that that v is far more likely to be infected when u is seeded, relative to its back-ground probability of being infected.
 In this section we will consider a modified version of the lift algorithm more suitable to general networks. This al-gorithm (which we call K -lifts ), expects as input estimates of lifts  X  L (  X  |  X  ) , as before. It then simply greedily selects the K largest lifts, and constructs the edges between the vertices responsible for those lifts (see Algorithm 2). Algorithm 2 K -lifts algorithm. 1: Input: Estimates  X  L (  X | X  ) , vertex set V , K &gt; 0 . 2: Set  X  E =  X  3: while |  X  E |6 = K do 5: end while 6: Return ( V,  X  E ) We will demonstrate that even when the network we are try-ing to learn is no longer a tree, this  X  X ift-based X  approach may still be effective. In other words, there is robustness to this approach even when the assumptions of Section 4 are violated. We first study the performance of the algo-rithm when learning Erd  X  os-Re  X  nyi networks. We discover that lifts continue to predict network structure under cer-tain conditions. We then provide evidence that the algo-rithm can also perform well on more realistic networks. We conclude by proving some simple facts about the K -lifts algorithm. On the one hand, we show that the algo-rithm learns simple cycles. However, we also find a net-work which is sparse and nearly a tree on which the al-gorithm will make many mistakes. Thus, there are indeed networks which will provably foil the lift algorithm. 5.1. Experiments Let G ( n, X  ) be an Erd  X  os-R  X  enyi random network 7 of size n , where each edge is present in the network indepen-dently with probability  X  (which implies the edge density of the network is close to  X  with high probability). For the underlying network in our experiments, we will consider G (100 , X  ) for  X  = 0 . 01 ,..., 0 . 1 . In all the experiments, each vertex becomes initially infected independently and with probability 0 . 05 (so roughly 5 vertices are seeded.) Denote the number of edges in a network G by m ( G ) . For simplicity, we will set the algorithm X  X  parameter K = m ( G ) , the correct number of edges, in each experiment. In other words, the algorithm is told the correct number of edges. Since we are setting K = m ( G ) , it is mean-ingful to consider err( E,  X  E ) = |  X  E  X  E | m ( G ) edge set of G and  X  E is the edge set returned by the al-gorithm. In other words, err( E,  X  E ) is the fraction of those edges which the algorithm either fails to find or add mis-takenly. For estimating the lifts, we need sufficient number of samples. Let M denote the number of samples avail-able to the learning algorithm. We run the experiments for M  X  X  1000 , 10000 , 100000 , 1000000 } and observe the relation between the performance of the K -lifts algorithm and the number of samples.
 The results of the experiments are summarized in Figure 1. Each of these plots depicts the error of the K -lifts algorithm err( E,  X  E ) against the density of the network D ( G ) = 2 m ( G ) / ( n ( n  X  1)) , and each plot corresponds to a particular value of p . The experiments demonstrate that indeed the K -lifts algorithm enjoys some robustness even when the tree assumption is violated. This is most obvious in Figure 1 (a). However, this is not unconditional. There are two forces that make learning more difficult: increasing the edge density, or increasing the transmission probability p . Increasing either will cause the sample size required in order to attain a low error rate to increase in turn. Why might this be the case? We hypothesize that the lift al-gorithm X  X  performance begins to degrade when, with high probability, large portions of the network are infected after each contagion process. Alternatively, this is the point at which seeds no longer cause infections local to the seed X  X  neighborhood. The lift algorithm fundamentally measures the difference in infection rate of a vertex u under two dif-ferent circumstances. When any u tends to frequently be infected, more and more samples are required to detect this difference. As the network density  X  and transmis-sion probability p increase, at some point the probability of large-scale non-local infections becomes certain.
 It is interesting to note that small p can hinder learning as well. After all, if p = 0 , seed infections never spread, and there is nothing to learn. We see this phenomenon in Figure 1 (a). When M = 1000 and p = 0 . 1 we simply do not see enough infections to be able to accurately estimate lifts. Notice also that since the algorithm is greedy, in the cases where it exactly learns the structure, the assumption K = m ( G ) is mild. If a good estimate of m ( G ) is provided in-stead, the algorithm will only make an additional number of mistakes equal to missing/extra edges in the estimate. Other networks. We also ran the generalized lift algo-rithm on more realistic models and data sets, including a real collaboration network, NetScience (Newman, 2003; Boccaletti et al., 2006; Newman, 2006), and networks generated by the Small Worlds model (Watts &amp; Strogatz, 1998).
 On the real collaboration network, NetScience, which con-sists of 1589 vertices and 2742 edges, we continue to ob-serve low reconstruction error. We obtain an error rate near 0 when edges have an infection transmission probability of 10%, an error rate of 0.06 when the transmission probabil-ity is 20%, and an error rate of 0.16 when the transmission probability is 30%.
 On networks generated from the Small Worlds model, re-construction error ranges from 0 to a bit above 0.1 at transmission probability 20% or smaller. As perhaps ex-pected, performance improves as we increase the amount of rewiring in the Small Worlds model, moving from net-works with very high clustering and many short cycles to-wards random connectivity. At transmission probability 30% the degradation of the algorithm is more rapid. Re-sults are similar but slightly better for networks generated from a less symmetric model that also balances high clus-tering with low diameter. 5.2. Theoretical Robustness and Limitations We conclude with theoretical results that partially charac-terize the behavior of the K -lift algorithm on cyclical net-works. To prove these results we will need a utility lemma that helps us characterize the lift between two vertices. Let X ( u ) be the active component of u (see Section 4.1). Lemma 5. On an arbitrary network G , L ( v | u ) = P ( v  X  X ( u ) ,X ( u ) is not infected ) = P ( u  X  X ( v ) ,X ( v ) is not infected ) .
 Proof. The proof follows along identically to the proof of Lemma 3 by replacing the event C ( u,v ) with { v  X  X ( u ) } , and omitting the final substitutions for  X , X  .
 We can now show that the K -lift algorithm exactly learns simple cycles.
 Theorem 5. Let C = ( V,E ) be a cycle on n &gt; 5 vertices and p ( u,v ) = p  X  1 2 , q u = q for all u,v . There exists a  X  &gt; 0 such that any ( u,v )  X  E and ( w,z ) 6 X  E , satisfy L ( v | u )  X  L ( w | z )  X   X  .
 Proof. By symmetry, L ( v | u ) is the same for any edge ( u,v ) in E and L ( w | z ) is the same for any w and z dis-tance d apart. So without loss of generality, we show that L ( v | u )  X  L ( w | u )  X   X  where v is immediately clock-wise to u and w is of distance d  X  2 clockwise to u . Let E + be the edges between v and w moving clockwise, and  X  be the probability that one of these edges is inactive. Let E  X  be the edges between u and w moving counter-clockwise, and V  X  be the edges between u and w moving counterclockwise, inclusive. Define A to be the event that v  X  X ( u ) , w 6 X  X ( u ) and X ( u ) is not infected. Define B be the event that w  X  X ( u ) , v 6 X  X ( u ) and X ( u ) is not infected. And Define F the event that w,v  X  X ( u ) and X ( u ) is not infected. Lemma 5 and the fact that these events are disjoint tells us that L ( v | u )  X  L ( w | u ) = P ( A ) + P ( F )  X  P ( B )  X  P ( F ) = P ( A )  X  P ( B ) . How-ever B can only occur if ( u,v ) is inactive, every edge in E  X  is active, and some edge in E + is inactive. Fur-thermore B requires that all of V  X  be unseeded. Thus, P ( B )  X  (1  X  p ) p n  X  d  X  (1  X  q ) n  X  d +1 . Since d  X  and  X   X  1 , P ( B )  X  (1  X  p ) p n/ 2  X  (1  X  q ) n/ 2+1  X  (1  X  p ) p n/ 2 (1  X  q ) n/ 2+1 . Also, A occurs if ( u,v ) is active, the edge immediately clockwise to v and the edge imme-diately counterclockwise of u are inactive, and u and v are not seeded. Thus, P ( A )  X  p (1  X  p ) 2 (1  X  q ) 2 . The fact that n &gt; 5 and p  X  1 2 , proves the theorem.
 While this may seem promising, there exist networks that are almost trees  X  trees with a single edge added  X  on which the K -lift algorithm would make many mistakes. Consider the following network on 2 n  X  1 vertices: Let S be a star consisting of n  X  1 vertices centered at v 0 . Let C be a cycle consisting of the remaining n vertices as well as v 0 . Define G = S  X  C . We use the notation V ( C ) to refer to the vertex set of C , with E ( C ) ,V ( S ) ,E ( S ) defined analogously. Theorem 6. Suppose p ( u,v ) = q u = 1 / 2 for all u,v . When run on G with K = m ( G ) = 2 n  X  1 , the K -lifts algorithm returns  X  E satisfying err( E,  X  E ) &gt; 1 / 2  X  o (1) . Proof. Name the vertices in C , v 0 ,v 1 ,...,v n according to the order in which they appear in the cycle. Let U consist of all pairs of vertices on C which are dis-tance 2 apart without crossing v 0 . More explicitly U = ( v any ( v,v 0 )  X  E ( S ) , and any ( x,y )  X  U , L ( x | y ) &gt; L ( v | v 0 ) , which means, the K -lift algorithm will build the chords in U before it considers the edges in the star S , making n  X  2 mistakes. By Lemma 5, L ( v | v 0 ) &lt; P ( v A ) &lt; (1  X  qp ) n  X  1 . On the other hand, for any ( x,y )  X  U the probability that x and y belong to the same uninfected component is at least p 2 (1  X  p ) 2 (1  X  q ) 4 . Therefore Lemma 5 also implies that L ( x | y )  X  p 2 (1  X  p ) 2 (1  X  q ) ficiently large n proves the theorem.
 Acknowledgement. We give warm thanks to Moez Draief for discussions on the results presented here.
 Abrahao, Bruno, Chierichetti, Flavio, Kleinberg, Robert, and Panconesi, Alessandro. Trace complexity of net-work inference. In KDD , 2012.
 Boccaletti, S., Latora, V., Moreno, Y., Chavez, M., and
Hwang, D.-U. Complex networks: Structure and dy-namics. Physics Reports , (424), 2006.
 Du, Nan, Song, Le, Smola, Alexander J, and Yuan, Ming.
Learning networks of heterogeneous influence. In NIPS , 2012.
 Goldenberg, Jacob, Libai, Barak, and Muller, Eitan. Talk of the network: A complex systems look at the underlying process of word-of-mouth. In Marketing Letters, 123: 211-223 , 2001a.
 Goldenberg, Jacob, Libai, Barak, and Muller, Eitan. Using complex systems analysis to advance marketing theory development. In Academy of Marketing Science Review , 2001b.
 Gomez-Rodriguez, Manuel, Leskovec, Jure, and Krause, Andreas. Inferring networks of diffusion and influence. In KDD , 2010.
 Gripon, Vincent and Rabbat, Michael. Reconstructing a graph from path traces. In CoRR abs/13016916 , 2013. Kempe, David, Kleinberg, Jon, and Tardos, Eva. Maximiz-ing the spread of influence through a social network. In KDD , 2003.
 Lippert, Christoph, Stegle, Oliver, Ghahramani, Zhoubin, and Borgwardt, Karsten M. A kernel method for unsu-pervised structured network inference. In In Proceedings of the International Conference on Artificial Intelligence and Statistics AISTATS , 2009.
 Myers, Seth A and Leskovec, Jure. On the convexity of latent social network inference. In NIPS , 2010.
 Netrapalli, Praneeth and Sanghavi, Sujay. Learning the graph of epidemic cascades. In SIGMETRICS , 2012.
 Newman, Mark. The structure and function of complex networks. SIAM Review , (45), 2003.
 Newman, Mark. Finding community structure in networks using the eigenvectors of matrices. Preprint , 2006. Rodriguez, Manuel and lkopf, Bernhard. Submodular in-ference of diffusion networks from multiple trees. In ICML , 2012.
 Rodriguez, Manuel Gomez, Balduzzi, David, and
Schlkopf, Bernhard. Uncovering the temporal dynam-ics of diffusion networks. In ICML , 2011.
 Vert, Jean-Philippe and Yamanishi, Yoshihiro. Supervised graph inference. In NIPS , 2005.
 Watts, Duncan and Strogatz, Steven. Collective dynamics
