 Germanic and agglutinative languages (e.g. German, Swedish, Finnish, Korean) have a productive mor-phology that allows the formation of not space-separated compounds in a much larger extent than e.g. in English. The task of separating such com-pounds into their corresponding single word (sub-) units is called compound splitting or decompound-ing.

Decompounding showed impact in several NLP applications, e.g. ASR (Adda-Decker and Adda, 2000), MT (Koehn and Knight, 2003) or IR (Monz and de Rijke, 2001), and is generally perceived as a crucial component for the processing of respec-tive languages. However, most existing systems rely on dictionaries or are trained in a supervised fash-ion. Both approaches require substantial manual work and do not adapt to vocabulary change. In this paper we introduce an unsupervised method for de-compounding that relies on distributional semantics. For the computation of the semantic model we solely rely on a tokenized monolingual corpus and do not require any further linguistic processing. Most pre-vious research on compound splitting concentrates on the detection of lemmas that form the compound. Whereas this is important for several tasks, in this work we focus on the splitting of a compound into its word units without any base form reduction, ar-guing that lemmatization is either part of the task pipeline anyways (e.g. IR) or not required (e.g. for ASR). Approaches to automatic decompounding can be classified into corpus-driven approaches and super-vised approaches. Corpus-driven approaches are usually informed by a frequency list (Koehn and Knight, 2003), by a probabilistic model (Schiller, 2005), by parallel corpora (Koehn and Knight, 2003; Macherey et al., 2011) or by the existence of pe-riphrases (i.e. reformulations) in large monolingual corpora (Holz and Biemann, 2008). As with other tasks, supervised approaches are usually superior to unsupervised approaches if sufficient training ma-terial is provided. A straightforward yet effective supervised decompounding system is contained in the ASV Toolbox (Biemann et al., 2008), which uses trie-based datastructures for recursively split-ting compounds based on learned splits. Alfonseca et al. (2008) combine several signals, including web anchor text, in an SVM-based supervised splitter. A widely used German decompounder is JWordSplit-as well as manually crafted blacklists and whitelists. compound decomposition. An unsupervised ap-proach is presented in (Koehn and Knight, 2003): out of several splits as given by matching parts of the compound to a vocabulary list, they pick the split with the highest geometric mean of word fre-quencies, which is entirely corpus-driven but ignores semantic relations between the compound and its parts. Another unsupervised system is proposed by Daiber et al. (2015). They propose an analogy-based approach, which relies on word embeddings.

Decompounding is evaluated either intrinsically or in a task that benefits from it, e.g. IR (Monz and de Rijke, 2001), MT (Koehn and Knight, 2003; Macherey et al., 2011) or ASR (Adda-Decker and Adda, 2000; Ordelman et al., 2003). The introduced method, called SECOS (SEman-sis that compounds are similar to their constituting word units. Our method is based on a distributional thesaurus (DT) that is computed, based on the dis-tributional hypothesis (Harris, 1951), using a mono-lingual background corpus and does not require any language-specific rules or preprocessing. We ex-emplify the method based on the compound noun Bundesfinanzministerium ( federal finance ministry ), which is assembled of the words Bundes ( federal ), Finanz ( finance ) and Ministerium ( ministry ).
Our method consists of three stages: First we ex-tract a candidate word set that defines the possible word units of compounds. We present several ap-proaches to generate such candidates. Second, we use a general method that splits the compound based on a candidate word set. Using the different candi-date sets, we obtain different compound splits. Fi-nally, we define a mechanism that ranks these splits and returns the top-ranked one. 3.1 Candidate Extraction For the extraction of all candidates in C , we use a distributional thesaurus (DT) that is computed on a background corpus. We present three approaches for the generation of candidate sets.

When we retrieve the l most similar terms for a word w from a DT, we observe well-suited candi-dates that are nested in w . For example Bundesfi-nanzministerium is similar to Bund , Bundes and Fi-nanzministerium . Extracting the most similar terms that are nested in w results in the first split candidate set, called similar candidate units .

However, only for few terms we observe nested candidates in the most similar words. Thus, we re-quire methods to generate  X  X ack-off X  candidates.
First, we introduce the extended similar candidate units . Here, we extract the l most similar terms for w and then grow this set by again adding their re-spective l most similar words. Based on these terms, we extract all words that are nested in w . This re-sults into more but less precise decompounding can-didates.

As the coverage might still be insufficient to decompound all words (e.g. entirely unseen com-pounds), we propose a method to generate a global dictionary of single atomic word units. For this, we iterate over the entire vocabulary of the background corpus, apply the compound splitter (see Section 3.2) to all words where we find similar candidate units . Then, we add these detected units to the dic-tionary. Finally, for word w subject to decompound-ing, we first extract all nested words NW from this dictionary. Then, we remove all words in NW that are nested itself in NW , resulting in the candidate set we call generated dictionary . 3.2 Compound Splitting Here, we introduce the decompounding algorithm for a given candidate set. For decompounding the word w , we require a set of candidate words C . Each word in the candidate set needs to be a sub-string of w . We do not include candidates in C that have less than ml characters. Additionally, we ap-ply a frequency threshold of wc . These mechanisms are intended to rule out spurious parts and  X  X ords X  that are in fact short abbreviations. We show can-didates, extracted from the similar candidate unit , with ml = 3 for the example term in Table 1. Then, we iterate over each candidate c i  X  C and add its beginning and ending position within w to the set S . This set is then used to identify possible split po-sitions of w . For this, we iterate from left to right and add all split possibilities to the word w . This approach over-generates split points, as can be ob-served for the example word, which is split into 6 units: Bund-e-s-finanz-minister-ium .

To merge character n-grams, we use a suffix-and prefix-based method. The suffix merging method appends all character n-grams with n below ms to the left word. The prefix method merges all charac-ter n-grams with n below mp to the word on the right side. To avoid remaining prefixes/suffixes, we ap-ply the opposite method afterwards. For the German language, the suffix-prefix ordering mostly yields the best output. The suffix-prefix-based approach results to Bundes-finanz-ministerium and the prefix-suffix method to Bund-esfinanz-ministerium . How-ever for some words, the prefix-suffix generates the correct compound split, e.g. for the word Zuschauer-er-wartung ( audience + he + service ), which is cor-rectly decompounded as Zuschauer-erwartung ( au-dience + expectation ).

In order to select the correct split, we compute the geometric mean of the joint probability for each split variation. For this we use word counts from a back-ground corpus. In addition to the geometric mean formula introduced in (Koehn and Knight, 2003), in order to assign non-zero values to unknown units. This yields the following formula for a compound w , which is decomposed into the units w i ,...,w N : Here, # word denotes the total number of words in the background corpus and total wordcount is the sum of all word counts. Then, we select the split example, this is the prefix-suffix-merged candidate Bundes-finanz-ministerium . 3.3 Split Ranking We have examined schemes of priority ordering for integrating information from different candidate sets, e.g. using the similar candidate units first and only apply the other candidate sets if no split was found. However, preliminary experiments revealed that it was always beneficial to generate splits based on all three candidate sets and use the geometric mean scoring as outlined above to select the best split as decomposition of a word. For testing the performance of our method, we chose four datasets. The first dataset was manually label-ed by Holz and Biemann (2008) and consists of 700 German nouns from different frequency bands. The second dataset consists of 158,653 nouns from ated by Marek (2006). As third dataset we use a noun compound dataset of 54,571 nouns from Ger-for the task of compound splitting, we do not sep-arate words in the gold standard, which comprise of prepositions, e.g. the word Abgang (outflow) is not split into Ab-gang (off walk) . To show the lan-guage independency of our method, we apply it to a Dutch compound dataset proposed by van Zaanen et al. (2014). This dataset comprises of 21,997 nouns. The corpus-based DT is computed following the ap-proach by Biemann and Riedl (2013). For each word, we use the left and the right neighboring word as context representation to compute the DT. For the generation of the split candidates we rely on the l = 200 most similar entries for each word.

The German DT is computed based on 70 mil-lion newspaper sentences, which are extracted from the Leipzig Corpora Collection (LCC) (Richter et al., 2006). For the generation of the Dutch DT, we use the Dutch web corpus (Sch  X  afer and Bildhauer, 2013), which is composed from 259 million sen-
We evaluate the performance of the algorithms us-ing precision and recall as defined by Koehn and Knight (2003). As unsupervised baselines we use the split ranking by (Koehn and Knight, 2003), called KK, and the semantic analogy-based splitter tems we apply the lexicon-and rule-based JWord-Splitter (JWS) and the supervised decompounding algorithm (ASV), introduced by Holz and Biemann to capture all characters in the words, reverting base forms to full forms. For Dutch, we apply the KK baseline and the NL Splitter. We use the small dataset with the 700 German nouns to find the best parameter settings of our method. The highest F1-scores are obtained using candidates with a frequency above 50 ( wc =50) and that have more than 4 characters ( ml =5). Further we append only prefixes and suffixes equal or shorter than 3 characters ( ms =3 and mp =3).

The highest precision is achieved with the similar candidate units (see Table 2). However, the recall is lowest as for many words no information is avail-able. Using the extended similarities, the precision similar cand. 0.9880 0.6798 0.8054 ext. sim. cand. 0.9617 0.7304 0.8303 gen. dictionary 0.9576 0.9199 0.9384 geom. mean scoring 0.9698 0.9338 0.9515 decreases and the recall increases. The best overall performance is achieved with the generated dictio-nary, which yields an F1-measure of 0.9384. The selection mechanism using the geometric mean scor-ing brings F1-measure up to 0.9515 on this dataset. In this section we compare the performance of our method against the unsupervised baselines and the knowledge-based systems (see Table 3).
 700 c X  X  Germa-Net Dutch
For the 700 nouns we achieve the highest preci-sion, recall and F1-measure using our method. How-ever, we have tuned our parameters on this dataset. Our improvement in terms of F-score is not signifi-spect to all other systems on this dataset. Never-theless, JWS is based on a manually created dictio-nary and ASV uses a supervised algorithm. On this dataset, ASV outperforms JWS. Due to their low recall, both unsupervised baselines (SAS and KK) achieve significantly lower F1-scores than SECOS . Using the c X  X  dataset we observe a different trend. Here, the best results are observed by using JWS fol-lowed by ASV and our method. Nevertheless, our method yields the highest precision value. Again, SAS and KK score lowest.

For the GermaNet dataset, our method signifi-cantly outperforms all others. Similar to the evalu-ation with the 700 nouns, JWS performs lower than the decompounding method from the ASV toolbox. Whereas our method obtains lower recall than ASV and JWS, it still significantly outperforms the un-supervised baselines and yields the overall highest precision.

In a last experiment, we show the performance on the Dutch dataset. As no trained models for JWS and ASV are available, we did not use these tools but compare to NL splitter, achieving a competitive precision but lower recall. This is caused by many short split candidates that are not detected due to the ml parameter. However, our method still beats the KK baseline significantly. In order to understand the errors of our method, we analyzed the compounds that have been split incorrectly. Considering the 700 German com-pounds our method splits 12.17% incorrectly, for the Dutch dataset, we observe the highest percentage of 32.60% incorrectly split compounds (see Table 4).
In addition, we analyzed how many compounds have been split in fewer parts ( under-split ), more parts ( over-split ) than the gold data or have the same number of splits, which, however, are incor-rect ( wrongly-split ). For all datasets we observe a general trend: our method tends to suppress splitting compounds, due to the parameters ms and mp that suppress very short parts. Compounds that are split at entirely incorrect positions constitute the lowest error class. We also analyzed for incorrectly split compounds how often our method missed a split, performed a wrong split and split correctly (see bot-tom three lines in Table 4). This analysis supports the previous finding: most errors of our SECOS method consist of missed splits. In this paper we have introduced an unsupervised method for decompounding words that is based on distributional semantics. We show the impact of its components and tune its parameters on a small Ger-man dataset. On two large German datasets, we demonstrate a performance of our method that is competitive to supervised and rule-based tools and outperforms two unsupervised baselines by a large margin. Further, we demonstrated its language-independence by achieving a good performance on a Dutch dataset. In the future, we would like to assess the impact of SECOS in task-based settings as well as apply it to other compounding languages.
 This work has been supported by the Deutsche Forschungsgemeinschaft (DFG) within the SeMSch project. Additionally, we want to thank the anony-mous reviewers for their helpful comments.

