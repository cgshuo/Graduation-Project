 Recent advances in click models have positioned them as an effective approach to the improvement of interpreting click data, and some typical works include UBM, DBN, CCM, etc. After formulating the knowledge of user search behav-ior into a set of model assumptions, each click model devel-oped an inference method to estimate its parameters. The inference method plays a critical role in terms of accuracy in interpreting clicks, and we observe that different inference methods for a click model can lead to significant accuracy differences. In this paper, we propose a novel Bayesian infer-ence approach for click models. This approach regards click model under a unified framework, which has the following characteristics and advantages: 1. This approach can be widely applied to existing click models, and we demonstrate how to infer DBN, CCM and UBM through it. This novel inference method is based on the Bayesian framework which is more flexible in character-izing the uncertainty in clicks and brings higher generaliza-tion abilities. As a result, it not only excels in the inference methods originally developed in click models, but also pro-vides a valid comparison among different models; 2. In contrast to the previous click models, which are exclusively designed for the position-bias, this approach is capable of capturing more sophisticated information such as BM25 and PageRank score into click models. This makes these models interpret click-through data more accurately. Experimental results illustrate that the click models inte-grated with more information can achieve significantly bet-ter performance on click perplexity and search ranking;  X 
This work was done when the first author was visiting Mi-crosoft Research Asia.
 3. Because of the incremental nature of the Bayesian learning, this approach is scalable to process large scale and constantly growing log data.
 H.3.3 [ Information Search and Retrieval ]: Algorithms, Experimentation, Performance Click Log Analysis, Click Model, Probit Bayesian Inference
In a commercial search engine, terabytes of click-through logs are generated every day at very low cost. These click-through logs encode valuable user preferences with regard to search results and reveal the latest tendency of user click be-haviors. Naturally, many studies have attempted to discover user preferences from click-through logs in order to improve web search ranking. Indeed, after the pioneering work of Joachims et al [11], which uses preferences automatically generated from click-through logs to train a ranking func-tion, many interesting works have been proposed to estimate document relevance from user clicks [1, 2, 5, 14].

It has been noticed in existing works that one major dif-ficulty in estimating relevance from click data comes from a so-called position bias: a document appearing in a higher position is more likely to attract user clicks even though it is not as relevant as documents in lower positions. Richard-son et al [15] proposed to increase the relevance of docu-ments in lower positions by a multiplicative factor. This idea was later formalized as the examination hypothesis [7] and adopted in the position model [6]. The examination hy-pothesis assumes the user will click a search result only af-ter examining the search snippet. Craswell et al [7] extended the examination hypothesis and proposed the cascade model by assuming that the user will scan search results from top to bottom. Dupret and Piwowarski [8] introduced the po-sitional distance into their UBM model. Recently, Guo et al [9] proposed the CCM model and Chappell et al [6] pro-posed the DBN model, both of which generalize the cascade Figure 1: The perplexity score on different query frequencies achieved by the UBM model with max-imum log-likelihood and maximum posteriori meth-ods, respectively. Lower perplexity score indicates better prediction performance. model by assuming that the probability of examining the current document is related to the relevance of the docu-ment in the previous position.

Click models, such as UBM, DBN and CCM, have been demonstrated to be much more successful than the sim-ple counting approach in interpreting click data. Each of these models introduced a set of assumptions integrating the knowledge of user browsing and click behaviors and devel-oped an inference method. The inference methods in exist-ing click models are often different with each other. For ex-ample, UBM used the EM algorithm to optimize the param-eter through maximizing the likelihood function. DBN also used the EM algorithm, however, it tried to maximize the posterior function (MAP). CCM was a Bayesian approach and it approximated the posterior distribution through a multinomial distribution. We observe that the inference method plays an important role in terms of accuracy in click prediction. As illustrated in Figure 1, the click perplexity of UBM can be significantly improved for low frequent queries after switching from the maximum likelihood estimation to the maximum posterior estimation method. Thus, the dif-ference of the inference methods in click models makes the comparison difficult. We cannot identify that the perfor-mance difference between two click models is due to either the model assumption or the inference method. Moreover, we find there is large space to develop a new inference ap-proach for improving the existing model accuracy.

In this paper, we propose a novel inference approach which can be widely applied to existing click models. The new approach is based on the Bayesian framework. It replaces each probability variable in click models with a new vari-able following the Gaussian distribution through a probit link function, such that both the prior and the posterior distribution of the Bayesian learning can be approximated by Gaussians. We show that this inference approach is com-putationally tractable for all click models in which the like-lihood functions are in the multinomial form. This require-ment is general enough to be fitted into most of the existing click models.
 We call the new proposed inference approach the Probit Bayesian Inference (PBI). Accordingly, the PBI approach can provide valid evaluation to compare different click mod-els. We apply PBI to three state-of-the-art click models, such as UBM, DBN and CCM, and the experiments show that the new approach consistently achieves better perfor-mance than the original inference algorithm of these models.
Another challenge with previous click models is that they are designed for position-bias exclusively. However, we ob-serve that a click may be affected by other factors besides the position. For example, a user click is affected by the relevance between a query and a snippet, which can be mea-sured by the BM25 score. The PBI approach is capable of capturing more sophisticated information into a click model to interpret user clicks accurately. In this paper, we include seven measures such as BM25 and PageRank scores into the previous click models through PBI, and the experimental results demonstrate that the integration of these measures yields significant improvement in perplexity and relevance. Furthermore, PBI is an incremental approach, thus it is nat-ural to handle very large-scale data set.

The paper is organized as follows: Section 2 briefly intro-duces previous works on click models including their specifi-cations and hypothesis. In Section 3, the PBI approach will be presented in detail. In section 4, we give three examples on how the PBI approach is applied to the UBM, CCM and DBN click models. In section 5, we demonstrate how to in-tegrate additional measures into the click models. Section 6 reports the experimental results and the conclusion follows.
The user starts a search session by submitting a query to the search engine, the search engine returning the user some ranked documents as search results. The user then browses the returned documents and clicks some of them. We use a binary random variable C j to represent the click events of the document at position j . C j = 1 indicates the user clicks the document at the j th position, while C j = 0 indicates the user does not click this document. We assume that all queries and documents are indexed, so that we can use q i and d j to represent the the query and the document with the index i and j , respectively. Suppose that q i is the query for the current session, the index of the document at the position j is represented by a mapping function  X  ( j ).
The examination hypothesis and the cascade hypothe-sis [7] are proposed in order to simulate user browsing habits. Many existing click models are dependent on these two hy-potheses. When a document is examined it means that the user has checked this document in the search results, and this event is denoted by a binary random variable E which j indicates the position of the document. E j = 1 means that the document at position j is examined and E j = 0 otherwise. The examination hypothesis assumes that a displayed document is clicked if and only if this doc-ument is both examined and perceived as relevant: where R i X  ( j ) measures the degree of relevance between q and d  X  ( j ) . The cascade hypothesis assumes that the user scans linear to the search results, thus, a document is ex-amined only if all the above documents are examined. The first document is always examined:
The CCM model[9] assumes that user starts the exami-nation of the search results from the top ranked document. At each position j , the user can choose to click or skip the document d  X  ( j ) according to the perceived relevance. Ei-ther way, the user can choose to continue the examination or abandon the current query session. The probability of continuing to examine d  X  ( j +1) depends on his action at the current position j :
The DBN model is designed based on the fact that a click does not necessarily indicate that the user is satisfied with this document. Thus, the DBN model[6] distinguishes the document relevance as the perceived relevance and the real relevance, where whether the user clicks a document depends on its perceived relevance while whether the user is satisfied with this document and examines the next document de-pends on the real relevance. Thus, the DBN click model is characterized as: where S j is a binary variable indicating whether the user is satisfied with the document d  X  ( j ) at position j , and the parameters a i X  ( j ) and s i X  ( j ) measure the perceived relevance and real relevance between q i and d  X  ( j ) , respectively.
Different from CCM and DBN, the UBM model[8] does not adopt the cascade hypothesis. Instead, it introduces a series of global parameters  X  rd to measure the probability that the user examines the document d i X  ( r ) at position r after his last click at position r  X  d : where a i X  ( j ) measures perceived relevance. The term C 0 is the abbreviation for C i = C i +1 =  X  X  X  = C j = 0. Figure 2: Graphical representation of the probit Bayesian inference approach, in which  X  1 ,  X  X  X  , X  n are probability parameters of the click model M . Each parameter  X  i is connected to a Gaussian distributed variable x i via the probit link  X  i =  X ( x i ) . Here,  X ( x ) = R x  X  X  X  N ( t ; 0 , 1)d t is the normal cumulative dis-tribution function.
Typically, a click model M is parameterized by a set of unknown variables  X  1 ,  X  X  X  , X  n . The performance of M relies on the values of these parameters. However, since it usu-ally assumes  X  i  X  (0 , 1) for i = 1 ,...,n , this would limit the flexibility of inference algorithms. In this section, we propose a new framework for handling this limitation. The key idea is to associate each  X  i with a Gaussian auxiliary variable x i  X  (  X  X  X  , +  X  ) via a so-called probit link. This approach also facilitates the incorporation of more sophis-ticated information such as PageRank and BM25 into the click model, thus it significantly enhances the generalization of the resulting model (details in Section 5).
We are given a click model M on a series of query ses-sions. When a session is loaded in, we assume that this session contains M impressions, in which C j  X  { 0 , 1 } in-dicates whether the j -th impression has been clicked. Let C j : k denote the vector ( C j ,...,C k ), we define the likelihood function P ( C 1: M |  X  1 ,  X  X  X  , X  n ) as In this paper, we assume that f is a polynomial function of  X  1 ,  X  X  X  , X  n . This assumption is satisfied in most existing click models, such as the Cascade model, UBM, DCM, CCM and DBN.

In our framework, we introduce the auxiliary variables x i for each  X  i . The connection between  X  i and x i is further defined by where  X ( x ) = R x  X  X  X  N ( t ; 0 , 1)d t , the cumulative distribution function of the standard normal distribution is referred to as the probit link [3, 4]. Thus, we rewrite the likelihood function in (1) as
Given a session, the order of  X ( x i ) is defined as its highest-order power in (2). Moreover, x i is called an active variable if the order of  X ( x i ) is non-zero. Furthermore, we assume that x i independently comes from the Gaussian distribution N ( x i ;  X  i , X  2 i ). Figure 2 illustrates the framework.
In this framework, the motivation of using the probit link instead of using other links (the logistic link, for example), is mainly due to the desirable computational property pro-vided by the probit link. More specifically, it is because that there are several non-trivial integration steps involved in the inference, whose computational tractability and ef-ficiency relies on the close relationship between the probit link and the Gaussian distribution. For example, in order to compute the marginal distribution of a specific variable from the joint distribution, we have to integrate out all other variables; furthermore, when we try to use the variational method to approximate a density function to be the Gaus-sian density, we will encounter a similar integration prob-lem. In these cases, if the function to be integrated does not hold an appropriate property, the integration may be-come very inefficient or even intractable. Recall that the inference algorithm in this paper is designed to process ter-abytes amounts of data, so we have to make sure that the computation can be carried out very fast, as well as with sufficiently high precision. In the Appendix, we introduce an efficient integration algorithm to handle these problems. As suggested above, this algorithm requires that the probit link is employed instead of other links.

In addition, since we allow the form of the likelihood function f to be arbitrary polynomial, the computational tractability is also a big reason to encourage us to propose a Bayesian framework for click model inference, instead of other methods such as logistic regression. We now develop an inference algorithm for the framework. That is, we want to estimate the parameters (  X  i , X  2 i ) from pose, we use an online learning scheme referred to as Gaus-sian density filtering [13]. This scheme first approximates p ( x i | C 1: M , X  i , X  2 i ) as a Gaussian distribution and then up-dates the estimates of (  X  i , X  2 i ) based on this Gaussian. For each query session, this updating procedure is executed once.
For a specific active variable x i , the posterior distribution of x i under the click observation C 1: M is of x j . The marginal likelihood function of x i is then obtained by integrating out all x j  X  X  but x i from the function f , namely
P ( C 1: M | x i ) (4) Suppose that the function (2) is expanded as the form where k tj is the power of  X ( x j ) in the t -th term of the ex-pansion. Accordingly, we write the integral in (4) as where Algorithm 1 The Inference Algorithm 1: for each query session do 2: Derive the likelihood function (2). 3: For each active variable x i  X  { x 1 ,  X  X  X  ,x n } , compute 4: for each active variable x i  X  X  x 1 ,  X  X  X  ,x n } do 5: Evaluate P ( C 1: M | x i ) according to (5). 6: Approximate p ( x i | C 1: M , X  i , X  2 i ) by Gaussian distri-7: Update the parameters  X  i and  X  2 i by setting  X  i  X  8: end for 9: end for for all active variables x i and all powers 0  X  k  X  K i . Here K i is the order of  X ( x i ). The details of computing c ik given in the Appendix. We now rearrange (5) into the stan-dard form
We can see from (3) and (7) that p ( x i | C 1: M , X  no longer Gaussian. This makes the inference inefficient. The idea behind the Gaussian density filtering method is to N ( x i ;  X   X  i ,  X   X  2 i ) (denoted by q ( x i |  X   X  i prior distribution for the next session.
 Kullback-Leibler (KL) divergence between p ( x i | C 1: M first three-order moments of p ( x i | C 1: M , X  i , X  2 i reduced to the evaluation of the integrals in (6) and It is worth pointing out that it is not trivial to compute the integrals in (6), (8) and (9). In the Appendix, we de-vise an efficient approach to the computation of (6), (8) and (9) by using Expectation Propagation [13] and the iterative message passing on factor graphs[12].

It then follows from (7) that  X   X  i and  X   X  2 i is given by Thus, we complete the updating procedure (  X  i , X  2 i ) for x Similar updates should be performed on all active variables before the next session is loaded into the model.

The inference algorithm is summarized in Algorithm 1. It is an incremental updating algorithm as query sessions are sequentially loaded into the click model.
After the (  X  i , X  2 i ) have been obtained, the value of each parameter  X  i is estimated by the expectation of  X ( x i ) with respect to p ( x i |  X  i , X  2 i ); that is,  X  i is given by With the estimated  X  i , the click model M (  X  1 ,..., X  make predictions following its own predictive algorithm.
We demonstrate how specific click models are inferred by using the probit Bayesian inference. According to the PBI framework defined in Section 3.1, it is sufficient to show how the likelihood function (2) is derived from each click model, so that the general inference algorithm in Section 3.2 can be immediately adopted. In this section, queries and documents are indicated by q i and d j , where i or j indicates the index of a specific query or document. We assume that q i is the query for the current session, and  X  ( j ) indicates the index of the document at the j position. There are M documents in the current session. Moreover, since the probit Bayesian inference introduces a hierarchical-style framework (see Figure 2), we call the click model M inferred under PBI the hierarchical M , and abbreviated as H-M .
For UBM, the relevance parameters a kl and the global parameters  X  rd are defined via probit links, i.e., a kl =  X ( u and  X  rd =  X (  X  rd ). Therefore the likelihood function is
Y where d j represents the distance between the position j and the last clicked position before j . If there is no click before the j th position, we set d j = j . This likelihood function is obviously polynomial. For active u or  X  , the orders of  X ( u ) and  X (  X  ) are always 1.
For CCM, the relevance parameters R kl and the global parameters  X  1 , X  2 , X  3 are defined as R kl =  X ( r kl ) and  X   X (  X  k ). In the current session, there are M +3 active pa-likelihood function is given by which is a polynomial function of  X ( r ) X  X  and  X (  X  ) X  X . Fur-thermore, the order of  X ( r i X  ( j ) ) is at most 2, and the order then (13) can be derived via recursive formulas based on the specification of CCM: p 0 ( r , X  ) = 0; p p j ( r , X  ) = (1  X  C j ) p p j ( r , X  ) = p It is straightforward to see that p (0) M ( r , X  ) + p (1) actly equal to (13).
For DBN, the perceived relevance parameters a kl are de-fined as a kl =  X ( u kl ), while the actual relevance parameters s kl are defined as s kl =  X ( v kl ). The likelihood function the function then the following recursions hold: p 0 ( u , v ) = 0; p p j ( u , v ) = (1  X  C j ) p p j ( u , v ) = p It is straightforward to see that p (0) M ( u , v ) + p (1) actly equal to (14). In DBN, the order of  X ( u ) or  X ( v ) is always 1, if it is positive.
In the PBI approach of Section 3, we assume that the auxiliary variables x i follow the Gaussian distributions. In order to capture more sophisticated information into the click model, we develop a Deep Probit Bayesian Inference approach (Deep-PBI). That is, we further extend the frame-work described in Figure 2 to the framework shown in Figure 3. In this new framework, each auxiliary variable x i is in-terpreted as a linear combination of several factors. Each of these factors follows the Gaussian distribution. In this paper, we define two kinds of factors: the historical mirror and the weighted combination of additional measures.
The historical mirror is written as h i . Its distribution corresponds to the posterior distribution of x i computed in its last training epoch. Note that h i should be initialized by a default Gaussian before the first training epoch of x i
The weighted combination of additional measures is writ-ten as the inner product y T w of two vectors y and w . Here Figure 3: Graphical representation of the deep pro-bit Bayesian inference approach. This framework is capable of integrating additional measures into click models.
 Figure 4: Factor graph for updating h i and w. The large rectangles or plates indicate parts of the graph which are repeated with repetition indexed by the variable in the corner of the plate. The factors la-beled  X  are sum factors of the form I [ z = x + y ] . y represents the values of measures extracted from the cur-rent session, which may contain the background description associated with x i , such as the BM25 score and the PageR-ank score. w is a global vector, regulating the weight of each measure. Each element of w is a Gaussian variable, which is updated throughout the training process. Thus, x is defined by where  X  ( h i ) is a function of h i and regulates the proportion of contribution of additional measures. Generally speak-ing,  X  ( h i ) should be a monotonically decreasing function of  X  ( h i ), which means that the value of additional measures gives a major contribution to x i only if we have little confi-dence on the precision of h i .

The factor graph in Figure 4 is constructed according to the joint distribution of x i , h i and w . Based on this graph, the prior of x i can be evaluated from the priors of h i and w following the sum-product algorithm [12], computed in a forward manner. Once we obtain the prior of x i , we can apply the inference algorithm in Section 3 to do inference and prediction.

In the deep probit Bayesian inference, the posterior of x is approximated by a Gaussian distribution N ( x i ;  X   X  Moreover, this Gaussian posterior is used for setting the observation factor of the factor graph, instead of updating Table 1: The summary of the data set in the exper-iment x directly. The observation factor is given by where p ( x i ) represents the prior of x i . Then, the sum-product algorithm is applied once again to compute the posterior marginal distribution for each element of w , in a backward manner. Since all factors in the factor graph are Gaussian, the computation is very efficient. The final step is to update the mean and variance of each weight according to its posterior. On the other hand, the mean and variance of h i is updated by  X   X  i and  X   X  2 i .

The deep probit Bayesian framework proposed in this sec-tion leads to the deep hierarchical version of UBM, CCM and DBN, which are denoted by H 2 -UBM, H 2 -CCM and H -DBN.
In the experiment, we include three state-of-the-art mod-els: UBM, CCM and DBN, into the evaluation. The exper-imental results are compared between the original inference algorithm of click models, the probit Bayesian inference, and the deep probit Bayesian inference (with additional informa-tion integrated). There are three parts of the evaluation con-sidered in this paper. In this first part, the click perplexity is evaluated to demonstrate the performance improvement after the previous click models are inferred using the new approach. Moreover, we give a comparison among differ-ent click models inferred by the same version of PBI. In the second part, we use NDCG to evaluate the relevance ob-tained from click models. Finally, the efficiency of the PBI and Deep-PBI approach is discussed. All of the experiments were carried out on a 64-bit server with 32GB RAM and sixteen 2.53GHz processors.
The click logs used to train the click models are collected from a large commercial search engine which comprises 13,679 queries and 12.2 million sessions. The dataset is divided ran-domly and evenly into a training set with 6,104,278 sessions and a testing set with 6,095,267 sessions. When generating the training and testing data, we restrict all queries in the testing set to have at least one session included in the train-ing set. Table 1 reports the number of queries with respect to the query frequency.
 In the experiments, the original inference algorithm of UBM, CCM and DBN exactly follows their conventions in [8, 9, 6]. The original DBN inference requires a input of the Figure 5: Perplexity of the DBN model as a function of  X  . The leftmost figure is on all queries; the mid-dle figure is on the queries with frequencies lower than 10 ; the rightmost curve is on the queries with frequencies higher than 30 , 000 . parameter  X  . To determine this input, the click perplexi-ties (see definition in Section 6.2) on the entire data set are evaluated using the original DBN inference algorithm with a series of distinct  X  values. We found the optimal value of  X  is 1, while  X  = 0 . 7 is a local minimum, as shown in Figure 5(a). To understand this phenomenon, we plot the curve of the perplexity on the queries with the lowest and the high-est frequency respectively for distinct  X  , which are ploted in Figure 5(b) and Figure 5(c). The curves show that the optimal  X  is not uniform on different frequencies. According to the figure, the optimal  X  is close to 0 . 7 for low frequency queries, but this value is 1 for high frequency queries. Hence, in order to guarantee fairness in experiments, we evaluate the performance of DBN on both  X  = 0 . 7 and  X  = 1. For the original CCM inference algorithm, the global parame-ters  X  1 , X  2 , X  3 are trained following the method introduced in [9]. Since the original inference algorithm of CCM is not capable of handling highly frequent queries, the experiments for CCM are restricted on the queries with frequencies less than 3 , 000, which is in accordance with the experimental setup in [9].

When click models are inferred using the PBI approach, besides all perceived relevance or actual relevance parame-ters, global parameters such as  X  rd of UBM and  X  1 , X  2 , X  of CCM are also trained under the PBI approach. The prior distributions of all variables are initialized to the standard normal distribution. In other words, we did not include any prior knowledge on the data set. The click models inferred by their original inference algorithms (named as UBM, CCM and DBN as usual), the click models inferred by PBI (named as H-UBM, H-CCM and H-DBN, as described in Section 3) and the click models inferred by Deep-PBI (named as H 2 -UBM, H 2 -CCM and H 2 -DBN, as described in Section 5) are evaluated separately. When additional measures are inte-grated, the historical mirrors are initialized to follow the standard normal distribution. Moreover, we define In other words, the weights of measures are updated and take effect only when a variable x i is activated at the first time. This strategy restricts the influence of additional mea-sures only for initialization, and has been shown to be very effective in the experiment.

There are seven measures extracted in the experiment, which are summarized in the below table.
 Table 2: The percentage on the click perplexity im-provement achieved by Deep-PBI (with seven addi-tional measures integrated).
 In experiments, the vector y is the catenation of seven sparse binary vectors y = f 1 f 2  X  X  X  f 7 . Since the number of possible values of each measure is finite, we can give all possible values an order and denote the j -th possible value of the i -th measure by v ij . Then we set the j -th element of f if v ij is the current value of the i -th measure, and set to 0 otherwise. All weight variables are initialized to the normal distribution N (0 , 1 7 ), so that the inner product y T w in the initialization satisfies the standard normal distribution.
Click perplexity is widely used to measure model accu-racy in click prediction. It has been used as the evaluation metric in [8, 9] for the UBM and CCM click models. It is computed for binary click events at each position of a query session independently. We assume that q i j is the probability of the click derived from the click model, i.e. P ( C i j position j and C i j is a binary value indicating the click event at position j on the i th session. Thus, the click perplexity at position j is computed as follows: The perplexity of a data set is defined to be the average of all position perplexities. Thus, a smaller perplexity value in-dicates a better prediction. The improvement of perplexity value p 1 over p 2 is given by p 2  X  p 1 p
The click perplexity on the testing set is reported in Fig-ure 6 over different frequencies. It is observed that H 2 H -CCM and H 2 -DBN significantly and consistently outper-form the original version of UBM, CCM and DBN. The per-formance of H-UBM and H-CCM is also much better than the original UBM and CCM, while H-DBN is fairly compa-rable with the original DBN on both  X  = 0 . 7 and  X  = 1. The perplexity improvement achieved by PBI and Deep-PBI are relatively higher on low-frequency queries, which is summa-rized in Table 2. In the original UBM inference, the docu-ment that is never clicked in the training set will eventually have its relevance converging to zero. However, if this docu-ment is clicked in the testing set, the corresponding perplex-ity punishment will be considerable. This explains the poor performance of UBM on low frequency queries. As shown in Figure 6(a), the Bayesian inferred version, either H-UBM or UBM, H-CCM and H-DBN stand for the models inferred by PBI ; H H -UBM, achieves a good perplexity score on both low and high frequency queries. Since the gap between the H-UBM curve and the H 2 -UBM curve is small, we can conclude that the additional measures are not very helpful to UBM.
For CCM, the curves of H-CCM and H 2 -CCM are signifi-cantly lower than the original CCM curve in both high and low frequency queries, as shown in Figure 6(b). Moreover, the integration of seven measures helps a lot on low fre-quency queries. However, as query frequency increases, the improvement provided by additional measures continuously fades and eventually disappears.

For DBN, we observed from Figure 6(c)(d) that the curve of H-DBN is fairly close to that of the original DBN. In other words, the Bayesian inference and the EM inference have similar performance on the DBN model. However, as addi-tional measures are integrated, H 2 -DBN performs much bet-ter than the original DBN on low frequency queries. When  X  = 0 . 7, the original DBN gives a relatively precise pre-diction on low frequencies, but not precise enough on high frequency queries. Thus, it does not gather the best per-plexity score on the entire dataset, as shown in Figure 5. On the other hand, we discovered that H-DBN with  X  = 0 . 7 does not hold this defect, which indicates the Bayesian in-ference is less sensitive to inappropriate values of  X  on high frequency queries.

One advantage of the PBI approach is that it provides a convincing comparison among click models, because after a click model is inferred by the new approach, none of the model assumptions are changed and the inference method Figure 7: Performance comparison between H-UBM, H-CCM and H-DBN on perplexity metrics.
 The test is reported on queries with frequencies no more than 3,000. becomes the same. In Figure 7, this comparison is reported on H-UBM, H-CCM and H-DBN. Since CCM is evaluated on queries with frequencies less than 3 , 000 and other models perform very similarly on higher frequency queries, thus, in this experiment the comparison is reported on the range of frequencies that CCM can handle. It is observed that H-UBM gains the best perplexity score on all frequencies, followed by the  X  = 0 . 7 version of H-DBN. This is because the PBI approach improves the performance of UBM on low frequencies, and improves the performance of DBN on high frequencies. The perplexity score achieved by H-CCM is Figure 8: Average number of iterations when c ik , u ik and v ik are computed following the iterative al-gorithm in the Appendix. This curve is plotted in the training process of H-CCM. a bit worse than H-DBN(  X  = 0 . 7), which implies that the assumptions of CCM are not as reasonable as that of UBM and DBN on this dataset. The  X  = 1 version of H-DBN gets the worst perplexity score on low frequencies, which implies that  X  = 1 is not a reasonable setting for low frequency queries.
The Normalized Discounted Cumulated Gain (NDCG)[10] is used as a metric for DBN[6] to measure the document relevance inferred for search ranking. In the NDCG eval-uation, we compare DBN with H-DBN and H 2 -DBN. The relevance obtained from UBM and CCM are not reported because UBM and CCM are designed to estimate the doc-ument perceived relevance rather than the actual relevance. Thus, both of them perform significantly worse than DBN on NDCG metrics.

Here, the  X  parameter is fixed at 1 because  X  = 1 leads to much better NDCG scores than  X  = 0 . 7 (the  X  = 1 version is 13 . 2% and 9 . 4% better than the  X  = 0 . 7 version on NDCG@1 score and NDCG@5 score over all frequency queries). For each query, we rank the returned documents according to the actual relevance a i X  ( j )  X  s i X  ( j ) , then the NDCG scores are evaluated on queries and related documents whose relevance ratings exist in the HRS (Human Relevance System), where professional editors provide a five grade rating between a query and a document (4: perfect, 3: excellence, 2: good, 1: fair, 0: bad ).

The NDCG scores are reported in Figure 6. Similar to the result in click perplexity, H-DBN achieves similar NDCG scores with the original DBN model. However, when ad-ditional measures are integrated in, the NDCG scores are substantially increased. The NDCG@1 scores of DBN, H-DBN and H 2 -DBN are 0 . 6215, 0 . 6207 and 0 . 7113 over all frequencies; the corresponding NDCG@5 scores are 0 . 6361, 0 . 6342 and 0 . 6826. Compared with the performance from H -DBN and DBN, H 2 -DBN is 14 . 6% better on NDCG@1 and 7 . 63% better on NDCG@5, both of which are considered to be very significant improvements.
The efficiency of Algorithm 1 is mainly determined by the efficiency of its third step. That is, the procedure in which c ik , v ik and v ik are computed. These computations are archived by the evaluation algorithm presented in the Appendix. For UBM and DBN, the order of  X ( x ) is always 0 or 1, hence the evaluation algorithm is never iterative, which guarantees the whole inference efficient. For CCM, the order of  X ( x ) may equal to 2 when x represents the document relevance, or be even higher when x represents the global parameters. In these cases, the assignments of Step A and Step B in the Appendix may need to be iterated for several times before  X  B and  X  B converges. Obviously, the average number of iterations should be small enough to keep the whole inference efficient.

In the experiment, we require that the iterative assign-ment terminates when  X  B and  X  B varies no more than 10  X  8 between two consecutive iterations. The average number of iterations for computing c ik , u ik and v ik is then evaluated in the training process of H-CCM and the result is reported in Figure 8. As we see, this number continuously decreases as an increasing number of sessions are processed. After the training finishes, we find that the average number of iterations is only 2 . 04. According to this result, and also considering the incremental nature of our approach, we can conclude that the efficiency of PBI and Deep-PBI is com-petitive compared with the original inference algorithm of click models.
In this paper, we have proposed a probit Bayesian in-ference approach for click models and have applied it to the DBN, CCM and UBM models. We have developed a framework that can be applied on most of the existing click models, and an efficient algorithm for training and predic-tion. The experiments have demonstrated that new ap-proach achieves higher generalization ability than the origi-nal inference algorithm of previous click models. Moreover, this new approach has provided a unified inference method so that different click models have been compared with each other. Our proposed approach has been capable of inte-grating more sophisticated information into a click model through a deep hierarchical extension, and it has provided a significantly better performance on click perplexity and search ranking. In our future work, we will further explore its potential applications in click models.
 This work was supported in part by the National Basic Re-search Program of China Grant Nos.2007CB807900, 2007CB 807901, the National Natural Science Foundation of China Grant Nos.60604033, 60553001,and the Hi-Tech research and Development Program of China Grant No.2006AA10Z216 [1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. [2] R. Agrawal, A. Halverson, K. Kenthapadi, N. Mishra, [3] J. H. Albert and S. Chib. Bayesian analysis of binary [4] C. M. Bishop. Pattern Recognition and Machine [5] B. Carterette and R. Jones. Evaluating search engines [6] O. Chapelle and Y. Zhang. A dynamic bayesian [7] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [8] G. Dupret and B. Piwowarski. User browsing model to [9] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, [10] K. Jarvelin and J. Kekalainen. Cumulated gain-based [11] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [12] F. R. Kschischang, B. J. Frey, and H.-A. Loeliger. [13] T. Minka. A family of algorithms for approximate [14] F. Radlinski and T. Joachims. Query chains: learning [15] M. Richardson, E. Dominowska, and R. Ragno.
 Suppose we have a Gaussian variable x i with mean  X  i and variance  X  2 i . We now compute c ik , u ik and v ik mentioned in Section 3.2. If k = 0, the computation is trivial. If k  X  1, it is worthy noting that gives the approximation to with the minimum KL-divergence between both. Thus, if we find a function in the form of (15) which approximates (16) by minimizing the KL-divergence, we can retrieve the values of c ik , u ik , v ik from this function X  X  parameters directly.
We give the computational approach as follow. Essen-tially, this approach is implemented by first approximat-ing (16) and then retrieving the values we need from (15). The approximation is made by using Expectation Propa-gation[13] and the iterative message passing algorithm on factor graphs[12]. Several intermediate variables  X  A  X  ,  X  B and d 0 , d 1 , d 2 are introduced to simplify the rep-resentation of formulas. At the beginning of the algorithm,  X 
B and  X  B are initialized to be 0 and 1 respectively, if no prior knowledge is available. The following two steps of as-signments should be sequentially executed.
 Step A Step B If k = 1, the above assignments are performed only once. If k  X  2, Step A and Step B should be iteratively executed for several times until  X  B and  X  B do not change any more. Then we have
After the computation completes, we store the values of  X 
B and  X  B . Thus at the next time when c ik , u ik and v are evaluated,  X  B and  X  B can be initialized by the stored values.
