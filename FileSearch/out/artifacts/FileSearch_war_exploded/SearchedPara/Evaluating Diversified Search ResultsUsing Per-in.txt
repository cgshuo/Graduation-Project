 Search queries are often ambiguous and/or underspecified. To ac-comodate different user needs, search result diversification has re-ceived attention in the past few years. Accordingly, several new metrics for evaluating diversification have been proposed, but their properties are little understood. We compare the properties of ex-isting metrics given the premises that (1) queries may have multiple intents; (2) the likelihood of each intent given a query is available; and (3) graded relevance assessments are available for each intent. We compare a wide range of traditional and diversified IR metrics after adding graded relevance assessments to the TREC 2009 Web track diversity task test collection which originally had binary rel-evance assessments. Our primary criterion is discriminative power, which represents the reliability of a metric in an experiment. Our results show that diversified IR experiments with a given number of topics can be as reliable as traditional IR experiments with the same number of topics, provided that the right metrics are used. Moreover, we compare the intuitiveness of diversified IR metrics by closely examining the actual ranked lists from TREC. We show that a family of metrics called D -measures have several advantages over other metrics such as  X  -nDCG and Intent-Aware metrics. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Experimentation ambiguity, diversity, evaluation, graded relevance, test collection
Traditional information retrieval (IR) research has mostly fo-cussed on satisfying clearly specified information needs. However, in Web search, queries are often ambiguous and/or underspeci-fied [9]. When a search engine has no or little knowledge of the user, the best it can do may be to produce an output that reflects several interpretations (or intents ) of such queries to accomodate a large population of users. In light of this, search result diversi-fication is beginning to receive attention [1, 6, 10, 15, 24]. While modern search engines can display multiple blocks of information, some textual and others nontextual, a ranked list of URLs remains the primary output feature today. Hence how to appropriately eval-uate a diversified list of URLs is an important research problem for advancing the state-of-the-art of search engines.
 A popular method for evaluating traditional (i.e. non-diversified) IR is to use normalised Discounted Cumulative Gain (nDCG) [11] based on graded relevance assessments, taking into account the fact that some relevant documents are more relevant than others. Assuming that we can list up some possible intents for a given ambiguous or underspecified query in advance, a natural exten-sion of the above evaluation framework would be to hire asses-sors to conduct graded relevance assessments for each intent of that query. For example, for the ambiguous query  X  X pple, X  we should be able to collect documents that are highly/marginally relevant to the intent  X  X pple the Steve Jobs company, X  and those that are highly/marginally relevant to the intent  X  X pple the fruit. X  More-over, if we know that some intents are more likely than others ,we should probably reflect that information in evaluating search en-gines. For example, if a search query log suggests that users are more likely to search for  X  X pple the company X  than for  X  X pple the fruit, X  the search engine may choose to return more URLs relevant to the former than ones relevant to the latter.

Recently, several evaluation metrics for handling search result diversification have been proposed. Surprisingly, however, not all of them accomodate per-intent graded relevance and intent prob-abilities . The objective of this study is to compare the reliability of diversified IR metrics as well as traditional IR metrics on which the diversity metrics are based. By reliability, we informally mean the ability of a metric to detect  X  X eal X  performance differences as opposed to those observed by chan ce. As a measure of reliability, we use discriminative power [19] which has been used in several evaluation studies over the past five years [7, 17, 20, 25, 29]. In addition, we closely examine some actual ranked lists from TREC and discuss the intuitiveness of different diversified IR metrics.
To our knowledge, our work is the first to use a diversity test collection with per-intent graded relevance data for the purpose of comparing diversified IR metrics. This is somewhat surprising, as many of the existing diversity metrics can handle per-intent graded relevance. We first add graded relevance assessments to the TREC 2009 Web track diversity task test collection which originally had binary relevance assessments. We then examine a wide variety of metrics: six for traditional IR, and eleven for diversified IR. We show that a family of metrics called D -measures have several ad-vantages over  X  -nDCG [8] and Intent-Aware metrics [1], including high discriminative power and intuitiveness. We also show that the  X  X est X  diversified IR metrics can be as discriminative as the  X  X est X  traditional IR metrics, given the same number of topics.
The remainder of this paper is organised as follows. Section 2 discusses previous work related to the present study, and Section 3 formally defines the traditional and diversified IR metrics that we examine. Section 4 describes discriminative power, our primary criterion for choosing reliable metrics. Section 5 describes our test collections with per-intent graded relevance data, and Section 6 re-ports on our experiments. Finally, Section 7 concludes this paper.
Zhai, Cohen and Lafferty [31] used subtopic recall for evaluating subtopic retrieval . In the present study, we evaluate subtopic recall, but call it intent recall . Zhai, Cohen and Lafferty also defined S-precision and WS-precision at a given subtopic recall level, but the computation of these two metrics involves an NP-hard problem and an approximation is required. Carterette and Chandar [4] evaluated faceted topic retrieval , a task similar to subtopic retrieval. Their evaluation metric was subtopic recall at l min ,where l min minimum rank at which perfect S-recall can be achieved. But they point out that computing l min is NP-hard.

Clarke et al. [8] proposed a metric called  X  -nDCG for evaluating diversified search results. They view both intents and documents as sets of nuggets , and assume that the number of nuggets covered by a document determines the graded relevance of that document. Prior to discounting the gain of a document based on its rank (as traditional nDCG does [11]),  X  -nDCG discounts the gain based on  X  X uggets already seen. X  This metric was one of the official metrics used at the TREC 2009 Web track diversity task [6], and the present study uses the official  X  -nDCG values from TREC for computing its discriminative power.
 Clarke, Kolla and Vechtomova [9] proposed Novelty-and Rank-Biased Precision (NRBP) by combining the ideas of  X  -nDCG and Rank-Biased Precision [13]. NRBP was used at the recent TREC 2010 Web track diversity task. However, Sakai et al. [20] argue that NRBP inherits the weaknesses of both  X  -nDCG and RBP: one practical weakness of (N)RBP is that it is heavily undernomalised for topics with few relevant documents and does not average well. Moreover, the advantage of NRBP over  X  -nDCG is not clear (at least in terms of discriminative power) in subsequent experiements reported by Clarke et al. [7]. We therefore do not consider NRBP.
Agrawal et al. [1] proposed Intent-Aware (IA) metrics for eval-uating diversified search results. They were the first to explicitly incorporate intent probabilities in IR evaluation. Their approach is simple: compute a traditional metric for each intent andthenfinally take an expectation based on the intent probabilities. Clarke, Kolla and Vechtomova [9] have discussed the possibility of evaluation with IA versions of  X  -nDCG and NRBP.

Chapelle et al. [5] proposed Expected Reciprocal Rank (ERR) for traditional IR evaluation, and claimed that its IA version can handle diversified IR evaluation. ERR-IA was used at the TREC 2010 Web track diversity task. The essence of ERR is that relevant documents are discounted based on the number of relevant docu-ments already seen rather than the absolute document ranks.
Robertson, Kanoulas and Yilmaz [17] recently proposed a met-ric for traditional IR evaluation called Graded Average Precision (GAP). GAP assumes that the user has a binary notion of relevance, but that different users have different thresholds over the relevance levels. The present study examines (for the first time) an IA version of GAP, as well as its normalised version.

Sakai et al. [20] proposed an alternative method for evaluat-ing diversified search results. Their key idea is to define (global) graded relevance by combining intent probabilities and per-intent graded relevance. Based on an ideal ranked list thus defined, they introduced a family of metrics which we call D-measures and D -measures . Using the TREC 2009 Web diversity task data which have per-intent binary relevance assessments, they reported that D -measures perform at least as well as  X  -nDCG and intent recall in terms of discriminative power, while solving some shortcomings of  X  -nDCG and IA metrics.

Using the same TREC 2009 data set, Clarke et al. [7] also com-pared different diversified IR metrics (with and without  X  X ollection-dependent X  normalisation) in terms of discriminative power. They pointed out that IA metrics do not necessarily reward high intent recall. Surprisingly, all of the diversified IR metrics they examined (including  X  -nDCG and NRBP) underperformed the simple intent recal l in terms of discriminative power.
 The present study is similar to the work by Sakai et al. [20] and Clarke et al. [7], but has the following new contributions: (a) It is the first to compare different diversified IR metrics when per-intent graded relevance assessments are available . (b) It examines the most extensive set of diversified IR metrics, including (normalised) GAP-IA which is being examined for the first time. All of these metrics can handle graded relevance: hence, in previous work, the full potential of these metrics for diversity evaluation has not been demonstrated. (c) While the previous two studies considered either a uniform [7] or non-uniform [20] intent probability distribution, this study considers both. (d) This study investigates the effect of measurement depth on discriminative power, in contrast to previ-ous work which only considered a document cutoff of 10 [20] or 20 [7]. (e) It uses multiple test collections for evaluating traditional IR metrics on which our diversified IR metrics are based.
By employing Mechanical Turk users, Sanderson et al. [23] ex-amined the predictive power of metrics: if a metric prefers one ranked list over another, does the user also prefer the same list? Using the same TREC 2009 Web diversity task data, they exam-ined some traditional and diversified IR metrics. One of their find-ings was that diversified IR metrics agree reasonably well with human preferences. Clearly, discr iminative power is not the only way to evaluate evaluation metrics (See Section 4), and other ap-proaches, especially those that rely on human subjects (e.g. predic-tive power), should complement our work.

More recently, Brandt et al. [3] proposed a framework for pre-senting a tree of retrieved URLs dynamically, which includes an evaluation method that is a tree version of the IA approach. This probably deserves an investigation in terms of the user X  X  physical and cognitive load when compared to our flat-list approach.
First, we formally define some graded-relevance evaluation met-rics that have been designed for traditional IR evaluation.
Our first premise is that we have relevance levels { 0 ,...,h with 0 representing nonrelevance and h representing the highest level. Hence h =1 implies a binary relevance environment. We say that a document is Lx -relevant if its relevance level is x ( 0 &lt; x  X  h ). Let R x denote the number of Lx -relevant documents for a topic and let R = x R x .Let J ( r )=1 if a document at rank r is Lx -relevant ( x&gt; 0 )and J ( r )=0 otherwise. Let C ( r )= r k =1 J ( k ) .

Let GV x denote the gain value for retrieving an Lx -relevant document [11]. Let g ( r )= GV x if a document at rank r is Lx -relevant and g ( r )=0 otherwise. Further, let cg ( r )= r We call g ( r ) and cg ( r ) the (cumulative) gain at rank r . Also, let g ( r ) and cg  X  ( r ) denote the (cumulative) gain at rank r in an ideal ranked list , obtained by exhaustively listing up Lx -relevant docu-ments in descending order of relevance levels.

We define nDCG and Q-measure at document cutoff l as fol-lows [20]: where  X  (  X  0 ) is a user persistence parameter for Q. (  X  =0 re-duces Q to binary Average Precision.) We let  X  =1 throughout this paper.

Next, we discuss Graded Average Precision (GAP) [17]. GAP assumes that the user considers only relevance levels x,...,h as relevant ( x&gt; 0 ) and the rest as nonrel evant with probability p and that h x =1 p x =1 . Following a recommendation by Robert-son, Kanoulas and Yilmaz [17], w e consider a uni form probability distribution: p x =1 /h for x  X  1 . In this particular case, GAP can be defined as follows.

Let X ( r )  X  X  0 ,...,h } denote the relevance level of a document at rank r ,andlet M ( r, k )=min( X ( r ) ,X ( k )) for any pair of ranks ( r, k ) . Then the Expected Precision (EP) [17] at rank r is given by EP ( r )= 1 r r k =1 M ( r,k ) x =1 p x . Under the uniformity assumption, it can be rewritten as:
EP ( r )=
Similarly, under the same assumption, the maximum possible value of cumulated EP is: h x =1 R x x y =1 p y = Note that the above denominator represents a summation over all Lx -relevant ( x&gt; 0 ) documents, and therefore can also be written as R r =1 X  X  ( r )( X  X  ( r )+1) ,where X  X  ( r ) is the relevance level of a document at rank r in an ideal output. For the purpose of Web search evaluation where the number of documents to be evaluated is typically very small (e.g. l =10 ), a division by the above sum over all relevant documents yields a heavily undernormalised metric. (It is usually impossible to list up all relevant documents if there is space for only 10 URLs.) Hence, we also define normalised GAP (nGAP) for evaluation with a small document cutoff:
Next, we define Expected Reciprocal Rank (ERR) [5]. Let Pr ( r ) denote the relevance probability of a document at rank r .Let dsat ( r )= r k =1 (1  X  Pr ( k )) , which is interpreted as the prob-ability that the user is dissatisfied with documents from ranks 1 to r . Then ERR is defined based on the expected probability that the user is finally satisfied at rank r : Following Chapelle et al. [5], we let Pr ( r )=(2 X ( r )  X  Note that this makes ERR a very top-heavy metric: any ranked list that has a document of the highest relevance level ( h )atrank1 receives an ERR of (2 h  X  1) / 2 h or higher. Thus, if we have h =3 relevance levels, the ERR is 7 / 8= . 875 or higher; if h =4 ,the ERR is .938 or higher. Also, in accordance with the above setting for ERR, we let g ( r )=2 X ( r )  X  1 for nDCG and Q. This gain value setting appears to be the de facto standard for nDCG 1 .
Finally, for completeness, we also consider normalised ERR [7] for evaluation with a small document cutoff: where Pr  X  ( r ) is the relevance probability of a document at rank r in an ideal ranked list and dsat  X  ( r )= r k =1 (1  X  Pr  X  ever, we expect the impact of normalisation for ERR to be small since unnormalised ERR already tends to take very large values, as we have discussed earlier.
Clarke et al. [8] proposed  X  -nDCG for evaluating diversified search results. They view information needs (i.e. intents) and doc-uments as sets of nuggets . In their framework, the assessor judges whether each document contains a nugget or not (i.e. makes a bi-nary decision). Let J n ( r )=1 if a document at rank r is relevant to the n -th nugget and 0 otherwise; let C n ( r )= r k =1 the number of documents observed within top r that contained the n -th nugget. Then novelty-biased gain NG ( r ) is defined as: where m is the total number of nuggets for the query and  X  ( &lt; 1) is a parameter. Then  X  -nDCG can be defined by replacing the raw gain values g ( r ) and g  X  ( r ) in Eq. 1 with the novelty-biased gains. Thus,  X  -nDCG discounts gains first based on  X  X uggets al-ready seen, X  and then based on document ranks. Note that Eq. 9 defines graded relevance simply based on the number of nuggets that a document covers as well as nugget novelty.

Computing the ideal ranked list for  X  -nDCG is NP-complete [8] and an approximation is required. In this study, we simply use the official  X  -nDCG values reported at the TREC 2009 Web diversity task where  X  = . 5 [6]. Note that this setting of  X  corresponds to the assumption that the assessor  X  X inds X  a nonexistent nugget in a document 50% of the time but never misses an existing nugget, which is arguably counterintuitive [20].
Agrawal et al. [1] proposed a simple methodology for evaluat-ing diversified search results. First, we assume that, given a query q with several different intents i , the probability of each intent Pr ( i | q ) can be estimated, where i Pr ( i | q )=1 . Second, we assume that document relevance assessments are available for each intent. Then, for example, nDCG for a particular intent i (nDCG can be computed first, and finally nDCG-IA can be computed as:
GAP relies on a different notion of graded relevance and its graded relevance setting in our paper is not strictly equivalent to that we use for nDCG, Q and ERR. In the case of GAP with h =3 ,forex-ample, the uniform probability distribution implies that all users re-gard L 3 -relevant as relevant, while 67% of them regard L 2 -relevant as relevant, and only 33% regard L 1 -relevant as relevant [17]. Thus, unlike  X  -nDCG, IA metrics accomodate both intent proba-bilities and per-intent graded relevance 2 . Other IA metrics can be computed similarly: in the present study, we consider nDCG-IA, (n)GAP-IA and (n)ERR-IA.

One of the disadvantages of IA metrics is that their maximum value is not 1: a single ranked list is almost never ideal for every intent [20]. More importantly, IA metrics tend to be counterintu-itive, in that they practically disregard minor intents (i.e. those with relatively low Pr ( i | q ) values) [7, 20].
Sakai et al. [20] proposed an alternative way to evaluate diversi-fied search results, given intent probabilities and per-intent graded relevance assessments. This solves the undernormalisation prob-lem of IA metrics and also includes a mechanism for explictly boosting intent recall , i.e. number of intents covered by a ranked list.

Let us say that document d is relevant to q iff it is relevant to at least one of its intents (  X  X  i } ). If we assume that the intents are ex-clusive (i.e. a user searching with q has exactly one intent), then the Probability Ranking Principle (PRP) [16] reduces to ranking doc-uments by i Pr ( i | q ) Pr ( rel =1 | i, d ) ,where Pr ( rel =1 is the probability that document d is relevant to i . Note that rele-vance is a binary notion here. Let GV i,d denote a gain value for document d with respect to intent i . If we can assign these values so that Pr ( rel =1 | i, d )  X  GV i,d , then the PRP reduces to rank-ing documents by i Pr ( i | q ) GV i,d . This defines a globally ideal ranked list, which retrieves documents that are highly relevant to major intents above those that are marginally relevant to minor in-tents. Based on this ideal list, cumulative-gain-based metrics such as nDCG and Q can be computed, by replacing the raw gain g ( r ) discussed in Section 3.1 with the global gain : where g i ( r ) is the gain value for document at rank r for intent i . Following Sakai et al. , we apply this idea to nDCG and Q, and call the resultant metrics D-nDCG and D-Q (D stands for diversity). Moreover, we collectively call such metrics D-measures .
Note that D-measures rely on a single ideal ranked list (in con-trast to IA metrics which use multiple  X  X ocally ideal X  lists), and that the relevance levels are defined more dynamically and implic-itly than for traditional nDCG and Q. For example, suppose that we have three local (i.e. per-intent) relevance levels L 1 -L 3 . Then, for a topic with three intents, we will have at most 3  X  3=9 global rel-evance levels. The number of global relevance levels will differ for other topics. Thus, an ideal list is defined not based on a discrete set of relevance levels (which is usually pre-defined over the entire test collection), but based on a sort by the global gain, defined per topic.

Apart from the fact that D-measures avoid the undernomalisa-tion problem of IA metrics by relying on a single  X  X lobally ideal X  list, these two metric families are quite similar. However, Sakai et al. [20] also proposed a simple met hod to explicitly encourage high intent recall in a search output within the D-measure frame-work. Let I -rec @ l denote the intent recall at document cutoff l . Then D -measure ( X  X ee sharp measure X ) is defined as 3
Clarke, Kolla and Vechtomova [9] have discussed the possibility of incorporating Pr ( i | q ) into  X  -nDCG and NRBP.
WecallitD -measure because it  X  X harpens up X  D-measure in terms of discriminative power, as we shall demonstrate later. where  X  is a parameter. As Sakai et al. showed that the effect of the choice of  X  on IR experiments is relatively small due to the fact that I-rec and D-measures are already highly correlated with each other (evidence will be given in Section 6.2), we let  X  = . 5 throughout this study. We examine D -nDCG and D -Q in this paper. Provided that the document cutoff l is larger or equal to the maximum number of intents, I-rec ranges between 0 and 1, and therefore D -measures also range between 0 and 1.
Our primary method of comparing evaluation metrics is discrim-inative power [19]. We want metrics that are robust to variation across topics, so that the same conclusion can be reached as to which of two given systems is better, regardless of the choice of the topic set. More precisely, we measure discriminative power by conducting a statistical significance test for different pairs of runs, and counting the number of significantly different pairs. In this study, we randomly sample 20 runs from each test collection so 20*19/2=190 run pairs are tested in each case 4 . For significance testing, we use the two-tailed paired bootstrap test , with 1,000 bootstrap samples [19]. Note that this experiment is not about whether the metrics are right or wrong; it is about how metrics can be consistent across experiments and as a result how often dif-ferences between systems can be detected with high confidence. We regard high discriminative power as a necessary condition for a good evaluation metric, not as a suf ficient condition. Later in this paper, we shall complement our discriminative power results by ex-amining the actual ranked lists and comparing the intuitiveness of different diversity IR metrics.

It has been pointed out that discriminative power is not useful when, for example, the  X  X etric X  in question sorts systems alpha-betically by the system name as this produces perfectly consistent judgments regardless of the data used [22, 29]. However, we are interested in metrics that are strictly functions of a ranked list of items (i.e. system output) and a set of judged items (i.e. right an-swers). We are not interested in a  X  X etric X  that knows that (say) one ranked list is from Google and that the other is from Bing, and uses this knowledge to say which is better than the other.
The discriminative power method also provides a natural esti-mate of the performance difference (  X  ) between two systems re-quired to achieve statistical significance. This is done by record-ing, for every run pair, the  X  that corresponds to the borderline between significance and nonsignificance among the 1,000 trials, and then by selecting the largest value among all run pairs (i.e. a conservative estimate). This is one of the advantages of using the bootstrap test, although other significance tests may be used just for computing discriminative power.

Other methods for evaluating evaluation metrics exist. The swap method proposed by Voorhees and Buckley [28] yields results simi-lar to the discriminative power method but cannot directly examine the situation with the full topic set [19, 25]. The maximum en-tropy method [2, 17] can measure the informativeness of metrics but requires a mathematical derivation for each metric. Comparing the metrics with user clicks [5, 14] or with user preferences [23] should also be useful, and we expect these user-based approaches to complement our work.
Note that if we take the  X  X op X% runs X  from the run pool based on a metric, this may bias the metrics comparison results.
Our experiments rely on the TREC2009 Web track diversity test collection with Category A runs [6], which we call TR09DIV .Some statistics are shown in Table 1(a). Unfortunately, TR09DIV con-tains neither the intent probabilities ( Pr ( i | q ) ) nor per-intent graded relevance assessments 5 . Hence, while this is a suitable situation for  X  -nDCG, it is difficult to demonstrate the advantages of other di-versified IR metrics using TR09DIV .

Sakai et al. [20] compared D -measures,  X  -nDCG and nDCG-IA in terms of discriminative power and intuitiveness when the intent probability distribution is non-uni form. However, they did not use per-intent graded relevance. In contrast, this study fully utilises the capability of the diversified IR metrics to handle graded relevance, and considers both non-uniform and uniform distributions. To this end, we hired assessors to enrich the per-intent binary relevance data from TREC, as follows.

As Table 1(a) shows, we have 4,942 &lt;topic, relevant document&gt; pairs, or 6,499 &lt;intent, relevant document&gt; pairs. We assumed that all of these documents were judged as  X  X artially relevant X  by the TREC assessors 6 . Then, each intent-document pair was re-assessed by two assessors: only this time, each assessor had a choice between  X  X elevant X  (the document fully satisfies the infor-mation need expressed in the subtopic field) and  X  X artially rele-vant X  (the document only partially satisfies the information need). The assessors used an assessment tool on which the TREC descrip-tion and subtopic fields as well as the document content were dis-played. To URLs that no longer exist, the default assessment  X  X ar-tially relevant X  was given. Finally, we defined a relevance level for each document based on the three assessments (including the original one from TREC): L 3 (two relevants and one partially rele-vant); L 2 (one relevant and two partially relevants); and L 1 (three partially relevants). We call the resultant data set TR09DIV+gr , where  X  gr  X  stands for (per-intent) graded relevance. The inter-assessor agreement between the new graded assessments is 69.9% (Cohen X  X  kappa: .325). In this way, we obtained 1,173 L 1 , 1,959 L 2 and 3,367 L 3 documents across intents. Note that we did not re-examine any documents that were judged nonrelevant at TREC.
As for the intent probability distribution, we considered  X  X on-uniform X  and  X  X niform X . Uniform means that all intents are equally likely, and this is the assumption currently used at the TREC Web diversity task. As for Non-uniform, we followed Sakai et al. [20]: for each topic with n intents, and assumed that the j -th intent has probabilities exist [1, 26], but our focus is on the inherent property of different diversified IR metrics given these probabilities.
In addition to evaluating diversified IR metrics using TR09DIV+gr , we evaluated traditional graded-relevance IR metrics on which the diversified IR metrics are based. Sanderson et al. [23] treated each
For the TREC 2010 Web track, a kind of graded relevance was introduced to the ad hoc task but not to the diversity task.
TREC binary relevance assessments are known to be  X  X iberal, X  at least for early collections [27]. intent (i.e. subtopic) from TR09DIV as an independent topic to study the predictive power of traditional metrics. However, we avoided this approach because (a) This means that the traditional version of the test collection has many more topics than the origi-nal diversity version, and makes our traditional/diversity compari-son rather difficult; and (b) The topic set thus constructed probably violates the i.i.d. assumption.
 Instead, we constructed traditional test collections in two ways. The first method reduces TR09DIV+gr ,bytakingthe maximum relevance level across intents for each topic-document pair. For example, if a document is L 1 -relevant to intent i 1 and L 3 -relevant to intent i 2 for a topic that has these two intents, then we treat this document as L 3 -relevant to this topic in the new collection. We call the new collection TR09DIV+gr2T ,where X  2T  X  means  X (converted) to traditional. X  TR09DIV+gr2T has 601 L 1 , 1,328 L 2 and 3,013 L 3 documents across topics.

For comparison, we also constructed another traditional test col-lection using the original TR09DIV , not TR09DIV+gr .Thiswas accomplished by simply defining graded relevance in terms of how many intents a document covers. The resultant collection, which we call TR09DIV2T , has 3,622 L 1 , 1,113 L 2 , 178 L 3 ,28 L 4 and 1 L 5 documents across topics. (As shown in Table 1(a), the max-imum number of intents covered by a document in TR09DIV is 5.) Note that TR09DIV2T does not rely on our per-intent graded relevance data.
In general, it is dangerous to try to draw strong conclusions from experiments that rely on a single test collection. We there-fore conduct an additional set of tr aditional IR experiments using another data set. Our choice is the NTCIR-6 CLIR Chinese data ( NTCIR6C ) [12]: Table 1(b) shows its statistics. We selected NT-CIR6C because (a) It is radically different from the TREC data discussed above in that it is from outside TREC and outside Web search (The NTCIR-6 task was a newspaper search task); and yet (b) It is similar to our TREC data in that it also has 50 topics and comes with relevance levels L 1 -L 3 .
Using the three traditional graded-relevance IR test collections ( NTCIR6C , TR09DIV+gr2T and TR09DIV2T ), we evaluated nDCG, Q, (n)GAP and (n)ERR in terms of discriminative power, for document cutoffs l = 1000 and l =10 . The cutoff of 1,000 represents classical TREC, while 10 represents the more recent shallow-depth evaluation practices as exemplified by the TREC Web tracks.

Figure 1 shows the Achieved Significance Level (ASL) curves [19] for nDCG, Q, (n)GAP and (n)ERR for each experimental condi-tion. (Note that normalisation does not affect GAP and ERR when l = 1000 .) The x axis represents the 190 run pairs sorted by ASL, and the y axis represents the ASL (i.e. p -value). Metrics whose graphs are closer to the origin are more discriminative than others, i.e. they are able to detect more significant differences. Table 2 cuts these graphs in the middle (horizontally) to compare the discrimi-Figure 1: ASL curves for traditional metrics. The horizontal axes represent the 190 system pairs sorted by ASL; the vertical axes represent the ASL values. native power at the .05 significance level. The table also shows the performance  X  that corresponds to a statistical significant differ-ence. For example, with NTCIR6C at l = 1000 , the discriminative power of Q-measure at the .05 level is (137 / 190 =)72 . 1% ,andif the  X  between two systems is 0.09 or larger, then that is usually statistically significant [19]. Figure 1 and Table 2 show that: 1. nDCG appears to be the most consistently discriminative met-2. Comparisons between l = 1000 and l =10 show that, ex-3. Comparisons across the three data sets with the same cutoff l 4. Normalisation improves the discriminative power of GAP, Table 2: Discriminative power of traditional metrics at  X  = . 05 : Columns (a) and (b) show the discriminative power; (c) and (d) show the  X  required to achieve statistical significance.
We now present our main results using TR09DIV+gr with per-intent graded relevance data. We evaluated I-rec, D( )-nDCG, D( )-Q, nDCG-IA, (n)GAP-IA, (n)ERR-IA as well as the official  X  -nDCG values at l =10 . As mentioned earlier, we considered both Non-uniform and Uniform intent probability distributions.
Figure 2 shows the ASL curves of the diversified IR metrics for four experimental conditions (two cutoffs  X  two intent probability distributions), and Table 3 compares the discriminative power at the .05 level. As  X  -nDCG and I-rec are not affected by intent prob-abilities, we show their results only in the Uniform results. More-over, we do not consider I-rec (and therefore D -measures) when l = 1000 because I-rec is not useful with large document cutoffs: it would equal one most of the time. Also, we do not consider  X  -nDCG when l = 1000 as such values are not available from TREC. Figure 2 and Table 3 show that: 1. The Non-uniform and Uniform results are very similar. Thus, 2. The most discriminative diversified IR metrics when l = 3. The aforementioned  X  X est X  diversified IR metrics are at least 4. In all four experimental conditions, (n)GAP-IA is substan-5. Normalisation improves the discriminative power of GAP-IA Figure 2: ASL curves for diversified IR metrics. The horizontal axes represent the 190 system pairs sorted by ASL; the vertical axes represent the ASL values. Note that the D -measures, which combine D-measures with I-rec, are slightly more discriminative than their components: for exam-ple, for the l =10 , Uniform case, the discriminative power of D-Q is 51.1%, that of I-rec is 62.6%, and that of D -Q is 66.3% at the .05 level (Table 3(ii)). Thus, D-measures and I-rec complement each other to achieve reliable evaluation results, by looking at a ranked list from two different (but not unrelated) angles. This generalises an observation by Sakai et al. [20].

Figure 3 plots all 25 runs for the TR09DIV+gr , l =10 , Uniform case with I-rec as the x axis and D-measure (i.e. D-nDCG or D-Q) as the y axis. The dotted lines represent the contour lines for a D -measure with  X  = . 5 : if multiple runs lie on the same contour line, they are equally effective in terms of a D -measure. It can be observed that I-rec and D-measures are indeed already highly correlated with each other. A similar graph was shown by Sakai et al. [20] but they did not use per-intent graded relevance; the TREC 2009 overview paper [6] shows a similar graph for I-rec and the traditional precision metric.
 Table 3: Discriminative power of diversified IR metrics at  X  = . 05 : Columns (a) and (b) show the discriminative power; (c) and (d) show the  X  required to achieve statistical significance.
Highly discriminative metrics, while desirable, may not neces-sarily measure what we want to measure. How do the different diversified IR metrics differ from one another, and which ones are more intuitive than others for the purpose of search result diversifi-cation?
Table 4 shows the Kendall X  X   X  and  X  ap [30] values for different pairs of metrics, when the 20 runs are ranked in the TREC09DIV+gr , l =10 , Uniform setting. Kendall X  X   X  is a monotonic function of the probability that a randomly chosen pair of ranked items is or-dered identically in the two rankings. Hence a swap near the top of a ranked list and that near the bottom of the same list have an equal impact. Whereas,  X  ap is  X  X op-heavy, X  in that it is a monotonic function of the probability th at a randomly chosen item and one ranked above it are ordered identically in the two rankings. Like  X  ,  X  ap lies between  X  1 and 1, but unlike  X  , it is not symmetrical: one of the input rankings is taken as the gold standard. When the errors (i.e. pairwise item swaps with respect to the gold standard) are uniformly distributed over the ranking being examined,  X  equivalent to  X  . For example, the  X  between I-rec and D -Q is .96, while the  X  ap between the same pair of metrics is .97 when D -Q is taken as the ground truth. The main message Table 4 conveys is that all of these metrics (including the simple I-rec) are reasonably correlated with one another.

Based on the bootstrap test results used for our discriminative power experiments, Table 5 shows the agreement between metrics, focussing on I-rec, D -measures,  X  -nDCG and nDCG-IA in the l =10 , Uniform setting. Let A and B denote the sets of sigfini-cantly different run pairs at the .05 level according to two metrics, respectively. We define agreement as | A  X  B | / | A  X  B | ample, the agreement between D -nDCG and  X  -nDCG is 86%, as |
A  X  B | =9 , | A  X  B | = 116 and | B  X  A | =10 for these two Table 5: Agreement of significant differences at the .05 level ( TR09DIV+gr ; l =10 ;Uniform).
 metrics. That is, for nine run pairs, D -nDCG says that they are sig-nificantly different while  X  -nDCG says they are not; the situation is reversed for another set of ten run pairs 7 . One observation from this table is that the agreement between I-rec and nDCG-IA is rel-atively low (72%): this supports the arguments by Sakai et al. [20] and Clarke et al. [7] that IA metrics do not necessarily reward high intent recall, i.e. diversification. (See also the relatively low rank correlation values between I-rec and the IA metrics in Table 4.) On the other hand, it can be observed that the D -measures,  X  -nDCG and nDCG-IA agree with one another for around 80% of the time or more. (D -Q is virtually identical to D -nDCG: the agreement between them is 99%.) The important question is: when do they disagree? Hereafter, we focus our attention on D -nDCG,  X  -nDCG and nDCG-IA, as our experiments suggest that nDCG is the most reliable traditional IR metric when the document cutoff is small, and as these three metrics represent three different approaches to diversified IR evaluation.

To examine how D -nDCG,  X  -nDCG and nDCG-IA differ from the viewpoint of intuitiveness, we selected ten pairs of actual ranked lists from TREC 2009 Web track diversity runs as follows. First, from the aforementioned nine run pairs which were significantly different with D -nDCG but not with  X  -nDCG (Table 5), we ob-tained five pairs of ranked lists (i.e. run pairs for a particular topic) with the largest per-topic  X   X  X  in terms of D -nDCG, under the con-straint that there is a disagreement among D -nDCG,  X  -nDCG and nDCG-IA as to which run is better. We refer to these five cases as A -E , as shown in Table 6. For example, Case A in this table rep-resents two runs watd3 and Sab9wtBfDiv for Topic 47 which has two intents. The middle column shows which of the top 10 docu-ments retrieved by each run are relevant to which intent (where and i indicate informational and navigational intents according to the TREC diversity topic file, respectively); the last three columns show the per-topic  X   X  X  (e.g. performance of watd3 minus that of Sab9wtBfDiv ), and the arrows indicate which run is rated higher with each metric. Similarly, from the ten run pairs which were sig-nificantly different with  X  -nDCG but not with D -nDCG shown in Table 5, five cases with the largest per-topic  X   X  X  in terms of  X  -nDCG were selected. These are shown as cases F -J in Table 6. In short, these ten cases are the ones that contributed most to the dis-crepancy (in terms of statistical significance) between D -nDCG and  X  -nDCG. We shall closely examine these cases below from the viewpoint of intuitiveness. Note that these results are from the Uniform setting: Sakai et al. [20] have already compared the in-tuitiveness of D -nDCG,  X  -nDCG and nDCG-IA when the intent p robability distribution is Non-uniform, a situation which  X  -nDCG does not handle.

We examined the ten cases shown in Table 6 and categorised the results into the following four classes. The arguments below are In theory, conflicts can also occur, where one metric says that run X significantly outperforms run Y while another metric says that run Y significantly outperforms X . There was no such case in our experiments. somewhat subjective, as the right balance between diversity and relevance is hard to define. Nevertheless, we believe that they are useful for understanding the diversity metrics. In Case B , we argue that  X  -nDCG is counterintuitive. Both twC-SodpRBB and MSDiv1 completely failed to diversify: they cover the fifth intent i 5 only. However, twCSodpRBB has only one rel-evant document (though at rank 1), while MSDiv1 has eight, all of which are L 3 -relevant. Since i 5 is informational , MSDiv1 should probably be preferred, since the two runs are equally poor in terms of diversity (i.e. intent recall) but MSDiv1 has much better overall relevance. The rightmost columns of Table 6 show that D -nDCG and nDCG-IA agree with this intuition, while  X  -nDCG does not. This counterintuitiveness of  X  -nDCG is precisely because of  X  , which tends to ignore r epetition of relevant documents for the same intent. It should be remembered that, unless the intent is purely navigational, providing the user with multiple documents that are relevant to the same intent does not necessarily imply redundancy in practice: different relevant documents may carry different pieces of information. Of course, a smaller value of  X  may remedy this particular situation, but how to appropriately set  X  in advance is an open question.
 On the other hand,  X  -nDCG may be the most intuitive for Case C , which is similar to Case B in that both runs failed to diver-sify but different in that the intent involved is navigational ( X  X o to the Alexian Brothers Health System homepage X ). Thus, even though D -nDCG and nDCG-IA prefer MSDiv1 which returned three documents that are L 3 -relevant to i 1 , the second and the third L 3 -relevant documents may not be useful in practice. In particular, D -nDCG appears to favour MSDiv1 perhaps too much (the differ-ence is over .15). Note that the raw nDCG is inherently suitable for evaluation with informative queries. We will discuss this point further in Section 6.3.5.
Next, let us discuss Case A , Case F and Case H , where only informational intents are involved and D -nDCG disagreed with  X  -nDCG and nDCG-IA. It can be observed that, in all three cases,  X  -nDCG and nDCG-IA prefer the non-diversified run, even though they are supposed to reward diversified ranked lists. In contrast, D -nDCG (with  X  =0 . 5 ) consistenly favours a more diversified run, due to its explict intent recall component. However, as we have discussed earlier, the right balance between relevance and diversity (which in the case of D -nDCG is represented by the  X  parameter) is hard to define.
Next, we discuss Case D and Case E , where only informational intents are involved and  X  -nDCG disagreed with D -nDCG and nDCG-IA. In both cases,  X  -nDCG prefers the less diversified run which missed the fourth intent (but nevertheless returned a docu-ment relevant to two intents at rank 1).
In Case G , nDCG-IA is clearly counterintuitive. It can be ob-served that THUIR09FuClu has a document L 2 -relevant to i rank 2, and one L 2 -relevant to i 2 at rank 3. (These two intents are informational.) On the other hand, MSRABASE has only a document L 2 -relevant to i 3 at rank 2. (This intent is navigational.) Thus, since we are examining the uniform intent probability setting, THUIR09FuClu should definitely be preferred over MSRABASE , and both D -nDCG and  X  -nDCG satisfy this requirement.

The above counterintuitive behaviour of nDCG-IA arises from the inherent property of IA metrics, namely that high IA metric values can be achieved by doing extremely well for a single in-tent. In Case G , MSRABASE did extremely well for i 3 : this in-tent only has one relevant document ( L 2 ), and the run returned this document at rank 2. The gain value for an L 2 -document is 2 2  X  1=3 , and therefore nDCG 3 =(3 / log(2+1)) / (3 / log(1+ 1)) = log 2 / log 3 = . 631 . Thus, nDCG-IA, averaged over four intents, is . 631 / 4= . 158 . Whereas, THUIR09FuClu achieves only nDCG 2 = . 119 and nDCG 5 = . 144 for the two intents and therefore its nDCG-IA is only ( . 119 + . 144) / 4= . 066 .
Case I and Case J are similar to Case G in that only nDCG-IA prefers a run that failed to diversify. (All intents involved are in-formational.) We argue that nDCG-IA is rather counterintuitive for these cases as well, as MSRABASE covers three intents in Case I and Sab9wtBfDiv covers two in Case J .
To sum up the above analysis: while the right balance between relevance and diversity is difficult to define, D -nDCG consistently prefers the more diversified run compared to  X  -nDCG and nDCG-IA. If we want diversity more than we want high relevance, then D -nDCG would be the clear winner, as it explitly incorporates in-tent recall. This is in contrast to  X  -nDCG which tries to encourage high intent recall by discouraging retrieval of  X  X edundant X  docu-ments. As for IA metrics, we have demonstrated that they can be clearly counterintuitive, as high IA metric values can be achieved by retrieving highly relevant documents for one (major) intent.
The only case where D -nDCG may be less intuitive than  X  -nDCG is Case C , where the intent was navigational and there-fore retrieving multiple relevant documents may not be practically useful. For navigational intents, it may be better to use graded-relevance versions of Reciprocal Rank such as ERR and P + Finally, recall that the above analysis used our Uniform results. Given a non-uniform intent probability distribution,  X  -nDCG can be more counterintuitive as it disr egards the proba bilities [20]. Also, recall that  X  -nDCG completely disregards local relevance levels (e.g. L 1 vs L 3 in Table 6).
To our knowledge, our work is the first to have studied the prop-erties of different diversity evaluation metrics using per-intent graded relevance data. Moreover, our experiments are more extensive than similar studies that have been reported recently [7, 20]. Our main findings from the discriminative power experiments are:
Q-measure, P + and ERR are all members of a family of met-rics called Normalised Cumulative Utiliy (NCU) [21]. An NCU is defined in terms of a user X  X  stoppi ng probability distribution over ranks and a utility function at a given rank. Q-measure uses a uni-form distribution over all relevant documents; P + uses a uniform distribution over all relevant documents retrieved within top r where r p is the rank of one of the most relevant documents in the ranked list. Whereas, ERR X  X  st opping proba bility at a given rank depends on the relevance of previously seen documents.
Recall, however, that we have only examined a version of GAP which uses a flat probability distribution over the relevance levels, a setting recommended by Robertson, Kanoulas and Yilmaz [17]. The second finding suggests that diversified IR experiments with a given number of topics can be as reliable as traditional IR experi-ments with the same number of topics, provided that the aforemen-tioned discriminative metrics are used. This is good news for IR test collection builders and users.
 Moreover, our analysis showed that, while different diversified IR metrics are generally highly correlated with one another, D -nDCG is more intuitive than  X  -nDCG and nDCG-IA at least when high diversity is considered more important than high relevance.
Table 7 summarises the properties of diversified IR metrics ex-amined in this study. The original  X  -nDCG (as used in TREC) can handle neither intent probabilities nor per-intent graded relevance; IA metrics tend to ignore minor intents;  X  -nDCG and IA metrics have normalisation issues. In contrast, D-measures range fully be-tween0and1,andsodoD -measures provided that the measure-ment depth l is not smaller than the number of intents. In addition to these inherent differences, our present study showed that IA met-rics have relatively low discriminative power, and that D -measures have strengths in terms of intuitiveness. (The latter observation is not included in Table 7 as it is somewhat subjective.) It is probably fair to say that the D -measures are promising for diversified IR evaluation. A practical recommendation for diversified IR evalua-tion would be to plot I-rec against D-nDCG as we have shown in Figure 3 and to discuss the contour lines that represent D -nDCG. Our results on diversified IR metrics, however, rely solely on TR09DIV+gr (just as other studies [7, 20, 23] relied solely on TR09DIV ). As future work, we plan to construct more diversity test collections (at the NTCIR  X  X NTENT X  task 10 ) and strengthen our conclusions. We also plan to explore related questions such as: (1) how to seamlessly evaluate diversity and relevance for naviga-tional and informational queries; and (2) how to evaluate diversity across verticals in aggregated sear ch and across queries in a session, and formulate a unified, general framework for diversity evaluation.
