 Xuan Hong Dang  X  Wee-Keong Ng  X  Kok-Leong Ong Abstract For most data stream applications, the volume of data is too huge to be stored in permanent devices or to be thoroughly scanned more than once. It is hence recognized that approximate answers are usually sufficient, where a good approximation obtained in a timely manner is often better than the exact answer that is delayed beyond the window of opportunity. Unfortunately, this is not the case for mining frequent patterns over data streams where algorithms capable of online processing data streams do not conform strictly to a precise error guarantee. Since the quality of approximate answers is as important as their timely delivery, it is necessary to design algorithms to meet both criteria at the same time. In this paper, we propose an algorithm that allows online processing of streaming data and yet guaranteeing the support error of frequent patterns strictly within a user-specified threshold. Our theoretical and experimental studies show that our algorithm is an effective and reliable method for finding frequent sets in data stream environments when both constraints need to be satisfied.
 Keywords Data mining  X  Frequent set mining  X  Data stream  X  Online algorithm  X  Error guarantee 1 Introduction In the last few years, data streams have emerged as a new data type that have attracted much attention from the data mining community. They arise naturally in a number of applications, including financial services (stock ticker, financial monitoring) [ 11 , 22 ], sen-sor networks (earth sensing satellites, astronomical observatories) [ 16 , 24 ], call records in telecommunications [ 8 , 15 ], web tracking and personalization (web log entries or web-click the applicability of most traditional algorithms: (1) data arriving continuously at high and unpredictable arrival rate; (2) the volume of data is unbounded, making it impractical to store the entire content of the stream; and (3) the need to analyze data in real-time over limited computing resources. Furthermore, stream data can be lost under high speed conditions, become outdated in the analysis context, or intentionally dropped through techniques like sampling [ 2 , 13 ] or load shedding [ 9 , 19 , 20 ]. This makes it imperative to design algorithms that compute the answer in an online fashion with only one scan of the data stream whilst operating under resource limitations. Consequently, it is not possible to compute an exact answer for complex queries often found in data analysis.

Fortunately, approximate answers are usually sufficient for these applications. While approximates are desirable in the context of data stream applications, it is important to remem-ber that the quality of the approximation is equally important as the timely processing of its query. Consider the case of processing financial data streams. Clearly, if a large error margin is to exist in the approximate answers, there would be serious financial consequences despite delivering the results within the given timeframe. Therefore, while approximate answers are sufficient, keeping the accuracy to within some error bound is necessary.

In the existing literature (e.g., [ 12 , 17 , 23 ]) the quality of the approximate answer is often governed by an error parameter. Although this defines the allowable error margin in the approximate answer, we observed that this is only the case when the algorithms analyze the data stream in batch mode. In other words, algorithms that are capable of online processing do not conform strictly to a precise error guarantee. Since online processing is essential to these applications and so is the precision of the error guarantee, it is essential that algorithms for data streams meet both criteria. Yet, this is not the case especially on the discovery of frequent sets in transactional data streams.

The goal of discovering frequent sets is to find the set of collections of items (or any objects), whose occurrence count is at least greater than a certain threshold based on a frac-tion of the stream seen so far. Given a domain of m distinct items, there are 2 m distinct collections (or itemsets) that may appear in the data stream. In practice, given limited space and the need for real-time analysis, it is impossible to enumerate and count all collections for each of the incoming transactions in the stream. Hence, the downward closure property is exploited by delaying the counting of larger collections until all its subsets are found to be above the given threshold [ 4 , 5 , 21 ]. The implication of this is a larger error margin on collections of larger sizes. As a result, existing methods cannot guarantee the same error threshold for frequent sets of different sizes.

In this paper, we present EStream, a proposed method to discover frequent sets along with their estimated supports and error guarantees. Underpinning the design of EStream is the ability to give a precise error estimation on the support of frequent sets based on their length during online processing . Specifically, our proposed algorithm ensures that (a) there is no error in counting 1-itemsets; (b) for 2-itemsets, the error in the estimate is no more than ; and (c) for itemsets of length k &gt; 2,theerrorisnomorethan2 ,where is the preset error threshold and k is the maximal length of the frequent set in the stream. Furthermore, to improve the efficiency of EStream, we propose a trie-like data structure called Significant Trie ITemset ( STrieIT ). This data structure is not only used to maintain potentially frequent sets, it also enables our algorithm to count the frequency of each itemset separately in each part of the data stream, which in turn helps to minimize the generation of redundant itemsets.
The rest of the paper is organized as follows. In the next section, we introduce the prelim-inaries of discovering frequent sets in the context of data streams. Section 3 describes our approach. The data structure is introduced first, then the algorithm is represented, followed by the error analysis. Section 4 reports our experimental results. Related work is presented in Sect. 5 . Finally, conclusions are presented in Sect. 6 . 2 Problem statement Let I ={ a 1 , a 2 ,..., a m } be a set of literals called items (or objects). Let DS be a trans-actional data stream, which is a sequence of continuously incoming transactions DS = { t is the current length (or timestamp) of the data stream. We define the following concepts. Definition 2.1 An itemset X is a set of items such that X  X  ( 2 I  X  X   X  } ) .A -itemset is an itemset that contains exactly items. For a given itemset X  X  I and a transaction t i ,wesay that t i contains X if and only if X  X  t i .
 Definition 2.2 Given an itemset X of size (i.e., a -itemset), a set of all its subsets, denoted by the power set P ( X ) , is composed of all possible itemsets that can be generated by one or more items of the itemset X ; i.e., P ( X ) ={ Y | Y  X  ( 2 X  X  X  X } )  X  Y =  X  } . An immediate subset Y of X is an itemset such that Y  X  P ( X ) and | Y |= (  X  1 ) . 1 Definition 2.3 Let n be the number of transactions seen so far in DS . Then the frequency of an itemset X , denoted by freq ( X ) , is defined as the number of transactions in DS that contain X ; the support of itemset X , denoted by supp ( X ) , is defined as the ratio of the frequency of X to the number of transactions seen so far; i.e., supp ( X ) = freq ( X )  X  n  X  1 .
Let  X  be a given threshold called minimum support :  X   X  ( 0 , 1 ] .Let beagivenerror parameter such that  X  ( 0 , 0 . 5  X  ] . We define three types of itemsets as follows: Definition 2.4 An itemset X is called a frequent pattern at the point of output timestamp n if supp ( X ) is no less than  X  ; it is called an infrequent pattern if supp ( X ) is no more than 2 ; otherwise, it is called a sub-frequent pattern . Both frequent and sub-frequent patterns are called significant patterns.

Given three user-defined parameters  X  , , k (the maximal length of expected patterns, k &gt; 2), our goal is to develop an online algorithm that, at any time instant, discovers all frequent sets of size up to k from a transactional data stream and guarantees: (a) no error on the support of 1-itemsets; (b) an error of no more than on the support of 2-itemsets; (c) an errorofnomorethan2 on the support of k -itemsets. 3 Online patterns mining with error guarantee In this section, we present our proposed EStream algorithm to address the problem formulated in Sect. 2 . We first describe our data structure in Sect. 3.1 that will be used as a synopsis to capture potentially frequent sets in our algorithm. Then, in Sect. 3.2 , we present our algorithm for processing data streams where the support errors on output frequent sets are guaranteed within the given threshold. Finally, the correctness of this proposed algorithm is theoretically provedinSect. 3.3 . 3.1 Data structure To capture potentially frequent sets during the online mining process, we use a trie-like data structure. A trie is a tree structure whose organization is based on key-space decomposition. In key space decomposition, the key range is equally subdivided and the splitting position within the key range for each node is predefined. An alphabet trie isatrieusedtostorea dictionary of words. In EStream, we adopt a variation of the alphabet trie.
 I ( 1 i , j m ), a i  X  a j if and only if i &lt; j . Likewise, every transaction t i  X  I is also ordered with respect to the ordering in I . We now define the following: Definition 3.1 A TrieIT ( a.k.a. Trie Itemset) is a set of tree nodes where each node v is a 2-tuple (v ,v c ) such that v  X  I is the node X  X  label and v c is the frequency. Since each node corresponds to an item a i  X  I ,wealsouse v i (for brevity) to refer to a node that corresponds to a i  X  I . Then, the following conditions hold: 1. Let C (v i ) be the ordered set of children nodes of v i .If C (v i ) =  X  ,then C (v i )  X  2. Given a node v i ,let v ,v + 1 ,...,v i  X  1 ( 1 i  X  1 ) be the set of nodes on the path Each TrieIT W i corresponds to some a i  X  I such that the root node has label a i . Based on the definition of a TrieIT, we define a STrieIT as follows: Definition 3.2 A STrieIT ( a.k.a. Significant TrieIT) is a set of TrieITs that maintains the set of significant itemsets found in the stream. It has a special root node that references the root node of each TrieIT. Except for the root node, each node in the STrieIT is a 4-tuple v i , f c , f p ,w where v i  X  I is the node label; f c is the frequency of the itemset (formed by the set of node labels from root to v i ) in the current window; 2 f p is its accumulated frequency before the current window; and w indexes the window at which the itemset is inserted into the STrieIT. 3.2 Algorithm In this section, we describe the EStream algorithm that efficiently discovers all frequent sets over a stream of transactions and guarantees that the error on the support of each fre-quent pattern is within some pre-specified error threshold. Three parameters are required for the algorithm: minimum support threshold  X  , error parameter , and maximal length k of expected frequent sets. The algorithm utilizes a STrieIT data structure S as a synopsis in order to maintain all potentially frequent itemsets (Fig. 1 ).

EStream processes each transaction on the fly and discards it immediately from memory after processing. As each transaction arrives, the algorithm inserts each frequent set candidate identified from the transaction into S . After processing transactions, the algorithm scans S to remove those itemsets that now become infrequent. In the algorithm, is determined by 2 k  X  2  X   X  1 , which defines the size of the conceptual window. Since transactions are processed one by one, each window provides a conceptual grouping of the transactions and a logical time for scanning S .
For each itemset of length (1 k ), we identify a corresponding minimal frequency threshold. This threshold is used to specify whether a -itemset can be generated as a poten-tially frequent set in the current window. More specifically, a -itemset is inserted into S if the frequencies of all its immediate subsets in the window are above this threshold. In the algorithm, the minimum threshold of every pattern length is stored in an array arr of k elements. Element j th of arr is determined by arr [ j ]= 2 k  X  1 ( 1  X  1 / 2 j  X  1 ) for 2 j k and arr [ 1 ] is set to 0 for 1-itemsets since they have no subset except the trivial one {  X  } . Example 3.1 Suppose k = 4and = 25%. Then = 2 k  X  2  X   X  1 = 16. In this setup, be inserted into S if all its 1-itemsets have a frequency of at least 4 in the window. Similarly, for its 3-itemsets to be inserted into S , the frequencies of all its immediate subsets must be at least 6. And for 4-itemsets, the frequencies of all its immediate subsets must be at least 7.
Let us denote the index of the current window by w c = n / . Whenever a new transac-tion t n arrives, the algorithm processes it as follows: Increment: If an itemset X appearing in t n is also maintained in S , then its frequency in f c is increased by 1.
 Insert: For each X  X  t n not in S ,insert X into S with initial values of X , 1 , 0 ,w c if X is a singleton; 3 otherwise, let Y be any immediate subset of X ,then X is inserted into S if the three conditions below hold:  X  Y is in S ; We note that when processing the current transaction t n , the frequency of all X  X  X  imme-diate subsets has already been incremented by one before the algorithm checks the above conditions for X insertion.
 Example 3.2 We continue with Example 3.1 to illustrate the above. Suppose after processing more, assume that some transactions have been processed in the current window and now t n contains { a , b , c } . To insert this itemset, we check the frequencies of its immediate subsets; maintained from previous windows, its frequency in the current window must be at least 6 (i.e., arr [ 3 ] ).
 In cases where X is not inserted into S , all its supersets in t n need not be further checked. Prune: This step is invoked each time n  X  0 mod and after t n is processed. The algorithm prunes S by removing all but 1-itemsets that satisfy f c ( X ) + f p ( X ) + w( X )  X  arr [| X |] w c  X  arr [| X |] . Consequently, if an itemset is removed, all its supersets are also removed. An exception are those itemsets recently inserted into S in the current window. These itemsets are generated after their immediate subsets became sufficiently frequent. Also in this step, for and then reset f c ( X ) = 0.

At any instant upon the request of the analyst, the algorithm scans S to produce all 1-item-sets satisfying f c ( X ) + f p ( X )  X   X  n and those -itemsets satisfying f c ( X ) + f p ( X ) + w( X )  X  arr [| X |]  X   X  n .

Of interest to the reader is that EStream does not rely on the frequencies of all its subsets seen in the stream to generate a new candidate. Instead, only those frequencies in the current window are considered. This can be understood by the fact that frequencies of itemsets are not uniformly distributed in all parts of the stream. For some itemsets whose occurrences in some parts of the stream may not be sufficiently frequent, they are still always globally frequent in the stream since their occurrences in the other parts of the stream are very high. If always treating them as frequent ones, it is possible that we may need to generate their longer supersets according to the downward closure property.

Yet, the frequencies of such supersets are very close to the threshold used to eliminate infrequent itemsets. Therefore, they are likely to be deleted again due to the time-variation in many data streams. In view of this, EStream records the variations on the frequency of each itemset in different parts of the stream by separating its frequency in the current window ( f c ) and the accumulated one in previous windows ( f p ). This helps identify frequent itemsets in the entire stream so far but may not locally frequent in the current window. As a result, EStream can efficiently reduce the generation of redundant supersets and therefore, improves its performance.
 frequent singletons. While their frequency is insufficient to make them locally frequent in the current window, it is possible that they can still be frequent in the entire stream seen so far (due to their high frequency in previous windows). If we do not distinguish their frequency in the current window from the ones in previous windows, they all certainly must be treated as current window, their supersets must be generated and counted. Even with a small number of such itemsets, this can lead to an exponential increase in the number of redundant candidates determined as not frequent in the current window (based on f c value), only 7 j candidates of the same size needs to be considered. Accumulating for all j  X  2, we can save considerable computation and memory space with our approach. 3.3 Error analysis We now discuss the accuracy of frequency estimates produced by EStream. In the process, we prove the error guarantees presented at Section 2 of this paper. In this case, 1-itemsets will not be discussed since their frequencies have been counted precisely and thus, there is no error on their support.

For each itemset X , we denote its true frequency by freq T ( X ) and estimated frequency denote its true and estimated supports .
 Lemma 3.1 If an itemset X is deleted in the current window w c , then its true frequency freq T ( X ) seen so far in the stream is no more than w c  X  arr [| X |] .
 Proof We prove by induction. In the base case when w c = 1, nothing is deleted at the end of this window since an itemset is inserted into S only if its immediate subsets are sufficiently significant. Therefore, any itemset X not inserted into S in the first window will not have their true frequency greater than arr [| X |] ; i.e., freq T ( X ) arr [| X |] .

When w c = 2, only those itemsets in the first window can be removed. Let X be such an X was inserted in the first window. Thus, freq T ( X ) freq E ( X ) + arr [| X |] . When combined with the condition for deleting an itemset; i.e., freq E ( X ) + arr [| X |] w c  X  arr [| X |] ,we have freq T ( X ) w c  X  arr [| X |] .
Induction case: Suppose X is deleted at w c &gt; 2. Then, X must be inserted at some window w + 1before w c ; i.e., w( X ) = w i + 1. In the worst case, X could possibly be deleted in the previous window w i . By induction, X  X  X  true frequency was no more than w i  X  arr [| X |] when it was deleted in window w i .Since arr [| X |] is the maximum error at window w i + 1and freq E ( X ) is the frequency count since insertion in window w i + 1, it follows that freq T ( X ) freq T ( X ) freq E ( X ) + w( X )  X  arr [| X |] . Again, combined with the deletion rule that freq E ( X ) + w( X )  X  arr [| X |] w c  X  arr [| X |] ,wehave freq T ( X ) w c  X  arr [| X |] . Lemma 3.2 The true frequency of any itemset X in S is bounded as follows: freq E ( X ) freq T ( X ) freq E ( X ) + arr [| X |]  X  w( X ) .
 Proof As freq E ( X ) is the frequency count of X since it was inserted into S , it is the lower bound on the true frequency of X . Thus, the lower limit of the inequality is always true. For the upper limit of the inequality, we prove as follows. If X is inserted in the first window, the otherwise, X is possibly deleted some time earlier in the first w i windows and then inserted into S at window w i + 1. By Lemma 3.1 , freq T ( X ) is at most w i  X  arr [| X |] when such a w( X )  X  arr [| X |] .
 supp E ( X ) supp T ( X ) supp E ( X ) + .
 Proof By definition, we have arr [ 2 ]= 2 k  X  1 ( 1  X  1 / 2 ) = when | X |= 2. Since  X  w(
X ) n ,wehave arr [| X |]  X  w( X ) n . Dividing the inequality in Lemma 3.2 by n ,we have supp E ( X ) supp T ( X ) supp E ( X ) + .
 supp E ( X ) supp T ( X )&lt; supp E ( X ) + 2 .
 Proof By definition, we know arr [ k ]= 2 k  X  1 ( 1  X  1 / 2 k  X  1 ) 2  X  1 &lt; 2 when |
X |= k .Again,since  X  w( X ) n and arr [| X |]  X  w( X )&lt; 2 n , dividing the inequality in Lemma 3.2 by n , we obtain the above result.

With the above analysis, we conclude that the algorithm guarantees: (a) no error on the support of 1-itemsets; (b) the maximum error on the support of 2-itemsets is no more than ; and (c) the maximum error on the support of k -itemsets is no more than 2 . 4 Performance evaluation 4.1 Experimental setup Our algorithm is written in C++ and compiled using Microsoft Visual C++ version 6.0. All experiments are performed on a Pentium machine with a CPU clock rate of 1.9 GHz, 1 GB of main memory and running on the Windows XP platform. The method used for generat-ing datasets is similar to the one described by [ 1 ]. Specifically, datasets are generated by the IBM synthetic market-basket data generator. 4 To describe a dataset, we use the notation T w .I x .D y where w is the average size of transactions, x is the average size of maximal potentially frequent itemsets, and y is the size of the dataset. 4.2 Scalability study
To evaluate the scalability of the proposed algorithm, both the environments of long trans-actions with fewer items and short transactions with more items are considered. Therefore, two data streams of size 1 million transactions are generated for studies. One has an average transaction size of eight items with an average frequent itemset size of 4. This dataset is generated by using 5 , 000 distinct items. The other has an average transaction size of 5 with average frequent itemset of size 3. It is generated using 10 , 000 distinct items. Using the above notation, we denote the two datasets by T8.I4.D1000K and T5.I3.D1000K, respectively. We measured two parameters that are important for stream applications: (a) execution time and (b) memory consumption.

To simulate an online data stream, transactions are processed individually and discarded immediately. This leaves the available memory for synopsis S . Figure 2 shows the memory usage on the two datasets using EStream. The usage levels indicated in the figure represents the maximum number of itemsets generated during processing. This is equivalent to the max-imum number of nodes allocated in S before pruning. We measure the usage levels over a number of minimum supports, with  X  ranging from 0.1 to 1% (and = 0 . 1  X  ) after 200 , 000 transactions. Here we note that since is determined by 2 k  X  2  X   X  1 and  X  is varied from 0.1 to 1% (with = 0 . 1  X  ), is therefore changed accordingly. In these experiments, we took measurements at the window around this value, which is 200 K transactions. As expected, a drop in the minimum support increases the memory usage. For example, when  X  decreases from 1 to 0.1%, the number of nodes maintained in S increases from 7 to 56K with T5.I3.D1000K, and respectively from 5 to 230 K with T8.I4.D1000K. This is due to the fact that, at small minimum support thresholds, we can find more frequent itemsets with larger sizes. Nevertheless, the interesting point here is that the memory usage remains con-stant at any given support threshold throughout the lifetime of processing the stream. This happens because EStream is designed to maintain only potentially frequent sets in S ; and by the pruning step, all insignificant candidates are removed periodically after each window.
With the same range of support thresholds, Fig. 3 shows the execution times on the two selected datasets. As we can see, the cumulative execution time of EStream grows linearly with the number of transactions processed in the streams. At support threshold  X  = 0 . 1%, for instance, the time to process every 200 K transactions of T5.I3.D1000K is approximately 26 s, and T8.I4.D1000K is approximately 95s. This indicates the uniform processing time over each window; or uniform processing time for each transaction arriving from the stream. This stable execution time is also important in a data stream environment where the size of the stream is potentially unbounded. 4.3 Time-varying studies In the previous experiments, we have excluded a time-variation parameter that has an effect on memory consumption and execution time. In this section, we evaluate the impact and show how EStream addresses this issue effectively in its design.

We have mentioned that the data stream distribution may change over time. In EStream, we separate the counting of itemsets in the current window from the previous ones. New item-sets are generated only based on the presence of significant subsets in the current window. Consequently, a large number of redundant candidates are reduced. While we have discussed this at the end of Sect. 3.2 , we confirm the practicality of this approach in the experiments below.

Using T8.I4.D1000K, the effects of time-variation is simulated on the dataset using the method proposed by Giannella et al. [ 12 ], where an item mapping table is utilized. Here, 20% of items are randomly chosen for itemset generation with low frequencies and periodically after every 50,000 transactions, random permutations among all items are applied to the table. To test the difference, we have a variation of EStream that generates candidates using all windows since starting the algorithm; i.e., relying on f p only. We denote this variant as EStream (w/o f c ). With the same range of support thresholds, Fig. 4 shows the compari-son in terms of execution and memory usage of EStream and its variant that uses only the cumulative counts; i.e., EStream (w/o f c ). Although a decrease in support threshold causes both to increase their number of nodes used in S , we see that the memory utilization grows sharply in the case of EStream (w/o f c ). This happens as 1-itemsets are often significant if considering their frequency in the entire data stream but may not be sufficiently frequent in some windows due to time-variations.

In the case of EStream, a distinction is made through the use of f p (the cumulative frequen-cies of previous windows) and f c (the frequencies of the current window). By distinguishing the frequencies of itemsets in the current window, our algorithm is able to detect the pres-ence of such spurious items and therefore avoid unnecessary processing on transactions in which those items appear. In contrast, EStream (w/o f c ) would tend to view them as frequent which in turn leads to the generation of larger itemsets causing an increase in memory and processing demand. 5 Related work Problems related to frequency counting that have been studied in the context of data streams can be classified into two groups: finding frequent items, and finding frequent sets.
The motivation for finding frequent items over streaming data arises from a variety of applications such as traffic engineering, network system analysis [ 3 , 6 ], telecommunication call records [ 8 ] where each generated data instance in the data stream can be viewed as a single item. In this category, solutions have online processing capabilities and precise error guarantees while working within bounded memory. Examples of such algorithms include Lossy-Counting [ 17 ], Sticky-Sampling [ 17 ], FDPM-1 [ 23 ], Group-Test [ 7 ].

On the other hand, the emergence of applications that generate data in streams of transac-tions, such as web click streams, retail transaction flows, stock market transaction flows [ 3 , 10 , 18 ] motivates the discovery of frequent sets instead of frequent singletons. However, this problem has not been well-addressed in terms of satisfying both online processing and precise error guarantees. The main difficulty is due to the exponential explosion of itemsets, which caused most existing algorithms to approach the constraints in separation.
 One of the first online processing algorithm called CARMA is introduced by Hidber [ 14 ]. In CARMA, the error on an itemset X  X  frequency is based on the highest frequency error among its subsets. This estimation is often too conservative as itemsets are frequently deleted and inserted throughout the runtime of the algorithm. More importantly, a loose estimate on smaller itemsets often causes a looser estimate in their supersets. This is why a vast majority (&gt;95%) of the itemsets generated in CARMA turns out to be false positives. Chang and Lee [ 5 ] recently introduce an extension of CARMA in a forgetful model. In Chang X  X  extension, the weights on older transactions are gradually decreased as new ones arrive in the stream. Despite this non-uniform threshold, the maximum error of a new itemset is derived by choos-ing the highest frequency error from its subsets. Consequently, the error guarantee varies for each pattern.

In contrast, EStream X  X  strategy is not based on the highest error among subsets. Rather, a set of frequency thresholds are specified in advance for every itemset of specific lengths. These thresholds are then used to identify the candidates within each window. If an itemset is not generated, its frequency in the window is guaranteed to be no more than the corre-sponding threshold. Since the thresholds and the window size are pre-specified, the support of each itemset is guaranteed within the same error.

Lossy-Counting by Manku and Motwani [ 17 ] is the first attempt to guarantee a precise error on the patterns discovered. Our algorithm is similar to theirs by virtue that both find all frequent sets and guarantees error on each pattern to be within user-specified limits. In order to achieve this, however, their approach processes transactions in batches and thus, results cannot be produced online due to buffering. Another work that provides error guarantees on mining results is the FDPM [ 23 ]. Yu et al. X  X  work divides the stream into buckets, where the size of each bucket is determined by the Chernoff bound and a user-specified error. Each bucket is then processed separately and the results are updated into a global data structure recording the overall frequent patterns. Like the Lossy-Counting approach, FDPM is also a batch-processing algorithm. Furthermore, the lossy nature of FDPM causes truly frequent patterns to be missed in the final result. On the other hand, EStream guarantees that all patterns (if truly frequent) will be discovered while operating in online-processing mode. 6 Conclusions In this paper, we have argued the need for critical stream applications to both satisfy the online processing requirements and a strict error guarantee in the estimate of its results. We demonstrate this using the case of finding frequent sets on transactional streams in which the class of algorithms in existence address either the problem of online discovery or that of error guarantees separately . EStream represents our first effort to address these two constraints simultaneously in a single solution. As shown through theoretical analysis and empirical results, our algorithm accomplishes these two constraints while capturing the inherent char-acteristics of time-variation common in many data streams. We believe this will be an impor-tant step towards effective data stream applications. Our proposal in this paper contributes in part to this goal.
 References Author Biographies
