 This paper introduces a song wave retrieval system that is vocabulary indepen-dent and that uses frame-wise phoneme recognition. Our purpose is to show the system X  X  applicability from a song query tool to a continuous song database, and find the most suitable threshold for retrieval dicisions in this system. In recent years, computers have become capable of recognizing natural human voices from the news or other television programs in real time and recording the speech of many unspecified speakers in sentences and then retrieving them [1]. Related to this is the great need for song wave recognition and retrieval applications that can easily retrieve voices or songs for music businesses that sell CDs or karaoke [2], [3]. However, research has not been conducted singing, that is one of voice in comparison with speaking. One of the retrieval methods of song data with music is this: pick up the melody and retrieve the database melody area using a query melody [4], but there is little research about song wave retrieval that employs phonological character [5]. Oka [6] proposed a speech retrieval system. This system converts frames of data-base and query wave data to phoneme labels by using a Bayesian method, and retrieves intervals of the database, each of which are similar to the query by spotting, using CDP [7] the same way as a query. This retrieval method is com-pletely vocabulary independent because this matching is not affected by words or sentences. 3.1 Create the Feature Vector for Each Frame A frame-wise sound retrieval system operates to create a feature vector for each frame of song data. This vector features sound directional patterns because this pattern method is more effective than the Cepstrum method [8]. First, this method makes 8-msec intervals and a 20-channel spectrum field from speech waveforms. Second, the spectrum field is extracted by gradients of a 4-direction pattern (up, down, left and right vector) from the middle 18 channels of the 20 channels (4  X  18 = 72). Third, this 4-direction pattern is smoothed only in the time axis direction to seven frames from three frames before to three frames after a target. Fourth, the feature vector data can be set into 216-vector pattern data as a target frame from a target, 2 frames before and 2 frames after a target. This smoothing can reduce the range of speech melody and reduce the pitch distance in songs. Finally, these 216-vector patterns are converted to phoneme labels by using a Bayes calculation. 3.2 Frame-Wise Phoneme Recognition Discriminate functions in this system were obtained from a collection of similar phoneme label frames by using the well-known Bayes estimation where every frame already has phoneme labels in speech waveforms.  X  l,i : i th eigen value of sample of learned feature vectors about phoneme l .  X  l,i : i th eigenvector of phoneme l . k : total number of usable eigen values. p (  X  l ): Preliminarily incidence probability of phoneme l .
 These discriminate functions are made of specific kinds of phonemes (26 kinds). In this paper, 26 types of Bayes estimation (Table 1) made by 5030 sentences of speech data in ATR Speech Database [9]. These 26 types of phonemes are chosen from common labels of 50 label types in the ATR Speech Database and 29 label types in the RWC Music Database [10]. Every frame of song waveforms, that is those in the database and query waves, is checked for these 26 phoneme labels used in the Bayes estimation. If one of the phoneme labels lies at a max value of some frame, that phoneme labels the frame. 3.3 Frame-Wise Phoneme Labeling of Database and Query Line This phoneme label has not a small error (Table 2) but this phoneme labeling is thought of as a filter that picks up some feature quantity and converts other data types. Thus, the experiment in this research uses this labeling method because this phoneme labeling codes the database and query data that makes the same error [11]. 4.1 Application of Continuous Dynamic Programming CDP [12] is used as the retrieval algorithm of the database for queries. Each frame is first expressed as the frame expression of a voice, and this is accom-plished by labeling the phonemic symbol. In this research, we use the candidate in the first place in the database (input) line and the third place in the query label of the 1st candidate in the t frame. The parameter t indicates the time axis of the database, while  X  indicates one of the query data. Then they are described by F ( t ) ,t =1 , 2 , 3 , ... G (  X  )=( g 1 (  X  ) ,g 2 (  X  ) ,g 3 (  X  )) (1  X  T ). row of a query is used for the reference pattern. This setup of the input is used to enable input of the data infinitely into the CDP algorithm when we obtain of minimum distance between the reference pattern and a suitable best interval database. The suitable interval is determined in a posterior way after detecting a local minimum in a stream of output of CDP.
 tern.
The CDP recurrence formula that uses the local distance can be shown as follows.
 Initial Condition: Iteration ( t =1 , 2 , ... ) : For  X  =1 For  X  =2 For  X  =3 Hence, the output is as follows: The local distance used with CDP and the weight are assumed to be that shown in Figure 2, and the weight shown in Figure 2 is asymmetric. The accumulation of weight steadies with 3 T , even the weight obtained from the best pass by accumulation distance P ( t, T ) calculation. The logical conclusion is that this becomes easily performed as a regularized operation as in equation (7) to prevent the value of A ( t ) from depending on the length of a standard pattern. 4.2 Extract Range of Retrieved Internal This singing voice retrieval, the CDP value A ( t )ateachtime t , is output by applying CDP as in (Figure 5). Because the best pass that gives the output value at this time is decided, the starting point on the axis of the input time of the best pass is also decided. In this system, backtrace controls are applied at the starting point decision. Section N at S ( t ) and the input time at each time t can be decided as A with the starting point on the axis of the input time to give A ( t )  X   X  .
 If N ( t : A,  X  ) have a common section in N ( t : A,  X  ), this system can select the N corresponding to the smallest one as compared with A ( t )and A ( t ) because it never has a section that shares the same time as that of the retrieved sections. In general, the part of retrieval area that should not be retrieved increases, too, when this retrieval enlarges the value of A while the retrieval leakage decreases because the proportion of the section retrieved in the database grows. Moreover, the result is different from the evaluation for a singing voice recognition rate because this retrieval output is evaluated only of whether it resembles a query or not. We conducted an experiment to compare the results of retrieval with five types of query: 1 clause, 2 clauses and 4 clauses of singing voice and 2 clauses and 4 clauses of speaking voice. This experiment used 78 songs from RWC Music Database: Popular Music [10], Japanese solos but reduced BGM, and 4 men and 1 woman singing and speaking each 10 query phrase. Those were from the database and trimmed of query length (database example: Figure 3, query example: Figure 4). In the retrieval experiment, the retrieval rate of 36% for 1 clause, 48% for 2 clauses and 78% for 4 cl auses were obtained until the 10th order when phonemes from the singing voice database were retrieved by using the phonemes of singing voice queries in the retrieval experiment. One of the miss-retrieved song is Figure 4-5. This song was difficult to express for query like a database phrase because this song had a strong echo and very high female voice. Some other songs had also weak or strong effects like delay and seleste, but these songs were obtained from the collecting area. However, the retrieval rate of speaking voices was quite lo w; 12% for 2 clauses and 14% for 4 clauses were obtained until the 10th order when using the speaking voice query because the phoneme pattern is quite different. For example, between song rhythm and speech rhythm pattern is quite different, also between each singing voice and speech voice phoneme variation is different. We described an experiment that investigated a voice retrieval method that does not depend on vocabulary. The method was applied to a singing voice that was retrieved by singing voice queries and speaking voice queries. In this experi-ment, singing voice retrieval rates increase if we use longer queries. However, this method could not retrieve speaking voice queries because it is quite differ-ent from singing voice in respect to rhythm and phoneme variation. Also some singing voice effects are made difficult to pick up collect area.
 ing to improve singing voice recognition rates, investigate how much effect is added by vocals, what approaches exist for improving singing voice recognition rates by processing, research some noise reduction or noise free method for CD music retrieving with this retrieval method.
 Acknowledgement: We wish to express our gratitude to Mr. Jian Xin Zhang (Media Drive) and Mr. Masanori Ihara (SHARP) who supported the advance-ment of this research.

