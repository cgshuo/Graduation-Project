 Co-clustering is a powerful data mining technique with var-ied applications suc h as text clustering, microarra y anal-ysis and recommender systems. Recen tly, an information-theoretic co-clustering approac h applicable to empirical join t probabilit y distributions was prop osed. In man y situations, co-clustering of more general matrices is desired. In this pap er, we presen t a substan tially generalized co-clustering framew ork wherein any Bregman div ergence can be used in the objectiv e function, and various conditional exp ec-tation based constrain ts can be considered based on the statistics that need to be preserv ed. Analysis of the co-clustering problem leads to the minim um Bregman infor-mation principle, whic h generalizes the maxim um entrop y principle, and yields an elegan t meta algorithm that is guar-anteed to achiev e local optimalit y. Our metho dology yields new algorithms and also encompasses sev eral previously kno wn clustering and co-clustering algorithms based on alternate minimization.
 Categories and Sub ject Descriptors: I.2.6 [Arti cial Intelligence]: Learning General Terms: Algorithms Keyw ords: Co-clustering, Matrix Appro ximation, Breg-man div ergences
Co-clustering, or bi-clustering [9, 4], is the problem of si-multaneously clustering rows and columns of a data matrix. The problem of co-clustering arises in div erse data mining applications, suc h as sim ultaneous clustering of genes and exp erimen tal conditions in bioinformatics [4, 5], documen ts and words in text mining [8], users and movies in recom-mender systems, etc. In order to design a co-clustering framew ork, we need to rst characterize the \go odness" of a co-clustering. Existing co-clustering techniques [5, 4, 8] achiev e this by quan tifying the \go odness" of a co-clustering in terms of the appro ximation error between the original data matrix and a reconstructed matrix based on co-clustering. Of these techniques, the most ecien t and scalable algo-rithms are those based on alternate minimization schemes [5, 8], but these are restricted to only two distortion measures namely , KL-div ergence and the squared Euclidean distance, and a few speci c matrix reconstruction schemes. These two limitations restrict the applicabilit y of these techniques to a small range of data matrices.

In this pap er, we address the follo wing two questions: (a) what class of distortion functions admit ecient co-clustering algorithms based on alternate minimization? , and (b) what are the di er ent possible matrix reconstruction schemes for these co-clustering algorithms? We sho w that alternate minimization based co-clustering algorithms work for a large class of distortion measures called Bregman div ergences [3], whic h include squared Euclidean distance, KL-div ergence, Itakura-Saito distance, etc., as special cases. Further, we demonstrate that for a given co-clustering, a large variet y of appro ximation mo dels are possible based on the type of summary statistics that need to be preserv ed. Analysis of this general co-clustering problem leads to the minimum Bregman information principle that sim ultaneously general-izes the maxim um entrop y and the least squares principles. Based on this principle, and other related results, we dev elop an elegan t meta-algorithm for the Bregman co-clustering problem with a num ber of desirable prop erties. Most pre-viously kno wn parametric clustering and co-clustering al-gorithms based on alternate minimization follo w as special cases of our metho dology .
We start by reviewing information-theoretic co-clustering [8] and motiv ating the need for a more general co-clustering framew ork. Let [ u ] m 1 denote an index u running over f 1 ; ; m g and let X and Y be discrete random variables that tak e values in the sets f x u g ; [ u ] m 1 ; and f y v g ; [ v ] Supp ose we are in the idealized situation where the join t probabilit y distribution p ( X; Y ) is kno wn. In practice, p may be estimated from a con tingency table or co-o ccurrence matrix. Supp ose we want to co-cluster, or, sim ultaneously cluster X into k disjoin t (row) clusters f ^ x g g ; [ g ] into l disjoin t (column) clusters, f ^ y h g ; [ h ] l 1 denote the corresp onding clustered random variables that range over these sets. An information theoretic form ulation of nding the optimal co-clustering is to solv e the problem where I ( X ; Y ) is the mutual information between X and Y [6]. In [8], it was sho wn that where q ( X; Y ) is a distribution of the form and D ( jj ) denotes the Kullbac k-Leibler(KL) div ergence. Thus, the searc h for the optimal co-clustering may be con-ducted by searc hing for the nearest appro ximation q ( X; Y ) that has the form in (1.3). We note that q ( X; Y ) dep ends only on ( m k + n l + kl 1) indep enden t parameters, whic h is much smaller than the ( mn 1) parameters that determine a general p ( X; Y ). Hence, we call q ( X; Y ) a \low complexit y" or low parameter matrix appro ximation.
The above is the viewp oint presen ted in [8]. We now presen t an alternate viewp oint that highligh ts the key maxi-mum entrop y prop erty that mak es q ( X; Y ) a \low complex-ity" or low parameter appro ximation. 1 Lemma 1 Given a xed co-clustering ^ X , ^ Y , consider the set of joint distributions p 0 that preserve the following statis-tics of the input distribution p : Among all such distributions p 0 , the distribution q in (1.3) has the maximum Shannon entr opy, i.e., H ( q ) H ( p 0 ) . Thus, among all distributions that preserv e marginals and co-cluster statistics, the maxim um entrop y distribution has the form in (1.3). Thus, by (1.2) and Lemma 1, the co-clustering problem (1.1) is equiv alen t to the problem of nd-ing the nearest (in KL-div ergence) maxim um entrop y dis-tribution that preserv es the marginals, and the co-cluster statistics of the original data matrix.

The above form ulation is applicable when the data ma-trix directly corresp onds to an empirical join t distribution. However, there are imp ortan t situations when the data ma-trix is more general, for example, the matrix may con tain negativ e entries and/or a distortion measure other than KL-div ergence, suc h as the squared Euclidean distance, or the Itakura-Saito distance migh t be more appropriate.
This pap er addresses the general situation by extending information-theoretic co-clustering along three directions. First, \nearness" is now measured by any Bregman div er-gence. Second, we allo w speci cation of a larger class of con-strain ts that preserv e various statistics of the data. Lastly , to accomplish the above, we generalize the maxim um en-trop y approac h: we guide our co-clustering generalization by app ealing to the minimum Bregman information principle that we shall introduce shortly . The optimal co-clustering is guided by the searc h for the nearest (in Bregman div er-gence) matrix appro ximation that has minim um Bregman information while satisfying the desired constrain ts.
In this section, we form ulate the Bregman co-clustering problem in terms of the Bregman div ergence between a given matrix and an appro ximation based on the co-clustering. Pro ofs omitted due to lack of space, see [1] for details.
We start by de ning Bregman div ergences [3, 2]. Let be a real-v alued strictly con vex function de ned on the con-vex set S = dom( ) R , the domain of , suc h that is di eren tiable on int( S ), the interior of S . The Breg-man div ergence d : S int( S ) 7! [0 ; 1 ) is de ned as d ( z 1 ; z 2 ) = ( z 1 ) ( z 2 ) h z 1 z 2 ; r ( z 2 ) i , where r is the gradien t of .
 Example 1.A (I-Div ergence) Giv en z 2 R + , let ( z ) = z log z . For z 1 ; z 2 2 R + , d ( z 1 ; z 2 ) = z 1 log( z Example 2.A (Squared Euclidean Distance) Giv en z 2 R , let ( z ) = z 2 . For z 1 ; z 2 2 R , d ( z 1 ; z 2 ) = ( z Based on Bregman div ergences, it is possible to de ne a use-ful concept called Bregman information , whic h captures the \spread" or the \information" in a random variable. More precisely , for any random variable Z taking values in S = dom( ), its Bregman information is de ned as the exp ected Bregman div ergence to the exp ectation, i.e., I ( Z ) = E [ d ( Z; E [ Z ])].
 Example 1.B (I-Div ergence) Giv en a real non-negativ e random variable Z , the Bregman information corresp onding to I-div ergence is given by I ( Z ) = E [ Z log Z E [ Z ] Z is uniformly distributed over the set f p ( x u ; y v ) g [ u ] i.e., Pr( Z = p ( x u ; y v )) = 1 mn ; 8 u; v , then E [ Z ] = the Bregman information of Z is prop ortional to D ( p jj p
H ( p ) + constan t, where D ( jj ) is KL-div ergence, p 0 uniform distribution and H ( ) is the Shannon entrop y. Example 2.B (Squared Euclidean Distance) Giv en any real random variable Z , the Bregman information corre-sponding to squared Euclidean distance is given by I ( Z ) = E [( Z E [ Z ]) 2 ], and when Z is uniformly distributed over the elemen ts of a matrix, it is prop ortional to the squared Frob enius norm of the matrix + constan t.
 We focus on the problem of co-clustering a given m n data matrix Z whose entries tak e values in a con vex set S = dom( ), i.e., Z 2 S m n . With a sligh t abuse of nota-tion, we can consider the matrix Z as a random variable in S that is a kno wn deterministic function of two underlying ran-dom variables U and V , whic h tak e values over the set of row indices f 1 ; ; m g and the set of column indices f 1 ; ; n g resp ectiv ely. Further, let = f uv : [ u ] m 1 ; [ v ] n join t probabilit y measure of the pair ( U; V ), whic h is either pre-sp eci ed or set to be the uniform distribution. Through-out the pap er, all exp ectations are with resp ect to . Example 1.C (I-Div ergence) Let ( X; Y ) p ( X; Y ) be join tly distributed random variables with X; Y taking values be written in the form of the matrix Z = [ z uv ], [ u ] m where z uv = p ( x u ; y v ) is a deterministic function of u and v . This example with a uniform measure corresp onds to the setting describ ed in Section 1.1 (originally in [8]) 2 . Example 2.C (Squared Euclidean Distance) Let Z 2 R m n denote a data matrix whose elemen ts may assume
Note that in [8] KL-div ergence was used, whic h is a special case of I-div ergence applicable to probabilit y distributions. any real values. This example with a uniform measure corresp onds to the setting describ ed in [5, 4].
 A k l co-clustering of a given data matrix Z is a pair of maps: : f 1 ; ; m g 7! f 1 ; ; k g ; : f 1 ; ; n g 7! f 1 ; ; l g : A natural way to quan tify the \go odness" of a co-clustering is in terms of the accuracy of an appro ximation ^ Z = [^ z uv ] obtained from the co-clustering, i.e., the qualit y of the co-clustering can be de ned as where ^ Z is uniquely determined by the co-clustering ( ; ).
Giv en a co-clustering ( ; ), there can be a num ber of di eren t matrix appro ximations ^ Z based on the informa-tion we choose to retain. Let ^ U and ^ V be random vari-ables for row and column clusterings resp ectiv ely, taking values in f 1 ; ; k g and f 1 ; ; l g suc h that ^ U = ( U ) and ^ V = ( V ). Then, the co-clustering ( ; ) involves four un-derlying random variables U , V , ^ U , and ^ V corresp onding to the various partitionings of the matrix Z . We can now obtain di eren t matrix appro ximations based solely on the statistics of Z corresp onding to the non-trivial com binations of f U; V; ^ U ; ^ V g given by If ( ) denotes the power set of , then every elemen t of ( ) is a set of constrain ts that leads to a (possibly) di eren t matrix appro ximation. Hence, ( ) can be considered as the class of matrix appr oximation schemes based on a given co-clustering ( ; ). For the sak e of illustration, we consider four examples corresp onding to the non-trivial constrain t sets in ( ) that are symmetric in U; ^ U and V; ^ V :
C 3 = ff ^ U ; ^ V g ; f U g ; f V gg ; C 4 = ff U; ^ V g ; f
Now, for a speci ed constrain t set C 2 ( ) and a co-clustering ( ; ), the set of possible appro ximations M consists of all Z 0 2 S m n that depend only on the rel-evant statistics of Z , i.e., E [ Z j C ] ; C 2 C , or more pre-cisely , satisfy the follo wing conditional indep endenc e condi-tion: Z ! f E [ Z j C ] : C 2 Cg ! Z 0 . Hence, the appro xima-tions Z 0 can be a function of only f E [ Z j C ] : C 2Cg .
We can now de ne the \best" appro ximation ^ Z corre-sponding to a given co-clustering ( ; ) and the constrain t set C as the one in the class M A ( ; ; C ) that minimizes the appro ximation error, i.e.,
Interestingly , it can be sho wn [1] that the \best" matrix appro ximation ^ Z turns out to be the minim um Bregman information matrix among the class of random variables M
B ( ; ; C ) consisting of all Z 0 2 S m n that preserve the relev ant statistics of Z or more precisely , satisfy the line ar constr aints : 8 C 2 C , E [ Z j C ] = E [ Z 0 j C ] : Hence, the best appro ximation ^ Z of the original matrix Z for a speci ed co-clustering ( ; ) and constrain t set C is given by ^ Z = argmin This leads to a new minim um Bregman information principle : the best estimate given certain statistics is one that has the minim um Bregman information sub ject to the linear constrain ts for preserving those statistics. It is easy to see that the widely used maximum entr opy principle [6] is a special case of the prop osed principle for I-div ergence since the entrop y of a join t distribution is negativ ely related to the Bregman information (Example 1.B). In fact, even the least squares principle [7] can be obtained as a special case when the Bregman div ergence is squared Euclidean distance.
The follo wing theorem characterizes the solution to the minim um Bregman information problem (2.6). For a pro of, see [1].
 Theorem 1 For a Bregman diver genc e d , any random vari-able Z 2 S m n , a speci e d co-clustering ( ; ) and a speci-ed constr aint set C , the solution ^ Z to (2.6) is given by wher e ? f ? r g are the optimal Lagrange multipliers corre-sponding to the set of line ar constr aints: E [ Z 0 j C r 8 C r 2C : Furthermor e, ^ Z always exists, and is unique.
We can now quan tify the goodness of a co-clustering in terms of the exp ected Bregman div ergence between the orig-inal matrix Z and the minim um Bregman information so-lution ^ Z . Thus, the Bregman co-clustering problem can be concretely de ned as follo ws: De nition 1 Giv en k , l , a Bregman div ergence d , a data matrix Z 2 S m n , a set of constrain ts C 2 ( ), and an underlying probabilit y measure , we wish to nd a co-clustering ( ? ; ? ) that minimizes: where ^ Z = argmin The problem is NP-complete by a reduction from the kmeans problem. Hence, it is dicult to obtain a globally optimal solution. However, in Section 3, we analyze the problem in detail, and pro ve that it is possible to come up with an itera-tive update scheme that pro vides a locally optimal solution. Example 1.D (I-Div ergence) The Bregman co-clustering objectiv e function is given by E [ Z log( Z ^ where ^ Z is the minim um Bregman information solution (Ta-ble 1). Note that for the constrain t set C 3 and Z based on a join t distribution p ( X; Y ), the objectiv e function reduces to D ( p jj q ) where q is of the form (1.3) that was used in [8] Example 2.D (Squared Euclidean Distance) The Breg-man co-clustering objectiv e function is E [( Z ^ Z ) 2 ] where ^ Z is the minim um Bregman information solution (Table 2). Note that for the constrain t set C 4 , this reduces to E [( Z E [ Z j U; ^ V ] E [ Z j ^ U ; V ] + E [ Z j ^ U ; tical to the objectiv e function in [5, 4].
In this section, we shall dev elop an alternating minimiza-tion scheme for the general Bregman co-clustering problem (2.8). Our scheme shall serv e as a meta algorithm from whic h a num ber of special cases (both new and previously kno wn) can be deriv ed. Throughout this section, let us sup-pose that the underlying measure , the Bregman div ergence d , the data matrix Z 2 S m n , num ber of row clusters k , num ber of column clusters l , and the constrain t set C are speci ed. We rst outline the essence of our scheme. Step 1: Start with an arbitrary row and column clustering, Step 2: Rep eat one of the two steps below till con vergence: Note that at any time in Step 2, the algorithm may choose to perform either Step 2A or 2B.
From the above outline, it is clear that the key steps in our algorithm involve nding a solution of the minim um Breg-man information problem (2.6) and appropriately updating the row and column clusters. First, we focus on the latter task. Consider matrix appro ximations based on the func-tional form for the minim um Bregman solution ^ Z given in (2.7). For a given ( ; ; C ), there exist a unique set of optimal Lagrange multipliers so that (2.7) uniquely speci es the minim um Bregman information solution ^ Z . In general, (2.7) pro vides a unique appro ximation, say ~ Z , for any set of La-grange multipliers (not necessarily optimal) and ( ; ; C ) since r ( ) is a monotonic function [3, 2]. To underscore the dep endence of ~ Z on the Lagrange multipliers, we shall use the notation ~ Z = ( ; ; ) = ( r ) 1 ( P s r =1 r ). The ba-sic idea in considering appro ximations of the form ( ; ; ) is that alternately optimizing the co-clustering and the La-grange multipliers leads to an ecien t update scheme that does not require solving the minim um Bregman information problem anew for eac h possible co-clustering.

Further, the matrix appro ximations of the form ( ; ; ) have a nice separabilit y prop erty [1] that enables us to de-comp ose the matrix appro ximation error in terms of either the rows and columns: where ~ Z = ( ; ; ) and ( U; ( U ) ; V; ( V )) = d ( Z; Using the above separabilit y prop erty, we can ecien tly ob-Table 1: Minim um Bregman information solution for I-Div ergence leads to multiplicativ e mo dels. Table 2: Minim um Bregman information solution for squared Euclidean distance leads to additiv e mo dels.
 tain the best row clustering by optimizing over the indi-vidual row assignmen ts while keeping the column clustering xed and vice versa. In particular, optimizing the con tribu-tion of eac h row to the overall appro ximation error leads to the row cluster update step, Similarly , we obtain the column cluster update step,
So far, we have only considered updating the row (or column clustering) keeping the Lagrange multipliers xed. After row (or column) updates, the appro ximation ~ Z t = ( t +1 ; t +1 ; t ) is closer to the original matrix Z than the earlier minim um Bregman information solution ^ Z t , but it is not necessarily the best appro ximation to Z of the form ( t +1 ; t +1 ; ). Hence, we need to now optimize over the Lagrange multipliers keeping the co-clustering xed. It turns out [1] that the Lagrange multipliers that result in the best appro ximation to Z are same as the optimal Lagrange multipliers of the minim um Bregman information problem based on the new co-clustering ( t +1 ; t +1 ). Based on this observ ation, we set ^ Z t +1 to be the minim um Bregman in-formation solution in steps 2A and 2B.
Finally , we state the meta algorithm for generalized Breg-man co-clustering (see Algorithm 1) that is a concrete \im-plemen tation" of our outline at the beginning of Section 3. Further, since the row/column cluster update steps and the minim um Bregman solution steps all progressiv ely decrease the matrix appro ximation error, i.e., the Bregman co-clustering objectiv e function, the alternate minimization scheme sho wn in Algorithm 1 is guaran teed to achiev e local optimalit y. Theorem 2 The gener al Bregman co-clustering algorithm (Algorithm 1) conver ges to a solution that is locally optimal for the Bregman co-clustering problem (2.8), i.e., the obje c-tive function cannot be impr oved by changing either the row clustering, or the column clustering. Table 3: Row and column cluster updates for I-div ergence.
 Table 4: Row and column cluster updates for squared Euclidean distance.
 When the Bregman div ergence is I-div ergence or squared Euclidean distance, the minim um Bregman information prob-lem has a closed form analytic solution as sho wn in Tables 1 and 2. Hence, it is straigh tforw ard to obtain the row and column cluster update steps (Tables 3 and 4) and implemen t the Bregman co-clustering algorithm (Algorithm 1). The resulting algorithms involve a computational e ort that is linear in the size of the data and are hence, scalable. In general, the minim um Bregman information problem need not have a closed form solution and the update steps need to be determined using numerical computation. However, since the Lagrange dual L () in the minim um Bregman informa-tion problem (2.6) is con vex in the Lagrange multipliers , it is possible to obtain the optimal Lagrange multipliers using con vex optimization techniques [3]. The minim um Bregman information solution and the row and column cluster update steps can then be obtained from the optimal Lagrange mul-tipliers.
There are a num ber of exp erimen tal results in existing literature [4, 5, 8, 10] that illustrate the usefulness of par-ticular instances of our Bregman co-clustering framew ork. In fact, a large class of parametric partitional clustering al-gorithms [2] including kmeans can be sho wn to be special cases of the prop osed framew ork wherein only rows or only columns are being clustered.

In recen t years, co-clustering has been successfully applied to various application domains suc h as text mining [8] and analysis of microarra y gene-expression data. Hence, here we do not exp erimen tally re-ev aluate the Bregman co-clustering algorithms against other metho ds. Instead, we presen t brief case studies to demonstrate two salien t features of the pro-posed co-clustering algorithms: (a) dimensionalit y reduc-tion, and (b) missing value prediction.
Dimensionalit y reduction techniques are widely used for text clustering to handle sparsit y and high-dimensionalit y of text data. Typically , the dimensionalit y reduction step comes before the clustering step, and the two steps are al-most indep enden t. In practice, it is not clear whic h dimen-Algorithm 1 Bregman Co-clustering Algorithm Table 5: E ect of Implicit Dimensionalit y Reduction by Co-clustering on Classic3 . Eac h subtable is for a xed num ber of (do cumen t,w ord) co-clusters. sionalit y reduction technique to use in order to get a good clustering. Co-clustering has the interesting capabilit y of interle aving dimensionalit y reduction and clustering. This implicit dimensionalit y reduction often results in sup erior results than regular clustering techniques [8].

Using the bag-of-w ords mo del for text, let eac h column of the input matrix represen t a documen t, and let eac h row represen t a word. Keeping the num ber of documen t clus-ters xed, we presen t results by varying the num ber of word clusters. We ran the exp erimen ts on the Classic3 dataset, a documen t collection from the SMAR T pro ject at Cornell Univ ersit y with 3 classes. Co-clustering was performed with-out looking at the class lab els. We presen t confusion matri-ces between the cluster lab els assigned by co-clustering and the true class lab els, over various num bers of word clusters. The num ber of documen t clusters were xed at 3 for all ex-perimen ts rep orted. As we can clearly see from Table 5 (for Classic3 ), implicit dimensionalit y reduction by co-clustering actually gives better documen t clusters, in the sense that the cluster lab els agree more with the true class lab els with few er word clusters.
To illustrate missing value prediction, we consider a col-lab orativ e ltering based recommender system. The main problem in this setting is to predict the preference of a given user for a given item using the kno wn preferences of all other users. A popular approac h to handle this is by computing the Pearson correlation of eac h user with all other users Table 6: Mean Absolute Error for Mo vie Ratings Algo. C 2 ,SqE C 3 ,SqE C 2 ,IDiv C 3 ,IDiv Pearson
Error 0.8398 0.7639 0.8397 0.7723 1.4211 based on the kno wn preferences and predict the unkno wn rating by prop ortionately com bining the other users' rat-ings. We adopt a co-clustering approac h to address the same problem. The main idea is to sim ultaneously compute the user and item co-clusters by assigning zero measure to the missing values. As a result, the co-clustering algorithm tries to reco ver the original structure of the data while disregard-ing the missing values and the reconstructed appro ximate matrix can be used for prediction.
 For our exp erimen tal results, we use a subset of the Eac h-Mo vie dataset 3 consisting of 500 users, 200 movies and con-taining 25809 ratings, eac h rating being an integer between 0 (bad) to 5 (excellen t). Of these, we use 90% ratings for co-clustering, i.e., as the training data and 10% ratings as the test data for prediction. We applied four di eren t co-clustering algorithms ( k = 10 ; l = 10) corresp onding to con-strain t sets C 2 and C 3 with squared Euclidean (SqE) distance and I-div ergence (IDiv) to the training data and used the re-constructed matrix for predicting the test ratings. We also implemen ted a simple collab orativ e ltering scheme based on Pearson's correlation. Table 6 sho ws the mean absolute error between the predicted ratings and the actual ratings for the di eren t metho ds. From the table, we observ e that the co-clustering techniques achiev e sup erior results. For constrain t set C 3 , the individual biases of the users (row av-erage) and the movies (column average) are accoun ted for, hence resulting in a better prediction. The co-clustering al-gorithms are computationally ecien t since the pro cessing time is linear in the num ber of the kno wn ratings.
Our work is primarily related to three main areas: co-clustering, matrix appro ximation and learning based on Breg-man div ergences.

Co-clustering has been a topic of much interest in the recen t years because of its applications to problems in mi-croarra y analysis [4, 5] and text mining [8]. In fact, there exist man y form ulations of the co-clustering problem suc h as the hierarc hical co-clustering mo del [9], the bi-clustering mo del [4] that involves nding the best co-clusters one at a time, etc. In this pap er, we have focussed on the partitional co-clustering form ulation rst introduced in [9].
Matrix appro ximation approac hes based on singular value decomp osition (SVD) have been widely studied and used. However, they are quite often inappropriate for data ma-trices suc h as co-o ccurrence and con tingency tables, since SVD-based decomp ositions are dicult to interpret, whic h is necessary for data mining applications. Alternativ e tech-niques involving non-negativit y constrain ts [11] using KL-div ergence as the appro ximation loss function [10, 11] have been prop osed. However, these approac hes apply to spe-cial types of matrices. A general form ulation that is both interpretable and applicable to various classes of matrices would be invaluable. The prop osed Bregman co-clustering form ulation attempts to address this requiremen t.
Recen t researc h [2] has sho wn that sev eral results involv-ing the KL-div ergence and the squared Euclidean distance http://www.researc h.compaq.com/src/eac hmo vie/ are in fact based on certain con vexit y prop erties and hence, generalize to all Bregman div ergences. This intuition mo-tivated us to consider co-clustering based on Bregman di-vergences. Further, the similarities between the maxim um entrop y and the least squares principles [7] prompted us to explore a more general minim um Bregman information prin-ciple for all Bregman div ergences.
This pap er mak es three main con tributions. First, we generalized parametric co-clustering to loss functions cor-resp onding to all Bregman div ergences. The generalit y of the form ulation mak es the technique applicable to practi-cally all types of data matrices. Second, we sho wed that appro ximation mo dels of various complexities are possible dep ending on the statistics that are to be preserv ed. Third, we prop osed and extensiv ely used the minim um Bregman information principle as a generalization of the maxim um entrop y principle. For the two Bregman div ergences that we focussed on, viz., I-div ergence and squared Euclidean distance, the prop osed algorithm has linear time complexit y and is hence scalable.
 Ackno wledgemen ts: This researc h was supp orted by NSF gran ts IIS-0307792, IIS0325116, NSF CAREER Aw ard ACI-0093404, and by an IBM PhD fello wship to Arindam Banerjee. [1] A. Banerjee, I. Dhillon, J. Ghosh, S. Merugu, and [2] A. Banerjee, S. Merugu, I. Dhillon, and J. Ghosh. [3] Y. Censor and S. Zenios. Par allel Optimization: [4] Y. Cheng and G. M. Churc h. Biclustering of [5] H. Cho, I. S. Dhillon, Y. Guan, and S. Sra. Minim um [6] T. M. Cover and J. A. Thomas. Elements of [7] I. Csiszar. Wh y least squares and maxim um entrop y? [8] I. Dhillon, S. Mallela, and D. Mo dha.
 [9] J. A. Hartigan. Direct clustering of a data matrix. [10] T. Hofmann and J. Puzic ha. Unsup ervised learning [11] D. L. Lee and S. Seung. Algorithms for non-negativ e
