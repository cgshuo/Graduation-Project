 In recent years, a number of open databases have emerged on the Web, providing Web users with platforms to collaboratively create structured information. As these databases are intended to accom-modate heterogeneous information and knowledge, they usually comprise a very large schema and billions of instances. Browsing and searching data on such a scale is not an easy task for a Web user. In this context, interactive query construction offers an intuitive in-terface for novice users to retrieve information from databases nei-ther requiring any knowledge of structured query languages, nor any prior knowledge of the database schema. However, the existing mechanisms do not scale well on large scale datasets. This paper presents a set of techniques to boost the scalability of interactive query construction, from the perspective of both, user interaction cost and performance. We connect an abstract ontology layer to the database schema to shorten the process of user-computer inter-action. We also introduce a search mechanism to enable efficient exploration of query interpretation spaces over large scale data. Ex-tensive experiments show that our approach scales well on Freebase -an open database containing more than 7,000 relational tables in more than 100 domains.
 H.3.3 [ Information Search and Retrieval ]: Query formulation Query construction, Freebase, Ontology
With the prevalence of Web 2.0, a number of open databases have emerged on the Web, attempting to provide a platform for users to collaboratively create and maintain structured information. A typical example is Freebase 1 , which currently contains more than 22 million entities and 350 million facts from more than 100  X 
C orresponding author: zhou.xuan@outlook.com www.freebase.com domains, organized in 7,500 tables. Other examples include DB-pedia 2 , WikiTaxonomy 3 , and Probase 4 , whose sizes have already reached the magnitude of several GBs. Databases of this kind are intended to accommodate heterogeneous information and knowl-edge. It is natural that each of these datasets contains a very large schema and a large volume of data. For a typical Web user, in-formation seeking over such large and heterogeneous database is a challenge.

The technology of interactive query construction [7] enables novice users to interactively create structured queries and retrieve desired information from a database without knowing any structured query language or studying the database schema a-priori. The existing approaches of interactive query construction work well for small or medium sized databases of a particular domain, such as IMDB Lyrics [16], which contain around 20 tables [7]. However, we found that existing approaches fail to scale on a heterogeneous Freebase database composed of several thousand tables. To this extent, the FreeQ system presented in this paper enables us to scale interactive query construction over a very large database.

The interface of interactive query construction combines the us-ability of keyword queries with the expressiveness of structured queries. It enables a user to start with a keyword query and refine it into a structured query by interacting with the system. Through interaction, the user can provide additional information to disam-biguate the semantics of the keyword query, and finally determine the structured expression reflecting her informational need. The resulting structured queries offer enhanced expressiveness to re-trieve results with complex semantics, including collective results, e.g.  X  X ll films starring Tom Hanks X  , or results involving more than one entity, e.g.  X  X he role of Tom Hanks in the film The Terminal X  . In this way, interactive query construction opens the world of struc-tured queries to unskilled users, who are not familiar with struc-tured query languages, without actually requiring them to learn such query language. This interface is also a useful tool for ex-pert users, who want to explore data organized in an unfamiliar and complex database schema.

Interactive query construction is especially useful for seeking in-formation on large scale, where keyword queries become increas-ingly ambiguous. For instance, in Freebase, the phrase  X  X ack Lon-don X  occurs in more than sixty attributes and can be matched with the entities of types author , film , olympic athlete , tourist attraction , astronomical discovery , and others. As a result, there can be a large number of plausible answers to the keyword query  X  X ack London X  , such as author Jack London, asteroid  X 2625 Jack London X , a British w ww.dbpedia.org http://www.h-its.org/english/research/nlp/index.php http://research.microsoft.com/en-us/projects/probase www.imdb.com athlete John Edward  X  X ack X  London, and the Jack London Distri ct in Oakland, California. To find the desired information through a keyword search interface, a user may have to scan through a long list of search results. In contrast, an interactive query construction interface suggests interaction options, such as  X  X ack London is a book author X  for the user to clarify her intent. By clicking the cor-rect options, the user helps the system to form structured queries to precisely retrieve the desired information.

There are two main reasons why existing approaches fail to scale on heterogeneous databases composed of several thousand tables. First, when the database schema is very big, the interaction op-tions generated by the existing schemes are usually not informative enough. As a result, a user may have to go through a laborious interaction procedure to construct the desired query. For example, as the phrase  X  X ack London X  appears in more than sixty Freebase attributes, this phrase can be interpreted into more than sixty mean-ings. Using the existing interaction schemes, in the worst case, the user may have to respond to each of the sixty interpretations to finally clarify her intent. For a more complex keyword query, the procedure of interaction can become unacceptably long. Sec-ond, the interpretation space of a keyword query on a very large database is usually too big to be materialized completely. The ex-isting approaches to database keyword search usually rely on an en-tirely materialized interpretation space to retrieve top-k structured queries and interaction options. These approaches become infeasi-ble in face of large scale databases. Thus we need new methods to explore the interpretation spaces of keyword queries.

The FreeQ system presented in this paper enables scalable in-teractive query construction over a very large database. First, we propose to connect a hierarchical ontology to the database schema. Using the general concepts in the ontology, we can form more in-formative interaction options that enable more efficient query con-struction. The hierarchical ontology can be a manually constructed one, such as the domain set of Freebase. It can also be a generic ex-ternal ontology, such as YAGO [21]. We conducted both theoretical and experimental studies to evaluate how a hierarchical ontology can speedup interactive query construction. Second, to explore the interpretation space of keyword queries, we design a scheme that is able to efficiently generate top-k structured queries and the optimal interaction options without the complete knowledge of the search space. Finally, we conducted extensive experiments on Freebase demonstrating the effectiveness and the efficiency of our approach.
In summary, we have made the following contributions: (i) A new type of interaction options based on ontologies to enable scal-able interactive query construction, and a theoretical justification about the effectiveness of these options; (ii) A scheme to enable efficient generation of top-k structured queries and interaction op-tions, without the complete knowledge of the query interpretation space; (iii) An experimental study on Freebase to verify the ef-fectiveness and efficiency of the proposed approach; (iv) To the best of our knowledge, this is the first attempt to enable effective keyword-based query construction on such a large scale database as Freebase, considering that most existing work on database key-word search uses only test sets of small schemas, such as DBLP, IMDB, etc.
A user interface for interactive query construction is presented in Figure 1. This interface is composed of four parts: (1) an in-put field for keyword queries, (2) a query construction panel for presenting interaction options, (3) a query window for presenting structured queries, and (4) a result window for query results. Sup-pose a user, whose name is Alice, issues a keyword query to the Figure 1: FreeQ GUI. Its components include: (1) an input field for keyword queries, (2) a query construction panel, (3) top-k struc-tured queries, and (4) query results. system. The system first tries to guess Alice X  X  intent and generates the top-k most likely structured queries in the query window (3). If one of the top-k structured queries matches Alice X  X  intent, she can click the query to obtain the results (4). If no query in the top-k list makes sense to Alice, she can interact with the query construction panel (2) to construct the desired structured query. Whenever Al-ice clicks on an interaction option in the query construction panel, the structured queries in the query window (3) are refined, such that only the queries complying with Alice X  X  selection are preserved. Si-multaneously, a new set of query construction options is presented in the query construction panel (2). The interaction continues until Alice obtains the desired structured query and results.

In this section, we introduce the basic model for enabling such interface for interactive query construction. We also elaborate on the challenges posed by large scale databases.
We start the discussion of the process of interactive query con-struction with the definition of the key concepts of this process. First of all, we model the schema of a database as a graph.
D EFINITION 2.1. A schema graph is a graph G = ( V , E ) , where each vertex v  X  V represents a relational table and each edge e  X  E represents a foreign key relationship. In the graph, each node v is associated with a set of attributes, denoted by A ( v ) , where the i attribute is represented by v . a i  X  A ( v ) . 2 We use the number of vertices to represent the size of a schema graph. Using the structures in a schema graph, we can create struc-tured queries.

D EFINITION 2.2. Given a schema graph G = ( V , E ) , a struc-tured query is an edge preserving map G  X  = ( V  X  , E  X  ) , such that there is a function L : V  X   X  V which satisfies: for each vertex v in the structured query, there is a vertex L ( v  X  )  X  V in the schema graph such that v  X  and L ( v  X  ) represent the same relational table, and for each edge { v  X  1 , v  X  2 }  X  E  X  in the structured query, there is an edge { L ( v  X  1 ) , L ( v  X  2 ) }  X  E in the schema graph.
In addition, each vertex v  X  in the structured query can be asso-ciated with a number of predicates. Each predicate is in the form v . a i op c i , where v  X  . a i is an attribute of v  X  , op is a comparison operator, and c i is a constant. 2 For instance, given a film database, a query looking for all the ac-tors who have collaborated with Tom Hanks can be expressed as:
Q 1 = {structure: actor 1  X  X  X  acts  X  X  X  film 1  X  X  X  acts  X  X  X  actor predicates: actor 1 . name =  X  Tom Hanks  X  }.
 It is worth mentioning that each table in the database occurs only once in the schema graph. In contrast, a table can occur multiple times in a structured query.

In the process of interactive query construction, users express their informational needs as keyword queries.

D EFINITION 2.3. A keyword query is a bag of terms K = { k k ,..., k n } , where duplicates are allowed. 2 In Definition 2.3, a term is a normalized class of tokens that is in-cluded in the system X  X  dictionary. For token normalization, state-of-the-art Information Retrieval techniques such as case folding and word segmentation can be applied [18].

The main function of FreeQ is to translate a user X  X  keyword query into the intended structured query. We call the structured query resulting from such translation a query interpretation . We say that a query interpretation is complete if this query interpretation contains all keywords from the initial user query. Otherwise we talk about partial query interpretation. We call the set of all com-plete query interpretations of a keyword query K an interpretation space of K .

For instance, the keyword query  X  X om Hanks Film X  seeking the movies starring Tom Hanks can be interpreted to:
Q 2 ={structure: actor  X  X  X  acts  X  X  X  film , predicates: actor . name =  X  Tom Hanks  X  } .
 In this interpretation, keywords  X  X om Hanks X  are mapped to the constant of a predicate, and  X  X ilm X  is mapped to a table name. The following query is a partial interpretation of  X  X om Hanks Film X  , where only  X  X om Hanks X  is interpreted:
O 1 = {structure: actor , predicates: actor . name =  X  Tom Hanks  X  }.

In the process of query construction we interpret user X  X  key-words and generate query construction options (QCOs) to assess the meaning of the keywords intended by the user. We can do that in two ways: First, we can interpret keywords very specifically as a part of a structured query (as performed in [7]). We refer to these QCOs as query-based QCOs . For instance, O 1 can be used as a QCO, which indicates that  X  X om Hanks X  should be interpreted as an actor X  X  name. Second, we can also assess the general meaning of the keywords and interpret them as a generic concept representing a class of structured queries. For example, we can interpret  X  X om Hanks X  as a more generic class person, which is a superclass of actor:
O 2 = {structure: person , predicates: person . name =  X  Tom Hanks  X  }.

To enable efficient user interaction over large database schema, in this paper we introduce ontology-based QCOs. In a generic object-relational database, a table can be regarded as an entity type, and an attribute of the table can be regarded as a property. Us-ing a hierarchical ontology, a set of entity types can be abstracted into a superclass, and a set of properties can be abstracted into a super-property. For instance, entity types painter and musician can be abstracted into artist , and their properties painting and music can be abstracted into work . Using these superclasses and super-properties, we can create general QCOs that subsume larger pro-portions of a query interpretation space than query-based QCOs.
D EFINITION 2.4. Given a schema graph G = ( V , E ) , we use sv  X  v to denote that sv is a superclass of v  X  V and se  X  e to de-note that se is a super-property of e  X  E. Superclass and super-properties are partial order relationships. 2
With the concepts of superclass and super-property, we define ontology-based query interpretations and ontology-based QCOs. D EFINITION 2.5. Let K = { k 1 , k 2 ,..., k n } be a keyword query. Let Q = ( V , E ) be a query interpretation of K. Let Q o to Q, where the isomorphism function is f ( . ) (that applies to the predicates too). Q o is an ontology-based interpretation of K, iff (3) for all attributes v . a i  X  A ( v ) in the predicates, f ( v . a say that Q o is a super-interpretation of Q. 2
When we use ontology-based interpretations as QCOs, we call them ontology-based QCOs . In summary, QCOs generated by our system can be either query-based or ontology-based QCOs.
D EFINITION 2.6. A Query Construction Option (QCO) is a mapping from a subset of keyword query K  X   X  K to either:
In the interaction process the user is supposed to select the op-tions that subsume her intended query interpretation.
 D EFINITION 2.7. Given a QCO O and a QCO O  X  , we say that O subsumes O  X  , if either: Subsumption relationship is transitive, i.e. if O subsumes O O subsumes O  X  X  , then O subsumes O  X  X  . 2 Certainly, as O 1 is a subgraph of the interpretation Q 2 sumes Q 2 . O 2 is an ontological interpretation of  X  X om Hanks X  , and it is a super-interpretation of O 1 . As subsumption relationship is transitive, both O 2 and O 1 subsume Q 2 , which is a complete in-terpretation of the query  X  X om Hanks Film X  .

With the above concepts, the conceptual process of interactive query construction can be modeled as follows: 0. Given a database whose schema is G = ( V , E ) , a user issues 1. Initialization : Let  X  be an interpretation space of K based on 2. Top-k Generation : The system retrieves the top-k interpre-3. QCO Generation : The system generates a QCO O and lets 4. P ost Interaction : If the user indicates that O subsumes the
Each iteration of the process requires one round of interaction with the user. As the interaction goes on, the interpretation space  X  keep shrinking. Because  X  is finite, the process guarantees to ter-minate at a certain point. Nevertheless, we would like the process to be short, so that users can obtain desired information as early as possible. The efficiency of query construction can be measured naturally by the number of iterations of the process. We call this measure interaction cost.

D EFINITION 2.8. Given a process of interactive query construc-tion, its interaction cost is the number of iterations it has been exe-cuted, which is equivalent to the number of QCOs evaluated by the user. 2
When a database, and especially its schema graph, becomes big, it is difficult for the existing approaches to realize an efficient query construction process. This is due to the following two limitations: P ROBLEM 1. Inefficient query-based QCOs:
To minimize the interaction cost, the query construction process needs to shrink the query interpretation space quickly. In other words, the evaluation of each QCO should be able to remove a sig-nificant proportion of the interpretation space. Therefore, we desire the proportion of query interpretations subsumed by each QCO to fall in a certain range. This proportion should not be too small, as in this case the denial of a QCO could not reduce the interpretation space effectively. It should not be too big either, as in this case the acceptance of a QCO could not reduce the interpretation space effectively.

When the schema graph is big, a keyword can have a large num-ber of occurrences spread across the database, resulting in a vast number of partial interpretations (query-based QCOs). The pro-portion of the interpretation space subsumed by each query-based QCO will be very small. As a result, query construction processes using only query-based QCOs, such as [7], cannot be efficient. Apart from query-based QCOs, we need more general QCOs to enable efficient query construction.
 P ROBLEM 2. Very large query interpretation space:
When the schema graph becomes big, it is no longer feasible to materialize the interpretation space of a complex keyword query entirely. On the one hand, with an increasing size of a schema graph, the number of its subgraphs grows very sharply. On the other hand, the occurrences of keywords are more numerous in a larger database. As a result, the number of the possible interpretations of a keyword query grows quickly with an increasing size of the schema.

The existing approaches to interactive query construction [7], [8], as well as the state-of-the-art approaches to schema-based database keyword search [2], [12], [16], [17] rely on an entirely material-ized query interpretation space. These approaches can hardly work with a big schema, as it is infeasible to generate all the query in-terpretations at the query time. Therefore, we need a new mecha-nism which can enable efficient identification of the most efficient QCOs and the top-k most probable query interpretations, without the knowledge of the complete interpretation space.

Compared with the most recent approach to interactive query construction [7], in this paper we addressed the problems listed above and made the following contributions:
We present our solutions to each of these problems in Sections 3 and 4, respectively.
As pointed out by Problem 1 in Section 2.2, to minimize the interaction cost, the QCOs presented to the user need to shrink the query interpretation space quickly. In a large scale database, each single keyword can have numerous occurrences. As a result, the query-based QCOs utilized by the existing approaches [7] be-come inefficient in reducing interpretation spaces. In this section, we introduce a novel type of QCOs, called ontology-based QCOs. An ontology-based QCO can subsume a wider proportion of an interpretation space, such that it is usually more efficient than a query-based QCO. Intuitively, if the user provides feedback on an ontology-based QCO, we get an implicit user X  X  feedback on mul-tiple partial interpretations (i.e. query-based QCOs) subsumed by this ontology-based QCO within a single user interaction. Thus a query construction process using ontology-based QCOs requires less steps. In this section, we justify the efficiency of ontology-based QCOs from the perspective of information theory.
Ontology-based QCOs can be created based on a hierarchical on-tology or taxonomy. In order to create ontology-based QCOs, we need an ontology on top of the database schema, which defines su-perclasses and super-properties. This ontology can be a manually defined one, such as the domain hierarchy of Freebase. Alterna-tively, we can utilize external ontologies, such as e.g. WordNet [9], and YAGO [21], by mapping the elements of the database schema to the concepts in these ontologies. In this case state-of-the-art schema matching techniques can be used (see [6] for details).
With ontology-based QCOs, we can enable more efficient query construction, especially when confronted with a big database schema. To illustrate a query construction process using ontology-based QCOs, we consider the query  X  X mperor Album X  , which intends to retrieve the albums of the artist Emperor from Freebase. To create the ontology-based QCOs, we make use of the domain hierarchy of Freebase. This hierarchy groups together Freebase tables such as artist , album , and monarch in the domains e.g. music and royalty , and further organizes these domains into the categories such as Arts &amp; Entertainment and Society .

For this particular query, if we use only query-based QCOs (as performed by [7]), our system requires a user to interact with 74 QCOs to identify the intended interpretation. Using ontology-based QCOs, the user only needs to interact with the 10 QCOs listed in Table 1. The keyword  X  X lbum X  is not very ambiguous, as it oc-curs mostly in the domain of music . To disambiguate this keyword, FreeQ does not utilize any ontology-based QCOs. In contrast, the Table 1: A Query Construction Example for the Query  X  Emperor Album X  using Ontology-based QCOs keyword  X  X mperor X  is very ambiguous.  X  X mperor X  occurs in 221 attributes of Freebase, which are spread across multiple categories and domains. To disambiguate  X  X mperor X  , it is much faster if we use ontology-based QCOs. With ontology-based QCOs, we man-age to first restrict the meaning of  X  X mperor X  to the category of Arts &amp; Entertainment . Within this category, the exact meaning of  X  X mperor X  can be identified easily.

In what follows, we analyze how ontology-based QCOs achieve such efficiency.
As depicted in Section 2.2, in each round of interactive query construction, FreeQ needs to select one QCO to present to the user. For a keyword query, there is usually a large number of available QCOs. In principle, FreeQ should always select the most efficient QCO that can minimize the final interaction cost. The efficiency of a QCO can be quantified using information theory.
 Let  X  denote the interpretation space of a keyword query K . Then, the uncertainty of K  X  X  interpretation can be measured by the entropy H (  X  ) , which can be computed as: where P ( I ) denotes the probability that the interpretation I is the interpretation intended by the user.

The process of query construction is the process of reducing the uncertainty of K  X  X  interpretation. After one round of interaction with the user, FreeQ obtains the knowledge of one QCO, say O . Then, the uncertainty is reduced to H (  X  | O ) , i.e., the conditional entropy of  X  given O . The difference between H (  X  ) and H (  X  | O ) is known as the expected information gain provided by O , denoted by: To minimize the interaction cost, we need maximize the informa-tion gain of each QCO presented to the user.
 Obviously, the knowledge about  X  contains the knowledge of any O . In other words, the information gain provided by O is exactly the entropy of O . Therefore, we have: In turn, the entropy of O can be calculated as: where P ( O ) is the probability that O is accepted by the user. Let  X  ( O ) denote the complete set of query interpretations subsumed by O . Then, P ( O ) can be computed as: Figure 2: Efficiency of QCO and Interaction Cost vs. Schema Size
To summarize, the efficiency of a QCO can be measured by its entropy. Therefore, FreeQ is supposed to select the QCO with the highest entropy in each round of user interaction.
As discussed in Section 3.2, to achieve fast query construction, in each round of interaction, FreeQ is supposed to present the user with an efficient QCO, i.e., a QCO whose entropy is sufficiently high. The question is whether such QCOs would be available at all. To answer this question, we first propose the following measure to quantify the efficiency of an entire query construction process.
D EFINITION 3.1. During an interactive query construction pro-cess, if we can ensure with a high probability that the entropy of each QCO presented to the user is larger than  X  ( 0 &lt;  X   X  1 ), we say that the query construction process is  X  -efficient .
According to Definition 3.1, to minimize the interaction cost, we should maximize the lower bound (  X  ) of the efficiency of the QCOs presented to the user. We can show that query-based QCOs, i.e. the QCOs used in the existing approaches [7], cannot guarantee a lower bound. In contrast, if we have an ontology with a sufficient number of concepts of diverse generality, by using ontology-based QCOs, we can achieve  X  -efficient query construction with a good lower bound  X  .

To simplify our analysis, we assume: (1) the number of possi-ble interpretations of a keyword grows linearly with the size of the schema graph; (2) the number of the possible interpretations of an entire keyword query increases polynomially; (3) all the complete query interpretations are equally probable.

Let the size of the schema be x . According to (1), the number of the query-based QCOs for a keyword can be expressed as  X   X  x , where  X  is a constant. Then the entropy of a query-based QCO for each keyword will be H ( 1  X   X  x ) , which can be plotted as the solid curve in Figure 2a. We can see that when the database schema grows, the efficiency of each query-based QCO will decrease. This efficiency can drop to a very small value when the database schema is big.

According to (2), the size of an interpretation space can be mod-eled as  X   X  x  X  , where  X  and  X  are constants. Based on (3), the most efficient query-based QCO will be an interpretation for a single keyword, whose entropy can be modeled as: H ( 1  X   X  x ) . Therefore, the average interaction cost can be calculated as the entropy of the whole interpretation space divided by the maximum entropy of an curve in Figure 2b. As we can see, if we use only query-based QCOs, the interaction cost can increase quickly with the size of the database schema.

In contrast, if we can achieve a 0 . 7-efficient query construction process, that is, the entropy of each QCO be no less than 0 . 7 (as illustrated by the dashed line in Figure 2a), the growth of the inter-action cost can be significantly reduced (as illustrated by the dashed line in Figure 2b). This is achievable, if we utilize ontology-based QCOs.

The concepts in an ontology normally have a variety of general-ity. There are very specific concepts that can subsume small sets of entity types, such as artist and book . There are also very general concepts that can subsume larger proportions of entity types, such as person and artifact . As a result, no matter how big the query in-terpretation space is, it is always possible to find suitable concepts to form QCOs that can subsume a certain proportion of the inter-pretation space that would yield big entropies. As a simple anal-ysis, we assume that the probability of a random ontology-based QCO is a random value between 0 and 1. Then, within the set of N ontology-based QCOs, the probability that we can find a QCO whose entropy is larger than  X  is: where H  X  1 () is the inverse function of the binary entropy. In this function, no matter how big  X  is, we can always find a big enough N , such that the resulting probability is close to 1. (As N is an ex-ponent in the formula, it normally does not need to be very big.) In other words, as long as there is a rich ontology with a suffi-cient number of concepts of diverse generality, we can achieve  X  -efficiency for interactive query construction.
To perform query construction, FreeQ is required to quickly gen-erate the most efficient QCOs and the most probable complete query interpretations. As mentioned in Problem 2, Section 2.2, the exist-ing approaches to interactive query construction and schema-based database keyword search in general require a complete materializa-tion of the query interpretation space [2], [12], [16], [17], [7]. For a large scale dataset, the interpretation space of a keyword query is usually very big, such that it is no longer feasible to materialize this space entirely. To this end, we develop new algorithms that can generate top-k most probable query interpretations and QCOs without the knowledge of the entire interpretation space.
To enable efficient generation of QCOs and query interpreta-tions, we organize the QCOs of a keyword query in a query hi-erarchy based on their subsumption relationships. Using the query hierarchy, we can materialize the interpretation space of a keyword query step-by-step by following the subsumption relationships of the QCOs. During the progress of the materialization, a lot of QCOs and interpretations can be eliminated based on the informa-tion provided by user interaction. Such progressive materialization is much less costly than the generation of an entire interpretation space.

Figure 3 illustrates a query hierarchy, in which the arrows repre-sent the reversed subsumption relationships. The more general the QCOs, the lower their positions in the query hierarchy. The most general QCOs are the single-node QCOs, i.e. the QCOs involv-ing only one entity type such as O 2 , O 1 , and O 3 . These QCOs are located at the bottom of the hierarchy. The top level of the query hierarchy consists of the complete query interpretations (e.g. Q and Q 4 ), which constitute the interpretation space of a keyword query. The entire query hierarchy looks like an upside-down trape-zoid, as there are much more complete query interpretations than single-node QCOs.
 Figure 3: An Example of a Query Hierarchy for the Query  X  X om H anks Terminal X  (the arrows represent reversed subsumption rela-tionships, e.g. O 2 subsumes O 1 ).

As mentioned previously, it is infeasible to instantiate the com-plete query hierarchy at the query time. Given a big schema, the top levels of the query hierarchy can even be too big to be accommo-dated in the main memory. Therefore, FreeQ chooses to instantiate the query hierarchy incrementally throughout the process of query construction. The query construction process of FreeQ conforms to the generic process defined in Section 2.1, except that it does not materialize the entire query interpretation space. This process works as follows:
As we can see, as the process of the interactive query construc -tion goes on, the instantiated part of the query hierarchy remains stable in size. On the one hand, this part is truncated as the user reveals more information in the interaction process. On the other hand, the instantiated part is continuously expanded by the BF-k and DF-k to ensure that its size is sufficient to generate efficient QCOs and high quality top-k interpretations.
FreeQ obtains top-k query interpretations and query construc-tion options by traversing the query hierarchy. To make interactive query construction smooth to the user, it is important to ensure the efficiency of the hierarchy traversal. Although in the related work the basic Breadth First (BF) and Depth First path search (DF) al-gorithms are typically used to materialize interpretation spaces of keyword queries [2], [12], [11], [16], these algorithms not scale on large database schemas.

The basic BF and DF path search applied in the state-of-the-art database keyword search seek to connect occurrences of the user X  X  keywords in the database schema to materialize query in-terpretation spaces. For example, to materialize query interpre-tation space, Discover [12] explores the schema graph in a BF-manner starting from all tables containing occurrences of a key-word k i  X  K . The time complexity of this approach can be com-containing keyword k i , avg ( E t ) is an average fan-out of a table in the schema graph, K is the number of terms in the keyword query, and r is the maximum length of the path within the schema graph explored to connect occurrences of two keywords. The time com-plexity of this approach increases exponentially with the number of user X  X  keywords and the length of the explored path. This makes materialization of the entire query interpretation space infeasible for longer user queries.

To address these limitations, we propose several important adap-tations of the basic BF and DF path search algorithms, to enable ef-ficient generation of the top-k query interpretations and QCOs over the large database schemas. We refer to these algorithms as BF-k and DF-k , respectively. Both BF-k and DF-k traverse the query hi-erarchy by following the subsumption relationships. Each traversal step expands a QCO to one of the QCOs it subsumes, e.g. expand-ing O 2 to O 1 , or O 1 to Q 3 in Figure 3. Therefore, it is crucial to make the QCO expansion efficient.

In principle, a QCO can be expanded in two different ways. The first type of QCO expansion is to replace an entity type or a prop-erty of the QCO with its subclass or sub-property. Such expansion does not add any additional keywords to the QCO. For example, the expansion of O 2 to O 1 in Figure 3 belongs to the first type. This type of expansion requires only the knowledge of the ontol-ogy structure. Once we have indexed the ontology, such that its subclasses or sub-properties can be retrieved quickly, this type of expansion will be very efficient. The second type of QCO expan-sion is to find the paths in the schema graph to connect a QCO and an additional keyword occurrence. For instance, to expand O Q 3 in Figure 3, we need to identify the path  X  actor  X  X  X  acts  X  X  X  film  X  to connect O 1 and the keyword occurrence film . title : terminal .
To identify the paths for connecting a QCO with a keyword oc-currence efficiently, FreeQ pre-indexes all the paths leading to key-word occurrences in the schema graph starting from every table in the schema graph. The index is constructed at the beginning of the query construction, as the user issues a keyword query. To ensure scalability of the indexing, we restrict the maximal size of the path connecting two keyword nodes in the structured queries that can be constructed by FreeQ and perform bi-directional search. The time complexity of the indexing procedure of FreeQ can be computed as T  X  avg ( E t ) r / 2 , where T is the number of tables in the database schema, avg ( E t ) is an average fan-out of a table in the schema graph, and r / 2 is the half of the maximum length of the explored path. As we can observe, the time complexity of this approach is significantly reduced compared to Discover [12] (discussed above) and does not depend on the size of the keyword query.
 To maximize the quality of the top-k query interpretations and QCOs generated by FreeQ, we always start the expansion with the QCOs that are most likely to be intended by the user. This applies to both DF-k and BF-k . As a result, the top-k interpretations generated by DF-k will be more likely to meet the user X  X  requirements, and the QCOs instantiated by BF-k will be more likely to have high entropy. To ensure a short response time of FreeQ, we also enforce a time limit on both BF-k and DF-k . That is, we stop BF-k and DF-k once the time limit has been reached, even though the top-k interpretations may have not been completely generated, or the number of the instantiated QCOs has not yet reached a predefined upper bound. We demonstrate through the experiments that the resulting hierarchy traversal procedure is effective and efficient.
The QCOs in the query hierarchy are all the candidates to be pre-sented to the user. According to Section 3.2, the most efficient QCO is the QCO with the maximum entropy. To identify this QCO, we need to estimate the probabilities of the QCOs. These probabilities cannot be computed by Equation 5, as we do not have the entire query interpretation space materialized. Instead, we consider the frontier of the instantiated part of the query hierarchy. This frontier contains the most specific query-based QCOs (i.e. partial interpre-tations) that have been materialized so far. We treat all the QCOs in the frontier as samples, such that each QCO in the frontier rep-resents the complete queries that can be derived from this QCO by further expansion of the hierarchy. For each sample represented by s , we compare this sample against each candidate QCO, say o . The comparison can end up with one of the following conclusions: (1) o subsumes s ; (2) s conflicts with o (e.g. one keyword is being in-terpreted into two different meanings); (3) none of the above. If the conclusion is (3), s does not provide any helpful information. With (1) or (2), we can know that either  X  ( s )  X   X  ( o ) , or  X  ( s )  X   X  ( o ) = respectively. By applying the maximum likelihood principle, we can estimate the probability P ( o ) using P ( s ) , that is, where P ( o ) and P ( s ) are the probabilities that o and s are accepted by the user, respectively. With P ( o ) , we can obtain the entropy of o based on Equation 5. Therefore, the problem of finding the QCO with the maximum entropy is reduced to the estimation of the probabilities of (partial) query interpretations P ( s ) . FreeQ applies the probabilistic model in [7] to estimate these probabilities.
We have implemented FreeQ, an interactive query construction system based on the mechanisms proposed in this paper. To evalu-ate the performance of FreeQ, we conducted extensive experiments. Our experiments include two parts. First, we evaluated the impact of ontology-based QCOs on the efficiency of query construction processes. Then, we evaluated the time efficiency of the proposed algorithms in handling large scale databases.
Our experiments were performed on a Freebase dataset down-loaded in June 2011 [10]. This dataset contains approximately 7,500 tables with more than 20 million entities in about 100 do-mains. We imported the dataset into a MySQL database and in-dexed the data and the schema using Apache Solr 6 . FreeQ was implemented as a client-server Java application. For our experi-ments, we used two cores and 10GB of memory on a server, which was equipped with 8x Quad-Core AMD Opteron 2.7GHz proces-sors and 256GB of memory.
 Our query set was based on the user-defined views of Freebase. Freebase allows users to create views using a dedicated query lan-guage called MQL. Each of the views is given a descriptive title in a natural language. Some examples of the view titles are:  X  X ionel Richie discography X  ,  X  X irecting Award: U.S. Dramatic -Winning Films X  , and  X  X V Celebrities on Twitter X  . For evaluation, we can regard the title of each view as a keyword query and the MQL def-inition of the view as the structural interpretation. Then we can study how FreeQ can assist users to construct the structural inter-pretation from the keyword query. The ground truth is a plausible mapping between the keyword query and the structural interpre-tation and can be automatically established through a projection program. For our experiments, we randomly selected a set of 615 keyword queries (views). The number of keywords in each query ranges from 1 to 8 keywords. The structural interpretations of the keyword queries are of different complexity. We measure the com-plexity of an interpretation by the number of keyword nodes it con-tains. (A keyword node is a table containing at least one keyword occurrence.) The complexity of the queries in our query log ranges from 1 to 3. Table 2 presents the average complexity of queries with different number of keywords. As we can observe, the rela-tively complex queries mostly contain 3-5 keywords.
Our first set of experiments was intended to evaluate the effec-tiveness of the ontology-based QCOs. To this extent, we perform two sets of experiments: First, we compare our approach to that in [7], while using different ontologies to generate QCOs. Second, we compare our approach against that of ranking using different ranking functions.

For each query in our query set, its correct interpretation is al-ready known. Thus, the correctness of each QCO generated by FreeQ can be determined without any user intervention. In [7] we evaluated accessibility of the user interface of incremental query construction in a user study, and demonstrated that the users could make the right choice of QCOs using this interface. In this paper we evaluate how big the interaction cost of query construction is, given that the user makes the right choices. In our experiments, we let the computer interact with FreeQ automatically. In this inter-action, the computer will always accept the QCO presented by the h ttps://lucene.apache.org/solr s ystem if this QCO subsumes the correct query interpretation and reject the QCO otherwise.

To evaluate the effectiveness of the ontology-based QCOs, we ran the experiments in a number of scenarios, in which different ontologies were used. The ontologies are of different size and complexity, as summarized in Table 3. NoOntology represents the baseline approach [7], where no ontology is used. Freebase repre-sents the ontology based on the domains and categories given in the Freebase website. YAGO represents an external ontology known as YAGO. To associate each table of Freebase with a suitable YAGO category we used YAGO+F mapping described in [6].

In the experiments, we measured the interaction cost of query construction for the queries with different complexity in different scenarios. According to Definition 2.8, interaction cost is the num-ber of the QCOs a user needs to evaluate to construct a structured query. The results for the 615 test queries are presented in Fig-ure 4a.

Figure 4a presents the mean and the standard deviation of the interaction cost, with respect to query complexity (the number of keyword nodes) and number of query terms. As we can see, com-pared to the baseline NoOntology approach, the interaction cost can be significantly reduced by using the ontology-based QCOs. With a larger ontology, such as YAGO, the interaction cost tends to be smaller. For example, a 2-node query  X  X mi suzuki album X  re-quires 38 QCOs with NoOntology , 19 QCOs with Freebase , and 10 QCOs with YAGO . The benefits of the ontology-based QCOs be-come stronger with an increasing number of keyword nodes in the structural interpretation too. As we can observe, the majority of the 1-node queries, such as  X  X niversity X  and  X  X ootball game X  , can still be efficiently answered in the baseline NoOntology scenario, where an average cost amounts to 3. With the more complex 2-node, and 3-node queries, such as  X  X on shirley album X ,  X  X ilm performance tom everett X , and  X  X ocation leonardo da vinci lived X  , the interac-tion cost of NoOntology goes up quickly to about 30 and can oc-casionally exceed 70. By applying the ontology-based QCOs, this cost can be limited to a much smaller range. For example, the av-erage cost for 2-node queries in the YAGO scenario is around 10. In addition, we can observe that the interaction cost tends to in-crease with the number of keywords, while this trend may not hold in all the cases. This is because a longer keyword query does not necessarily imply the more complex structural interpretation (see Table 2).

In the next set of experiments we compared the interaction cost of query construction using Freebase ontology against that of query ranking to evaluate the effectiveness of interactive query construc-tion in general. To this end, we generated the top-500 interpreta-tions using the DF-k algorithm of FreeQ. We ranked these interpre-tations using two ranking functions proposed recently: the ranking function of FreeQ described in [7], and SQAK [22]. The interac-tion cost of ranking corresponds to the number of interpretations a user needs to evaluate before the intended interpretation is identi-fied, which is exactly the rank of the intended interpretation. If the intended interpretation was not contained within the top-500, we set its rank to 500. Figure 4b presents the results.
As we can observe, the interaction cost of both ranking and con-struction for the simplest queries (with 1-2 keywords or 1 keyword node) is acceptable. For the keyword queries that are relatively complex, ranking become ineffective. This is because the query in-terpretation space on Freebase is too big, such that there can always be a large number of non-intended interpretations that receive good ranks.
Existing schema-based approaches to keyword search and inter-active query construction in databases typically rely on the com-pletely materialized interpretation space of a keyword query [2, 12, 7]. We have tried to run a number of existing algorithms, includ-ing [12], and [7] on Freebase. However, as the schema of Freebase is much bigger than the schemas these algorithms were designed for, they could not finish in a reasonable time. In contrast, the BF-k and DF-k approach allows FreeQ to explore the query interpreta-tion space smartly. Thus it can achieve reasonable response time on a large scale dataset.

To assess the time efficiency of FreeQ, we measured its response time in two phases: (1) The initial response time when a user sub-mits a query, and (2) the interaction response time when a user interacts with the query construction panel.

When a user issues a keyword query, FreeQ will perform a se-ries of pre-processing, including the identification of the keyword occurrences in an inverted index and the creation of a bi-directional index for performing BF-k and DF-k expansions. The mean and the standard deviation of the initial response time for our query set are shown in Figure 4c. As we can see, the mean initialization time stays around 1 second, and its maximum does not exceed 2 seconds. With an increasing number of keywords, the initial re-sponse time increases slowly. This is because the time required for the identification of keyword occurrences in an inverted index in-creases with the number of query terms. This index access time ranges from 70 to 700ms for our query set. We can also see that the initialization time does not increase with the increasing query com-plexity. As presented in Table 2, a more complex query does not necessarily contain more query terms. The time required for the creation of a bi-directional index for DF-k and BF-k ranges from 360 to 460ms for our query set. This time mainly depends on the size and the complexity of the schema graph rather than the com-plexity of queries. We can also observe that the use of different ontologies has limited impact on the initial response time.
We also measured the interaction response time , i.e. the time required by FreeQ to present a new set of QCOs and top-k struc-tured queries after each user interaction. This time comprises the time consumed by the BF-k and DF-k algorithms and the QCO se-lection. Figure 4d presents the results. As we can observe, the interaction response time of FreeQ varies from 1 to 130ms, mean-ing that in most cases the user would feel that the system is reacting instantaneously [20]. The interaction response time increases with an increasing query complexity. This is expected. As more key-words imply a larger query hierarchy and more QCOs to evaluate, the BF-k and DF-k algorithms and the process of QCO selection will all be more time consuming. Nevertheless, this increase does not appear steep in the results. Moreover, a larger ontology seems to incur higher interaction time too. This is because a larger ontol-ogy enables FreeQ to generate more QCOs to evaluate.

In summary, the interaction response time of our system is al-ways below one second. Its initialization time stays within one second most of the time, except for some long queries, whose ini-tialization time can take up to 2 seconds. While further optimiza-tion can be conducted, such performance can already ensure that the user X  X  flow of thought stays uninterrupted [20].
R ecently the problem of structured query generation from user X  X  keywords has attracted a lot of research attention, e.g. [7], [22], [23]. The methods for interpreting keywords evolved from consid-ering attribute values only, to including schema terms and operators [16], [22]. In [15], [1], the author modeled the problem as question guided search.

Demidova et al. proposed a probabilistic incremental query con-struction model for an interactive user interface [7]. These methods do not provide a sufficient solution to large scale datasets with flat schemas, such as the schema of Freebase. This is because these methods relied only on the database internal statistics to generate query construction options. In large scale databases, such options are not informative enough to efficiently reduce the search space. FreeQ alleviates this problem by using ontologies, such as the do-main hierarchy of Freebase, and the YAGO ontology [21], in the option generation process. Furthermore, previous work on incre-mental query construction relied on a set of query templates gen-erated a-priori. In contrast, FreeQ presents algorithms to generate structured queries and query construction options on the fly over a large scale database.

Database usability is a long-term research issue [13]. One of the early approaches to address this problem was the Query by Exam-ple [25]. Recent approaches include NL query interfaces [3], [5], query auto-completion, e.g. [19], and adaptive forms [14]. Some commercial DB products such as Microsoft Access offer visual query builder interfaces. Query graphs in a typical visual query builder interface have to be created starting from scratch. The user has to study the database schema and manually put together pieces of the query graph. In contrast, FreeQ enables users to focus on in-terpretations of keyword queries, and automatically suggests struc-tured queries, without requiring any prior schema knowledge.
User-driven query disambiguation has been successfully applied in Information Retrieval in the context of faceted search. Faceted search engines, such as the product search engine of Google and the Yippy 7 , organize search results into meaningful groups, called facets, by applying some clustering or categorization algorithms. Users can easily shrink the scope of the search by focusing on a small number of facets. Several navigational techniques, e.g. [4, 24] were proposed to support users in finding information in a hi-erarchy of faceted categories. The interface of FreeQ is similar to a faceted interface, whereas each facet corresponds to a query con-struction option.
In this paper, we considered the problem of scaling interactive query construction on a very large dataset, such as Freebase. To achieve this goal, we analyzed the efficiency of query construction options and extended the database schema with an ontology layer to reduce the interaction cost. Furthermore, we developed a set of algorithms to explore the interpretation spaces of keyword queries incrementally, such that top-k query interpretations and query con-struction options can be generated efficiently. Our results confirm the efficiency of the proposed algorithms and the effectiveness of the ontology layer created using the native taxonomy of Freebase. Furthermore, we have demonstrated that external ontologies, such as YAGO ontology, can be used to further increase the effective-ness of query construction. This is especially meaningful for the portability of the FreeQ system, as it can be applied to databases without any pre-defined ontologies. h ttp://search.yippy.com/
This work is partly funded by the NSFC, Project No. 61272138, the European Commission, grant agreement 270239 (ARCOMEM), BMBF ("Gute Arbeit", Re-SozIT) and KEYSTONE COST Action.
