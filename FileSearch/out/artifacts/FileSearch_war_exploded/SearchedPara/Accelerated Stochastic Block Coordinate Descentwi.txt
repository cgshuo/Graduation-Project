 We study the composite minimization problem where the objective function is the sum of two convex functions: one is the sum of a fi-nite number of strongly convex and smooth functions, and the other is a general convex function that is non-differentiable. Specifically, we consider the case where the non-differentiable function is block separable and admits a simple proximal mapping for each block. This type of composite optimization is common in many data min-ing and machine learning problems, and can be solved by block co-ordinate descent algorithms. We propose an accelerated stochastic block coordinate descent (ASBCD) algorithm, which incorporates the incrementally averaged partial derivative into the stochastic par-tial derivative and exploits optimal sampling. We prove that AS-BCD attains a linear rate of convergence. In contrast to uniform sampling, we reveal that the optimal non-uniform sampling can be employed to achieve a lower iteration complexity. Experimental results on different large-scale real data sets support our theory.  X  Information systems  X  Data mining;  X  Computing methodo-logies  X  Machine learning; Stochastic block coordinate descent; Sampling
We consider the problem of minimizing a composite function, which is the sum of two convex functions: where F ( w ) = n  X  1 P n i =1 f i ( w ) is a sum of a finite number of strongly convex and smooth functions, and R ( w ) is a block sep-arable non-differential function. To explain block separability, let {G 1 ,..., G k } be a partition of all the d coordinates where G a block of coordinates. A subvector w G j is [ w k 1 ,...,w where G j = { k 1 ,...,k |G j | } and 1  X  j  X  m . The fact that R ( w ) is block separable is equivalent to
The above problem is common in data mining and machine learning, such as the regularized empirical risk minimization, where F ( w ) is the empirical loss function averaged over the train-ing data sets, and R ( w ) is a regularization term. For example, suppose that for a data mining problem there are n instances in a training data set { ( x 1 ,y 1 ) , ( x 2 ,y 2 ) ,..., ( x the squared loss f i ( w ) = (  X  w , x i  X  X  X  y i ) 2 / 2 and R ( w ) = 0 , a least square regression is obtained. If R ( w ) is chosen to be the sum of the absolute value of each coordinate in w , it becomes a lasso regression [46]. In general, the problem in (1.1) can be ap-proximately solved by proximal gradient descent algorithms [32] and proximal coordinate descent algorithms [23].

Coordinate descent algorithms have received increasing atten-tion in the past decade in data mining and machine learning due to their successful applications in high dimensional problems with structural regularizers [12, 11, 28, 2, 47]. Randomized block co-ordinate descent (RBCD) [31, 36, 26, 39, 4, 14, 21] is a special block coordinate descent algorithm. At each iteration, it updates a block of coordinates in vector w based on evaluation of a random feature subset from the entire training data instances. The iteration complexity of RBCD was established and extended to composite minimization problems [31, 36, 26]. RBCD can choose a con-stant step size and converge at the same rate as gradient descent algorithms [31, 36, 26]. Compared with gradient descent, the per-iteration time complexity of RBCD is much lower. This is because RBCD computes a partial derivative restricted to only a single co-ordinate block at each iteration and updates just a single coordinate block of vector w . However, it is still computationally expensive because at each iteration it requires evaluation of the gradient for all the n component functions f i : the per-iteration computational complexity scales linearly with the training data set size n .
In view of this, stochastic block coordinate descent was proposed recently [8, 51, 48, 35]. Such algorithms compute the stochastic partial derivative restricted to one coordinate block with respect to one component function, rather than the full partial derivative with respect to all the component functions. Essentially, these al-gorithms employ sampling of both features and data instances at each iteration. However, they can only achieve a sublinear rate of convergence.

We propose an algorithm for stochastic block coordinate descent using optimal sampling, namely accelerated stochastic block co-ordinate descent with optimal sampling (ASBCD). On one hand, ASBCD employs a simple gradient update with optimal non-Algorithm 1 ASBCD: Accelerated Stochastic Block Coordinate Descent with Optimal Sampling 1: Inputs: step size  X  and sampling probability set P = 2: Initialize:  X  (0) i = w (0)  X  R d 3: for t = 1 , 2 ,... do 4: Sample a component function index i from { 1 ,...,n } at 6: Sample a coordinate block index j from { 1 ,...,m } uni-9: end for uniform sampling, which is in sharp contrast to the aforementioned stochastic block coordinate descent algorithms based on uniform sampling. On the other hand, we incorporate the incrementally averaged partial derivative into the stochastic partial derivative to achieve a linear rate of convergence rather than a sublinear rate.
To be specific, given error and number of coordinate blocks m , for strongly convex f i ( w ) with the convexity parameter  X  and the Lipschitz continuous gradient constant L i ( L M = max iteration complexity of ASBCD is Notation. Here we define and describe the notation used through this paper. Let w k be the k th element of a vector w = [ w 1 ,...,w d ] &gt;  X  R d . We use k w k = k w k 2 = P d k =1 denote the ` 2 norm of a vector w and k w k 1 = P d k =1 | w subvector of w excluding w G j is denoted by w \G j . The simple proximal mapping for each coordinate block, also known as the proximal operator, is defined as
We propose ASBCD (Algorithm 1), an accelerated algorithm for stochastic block coordinate descent with optimal sampling. It starts with known initial vectors  X  (0) i = w (0)  X  R d for all i .
In sharp contrast to stochastic block coordinate descent with uni-form sampling, ASBCD selects a component function according to non-uniform probabilities (Line 4 of Algorithm 1).

In Algorithm 1, we define the gradient of any function f (  X  ) with respect to a coordinate block G j of  X  as  X  G j [  X  f (  X  )] G { k
Algorithm 1 has a lower computational cost than either proximal gradient descent or RBCD at each iteration. The update at each iteration of Algorithm 1 is restricted to only a sampled component function (Line 4) and a sampled block of coordinates (Line 6).
The key updating step (Line 7) with respect to a stochastic block of coordinates incorporates the incrementally averaged partial derivative into the stochastic partial derivative with the At each iteration with i and j sampled, this summation  X 
R EMARK 2.1. For many empirical risk minimization problems with each training data instance ( x i ,y i ) and a loss function ` , the gradient of f i ( w ) with respect to w is a multiple of x ` memory by only saving scalars ` 0 (  X   X  i , x i  X  ,y i ) with the same space cost as those of many other related algorithms MRBCD, SVRG, SAGA, SDCA, and SAG described in Section 5.

R EMARK 2.2. The sampling probability of component func-tions f i in Line 4 of Algorithm 1 is according to a given prob-ability set P = { p 1 ,...,p n } . The uniform sampling scheme em-ployed by stochastic block coordinate descent methods fits under this more generalized sampling framework as a special case, where p = 1 /n . We reveal that the optimal non-uniform sampling can be employed to lower the iteration complexity in Section 3.
When taking the expectation of the squared gap between the it-erate w ( t ) and the optimal solution w  X  in (1.1) with respect to the stochastic coordinate block index, the obtained upper bound does not depend on such an index or the proximal operator. This property may lead to additional algorithmic development and here it is important for deriving a linear rate of convergence for Al-gorithm 1. We prove the rate of convergence bound in Appendix A after presenting and discussing the main theory in Section 3.
In this section, we present and discuss the main theory of our proposed algorithm (Algorithm 1). The proof of the main theory is presented in the appendix.

We begin with the following assumptions on F ( w ) and R ( w ) in the composite objective optimization problem as characterized in (1.1). These assumptions are mild and can be verified in many regularized empirical risk minimization problems in data mining and machine learning.
 Each gradient  X  f i ( w ) is Lipschitz continuous with the constant L , i.e. , for all w  X  R d and u  X  R d we have f ( w ) is strongly convex, i.e. , there exists a positive constant  X  such that for all w  X  R d and u  X  R d we have
Assumption 3.2 implies that F ( w ) is also strongly convex, i.e. , there exists a positive constant  X  such that for all w  X  R u  X  R d we have ation function R ( w ) is convex but non-differentiable, and a closed-form solution can be obtained for the proximal operator defined in (1.3) . Importantly, R ( w ) is block separable as defined in (1.2) .
With the above assumptions being made, now we establish the linear rate of convergence for Algorithm 1, which is stated in the following theorem.
T HEOREM 3.4. Let L M = max that Assumptions 3.1 X 3.3 hold. Based on Algorithm 1 and with w  X  defined in (1.1) , by setting  X  = max np I / ( L M  X  )  X  1 &gt; 0 ,  X  = L 2 M m/ [2 n X  ( L M  X   X  + L and 0 &lt;  X  = 1  X   X  X /m &lt; 1 , it holds that
R EMARK 3.5. Theorem 3.4 justifies the linear rate of conver-gence for Algorithm 1. Parameter  X  depends on the number of coordinate blocks m . It may be tempting to set m = 1 for faster convergence. However, this is improper due to lack of considera-tions for the computational cost at each iteration. When m = 1 , at each iteration the gradient is updated with respect to all coordin-ates. When m &gt; 1 , at each iteration of Algorithm 1 the gradient is updated with respect to only a sampled coordinate block among all coordinates, so the computational cost is lower than that of m = 1 per iteration. Therefore, comparing algorithms that up-date the gradient with respect to different numbers of coordinates per iteration should be based on the same number of entire data passes (the least possible iterations for passing through the entire data instances with respect to all coordinates). We perform experi-ments to compare such different algorithms in Section 4.
R EMARK 3.6. Theorem 3.4 implies a more generalized itera-tion complexity of Algorithm 1, which is given the error &gt; 0 . The uniform sampling scheme fits this more generalized result with p i = 1 /n . With L M = max p = 1 /n ,  X  = 1 / [2( L M + n X  )] &gt; 0 ,  X  = ( L M + 2 n X  ) /L 0 ,  X  = m/ [2 n X  (1  X   X  X  )] &gt; 0 , and 0 &lt;  X  = 1  X   X / [2 m ( L n X  )] &lt; 1 , Theorem 3.4 still holds. The iteration complexity of ASBCD with uniform sampling is
Now we show that the iteration complexity in (3.2) can be further improved by optimal sampling. To begin with, minimizing  X  can be achieved by maximizing  X  with respect to p i . It is easy to show that  X  is maximized when p i = ( n + L i / X  ) / P n k =1 ( n + L by setting  X  = n/ 2 P n i =1 ( n X  + L i ) &gt; 0 we obtain the iteration complexity of ASBCD with optimal sampling:
C OROLLARY 3.7. Let L M = max tions 3.1 X 3.3 hold. Based on Algorithm 1 and with w  X  defined in (1.1) , by setting p i = ( n + L i / X  ) / P n k =1 ( n + L P 1  X  n X / [2 m P n i =1 ( n X  + L i )] &lt; 1 , we chose  X  = n/ [2 P L )] &gt; 0 and it holds that
Comparing the iteration complexity of ASBCD in (3.3) and (3.2), it is clear that the optimal sampling scheme results in a lower iteration complexity than uniform sampling.
We conduct experiments to evaluate the performance of our pro-posed ASBCD algorithm in comparison with different algorithms on large-scale real data sets.
We define the problems and measures used in the empirical eval-uation. Classification and regression are two corner-stone data min-ing and machine learning problems. We evaluate the performance of the proposed ASBCD algorithm in solving these two problems.
As a case study, the classification problem is ` 1 , 2 logistic regression:
For the the regression problem in this empirical study, the elastic net is used: w  X  = argmin
The regularization parameters  X  1 and  X  2 in both problems are tuned by proximal gradient descent using five-fold cross-validation on the training data sets.
Recall the problem of composite function minimization as form-alized in (1.1). In evaluation of the algorithm performance on the convergence effect, we use the measure of objective gap value: P ( w )  X  P ( w  X  ) .

To further study model prediction capabilities that are trained by different algorithms, we evaluate testing accuracy using two differ-ent measures:  X  AUC : For the classification problem, area under receiver oper-ating characteristic curve (AUC) is measured [13]. Note that a higher testing accuracy can be reflected by a higher AUC.  X  MSE : For the regression problem, mean squared error (MSE) is compared. Note that a higher testing accuracy can be reflected by a lower MSE.
The empirical studies are conducted on the following four real data sets that are downloaded using the LIBSVM software [3]:  X  KDD 2010 : Bridge to Algebra data set from KDD Cup 2010
Educational Data Mining Challenge [44].  X  COVTYPE : Data set for predicting forest cover type from car-tographic variables [22].  X  RCV1 : Reuters Corpus Volume I data set for text categorization research [20].  X  E2006-TFIDF : Data set for predicting risk from financial re-ports from thousands of publicly traded U.S. companies [16].
Each of these real data sets has a large size in either its instance count or feature size, or both. For instance, the KDD 2010 data set has over 19 million training instances with nearly 30 million fea-tures. Summary statistics of these data sets are provided in Table 1.
We evaluate the performance of ASBCD in comparison with re-cently proposed competitive algorithms. To comprehensively eval-uate ASBCD, we also compare variants of ASBCD with different sampling schemes.

Below are the seven algorithms for comparison.  X  SGD (SG) : Proximal stochastic gradient descent. This algorithm has a sublinear rate of convergence. To ensure the high competit-iveness of this algorithm, the implementation is based on a recent work [1].  X  SBCD (SB) : Stochastic block coordinate descent. It is the same as SGD except that SBCD updates the gradient with respect to a randomly sampled block of coordinates at each iteration. SBCD also converges at a sublinear rate.  X  SAGA (SA) : Advanced stochastic gradient method [9]. This al-gorithm is based on uniform sampling of component functions.
It updates the gradient with respect to all coordinates at each it-eration. SAGA has a linear rate of convergence.  X  SVRG (SV) : (Proximal) stochastic variance reduced gradi-ent [15, 50]. This algorithm is based on uniform sampling of component functions. It updates the gradient with respect to all coordinates at each iteration. Likewise, SVRG converges to the optimum at a linear rate.  X  MRBCD (MR) : Mini-batch randomized block coordinate des-cent [54]. This algorithm uses uniform sampling of component functions. MRBCD converges linearly to the optimum.  X  ASBCD-U (U) : The proposed ASBCD algorithm with uniform sampling of component functions. The sampling probability p for component function f i is p i = 1 /n . The sampling probabil-ity p i for component function f i : p i = L i / P n k =1  X  ASBCD-O (O) : The proposed ASBCD algorithm with optimal sampling as described in Corollary 3.7. The sampling probabil-ity p i for component function f i is p i = ( n + L i / X  ) / P
L k / X  ) .
Note that algorithms SBCD, MRBCD, and ASBCD update the gradient with respect to a sampled block of coordinates at each it-eration. In contrast, SGD, SAGA, and SVRG update the gradi-ent with respect to all the coordinates per iteration. Recalling Re-mark 3.5, comparison of these algorithms is based on the same en-tire data passes.
We evaluate convergence and testing accuracy with respect to training time. The experiments on the KDD 2010 data set are con-ducted on a computer with two 14-core 2.4GHz CPUs and a 256GB RAM while the experiments on the other data sets are conducted on a computer with an 8-core 3.4GHz CPU and a 32GB RAM.
Different from the other algorithms in comparison, the SVRG and MRBCD algorithms both have multiple stages with two nes-ted loops. The inner-loop counts in SVRG and MRBCD are set to the training data instance counts as suggested in a few recent studies [15, 50, 54].

For each algorithm, its parameters, such as the step size (  X  in this paper), are chosen around the theoretical values to give the fastest convergence under the five-fold cross validation. Here we describe the details. The training data set is divided into five subsets of approximately the same size. One validation takes five trials remaining four subsets are used. The convergence effect in one cross-validation is estimated by the averaged performance of the five trials. All the experimental results are obtained from 10 replications. Both the mean and standard deviation values are reported in Tables 2 X 6. For clarity of exposition, Figures 1 X 4 plot the mean values of the results from all these replications.
For classification on KDD 2010, Figure 1 compares convergence of all the algorithms for the same number of entire data passes. In general, among all the seven algorithms in comparison, AS-BCD with optimal sampling converges fastest to the optimum for the same number of entire data passes. Sublinearly-convergent al-gorithms SGD and SBCD converge much more slowly than the other linearly-convergent algorithms.

We also observe from Figure 1 that, stochastic block coordinate descent algorithms generally converge faster than those algorithms without using stochastic block coordinates. For instance, SBCD converges faster than SGD while MRBCD converges faster than SVRG for the same number of entire data passes.

The varied convergence effects across all the seven algorithms can be visualized more clearly when they are compared for the same training time. Figure 2 exhibits such performance variations. Clearly, ASBCD with optimal sampling achieves the fastest con-vergence for the same training time. Similar to the results in Fig-ure 1, for the same training time, stochastic block coordinate des-cent algorithms still generally converge faster than those algorithms without using stochastic block coordinates.

It is not surprising that the convergence effects influence the test-ing accuracy. Tables 2 and 3 report the AUC comparison of al-gorithms for the same entire data passes and training time. Consist-Figure 1: Classification on KDD 2010: Convergence compar-ison of algorithms for the same number of entire data passes. In general, ASBCD with optimal sampling (O) converges fast-est to the optimum for the same number of entire data passes. Figure 2: Classification on KDD 2010: Convergence compar-ison of algorithms for the same training time. In general, AS-BCD with optimal sampling (O) converges fastest to the op-timum for the same training time. ent with the observed convergence performance in Figures 1 and 2, ASBCD with optimal sampling generally achieves the highest test-ing accuracy for both the same number of entire data passes and the same training time. We further compare the algorithms on three more data sets COV-TYPE, RCV1, and E2006-TFIDF as described in Section 4.2 and summarized in Table 1. COVTYPE and RCV1 are used for the classification problem, while E2006-TFIDF is for the regression problem. All the results are reported in Tables 4 X 6, Figure 3, and Figure 4. To begin with, we describe the convergence effects. Fig-ures 3 and 4 compare convergence of algorithms for the same entire data passes and for the same training time. In general, ASBCD with optimal sampling (O) converges fastest to the optimum for both the same number of entire data passes and the same training time.
Tables 4 X 6 present the testing accuracy comparison results for the same training time. Note that a lower MSE for regression on E2006-TFIDF indicates a higher testing accuracy. These testing accuracy results agree with the varied convergence effects of dif-ferent algorithms on the same data set. Among all the algorithms, ASBCD with optimal sampling generally achieves the highest test-ing accuracy for the same training time.
The first line of research in modern optimization is randomized block coordinate descent (RBCD) algorithms [11, 49, 25, 39, 36]. These algorithms exploit the block separability of regularization function R ( w ) . With separable coordinate blocks, such algorithms only compute the gradient of F ( w ) with respect to a randomly selected block at each iteration rather than the full gradient with respect to all coordinates: they are faster than the full gradient des-cent at each iteration [11, 49, 25, 39, 36]. However, such algorithms still compute the exact partial gradient based on all the n compon-ent functions per iteration, though accessing the entire component functions is computationally more expensive when the training data set has a larger number of instances [52].

Recently, an MRBCD algorithm was proposed for randomized block coordinate descent using mini-batches [54]. At each itera-tion, both a block of coordinates and a mini-batch of component functions are sampled but there are multiple stages with two nes-ted loops. For each iteration of the outer loop, the exact gradient is computed once; while in the follow-up inner loop, gradient es-timation is computed multiple times to help adjust the exact gradi-ent. MRBCD has a linear rate of convergence for strongly con-vex and smooth F ( w ) only when the batch size is  X  X arge enough X  although batches of larger sizes increase the per-iteration compu-tational cost [54] (Theorem 4.2). Similar algorithms and theor-etical results to those of MRBCD were also proposed [48, 18]. Chen and Gu further considered related but different sparsity con-strained non-convex problems and studied stochastic optimization algorithms with block coordinate gradient descent [6].

Our work departs from the related work in the above line of re-search by attaining a linear convergence using optimally and non-uniformly sampling of a single data instance at each of iterations.
The second line of research in modern optimization is proximal gradient descent. In each iteration, a proximal operator is used in the update, which can be viewed as a special case of splitting al-gorithms [24, 5, 35]. Proximal gradient descent is computationally expensive at each iteration, hence proximal stochastic gradient des-cent is often used when the data set is large. At each iteration, only one of the n component functions f i is sampled, or a subset of f are sampled, which is also known as mini-batch proximal stochastic gradient [43]. Advantages for proximal stochastic gradient descent are obvious: at each iteration much less computation of the gradi-ent is needed in comparison with proximal gradient descent. How-ever, due to the variance in estimating the gradient by stochastic sampling, proximal stochastic gradient descent has a sublinear rate of convergence even when P ( w ) is strongly convex and smooth.
To accelerate proximal stochastic gradient descent, variance re-duction methods were proposed recently. Such accelerated al-gorithms include stochastic average gradient (SAG) [38], stochastic dual coordinate ascent (SDCA) [42], stochastic variance re-duced gradient (SVRG) [15], semi-stochastic gradient descent (S2GD) [19], permutable incremental gradient (Finito) [10], min-imization by incremental surrogate optimization (MISO) [27], and advanced stochastic gradient method (SAGA) [9]. There are also some more recent extensions in this line of research, such as proximal SDCA (ProxSDCA) [40], accelerated mini-batch SDCA (ASDCA) [41], adaptive variant of SDCA (AdaSDCA) [7], ran-domized dual coordinate ascent (Quartz) [34], mini-batch S2GD (mS2GD) [17], and proximal SVRG (ProxSVRG) [50].

Besides, several studies show that non-uniform sampling can be used to improve the rate of convergence of stochastic optimization denote the highest AUC among all the algorithms for the same training time. algorithms [45, 31, 29, 50, 34, 53, 37, 33]. However, the proposed sampling schemes in these studies cannot be directly applied to our algorithm, because they are limited in at least one of the following two aspects: (1) the algorithm does not apply to composite ob-jectives with a non-differentiable function; (2) it does not support randomized block coordinate descent. Research on big data is increasingly important and common. Training data mining and machine learning models often involve minimizing empirical risk or maximizing likelihood over the train-ing data set, especially in solving classification and regression problems. Thus, big data research may rely on optimization al-gorithms, such as proximal gradient descent algorithms. At each iteration, proximal gradient descent algorithms have a much higher computational cost due to updating gradients based on all the data instances and features. Randomized block coordinate descent al-gorithms are still computationally expensive at each iteration when the data instance size is large. Therefore, we focused on stochastic block coordinate descent that samples both data instances and fea-tures at every iteration.

We proposed the ASBCD algorithm to accelerate stochastic block coordinate descent. ASBCD incorporates the incrementally averaged partial derivative into the stochastic partial derivative. For smooth and strongly convex functions with non-differentiable reg-ularization functions, ASBCD is able to achieve a linear rate of convergence. The optimal sampling achieves a lower iteration com-plexity for ASBCD. The empirical evaluation with both classifica-tion and regression problems on four large-scale real data sets sup-ported our theory.
 Acknowledgement. We would like to thank the anonymous re-viewers for their helpful comments. This research was partially supported by Quanquan Gu X  X  startup funding at Department of Sys-tems and Information Engineering, University of Virginia. [1] L. Bottou. Stochastic gradient descent tricks. In Neural [2] P. Breheny and J. Huang. Coordinate descent algorithms for [3] C.-C. Chang and C.-J. Lin. Libsvm: a library for support [4] K.-W. Chang, C.-J. Hsieh, and C.-J. Lin. Coordinate descent [5] G. H. Chen and R. Rockafellar. Convergence rates in [6] J. Chen and Q. Gu. Accelerated stochastic block coordinate [7] D. Csiba, Z. Qu, and P. Richt X rik. Stochastic dual coordinate [8] C. D. Dang and G. Lan. Stochastic block mirror descent [9] A. Defazio, F. Bach, and S. Lacoste-Julien. Saga: A fast [10] A. J. Defazio, T. S. Caetano, and J. Domke. Finito: A faster, denote the highest AUC among all the algorithms for the same training time. denote the highest AUC among all the algorithms for the same training time. denote the lowest MSE among all the algorithms for the same training time. [11] J. Friedman, T. Hastie, H. H X fling, R. Tibshirani, et al. [12] W. J. Fu. Penalized regressions: the bridge versus the lasso. [13] J. A. Hanley and B. J. McNeil. The meaning and use of the [14] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and [15] R. Johnson and T. Zhang. Accelerating stochastic gradient [16] S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A. [17] J. Kone  X  cn ` y, J. Liu, P. Richt X rik, and M. Tak X   X  c. ms2gd: [18] J. Kone  X  cn ` y, Z. Qu, and P. Richt X rik. Semi-stochastic [19] J. Kone  X  cn ` y and P. Richt X rik. Semi-stochastic gradient [20] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new [21] Y. Li and S. Osher. Coordinate descent optimization for  X  [22] M. Lichman. UCI machine learning repository. [23] Q. Lin, Z. Lu, and L. Xiao. An accelerated proximal [24] P.-L. Lions and B. Mercier. Splitting algorithms for the sum [25] H. Liu, M. Palatucci, and J. Zhang. Blockwise coordinate [26] Z. Lu and L. Xiao. On the complexity analysis of [27] J. Mairal. Incremental majorization-minimization [28] R. Mazumder, J. H. Friedman, and T. Hastie. Sparsenet: [29] D. Needell, R. Ward, and N. Srebro. Stochastic gradient [30] Y. Nesterov. Introductory lectures on convex optimization: A [31] Y. Nesterov. Efficiency of coordinate descent methods on [32] Y. Nesterov et al. Gradient methods for minimizing [33] Z. Qu and P. Richt X rik. Coordinate descent with arbitrary [34] Z. Qu, P. Richt X rik, and T. Zhang. Randomized dual [35] S. Reddi, A. Hefny, C. Downey, A. Dubey, and S. Sra. [36] P. Richt X rik and M. Tak X   X  c. Iteration complexity of [37] M. Schmidt, R. Babanezhad, M. O. Ahmed, A. Defazio, [38] M. Schmidt, N. L. Roux, and F. Bach. Minimizing finite [39] S. Shalev-Shwartz and A. Tewari. Stochastic methods for l [40] S. Shalev-Shwartz and T. Zhang. Proximal stochastic dual [41] S. Shalev-Shwartz and T. Zhang. Accelerated mini-batch [42] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate [43] S. Sra, S. Nowozin, and S. J. Wright. Optimization for [44] J. Stamper, A. Niculescu-Mizil, S. Ritter, G. Gordon, and [45] T. Strohmer and R. Vershynin. A randomized kaczmarz [46] R. Tibshirani. Regression shrinkage and selection via the [47] R. Tibshirani, J. Bien, J. Friedman, T. Hastie, N. Simon, [48] H. Wang and A. Banerjee. Randomized block coordinate [49] T. T. Wu and K. Lange. Coordinate descent algorithms for [50] L. Xiao and T. Zhang. A proximal stochastic gradient [51] Y. Xu and W. Yin. Block stochastic gradient iteration for [52] A. Zhang, A. Goyal, R. Baeza-Yates, Y. Chang, J. Han, C. A. [53] P. Zhao and T. Zhang. Stochastic optimization with [54] T. Zhao, M. Yu, Y. Wang, R. Arora, and H. Liu. Accelerated We provide the proof for the main theory delivered in Section 3. Note that all the expectations are taken conditional on w each  X  ( t  X  1) i unless otherwise stated. For brevity, we define Let us introduce several important lemmas. To begin with, since Algorithm 1 leverages randomized coordinate blocks, the follow-ing lemma is needed for taking the expectation of the squared gap between the iterate w ( t ) and the optimal solution w  X  in (1.1) with respect to the coordinate block index j .

L EMMA A.1. Suppose that Assumption 3.3 holds. Let j be a coordinate block index. With g i defined in (A.1) and w  X  (1.1) , based on Algorithm 1 we have
Lemma A.1 takes the expectation of the squared gap between the iterate w ( t ) and the optimal solution w  X  in (1.1) with respect to the randomized coordinate block index. The obtained upper bound does not have a randomized coordinate block index or the proximal operator. Block separability and non-expansiveness of the prox-imal operator are both exploited in deriving the upper bound. This upper bound is used for deriving a linear rate of convergence for Algorithm 1.

L EMMA A.2. Based on Algorithm 1 and as defined in (A.1) , we Lemma A.2 guarantees that g i is an unbiased gradient estimator of F ( w ) . The proof is strictly based on the definition of g
L EMMA A.3. With g i defined in (A.1) and w  X  defined in (1.1) , based on Algorithm 1 and for all  X  &gt; 0 we have
Lemma A.3 makes use of the property that E [ k x k 2 ] = E E [ x ] k 2 ] + k E [ x ] k 2 for all x and the property that k x + y k (1 +  X  ) k x k 2 + (1 +  X   X  1 ) k y k 2 for all x , y , and  X  &gt; 0 .
L EMMA A.4. Let f be strongly convex with the convexity para-meter  X  and its gradient be Lipschitz continuous with the constant L . For all x and y , it holds that
Lemma A.4 leverages properties of strongly convex functions with Lipschitz continuous gradient. L EMMA A.5. Algorithm 1 implies that
Lemma A.5 is obtained according to the non-uniform sampling of component functions in Algorithm 1.
 R EMARK A.6. Similar to Lemma A.5, we have
Now we develop the main theorem of bounding the rate of con-vergence for Algorithm 1.
 P ROOF OF T HEOREM 3.4. By applying Lemma A.1, A.2, and Lemma A.3,
Substituting x , y , and f with w  X  , w ( t  X  1) , and f i and taking average on both sides of the inequality in Lemma A.4, we obtain Recall the property of any function f that is convex and has a Lipschitz continuous gradient with the constant L : f ( y )  X  f ( x )+ (Theorem 2.1.5). Taking average on both sides, we have after substituting y , x , and f with  X  ( t  X  1) i , w  X  arranging terms.
 Before further proceeding with the proof, we define Following the definition in (A.6), for all  X  &gt; 0 ,
Recall the property of any strongly convex function f with the convexity parameter  X  that f ( y )  X  f ( x ) +  X  X  X  f ( x ) , y  X  x  X  + k X  f ( x )  X   X  f ( y ) k 2 / (2  X  ) for all x and y [30] (The-orem 2.1.10). We can obtain  X  X  X  f i ( w ( t  X  1) )  X  X  X  f i  X  2  X  f i ( w ( t  X  1) )  X  f i ( w  X  )  X  X  X  X  f i ( w  X  ) , w
Combining (A.3) with a positive constant  X  , (A.4), and (A.5), after simplifying terms, by Lemma A.5 and (A.2), with defining L M = max i L i and p I = min i p i we have where the four constant factors are and the four corresponding terms are
There are four constant factors associated with four terms on the right-hand side of (A.7). Among the four terms, obviously T and T 3  X  0 . By the convexity property of f i , we have T and T 4  X  0 . We choose  X  = max ting c 1 = 0 with  X  = np I / ( L M  X  )  X  1 &gt; 0 , c 2 = 0 with  X  = L 2 M m/ 2 n X  ( L M  X   X  + L M  X  X  X  ) &gt; 0 , and c 3 = 0 with 0 &lt;  X  = 1  X   X  X /m &lt; 1 , it can be verified that c 4  X  0 .
With the aforementioned constant factor setting, E i,j  X H ( t  X  1)  X  0 , where the expectation is conditional on information from the previous iteration t  X  1 . Taking expectation with this previous iteration gives E i,j [ H ( t ) ]  X   X  E i,j [ H ( t  X  1) three terms in (A.6) is non-negative by the convexity of F , we have  X  k w ( t )  X  w  X  k 2  X  H ( t ) . Together with the aforementioned results by chaining over t , the proof is complete.
