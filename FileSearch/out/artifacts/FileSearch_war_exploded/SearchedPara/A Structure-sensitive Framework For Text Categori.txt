 This paper presents a framework called Structure Sensitive CATegorization(SSCAT), that exploits document structure for improved categorization. There are two parts to this framework, viz. (1) Documents often have layout structure, such that logically coherent text is grouped together into fields using some mark-up language. We use a log-linear model, which associates one or more features with each field. Weights associated with the field features are learnt from training data and these weights quantify the per X  X lass im-portance of the field features in determining the category for the document. (2) We employ a technique that ex-ploits the parse tree of fields that are phrasal constructs, such as title and associates weights with words in these con-structs while boosting weights of important words called fo-cus words . These weights are learnt from example instances of phrasal constructs, marked with the corresponding focus words . The learning is accomplished by training a classifier that uses linguistic features obtained from the text X  X  parse structure. The weighted words, in fields with phrasal con-structs, are used in obtaining features for the corresponding fields in the overall framework. SSCAT was tested on the supervised categorization task of over one million products from Yahoo! X  X  on-line shopping data. With an accuracy of over 90%, our classifier outperforms Naive Bayes and Sup-port Vector Machines. This not only shows the effectiveness of SSCAT but also strengthens our belief that linguistic fea-tures based on natural language structure can improve tasks such as text categorization.
 Categories and Subject Descriptors: H.3.0 Information Storage and Retrieval: Indexing.
 General Terms: Algorithms, Experimentation.
 Keywords: Text Categorization, Log-linear Model, Nat-ural Language Processing.  X 
Work done while interning at Yahoo!, Bangalore.  X 
Contact Author.Work done while interning at Yahoo!, Ban-galore.

We present categorization experiments on the products listed in Yahoo! X  X  online shopping data. The data for our experiments came from more than 1 million Yahoo! prod-ucts categorized into 67 categories [2]. We report average accuracies, precision and recall along with standard devia-tions for each experiment, over 10 random test X  X rain splits.
In this section, we present the results of experiments in-volving the product name learner and the brand name learner and compare our method against the dictionary lookup based baseline method [2]. Though we engineer the structure-sensitive features for title using LR 2 [3], we also report ac-curacies with J48 [3], to assess the suitability of the LR for this task. For product name detection, we compare the per-formance of LR and J48 against CRF [1].
We observe that J48 performs better than LR. When to-kens are used as instances [2], CRFs do not perform as good as J48 and LR. Also, CRFs are not adept to learn with phrases as instances, since the sequence information is lost when phrases are used as instances. The results, reported over a 10-fold cross validation run, are shown in Figure 1. Figure 1: Comparison of different classifiers and differ-
The results obtained for brand name detection, reported over a 10-fold cross validation run, are shown in Figure 2.
When we incorporate the product-name and brand-name detection into the complete categorization framework, we give weights/boost-factors to individual tokens in a title rather than sub-setting the tokens by applying a threshold to the classifier output.

The SSCAT categorizer was tried with different lone fea-tures and the corresponding accuracies, micro-averaged pre-
Abbreviations: LR for Logistic Regressor, J48 for the deci-sion tree, CRF for Conditional Random Fields, PName for the product name and BName for the Brand Name
