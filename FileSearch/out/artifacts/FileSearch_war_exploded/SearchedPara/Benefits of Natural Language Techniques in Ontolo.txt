 Developing ontologies manually is a complex and time consuming process, which involves both ontology engineers and domain experts. Natural language (NL) t echniques have been traditionally used for extracting knowledge from texts to build semantic resources. In fact knowledge acquisition from text plays an i m-portant role in Ontology Engineering. It is divi d-ed into several steps, according to the  X  X ntology le arning layer -cake X  (Cimiano, 2006): (a) ident i-fying and extracting terms, (b) eliciting concepts and relations linking concepts from these terms, (c) organizing concepts and relations into hiera r-chies, and (d) identifying axioms. 
During the ontology building, a wide range of difficulties and handicaps can appear. These situations may have as consequence the inclusion of anomalies in the ontology. Thus, the ontology evaluation process plays a key role in ontology engineering develop ments. Currently, the general trends in ontology evaluation involve different approaches (e.g., the comparison of the ontology to a  X  X old standard X  or the detection of common errors in the ontology). However, what seems to be less present in the ontology e valuation field is the intensive use of NL techniques. For example, some structural or naming errors in the ontology may be automatically pointed out with a lingui s-tic analysis of concept labels. Thus, our intention in this paper is to aim at the use of NL tec h-niques during the ontology evaluation process. In particular, we propose a first attempt of impro v-ing the pitfall detection methods implemented within OOPS! by means of NL techniques.

The remainder of this paper is structured as follows: Section 2 summarizes different NL techniques used in Ontology Engineering. Se c-tion 3 presents the relation between ontology evaluation and NL -based tec hniques. Section 4 briefly describes OOPS!. In Section 5 our pr o-posal towards a language -based enhancement of the pitfall detection process within OOPS! is presented. Finally, Section 6 outlines some co n-clusions and future steps. NL techniques traditionally help on the (semi) -automatic building of ontologies and on the po p-ulation of ontologies with instances.

Most of the approaches for building ontologies from text, known as ontology learning me thods, usually implement lexico -syntactic patterns (Hearts, 1992; Montiel -Ponsoda and Aguado de Cea, 2010), clustering methods or machine lear n-ing algorithms (essentially unsupervised) (Poelmans et al., 2010), to exploit various li n-guistic clues. Some plat forms exist and impl e-ment one or a combination of these methods using different NLP tools (term or relation e x-tractors, parsers, etc.). Examples are Text2Onto (Cimiano and V X lker, 2005), which discovers concepts and hyperonimic relations between concepts, thanks to lexico -syntactic patterns and associative rules automatically learned from examples and OntoLearn (Velardi et al., 2005), which uses Wordnet (Fellbaum, 1998) for ident i-fying lexical relations .

Regarding the population of ontologies, tools like T EXCOMON (Zouaq and Nkambou, 2008) uses linguistic patterns for instance identific a-tion, using named entity recognition techniques. 
Linguistic approaches have been also applied to ontology matching where Euzenat and Shva i-ko (2007) distinguish between language -based methods and methods which are based on li n-guistic resources, whereas the more general class of terminological approaches also includes string -based methods. We can mention the work by Ritze et. al (2010) that show s how complex matching can benefit from NL techniques. Ontology evaluation process, which checks the technical quality of an ontology against a frame of reference (Su X rez -Figueroa, 2010), plays a key ro le in ontology engineering projects.

To help developers during the ontology eval u-ation process, there are different approaches (Sabou and Fernandez, 2012; Poveda -Villal X n et al., 2012): (a) comparison of the ontology to a  X  X old standard X , (b) detection of common errors from catalogues in the ontology, (c) use of d i-mensions and criteria for describing the quality and goodness of the ontology, (d) use of the o n-tology in an application and evaluation of the results, (e) comparison of the ontology with a source of data about the domain to be covered, and (f) evaluation by experts who check the o n-tology against the requirements.

In addition, ontology evaluation can be su p-ported by NL techniques in several ways (Ga n-gemi et. al, 2005): mation retrieval or text mining applications and thus concerns objects mentioned in texts. NLP can be used to identify mentions of instan c-es (i.e. occurrences in text) of classes and rel a-tions which a re mentioned in the text. A corpus -based evaluation of the ontology can reveal i m-portant properties of the ontology that might not be discovered otherwise. ontology is performed, NLP can help in the ide n-tification of new senses of already known i n-stances, for example because the instance is polysemous and/or ambiguous (e.g.,  X  X ashin g-ton X  is a person and a location).

However, ontology evaluation approaches could take more advantage of NL techniques. In this sense, we propose here a first attempt t o-wards a NL -based upgrade of OOPS!. OOPS! 1 (Poveda -Villal X n et al., 2012) is a web -based tool, independent of any ontology deve l-opment environment, for detecting potential pi t-falls that could lead to modelling errors. Currently, OOPS! provides mechanisms to a u-tomatically detect as many pitfalls as possible, thus it helps developers in the diagnosis activity, which is part of the ontology validation process.
OOPS! takes as input an ontology to be eval u-ated and a pitfall catalogue in order to produce a list of evaluation results. The current version of the catalogue 2 consists on 35 pitfalls. Some e x-amples are c reating synonyms as classes, defi n-ing wrong inverse relationships, miss ing annotations, missing domain or range in prope r-ties, or defining wrong equivalent classes. Up to now, OOPS! detects semi -automatically a subset of 21 pitfalls related to the following dimensions: human understanding, logical consistency, mo d-elling issue s, ontology language specification and real world representation. In this section we propose a first attempt towards a language -based enhancement of the pitfall d e-tection process within the ontology evaluation tool OOPS!. To do this, we have reviewed the current catalogue of pitfalls in order to determine (a) which pitfalls, already implemented, could be detected in a better way by means of applying linguistic techniques and (b) which one s, not detected yet by OOPS!, could be implemented based on linguistic aspects.

Regarding the proposals for enhancing pitfalls already detected by OOPS!, we can mention the following ones: classes whose identifie rs are synonyms are crea t-ed and defined as equivalent. Its detection could be improved by using linguistic resources such as WordNet and EuroWordNet, particularly by looking for the synonymy information of the class name. of using ''rdfs:subClassOf'', ''rdf:type'' or ''owl:sameAs' ': the  X  X s X  relationship is created in the ontology instead of using OWL primitives for representing the subclass relationship ( X  X u b-classOf X ), the membership to a class ( X  X nstanc e-Of X ), or the equality between instances ( X  X ameAs X ). The detection could be enriched by creating specific language -dependent lexico -syntactic patterns to discover the use of  X  X s X  and by using named entity recognition tools for cha r-acterizing the  X  X nstanceOf X  relatio n. two relationships are defined as inverse relations when they are not necessarily. As first attempt, the implementation of this pitfall could be i m-proved by creating specific lexico -syntactic pa t-terns for direct/ inverse relationship name structure. class : a class is created whose identifier is refe r-ring to two or more different concepts (e.g.,  X  X tyleAndPeriod X , or  X  X roductOrService X ). As first attempt, its detection coul d be enhanced by creating specific language -dependent lexico -syntactic patterns and regular expressions to discover the use of  X  X nd X  or  X  X r X  in the concept name. an ontology is imported into another, developers nor mally miss the definition of equivalent pro p-erties in those cases of duplicated relations and attributes (e.g.,  X  X asMember X  and  X  X as -Member X  in two different ontologies). The detection could be enriched by (a) using linguistic resources such as WordNet and EuroWordNet, specifically by looking for the synonymy information of the property name and (b) creating specific la n-guage -dependent lexico -syntactic patterns. fall appears when a relationship (except for the sym metric ones) has not an inverse relationship defined within the ontology. As first attempt, its implementation could be improved by creating specific lexico -syntactic patterns for d i-rect/inverse relationship name structure (e.g., isSoldIn -sells; hasAuthor -isAuthorOf; hasParent -isParentOf). in a hierarchy a class that contains the instances that do not belong to the sibling classes instead of classifying such instances as instances of the class in the upper level o f the hierarchy. This class is normally named  X  X ther X  or  X  X iscellan e-ous X . As first attempt, its detection could be i m-proved by creating a set of lexico -syntactic patterns that represent different ways of naming concepts that are usually miscellaneous entities . 
With respect to those pitfalls not detected yet by OOPS!, we can propose the following ideas for their implementation based on NL aspects : tology element whose name has different mea n-ings is included in the ontology to represent more than one conceptual idea. As first a p-proach, its detection could be implemented by (a) using linguistic resources such as WordNet and EuroWordNet, specifically by analyzing the di f-ferent synsets in which t he element name appears and (b) by analysing labels of neighbourhood concepts for disambiguation. that is required and/or useful is not included in the ontology. As first approach and in certain situations, this pitfall could be implemented by using linguistic resources such as WordNet and EuroWordNet, specifically by analyzing the a n-tonym information of the relationships name. ontology is imported into ano ther, classes with the same conceptual meaning that are duplicated in both ontologies should be defined as equiv a-lent classes. As first step, this pitfall could be detected by using linguistic resources such as WordNet and EuroWordnet, specifically by loo k-ing for the synonymy information of the class name. two classes are defined as equivalent when they are not necessarily. As first step, this pitfall could be implemented by using linguistic r e-sources such as WordNet and EuroWordNet, specifically by looking for the hyperonym i n-formation of the class name. In this paper, we have presented the first efforts towards a NL -based enhancement of the pitfall detection process within the ontology ev aluation tool OOPS!. We have reviewed the 35 pitfalls in the OOPS! catalogue and analyzed which pitfall detections could be linguistically improved and which pitfalls could be implemented based on NL as first attempt. In summary, we have pr o-posed the impro vement of 7 pitfall detection processes and the automation of 4 pitfalls not detected yet by OOPS!. Thus, we have planned to enhance OOPS! with the NL techniques pr e-sented in this paper. This work has been supported by the Spanish project B abelData ( TIN2010 -17550 ).

