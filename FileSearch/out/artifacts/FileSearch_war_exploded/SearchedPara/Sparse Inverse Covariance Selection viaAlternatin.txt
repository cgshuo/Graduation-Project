 In multivariate data analysis, graphical models such as Gaussian Markov Random Fields pro-an n -dimensional random vector following an n -variate Gaussian distribution N (  X , ) , and let G = ( V, E ) be a Markov network representing the conditional independence structure of N (  X , ) . remaining variables; i.e., the lack of an edge between i and j denotes the conditional indepen-zero-pattern of  X  1 . To estimate this sparse inverse covariance matrix, one can solve the following sparse inverse covariance selection (SICS) problem: max X  X  S n X , ^ = 1 p mean and Y i is the i -th random sample of Y . This problem is NP-hard in general due to the com-can replace the cardinality term  X  X  X  0 by  X  X  X  1 := {
X  X  R n  X  n :  X  X  X   X   X  1 } (see [3]). This results in the convex optimization problem (see e.g., [4, 5, 6, 7]): Note that (1) can be rewritten as min X  X  S n Both the primal and dual problems have strictly convex objectives; hence, their optimal solutions are unique. Given a dual solution W , X = W  X  1 is primal feasible resulting in the duality gap The primal and the dual SICS problems (1) and (2) are semidefinite programming problems and can be solved via interior point methods (IPMs) in polynomial time. However, the per-iteration com-putational cost and memory requirements of an IPM are prohibitively high for the SICS problem. Although an approximate IPM has recently been proposed for the SICS problem [8], most of the methods developed for it are first-order methods. Banerjee et al. [7] proposed a block coordinate descent (BCD) method to solve the dual problem (2). Their method updates one row and one column of W in each iteration by solving a convex quadratic programming problem by an IPM. The glasso method of Friedman et al. [5] is based on the same BCD approach as in [7], but it solves each sub-problem as a LASSO problem by yet another coordinate descent (CD) method [9]. Sun et al. [10] proposed solving the primal problem (1) by using a BCD method. They formulate the subproblem as a min-max problem and solve it using a prox method proposed by Nemirovski [11]. The SINCO method proposed by Scheinberg and Rish [12] is a greedy CD method applied to the primal problem. All of these BCD and CD approaches lack iteration complexity bounds. They also have been shown to be inferior in practice to gradient based approaches. A projected gradient method for solving the dual problem (2) that is considered to be state-of-the-art for SICS was proposed by Duchi et al. [14, 15] have been applied to solve the SICS problem. d X  X spremont et al. [16] applied Nesterov X  X  optimal first-order method to solve the primal problem (1) after smoothing the nonsmooth  X  1 term, in [16] was very slow and did not produce good results. Lu [17] solved the dual problem (2), which is a smooth problem, by Nesterov X  X  algorithm, and improved the iteration complexity to O (1 / However, since the practical performance of this algorithm was not attractive, Lu gave a variant (VSM) of it that exhibited better performance. The iteration complexity of VSM is unknown. Yuan [18] proposed an alternating direction method based on an augmented Lagrangian framework (see the ADAL method (8) below). This method also lacks complexity results. The proximal point algo-of the problem making it impractical for solving large-scale problems. Also, there is no iteration complexity bound for this algorithm. The IPM in [8] also requires such a reformulation. Our contribution. In this paper, we propose an alternating linearization method (ALM) for solving the primal SICS problem. An advantage of solving the primal problem is that the  X  1 penalty term in the objective function directly promotes sparsity in the optimal inverse covariance matrix. Although developed independently, our method is closely related to Yuan X  X  method [18]. Both methods exploit the special form of the primal problem (1) by alternatingly minimizing one of the terms of the objective function plus an approximation to the other term. The main difference between the two methods is in the construction of these approximations. As we will show, our method has a theoretically justified interpretation and is based on an algorithmic framework with complexity bounds, while no complexity bound is available for Yuan X  X  method. Also our method synthetic data and real problems have shown that our ALM algorithm significantly outperforms other existing algorithms, such as the PSM algorithm proposed by Duchi et al. [13] and the VSM algorithm proposed by Lu [17]. Note that it is shown in [13] and [17] that PSM and VSM outperform the BCD method in [7] and glasso in [5].
 Organization of the paper. In Section 2 we briefly review alternating linearization methods for minimizing the sum of two convex functions and establish convergence and iteration complexity results. We show how to use ALM to solve SICS problems and give intuition from a learning perspective in Section 3. Finally, we present some numerical results on both synthetic and real data in Section 4 and compare ALM with PSM algorithm [13] and VSM algorithm [17]. We consider here the alternating linearization method (ALM) for solving the following problem: where f and g are both convex functions. An effective way to solve (4) is to  X  X plit X  f and g by introducing a new variable, i.e., to rewrite (4) as and apply an alternating direction augmented Lagrangian method to it. Given a penalty parameter 1 / X  , at the k -th iteration, the augmented Lagrangian method minimizes the augmented Lagrangian function with respect to x and y , i.e., it solves the subproblem and updates the Lagrange multiplier  X  via: respect to x and y alternatingly can often be done efficiently, the following alternating direction version of the augmented Lagrangian method (ADAL) is often advocated (see, e.g., [20, 21]): If we also update  X  after we solve the subproblem with respect to x , we get the following symmetric version of the ADAL method.  X  Algorithm (9) has certain theoretical advantages when f and g are smooth. In this case, from the first-order optimality conditions for the two subproblems in (9), we have that: Substituting these relations into (9), we obtain the following equivalent algorithm for solving (4), which we refer to as the alternating linearization minimization (ALM) algorithm. Algorithm 1 Alternating linearization method (ALM) for smooth problem
Input: x 0 = y 0 for k = 0 , 1 ,  X  X  X  do end for Algorithm 1 can be viewed in the following way: at each iteration we construct a quadratic approxi-f ( x ) . The approximation is based on linearizing g ( x ) (hence the name ALM) and adding a  X  X rox X   X  g ) this quadratic function, g ( y k )+ build an upper approximation to f ( x ) at x k +1 , f ( x k +1 )+ and minimize the sum Q f ( x k +1 , y ) of it and g ( y ) .
 convex. Then from the first-order optimality conditions for the second minimization in (9), we have  X  definition of Q g ( x, y k ) by  X   X  k +1 y in (9), we obtain the following modified version of (9). Algorithm 2 Alternating linearization method with skipping step
Input: x 0 = y 0 for k = 0 , 1 ,  X  X  X  do end for property and iteration complexity bound. For a proof see the Appendix.
 where 0 &lt;  X   X  1 , Algorithm 2 satisfies iteration complexity bound in Theorem 2.1 can be improved. Nesterov [15, 22] proved that one can obtain an optimal iteration complexity bound of O (1 / acceleration technique is based on using a linear combination of previous iterates to obtain a point where the approximation is built. This technique has been exploited and extended by Tseng [23], Beck and Teboulle [24], Goldfarb et al. [25] and many others. A similar technique can be adopted to derive a fast version of Algorithm 2 that has an improved complexity bound of O (1 / keeping the computational effort in each iteration almost unchanged. However, we do not present this method here, since when applied to the SICS problem, it did not work as well as Algorithm 2. The SICS problem defined for positive definite matrices while g ( X ) is defined everywhere. These properties of the objective function make the SICS problem especially challenging for optimization methods. Nev-ertheless, we can still apply (9) to solve the problem directly. Moreover, we can apply Algorithm 2 and obtain the complexity bound in Theorem 2.1 as follows. 3.1 in [17], the optimal solution of (12) X  X   X   X I , where  X  = 1  X  ^  X  C := { X  X  S n : X  X  2 I } , the SICS problem (12) can be formulated as: We can include constraints X  X  C in Step 1 and Y  X  C in Step 3 of Algorithm 2. Theorem 2.1 can then be applied as discussed in [25]. However, a difficulty now arises when performing the minimization in Y . Without the constraint Y  X  C , only a matrix shrinkage operation is needed, but with this additional constraint the problem becomes harder to solve. Minimization in X with or without the constraint X  X  C is accomplished by performing an SVD. Hence the constraint can be easily imposed.
 Instead of imposing constraint Y  X  C we can obtain feasible solutions by a line search on  X  . We know that the constraint X  X  2 I is not tight at the solution. Hence if we start the algorithm with X  X   X I and restrict the step size  X  to be sufficiently small then the iterates of the method will remain in C .
 theoretical convergence rate bound holds in only a small neighborhood of the optimal solution. We now present a practical version of our algorithm applied to the SICS problem.
 Algorithm 3 Alternating linearization method (ALM) for SICS
Input: X 0 = Y 0 ,  X  0 . for k = 0 , 1 ,  X  X  X  do end for We now show how to solve the two optimization problems in Algorithm 3. The first-order optimality conditions for Step 1 in Algorithm 3, ignoring the constraint X  X  X  are: Consider V Diag ( d ) V  X  -the spectral decomposition of Y k +  X  k +1 ( k  X  ^ ) and let the constraint X  X  C is imposed, the optimal solution changes to X k +1 := V Diag (  X  ) V  X  with  X  i = max The first-order optimality conditions for Step 2 in Algorithm 3 are: Since g ( Y ) =  X   X  Y  X  1 , it is well known that the solution to (16) is given by where the  X  X hrinkage operator X  shrink( Z,  X  ) updates each element Z ij of the matrix Z by the for-mula shrink( Z,  X  ) ij = sgn( Z ij )  X  max {| Z ij | X   X , 0 } .
 The O ( n 3 ) complexity of Step 1, which requires a spectral decomposition, dominates the O ( n 2 ) complexity of Step 2 which requires a simple shrinkage. There is no closed-form solution for the subproblem corresponding to Y when the constraint Y  X  C is imposed. Hence, we neither impose mance of the algorithm substantially. Thus, the resulting iterates Y k may not be positive definite, become positive definite and the constraint Y  X  X  is satisfied.
 Let us now remark on the learning based intuition behind Algorithm 3. We recall that  X  k  X   X  X  ( Y k ) . The two steps of the algorithm can be written as and in an alternating manner. Given an initial  X  X uess X  of the sparse matrix Y k we update this guess a regularization term which imposes a Gaussian prior on X whose mean is the current guess for the (18) seeks a sparse solution Y while also imposing a Gaussian prior on Y whose mean is the guess a sequence of positive definite inverse covariance matrices that converge to a sparse matrix, while covariance matrix.
 An important question is how to pick  X  k +1 . Theory tells us that if we pick a small enough value, then we can obtain the complexity bounds. However, in practice this value is too small. We discuss the simple strategy that we use in the next section. In this section, we present numerical results on both synthetic and real data to demonstrate the efficiency of our SICS ALM algorithm. Our codes for ALM were written in MATLAB. All nu-merical experiments were run in MATLAB 7.3.0 on a Dell Precision 670 workstation with an Intel Xeon(TM) 3.4GHZ CPU and 6GB of RAM.
 Since  X  k  X   X  X  ( Y k ) ,  X  k  X   X   X   X  ; hence ^  X  k is a feasible solution to the dual problem (2) as long as it is positive definite. Thus the duality gap at the k -th iteration is given by: changes of objective function value F ( X ) and the iterates X and Y as follows:
F rel := We terminate ALM when either available (see (14) and (15)), but computing log det( ^  X  k ) requires another expensive spectral decomposition. Thus, in practice, we only check (20)(i) every N gap iterations. We check (20)(ii) at every iteration since this is inexpensive.
 A continuation strategy for updating  X  is also crucial to ALM. In our experiments, we adopted the  X  by a constant factor  X  every N iterations until a desired lower bound on  X  is achieved. We compare ALM (i.e., Algorithm 3 with the above stopping criteria and  X  updates), with the projected subgradient method (PSM) proposed by Duchi et al. in [13] and implemented by Mark Schmidt 1 and the smoothing method (VSM) 2 proposed by Lu in [17], which are considered to be the state-of-the-art algorithms for solving SICS problems. The per-iteration complexity of all three algorithms is roughly the same; hence a comparison of the number of iterations is meaningful. The parameters used in PSM and VSM are set at their default values. We used the following parameter 100 / X , if  X  &lt; 0 . 5 ,  X  0 =  X  if 0 . 5  X   X   X  10 , and  X  0 =  X / 100 if  X  &gt; 10 . 4.1 Experiments on synthetic data We randomly created test problems using a procedure proposed by Scheinberg and Rish in [12]. Similar procedures were used by Wang et al. in [19] and Li and Toh in [8]. For a given dimension n , we first created a sparse matrix U  X  R n  X  n with nonzero entries equal to -1 or 1 with equal proba-mvnrnd function in MATLAB, and computed a sample covariance matrix ^ := 1 p We compared ALM with PSM [13] and VSM [17] on these randomly created data with different  X  . The PSM code was terminated using its default stopping criteria, which included (20)(i) with  X  dual problem (2), the duality gap which is given by (3) is available without any additional spectral decompositions. The results are shown in Table 1. All CPU times reported are in seconds. From Table 1 we see that on these randomly created SICS problems, ALM outperforms PSM and VSM in both accuracy and CPU time with the performance gap increasing as  X  increases. For example, for  X  = 1 . 0 and n = 2000 , ALM achieves Dgap = 9 . 58 e  X  4 in about 1 hour and 15 minutes, while PSM and VSM need about 3 hours and 25 minutes and 10 hours and 23 minutes, respectively, to achieve similar accuracy. 4.2 Experiments on real data We tested ALM on real data from gene expression networks using the five data sets from [8] provided to us by Kim-Chuan Toh: (1) Lymph node status; (2) Estrogen receptor; (3) Arabidopsis thaliana; (4) Leukemia; (5) Hereditary breast cancer. See [8] and references therein for the descriptions of we see that ALM is much faster and provided more accurate solutions than PSM and VSM. 4.3 Solution Sparsity In this section, we compare the sparsity patterns of the solutions produced by ALM, PSM and VSM. For ALM, the sparsity of the solution is given by the sparsity of Y . Since PSM and VSM solve of X . Instead, we measure the sparsity of solutions produced by PSM and VSM by appealing to complementary slackness. Specifically, the ( i, j ) -th element of the inverse covariance matrix is deemed to be nonzero if and only if | W ij  X  ^ ij | =  X  . We give results for a random problem ( n = 500 ) and the first real data set in Table 3. For each value of  X  , the first three rows show the number of nonzeros in the solution and the last three rows show the number of entries that are nonzero in the solution produced by one of the methods but are zero in the solution produced by the other method. The sparsity of the ground truth inverse covariance matrix of the synthetic data differences. We note that the ROC curves depicting the trade-off between the number of true positive elements recovered versus the number of false positive elements as a function of the regularization parameter  X  are also almost identical for the three methods.
 We would like to thank Professor Kim-Chuan Toh for providing the data set used in Section 4.2. The research reported here was supported in part by NSF Grants DMS 06-06712 and DMS 10-16571, ONR Grant N00014-08-1-1118 and DOE Grant DE-FG02-08ER25856.
