 Mixture models form one of the most widely used classes of generative models for describing structured and clustered data. In this paper we develop a new approach for the anal-ysis of hierarchical mixture models. More specifically, using a text clustering problem as a motivation, we describe a nat-ural generative process that creates a hierarchical mixture model for the data. In this process, an adversary starts with an arbitrary base distribution and then builds a topic hier-archy via some evolutionary process, where he controls the parameters of the process. We prove that under our assump-tions, given a subset of topics that represent generalizations of one another (such as baseball  X  sports  X  base ), for any document which was produced via some topic in this hierarchy, we can efficiently determine the most specialized topic in this subset, it still belongs to.

The quality of the classification is independent of the total number of topics in the hierarchy and our algorithm does not need to know the total number of topics in advance. Our approach also yields an algorithm for clustering and unsupervised topical tree reconstruction.

We validate our model by showing that properties pre-dicted by our theoretical results carry over to real data. We then apply our clustering algorithm to two different datasets: (i)  X 20 newsgroups X  [19] and (ii) a snapshot of abstracts of arXiv [2] (15 categories,  X  240,000 abstracts). In both cases our algorithm performs extremely well.
 Categories and Subject Descriptors: F.2.2: Nonnu-merical Algorithms and Problems, H.1.0: Information Sys-tems: Models and Principles General Terms: Algorithms, theory Keywords: Mixture Models, probabilistic analysis, hier-archical clustering Mixture models form one of the most widely used classes of generative models for describing structured and clustered Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. data [18]. Various applications of mixture models include problems in computer vision [27, 13, 24], text clustering [22, 4], collaborative filtering [12, 15, 4], and bioinformatics [14]. One of the common formulations of mixture models can be described as follows. The data (e.g., texts, images, user profiles, etc.) is a collection of independent samples (e.g., individual documents, image features, etc.), each created by a sequence of independent draws from some hidden distribu-tion over a feature space (terms, pixels, etc.). For example, in the case of text, each document is modeled as a sequence of independent trials from some underlying term distribu-tion. More precisely, there is a set of topics, where each topic is defined by a hidden distribution over the space of all possible terms. A given document can be related to sev-eral topics (e.g., discussing the role of religion in physical sciences), and is modeled as a sample from a linear mix-ture (hence the name) of corresponding topical distributions. The actual mixing coefficients are defined by the document X  X  quantitative relevance to each of the topics. A similar in-terpretation is possible for collaborative filtering where each user X  X  profile (e.g. the books he bought) is modeled as a se-quence of independent trials from some hidden distributions over the item space.

In this work we will be discussing mixture models in the context of text, however the results apply in any other frame-work.

Inspired by statistical approaches there has been a vast amount of work on mixture models [18]. Most of it is based on local search techniques, such as different flavors of EM[4, 17, 11], 1 and Naive-Bayes methods[8, 16].

Recently, Kleinberg and Sandler [15] have shown that there is a combinatorial algorithm, that, given a sufficient amount of unlabeled data, reconstructs the underlying term distributions for each document (with a small error and with high probability). Then, given the topical distributions, the algorithm reconstructs accurately the relevances to each of the topics for each document. In order to guarantee fixed accuracy, the required length of a document depends only on the total number of topics, and two special parameters introduced in [15], but not on the total number of terms in the vocabulary.

However, while providing a mathematically clean and ap-pealing framework, plain mixture models are known to be too simplistic [10] for real data with a large number of topics.
It should be noted, that in the case of gaussian mixtures, and under some specific conditions, EM is known to converge to a global optimum [6], however no results like that are known for the models that we are interested in here. Indeed, most, if not all, of the existing algorithms for mix-ture models need to reconstruct the entire mixture model and/or need to know the total number of topics in the sys-tem in advance in order to perform classification. However, even the number of topics is hard to learn in the context of the Web or any other large unmoderated text collection. To deal with this problem, hierarchical mixture models were introduced (see [10, 26, 3] among others). The common assumption behind hierarchical models is that topics form some natural hierarchy based on the level of their specificity. For instance, topics like  X  X hysics X  and  X  X athematics X  would be under a more general topic  X  X cience. X  The hierarchy it-self might have been inferred from labeled data [26, 10], or in the case of Latent Dirichlet Allocation [4], by assuming a specific generative process called the Chinese Restaurant Process[3, 25] and then applying EM to it. To the best of our knowledge, none of the previous results would provide measurable guarantees of the quality of the produced clas-sification and would use flavors of local search techniques to do final estimations of the parameters and/or the hierarchy. In the present work we suggest a new model of the topic hierarchy and prove that under this model we can provide classification guarantees even if only a small part of hierar-chy is known.

At a high level we assume the following evolutionary pro-cess that builds the topic structure. There is a  X  X roto-topic X  (which we also sometimes call the baseline topic or the root topic) that is the root of the hierarchy tree. Each topic has zero or more children. Each child topic distribution evolves from parent distribution by altering the probabilities of oc-currence of some (possibly all) terms. The exact process by which we obtain children topics from a parent topic will be described later. The very important assumption that we make here is that given a parent topic, all children topics are generated independently from each other. This assump-tion is inspired by the following intuition: related topics such as mathematics and physics share inherited bias for generic science terms from their parent science topic, but otherwise are independent. This independence property is a key assumption that enables us to guarantee classification accuracy without knowing the total number of topics and without ever having to learn the full hierarchy.
 Remark: A similar concept of evolution is widely used in studying the origin of the species, and in particular for phy-logenentic tree reconstruction[21]. There, the assumption is that there is an evolutionary tree of all the species, where each node contains some aggregate description of a partic-ular species (e.g. number of legs, presence of specific gene, etc), the root contains a description of a common ancestor of all currently existing species, and the transition from the parent is modeled via a mutation process. There has been a lot of work done on phylogenetic trees including several results that guarantee close to optimal reconstruction. We refer to [1, 23, 7] for more details. Our problem is different in several aspects. First, our attributes (terms) are defined on a continuous domain and share the same normalization, whereas in phylogenetic trees they could be drawn from dif-ferent domains. Second, our samples (documents) do not contain full information about the node they belong to, but rather a small sample drawn from it (whereas a single ani-mal sample would contain all or most of the properties for a given species.) Finally, document distributions could be a mixture of distributions from several topics (in the case of a phylogenetic tree this would correspond to observing a mix of a dog and a mouse). However it would be very interesting to connect these different lines of research. 1.1 Our Contributions. There are several contribu-tions of this paper. First, we introduce a new theoretical model to describe topical hierarchies. Combined with its empirical validation, this is an important step in the under-standing of the topical structure of textual data.
Second, we prove that within this model one can com-pute how a given document relates to a known topic (or several topics that are generalizations of each other) with very little knowledge of the rest of the hierarchy. We are not aware of any other theoretical work that would analyze the possibility of accurate predictions without knowing the full topical structure. We also show that the accuracy is independent of the total number of topics in the full hierar-chy; this makes the approach particularly useful when the available data contains a very large number of topics.
Third, we present an algorithm that can reconstruct a topical hierarchy by analyzing unlabeled data. One other important property of our algorithm is that it scales ex-tremely well and most of the processing can be readily parallelized. We evaluate our algorithm by clustering two datasets: arXiv [2], and  X 20 newsgroups X  [19], and in both cases our algorithms perform extremely well.

Finally, we demonstrate that the theoretical properties predicted by our analysis carry over to the real data, which further supports the validity of our model. 1.2 Organization of the paper. In the next section we introduce some background on Mixture Models and all the necessary tools we need for analysis. Then, in Section 3 we formally describe our model, prove the main theoretical results of the paper and describe our algorithms. Finally, in Section 4 we present our experimental results. 2.1 Concentration bounds. The main concentration result we use is Hoeffding X  X  inequality, which is a generaliza-tion of Chernoff X  X  bound [20].

Lemma 2.1 (Hoeffding X  X  Inequality [9]). Suppose { x i } is a sequence of independent random variables, such that x i  X  [ a i , b i ] . Then for any t &gt; 0 ,
Pr h  X   X  where E [ P x i ] denote the expected value of the sum of the random variables. 2.2 Mixture models, topic independence and gen-eralized pseudoinverses. In this section we provide a for-mal definition of mixture models and necessary results from earlier work [15]. We start with a brief overview of genera-tive mixture models. (i) There is a set D that contains all possible terms in the (ii) There is a set T of k topics, where each topic T ( i ) (iii) Each document has hidden relevance to one or more (iv) We represent a document by a count vector  X  D of the It has been shown previously[22, 15] that mixture models allow efficient algorithms to find a document X  X  relevance to each topic. The required length of a document needed to guarantee a fixed accuracy of classification is independent of the size of a vocabulary. However, in order to make this guarantee the algorithms in [22, 15] require knowledge of the number of topics in advance, and also need to recover all the topics. At a high level the algorithm of [15] is as follows: Algorithm 2.1 (Semiomniscient algorithm of [15]). Input: Topic matrix W , document vector  X  D .
 Output: Vector of approximate relevances to all the topics. If max i,u V iu is bounded, it was proven that for any  X  &gt; 0 and given a sufficiently long document we have ||  X  r  X  r ||  X  with high probability. It was also shown that for an op-timal pseudoinverse the maximal element could be upper bounded in terms of the original topic matrix W : It was proven in [15] that  X  &gt; 0 is necessary for any mixture model to be fully learnable. We refer the reader to the orig-inal paper [15] for an in-depth discussion and comparison with other ways of obtaining pseudoinverses (such as using Singular Value Decomposition). We conclude this part by a simple concentration lemma. 2
Lemma 2.2. Let D be an arbitrary distribution over terms, and let R  X  &lt; n be such that || R ||  X   X  c . Let be a normalized count vector of a sample of size t from D (e.g.  X  D i = 1 t if term i was sampled exactly once). Then for any  X  &gt; 0 , we have:
A weaker variant of this based on Chebyshev inequality was proven in [15] Proof. We represent  X  D as a sum of t independent samples  X  D , where  X  D i is an indicator vector for the i -th term in the that E h  X  R  X  D i  X  i =  X  RD  X  . Now we use Hoeffding X  X  inequality (Lemma 2.1) and substituting  X  R  X  D i  X  for x i ,  X  for  X  and  X  c/t and c/t for a i and b i respectively, we have: 3.1 Model for topical hierarchy. In this section we formally describe our model. It is important to emphasize that we still use the regular mixture model as a generative model for text. However, in addition to that we introduce a hierarchical generative process on the topical distributions. In other words, we assume that there are two underlying generative processes happening: the first process generates the underlying mixture model, and this model in turn defines the parameters of the second generative process that creates all the documents. We show that with very high probability the produced mixture model will have  X  X ood X  properties, and then given a  X  X ood X  model we can produce a proper classification.

The topic generation is a multistep process driven by an adversary  X  he chooses initial parameters at will and then the random process starts by using the adversary X  X  param-eters. At the intermediate steps the adversary can analyze the current hierarchy and choose additional parameters. We show that no matter how the adversary behaves we can still provide guarantees.

The construction proceeds as follows. First the adversary chooses the baseline topic T (0) , which will be at the root of our hierarchy tree; we don X  X  restrict the adversary regarding how he chooses the base topic. Given the root of the tree, the adversary starts building the rest of the tree. On every topic he decides how many children it will have, and then chooses the distributions for all immediate children topics at once. Suppose topic T is a parent topic, and the adversary is choosing the parameters of a child topic T 0 . For each term i in a child topic, he chooses a probability distribution D i satisfying some specific feasibility conditions. Then, the random process samples a value E 0 i for each D 0 i and sets T i  X  T i + E 0 i . After the change is applied, the vector T is normalized by a factor  X  0 so that it represents a valid distribution. In general, for a topic T ( i ) , the change vector that was applied to its parent to generate T ( i ) is denoted by E ( i ) (we also sometimes refer to E ( i ) is as a mutation vector ) and the normalization constant by  X  i .

At each step the adversary can expand any leaf in the constructed hierarchy. The change vector E 0 satisfies the following constraints: (i) The resulting topic cannot contain any negative ele-(ii) For any term we have E [ E 0 i ] = 0. It is important to (iii) The difference between child and parent topics should (iv) Finally, changes cannot be concentrated on just a few The first two conditions guarantee that the resulting vector contains only positive values and that in expectation the resulting norm stays one.

The third condition guarantees that ancestors differ enough from the parents so that it is possible to differen-tiate between them.

The last condition guarantees that changes are spread among many items and deserves some additional explana-tion. Note the two different norms used in the condition. This condition captures two properties. First, since we are looking at the maximal possible change in any probability, it guarantees absence of  X  X ow probability -very high impact X  events. Or, in other words, it says that there cannot be a term that with small probability would dominate the entire vocabulary; however it is perfectly feasible to have many terms whose probability will multiply by a large factor.
Also note that by the condition (iii) the expected L 1 norm of a change is large and thus the condition (iv) essentially requires the maximal possible change in the Euclidean norm to be small. Since norms are monotone, this could poten-tially lead to two conflicting conditions. However, if the changes are distributed across many items (and that X  X  what we observe in practice), the value of the Euclidean norm is smaller than the L 1 norm by an  X ( slope[ E 0 ]  X   X (
For example, starting with a uniform parent distribution, the adversary might want to create a topic with support on half of the terms. Then, choosing a probability distri-bution that is  X  1 n with probability 1/2 would produce the desired result. Note that all constraints above are satisfied struction and starting with a uniform parental distribution, the adversary can produce any power-law distribution in a child. However, the adversary does not have control over whether the probability of any individual term will increase or decrease X  X nly over the general shape of the distribution.
The random topic generation process might appear coun-terintuitive at first. However, one can argue that, if we did not know any meaning behind any term, then changes in probabilities for different topics would appear random and uncorrelated (given the parent distribution) among different child topics; this is exactly what our process models.
A somewhat similar measure has appeared before in [5], in the context of continuous models where it was called  X  X lope ratio X  (hence our name) and it was shown that this quantity is one of the necessary parameters which measure  X  X earnabil-ity X  of their model, it is interesting that it appears in our context as well. 3.2 Analysis: Overview. Our analysis contains two major parts. In Section 3.3 we prove intuitive albeit fairly technical results. Namely, if we have a sequence of topics such that one is obtained from another by introducing ran-dom changes and re-normalizing the resulting topical vector, then these topics will stay independent in linear algebraic sense (as measured by the independence coefficient from the previous section), with high probability, provided that the length of the sequence is small compared to the number of terms in the vocabulary. It can be shown that the linear independence is necessary to be able to uniquely determine the relevance of topics to a document. In the process of do-ing that we also prove a few auxiliary lemmas which we will reuse later.

Then, in Section 3.4 we analyze our perturbation scheme and in particular we prove that if we consider a sequence of topics lying on the path of the hierarchy starting from the root, and consider it as a standalone mixture problem, then applying it to a document produced by a topic from outside of the path, it will get assigned to the closest node 4
Finally in Section 3.5 we bring all the results together and present out algorithms 3.3 Connecting probabilistic and linear indepen-dence. We start with a simple lemma which provides con-centration guarantees for a sum of a fixed and random vec-tor.
 Lemma 3.1 (Sum of a fixed and a random vector).
 Fix some  X  and  X  . Suppose Z  X  &lt; n is an arbitrary vector, E are independent random variables, such that E [ E i ] = 0 , each variable is bounded  X   X  i  X  E i  X   X  i , and the slope is high: then the following conditions are satisfied with probability at least 1  X   X  : ( i ) || Z + E|| 1  X  (1  X   X  ) max( || Z || 1 , E[ ||E|| 1 ( ii ) ||E|| 1  X  3 2 E [ ||E|| 1 ] , Proof. First of all we show that for all i  X  [1 , n ] we have: Indeed, we immediately have where the first transition uses the triangle inequality, and the second uses the linearity of expectation. Combining triangle inequality in a different way and using the previous lower bound we get:
E [ | Z i + E i | ]  X  max( | Z i | , E [ |E i | X  X  Z i | ])  X  E
Exactly the same result applies if a document is produced by a mixture of topics  X  the weight assigned to each of the node on the path would reflect the total relevance weight to the topics closest to the given nodes. Combining (4) and (5) we immediately have To finish the proof we apply Hoeffding X  X  inequality to the sum of the individual components of the L 1 norm, to show desired concentration around the expectation. Indeed, using (6) we have: choosing t =  X  E[ ||E|| 1 ] 2 we immediately have: : we have used our slope constraint (3) in the last transition.
The part (ii) immediately follows from yet another appli-cation of Hoeffding X  X  inequality.

For the part (iii) of the theorem, we just note that if sgn[ Z i + E i ] = sgn[ Z i ], then we have E [ | Z i + E and so we can use both upper and lower bounds provided by Hoeffding X  X  inequality and the result immediately follows. Remark: Somewhat surprisingly, the factor 1 2 in the norm ||E|| 1 in the lemma above is tight (which would not be the case if Z was random too). For example, let Z = ( 1 n , . . . , and let for some  X   X  1 / 2 then we have whereas thus if  X   X  1, then E [ || Z + E|| 1 ]  X  E[ E ] 2 . As n grows (and  X  stays constant), we can apply concentration bounds, and show that for any  X  , one can choose  X  and n 0 , so that  X  n &gt; n 0 || Z + E|| 1  X  (1 +  X  ) ||E|| 1 2 with probability at least 1 / 2.
An immediate corollary of the previous lemma, is that the normalization coefficients for the topics, are in fact very close to one.
 Corollary 3.2 (Bounded normalization constants). Consider T 0 with parent topic T , then if the distributions of change vector E 0 satisfy the constraints (i)-(iv) of the hierarchical process, and in particular for some  X  : Then with probability at least 1  X   X  , the normalization con-stant  X  0  X  [1  X   X , 1 +  X  ] .
 Proof. The result immediately follows from Lemma 3.1, since E 0 + T  X  0 and T  X  0, thus || T ( j ) + E ( i ) || with probability at least 1  X   X 
Now, we prove a more technical lemma which connects the notion of linear independence from [15, 22] with the notion of probabilistic independence. In this lemma we assume that we have a sequence of random vectors, and we prove that with high probability they will have high independence.
Lemma 3.3. Let Z be an arbitrary vector such that given the values of E ( j ) , the distribution of E ( i ) following constraints: (1) E h E ( i ) i = (0 , 0 , . . . , 0) , 2  X  E h ||E ( i ) (2) The slope ratio of each random vector is high: Fix 0  X  l  X  t . Let W l denote a matrix comprised of columns Proof. The proof goes by induction on l . The result for the base l = 0, is immediate because of the normalization of Z . Suppose we have proved for l  X  1 that  X ( W l  X  1 )  X  with probability at least 1  X  ( l  X  1)  X  t and would like to extend to l . The proof of the induction hypothesis consists of three parts, first we show that for an arbitrary fixed unit vector p = ( p 0 , . . . , p l ), the probability is exponentially small, then we use the union bound to ex-tend the result to sufficiently dense discrete subset of all possible vectors p , and finally we will use the continuity of a linear operator to extend it to all unit p  X  &lt; l +1 to com-plete the proof.

Consider an arbitrary normalized vector p  X  &lt; l +1 , and hypothesis we have  X ( W l  X  1 )  X   X  6 l with probability 1  X  thus we have: We also have E h || p l E ( l ) || 1 i  X  p l  X . But E ( l ) of X , thus, applying Lemma 3.1 part (i) we have Pr  X 
Now we compute a lower bound for the max in this equa-tion. If p l  X  1 2( l +1) then we have: max Combining (13) and (12) and substituting  X  = 1 6( l +1) we have: therefore from (11) we have: Now consider a set P  X &lt; l , such that for any p  X &lt; l 1, there exists p  X   X  P , such that || p  X  p  X  || 1  X   X  54 l 2 possible choice is to take P such that it contains all possible such that || p || 1  X  1. An immediate calculation gives that this set would contain at most ( 54( l +1) 3  X  ) l elements. Using bility at least 1  X   X  t , we have that for all p  X  X  , Finally, for any p , there exists p 0  X  X  , such that || p  X  p 54 l 2 and thus we have: where the transition in (16) follows from the fact that all columns in W l have norm at most 3 with high probability. Thus given the induction hypothesis the  X ( W l )  X   X  6( l +1) with probability at least 1  X   X  t , combining it with the fact that the induction hypothesis holds with probability at least probability at least 1  X  l X  t . This finishes the proof of the induction hypothesis, and in turn completes the proof of the lemma.

Now we fulfill the main promise of this section and prove that topics along any path are independent in linear alge-braic sense:
Theorem 3.4. Suppose W = [ T (0) , . . . , T ( l  X  1) ] is a path between the root and some node in the topic hierarchy, and such that topic T ( i ) was obtained by applying a mutation vector E ( i ) , satisfying the conditions (i)-(ii) of Lemma 3.3 . Then with probability 1  X   X  ,  X ( W )  X   X  18 l Proof. Recall T ( i ) is obtained from T ( i  X  1) by adding a random change vector and re-normalizing: where  X  i is a normalization coefficient introduced to main-tain || T ( i ) || 1 = 1 . Using lemma 3.1, we immediately have that  X  i  X  2 with high probability. Now, consider any linear combination of topics: and we need to show that if || p || 1 = 1, then || X || 1 To prove this we substitute (19) into (20) and regroup: where  X  where we define  X  0 = 1. By the lemma 3.3 we have || X || ||  X  || 1 . Thus to prove the lemma it suffices to show that P |  X  i |  X  1 / 3 and the result would immediately follow. To prove that, we rewrite equation (21) as but |  X  i | + |  X  i +1  X  i | X |  X  i  X   X  i +1  X  i | and thus dividing both sides by  X  i , summing over i and use Lemma 3.2, to show that  X  i  X  0 . 5 we have: Therefore we have proved that ||  X  || 1  X  1 / 3 and the lemma follows. 3.4 Hierarchical Models: Analysis. We start with a simple definition, which quantifies a relationship between document generated by any topic in the hierarchy, and an arbitrary root based path in the hierarchy.
 Definition 1 (Projection of a mixture on to a path). Given some path in the hierarchy T (0) , T (1) , ... , T ( l ) a linear combination of topics (not necessarily on the path) path is a linear combination of topics D 0 , such that where  X  ( b i ) denotes the closest to T ( b i ) topic in the path. Now we prove the key lemma of this section.

Lemma 3.5. Let T (0) , T (1) , ... , T ( l ) be a path in the hi-erarchy. Let R be some vector which was only chosen based on T (1) , . . . , T ( l ) , but not any other topics, and such that its maximal element is bounded by some constant  X  . Then for any topic T ( c ) , let T ( a j ) be the closest topic to T suppose the path between them has length c nodes, then we have with high probability. 5 Furthermore, if M is a mixture of topics, then where M 0 is a projection of M on to the path T (0) , . . . , T and m is the maximal length of the path between any topic in the mixture and a path.
It is also possible to improve the factor of c to using martingales and increasing the length of the proof sig-nificantly. Proof. Let T ( j ) be the closest node in the path to T ( c ) proof is based on the observation that T ( j ) is an ancestor of T prove our lemma it is sufficient to prove that if T 0 is a child of T and R is independent of T 0 then and apply triangle inequality.

Recall that T 0 =  X  0 ( T + E 0 ), where  X  0 and E 0 are respec-tively the normalization constant and the mutation vector from the generative process. Therefore we have: Now we just need to compute an upper bound for the right hand side. For the first term, using Lemma 3.2, we have | 1  X   X  0 |  X   X / 2 with high probability. To bound the second term note that E 0 i are chosen independently of R , so we can independent and have zero expectation, so we can apply Hoeffding X  X  inequality:
Pr where M 0 denotes the vector of absolute maximal values that E 0 can take and we used  X  2 from the constraint (iv) on the slope ratio of the hierarchical construction.
The generalization to a mixture of topics follows automat-ically by applying the previous result to each of the term in the linear combination and using the fact that the mixture is normalized.

Now we conclude this section by building a connection between documents and topics in the hierarchy. Recall that a document is a sample from a mixture of topic distributions. The next corollary shows that we can use the document to infer the relevance to each of the topics to an arbitrary path in the hierarchy.
 Corollary 3.6 (Document relevance to a path).
 Consider a path starting from the root T ( a 0 ) , . . . , T the corresponding weight matrix W = [ T (0) , . . . , T ( l ) let V be a pseudoinverse of W . Let a document is sampled from a mixture (not necessarily overlapping with the topics in the path) and let  X  D be the term count vector of this document, then the vector  X  r = V  X  D would approximate the coefficients of the projection of D onto the path.
 Proof. The proof follows immediately from the previous lemma and the lemma 2.2 3.5 Algorithms. In this section we present our algo-rithms and make some additional remarks on the analy-sis. There are three algorithms to be discussed here. First, given a path in the hierarchy (such as machine learning  X  computer science  X  science  X  base ), we would like to compute where in the hierarchy a given document belongs. If, for example, a document is related to machine learning and biology , when for the path above, we will learn that it is related to both machine learning and science (as science is the closest ancestor of biology that is in the path). Or, alternatively, if a document is about soccer , it will be as-signed to the baseline topic. From Corollary 3.6 it follows that the relevances computed by using pseudoinverse are an accurate approximation of the projection of the real mixture onto the path.

Our second algorithm is concerned with reconstructing the hierarchy given the term distributions for topics. 6 Fi-nally, we present an algorithm that finds topics and builds a topical hierarchy from unlabeled data. Here we use a mod-ification of an algorithm from [22] that extracts topics from the co-occurrence matrix.
 the fact that that topics along the path are sufficiently in-dependent, which implies that we can build a pseudoin-verse matrix for those topics with bounded maximal ele-ment. Corollary 3.6 can then be used to prove that the produced relevances are a projection of the real relevances on that path.
 Algorithm 3.1 (Computing document relevance).
 Input: A path in the hierarchy T (0) , T ( i 1 ) , . . . , T document X  X  indicator vector  X  D Output: Relevance to each of the topics along the path 2. Compute the pseudoinverse matrix V such that V W = 3. Return  X  r = V  X  D 3a. To cluster: return the topic i which maximizes  X  r i following observation: suppose we have a base topic T (0) some leaf topic T ( c ) . Let W = [ T (0) ; T ( c ) ] and let V = W From Lemma 3.5 it follows if a topic T ( d ) is in different sub-tree than T ( c ) , then we would have since T (0) is the closest node in the path to T ( d ) . However if T ( d ) is in the same subtree then we don X  X  have any guar-antees for the value V T ( d ) . Indeed, our lemma says that the value would be close to the value of the closest topic which lies on the path between T ( c ) and T (0) . However we don X  X  know that topic. We conjecture, 7 that V T ( d )  X   X   X   X   X   X  +  X  = 1 and their ratio is defined by the ratio of distances (under some proximity measure) to the base topic and the topic T ( c ) . This gives us a foundation for the algorithm: we can approximate baseline topic by computing cumulative
An application for this algorithm would be if we have la-beled data and would want to build automatic topic hierar-chy.
We can prove this for a special case of gaussian change function. term distributions across all the documents available. Then for each topic T 0 we compute all the topics which lie outside of the subtree where T 0 belongs to, and then we combine all the leafs in the subtree to build a new root for the sub-tree and iterate. A high level description of the algorithm is below.
 Algorithm 3.2 (Topic Hierarchy).
 Input: A collection of topical distributions, a threshold value  X   X  0 Output: A topic hierarchy 1. Estimate baseline topic T (0) by computing average dis-2. For each leaf topic T ( c ) consider weight matrix W = 2a. For all other leaves topics T ( d ) , compute r = V T 3. As a result of step [2], we get a disjoint family of topic 4. Apply steps 1-3 recursively on each subset.
 section we give an empirical algorithm on how to construct the topics out of a large collection of unlabeled data. The algorithm could be used for both leaf topic reconstruction and hierarchy reconstruction, and produces classification as a byproduct. Consider a co-occurrence matrix P ij , which for every pair of terms i and j measures how often they occur in the same document across the entire collection. Then we normalize all the columns so that they represent valid probability distributions. Then we approximate the root distribution by the aggregate distribution of terms across all documents. After that, we choose a column such that the L distance between the column and the baseline distribution T (0) is maximal. We treat it as a topic and run Algorithm 3.1 to do binary classification between T (0) and the found column. The promise is that all the documents which are outside of the same tree will be assigned to the T (0) topic, and the documents which are in the same subtree topic will get assigned to the found topic. After that we iterate the entire algorithm on remaining data and/or we can further iterate the algorithm on the constructed cluster to build subtree. 8 Algorithm 3.3 (Reconstructing the topics).
 Input: Unlabeled Data Output: Topical hierarchy and classification. 1. Build the co-occurrence matrix P from the data and 2. Estimate the baseline distribution T (0) by computing 3. Choose a column P i of the co-occurrence matrix which
Due to space constraints, we omit a few important details on how to deal with the fact some of the columns might not be well approximated. We refer to the actual code which is available upon request, for more details. Table 1: First row contains relevance of each of the 20 4. Use P i and T (0) as two topics and perform the clus-5. Remove clustered documents from the collection and 7. Apply the algorithm on the each cluster. We perform two kinds of experiments. First we validate our model by performing some experiments on labeled data. In the second part all our experiments are performed in fully unsupervised manner, we reconstruct both the topics and the hierarchy, and perform clustering and compare it with the ground truth. . 4.1 Model validation and reconstructing hierarchy from labeled data. In this section we perform a few exper-iments on labeled data to reconstruct topic hierarchy given topical distributions themselves. In particular for each topic distribution we create the set of other topics which our al-gorithm deem related to it.

We use 20 newsgroup[19] dataset which contain 20 follow-ing newsgroups: 1. graphics, 2. os.ms-windows-misc, 3. ibm.hardware, 4. mac.hardware, 5. windows.x, 6. sci.electronics 7. misc.forsale, 8. rec.autos, 9. rec.motorcycles 10. rec.sport.baseball, 11. rec.sport.hockey 12. sci.crypt, 13. sci.med 14. sci.space 15. soc.religion.christian, 16. alt.atheism 17. religion.misc, 18. politics.guns 19. politics.mideast, 20. politics.misc We use the numbering above consistently throughout the rest of the paper.
 Let c be one of the topics (say rec.sport.hockey ), let W = [ T (0) ; T ( c ) ] and V = W  X  1 is generalized pseudoinverse matrix with minimal maximal element. Now, for each of the topics (including rec.sport.baseball ) we compute the 2-dimensional vector V T ( c ) and the results are presented in Table 1. The most striking result in the table above is that for a given topic T ( d ) which is semantically unrelated to T the product V T ( d ) is either very close to V  X  T (0) = ( (topics 6, 7 and 13-20), or has the form V T ( d ) =  X   X   X  for some positive  X  (topics 1-4). For semantically related topics, such as topic 11 ( rec.sport.baseball ) we have rel-evance ( 0 . 46 0 . 48 ) and somewhat related 8 ( rec.autos ) and 9 for most of the topics, the prediction from the theorem 3.5 carry out almost precisely.

The negative relevance phenomenon can be easily ex-plained in the framework of the model. Recall that we approximate the baseline topic distribution T (0) by aver-aging the term distribution across all the documents in the data. However, if we have a bias for some topic (or many related topics) in the collection, then our baseline distribu-tion might become biased towards that topic. E.g. if a leaf of finding the true T (0) we found then topics c and d are no longer probabilistically indepen-dent from each other given  X  T (0) . Instead we have: Let V be a pseudoinverse matrix of W = [  X  T (0) , T ( c ) plying both parts of (26) by V we immediately have: Tuning the constant in the bias formula (25) we get that V T ( d ) =  X  1+  X   X   X   X  , which is exactly what we observe in our experiments. Furthermore, topics 1-4 in the table 1 that exhibit this behavior are indeed a part of the largest topic in the dataset (computer related documents).

Now we apply Algorithm 3.2 with the thresh-old parameter  X  = 0 . 3. It produces 8 high level topics which we roughly name as: all com-puter related, sci.crypt , sci.space , sci.med, politics, sports, motor vehicles and religion . Iteration on each component produces a collection of individual topics with the exception of ibm.hardware and mac.hardware and the latter remain grouped and needed one more iteration to split. Note that produced hierarchy is a perfectly reason-able hierarchy on the newsgroups. 4.2 Clustering arXiv. In this section we perform un-supervised clustering of arXiv dataset[2]. The snapshot has approximately 250,000 abstracts on various areas of physics, and also computer science and math. The first level of the hierarchy produced by our algorithm is presented in Table 3. Also the recall/precision table corresponding to the full hier-archy is presented in Table 2. Note that the arXiv contains many heavily overlapping topics, and it is not clear, that there would be a consensus if we did classification manually. For example, many abstracts assigned to quantum-ph and cs (cluster 5), are about quantum computing, and it is prob-ably not possible to differentiate them in a meaningful way. Nevertheless, many topics do get separated cleanly with Table 2: Recall/precision tables for arXiv hierarchy. high precision and recall (in particular astro-ph , hep-ph , condensed matter , computer science and math ). Another interesting and exciting property of the algorithm is that it successfully separated topics of very different sizes. The most striking example is computer science (3K documents, 64% recall/78% precision in a single cluster), and one of the largest topics astro-ph (45K documents, 58% recall, 93% precision in the largest cluster, or 86%/94% if we combine the 5 clusters where astrophysics is a majority). We also note that for two topics math-ph and physics we did not succeed finding clusters where they would form a majority  X  which however, comes hardly as a surprise, as they don X  X  have well defined boundaries (especially physics !) and span across many areas of physics and mathematics. 4.3 Clustering of Newsgroups 20. We present the first level topics of the newsgroup hierarchy that our al-gorithm has reconstructed in table 4.3. Note that for reli-gion/political newsgroups our algorithm produced high level clusters which go across groups boundary, but yet make per-fect sense: Cluster 8 contains mostly documents related to wars (politics.mideast and politics.guns). Cluster 4 contains mostly documents related to religion, and cluster 9 contains medical and health related documents contains sci.med and partly some politics. We have proposed a new generative model to describe hi-erarchical topic structure in discrete mixture models. Our analysis provides a mathematical framework and enables ef-ficient algorithms for text classifications and topic hierarchy reconstruction. One of the key features of our approach, is that it provides guarantees without the assumption that the entire model is reconstructed. Now we outline a few Table 3: The clustering or arXiv that only uses the top open questions and further directions for this framework. We know how to classify documents along the path in the tree. However the algorithm which reconstructs the path (and the tree) is based on a conjecture that says that topics on the path between root and leaf note behave in a continu-ous manner, and this allows to differentiate between topics which belong to the same subtree. It would be interesting to prove this conjecture. Another related direction would be to develop a connection between proposed model and phylogenetic trees reconstruction problem. We believe that out approach can provide a valuable tool for the analysis of the origin of species. Another interesting direction is to use our algorithm to build hierarchy on terms. In particular we could apply the algorithm which reconstructs topic hierar-chy to the co-occurrence matrix, and that would create an hierarchy on terms. Exploring this, would be an interesting and exciting direction. Author would like to thank Corinna Cortes, Jon Feldman and S. Muthukrisnhan for useful discussions. Table 4: The top part of the hierarchy for Newsgroups.
