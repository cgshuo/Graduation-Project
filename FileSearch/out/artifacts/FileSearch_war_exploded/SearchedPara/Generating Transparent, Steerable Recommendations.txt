 We propose a recommendation technique that works by col-lecting text descriptions of items and using this textual aura to compute the similarity between items using techniques drawn from information retrieval. We show how this rep-resentation can be used to explain the similarities between items using terms from the textual aura and further how it can be used to steer the recommender. We describe a system that demonstrates these techniques and we X  X l detail some preliminary experiments aimed at evaluating the qual-ity of the recommendations and the effectiveness of the ex-planations of item similarity.
 H.3.3 [ Information Search And Retrieval ]: Information Filtering Algorithms, Measurement, Performance Steerable recommender, Explainable recommende
One of the problems faced by current recommender sys-tems is explaining why a particular item was recommended for a particular user. Tintarev and Masthoff [5] provide a good overview of why it is desirable to provide explanations in a recommender system. Among other things, they point out that good explanations can help inspire trust in a rec-ommender, increase user satisfaction, and make it easier for users to find what they want.

If users are unsatisfied with the recommendations gener-ated by a particular system, often their only way to change how recommendations are generated in the future is to pro-vide thumbs-up or thumbs-down ratings to the system. Un-fortunately, it is not usually apparent to the user how these gestures will affect future recommendations.

Finally, many current recommender systems simply present a static list of recommended items in response to a user viewing a particular item. Our aim is to move towards ex-ploratory interfaces that use recommendation techniques to help users find new items that they might like and have the exploration of the space part of the user experience.
In this paper, we X  X l describe a system that builds a textual aura for items and then uses that aura to recommend similar items using textual similarity metrics taken from text infor-mation retrieval. In addition to providing a useful similarity metric, the textual aura can be used to explain to users why particular items were recommended based on the relation-ships between the textual auras of similar items. The aura canalsobeusedto steer the recommender, allowing users to explore how changes in the textual aura create different sets of recommended items.
Tintarev and Masthoff X  X  [5] survey of explanations in rec-ommender systems provides an excellent description of the aims of explanations and what makes a good explanation. We will adopt their terminology wherever possible. Za-nardi and Capra [8] exploit tags and recommender system techniques to provide a social ranking algorithm for tagged items. Their approach uses straight tag frequency, rather than more well-accepted term weighting measures.

Vig et al. [6] use social tags to generate descriptions of the recommendations generated by a c ollaborative filtering rec-ommender. Although they explicitly forego  X  X eyword-style X  approaches to generating recommendations, we believe that it is worthwhile to try techniques from information retrieval that are known to provide good results.

Wetzker et al. [7] use Probabilistic Latent Semantic Anal-ysis (PLSA) to combine social tags and attention data in a hybrid recommender. While PLSA is a standard informa-tion retrieval technique, the dimensionality reduction that it generates leads to a representation for items that is difficult to use for explanations.
The key aspect of our approach is to use an item represen-tation that consists of text about the item. For some item types like books or blog posts, this could include the actual text of the item, but for most items, the representation will mainly consist of text crawled from the Web. In our rep-resentation, each item in the system is considered to be a document. The  X  X ext X  of the document is simply the con-glomeration of whatever text we can find that pertains to the item. Because we may collect text about the item from a number of sources, we divide an item X  X  document into fields, where each field contains text from a particular source.
Therepresentationweuseforanitemisavariantofthe standard vector space representation used in document re-trieval systems. Rather than using a single vector to repre-sent a document, the system keeps track of separate vectors for each of the fields that make up the document. This al-lows us to calculate document similarity on any single field or any combination of fields.

Once one is treating items as documents in this fashion, it becomes easy to do other things like treat individual social tags as documents whose words are the items to which the social tag has been applied. Similarly, a user can be modeled as a document whose words are the social tags that the user has applied to items, allowing us to compute user-user and user-item similarity.

An advantage of a vector space representation is that there are well known techniques for dealing with a number of phe-nomena that are typically encountered when dealing with tags. For example, the issues of tag quality and tag redun-dancy encountered by Vig et al. [6] are usually handled quite well using the standard vector space model for document re-trieval. Another advantage is that there are also well-known techniques for building and scaling systems that use such a representation.

Our current work has focused on recommending musical artists. The data sources that we use to represent a musical artist include social tags that have been applied to the artist at Last.fm [1] and the Wikipedia entry for the artist. Fig-ure 1 shows a portion of the textual aura for Jimi Hendrix derived from the tags applied to him at Last.fm as a tag cloud. Unlike typical tag clouds, where the size of a tag in the cloud is proportional to the frequency of occurrence of the tag, here the size of a tag is proportional to it X  X  term weight in the vector space representation for the artist.
By tying the size of a tag in the cloud to the term weight, we are giving the user an indication of what kind of artists we will find when we look for similar artists. In this case, we will tend to find a guitar virtuoso , who plays blues in-fluenced psychedelic rock , as these are the largest tags and therefore the tags with the most influence over the similarity calculation.
We can use the textual aura in a number of recommenda-tion scenarios. We can find similar artists based on a seed artist X  X  aura, or we can find artists that a user might like based on tags that they have applied or based on the com-bined aura of their favorite artists. We allow users to save tag clouds and use them to generate recommendations over time.

Since the implementations of these recommendation sce-narios are based on similar techniques, we X  X l illustrate the general approach by explaining the specific case of finding artists that are similar to a seed artist. First, we retrieve the document vector for the seed item. This may be a vector based on a single field of the item X  X  document or a composite vector that incorporates information for a number of fields.
In any case, for each of the terms that occurs in the vector for the seed item, we retrieve the list of items that have that term in their textual aura. As each list is processed, we accumulate a per-item score that measures the similarity to the seed item. This gives us the set of items that have a non-zero similarity to the seed item. Once this set of similar items has been built, we can select the top n most-similar items. This selection may be done in the presence of item filters that can be used to impose restrictions on the set of items that will ultimately be recommended to a user.
For example, we can derive a popularity metric from the listener counts provided by Last.fm and use this metric to show a user artists that everyone else is listening to or that hardly anyone else is listening to.
Given the auras for two items, it is straightforward to determine how each of the terms in the textual aura for a seed item contributes to the similarity between the seed item and a recommended item. For items A and B ,andthe vectors used to compute their similarity, we can build three new tag clouds that describe both how the items are related and how they differ.

We can build an overlap tag cloud from the terms that occur in the vectors for both items. The tags in this cloud will be sized according to the proportion of the similarity between A and B contributed by the term. This explanation of the similarity between two items is directly related to the mechanism by which the similarity was computed. We can also build two difference clouds that show the terms in A B that are not in the set of overlapping terms. The difference cloud can be used to indicate to a user how a recommended item differs from the seed item, which may lead them into an attempt to steer the recommendation towards a particular aspect.
In addition to providing a straightforward way of gener-ating explanations of the recommendations made, our ap-proach offers an equally straightforward way of steering the recommendations.

We allow the user to directly manipulate the document vector associated with an item and use their manipulations to interactively generate new sets of recommended items on the fly. If the vector for an item is presented as a tag cloud, then the user can manipulate this representation by dragging tags so that the tags increase or decrease in size. As with any other recommendation, we can provide an explanation like the ones described above that detail why a particular item was recommended given a particular cloud.
 The user can also make particular tags sticky or negative . A sticky tag is one that must occur in the textual aura of a recommended artist. A sticky tag will still contribute some portion of the similarity score, since it appears in both the seed item and in the recommended item. A negative tag is one that must not occur in the textual aura of a recom-mended artist. Negative tags are used as a filter to remove artists whose aura contains the tag from the results of the similarity computation.

The overall effect of the steerability provided to the users is that they can begin to answer questions like  X  X an I find an artist like Britney Spears but with an indie vibe? X  or  X  X an I find an artist like The Beatles, but who is recording in this decade? X 
As part of the AURA project in Sun Labs, we have de-veloped the Music Explaura [2], an interface for finding new musical artists that incorporates all of the techniques de-scribed above. Users can start out by searching for a known artist, by searching for an artist with a particular tag in their aura, or by searching for a tag and seeing similar tags.
The aim is to provide as much context as possible for the user. In addition to the tag cloud for the artist (based on Last.fm tags by default) and the top 15 most-similar artists to the seed artist, we display a portion of the artist X  X  biography from Wikipedia, videos crawled from YouTube, photos crawled from Flickr, albums crawled from Amazon, and events crawled from Upcoming.
To get a better understanding of how well the aura-based recommendations perform, we conducted a web-based user survey that allowed us to compare the user reactions to recommendations generated by a number of different rec-ommenders. We compared two of our research algorithms, our aura-based recommender and a more traditional collab-orative filtering recommender, to nine commercial recom-menders. The aura-based system used a data set [4] con-sisting of about 7 million tags (with 100,000 unique tags) that had been applied to 21,000 artists. The CF-based sys-tem generated item-item recommendations based on the lis-tening habits of 12,000 Last.fm listeners. The nine com-mercial recommenders evaluated consist of seven CF-based recommenders, one expert-based recommender and one hy-brid (combining CF with content-based recommendation). We also included the recommendations of five professional critics from the music review site Pitchfork [3].
To evaluate the recommenders we chose the simple rec-ommendation task of finding artists that are similar to a single seed artist. This was the only recommendation sce-nario that was supported by all recommenders in the survey. We chose five seed artists: The Beatles, Emerson Lake and Palmer, Deerhoof, Miles Davis and Arcade Fire. For each recommender in the study, we retrieved the top eight most similar artists. We then conducted a Web-based survey to rank the quality of each recommended artist. The survey asked each participant in the survey to indicate how well a given recommended artist answers the question  X  X f you like the seed artist you might like X . X  Participants could answer  X  X xcellent X   X  X ood X   X  X on X  X  Know X   X  X air X  or  X  X oor X . Two Table 1: Survey Results. CF-X represents a col-laborative filtering-based system with an estimated number of users X hundred individuals participated in the survey, contributing a total of over ten thousand recommendation rankings. We used the survey rankings to calculate three scores for each recommender: Average Rating The average score for all recommenda-Relative Precision The average score for all recommen-Novelty The fraction of recommendations that are unique
Table 1 shows the results of the survey. Some observations about the results: CF-based systems with larger numbers of users tend to have higher average ratings and relative pre-cision. Somewhat surprisingly, human-based recommenders (Expert and Music Critic) do not rate as well as larger CF-based recommenders, but do tend give much more novel rec-ommendations. Poorly rated recommenders tend to have a higher novelty score. The aura-based recommender provides good average rating and relative precision results while still providing somewhat novel recommendations.

It is important not to draw too many conclusions from this exploratory evaluation. The recommendation task was a simple one using popular artists, the number of participants was relatively small and the participants were self-selected. Nevertheless, the survey does confirm that the aura-based approach to recommendation is a viable approach yielding results that are competitive with current commercial sys-tems.
In order to gain preliminary insight into the effectiveness of our approach to transparent, steerable recommendations, we conducted a small sca le qualitative usability evaluation using The Music Explaura, contrasting it against other mu-sic recommendation sites.

We asked users to evaluate The Explaura X  X  recommen-dations with respect to the familiarity and accuracy of the recommendations, their satisfaction with the recommenda-tions, how novel the recommended artists were, the trans-parency and trustworthiness of the recommender, and the steerability of the recommendations.

Ten participants were recruited from the student popula-tion of Bentley University and musician forums on craigslist. The participants were all web-savvy, regular Internet users.
The Explaura interface presents a number of interaction paradigms very different from familiar interaction conven-tions, creating a pronounced learning curve for new users. Users X  ratings of different sites X  recommendations appeared to largely correspond to how much they agreed with the rec-ommendation of artists that were familiar to them. Included in this assessment of the quality of the recommendations was often an assessment of rank.

Ten of ten users agreed, though not always immediately (some required prompting to investigate the similarity tag clouds) that they understood why Explaura recommended the items it did, and that the list of recommendations made sense to them.

It was clear that the value of the tag clouds as an expla-nation is highly vulnerable to any perception of inaccuracy or redundancy in the tag cloud. Also, sparse tag clouds pro-duce more confusion than understanding. Almost none of the participants immediately grasped the meaning of the tag clouds. The general first impression is of confusion. Further-more, no one expected to be able to manipulate the cloud.
Despite these problems, the users continually expressed the desire to limit and redefine the scope of an exploration. When the concept is presented, virtually all users express surprise, interest, and pleasure at the idea that they can do something with the results.
The current system uses the traditional bag-of-words model for the tags. While this has provided some worthwhile re-sults, it seems clear that we should be clustering terms like canada and canadian so that their influence can be com-bined when generating recommendations and explanations. At the very least, combining such terms should lead to less confusion for users.

We X  X e interested in generating more language-like descrip-tions of the similarities and differences between items. It seems like it should be possible to use the term weight-ings along with language resources like WordNet to generate Pandora-like descriptions of the similarity tag clouds, which may make them more approachable for new users.

Our ultimate aim is to provide hybrid recommendations that include the influence of the textual aura as well as that of collaborative filtering approaches. An obvious problem here is how to decide which approach should have more in-fluence for any given user or item.

The textual aura provides a simple representation for items that can produce novel recommendations while providing a clear path to a Web-scale recommender system. The ini-tial evaluation of the quality of the recommendations pro-vided by an aura-based recommender provides strong evi-dence that the technique has merit, if we can solve some of the user interface problems.

Our initial usability study for the Music Explaura showed that the tag cloud representation for the artist can be con-fusing at first viewing. While the users expressed a real desire to explore the recommendation space via interaction with the system, the current execution needs further usabil-ity testing and interface design refinement to enhance user acceptance of the model.
 Thanks to Professor Terry Skelton of Bentley University. Susanna Kirk (Bentley University, kirk_susa@bentley.edu ), Jessica Holt (Bentley University, holt_jess@bentley.edu ), Jackie Bourque (Bentley University, bourque_jacq@bentley.edu ), Xiao-Wen Mak (Bentley University, mak_xiao@bentley.edu ) [1] Last.fm. http://last.fm . [2] The Music Explaura. http://music.tastekeeper.com . [3] Pitchfork magazine. [4] P. Lamere. Last.fm artist tags 2007 data set. [5] N. Tintarev and J. Masthoff. A survey of explanations [6] J. Vig, S. Sen, and J. Riedl. Tagsplanations: explaining [7] R.Wetzker,W.Umbrath,andA.Said.Ahybrid [8] V. Zanardi and L. Capra. Social ranking: uncovering
