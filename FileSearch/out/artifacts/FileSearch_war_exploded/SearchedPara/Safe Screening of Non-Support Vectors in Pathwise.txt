 Here, we prove the theorem by explicitly deriving our non-SV screening rule in section 3.
 The proof First, we proove (15) and (16) by solving the minimization problem in (13) with the Lagrange multiplier method. The minimization problem can be calculated as follows: where  X  0 and  X  0 are the Lagrange multipli-grangian: At the optimal solution, we must satisfy Since the Lagrange multiplier  X  0, we can write In this case, noting that the Lagrange multiplier  X  0, we have In this case, we have marized as bound min as a non-SV. It suggests that we can only consider the non-negative. It means that we have a chance to screen is why the screening rule for R has the form in (15) and (16).
 Next, we prove (15) and (16). In the same way, we can write the upper bound of y i f ( x i ) as follows: Here, we might have a chance to screen out the i th second squared-root term is non-negative. However, for notational simplicity, we just write the screening rule for L in the form of (17) and (18). Q.E.D. and submatrices. For example, v A for a vector v in-dicates a subvector of v having only the elements in submatrix of M having only the rows in the index set A and the columns in the index set B . In addition, we write 1 n to represent an n -dimensional vector of 1s. In order to prove Lemmas 3, we first summarize the op-based on them, we clarify the sensitivity of C in C -form to s in s -form.
 Proposition 4 Consider the optimal solution f (  X | s ) problem is formulated as max where  X  R n and C  X  R is the Lagrange multipliers. is written as If we de ne the following three index sets: then, the Lagrange multipliers  X  R n satisfy where, remember that, C is the optimal Lagrange mul-tiplier for the constraint (7b) which is also obtained after solving the optimization problem (27). At the optimal solution, unless all the training in-(7b) is active, i.e., We omit the proof of this proposition because it is easily shown by using standard Lagrange multiplier theory (Boyd &amp; Vandenberghe, 2004).
 The Proof In order to prove the lemma, we derive the optimality conditions of the problem (20) at s . Given the three index set L , E , R in (30), the opti-mality conditions of the problem are written as If we rewrite these conditions in matrix vector form, we have From (33c), Substituting (34) into (33d), we have
C =  X  plement of the block Q EE of a positive semi-definite matrix berghe, 2004). It indicates that It suggests that, as long as the three index sets L ;
E ; R are unchanged, C is linearly decreasing with s . When one or more instances move from one index set to the other, then, the decreasing rate piecewise-linear function of s and each linear segment has non-positive slope, meaning that C is monotoni-cally decreasing with s . Q.E.D.
 Here, we provide the pseudo-codes of FindSb and Com-puteSubPath functions in Algorithm 2 and some ad-ditional information about the algorithm.
 Algorithm 3 FindSb 2: Output: s b , w ( s b ); 4: isSbSafe  X  false ; 5: while isSbSafe = false do 6: Compute  X  w ( s b ) by solving (21); 10: if isOpt then 13: isSbSafe = true ; 14: else 15: s b  X  DecreaseSb ( s b ); 16: end if 17: end while Algorithm 4 ComputeSubPath 3: u  X  u 1 4: while u  X  u 2 do 6: u  X  u + 1; 7: end while  X  We need to plug-in SVMSolver that can solve  X  At line 4, the optimal solution at C (1) is computed  X  At line 11, FindSb function is called. This func- X  At line 12, we construct a non-SV screening rule
