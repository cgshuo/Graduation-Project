 A key challenge in recommender system research is how to effectively profile new users, a problem generally known as cold-start recommendation. Recently the idea of progres-sively querying user responses through an initial interview process has been proposed as a useful new user preference elicitation strategy. In this paper, we present functional ma-trix factorization (fMF), a novel cold-start recommendation method that solves the problem of initial interview construc-tion within the context of learning user and item profiles. Specifically, fMF constructs a decision tree for the initial in-terview with each node being an interview question, enabling the recommender to query a user adaptively according to her prior responses. More importantly, we associate latent pro-files for each node of the tree  X  in effect restricting the latent profiles to be a function of possible answers to the interview questions  X  which allows the profiles to be gradually refined through the interview process based on user responses. We develop an iterative optimization algorithm that alternates between decision tree construction and latent profiles ex-traction as well as a regularization scheme that takes into account of the tree structure. Experimental results on three benchmark recommendation data sets demonstrate that the proposed fMF algorithm significantly outperforms existing methods for cold-start recommendation.
 H.3.3 [ Information Search and Retrieval ]: Information filtering; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance, Experimentation Recommender systems, Cold-start problem, Collaborative filtering, Decision tree, Functional matrix factorization
Recommendation systems have become a core component in today X  X  online business world. While E-commerce giants such as Amazon have been greatly benefitted from the abili-ties of their online recommenders in effectively delivering the right items to the right people, online startups are increas-ingly becoming aware of this secret and aggressively sharp-ening their weapons in order to compete. A key challenge for building an effective recommender system is the well-known cold-start problem  X  how to provide recommendations to new users? While existing collaborative filtering (CF) ap-proaches to recommendation perform quite satisfactorily for warm-start users (e.g. users purchasing a lot from an on-line retailer), it could fail completely for fresh users, simply because the system knows very little about those users in terms of their preferences.

Providing effective cold-start recommendations is of fun-damental importance to a recommender system. Offering bad recommendations could risk severely the webshop X  X  rev-enue, market share and overall reputations, because disap-pointed customers may turn to other competitors and never come back. This problem is more acute for newly-launched recommender systems (e.g. online startups) because their ex-isting user base is small and attracting more new customers are crucial to their survival. Even for relatively mature rec-ommender systems, a vast majority of the (potential) cus-tomers are actually cold-start users  X  in principle, the popu-lation of customers typically follows a power-law distribution w.r.t. the participation frequency such that most customers lie in the long-tail part.

A natural approach to solving the cold-start problem is to elicit new user preferences by query users X  responses progres-sively through an initial interview process [19]. Specifically, at the visit of a new user, the recommender initiates sev-eral trials. At each trial, the recommender provides a seed item as a question and ask the user for her opinion; based on the user X  X  responses to the questions, the recommender gradually refines its characterization (i.e. profile) of the user so that it can provide more satisfactory recommendations to the user in the future. A good initial interview process should not be time-consuming: the recommender can only ask a very limited number of questions or otherwise the user will become impatient and leave the system. Equally im-portant, the process should also be effective, i.e. the answers collected from the user should be useful for constructing at least a rough profile for the user. It has been convincingly argued that an effective model for designing the interview process should ask the interview questions adaptively, taking i nto account the user X  X  responses when asking the next ques-tion in the interview process, and decision trees have been found to work especially well for this purpose [7,19,20]. As an illustration of movie recommendation, in an interview process depicted in Figure 1, the system asks a new user the question  X  X o you like the movie Lethal Weapon 4? X . The user is allowed to answer with either X  X ike X , X  X islike X  X r X  X n-known X . The recommender refines its impression about the user and then direct the user to one of the child nodes and then presents the next interview question according to her previous response. For example, if a user chooses  X  X ike X  for  X  X ethal Weapon 4? X  at the root node of Figure 1, she will be directed to the left child node and will be asked the question  X  X o you like the movie Taxi Driver? X 
In this paper, we propose functional matrix factorization (fMF), a novel method for the construction of such inter-view decision trees. We argue that it is more effective to combine the matrix factorization model for collaborative fil-tering and the decision-tree based interview model into a single framework. Our proposed method is based on the low rank factorization of the incomplete user-item rating matrix with an important twist: it restricts the user profiles to be a function of the answers to the interview questions in the form of a decision tree, thus the name functional matrix fac-torization. The function  X  playing the role of the initial interview  X  is in the form of a decision tree with each node being an interview question [7, 20]. Both the decision tree and the item profiles need to be learned; for that we propose an iterative optimization algorithm that alternates between decision tree construction and latent profiles extraction.
Our proposed method tends to explore more effectively the correlation between different items. In particular, the low dimensional user profiles at each node of the decision tree and the item profiles adapt to each other to better fit the training data. Thus, correlation between different items is captured by the low dimensional profiles. As a result, the prediction of ratings can be enhanced by making use of the ratings of different items. Experimental results on three benchmark recommendation data sets, the MovieLens data set, the EachMovie data set and the Netflix data set, demonstrate that the proposed fMF algorithm significantly outperforms existing methods in cold-start recommendation.
Outline : In Section2, we briefly review existing studies for cold-start collaborative filtering. In Section 3, we first introduce matrix factorization for collaborative filtering and then present functional matrix factorization for constructing the interview process for cold-start collaborative filtering by restricting the user profiles to be a function in the form of a decision tree. The learning algorithm based on alterna-tive optimization is then proposed with detailed derivations. Then, we evaluate the proposed method on three data sets and analyze the results in Section 4. Finally we conclude our work and present several future directions in Section 5.
Many studies on recommender systems have been focused on collaborative filtering approaches. These methods can be categorized into memory-based and model-based. The reader is referred to the survey papers [1,26] for a compre-hensive summary of collaborative filtering algorithms.
Recently, matrix factorization becomes a popular direc-tion for collaborative filtering [12,14,16,21,24]. These meth-ods are shown to be effective in many applications. Specifi-cally, matrix factorization methods usually seek to associate both users and items with latent profiles represented by vec-tors in a low dimension space that can capture their char-acteristics. In [22], a convex relaxation for low rank matrix factorization is proposed and applied to collaborative filter-ing. A probabilistic model for extracting low dimensional profiles is studies in [12], in which latent variables are intro-duced to capture the users and items and the EM algorithm is used to estimate the parameters. More recently, fueled by the Netflix competition, several improvements have been proposed including the use of regularized SVD [16], and the idea of matrix factorization combined with neighborhood-based methods [14].
There have been several studies on the elicitation strate-gies for new user preferences. The work [18] surveys sev-eral guidelines for user preference elicitation. Several mod-els such as [9,19,20] constructed the interview process with a static seed set selected based on measures such as infor-mativeness, popularity and coverage. The recent work of [7] proposed a greedy seed selection algorithm by optimizing the prediction performance. Such seed-based methods are not completely satisfactory because the seeds are selected stat-ically in batch, and they do not reflect the user responses during the interview process. In [20], while discussing the IGCN algorithm, it was mentioned the idea of adaptively selecting the interview questions by fitting a decision tree to predefined user clusters. In particular, each node represents an interview question and the user is directed to one of the child nodes according to her response to the parent ques-tion. In [8], the authors proposed an algorithm that fits the decision to the users X  ratings. This seems to provide a more disciplined approach than that based on the predefined user clusters discussed in IGCN. Our proposed framework goes one step further by integrating the decision tree construction into the matrix factorization framework of collaborative fil-tering. We should also mention that the decision tree struc-ture used in those methods exhibit interesting resemblance to the Bayesian network approach in [5].

A complimentary line of research for solving the cold-start problem is to utilize the features of users or items. The con-tent features can be used to capture the similarities between users and items and thus reduce the amount of data required to make accurate predictions [2,3,10,15,23,25]. For exam-ple, the work of [2] utilizes features of items and users as the prior distribution for latent profiles in matrix factorization. Our method is based on the interview process and does not solely rely on features of users and items.

We also want to mention active learning for collaborative filtering [4,11,13,17]. These methods usually select questions that are optimal with respect to some selection criteria, such as the expected value of information or the distance to the true user model. These methods generally are not suitable for interview process since the selection process usually in-volves optimizing the selection criteria, which is not efficient enough for online interview.
In this section, we describe the functional matrix factor-ization (fMF) method for cold-start collaborative filtering which explores the well-known matrix factorization methods for constructing the interview process. The key innovation is that we parameterize user profiles to be a function of the responses to the possible questions of the interview process and use matrix factorization to compute the profiles.
Before describing fMF, we first review the matrix factor-ization method for collaborative filtering. Let r ij denote the rating of user i for item j , where i = 1 , 2 , . . . , N and j = 1 , 2 , . . . , M . In practice, only a small subset of ratings are observed, denoted by K = { r ij | ( i, j )  X  O } . The goal of collaborative filtering is to predict the unknown ratings based on ratings in K . Collaborative filtering exploits a ba-sic heuristic that similar users tend to rate similar items in a similar way. One important class of methods for collab-orative filtering are based on matrix factorization [12, 24]. Specifically, we associate a K -dimensional vector u i  X  R for each user i and v j  X  R K for each item j . The vectors u i and v j are usually called user profiles and item profiles since they are intended to capture the characteristics of users and items. The rating r ij of user i for item j can be ap-proximated by a similarity function of u i and v j in the low dimensional space, for example, r ij = u T i v j .
Given the set of known ratings K , the parameters u i and v j can be estimated through fitting the training data by solving the following optimization problem: Regularization terms such as the Frobenius norms on the profile vectors can be introduced to avoid overfitting. The problem can be solved by existing numerical optimization methods such as alternating minimization and stochastic gradient descent. In our implementation, we use the al-ternating optimization for its amenability for the cold-start settings. Specifically, the optimization process performs the following two updates alternatively.

First, for i = 1 , 2 , . . . , N , minimizing with respect to u with all u j , j 6 = i and all v j fixed: which is a linear regression problem with squared loss. The closed form solution can be expressed as Then, for j = 1 , 2 , . . . , M , minimizing with respect to v with all v i , i 6 = j and all u i fixed: which is also a linear regression problem with similar closed-form solution.
Now we consider constructing the interview process for cold-start collaborative filtering. Assume that a new user registers at the recommendation system and nothing is known about her. To capture the preferences of the user, the system initiates several interview questions to query the responses from the user. Based on the responses, the system con-structs a profile for the user and provides recommendations accordingly.

In the plain matrix factorization model described in Sec-tion 3.1, the user profile u i is estimated by optimizing the  X  loss on the history ratings r ij . This model does not directly apply to cold-start settings because no rating is observed for the new user prior to the interview process. To build user profiles adaptively according to the user X  X  responses in the course of the interview process, we propose to param-eterize the user profile u i in such a way that the profile u is tied to user i  X  X  responses in the form of a function, thus the name functional matrix factorization (fMF). More pre-cisely, assume there are P possible interview questions . We assume that an answer to a question takes value in the fi-nite set { 0 , 1 , Unknown } , representing  X  X islike X ,  X  X ike X  and  X  X nknown X , respectively. Furthermore, let a i denote the P -dimensional vector representing the answers of user i to the P questions. And we tie the profile to the answers by as-suming u i = T ( a i ), where T is a function that maps the responses a i to the user profile u i  X  R k . To make recom-mendations for user i , we simply use r ij = v T j T ( a i
Our goal is to learn both T and v j from the observed ratings K . To this end, substituting u i = T ( a i ) into the low-rank matrix factorization model, we have the following optimization problem: where V = ( v 1 , . . . , v M ) is the matrix of all item profiles, H is the space from which the function T ( a ) is selected and the second term is the regularization term.

Several issues need to be addressed in order to construct the interview process by the above functional matrix factor-ization. First, the number of all possible interview questions can be quite large (e.g. up to millions of items in movie rec-ommendation); yet a user is only patient enough to answer a few interview questions. Second, the interview process should be adaptive to user X  X  responses, in other words, a follow-up question should be selected based on the user X  X  re-s ponses to the previous questions. Therefore, the selection process should be efficient to generate interview questions in real time after the function T ( a ) is constructed. In addition, since we allow a user to choose  X  X nknown X  to the interview questions, we need to deal with such missing values as well.
Following prior works of [8,20], we use a ternary decision tree to represent T ( a ). Specifically, each node of the deci-sion tree corresponds to an interview question and has three child nodes. When the user answers the interview question, the user is directed to one of its three child nodes according to her answer. As a result, each user follows a path from the root node to a leaf node during the interview process. A user profile is estimated at each leaf node based on the users X  responses, i.e., T ( a ). The number of interview ques-tions presented to any user is bounded by the depth of the decision tree, generally a small number determined by the system. Also, non-responses to a question can be handled easily in the decision tree with the introduction of a  X  X n-known X  branch.
The objective function defined in Eq. (1) can be optimized through an alternating minimization process. Specifically, we alternate between the following two steps: 1. Given T ( a ), we can compute v j by regularized least v =  X   X  2. Given v j , we try to fit a decision tree T such that A critical challenge is that the number of possible trees grows exponentially with the depth of the trees, which can be ex-tremely large even for trees of limited depth. It is therefore computationally extensive to obtain a global optimal solu-tion for the above. We address this problem by proposing an efficient greedy algorithm for finding an approximate so-lution.
Traditional decision tree algorithms such as C4.5 or CART [6] usually minimize objective functions such as classification error and square loss for regression. In our scenario, the objective function is defined in Eq. (3), and we now describe how to build a decision tree in a greedy and recursive fashion to minimize it. Specifically, at each node, we select the best interview question by optimizing the objective defined in Eq. (3) based on the response to this question; the decision tree then splits the user set into three subsets corresponding to the child nodes (i.e.  X  X ike X ,  X  X islike X  and  X  X nknown X ). We carry out this procedure recursively until the tree reaches certain depth. In our experiments, the depth is usually set to a small number between 3 and 7.

Formally, starting from the root node, the set of users at current node are partitioned into three disjoint subsets R
L ( p ), R D ( p ) and R U ( p ) corresponding to  X  X ike X ,  X  X islike X  and  X  X nknown X  of their responses to the interview question p : To find the optimal question p that leads to the best split, we minimize the following objective: min where u L , u D and u U are the optimal profiles for users in the child nodes corresponds to the answers of  X  X ike X ,  X  X islike X  and  X  X nknown X , respectively: There also exist closed-form solutions to u L , u D and u follows:
After the root node is constructed, its child nodes can be constructed in a similar way, recursively. We summarize our algorithms for functional matrix factorization and decision tree construction in Algorithm 1 and Algorithm, 2 respec-tively.
 Implementation and Computational Complexity.
 The time complexity for computing v j with Equation (2) is O ( M K 3 ), where M is the number of items and K is the di-mension of the latent space. In order to compute u L , u D u
U at each split, we need to compute the inverse of a square matrix of size K , which takes O ( K 3 ) time. The brute-force approach for generating the matrix itself requires O ( U M K time. In order to reduce the time complexity, we observe that the coefficient matrix can be computed based on the sum of ratings, the sum of squares of ratings and the num-ber of ratings for each item within time O ( M K 2 ). With a similar method proposed in [8], the time complexity of com-puting these statistics of all items is O ( P N 2 i ) for each level of the decision tree, where N i is the number of ratings by the user i . Thus, the computation complexity for constructing Algorithm 1 A lternating Optimization for Functional Ma-trix Factorization Require: T he training data K = { r ij | ( i, j )  X  O } . Ensure: Estimated decision tree T ( a ) and item profiles 1: Initialize v j randomly for j = 1 , . . . , M . 2: while not converge do 3: Fit a decision tree T ( a ) using Algorithm 2. 4: Update v j with Equation (2). 5: end while 6: return T ( a ) and v j , j = 1 , 2 , . . . , M . Algorithm 2 Gr eedy Decision Tree Construction 1: f unction FitTree(UserSet) 2: // UserSet: the set of users in current node. 3: Calculate u L , u D , u U by Equation (5), (6) and (7) for 4: Compute the split criteria L p by Equation (4) for p = 5: Find the best interview question p = argmax p L p . 6: Split user into three groups R L ( p ), R D ( p ) and R 7: if square error reduces after split and depth &lt; 8: Call FitTree( R L ( p )), FitTree( R D ( p )) and 9: end if 10: return T ( a ). 11: end function the decision tree is O ( D P i N 2 i + L M K 3 + LM 2 K 2 D is the depth of the tree and L represents the number of nodes in the tree. The computation time can be reduced by selecting a subset of items based on some criteria such as the rating variance, and using only the selected items as candidates for the interview questions.
In the above section, u L , u D and u U for each node are estimated by linear regression. When the amount of train-ing data is small, the estimate may overfit the training data. The problem becomes more severe especially for the nodes close to the leaves of the decision tree, because as the split process progresses, users belonging to those nodes become fewer and fewer. To avoid overfitting, regularization terms need to be introduced. Although traditional  X  2 regulariza-tion can be applied in this case [16], but such approach does not take into account the rich structure of the decision tree.
Here, we propose to apply hierarchical regularization that utilize the structure of the decision tree. Specifically, we shrink the coefficient u of a node toward the one at its parent node. For example: u L = argmin where u C is the estimation at the current node and u L is the estimation at its child node corresponding to the answer  X  X ike X  and the parameter  X  h controls the trade-off between training error and regularization. We use  X  h = 0 . 03 in our experiments which seems to give good results. The similar idea using parent node as the prior for the child node is also studied in [8], but the ratings for the parent node is used to improve the prediction in the child node.

I n order to evaluate the proposed method for cold-start collaborative filtering, we carry out a set of controlled ex-periments on three widely used benchmark data sets: Movie-Lens, EachMovie and Netflix.
We first briefly describe the data sets.
The performance of an collaborative filtering algorithm will be evaluated in terms of the widely used root mean square error (RMSE) measure, which is defined as follows where T represents the set of test ratings, r ij is the ground-truth values for movie j by user i and  X  r ij is the predicted value by a collaborative filtering model. We can also nor-malize the error of each user by the number of their ratings. Our initial results show that the two metrics are consistent in general, so we only report RMSE in our experiments.
The responses a i of the interview process for user i is re-quired in order to train and evaluate the interview process. Following the standard settings [8,20], we restrict the format of interview questions to be X  X o you like item j ? X  For exam-ple, for movie recommendation system, we can ask questions like  X  X o you like the movie Independence Day? X  Then, we can infer the responses for existing users according to their ratings for the corresponding items. For instance, with rat-ings in 1-5 scale, we assume that the responses a ij of user i to the question  X  X o you like item j ? X  can be inferred from her rating r ij as follows: h ttp://www.grouplens.org/node/73 http://www.netflixprize.com/ Intuitively, we assume that the user will response 0 ( d islike ) and 1 ( like ) for items she rates with  X  3 rating or &gt; 3 rating. If the user did not rate an item selected for the interview process, we assume that she will respond with unknown . For EachMovie data set, we use a similar method except setting the threshold to be 4 since its ratings range from 1 to 6.
We seek to address the following questions: 1. For new users, how does the proposed algorithm per-2. We utilize the ratings from users to simulate their re-3. How does fMF perform in warm-start settings com-4. How do the parameters impact the performance of the
We first evaluate the performance of fMF in cold-start set-tings. For each data set, we split the users into two disjoint subsets, the training set and the test set, containing 75% and 25% users, respectively. The users in the training set are assumed to be warm-start users whose ratings are known by the system. We learn the models and construct the in-terview process based on these training users. In contrast, the users in the test set are assumed to be cold-start users. The ratings of each user in the test set are partitioned into two sets: the first set is called the answer set which is used to generate the user responses in the interview process while the second set is called the evaluation set which is used to evaluate the performance after the interview process. The sizes of the answer set and the evaluation set are 75% and 25% of the rated items of each user, respectively. Note that the interview process typically includes only a small number (  X  7 in our experiments) of questions although the answer set contains 75% of the ratings when deriving the responses for the cold-start users. The evaluation process is summa-rized in Figure 2.

We compare the performance with two baseline methods, named as Tree and TreeU and briefly described as follows: Figure 2: Evaluation process for cold-start users Ou r proposed fMF algorithm differs from these two algo-rithms in that it is a natural integration of both decision tree and matrix factorization.

We use the following parameter settings: For TreeU , we use as default the regularization weight of  X  = 0 . 01. For Tree and fMF , we apply 4-fold cross validation to determine the parameters.

The results on MovieLens, EachMovie and Netflix data sets are reported in Table 2, Table 3 and Table 4, respec-tively. Our first observation is that, as expected, the perfor-mance is gradually improved (i.e. the RMSE decreases) as the number of interview questions increases. This is true for all three methods, suggesting that these three algorithms are all capable of refining user preference through the interview process. Therefore, all the three methods can be applied to solve the cold-start problem.

Comparing the performance of fMF and Tree , we can see that fMF consistently outperforms Tree in all the three data sets. The improvements are significant according to t -test with significance level p = 0 . 05. This observation illustrate that the interview processes learned by fMF is more effective than those by Tree . We attribute this to the fact that fMF naturally integrates the matrix factorization model into the interview process for cold-start collaborative filtering. In particular, fMF inherits the ability of matrix factorization models in collaboratively uncovering the user-item ratings. In contrast, Tree assumes that users/items are independent from one another, and therefore cannot capture the vital collaborative effect.

We can also see that TreeU does not work well, too. Re-call that TreeU is a two-stage method  X  It first extracts the user profiles and item profiles using plain matrix factoriza-tion, and then constructs a decision tree on the answers of the interview questions. A possible reason for why it does not perform well is that the user profiles and item profiles obtained from the matrix factorization in warm-start setting usually capture the refined preferences of users and refined characteristics of items. However, in cold-start setting, we are only allowed to ask users a few questions so that the model usually captures only very coarse interests of users. As a result, TreeU usually fails to fit the user profiles from matrix factorization accurately. Thus, its prediction accu-racy is relatively low. On the other hand, our method es-timates the decision tree and item profiles simultaneously in a combined optimization process. Thus, the user profiles obtained by decision tree and the item profiles can adapt to each other and improve the prediction accuracy.

To provide more comprehensive views of the interview pro-Table 2: RMSE on MovieLens data set for cold-start u sers with respect to the number of interview questions
No. Questions 3 4 5 6 7 Table 3: RMSE on EachMovie data set for cold-start u sers with respect to the number of interview questions
No. Questions 3 4 5 6 7 Table 4: RMSE on Netflix data set for cold-start user w ith respect to the number of interview questions
No. Questions 3 4 5 6 7 Table 5: Examples of interview process with 6 ques-t ions
Rank Movie Title cess, we further look into particular cases. In Table 5 and T able 6, we present the interview process for users in Netflix data set as well as the top-5 recommendations for them af-ter the interview process. From Table 5, we can see that the user chooses  X  X ike X  on the movie Armegeddon and Reser-voir Dogs, which belong to Fiction and Adventure movies. The recommendation includes Lord of the Rings series and Star Wars. They are quite related to the movie Armeged-don based on their genres. Similarly, the interview process of Table 6 shows that the user likes Drama and Romance movies and the top recommendations for her contain both those two types of movies. Those results illustrate that the recommendations generated by fMF are indeed reasonable.
In our experiments, we utilize the ratings of users to sim-ulate their responses to the interview questions as described in Section 4.2 since the users X  responses to the interview questions are not available for the benchmark data sets. In particular, we assume that the user will respond  X  X nknown X  Table 6: Examples of interview process with 6 ques-tions
Rank Movie Title Figure 3: Performance measured by RMSE on Net-fl ix data set with respect to the fraction of users with the most known ratings. to an interview question if she does not rate the correspond-ing item. This assumption, however, might be inaccurate in practice. For example, a user X  X  rating to an item might be missing simply because she does not have time to rate it. In this case, the user may actually responds with  X  X ike X  or  X  X islike X  rather than  X  X nknown X  if she is asked to respond to the interview questions. As a result, the missing values in data sets may introduce bias in the training and interview process. 3
Here, we explore how the missing ratings influence the initial interview process. We investigate the performance of the proposed method on users with different numbers of ratings. To this end, we sort users in the test set by the their numbers of ratings in descending order and then plot, in Figure 3, the performance measured by RMSE with respect to the fraction of users with the most ratings. We can see that the RMSE increases when more users with few ratings are included, which indicates that the performance for users with more ratings is better then the ones with less ratings. This is because that the users with more ratings are less likely to select  X  X nknown X  in our simulation. Thus, they are less likely prone to the influences by the bias introduced by the missing values in the training data.

With this basic understanding, we carry out a set of exper-iments to investigate the impact of missing values in training
T he bias in training data seems to have been largely ignored in previous work. Figure 4: Performance measured by RMSE on three d ata sets with respect to the fraction of users with the most known ratings. data. For each of the three data sets, we sort users in train-ing set in descending order. Then, the users in the test set (labeled by MT) is sampled from top 20% users with the most ratings. We only consider the users with sufficiently large number of ratings since we would like to rule out the impact of bias during the interview process and focus on the missing values for training users. We generate five train-ing sets, namely M1 to M5, from the rest of the users in the training set, including the top 20%, 40%, 60%, 80% and 100% of the users that are not selected in the test set, re-spectively. Again, 75% ratings for each user in MT is used to simulate the interview process and the performance is evaluated on the remaining 25% ratings.

We perform experiments by training the proposed models on M1 to M5 and evaluate them on MT for all three data sets. The results are shown in Figure 4. It can be observed from Figure 4 that the prediction error measured by RMSE increases when more users with few training data are in-cluded in the training set. This observation suggests that the performance are indeed affected by the bias introduced by the missing values in training set.
In order to answer the third question proposed in Section 4.3, we evaluate the proposed algorithm in warm-start set-tings and show the relative performance between the cold-start methods and warm-start methods. In particular, we consider the matrix factorization method described as fol-lows: We include this method to present a concrete comparisons of the relative performance of the proposed cold-start method and the warm-start methods that are widely studies in pre-vious researches. We compare the proposed algorithm ( fMF ) with all three methods for warm-start settings: Tree , TreeU and MF . To this end, we perform experiments with differ-ent depths of decision trees and report the RMSE of the all the methods over three data sets. For MovieLens and Each-Movie data set, we randomly split the ratings for each users into training set and test set and perform 4-fold cross valida-tion. For Netflix data set, we follow the popular evaluation protocol on this data set. Therefore, we train our models on the training set and report the performance on the test set. The performance measured by RMSE with respect to the depths of decision trees is reported in Table 7, Table 8 and Table 9.

We can observe that all the cold-start methods perform worse than the matrix factorization method. This observa-tion is consistent with our expectation because all the three cold-start algorithms are constrained in some ways in or-der to deal with cold-start users  X  the goal of the cold-start algorithms is to provide cold-start users with reasonable pre-dictions within a few quick interview questions. For exam-ple, the fMF model is restricted in its maximum depth. We should expect comparable performance, if this constraint is eliminated.

As an empirical validation, we further carry out experi-ments on the MovieLens data set by fitting the decision trees in our model with relatively larger depth. We note that the decision trees of very large depth correspond to interview processes with many questions, which are not proper for initial interview process for cold-start users since users are typically not willing to answer many questions. However, we can show that the performance of the proposed method can be quite close to the matrix factorization method if we relax the constraint and allow the model to use decision trees with large depth. We report the RMSE of fMF with respect to the depth of the decision tree in Figure 5. We also include Tree and MF for comparison. We can see that the RMSE of fMF monotonically decreases as the depth of decision trees increases. Moreover, its performance can be quite close to the matrix factorization method ( MF ). When the depth of the trees grows, the number of leaf nodes increases. There-fore, there are only a few users at each node. Thus, the user profiles estimated by fMF can be quite close to those by MF . We conclude that our method can be a reasonably good method of general collaborative filtering as well. On the other hand, we can see that the performance of Tree is much worse than MF even with a large number of questions. Moreover, the gap between fMF and Tree becomes larger when the depth of the decision tree grows. This is because Tree predicts the ratings of different items independently at each leaf node, which is not a good model for collaborative filtering in general. We also perform similar experiments with the cold start setting. In this case, the RMSE of fMF decreases with the depth of the decision tree when the depth is not very large (e.g.  X  14). We emphasize that the depth of the decision tree is usually set to be a small number since we can not ask a lot of questions to users during the inter-view process.
There are several parameters that affect the performance of the proposed model. In this section, we carry out exper-iments to investigate the impacts of these parameters. We only report the results on MovieLens data set as the observa-tions are similar when other data sets are used. By default, the cold-start setting described in Section 4.4 is applied un-less otherwise stated.

The parameter K controls the dimension of the user pro-files and item profiles for the matrix factorization model. We fix the depth of the decision tree to be 6, and report the performance obtained with different K . The results are depicted in Figure 6. Particularly, as K increases, the factor-ization model becomes more and more flexible, as a results, Figure 5: RMSE of f MF , Tree and MF on MovieLens data set with respect to the number of interview questions Table 7: RMSE on MovieLens data set in warm-start setting
No. Questions 3 4 5 6 7 Table 8: RMSE on EachMovie data set in warm-start s etting
No. Questions 3 4 5 6 7 Table 9: RMSE on Netflix data set in warm-start set-t ing
No. Questions 3 4 5 6 7 the RMSE first decreases, and reaches the optima around K = 20; thereafter, the model becomes increasingly overpa-rameterized and the performance in turn starts to degrades. For example, the performance with K = 30 is worse than the that with K = 20. In principle, the optimal K should provide the best trade-off between fitting bias and model complexity.

One of our contributions is that we propose to use hierar-chical regularization to avoid overfiting. The impact of the hierarchical regularization is controlled by the parameter  X  We evaluate fMF with different values of  X  h on MovieLens data set. The performance obtained by  X  2 regularization is also reported for comparison. We can see from Figure 7 that the hierarchical regularization always performs better than the basic  X  2 regularization. This observation suggest that hi-erarchical regularization, by exploiting the structure of the decision tree, provides better regulatory to the functional matrix factorization model.

Another parameter of interest is the regularization weight  X  for the item profiles v j . We vary the value of  X  and report the performance measured by RMSE in Figure 8. We can see Figure 6: Performance measured by RMSE with re-s pect to different values of K on MovieLens data set. The performance is reported by setting the depth of the decision tree to be 6. Figure 7: The performance of hierarchical regular-i zation. The performance is reported by setting the depth of the decision tree to be 6. that the optimal performance is achieved when a moderate  X  is used. Especially, the model has a high risk of overfitting if  X  is too small.

The learning algorithm of the proposed model is an itera-tive process. In Figure 9, we plot the both training and test performance measured by RMSE with respect to the num-ber of iterations. We can see that the algorithm converges very quickly, usually within 5 iterations.
The main focus of this paper is on the cold-start problem in recommender systems. We have presented the functional matrix factorization, a framework for simultaneously learn-ing decision tree for initial interview and latent factors for user/item profiling. The proposed fMF algorithm seamlessly integrates matrix factorization for collaborative filtering and decision-tree based interview into one unified framework by reparameterizing the latent profiles as a function of user responses to the interview questions. We have established learning algorithms based on alternating minimization and demonstrated the effectiveness of fMF on real-world recom-mendation benchmarks.

The current fMF model is based on a basic matrix fac-torization formulation and does not take into account the content features such as the demographical information of users. For future work, we plan to investigate how to utilize such content features to further enhance the performance of fMF in cold-start recommendations. Moreover, we also plan to conduct in-depth examination on the missing value problem, which seems to introduce considerable bias to the learning process as revealed by our experiments. Figure 8: Performance measured by RMSE with re-s pect to different values of  X  on MovieLens data set. Figure 9: Performance measured by RMSE with re-s pect to the number of iterations on MovieLens data set.
We would like to thank the reviewers for their constructive and insightful comments. Part of this work was supported by NSF Grant IIS-1049694, a Yahoo! faculty grant and 111 Project B07022.
