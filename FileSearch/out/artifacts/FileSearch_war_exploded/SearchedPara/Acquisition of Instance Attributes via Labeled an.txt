 This paper presents a method for increasing the quality of automatically extracted instance attributes by exploit-ing weakly-supervised and unsupervised instance relatedness data. This data consist of (a) class labels for instances and (b) distributional similarity scores. The method organizes the text-derived data into a graph, and automatically prop-agates attributes among related instances, through random walks over the graph. Experiments on various graph topolo-gies illustrate the advantage of the method over both the original attribute lists and a per-class attribute extractor, both in terms of the number of attributes extracted per in-stance and the accuracy of top ranked attributes.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing; H.3.1 [ Information Storage and Retrieval ]: Con-tent Analysis and Indexing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Information extraction, instance attributes, unstructured text, distributional similarities, labeled instances Motivation : Due in part to the quantitative limitations of the then-available textual data sources, early work on in-formation extraction focuses on training supervised systems on small to medium-sized document collections, requiring relatively expensive manual annotations of data [5]. Some authors investigate the possibility of obtaining manual an-notations more easily, through the creation of manual or semi-automatic annotations [6]. But as larger amounts of textual data sources have become available at lower com-putational costs, either directly as document collections or indirectly through the search interfaces of the larger Web search engines, information extraction has seen a shift to-wards large-scale acquisition of open-domain information [8]. In this framework, information at mainly three levels of granularity is extracted from text, with weak or no super-vision: classes (having a label, e.g., painkillers ), class ele-ments or instances (e.g., vicodin , oxycontin ); and relations among instances (e.g., france -capital -paris ) or classes (e.g., countries -capital -cities ).

Among other types of relations targeted by various ex-traction methods, attributes (e.g., side effects and maxi-mum dose ) have emerged as one of the more popular types, as they capture properties of their respective classes (e.g., painkillers ), and thus serve as building blocks in many knowl-edge bases. Consequently, a variety of attribute extraction methods mine textual data sources ranging from unstruc-tured [23] or structured [26, 4] text within Web documents, to human-compiled encyclopedia [25, 7] and Web search query logs [17, 16], attempting to extract, for a given class, a ranked list of attributes that is as comprehensive and ac-curate as possible.
 Contributions : This paper introduces a method that ac-quires instance relatedness information, and applies it for attribute extraction, in order to produce ranked lists of at-tributes of higher coverage and accuracy than current state of the art. The algorithm proposed is general enough that can be plugged to any generic attribute acquisition algo-rithm to increase precision and coverage. We explore this method, based on the identification of attributes of individ-ual instances ( vicodin ), in contrast to most previous work on attribute extraction [23, 16], why acquire attributes of classes (e.g., painkillers ). Thus, since a large majority of popular search queries are precisely instances of various kinds, the method operates over an input vocabulary (i.e., instances rather than classes) that better approximates the overall composition of the sets of millions of search queries submit-ted daily to Web search engines.

Instance relatedness is approximated through two types of data: pairwise distributional similarities [13], quantifying the extent to which any two instances occur in similar con-texts in text; and labeled classes of instances, capturing the class labels (e.g., painkillers , prescription drugs , medications and substances ) applicable to various instances ( vicodin ). It produces a significant improvement over either learning the attributes for class labels, and over learning the attributes for instances without propagation. Applications : The special role played by attributes, among other types of relations, is documented in earlier work on language and knowledge representation [20, 9]. It inspired the subsequent development of text mining methods [14] aiming at constructing knowledge bases automatically. In Web search, the availability of instance attributes is useful for applications such as search result ranking and suggestion of related queries [2], and has also been identified to be a useful resource in generating product recommendations [19].
Previous work on attribute e xtraction uses a variety of textual data sources for mining attributes. Taking advan-tage of structured and semi-structured text, the method pre-sented in [26] submits list-seeking queries to general-purpose Web search engines, and analyzes retrieved documents to identify common structural (HTML) patterns around class labels given as input, and potential attributes. Similarly, layout and other HTML tags serve as clues to acquire at-tributes from either domain-specific documents such as those from product and auction Web sites [24] or from arbitrary documents, optionally relying on the presence of explicit itemized lists or tables [4]. As an alternative to Web doc-uments, articles within online encyclopedia can also be ex-ploited as sources of structured text for attribute extraction, as illustrated by previous work using infoboxes and category labels [22, 15, 25, 7] associated with Wikipedia articles.
Working with unstructured text within Web documents, the method described in [23] applies manually-created lexico-syntactic patterns to document sentences in order to extract candidate attributes, given various class labels as input. The candidate attributes are ranked using several frequency statistics. If the documents are domain-specific, such as doc-uments containing product reviews, additional heuristically-motivatedfiltersandscoringmetricscanbeusedtoextract and rank the attributes [21]. In [2], the extraction is guided by a small set of seed instances and attributes rather than manually-created patterns, with the purpose of generating training data and extract new pairs of instances and at-tributes from text. The set of seeds is acquired automati-cally in [19], thus further reducing the overhead associated to the preparation of the input data, while exploiting words and part-of-speech labels as features during extraction. Web search queries have also been considered as a textual data source for attribute extraction, using lexico-syntactic pat-terns [17] or seed attributes [16] to guide the extraction.
Virtually all existing methods for attribute extraction pro-duce ranked list of attributes for each input item (i.e., class label or, more rarely, class inst ance). Therefore, the method presented in this paper is generally applicable. It acts as a wrapper that takes as input the output from an existing method, propagating attributes across related instances to improve recall, while re-ranking top attributes to improve precision.
Givenaninstance(e.g., cloxacillin as element of the set antibiotics ), the goal of instance attribute extraction is the automatic acquisition of the set of relevant attributes (e.g., side effects , mechanism of action , cost , therapeutic range ) capturing the most prominent properties of the instance. Ideally, the resulting set would be complete (perfect cov-erage), and contain only relevant attributes (perfect preci-sion). In practice, the task of attribute extraction can be ap-proximated by the acquisition of a ranked list of attributes [ a , a 2 , a 3 , ..., a N ], such that as many relevant attributes as possible are among the top items in the ranked list.
The task of instance attribute extraction is related to, and more difficult than, the task of class attribute extrac-tion. Indeed, although both tasks generate ranked lists of at-tributes, the former does so for an individual instance (e.g., cloxacillin ), whereas the latter works for a class of instances (e.g., antibiotics or penicil lins ). Since a class of instances is often available as input in the form of a set of instances (e.g., { ampicillin , oxacillin , cloxacillin , benzylpenicillin sociated with a corresponding class label (e.g., penicil lins ), class attribute extraction methods [17] enjoy access to signif-icantly more input data (i.e., a set of instances instead of a single instance), which allows them to mitigate the negative impact of data sparseness on the extraction outcome, by ac-quiring attributes of a class (e.g., car manufacturers ), based on associations with some of the more popular instances of the class (e.g., jaguar , audi , toyota , chevrolet ), even if a few of the input instances (e.g., tesla } ) of the class may be ab-sent from the underlying source of textual data from which attributes must be extracted;
Since instance attribute extraction is equivalent to class attribute extraction where the class contains only one ele-ment (the instance itself), none of the above positive prop-erties hold in the case of instance attribute extraction. In particular, data sparseness may strongly affect the quality and coverage of the ranked lists of attributes extracted for long-tail instances, that is, instances that occur relatively infrequently in the underlying textual data source. Con-cretely, a previous method operating over query logs [17] extracts the following ranked lists as attributes for the re-spective instances: [solubility, analytical methods, inventor, dossier development, last patent, adverse effects] for bicalu-tamide ; [map, opening] for agua caliente casino ; and [] (i.e., the empty list) for ct scan . Clearly, the extracted lists suffer from occasional less-than-optimal extractions (e.g., dossier development for bicalutamide ), but more importantly from occasionally too few or no elements in the lists, which sug-gests a low coverage.
The operational advantage of class attribute extraction over instance attribute extraction stems from its ability to aggregate attributes of the class, from the attributes ex-tracted for individual instances of the class. In order to emulate a similar behavior in instance attribute extraction, one would need a set of instances as input, which are simply not available. However, given the input instance, a rough approximation of a set of instances could be generated dy-namically, based on access to a large resource of instance relatedness data. In that case, instance attribute extraction becomes equivalent to aggregating attributes of the instance, from the attributes extracted for other instances that are strongly related to it, based on the following observation:
Hypothesis 1 :Let i 1 and i 2 be two instances. The more strongly related i 1 and i 2 are semantically, the more likely it is for them to share common attributes.
Two types of relatedness data are explored in this paper for the purpose of attribute extraction: IsA pairs and dis-tributional similarities.

Distributional similarities are a technique for calculating similarities among words or phrases [12]. They capture the extent to which the textual contexts in which phrases occur are similar, with the intuition that phrases that occur in sim-ilar contexts tend to have similar meanings. Distributional similarities scale well to large text collections, since their acquisition can be done through parallel implementations, yet they perform well against more expensive, knowledge-based similarity metrics [1]. Distributional similarities are assumed to be available, as pairs of similar phrases with an associated similarity score (e.g., cloxacillin and erythromycin with a score of 0.36; or cloxacillin and alcohol with a score of 0.05).

In addition to distributional similarities, IsA pairs are as-sumed to be available as weighted pairs of a class instance and an associated class label (e.g., audi and car manufactur-ers ;or cloxacillin and antibiotics ). IsA pairs indirectly cap-ture instance relatedness, since any two instances involved in IsA pairs with the same class (e.g., audi and nissan ,as distinct instances of the class car manufacturers ) are related to each other. Graph Representation : Given an instance i j and some extraction method, let be the weighted list of attributes extracted by the method for the instance i j ,where A is the set of all attributes of all instances. One can define a probability of transitioning from instance i j to attribute a k by normalizing the weights to sum up to 1: and represent the instances and attributes as a bipartite graph, with weighted edges from instance nodes to attribute nodes.
 Injecting Class Labels : The availability of IsA pairs al-lows for the extension of the instance-attribute graph, by adding a layer for the class labels. Given an instance i j be the weighted list of the class labels of the instance, where C is the set of all class labels. The normalization of the weights produces a class-label probability distribution p c where c m is a class label of the instance i j .Inthescenario of a random walk across the graph representation, this prob-ability distribution would provide, for each instance i j probability of transitioning to a class label c m .Conversely, the aggregation and normalization of the weights by class la-bel rather than instance produces a probability distribution mj , indicating the probability of transitioning from a class label c m to an instance i j within the graph, as illustrated in Figure 1. The resulting graph can be used to propagate at-tributes across instances of the same class, using a three-step random walk process: 1. From an instance (e.g., i 0 ), execute a random step to one of its class labels (e.g., c 0 ). The probability of each step Figure 1: Graph for the transitions from instances to in-is governed by the distribution p c (  X  ), following the dotted edges in Figure 1. 2. From a class label (e.g., c 0 ), execute a random step to move to an instance of that class (e.g., i 0 ). The probability is given by p i (  X  ), following the dashed edges. 3. From an instance (e.g., i 0 ), execute a random step to an attribute (e.g., a 0 ), using the probability distribution following the solid edges.

This random walk process defines, for each original in-stance i j , a set of reachable attributes, and the probability of reaching each of them from i j . These attributes can be ranked by this probability.

Note that, if the input data does not contain any class la-bel at all for any instance, no ranked list of attributes can be returned for that instance. This can be avoided by adding, for each instance i , a pseudo-class label c i representing a class that only contains that instance. In this way, for in-stances without any class label at all no propagation will be done, but they will at least retain their original ranked list of attributes.
 Injecting Distributional Similarities : The second type of information used to propagate attributes across instances are distributional similarities, used to identify related in-stances. Given an instance i j and its list of similar instances: where I is the set of all instances, the graph can be extended by again normalizing the weights, and adding edges to the graph corresponding to transitions from each instance i j similar instances. The resulting graph is depicted in Fig-ure 2. In this case, the propagation would be defined by a two-step random walk, first transitioning from an instance to similar instances and then transitioning to attributes. Us-ing distributional similarities the similarity of any instance with itself will always be 1.0, value which will be normalized together with other the similary scores.
 Injecting Class Labels and Distributional Similari-ties : Figure 3 shows an example of a graph topology that Figure 2: Graph for the transitions from instances to includes both class labels and distributionally similar in-stances. The outgoing probabilities for every node are nor-malized so their sum is 1.

Using this graph topology, the propagation is a two-step algorithm. In the first step, the algorithm calculates, for each instance, the probability of transitioning to any in-stance in the dataset either by following the self-loop, by randomly walking in two steps through the class labels, or by stepping randomly to a distributionally similar instance. Next, the algorithm calculates the probability of transition-ing to any of the attributes. These probabilities are used to rank the attributes, and thus generate a ranked list of attributes as output, for each instance. Textual Data Sources : The acquisition of instance at-tributes relies on unstructured text available within Web documents and search queries. The collection of queries is a random sample of fully-anonymized queries in English sub-mitted by Web users in 2006. The sample contains about 50 million unique queries. Each query is accompanied by its frequency of occurrence in the logs. The document col-lection consists of around 200 million documents in English, as available on the Web in 2009. The textual portion of the documentsiscleanedof html , tokenized, split into sentences and part-of-speech tagged using the TnT tagger [3]. Parameters for IsA Pairs : Pairs of a class label (e.g., an-tibiotics ) and a class instance (e.g., cloxacillin ), along with associated frequency-based scores, are extracted from the collection of Web documents by applying a few IsA extrac-tion patterns selected from [11], as described in [18]. The pairs are organized as ranked lists of class labels per class in-stance, e.g., [antibiotics, resistant penicillins, lactams, peni-Figure 3: Graph including all possible transitions ( cillins, penicillinase-resistant penicillins, spectrum antibiotics, semisynthetic penicillins,..] for cloxacillin .
 Parameters for Distributional Similarities : When ap-plied to the collection of Web documents, the pipeline for extracting distributional similarities described in [1], which scales to millions of instances and billions of Web documents, and identifies pairwise similarity scores among all instances involved in any IsA pairs. Following standard settings [12], the similarity score between two instances is the cosine be-tween their vectors of context windows, where a window consists of three words to the left and three words to the right of each occurrence, within Web documents, of the two instances respectively.
 Parameters for Initial Instance Attributes :Theex-traction method introduced in [17] applies a few patterns (e.g., the A of I ,or I  X  X  A ,or A of I )toquerieswithin query logs, where I is an instance from the IsA pairs, and A is a candidate attribute. For each instance, the method extracts ranked lists containing zero, one or more attributes, along with frequency-based scores.
 Parameters for Graph Representation : As described in the previous section, the complete version of the graph rep-resentation of the instances, attributes and class labels con-tains three types of nodes: instance, attribute and class label nodes. An instance node is created for each instance from the IsA pairs. For efficiency, at most 250 of the attributes extracted initially for each instance are used to populate attribute nodes and the associated instance-attribute edges in the graph; and distributional similarities below 0.001 are discarded. Even if they were kept, due to the low weights associated to the attributes and the similar queries below those thresholds, it would not have a notable impact on the relative ranking of attributes. Table 1: Set of 75 target instances, used in the evalua-Target Instances : In order to assemble a set of instances whose attributes are evaluated for accuracy and relative cov-erage, the set of all instances from the graph is partitioned according to the number of attributes extracted initially for each instance. The partitions contain all instances with: 0 attributes; 1 to 5 attributes; 6 to 20 attributes; 21 to 50 attributes; and more than 51 attributes, respectively. From each partition, a random sample of 60 instances is automat-ically selected. The sample is further inspected manually, in order to eliminate instances for which human annotators would likely need a long time to become familiar with the instance and its meanings, before they can assign correct-ness labels to the attributes extracted for the instance. The only purpose of the manual selection step is to keep the costs associated with the subsequent, manual evaluation of attributes within reasonable limits. To remove any possi-ble bias towards instances with more or better attributes, the extracted attributes, if any, remain invisible during the selection of instances. For example, the instance it (which, among other meanings, is as an acronym for information technology) is discarded due to extreme ambiguity. Con-versely, agua caliente casino is retained, since it is relatively less difficult to notice that it refers to a particular casino. The manual selection of 15 instances, from the random sam-ple of each of the 5 partitions, results in an evaluation set containing 75 target instances, as shown in Table 1.
While the comprehensiveness of any attribute extraction experiments increases with the the number of target in-stances used for evaluation, the time intensive nature of manual accuracy judgments often required in the evaluation of information extraction systems [8] sets a practical limit to the size of the test set. With this in mind, we choose what we feel to be a large enough size for our test set (75 instances with 4,833 attributes in total) to ensure varied ex-perimentation on several dimensions.
 Experimental Runs : The experiments consist of four dif-ferent runs: S n C n ,S y C n ,S n C y and S y C y , where S stands for transitions from instances to distributionally similar in-stances, C stands for transitions from instances to class la-bels, and y / n indicate whether the respective transitions are included in the graph topology ( y )ornot( n ). The first run, S
C n , corresponds to using the ranked lists of attributes initially extracted from text, without any propagation. Table 2: Number of graph nodes and number of edges Table 3: Correctness labels for the manual assessment IsA Pairs : The IsA pairs, extract edaccordingtothepat-terns from [18] from the document collection, cover a total of 2.12 million of the instances that each have two or more class labels, with an average of 19.72 class labels per instance. This set of 2.12 million instances was used for propagation via class labels.
 Distributional Similarities : The distributional similari-ties, for the set of 2.12 million instances that have two more class labels, capture one or more similar instances for 13% of them. For the ones that have at least one similar instance, an average of 9 similar instances are returned.
 Initial Instance Attributes : The initial set of instance attributes, extracted according to [17] from the collection of search queries, is available in the form of ranked lists con-taining zero, one or more attributes. There is one (possibly empty) ranked list for each of the 2.12 million instances. The extracted ranked lists contain 0 attributes for 1.63 million instances; 1 to 5 attributes for 315,052 instances; 6 to 20 at-tributes for 56,721 instances; 21 to 50 attributes for 13,902 instances; and more than 50 attributes for 9,249 instances. Graph Representation : Table 2 shows the number of nodes and edges of various types, for the three runs that apply propagation for extracting attributes. Evaluation Procedure : The measurement of recall re-quires knowledge of the complete set of items (in our case, attributes) to be extracted. Unfortunately, this number is often unavailable in information extraction tasks in gen-eral [10], and attribute extraction in particular. Indeed, the manual enumeration of all attributes of each target instance, to measure recall, is unfeasible. Therefore, the evaluation focuses on the assessment of attribute accuracy.

To remove any bias towards higher-ranked attributes dur-ing the assessment of instance attributes, the top 50 at-tributes within the ranked lists of attributes produced by each run to be evaluated are sorted alphabetically into a merged list. Each attribute of the merged list is manually assigned a correctness label within its respective instance. In accordance with previously introduced methodology, an attribute is vital if it must be present in an ideal list of attributes of the instance (e.g., side effects for cloxacillin ); okay if it provides useful but non-essential information; and wrong if it is incorrect [16]. Thus, a correctness label is manually assigned to a total of 4,833 attributes extracted for the 75 target instances, in a process that confirms that evaluation of information extraction methods can be quite time consuming. Two computational linguists performed the evaluation, with each of the attributes rated by the two of them. The inter-annotator agreement of 88.79%, resulting in a Kappa score of 0.85, indicating substantial agreement.
To compute the precision score over a ranked list of at-tributes, the correctness labels are converted to numeric val-ues ( vital to 1, okay to 0.5 and wrong to 0), as shown in Ta-ble 3. Precision at some rank N in the list is thus measured as the sum of the assigned values of the first N attributes, divided by N .
 Accuracy of Instance Attributes : Figure 4 plots pre-cision values for ranks 1 through 50, for each of the four experimental runs. The first three graphs in the figure show the precision over three individual target instances. Sev-eral conclusions can be drawn from these. First, the qual-ity of the attributes extracted by a given run varies among instances. For instance, the attributes extracted for the in-stance spandex are better than for habbo hotel . Second, the experimental runs have variable levels of accuracy. The bot-tom left graph in Figure 4 shows the average precision over all target instances. Although none of the runs outperforms the others on each and every target instance, on average, S
C y performs the best and S n C n (i.e., the baseline) the worst, with S y C n placed in-between and S n C y almost as accurate as S y C y . In other words, propagation based on distributionally similar instances (S y C n ) gives a significant improvement over the baseline S n C n , and propagation based on class labels rather than distributionally similar instances (S n C y ) gives an even larger improvement. In order to dis-cover whether these results are due to the fact that class labels are more reliable or whether it is because class labels are available for more instances, The bottom right graph in Figure 4 shows the precision of the experimental runs over the subset of instances that have at least one distri-butionally similar instance. This subset contains exactly 50 of the 75 target instances. The results indicate that class labels and distributional similarities (as well as their combi-nation), when available, lead to similar improvements when both types of relatedness data are available.

Table 4 provides a more detailed view on the accuracy of the extracted attributes, for various subsets of target instances obtained according to the number of initial at-tributes (i.e., extracted in run S n C n )fortherespectivein-stances. For example, rows with an attribute count  X  X 1,5] X  refer to instances that have one through five attributes in run S
C n . For this range, the baseline S n C n is outperformed by the three propagation-based runs at all ranks, with precision scores of 0.22 at rank 5 and 0.02 at rank 50 for S n C n , vs. 0.60 at rank 5 and 0.38 at rank 50 for S y C y , (fifth and eigth rows of the table). The table indicates that: 1) with few excep-tions, S y C n ,S n C y and S y C y outperform S n C n for all ranges, with S n C y and S y C y giving the best results; 2) S y C and S y C y produce more accurate attributes than S n C n for the range [51,  X  ), which indicates that the attribute re-ranking caused by the propagation is better than the initial attribute ranking given by S n C n ; and 3) attribute propaga-tion is quite resistant to sparseness of initial attributes, as illustrated by precision scores of, e.g., S y C y that are com-Table 4: Precision scores at various ranks, as an av-petitive for the range [0,0], where sparseness is extreme (no initial attributes) vs. the range [51,  X  ), where sparseness over the top 50 attributes is non-existent.

In the lower part of Table 4, for the range [0,  X  ), the precision scores are computed over the entire set of target instances, and therefore they correspond to points on the curves from the last graph of Figure 4. Also shown in the table are the relative increases ( Rel ) and the reduction in the error rates ( Err ) at various ranks, for each of S y C n or S y C y , on one hand, relative to S n C n , on the other hand. Per-class attributes : For a comparison, we have used the procedure in [17] as another baseline. This is a per-class ex-traction procedure that works, from the same input data as the per-instance attribute extraction ( S n C n ) in the follow-ing way: first, from the IsA data, each instance is associated with its highest-ranking class label. For every class label, it looks for attributeness-denoting lexicosemantic patterns in the query logs (e.g. XofY ) involving any element in the class. The extracted attributes are ranked by frequency in the whole class, and assigned, equally, to all the class ele-ments. There are more than one million class labels, to en-sure that they are fine-grained enough that only very closely related instances belong to the same class.
 The last row in Table 4 shows the results using this method. The scores consistently improve over the per-instance at-tribute extraction ( S n C n ). This is mainly due to the sparse data affecting S n C n : the fact that the instances are grouped together and all their attributes are shared allows the per-class extraction to collect at least 50 attributes for 98% of the instances. On the other hand, since all the instances in the same class have the same relative weight, attributes that are vital only to some members of the class may receive a high standing for all of them. Some examples are king for u.s. ,or senator for countries that do not have a Senate. Us-ing the graph structure this is partially avoided because (a) each instance has the highest distributional similarity with itself, and (b) every instance share all its class labels with itself. Therefore, the per-instance original attributes tend to end up higher in the final lists after the propagation. Examples: Table 5 shows the top items in the ranked lists of attributes extracted for a few of the 75 target instances. For angioplasty and bicalutamide ,S n C n extracts 0 and 6 attributes respectively. The other three runs extract more instances via propagation, with the exception of S y C n for the instance bicalutamide . The per-class procedure does well for angioplasty , but bicalutamide shows some drawbacks of this procedure: although its class label is correctly identified in the IsA data ( antiandrogens ), this class mistakenly contains a couple of sicknesses, which pollute the list of attributes.
Data sparseness is a problem affecting the precision and coverage of open-domain information extraction tasks. In-stance attribute extraction is not an exception. Data captur-ing the degree to which various instances may be related to one another is useful in re-ranking and expanding attributes extracted from text with standard techniques. The injec-tion of instance relatedness data into a graph representing the initially extracted instance attributes, allows for relevant attributes to be propagated across related instances. The re-sulting ranked lists of attributes have higher accuracy levels than previous results. When both class labels and distri-butionally similar instances are available, the improvements using the two methods are comparable. Current work inves-tigates the utility of distributional similarities among class labels, as signals during propagation. [1] E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, [2] K. Bellare, P. Talukda r, G. Kumaran, F. Pereira, [3] T. Brants. TnT -a statistical part of speech tagger. In [4] M. Cafarella, A. Halevy, D. Wang, and Y. Zhang. [5] N. Chinchor. Overview of MUC-7/MET-2. In [6] T. Chklovski and Y. Gil. An Analysis of Knowledge [7] G.Cui,Q.Lu,W.Li,andY.Chen.Automatic [8] O. Etzioni, M. Banko, S. Soderland, and S. Weld. [9] N. Guarino. Concepts, Attributes and Arbitrary [10] T. Hasegawa, S. Sekine, and R. Grishman. Discovering [11] M. Hearst. Automatic acquisition of hyponyms from [12] L. Lee. Measures of Distributional Similarity. In [13] D. Lin and P. Pantel. Concept Discovery from Text. [14] R. Mooney and R. Bunescu. Mining knowledge from [15] V. Nastase and M. Strube. Decoding wikipedia [16] M. Pa  X  sca. Organizing and searching the World Wide [17] M. Pa  X  sca and B. Van Durme. What you seek is what [18] M. Pa  X  sca and B. Van Durme. Weakly-supervised [19] K. Probst, R. Ghani, M. Krema, A. Fano, and Y. Liu. [20] J. Pustejovsky. The Generative Lexicon: a Theory of [21] S. Raju, P. Pingali, and V. Varma. An Unsupervised [22] F. Suchanek, G. Kasneci, and G. Weikum. Yago: a [23] K. Tokunaga, J. Kazama, and K. Torisawa. Automatic [24] T. Wong and W. Lam. An Unsupervised Method for [25] F. Wu, R. Hoffmann, and D. Weld. Information [26] N. Yoshinaga and K. Torisawa. Open-Domain
