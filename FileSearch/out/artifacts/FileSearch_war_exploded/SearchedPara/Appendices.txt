 In general, the subtree rooted at vertex x defines a partial ordering on its own mentions. To sample a total ordering i x uniformly at random from among those compatible with that partial ordering, first recursively sample M orderings i y 1 ,..., i y patible with the M subtrees rooted at x  X  X  children. Then uniformly sample an interleaving of the M orderings,and prepend x itself to this interleaving to obtain i x . To sample an interleaving, select one of the input orderings i y at random, with probabil-ity proportional to its size | i y | , and print and delete its first element. Repeating this step until all of the input orderings are empty will print a random interleaving. Note that in the base case where x is a leaf (so M = 0 ), this procedure terminates im-mediately, having printed the empty ordering. Our i  X  is the output of running this recursive process with x =  X  .
 B.1 Collection Using the Twitter 1% streaming API, we collected all tweets during the 2013 Grammy music awards ceremony, which occurred on Feb 10, 2013 be-tween 8pm eastern (1:00am GMT) and 11:30pm (4:30 GMT). We used Carmen geolocation (Dredze et al., 2013) to identify tweets that originated in the United States or Canada and removed tweets that did not have a language of English selected as the UI for the tweet author. This yielded a total of 564,892 tweets. We then selected tweets that contained the string  X  X rammy X  (case insensitive), reducing the set to 50,429 tweets. These tweets were processed for POS and NER using the Uni-versity of Washington Twitter NLP tools 13 (Ritter et al., 2011). Tweets that did not include a person mention were removed. For simplicity, we selected a single person reference per tweet. The final set contained 15,736 tweets. Of these, 5000 have been annotated for entities.
 B.2 Annotation A first human annotator made a first pass of 1,000 tweets and then considered the remaining 4,000
 Yu-Xiang Wang luke.yxwang@gmail.com Huan Xu mpexuh@nus.edu.sg Our main deterministic result Theorem 1 is proved by duality. We first establish a set of conditions on the op-timal dual variable of D 0 corresponding to all primal solutions satisfying self-expression property. Then we construct such a dual variable  X  , hence certify that the optimal solution of P 0 satisfies the LASSO Subspace Detection Property.
 A.1. Optimality Condition Define general convex optimization: We may state an extension of the Lemma 7.1 in Soltanolkotabi &amp; Candes X  X  SSC Proof.
 Lemma A.1. Consider a vector y  X  R d and a matrix Ac + e and c has support S  X  T , furthermore the dual certificate vector  X  satisfies Proof. For optimal solution ( c  X  ,e  X  ), we have: = k c S k 1 + k c With the inequality constraints of  X  given in the Lem-ma statement, we know Substitute into (A.2), we get: k c where (1  X  X  A T T c  X  k  X  ) is strictly greater than 0. k c and ( c,e ) is also an optimal solution. This concludes the proof.
 Apply Lemma A.1 with x = x ( ` ) i and A = X  X  i , we know that if we can construct a dual certificate  X  optimal solution of (4.1) satisfies SEP, in other word c = h 0 ,..., 0 , ( c ( ` ) i ) T , 0 ,..., 0 i By definition of LASSO detection property, we must sufficiently large  X  , k c ( ` ) i k 1 6 = 0 never occurs. Our strategy of avoiding this trivial solution is hence showing the existence of a  X  such that the dual optimal value is smaller than the trivial optimal value, namely:
OptV al ( D 0 ) =  X  x i , X   X  X  X  A.2. Constructing candidate dual vector  X  A natural candidate of the dual solution  X  is the du-al point corresponding to the optimal solution of the following fictitious optimization program.

P 1 : min
D 1 : max This optimization is feasible because y ( ` ) ble solution. Then by strong duality, the dual program is also feasible, which implies that for every optimal so- X  satisfying: This construction of  X  satisfies all conditions in Lem-ma A.1 with respect to except i.e., we must check for all data point x  X  X  \X ` , Showing the solution of (A.5)  X  also satisfies (A.7) ma A.1, hence implies that the candidate solution (A.6) associated with optimal ( c,e ) of (A.4) is indeed the optimal solution of (4.1).
 A.3. Dual separation condition In this section, we establish the conditions required for (A.7) to hold. The idea is to provide an upper bound of | X  x, X   X  X  then make it smaller than 1.
 First, we find it appropriate to project  X  to the sub-space S ` and its complement subspace then analyze separately. For convenience, denote  X  1 := P S  X  To see the last inequality, check that by Definition 3, Since we are considering general (possibly adversarial) cosine terms (a better bound under random noise will be given later). Now all we have to do is to bound k  X  1 A.3.1. Bounding k  X  1 k We first bound k  X  1 k by exploiting the feasible region of  X  1 in (A.5). is equivalent to the condition into Now we relax each of the term into The relaxed condition contains the feasible region of  X  1 in (A.5).
 It turns out that the geometric interpretation of the relaxed constraints gives a upper bound of k  X  1 k . Definition A.1 (polar set) . The polar set K o of set K X  R d is defined as By the polytope geometry, we have Now we introduce the concept of circumradius. Definition A.2 (circumradius) . The circumradius of radius of the smallest Euclidean ball containing P . The magnitude k  X  1 k is bounded by R ( T o ). Moreover, by the the following lemma we may find the circum-gives the tightest convex envelope of original set, i.e., ( K is convex in the first place, the polar set of T o is es-sentially T .
 Lemma A.2. For a symmetric convex body P , i.e. P =  X  X  , inradius of P and circumradius of polar set of P satisfy: Lemma A.3. Given X = Y + Z , denote  X  := subspace, then we have: Proof. First note that projection to subspace is a lin-the largest ball containing in the convex body, hence bound of it: k y k X k Y c k X  X  P S Zc k X  r ( P ( Y ))  X  X This concludes the proof.
 A bound of k  X  1 k follows directly from Lemma A.2 and Lemma A.3: k  X  1 k X  (1 +  X  k  X  2 k ) R ( P ( Y ( ` )  X  i + P S This bound unfortunately depends k  X  2 k . This can be extremely loose as in general,  X  2 is not well-constrained (see the illustration in Figure C.2 and C.3). That is why we need to further exploit the fact  X  is the optimal solution of (A.5), which provides a reasonable bound of k  X  2 k .
 A.3.2. Bounding k  X  2 k By optimality condition: and so Now we will bound k c k 1 . As c is the optimal solution, k c k 1  X  k c k 1 +  X  solution (  X  c,  X  e ). Let  X  c be the solution of then by strong duality, P j k z j k|  X  c j | ) 2  X  (  X  + k  X  c k 1  X  ) k c k 1  X k  X  c k 1 + This gives the bound we desired: k  X  2 k X   X  By choosing  X  satisfying the bound can be simplified to: A.3.3. Conditions for | X  x, X   X  X  &lt; 1 Putting together (A.8), (A.11) and (A.15), we have the upper bound of | X  x, X   X  X  : | X  x, X   X  X  X  (  X  ( X ` ) + k P S k  X   X  + For convenience, we further relax the second r ( Q ` thus guaranteed with Denote  X  :=  X  X  (1 +  X  ), assume  X  &lt; r Q `  X  i , (  X  ( X  X  ) &lt; 1 and simplify the form with we get a sufficient condition To generalize (A.16) to all data of all subspaces, the following must hold for each ` = 1 ,...,k :  X  ( X ` ) + 3  X  +  X  1 &lt; (1  X  2  X  ) min This gives a first condition on  X  and  X  , which we call it  X  dual separation condition  X  under noise. Note that this reduces to exactly the geometric condition in Soltanolkotabi &amp; Candes X  X  Theorem 2.5 when  X  = 0. A.4. Avoid trivial solution optimal solution. For any optimal triplet ( c,e, X  ) we Now we will establish the condition on  X  such that: An upper bound of k  X  k and a lower bound of  X  k x ( ` ) i are readily available: So the sufficient condition on  X  such that solution is non-trivial is Reorganize the condition, we reach need: Relax  X  1 to  X  and solve the system of inequalities, we get:  X  &lt; Use condition for every ` = 1 ,...,L : A.5. Existence of a proper  X  Basically, (A.17), (A.18) and (A.14) must be satis-ly (A.18) gives condition of  X  from below, the oth-er two each gives a condition from above. Denote r tion on  X  is: Note that on the left On the right To understand this, when  X  and  X  is small then any separation condition. We will now derive the condition on  X  such that (A.20) is not an empty set.
 A.6. Lower bound of break-down point (A.19) gives one requirement on  X  and the range of (A.20) being non-empty gives another. Combining these two leads to lower bound of the breakdown point. trary corruptions with magnitude less than this point for some  X  .
 Again, we relax  X  1 to  X  in (A.20) to get: The first inequality in standard form is: with This is an extremely complicated 3 rd order polyno-condition. First extract and regroup  X  ` in first three drop it. Second we express the remaining expression using: where Note that since  X  &lt; 1, we can write f ( r, X  )  X  f ( r, 0) = 3 r ` r + 6 r ` + 2 r + 2  X  3 r 2 Thus, a stronger condition on  X  is established: The second inequality in standard form is: By definition r &lt; 1, we solve the inequality and get: The lower constraint is always satisfied. Rationalized the expression of the upper constraint, 1  X  r gets can-celled out: equality to hold. This is by  X  9 r 2 + 24 r + 16 = 3 r + 4. Combine with (A.21) we reach the overall condition: The first expression is always smaller because: Verify that when (A.22) is true for all ` , there exists a single  X  for solution of (2.2) to satisfy subspace de-tection property for all x i . The proof of Theorem 1 is now complete.
 In this section, we provide proof to the Theorems about the three randomized models:  X  Determinitic data+random noise  X  Semi-random data+random noise  X  Fully random cos(  X  ( y, X  2 )) when the Z follows Random Noise Mod-be obtained. Moreover, for Semi-random and Ran-samples from each subspace are drawn uniformly and bound  X  ( X ` ) when subspaces are randomly generated. These requires the following Lemmas.
 Lemma B.1 (Upper bound on the area of spherical cap) . Let a  X  R n be a random vector sampled from a unit sphere and z is a fixed vector. Then we have: This Lemma is extracted from an equation in page 29 adapted from the upper bound on the area of spher-ical cap in Ball (1997). By definition of Random Noise Model, z i has spherical symmetric, which im-plies that the direction of z i distributes uniformly on an n -sphere. Hence Lemma B.1 applies whenever an inner product involves z .
 As an example, , we write the following lemma Lemma B.2 (Properties of Gaussian noise) . For Gaussian random matrix Z  X  R n  X  N , if each entry Z i,j  X  N (0 ,  X   X  n ) , then each column z i satisfies: where z is any fixed vector(or random generated but independent to z i ).
 Proof. The second property follows directly from Lem-ma B.1 as Gaussian vector has uniformly random di-rection.
 To show the first property, we observe that the sum of n independent square Gaussian random variables follows  X  2 distribution with d.o.f n , in other word, we have By Hoeffding X  X  inequality, we have an approximation of its CDF (Dasgupta &amp; Gupta, 2002), which gives us Substitute  X  = 1 + t , we get exactly the concentration statement.
 By Lemma B.2,  X  = max i k z i k is bounded with high probability.  X  1 has an even tighter bound because each S a small value with high probability. Moreover, since  X  =  X e =  X  ( x i  X  X  X  i c ),  X  2 =  X  P S  X   X  2 is merely a weighted sum of random noise in a ( n  X  d )-dimensional subspace. Consider y a fixed vector, cos(  X  ( y, X  2 )) is also bounded with high probability. ual separation condition for under Random noise mod-el.
 Lemma B.3 (Dual separation condition under ran-dom noise) . Let  X  :=  X  X  (1 +  X  ) and for some constant C . Under random noise model, if for each ` = 1 ,...,L then dual separation condition (A.7) holds for all data points with probability at least 1  X  7 /N .
 Proof. Recall that we want to find an upper bound of | X  x, X   X  X  . Here we will bound the two cosine terms and  X  1 under random noise model.
 As discussed above, directions of z and  X  2 are inde-pendently and uniformly distributed on the n -sphere. Then by Lemma B.1, Using the same technique, we provide a bound for  X  1 . Given orthonormal basis U of S ` , P S Apply Lemma B.1 for each i , then apply union bound, we get: Since  X  1 is the worse case bound for all L subspace and all N noise vector, then a union bound gives: Moreover, we can find a probabilistic bound for k  X  1 k too by a random variation of (A.9) which is now y Substituting the upper bound of the cosines, we get: | X  x, X   X  X  X   X  k  X  1 k + k y kk  X  2 k k  X   X  :=  X  ( X ` ) we can further relax the bound into Note that here in order to get rid of the higher order Now impose the dual detection constraint on the upper bound, we get: Replace  X  :=  X  X  (1 +  X  ) and reorganize the inequality, we reach the desired condition: related to the consine value, apply union bound we get the proof.
 B.1. Proof of Theorem 2 Lemma B.3 has already provided the separation con-dition. The things left are to find the range of  X  and update the condition of  X  .
 The range of  X  : Follow the same arguments in Sec-tion A.4 and Section A.5, re-derive the upper bound from the relationship in Lemma B.3 and substitute r the range of  X  under random noise model:  X   X   X   X   X   X   X   X  &gt;  X  &lt; min Remark B.1. A critical difference from the deter-expanded an order to  X (1 /r )  X   X  &lt;  X ( r/ X  2 ) . The condition of  X  : Re-derive (A.19) using  X  1  X   X  , we get: Likewise, we re-derive (A.21) from the new range of  X  in (B.3). The first inequality in standard form is, with  X   X   X   X   X   X   X   X   X  A = 6 2  X  6 , B =  X  (3 + 4 2 + r `  X  2 r ` + 6 r + 2  X  `  X  3  X  ` ) ,
D =  X  r ( r `  X   X  ` ) , apply the same trick of removing the negative  X  term and define such that the 3 rd -order polynomial inequality becomes drop negative terms, we get f ( r, X  ) &lt; B X  + C =  X  3 + 4 2 + 2 ( r `  X   X  ` ) + 6 r  X  + 2( r `  X   X  ` )  X  + ( r `  X   X  ` )  X  + 2  X  `  X   X   X  ` Therefore, a sufficient condition of  X  is  X  &lt; r &lt; r ` , Combining the two cases, we have: For the second inequality, the quadratic polynomial is now Check that 1 + 5 r  X  6 r &gt; 0. We solve the quadratic (B.4), which is Note that (B.6)  X  (B.7), so (B.6) alone is sufficient. expression is: the output of the min function at the smallest bound the proof for Theorem 2.
 B.2. Proof of Theorem 3 To prove Theorem 3, we only need to bound inradii r and incoherence parameter  X  under the new assump-tions, then plug into Theorem 2.
 Lemma B.4 (Inradius bound of random samples) . In random sampling setting, when each subspace is sam-pled N ` =  X  ` d ` data points randomly, we have:
Pr This is extracted from Section-7.2.1 of Soltanolkotabi number of iid samples. c (  X  ) is some positive value for all  X  &gt; 1 and for a numerical value  X  0 , if  X  &gt;  X  0 can take c (  X  ) = 1  X  bound of r in Theorem 3.
 Lemma B.5 (Incoherence bound) . In deterministic subspaces/random sampling setting, the subspace in-coherence is bounded from above: Pr n  X  ( X ` )  X  t (log[( N ` 1 + 1) N ` 2 ] + log L ) B.2.1. Proof of Lemma B.5 The proof is an extension of the same proof in distribution.
 Now we will prove the claim. First by definition,  X  is the unique optimal solution of D 1 (A.5). Fix  X  , D 1 depends on two inputs, so we denote  X  ( x,X ) and consider  X  a function. Moreover,  X  1 = P S  X  and  X  2 = P
S  X   X  . Let U  X  n  X  d be a set of orthonormal basis of d -dimensional subspace S and a rotation matrix R  X  R d  X  d . Then rotation matrix within subspace is hence URU T . As y is distributed uniformly on unit sphere of S , and z is spherical symmetric noise(hence z 1 and z 2 are also spherical symmetric in subspace), for any fixed k x 1 k , the distribution is uniform on the sphere. It suffices to show the uniform distribution of  X  1 with fixed k x 1 k . gue that if  X  is optimal solution of then the optimal solution of R -transformed optimiza-tion is merely the transformed  X  under the same R :  X  ( R ) =  X  ( URU T x 1 + x 2 ,URU T X 1 + X 2 ) and for all inner products in both objective function and constraints, preserving the optimality.
 By projecting (B.8) to subspace, we show that oper-URU T , i.e., v ( R ) = P S ` On the other hand, we know that where A  X  B means that the random variables A and B follows the same distribution. When k x 1 k is fixed and each columns in X 1 has fixed magnitudes, URU T x 1  X  x 1 and URU T X 1  X  X 1 . Since ( x 1 ,X 1 and ( x 2 ,X 2 ) are independent, we can also marginalize out the distribution of x 2 and X 2 by considering fixed ( x 2 ,X 2 ). Combining (B.9) and (B.10), we conclude that for any rotation R , l ( x 2 ,X 2 ), we showed that the overall distribution of v i is indeed uniformly distributed in the unit sphere of S .
 After this key step, the rest is identical to Lemma 7.5 of Soltanolkotabi &amp; Candes (2012). The idea is to use Lemma B.1(upper bound of area of spherical caps) to bound pairwise inner product and Borell X  X  inequality to bound the deviation from expected consine canoni-B.3. Proof of Theorem 4 The proof of this theorem is also an invocation of The-orem 2 with specific inradii bound and incoherence bound. The bound of inradii is exactly Lemma B.4 with  X  = 0 . 5,  X  ` =  X  , d ` = d . The bound of incoher-ence is given by the following Lemma that is extracted from Step 2 of Section 7.3 in Soltanolkotabi &amp; Candes (2012).
 Lemma B.6 (Incoherence bound of random sub-spaces) . In random subspaces setting, the projected subspace incoherence is bounded from above: Now that we have shown that projected dual directions are randomly distributed in their respective subspace, as the subspaces themselves are randomly generated, all clean data points y and projected dual direction v from different subspaces can be considered iid gen-erated from the ambient space. The proof of Lem-ma B.6 follows by simply applying Lemma B.1 and union bound across all N 2 events.
 By plug in these expressions into Theorem 2, we showed that it holds with high probability as long as the conditions in Theorem 4 is true.
 In this section, we attempt to give some geometric interpretation of the problem so that the results stat-ed in this paper can be better understood and at the section are drawn with  X  X eom3d X  (Legland, 2009) and  X  X BT7.3 X  (Veres, 2006) in Matlab.
 We start with an illustration of the projected du-tion(Soltanolkotabi &amp; Candes, 2012).
 Dual direction v.s. Projected dual direction: Figure C.1 for data point y .
 The projected dual direction can be easier understood mal solution of (A.5) to the true subspace. To see it more clearly, we plot the feasible region of  X  in Fig-ure C.2 (b), and the projection of the feasible region in Figure C.3. As (A.5) is not an LP (it has a quadratic term in the objective function), projected dual direc-Figure C.1. Nevertheless, it turns out to be sufficient to know the feasible region and the optimality of the solution.
 Magnitude of dual variable  X  : A critical step of our proof is to bound the magnitude case as Soltanolkotabi and Candes merely take the cir-cumradius of the full feasible region as a bound. This is sufficient because the feasible region is a cylinder per-pendicular to the subspace and there is no harm choos-ing only solutions within the intersection of the cylin-der and the subspace. Indeed, in noiseless case, we can choose arbitrary  X  2 because Y T (  X  1 +  X  2 ) = Y T  X  1 . In the noisy case however, the problem becomes a bit involved. Instead of a cylinder, the feasible region is now a spindle shaped polytope (see Figure C.2(b)) and the choice of  X  2 has an impact on the objective value. That is why we need to consider the optimality condi-tion and give k  X  2 k a bound.
 In fact, noise may tilt the direction of the feasible re-gion (especially when the noise is adversarial). As k  X  2 grows, k  X  1 k can potentially get large too. Our bound pendent on k  X  2 k (see (A.11)). We remark that in the case of random noise, the dependency on k  X  2 k becomes much weaker (see the proof of Lemma B.3).
 Geometrically, the bound of  X  2 can be considered a cylinder 1 ( ` 2 constrained in the S  X  and unbounded in S subspace) that intersect the spindle shaped feasible region, so that we know the optimal  X  may never be Algebraically, we can consider this as an effect of the quadratic penalty term of  X  in the (A.5).
 The guarantee in Theorem 1: The geometric interpretation and comparison of the noiseless guarantee and our noisy guarantee are given ways. One is subtractive, in a sense that the inradius plicative, as the entire successful region shrinks with a Readers may refer to (A.16) for an algebraic point of view.
 robust optimization point of view, where the projec-tion of every points inside the uncertainty set (the red balls in Figure C.5) must fall into the successful region (the dashed red polygon).
 provably tolerate is proportional to the geometric gap r  X   X  given in the noiseless case.
 In this section we outline the steps of solving the ma-While this convex optimization can be solved by some off-the-shelf general purpose solver such as CVX, such approach is usually slow and non-scalable. An ADMM (Boyd et al., 2011) version of the problem is described here for fast computation. It solves an equivalent op-timization program We add to the Lagrangian with an additional quadrat-ic penalty term for the equality constraint and get the augmented Lagrangian
L = k C k 1 + where  X  is the dual variable and  X  is a parameter. Op-timization is done by alternatingly optimizing over J , Algorithm 1 Matrix-Lasso-SSC
Input: Data points as columns in X  X  R n  X  N , tradeoff parameter  X  , numerical parameters  X  0 and  X  .

Initialize C = 0, J = 0,  X  = 0, k = 0. while not converged do end while Output: Affinity matrix W = | C | + | C | T C and  X  until convergence. The update steps are de-rived by solving  X  L / X  X  = 0 and  X  L / X  X  = 0, it X  X  non-differentiable for C at origin so we use the now stan-dard soft-thresholding operator(Donoho, 1995). For both variables, the solution is in closed-form. For the update of  X , it is simply gradient descent. For details of the ADMM algorithm and its guarantee, please refer to Boyd et al. (2011). To accelerate the convergence, it is possible to introduce a parameter  X  and increase  X  by  X  =  X  X  at every iteration. The full algorithm is summarized in Algorithm 1.
 Note that for the special case when  X  = 1, the inverse of (  X Y T Y +  X I ) can be pre-computed, such that the iteration is linear time. Empirically, we found it good to set  X  =  X  and it takes roughly 50-100 iterations to converge to a sufficiently good points. We remark that the matrix version of the algorithm is much faster than column-by-column ADMM-Lasso especially for the cases when N &gt; n . See the experiments. (2012) had formulated a more general version of SSC to account for not only noisy but also sparse corrup-were preparing for submission. The ADMM algorithm for Matrix-Lasso-SSC described here can be considered as a special case of the Algorithm 2 in their paper. Ball, K. An elementary introduction to modern convex geometry. Flavors of geometry , 31:1 X 58, 1997. Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eck-stein, J. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R  X  in Machine Learning , 3(1):1 X 122, 2011.
 Dasgupta, S. and Gupta, A. An elementary proof of a theorem of johnson and lindenstrauss. Random Structures &amp; Algorithms , 22(1):60 X 65, 2002. Donoho, D.L. De-noising by soft-thresholding. Infor-mation Theory, IEEE Transactions on , 41(3):613 X  627, 1995.
 Elhamifar, E. and Vidal, R. Sparse subspace clus-tering: Algorithm, theory, and applications. arXiv preprint arXiv:1203.1005 , 2012.
 Legland, David. geom3d toolbox [computer soft-ware], 2009. URL http://www.mathworks.com/ matlabcentral/fileexchange/24484-geom3d .
 Soltanolkotabi, M. and Candes, E.J. A geometric anal-ysis of subspace clustering with outliers. To appear in Annals of Statistics , 2012.
 Veres, Sandy. Geometric bounding toolbox 7.3 [computer software], 2006. URL http://www. mathworks.com/matlabcentral/fileexchange/
