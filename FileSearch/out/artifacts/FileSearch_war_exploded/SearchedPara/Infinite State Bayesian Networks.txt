 Bayesian networks remain the cornerstone of modern AI. They have been applied to a wide range of problems both in academia as well as in industry. A recent development in this area is a class of Bayes nets known as topic models (e.g. LDA [1]) which are well suited for structured data such as text or images. A recent statistical sophistication of topic models is a nonparametric extension known as HDP [2], which adaptively infers the number of topics based on the available data. This paper has the goal of bridging the gap between these three developments. We propose a general We consider models where the variables may have the nested structure of documents and images, intuitive causal dependencies of a Bayes net. ISBN X  X  can be viewed as collections of HDP  X  X odules X  connected together to form a network. Inference in these networks is achieved through a two-stage Gibbs sampler, which combines the  X  X orward-filtering-backward-sampling algorithm X  [3] extended to junction trees and the direct assignment sampler for HDPs [2]. Consider observed random variables x A , { x a } , a = 1 ..A . These variables can take values in an arbitrary domain. In the following we will assume that x a is sampled from a (conditional) distribution in the exponential family. We will also introduce hidden (unobserved, latent) variables { z b } , b = 1 ..B which will always take discrete values. The indices a, b thus index the nodes of the Bayesian network.
 We will introduce a separate index, e.g. n a , to label observations. In the simplest setting we assume IID data n = i , i.e. N independent identically distributed observations for each variable. We will
N however also be interested in more structured data, such as words in documents, where the index n can be decomposed into e.g. n = ( j, i j ) . In this notation we think of j as labelling a document straightforward to generalize to deeper nestings of indices, e.g. n = ( k, j k , i j can index e.g. books, j chapters and i words. We interpret this as the observed structure in the data, as opposed to the latent structure which we seek to infer. The unobserved structure is labelled with the discrete  X  X ssignment variables X  z a topics, factors, components).
 The assignment variables z together with the observed variables x are organized into a Bayes net, where dependencies are encoded by the usual  X  X onditional probability tables X  (CPT), which we denote with  X  a the variables to the left of it, e.g. P variables and may themselves be indexed by (a subset of) n , e.g.  X  We assume that each  X  b is sampled from a Dirichlet prior: e.g.  X   X  z a  X  D [  X  a /K a ] , where K a is the number of states for variable z a . We can put gamma priors on  X  a ,  X  a and consider them as random variables as well, but to keep things simple we will consider them fixed variables here. We refer to [4] for algorithms to learn them from data and to [5] and [2] for ways to infer them through sampling. In section 5 we further discuss these hierarchical priors. In drawing BNs we will not include the plates to avoid cluttering the figures. However, it is always possible to infer the number of times variables in the BN are replicated by looking at its indices. For instance, the variable node labelled with  X  z 1 | z 2 j in Fig.3a stands for K (2)  X  J IID copies of  X  1 sampled from  X  1 . In Fig.1b we have drawn the finite version of the HDP. Here  X  is a distribution over words, one for each topic value z , and is often referred to a  X  X opic distribution X . Topic values are generated from a document specific distribution  X  which in turn are generated from a  X  X other distribution X  over topics  X  . As was shown in [2] one can take the infinite limit K  X   X  in this model and arrive at the HDP. We will return to this infinite limit when we describe Gibbs sampling. In the following we will use the same graphical model for finite and infinite versions of ISBNs.
W N One of the key features of the HDP is that topics are shared across all documents indexed by j . The reason for this is the distribution  X  : new states are  X  X nvented X  at this level and become available to all other documents. In other words, there is a single state space for all copies of  X  . One can interpret j is an instantiation of a dummy, fully observed random variable  X  . We could add this node to the BN as a parent of z (since  X  depends on it) and reinterpret the statement of sharing topics as a fully connected transition matrix between states of  X  to states of z . This idea can be extended to a combination of fully observed parent variables and multiple unobserved parent variables, e.g. x  X  z . In this fashion we can connect together multiple vertical stacks  X   X   X   X  z where each such module is part of a  X  X irtual-HDP X  where the joint child states act as virtual data and the joint parent states act as virtual document labels. Examples are given in Figs. 1a (infinite extension of a Bayes net with IID data items) and 3a (infinite extension of Pachinko Allocation). To simplify the presentation we will now restrict attention to a Bayesian network where all CPTs are shared across all data-items (see Fig.1a). In this case data is unstructured, assumed IID and indexed by a flat index n = i . Instead of going through the detailed derivation, which is an extension of the derivation in [2] for HDP, we will describe the sampling process in the following.
 There is a considerable body of empirical evidence which confirms that marginalizing out the vari-ables  X  ,  X  will result in improved inference (e.g. [6, 7]). In this collapsed space, we sample two sets z and notice that all  X  are conditionally independent given z , x .
 Sampling  X  | ( z , x ) : Given x , z we can compute count matrices 2 N N of We then draw a number N k | l Bernoulli random variables with probability of success given by the cedure is equivalent to running a Chinese restaurant process (CRP) with N k | l customers and only after Antoniak [8]. Next we compute S k = P K a the number of occupied states. If the state corresponding to  X  is picked, a new state is created and we increment K a  X  K a + 1 . If on the other hand a state becomes empty, we remove it from the list and we decrement K a  X  K a  X  1 . This will allow assignment variables to add or remove given by, to group  X  b Importantly, equation 1 follows the structure of the original Bayes net , where each term has the form of a conditional distribution P ( z a the other data-cases. Hence, we can use the structure of the Bayes net to sample the assignment variables jointly across the BN (for data-case i ). The general technique that allows one to exploit network structure is  X  X orward-filtering-backward-sampling X  (FFBS) [3]. Assume for instance that the network is a tree. In that case we first propagate information from the leaves to the root, comput-on  X  X ownstream X  evidence. When we reach the root we draw a sample from P ( z root |{ x b } ) . Finally, we work our way back to the leaves, conditioning on drawn samples (which summarize upstream represent downstream evidence. For networks with higher treewidth we can extend this technique to junction trees. Alternatively, one can use cut-set sampling techniques [9]. In section 2 we introduced an index n to label the known structure of the data. The simplest nontrivial example is given by the HDP, where n = ( ji ) indexing e.g. documents and words. In this case the CPT  X  z | j is not shared across all data, but is specific to a document. Next consider Fig.1c where specific to chapters (and hence books) and is sampled from a Dirichlet distribution with mean given by a second level CPT  X  with mean  X  z , which finally is sampled from a Dirichlet prior with parameters  X  . Sampling occurs again in two phases: sampling  X  ,  X  | x , z and z |  X  ,  X  , x while marginalizing out  X  ,  X  . To sample from  X  ,  X  we compute counts N u | m,jk which is the number of times words were assigned in chapter j and book k to the joint state z = u,  X  = m . We then work our way up the stack, sampling new count arrays S, R as we go, and then down again sampling the CPTs (  X  ,  X  ) using not (unlike the other phase for z ) generate an equilibrium sample from this conditional distribution.  X  :  X  u  X  X  [(  X , R u )]  X   X  u | k  X  X  [  X   X  u + S u | k ] (3) A similar procedure is defined for the priors of  X  and extensions to deeper stacks are straightforward. If all z variables carry the same index n , sampling z n given the hierarchical priors is very similar to the FFBS procedure described in the previous section, except that the count arrays may carry a subset of the indices from n , e.g. N  X  ijk typically smaller resulting in a higher variance for the samples z . If two neighboring z variables carry different subsets of n labels, e.g. node z 0 compute. The general rule is to identify and remove all z 0 variables that are impacted by changing the value for z under consideration, e.g. { z 1 compute the conditional probability we set z = k and add the impacted variables z 0 back into the system, one-by-one in an arbitrary order and assigning them to their old values.
 directly proportional to N z b = k that was discovered for some parent state  X  b = l will not be available to other parent states, move down the Bayes net. When the network structure is a linear chain, this model is equivalent to the  X  X ested-DP X  introduced in [10] as a prior on tree-structures. The corresponding Bayes net is depicted in Fig.3b. A chain of length 1 is of course just a Dirichlet process mixture model. A DP prior is certainly appropriate for nodes z b with CPTs that do not depend on other parents or additional labels, e.g. nodes z 3 and z 4 in Fig.1a. Interestingly, an HDP would also be possible and would result in a different model. We will however follow the convention that we will use the minimum depth necessary for modelling the structure of the data. Example: HDP Perhaps the simplest example is an HDP itself, see Fig.1b. It consists of a single topic node and a single observation node. If we make  X  depend on the item index i , i.e.  X  obtain the infinite version of the  X  X ser rating profile X  (URP) model [11]. If we make  X  depend on j instead and add a prior:  X  the benefit that shared topics across documents can vary slightly relative to each other. Example: Infinite State Chains The  X  X achinko allocation model X  (PAM) [13] consists of a linear chain of assignment variables with document specific transition probabilities, see Fig.3a. It was proposed to model correlations between topics. The infinite version of this is clearly an example of an ISBN. An equivalent Chinese restaurant process formulation was published in [14]. A slight variation on this architecture was described in [15] (POM). Here, images are modeled as mixtures over parts and parts were modeled as mixtures over visual words. Finally, a visual word is a dis-tribution over features. POM is only subtly different from PAM (see Fig.3a) in that parts are not image-specific distributions over words, and so the distribution  X  z 1 | z 2 does not depend on j . Example: BiHDP This model, depicted in Fig.2a has a data variable x ji and two parent topic variables z 1 repeated index). The value of x is the rating of that customer for that product. The hidden variables customer group and a product group which together determine the factor from which we sample the rating. Note that the difference between the assignment variables is that their corresponding instead of two modalities, we can model multiple modalities (e.g. customers, products, year). Also, single topics can be generalized to hierarchies of topics, so every branch becomes a PAM. Note that for unobserved x ji values (not all products have been rated by all customers) the corresponding the Gibbs sampler.
 Example: The Mixed-Membership Stochastic Block Model[16] This model is depicted in Fig.2b. The main difference with HDP is that (like BiHDP)  X  depends on two parent states z i  X  j and z j  X  i by which we mean that item i has chosen topic z i  X  j to interact with item j and vice versa. However, (unlike BiHDP) those topic states share a common distribution  X  . Indices only run over distinct pairs i &gt; j . These features make the model suitable for modeling social interac-tion networks or protein-protein interaction networks. The hidden variables jointly label the type of interaction that was used to generate  X  X atrix-element X  x ij .
 Example: The Multimedia Model In the above examples we had a single observed variable in the graphical model (repeated over ij ). The model depicted in Fig.2c has two observed variables and an assignment variable that is not repeated over items. We can think of the middle node z 0 the class label for a web-page j . The left branch can then model words on the web-page while the right branch can model visual features on the web-page. Since no sharing is required for z 0 a Dirichlet prior. The other variables have the usual HDP priors. To illustrate the ideas we implemented two models: BiHDP of Fig.2a and the  X  X robabilistic object model X  (POM), explained in the previous section.
 Market Basket Data In this experiment we investigate the performance of BiHDP on a synthetic market basket dataset. We used the IBM Almaden association and sequential patterns generator to create this dataset [17]. This is a standard synthetic transaction dataset generator often used by the association research community. The generated data consists of purchases from simulated groups of customers who have similar buying habits. Similarity of buying habits refers to the fact that customers within a group buy similar groups of items. For example, items like strawberries and cream are likely to be in the same item group and thus are likely to be purchased together in the same market basket. The following parameters were used to generate data for our experiments: 1M transactions, 10K customers, 1K different items, 4 items per transaction on average, 4 item groups per customer group on average, 50 market basket patterns, 50 customer patterns. Default values were used for the remaining parameters.
 The two assignment variables correspond to customers and items respectively. For a given pair of customer and item groups, a binomial distribution was used to model the probability of a customer group making a purchase from that item group. A collapsed Gibbs sampler was used to fit the model. After 1000 epochs the system converged to 278 customer groups and 39 item factors. Fig.4 shows the results. As can be seen, most item groups correspond directly to the hidden ground truth data. The conclusion is that the model can learn successfully the hidden structure in the data. Learning Visual Vocabularies LDA has also gained popularity to model images as collections of features. The visual vocabulary is usually determined in a preprocessing step where k-means is run to cluster features collected from the training set. In [15] a different approach was proposed in which the visual word vocabulary was learned jointly with fitting the parameters of the model. This can have the benefit that the vocabulary is better adapted to suit the needs of the model. Our extension of their PLSA-based model is the infinite state model given by Fig.3a with 2 hidden variables (instead of 3) and  X  z 1 | z 2 independent of j . x is modeled as a Gaussian-distributed random variable over feature values, z 1 represents the word identity and z 2 is the topic index.
 We used the Harris interest-point detector and 21  X  21 patches centered on each interest point as input to the algorithm. We normalized the patches to have zero mean. Next we reduced the dimensionality of detections from 441 to 100 using PCA. The procedure described above generates a set of between Figure 4: The 10 most popular item groups learned by the BiHDP model (left) compared to ground truth 50 and 400, 100-dimensional detections per image. Experiments were performed using the Caltech-4 and  X  X urntable X  datasets. For Caltech-4 we used 130 randomly sampled images from each of the 4 categories for training. LDA was fit using 500 visual words and 50 parts (which we found to give the best results). The turntable database contains images of 15 toy objects. The objects were placed on a turntable and photographed every 5 degrees. We have used angles 15, 20, 25, 35, 40, and 45 for training, and angles 10, 30, and 50 for testing. LDA used 15 topics and 200 visual words (which again was optimal). LDA was then fitted to both datasets using Gibbs sampling. We initialized POM with the output of LDA to make sure the comparison involved similar modes of the distribution. The precision-recall curves for this dataset are shown in Fig.5. Images were labelled by choosing probability of the query image given the part probabilities of the retrieved image.
 These experiments show that ISBNs can be successfully implemented. We are not interested in claiming superiority of ISBNs, but rather hope to convey that ISBNs are a convenient tool to design models and to facilitate the search for the number of latent states. We have presented a unified framework to organize the fast growing class of  X  X opic models X . By merging ideas from Bayes nets, nonparametric Bayesian statistics and topic models we have arrived at a convenient framework to 1) extend existing models to infinite state spaces, 2) reason about and design new models and 3) derive efficient inference algorithms that exploit the structure of the underlying Bayes net.
 Not every topic model naturally fits the suit of an ISBN. For instance, the infinite HMM [18] is like a POM model with emission states, but with a single transition probability shared across time. When marginalizing out  X  this has the effect of coupling all z variables. An efficient sampler for this model was introduced in [19]. Also, in [10, 20] models were studied where a word can be emitted at any node corresponding to a topic variable z . We would need an extra switching variable to fit this into the ISBN framework.
 We are currently working towards a graphical interface where one can design ISBN models by attaching together H k DP modules and where the system will automatically perform the inference necessary for the task at hand.
 Acknowledgements This material is based upon work supported by the National Science Foundation under Grant No. 0447903 and No. 0535278 and by ONR under Grant No. 00014-06-1-0734.

