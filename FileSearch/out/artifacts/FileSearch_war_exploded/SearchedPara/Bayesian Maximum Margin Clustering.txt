
Clustering has been studied extensively in machine learn-ing and data mining. Generally speaking, the target of clustering is to find out how data are organized. Generative approaches seem natural for clustering. Given x representing the instance and  X  denoting the cluster assignment of an instance, generative methods model the class-conditional The contribution of unlabeled data is obtained by improving the fit of  X  ( x ) . Prediction is given by posterior probability which is obtained through the Bayes theorem. Obviously, Bayesian approaches are convenient to be integrated in generative models. However, generative models add restrict assumptions on  X  ( x  X   X  ) and  X  (  X  ) , the clustering results may be unconvincing when these assumptions are violated.
Recently, discriminative clustering methods have at-tracted more and more renewed attentions, e.g., spec-tral clustering (SC) [2][3] and maximum margin clus-tering (MMC) [4][5][6][7]. Both of them are learning a discriminative model from unlabeled data by connecting the posterior probability  X  (  X   X  x ) and  X  ( x ) directly through the low-density separation assumption. To express low-density separation preference, all of these algorithms learn in a non-Bayesian way that employ loss functions as the implementation of the assumption. Recent research in com-puter vision [8][6] demonstrates the power of discriminative clustering methods. Besides its usefulness in practice, these methods are also theoretically appealing [9][10].
Although non-Bayesian approaches provide additional flexibility and convenience in designing loss functions, they neither capture the uncertainty in predictions, nor have the ability to learn the hyperparameters. These limitations can be skirted via Bayesian approaches. However, how to extend Bayesian approaches to implement the low-density separation assumption in the unsupervised setting has not been considered yet.

To address the dilemma mentioned above, we pro-pose a Bayesian low-density separation clustering method in this paper, named Bayesian maximum margin cluster-ing (BMMC), which combines the advantages of both the Bayesian learning and the discriminative model. More specifically, BMMC is based on the Bayesian support vector machine (Bayesian SVM) [11]. However, the same as the other discriminative models, the Bayesian SVM is incapable in the unsupervised setting. Following the way in [12] for semi-supervised problems, we adopt the augmented model to extend the learning ability of Bayesian SVM to unlabeled data and to capture the spirit of the low-density separation. The proposed BMMC is the counterpart of the existing MMC in the Bayesian framework. Compared with the exist-ing MMC, our model eliminates the integer variables, and thus makes the optimization much easier to solve.
On the other hand, prior knowledge plays an important role in machine learning, especially in clustering prob-lems. From the regularization perspective, the low-density separation assumption can be viewed as a general priori for clustering. Both of SC and MMC are learning under the priori where domain-dependent prior knowledge can be embedded into kernel, and it seems that these algorithms are flexible enough to different problems in practice. Never-theless, choosing a suitable kernel for a special problem is equivalent to selecting a prior distribution of functions [13] and is indeed a job for oracle. Although several kernel learning methods have been proposed recently, they are still limited in the learning environment in which side information is sufficient. In contrast, the proposed model allows the users to encode prior knowledge via a set of data, which is called Universum [14]. Universum, introduced by Vapnik firstly through maximal contradiction on Universum principle [15][16] and applied to semi-supervised learn-ing [17][18], has not been utilized in unsupervised problems yet. Defining a Universum set is approximately equivalent to specifying a kernel, but much easier. The Universum can be integrated into our model as a natural extension. It is notable that our model can also handle the classical clustering problems in which no Universum is available.
The remainder of this paper is organized as follows. After introducing the clustering problem with the Universum, MMC and Bayesian SVM in the next section, we present the probabilistic model and the inference method in Section 3. The relationship between MMC and the log-posteriori of BMMC is discussed in Section 4. Two different perspectives of Universum are also explained. To validate our model, experimental results are reported in Section 5. The last section gives the conclusion.
 A. Contributions -A low-density separation clustering method is proposed -Universum in which prior knowledge embeds can be -The relationship between MMC and BMMC is studied. A. Clustering with the Universum
Since our model can utilize the Universum , the problem which we could solve is more general than the classical clustering algorithm. We first define the problem as well as the notations used in this paper formally.

Assume we are given a data set  X  consisting of  X  instances { x 1 , x 2 ,..., x  X  } drawn i.i.d from a certain distri-bution  X  .Here x  X   X   X   X  (  X  =1 , 2 ,..., X  ) is the input feature vector. In addition to these instances, a collection of exam-the same domain as the problem of interest but not belonging to either class may be also available. The set  X  is called Universum set in which meaningful prior information about the task at hand can be embedded. Without loss of generality, we assume the label of instance  X   X   X  X  X  1 , +1 } as MMC [4]. Two different approaches for multi-class clustering extension are discussed later. Our task is to assign each instance in a label  X  or an uncertainty prediction. In the absence of the Universum, this problem is the same as classical clustering and can also be solved by BMMC.
 B. Maximum Margin Clustering
Large margin methods, e.g., support vector ma-chine (SVM) and adaboost, have been applied in many supervised tasks successfully. Given the training instances and corresponding labels, the goal of SVM is to find a discriminant function  X  ( x )= w  X  x +  X  by solving the following optimization: where  X  is the slack variable and  X  balances the regulariza-tion and the loss function.

The dual of the above optimization is: where  X  =[  X  1 , X  2 ,..., X   X  ] , e =[1 , 1 ,..., 1] and  X  the elementwise product between matrics. By kernel trick, the linear function can be easily extended to non-linear form.
Maximum margin clustering, which is proposed by [4], can be viewed as an unsupervised extension of support vector machine. The key idea of maximum margin clustering is to find a labelling so that the obtained margin would be maximal over all candidate labellings. Based on this intuition, the optimization is written as To prevent the meaningless solution that assigns all the instances into the same class, the class-balance constraint,  X   X   X  e  X  y  X   X  , had been introduced by [4].

The optimization (2) is difficult to solve because it contains integer variables and is non-convex. To make the optimization tractable, [4] relaxed it as a semi-definite program (SDP). Valizadegan and Jin [5] proposed the gen-eralized maximum margin clustering that reduce the  X  2 optimization variables to  X  , thus made a significant reduction of computational cost. To make the MMC more practical, many optimization methods have been proposed [6][7][19]. However, all of these methods are limited in the non-Bayesian domain. Moreover, they relaxed the integer op-timization technically. In contrast, our model avoids the integer optimization from Bayesian approach naturally. C. Bayesian Support Vector Machine The Bayesian support vector machine (Bayesian SVM) [11] provides a probabilistic interpretation of the margin concept in supervised setting. Our model focuses on dividing instances through low-density region without labels by extending the Bayesian SVM. Thus, it is worth establishing the background of the Bayesian SVM.
The Bayesian SVM can be viewed as a special case of the Gaussian process classifier. The only difference between the Bayesian SVM and the traditional Gaussian process classifier is their noise model. Traditional Gaussian process classifiers adopt logistics or probit function while the Bayesian SVM defines a distribution which has some special properties, e.g., sparsity and facility in introducing the Universum.

More formally, the Bayesian SVM adopts the probability of obtaining output  X  given x as where  X  (  X , X  ( x )) is the hinge loss,  X  (  X  )= 1 1+  X  X  X  X  (  X  2  X  ) is chosen to make the probabilities less than 1. It is still necessary to introduce a null category (labeled by  X  =0 )to make the noise model consistent, The null category can be considered as a probabilistic inter-pretation of the  X  X argin X  concept in standard SVM. Noticing the Gaussian process priori over function with covariance  X  (  X ,  X   X  ) , the log-posteriori of the model formulates as =  X  Since the maximum achieves at  X   X  ( x )= the maximum a posteriori (MAP) is identical to standard SVM.
 As studied in [20], the phenomenon that the Bayesian SVM cannot distinguish instances in the unsupervised set-ting is caused by the independence between  X  and unlabeled data. This means that the knowledge of instances X cannot affect the posterior function distribution when labels are unobserved. In this section, we begin with specifying the augmenting probabilistic model of Bayesian SVM following the way studied in [12] to handle unsupervised problems in the spirit of low-density separation assumption. Prediction and multi-class clustering extension methods are introduced lastly.
 A. Probabilistic Model
To make unlabeled data have an effect on the posterior distribution of  X  , we could restore the dependence by aug-menting the model following [12]. Introducing an additional variable  X  which is the child of the label variable and always observed, the traditional Gaussian process transforms as shown in Figure 2. The shaded nodes can always be observed while the unshaded nodes are latent variables. Assuming the variable  X  is an indicator identifying whether the instance belongs to the Universum set or is an instance that needs to be assigned label, e.g.,  X  takes the value 0 when the instance belongs to the Universum set, and vice versa, the dependence between  X  and unlabeled data has been established. Based on the definition of variable  X  and the problem defined in Section 2, we have  X  (  X   X  =0  X   X   X  =0)=1 meaning that any samples from the Universum set must be observed. Meanwhile, we assign the probabilities of unlabeled data by where  X  + +  X   X  equals to 1 implicitly. Through modifying the value of  X  + and  X   X  , we can control the size of the two clusters similarly to the effect of class-balance constraint [4]. Thus, if instances belong to the Universum set, the posterior process is updated by  X  (  X   X   X  ( x )) ; otherwise, the instances need to be assigned labels and the process is updated by  X  (  X   X   X  ( x )) which can be computed as Recall the special noise model defined in the Bayesian SVM:  X  (  X   X   X  ( x ))= we obtain the effective likelihood function  X   X  (  X  ) by com-bining (3) into the augmenting model, (4a) encourages  X  at each instance far away from margin leading to low-density separation and (4b) reflects the effect of prior knowledge incorporated in the Universa which helps locating margin. The derivations of Universum from two different perspectives will also be discussed in Section 4. B. Prediction
To provide the confidence besides the labelling, we take a fully Bayesian approach that needs to compute the posterior distribution over the latent variables. However, the poste-riori cannot be solved in an analytic way. We thus adopt Laplace X  X  method to find an approximation. Doing a second order Taylor expansion of log  X  ( f  X  X  X  , Z , X  X  , Y  X  ) around the maximum, a Gaussian approximation of posteriori will be obtained. The log-posteriori of BMMC is non-convex. It is because of the intrinsic difficulty of clustering problem, i.e., the exchangeability of labels. For clustering, an assignment is equal to another one that exchanges labels with counter-part in the sense of distinction. Thus, one local minimum is enough and it is reasonable that we employ Laplace X  X  method to inference approximately.

Based on the effective likelihood function (4a) ,(4b) and the priori over f specified through Gaussian process, the log-posteriori of latent variable f is formulated as We will prove that the optimization in MMC proposed by [4] is an approximation of (5) without considering the Univer-sum later. However, the existing MMC methods add labels of instances, which are integer variables, into optimization while our model eliminates them making the optimization much easier to solve.

To find a nontrivial maximum of (5), we utilize New-ton X  X  method with special initial solution inspired by recent development of deep learning which claimed that the 2  X  X  X  -order optimization method can handle highly non-linear criteria [21]. We also implement an EM style algorithm to optimize this criterion. We neglect the EM algorithm because of the page limitation. The performances of these two op-timize algorithms are similar in classical clustering setting. Differentiating (5) w.r.t. f , we obtain  X  X  X  = g  X   X   X  1 f  X  X  X  X  =  X   X   X   X   X  1 , where g =[  X   X  ] ,  X  =1 , 2 ,..., X  +  X   X   X  and  X  =  X  X  X  X  X  X  (  X   X  ) ,  X  =1 , 2 ,..., X  +  X   X   X  ,  X   X  is computed by  X   X   X   X   X  Algorithm 1: Bayesian Maximum Margin Clustering
Input :  X  = { x 1 , x 2 ,..., x  X  } (clustering dataset),
Output : y or  X  ( y  X  X  X  ,  X  ) begin end return y =  X  X  X  X  ( f ) or  X  ( y  X  X  X  ,  X  )  X  (  X   X  , X   X  ) denotes exp(  X   X  X  X  (  X   X  , X   X  )) , and  X  (  X   X   X   X   X  1 , otherwise,  X  (  X   X  )=0 .

In each iteration, we update f by (8) until a convergence is reached,
After finding the maximum  X  f , we specify the Laplace approximation of the posteriori
The uncertain prediction is making by computing
Remark 1: In (8), computing (  X   X  1 +  X  )  X  1 costs too much and may cause numerical instability. As specified in [13] that it is trivial to get where b =  X  f +  X  ,  X  =(  X  +  X  1 2  X  X  X  1 2 ) , and  X  is the Cholesky decomposition of  X  . By this trick, we can achieve numerical stability rather than updating by (8) directly.
Remark 2: To find a better minimal, a good initial solu-tion is needed. In our implementation, the initial solution f 0 =[  X  X  X  X  ( v 2  X  1  X  v  X  2 1 ) , 0 ,..., 0] is used, v 2 being the second smallest eigenvector of  X   X  1  X   X   X  . However, computing  X   X   X   X  is costly and not stable. We adopt the Laplacian ma-trix L as the pseudo-inverse of  X   X   X   X  [5]. This initialization is inspired by the pre-training phase in deep learning that finds a point near the maximum of hidden layer greedily.
Remark 3: When we just focus on finding the labels and do not need the confidences, we may compare  X  (1 ,  X   X   X  )  X  (  X  1 ,  X   X   X  ) to get a guess directly. Compared with MMC and GMMC whose computational complexities are  X  (  X  6 . 5 ) and  X  (  X  4 . 5 ) , our method costs  X  (  X  X  X  3 ) in such situation, where  X  is the number of iterations. If we use the pseudo-inverse of  X  specified above and subgradient descent method to optimize the MAP (5) directly, the computational complexity could reduce to  X  (  X  X  X  2 ) .

Remark 4: An important property of BMMC is that it can provide both uncertain predictions and labels. For different clustering tasks targets, different output of the BMMC could be utilized. Moreover, the uncertain predictions open doors to apply BMMC to active learning by querying the most uncertain instances.
 C. Multi-Class Clustering Extension
Although the discussion about the Bayesian maximum margin clustering focused on the two-class clustering setting above, the algorithm can be extended to multi-class cluster-ing problems easily. In this part, we propose two different methods.

Firstly, a heuristic method [8][6] can be adopted to extend the BMMC to the multi-class clustering setting. We can execute the two-class clustering method recursively. By this approach, a hierarchical clustering algorithm is formed. The other method is modifying the noise model as [22]. Assume that there is  X  Gaussian processes, f  X  , each one corresponding to a class and y  X  is the class indicator, the noise model  X  ( y  X   X  f  X  ( x )) is {  X  (  X   X  X  X  &gt; X   X  X  X  +  X ,  X   X   X  =  X  )  X  ( f  X  , I  X  ) , X   X  X  X  =1 , 1  X  Thus, the  X   X  (  X  ) is {  X  1  X  In this paper, we mainly focus on the first extension method and the results presented in experiment are all based on hierarchical clustering. The second extension method is our future work.

It is natural to investigate the relationship between maxi-mum margin clustering [4] and the proposed model. As the MAP of Bayesian SVM is equivalent to SVM [11], we will prove that MMC is an approximation of the log-posteriori of the proposed model in this section. In addition, two different derivations of the Universum will be discussed here to clarify its role in learning process.
 A. Relationship between MMC and BMMC
Theorem 1: Maximum margin clustering is an approxi-mation of (5) without considering the Universum. likelihood is defined as Thus, the log-posteriori transforms as
 X  = For convenience, we denote  X  (1 , X  ( x  X  )) as  X   X  + and  X  (  X  1 , X  ( x  X  )) as  X   X   X  . Noticing that the first term on the right hand of (11) is the soft maximum function which is an approximation of the max-function, we have argmax which is equivalent to Set  X  + =  X   X  and realize  X  ( x )= transforms as Recall the primary form of MMC, Substituting KKT condition w = optimization, we obtain (13).

Corollary 2: From regularization perspective, the second term on the right hand of (5) can be viewed as another loss function which implements the maximal contradiction on Universum principle , different from [14] which adopts insensitive loss. Thus, (5) is an implementation of Maximum Margin Clustering with Maximal Contradiction on Univer-sum principle .
 B. Influence of the Universum
In this section, we discuss the details about the influence of the Universum in clustering problems to clarify its role in learning process. Although the Universum derived from two different perspectives, the same conclusion about the effect of Universum is achieved: incorporating prior knowledge about the discriminant function.

Inference with the Universum could be derived from maximal contradiction on Universum principle [14]. This principle, introduced by Vapnik [15][16] firstly, is a struc-tural risk minimization (SRM) principle which is parallel to the well-known maximal margin principle but more flexible. The principle prefers the equivalence function class that makes more different predictions on Universum set. Given a Universum set, the corresponding prior distribution about functions has been specified. Defining a prior function distribution is a direct way to construct a structure on the set of admissible functions. However, finding a suitable domain-dependent function distribution is very hard. Embedding priori into the Universum set makes the priori choosing problem much easier.
 Rather than being introduced by a novel SRM principle, Universum in the proposed model is just a natural exten-sion of the null category. As presented in [11][12], the null category is introduced to make the probability  X  (  X   X   X  ) consistent. Based on the interpretation of null category, i.e., the range of margin, we can claim the influence of the Universum: providing the prior knowledge about the range of decision boundary location. From this perspective, a Universum selection criterion is clear as Remark 5 . A similar selection criterion is also obtained by rigorous mathematical analysis in [23].

Remark 5: The nearer the mean of Universa locates to the margin, and the tighter Universa are, the more helpful they are.

Of course one can point out the Universa directly when has some knowledge about the boundary location. In this paper, we consider two ways to generate the Universum from the dataset which is needed to be separated for experiment following [14][18]:
From the discussion above, the influence of the Universum is clear. The Universum provides a convenient way to impose the domain-dependent prior knowledge on the proposed model while most of the state-of-the-art algorithms have not considered yet. However, the Universum is not a requisite and the proposed model can also handle clustering problems without the Universum.

Although we verified the helpfulness of these two Uni-versa generating methods on some special datasets in Section 5, these two Universum set generating methods are just general strategies. As [14][23] claimed, the Universum needs to be chosen quite carefully in order to be helpful. Thus, the Universa choice method is still an open problem for different tasks.

To demonstrate that the Bayesian maximum margin clus-tering method (BMMC) is efficient and effective, we de-sign a series of experiments on synthetic and real-world benchmark datasets. For the datasets that the instances with different labels form different clusters, evaluating clustering on such datasets is reasonable [24]. For comparison, the k -means algorithm, the generalized maximum margin cluster-ing (GMMC) [5], the normalized spectral clustering (NC) [2] and the proposed method are implemented in Matlab. The CVX [25] package is employed to solve SDP involved in GMMC. In the second part, we demonstrate the effect of Universum in small-sample clustering problems. At last, time costs are listed.
 A. Classical Clustering
Firstly, we examine our method on some synthetic data sets with RBF kernel. We extend the BMMC to multi-class clustering problems by top-down hierarchical ap-proach [8][6]. The kernel width  X  is set to 0 . 08 for all the synthetic data sets expect the last one which is set to 0 . 16 and the  X  + =  X   X  . Experimental results are illustrated in Figure 3. The results of the proposed BMMC on four clas-sical synthetic two-class clustering datasets are illustrated in the first line. The second line are results of some multi-class clustering problems. It is obvious that although the proposed BMMC is focused on two-class problems originally, top-down hierarchical approach can extend our method to multi-class clustering problems efficiently.

Because the clustering ability of divisive hierarchical clustering methods for multi-class clustering problems is based on their discriminative ability on two-class clustering problems, we focus on comparing the performances in the two-class clustering setting on real-world datasets. Thus, we follow the evaluation methods of [4][5][6][19] that focused on two-class clustering problems. Besides the best perfor-mance comparison on benchmark following these literatures, we also verify the stability and convergence of BMMC by experiments which means that the solution of BMMC does not change much with respect to the sampling process and converges to a certain labelling [26].

Benchmarks of handwritten digits and text, MNIST , USPS and 20Newsgroups , are taken for performance evaluation. It has been demonstrated that the instances with different labels form different clusters in handwritten digits and text datasets [27]. Thus, they are suitable for evaluating the clustering methods.

Handwritten Digits In our experiments, we use 1 vs. 7, 3vs.8,5vs.8,8vs.9on MNIST and USPS for difficult discrimination visually. Each digit in MNIST is represented by a 784 dimensional vector while in USPS is represented by a 256 dimensional vector. Among the experiments, we adopt RBF kernel for all the methods. For NC and GMMC, the kernel width is tuned beforehand from { 10  X  2 ,..., 10 3 } NC, the number of nearest neighbors  X  of adjacency matrix is tuned from 3 to 12. For GMMC, the  X   X  is chosen from { 10 0 ,..., 10 6 } . Although we can adopt type-II maximum likelihood to learn the parameters of BMMC, we tuned the parameters of BMMC in the same range as in NC for a fair comparison. We fix  X   X  in GMMC and  X  in BMMC. The best results and the average performance of 10-trials are reported in Figure 4 and 5 where the error is measured following [4][5][6][19]. Since the performance of GMMC is not stable, the average performance of GMMC is not reported. We do not perform GMMC when  X &gt; 700 because its computational complexity is too high.

Text clustering We present the experimental result on the 20Newsgroups data set, in which the instances have 26214 features. The classes which have most instances are selected. We choose kernel from the cosine kernel and RBF kernel to have better performance for all the methods. The parameters are tuned from the same sets for NC, GMMC and BMMC. The results are reported in Figure 6. The performances of BMMC and NC are similar on this dataset. Maybe the reason is that the NC achieves the global minimum and BMMC trapped in the same optimum.

Results From the comparison with NC and GMMC, both the best result and the average accuracy of the proposed method achieve comparable or even better performance in many situations on the image and text clustering problems. Moreover, with the number of instances increasing, the average performance of the proposed BMMC converges. The more instances, the better performance BMMC achieves. The stability of the BMMC could be verified by the variance of each 10-trials performance on random sampling subsets. The variances of BMMC on most subsets is comparable to NC which is known as a stable clustering algorithm [10]. Thus, we demonstrate that the convergence and stability of the proposed BMMC empirically.
 B. Universum in Small-Sample Clustering
In this part, we evaluate the effect of Universum in small-sample clustering. The solution of BMMC here is got by EM algorithm. First, a synthetic data set and two different Universum sets are constructed. Considering clustering on two Gaussian distributions centered at (  X  1 . 5 ,  X  1 . 5) (+1 . 5 , +1 . 5) respectively, we generate two different Uni-versum sets to verify Remark 5 . One is around the origin and the other one is far from the boundary. The results are illustrated in Figure 7. Figure 7(a) is the ground truth of the instances. Figure 7(b) is the result of BMMC. Figure 7(c) and 7(d) are the results of BMMC with different Universum sets respectively. Obviously, the Universa around the mean of instances provides correct information about the range of margin and is much more helpful for the clustering task than Figure 7(d). Error prior knowledge contained in Universa in Figure 7(d) even debases the performance of BMMC.
Next, we examine the effect of Universum on real-world datasets. To examine the effect of prior knowledge closely, we reduce the data set scale. We select one-half of the fea-tures from MNIST and 20Newsgroups respectively. For each trial, 100 instances are chosen randomly from the most two difficult problems in MNIST and 20Newsgroups , and 20 Universa are generating randomly. We verify the two approaches for constructing Universum set mentioned above. The instances in  X   X  X  X  X  X  X  are generated nearby the mean of given dataset, while the features of the instances in  X   X  X  X  X  are generated according to the whole datasets empirical distribution. The results are showed in Table I.
In the small-sample datasets, the performances by adding the Universum chosen by these two criteria are similar. We cannot assert which one is better. Like adding Universa under supervised and semi-supervised setting in [14][18], some progress are made in clustering problem, especially in handwritten digits dataset. However, the improvements on text clustering are not significant. The same phenomenon is observed in [14] on RCV1 . Maybe it is caused by these two Universum choice strategies are just general criteria, and they cannot reflect the location of margin for different topic texts. Thus they are not suitable for text clustering problem. We also observed the phenomenon, described in [14], that when dataset is large, the instances provide enough information about the  X  X argin X  themselves and the influence of the Universum diminishes.

For special tasks, the Universum choice is indeed a important work. Generating Universa carelessly may even hurt the performance. We will investigate this phenomenon in our future work.
 C. Speed
Speed is another important issue besides accuracy. SDP based MMC solving methods are computationally expensive thus not practical. Because of the need of running GMMC, we examine the time cost of each algorithm on several medium datasets. Heart and Ionosphere are adopted, which contain 270 instances and 351 instances, respectively. We also record the running time on subset of MNIST and 20Newsgroups . We select 400 instances from the pair 3 vs. 8, 5 vs. 8 in MNIST randomly and 600 instances from the pair 6 vs.14 and 9 vs. 10 in 20Newsgroups randomly. The empirical running time is showed in Table II .
In this paper, we proposed a probabilistic model which extends the maximum margin clustering method to the Bayesian framework. We proved that MMC is an approxima-tion of the log-posteriori of BMMC without considering the Universum. Compared with the existing MMC, the proposed model can provide not only a guess but also the confidence when it is necessary. The existence of the integer variables in the optimization, i.e., labels of instances, is one of the intrinsic difficulties of the MMC. They are eliminated in our model making the optimization easier to solve. Moreover, our model can utilize Universum naturally which makes the algorithm more flexible to different problems. Finally, em-pirical results show promising performance of the proposed model and verify the helpfulness of Universum in clustering, especially in the small-sample clustering problem.
For future work, a more efficient approximate inference method and the second multi-class clustering extension method mentioned in Section 3 are needed to be investigated. Moreover, the strategy for choosing the appropriate Universa still needs more research.

This work is supported by National Science Foundation of China (NSFC #60275025). We thank the reviewers for many helpful suggestions to improve this paper.

