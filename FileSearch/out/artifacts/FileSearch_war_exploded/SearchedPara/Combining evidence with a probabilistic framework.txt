 1. Introduction
Question answering (QA) aims at finding exact answers to a user X  X  natural language question. For simple questions, direct database lookup has been applied to find answers from large knowledge-bases. In this approach, the question is transformed into a database query and the answer is extracted from the knowledge-bases using the query. However, this approach is not sufficient for many questions because of the limited information in knowledge-bases, and was applied for specific domains.
For the past several years, Text REtrieval Conference (TREC) facilitated the development of open-domain question answering to answer more general and arbitrary questions from a large collection of documents. Most QA systems (Clarke, Cormack, &amp; swer selection.
Question analysis is a step to produce a strategy for answering the question, which includes a list of question keywords, alternative forms of the query terms, and the answer type of the question. In recent QA systems which use predicate struc-retrieval step to search for relevant documents or passages from a large collection of documents. In the answer extraction step, a list of answer candidates is extracted from the retrieved documents or passages. Finally, answer selection pinpoints correct answer(s) from the extracted candidate answers. Since the first three processes in the QA pipeline may produce erro-neous outputs, the final answer selection process entails identifying correct answer(s) amongst many incorrect ones.
Fig. 1 shows a traditional QA architecture with an example question. Given the question  X  X  Which city in China has the larg-est number of foreign financial companies?  X , the answer extraction component produced a list of five answer candidates. Due to imprecision in answer extraction, an incorrect answer ( X  X  X eijing X ) was ranked at the top position. The correct answer ( X  X  X hanghai X ) was extracted from two documents with different confidence scores and ranked at the third and the fifth posi-tions. In order to rank  X  X  X hanghai X  in the top position, we have to address two interesting challenges:
Answer relevance : How do we identify relevant answer(s) amongst irrelevant ones? This task may involve searching for evidence of a relationship between the answer and the answer type or a question keyword. For example, we might wish to query a knowledge-base to determine if  X  X  X hanghai X  is a city ( China ( IS-IN(Shanghai, China) ).

Answer similarity : How do we exploit similarity among answer candidates? For example, when the candidate list contains redundant answers (e.g.,  X  X  X hanghai X  as above) or several answers which represent a single instance (e.g.  X  X  X SA X  and  X  X  X he
United States X ), to what extent should we boost the rank of the redundant answers? Note also that effective handling of redundancy is particularly important when identifying a set of novel answers for list or definition questions.
Although many QA systems address these issues separately, there has been little research on generating a probabilistic framework that considers both answer relevance and similarity. In our recent work (Ko, Si, &amp; Nyberg, 2007b), we proposed a probabilistic framework which uses an undirected graphical model to consider answer relevance and similarity. This Even though this joint approach provides a formal framework for answer ranking, it requires O(2 is the size of the graph (i.e. number of answer candidates). This paper focuses on a simpler but effective framework which uses logistic regression to estimate the probability that an answer candidate is correct given the degree of answer correct-ness and the amount of supporting evidence provided in a set of answer candidates.

The hypotheses of the paper are that: (1) the framework effectively combines multiple evidence for identifying answer relevance and their correlation in answer ranking, (2) the framework supports answer merging on answer candidates re-turned by multiple extraction techniques, (3) the framework can support list questions as well as factoid questions, (4) the framework can be easily applied to a different QA system, and (5) the framework significantly improves performance of a QA system.

The rest of the paper is arranged as follows: Section 2 describes related work. Section 3 describes our answer ranking 6 describes experimental results on TREC factoid question and Section 7 describes experimental results on list questions. Section 8 discusses the significance of the work. Finally, Section 9 concludes with suggestions for future research. 2. Related work
To select the most probable answer(s) from the answer candidate list, QA systems have applied several different answer ranking approaches. One of the common approaches is filtering. Filtering relies on precompiled lists or ontologies such as
WordNet, CYC and gazetteers to compare the expected answer type of the question with the type of answer candidates and then remove a candidate whose type does not match the expected answer type (Nyberg et al., 2004; Prager et al., 2004; Schlobach, Olsthoorn, &amp; de Rijke, 2004; Xu, Licuanan, May, Miller, &amp; Weischedel, 2003).
Answer validation is another popular approach for answer ranking. Xu et al. (2003) applied several type-specific con-that best satisfied the constraints. Moldovan, Clark, Harabagiu, and Maiorano (2003) converted question and answer candi-dates into logic representation, and used a logic prover to prove answer correctness using axioms obtained from WordNet.
Magnini, Negri, Pervete, and Tanev (2002) reranked answers according to the validation scores calculated by co-occurrence of question keywords and an answer candidate in the Web text snippets.

Even though each of these approaches uses one or more resources to independently support an answer candidate, few have considered the potential benefits of combining resources together as evidence. As research that does com-bine resources, Schlobach et al. (2004) combined geographical databases with WordNet in order to use more than one resource for answer type checking of location questions. However, in their experiments the combination actually hurt performance because of the increased semantic ambiguity that accompanies broader coverage of location names.
This demonstrates that the method used to combine potential answers may matter as much as the choice of resources.

Collecting evidence from similar answer candidates to boost the confidence of a specific answer candidate is also impor-tant for answer ranking. As answer candidates are extracted from different documents, they may contain identical, similar or complementary text snippets. One approach to exploit redundancy is to incorporate answer clustering (Jijkoun, van Rant-2007). For example, we might merge  X  X  X pril 1912 X  and  X  X 14 April 1912 X  into a cluster and then choose one answer as the clus-tered answers. Some simple approaches calculated the cluster scores by counting the number of answers in the cluster the individual answer scores in the cluster (Lin &amp; Katz, 2003).

Exploiting redundancy is even more important in multi-strategy QA, in which multiple answering agents are used candidates come from different agents with different score distributions, exploiting answer redundancy plays an impor-tant role in answer ranking. To exploit this type of redundancy, simple confidence-based voting has been used to merge the top five answers returned by multiple QA agents ( Chu-Carroll et al., 2003 ). As a more advanced approach, a maximum-entropy model has been used to rerank the top 50 answer candidates returned from three different answering strategies ( Echihabi et al., 2004 ). This model contained more than 30 features generated from the internal QA components (e.g. the rank of answer candidates, the count of each answer in the document corpus) and improved the performance of answer selection by combining complementary results provided by different answering strategies. Although previous work has utilized evidence from similar answer candidates for a specific answer candidate, the algorithms only modeled each an-swer candidate separately and did not consider both answer relevance and answer correlation to prevent the biased influ-ence of incorrect similar answers. 3. Answer ranking framework
In the previous section, we raised two challenges for answer selection: how to identify relevant answers and how to ex-ploit answer redundancy to boost the rank of relevant answers. In order to address the two issues, the answer ranking pro-cess should be able to conduct two subtasks. One task is to estimate the probability that an answer is relevant to the question. This task can be estimated by the probability P ( correct ( A ability P ( correct ( A i ) j A i , A j ) by considering another answer candidates (e.g., A answer selection performance, we believe it is important to combine the two tasks in a unified framework and estimate the probability of an answer candidate P ( correct ( A i ) j Q , A
Our answer ranking framework estimates the probability P ( correct ( A method that directly models P ( Y j X ) by learning parameters from training data and has been successfully employed in many logistic regression to predict the probability that an answer candidate is correct given the degree of answer correctness tion can be found in our preliminary research work in Ko, Si, and Nyberg (2007a) .
 individual answer candidate A i . Each sim 0 k  X  A i ; A j
A .

Each sim k ( A i ) represents one similarity feature for an answer candidate A answer relevance and similarity features in detail.

For simplicity, we sum the N 1 answer similarity scores to exploit answer redundancy. But other approaches can be culated by multiplying extractor scores with the string similarity scores.
 is the number of training questions and N j is the number of answer candidates for each question Q tion, we used the Quasi-Newton algorithm ( Minka, 2003 ).
The estimated answer probability is used to rank answer candidates. For factoid questions, the top answer is selected as a we may use the framework to identify incorrect answers: if the probability of an answer candidate is lower than 0.5, it may given corpus ( Voorhees, 2003 ). The estimated probability can also be used in conjunction with a cutoff threshold when selecting multiple answers to list questions. 3.1. Extension to support answer merging for multi-strategy QA
Many QA systems utilize multiple strategies to extract answer candidates, and then merge the candidates to find the most approach assumes that a combination of similar answers extracted from different sources with different strategies performs better than any individual answering strategy alone. As answer candidates come from different agents with different score distributions, it is important to consider how the results proposed by alternative approaches can be combined.
Our framework can be extended to support multi-strategy QA by combining the confidence scores returned from individ-ual extractors with the answer relevance and answer similarity features. Eq. (3) shows the extended answer ranking frame-work to merge answer candidates provided by multiple extraction techniques, where m is the number of extractors, n is the number of answers returned from one extractor, and conf k answer is same as A i . When an extractor extracts more than one answer from different documents with different confidence use 0.64 as conf k . This is to prevent double counting of redundant answers because sim information.

When extractors do not provide confidence scores, we can use answer ranks instead of scores. If the variance of scores is high, we can apply simple score normalization approaches such as CombSum, CombMNZ (Manmatha &amp; Sever, 2002) as a preprocessing step. 4. Feature representation
This section presents details of the feature functions and explains how answer relevance scores and answer similarity scores are generated for the answer ranking framework. 4.1. Answer relevance features
Each answer relevance feature produces a relevance score to predict whether or not an answer candidate is correct given the question. For factoid and list questions, we used gazetteers and WordNet in a knowledge-based approach; we also used
Wikipedia and Google in a data-driven approach. For more complex questions (such as how, why and opinions), these fea-tures are not effective. Extension of relevance features to complex questions is one of future work.

The approaches described in this section use information produced by the question analysis process (e.g., question key-words and the expected answer type). The question analysis process uses semantic resources such as WordNet to produce that information. For example, given the question  X  X  X ho wrote the book Song of Solomon ? X ,  X  X  X riter X  is the expected answer type by considering normalization of the question focus verb  X  X  X rote X  (Nyberg et al., 2003). These two approaches are de-scribed below. 4.1.1. Knowledge-based features (a) Gazetteers : For answer ranking, we used three gazetteer resources: the Tipster Gazetteer, information about the US (b) WordNet : The WordNet lexical database includes English words organized in synonym sets, called synsets ( Fellbaum, 4.1.2. Data-driven features (a) Wikipedia : We used the Wikipedia documents in a data-driven approach using term frequency (TF) and inverse doc-
For many numeric and temporal questions, answer candidates tend to be numeric expressions and Wikipedia may not have information related with the numeric expressions. For example, for the question  X  X  X ow many stories are in the Sears the answer candidate in the retrieved documents. For example, to validate the answer candidate  X 110 X , we search for Wiki-pedia for the question keywords ( X  X ears Tower X  and  X  X tory X ). As the Wikipedia document about  X  X ears Tower X  contains the answer candidate  X 101 X , we calculate the Wikipedia score from this document. When there is a Wikipedia document whose there is a Wikipedia document for  X 1912 X ), the back-off strategy is not applied.

To obtain word frequency information, the TREC Web Corpus was used as a large background corpus ( http://ir.dcs.gla.a-c.uk/test_collections/wt10g.html ). (b) Web : Following Magnini et al. (2002), we used the Web to generate a numeric score for each answer candidate. A 4.2. Answer similarity features
As factoid and list questions require short text phrases as answer(s), the similarity between two answer candidates can be calculated with string distance metrics. We calculate the similarity between two answer candidates using multiple string distance metrics and a list of synonyms. 4.2.1. String distance metrics
There are several different string distance metrics which calculate the similarity of two strings. We used Levenshtein dis-similarity feature to calculate answer similarity scores. 4.2.2. Synonyms
Synonym information can be used as another metric to measure answer similarity. We defined a binary similarity score for synonyms as following:
To obtain a list of synonyms, we used three knowledge-bases: WordNet, Wikipedia and the CIA World Factbook. WordNet includes synonyms for English words. We used the most common senses (i.e., sense 1 and sense 2) to get synonyms from in Wikipedia. The CIA World Factbook is used to find synonyms for a country name. The Factbook includes five different names for a country: a conventional long form, a conventional short form, a local long form, a local short form and a former name. These names are considered to be synonyms.

In addition, manually generated rules are used to obtain synonyms for different types of answer candidates (Nyberg et al., 2003). Temporal expressions are converted into the ISO 8601 format (YYYY-MM-DD HH:MM:SS ) (e.g.,  X  X  X pril 12 1914 X  and  X  X 12th April 1914 X  are converted into  X  X 1914-04-12 X  and are considered synonyms). Numeric expressions are converted into numbers (e.g,  X  X  X ne million X  and  X  X 1,000,000 X  are converted into  X  X 1e+06 X  and are considered synonyms). In addition, we used manually-created rules for a representative entity so that it is associated with a specific country when the expected answer type is COUNTRY (e.g.,  X  X  X he Egyptian government X  is considered  X  X  X gypt X  and  X  X  X linton administration X  is considered  X  X  X S X ). 5. Experiments: answer ranking
This section describes the experiments performed to evaluate our answer ranking framework. The experiments were done with TREC factoid and list questions using two QA systems: JAVELIN (Nyberg et al., 2005) and EPHYRA ( Schlaefer, Gies-uate the degree to which the framework is applicable to another QA system.

Even though both JAVELIN and EPHYRA are open-domain question answering systems, implementation details differ. One major difference is how a list of answer candidates is produced. JAVELIN directly searches the given corpus (the TREC/AQUA-
INT corpora) to extract documents, so that the list of answer candidates contains several redundant or similar answers re-trieved from different documents. On the other hand, EPHYRA extracts answer candidates from Web snippets and clusters the answer candidates whose surface strings are the same. The clustered answer score is calculated by summing the scores in the cluster. Then it conducts answer projection to find supporting documents from the TREC/AQUAINT corpus. the input to the answer ranking framework does not include redundant answers and the score is not normalized between 0 and 1. In addition, the two systems have a different answer type hierarchy.

In this section, we first describe the baseline systems and then report the effectiveness of the answer ranking framework in both systems. 5.1. Baseline systems
Several baseline algorithms were used to compare the performance of our answer ranking framework. These algorithms have been used for answer ranking in many QA systems.

Extractor : Answer extractors apply different techniques to extract answer candidates from the retrieved documents or passages, and assign a confidence score for each individual answer. As a simple baseline, we reranked the answer candi-dates according to the confidence scores provided by answer extractors.

Clustering : This approach clusters identical or complementary answers and then assigns a new score to each cluster. In our experiments, we used the approach reported in ( Nyberg et al., 2003 ). For a cluster containing N answers whose extrac-tion confidence scores are S 1 , S 2 , ... , S N , the cluster confidence is computed with the following formula:
Filtering : We used both ontologies and gazetteers to filter out improper answer candidates. The algorithms described in Section 4.1.1 were used to identify improper answer candidates, and then these candidates were removed from the answer candidate list.

Web validation : The approach proposed by Magnini et al. (2002) was used as another baseline. We implemented three variants to rerank answer candidates using Web validation scores: (1) rerank answer candidates according to the Web validation scores, (2) add the Web score to the extractor score (called CombSum Fox &amp; Shaw, 1994 ) and then rerank answer candidates according to the sum, and (3) use a linear regression to learn the weight for both extractor scores and Web scores and then rerank candidates according to the results from the linear regression. In our experiments, the linear regression method was more accurate than the other methods. Therefore, we used linear regression to combine the Web validation scores with the extractor scores.

Combination : We also combined three baseline systems (Clustering, Filtering, and Web validation) using linear regres-sion in order to see the extent to which combined approaches could improve answer ranking performance. Three combi-nations were tested: Clutering + Filtering(C + F), Clustering + Web(C + W) + Web (C +F+W) .

MaxEnt with internal resources : Maximum entropy has been used for the answer ranking task in several QA systems ( Echihabi et al., 2004; Ravichandran, Hovy, &amp; Och, 2003) using internal resources such as answer type and frequency of answers in the candidate list. We implemented a maximum entropy reranker as another baseline with popularly used internal features: (1) frequency of answers in the candidate list, (2) answer type matching, (3) question word absent in the answer sentence, (4) inverse term frequency of question keywords in the answer sentence, (5) confidence of individual answer candidate provided by an extractor, and (6) the expected answer type. As this baseline is not using any external resources, the comparison of it with our framework can show the degree to which the external resources are useful in answer ranking.

Performance was measured by average top answer accuracy: the number of correct top answers divided by the number of questions where at least one correct answer exists in the candidate list provided by an extractor. As the performance of an-swer ranking depends on the quality of answer extraction (when there is no correct answer from extractor, we cannot eval-uate the performance of the framework), we only used the number of questions for which an answer extractor finds at least one correct answer in its candidate list. In this way, we can focus on evaluating how much the framework can boost the rank of correct answers.

Fivefold cross-validation was performed to evaluate our answer ranking framework. Fivefold cross-validation splits the fitting to a specific data set when applying machine learning techniques.

As the number of answer candidates affects answer ranking performance, we used only top N answer candidates for the experiments. As EPHYRA produced much more answer candidates than JAVELIN (as shown in Tables 1 and 6 ), we used a dif-ferent threshold for them: top 100 answers for JAVELIN and top 120 answers for EPHYRA. 5.2. Experiments with the JAVELIN QA system
A total of 1760 factoid questions from the TREC8-12 QA evaluations served as a data set. To better understand how the performance of our framework can vary for different extraction techniques, we tested our answer ranking framework with three JAVELIN answer extraction modules: FST, LIGHT and SVM. FST is an answer extractor based on finite state transducers that incorporate a set of extraction patterns (both manually-created and generalized patterns). LIGHT is an extractor that selects answer candidates using a non-linear distance heuristic between the keywords and an answer candidate. SVM is an extractor that uses Support Vector Machines to discriminate between correct and incorrect answers.
Table 1 compares extractor performance on the test questions. The second column shows the number of questions for which each extractor finds answer candidates. For example, FST can find answer candidates only for 837 questions among the 1760 questions. The third column in the table shows the number of questions which contain at least one correct answer in the candidate list. The fourth column shows the average number of answer candidates per question. As can be seen in the third column of Table 1 , for many questions they did not find correct answers. Therefore, we only used the questions for how much the framework can boost the rank of correct answers, which makes sense only when some correct answer(s) exist in the extractor outputs.

Table 2 shows the average top answer accuracy for the baseline systems and the answer ranking framework. The result shows that baseline systems improved performance over Extractor score. Although FST covers fewer questions than LIGHT and SVM, its answers are more accurate than answers from the other extractors. On the other hand, SVM score was lower than FST and LIGHT. As the SVM extractor was not well-tuned and did not have an intelligent way to break ties, it produced many answer candidates with the same confidence score. When eval-uating the extractor performance, we just pick the first answer candidate in the extractor output even though its score is same to the second answer. This is why the score of the SVM extractor is much lower than the score of the FST and LIGHT extractors.

Among the baselines that uses a single feature ( Clustering, Filtering, Web validation duced the best performance for all three extractors. Among the combination of baseline systems, performance. This suggests that combining more resources was useful in answer ranking for JAVELIN. As for proved performance over Clustering , but did not gain benefit from the use of external resources such as Web, gazetteers and WordNet because it used only internal resources.

When compared with the baseline systems, the answer ranking framework obtained the best performance gain for all three extractors. The highest gain was achieved for the SVM extractor mostly because SVM often produced multiple answer candidates with the same confidence score, and the framework could select the correct answer by considering additional relevance and similarity features.

Further analysis examined the degree to which the average top answer accuracy was affected by answer similarity fea-tures and answer relevance features. Table 3 compares the average top answer accuracy using the answer similarity features, the answer relevance features and all feature combinations. As can be seen, similarity features significantly improved the performance, implying that exploiting redundancy improves answer ranking. Relevance features also significantly improved the performance, and the gain was larger than the gain from the similarity features.

When combining both types of features together, the answer selection performance increased for all three extractors. The biggest improvement was found with candidates produced by the SVM extractor: a 247% improvement over
We also analyzed the average top answer accuracy when using individual features. Table 4 shows the effect of the indi-vidual answer relevance feature on different extraction outputs. The combination of all features significantly improved per-formance compared to answer ranking using a single feature. Comparing data-driven features with knowledge-based features, we note that the data-driven features (such as Wikipedia and Web) increased performance more than the knowl-edge-based features (such as gazetteers and WordNet), mostly because the knowledge-based features covered fewer ques-tions. The biggest improvement was found using the Web, which provided a performance increase of an average of 74% over Extractor .

Table 5 shows the effect of individual similarity features on different extractors. Table 5 compares the performance when using 0.3 and 0.5 as a threshold, respectively. When comparing five different string similarity features (Levenshtein, Jaro,
Jaro-Winkler, Jaccard and Cosine similarity), Levenshtein, Cosine and Jaccard tend to perform better than others. When com-paring synonym with string similarity features, the synonym feature performed slightly better than the string similarity features.

As Levenshtein, Cosine and Jaccard performed well among the five string similarity metrics, we also compared the com-bination of each of the three metric with synonyms, and then chose Levenshtein and synonyms as the two best similarity features in the answer ranking framework. 5.3. Experiments with the EPHYRA QA system
We also evaluated the answer ranking framework with the answer candidates provided by the EPHYRA QA system. For this evaluation, 998 factoid questions from the TREC13-15 QA evaluations served as a data set.

EPHYRA has two extractors: Extractor 1 and Extractor 2. Extractor 1 exploits answer types to extract associated named entities, and Extractor 2 uses patterns which were automatically obtained from question X  X nswer pairs in the training data. Table 6 shows the characteristics of the EPHYRA extractors.

Table 7 shows the performance of baseline systems and the answer ranking framework on the EPHYRA answer candi-dates. Clustering did not affect performance even though it was useful in the JAVELIN case. As EPHYRA already combined similar answer candidates, there were significantly fewer answers to be clustered. On the other hand, JAVELIN extractors return multiple redundant answer candidates from different documents. Therefore, exploiting answer redundancy was important in JAVELIN, but not in EPHYRA.
 Filtering did not significantly affect performance because the number of EPHYRA answer types is smaller than that in
JAVELIN. In addition, JAVELIN has two-tier answer types: one for the named entity information (e.g. location, person, orga-nization) and the other for more specific information such as city, river, and writer. As second answer type and EPHYRA does not produce this information, the gain from
Among the baseline systems, Web validation produced the best performance for both Extractor 1 and Extractor 2. One interesting result is that C+F+W produced lower performance than (
Clustering, Filtering, Web validation , respectively) sometimes made a conflict decision. For example, given the and two  X  X  X hanghai X  extracted from five different documents. In this case, because it occurs more than  X  X  X hanghai X , but Web produces a higher score for  X  X  X hanghai X  because there is a Web document which mentions Shanghai has largest number of foreign financial companies. This demonstrates that combination of multi-ple approaches is hard. However, our answer ranking framework made a small but significant improvement over one single baseline even though it merged multiple approaches. When compared with proved performance. 6. Experiments on answer merging
We also evaluated the framework to merge multiple extractor outputs. 6.1. JAVELIN
The same data set used to evaluate each individual JAVELIN extractor was used to evaluate the answer ranking framework for multi-strategy QA. As different extractors returned different numbers of answer candidates, we only considered the top 50 answer candidates produced by each individual extractor and created a new list by combining them. Among 1760 ques-tions in the new list, there were 978 questions for which at least one extractor identified a correct answer.
The performance was measured using the average answer accuracy, calculated by the number of correct top answers among the 978 questions for which at least one correct answer exists. As there are more answer candidates when merging multiple extractor outputs, we also used MRR, which is the average of the reciprocal rank of the top correct answer among the top N (here N = 5) candidates.

Two baselines were used: MaxScore and CombSum . MaxScore picks the highest score among the scores provided by the three JAVELIN extractors. CombSum sums the scores from the three extractors and then reranks the answers according to the sum.

Table 8 shows the experimental results of MaxScore, CombSum, Framework
CombSum produced better performance. Framework combines answers using the framework. Answer merging with the framework significantly improved both average top answer accuracy and MRR over 6.2. EPHYRA We conducted similar experiments to merge answer lists produced by each individual EPHYRA extractor (Extractor 1 and
Extractor 2). The same 998 questions used to evaluate each individual EPHYRA extractor were used to evaluate answer merg-ing. Again, we only considered the top 50 answer candidates produced by each individual extractor and created a new list by combining them. Among the total of 998 questions, 524 questions had at least one correct answer.
 Table 9 shows the experimental results. Similar to the JAVELIN case, framework produced the best performance. When comparing JAVELIN and EPHYRA, we had more performance gain when merging JAVELIN extractors because JAVELIN had one more extractor than EPHYRA and the coverage of the LIGHT and
SVM extractors was higher than for the EPHYRA extractors. 7. Experiments with list questions used to evaluate the performance. To answer list questions, most QA systems first produce a list of answers, and then use a cutoff threshold to get top N answers whose score is higher than the threshold. As the performance highly depends on the cutoff threshold, it is hard to setup baseline, and we report the results from the latest TREC 2007 evaluation.
In the TREC 2007 QA track, we applied the answer ranking framework for list questions by merging the results from three extractors: answer type-based extractor, pattern-based extractor and semantic extractor. The first two extractors were used oped for TREC 2007; it converts the question and answer candidates to semantic structures using ASSERT semantic role la-beler and then compares their semantic similarity based on a fuzzy similarity metric. More details on the extractors can be found at Schlaefer et al. (2008) .

Table 10 shows the performance of our system which incorporated the answer ranking framework. It can be seen that our system worked much better than the median over all 51 runs and was the top five system.

As this was the first time we used the answer ranking framework for list questions and TREC allowed the participants to submit maximum three runs, we submitted one run without using the framework to answer list questions and another run answers for the TREC 2007 list questions. The performance of this run was 0.123. When comparing this run (score: 0.123) with the run which used the framework (score: 0.144), we achieved 17.1% improvement. This demonstrates the effectiveness of the answer ranking framework in list questions. 8. Discussion
In our experiments, we tested the framework using two QA systems: JAVELIN and EPHYRA. Each QA system is a repre-sentative of a different answering strategy. JAVELIN searches the given corpus (the TREC/AQUAINT corpora) to extract an-swer candidates. EPHYRA exploits larger corpus such as Web to get answer candidates and then conducts answer projection to find supporting documents from the TREC/AQUAINT corpus.

The experimental results demonstrate that the framework improved answer ranking performance in both systems. When comparing the performance gain in JAVELIN and EPHYRA, we had more performance gain in the JAVELIN case because of tures was different (e.g., similarity features played an important role in JAVELIN, but not in EPHYRA because answer candi-dates provided by EPHYRA tend to have many fewer redundancy). This tells that answer ranking performance is inherently system-dependent and applying the framework to other QA systems requires retraining. More specifically, the re-training process should consider the following steps:
Tuning of relevance features: Our experimental results show that the effect of the relevance features highly depends on the characteristics of the extractor outputs. In addition, some relevance features require manual effort to provide access to language-specific resources and tools (e.g. segmentation for Chinese and Japanese). Therefore, tuning relevance features for a specific system and/or a language is one important task when applying the framework to another QA system. to learn the best similarity features and cutoff thresholds for each system.
 Training dataacquisition: To retrain the framework for another QA system, we need to obtain training data provided by the
QA system. The training data should contain question keywords, answer type information, answer strings and their asso-ciated scores. When scores are not available, answer ranks can be used.

Mapping of answer type classification: Each QA system has a different answer type hierarchy. As our answer ranking frame-work highly depends on answer types (e.g., the Web feature was less useful in validating temporal and numeric types of questions), we need to convert the answer types of each individual QA system into the answer types which the framework uses.
 decide NIL answers for factoid questions ( Voorhees, 2004 ). As a default strategy, we can do the binary answer classifica-tion using a default threshold of 0.5: if the probability of an answer candidate is lower than 0.5, it is considered to be a wrong answer and is filtered out of the answer list. However, this can be tuned for a specific QA system. In our experi-ments on list questions, we found that the candidates whose estimated probabilities are at least 25% of the probability of the top answer produced the best performance. It was because the variance of the extractor scores was high (e.g., the score of the top answer candidate was sometimes very low, which resulted in the estimated probabilities below a threshold of 0.5). Therefore, it is often useful to tune the framework using cross-validation to find the best threshold for a specific system.

Extractor score normalization: Related with the previous item, when the scores from extractors have a high variance, it is necessary to normalize the extractor scores for answer merging. Some simple score normalization approaches such as CombSum, CombMNZ (Manmatha &amp; Sever, 2002) can be used as a preprocessing step.

Learning weights for theframework: The weights ~ a ; ~ b ; bines answer merging with answer ranking, the weight learning process is much easier. For example, IBM X  X  PIQUANT QA system ( Chu-Carroll, Czuba, Prager, Ittycheriah, &amp; Blair-Goldensohn, 2005 ) has multiple answering agents to extract answer candidates and an answer merger to combine the candidates provided by each answer agent. This requires sep-arate modules to validate answer candidates and to merge answers provided by multiple answering agents; each of which requires a separate training. As our answer ranking framework unifies answer validation and answer merging, it can be easily integrated into another QA system with only one trainable step to learn the weights. This is a novel approach to design a flexible and extensible system architecture for answer ranking and answer merging in question answering.
Tuning of the extractor results: When measuring answer ranking performance, we used the number of questions where at least one correct answer exists in the candidate list provided by an extractor so that we can focus on evaluating how much the framework can boost the rank of correct answers. However, the extractors can have an internal threshold to decide which  X  X  X recandidates X  are to be returned as answer candidates. Or we can have a threshold to limit the number of answer candidates for answer reranking (e.g. consider only the top 100 answer candidates or consider answer candidates whose confidence score is higher than a threshold). By tightening up on that threshold, we will have fewer questions with answer candidates in the list. As this list will in general be more accurate, the precision of the answer ranking framework will be higher. However, this will lower the recall. Therefore, to find the trade-off between precision and recall, more experiments on how many answer candidates to be considered for answer ranking would be important. 9. Conclusions and future work
In this paper, we proposed a answer ranking framework which combines answer relevance and similarity features. An extensive set of empirical results on TREC factoid and list questions show that the framework significantly improved answer ranking performance over extractor results and always produced better performance than other answer ranking algorithms.
Five different extractors were used for the evaluation: (1) an extractor based on finite state transducers, (2) an extractor that selects answer candidates using non-linear distance heuristics, (3) an extractor that uses Support Vector Machines to discriminate between correct and incorrect answers (4) an extractor that uses patterns automatically obtained from ques-show that the answer ranking framework improved answer ranking performance on answer candidates provided by differ-ent extraction techniques. This is evidence that our framework is robust and generalizable for different extraction techniques.
 Furthermore, the framework was extended to merge answer candidates provided by multiple extraction strategies.
Experiments were done to merge three JAVELIN English extractors and merge two EPHYRA extractors. The experimental re-sults show that the framework was also effective in merging answer candidates in both systems.

We plan to extend the framework to support complex questions, which require longer answers representing facts or rela-different features should be used for answer ranking. Possible relevance features include question keyword inclusion and answer similarity, we intend to explore novelty detection approaches evaluated in Allan, Wade, and Bolivar (2003) . Incor-porating more resources to estimate answer relevance and answer similarity is another future work. We also plan to merge the results from answers provided by several QA systems in order to have higher quality answers.
 Acknowledgment This work was supported in part by ARDA/DTO Advanced Question Answering for Intelligence (AQUAINT) Program Award No. NBCHC040164.
 References
