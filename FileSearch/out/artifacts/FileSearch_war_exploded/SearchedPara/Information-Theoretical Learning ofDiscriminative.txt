 Yuan Shi yuanshi@usc.edu U. of Southern California, Los Angeles, CA 90089 USA Fei Sha feisha@usc.edu U. of Southern California, Los Angeles, CA 90089 USA Supervised learning algorithms often assume that the training and the test data are randomly sampled from the same joint distribution. While the assumption facilitates rigorous theoretical analysis and empirical comparison of different algorithms, its validity is of-ten challenged outside of laboratory settings. In real-world applications, there are many factors causing a mismatch between the training and the test data. For instance, imagine developing a face detection system for Facebook mobile users. A tuned classifier on im-ages captured by webcams could be applied to images from mobile phones. In this case, the imaging con-ditions vary significantly due to background illumina-tion, motion blurring, pose, etc.
 Techniques for addressing learning problems with mis-matched distributions are often referred as domain adaptation, or sometimes transfer learning (Daum  X e III &amp; Marcu, 2006; Pan &amp; Yang, 2010; Qui  X nonero-Candela et al., 2009). The source domain refers to the labeled training data, while the target domain refers to the test data. When there is no labeled data from the target domain to help learning classifiers, the problem setting is termed unsupervised domain adaptation . Unsupervised domain adaptation is especially chal-lenging as the target domain does not provide explic-itly any information on how to optimize classifiers. Note that the objective of domain adaptation is to derive a classifier for the unlabeled (target) data from the labeled (source) data. This goal sets domain adap-tation apart from semi-supervised learning, whose pri-mary goal is to improve the performance on the la-beled data with unlabeled data (Chapelle et al., 2006). The difference is subtle yet fundamental. For example, model selection or cross-validation using classification accuracy on the target domain is generally impossible. Existing approaches thus rely on making strong as-sumptions on how the data distribution have shifted between the two domains in order to derive classi-fication rules for the target domain. For instance, in covariate shift (Shimodaira, 2000; Bickel et al., 2007; Huang et al., 2007), the marginal distributions of the features are different across domains while the posterior distribution of the label remains the same. This naturally leads to a two-stage learning paradigm: the labeled instances from the source domain are first weighted so as to compensate the difference in marginal distributions. Then, a classifier is trained us-ing the labels and then applied to the unlabeled data. Other works have also followed similar paradigms (Pan et al., 2011; Gopalan et al., 2011). In the structural correspondence learning, the original features are first augmented with features that are more likely to be do-main invariant and then a classifier is trained (Blitzer et al., 2006). The augmenting features are linear trans-formation of the original features. Alternatively, in deep learning architecture for domain adaptation, the augmenting features are highly nonlinear transforma-tion of the original ones (Glorot et al., 2011). Underlying all these methods is the assumption that there exists a domain-invariant feature space such that the marginal distributions of two domains are the same in the new feature space. Thus, classifiers learnt in the new space will perform equally well on both the source and the target. Theoretical analysis have showed that the loss on the target domain for any labeling func-tions depends on the difference between the marginal distributions, thus justifying the need to identify a fea-ture space such that the two domains look alike to each other (Ben-David et al., 2007; Mansour et al., 2009). We hypothesize that this view and practice of two-stage learning are restrictive. One possible fallacy is that maximizing the similarity in marginal distribu-tions bear no direct consequence on (dis)similarities between posterior distributions. Thus, if there are multiple feature spaces where the source and the tar-get domains have similar marginals, there is no reason to believe that a classifier trained on an arbitrarily chosen one would necessarily perform well on the tar-get domain. As an extreme case, projecting features into irrelevant feature dimensions would make the two domains look very much alike! Hence, the caveat is to retain discriminative informa-tion for constructing classifiers while we search for the domain-invariant feature space. This seems relatively straightforward to achieve if all we care is the discrim-inative information about the labels in the source do-main. However, our main goal is to have good clas-sifiers for the target domain. Thus, our challenge is how to be discriminative without labels? To address this challenge, we propose a novel learn-ing algorithm for unsupervised domain adaptation. As opposed to existing two-stage approaches where new feature spaces and classifiers are separately optimized, our approach combines the two in a single stage. More-over, the new feature space is discriminative with re-spect to the target domain. We give a brief account in the following, leaving details to sections 2 and 3. Main Idea We assume discriminative clustering , namely, data in both the source and the target do-mains are tightly clustered and clusters corresponds class boundaries. For the same class, the clusters from the two domains are geometrically close to each other. Leveraging these assumptions, our formula-tion of learning the optimal feature space balances two forces: maximizing domain similarity that makes the source and the target domains look alike, and (approx-imately) minimizing the expected classification error on the target domain. We define those two forces with information-theoretical quantities: the domain simi-larity being the negated mutual information between all data and their binary domain labels ( source ver-sus target ) and the expected classification error be-ing the negated mutual information between the target data and its clusters (ie class) labels estimated from the source data. These two quantities are directly mo-tivated by the nearest neighbor classifiers we use in the new feature space.
 We show how simple gradient-based methods can be effectively used for numerical optimization to learn the optimal feature space. We evaluated extensively our approach on two benchmark tasks: visual object recognition and sentiment analysis of product reviews. On both of them, the proposed approach outperforms other state-of-the-arts methods significantly. Contributions To summarize, we contribute to do-main adaptation by advocating discriminative clus-tering as a possible mechanism for adaptation; cf. section 2. We hypothesize that existing approaches of two-stage learning can be significantly improved by taking those cluster structures into consideration. Thus, we propose an one-stage approach jointly learn-ing a domain-invariant feature space and optimizing information-theoretic metrics directly related to dis-criminative classification on the target domain; cf. sec-tion 3. Our empirical results support strongly our modeling assumptions and hypothesis; cf. section 4. At the core of our approach is the assumption of dis-criminative clustering. Specifically, we assume that, in a suitable feature space, 1) separation . Data in the source and the target domains are discriminatively clustered, where the cluster ids correspond to class la-bels; 2) Alignment . The clusters from the two do-mains that correspond to the same label are geomet-rically close. Fig. 1 illustrates these two assumptions and how they can be exploited for adaptation.
 Arguably, the assumptions are more  X  X elaxed X  than those in existing works for adaptation. Specifically, they do not imply that the marginal distributions are the same across domains and certainly do not imply the same posterior distributions either. In fact, these assumptions are readily satisfiable in applications. For example, many datasets exhibit multi-modal marginal distributions where the modes correspond to class labels, particularly if these data are sampled from a generative process of mixture models.
 As we will show in the following, these two assump-tions allow us to define quantitively what to be opti-mized  X  in our case, we would like to identify a domain-invariant feature space such that the expected misclas-sification error on the target data is minimized. De-spite the paucity in labels from the target domain, we will show how the alignment assumption will allow us to define a proxy to the error so as to be optimized. In what follows, we are given N labeled instances from the source domain: { ( x s ,y s ) } where x s  X  X  X  R D and y s takes a value from C class labels: y s  X  Y = { 1 , 2 ,..., C } . We also have M unlabeled instances from the target domain: { x t } where x t  X  X  . For simplicity, we assume x t and x s have the same domain X , thus the same dimensionality. Extensions to more general cases are possible, analogous to (Kulis et al., 2011). Our objective is to construct a classifier f : x  X  X  X  y  X  Y . We would like the classifier performs well on the target domain D T from which x t is sampled. This is inherently an ill-posed problem as we do not have any labels from the target domain.
 To overcome this difficulty, we leverage the discrimina-tive clustering assumptions which we have previously described. We assume that there is a latent feature space z  X  R d such that i) data in the source and target domains form well-separated clusters and the clusters correspond to labels; ii) the clusters from the source domain are geometrically close to those from the target domain if they are from the same labels.
 We show how these assumptions can be used to derive information theoretical quantities which reflect data characteristics in each domain. These quantities are parameterized in terms of the latent feature which is in turn a linear transformation of the original feature x . We then show how to combine these quantities so that the optimal linear transformation can be learnt from data. We begin by describing a few key notions. 3.1. Conditional models in the feature space We consider the latent feature space induced by a lin-ear transformation L  X  R d  X  D . In the new feature space, we use k-nearest neighbors (kNN) to classify as we have assumed that data form well-separated clusters. Moreover, we choose k = 1 to avoid cross-validating this parameter.
 The squared distance between two points x i and x j in this feature space is thus given by where M = L T L defines a (low-rank) Mahalanobis distance metric in the original space.
 Given a point x i and a set of data points { x j } , we use the following model to define the conditional probability of having x j x  X  X  nearest neighbor.
 The above conditional model has been used in many contexts, including metric learning (Goldberger et al., 2004), dimensionality reduction (Hinton &amp; Roweis, 2002), etc. Characterizing how close a point x i is to other points, this model gives rise to an estimate of the posterior p ( y i = c | x i ) for labeling x i with the class la-bel c , assuming the class labels of { x j } are known, where  X  jc is 1 if x j  X  X  label is c , and 0 otherwise. Since p ij is a normalized probability,  X  p ic is normalized too. For example, if the label of x i is known, P c  X  p ic would be the probability of correctly classifying x i . 3.2. Discriminative clustering in the source To derive a classifier that can perform well on the tar-get domain, we would certainly need the classifier to perform well on the source domain because we have assumed that the two domains share similar clustering structures. Thus, our first desideratum is to minimize the expected classification error on the source domain, when we classify it using 1-NN. This error is estimated using the empirical average of the leave-one-out accu-racy for any given point x s in the source domain D S : Note that, if we minimize this error only and ignore the target domain, we will arrive at the metric learning technique in (Goldberger et al., 2004). 3.3. Discriminative clustering in the target Since we do not have labels on the target domain, we cannot define the expected classification error as we did in eq. (4) for the source domain. How to be dis-criminative without using labels? Consider an instance x t from the target domain and all the instances { x s } from the source domain, the condi-tional model p ts of eq. (2) gives rise to the probability of having a particular x s as the nearest neighbor of x t Using this conditional model as well as the source la-bels to compute the posterior as in eq. (3) would not be the correct posterior for the target domain. However, if our assumptions about two sets of clusters being ge-ometrically close indeed hold in the dataset, then the estimation  X  p tc should be close to the true posterior. If  X  p tc approximates the true posterior well and our as-sumption that the target data is well clustered, then we can reasonably expect that the C -dimensional prob-ability vector  X  p t = [  X  p t 1 ,  X  p t 2 ,...,  X  p t C an ideal posterior probability vector [0 , 0 ,..., 1 ,..., 0] where the only nonzero element 1 occurs at the posi-tion corresponding to the correct label.
 Since we do not know the true label, we cannot mea-sure directly the similarity of  X  p t to the correct and ideal posterior vector. Nonetheless, we can express our desideratum as reducing the entropy of  X  p t such that it contains the least amount of confusing labels. Let H [ p ] denote the entropy of a probability vector p . If we minimize P t H [  X  p t ] only, we could arrive at a degenerate solution where every point x t is assigned to the same class. To avoid this, we instead maximize the mutual information between the data and the es-timated label  X  Y using  X  p , and the prior distribution  X  p 0 is given by  X  p 0 = 1 / M P t  X  p t . Note that using the empirical distribu-tion of the labels in the source domain to estimate the prior  X  p 0 could still lead to degenerate solutions when the labels are uniformly distributed.
 Minimizing the entropy (or similarly, maximizing the mutual information) has been previously studied in the context of (discriminative) clustering, cf. (Gomes et al., 2010; Dhillon et al., 2003). This criterion will identify a feature representation that classifiers can use to achieve a low lower-bound of misclassification error, due to Fano X  X  inequality (Fisher III &amp; Principe, 1998). 3.4. Discriminability: source versus target The previous discussion on discriminative clustering in the target domain hinges on the assumption that clus-ters for the source and the target domain should not be too far from each other. We quantify this notion more precisely in the following. Conceptually, this no-tion is similar to the idea in existing works to make marginal distributions similar across domains. Why such notion is desirable? To use the source domain X  X  labels as an proxy to estimate the poste-rior probabilities for the target data (as in eq. (3)), we would desire the source and the target domain share some common probability supports in the feature space. In particular, consider the case we classify two instances x t and x t 0 from the target domain. They are deemed to have the same label c if there are plenty of labeled source data in class c in their neighborhoods. Then we would expect that with high likelihood, x t and x t 0 are in each other X  X  nearest neighbors too  X  otherwise, the cluster corresponding to class c in the target domain would not be very  X  X ight X .
 Having instances from both domains in x t  X  X  nearest neighborhoods thus entails the following. If we create a binary classification problem and assign q i = 1 if x i is from the source and q i = 0 if x i from the target, then given x i , we cannot determine well above chance level where this instance comes from.
 Instead of constructing an actual binary classifier, we express our desideratum as minimizing the mutual in-formation between the data instance X and its (bi-nary) domain label Q . Analogous to eq. (5), the mu-tual information is given by, where  X  q i is the two-dimensional posterior probability vector of assigning x i to either the source or the tar-get, given all other data points from the two domains. Concretely, the probability is computed according to eq. (3), except the class label  X  jc being replaced by the domain label of x j . The estimated prior distribution  X  q 0 is computed as 1 / ( N + M ) P i  X  q i .
 One might wonder why we do not compute and mini-mize the expected error as in the source domain classi-fication eq. (4). This is because we would like to leave some room for the possibility that a certain portion of data in either domain could be  X  X utliers X  to the other domain, and thus indeed distinguishable with respect to their origins. Minimizing domain classification er-ror would have the adverse effect of forcing the two domains to be exactly the same. For instance, a de-generate solution would be to map every point to the origin of the feature space.
 We mention in passing that it is found that the accu-racy of a binary domain classifier reflects similarities between domains (Blitzer et al., 2007), thus approxi-mating the original intractable combinatorial measure of similarities (Ben-David et al., 2007). 3.5. Learning and model selection We have described three information-theoretical quan-tities: classification accuracies on the source domain  X  S of eq. (4), discriminative clustering on the target I ( X ;  X  Y ) of eq. (5), and discriminability between the source and the target I st ( X ; Q ) of eq. (6). These quantities have been derived from our assump-tions about the source and target domains, specifically, the discriminative clustering structures. They are all parameterized in the linear transformation L . We learn the optimal L by balancing these quantities with the following optimization problem where the constraint is to control the scale of distances computed using L .
 The regularization coefficient  X  needs to be cross-validated. We choose the optimal  X  that attains the minimum of  X  S . Intuitively,  X  S is defined on the source domain with labeled data and thus, more sensible to be used for model selection (Other ways of combining these quantities were also experimented, though the above performs the best in practice.) We comment briefly on the difference between our for-mulation and the entropy minimization framework for semi-supervised learning (Grandvalet &amp; Bengio, 2005). Their goal is to reduce uncertainty of labeling the un-labeled data. Thus, they use only the entropy term eq (3). More distinctively, they do not need to make the two domains look alike thus there is no need for them to learn a feature space, nor to include a term to minimize the discriminability between the domains. 3.6. Numerical Optimization Eq. (7) is non-convex optimization. We use gradient-based methods to optimize the objective function. While in theory the methods are susceptible to lo-cal optimum, we use heuristics to initialize: either the PCA of the target domain data, or the low-rank fac-torization of a discriminatively trained metric on the source data, such as the one in large margin nearest neighbor (LMNN) (Weinberger &amp; Saul, 2009). In most cases, these heuristics work well and lead to substan-tially improved results over initialization points. De-tails are described in the Supplementary Material. 3.7. Extensions When the target domain has a few labeled instance, the domain adaptation problem is referred as semi-supervised adaptation . Our approach can be readily extended to incorporate those labeled target domain instances. Details, including experimental results are described in the Supplementary Material. We evaluate the proposed method on two benchmark tasks: object recognition and sentiment analysis of product reviews. We compare the method to baselines and other recently proposed ones for unsupervised do-main adaptation (Gopalan et al., 2011; Blitzer et al., 2006; Pan et al., 2011). In the Supplementary Mate-rial, we report results on semi-supervised adaptation, where the target domain has a few labeled instances. 4.1. Setup We start by describing the datasets for the two tasks. Object recognition . We use four databases of ob-ject images: Caltech-256 (Griffin et al., 2007), Amazon (images from online merchants X  X  catalogues), Webcam (low-resolution images by web cameras), and DSLR (high-resolution images by digital SLR cameras). The last three datasets were studied in (Gopalan et al., 2011; Saenko et al., 2010). Caltech-256 is added to increase the diversity of the domains.
 We treat each dataset as a domain. There are 10 com-mon object categories: backpack, coffee-mug, calcu-lator, computer-keyboard, computer-monitor, computer-mouse, head-phones, laptop-101, touring-bike , and video-projector . There are 2533 images in total, with 8 to 151 images per category per domain.
 Following the experimental protocols in previous work (Saenko et al., 2010), we extract SURF features (Bay et al., 2006) and encode each image with a 800-bin histogram (the codebook is trained from a subset of Amazon images). The histograms are first normal-ized to have zero mean and unit standard deviation in each dimension.
 For each pair of source and target domains, we conduct experiments in 20 random trials. In each trial, we randomly sample labeled data in the source domain as the training set, and unlabeled data in the target domain as the testing set. For semi-supervised domain adaptation, we also sample a few labeled examples in the target domain to augment the training set, see the Supplementary Material for details.
 Sentiment analysis . We use the dataset that con-sists of Amazon product reviews on four product types: kitchen appliances, DVDs, books and electronics (Blitzer et al., 2007). Each product type is used as a separate domain. Each domain has 1,000 positive and 1,000 negative reviews. To reduce computational cost, we select top 400 words of the largest mutual information with the labels. We then represent each review with a 400-dimensional vector of term counts (ie, bag-of-words). The vectors are normalized to have zero mean and unit standard deviation in each dimension. For each pair of source and target domains, we con-duct experiments in 10 random trials. In each trial, we randomly sample 1,600 labeled data in the source domain as the training set, and all data in the target domain as the testing set.
 Classification We learn the feature transformation L by solving the optimization problem eq. (7). We then transform all the data using the matrix and apply 1-nearest neighbor (1-NN) to classify instances from the target domain. 1-NN is used to avoid tuning the number of nearest neighbors. (In the Supplementary Material, we also report results of using SVMs.) Hyperparameter tuning Our method has two hyper-parameters: the dimensionality of the new fea-ture subspace and the regularization coefficient  X  in eq. (7). We cross-validate them using the model se-lection procedure described in section 3.5. The range of search for the dimensionality is { 20 , 40 , 70 , 100 } and { 0 , 0 . 25 , 1 , 4 , 16 , 64 } for  X  .
 For baselines and other methods we have compared to, if there are hyper-parameters to be tuned, we either follow the procedures in those algorithms or give those methods the benefits of doubts by reporting their best performance by using labels from the target domain. 4.2. Results on unsupervised adaptation We compare extensively to several methods.  X  Baselines. We compare to PCA , where we  X  Transfer Component Analysis ( TCA ) (Pan et al.,  X  Structural Correspondence Learning ( SCL )  X  Geodesic Flow Subspaces ( GFS ) (Gopalan et al.,  X  Metric Learning ( Metric ) (Saenko et al., 2010). Table 1 and Table 2 summarize the classification ac-curacies as well as standard errors of all the above methods, as well as ours (we did not apply SCL to object recognition as it is difficult to define what pivot features are for those types of data). We had chosen a subset of all pairs for saving experiment time. The best performing algorithm(s) (statistical significant up to one standard error) for each pair are in bold font. In Table 1 on object recognition, our method per-forms the best on 5 out of 6 pairs, outperforming other competing methods with a large margin. On the DSLR-Amazon pair, our method performs worse than LMNN , but still significantly better than others. Of particular interest is that LMNN outperforms other methods specifically designed for domain adap-tation (excluding ours). This confirms our hypothesis: the two-stage learning schemes adopted by TCA and GFS suffer from the fallacy that maximizing marginal similarity does not necessarily lead to well-performing classifiers on the target domain. In particular, we be-lieve such methods could actually destroy discrimina-tive information by forcing the domains to be similar. The results thus support our argument that one-stage learning, namely identifying jointly discrimina-tive clustering and low-dimensional feature spaces, is crucial for domain adaptation.
 The results on sentiment analysis in Table 2 also strongly support similar conclusions. Note that both SCL and our methods outperform other methods sig-nificantly. Our methods perform better on 2 out of 4 pairs, thought slightly worse than SCL on the other two. Exploring strengths and weakness of each of these two methods is a subject of future research. Information-theoretical approach has been applied to semi-supervised learning (Grandvalet &amp; Bengio, 2005) where the core idea is to reduce the confusability (among possible labels) on unlabeled data by classi-fiers trained on the labeled data. However, they have assumed that the data are drawn from the same distri-bution so there is no need to learn a domain-invariant feature space.
 Rastrow et al. described an information-theoretical based criterion for model selection in domain adap-tation (Rastrow et al., 2010). Model selection is a challenging problem when cross-validation is not pos-sible due to the lack of labeled data on the target domain. However, their approach is two-stage: they refine model parameters on unlabeled data by mini-mizing the conditional entropy of the labeling func-tion from the initial model tuned on the labeled source data. Consequently, their formulation does not learn an invariant feature space.
 Our work is also related to the recent study of regular-ized information maximization for discriminative clus-tering (Gomes et al., 2010). The authors there used a parametric model to compute the posterior proba-bilities of assigning a data point to various clusters. The objective is to find clustering assignments of all data points such that the mutual information between the data and the cluster ids are maximized. While their work is generalized to semi-supervised cluster-ing, they do not consider domain adaptation, which has fundamentally different goals and constraints from semi-supervised learning, as pointed out previously. In particular, the above-mentioned work does not learn a new feature space. We propose an one-stage approach that jointly learns a domain-invariant feature space and optimizes information-theoretic metrics directly related to dis-criminative classification on the target domain. Our empirical results support the validity of our modeling assumptions that data in both source and target do-mains are discriminatively clustered. We show that ex-isting approaches where learning feature is decoupled from learning discriminative classifiers, can be signif-icantly improved by taking the clustering structures into consideration. For future work, we plan to study discriminatively learning of nonlinear feature transfor-mation for domain adaptation.
 This work was partially supported by DARPA D11AP00278, NSF IIS-1065243 and a USC Annenberg Fellowship (Y. Shi).

