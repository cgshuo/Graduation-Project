 forms best under the worst possible parameters. This approa ch, termed the  X  X obust X  approach, has been used in both single stage ([1]) and multi-stage decisio n problems (e.g., [2]). Under the assumption that the uncertainty is state-wise ind ependent (an assumption made by all i.e., a solution which provides mediocre performance under all possible parameters. Second, the the real parameters, so that the performance of the solution under nominal parameters may provide a certain tradeoff relationship between the worst-case per formance and the nominal performance, dramatically. On the other hand, relaxing both criteria may lead to a well balanced solution with both satisfactory nominal performance and also reasonable robustness to parameter uncertainty. In this paper we capture the Robustness-Performance (RP) tr adeoff explicitly. We use the worst-parameters simultaneously. Here,  X  X imultaneously X  is ach ieved by optimizing the weighted sum of attempt to address the overly conservativeness of worst-ca se analysis in robust MDP. her preference, which is usually complicated and an explici t form is not available. Our approach line without trial and error.
 The paper is organized as follows. Section 2 is devoted to the RP tradeoff for Linear Programming. In Section 3 and Section 4 we discuss the RP tradeoff for MDP wi th uncertain rewards, and uncer-concluding remarks are offered in Section 6. how it can be used to find the whole set of Pareto efficient solut ions for RP tradeoffs in Linear Programming. This serves as the base for the discussion of RP tradeoffs in MDPs. 2.1 Parametric Linear Programming A Parametric Linear Programming is the following set of infin itely many optimization problems: Program (LP) is feasible and bounded for both objectives. Al though there are uncountably many  X  specific way. See [10] for a precise description.
 Algorithm 1. 1. Find a basic feasible optimal solution for  X  = 0 . If multiple solutions exist, the PLP. Furthermore, we can find this subset by sequentially pivoting among neighboring extreme shared by all simplex based algorithm. A detailed discussio n on PLP can be found in [10, 11, 12]. 2.2 RP tradeoffs in Linear Programming Consider the following LP: Suppose that the constraint matrix A is only a guess of the unknown true parameter A r which is known to belonging to set A (we call A the uncertainty set ). We assume that A is constraint-wise independent and polyhedral for each of the constraints. Tha t is, A = Q n exists a matrix T ( i ) and a vector v ( i ) such that A ing criterion to be minimized as its robustness measure (more accurately, non-robustness measure). i Using the weighted sum of the performance and robustness obj ective as the minimizing objective, we formulate the explicit tradeoff between robustness and p erformance as: By duality theorem, for a given x , sup following LP on y ( i ) : Thus, by adding slack variables, we rewrite GENERAL PROBLEM as the following PLP and solve it using Algorithm 1: Here, 1 stands for a vector of ones of length n , z optimization variables. A (finite) MDP is defined as a 5-tuple &lt; T, S, A infinite) set of decision stages; S is the state set; A transition probability; and r ( s, a ) is the expected reward of state s with action a  X  A r to denote the vector combining the reward for all state-acti on pairs and r combining all reward of state s . Thus, r ( s, a ) = r and r are time invariant.
 In this section, we consider the case where r is not known exactly. More specifically, we have a reward r is known to belong to a bounded set R . We further assume that the uncertainty set R is state-wise independent and a polytope for each state. That i s, R = Q there exists a matrix C set of randomized history dependent policies, which we deno te by  X  HR .
 infinite horizon under a unichain assumption. 3.1 Finite horizon case and use S only one state s the performance measure and the robustness measure of a poli cy  X   X   X  HR : The minimum is attainable, since R is compact and the total expected reward is a continuous func -be found in the full version of the paper.
 For 0  X  t  X  N , s  X  S We set P follows similarly to standard backward induction in finite h orizon robust decision problems. Theorem 1. For s  X  S We now consider the maximin problem in each state and show how to find the solutions for all  X  in one pass. We also prove that c  X  for all j  X  X  1 ,  X  X  X  , k } , c  X  are linear. That is, there exist constants l j By the duality theorem, we have that c  X  q .
 Observe that the feasible set is the same for all  X  . Substituting c  X  that for  X   X  [  X  Thus, for  X   X  [  X  that the resulting c  X  continuous and piecewise linear value functions holds by ba ckward induction. 3.2 Discounted reward infinite horizon case trying to maximize the weighted sum and Nature trying to minimize it by selecting an adversarial policy exists; see Proposition 7.3 in [13].
 one correspondence relationship between the state-action frequencies P  X  stationary strategies and vectors belonging to the followi ng polytope X : Since it suffices to consider a stationary policy for Nature, the tradeoff problem becomes: 3.3 Limiting average reward case (unichain) discounted case, the tradeoff problem can be converted to th e following PLP: In this section we provide a counterexample which demonstra tes that the weighted sum criterion optimal policies.
 We look for the strategy that maximizes the sum of the nominal reward and the worst-reward (i.e., choose a (5 , 2) .
 The unique optimal strategy for this example is thus non-Mar kovian. This non-Markovian property problem intractable. The optimal strategy is non-Markovia n because we are taking expectation over be used in finding the optimal strategy. replacing cost is perfectly known to be c that the realization of the running cost lies in the interval [ c and  X  shows the tradeoff of this MDP.
 For each solution found, we sample the reward 300 times accor ding to a uniform distribution. We normalize the cost for each simulation, i.e., we divide the c ost by the smallest expected nominal cost. Denoting the normalized cost of the i function to compare the solutions: parameters are exactly formulated. Even in such case, we see that risk-averse decision makers can benefit from considering the RP tradeoff. off by treating the robustness as an optimization objective . Based on PLP, for MDPs where only simulation for different values of  X  . rewards are uncertain, we presented an efficient algorithm t hat computes the whole set of optimal reward (unichain). For MDPs with uncertain transition prob abilities, we showed an example where the solution may be non-Markovian and hence may in general be intractable.
 the decision maker from the need to make probabilistic assum ptions on the problems parameters. It also allows the decision maker to determine the desired robu stness-performance tradeoff based on observing the whole curve of possible tradeoffs rather than guessing a single value.
