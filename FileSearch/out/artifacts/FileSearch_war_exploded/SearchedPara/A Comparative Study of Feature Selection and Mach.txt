 The web has become the most important place for expressin g sentiments, eval u ations, participate in and exchange their opinions through online community -based social media, such as web forums, shopping sites, and blogs. The explosive growth of the user -generated content of  X What other people think X   X  represents an extremely impor-tant so u rce of information for many special g ro u ps. Identifyin g and analyzin g helpf u l reviews efficiently and acc u rately to satisfy both c u rrent and potential c u stomer needs have become a critical challen g e for market-driven prod u ct desi g n. Recently, there in g people  X  s opinions from lar g e vol u mes of u nstr u ct u red review text. cate g ories. There is a diversity of methods and approaches for sentiment classification and opinion minin g . The majority of techniq u es fall into two main methodolo g ies: s u pervised and u ns u pervised learnin g approaches. In the s u pervised machine learnin g approach, sentiment corpora are u sed to train classifiers[1, 2]. 
The u ns u pervised approaches, or semantic orientation (SO) approaches, u tilize lex-ical reso u rces, s u ch as SentiWordNet, to meas u re the polarity orientation of the words to classify the reviews [3].Both approaches have their own advanta g es and draw-depends on the availability of labeled data sets (trainin g data), which are often im-possible or diffic u lt to find, partially d u e to the novelty of the task. In contrast, the SO cally depends on the lan gu a g e [1]. 
Most of the st u dies on sentiment classification consider only En g lish reviews, perhaps d u e to the lack of reso u rces in other lan gu a g es. Work on other lan gu a g es is still g rowin g , incl u din g German, Chinese, Spanish, and Arabic [1, 4, 5]. Despite the fact that Arabic is c u rrently amon g the top 10 lan gu a g es u sed most freq u ently on the Internet accordin g to the Internet World State rankin g s, there are very few reso u rces and tools for sentiment or opinion classification in the Arabic lan gu a g e. However, points of view. Conseq u ently, the need for constr u ctin g reso u rces and tools for s u b-work presented in this paper is mainly motivated by the need to develop sentiment classification systems in lan gu a g es other than En g lish, and the paper specifically foc u ses on Arabic sentiment classification. In this paper, we have desi g ned several sentiment classification models for Arabic sentiment analysis. We present an empirical comparison of seven feat u re selection methods (information g ain (IG), principal components analysis ( P CA), Relief-F, Gini Index, u ncertainty, Chi-sq u are, and s u pport vector machines (SVMs)). The classification performance of the feat u re selection methods (FSMs) is investi-g ated by u sin g three machine learnin g classifiers (SVM, Naive Bayes (NB), and K-Arabic (OCA) that was collected from a variety of web pa g es abo u t movie reviews in the Arabic lan gu a g e[6]. 
The remainder of this paper is or g anized as follows. Section 2 reviews related work in the area of sentiment analysis. The methodolo g y and the different key tech-niq u es and approaches are described in Section 3. In Section 4, we present the expe-riment set u p and disc u ss the experimental res u lts. Finally, we concl u de o u r work and disc u ss f u t u re directions of research in Section 5 .
 The research literat u re incl u des some comprehensive reviews related to sentiment doc u ments. Many of the research st u dies on sentiment and s u bjectivity analysis have man, Chinese, Spanish, and Arabic [1], [4], [5]. Most of the existin g sentiment analy-sis work foc u ses on determinin g the sentiment orientations at one of three levels: the doc u ment level [9, 10], the sentence level [11-14], or the feat u re level [15-19]. 
In doc u ment-level sentiment analysis, reviews are classified into positive, ne g ative, machine learnin g techniq u es have been employed in several sentiment classification [21], decision tree [22, 23], and KNN [24] have been applied to sentiment classification. 
However, the s u ccess of these methods depends on the domain, topic, and time pe-riod represented by the trainin g data. A st u dy on the effectiveness of machine learnin g s u pervised classification methods to sentiment classification. The experiments indi-prod u ced baselines. 
In addition, the semantic orientation approach or u ns u pervised learnin g method is also u sed in sentiment classification. The semantic orientation approach does not re-q u ire any prior trainin g . Instead, it u ses lexical reso u rces to calc u late how far a word ment classification. Es u li and Sebastiani[28] propose a semi-s u pervised learnin g me-thod u sin g WordNet as a lexical reso u rce. Their method starts by expandin g an initial seed set from WordNet. Their basic ass u mption is that terms that have similar orienta-tions tend to have similar sentiments. These a u thors u se a statistical techniq u e to de-termine an expanded seed term's semantic orientation thro ug h classification of the word sentiments. Finally, transfer learnin g approaches are also employed, partic u larly when there are no labels on the data [29, 30]. The transfer-based approaches aim to u tilize data from other domains or time periods to help the c u rrent learnin g task. Re-cently, transfer learnin g has drawn increasin g attention as an important research field in machine learnin g [31]. 
Most of the other Arabic sentiment analysis papers [32, 33] have u sed s u pervised approaches b u t have not incl u ded any comparison of FSMs and machine learnin g methods. Some of the FSMs mentioned above have been st u died individ u ally. Accor-din g ly, the performance of different types of FSMs m u st be compared and analyzed when u sed with different machine learnin g techniq u es. In addition, those FSMs that have not yet been applied to any Arabic text classification tasks (incl u din g sentiment classifications s u ch as SVM-based and P CA) m u st be investi g ated, and their effect on the performance of state-of-the-art machine learnin g al g orithms m u st be compared with those of other FSMs. The methodolo g y u sed in o u r sentiment classification system is shown in Fi g . 1. First, Data m u st be preprocessed to perform any f u rther data minin g f u nctionality. Then, we apply FSMs to identify discriminatin g terms for trainin g and classification. Finally, a machine learnin g method is u sed to classify sentiments into positive and ne g ative classes. 3.1 Pre-processing Data preprocessin g comprises three steps: 1) tokenization, 2) normalization, and 3) stop word removal. All of the reviews involve a preprocessin g sta g e. In the normali-zation process, diacritics, repeated characters, and social media ta g s are removed. Second, in this phase, we remove certain stop words that are common in all of the reviews to avoid misclassifyin g the reviews. Finally, a stemmin g process is cond u cted to excl u de the root forms of the words. 3.2 Feature Selection Methods A FSM is an important component for an effective sentiment classification system. An FSM improves the performance for text classification tasks in terms of their learn-in g speed and effectiveness. A FSM also red u ces the n u mber of data dimensions and removes irrelevant, red u ndant, and noisy data [34]. O u r selection of the FSM metrics also infl u enced by the need to investi g ate the most efficient FSMs, s u ch as SVM-based, Relief, and P CA; these methods have not yet been applied to any Arabic text classification tasks, incl u din g sentiment classification. We compare their performance with other common FSM metrics, partic u larly those that have been proven effective in feat u re selection al g orithms will be st u died with three state-of-the-art machine learn-in g al g orithms. In this section, we provide a brief introd u ction to seven effective FSMs: IG, P CA, Relief, Gini Index, u ncertainty, Chi-sq u ared, and SVM-based methods. These seven methods comp u te a score for each individ u al feat u re and then select a predefined size for the feat u re set. Information Gain. IG is a well-known al g orithm for feat u re selection. It has been u sed as a term g oodness meas u re in the field of machine learnin g [37]. IG meas u res correct classification decision on any class. where p  X  c  X   X  denotes the probability that class c  X  occ u rs; p X  X  X  denotes the probability that word t occ u rs; an d p X  X   X   X  denotes the probability that word t  X  does not occ u r. Principal Components Analysis. The p u rpose of P CA [38] is to derive new variables that are linear combinations of the ori g inal variables and are u ncorrelated. Geometri-cally, P CA can be tho ug ht of as a rotation of the axes of the ori g inal coordinate sys-tem to a new set of ortho g onal axes that are ordered in terms of the amo u nt of varia-tion in the ori g inal data for which they acco u nt. 
Let x be the ori g inal D-dimensional observation vector that represents a review and let tion: 
The coefficients of the D  X  d matrix A can be fo u nd as follows. Let  X  be the co-variance matrix of x. This matrix has D ei g enval u es: 
It can be ded u ced that matrix A is composed of d D ei g envectors that correspond to the d lar g est ei g enval u es ,1 i D i  X   X  X  X  . Every ei g envector is a col u mn of the ma-trix A. Additional details abo u t the P CA as a feat u re selection al g orithm can be fo u nd in [38, 39].
 SVM-Based Feature Selection. In this st u dy, the SVM linear kernel is u sed. the n u mber of distinct feat u res in the model. In g eneral, the class predictor trained by SVM has the form However, in the case of a linear kernel K(x, z) = xTz, this statement can be rewritten as where the vector of wei g hts w = (w1,...,wd) can be comp u ted and accessed direct-ly. Geometrically, the predictor u ses a hyperplane to separate the positive from ne g a-tive instances, and w is the normal to this hyperplane. The linear classifier cate g orizes of the components of vector x = (x1,...,xd) is above or below a g iven threshold. For a Only the feat u res for which the val u e of |wj| exceeds the threshold val u e are retained. proportional to ||w||, the len g th of w. Beca u se ii model, one can re g ard xxx = ; th u s, we can eval u ate the infl u ence of feat u re j on 2 w by considerin g the absol u te val u es of the partial derivatives of Relief. Relief-f [40, 41] is a commonly u sed metric for feat u re rankin g that estimates instance from its nearest hit (instance of the same class) and nearest miss (opposite class). The Relief feat u re selection al g orithm selects feat u re instances randomly from the trainin g data. For each sampled instance, the nearest hit and nearest miss are from different classes and has the same val u e for instances of the same class. Specifi-allocate that val u e as the wei g ht for each term feat u re f [42]: Chi-Squared Statistic ( feat u re selection al g orithms. The 2  X  statistic meas u res the lack of independence be-tween the term and cate g ory [43] and is defined as follows: where A is the n u mber of times that t and c co-occ u r, B is the n u mber of times that of times that neither c nor t occ u rs, and N is the total n u mber of doc u ments. Gini Index. A novel Gini Index al g orithm, introd u ced by Wenqian Shan g et al. [44], attrib u tes in decision trees. The q u ality of the attrib u te improves with decreasin g im-p u rity size. 
In this form u la, if the feat u re t appears in every doc u ment of class  X   X  , then the max-im u m val u e, (t) 1 G i n i = , is obtained. Uncertainty-Based Term Selection. This operator calc u lates the relevance of the respect to the class. The relevance is calc u lated by the followin g form u la: 3.3 Classification Methods In this st u dy, three classifier methods are u sed in Arabic sentiment classification; the NB, SVM, and KNN methods are u sed d u e to their simplicity, effectiveness, and ac-c u racy. Brief descriptions of these methods are provided below. SVM Classifier. A SVM is a relatively new class of machine learnin g techniq u es that was first introd u ced by [45]. SVMs are a very pop u lar techniq u e for text cate g oriza-most effective classification methods accordin g to their performance on text classifi-cation, as proven by many researchers [46, 47] 
Based on the str u ct u ral risk minimization principle from comp u tational learnin g only effective elements in the trainin g set. 
M u ltiple variants of SVMs have been developed [48]. In this paper, o u r disc u ssion is limited to linear SVMs d u e to their pop u larity and hi g h performance in text cate g o-rization [37]. The optimization proced u re of SVMs (d u al form) is to minimize the followin g : Naive Bayes. The NB al g orithm is a widely u sed al g orithm for review classification. Given a feat u re vector table, the al g orithm comp u tes the posterior probability that the review belon g s to different classes and assi g ns it to the class that has the hi g hest post-erior probability. There are two commonly u sed models (i.e., the m u ltinomial model tion. NB ass u mes a stochastic model of doc u ment g eneration and u ses Bayes  X  r u le. To classify the most probable class c* for a new doc u ment d, NB comp u tes 
The NB classifier calc u lates the posterior probability as follows: K-Nearest Neighbor Classifier. The KNN is a well-known example-based classifier. The KNN has been called lazy learners beca u se it defers its decision on how to g ene-ralize beyond the trainin g data u ntil each new q u ery instance is enco u ntered. To cate-g orize a review, the KNN classifier ranks the review  X  s nei g hbors amon g the trainin g reviews. Then, the KNN u ses the class labels of the K most similar nei g hbors. 
Given a test review d, the system finds the K nearest nei g hbors amon g the trainin g reviews. The similarity score of each nearest nei g hbor review to the test review is u sed as the wei g ht of the classes of the nei g hbor review. The wei g hted s u m in KNN classification can be written as follows: 
Where KNN(d) indicates the set of K nearest nei g hbors of review d . If  X   X  belon g s to to the class that has the hi g hest res u ltin g wei g hted s u m. 3.4 Experimental Setup We cond u ct several experiments to eval u ate o u r model. First, we eval u ate the perfor-mance of the classification al g orithms. We meas u re the performance of these classifi-cation al g orithms on an OCA. The corp u s contains 500 movie reviews collected from different web pa g es and blo g s in Arabic; 250 of them are considered positive reviews, and the other 250 are considered ne g ative. 
All of the al g orithms are eval u ated u sin g K-fold cross-validation. The objective of analysis. To meas u re the performance of these classification methods, experimental correctly assi g ned to the g iven cate g ory, False P ositive (F P ) is the set of reviews that is incorrectly assi g ned to the cate g ory, False Ne g ative (FN) is the set of reviews that is incorrectly not assi g ned to the cate g ory, and Tr u e Ne g ative (TN) is the set of the set of reviews that is correctly not assi g ned to the cate g ory. However, we u se the F1 and Macro-F1 meas u res. The followin g describes these metrics: To examine the classifiers  X  overall performance on Arabic sentiment analysis witho u t doc u ment-term feat u re space. The experimental res u lts u sin g the NB, SVM, and KNN classifiers are s u mmarized in Table 1. In applications made witho u t u sin g any feat u re red u ction method, the hi g hest performance is obtained with the SVM classifier, and the worst performance is obtained with the NB classifier. Next, the seven FSMs (IG, P CA, SVM, Relief, Chi, Gini, and u ncertainty) are applied to select the feat u re spac-es. In this phase, the effects of the individ u al feat u re rankin g methods on clas-sifier performance are examined. 
The macro-avera g in g F-meas u re res u lts for the NB classifier with the seven FSM selection methods at different feat u re s u bset sizes, as it presented in Table 2. The sev-en FSMs (IG, P CA, SVM, Relief, Chi, Gini, and u ncertainty) perform better than the ori g inal classifier. The SVM-based and IG FSMs typically yield the best performance in terms of the macro-avera g in g F-meas u re (see the avera g e row in Table 2). Accord-in g to Table 2, the hi g hest performance (90.58) of the NB classifier is obtained when u sin g 100 of the wei g hted feat u res from the SVM-based methods. 
The macro-avera g in g F-meas u re res u lts for the SVM classifier with the seven FSM selection methods at different feat u re s u bset sizes, as it presented in Table 2. Fo u r of the seven FSMs (SVM, IG, CHI, and GINI) perform better than the ori g inal classifier. SVM-based and IG tend to yield the hi g hest performance in terms of macro-avera g in g performance (92.38) of the SVM classifier is obtained when u sin g 300 of the wei g hted feat u res by the SVM-based method. 
For the KNN classifier with the seven FSM selection methods at different feat u re the seven FSMs (SVM, IG, and GINI) perform sli g htly better than the ori g inal clas-sifier. GINI performs best in terms of macro-avera g in g the F-meas u re (see the av-era g e row in Table 4). Accordin g to Table 4, the hi g hest performance (88.00) of the KNN classifier is obtained when u sin g 100 of the wei g hted feat u res from the Relief method. 
Comparin g the classifier performances (Tables 2, 3, and 4), the SVM al g orithm o u tperforms the NB and KNN al g orithms. F u rthermore, the hi g hest acc u racies are obtained when the feat u re selection operations are made by the SVM-based method. (Tables 1, 2, 3, and 4) in an affirmative manner .

Comparin g the behaviors of the three classifiers in terms of the macro-avera g in g F-meas u re res u lts with each FSM, u sin g different sizes of feat u re sets as shown in Fi g . one FSM vary when u sin g different n u mbers of feat u res. In addition, there is no s u pe-classifier o u tperforms the SVM and KNN classifiers when the Relief, Chi, and u ncer-tainty FSMs are u sed, whereas the SVM classifier o u tperforms the NB and KNN clas-sifiers when the SVM and P CA FSMs are u sed and the KNN classifier o u tperforms the NB and SVM classifiers when the GINI FSM is u sed. However, when the IG FSM method. This paper presented an extensive comparative st u dy of seven FSM methods and three selections for Arabic sentiment classification tasks. The main contrib u tion of machine learnin g methods in terms of the macro-F meas u re. The res u lts indicate that s u perior FSM method for all dataset sizes. The res u lts also demonstrate that the u se of the best seven FSMs yields improved res u lts compared to those obtained u sin g the demonstrate that the SVM-based FSM performs the best amon g the sentiment FSMs and that the SVM classifier demonstrates the best performance for the Arabic senti-ment classification with an acc u racy of 92.4%, whereas NB classifier yields better res u lts when u sed with the other FSMs considered .
 Arabic lar g e sentiment corp u s, investi g atin g the implementation of Semantic orienta-tion approaches, and investi g atin g the implementation of some optimization al g o-rithms to address the feat u re selection problem for Arabic sentiment classification. 
