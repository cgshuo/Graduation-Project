 Searching an organization X  X  document repositories for ex-perts provides a cost effective solution for the task of expert finding. We present two general strategies to expert search-ing given a document collection which are formalized using generative probabilistic models. The first of these directly models an expert X  X  knowledge based on the documents that they are associated with, whilst the second locates docu-ments on topic, and then finds the associated expert. Form-ing reliable associations is crucial to the performance of ex-pert finding systems. Consequently, in our evaluation we compare the different approaches, exploring a variety of as-sociations along with other operational parameters (such as topicality). Using the TREC Enterprise corpora, we show that the second strategy consistently outperforms the first. A comparison against other unsupervised techniques, reveals that our second model delivers excellent performance. H.3 [ Information Storage and Retrieval ]: H.3.1 Con-tent Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4 Systems and Software; H.4 [ Information Systems Applications ]: H.4.2 Types of Systems; H.4.m Miscellaneous Algorithms, Measurement, Performance, Experimentation Expert finding, enterprise search
A major challenge within any commercial, educational, or government organization is managing the expertise of em-ployees such that experts in a particular area can be iden-tified. Finding the right person in an organization with the Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. appropriate skills and knowledge is often crucial to the suc-cess of projects being undertaken [14]. For instance, an employee may want to ascertain who worked on a partic-ular project to find out why particular decisions were made without having to trawl through documentation (if there is any). Or, they may require a highly trained specialist to consult about a very specific problem in a particular pro-gramming language, standard, law, etc. Identifying experts may reduce costs and facilitate a better solution than could be achieved otherwise.

Initial approaches to expert finding employed a database housing the skills and knowledge of each individual in the organization [5, 11]. The database would be manually con-structed and consulted. Such approaches required consid-erable effort to set up and maintain. Consequently, various automated approaches have been devised to mine the in-formation repositories of organizations from which to build a profile of an employee X  X  expertise. Most approaches fo-cus on expert finding in specific domains extracting rep-resentations from known document types. More recently there has been a move to automatically extract such rep-resentations from heterogeneous document collections such as those found within a corporate intranet [2]. With much of this work performed in industry, details of solutions are restricted to high level designs and products tend to be sub-jected to in-house evaluations only.

The Text REtrieval Conference (TREC) has now provided a common platform with the Enterprise Search Track for re-searchers to empirically assess methods and techniques de-vised for expert finding [17]. The following scenario is pre-sented: Given a crawl of the World Wide Web Consortium X  X  web site, a list of candidate experts and a set of topics, the task is to find the experts for each of these topics.
We propose two models for accomplishing this task which draw upon and formalize existing strategies to expert find-ing. Our models are based on probabilistic language model-ing techniques which have been successfully applied in other Information Retrieval (IR) tasks. Each model ranks can-didates according to the probability of the candidate being an expert given the query topic, but the models differ in how this is performed. In our first model, we create a tex-tual representation of the individuals X  knowledge according to the documents with which they are associated. From this representation we assess how probable the query topic is to rank candidates. Our second model ranks documents ac-cording to the query, and then we determine how likely a candidate is an expert by considering the set of documents associated. Here, the documents act as a latent variable between the query and the candidate, and we model the process of finding experts via documents in the collection.
Then, we use the TREC test collection to evaluate and compare these models. This is achieved through a system-atic exploration of different types of associations between documents and candidate experts, since such associations are crucial to the models X  success. Our main research goals in experimentation are to understand how the quality of as-sociations, along with the different search strategies, impact on the ability to successfully identify candidates.
The remainder of the paper is organized as follows. In Sec-tion 2 we briefly discuss related work. Then, in Section 3 we describe two ways of modeling the expert search task. Sec-tion 4 is devoted to different candidate extraction methods examined. Next, in Section 5 we present an experimental evaluation of our expert finding methods, using resources from the TREC 2005 Enterprise track. We conclude with a discussion of our main findings in Section 6.
Addressing the problem of identifying expertise within an organization has lead to the development of a class of search engines known as expert finders [19]. McDonald and Ack-erman [12] distinguish several aspects of expert finding, in-cluding what they call expertise identification ( X  X ho are the experts on topic X? X ) and expertise selection ( X  X hat does expert Y know? X ). In this paper we are focused exclusively on the first question.

Early approaches to this type of expert search used a database containing a description of peoples X  skills within an organization [20]. However, explicating such information for each individual in the organization is laborious and costly. The static nature of the databases often renders them incom-plete and antiquated. Moreover, expert searching queries tend to be fine-grained and specific, but descriptions of ex-pertise tend to be generic [10]. To address these disadvan-tages a number of systems have been proposed aimed at automatically discovering up-to-date expertise information from secondary sources. Usually, this has been performed in specific domains. For instance, there have been attempts to use email communications for expert finding in discus-sion threads and personal emails. Campbell et al. [1] an-alyzed the link structure defined by authors and receivers of emails using a modified version of the Hyperlink-Induced Topic Search (HITS) algorithm to identify authorities. They showed that improvements over a content-based approach were possible given two organizations, but the number of candidates used was limited, only fifteen and nine. An alter-native approach to using email communications focused on detecting communities of expertise, positing that the signal-ing behavior between individuals would indicate expertise in a specific area, again using the HITS algorithm [4].
Instead of trying to find expertise residing in email com-munications others have examined the problem in the con-text of software engineering development. In [14], rules of thumb were applied to identifying candidates which had ex-pertise on a particular software project and, more specifi-cally, a piece of source code. Such heuristics were generated manually based on the current working practice. E.g., an-alyzingthesourcecodetoseewholastmodifiedthecode and what part.

Instead of focusing on just specific document types there has been increased interest in systems that index and mine published intranet documents as sources of expertise evi-dence [7]. Such documents are believed to contain tacit knowledge about the expertise of individuals, as can be seen from the above examples of email and source code. However, by considering more varied and heterogeneous sources such as web documents, reports, and so forth, an expert finding system will be more widely applicable. One such published approach is the P@noptic system [2], which builds a repre-sentation of each candidate by concatenating all documents associated with that candidate. When a query is submitted to the system it is matched against this representation, as if it were a document retrieval system. This approach is the most similar to our proposed models. In our Model 1, we do exactly this but formalized in a language modeling framework, where we can weight the association between a candidate and a document instead of naively concatenating documents to form a representation. This can be considered as the baseline for comparison purposes. Our Model 2 (Sec-tion 3.4), takes a decidely different approach, where we rely upon how users themselves would find experts given a stan-dard search engine [8] (i.e., look for documents, find out who wrote them, and then contact the author). Our approach automates this process in a principled manner, where we first find documents which are relevant to the query topic and then score each candidate by aggregating over all doc-uments associated to that candidate.

While our work focuses exclusively on core algorithms for expert finding, it is important to realize that expert finders are often integrated into organizational information systems, such as knowledge management systems, recommender sys-tems, and computer supported collaborative work systems, to support collaborations on complex tasks [6]. For a more complete account of expert finding systems we refer the reader to [20].
In this section we detail the ways in which we model the expert finding task. First, we provide some background to language modelling applied to Information Retrieval; then we turn our attention to the document-candidate associa-tions that are required for the definition of our two models of expert finding. Model 1 uses candidate models to dis-cover a candidate X  X  expertise, while Model 2 uses document models to get to a candidate X  X  areas of expertise.
In recent years, language modeling approaches to infor-mation retrieval have attracted a lot of attention [9, 13, 15]. Language models are attractive because of their foundations in statistical theory, the great deal of complementary work on language modeling in speech recognition and natural lan-guage processing, and the fact that very simple language modeling retrieval methods have performed quite well em-pirically. The basic idea of these approaches is to estimate a language model for each document, and then rank doc-uments by the likelihood of the query according to the es-timated language model. In our modeling of expert search we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with a restricted named entity extraction task X  X he language modeling set-ting allows us to do this in a transparent manner.
Our approach to expert search assumes that we have a heterogeneous document repository, such as a corporate in-tranet, containing a mixture of different document types (e.g., technical reports, email discussion, web pages, etc). We assume that a document d in this collection is associ-ated with a candidate ca , if there is a non-zero association a ( d, ca ) &gt; 0. This association may capture various aspects of the relation between a document and a candidate expert; e.g., it may quantify the degree to which this document is representative of the candidate X  X  expertise, or, vice-versa, it may capture the extent to which the candidate is responsible for the document X  X  content. Forming document-candidate associations is a non-trival problem, which we consider in detail later in this paper (Section 4). For now, we present our formal models assuming we have these associations.
We state the problem of identifying candidates who are experts for a given topic, as follows: That is, we determine p ( ca | q ), and rank candidates ca ac-cording to this probability. The top k candidates are deemed the most probable experts for the given query. The chal-lenge, of course, is how to estimate this probability accu-rately. Instead of computing this probability directly, we apply Bayes X  Theorem, and obtain where p ( ca ) is the probability of a candidate and p ( q )is the probability of a query. Thus, the ranking of candidates is proportional to the probability of the query given the candidate p ( q | ca ).

To determine p ( q | ca ) we adapt generative probabilistic language modeling techniques from Information Retrieval in two different ways. In our first approach (Model 1), we build a representation of the candidate (i.e., we build a candidate model) using the documents associated with the candidate, and from this model the query is generated. In our second approach (Model 2), the query and candidate are consid-ered to be conditionally independent, and their relation is resolved through the document-candidate associations.
For both Model 1 and Model 2 (still to be defined), we need to be able to estimate the probability that a document d is associated with candidate ca . To define this proba-bility, we assume that non-zero associations a ( d, ca )have been calculated for each document and for each candidate. We distinguish two ways of converting these associations to into probabilities, thus estimating their strength. The first, document-centric perspective is to estimate the strength of the association between d and ca in terms of the probability p ( d | ca ). Here, we define where D is the set of documents. Intuitively, if we rank documents using (1) (for a given candidate ca ), the top doc-uments will be the ones that the candidate expert is most strongly associated with.

In the second, candidate-centric way of estimating the strength of the association between documents d and candi-dates ca , we use the probability p ( ca | d ), and put where C denotes the set of possible candidate experts. The idea here is this: d is a document produced by our enterprise, and ca is one of the people in the enterprise, who made some kind of contribution to d ; when we rank candidates using (2) (for a fixed d ), we find the candidate who made the biggest contribution to d .

Observe that there is a proportional relation between the document-centric and candidate-centric views, via Bayes X  rule:
Our first formal model for the expert finding task (Model 1) builds on well-known intuitions from standard language modeling techniques applied to document retrieval. A can-didate ca is represented by a multinomial probability dis-tribution over the vocabulary of terms (i.e., p ( t | ca )). Since p ( t | ca ) may contain zero probabilities, due to data sparsity, it is standard to employ smoothing. Therefore, we infer a candidate model  X  ca for each candidate ca , such that the probability of a term given the candidate model is p ( t |
We can then estimate the probability of the query being generated by the candidate model  X  ca . As usual, the query is represented by a set of terms, such that t is in q if the number of times t occurs in q , n ( t, q ), is greater than zero. Each query term is assumed to be generated independently, and so the query likelihood is obtained by taking the product across all the terms in the query, such that: To obtain an estimate of p ( t |  X  ca ), we first construct an em-pirical model p ( t | ca ) using our list of associations, and then smooth this estimate with the background collection prob-abilities. Specifically, within the document-centric perspec-tive, the probability of a term given a candidate, can be expressed as and under the candidate-centric perspective it is expressed as where p ( t | d ) is the maximum likelihood estimate of the term in a document. By marginalizing over all documents we ob-tain an estimate of p ( t | ca ). The candidate model is then con-structed by a linear interpolation of the background model p ( t ), and the smoothed estimate: Now, if we let f ( d, ca ) denote either p ( d | ca )or p ( ca put together our choices so far, we obtain the following final estimation of the probability of a query given the candidate model: This, then, is Model 1. In words, Model 1 amasses all the term information from all the documents associated with the candidate and uses this to represent that candidate. This model is used to predict how likely this candidate would pro-duce a certain query q , which can be intuitively interpreted as the probabilty of this candidate talking about this topic, where we assume this is indicative of their expertise.
Instead of directly creating a candidate model as in Model 1, we can compute the probability p ( q | ca ) by assuming con-ditional independence between the query and the candidate. Thus, the probability of a query given a candidate can be viewed as the following generative process: By taking the sum over all documents d , we obtain p ( q | Formally, this can be expressed in either a document-centric way, as or a candidate-centric way, as Conceptually, Model 2 differs from Model 1 because the can-didate is not directly modelled. Instead, the document acts as a hidden variable in the process, separating the query from the candidate. Under this model, we can think of the process of finding an expert as follows. Given a collection of documents ranked according to the query, we examine each document and if relevant to our problem, we then see who is associated with that document (we assume they have knowledge about that topic). Here, the process is taken to the extreme where we consider all documents in the collec-tion.

To determine the probability of a query given a docu-ment, we infer a document model  X  d for each document d . The probability of a term t given the document model  X  d becomes: and the probability of a query given the document model is: The final estimation of Model 2, then, is: where f ( d, ca )is p ( d | ca )or p ( ca | d ), depending on whether we adopt a document-centric or candidate-centric perspec-tive (as with the final estimation of Model 1, cf. Equation 3).
An advantage of Model 2 over Model 1 is that, given a set of document-candidate associations it can easily be im-plemented on top of a standard document index, whereas Model 1 requires that a separate candidate-term index be created and maintained.
So far, we have assumed that all documents in the collec-tion are used in the computations of Model 1 and 2. This implicitly assumes that all the documents associated with a candidate are related and about one particular topic of expertise. Often a candidate will have expertise in several areas. Therefore, it may be more appropiate to only use a subset of the collection for expert finding, those that are related to the query topic at hand.

We can simply obtain a query-biased cluster of documents by submitting a query to the collection and only using the top n documents retrieved for the computations underlying Model 1 and 2. One of our experiments in Section 5 is aimed at understanding the impact on the overall expert finding task of increasing the topicality of the underlying document collection in this manner.
Document-candidate associations form an essential part of the models presented in Section 3. We now turn to the task of building such associations. Specifically, we need to assign non-negative association scores a ( d, ca ) to pairs of documents d and candidate experts ca .

We assume that a list of possible candidates is given, where each candidate is represented with a unique person id , one or more names and one or more e-mail addresses. While this is a specific choice, and while different choices are possi-ble (e.g., involving social security number, or employee num-ber instead of, or in addition to, the representations just listed), the representations chosen are generic and nothing in our modeling depends on this particular choice.
The recognition of candidates (through one of these rep-resentations) is a (restricted and) specialized named en-tity recognition task. We approach it in a rule-based man-ner. We introduce four binary association methods A i ( i = 0 ,..., 3) that return 0 or 1 depending on whether the doc-ument d is associated with the candidate ca ; the first three attempt to identify a candidate by their name, the last uses the candidate X  X  email address: Note that for i =1,2,themethod A i extends upon the re-sults achieved by A i  X  1, thus increasing recall while (prob-ably) giving up some precision. The results of A3 are inde-pendent from the other three methods.
Since A0  X  A2 on the one hand, and A3 on the other, identify candidates by orthogonal means, it makes sense to combine extraction methods from the two groups, and to consider linear combinations of their outcomes. This, then, is how we define association scores: where  X  = {  X  1 ,..., X  k } and we will use the general form (5) when refering to an associ-ation method.

One of the questions we will address below is to which extent the quality of the candidate extraction method (and hence of the document-candidate associations) impacts the task of expert finding.
We now present an experimental evaluation of our mod-els for expert finding. We specify our research questions, describe our data set, and then address our questions.
We address the following research questions: Each of the research questions above identifies a dimension along which we explore the models we introduced. To re-main focused, and because of lack of space, we will usually consider each dimension in isolation and forego exploring large numbers of combinations of parameter settings, etc. We use the data set used at the 2005 edition of the TREC Enterprise track [17]. The document collection used is the W3C corpus [18], a heterogenous document repository con-taining a mixture of different document types crawled from the W3C website. The six different types of web pages were lists (email forum), dev, www, esw, other, and people (per-sonal homepages). In our experiments these were all handled and processed in the same way, as HTML documents. While different documents might require special treatment in order to exploit the richer representation of the content X  X  type, we leave this as future work. The W3C corpus contains 330,037 documents, adding up to 5.7GB.

We took the topics created within the expert finding task of the 2005 Enterprise track: 50 in total; these are the topics for which experts have to be sought. In addition, a list of 1092 candidate experts was made available, where each candidate is described with a uniqe candidate id, name(s) and one or more e-mail addresses.

Evaluation measures used for the expert finding task were mean average precision (MAP), R-precision, mean recipro-cal rank (MRR), and precision@10 and precision@20 [17]. Evaluation was done using the trec eval program. 1
How effectively can names be identified (and associations be established) in the document collection, using the ex-traction mechanisms described in Section 4? This issue can be addressed in an intrinsic way (assessing the extraction component in isolation, which is what we do in the present subsection) and in an extrinsic way (which is what we do in Subsection 5.7). Table 1: Results of different candidate extraction methods.

Table 1 contains the results of the different candidate ex-traction methods: %cand denotes the percentage of all pos-sible candidates that have been identified; %rel cand is the percentage of relevant candidates, that is, the real experts, that have been identified; #avg is the average number of associations (different documents) per candidate. Finally, %docs is the percentage of documents in the data set which have been associated to at least one candidate. The three horizontal sections of the table show the performance of name extraction (top), e-mail extraction (center), and the combination of these methods (bottom).

As expected, the LAST NAME MATCH method has the high-est recall of all the methods based on extraction by name. The EMAIL MATCH method seems to pick up a few candidates (and associations) not covered by the name-based methods, and combinations with this method lead to (small) improve-ments for each of the name-based methods.
Next, we turn to smoothing. We experimented with two smoothing methods: Jelinek Mercer and Bayes Smooth-ing [21], and observed that these behave differently in Model 1 and 2. In the case of Model 1 both methods performed similarly, with a slight advantage of Bayes Smoothing, while in case of Model 2, Jelinek Mercer was significantly better. For registered participants trec eval is available from the TREC web site http://trec.nist.gov .
To determine whether the observed differences between two Figure 1: Mean average precision of Model 1 and Model 2, using A0 and different  X  values.

The optimal  X  parameter for Bayes Smoothing differs for the two models. Model 1 performed best with a high  X  value (50000), but for Model 2 it is the opposite: the highest performance was achieved by using low values for  X  (100); see Figure 1. This behavior can be explained by considering the details of the two models. Low values of  X  have been found to be optimal for short documents, while high values are known to be optimal for long documents. Model 2 uses normal (short) documents, while for Model 1 we represent each candidate as the sequence of terms in the documents associated with the person X  X esulting very long documents.
In the following sections we use the Jelinek Mercer smooth-ing method, with  X  =0 . 5. While this is not the best possible setting for all extraction methods, models, and estimations, it is a reasonable setting that provides acceptable perfor-mance across the parameter space. Next we turn to our core question: which of Model 1 and Model 2 is most effective for finding experts? In the follow-ing set of experiments we compare the two formal models introduced in Section 3. A quick scan of Table 2 reveals that Model 2 outperforms Model 1 for nearly all settings. Let us focus on the best performing configurations for both, i.e., the candidate-centric probability estimation using the A0 extrac-tion method. Figure 2 shows the MAP scores over the 50 topics achieved by the two models; the topics are sorted ac-cording to the result achieved by Model 1. Clearly, Model 2 outperforms Model 1 on nearly all topics. The differences between Model 1 and Model 2 are statistically significant, for both estimation methods, and all extraction methods except for A3 ; this may mean that Model 2 performs better only when there are sufficiently many associations.
Which of the two ways of estimating the strength of associ-ation between a document and a candidate performs better? Document-centric or candidate-centric? Consider Table 2 expert finding approaches are statistically significant, we use Student X  X  t-test, and look for significant improvements (one-tailed) at a significance level of 0.95 [16]. again. If we do a cell-by-cell comparison of document-centric and candidate-centric estimation, we see that candidate-centric estimation outperforms document-centric estimation for almost all measures. For Model 2, candidate-centric es-timation is significantly better than document-centric esti-mation, for all extraction methods. For Model 1, we find a significant improvement only in the case of the extraction method A3 ; Model 1 seems insensitive to the probability es-timation if there are sufficiently many associations.
Based on the outcomes of our experiments we froze this parameter and use the candidate-centric estimation method in the following parts of the section.
The effectiveness of different candidate extraction meth-ods in terms of the number of candidates extracted and doc-uments associated, has already been examined in Subsec-tion 5.3. At this point we compare the accuracy achieved by these methods in an extrinsic manner, based on their impact on the effectiveness of expert finding.

As discussed previously, results (candidates and associ-ations) found by extraction the methods A i ( i =0 , 1) are also found by A i + 1. Table 2 shows that for each i =1, 2, the extraction method A i  X  1 outperforms the methods A i , according to nearly all models, measures, and estimation methods. Moreover, in spite of the low number of iden-tified candidates and associations, A3 achieves reasonably high precision in several cases. Hence, the quality and not the number of the associations has the main impact on pre-cision. According to our experiments, A0 (EXACT MATCH) performs best for building associations. In the followings, we refer to this configuration as a baseline ( BASE ). Table 2: Results of the different models, extraction methods and document/candidate-centric probabil-ity estimations. The columns of the table are: ex-traction method, number of relevant retrieved can-didates, mean average precision, R-precision, mean reciprocal rank, precision after 10 and 20 candidates retrieved. Best results are in bold face. Figure 2: Comparison of Model 1 and Model 2, using A0 extraction method and candidate-centric probability estimtion. Table 3: Results of combining name extraction ( A0 ) and e-mail extraction ( A3 ). Candidate-centric esti-mation.
Does a combination of candidate extraction methods in-crease performance on the expert finding task? A combi-nation could consist of any number of non-zero associations (4); we restricted ourselves to combinations of the best per-forming name extraction method ( A0 )andthee-mailex-traction method ( A3 ). Let  X  0 the weight on A0 and  X  3 weight on A3 , where  X  0 +  X  3 = 1. Figure 3 shows the effect of varying the weights on these methods. Table 3 contains detailed results of the best performing combination ( COMB ) with weights  X  0 =0 . 9,  X  3 =0 . 1 compared to A0 and A3 .
Our experiments show promising but mixed results. Com-binations of extraction methods did not achieve improve-ments on all measures. The optimal weights are rather am-biguous; the two models and the various measures might involve different  X  values, however the number of relevant retrieved candidates is increased in case of both models. The significant difference observed here was for Model 2 with the combination vs A3 .
The aim of this, our final set of experiments is to find out how the topicality of documents used to build the repre-sentations influences the performance on the expert finding task. Instead of using the full collection, we use a subset of documents defined by taking the top n documents returned by a standard document retrieval run when using the topic as query. Table 4 shows the results achieved by using dif-Figure 3: The impact of varying the weight on A0 (  X  0 ). Note that the left (  X  0 =0 )andright(  X  0 =1 ) boundaries of the plot refers to a single extraction method A3 and A0 , respectively. ferent values for n , the document cut-off. Note that BASE corresponds to n = | D | , in other words, models are built on the full document set. Table 4: Results of using a subset of top documents
For Model 1, the use of a restricted subset of documents was not of benefit to retrieval across the set of thresholds applied. In contrast, for Model 2 we witnessed improve-ments in some cases. Importantly, the time needed for model building is proportional to the number of documents. Us-ing a subset of documents could speed up the process of expert finding significantly while the desired accuracy could be controlled by adjusting the value of n (size of the sub-set). None of the differences observed between the runs for Model 2 was significant. This means that topicality does not hurt performance in the case of Model 2: using a restricted set of documents improves responsiveness.
Compared to the runs submitted by the TREC 2005 En-terprise Track participants [3], our best results would be in the top 5. Note that for a fair comparison it is important to take into account how others approached the task. Unlike some of the top 5 performing approaches, our models are unsupervised; no manual efforts were made to increase the performance. Moreover, unlike some other systems we did not make any assumptions with regard to the data collection and the topics. In particular, we did not resort to a special treatment of some of the documents (such as e.g., discussion lists), and we did not utilize the fact that the test topics were names of W3C working groups. In our approach the former could be exploited, e.g., to build high-quality associations, whilst the latter approaches the task as a  X  X orking group finding X  task rather than an expert finding task and would, we believe, suffer from not be generalizable beyond searching for W3C working groups; with our approach we can search for experts on any topic.
We focused on developing models for searching an or-ganization X  X  document repositories for experts on a given topic using generative probabilistic models. We explored our models along many dimensions; evaluations of these models were performed on the W3C TREC Enterprise collection.
We found that Model 2 performs better than Model 1 on practically all measures. However, this is not the only reason that leads us to favor Model 2. Various things we have tried out behaved more according to our expectations on Model 2 than on Model 1 (see e.g., in terms of how additions to the extraction method worked (or did not work) according to expectation). On top of that, Model 2 X  X  online behavior is much better than Model 1 X  X : Model 2 produces the results in a reasonable time which makes it useable even in an on-line application. It does not require a separate index to be created, like Model 1, but can immediately be applied to an indexed collection, given a set of associations. In prac-tical terms it means that it could be implemented on top of a standard search engine with very limited effort, only requiring a list of candidate-document associations.
Looking forward, possible improvements might be pur-sued in the named entity extraction (NE) component of the system. E.g., looking at other ways of forming associations, using NE recognition and trying to ascertain whether that person wrote the document, or is the owner of the document (e.g., their personal website). It would also be interesting to extract not only individuals, but also groups and communi-ties and methods for identifying people within these.
This research was supported by the Netherlands Organi-zation for Scientific Research (NWO), project 220-80-001.
