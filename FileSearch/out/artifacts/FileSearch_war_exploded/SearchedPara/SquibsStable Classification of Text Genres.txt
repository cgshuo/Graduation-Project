 University of Edinburgh University of Edinburgh
Every text has at least one topic and at least one genre. Evidence for a text X  X  topic and genre comes, in part, from its lexical and syntactic features X  X eatures used in both Automatic Topic
Classification and Automatic Genre Classification (AGC). Because an ideal AGC system should be stable in the face of changes in topic distribution, we assess five previously published AGC methods with respect to both performance on the same topic X  X enre distribution on which they were trained and stability of that performance across changes in topic X  X enre distribution. Our experiments lead us to conclude that (1) stability in the face of changing topical distributions should be added to the evaluation critera for new approaches to AGC, and (2) part-of-speech features should be considered individually when developing a high-performing, stable AGC system for a particular, possibly changing corpus. 1. Introduction
This article concerns Automated Genre Classification (AGC). Genre has a range of definitions, but for Language Technology, a good one is a class of documents that share a communicative purpose (e.g., Kessler, Nunberg, and Sch  X  utze 1997). Although com-municative purpose may be difficult to recognize without document understanding, researchers have found low-level features of texts to correlate with genre, making it a useful proxy.
 where users may want documents that serve a particular communicative purpose (instructions, reviews, user guides, etc.). AGC can also benefit Language Technology indirectly, where differences in the low-level properties that correlate with genre may impact system performance. For example, if a part X  X f X  X peech (PoS) tagger or Statistical
Machine Translation system trained on a corpus of editorials was then used for PoS tagging or translating a corpus of letters to the editor , it would benefit from the knowledge that inter alia the likelihood of the word  X  X tates X  being a verb is considerably higher in letters (  X  20%) than in editorials (  X  2%). 1 given genre can be about any given topic (Finn and Kushmerick 2006), yet it is clear that co-variances exist between genre and topic , with some genre X  X opic combinations more likely than others (cf. fiction vs. news reports about dragons).
 of text as a basis for their predictions, a feature indicative of topic might benefit a genre classifier through correlations in the training corpus. However, if the topics addressed in different genres can change unpredictably over time, such correlated features can then harm performance. Although domain adaptation techniques might remedy this, they typically require extensive data in the target domain, and the remedy may fail as soon as the distribution changes again.
 the extent to which it may change in an actively growing corpus. In order to motivate research on stability in AGC, 2 we analyzed a large, publicly available newspaper corpus and found that (1) genres and topics do correlate substantially and (2) these correlations vary substantially over time. 3 degrade the performance of AGC systems and are best removed unless the genre X  X opic distribution is guaranteed to be fixed, and (2) PoS features should not be lumped to-gether in AGC because they have different correlations with genre and topic. Although the experiments used to make these points reflect an extreme situation X  X  complete change in genre X  X opic distribution X  X hey do allow us to make these points convincingly. 2. Method
The data for our experiments come from the New York Times Annotated Corpus ( NYTAC ) (Sandhaus 2008), covering 21 years of publication (1987 X 2007) and more than 1.8 million articles. 4 Articles are richly annotated with meta-data, including fields whose values can be used to infer their genre and topic.

Genre: Two meta-data fields are related to the notion of genre as communicative pur-pose: Types of Material and Taxonomic Classifier . The former, appearing with 41.5% of articles, specifies the editorial category of an article. Usually the field has a single value, sometimes more than one. Although these values are not drawn from a fixed set, they can be used to infer genre after spelling errors are corrected (e.g., from Reivew to Review ) and similar values are merged (e.g., Editorial , editorial , Op-Ed ,and Editors X  Note ). a hierarchy, some of whose divisions indicate the section of the newspaper in which a document appears. In total, 99.5% of documents in the corpus contain a Taxonomic
Classifier field, with an average of 4.5 values per article. Although the hierarchy varies in depth, its second level comprises a set of four fairly high level genres X  X hat is, Top/ Classifieds , Top/Features , Top/News ,and Top/Opinion .
 nomic Classifier field to recognize documents from this genre. Specifically, we considered 386 any document with no Types of Material tag as a news report ,ifatleastoneofits Taxonomic Classifier values started with Top/News .
 Topic: Topic descriptors were drawn from the General Online Descriptors meta-data field.
The field appears with 79.7% of documents, with 3.3 descriptors per document on average. 5 Whereas General Online Descriptors are structured in a hierarchy, a document tagged with the more specific United States Politics and Government will also typically be tagged with the less-specific (i.e., closer to the root) value Politics and Government ,but not vice versa.

Framework: Our experiments use news reports , editorials ,and letters as target variables because similar classes have been used elsewhere in AGC research (e.g., Finn and
Kushmerick 2006; Karlgren and Cutting 1994). For topics, we chose three that occur frequently and that were distinct from each other, in order to maximize differences in the formal cues used by classifiers. We based distinctiveness on the percentage overlap of topic tags in the corpus: Topics were taken to be distinct if (for our three chosen genres) fewer than 5% of texts that had one of the tags had another of them. The degree of overlap in the three topic tags we chose, Education and Schools , Armament, Defense and Military Forces ,and Medicine and Health , ranges from 0.5% to 2.9%. For comparison, the degree of overlap of the pair Politics and Government and
International Relations ranges from 28.4% to 38.1% for our three genres. For all ex-periments, we only used texts which were unambiguously about Education, Medicine, or Defense. The small proportion of documents with more than one of these tags was ignored.
 topics, we varied the topical distribution of the test sets with respect to the training set. The training set consisted of 12,927 texts: News reports (News) about Education (Edu),
Editorials (Edit.) about Defense (Def), and Letters to the Editor (LttE) about Medicine (Med). (This pairing yields more articles in the corpus than any of the five other possible combinations.) Table 1 shows the genre X  X opic distribution of both the training set and the two test sets used in these experiments. The first test set (6,465 articles) had the same distribution as the training set. In the second test set (13,710 articles), genre X  X opic pairings were inverted (see Table 1). All sets were balanced with an equal number of texts for each genre X  X opic combination (where not zero). The difference in the size of the test sets reflects the number of articles available with the desired topic X  X enre pairing. The training set and test set 1 were created by a random 2:1 split within each genre class.
Because test set 2 comes from a different distribution than the training set, we report results on these large holdout sets rather than performing cross-validation. We inferred confidence intervals by assuming that the number of misclassifications is approximately normally distributed with mean  X  = e  X  n and standard deviation  X  =  X  where e is the percentage of misclassified instances and n is the size of the test set.
We took two classification results to differ significantly only if their 95% confidence intervals (i.e.,  X   X  1 . 96  X   X  ) did not overlap. 3. Assessing Performance on Static and Altered Genre X  X opic Distributions
To make our first point X  X hat low-level features that correlate with topic can degrade the performance of AGC systems and are best removed unless the genre X  X opic dis-tribution is fixed X  X e show how five published approaches to AGC perform in our experimental framework (Section 3). The choice of methods was partly motivated by the study by Finn and Kushmerick (2006), which compares bag-of-words, PoS frequen-cies, and text statistics as document representations in genre classification tasks across topical domains. The methods we assessed were chosen so that all these features were represented to different degrees. All were implemented on the same platform (Petrenz 2009).

KC: Karlgren and Cutting (1994) use a small set of textual features and discriminant analysis to predict genres. Most of these features involve either PoS frequencies or text statistics. Counts based on the fixed length of texts used in their experiments were adjusted to represent frequencies rather than absolute counts.

KNS / KNSPOS: Kessler, Nunberg, and Sch  X  utze (1997) predict genre based on surface cues. Because the paper gives few details about the specific features they use, we communicated with the authors directly. The list they gave us included features that require PoS tagging. As their published experiments do not make use of such features, we included two versions of their method, one version with PoS-based features and one without. 6
FCT: Freund, Clarke, and Toms (2006) predict genre using a support vector machine on a simple bag-of-words representation of a text. This feature set is not filtered using stop words or other techniques.

FMOG: Feldman et al. (2009) use part-of-speech histograms and principal component analysis to construct features. The authors classify genres using the QDA and Naive
Bayes algorithms. We followed their decision to compute histograms on a sliding win-dow of five PoS tags.

SWM: Sharoff, Wu, and Markert (2010) found a character n -gram based feature set to perform better than PoS n -grams and word n -grams in extensive AGC experi-ments using nine data collections and different choices of n . Although variable length n -grams as features for genre classification had previously been proposed by Kanaris and Stamatatos (2007), we followed Sharoff, Wu, and Markert in using fixed length 4-grams, which they found to yield higher accuracies.
 proaches, here we just used the SVM implementation by Joachims (1999) because other
ML methods produced similar, albeit poorer, results (Petrenz 2009). For PoS tagging, we used the Stanford maximum entropy tagger described in Toutanova et al. (2003). 388 tion (test set 1). These results confirm the findings of SWM that binary character n -grams are good features in AGC. Both their approach and the bag-of-words approach used by
FCT significantly outperform all other methods when the genre X  X opic distribution is the same for training and testing.
 distributions differ from the training set. It shows that some feature sets owe their good results on test set 1 to the strong correlation between topics and genres. The performance of both SWM and FCT is significantly worse, with the latter even worse than the 33.3% that a random guess classifier would achieve in this balanced 3-class classification task. The performance drop for both KC and KNS is slight but still significant. Whereas
KNSPOS had significantly outperformed KNS on test set 1, its performance is signifi-cantly worse than that of KNS on test set 2. (More on this shortly.) flect lexical differences of texts, systems that rely on them (SWM and FCT) will be misled by a major change in genre X  X opic distribution. Similar findings were reported in Finn and Kushmerick (2006) for bag-of-words and in (SWM) for character n -grams, although no explicit tests with topical distributions were carried out in the latter. More surprising are the results involving PoS tags: Two of the methods that used PoS tags X  X MOG and KNSPOS X  X uffered when the genre X  X opic distribution changed, even though PoS frequencies had previously been reported to perform well as a feature set when used in new topical domains (Finn and Kushmerick 2006). However, the PoS frequencies in the
KC feature set did not seem to harm stability much. This brings us to the second point of this squib. 4. Impact of PoS Features on Performance and Stability We justify our second point X  X hat PoS features should not be lumped together in
AGC because they have different correlations with genre and topic X  X hrough a set of experiments that assess the effect of adding PoS features to a set of basic non-PoS features similar to those used earlier by Karlgren and Cutting (1994), here normalized by document length (in words). These basic non-PoS features include character count per document, sentence count per document, average character count per sentence, average word count per sentence, average character count per word, type/token ratio, frequency of long words (ones with more than 6 letters), and the frequencies of the words therefore , I , me , it , that ,and which .

Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993). All 36 non-punctuation tags were used, and counts of PoS-tags were normalized by document length.
 feature (36 sets). Comparing performance with that on the non-PoS features alone (as a baseline) demonstrated the effect of adding each PoS frequency feature on classifier accuracy and stability. 7 All 36 feature sets as well as the baseline set were trained on the same training set. They were then tested on both test sets described in Section 2. As before, we use the same Support Vector Machines (SVM) classifier in all experiments, with the set of features as the experimental variable.
 the accuracy of a system with the baseline features plus all 36 PoS features. Recall from
Table 1 that the genre X  X opic distribution for test set 1 is the same as in the training set, whereas in test set 2 it is different. The first thing to note in Table 2 is that the classifier performs significantly better on the larger set of 49 features than on the smaller set of 13 basic features when the genre X  X opic distribution is not altered (column 2). When it is altered (column 3), the losses are less severe on our baseline than on the set that includes PoS features. This can be explained by looking at the contributions of each feature. to the basic set of non-PoS features. To highlight the most interesting results, we only show features which cause a deviation of more than 1% from the baseline for at least one of the two test sets. 8 When the topic X  X enre distribution remains the same (i.e., test set 1), PoS frequencies appear to have a positive impact on prediction accuracy: When added to the basic feature set, accuracy increases. This is especially true for the tags VBD (past tense verb), JJ (adjective), RB (adverb), NN (singular noun), VB (base form verb), and NNP (plural proper noun).

Figure 2 shows that the PoS tags NN (singular noun), NNS (plural noun), and NNPS (plural proper noun) all have a toxic effect when added to the baseline set. This means the NN, NNS, and NNPS frequency features improve accuracy in stable conditions, whereas they severely harm it as topics change. This is not good for stability . A similar, 390 if somewhat weaker effect, can be observed for the CD (cardinal number) frequency.
Other PoS tags like CC (coordinating conjunction), RB (adverb), and VB (base form verb) increase predictive power while not impairing stability. Even more interesting are the results for the tags VBD (past tense verb) and VBZ (third-person singular present verb). Adding these features eliminates the significant difference between accuracies on test set 1 and 2, which we observed on the baseline feature set.
 of different parts of speech. In our data sets, for example, the frequency of plural noun (NNS) varies more by topic (on average, 8.0% of words are NNS in texts on Education and Schools ,7.6%intextson Medicine and Health , and 6.3% in texts on Defense and Military
Forces ) than it does by genre (on average, 7.3% of words are NNS in news reports, 7.1% in letters, and 7.5% in editorials). The opposite holds for past tense verbs (VBD): Their average frequency varies less by topic (2.9%, 2.9%, and 3.3%) than by genre (4.8%, 1.8%, and 2.5%).
 classifier stability: Unlike NN, NNS, and NNPS frequencies, they are topic-independent.
This is because the most commonly tagged singular proper nouns in news reports are titles such as  X  X r. X ,  X  X s. X ,  X  X r. X , etc., regardless of the topic. The frequency of titles among proper nouns is much lower in editorials and even lower in letters , across all three topical domains. NNP frequency in news reports is increased by the fact that titles are usually followed by one or more names, which are also singular proper nouns. We assume that this is the reason that the fluctuation between NNP frequencies is relatively low across topics (for the three topics and genres we examined) and hence a stable contributor to genre prediction.
 that NN frequencies are bad for stability) hold for settings with different genres and topics. Rather, our point is that PoS tags should not be included or excluded wholesale for AGC. If one is going to the expense of PoS-tagging texts, only a subset of PoS tags should be used as features in AGC in order to maintain performance across changes in the topical distribution. 5. Conclusion
Our results suggest that prediction accuracy on a static topic distribution should not be the sole basis for assessing the quality of Automatic Genre Classification sys-tems: In particular, approaches that perform well on a static topic distribution can be severely impacted by changes in topical distributions. The notion of topic indepen-dence for features has rarely been explored in the literature on genre classification. Nevertheless, this is an important issue, especially in dynamic environments like the
World Wide Web, where new topics emerge rapidly and unpredictably. In topical domains with little or no labeled data to train on, instability can impede any useful application of classifiers. Because of this, we believe that stability should join accu-racy as a criterion for assessing any new developments in genre classification. To this end, we introduced a cross-product methodology in Section 2 as a way of assessing stability.
 use of PoS-based features can yield high performance that is stable even when topical distribution differs from training to test sets. Although we have not identified a set of
PoS-based features that supports classification among an arbitrary set of genres, we can say that it is crucial to evaluate and select PoS-based features carefully in order to achieve genre classification which is as topic-independent as possible.
 Acknowledgments References 392
