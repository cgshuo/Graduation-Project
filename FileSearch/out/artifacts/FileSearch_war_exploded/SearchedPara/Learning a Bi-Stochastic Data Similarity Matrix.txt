 Ping Li
Clustering [13], [6], which aims to organize data in an unsupervised fashion, is one of the fundamental problems in data mining and machine learning. The basic goal is to group the data points into clusters such that the data in the different clusters are  X  X ifferent X  from each other.
In this paper, we view clustering from the perspective of matrix approximation. Suppose we are given a data set X = { x i } n i =1 , which comes from k clusters. We can denote the cluster memberships by an n  X  k matrix F , such that where  X  j denotes the j -th cluster. It is often more convenient to proceed with the scaled version e F [17][24], such that where n j = |  X  j | is the cardinality of cluster  X  j e F has (at least) the following properties (constraints):  X  F &gt; 0 ( i.e., e F ij &gt; 0  X  i, j ) , e F &gt; e F = I , identity matrix.

If we define G = e F e F &gt; , we can hope to discover the cluster structure of X from G . The constraints on e F can be transferred to the constraints on G as In other words, G is a symmetric, nonnegative, and bi-stochastic (also called doubly stochastic ) matrix [12]. A. Deriving a Bi-Stochastic Matrix from a Similarity Matrix
The bi-stochastic matrix G , constructed from the cluster-membership matrix ( F or  X  F ) can be viewed as a special type of similarity matrix . Naturally, one might conjecture: If we a (useful) bi-stochastic matrix from a similarity matrix?
For example, a popular family of data similarity matrix is the Gaussian kernel matrix, K  X  R n  X  n , where each entry Here,  X  is a tuning parameter. Obviously, an arbitrary sim-ilarity matrix can not be guaranteed to be bi-stochastic. For a given similarity matrix, there are multiple ways to derive a bi-stochastic matrix. We first review a straightforward solution known as the Sinkhorn-Knopp (SK) algorithm. B. The Sinkhorn-Knopp (SK) Algorithm
The following Sinkhorn-Knopp Theorem [18] says that, under mild regularity conditions, one can construct a bi-stochastic matrix from a similarity matrix.
 nonnegative square matrix. A necessary and sufficient condition that there exists a bi-stochastic matrix P of the form: P = UAV , where U and V are diagonal matrices with positive main diagonals, is that A has total support. If P exists, then it is unique. U and V are also unique up to a scalar multiple if and only if A is fully indecomposable. Based on this theorem, [18] proposed an method called the Sinkhorn-Knopp (SK) algorithm to obtain a bi-stochastic ma-trix from a nonnegative matrix A , by generating a sequence of matrices whose columns and rows are normalized alter-if A is symmetric, then the resulting matrix P = UAV is also symmetric with U and V being equal (up to a constant multiplier). The following example illustrates the procedure:
The SK algorithm is not the unique construction. In statistics, this procedure is also known as the iterative proportional scaling algorithm [5], [20].
 C. Connection to the Normalized Cut (Ncut) Algorithm
Interestingly, the well-known Normalized Cut (Ncut) algo-rithm [17] can be viewed as a one-step construction towards producing bi-stochastic matrices. The Ncut algorithm nor-malizes a similarity matrix K  X  R n  X  n with D = diag ( K1 ) , [23] showed that if one keeps normalizing K with then K (  X  ) will be bi-stochastic.
 D. Our Proposed General Framework: BBS In this paper, we propose to obtain a bi-stochastic matrix by solving the following optimization problem where is the Bregman divergence between x and y with  X  being a strictly convex function. The problem (7) is a standard convex optimization program. We name the solution G the Bregmanian Bi-Stochastication ( BBS ) of K . 1
Two choices of the Bregman divergence D  X  are popular: 1)  X  ( x ) = x 2 / 2 : (squared) Euclidian distance, 2)  X  ( x ) = x log x  X  x : Kullback-Leibler (KL) divergence. It can be shown that the SK algorithm is equivalent to BBS using KL divergence. We will demonstrate that BBS with  X  ( x ) = x 2 / 2 often produces superior clustering results over the SK algorithm (and other algorithms such as Ncut).
The BBS algorithm seeks a bi-stochastic matrix G which optimally approximates K in the Bregman divergence sense, by solving the optimization problem (7). For the two popular choices of the Bregman divergence D  X  in Eq. (8), we study specially designed optimization strategies, for better insights. A.  X  ( x ) = x 2 / 2 For this choice of  X  ( x ) , we have Thus, the BBS problem with  X  ( x ) = x 2 / 2 is equivalent to Problem (10) is a Quadratic Programming program [2], [16] and can be solved by standard methods such as the interior point algorithm. Here, we adopt a simple cyclic constraint projection approach, known as the Dykstra algorithm [10]. First we split the constraints into two sets C 1 and C 2 : where C 1 defines an affine set and C 2 defines a convex set.
For the constraint set C 1 , we need to solve 2 for which we first introduce a Lagrangian function constraint G = G &gt; , we know  X  1 =  X  2 =  X  . Thus Setting  X  G L ( G ) = 0 yields Since G must satisfy the constraint G1 = 1 , we can right-multiply 1 on the both sides of Eq. (16) as Then we obtain By making use of the Woodbury formula [11], we obtain We can then write the solution in a closed form:
G = K +
For the constraint set C 2 , we need to solve another optimization problem: whose solution is simply where K + denotes the positive part of K .

The overall algorithm of BBS with  X  ( x ) = x 2 / 2 is summarized in Alg. 1. The total computational complexity of Alg. 1 is O ( Tn 2 ) with T being the number of iterations needed for the algorithm to converge.
 Algorithm 1 BBS WITH  X  ( x ) = x 2 / 2 Require: An initial similarity matrix K 1: t = 0 , G ( t ) = K . 2: repeat 3: t  X  t + 1 5: until Some convergence condition is satisfied B.  X  ( x ) = x log x  X  x The BBS problem becomes We construct the following Lagrangian L ( G ) = KL ( G k K )  X   X  &gt; 1 ( G &gt; 1  X  1 )  X   X  &gt; where we drop the constraint G &gt; 0 for the time being, and we will later show it is automatically satisfied. Therefore where log represents the elementwise logarithm. Setting  X 
G L ( G ) = 0 yields Thus the solution satisfies Next, we define the following two vectors and two diagonal matrices diag (  X  1 )  X  R n  X  n , diag (  X  As G is symmetric, we know  X  1 =  X  2 =  X  and  X  1 =  X  2 =  X  . By comparing with the Sinkhorn-Knopp Theorem, we can immediately see that the BBS algorithm with  X  ( x ) = x log x  X  x actually recovers the symmetric SK algorithm, and diag (  X  ) is used for scaling K to be bi-stochastic.
We should mention that, it appears that the fact that the symmetric SK algorithm minimizes the KL divergence was essentially discovered in statistics [4], [19].
 A. Data Sets
Table I summarizes the data sets used in our experiments: B. Experiment Procedure
For all data sets, we always normalized each data point (vector) to have a unit l 2 norm, and we always used the Gaussian kernel Eq. (4) to form the initial similarity matrix K . For the tuning parameter  X  in Eq. (4), we experimented
We ran the BBS algorithm with  X  ( x ) = x 2 / 2 for 1000 iterations at each  X  . We also ran the SK algorithm (i.e., BBS with  X  ( x ) = x log x  X  x ) for 1000 iterations at each  X  .
We eventually used spectral clustering [17], [3], [8], [15] to evaluate the quality of the produced bi-stochastic matrices. In particular, we used the procedure described in [15]. That is, we computed the top-k eigenvectors of the bi-stochastic matrix to form a new n  X  k matrix and normalized each row to have a unit l 2 norm. Denote the resulting new  X  X ata matrix X  by Z . We then used Matlab kmeans function: We ran kmeans 100 times and reported both the average and maximum clustering results.

However, we would like to first introduce two measures stochastic matrices independent of the clustering algorithms. C. Quality Measurements of the Bi-Stochastic Matrices After we have generated a (hopefully) bi-stochastic matrix Basically, M B measures how far P is from being a bi-stochastic matrix, and M C roughly measures the potential of producing good clustering results. Lower values of M B and M
C are more desirable. We use the M C measure because it is independent of the specific clustering algorithms.
Fig. 1 presents the quality measurements on the MNIST measurements on a variety of data sets for  X  = 1 :  X  In terms of M B , the SK algorithm performs well in  X  In terms of M C , the BBS algorithm using the Euclidian D. Comparing Clustering Results
We ultimately rely on the standard clustering procedure, e.g., [15], to assess clustering quality. Tables II to V provide the results for BBS (Alg. 1), SK , and three other methods:  X  K-means : We directly used the original data sets (after  X  RA : We ran spectral clustering directly on the similarity  X  Ncut : We ran spectral clustering on the normalized
We report the clustering results on two metrics: 1) Clustering Accuracy : 2) Normalized Mutual Information ( NMI ) [21]:
We still need to address two more issues:  X  For each case, we always ran kmeans 100 times. We  X  For RA, Ncut, SK and BBS , we experimented with Tables II to V demonstrate that, for many data sets, BBS (Alg. 1) can achieve considerably better clustering results than other methods, especially when evaluated using maximum accuracy and maximum NMI .

It is beneficial to gain some intuitive understanding on why the BBS algorithm with  X  ( x ) = x 2 / 2 (i.e., Alg. 1) can perform well in clustering. We show that it is closely related to various relaxed k -means algorithms.
 The k -means clustering aims to minimize the objective where  X  c is the mean of cluster  X  c . Some algebra can show that the minimizing J 1 is equivalent to minimizing J 2 : where e F is the scaled partition matrix introduced at the is the data matrix. Let G = e F e F &gt; and K = XX &gt; Then J 2 =  X  tr ( KG ) , which in fact can be viewed as a special case of the objective of BBS defined in Eq. (10): tr  X  treated as a constant in this case: tr
In addition, K = XX T is the linear kernel, which may be replaced by more flexible kernels, e.g., Eq. (4) as we use.
There are more than one way to formulate the relaxed k -means algorithm. For example, which is quite similar to our formulation of the BBS problem with the Euclidian distance. Our formulation discards the constraints (39) and hence its optimization task is easier.
Our detailed experiments reported in the Appendix illus-trate that the clustering performance of the BBS algorithm initial similarity matrix K . This section extends BBS to combine the power of multiple input similarity matrices, e.g., boost the performance. We name this scheme Multiple BBS or MBBS . This is in spirit related to cluster ensemble [21] and Generalized Cluster Aggregation [22].

Suppose we have m similarity matrices { K ( i ) } m i =1 would like to obtain a bi-stochastic similarity matrix G by solving the following optimization problem: We constrain the weight coefficients  X  = {  X  i } m i =1 to be in a simplex .  X (  X  ) is some regularizer to avoid trivial solutions.
There are two groups of variables  X  and G . Although the problem (40) is not jointly convex, it is convex with respect to one group of variables with the other group being fixed. Thus, it is reasonable to apply block coordinate descent [1]. A. Fix  X  , Solve G problem (40) becomes Note that  X (  X  ) is irrelevant at this point. This is similar to problem (7) except for the summation form in the objective. The solution procedures are consequently also similar. Here we assume  X  ( x ) = x 2 / 2 for the illustration purpose. = 1 2 where we use the fact P which is the same as Problem (10) if we make B. Fix G , Solve  X 
When G is fixed with G = G ( t ) and for simplicity we only consider  X (  X  ) = k  X  k 2 =  X  &gt;  X  , the problem becomes which is a standard Quadratic Programming (QP) problem.
Here we will reformulate this problem to facilitate more efficient solutions. For the notational convenience, we denote We first rewrite the objective of Problem (44) as  X  which is an Euclidian projection problem under the simplex constraint and can be solved efficiently, e.g., [9][14]. We will report extensive experiment results of Multiple BBS in a more comprehensive technical report.

We present BBS ( Bregmanian Bi-Stochastication ), a gen-eral framework for learning a bi-stochastic data similarity matrix from an initial similarity matrix, by minimizing the Bregmanian divergences such as the Euclidian distance or the KL divergence. The resultant bi-stochastic matrix can be used as input to clustering algorithms. The BBS framework is closely related to the relaxed k -means algorithms. Our extensive experiments on a wide range of public data sets demonstrate that the BBS algorithm using the Euclidian distance can often produce noticeably superior clustering results than other well-known algorithms including the SK algorithm and the Ncut algorithm.
 This work is partially supported by NSF (DMS-0808864), ONR (YIP-N000140910911), and a grant from Microsoft. We generated the base similarity matrix K using the Gaussian kernel (4) which has a tuning parameter  X  &gt; 0 . The clustering performance can be, to an extent, sensitive to  X  ; and hence we would like to present the clustering results for  X  values ranging from 2  X  2 = 0 . 25 to 2 10 = 1024 , for four algorithms: RA, Ncut, SK , and BBS (using Euclidian distance), and two performance measures: Accuracy and NMI , as defined in Eq. (34) and Eq (35), respectively.
In the tables, each entry contains the average and maxi-mum (in parentheses) clustering results from 100 runs of the Matlab kmeans program. Due to the space limit, we could not present the experiments for all the data sets.

