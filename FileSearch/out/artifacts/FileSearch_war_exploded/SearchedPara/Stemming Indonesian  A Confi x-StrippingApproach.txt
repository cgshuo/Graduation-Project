 MIRNA ADRIANI, University of Indonesia JELITA ASIAN, RMIT University BOBBY NAZIEF, University of Indonesia S.M.M. TAHAGHOGHI, RMIT University and HUGH E. WILLIAMS, Microsoft 1. INTRODUCTION Stemming is a basic text processing tool often used for effici ent and effective text retrieval [Frakes 1992], machine translation [Bakar a nd Rahman 2003], document summarization [Or  X  asan et al. 2004], and text classification [Gaustad and Bouma 2002]. Stemmers remove affixes to cluster words der ived from a common stem or root ; for example, the words  X  X pens, X   X  X pened, X  and  X  X pener X  are clustered with the stem  X  X pen. X  Identifying words from a common root increases the sensitivity of retrieval by improving the abi lity to find relevant documents, but is often associated with a decrease in select ivity, where the clustering causes useful meaning to be lost. For example, ma pping the word  X  X tranger X  to the same cluster as  X  X trange X  is likely to be de sirable if the former is used as an adjective, but not if it is used as a noun. Stemmin g is expected to increase recall but possibly decrease precision.
 Slovene [Popovi  X  c and Willett 1992], and Arabic [Larkey et al. 2002], stemmin g leads to increased precision. Popovi  X  c and Willett [1992] claim that morpho-logically complex languages X  X uch as Slovene X  X re more like ly to benefit from stemming. For the same reason, we suspect that Indonesian mi ght benefit as well.
 [Paice 1996], with techniques such as those of Lovins [1968] and Porter [1980] in widespread use. However, despite the growing volume of wr itten content in other languages [Ballesteros and Croft 1997; Hollink et al. 2004], stemming for other languages is less well-known; while there are several approaches avail-able for languages such as French [Savoy 1993], Spanish [Xu a nd Croft 1998], Malaysian [Ahmad et al. 1996; Idris 2001], and Indonesian [A rifin and Setiono 2002; Nazief and Adriani 1996; Vega 2001], there is almost no consensus about their effectiveness.
 has attracted in recent years, there has been relatively lit tle interest in the Indonesian language X  X lso known as Bahasa Indonesia 1  X  X rom the linguistic processing and information retrieval communities. Stemmi ng is essential to support effective Indonesian information retrieval, and h as uses as diverse as defense intelligence applications, document translation , and Web search. We explore stemming for this language.
 language. It belongs to the Austronesian language family, w hich includes Tagalog, Javanese, Balinese, Malagasy, and Maori. During t he spread of Is-lam between the 14th and the 15th centuries CE, Javanese and A rabic scripts were used to write Malay. From the second half of the 19th cent ury, due to the influence of European missionaries, 2 Latin script came into widespread use. By the early 20th century, all Malay words were written in Lat in script. original spelling intact. Examples of such loan words inclu de  X  X oderator X  h moderator i from English;  X  X empo X  h tempo i from Italian; and  X  X ekening X  h account i and  X  X ante X  h aunt i from Dutch. Foreign words or phrases that are not assimilated are shown italicized, for example  X  ad hoc , X   X  curriculum vitae , X  and  X  pianissimo . X  Other foreign words may also be transliterated [Dwipayan a 2001]. Dwipayana [2001] recommends that transliterated na mes be Roman-ized according to ISO standards, common English spelling, o r Chinese Pinyin. Different languages have different ISO standards for trans literation: for ex-ample, Japanese uses ISO 3602:1989, Korean uses ISO/TR 1194 1:1996, and Arabic uses ISO 233:1984. 3  X  X eknologi X  h technology i ,  X  X ompas X  h compass i , and  X  X arkotika X  h narcotics i are examples of transliterated words.

Indonesian prefixes and suffixes have been influenced by forei gn languages, especially Indo-European languages [Dwipayana 2001; Widy amartaya 2003; Wilujeng 2002]. These prefixes and suffixes can either be reta ined unchanged or transliterated into Indonesian. Examples of foreign pre fixes adopted in Indonesian include  X  X ono- X  h mono-i ,  X  X kstra- X  h extra-i ,  X  X iper X  h hyper i ,  X  X in- X  h syn-i , and  X  X ltra- X  h ultra-i . Likewise, suffix examples include  X -si X  h -sion and consider these to be native affixes.

Indonesian does not have accented characters, and so accent s are removed ten as  X  X eja vu X  and  X  X aive X  respectively. Indonesian verbs do not change with tense; instead, tense is implied by the context of the se ntence and the presence of words specifying time, such as  X  X emarin X  h yesterday i and  X  X esok X  h tomorrow i [Woods et al. 1995].

Indonesian employs affixes more heavily than English, and th e application and order of stemming rules requires careful consideration . In addition to prefixes and suffixes, it has infixes (insertions) and confixes  X  X lso referred to as circumfixes X  X hat are combinations of prefixes and suffixes . Consider the root word  X  X erintah X  h rule, order i . Examples of words derived from this stem include the words  X  X erintahnya , X   X  X i perintah, X  and  X  X e m erintah. X  Here, af-fixes are shown underlined and the recoding character X  X xpla ined in the next paragraph X  X s shown in italics.

A prefix may change letters of the word it is added to. For examp le, the prefix  X  X eny X  added to the root word  X  X apu X  h broom i produces  X  X enyapu X  h to sweep i ; the letter  X  X  X  of the root word does not appear in the derived form. Hence, an automatic stemming algorithm must be able to resto re (or recode ) such letters during the stemming process.

Some derived words are made from a repeated root with a confix s panning the constituent words. For example, the word  X  se baik-baik nya  X  h as good as possible i is derived from the word  X  X aik X  h good i . Finally, countable words are repeated to indicate the plural. Hence,  X  X uku-buku X  h books i is the plural form of  X  X uku X  h book i .
 Stemming Indonesian is clearly a more complex endeavor than stemming English, but also more important for effective information retrieval. In this article, we explore the major stemming approaches for Indon esian, including two for the related Malaysian language. We introduce the nov el CS stemmer, which uses detailed rules of the Indonesian language. Again st a baseline de-rived from multiple human subjects, we show that it is the bes t-performing stemmer for Indonesian. We also explore whether stemming ca n improve re-trieval performance, and observe that X  X s with English X  X te mming has little effect on accuracy. Finally, we describe extensions to the CS stemmer that use n -grams to find proper nouns that should not be stemmed, and in t his way help to improve retrieval performance. We find that stemming all words except proper nouns by using our CS stemmer and 4-grams produces higher average precision and R-precision than not stemming at all. This is s tatistically signif-icant at the 95% confidence level using the one-tailed Wilcox on signed ranked test. While this combination produces a precision at 10 that is slightly worse than for no stemming, the difference is not statistically si gnificant. The recall value for the unstemmed collection is 0.728. Our CS stemmer without using n -grams and proper noun identification increases the recall v alue to 0.781, while the CS stemmer with the optimal combination of techniques produce s the slightly lower recall value of 0.779.
 how affixes are used in Indonesian. In Section 3, we describe s everal existing stemming algorithms that can be applied to Indonesian text. We introduce our new CS stemmer in Section 4, and continue in Section 5 with a descrip -tion of how we investigate the effectiveness of our approach using stemming and information retrieval experiments. We present extensi ons to our method in Section 6, and conclude in Section 7 with a discussion of ou r findings and consideration of future work in the area. 2. AFFIXES IN INDONESIAN Affixes can be inflectional or derivational [Payne 1997]. In English, inflec-tional affixes add context such as plurality or tense, wherea s derivational af-fixes modify the lexical form of the word. For example, the Eng lish verb  X  X each X  can take the inflectional suffix  X -es X  to form the present sing ular verb  X  X eaches X ; it can also take the derivational suffix  X -er X  to convert the o riginal verb to the noun  X  X eacher. X  derivational affixes may be prefixes, suffixes, or a combinati on of both (con-fixes). 4 We now examine each category in further detail.
 1988]: (1) Particles (P) {  X -kah, X   X -lah, X   X -tah, X   X -pun X  } .
 (2) Possessive pronouns (PP) {  X -ku, X   X -mu, X  and  X -nya X  } .
 Where the two types of inflectional suffixes appear together, the particle is always added after the possessive pronoun. For example,  X  buku X  h book i may be appended with the possessive pronoun PP  X -mu X  to give  X  bukumu X  h your book i , followed by the particle P  X -lah X  to give  X  X ukumulah X  h it is your book that i .

Derivational prefixes {  X  X e-, X   X  X i-, X   X  X e-, X   X  X e-, X   X  X e-, X   X  X e-, X  and  X  X e- X  } . 5 These may have variants; for instance, the prefix  X  X e- X  can appear a s  X  X e-, X   X  X em-, X   X  X en-, X   X  X eng-, X  or  X  X eny- X  according to the first letter of t he root word. Up to three derivational prefixes may be added to a root word. For example, the derivational prefixes  X  X e-, X   X  X eng- X  (a variant of  X  X e- X  ) and  X  X e X  6 may be prepended to  X  X ahu X  h know i , and the suffixes  X -an X  and  X -ku X  appended to this word, to produce  X  X epengetahuanku X  h as far as I know i ( X  X e- X + X  X eng- X +  X  X k]e- X +  X  X ahu X   X -an X  +  X -ku X ). 7
Derivational suffixes {  X -i, X   X -kan, X   X -an X  } . Only one derivational suffix may be applied to a root word. For example, the word  X  X apor X  h to report i can be suffixed by the derivational suffix  X -kan X  to become  X  X aporka n X  h go to report i .
Derivational confixes, for example, {  X  X e-an, X   X  X e-i, X   X  X e-kan, X   X  X i-i, X   X  X i-kan X  } . Some derivational prefix-suffix pairs form derivational st ructures. For example, the prefix  X  X e- X  and the suffix  X -an X  added to the root word  X  X alam X  h inside, deep i form the word  X  X edalaman X  h depth, profundity i .

Not all prefix and suffix combinations form a confix [Moeliono a nd Dard-jowidjojo 1988], but we choose to treat them as such during st emming. There is no official and complete list of Indonesian confixes; from M oeliono and Dardjowidjojo [1988], we conclude that the most common Indo nesian con-fixes are  X  X e-an X  and  X  X e-an. X  These pairs of prefixes and suffi xes can form either confixes or affix combinations depending on the root wo rd to which they are appended. may be applied directly to root words or to words that have a de rivational suffix. For example, the root word  X  X apor X  h to report i may take the derivational suffix  X -kan X  to become  X  X aporkan X  h go to report i , and then take the inflectional suffix  X -lah X  to become  X  X aporkanlah X  h please go to report i .
 valid affix pairs. There is one exception to this list: the pre fix  X  X e- X  and the suffix  X -i X  can appear together on the root word  X  X ahu X  h to know i to form  X  X e-tahui X  h to know i . Consider the word  X  X isarikan X  h to paraphrase i . An auto-mated algorithm may stem this as  X  di -sari-kan  X  to give the root word  X  X ari X  h tomatic stemming, we can perform confix restriction to rule out incorrect affix combinations, in this case  X  X i-. . . -an. X  3. RELATED WORK We now examine existing stemming approaches that can be appl ied to Indone-sian text. With the exception of the V EGA algorithm of Section 3.1, all the algorithms described here use a dictionary to verify word st ems; none speci-fies a particular Indonesian dictionary. To provide a unifor m baseline for all algorithms that use a dictionary, we choose to use the Univer sity of Indone-sia dictionary of 29,337 root words described in Nazief and A driani [1996]; we refer to this dictionary as DICT -UI . 3.1 Vega [Vega 2001] describes an approach X  X hat we refer to as V EGA  X  X hat uses rule sets to determine whether affixes can be removed from a word. T he rules are accessed in the order they are presented in the code. When one rule fails, the algorithm proceeds to the next. Consider processing the wor d  X  X edatangan X  h arrival i using the following set of rules: Rule 1: word(Root)  X  circumfix(Root) Rule 2: word(Root): StemWord Rule 3: circumfix(Root)  X  ber-(Root), (-kan | -an) Rule 4: circumfix(Root)  X  ke-(Root), -an Rule 5: ber-(Root)  X  ber-, stem(Root) Rule 6: ke-(Root)  X  ke-, stem(Root) Processing starts with Rule 1, which requires us to test for a circumfix. We look up the first rule having circumfix on the left-hand side (Rule 3). This tests for the prefix  X  X er- X  by applying Rule 5. Since this prefix does not appear in the word  X  X edatangan, X  Rule 5 fails, and consequently the ca lling rule (Rule 3), fails as well.

The next rule listing circumfix on the left-hand side is Rule 4, which in turn calls Rule 6. This tests whether the word starts with  X  X e-. X  S ince this is true for  X  X edatangan, X  we remove the prefix  X  X e- X  to leave  X  X atang an. X  On returning to Rule 4, we check whether  X  X atangan X  ends with  X -an, X  and si nce it does, we remove the suffix to obtain the stem  X  X atang X  h arrive i .

Had Rule 1 not been satisfied, Rule 2 would have been triggered , indicating that the input word is a stem word. The algorithm allows for ex plicit listing of exceptions; for example, we can prevent stemming  X  X egawati  X  (the name of a former Indonesian president) even though it contains the co nfix  X  X e-. . . -i. X 
This algorithm has iterative versions that stem recursively, and extended variants that X  X niquely among Indonesian stemming algorit hms X  X onsider nonstandard colloquial affixes. In our results, we report re sults with only the first scheme, which we refer to as V EGA -1; the other variants are ineffective, performing between 10% X  25% worse than V EGA -1. 8
A major shortcoming of the V EGA approach is the absence of a lookup stage where words are compared to a dictionary of known root words; stemming con-tinues as long as the word contains affix letters, often leadi ng to overstemming. Moreover, the algorithm does not cater for cases where recod ing is required. Finally, the reliance on strict rules necessitates that the rules be correct and complete, and prevents ad hoc restoration of affix combinati ons. 3.2 Arifi n and Setiono Arifin and Setiono [2002] describe a stemming scheme X  X hich w e refer to as A
RIFIN  X  X hat first removes up to two prefixes, and then removes up to th ree suffixes. After removal of each prefix or suffix, a dictionary l ookup is performed, and stemming stops if the word in its current form appears in t he dictionary.
If the word has not been found in the dictionary by the time the maximum number of prefixes and suffixes have been removed, the algorit hm progres-sively restores different combinations of prefixes and suffi xes in order, and checks against the dictionary at each step. For example, the word  X  X esendiri-anmu X  h your solitude i has the prefix  X  X e- X  and the suffixes  X -an X  and  X -mu. X  The algorithm removes these three affixes, and also the apparent affixes  X  X e- X  and  X -i X  to produce  X  X dir, X  which is not a valid word. The prefixes and suffixes are then progressively replaced. Restoring the prefixes  X  X e- X  a nd  X  X e- X  to  X  X dir X  pro-duces  X  X esendir, X  which is not a valid root word. The algorit hm then restores only the prefix  X  X e- X  to  X  X dir, X  producing  X  X endir. X  This is s till not valid. Simi-larly, the algorithm would first restore the suffix  X -i, X  and t hen the suffix  X -an X  and  X -mu. X  It would then restore the suffix  X -i X  together with the prefixes  X  X e- X  and  X  X e- X  to produce the invalid word  X  X esendiri. X  The algor ithm then tries to add only the prefix  X  X e- X  with the suffix  X -i X  to produce  X  X endi ri X  h self, own i , which is the correct root. Had the dictionary lookup failed, the restoration process would have stepped through  X  X esendirian X  h solitude i to  X  X endirian X  h being alone i (both are valid words but not stems). 9 not found in the dictionary, a lookup is performed using the r ecoded form. If this also fails, the word is returned to the prerecoding form before proceeding to the next removal. Consider the word  X  X enyendirian X  h isolation i . This has the root word  X  X endiri X  h self i . The algorithm removes the prefix letters  X  X e- X  to obtain  X  X yendirian. X  This is not a root word, so the suffix let ters  X -an X  are also removed to give  X  X yendiri. X  Not finding the word  X  X yendiri X  i n the dictionary, the algorithm tries combinations of the removed prefixes and suffixes including  X  X yendir, X   X  X enyendir, X  and  X  X enyendiri X  h loner i . If this is unsuccessful, the algorithm then considers the letter  X  X e- X  to be part of the pr efix  X  X eny-, X  and so removes the letters  X -ny X  to obtain  X  X ndiri. X  10 Adding the recoding letter  X  X - X  results in  X  X endiri X ; this appears in the root word dict ionary, and so the operation ends.
 fix letters even though affixes are never repeated in Indonesi an; this leads to overstemming. For example, in the word  X  X eranan X  h role, part i , the suffix letters  X -an X  appear twice. A RIFIN removes these in succession to obtain the valid word  X  X er X  h spring i instead of the correct root word  X  X eran X  h to play the role of i .
 processes the word  X  X emberikan X  h to give away i by removing first  X  X em- X  to obtain  X  X erikan X  h please give away i , which is not a root word, and then  X  X er- X  to obtain  X  X kan X  h fish i . The word  X  X emberikan X  is actually formed from the root word  X  X eri X  h to give away i and the confix pair  X  X e- X  and  X -kan. X  3.3 Ahmad, Yusoff, and Sembok The approach described by Ahmad et al. [1996] is designed for the Malaysian language, also known as Bahasa Malaya , rather than for Indonesian. How-ever, the two languages are closely related and share many st emming rules. tions in the form of templates. Ahmad et al. [1996] report tha t the original al-gorithm uses a Malaysian dictionary called Kamus Dewan [Dew an Bahasa dan Pustaka 1991] containing 22,293 root words. Since we deal wi th Indonesian, we replace this with the University of Indonesia dictionary , DICT -UI described at the beginning of Section 3. At the start of the stemming pro cess and at each step, a dictionary lookup is performed with the current form of the word, and stemming concludes if the word appears in the dictionary . After each un-successful lookup, the word is compared to the next matching affix template, and, where possible, affixes are removed. If all matching tem plates are ex-hausted without a successful dictionary lookup, the origin al word is returned unstemmed.

Consider the affix template  X  X e-. . . -kan. X  The word  X  X emberi kan X  h to give away i matches this template, and removing the letters correspond ing to the prefix and suffix leaves  X  X eri X  h to give away i , which is the correct stem. A word may match several affix templates, and so this algorithm is sensitive to the order in which the templates appear in the list. For examp le, the word  X  X erasal X  h to come from i can match both the templates  X . . . -er-. . .  X  and  X  X er-. . . . X  Applying the first produces the incorrect stem  X  X asal X  h basalt i , whereas the second template produces the correct stem  X  X sal X  h origin, source i . Ahmad et al. use three different template sets referred to as A , B , and C . They state that template A with its 121 rules is a direct implementation of the work of Othman [1993]. Template B , which consists of 432 rules, extends template A with rules derived from the Qur X  X n, and this is in turn extend ed by template C with an additional 129 rules to cater for modern Malay words adapted from foreign languages, such as the prefix  X  X nfra- X  a s in  X  X nframerah X  h infared i and the suffix  X -tual X  as in  X  X onseptual X  h conceptual i . All three sets have a single list of suffixes and infixes, sorted alphabetica lly, followed by a similarly sorted list of prefixes and confixes. The authors li st the rules added for B and C , but do not specify how each incorporates the rules of the pre vious set. We explore three orderings for each of the B and C template sets: B 1 , B 2 , B 3 , C 1 , C 2 , and C 3 . In the B 1 and C 1 variants, the additional rules are appended to the previous rules as shown in Ahmad et al. [1996] . The B 2 and C 2 variants order the rules alphabetically without consideri ng the affix types. In the B 3 and C 3 variants, the suffix and infix rules are listed alphabeticall y first, and are followed by the prefix and confix rules, also list ed alphabetically. In preliminary experiments using several orderings, we hav e observed that they exhibit very similar performance. In this article, we d escribe results for the ordering ( B 2 ) that we have found to perform the best in earlier experiment s [Asian et al. 2005]. We suspect that the better performance o f this variant X  which we refer to as A HMAD -B 2  X  X s due to its catering for general affixes before considering more specific affixes such as those from the Qur X  X  n and those found only in modern Malaysian usage. 3.4 Idris Idris [2001] extends the scheme of Ahmad et al. [1996] to perf orm progressive stemming and recoding. The algorithm alternates between re moving prefixes and suffixes until the root word is found in a dictionary or a re moval limit is reached. Since Idris does not specify recommended limits , we adopt the assumption of Arifin and Setiono [2002] that Indonesian word s can have at most two prefixes and three suffixes.

A feature of this algorithm is that it uses two dictionaries: one general, and another specific to the document content, for example contai ning medical or legal terms. For Web retrieval applications, it is unlikely that the document content will be known beforehand, and so we use only the gener al dictionary in the experiments we report.
 and the other performs the reverse. The first assumes that a wo rd may have different prefixes. For example, the word  X  X emasukkan X  h to enter something in i with the root  X  X asuk X  h to be present i could be  X  mem -asuk-kan  X  or  X  me -masuk-kan . X  Removing the prefix  X  X em- X  results in  X  X suk, X  which is inva lid; the algorithm then adds the letter  X  X  X  back to obtain the vali d stem  X  X asuk. X  ing the prefix  X  X em-, X  we obtain  X  X suk X  which is not in the dict ionary. From recoding rules, we know that for the prefix  X  X em-, X  the letter  X  X  X  could have been dropped, so we prepend this letter to  X  X suk X  to obtain th e valid root word  X  X asuk X  h troop i .
  X  X asuk X  X  X or  X  X emasukkan. X  We have found that the latter var iant X  X hat we call I DRIS -2 X  X erforms slightly better, and we report only experiment s using this variant.
  X  X edannya X  h his or her field, plain or square i , with the root  X  X edan X  h field, plain or square i . Since I DRIS tries to first remove prefixes, it will remove the prefix letters  X  X e-, X  to obtain the invalid candidate roo t word  X  X annya. X  Since this does not appear in the dictionary, the suffix  X -nya  X  is then removed to produce  X  X an X  h and i . This is a valid root word, but not the correct one. Being designed for Malaysian, this algorithm uses a set of pr efixes and suffixes that are slightly different from those used in Indonesian, a nd this can also contribute to overstemming. 4. THE CS STEMMER We now present our confix-stripping approach to stemming Ind onesian. 11 This scheme, which we refer to as CS , is based on a thorough understanding of the underlying rules of the language.
 shown below, with the square brackets indicating that an affi x is optional. We apply this order and knowledge of basic rules of the Indone sian language as the foundation of our stemming approach: (1) Words of three or fewer characters cannot contain affixes , so no stem-(2) Affixes are never repeated, so a stemmer should remove onl y one of a set (3) We can use confix restriction during stemming to rule out i nvalid affix com-(4) When restoring characters after prefix removal, we perfo rm recoding if nec-4.1 Detailed Approach We now describe our stemming technique in detail. (1) At the start of processing, and at each step, check the cur rent word against (2) Remove inflectional suffixes. As described earlier, infle ctional suffixes do (3) Remove any derivational suffixes {  X -i, X   X -kan, X  and  X -an X  } . In our affix (4) Remove any derivational prefixes {  X  X e-, X   X  X i-, X   X  X e-, X   X  X e-, X   X  X e-, X   X  X e-, X  and (5) If, after recursive prefix removal, the word has still not been found, we (6) If all steps are unsuccessful, the algorithm returns the original unstemmed removed indirectly during prefix and suffix removal. There ar e some exception cases; for example, the confix  X  X e-an X  in the word  X  X engusuta n X  could mean either  X  X ntanglement X  if derived from  X  X usut X  h tangled i , or  X  X xamination, in-vestigation X  if derived from  X  X sut X  h examine i . Without context, neither an au-tomatic stemmer nor humans can tell which is the correct stem .
 4.2 Prefi x Disambiguation When we encounter a complex prefix, we determine the prefix lim its according to the rules shown in Table II. Consider the word  X  X enangkap X  h to catch i . Looking at the rules for the prefix letters  X  X e-, X  we see that t he third letter of our word is  X  X  X  instead of  X  X , X  and so we exclude Rule 10, Rule 1 1, Rule 12, and Rule 13. We also exclude Rule 14 since the fourth letter  X  X   X  is not  X  X , X   X  X , X   X  X , X  or  X  X . X  We finally settle on Rule 15, which indicates that the prefix to be removed is  X  X e-. X  The resultant stem is either  X  X angkap, X  wh ich is not in the dictionary, or  X  X angkap X  h to catch i , which is in the dictionary. fix  X  X e-, X  the word  X  X engaku X  h to admit, to stiffen i can be mapped to either  X  X eng-aku X  with the root  X  X ku X  h I i or to  X  X eng-kaku X  with the root  X  X aku X  h stiff i . Both are valid root words, and we can only determine the corr ect root word from the context. 13 word. For example, the word  X  X ereka X  can be a stem, with the me aning  X  X hey, X  or an affixed word, which could be stemmed to  X  X eka X  h to invent, to devise i . This is a common stemming problem not unique to Indonesian [X u and Croft 1998]. To resolve these ambiguities, we must also consider t he context sur-rounding the word. 4.3 Rule Precedence The order in which rules are applied affects the outcome of th e stemming oper-ation. Consider an example where inflectional suffix removal fails. The word  X  X ertingkah X  h to behave i is formed from the prefix  X  X e- X  and the root word  X  X ingkah X  h behavior i . However, the algorithm will remove the suffix  X -kah X  to obtain the word  X  X erting, X  and then remove the prefix  X  X e- X  to obtain the valid word  X  X ing X  h lamp i . This particular problem arises only in limited cases with specific prefixes and particles.
 paired with certain derivational suffixes; for the word  X  X in ilai X  h to be marked i , we may obtain the construct  X  di -nilai X  with the correct stem  X  X ilai X  h mark i , or the construct  X  di -nila-i  X  with the incorrect (but valid) stem  X  X ila X  h indigo i . example, a word  X  X eli X  h to buy i can be mistaken as the word  X  X el X  h bell i with the suffix  X -i. X  the suffix when encountering the confix pairs  X  X e-. . . -lah X ;  X  be-. . . -an X ;  X  X e-fix should be removed before the prefix. For example, the word  X  mengalami X  h to experience i is derived from  X  meng -alam-i , X  and the correct stem is  X  X lam X  h experience i . Under our rule precedence, this is treated as  X  meng -alami, X  pro-ducing the valid but incorrect stem  X  X lami X  h natural i .
 the locative preposition  X  X i X  h in, at, on i appears mistakenly attached to a fol-lowing word ending with an inflection or derivation suffix suc h as  X -i. X  For example, the phrase  X  X i sisi X  h at the side i  X  X ith the correct stem  X  X isi X  h side i  X  is sometimes misspelled as  X  X isisi. X  If we were to first remov e the derivation suffix  X -i X  and then the derivation prefix  X  X i-, X  we would obta in the stem  X  X is X  h hissing sound i . Using the  X  X i-. . . -i X  precedence rule, we first remove the pr efix  X  X i-. X  Stemming stops here, since  X  X isi X  appears in the dict ionary. 4.4 Hyphenated Words In Indonesian, hyphenated words are of two types: word. For example,  X  X uku X  h book i becomes  X  X uku-buku X  h books i .
 rived from a common root X  X eparated by a hyphen.
 side of the hyphen are formed from the common root,  X  X aik X  h good i . In contrast, the components of  X  X olak-balik X  h to and fro i stem to  X  X olak X  (a valid word with no independent meaning) and  X  X alik X  h come back i .

When the words on either side of the hyphen have the same root, we return this root as the stem. Thus, for example,  X  X uku-buku X  is stem med to  X  X uku, X  and  X  X ebaik-baiknya X  is stemmed to  X  X aik. X  Where the compon ent stems are different, as with  X  X olak-balik, X  we assume that the origin al hyphenated word is the stem.

This approach does not always succeed. For example, the word  X  X enar-tidaknya X  h the right or wrong of i should be stemmed to  X  X enar-tidak X  h right or wrong i . However, our algorithm stems and compares the words on eith er side of the hyphen to get  X  X enar X  h right i and  X  X idak X  h wrong i . Since they are not the same, the stem is assumed to be the original word  X  X ena r-tidaknya. X  It is difficult to avoid this type of problem without incorporat ing explicit lookup lists. 5. EXPERIMENTAL EVALUATION To determine the effectiveness of our approach, we conduct t wo experiments. First, we examine how well it X  X nd other state-of-the-art st emmers X  X erform on basic stemming against a human baseline. Second, we explo re how stem-ming affects information retrieval from Indonesian text.

For these, we require data collections and appropriate grou nd truth. The absence of suitable existing testbeds led us to create our ow n; we describe these in the following sections. 5.1 Comparison with Humans The first experiment we report assesses the effectiveness of automated stem-mers against a baseline created from user experiments. 5.1.1 Collection. We formed a collection of words to be stemmed by extract-ing every fifth word from a set of 9,898 news stories 14 from the online edition of the Kompas 15 newspaper published between January and June 2002. We de-fine a word as a sequence of characters enclosed by white space , with a letter as the first character.

The mean word length (including short words) in this list is 6 .15, while the mean word length in DICT -UI is 6.75. We have found that words shorter than six characters are generally root words and so rarely re quire stemming. For our list containing words with five or fewer characters, o nly about 0.04% of such words (39 unique words) from 1,419,383 nonunique wor ds were not root words, and so we decided to omit words with fewer than six characters from our collection. Note that, by design, our algorithm doe s not stem words shorter than three characters; this is an orthogonal issue t o the collection creation process. words, reflecting X  X e believe X  X  good approximation of their frequency of use. We chose to extract nonunique words to reflect the real-world stemming prob-lem encountered in text search, document summarization, an d translation. The frequency of word occurrence in normal usage is highly sk ewed [Williams and Zobel 2005]; there are a small number of words that are ver y common, and a large number of words that are used infrequently. In Englis h, for example, the word  X  X he X  appears about twice as often as the next most co mmon word; a similar phenomenon exists in Indonesian, where  X  X ang X  (a r elative pronoun that is similar to  X  X ho, X   X  X hich, X  or  X  X hat, X  or  X  X he X  if used with an adjective) is the most common word. It is important that an automatic stemm er processes common words correctly, even if it fails on some rarer terms.
 stemming algorithms relative to manual stemming for the non unique word collection. This permits quantifying the overall error rat e of a stemmer for a collection of real-world documents, that is, it allows us to discover the total er-rors made. Second, we investigate the error rate when stemmi ng unique words only. This allows us to investigate how many different error s each scheme makes, that is, the total number of unique errors. Together, these allow effec-tive assessment of stemming accuracy. word should be stemmed, nor are they always consistent. When producing our ground truth, we deliberately cater for these characterist ics. We asked four na-tive Indonesian speakers to provide the appropriate root fo r each of the 3,986 words in the list. 16 The words were listed in their order of occurrence, that is, a word could be repeated at different points in the collec tion, and words were not grouped by prefix. Table III shows the level of agreem ent between the assessors: as expected, there is no consensus as to the ro ot words between the assessors, and indeed, the agreement ranges from around 93% (for asses-sors A and C) to less than 89% (for assessors C and D). For examp le, the word  X  X agian X  h part i is left unstemmed in some cases and stemmed to  X  X agi X  h divide i in others, and similarly  X  X dalah X  h to be i is sometimes stemmed to  X  X da X  h exists i and sometimes left unchanged. Indeed, the latter example il lustrates another problem: in some cases, an assessor was inconsistent, on som e occasions stem-ming  X  X dalah X  to  X  X da, X  and on others leaving it unchanged. decided to use the majority decision as the correct answer. T able IV shows the number of cases where three and four assessors agree. All fou r assessors are in agreement on only 82.6% of all words, and the level of agree ment between any set of three assessors is only slightly higher. The numbe r of cases where any three or all four assessors agree (shown as  X  X ny three X ) i s 95.3%. We use this latter case as our first baseline to compare to automa tic stemming: if a majority agree, we keep the original word in our collecti on and note its answer as the majority decision. We refer to this as the MAJORITY baseline; it contains 3,799 words. Words that do not have a majority ste mming decision are omitted from the collection.
 majority may make a mistake. For example, the word  X  X erakan X  h movement i can be correctly stemmed to either the root word  X  X era X  h to frighten i or  X  X erak X  h to move i . For this particular word, all four assessors stemmed  X  X era kan X  to  X  X erak. X  gan X  h cutting down i should be correctly stemmed to  X  X ebang X  h to cut down i . However, the majority misread this as  X  X enerbangan X  h flight i , and so stemmed it to  X  X erbang X  h to fly i .
 jority decision for individual words may in fact vary across the occurrences of that word. For example, the word  X  X dalah X  was stemmed by thre e assessors to  X  X da X  in some cases, and left unstemmed in others. From our collection of 3,799 words, the 1,751 unique words map to 1,753 roots acco rding to the majority decision.
 complement this with two further baselines. One is the set of 1,753 unique roots reported by the users. We refer to this set as UNIQUE and use it to as-sess algorithm performance on unique words. We also use a thi rd baseline formed from the answers provided by at least one assessor; th is set contains the original 3,986 nonunique words, and we refer to this set a s SUBJECTIVE . ming for the MAJORITY , UNIQUE , and SUBJECTIVE collections. Our scheme produces the best results, correctly stemming 94.8% of word occurrences in is some 6% better than the best-performing other scheme (A HMAD -B 2 ). Us-ing McNemar X  X  one-tailed test [Sheskin 1997], we have found this difference to be statistically significant at the 99% confidence level. W e observe that the only nondictionary scheme, V EGA -1, is less effective than even the I DRIS -2 and A HMAD -B 2 schemes designed for stemming Malaysian. It makes almost five times as many errors on the MAJORITY collection as CS , illustrating the importance of validating decisions using an external word s ource.
 A
HMAD -B 2 scheme on which it is based. However, on the MAJORITY collection, it is 0.9% or 34 words worse. This illustrates an important ch aracteristic of our experiments: stemming algorithms should be considered in the context of word occurrences and not unique words. While I DRIS -2 makes fewer er-rors on rare words, it makes more errors on more common words, and is less effective overall.
 incorporating it in our stemmer causes errors. For example, the word  X  X e-merintah X  h government i  X  X erived from the root  X  X erintah X  h rule, order i  X  X s incorrectly stemmed to  X  X erin X  (a valid word with no indepen dent meaning). Similarly, the words  X  X ibantah X  h to be denied i and  X  X embantah X  h to deny i  X  derived from the root  X  X antah X  h to argue, deny i  X  X re incorrectly stemmed to  X  X an X  h wheel i . Not catering for this prefix actually improves effectivene ss from 94.7% (with 201 errors) to 94.8% (with 196 errors) for MAJORITY , and from 95.2% (with 85 errors) to 95.3% (with 82 errors) for UNIQUE . This particle is not implemented by other Indonesian stemming algorithms but is handled by templates B and C of the Malaysian stemming algorithm of Ahmad et al. [1996]. All experiments we report here exclude this prefix.
 tions, with the CS stemmer performing the best. As expected, performance on SUBJECTIVE is slightly better than for MAJORITY or UNIQUE , since an au-tomated approach need only agree with a single assessor. A RIFIN produces slightly better results than I DRIS -2 for SUBJECTIVE , but the difference is very small (0.2%).
 prefix  X  X e- X  nor the prefix  X  X e- X  can ever appear with the prefix  X  X i-. X  The word  X  X endidik X  h to educate i is derived from the prefix  X  X e- X  and the stem  X  X idik X  h to educate i . However, none of the Indonesian stemming algorithms that u se a dictionary X  X xcept for the algorithm of which uses templat e rules X  X estrict the combination and order of derivational prefix removal; th is could lead to overstemming. If the word  X  X idik X  is not in the dictionary, t he fragment  X  X i- X  at the beginning of the word is considered to be a prefix, and th e determined stem is  X  X ik X  h a younger sibling i . Since all the algorithms check the dictionary after each removal, this problem is rare, and occurs only whe n the dictionary is not complete.
 produce the same result as the majority baseline. 17 The largest portion (46%) of problems are caused by a nonroot word appearing in the dict ionary, causing stemming to end prematurely. Another 16% of failures occur w hen the rel-evant root word is not in the dictionary, causing the algorit hm to backtrack unnecessarily. Other difficulties are caused by the absence in the dictionary of proper nouns, composite words, acronyms, and foreign words .
 herent in human languages. The understemming problem is als o indirectly related to ambiguity. If we include the prefix  X  X enge- X  to cor rectly stem the word  X  X engecek X  h to check i to  X  X ek X  h to check i , the word  X  X engenang X  h to rem-inisce about i will be wrongly stemmed to  X  X ang X  (a proper noun existing in the dictionary) instead of the correct stem  X  X enang X  h to think of i . To solve problems such as word-sense ambiguity and homonymity, we mu st incorporate more detailed knowledge of the language to be stemmed. Furth ermore, disam-biguation tasks require the context surrounding the words t o be stemmed, and a large data collection to allow collection of statistical d ata.
 5.2 Ad Hoc Queries A major application of text processing techniques is inform ation retrieval, where documents are retrieved in response to a user query. Th ese documents are typically provided to the user as a list, sorted by decrea sing estimated likelihood of being relevant to the user X  X  information need as expressed in the query [Jones et al. 2000]. precision  X  X he fraction of retrieved items that are relevant X  X nd its recall  X  X he fraction of relevant items that are retrieved. The related R-Precision measure computes the precision at the N th retrieved item, where N is the number of relevant items in the collection.
 trieval, we must evaluate whether each retrieved document i s relevant to a query. To automate what is essentially a manual task, we requ ire a testbed comprising a common document collection, a set of queries, a nd a list of docu-ments in the collection that are considered to be relevant to each query. 18 searchers with appropriate testbeds for evaluating inform ation retrieval (IR) techniques for several retrieval paradigms [Harman 1992]. The original TREC track was named the ad hoc track, and aimed to investigate searches for new topics in archived data. This is the approach used by most use rs of Web search engines, where the typical query is a phrase or set of keyword s that describe an information need.
 researchers. The Linguistic Data Consortium (LDC) [Liberm an and Cieri 1998] also provides data collections for use by the IR commun ity. However, there is no publicly available testbed for Indonesian IR. Th e Indonesian doc-ument collections that do exist [Fahmi 2004; Tala 2003; Vega 2001] either do not have topics and relevance judgments, or are not publishe d. To allow rig-orous evaluation of IR techniques for Indonesian, we formed a collection and associated ground truth using the first 3,000 news articles f rom the 9,898 ar-ticles described in Section 5.1.1. The resulting collectio n is around 750 KB in size and contains 38,601 distinct words. The rest of the arti cles are used as training data for the experiments described in Section 6.2.
 lection is relatively small. Two examples of collections co mmonly used for English text IR research are the Wall Street Journal articles (1987 X 1989) of size 276 MB with 98,732 documents, and the Associated Press n ewswire dis-patches (1989) of size 254 MB with 84,678 documents [Voorhee s and Harman 1999]. Nevertheless, it is a starting point for further deve lopment of Indone-sian IR research. The small size of our collection also allow s detailed ground truth to be prepared; with TREC document collections, not every document is judged, and a pooling method is used [Voorhees and Harman 199 9]. As [Zo-bel 1998] points out, if the pool is not deep enough, pooling m ay favor newer systems that combine and improve the retrieval techniques o f old systems. Ef-fectiveness measurement may also discount actual relevant documents that have not been seen by the reviewers during the relevance judg ment process. data as close to the original as possible, and did not correct any faults such as spelling mistakes or incomplete sentences. The collection is stored in a single file, marked up using standard TREC tags. An example document is shown in Figure 1; the tags &lt; DOC &gt; and &lt; /DOC &gt; mark the beginning and end of a document respectively, and each document has a document ide ntifier delimited by the &lt; DOCNO &gt; and &lt; /DOCNO &gt; tags. 5.2.2 Ground Truth for IR Queries. After examining the documents, we compiled 20 query topics that would have relevant answers in the collection. The topics are of two types: general , where many documents meet the informa-tion need, and specific , where the set of relevant documents is small. We define general topics as those containing ten or more relevant docu ments; an example of a general query on our collection is,  X  X orld Cup Report X  (t opic 13). Specific topics have fewer than ten relevant documents; for example, the query,  X  X hat are the symptoms and causes of asthma? X  (topic 10) is specific when applied to our collection. An example of an Indonesian topic and its Eng lish translation is shown in Figure 2. The queries follow the TREC format [Voorhees and Harman 2000], with a number, title, description, and narrative.
 expressed by each query, each of the 3,000 documents was read and judged manually. This resulted in an exhaustive tabulation of 20  X  3 , 000 = 60 , 000 relevance assessments. Additional information and the tes tbed itself are avail-able online. 19 are not necessarily reproducible with other collection set s [Sanderson and Zo-bel 2005]. However, we believe that the preliminary results they offer can aid further research in Indonesian IR. for Indonesian text, we evaluated its effect over the 20 quer ies on our col-lection of 3,000 documents. We used the zettair 20 search engine to query the stemmed and unstemmed data with the topic titles, returning 100 answers per query. Zettair has native support for the TREC format for collections, topics, and relevance assessments.
 sion for each query by taking the sum of the precision values a t each answer document for the first 100 answers, assuming that answers not in the top 100 to have precision values of 0, and divide the sum by the number of relevant documents in the collection. The mean average precision (MA P) is the average of precision values across all queries. This MAP measure ind icates that stem-ming improves retrieval performance by around 2%. The secon d row shows av-erage precision after processing 10 documents, averaged ov er all queries; this shows a 2% drop in performance when stemming is performed. Th e R-Precision results in the final row favor stemming by around 3%.
 mance are not statistically significant at the 95% confidence level. Indonesian words have many more variants than those in English, and we ex pected that the removal of prefixes, infixes, and suffixes should improve r etrieval perfor-mance. However, these results are consistent with those obs erved in English text retrieval [Hull 1996].
 shown: (1) to the left, the total number of relevant document s; (2) in the mid-dle, the number of relevant documents found without stemmin g; and, (3) on the right, the number of relevant documents found with stemm ing. The results show that X  X ith the exception of topics 2, 9, and 19 X  X here is l ittle difference between recall with and without stemming. The overall recal l value without stemming is 0.728; with stemming, this increases to 0.781. 6. EXTENSIONS The dictionary plays a very important role in algorithm effe ctiveness. From Table VI, we see that 51 of 196 stemming errors (26%) can be tra ced to an incomplete dictionary or to misspellings. In this section, we describe three extensions to the stemmer; two address the dictionary issue , while the third aims to prevent stemming of proper nouns. 6.1 Dictionary Augmentation Using n -grams To offset problems caused by words not appearing in the dicti onary, we propose a scheme based on n -grams, a common mechanism for evaluating similarity be-tween strings [Bakar et al. 2000; Ng et al. 2000]. A character string of length N can be decomposed into N  X  n + 1 substrings or n-grams of length n . For exam-ple, the string  X  X erintah X  can be decomposed into the 2-gram s  X  X e, X   X  X r, X   X  X i, X   X  X n, X   X  X t, X   X  X a, X  and  X  X h. X  corresponding n -grams would be  X  X e, X   X  X r, X   X  X i, X   X  X m, X   X  X t, X   X  X a, X  and  X  X h. X  Of these seven n -grams, five X  X r 71.4% X  X re identical to those of the correctl y spelled word.
 all stemming operations, the final dictionary lookup step fa ils, we can return the closest dictionary word using the approach of Zobel and D art [1996]. To de-termine the closest dictionary word, we can use one of the mea sures described in the literature; these include Q -grams [Ukkonen 1992], Priority [Zobel and Dart 1996], and Onechar [Hall and Dowling 1980].
 proach, using an n -gram length of between 5 and 7 characters, produces the best results. Here, the distance between two strings s and t is defined as: | G G | is the number of identical n -grams in the two strings. For the strings  X  X erintah X  and  X  X erimtah, X  we compute the distance to be 7 + 7  X  (2  X  | 5 | ) = 4. Table VIII shows that 6-grams and 7-grams produce results th at are sim-ilar to the CS stemmer without any extension, and slightly better than for 5-grams. With longer n -gram sizes, we can better avoid incorrectly stemming proper nouns. For example, with 5-grams, the proper nouns  X  X  idayatullah X  and  X  X smatullah X  are stemmed to  X  X ullah, X  while 6-grams and 7-g rams avoid this problem. However, in some cases, it is better to use shorter n -grams. For ex-ample, with 5-grams we correctly stem  X  X etabuhan X  h percussion instrument i to  X  X abuh X  h to hit a percussion instrument i , whereas we fail to find a stem when using 6-grams or 7-grams. While we aimed to use n -grams to correct misspellings, we found that none of the 19 misspellings that appeared in SUB -JECTIVE are correctly handled. The misspelling errors produced by n -grams are similar to CS without any extension, which implies that they do not ad-dress the problem. Overall, we consider that 5-grams offers the best trade-off; we present a scheme to handle proper nouns later in this secti on.
 best retrieval effectiveness. From Table IX, we see that the best average preci-sion and R-Precision results are actually produced when usi ng 4-grams. This is similar to the situation described by McNamee and Mayfield [2004] for Eu-ropean languages such as English, French, German, Italian, Spanish, and Swedish. Despite the increase in precision, combining the CS stemmer with n -grams does not help recall either. We conclude that using n -grams does not necessarily correct misspellings, but it is a useful techni que for increasing av-erage precision and R-Precision in an information retrieva l environment. 6.2 Other Issues In our earlier experiments, we have relied on the DICT -UI dictionary with 29,337 words. We now describe experiments that instead use t he  X  X amus Besar Bahasa Indonesia X  h Greater Indonesian Dictionary i ( KBBI ) with 27,828 entries, and also the KEBI online dictionary with 22,500 root words. 21 We refer to these dictionaries as DICT -KBBI and DICT -KEBI respectively.
 without any n -gram extensions. The performance obtained using the origi nal DICT -UI dictionary is consistently better than that obtained using the DICT -KBBI dictionary.
 due to the high latency of lookup requests. We were also unabl e to use it for experiments with n -grams, since we require access to the complete dictionary word list when creating the grams.
 We hypothesize that this is caused by the larger number of pro per nouns, such as  X  X akarta X  and  X  X ndonesia, X  contained in the DICT -KBBI . However, longer n -grams provide a higher probability of leaving a proper noun u nstemmed even if it is not in the dictionary, and so offer a more robust choic e. 6.3 Identifying Proper Nouns Thompson and Dozier [1997] state that proper nouns make up be tween 38.8% and 67.8% of queries. Proper nouns are considered to be root w ords, and should not be stemmed. In Table VI, around 13.27% of stemming errors are shown to be caused by improperly stemming proper nouns. To address th is problem, we use the training collection of 6,898 documents from the Komp as Web site to draw up a list of common proper nouns to use alongside our dict ionary. scribe; Curran and Clark [2003] describe similar aspects of Proper Noun Iden-tification. First, we identify words that are likely to be acr onyms and should not therefore be stemmed. Acronyms are typically written in uppercase (all-uppercase, or AU ). However, it is common to find acronyms written with only the initial letter in uppercase ( OIU ), or all lowercase ( AL ). In some cases, acronyms appear mid-sentence within parentheses: for exam ple, in Narkoba h drugs i , which is the acronym for  X  X arkotika dan obat-obatan terlar ang. X  22 We treat words containing only alphabetical characters, appe aring between paren-theses, and with at least the initial letter in uppercase, to be acronyms. We represent such words with the symbol PIU .
 nantly in uppercase are likely to be proper nouns. We may requ ire that at least the initial letter to be in uppercase ( IU ), or that only the initial letter be in uppercase ( OIU ). The first rule would match all of the words  X  X akarta, X   X  X ndonesia, X   X  X BRI X  (the acronym for the Indonesian army), and  X  X etroTV X  (a private Indonesian television station), whereas the secon d rule would match only the first two.
 case (either IU or OIU ), and do not appear in the beginning of sentences should be considered to be proper nouns. However, this assumption i ncludes words appearing in titles of documents or in organization or commi ttee names, such as  X  X eterlibatan X  h involvement i , which can be stemmed to  X  X ibat X  h involve i . Not stemming such words decreases the average precision of a d hoc retrieval. The average precision of the CS stemmer without extension is 0.4842, increas-ing to 0.5111 and 0.4842 with 4-grams and 5-grams respective ly. Stemming is still useful; not stemming this category of words resulted i n precision values of 0.4522 and 0.4533 respectively. sistent capitalization, led us to believe that we should app ly a required ratio between the capitalization types. For example, we could req uire that to be con-sidered a proper noun, a word should appear overwhelmingly w ith at least the first letter in uppercase.
 of words appearing in a particular way in the training collec tion. Figure 4 shows the retrieval effectiveness for varying thresholds. For 4-grams, the av-erage IU and OIU precision values are quite similar, and the same is the case for 5-grams; this leads to two overlapping lines in the figure . When the thresh-old is too low, too many words are considered to be proper noun s and are not stemmed (false positives). If the threshold is too high, som e proper nouns may be wrongly stemmed (false negatives). We need to determine t he threshold that affords the highest average precision. From the figure, we see that av-erage precision peaks for a threshold of 65 for 4-grams and at 40 for 5-grams in both IU and OIU . These numbers translate to approximately 66% of words with the initial letter in uppercase, IU , (62% for 4-grams and 69% for 5-grams) and approximately 69% for only the initial letter in upperca se, OIU , (75% for 4-grams and 63% for 5-grams). We determine that a threshold o f 65 is good for both IU and OIU for our collection, and use this ratio, along with n -grams of size 4 (which produce the best results for IR experiments) for further exper-iments as BEST -IU and BEST -OIU .
 glish documents to be proper nouns. We formed a list of these  X  English words X  ( EW ) from the documents of volumes 1 to 5 of the TREC Research Collection. 23 These documents comprise content from the Associated Press (AP), the San Jose Mercury News (SJM), the Wall Street Journal (WSJ), the Financial Times (FT), and the Los Angeles Times (LATimes).
 be probable proper nouns. We produced a list of such words by e xtracting words of 2-4 letters from our training data, and manually sel ecting valid titles that are always followed by a proper noun. The resultant list contained the these titles are considered to be proper nouns, with two exce ptions. First, multiple titles may appear together, as in  X  X rof. Dr. Ibrahi m. X  Second, single letters may follow titles, as in  X  X r. A. Salam X ; these are lik ely to be initials. For such exception cases, we do not consider the word followi ng the title to be a proper noun.
 plained earlier, the BEST -IU and BEST -OIU results were obtained by using the threshold 65. The last column is the combination of all metho ds that can in-crease average precision over CS without any extension. Lists are combined by merging the proper nouns in one list with those of another, and removing duplicates.
 English word documents ( EW ) as proper nouns. Using CS with 4-grams and the EW extensions actually decreases performance from 0.4842 for CS with-out any extension to 0.4746. This is surprising, since the li st itself appears to contain many valid proper nouns. These include personal n ouns such as {  X  X braham, X   X  X hatcher, X   X  X iponegoro X  } , place nouns such as {  X  X fghanistan, X   X  X hailand, X   X  X imbabwe X  } , and also English words that should not be stemmed, such as {  X  X reasury, X   X  X rontier, X   X  X oreman, X   X  X ialogue X  } . Nevertheless, the list of words may in fact contain some Indonesian words such as  X  X e rmodalan X  h capitalization i (of which the root word is  X  X odal X  h capital i ) and  X  X endidikan X  (of which the root word is  X  X idik X  h to educate i ). Clearly, not all these In-donesian words are root words. We believe that determinatio n of whether or not to stem nonroot words for improved retrieval effectiven ess merits further investigation.

The only technique that produces a statistically significan t improvement in the average precision and R-precision over unstemmed wor ds is the com-bination of AU + PIU + BEST -IU + BEST -OIU + WAT , as shown in the last column of Table XII. This combination technique produces a signific antly better av-erage precision and R-precision than no stemming, at a 95% co nfidence level using the Wilcoxon one-tailed signed ranked test. For precision at 10, this combination technique produces a slightly worse value than no stem-ming, but the difference is not statistically significant at the 95% confidence level. The basic CS stemmer continues to produce good results. While the re-maining techniques do increase average precision, the impr ovement is not statistically significant.

Adding a proper noun identification component does not incre ase recall val-ues. The best recall value of 0.781 is similar to the original recall value pro-duced by the CS stemmer. For the other variants that incorporate proper nou n identification, recall values range from 0.730 to 0.779. We c onclude that using n -grams and proper noun identification may increase precisio n, but not neces-sarily improve recall. 7. DISCUSSION AND FUTURE WORK In this work, we have described the principal structure of de rived words in the Indonesian language, and introduced a new confix-stripping approach for au-tomatic and highly accurate stemming of derived words to the ir morphological roots. We have compared the precision of this algorithm with other automated stemming approaches for Indonesian using a baseline create d from human ex-periments, and have shown that it is the most accurate.

We have also reported on results of ad hoc queries on a testbed for Indone-sian text retrieval that includes 3,000 newswire documents , 20 topics, and ex-haustive relevance judgments. We have shown that stemming d oes not sig-nificantly aid retrieval performance on this collection. We suspect that this is because some relevant documents answer the query implici tly, and do not contain the query terms. For instance the query for  X  X ama bos Manchester United X  h the name of the Manchester United boss i does not retrieve a docu-ment that discusses  X  X he MU manager. X  A human assessor under stands that  X  X anager X  is a synonym of  X  X oss X  and  X  X U X  is the acronym of  X  X a nchester United X ; automated retrieval systems generally use words d irectly from the query, and stemming is ineffective here.

There are other possible reasons why stemming might fail to i ncrease precision. Krovetz [1993] and Savoy [1999] suggest that sho rt documents benefit more from stemming than longer documents. Krovetz su ggests that a stemmer that caters for different meanings and disambigua tes them might improve precision. From experimentation on French data, Sa voy conjectures that more complex stemmers that remove derivational suffixe s may cause conflation errors.
 using n -grams did not in practice handle misspellings well, we obse rved that it increased the average precision and R-Precision in a text re trieval setting. We have also described schemes for identifying proper nouns in Indonesian text to avoid incorrect stemming. Not stemming proper nouns identi fied using combi-nations of different methods, except for English words, doe s improve precision, although it does not necessarily increase recall.
 language is inherently ambiguous, and even the best stemmin g algorithm will still make mistakes. The context where the word appears is im portant to choos-ing the right stem. For example, the word  X  X engurus X  h to take care of, to be-come thin i can be stemmed to either  X  X rus X  h to take care of, to handle i or  X  X u-rus X  h skinny i . Such natural language processing is beyond the scope of thi s article. The difference in stemming results between humans and an auto-matic stemmer could also be caused by human error. For exampl e, we have found  X  X dalah X  h to be i to be stemmed to  X  X da X  h to exist i at one time, and left unstemmed at another time, by the same person. These cases ar e difficult to address.
 sidered only generic stemming, but many words adopt differe nt meanings in different contexts. Xu and Croft [1998] show that schemes th at cater for differ-ent content perform better than a generic stemming scheme th at stems words independently of the corpus content. As part of our ongoing i nvestigation of stemming, we plan to explore the use of different general-pu rpose or domain-based dictionaries, including the CICC [1994] dictionary, on stemming effec-tiveness. Second, we intend to further study schemes of findi ng proper nouns to increase retrieval effectiveness; preliminary experim ents have shown good results. Finally, we plan to expand the number of queries and documents in our testbed to improve the statistical significance of any re sults. able advancement in the literature on Indonesian stemming, and will aid more effective information retrieval for Indonesian text.
 We thank Falk Nicolas Scholer for his advice on formulas and s tatistics; Vin-sensius Berlian Vega for the V EGA source code; Riky Irawan for the Kompas newswire documents; and Gunarso for the KBBI dictionary. We also thank Wahyu Wibowo for his help in answering our queries and Eric Dh armazi, Agnes Julianto, Iman Suyoto, and Hendra Yasuwito for their h elp in creating our human stemming ground truth.

