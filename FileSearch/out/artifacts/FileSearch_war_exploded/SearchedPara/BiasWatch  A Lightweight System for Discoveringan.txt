 We propose a lightweight system for (i) semi-automatically dis-covering and tracking bias themes associated with opposing sides of a topic; (ii) identifying strong partisans who drive the online discussion; and (iii) inferring the opinion bias of  X  X egular X  partic-ipants. By taking just two hand-picked seeds to characterize the topic-space (e.g.,  X  X ro-choice X  and  X  X ro-life X ) as weak labels , we develop an efficient optimization-based opinion bias propagation method over the social/information network. We show how this approach leads to a 20% accuracy improvement versus a next-best alternative for bias estimation, as well as uncovering the opinion leaders and evolving themes associated with these topics. We also demonstrate how the inferred opinion bias can be integrated into user recommendation, leading to a 26% improvement in precision. H.3.4 [ Information Storage and Retrieval ]: Systems and Soft-ware X  Information networks opinion analysis; bias; social media
Social media has increasingly become a popular and important platform for  X  X egular X  people to express their opinions, without the need to rely on expensive and fundamentally limited conduits like newspapers and broadcast television. These opinions can be ex-pressed on a variety of themes including politically-charged topics like abortion and gun control as well as fun (but heated) rivalries like android vs. iOS and Cowboys vs. 49ers. Our interest in this paper is in creating a flexible tool for discovering and tracking the themes of opinion bias around these topics, the strong partisans who drive the online discussion, and the degree of opinion bias of  X  X egular X  social media participants, to determine to what degree particular participants support or oppose a topic of interest. However, assessing topic-sensitive opinion bias is challenging. First, the opinion bias of  X  X egular X  users may not be as pronounced as prominent figures, so discerning this bias will require special care. Second, how opinion bias manifests will inevitably vary by t-c  X  opic, so a system should be adaptable to each topic. Third, the themes by which people express their opinions may change over time depending on the circumstances (e.g., gun control debates may take different forms based on the ebb and flow of elections, recent shooting incidents, and so forth). As a result, assessing bias should be adaptive to these temporal changes.

Hence in this paper, we develop a lightweight system  X  BiasWatch  X  for discovering and tracking opinion bias in social media. Bi-asWatch begins by taking just two hand-picked seeds to character-ize the topic-space (e.g.,  X  X ro-choice X  and  X  X ro-life X  for abortion) as weak labels to bootstrap the opinion bias framework. Concretely, we leverage these hand-picked seeds to identify other emerging (and often unknown) themes in social media, reflecting changes in discourse as new arguments and issues arise and fade from pub-lic view (e.g., an upcoming election, a contentious news story). We propose and evaluate two approaches for expanding the hand-picked seeds in the context of Twitter to identify supporting and opposing hashtags  X  one based on co-occurrence and one on signed information gain. We use these discovered hashtags to identify strong topic-based partisans (what we dub anchors ). Based on the social and information networks around these anchors, we propose an efficient opinion-bias propagation method to determine user X  X  opinion bias  X  based on both content and retweeting similarity  X  and embed this method in an optimization framework for estimat-ing the topic-sensitive bias of social media participants. In sum-mary, this paper makes the following contributions:  X  First, we build a systematic framework  X  BiasWatch  X  to dis-cover biased themes and estimate user-based opinion bias quan-titatively under the context of controversial topics in social me-dia. We propose an efficient optimization scheme  X  called User-guided Opinion Propagation [UOP]  X  to propagate opinion bias.
By feeding just two opposing hashtags, the system can discover bias-related hashtags, find bias anchors, and assess the degree of bias for  X  X egular X  users who tweet about controversial topics.  X  Second, we evaluate the estimation of users X  opinion bias by comparing the quality of the proposed opinion bias approach ver-sus several alternative approaches over multiple Twitter datasets.
Overall, we see a significant improvement of 20.0% in accuracy and 28.6% in AUC on average over the next-best method.  X  Third, we study the effect of different approaches for biased theme discovery to measure the impact of newly discovered bi-ased hashtags as additional supervision. We observe that the newly discovered hashtags are often associated with the underly-ing community of similar opinion bias, and that they temporally fluctuate due to the impact of new controversial events.  X  Finally, we demonstrate how these inferred opinion bias scores can be integrated into user recommendation by giving similar-minded users a higher ranking. We show that the integration can improve the recommendation performance by 26.3% in pre-cision@20 and 13.8% in MAP@20. This result implicitly con-firms the principle of homophily in the context of opinion bias, and demonstrates how topic-sensitive opinion bias can enrich user modeling in social media.
There has been considerable research effort devoted to exploring political polarization, assessing media bias of major news outlets, and assessing user sentiment towards particular topics.

Political polarization. Political polarization has been a topic of great interest in the past decade and studied in news articles [33], online forums [3] and social media [1, 5, 6, 7, 11, 16, 23]. Adamic and Glance [1] demonstrated the divided community structure in the citation network of political blogs. Conover et al. [7] and Livne et al. [16] showed that there exists a highly segregated network structure using modularity. Guerra et al. [11] compared polarized and non-polarized networks and proposed a new measure to deter-mine whether a network is polarized given that the network is also modular. Since knowing users X  political orientation can be of great importance for understanding the overall political landscape, many approaches have been proposed to classify a user X  X  political iden-tity. Conover et al. [6] and Pennacchiotti and Popescu [23] exploit text and network features for classification. Akoglu [3] proposed to use signed bipartite opinion networks for the classification and ranking of user X  X  political polarity on forum data. Zhou et al. [33] applied semi-supervised learning methods to classify news articles and users X  political standing. Cohen et al. [5] employ supervised methods to classify political users into groups with different politi-cal activities, and conclude that it is hard to infer  X  X rdinary X  users X  political orientation. In our work, instead of simply focusing on the classification of user X  X  political orientation, we are interested in developing a flexible tool to explore controversial themes and discover their underlying users X  degree of opinion bias on a topic basis. We show that user X  X  opinion bias can be leveraged to improve other applications such as user recommendation.

Media bias. Apart from user-oriented political orientation, some works have explored media bias. Groseclose et al. [10] proposed a new measure to quantify media bias by comparing the number of citations to think tanks and policy groups to those of Congress members. Gentzkow et al. [9] also proposed a media bias measure which considers the frequency of phrases quoted by Congressional members of Republican and Democratic parties in newspapers. Lin et al. [15] focused on the measure of coverage quantity to compare the extent of bias between blogs and news media. Wong et al. [29] quantified the political leanings of media outlets on Twitter using aggregated retweeting statistics. Our work differs from these in that we target the opinion bias of  X  X egular X  users instead of prominent media, and with respect to different controversial topics instead of only political leanings.

User sentiment. There are also prior works which infer user X  X  sentiment toward a topic in social media or online forums. Tan et al. [26] proposed a semi-supervised approach to inferring users X  sen-timent using social network information. Kim et al. [12] and Gao et al. [8] proposed to use a collaborative filtering like approach to estimate user-level sentiment. Lu et al. [17] proposed to use con-tent and social interactions to discover opinion networks in forum discussions. However, our work has two differences from these and other sentiment-oriented approaches. The first is that many of these works require a significant amount of manually labelled tweets or users as ground truth. In our work, we develop automatic approaches using crowdsourced hashtags as seeds to substantially reduce manual labor. The second is that we focus on intrinsic opin-ion bias instead of sentiment. Sentiment [22] centers around users X  attitude or emotional state, usually reflected by the use of emo-tional words. However, opinion bias can also be reflected by the news or factual information she chooses to post, which may lack any prominent emotional words. Problem Statement. We assume there exists a set of users U = { u 1 ,u 2 ,...,u n } sampled from Twitter. Each user has their corre-sponding tweets D = { d 1 ,d 2 ,...,d n } related to a controversial topic T , where d i is a collection of tweets by u i son X  X  opinion bias represents the intrinsic tendency that she chooses to support or oppose a concept under a controversial context, we choose to quantize the degree of her opinion bias by a numeric score ranging from -1 to 1. Specifically, we assign B = { b for each user in U , respectively, where b i  X  [  X  1 , 1] . When b close to 1, it denotes that user u i has a strong positive standing to-ward the topic; when b i is close to -1, it represents the opposite. Thus, given a controversial topic T , a sampled set of users U and their on-topic tweets D , we identify the following tasks of the sys-tem framework: (i) Discovering biased themes that are discussed by opposing sides of users. We denote P as the set of positive themes and N as the set of negative themes; (ii) Finding bias an-chors who show strong degree of opinion bias, which we denote as U anchor ; (iii) Determining  X  X egular X  participants X  opinion bias B . Overall Approach. In order to tackle these tasks, we propose a lightweight framework that propagates opinion bias scores based only on a few hand-picked seeds that characterize the topic-space (e.g.,  X  X ro-choice X  and  X  X ro-life X  for abortion). The BiasWatch framework, illustrated in Figure 1, takes as input these hand-picked seeds and then proceeds through the following three key steps:  X  Finding Bias Anchors. This first step identifies topic-based partisans whose opinion bias is strongly revealed through their choice of hashtags. We develop two automatic approaches to: (i) identify biased themes in the form of hashtags through initial seeds; (ii) expand the pools of bias anchors with these identified biased themes.  X  Propagating Bias. This second step builds a user similarity net-work around these expanded anchors and other  X  X egular X  partic-ipants, and propagates bias along this network. The edges here measure the similarities of two users through content and link features from tweets.  X  Noise-Aware Optimization. Lastly, we propose to embed the previous two steps into a noise-aware optimization framework where anchors X  opinion bias can be effectively propagated to each  X  X egular X  participant throughout the network. A key facet is that this optimization is tolerant of noisy labels on the initial bias anchors, so that initial errors made in identifying bias anchors need not lead to cascading errors in  X  X egular X  participants.
Our first challenge is to identify strong topic-based partisans (what we dub anchors ). These anchors serve as the basis for propa-Table 1: Top ten themes at different times for  X  X racking X  discovered by seed expansion; red for pro-fracking; blue for anti-fracking. #marcellus #nokxl #tcot #oil #naturalgas #tcot #dontfrackny #frack gating opinion bias throughout the social and information network. One reasonable method for identifying anchors is to manually label a number of users, among whom we hope that there exist a portion of users whose opinion bias is clearly shown. However, there are two disadvantages of this approach: (i) it is potentially expensive and time-consuming; and (ii) because of the random nature of la-beling, typically, we have to label many users whose opinion bias is not clear or neutral in order to obtain anchors.

To overcome these difficulties, we propose to exploit crowd-generated hashtags in Twitter. Hashtags are often used to tag tweets with a specific topic for better discoverability or to indicate the community to which the tweets are posted [30]. Some hashtags may be viewed as a rich source of expressing opinions [28], po-tentially indicating user X  X  opinion bias. For example, some most popular hashtags for  X  X un control X  include #guncontrolnow and #2ndamendment, which reveal strong user bias, with the former of-ten used by supporters of gun control and the latter by opponents. Hashtags of this nature can be essentially considered as weak la-bels of the polarity of tweets with respect to a controversial topic. Hence, to find bias anchors we first seek to identify a candidate set of hashtags that provide support for the topic and a candidate set of hashtags that express opposition. Since the hashtags used to ex-press an opinion may change over time as new issues arise and fade from public view (e.g., an upcoming election, a contentious news story) and as new arguments are reflected in online discourse, we leverage these seeds to identify emerging themes in social media via seed expansion. To show a concrete example, Table 1 lists top ranking opposing and supporting themes at different times for the topic  X  X racking X  after seed expansion is performed. We can see that new biased themes emerge as controversial events occur. In the fol-lowing, we consider two approaches for expanding the hand-picked seeds to identify supporting and opposing hashtags: Seed Expansion via Co-Occurrence. The first approach relies on hashtag co-occurrence statistics to find other hashtags used by users with similar opinion bias. The intuition is that if two hash-tags are often used together by different users (whether in the same tweet or different tweets with respect to a topic), these two hash-tags are likely to indicate the same opinion bias. Here, hashtag co-occurrence is based on users instead of tweets, considering that a user X  X  opinion bias is not likely to change. Thus, for hashtags which occur in different tweets, as long as they are used by the same user, they are still considered to occur together.

Let f i and f j represent the frequency of the hashtag h i related to a topic, respectively. Let f + and f  X  represent the fre-quency of the pro-seed h + and the anti-seed h  X  , respectively. The similarity between two hashtags, denoted as  X  ( h i ,h j ) , is computed by Jaccard coefficient (JC) as: where f h i  X  h j represents the co-occurrence frequency of h and f h i  X  h j represents the total occurrence frequency of either h or h j . Thus,  X  ( h + ,h i ) and  X  ( h  X  ,h i ) represents the similarity of h to the pro-seed and anti-seed, respectively. We select top hash-tags with the largest similarity to the pro-seed and anti-seed as the candidate set C + and C  X  . However, since some hashtags, for ex-ample,  X #guncontrol X , are generic and co-occur with both the pro-seed and anti-seed, these hashtags do not indicate any opinion bias. To filter out common hashtags like these, we impose the following constraint on hashtag from C + for pro-seed: where a large reflects more correlation with the pro-seed; similar constraint can also be imposed to filter hashtags in C  X  . We then use the resulting m top hashtags for pro-seed as positive theme set P , and m top hashtags for anti-seed as negative theme set N . Seed Expansion via Signed Information Gain. Although the ap-proach above does find other biased hashtags, there are two dis-advantages: (i) it often gives niche hashtags which are only used by a small number of participants; (ii) it often misses event-related short-lasting biased hashtags. In light of these issues, we propose the second approach which relies on weak supervision to select the most distinguishing hashtags for each side. Specifically, we per-form the following procedure: 1. Training with pro-seed and anti-seed . First, we aggregate a user X  X  tweets and use a bag-of-words model to compute TFIDF for each user. Users who have tweeted with at least one hashtag are then selected and used. From these users, we treat users with only pro-seed as positive class c + , users with only anti-seed as negative class c  X  , and the rest for prediction. Finally, an SVM classifier is learned on the training data and used to predict the polarity of users which are left. We now have an expanded set of users who are positive, and an expanded set of users who are negative. 2. Selecting hashtags . From the expanded sets of users, we use signed information gain (SIG) proposed by Zheng et al. [31] as the measure to select hashtags for pro-seed and anti-seed, respectively. where A is the number of users with h i and in class c + , B is the number of users with h i and in class c  X  , C is the number of users without h i and in class c + and D is the number of users without h and in class c  X  . Also,  X  h i represents hashtags other than h the probability p is obtained by maximum likelihood estimation. We select m hashtags with the largest SIG as the finalized hashtag set P for pro-seed, and m hashtags with the smallest SIG as the finalized hashtag set N for anti-seed.

As a result, this approach can not only filter out common hash-tags used by both sides of users without manually specifying any extra parameters, but also can discover popular yet distinguished biased hashtags.
 Bias Anchors . Given the expanded set of hashtags (both support-ing and opposing a particular topic), we identify as our strong par-tisans users who consistently adopt hashtags from only one opinion standpoint, which we denote as U anchor . We assign an initial bias score  X  b i to these anchors as follows: where U P and U N is the set of anchors adopting hashtags from P and N , respectively, and U anchor = U P  X  U N . These opinion bias anchors serve as the basis for propagating bias throughout the social and information network, which we tackle next.
After the discovery of bias anchors, how do we determine the opinion bias of those remaining participants? We propose to build a propagation network where two users are only connected if their similarity passes a threshold. In the following, we adopt both con-tent and link features to determine user similarity.
 Content-Based Propagation . The assumption of content induced propagation is that if two users have a high textual similarity in their posts, it is likely that they may share similar opinion bias. To compute the content similarity of two users, we aggregate each user X  X  topic-related tweets and treat each user as a document. Thus, content similarity of two users can be computed with document similarity. Here, we adopt cosine similarity of the TFIDF of the two documents with a standard bi-gram model. Tokenization of tweets is done through the tool provided by Owoputi et al. [21] for its robustness. Hashtags and mentions are also included in the model as features. To reduce the size of feature dimensions, we performed stop-word removal and kept only unigrams and bi-grams with occurrence frequency greater than two. The content similarity between u i and u j can be written as: where C ij is the cosine similarity of u i and u j . To reduce the prop-agation complexity, we construct a sparse network by only consid-ering k-nearest neighbors N c ( u i ) for each user u i . We choose k to be 10 for its efficiency without compromising much accuracy in experiments.
 Link-Based Propagation . Retweeting can be considered a form of endorsement for users in Twitter [4, 7]. Based on this observation, it is expected that if a user retweets another user on a topic, both users tend to share similar opinion bias. Thus, we define the link similarity between u i and u j as follows: where u j is in the neighbors of u i if u j retweeted u i u . Besides retweeting links, we can also take advantage of the fol-lower/following network information in Twitter. Here, we choose not to use it since following link is not as strong a signal as retweet-ing. One reason is that a user whose opinion is on one side may choose to follow someone on the opposite side for the purpose of receiving any topic related statuses or refuting their arguments. Furthermore, we notice that the resource of retweeting activities is usually sparse so that the propagation network constituted only from retweeting links is separated into many isolated networks. Thus, a retweeting-based propagation is not enough to be treated by itself but needs to be combined with content-based propagation, leading to the following fusion of similarity between users: where  X  is a weighting parameter for content and link similarity.
Finally, we embed the discovered anchors and bias propagation network into an optimization setting to propagate the opinion-bias score of all users more effectively. Since the initial input to the approach is a set of weak labels (two user-specified opposite hash-tags), we call this optimization User-guided Opinion Propagation [UOP] . Specifically, by allowing each user X  X  true opinion bias b change as an optimization variable, we force the following condi-tions: (i) for bias anchors, b i should be as close to the bias indicated by adopting biased hashtags (Eqn. 1); (ii) for other participants, b and b j should be close to the degree indicated by their content and link similarity (Eqn. 4). Their opinion bias is initialized randomly between [  X  1 , 1] and can now be iteratively propagated through op-timization. Thus, we have the following objective function: min subject to  X  1  X  b i  X  1  X  i  X  X  1 ,...,n } where  X  1 is the tradeoff weight for different components. Thus, by solving this optimization, each user X  X  opinion bias is propagated through the network in an optimized fashion. Since the objective function is convex, we can use the standard L-BFGS method with constraints to solve it efficiently. Another advantage of this frame-work is that other similarity signals such as location, profile demo-graphics, and so on can be easily incorporated into Equation 5. Handling Noisy Bias Anchors. The essence of the above opti-mization framework is that we propagate users X  opinion bias which we know with confidence to other users who we have seldom knowl-edge of. We can see that anchor X  X  opinion bias  X  b i is treated as the golden truth and stays unchanged. However, a prominent issue is that users who consistently adopt hashtags from either P or N are not guaranteed to have the corresponding opinion bias. To give an example, one user X  X  tweet reads,  X #Gosnell certainly a tragedy, also cautionary tale, but not an argument against abortion rights in the US. Want more Gosnells? Ban abortion. X  This sarcastic pro-choice user adopted the hashtag #Gosnell, as we can observe from many tweets, is a primary hashtag pro-life users would use. Hence, ac-cording to Equation 1, this user is falsely identified as a pro-life an-chor. Without manual inspection of user X  X  profile and their related tweets, it is very hard to judge whether a bias anchor determined from Equation 1 is correctly identified. To relieve this problem, we introduce another variable y i for u i  X  U anchor as the ideal opinion bias. Intuitively, it should satisfy: (i) y i should be close to the opin-ion bias inferred from neighbors; (ii) most y i should be consistent with  X  b i , with a few of them being noisy. Inspired by the annotation of noisy web images in [27], we propose a modified minimization function as follows: min subject to  X  1  X  b i  X  1  X  1  X  y i  X  1  X  i  X  X  1 ,...,n } where  X  1 and  X  2 are the weighting parameters. We use l -1 norm to constrain the ideal variable y i to  X  b i since normally, only a small portion of bias anchors are noisy and l -1 norm could force most anchors to stay as biased. We solve the above minimization through the following steps: (i) Initialize y i to  X  b i and solve Equation 5 to obtain b (ii) Solve the following sub-minimization problem with b i tain y i : We employ the package L1 General [24] to solve the problem. (iii) Replace  X  b i with y i in Equation 5 to get final b repeat step (i) and (ii) for several times for further optimization but usually two to three iterations are enough shown by experiments. In this way, errors made in identifying bias anchors can be mitigated, leading to more accurate opinion bias estimation.
In this section, we perform several sets of experiments to evalu-ate the BiasWatch framework for topic-sensitive opinion bias dis-covery. We investigate the impact of seed expansion, the quality of bias propagation via both content and retweeting links, and com-pare the performance versus alternative opinion bias approaches. We couple this study with an application of the system on two more controversial datasets. The datasets that we use are collected with Twitter X  X  streaming API from October 2011 to September 2013. To create topic-related datasets for opinion discovery, we selected three controversial top-ics:  X  X un control",  X  X bortion" and  X  X bamacare". We select these topics because they are popular controversial topics discussed by a large number of Twitter users with both opposing sides of opinion expressed in the time period. For each topic, we extracted a base set of tweets (and their corresponding users) containing at least one topic-related keyword: for  X  X un control": gun control, gun right, pro gun, anti gun, gun free, gun law, gun safety, gun violence ; for  X  X bortion": abortion, prolife, prochoice, anti-abortion, pro-abortion, planned parenthood ; and for  X  X bamacare": obamacare, #aca . Additionally, we created another two datasets on the top-ics  X  X accine X  and  X  X racking X  for demonstration. We select these two topics for further evaluation because they are relatively recent controversial topics compared to the previous ones, and also their opposing sides may not be fully entrenched in traditional left/right party politics. To extract  X  X accine X  related tweets, we use the fol-lowing keywords: vaccine, vaccination, vaccinate, #vaxfax ; for  X  X racking X , we use: fracking, #frack, hydraulic fracturing, shale, horizontal drilling . We summarize the datasets in Table 2.
In order to evaluate the framework, we need to know the true opinion of a randomly sampled user set against which we can com-pare the optimization results. Without direct access to user X  X  bias and considering the inherent difficulty of knowing a user X  X  bias de-gree with respect to a controversial topic, we rely on an external labeling scheme using Amazon Mechanical Turk. Since the bias score obtained from Equation 5 is continuous, we discretize the opinion bias of a Twitter user into the following five categories: strong support [+2], some support [+1], neutral or no evidence [0], some opposition [-1], strong opposition [-2].

Thus, we can map the continuous range into the above categories for evaluation. For each topic, we randomly selected 504 Twitter users from the total users in Table 2, and assigned eight users in each human intelligence task (HIT), then ask the turkers (human labeler) to select the most appropriate category for these users. For each user, we show her twitter user ID and her topic related tweets for each turker to examine. We also highlight the hyperlinks em-bedded in the tweets and make them clickable. To ensure good quality of assessment, we follow the suggestions by Marshall and Shipman [18]. For each human intelligence task, we put two addi-tional users in random positions, making a total of ten users in one HIT. Those users X  bias are already known through experts, which we refer to as the golden users. If the label given by a turker for any of these golden users is very different from that by experts, we discard the entire answer by this turker for the HIT. Moreover, we ask five turkers to label one user and take the majority vote as the final label for the user. The results are shown in Table 3. Agreement of Opinion Bias Labels. To measure the reliability of the above human labeling tasks, we investigate the inter-rater agree-ment of the obtained assessment with Fleiss X   X  statistic. Specif-ically, we obtained the 5-category  X  statistic of 0.264, 0.393 and 0.418 for  X  X un control X ,  X  X bortion X  and  X  X bamacare X , respectively. These values lie in the interpretation of fair agreement by Landis and Koch [13]. In addition, we also adopt the accuracy of agree-ment provided by Nowak [20] and adapt it into the following for-mula since each user is assessed by more than two turkers: accuracy = where N is the total number of users to be assessed by turkers for each topic. The accuracy ranges from 0.304 when the majority is obtained by chance to 1 when every user X  X  bias category is agreed by all turkers. The lower bound 0.304 is obtained by calculating the average number of people in a majority out of five when they select one category from five by chance. Hence, an accuracy of 0.6, for example, means that on average, 3 out of 5 turkers agree on a cate-gory. The accuracy for  X  X un control X ,  X  X bortion X  and  X  X bamacare X  is 0.646, 0.727 and 0.788, respectively, which means, on average, at least 3 turkers agree on the majority category.

Furthermore, we aggregated the same polarity into one category, namely, category [+1] and [+2] are combined to one category and vice versa. The 3-category  X  statistic increases to 0.461, 0.588 and 0.649, correspondingly, while the accuracy increases to 0.811, 0.857 and 0.906. The  X  values can now be interpreted as moderate agreement. The accuracy now means at least four out of five peo-ple agree on the majority bias polarity on average. This indicates that humans are more capable of discerning the polarity of users X  opinion bias than determining the extent of users X  bias.
In the following, we choose to use the 3 bias categories as ground truth since it has the most consistent and reliable performance by human labelers. We additionally consider only the support and op-position categories (ignoring the minority of users who are neutral or do not show evidence, namely, category [0]) so we can cast the evaluation as a binary class problem. We adopt the standard classi-fication measures of accuracy and area under the curve (AUC).
To evaluate our approach of determining users X  opinion bias, we consider the following alternative opinion bias estimators:  X  SentiWordNet [SWN] . This is a simple sentiment detection ap-proach, where we assign a sentiment score to each user X  X  tweets according to SentiWordNet and classify each user X  X  opinion bias with the relative portion of positive and negative tweets. Specif-ically, for each tweet d i of user u i , we classify it positive if classify user u i as positive if the number of positive tweets is greater than the number of negative tweets, and vice versa.  X  User Clustering with Content [uCC] . In this baseline, we con-struct a user graph and perform user clustering with tweets. The nodes of the graph are users and the edges are constructed based on k nearest neighbors with the largest content similarities of tweets. We choose to use cosine similarity of the TFIDF of bi-grams as the similarity measure. To perform graph clustering, we apply normalized cuts for graph partitioning by Shi and Malik [25] due to its simplicity and good performance. This is essen-tially a max-cut problem explored in [2, 19] to partition news-group and online debates into opposite positions, respectively.
The purpose of using this unsupervised baseline is to examine whether the selected biased hashtags as a form of weak supervi-sion can provide much improvement.  X  User Clustering with Content and Links [uCCL] . This base-line is the modified version of uCC in which the edge weight combines both content and link similarity. Specifically, if there exists a retweeting link between two users, we add a constant to its content similarity, i.e., w = w content +  X   X  w link , where  X  is used to balance the weights. The purpose of this baseline is to examine if retweeting links can help distinguish opposing sides of users X  opinion bias compared to uCC.  X  Weakly-supervised SVM [wSVM] . We train an SVM with a bi-gram model of bias anchors X  tweets, which is then used to classify the test dataset. The parameters of SVM are determined through 5-fold cross-validation. We denote the trained classifier with bias anchors found through initial seeds (IS) as wSVM+IS , and the other two classifiers trained on seed expansions as wSVM+JC and wSVM+SIG . Here, wSVM+IS is treated as the baseline, and the other two as our improved versions.  X  Local Consistency Global Consistency [LCGC]. This is a semi-supervised method proposed by Zhou et al. [32] and applied in [33] for the classification of the political learning of news ar-ticles. This method optimizes the tradeoff between local con-sistency and global consistency among node labels. Here, we use Equation 4 as the affinity between nodes for the method and adapt LCGC into our own version by incorporating seed expan-sion from SIG, denoted as LCGC+SIG .  X  UOP  X  . This is the framework in which we only consider content based bias propagation without handling noisy bias anchors.  X  UOP  X  . This is the framework in which we consider both con-tent and link based bias propagation without handling noisy bias anchors, indicated by Equation 5.  X  UOP . This is the full blown-approach indicated by Equation 6.
Before experiments, we first select the following pro-seed and anti-seed manually as the input to the system: #guncontrolnow and #2ndamendment for  X  X un control X ; #prochoice and #prolife for  X  X bortion X ; #ilikeobamacare and #defundobamacare for  X  X ba-macare X . We later show in the experiments the effect of different seed selections.

We then highlight the seed expansion methods  X  both hashtag co-occurrence (JC) and signed information gain (SIG)  X  used to identify biased hashtags adopted by users with similar opinion bias. For seed expansion via SIG, we need to determine the value of pa-rameter m . To study the influence of this parameter, we adopt the method UOP* to evaluate performance changes with values from { 2 , 6 , 10 , 14 , 18 } . Figure 2 shows that the accuracy is highest when m is approximately at 10 for  X  X un control X  and  X  X bortion X , and is slightly larger for  X  X bamacare X . After those values, accuracy lev-els or even decreases, possibly because the additional discovered hashtags are noisy or do not imply much opinion bias. Thus, in the following experiments, m is fixed at 10 for all topics. For seed ex-pansion via co-occurrence, we choose m to be 10 using the similar approach, and empirically set to 3. The expanded hashtags, as we observed from the output, can be approximately categorized as: (i) Sentiment-oriented. These hashtags can be easily discerned and used directly by participants to show opinion bias, such as #nowaynra for  X  X un control X  and #dontfundit for  X  X bamacare X . (ii) Community identification. These hashtags indicate personal or political identities and are often used in a community, such as #p2, #tcot, #teaparty and #fem2 (for feminists); (iii) Thematic. These hashtags often indicate arguments used by participants to express their opinion. For example, gun con-trol antagonists say #gunrights as a constitutional right protected by #2ndamendment (also #2a); abortion protagonists may empha-size #reprorights or #reprojustice in their arguments; (iv) Action-oriented. Examples include #momsdemandaction, #stand4life, #standwithcruz (stand with Ted Cruz), #swtw (stand with Texas women) and #demandaplan (as in  X #demandaplan to end gun violence X );
Overall, we can conclude that seed expansion through our pro-posed approaches is able to find other biased hashtags which are used by people with similar opinion bias. The above categoriza-tion also serves as guidance for users to pick initial opposing seeds as input to the system. Now that we have discovered the biased themes related to each side of polarity for different controversial topics, can we leverage those to determine  X  X egular X  participants X  opinion bias? Specifically, we ask the following questions: (i) Can these newly discovered biased hashtags help to identify user X  X  opinion bias? If so, how much better can they do? (ii) Do different pro-seed and anti-seed selections affect perfor-mance? If so, can seed expansion help us with the selection? (iii) Can social ties, in the form of retweeting links, help us de-termine user X  X  opinion bias? Effect of Seed Expansion. To evaluate the performance of these expanded hashtags, we use wSVM and UOP* as the base meth-ods since both of them only rely on the information provided by bias anchors and content. For UOP*, the weight parameter  X  Equation 5, is empirically determined to be 0.1.

We now compare the performance between the version when opinion bias is propagated only through initial seeds and the ver-sion when the seeds are expanded. Since the result of accuracy is similar with AUC, we only show AUC in Figure 3. We can see that both seed expansion approaches outperform initial seeds, with seed expansion via SIG giving the best performance. Specifically, AUC from seed expansion via SIG gives a 19.5% and 6.8% improvement over IS and JC for wSVM, respectively, while the improvement is 12.5% and 5.6% for UOP*, respectively. This shows that (i) the newly discovered hashtag set through seed expansion provides ad-ditional amount of bias information for users; and (ii) the quality of Figure 4: Performance for 25 pro-seed and anti-seed combinations. expanded hashtag set via SIG is generally the best, i.e., it is able to discover higher quality of biased themes.
 Effect of Different Seeds. Here, we are interested in studying the influence of different choices of initial seeds to the system. To that end, we first rank hashtags by occurrence frequency and then manually select top five pro-seeds and top five anti-seeds for  X  X un control X  by observing tweets with those hashtags. The top five pro-seeds are: 1. #nowaynra; 2. #guncontrolnow; 3. #demandaplan; 4. #newtown; 5. #whatwillittake. The top five anti-seeds are: 1. #tcot; 2. #2ndamendment; 3. #nj2as; 4. #2a; 5. #gunrights. We represent a pro-seed and anti-seed combination with their corre-sponding number. For example,  X 12 X  represents the combination  X #nowaynra #2ndamendment X . We then use the method UOP* to evaluate the performance of each combination for two cases: one with initial seeds; the other with seed expansion via SIG. The re-sults are shown in Figure 4. Overall, we can see that for different choices of input seeds, the performance without seed expansion is very sensitive to the choices, while it gives consistent high accu-racy for seed expansion with SIG. We can also observe that #nj2as is not a good anti-seed choice since all combinations with #nj2as performs bad. Also, all combinations with #gunrights give unsat-isfactory results except  X 55 X . These results indicate that it is dif-ficult to choose the most effective seed combinations, since very often a single pair of seeds can be noisy and do not cover enough bias related themes. However, seed expansion can mitigate this ef-fect by introducing other and often more complete biased hashtags for bias propagation. Hence, even though the initial seeds are not well selected, the final performance does not suffer much due to the benefits of seed expansion. For other topics, similar results are observed, and thus are not reported due to the space limit. Effect of Social Ties. Here, we evaluate the effect of social ties in the form of retweeting links for different seed expansion ap-proaches. The retweeting links are randomly sampled for ten times for each fraction since a different sample of retweeting links gives a different propagation network. Here, we adopt UOP  X  for the eval-uation, with the weight given to the link-based propagation  X  in Figure 5: Performance for different seed expansion approaches with respect to different fraction of retweeting links for abortion. Equation 4 empirically chosen to be 1. The final results are aver-aged and plotted in Figure 5. We only show the results for abor-tion since it is similar with other topics. We can see that as more retweeting links are added for propagation, the performance in-creases for all three approaches, which confirms the assumption that users linked via retweeting tend to share similar opinion bias.
Furthermore, among these three approaches, the performance obtained via SIG is always the best for different fractions of retweet-ing links at all topics. As the fraction gets larger, the improvement generally gets smaller. This indicates that the effect of seed ex-pansion gets reduced when the fraction increases. For seed expan-sion via JC, the performance is generally better than that of initial seeds when the fraction is small. However, this initial improvement gets reduced or even disappears when retweeting link is abundant, hence, making the extra hashtags obtained via co-occurrence less effective. This is probably because of the noisiness of those extra hashtags. Though beneficial at a small number of retweeting links, they prevent the correct bias from being propagated when more of them are added for optimization. This shows again that the key of seed expansion is that not only more biased hashtags should be discovered, but also they should be of high quality.
In this section, we compare our proposed BiasWatch framework with the alternative opinion bias estimators. For uCCL, the weight-to be 0.1 for best performance. For LCGC+SIG, we use the same parameter setting as in [33]. Other parameter settings in UOP*, UOP  X  and UOP are the same as in previous experiments. For  X  in Equation 6, we empirically set it to 0.2 for handling noisy bias anchors. We choose the best seed expansion approach (via SIG) for all methods. The final results are shown in Table 4.

Overall, user-guided approaches give much better performance than unsupervised methods, indicating that by just a small amount of human guidance  X  two opposite seed hashtags, the performance can be boosted significantly. Moreover, UOP gives the best perfor-mance, reaching an average accuracy and AUC of 0.923 and 0.913, respectively (an improvement of 20.0% and 28.6% over supervised baseline wSVM+IS). Note that the sentiment based approach SWN gives unsatisfactory results, probably because user X  X  opinion bias is multifaceted and can be reflected by the topical arguments or fac-tual information published by the user. For example, one of the anti-obamacare tweets reads, "Double Down: Obamacare Will In-crease Avg. Individual-Market Insurance Premiums By 99% For Men, 62% For Women. #Forbes". Also, we can see that UOP gives better results than LCGC+SIG, indicating that our framework works better in capturing user X  X  opinion bias. UOP, however, gives better performance than UOP  X  , confirming that initial bias anchors determined through biased hashtags are noisy, and that UOP is able to correct some wrongly determined bias anchors due to the l -1 norm regularization on ideal bias scores.  X  0 . 883  X  0 . 910 0 . 945  X  0 . 913  X 
Furthermore, we see from the table that UOP  X  has an improve-ment of 0.053 and 0.047 for accuracy and AUC over UOP*, re-spectively. Compared to the corresponding improvement of 0.016 and 0.027 by uCCL over uCC, it is considerably higher. This in-dicates that retweeting links are more effective in contributing to bias propagation with the help of bias anchors. When some users are correctly  X  X abeled X  by discovered biased hashtags, opinion bias can be propagated more effectively through retweeting links.
In previous experiments, we mainly evaluate our framework as a binary class problem. However, in this way, we lose the finer gran-ularity of the inferred opinion bias score by ignoring users who are neutral or do not show evidence. This category of users can not only be beneficial to our understanding of the general landscape of controversial topics, but also can be targeted by polarized activists to influence their bias. Thus, in this section, we are interested in a finer level of evaluation by casting it as a three-class problem. Here, instead of combining the category +1 and +2, and the cate-gory -1 and -2, we aggregate users in the category of +1, 0 and -1 into one neutral category 0 for a relatively larger pool of users in the middle. We then partition the datasets into 50% for training and 50% for testing, and compare the performance of wSWM+SIG, LCGC+SIG, UOP  X  and UOP. We are interested to see how these methods perform in the recall defined by 1 3 P i tp i tp sure indicates the ability of finding out the correct user category in a three-category setting for different methods.

For wSVM+SIG, we adopt the one-vs-rest multi-class imple-mentation. For LCGC, we modified it to consider three labels. For UOP based methods, we select two optimal thresholds as bound-aries:  X  p , used to distinguish category +2 and 0 and  X  distinguish category -2 and 0. These thresholds are selected by searching the range (-1,1) with a step of 0.05 with the training data. For UOP,  X  n and  X  p are determined as -0.25 and 0.2 for  X  X un con-trol X , -0.35 and 0.35 for  X  X bortion X , -0.2 and 0.2 for  X  X bamacare X . The final results are shown in Table 5. As we can see, UOP gives the best performance of all, indicating that it is the most capable in finding out users who are polarized and neutral. wSVM and LCGC perform worse, probably because of the small amount of training samples for category 0. UOP based methods are able to mitigate this effect by propagating constrained bias score without explicit consideration of neutral users and only with bias anchors. Thus, we can conclude that UOP is effective in representing the degree of user X  X  opinion bias under the controversial topics.
Now that we have demonstrated the effectiveness of the system in finding general users X  opinion bias, we would like to test the Figure 6: Temporal volumes of top anti-fracking themes for seed expansions via co-occurrence (Left) and via SIG (Right). framework further in another two datasets:  X  X racking X  and  X  X ac-cine X . Since these two topics are relatively new compared to the above politically-driven well-known topics, it would be interesting to discover what is being posted and debated by opinionated users of both sides. For  X  X racking X , we pick the seeds #dontfrackny and #jobs as the input to the system. We select #jobs as the pro-seed for  X  X racking X  because protagonists tend to emphasize the benefit of creating jobs as one of the arguments for  X  X racking X . For  X  X ac-cine X , we pick the seeds #health and #autism as the input to the sys-tem. Again, we select #autism as the anti-seed because antagonists tend to focus the negative effect of vaccination. Overall, selection of input seeds is intuitive due to the recognizability of some of the crowd-generated tags and thus does not require much human effort.
We first demonstrate the biased themes discovered via SIG. Ta-ble 1 shows the top-ranking biased hashtags for different time pe-riods. We can see that some themes such as #natgas and #frack remain constantly used by users supporting and opposing fracking, respectively; some other themes, such as #dontfrackny and #bal-combe, arise and fade as related controversial events occur. For example, the occurrence of #dontfrackny corresponds with a rally of New Yorkers in the mid February 2013 to urge Governor Cuomo to resist fracking in the state of New York; the occurrence of #bal-combe corresponds with a protest against a license to drill near Bal-combe in England granted by Environment Agency. These chang-ing themes indicate a strong degree of opinion bias and emerge as a group of users start to use them together, making them a very use-Table 6: Top ten themes at different times for  X  X accine X ; red for pro-vaccine; blue for anti-vaccine.
 ful signal to determine and propagate user X  X  opinion bias. Further-more, the transient property of these changing themes also makes the approach of seed expansion via co-occurrence less effective. Figure 6 illustrates the temporal characteristics of discovered anti-fracking themes for different seed expansion approaches. We can see that seed expansion via co-occurrence failed discovering #bal-combe and #nogas, as both of these hashtags do not co-occur with #dontfrackny. In contrast, SIG tackles the problem by tapping into the power of content to discover more related biased themes. Ta-ble 6 shows the top-ranking biased hashtags at different time pe-riods for the dataset  X  X accine X . To illustrate how the system can uncover strong partisans, Table 7 shows two uncovered bias an-chors  X  @Energy21, a pro-fracking account from the Institute for 21st Century Energy, and @Duffernutter, an anti-fracking account associated with TheEnvironmentTV.
In this section, we demonstrate the application of integrating opinion bias for user recommendation in social media. The prin-ciple of homophily, observed both in political blogosphere [1] and social media [7], states that people tend to associate with others who are like-minded. Thus, when social media services recom-mend users to follow, it is natural to consider recommending users who have similar opinion bias on shared topic interests. User rec-ommendation can also be considered as a task of link prediction in graphs, and there has already exist many works [14] which specifi-cally address this task, mostly taking advantage of graph structure. Here, our goal is to demonstrate how opinion bias can be utilized for user recommendation as a different dimension.

The task can be formally described as follows: Given a contro-versial topic T , a sampled set of users U and their corresponding on-topic tweets D , recommend k friends to follow for the target user. Note that we have only tweets to rely on to determine the recommendations. We provide two approaches for the task. Content-Based approach . In this collaborative filtering approach, users with the highest content similarities to the target user are rec-ommended. Specifically, we aggregated the corresponding tweets for each user and applied vector space model (VSM) with unigrams and bi-grams, with the similarity computed by cosine measure. Opinion-Weighted (OW) approach . Here, user similarity is con-sidered as a weighted sum of content similarity and opinion similar-ity. The content similarity is computed in the same way as VSM. The opinion similarity is obtained as follows. First, user X  X  opin-ion bias score is obtained through UOP. Then, we characterize the opinion similarity of two users as: where f ( x ) is a normalizing function with the form as 1 The value of parameter c should be chosen to make b  X  X  transition to opposite sign steep enough, so that two bias scores with small difference but opposite signs can still result in large difference after transformation. Also, when b i and b j have the same sign, they can have a relatively large opinion similarity. To this end, c is chosen to be 5. The final similarity is computed as the weighted sum (  X  is the weighting parameter), which we use for ranking users: sim ( u i ,u j ) =  X sim vsm ( u i ,u j ) + (1  X   X  ) sim opi
For evaluation, we additionally crawled following links for the dataset  X  X un control X ,  X  X bortion X  and  X  X bamacare X . These follow-ing links are used as the ground truth in our experiments. We ran-domly sampled 500 users who have at least 20 followees for each topic and used the following metrics to evaluate: (i) precision@K: which measures the percent of the correct followees out of the top K recommended users; and (ii) mean average precision@K: which is the average of the precision at the position of each correct followee out of the top K recommended users. This measure considers the positions of the recommended users. K is chosen to be 20.
Figure 7 shows the performance comparisons between the two approaches. Here,  X  is set to 0.5. For both metrics, the OW ap-proach has a better performance than the vanilla VSM for each topic. On average, it gives an improvement of 26.3% in preci-sion@20 and 13.8% in MAP@20. These results indicate that user X  X  opinion similarity boosted the rank of some of the true followees who have similar opinion bias as the target user, while lowering the similarity with users who hold different opinion. Hence, it implic-itly confirms the principle of homophily that people tend to make friends who share similar opinions.

In Figure 8, we also show the performance at different values of  X  . As we can see, the best performance is achieved neither at  X  = 0 or 1 for all topics, but at a mixed weight between con-tent and opinion similarity. Even when the weight given to opinion similarity is small, i.e., when  X  = 0 . 9 , the improvement can reach 20.2% for precision@20 and 9.2% for MAP@20 on average. The figure shows that the performance is not very sensitive to the value of  X  when  X  is approximately in the range of (0.2, 0.8), indicating it does not require fine tuning to reach a better performance.
We have seen how the BiasWatch system can lead to an im-provement of 20% in accuracy over the next-best alternative for bias estimation, as well as uncover opinion leaders and bias themes that may evolve over time. We also demonstrated how the in-ferred opinion bias can be integrated into user recommendation and showed that it gives a performance improvement of 26% in preci-sion. While our investigation has focused on textual and relational features, it does not limit us from integrating new signals such as location and profile demographics for better performance in future work. We are also interested in incorporating opinion bias into a social media dashboard, so that participants can be aware of their own opinion dynamics as well as those of others.
This work was supported in part by AFOSR grant FA9550-12-1-0363 and NSF grant IIS-1149383. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. [1] L. Adamic and N. Glance. The political blogosphere and the [2] R. Agrawal, S. Rajagopalan, R. Srikant, and Y. Xu. Mining [3] L. Akoglu. Quantifying political polarity based on bipartite [4] D. Boyd, S. Golder, and G. Lotan. Tweet, tweet, retweet: [5] R. Cohen and D. Ruths. Classifying political orientation on [6] M. Conover, B. Gon X alves, J. Ratkiewicz, A. Flammini, and [7] M. Conover, J. Ratkiewicz, M. Francisco, B. Gon X alves, [8] H. Gao, J. Mahmud, J. Chen, J. Nichols, and M. Zhou. [9] M. Gentzkow and J. Shapiro. What Drives Media Slant? [10] T. Groseclose and J. Milyo. A measure of media bias. The [11] P. Calais Guerra, W. Meira Jr., C. Cardie, and R. Kleinberg. [12] J. Kim, J. Yoo, H. Lim, H. Qiu, Z. Kozareva, and [13] J. Landis and G. Koch. The measurement of observer [14] D. Liben-Nowell and J. Kleinberg. The link-prediction [15] Y. Lin, J. Bagrow, and D. Lazer. Quantifying bias in social [16] A. Livne, M. Simmons, E. Adar, and L. Adamic. The party is [17] Y. Lu, H. Wang, C. Zhai, and D. Roth. Unsupervised [18] C. Marshall and F. Shipman. Experiences surveying the [19] A. Murakami and R. Raymond. Support or oppose?: [20] S. Nowak and S. R X ger. How reliable are annotations via [21] O. Owoputi, B. O X  X onnor, C. Dyer, K. Gimpel, N. Schneider, [22] B. Pang and L. Lee. Opinion mining and sentiment analysis. [23] M. Pennacchiotti and A. Popescu. Democrats, republicans [24] Mark Schmidt. L1 general, May 2015. [25] J. Shi and J. Malik. Normalized cuts and image [26] C. Tan, L. Lee, J. Tang, L. Jiang, M. Zhou, and P. Li. [27] J. Tang, R. Hong, S. Yan, T. Chua, G. Qi, and R. Jain. Image [28] X. Wang, F. Wei, X. Liu, M. Zhou, and M. Zhang. Topic [29] F. Wong, C. Tan, S. Sen, and M. Chiang. Quantifying [30] L. Yang, T. Sun, M. Zhang, and Q. Mei. We know what@ [31] Z. Zheng, X. Wu, and R. Srihari. Feature selection for text [32] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Sch X lkopf. [33] X. Zhou, P. Resnick, and Q. Mei. Classifying the political
