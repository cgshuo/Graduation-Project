 Since the seminal contribution of [14], so-called ROC curves ( ROC standing for Receiving Oper-ator Characteristic ) have been extensively used in a wide variety of applications (anomaly detec-tion in signal analysis, medical diagnosis, search engines, credit-risk screening) as a visual tool for evaluating the performance of a test statistic regarding its capacity of discrimination between two populations, see [8]. Whereas the statistical properties of their empirical counterparts have been only lately studied from the asymptotic angle, see [18, 13, 11, 16], ROC curves also have recently received much attention in the machine-learning literature through the development of statistical learning procedures tailored for the ranking problem , see [10, 2]. The latter consists of determining, based on training data, a test statistic s ( X ) (also called a scoring function ) with a ROC curve  X  X s high as possible X  at all points of the ROC space. Given a candidate s ( X ) , it is thus of prime impor-tance to assess its performance by computing a confidence band for the corresponding ROC curve, in a data-driven fashion preferably. Indeed, in such a functional setup, resampling-based procedures should naturally be preferred to those relying on computing/simulating the (gaussian) limiting dis-tribution, as first observed in [19, 21, 20], where the use of the bootstrap is promoted for building confidence bands in the ROC space.
 By building on recent works, see [17, 12], it is the purpose of this paper to investigate how the bootstrap approach should be practically implemented based on a thorough analysis of the asymp-totic properties of empirical ROC curves. Beyond the pointwise analysis developed in the studies mentioned above, here we tackle the problem from a functional angle, considering the entire ROC curve or parts of it. This viewpoint indeed appears as particularly relevant in scoring applications. Although the asymptotic results established in this paper are of a theoretical nature, they are con-siderably meaningful from a computational perspective. It turns out indeed that smoothing is the key ingredient for the bootstrap confidence band to be accurate, whereas a naive bootstrap approach would yield bands of low coverage probability in this case and should be consequently avoided by practicioners for analyzing ROC curves.
 The rest of the paper is organized as follows. In Section 2, notations are first set out and certain key notions of ROC analysis are briefly recalled. The choice of an adequate (pseudo-)metric on the ROC space, a crucial point of the analysis, is also considered. The smoothed bootstrap algorithm is presented in Section 3, together with the theoretical results establishing its asymptotic accuracy as well as preliminary simulation results illustrating the impact of smoothing on the bootstrap per-formance. In Section 4, the gain in terms of convergence rate acquired by the smoothing step is thoroughly discussed. We refer to [1] for technical proofs. Here we briefly recall basic concepts of the bipartite ranking problem as well as key results related throughout the paper. Although the results contained in this paper can be formulated without we intentionally connected them to this major statistical learning problem, which has recently revitalized the interest for the problem of assessing the accuracy of empirical ROC curves, see [4]. 2.1 Assumptions and notation In the bipartite ranking problem , the problem is to order all the elements X of a set X by degree of relevance, when relevancy may be observed through some binary indicator variable Y . Precisely, one has a system consisting of a binary random output Y , taking its values in { X  1 , 1 } say, and a random input X , taking its values in a (generally high-dimensional) feature space X , which models classification but the prediction task is different. In the case of information retrieval for instance, the goal is to order all documents x of the list X by degree of relevance for a particular request (rather than simply classifying them as relevant or not as in classification). This amounts to assigning to each document x in X a score s ( x ) indicating its degree of relevance for this specific query. The challenge is thus to build a scoring function s : X  X  R from sampling data, so as to rank the observations x by increasing order of their score s ( x ) as accurately as possible: the higher the score s ( X ) is, the more likely one should observe Y = +1 .
 True ROC curves. A standard way of measuring the ranking performance consists of plotting the ROC curve, namely the graph of the mapping where G s (respectively H s ) denotes s ( X )  X  X  cdf conditioned on Y = +1 (resp. conditioned on Y =  X  1 ) and F  X  1 (  X  ) = inf { x  X  R / F ( x )  X   X  } the generalized inverse of any cdf F on R . It boils down to plotting the true positive rate versus the false positive rate when testing the assumption  X  H 0 : Y =  X  1  X  based on the statistic s ( X ) . This functional performance measure induces a partial order on the set of scoring functions, according to which it may be shown, by standard Neyman-Pearson X  X  arguments, that increasing transforms of the regression function  X  ( x ) = P ( Y = +1 | X = x ) are the optimal scoring functions (the test statistic  X  ( X ) is uniformly more powerful , i.e.  X   X   X  (0 , 1) , ROC  X  (  X  )  X  ROC s (  X  ) , for any scoring function s ( x ) ).
 Empirical ROC curve estimates. Practical learning strategies for selecting a good scoring func-tion are based on training data D n = { ( X i ,Y i ) } 1  X  i  X  n and should thus rely on accurate empirical estimates of the true ROC curves. Let p = P ( Y = +1) . For any scoring function candidate s ( X ) , an empirical counterpart of ROC s is naturally obtained by computing from empirical cdf estimates: where n + = P n i =1 I { Y i = +1 } = n  X  n  X  is the (random) number of positive instances among where K  X  0 is a regularizing Parzen-Rosenblatt kernel ( i.e. a bounded square integrable function such that R K ( v ) dv = 1 ) and h &gt; 0 is the smoothing bandwidth, see Remark 1 for a practical view of smoothing. Here and throughout, I {A} denotes the indicator function of any event A . Metrics on the ROC space. When it comes to measure closeness between curves in the ROC space, various metrics may be used, see [9]. Viewing the ROC space as a subset of the Skorohod X  X  space D ([0 , 1]) of c ` ad-l ` ag functions f : [0 , 1]  X  R , the standard metric induced by the sup norm || . ||  X  appears as a natural choice. As shall be seen below, asymptotic arguments for grounding the bootstrapping of the empirical ROC curve fluctuations, when measured in terms of the sup norm || . ||  X  , are rather straightforward. However, given the geometry of empirical ROC curves, informative confidence bands. For analyzing stepwise graphs, such as empirical ROC curves, we shall consider the closely related pseudo-metric defined as follows: d a control on vertical and horizontal jumps of ROC curves both at the same time, treating both types of error in a symmetric fashion. Equipped with this pseudo-metric, two piecewise constant ROC curves may be close to each other, even if their jumps do not exactly match. This is clearly appropriate for describing the fluctuations of the empirical ROC curve (and the deviation between the latter and its bootstrap counterpart as well). This way, d B permits to construct builds bands of reasonable size, well adapted to the stepwise shape of empirical ROC curves, with better coverage probabilities. In this respect, the closely related Hausdorff distance ( i.e. the distance between the graphs completed by linear segments at jump points) would also be a pertinent choice. However, providing a theoretical basis in the case of the Hausdorff distance is very challenging and will not be addressed in this paper, owing to space limitations.
 As the goal pursued in the present paper is to build, in the ROC space viewed as a subspace of the Skorohod X  X  space D ([0 , 1]) equipped with a proper (pseudo-) metric, a confidence band for tities considered and denote by Z the r.v. s ( X ) (and by Z i , 1  X  i  X  n , the s ( X i )  X  X ) for notational simplicity. Throughout the paper, we assume that H ( dx ) and G ( dx ) are continuous probability distributions, with densities h ( x ) and g ( x ) respectively. Eventually, denote by P the joint distribution of ( Z,Y ) on R  X { X  1 , +1 } and by P n its empirical version based on the sam-p 2.2 Asymptotic law -Gaussian approximation In the situation described above, the next theorem establishes the strong consistency of the empirical ROC curve in sup norm and provides a strong approximation at the rate 1 / factors, for the fluctuation process : This (gaussian) approximation plays a crucial role in understanding the asymptotic behavior of the empirical ROC curve and of its bootstrap counterpart. The following assumptions are required. Theorem. 1 ( F UNCTIONAL LIMIT THEOREM ) Suppose that H 1  X  H 2 are fulfilled. Then, These results may be immediately derived from classical strong approximations for the empirical and quantile processes, see [5, 18]). Incidentally, we mention that the approximation rate is not always log 2 ( n ) / We point out that, owing to the presence of the term ( g/h )( H  X  1 (1  X   X  )) in it, the gaussian approx-imant can hardly be used for constructing ROC confidence bands. To avoid explicit computation of density estimates, bootstrap confidence sets should be certainly preferred in practice. Beyond consistency of the empirical curve in sup norm and the asymptotic normality of the fluctu-ation process, we now tackle the question of constructing confidence bands for the true ROC curve via the bootstrap approach introduced by [6], extending pointwise results established in [17]. The the conditional law given D n of the bootstrapped fluctuation process pairs with a common distribution e P n close to P n . We shall also consider whose random fluctuations, given D n , are expected to mimic those of d n = The difficulty is twofold. Firstly, the target of the bootstrap procedure is here a distribution on a path space , the ROC space being viewed as a subspace of D n ([0 , 1]) , equipped with either || . ||  X  or else d well-known that the naive bootstrap ( i.e. resampling from the raw empirical distribution) generally provides bad approximations of the distribution of empirical quantiles in practice: the rate of con-approximation is n  X  1 / 2 . As shall be seen below, the same phenomenon may be naturally observed for ROC curves. In a similar fashion to what is generally recommended for empirical quantiles, we suggest to implement a smoothed version of the bootstrap algorithm in order to improve the approx-resampling the data from a smoothed version of the empirical distribution P n . 3.1 The Algorithm Here we describe the algorithm for building a confidence band at level 1  X  in the ROC space from sampling data D n = { ( Z i ,Y i ); 1  X  i  X  n } . Set n + = P 1  X  i  X  n I { Y in four steps as follows. Before turning to the theoretical properties of this algorithm and related numerical experiments, a few remarks are in order.
 Remark 1 ( M ONTE -C ARLO APPROXIMATION ) From a computational angle, the true smoothed bootstrap distribution must be approximated in its turn, using a Monte-Carlo approximation scheme. A convenient way of doing this in practice, while reproducing theoretical advantages of smoothing, consists of drawing B bootstrap samples, of size n , with replacement in the original data and then perturbating each drawn data by independent centered gaussian random variables of variance h 2 (this procedure is equivalent to drawing bootstrap data from a smooth estimate e P n ( dz,dy ) com-choice of the number of bootstrap replications, picking B = n does not modify the rate of conver-gence. However, choosing B of magnitude comparable to n so that (1 + B ) is an integer may be more appropriate: the -quantile of the approximate bootstrap distribution is the uniquely defined and this will not modify the rate of convergence neither, see [15].
 Remark 2 ( O N TUNING PARAMETERS ) The primary tuning parameters of the Algorithm are those related to the smoothing stage. When using a gaussian regularizing kernel, one should typically choose a bandwidth h n of order n  X  1 / 5 in order to minimize the mean square error.
 Remark 3 ( O N RECENTERING ) From the asymptotic analysis viewpoint, it would be fairly equiva-in the computation of the bootstrap fluctuation process. However, numerically speaking, computing the sup norm of the estimate (2) is much more tractable, insofar as it solely requires to evaluate the distance between piecewise constant curves over the pooled set of jump points. It should also be noticed that smoothing the original curve, as proposed in [17], should be also avoided in practice, since it hides the jump locations, which constitute the essential part of the information. 3.2 Asymptotic analysis We now investigate the accuracy of the bootstrap estimate output by the Algorithm. The result stated in the next theorem extend those established in [17] in the pointwise framework. The functional ranking applications, assessing the uncertainty about the whole estimated ROC curve, or some part of it at least, is what really matters. In the sequel, we assume that the kernel K used in the smoothing step is  X  X yramidal X  ( e.g. gaussian or of the form I { u  X  [  X  1 , +1] } ). Theorem. 2 ( A SYMPTOTIC ACCURACY ) Suppose that the hypotheses of Theorem 1 are fulfilled. Assume further that smoothed versions of the cdf  X  X  e G and e H are computed at step 1 using a scaled kernel K h n ( u ) with h n  X  0 as n  X   X  in a way that nh 3 n  X   X  and nh 5 n log 2 n  X  0 . Then, the bootstrap distribution estimates output by the Algorithm are such that sup Hence, up to logarithmic factors, choosing h n  X  1 / (log 2+  X  n 1 / 5 ) with  X  &gt; 0 yields an approxima-tion error of order n  X  2 / 5 for the bootstrap estimate. Although its rate is slower than the one of the gaussian approximation (1), the smoothed bootstrap method remains very appealing from a compu-tational perspective, the construction of confidence bands from simulated brownian bridges being very difficult to implement in practice. As shall be seen below, the rate reached by the smoothed bootstrap distribution is nevertheless a great improvement, compared to the naive bootstrap approach (see the discussion below).
 Remark 4 ( B OOTSTRAPPING SUMMARY STATISTICS ) From Theorem 1 above, asymptotic validity of the smooth bootstrap method for estimating the distribution of the fluctuations of a functional  X ( [ ROC) of the empirical ROC curve may be deduced, as soon as the function  X  defined on D ([0 , 1]) is sufficiently smooth (namely continuously Hadamard differentiable). For instance, it could be applied to summary statistics involving a specific piece of the ROC curve only in order to focus on the  X  X est instances X  [3], or more classically to the area under the ROC curve ( AUC ). However, in the latter case, due to the fact that this particular summary statistic is of the form of a U -statistic [2], the naive bootstrap rate is faster than the one we obtained here (of order n  X  1 ). 3.3 Simulation results The striking advantage of the smoothed bootstrap is the improved rate of convergence of the resulting estimator. Furthermore, choosing d B for measuring the magnitude order of curve fluctuations has an even larger impact on the accuracy of the empirical bands. As an illustration of this theoretical result, we now display simulation results, emphasizing the gain acquired by smoothing and considering the pseudo-metric d B .
 We present confidence bands for a single trajectory and the estimation of the coverage probability of the bands for a simple binormal model : where  X  and X are independent standard normal r.v. X  X . In this example, the scoring function s ( x ) is the maximum likelihood estimator of the probit model on the training set. We choose here  X  0 =  X  1 = 1 , n = 1000 , B = 999 and  X  = 0 . 95 for the targeted coverage probability. Cov-erage probabilities are obtained over 2000 replications of the procedure, using the package ROCR of statistical software R . As mentioned before, choosing || . ||  X  yields very large bands with coverage probability close to 1 ! Though still large, bands based on the pseudo-metric d B are clearly much more informative (see Fig. 1). It should be noticed that the coverage improvement obtained by smoothing is clearer in the pontwise estimation setup (here  X  = 0 . 2) but much more difficult to evidence for confidence bands.
 Let us now give an insight into the reason why the smoothed bootstrap procedure outperforms the bootstrap without smoothing. In most statistical problems where the nonparametric bootstrap is useful, there is no particular reason for implementing it from a smoothed version of the empirical df rather from the raw empirical distribution itself, see [22]. However, in the present case, smoothing affects the rate of convergence. Suppose indeed that the bootstrap process (2) is built by drawing from the raw cdf X  X  b G and b H instead of their smoothed versions at step 2 of the Algorithm. Then, for rate induces an error of order O ( n  X  1 / 4 ) which cannot be improved, whereas it may be shown that the rate n  X  2 / 5 is attained by the smoothed bootstrap (in a similar fashion to the functional setup), provided that the amount of smoothing is properly chosen. Heuristically, this is a consequence of the oscillation behavior of the deviation between the bootstrap quantile H  X  X  X  1 (1  X   X  ) and its expected value b H  X  1 (1  X   X  ) given the data D n , due to the fact that the step cdf b H is not regular around b H  X  1 (1  X   X  ) : this corresponds to a jump with probability one.
 Higher-order accuracy. A classical way of improving the pointwise approximation rate consists of bootstrapping a standardized version of the r.v. r n (  X  ) . It is natural to consider, as standardization factor, the square root of an estimate of the asymptotic variance: An estimate smoothed density estimators  X  h = e H 0 and  X  g = e G 0 into (4) instead of their (unknown) theoretical counterparts. More interestingly, from a computational viewpoint, a bootstrap estimator of the vari-ance could also be used. Following the argument used in [17] for a smoothed original estimate of the ROC curve, one may show that a smoothed bootstrap of the studentized statistic r n (  X  ) / X  n (  X  ) yields a better pointwise rate of convergence than 1 / the Central Limit Theorem. Precisely, for a given  X   X  ]0 , 1[ , if the bandwidth used in the computation of  X  2 n (  X  ) is chosen of order n  X  1 / 3 , we have: ization step ( i.e. for estimating the variance) is not the same as the one used at the resampling stage of the procedure. This is a key point for achieving second-order accuracy. This time, the smoothed (studentized) bootstrap method widely outperforms the gaussian approach, when the matter is to build confidence intervals for the ordinate [ ROC(  X  ) of a point of abciss  X  on the empirical ROC curve. However, it is not clear yet, whether this result remains true for confidence bands, when considering the whole ROC curve (this would actually require to establish an Edgeworth expansion for the supremum || r n /
