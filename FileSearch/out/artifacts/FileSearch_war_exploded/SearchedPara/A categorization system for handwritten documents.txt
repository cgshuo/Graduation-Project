 ORIGINAL PAPER Thierry Paquet  X  Laurent Heutte  X  Guillaume Koch  X  Cl X ment Chatelain Abstract This paper presents a complete system able to categorize handwritten documents, i.e. to classify documents according to their topic. The categorization approach is based on the detection of some discriminative keywords prior to the use of the well-known tf-idf representation for docu-ment categorization. Two keyword extraction strategies are explored. The first one proceeds to the recognition of the whole document. However, the performance of this strat-egy strongly decreases when the lexicon size increases. The second strategy only extracts the discriminative keywords in the handwritten documents. This information extraction strategy relies on the integration of a rejection model (or anti-lexicon model) in the recognition system. Experiments have been carried out on an unconstrained handwritten doc-ument database coming from an industrial application con-cerning the processing of incoming mails. Results show that the discriminative keyword extraction system leads to better recall/precision tradeoffs than the full recognition strategy. The keyword extraction strategy also outperforms the full recognition strategy for the categorization task. Keywords Handwritten document categorization  X  Handwriting recognition  X  Keyword extraction  X  Shallow language model 1 Introduction These last years, the number of paper documents generated by administrative and economic activities has exploded. To facilitate the storage, processing and transferring of these documents, Electronic Document Management (EDM) sys-tems have been developed, where paper documents are scanned, stored and transferred electronically. In this con-text, the automatic reading of the document image content has seen a fast expansion. We have thereby observed the development of applications for processing targeted, specific problems, such as the automatic reading of forms, postal addresses or bank checks [ 23 , 25 , 28 ]. Besides these spe-cific applications, the automatic processing of handwritten documents remains a difficult and open problem: there is no system able to recognize an entire page of unconstrained cur-sive handwriting without using prior knowledge. This can be mainly explained by the huge variability in the writing style. In the literature, some recent works [ 5 , 42 , 49 ]have addressed the processing of lightly constrained handwrit-ten documents such as free mails. Among these projects, some address the full document recognition, whereas oth-ers are more oriented toward the rejection of misrecognized hypotheses or out-of-vocabulary words [ 23 , 48 ]. Some other projects e.g. [ 7 ] aim at indexing handwritten documents by their textual content for retrieval purposes. One alternative, called keyword spotting, has been proposed in order to pro-vide indexation facilities of a collection of handwritten docu-ments[ 2 , 26 , 30 ].Inthiscase,wordimagesareclusteredusing some appropriate features and elastic matching, thus avoid-ing the difficult task of recognition. Although interesting, these studies are not suitable for omni-writer mail documents since (i) they are based on a word image matching process, assuming word images boundaries are known and (ii) these collections exhibit some stability in the writing styles of the various writers encountered in the collection.

To the best of our knowledge, only one specific study pre-sented in [ 43 ] has been devoted to the categorization of hand-written documents. This pioneer study has been carried out using a mono-writer corpus built specifically. The proposed approach uses the word outputs of a mono-writer recogni-tion system to feed a word vector representation optimized forthecategorizationtaskattheend.Thecategorizationstage is performed using a classifier such as SVM or KNN.
In this paper, we address the categorization task of omni-writer handwritten documents such as incoming mail doc-uments. Thousands of such documents are received day by day in customer services of companies for various claims (address change, change of contract, contract cancelation, etc.). One example of such a document is illustrated on Fig. 1 (two other documents can be found in Appendix A ). Today, paper documents are scanned and then digitally sent to some remote service in charge of mail topic identification. The mail is finally sent to the appropriate department of the company. Automating this process requires the machine to read omni-writer handwritten documents for detecting its topic. This second task is known as categorization in the field of Infor-mation Retrieval [ 1 , 3 ] and for document images [ 9 ]. This task aims at classifying documents according to their subject matter. It is based on the detection of some specific key-words that are selected for their discriminative power among the various classes of documents. As opposed to keyword spotting where keywords are determined in an unsupervised manner, here keywords must be determined in a supervised manner considering the category (label) associated to each document. While the major difficulty in the categorization of electronic documents lies on the selection of these relevant keywords, the categorization task of handwritten documents also requires the system to detect these keywords in the docu-ment image, whatever the handwriting style. This is an addi-tional difficulty that has not received very much attention until now except in [ 43 ] with a limitation to a single writer.
This paper has two major contributions. First it addresses the question of omni-writer handwritten document categori-zation. The second contribution lies in the use of a shallow handwritten keywords extraction system on real-world doc-uments. The three main processing stages of the proposed system are as follows: layout analysis, keyword extraction and categorization (see Fig. 2 ).

This article is organized as follows. Section 2 is dedi-cated to the definition of the document categorization task. Section 3 presents the recognition system of omni-writer handwritten words, based on a lexicon directed analytical approach with an explicit segmentation. The keyword extrac-tion task is studied in Sect. 4 , where two statistical models of handwritten lines are proposed: the first one is based on a full recognition (FR) strategy, whereas the second one is based on a shallow language model dedicated to keyword extraction (KE). Section 5 is devoted to the experimental results for the incoming handwritten mail categorization task. Conclusion and future works are drawn in Sect. 6 . 2 Categorization of electronic transcriptions of documents This section is dedicated to the definition of the categoriza-tion task of textual documents, disregarding the writing rec-ognition system performance and the difficulty in extracting textual information in the image of a handwritten document. In the first part, we review the concepts used in the field of electronic document analysis to describe and categorize these documents. In the second part, the retained approach is eval-uated and compared to the literature on the Reuters 21578 reference corpus, then on the specific categorization task of incoming mail that we consider throughout this study. 2.1 Categorization of electronic documents A document categorization system aims at detecting the topic addressed in the document through the examination of its tex-tual information. It is a supervised classification task where each class is considered as a topic. To achieve such a task, one must first define a text characterization space (feature space) before defining a decision rule (of the classification system). This task has been widely studied in the literature for processing electronic documents. We can refer to [ 3 , 35 ] for a review of the possible techniques. One of the most effective approaches to characterize electronic documents is based on a vector model of documents known as  X  X ag of words X . This description is generally coupled with learning techniques such as neural networks. Like all pattern recogni-tion systems, a document categorization system consists in three main sequential steps following the traditional diagram on Fig. 3 . In the following paragraphs, we describe each step. 2.1.1 Preprocessing The first step consists in eliminating all undesired characters. We have chosen to accept only alphabetical characters and to eliminate all the others. Following this, empty words are filtered. This is performed using a list of words considered as the most frequent empty words (571 empty words for English and 463 for French). Finally, stemming (suffix elimination) is traditionally carried out using Porter X  X  algorithm [ 29 ]. The French version of the algorithm differs from the English ver-sion only by the set of rules used. We chose to implement the Tf.Idf feature, which is easy to compute and provides very good results in practice [ 1 , 3 , 34 ]. The weight w ij assigned to the term t i of the document d j is defined by the following expression: w Where:  X  N is the number of documents in the database  X  n i isthenumberofdocumentsinwhichtheterm t i appears  X  X req i , j is the number of occurrences of the term t i in the
Each document is represented by a vector of weights w i , The dictionary to refer to for the construction of the feature vector is determined during the learning stage of the system by performing feature selection. 2.1.2 Feature selection It is generally necessary to reduce the size of the feature space because preprocessing produces a high dimensional descrip-tion that consists in several thousands of words. Three com-mon approaches are reported in the Information Retrieval domain [ 45 ]. They are based on the frequency threshold, information gain and the  X  2 measure. For comparison pur-posewithotherstudies[ 17 ],wechoosetousethe  X  2 measure.
Let the categories of the K documents be denoted c , c mated as the ratio of documents in the database that belong to the class c i , and P ( t ) is estimated as the ratio of documents that contain term t . It follows that P ( c i , t ) can be computed by the fraction of documents of the class c i containing the term t . Similarly, P ( c i , t ) is the fraction of documents of the class c i that do not contain the term t .The  X  2 measure is the correlation between a term t and a category c , computed as follows:  X  2 ( t , c
Usually, two scores are used for feature selection based on this measure, they are the following:
The final list of retained terms is composed of those with the N best scores. 2.1.3 Classification The classification of a document in a topic can be per-formed using different classification methods. K nearest neighbors, neural networks, SVM, are some of the most popular approaches [ 17 , 35 , 43 ]. In this study, we retained a K nearest-neighbor classifier for its simplicity and perfor-mance.Weusedtheclassical X  X osine X  X imilaritymeasurethat is the most popular metric defined by the following relation: sim ( q , d j ) =
Where d j and q stand for the vector representations of, respectively, the document d on the learning database and the query document q to be categorized. 2.2 Evaluation The Reuters 21,578 corpus [ 24 ] is used to validate the meth-odology by comparing the performance with those reported in the literature. Then it was possible to evaluate the incom-ing mail document categorization task using the electronic transcriptions of each document with the same system. This experimentation allows the determination of the optimal performance that we expect to achieve on the handwritten documents. 2.2.1 Reuters 21,578 corpus This widely used corpus is a set of 21,578 articles anno-tated according to their topic, among nearly 120 topics. The topic distribution is unbalanced: some topics are represented by over 3,700 articles, while some others are represented by less than 50. A protocol (modApte) describes how to split the database into a learning set and an evaluation set. This gives 7,063 documents in the learning set and 2,742 in the evalu-ation set. After having carried out the preprocessing of the learning database, a vocabulary of 15,453 terms is obtained.
The various parameters of the system are first optimized using the learning set. They are reported in Table 1 and com-pared with the values reported in [ 17 ]. We observe a slight difference between the parameter values, which is certainly due to the character-filtering step.

Table 2 presents the results obtained with our categoriza-tion system as well as those obtained by [ 17 ]. For each of the ten most frequent topics, the Break Even Point (BEP) is reported. This value is obtained when recall equals precision. Let us recall that recall is similar to a detection rate, and precision is similar to a pertinence rate. More formally, one can define recall and precision as: recall = where tp, fp and fn stand, respectively, for true positive , false positive and false negative rates. The micro-average mea-sure corresponds to the computation of the global BEP. This unique measure allows summarizing the system performance by one single measure. The results obtained are very similar to those presented in [ 17 ], and thus validate our categoriza-tion approach. 2.2.2 Incoming mail corpus Performance of the categorization system is now determined on the incoming mail corpus. We use the ground truth of a handwritten mail corpus made of the electronic transcription of each handwritten document. These mails are classified into 43 topics:  X  X tandard cancellation X  (A500),  X  X hanging of bank address X  (A020), etc. The topic A500 ( X  X tandard can-cellation X ) contains over 300 documents whereas topic A020 ( X  X hanging of bank address X ) contains only 30 documents. Note that the annotation of topics comes from a real-world database of handwritten mails addressed to a French com-pany. The learning set contains two-thirds of the documents of each category and the evaluation set contains the remain-ing third.
 The optimal parameters of the system are as follows. The dictionary is composed of nearly 7,000 terms. The lex-icon is reduced to 980 words by removing the words that appear in less than five documents and using the  X  2 mea-sure for term selection. This leads to retain 450 discrimina-tive terms after stemming. Classification is carried out using a 5-nearest-neighbor classifier. Table 3 reports the results obtained on the 5 most frequent classes. The micro-average is computed using all the categories, which allows a global evaluation of the system. It appears that some topics are more difficult to model than others. The  X  X nformation requests X  (A240 and A255) are not as well classified as  X  X ancella-tions X  (A500 and A502). This can be due to more variability in this class. In fact, the two  X  X ancellation X  topics are very well defined whereas the  X  X nformation requests X  are more heterogeneous. With an equivalent amount of samples, it is not surprising that topic A502 is better recognized than topic A240. Finally, these results highlight the optimal categori-zation performance that can be expected on this particular corpus assuming that perfect recognition of the informative handwritten keywords can be achieved. The following sec-tion will now consider the adaptation of this categorization system for handwritten documents. 3 Recognition of handwritten words Despite the success of some very specific industrial applica-tions such as the reading of postal addresses or bank checks, off-line handwriting recognition remains an open problem.
From a methodological point of view, one can distinguish two major approaches in the literature [ 21 , 25 , 28 , 44 ]. The lexicon directed approaches, where the recognition process takes its decision at the lexical level only by discriminat-ing the words that belong to the lexicon. The lexicon free approaches, where the decision comes at the character level. In this case the lexicon is used in the post-processing phase to correct the character recognition errors. Beyond the lexi-con aspects, we can distinguish two main categories. The first one refers to the holistic approaches that consider the word as an indivisible entity. Words are recognized using global fea-tures extracted on the whole shape of the word. This kind of approach depends on a static and often small lexicon. Note that word spotting approaches generally fall into this cate-gory.Thesecondcategoryofapproachesreferstotheanalytic methods, where words are recognized through their constitu-it is possible to model any word and thus any lexicon during the recognition phase. Among the analytic approaches, we can then distinguish the implicit and explicit segmentation approaches. Explicit segmentation methods introduce a seg-mentation stage that proposes several character segmentation hypotheses. These hypotheses are then validated by the rec-ognition stage. Inversely, implicit segmentation approaches do not introduce any complex (adhoc) segmentation stage and they let the recognition process find the best segmen-tation into characters. Most of the recent approaches fall into this last category of methods by relying on the Hid-den Markov Models [ 15 ], including Vinciarelli X  X  work for noisy text categorization. This success is mainly due to the relative ease of implementation of the approach, as opposed to the explicit segmentation, even if one major well-known drawback of Hidden Markov Models is their low capacity to discriminate between classes. In fact, Hidden Markov Mod-els are generative models that are trained class by class by maximizing the likelihood of each training dataset (one per class). Some recent approaches have therefore propose the use of recurrent networks [ 14 ] that use discriminative train-ing to overcome this drawback, and this has proven to be efficient.

In this work, the handwritten word recognizer uses a lex-icon driven analytical approach with explicit segmentation derived from [ 20 ] with discriminative training of character models. Considering the state-of-the-art in handwriting rec-ognition, the proposed approach combines the strength of discriminative training with a limitation due to the segmen-tation stage. We briefly present the word recognition system and refer to the aforementioned paper for more implementa-tion details.

A first preprocessing step is carried out on the binary word images and allows the reduction in writing variabil-ity using slant and skew corrections inspired by [ 19 ]. The segmentation step splits the images into informative zones known as graphemes. The graphemes, or groups of graph-emes, are then submitted to a character recognizer. Finally, the word hypotheses are built by the exploration of the seg-mentation lattice.

The segmentation stage generates hypotheses of segmen-tation points through the analysis of the word contour: each local minimum and maximum of the upper contour of the word is considered as a potential segmentation point [ 27 ].
For each word, a segmentation lattice is built (see Fig. 4 ), containing elementary graphemes at level 1, and all pos-sible aggregations of n adjacent elementary graphemes at level n . The segmentation hypotheses of the first level are likely to be oversegmented, whereas hypotheses of the last levels are likely to be undersegmented. The segmentation statistics demonstrate that the distribution of the number of graphemesisstronglyunbalanceddependingonthecharacter classes. For example, character  X  c  X  is frequently segmented into only one grapheme, whereas character  X  m  X  is frequently segmented into 5 graphemes. We also observed that the max-imum number of levels needed was 7 to prevent from under segmentation. In order to benefit from this apriori knowl-edge, we have chosen to model the segmentation process by a duration statistical model, presented in the following para-graph.

In order to find the best segmentation path in the lattice, each aggregation hypothesis is submitted to a character rec-ognizer which aims at providing the a posteriori probability of the character classes { a , b ,..., z } . To estimate these prob-abilities,severalinformationsourcesarecombinedaccording to the diagram given on Fig. 5 .

Two classifiers are built to exploit complementary infor-mation on each grapheme, at each level. On one hand, struc-tural/statistical information such as curvatures, occlusions, horizontal and vertical strokes is coded into a 117 feature set according to [ 16 ]. On the other hand, directional informa-tion on the contours is coded into a 128 feature set accord-ing to [ 19 ]. These two information sources are exploited by two multi-layer perceptron (MLP) classifiers [ 6 ], called MLP-117 and MLP-128. MLP-117 and MLP-128 produce a posteriori probability estimates 2 of the character classes P ( L two feature vectors. We refer to [ 20 ] for more details concern-ing the production of the a posteriori probability P ( L i
A statistical duration model then combines the segmen-tation information with the character hypothesis. The char-acter distribution over the different segmentation levels is exploited during the recognition step. Let N s be the number of segments of a character, the term P ( L i / N s ) is estimated by counting the number of character samples on the learning database that occur on a particular number of segments N s
The final word recognition stage is performed through the exploration of the recognition lattice using dynamic pro-gramming. The introduction of lexical constraints at this stage reduces the number of solutions during the exploration. The lexicon is modeled by an automaton  X  with N charac-ter states, such as the one shown in Fig. 6 a. The complex-ity of this algorithm is of order ( max-levels  X  N ) 2  X  T ( 7  X  N ) 2  X  T , where T is the length of the lattice. As the complexity is a function of N 2 , a reduction in the number of states will have a large influence on the computation time. This can be done by adopting a tree-structured representation of the lexicon. For example, the model presented in Fig. 6 a can be reduced to the one in Fig. 6 b. In this example, the number of states can be reduced from 41 to 27. The com-plexity is then reduced by a factor of 2.3 whereas the number of states is only reduced by a factor of 1.5.
 Evaluation
The word recognition engine is evaluated using a learn-ing database of 4,600 words and a test database of 500 words, all coming from real incoming handwritten mail doc-uments. Table 4 gives the word recognition rates for MLP-128,MLP-117andtheaveragecombinationofthetwoMLPs.
 Results are presented with and without considering the char-acter duration model, and for different lexicon sizes N by randomly selecting N  X  1 words among a 1,400 word lexi-con (complete lexicon of the word database). We can observe that whatever the size of the lexicon, the combination of the two MLPs improves the word recognition performance sig-nificantly. These results also bring out the relevance of the character duration model. Figures 7 and 8 show some exam-ples of properly recognized words and misrecognized words from different writers. Our results appear to be fairly lower than state-of-the-art approaches such as [ 18 , 23 , 49 ], but our real-world database exhibits multiple significant degrada-tions due to: (i) low resolution (200 dpi) (ii) industrial dig-itizing stage (iii) strongly heterogeneous writing styles (see examples of Fig. 7 ). Let us also notice that our system was probably trained using less data than in some other studies. 4 Keyword extraction in handwritten documents As presented in Sect. 2 , the categorization of handwritten documents is based on a word vector model of discriminative keywords. We must therefore highlight that the main objec-tive of the handwritten word recognition system is to detect and recognize these relevant keywords. As opposed to the full recognition of handwritten content, some studies focus on the detection of keywords that are useful in indexation or categorizationtasks.Thebasicidealiesinthefactthatamajor part of the information contained in a document is useless to capture its overall meaning, e.g., empty or stop words. This strategy known as keyword spotting has been first proposed for printed documents. It became popular in the handwriting recognition community for querying databases of digitized historical documents, for instance, the Georges Washing-ton X  X  manuscripts [ 30 ]. Two different approaches can be distinguished depending on the nature of the documents considered. On the onehand, template-based methods try to match image queries with pre-labeled segmented word image templates [ 4 , 12 , 39 ]. This kind of approach is restricted to querying mono-writer document databases. On the other hand, recognition-based approaches allow to work on more heterogeneous data (from different writers for instance). The recognition process involves a classification stage either as a holistic process [ 33 , 40 ] or as an analytical process involv-ing character models [ 22 , 32 ]. A post-processing stage work-ing on the recognition scores is generally needed to reject false hypotheses. Obviously, this second approach is also subject to the limitations : it is assumed that word bound-aries are known (line segmentation issues are avoided) and rejection is often carried out using a simple threshold on nor-malized scores.

In this article, we introduce a general line model for the extraction of keywords. This analytical model take account of keywords and out of keywords vocabulary. It also introduces an space model between words that allow line segmenta-tion into words. This general model can be parameterized by any keyword lexicon and does not require any specific train-ing when using a new keyword lexicon. This stochastic line model allows keyword detection, line segmentation and out-of-vocabulary word detection in a combined manner by using a dynamic programming decoding algorithm of each line of text. Two statistical models of handwritten lines are proposed in order to proceed to keyword spotting. Both of them rely on dynamic programming and integrate an inter-word space model within the line. They differ in the lexicon they use. The first one corresponds to the full recognition (FR) of the docu-ments using a large lexicon (several thousands of words). The second one is based on a shallow language model dedicated to keyword extraction (KE). It is composed of a lexicon of relevant keywords and a stochastic bi-gram model of charac-ters that accounts for irrelevant words. The two recognition strategies (FR and KE) are evaluated for their capacity to extract the relevant keywords in the handwritten documents as defined in Sect. 2 . 4.1 Full recognition model (FR) Following the notations of Sect. 3 , we consider that each text line is composed of an observation lattice. The recognition of a text line consists in finding the best path in this lattice using dynamic programming and verifying the constraints of the considered model of the line. In this FR strategy, we con-sider that a line of text only contains words from the lexicon that are separated by an inter-word space. The line model is depicted in Fig. 9 where state BL refers to the beginning of the line, state EL refers to the end of the line and state IW refers to the inter-word space state.

We must highlight that the observation lattice contains the hypotheses of the 7 levels of segmentation as described in Sect. 3 , but also the observations characterizing spaces between connected components. The joint probability of the observation lattice and the word sequence can be decom-posed according to the following relation: P ( O , Q  X  ) = where:  X  o  X  M  X  O  X  M  X  N is the number of words in the optimal match  X  Q  X  is the optimal state sequence  X  M i is the model of the i th word  X  IW is the inter-word space
This expression can further be decomposed by rewriting the word likelihood of the optimal path. It finally yields:
P ( O , Q  X  ) = P o  X  M where  X  q  X  i , j is the j th character in the word i  X  o  X  i , j is the observation associated to the j th character of  X  o  X  i , j , j  X  1 is the observation corresponding to the space  X  EL is the state representing an inter-character space in a
We must notice that in the lexicon directed strategy, the character transition probability is equal to 1 if the transition belongs to the automaton (the transition belongs to a word of the lexicon), and to 0 otherwise. Furthermore, if we do not use a language model, the equation simply reduces to: This probability is computed using dynamic programming in the same way as for the recognition of isolated words. 4.2 Keyword extraction model (KE) The main objective of this second model is to limit the size of the vocabulary to the relevant keywords only. We expect to benefit from the reduced size of the keyword vocabulary so as to improve the recognition performance of the relevant information. We must, however, build a model of the irrel-evant information so as to consider the set of all the other words that are irrelevant for the categorization task. This later model will act as a rejection model for the recognition system. It is a model of irrelevant words or out of keyword vocabulary words. Similar approaches have been proposed for speech processing [ 46 ]. We can consider that a line of text is a sequence of relevant and irrelevant words. These words are naturally separated by a space. Figure 10 illustrates the line model used by the KE strategy. This Figure highlights the competition of the relevant lexicon model developed in the FR strategy with the model of irrelevant words that we clarify now.

The irrelevant lexicon is composed of many words because it is potentially made of all the words of the language except the relevant keywords. We have chosen to use a char-acter bi-gram stochastic model. More precisely, this model is composed of 28 states among which 26 states correspond to characters. In addition, an initial and a final Non-Lexi-con states are considered, modeling the beginning and the end of out-of-vocabulary words. Probabilities of bi-grams of characters can either be all equal or determined on a set of examples. We can now clarify the expression of the joint probability of the observation lattice and the best word sequence conforming to this KE model. At first, let us con-sider the observation sequence that corresponds to the i th word in the observation sequence. Its likelihood is expressed in two different ways depending on whether word i is part of the relevant lexicon or not.
 P ( O  X  Let us define:  X  = P ( M
Then the joint probability of the best observation sequence on a whole line is written as follows: P ( O , Q  X  ) =
Once again, this quantity can be computed using dynamic programming on each of the observation lattices associated to each line of text. When  X  = 0, one can notice that the model implements the full recognition strategy. When  X  = 1, the model implements a lexicon free recognition strategy. The whole KE model can be viewed as a model that puts in parallel models of the relevant keyword lexicon with the out-of-vocabulary word model that simply acts as a rejection model. The implementation of these two keyword extraction strategies (FR and KE) is depicted in the next subsection. Experimental results are presented in Sect. 5 . 4.3 Description of the keyword extraction system At first, layout analysis allows the segmentation of the docu-ment into lines of text. Once layout analysis has been carried out, additional preprocessing steps help the recognition pro-cess (slant correction, diacritic filtering). Derived from these pre-processing steps, each line of text is represented by an observation lattice. All these pre-processing steps have been described in detail in Sect. 3 and they are directly applied to the set of detected lines. In the following paragraph, we give some details concerning layout analysis and the detection of possible word separators within lines of text. 4.3.1 Layout analysis The line segmentation process is an important and difficult task, mainly due to variable skew angle along the document or even along a text line, and adjacent text lines. The recent handwriting segmentation contest [ 13 ] has shown that many strategies perform well, such as run length analysis [ 37 ], function minimization exploiting the distance between the separators and the local foreground density [ 38 ] or connected component bounding box analysis [ 47 ]. We have chosen a connected component-based approach, which has shown to perform well on real-world, free layout documents [ 8 ]. We now briefly describe this approach.
Lines of text are formed by successive merging of con-nected components based on a distance criterion. It is implemented in three steps after the detection of connected components (Fig. 11 ). The first step detects alignment seeds. Then each alignment seed is extended to its nearest connected component using the following pixel distance: d ( a , b ) = ( x a  X  x b ) 2  X   X ( y a  X  y b ) 2 where a and b are connected components and x and y are their centers of gravity. The parameter  X  allows to weight the horizontal (  X &gt; 1) or vertical direction (  X &lt; 1). The value of  X  has been experimentally fixed to 20. To build these alignments, only the most representative components (of sufficient size) are considered.

Segmentation results are reported in Table 5 . A line is considered as being well segmented if all its components are grouped together. Three types of errors are counted. Over-segmentation is counted if the line is segmented into several alignments. We can observe that nearly 80% of the lines are properly segmented. Concerning segmentation failure (over and under segmentation), only one connected component (therefore one or two words) is often involved. In this sit-uation, the other words of that line can still be recognized. We estimate that nearly 90 X 95% of the document words can be processed following this line segmentation process. The performance of document segmentation into lines of text is far from being perfect but seems sufficient to apply the two keyword extraction strategies that we have presented above. 4.3.2 Estimating inter-word and inter-character space Spaces between two consecutive components of a line are assigned to the corresponding class (IW or IC). The measure iscarriedoutusingminimalEuclideandistance[ 36 ].Toelim-inate the variability between different writers, the distance is normalized in reference to the median value of the width of the elementary graphemes. Two normalized distributions are obtained. Finally, the probability of an inter-word space having a distance d is given by the equation below: P (
IW / d ) = where #IW ( d ) (respectively #IC ( d ) ) matches the propor-tion of inter-word spaces (similarly inter-character spaces) that have a distance equal to d . We determine the same prob-ability of inter-character spaces: P ( IC / d ) = 1  X  P ( IW / d )
Thedistributionsofthesetwoprobabilitiesarerepresented on Fig. 12 . These two a posteriori probabilities are considered as likelihood scores in the observation lattice. The integration of these observations is done with straightforward modifica-tions of the observation lattice. 5 Experimental results In this section, we present the keyword extraction system performance for both FR and KE strategies. 5.1 A document database for evaluating FR and KE A database of incoming handwritten mails has been built for this purpose. A set of 1,100 real documents have been scanned with a resolution of 200 dpi, where the words of text bodies have been manually labeled. Of course, the word database used for training and testing the word recognizer has been design using a different set of documents than the document database. The 1,100 documents are made of over 46,000 word samples that compose a lexicon of 3,700 dis-tinct words. As this is an industrial database coming from real customers, it cannot be diffused, and personal informa-tion has been hidden for this article. Example of documents can be found in Appendix A . One can note the presence of noise due to imperfect numerization and binarization. This noise has been deleted using standard and simple operations such as filtering of too small connected component and math-ematical morphology. 5.2 Performance evaluation of the keyword extraction Let us remind that the first strategy for keyword extraction consists in carrying out the full document recognition (Full Recognition) then to retrieve keywords on the basis of the recognition results. The capacity of the system to detect keywords is directly related to the performance of the text recognition system that works with a large lexicon. For this experimentation, the entire lexicon composed of the 3,700 different entries of the test database is used. In order to depict the recall precision curve, a variable threshold is applied on the recognition scores of the word recognition hypoth-eses. The score of the words is calculated by averaging the output score of the neural network classifier and is normal-ized according to the length of the word.

The second strategy operates with a lexicon of rele-vant keywords and uses the particular strategy developed in Sect. 4.1 . This lexicon has a reduced size that can vary from 46 words up to 980 words. The irrelevant lexicon is modeled in our experimentation using a uniform ergodic stochastic model where parameter  X  varies between 0 . 01 and 0 . 99 so that the whole recall precision curve can be explored. In the various experimentations, the relevant lexicon used is defined as  X  X E n  X  where the value n is the number of words in the keyword lexicon. Figure 13 illustrates the results obtained by keyword extraction on an incoming mail.

Figure 14 presents the set of results for the two strategies examined. These results illustrate the superiority of the key-word extraction strategy as compared to the full recognition strategy and whatever the keyword lexicon. The FR strategy provides 22% recall for 50% precision while working with a 3,700 word lexicon. On the other hand, with a 295 key-word lexicon (lexicon  X  X E 295 X ) the performance is 36% recall with 70% precision. We can see that the performance is rather stable when increasing the lexicon size from 46 to 295 keywords (with the  X  X E 46 X ,  X  X E 96 X ,  X  X E 165 X  and  X  X E 295 X  lexicons). These various lexicon sizes correspond, respectively, to 10, 25, 50 and 100 radicals. These results clearly highlight the contribution of the relevant keyword extraction strategy that allows focusing the recognition sys-tem only on the keyword lexicon while modeling irrelevant information by an ergodic character model.
 5.3 Categorization of handwritten documents In this section, we analyze the interaction of the two sys-tems (keyword extraction and document categorization) in order to optimize the overall performance of the handwritten document categorization. First we compare categorization performance for both FR and KE strategies. Then the role of the keyword lexicon is analyzed in depth regarding both the keyword recognition performance and the categorization performance.

Table 6 presents the categorization results obtained on the database of incoming handwritten mails following keyword extraction (KE 980). A global  X  X EP X  of 62.3% is obtained despite relatively low performance of the key-word extraction system (27% recall and 57% precision). Compared to the ideal categorization system (TRANS) using the ground-truthed document transcription, catego-rization performance only degrades by 14 points. The FR strategy gives an overall BEP of 46.4% that is very low compared to the 62.3% BEP obtained with the KE strategy.

Figure 15 allows a finer analysis of the results. Figure 13 clearly demonstrates that the KE strategy outperforms the FR strategy when considering the categorization task. The break even point only decreases by 14 points when using the KE strategy while the FR strategy loses 30 points. These results confirm the robustness of the categorization system with respect to keyword extraction errors. They also highlight the relevance of the proposed keyword extraction as opposed to the full recognition strategy. In the following section, we study the influence of the lexicon size on the categorization performance.
 5.4 Control of the lexicon size As seen in Sect. 5.2 , the performance of the keyword extrac-tion engine improves when the number of words in the lex-icon decreases. Thus, a trade-off must be found between a reduction in categorization performance caused by an impor-tant lexicon reduction, and a keyword extraction performance gain. To find this trade-off, we can analyze the performance of the system on the ground-truthed transcription database (TRANS) when the size of the lexicon decreases. To achieve this, we have used the categorization system configuration presented in Sect. 2.2.2 (selection of 450 terms by the  X  X nfor-mation Gain X  measure, description of documents by the Td.Idf measure, classification carried out with 10 NN). The different lexicon sizes are determined on a learning data-base using the  X  2 measure. Figure 16 presents the categori-zation performance obtained for different lexicon sizes. We observe that a 75% reduction (from 450 to 100 terms (radi-cals)) results in a slight performance decrease. The lexicon is, however, reduced from 980 to 295 words (70% reduction). To observe a significant performance decrease, we must reach a much reduced lexicon (10 terms/46 words). To allow comparison, the performance obtained using the recognition system (KE 980) is recalled. We observe that it is also lower thantheperformanceofdifferentcategorizationsystemscon-structed with the transcription of the database (TRANS), in a situation of perfect recognition.

Figure 17 presents the handwritten categorization perfor-mance using the KE strategy for the same lexicon sizes. Without modifying the Break Even Point, which remains at approximately 62% in all cases, we observe nonetheless that a reduction in the lexicon causes a strong improvement of categorization performance for all values on the curve infe-rior to the BEP. In fact, with 96 keywords, the performance reaches 90% precision for 40% recall, which is very close to the performance obtained on the ground-truthed annotated database (TRANS 96) without recognition. We are in a sit-uation where categorization performance are very similar to the theoretical performance caused by a decrease in recogni-tion errors due to the reduction in the lexicon size.
However, we observe a performance decrease when the lexicon used for categorization becomes too small (46 key-words). This seems natural because, as the lexicon is sub-stantially reduced, a recognition error has more impact on the document description. We can conclude that the size of the lexicon is a key factor for handwritten document categori-zation. In our application, the best compromise seems to be a lexicon of 100 keywords. In fact, a more important reduction in the lexicon slightly improves the keyword extraction per-formance but it strongly reduces categorization performance.
Having only a single database of ground-truthed anno-tated documents, it is rather difficult at this stage to bring a full answer concerning the optimal performance that can be expected in more general conditions and other categori-zation tasks. A similar study with documents from another area (other terms, other topics, etc.) should allow confirm-ing these results. It seems, however, that below 30% recall for 70% precision in keyword extraction, the categorization performance drops quickly.

Wehavepresentedacompletesystemforhandwrittendoc-ument categorization based on a word extraction strategy rather than a full recognition approach. Apart from the exper-iments which shows that KE outperforms FR approach, we believe that the proposed modelization also outperforms an FR strategy for the following reasons:  X  In KE strategy, the OOV words are really modeled,  X  KE based on our line model is a dynamic approach able  X  As the lexicon size is smaller, the KE strategy is faster
As a conclusion, let us emphasize that a perfect recog-nition would make our approach obsolete. Unfortunately, recent systems are still far from having acceptable recogni-tion results on weakly constrained handwritten documents. 6 Conclusions and future works Building an automatic handwritten document, categoriza-tion system calls upon techniques proposed in several areas of document analysis: automatic document layout analysis for detecting lines of text, handwriting recognition tech-niques for extracting keywords and information retrieval for document categorization. For the first time, a complete categorization system of handwritten documents has been proposed with promising results on a real handwritten docu-ment database.

To overcome the limitations of a full recognition approach based on a large lexicon word recognition strategy, a new information extraction model has been designed, capable of locating and recognizing a restricted set of discriminative keywords. The information extraction method is based on an out-of-vocabulary word model that is able to handle irrele-vant information. We have shown in this study that it leads to better results than a more classical approach based on a full recognition strategy.

This first study raises a set of comments to be put in perspective for further studies concerning handwritten doc-ument processing. It is to be noticed that the keyword extraction strategy developed for the document categoriza-tion could also be used for the information retrieval. This study opens interesting prospects for future applications of the indexation of handwritten documents, regardless of the kind of documents to be processed: commercial documents, such as incoming mail or any other handwritten document with a certain interest like, for example, historical handwrit-ten documents preserved numerically in digital libraries. No doubt that these topics will be addressed by many of the forth-coming researches led by the research community in the field of handwriting recognition.
 Appendix A: Document samples References
