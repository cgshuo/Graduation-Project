
An IR effectiveness measure makes a prediction about the
In this tutorial, we cover IR evaluation methodologies and measures from a user perspective. These measures include precision, recall, MAP [30 X 32,49,50], NDCG [18], ERR [10], and RBP [27]. We discuss efforts to extend these measures to address novelty and diversity [10,12,13], and to incorporate observed user behavior.

We then discuss sources of user behavior that can be used to develop and calibrate user models for IR evaluation [23]. These sources can be divided into two broad classes: lab studies and implicit feedback. Such lab studies include di-rect relevance assessments, think aloud protocols, eye track-ing, as well as the observation of searchers completing search tasks. Implicit feedback requires the observation of users X  X n the wild X , including clickthroughs, dwell times, and mouse movements [29].

Finally, we introduce techniques for developing evaluation methodologies that directly model key aspects of the user X  X  interaction with the system. For example, for a standard web search result page, we might model the searcher X  X  inter-action with the summaries, their probability of clicking on a result, their time to read a page, and other factors. Both benefits and costs must be expressed in meaningful units. For example, benefits might be measured by the number of unique relevant documents seen, while costs might be mea-sure by the time spent interacting with the system. Alter-natively, benefits might be measured in time well spent, i.e., the time spent viewing relevant material [11].

By careful modeling of user behavior, we can evaluate systems not only in terms of their average behavior, but also in terms of variance across a population of users. In developing these measures, we must strike a balance between fidelity and complexity. We cannot model all aspects of user behavior, and simplification is required. Moreover, to make accurate predictions these models must be calibrated against actual user behavior; we provide detailed examples of how to achieve this aim.
We assume an audience of early doctoral students and es-tablished researchers from outside IR. These individuals may have seen basic IR measures, e.g., NDCG from work on web search, but require a broader understanding of the important role that evaluation plays with respect to IR research. As new IR tasks emerge, we hope to equip researchers working on these tasks with the tools to appropriately and mean-ingfully evaluate their efforts. A full bibliography, partialy reproduced below, allows participants to extend their un-derstanding of each topic after the tutorial.
A n outline of topics appears below, organized into four 1. Basic evaluation methodology (a) Cranfield paradigm (b) Test collections (c) Queries and topics (d) Relevance assessment; the pooling method (e) Precision; recall; mean average precision (f) Exercise: computing MAP (g) Significance testing; confidence intervals (h) Limitations on recall estimates (i) TREC and other evaluation experiments 2. A user oriented view (a) User models behind traditional metrics (b) Graded relevance and NDCG (c) The cascade model (d) Rank biased precision; expected reciprocal rank (e) Exercise: computing ERR (f) Expected browsing utility (g) Novelty and diversity; intent aware measures (h) Session based evaluation (i) Passage oriented evaluation; U-measure (j) Incorporating variability in user behavior 3. The role of user studies (a) Lab studies (b) Relevance assessment (c) Relevance, effort, and user satisfaction (d) Exercise: relevance judgments (e) Side-by-side and whole page judgments (f) Observational studies; eye tracking (g) Think aloud protocols (h) On-line evaluation; A/B testing (i) Clicks, skips and other implicit signals (j) Result interleaving 4. Calibration and validation (a) Costs and benefits; simulation (b) Time based calibration (c) Time well spent (d) Exercise: simulating a search task (e) Economic models of search (f) Foraging theory (g) Applications of reinforcement learning Charles Clarke is a Professor in the School of Computer Science at the University of Waterloo, Canada. His research interests include information retrieval, web search, and text data mining. He has published on a wide range of top-ics  X  including papers related to question answering, XML, guage processing  X  as well as the evaluation of information retrieval systems. He was a Program Co-Chair for SIGIR 2007 and SIGIR 2014. He is currently Chair of the SIGIR Executive Committee, and Co-Editor-in-Chief of the Infor-mation Retrieval Journal . He is a co-author of the graduate textbook Information Retrieval: Implementing and Evalu-ating Search Engines , MIT Press, 2010.
Mark Smucker is an Associate Professor in the Depart-ment of Management Sciences at the University of Water-loo. Mark X  X  recent work has focused on making informa-tion retrieval evaluation more predictive of actual human search performance. Mark has been a co-organizer of two TREC tracks, a co-organizer of the SIGIR 2013 workshop on modeling user behavior for information retrieval evaluation (MUBE) and the SIGIR 2010 workshop on the simulation of interaction. He is a recipient of the SIGIR best paper award (2012) for his work with Clarke on the time-based calibra-tion of effectiveness measures. He is also a recipient of the University of Waterloo, Faculty of Engineering X  X  Teaching Excellence Award.
Emine Yilmaz is an Assistant Professor in the Depart-ment of Computer Science University College London and a research consultant for Microsoft Research Cambridge. She is the recipient of the Google Faculty Award in 2014/15. Her main interests are evaluating quality of retrieval sys-tems, modeling user behavior, learning to rank, and infer-ring user needs while using search engines. She has pub-lished research papers extensively at major information re-trieval venues such as SIGIR, CIKM and WSDM. She has previously given several tutorials on evaluation at the SI-GIR 2012 and SIGIR 2010 Conferences and at the RuS-SIR/EDBT Summer School in 2011. She has also organized several workshops on Crowdsourcing (WSDM2011, SIGIR 2011 and SIGIR 2010) and User Modeling for Retrieval Eval-uation (SIGIR 2013). She has served as one of the organiz-ers of the ICTIR Conference in 2009, as the demo chair for the ECIR Conference in 2013, and as the PC chair for the SPIRE 2015 conference. She is also a co-coordinator of the Tasks Track in TREC 2015. The relationship between IR effectiveness measures and user satisfaction. In 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 773 X 774, 2007. statistical method for system evaluation using incomplete judgments. In 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 541 X 548, 2006. The maximum entropy method for analyzing retrieval measures. In 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 27 X 34, 2005. Yeung, and Ian Soboroff. Reliable information retrieval evaluation with incomplete and biased judgements. In 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 63 X 70, 2007. evaluation. In 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 55 X 62, 2007. user utility: A conceptual framework for investigation. In 34th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 903 X 912, 2011. Minimal test collections for retrieval evaluation. In 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 268 X 275, 2006. Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In 20th ACM International Conference on Information and Knowledge Management , pages 611 X 620, 2011. Yilmaz. Incorporating variability in user behavior into systems based evaluation. In 21st ACM International Conference on Information and Knowledge Management , pages 135 X 144, 2012. Pierre Grinspan. Expected reciprocal rank for graded relevance. In 18th ACM Conference on Information and Knowledge Management , pages 621 X 630, 2009. spent. In 5th Information Interaction in Context Symposium , pages 205 X 214, 2014. Azin Ashkan. A comparative analysis of cascade measures for novelty and diversity. In 4th ACM International Conference on Web Search and Data Mining , pages 75 X 84, 2011. Cormack, Olga Vechtomova, Azin Ashkann, Stefan B  X  uttcher, and Ian MacKinnon. Novelty and diversity in information retrieval evaluation. In 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 659 X 666, 2008. [14] Cyril W. Cleverdon. The Cranfield tests on index language devices. AsLib proceedings , 19:173 X 192, 1967. [15] Gordon V. Cormack, Christopher R. Palmer, and Charles L. A. Clarke. Efficient construction of large test collections. In 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 282 X 289, 1998. [16] Georges Dupret and Mounia Lalmas. Absence time and user engagement: Evaluating ranking functions. In 6th ACM International Conference on Web Search and Data Mining , pages 173 X 182, 2013. [17] Georges Dupret, Vanessa Murdock, and Benjamin Piwowarski. Web search engine evaluation using clickthrough data and a user model. In 16th International WWW Conference Workshop on Query Log Analysis: Social and Technological Challenges , May 2007. gain-based evaluation of IR techniques. ACM Transactions on Information Systems , 20(4):422 X 446, 2002. [19] Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay. Accurately interpreting clickthrough data as implicit feedback. In 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 154 X 161, 2005. [20] Evangelos Kanoulas, Ben Carterette, Paul D. Clough, and Mark Sanderson. Evaluating multi-query sessions. In 34th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 1053 X 1062, 2011. [21] Gabriella Kazai and Mounia Lalmas. eXtended cumulated gain measures for the evaluation of content-oriented XML retrieval. ACM Transactions on Information Systems , 24(4):503 X 542, 2006. [22] Gabriella Kazai, Emine Yilmaz, Nick Craswell, and Seyed M. M. Tahaghoghi. User intent and assessor disagreement in web search evaluation. In 22nd ACM International Conference on Information and Knowledge Management , pages 699 X 708, 2013. [23] Diane Kelly and Cassidy R. Sugimoto. A systematic review of interactive information retrieval evaluation studies, 1967 X 2006. Journal of the American Society for Information Science and Technology , 64(4):745 X 770, 2013. [24] Jinyoung Kim, Gabriella Kazai, and Imed Zitouni. Relevance dimensions in preference-based IR evaluation. In 36th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 913 X 916, 2013. [25] Yiqun Liu, Yupeng Fu, Min Zhang, Shaoping Ma, and Liyun Ru. Automatic search engine performance evaluation with click-through data analysis. In 16th International WWW Conference Workshop on Query Log Analysis: Social and Technological Challenges , May 2007. S trategic system comparisons via targeted relevance judgments. In 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 375 X 382, 2007. precision for measurement of retrieval effectiveness. ACM Transactions on Information Systems , 27(1):1 X 27, 2008. Lalmas. Sound and complete relevance assessment for XML retrieval. ACM Transactions on Information Systems , 27(1):1 X 37, 2008. interleaving for online retrieval evaluation. In 6th ACM International Conference on Web Search and Data Mining , pages 245 X 254, 2013. transformations. In 15th ACM International Conference on Information and Knowledge management , pages 78 X 83, 2006. precision. In 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 689 X 690, 2008. Emine Yilmaz. Extending average precision to graded relevance judgments. In 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 603 X 610, 2010. the bootstrap. In 29th Annual International ACM SIGIR Conference , pages 525 X 532, 2006. retrieval and sessions: A unified framework for information access evaluation. In 36th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 473 X 482, 2013. collections with no system pooling. In 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 33 X 40, 2004. Clough, and Evangelos Kanoulas. Do user preferences and evaluation measures line up? In 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 555 X 562, 2010. retrieval system evaluation: effort, sensitivity, and reliability. In 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 162 X 169, 2005. A comparison of statistical significance tests for information retrieval evaluation. In 16th ACM Conference on Conference on Information and Knowledge Management , pages 623 X 632, 2007. user variance in time-biased gain. In 6th Internation Symposium on Human-Computer Interaction and Information Retrieval , pages 3:1 X 3:10, 2012. [40] Mark D. Smucker and Charles L. A. Clarke.
 Stochastic simulation of time-biased gain. In 21st ACM International Conference on Information and Knowledge Management , pages 2040 X 2044, 2012. [41] Mark D. Smucker and Charles L.A. Clarke.
 Time-based calibration of effectiveness measures. In 35th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 95 X 104, 2012. [42] Ian Soboroff, Charles Nicholas, and Patrick Cahan. Ranking retrieval systems without relevance judgments. In 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 66 X 73, 2001. [43] Karen Sparck Jones and C. J. van Rijsbergen. Information retrieval test collections. Journal of Documentation , 32(1):59 X 75, 1976. [44] Jean Tague-Sutcliffe. The pragmatics of information retrieval evaluation. In Information Retrieval Experiment: Experiment , pages 59 X 102.
 Butterworth-Heinemann, 1981. [45] Jean Tague-Sutcliffe. The pragmatics of information retrieval experimentation, revisited. Readings in Information Retrieval , pages 205 X 216, 1997. [46] Ellen M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing &amp; Management , 36(5):697 X 716, 2000. [47] Ellen M. Voorhees and Donna K. Harman. TREC: Experiment and Evaluation in Information Retrieval . MIT Press, 2005. [48] William Webber, Alistair Moffat, and Justin Zobel. Statistical power in retrieval experimentation. In 17th ACM Conference on Information and Knowledge Management , pages 571 X 580, 2008. [49] Emine Yilmaz and Javed A. Aslam. Estimating average precision when judgments are incomplete. International Journal of Knowledge and Information Systems , 16(2):173 X 211, August 2008. [50] Emine Yilmaz, Evangelos Kanoulas, and Javed A. Aslam. A simple and efficient sampling method for estimating AP and NDCG. In 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 603 X 610, 2008. [51] Emine Yilmaz, Milad Shokouhi, Nick Craswell, and Stephen Robertson. Expected browsing utility for web search evaluation. In 19th ACM International Conference on Information and Knowledge Management , pages 1561 X 1564, 2010. [52] Emine Yilmaz, Manisha Verma, Nick Craswell, Filip Radlinski, and Peter Bailey. Relevance and effort: An analysis of document utility. In 23rd ACM International Conference on Conference on Information and Knowledge Management , pages 91 X 100, 2014. [53] Yuye Zhang, Laurence A. Park, and Alistair Moffat. Click-based evidence for decaying weight distributions in search effectiveness metrics. Information Retrieval , 13:46 X 69, February 2010.
