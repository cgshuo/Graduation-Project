 verify or identify the speaker.
 comparison is analogous to finding the distances between SVM expansion vectors. well in speaker comparison tasks.
 discriminant functions (IPDFs).
 used in an application specific way.
 different combinations of IPDF comparisons and compensati ons. model [1], derivatives.
 We map a sequence of feature vectors, x N x background model (UBM). Here, we use the shorthand x N x For the purpose of this paper, we will assume only the mixture weights,  X  we stack into a parameter vector, a y vectors, a function that compares the two parameter vectors, C ( a natural distances that result in inner products between par ameter vectors. speaker comparison, where L positive definite, usually diagonal, and possibly dependen t on  X  the literature. 4.1 Approximate KL Comparison ( C with parameters a Here, D ( k ) is the KL divergence, and  X  obtain a distance, d the inner product Note that (7) can also be expressed more compactly as where  X  is the block matrix with the  X  in the average mixture weights in (8) being replaced by  X  means. 4.2 GLDS kernel ( C function is defined as The mapping between an input sequence, x N x The corresponding kernel between two sequences is then where In the context of a GMM UBM, we can define an expansion as follow s where p ( j | x Using (13) in (10), we see that where m is the stacked means of the UBM. Thus, the GLDS kernel inner pr oduct is squaring of the p ( j | z approximation of  X  , see [4]. 4.3 Gaussian-Distributed Vectors More formally, suppose that we have a distribution with mean m from a distribution with the UBM mean m , then the discriminant function is [8], where c be dropped. We now apply the discriminant function to anothe r mean vector, m following comparison function 4.4 Other Methods methods or from work in the literature. We describe a few of th ese in this section. Geometric Mean Comparison ( C geometric mean. The resulting kernel is where  X  is the block diagonal UBM covariances.
 Fisher Kernel ( C use a diagonal data-trained covariance term. The resulting comparison function is where  X  is a diagonal matrix acting as a variance normalizer. Linearized Q-function ( C scoring shown in [6]. In this case, the scoring is given as ( m m the means to obtain the following comparison function Note that if we symmetrize C compensation is also asymmetric.
 KL Kernel ( K in the comparison function C recognition [2].
 perform and how they interact with compensation. with oblique and orthogonal projections.
 with respect to a metric, P where DU is a linearly independent set, and the metric is k x  X  y k process of projection, e.g. y = P onto the orthogonal complement of U , U  X  , as Q ularize the projection P remains linear but is no longer a projection.
 D and Q projection onto V is pensation. 5.1 Nuisance Attribute Projection (NAP) an orthogonal projection, m matrix multiply. The resulting projection has D =  X  1 / 2  X  I 5.2 Factor Analysis and Joint Factor Analysis where m experiments.
 replaced by a constant vector representing the true speaker model, m x . Typically, as a simplification, m estimation of x [10].
 works well in practice.
 The standard ML solution to FA [9] for one EM iteration can be w ritten as set of feature vectors. We first let N use relevance MAP with a small relevance factor and F and N to obtain m where  X  for the least-squares problem,  X  x = argmin distributed data [11].
 mean, m approximated by where ity. This can be expressed as a single oblique projection; i. e., the JFA process is 5.3 Comments and Analysis FA it is utterance and UBM dependent.
 applying JFA with linear scoring [6] gives where m respectively; also, D metrics, or asymmetric scoring? similar to [11] but specialized to the factor analysis probl em as in [5]. enroll/verify in different languages.
 Results are in Table 1. In the table, we use the following nota tion, where  X  are the UBM mixture weights,  X  utterance, and  X  the notation D Fisher comparison functions from Sections 4.2, 4.3, and 4.4 , respectively. nuisance subspace, Q functions whose metrics incorporate  X  from the UBM. In terms of best performance, C the 95% confidence interval for 2 . 90% EER is [2 . 6 , 3 . 3]% . We also observe that a nuisance projection with fixed D jection involving a  X  X ariable X  metric, D same error rate. porating them into an SVM system, and hyperparameter traini ng.
