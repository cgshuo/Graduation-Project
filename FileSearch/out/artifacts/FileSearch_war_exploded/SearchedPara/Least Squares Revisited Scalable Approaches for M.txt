 Microsoft Research New York, NY Microsoft Research Cambridge, MA Nikos Karampatziakis NIKOSK @ MICROSOFT . COM Microsoft Cloud and Information Services Lab, Redmond, WA College of Computing, Georgia Tech, Atlanta, Georgia Computer Science Department, Stanford University, CA The aim of this paper is to develop robust and scalable algorithms for multi-class classification problems with k classes, where the number of examples n and the number of features d is simultaneously quite large. Typically, such problems have been approached by the minimization of a convex surrogate loss, such as the multiclass hinge-loss or the multiclass logistic loss, or reduction to convex binary subproblems such as one-versus-rest. Given the size of the problem, (batch or online) first-order methods are typically the methods of choice to solve these underlying optimiza-tion problems. First-order updates easily scale to large d . To deal with the large number of examples, online meth-ods are very appealing in the single machine setting, while batch methods are often preferred in distributed settings. Empirically however, these first-order approaches are of-ten found to be lacking. Many natural high-dimensional data such as images, audio, and video typically result in ill-conditioned optimization problems. While each itera-tion of a first-order method is fast, the number of iterations needed unavoidably scale with the condition number of the data matrix (Nemirovsky &amp; Yudin, 1983), even for simple generalized linear models (henceforth GLM).
 A natural alternative in such scenarios is to use second-order methods, which are robust to the conditioning of the data. In this paper, we present simple second-order methods for multiclass prediction in GLMs. The meth-ods are parameter free, robust in practice and admit easy extensions. As an example, we show a more sophisti-cated variant which learns the unknown link function in the GLM simultaneously with the weights. Finally, we also present a wrapper algorithm which tackles the difficul-ties typically encountered in applying second-order meth-ods to high-dimensional problems. This can be viewed as a block-coordinate descent style stagewise regression pro-cedure that incrementally solves least-squares problems on small batches of features. The result of this overall devel-opment is a suite of techniques that are simple, versatile and substantially faster than several other state-of-the-art opti-mization methods. Finally, we empirically find that in ill-conditioned datasets, such as images, these methods con-sistently outperform first-order methods in generalization. Our Contributions: Our work has three main contri-butions. Working in the GLM framework: E [ y | x ] = g ( Wx ) , where y is a vector of predictions, W is the weight matrix, and g is the vector valued link function, we present a simple second-order update rule. The update is based on a majorization of the Hessian, and uses a scaled ver-sion of the empirical second moment 1 n P i x i x T i preconditioner. Our algorithm is parameter-free and does not require a line search for convergence. Furthermore our computations only involve a d  X  d matrix unlike IRLS and other Hessian related approaches where matrices are O ( dk  X  dk ) for multiclass problems 1 . Theoretically, the proposed method enjoys an iteration complexity indepen-dent of the condition number of the data matrix.
 We extend our algorithm to simultaneously estimate the weights as well as the link function in GLMs under a para-metric assumption on the link function, building on ideas from isotonic regression (Kalai &amp; Sastry, 2009; Kakade et al., 2011). We provide a global convergence guarantee for this algorithm despite the non-convexity of the problem. Practically this enables, for example, the use of our current predictions as features to improve the predictions in subse-quent iterations. Similar procedures are common for binary SVMs (Platt, 1999) and re-ranking(Collins &amp; Koo, 2000). Both the above algorithms are metric free but scale poorly with the dimensionality of the problem.Following ideas in the block-coordinate descent and stagewise regression lit-erature, we generate batches of features (through projec-tion or subsampling) and perform one of the above second-order updates on that batch only. We then repeat this pro-cess, successively fitting the residuals. In settings where the second order information is relevant, such as MNIST and CIFAR-10, we find that stagewise variants can be highly effective, providing orders of magnitude speed-ups over online methods and other first-order approaches. This is particularly noteworthy since we compare a simple MAT-LAB implementation of our algorithms with sophisticated C software for the alternative approaches. In contrast, for certain text problems where the data matrix is well condi-tioned, online methods are highly effective. Notably, we also achieve state of the art accuracy results on MNIST and CIFAR-10, outperforming the  X  X ropout X  neural net (Hin-ton et al., 2012), where our underlying optimization proce-dures are entirely based on simple least squares approaches. These promising results highlight that this is a fruitful av-enue for the development of further theory and algorithms, which we leave for future work.
 Related Work: A large chunk of the work on large-scale optimization builds on and around online and stochastic optimization, leveraging the ability of these algorithms to ensure a very rapid initial reduction of test error (see e.g. (Bottou &amp; Bousquet, 2008; Shalev-Shwartz, 2012)). These methods can be somewhat unsuited though for ill-conditioned problems, leading to recent works on hybrid methods (Shalev-Shwartz &amp; Zhang, 2013; Roux et al., 2012). There has also been a renewed interest in Quasi-Newton methods scalable to statistical problems using stochastic approximation ideas (Byrd et al., 2011; Bor-des et al., 2009). High-dimensional problems have also led to natural consideration of block coordinate descent style procedures, both in serial (Nesterov, 2012) and dis-tributed (Richt  X  arik &amp; Tak  X  ac, 2012; Recht et al., 2011) set-tings. Indeed, in some of our text experiments, our stage-wise procedure comes quite close to a block-coordinate descent type update. There are also related approaches for training SVMs that extract the most information out of a small subset of data before moving to the next batch (Chapelle, 2007; Matsushima et al., 2012; Yu et al., 2012). On the statistical side, our work most directly generalizes past works on learning in GLMs for binary classification, when the link function is known or unknown (Kalai &amp; Sas-try, 2009; Kakade et al., 2011; Friedman, 2001). In the statistics literature, the iteratively reweighed least squares algorithm (IRLS) is the workhorse for fitting GLMs and also works by recasting the optimization problem to a se-ries of least squares problems. However, IRLS can diverge, while the proposed algorithms are guaranteed to make progress on each iteration. In IRLS (and some other ma-jorization algorithms e.g., (Jebara &amp; Choromanska, 2012)) each iteration needs to work with a new Hessian since it depends on the parameters. In contrast, our algorithms use the same matrix throughout their run. We begin with the simple case of binary GLMs, before ad-dressing the more challenging multi-class setting. 2.1. Warmup: Binary GLMs The canonical definition of a GLM in binary classification (where y  X  X  0 , 1 } ) setup posits the probabilistic model where g : R 7 X  R is a monotone increasing function, and w  X   X  R d . To facilitate the development of better algo-rithms, assume that g is a L -Lipschitz function of its uni-variate argument. Since g is a monotone increasing univari-ate function, there exists a convex function  X  : R 7 X  R such that  X  0 = g . Based on this convex function, let us define a convex loss function.
 Definition 1 (Calibrated loss) . Given the GLM (1) , define the associated convex loss Up to constants independent of w , this definition yields the least-squares loss for the identity link function, g ( u ) = u , and the logistic loss for the logit link function, g ( u ) = e / (1 + e u ) . The loss is termed calibrated: for each x , minimizing the above loss yields a consistent estimate of the weights w  X  . Trivially:
E [  X  ` ( w  X  ; ( x,y )) | x ] = E [  X   X ( w  X  T x )  X  xy | x ] where the equality (a) follows since  X  0 = g and E [ y | x ] = g ( w  X  x ) by the probabilistic model (1). Similar observa-tions have been previously noted for the binary case (see Kakade et al. (2011)). Computing the optimal w  X  amounts to using a standard convex optimization procedure. We now discuss these choices for multi-class prediction. 2.2. Multi-class GLMs and Minimization Algorithms The first question in the multi-class case concerns the def-inition of a generalized linear model; monotonicity is not immediately extended in the multi-class setting. Follow-ing the definition in the recent work of Agarwal (2013), we extend the binary case by defining the model: where W  X   X  R k  X  d is the weight matrix,  X  : R k 7 X  R is a proper and convex lower semicontinuous function of k variables and y  X  R k is a vector with 1 for the correct class and zeros elsewhere. This definition essentially cor-responds to the link function g =  X   X  satisfying (maximal and cyclical) monotonicity (Rockafellar, 1966).
 This formulation immediately yields an analogous defini-tion for a calibrated multi-class loss.
 Definition 2 (Calibrated multi-class loss) . Given the GLM (4) , define the associated convex loss The loss function is convex as before and yields, for ex-ample, the multi-class logistic loss when the probabilistic model (4) is a multinomial logit model. It is Fisher consis-tent: the minimizer of the expected loss is W  X  (as in (3)). As before, existing convex optimization algorithms can be utilized to estimate the weight matrix W . First-order meth-ods applied to the problem have per-iteration complexity of O ( dk ) , but can require a large number of iterations as dis-cussed before. Here, the difficulty in utilizing second-order approaches is that the Hessian is of size dk  X  dk (e.g. as in IRLS); any direct matrix inversion method is now much more computationally expensive even for moderate k . Algorithm 1 provides a simple variant of least squares re-gression  X  which repeatedly fits the residual error  X  that exploits the second order structure in x . The algorithm uses a block-diagonal upper bound on the Hessian matrix in order to preserve the correlations between the covariates x , but does not consider interactions across the different classes to have a more computationally tractable update. The algorithm has several attractive properties. Notably, (i) the algorithm is parameter free 2 and (ii) the algorithm only inverts a d  X  d matrix. Furthermore, this matrix is inde-pendent of the weights W (and the labels) and can be com-puted once ahead of time. In that spirit, the algorithm can also be viewed as preconditioned gradient descent , with a block diagonal preconditioner whose diagonal blocks are all equal to the matrix b  X   X  1 . At each step, we utilize the residual error  X  E [( X  y  X  y ) x T ] , akin to a gradient update on least-squares loss. Note the  X  X tepsize X  here is determined by L , a parameter entirely dependent on the loss function and not on the data. For the case of logistic regression, sim-ply L = 1 satisfies this Lipchitz constraint. Also observe that for the square loss, where L = 1 , the generalized least squares algorithm reduces to least squares (and terminates in one iteration).
 We now describe the convergence properties of Algo-rithm 1. The results are stated in terms of the sample loss The following additional assumptions regarding the link function  X   X  are natural for characterizing convergence rates. Assuming that the link function g =  X   X  is L -Lipschitz amounts to the condition If we want a linear convergence rate, we must further as-sume  X  -strong monotonicity, meaning for all u,v  X  R k : Theorem 1. Define W  X  = arg min W ` n ( W ) . Suppose that the link function  X   X  is L -Lipschitz (8) . Using the gen-eralized Least Squares updates (Algorithm 1) with W 0 = 0 , Algorithm 1 Generalized Least Squares Input: Initial weight matrix W 0 , data { ( x i ,y i ) } , Lipschitz constant L , link g =  X   X  .

Define the (vector valued) predictions  X  y ( t ) i = g ( W and the empirical expectations: repeat until convergence then for all t = 1 , 2 ,... If, in addition, the link function is  X  -strongly monotone (9) and let  X   X  = L/ X  . Then The proof rests on demonstrating that the block-diagonal matrix formed by copies of L b  X  provides a majorization of the Hessian matrix, along with standard results in convex optimization (see e.g. (Nesterov, 2004)) and is deferred to the long version (Agarwal et al., 2013). Also, observe that the convergence results in Theorem 1 are completely inde-pendent of the conditioning of the data matrix b  X  . Indeed they depend only on the smoothness and strong convexity properties of  X  which is a function we know ahead of time and control. This is the primary benefit of these updates over first-order updates.
 In order to understand these issues better, let us quickly contrast these results to the analogous ones for gradi-ent descent. In that case, we get qualitatively simi-lar dependence on the number of iterations. However, in the case of Lipschitz  X   X  , the convergence rate is convergence rate is slowed down by factors depending on the singular values of the empirical covariance in both the cases. Similar comparisons can also be made for acceler-ated versions of both our and vanilla gradient methods. Algorithm 2 Calibrated Least Squares Input: Initial weight matrix W 0 , set of calibration func-tions G = { g 1 ,...g m } ( g i : R k  X  R k )
Initialize the predictions:  X  y (0) i = W 0 x i repeat until convergence 2.3. Unknown Link Function for Multi-class The more challenging case is when the link function is un-known. Our approach will generalize the Isotron algorithm for the binary case (Kalai &amp; Sastry, 2009). In particular we will iterate between fitting y and finding the best link function to calibrate our predictions. This raises two main difficulties: the statistical one of how to restrict the com-plexity of the class of link functions and the computational one of ensuring that this procedure convergences globally. With regards to the former, a natural restriction is to con-sider the class of link functions realized as the derivative of a convex function in k -dimensions. This naturally extends Isotron but, unfortunately, this is an extremely rich class; the sample complexity of estimating a uniformly bounded convex, Lipschitz function in k dimensions grows expo-nentially with k (Bronshtein, 1976). To avoid this curse of dimensionality, assume that there is a finite basis G such that g  X  1 = (  X   X )  X  1  X  lin ( G ) , (  X   X )  X  1 is the functional inverse of  X   X  . Without loss of generality, we also assume that G always contains the identity function. We do not consider the issue of approximation error here.
 Before presenting the algorithm, let us provide some more intuition about our assumption g  X  1 = (  X   X )  X  1  X  lin ( G ) . Clearly the case of G = g  X  1 for a fixed function g puts us in the setting of the previous section. More generally, let us consider that G is a dictionary of p functions so that g GLM (4), this yields an overall linear-like model 3 If we let p = k and G i ( y ) be the i th class indicator y the above equation boils down to meaning that an unknown linear combination of the class-conditional probabilities is a linear function of the data. More generally, we consider G i to also have higher-order monomials such as y 2 i or y 3 i so that the LHS is some low-degree polynomial of the class-conditional probability with unknown coefficients.
 Now, the computational issue is to efficiently form accurate predictions (as in the binary case (Kalai &amp; Sastry, 2009), the problem is not convex). We now describe a simple strat-egy for simultaneously learning the weights as well as the link function, which not only improves the square loss at every step, but also converges to the optimal answer, and empirically in very few steps. The strategy maintains two maintains our current predictions  X  y ( t ) i  X  R k for each data point i = 1 , 2 ,...,n . After initializing all the predictions and weights to zero, the updates shown in Algorithm 2 in-volve two alternating least squares steps. The first step fits the residual error to x using the weights W t . The second step then fits y to functions of updated predictions, i.e. to G (  X  y ( t ) ) . Finally, we project onto the unit simplex to obtain the new predictions, which can only decrease the squared error and can be done in O ( k ) time (Duchi et al., 2008). In the context of the examples of G i mentioned above, the algorithm boils down to predicting the conditional proba-bility of Y = i given x , based not only on x , but also on our current predictions for all the classes (and higher de-gree polynomials in these predictions).
 For the analysis of Algorithm 2, we focus on the noiseless case to understand the optimization issues. Analyzing the statistical issues, where there is noise, can be handled using ideas in (Kalai &amp; Sastry, 2009; Kakade et al., 2011). Theorem 2. Suppose that y i = g ( W  X  x i ) and that the link function g =  X   X  satisfies the Lipschitz and strong monotonicity conditions (8) and (9) with constants L and  X  respectively. Suppose also that  X   X (0) = 11 /k . Using the (calibrated) Least Squares updates (Algorithm 2) with W 0 = 0 , for all t = 1 , 2 ,... we have the bound Algorithm 3 Stagewise Regression Input: data { ( x i ,y i ) } , batch generator GEN, batch size p , iterations T
Initialize predictions:  X  y (1) i = 0 for t = 1 ,...,T do end for We again emphasize the fact that the updates (10) and (11) only require the solution of least-squares problems in a sim-ilar spirit as Algorithm 1. Finally, we note that the rules to compute predictions in our updates( (10) and (11)) require previous predictions (i.e. the learned model is not proper in that it does not actually estimate g , yet it is still guaranteed to make accurate predictions). 2.4. A Scalable Wrapper Algorithm When the number of features is large, any optimization al-gorithm that scales superlinearly with the problem dimen-sion faces serious computational issues. Here we propose a simple way to scale the previous algorithms to high di-mensional problems. We adopt a block coordinate descent style approach. To keep the presentation fairly general, we assume that we have an algorithm GEN that returns a small set of m features, where m is small enough so that least squares fitting with m features is efficient (e.g. we typi-cally use m  X  1000 ). The GEN procedure can be as sim-ple as sampling m of the original features (with or with-out replacement) or other schemes such as random Fourier features (Rahimi &amp; Recht, 2007). We call GEN and fit a model on the m features using either Algorithm 1 or Al-gorithm 2. We then compute residuals and repeat the pro-cess on a fresh batch of m features returned by GEN. In Algorithm 3 we provide pseudocode for this stagewise re-gression procedure. We stress that this algorithm is purely a computational convenience. It can be thought as the algo-rithm that would result by a block-diagonal approximation of the second moment matrix  X  (not just across classes, but also groups of features). Algorithm 3 bears some resem-blance to boosting and related coordinate descent methods, with the crucial difference that GEN is not restricted to searching for the best set of features. Indeed, in our exper-iments GEN is either sampling from the features without replacement or randomly projecting the data in m dimen-sions and transforming each of the m dimension by a sim-ple non-linearity. Despite its simplicity, more work needs to be done to theoretically understand the properties of this variant as clearly as those of Algorithm 1 or Algorithm 2. Practically, stagewise regression can have useful regular-ization properties but these can be subtle and greatly de-pend on the GEN procedure. In text classification, for ex-ample, fitting the most frequent words first leads to better models than fitting the least frequent words first. We consider four datasets MNIST, CIFAR-10, 20 News-groups, and RCV1 that capture many of the challenges en-countered in real-world learning tasks. We believe that the lessons gleaned from our analysis and comparisons of per-formance on these datasets apply more broadly.
 For MNIST, we compare our algorithms with a variety of standard algorithms. Both in terms of classification ac-curacy and optimization speed, we achieve close to state of the art performance among permutation-invariant meth-ods ( 1 . 1% accuracy, improving upon methods such as the  X  X ropout X  neural net). For CIFAR-10, we also obtain nearly state of the art accuracy ( &gt; 85% ) using standard features. Here, we emphasize that it is the computational efficiency of our algorithms which enables us to achieve higher accuracy without novel feature-generation.
 The story is rather different for the two text datasets, where the performance of our methods is less competitive with online approaches, though we do demonstrate substantial reduction in error rate in one of the problems. As men-tioned in the introduction, second order learning is less ap-pealing for well conditioned datasets such as text. 3.1. MNIST Nonlinear classifiers are needed to achieve state-of-the-art performance in MNIST dataset. Although it only con-tains 60K data points, the requirement for nonlinearity makes this dataset computationally challenging. For in-stance, a nonlinear support vector machine with a Gaus-sian RBF kernel needs to manipulate a 60K  X  60K kernel matrix; requiring substantial computation and memory in most modern desktop machines. Hence we use an explicit feature representation and train our classifiers in the pri-mal space. Specifically we construct random fourier fea-tures which are known to approximate the Gaussian kernel k ( x,x 0 ) = exp(  X  X  x  X  x 0 k 2 /s ) (Rahimi &amp; Recht, 2007) (more details in the long version (Agarwal et al., 2013)). We start by comparing linear regression with Algorithm 1 (Logistic), and Algorithm 2 (Calibration). For Calibration, we use a basis G ( y ) consisting of y , y 2 and y 3 (applied elementwise to the vector y ). We compare these algorithms on raw pixel features, as well as small number of random Fourier features described above. As seen in Table 1, the performance of Logistic and Calibration seems similar and consistently superior to plain linear regression.
 Next, we move to improve accuracy by using Algorithm 3, which allows us to scale up to larger number of random Fourier features. Concretely, we fit blocks of features (ei-ther 512 and 1024) with Algorithm 3 with three alternative update rules on each stage: linear regression, Calibration, and Logistic (50 inner loop iterations). Our calibrated vari-ant again uses the functions y , y 2 and y 3 of previous pre-dictions as additional features in our new batch of features. Our next experiment demonstrates that all three (extremely simple and parameter free) algorithms quickly achieve state of the art performance. Figure 1 shows the relation between feature block size, classification test error, and runtime for these algorithm variants. Importantly, while the linear and Calibration algorithms do not achieve as low an error for a fixed feature size (cf. Table 1), they are faster to optimize and are more effective overall. Recall that we used 50 in-ner loop iterations for Logistic hence it appears to not make as much progress as the other methods. Given enough time however, Logistic can reach a test error of 1.1% (not shown). In general we find that linear regression and rel-atively small size feature batches achieves better runtime and error trade-off than logistic regression while Calibra-tion further improves upon these results.
 In our next experiment, we compare the previous three vari-ants to other state-of-the-art algorithms in terms of classifi-cation test error and runtime (Figure 2(a) and (b)). We used the default convergence criteria for these other implemen-tations. Perhaps, given enough time, they could also reach Algorithm Linear Logistic (poly.) Calibration
Raw pixels 14.1% 7.8% 8.1% 4000 dims 1.83 % 1.48 % 1.54 % 8000 dims 1.48% 1.33% 1.36% the same quality solutions that our methods achieve, but they seem to have been tuned for well-conditioned prob-lems. The comparison includes VW, and six algorithms implemented in Liblinear (Fan et al., 2008) (see figure cap-tion). We searched for regularization paremeters. We timed these algorithms to measure their computation time, rather than their loading of the features (which can be rather large, making it very time consuming to run these experiments); our stagewise algorithms generate new features on the fly so this is not an issue (further discussion in the long ver-sion).
 From Figure 2 (a) and (b), Logistic is competitive with all the other algorithms, in terms of its error (while for lower dimensions linear and Calibration fared a little worse). In comparison to VW and Liblinear, the generalization error of our methods drops quickly as more features are added. We suspect that more features make the conditioning of the data worse and the first order methods have to give up almost all of the improvement in approximation error as excess optimization error. This raises an interesting issue as sometimes these methods are believed to be  X  X xact X  (in contrast to our stagewise approach which is obviously an approximate optimizer). In practice, bad enough condition-ing can completely defeat first order methods, which may be forced to terminate early based on lack of progress. Fi-nally, all of our algorithms were substantially faster. (Note the logarithmic scaling of the runtime axes).
 In conclusion, our methods produce models with a test er-ror of 1.1% while none of the competitors achieve this test error (only L1-L2 SVM comes close) Runtime wise, the stagewise linear and Calibration variants are extremely fast, consistently at least 10 times faster than the other highly optimized algorithms. This is particularly notable given the simplicity of this approach. 3.2. CIFAR-10 The CIFAR-10 dataset is more challenging and many im-age recognition algorithms have been tested (primarily il-lustrating different methods of feature generation; our work instead focuses on the optimization component). Algo-rithms such as the  X  X ropout X  and  X  X axout X  (Hinton et al., 2012; Goodfellow et al., 2013) achieve accuracies of 84% and 87% without dataset augmentation (through jitter or other transformations). We are able to robustly achieve over 85% accuracy with linear regression on standard con-volution features without dataset augmentation.
 Figure 3 illustrates the performance when we use two types of features: convolutions with random masks, or K-means masks (as in (Coates et al., 2011), though we do not use contrast normalization). The induced representations are ill-conditioned and fitting them with Liblinear is extremely slow. Our methods are simple and easy to embed in a tight loop together with this (or any) feature generation. We find that using only about 400 filters, along with poly-nomial features, is sufficient to obtain over 80% accuracy very quickly. Hence, using thousands of generated fea-tures, it is rather fast to build multiple models with disjoint features and model average them, obtaining 85% accuracy. 3.3. Well-Conditioned Problems We close by observing that in certain cases, it might be wasteful to employ our methods. To illustrate, we exam-ine two popular multiclass text datasets: 20 newsgroups 4 (henceforth NEWS20), which is a 20 class dataset and a four class version of Reuters Corpus Volume 1 (Lewis et al., 2004) (henceforth RCV1). We use a (log) term fre-quency representation of the data (more details in the long version). These data are sparse and very well conditioned: The ratio of the 2nd singular value to the 1000th one (as a proxy for the condition number) is 19.8 for NEWS20 and 14 for RCV1. In contrast, for MNIST, this number is about 72000 (computed with 3000 random Fourier features). Fig-ure 4 shows the respective normalized spectra.
 As expected, online first-order methods (VW) fare far more favorably in this setting, as seen in Table 2. We use a simple greedy procedure for our stagewise ordering (as discussed in the long version, though random also works well). Note that this data is well suited for online methods: it is sparse (making the updates cheap) and well conditioned (mak-ing the gap in convergence between first and second order methods small). Developing hybrid approaches applicable to both cases is an interesting direction. We presented a suite of fast and simple algorithms for tack-ling large-scale multiclass prediction problems. We stress that the key upshot of the methods developed in this work is their conceptual simplicity and ease of implementation. Indeed these properties make the methods quite versatile and easy to extend in various ways. We showed an instance of this in Algorithm 2. Similarly, it is straightforward to develop accelerated variants (Nesterov, 2004), by using the distances defined by the matrix b  X  as the prox-function in Nesterov X  X  work. These variants enjoy the usual improve-ments of O (1 /t 2 ) iteration complexity in the smooth and  X   X   X  dependence in the strongly convex setting, while re-taining the metric-free nature of Algorithm 1.
 It is also easy to extend the algorithms to multi-label set-tings, with the only difference being that the vector y now lives on the hypercube instead of the simplex. This amounts to a minor modification of (11) in Algorithm 2. Overall, we believe that our approach revisits many old and deep ideas to develop algorithms that are practically very effective. We believe that it will be quite fruitful to under-stand these methods better both theoretically and empiri-cally in further research.

