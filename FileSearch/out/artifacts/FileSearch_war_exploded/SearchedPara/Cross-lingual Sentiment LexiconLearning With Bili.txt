 The Hong Kong Polytechnic University Microsoft Research The Hong Kong Polytechnic University Microsoft Research Microsoft Research sentiment lexicons. We formalize the task as a learning prob lem on a bilingual word graph, in which the intra-language relations among the words in the same language and the inter-language relations among the words between different langu ages are properly represented. language. Particularly, we show that both synonym and anton ym word relations can be used to build the intra-language relation, and that the word alig nment information derived from classification. 1. Introduction
A sentiment lexicon is regarded as the most valuable resourc e for sentiment analysis (Pang and Lee 2008), and lays the groundwork of much sentimen t analysis research, for example, sentiment classification (Yu and Hatzivassilo glou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid m anually annotating sentiment words, an automatically learning sentiment lexi con has attracted consider-able attention in the community of sentiment analysis. The e xisting work determines word sentiment polarities either by the statistical inform ation (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff,
Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semanti c information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Takamura, Inui, and Okumura 2005; Rao and Ravichandran 200 9). However, current work mainly focuses on English sentiment lexicon generatio n or expansion, while sentiment lexicon learning for other languages has not been well studied. which aims to generate sentiment lexicons for a non-English language (hereafter re-ferred to as  X  X he target language X ) with the help of the avail able English sentiment lexicons. The underlying motivation of this task is to lever age the existing English sentiment lexicons and substantial linguistic resources t o label the sentiment polarities of the words in the target language. To this end, we need an app roach to transferring the sentiment information from English words to the words in the target language.
The few existing approaches first build word relations betwe en English and the target language. Then, based on the word relation and English senti ment seed words, they determine the sentiment polarities of the words in the targe t language. In these two steps, relation-building plays a fundamental role because it is responsible for the trans-fer of sentiment information between the two languages. Two approaches are often used to connect the words in different languages in the liter ature. One is based on a machine translation (MT) engine as a black box to translate the sentiment words in English to the target language (Steinberger et al. 2011). Th e two approaches in Duh,
Fujino, and Nagata (2011) and Mihalcea, Banea, and Weibe (20 07) tend to use a small set of vocabularies to translate the natural language, whic h leads to a low coverage of generated sentiment lexicons for the target language.
 cross-lingual sentiment lexicon learning. Specifically, w e model this task with a bilin-gual word graph, which is composed of two intra-language subgraphs and an inter-language subgraph. The intra-language subgraphs are used to model th e semantic relations among the words in the same languages. When buildi ng them, we incorporate both synonym and antonym word relations in a novel manner, re presented by positive and negative sign weights in the subgraphs, respectively. T hese two intra-language subgraphs are then connected by the inter-language subgrap h. We propose Bilingual word graph Label Propagation (BLP), which simultaneously t akes the inter-language relations and the intra-language relations into account in an iterative way. Moreover, we leverage the word alignment information derived from a parallel corpus to build the inter-language relations. We connect two words from dif ferent languages that are aligned to each other in a parallel sentence pair. Taking adv antage of a large parallel corpus, this approach significantly improves the coverage o f the generated sentiment lexicon. The experimental results on Chinese sentiment lex icon learning show the ef-fectiveness of the proposed approach in terms of both precis ion and recall. We further 22 evaluate the impact of the learned sentiment lexicon on sent ence-level sentiment clas-sification. When using words in the learned sentiment lexico n as features for sentiment classification of the target language, the sentiment classi fication can achieve a high performance.
 1. We present a generic approach to automatically learning s entiment 2. We build a bilingual word graph by using synonym and antony m word 3. We leverage the word alignment information derived from a large 2. Related Work 2.1 English Sentiment Lexicon Learning
In general, the work on sentiment lexicon learning focuses m ainly on English and can be categorized as co-occurrence X  X ased approaches (Hatziv assiloglou and McKeown 1997; Riloff, Wiebe, and Wilson 2003; Qiu et al. 2011) and sem antic-based approaches (Mihalcea, Banea, and Wiebe 2007; Takamura, Inui, and Okumu ra 2005; Kim and Hovy 2004).
 word according to the statistical information, like the co-occurrence of the word to pre-defined sentiment seed words or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of t he earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the con junction words can convey the polarity relation of the two words they connect. F or example, the conjunction word and tends to link two words with the same polarity, whereas the co njunction word but is likely to link two words with opposite polarities. Their a pproach only considers adjectives, not nouns or verbs, and it is unable to extract adjectives that are not conjoined by conjunctions. Riloff et al. (2003) define se veral pattern templates and extract sentiment words by two bootstrapping approaches. T urney and Littman (2003) calculate the pointwise mutual information (PMI) of a given word with positive and negative sets of sentiment words. The sentiment polarity of the word is determined by average PMI values of the positive and negative sets. To ob tain PMI, they provide queries (consisting of the given word and the sentiment word ) to the search engine.
The number of hits and the position (if the given word is near t he sentiment word) are used to estimate the association of the given word to the s entiment word. Hu and
Liu (2004) research sentiment word learning on customer rev iews and they assume that the sentiment words tend to be correlated with product featu res. The frequent nouns and noun phrases are treated as product features. Then they e xtract the adjective words as sentiment words from those sentences that contain one or m ore product features.
This approach may work on a product review corpus, where one p roduct feature may frequently appear. But for other corpora, like news article s, this approach may not be effective. Qiu et al. (2011) combine sentiment lexicon le arning and opinion target extraction. A double propagation approach is proposed to le arn sentiment words and to extract opinion targets simultaneously, based on eight m anually defined rules. word according to the word semantic relation, like the synon yms of sentiment seed words. The word semantic relation is usually obtained from d ictionaries, for example,
WordNet . 1 Kim and Hovy (2004) assume that the synonyms of a positive (ne gative) word are positive (negative) and its antonyms are negative ( positive). Initializing with a set of sentiment words, they expand sentiment lexicons bas ed on these two kinds of word relations. Kamps et al. (2004) build a synonym graph acc ording to the synonym relation (synset) derived from WordNet. The sentiment pola rity of a word is calculated by the shortest path to two sentiment words good and bad . However, the shortest path cannot precisely describe the sentiment orientation, cons idering there are only five steps between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura et al. (2005) construct a word graph with the gloss of WordNet . Words are connected if a word appears in the gloss of another. The word sentiment p olarity is determined by the weight of its connections on the word graph. Based on Wo rdNet, Rao and
Ravichandran (2009) exploit several graph-based semi-sup ervised learning methods like Mincuts and Label Propagation. The word polarity orien tations are induced by ini-tializing some sentiment seed words in the WordNet graph. Es uli et al. (2006, 2007) and
Baccianella et al. (2010) treat sentiment word learning as a machine learning problem, that is, to classify the polarity orientations of the words i n WordNet. They select seven positive words and seven negative words and expand them thro ugh the see-also and antonym relations in WordNet. These expanded words are then used for training. They train a ternary classifier to predict the sentiment polariti es of all the words in WordNet and use the glosses (textual definitions of the words in WordN et) as the features of classification. The sentiment lexicon generated is the well -known SentiWordNet . 2.2 Cross-Lingual Sentiment Lexicon Learning
The work on cross-lingual sentiment lexicon learning is sti ll at an early stage and can be categorized into two types, according to how they bridge the words in two languages. ing the English sentiment words into Romanian through bilin gual English X  X omanian dictionaries. When confronting multiword translations, t hey translate the multiwords word by word. Then the validated translations must occur at l east three times on the Web. The approach proposed by Hassan et al. (2011) learns sen timent words based on
English WordNet and WordNets in the target languages (e.g., Hindi and Arabic). Cross-lingual dictionaries are used to connect the words in two lan guages and the polarity of a given word is determined by the average hitting time from th e word to the English sentiment word set. These approaches connect words in two la nguages based on cross-lingual dictionaries. The main concern of these approaches is the effect of morphological inflection (i.e., a word may be mapped to multiple words in cro ss-lingual dictionaries). 24
For example, one single English word typically has four Span ish or Italian word forms (two each for gender and for number) and many Russian word for ms (due to gender, number, and case distinctions) (Steinberger et al. 2011). U sually, this approach requires an additional process to disambiguate the sentiment polari ties of all the morphological variants.
 and Wiebe (2010) translate the English sentiment lexicon in to the target language using manually produce two high-level gold-standard sentiment l exicons for two languages (e.g., English and Spanish) and then translate them into the third language (e.g., Italian) via Google Translator. They believe that those words in the t hird language that appear in both translation lists are likely to be sentiment words. T hese approaches connect the words in two languages based on MT engines. The main concern o f these approaches is the low overlapping between the vocabularies of natural doc uments and the vocabular-ies of the documents translated by MT engines (Duh, Fujino, a nd Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage.
 of product reviews. Generally, these studies use semi-supe rvised learning approaches and regard translations from labeled English sentiment rev iews as the training data.
The terms in each review are leveraged as the features for tra ining, which has proven to be effective in sentiment classification (Pang and Lee 200 8). We can regard the task of sentiment lexicon learning as word-level sentiment classi fication. However, for word-level sentiment classification, it is not straightforward t o extract features for a single word. Without sufficient features, it is difficult for these a pproaches to perform well in learning. Another line of cross-lingual sentiment class ification uses Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) or its variants , like Boyd-Graber and
Resnik (2010) or He, Alani, and Zhou (2010). These studies as sume that each review is a mixture of sentiments and each sentiment is a probability ov er words. Then they apply the LDA-like approach to model the sentiment polarity of eac h review. Nonetheless, this assumption may not be applicable in sentiment lexicon l earning because a single word can be regarded as the minimal semantic unit, and it is di fficult, if not impossible, to infer the latent topics from a single word. Recall that dif ferent from the sentiment classification of product reviews where the instances are no rmally independent, words in sentiment lexicon learning are highly related with each o ther, like synonyms and antonyms. Through these relations, the words can naturally form a word graph. Thus we use the graph-based learning approach to leverage the wor d distributions in senti-ment lexicon learning. In the next section, we will introduc e our proposed graph-based cross-lingual sentiment lexicon learning. 3. Cross-Lingual Sentiment Lexicon Learning
In this work, we model the task of cross-lingual sentiment le xicon learning with a bilingual word graph, where (1) the words in the two language s are represented by the nodes in two intra-language subgraphs, respectively; ( 2) the synonym and antonym word relations within each language are represented by the p ositive and negative sign weights in the corresponding intra-language subgraphs; an d (3) the two intra-language subgraphs are connected by an inter-language subgraph. Mat hematically, we build a subgraphs G E = ( X E , W E  X  e W E ) and G T = ( X T , W two subgraphs are connected by the inter-language graph G elements of W E , W T , and W A are positive real numbers, that is, W  X  R + , and e W E and e W T  X  R  X  . Because G incorporates the words in two languages, we call it a Bilingual Word Graph . Specifically, the positive weights, W represent the synonym intra-language relations, and the ne gative weights, e W , represent the antonym intra-language relations. The inte r-language relations, W represent the connections between the words in the two langu ages. For cross-lingual English and X T denotes the unlabeled words in the target language. Given th e labels Y = { y E words X T . In the remainder of this section, we will present the biling ual word graph construction and the algorithm of bilingual word graph labe l propagation. 3.1 Bilingual Word Graph Building With Parallel Corpus and W ord Alignment
We represent the words in English and in the target language a s the nodes of the bilingual word graph. We use the synonym and antonym relatio ns of the words in the same language to build W and e W in the intra-language graph, respectively. In the rest of this section, we will focus on how to build the inter-langu age relation. insert links to the words if there exist entry mappings betwe en the words in bilingual dictionaries (e.g., the English X  X hinese dictionary). Thi s method is simple and straight-forward, but it suffers from two limitations. (1) Dictionar ies are static during a certain period, whereas the sentiment lexicon evolves over time. (2 ) The entries in dictionaries tend to be the expressions of formal and written languages, b ut people prefer using colloquial language in expressing their sentiments or opin ions on-line. These limitations lead to the low coverage of the links from English to the targe t language. An alternative way is to use an MT engine as a black box to build the inter-lang uage relation. One can send each word in English to a publicly available MT engine an d get the translations in the target language. Edges can then be inserted into the gr aph between the English 26 words and their corresponding translations. This approach suffers from the problem of low coverage as well because MT engines tend to use a small set of vocabularies (Duh, Fujino, and Nagata 2011).
 readily available in the MT research community, to build the bilingual word graph. The parallel corpus consists of a large number of parallel sente nce pairs from two different languages that have been used as the foundation of the state-of-the-art statistical MT engines. Like the example shown in Figure 2, the two sentence s in English and Chinese are parallel sentences, which express the same meaning in di fferent languages. We can easily derive the word alignment from the sentence pairs, au tomatically using a state-of-the-art toolkit, like GIZA++ 4 or BerkeleyAligner . 5 In this example, the Chinese word aligned. Similarly the English words best and wishes are both aligned to (wish). the words from the two languages. We are therefore motivated to leverage the par-allel corpus and word alignment to build the bilingual word g raph for cross-lingual sentiment lexicon learning. We take the words from both lang uages in the bilingual parallel corpus as the nodes in the bilingual word graph, and build the inter-language relations by connecting the two words that are aligned toget her in a sentence pair from a parallel corpus. There are several advantages of using a pa rallel corpus to build the inter-language subgraph. First, large parallel corpora ar e extensively used for training statistical MT engines and can be easily reused in our task. T he parallel sentence pairs are usually automatically collected and mined from the Web. As a result, they contain the different and practical variations of words and phrases embedded in sentiment expressions. Second, the parallel corpus can be dynamicall y changed when necessary because it is relatively easy to collect from the Web. Conseq uentially, the novel senti-ment information inferred from the parallel corpus can easi ly update the existing sen-timent lexicons. These advantages can greatly improve the c overage of the generated sentiment lexicon, as demonstrated later in our experiment s. 3.2 Bilingual Word Graph Label Propagation As commonly used semi-supervised approaches, label propag ation (Zhu and
Ghahramani 2002) and its variants (Zhu, Ghahramani, and Laf ferty 2003; Zhou et al. 2004) have been applied to many applications, such as part-o f-speech tagging (Das and
Petrov 2011; Li, Graca, and Taskar 2012), image annotation ( Wang, Huang, and Ding 2011), protein function prediction (Jiang 2011; Jiang and M cQuay 2012), and so forth.
The underlying idea of label propagation is that the connect ed nodes in the graph tend to share the same sentiment labels. In bilingual word graph l abel propagation, the words tend to share same sentiment labels if they are connect ed by synonym relations or word alignment and tend to belong to different sentiment l abels if connected by antonym relations.
 words X . The loss function can be defined as where n and m denote the numbers of English words and words in the target la nguage. means that the prediction could not change too much from the i nitial label assignment.
Similar to Zhou et al. (2004), we define the smoothness function to indicate that if two words are connected by synonym relation or by word alignment , then they tend to share the same sentiment label. The smoothness function can be fur ther represented by two smoothness E intra s ( F ) D AL and D AR are defined as D AL = diag ( diag ( intra-language relations W E and W T , respectively. We then define the distance function to indicate that if two words are connected by the antonym rel ation they tend to belong to different sentiment labels. The distance function can be defined as where e D E and e D T are the degree matrices of the absolute value of the antonym i ntra-ness and the synonym intra-language smoothness, the nearer the words connect with each other, the better performance will be achieved, wherea s for the antonym intra-language distance, the farther the better. The objective fu nctions can be defined as 28
Thus, we define the whole objective function for cross-lingu al sentiment lexicon learning as
To obtain the solution to Equation (5), we differentiate the objective function according to F E and F T , and we have where P  X  is the transpose of the matrix P . The graph Laplacians S synonym intra-language relations are ( I  X  D  X  1 2 E W E D I is the identity matrix. The graph Laplacians e S E and e relations are ( I  X  e D  X  1 2 E e W E e D  X  1 2 E ) and ( I  X  be positive semi-definite (Kunegis et al. 2010). The graph La placian S optimal solutions where M E = 2  X  1 S E  X  2  X  3 e S E + 2  X  I and M T = 2  X  the inverse matrix in Equations (8) and (9), we apply the Jaco bi algorithm (Saad 2003) to calculate the solutions as described in Algorithm 1. In li ne 1, we set the label of the positive seed x i as y L E + i = (1, 0) and the label of the negative seed x set the label Y U E of the unlabeled words as zero, and then generate Y Line 2 sets Y T as zero matrix. In line 3, we compute the matrixes S then compute the matrixes M E and M T . The sentiment information is simultaneously Algorithm 1. Bilingual word graph label propagation
Input : Given G = ( X E  X  X T , W E  X  e W E  X  W T  X  e W T
Output : F T for X T and F E for X E 1. Initialize Y E with the English sentiment seeds 2. Set Y T as zero 3. Calculate S E , e S E , S T , e S T , and S A , then calculate M 4. Loop 7. Until F E and F T convergence propagated through lines 4 X 7 until the predicted labels F unlabeled word x i , if | f ( i , 0)  X  f ( i , 1) | &lt;  X  (  X  is set as 1 . 0 E as negative. 4. Experiment 4.1 Data Sets
We conduct experiments on Chinese sentiment lexicon learni ng. As in previous work (Baccianella, Esuli, and Sebastiani 2010), the sentim ent words in General In-quirer lexicon are selected as the English seeds (Stone 1997 ). From the GI lexicon we collect 2,005 positive words and 1,635 negative words. To bu ild the bilingual word graph, we adopt the Chinese X  X nglish parallel corpus, which is obtained from the news articles published by Xinhua News Agency in Chinese and English collections, using the automatic parallel sentence identification appro ach (Munteanu and Marcu 2005). Altogether, we collect more than 25M parallel senten ce pairs in English and
Chinese. We remove all the stopwords in Chinese and English ( e.g., (of) and am ) together with the low-frequency words that occur fewer than 5 times. After prepro-cessing, we finally have more than 174,000 English words, amo ng which 3,519 words have sentiment labels and more than 146,000 Chinese words fo r which we need to predict the sentiment labels. To transfer sentiment inform ation to Chinese unlabeled words more efficiently, we remove the unlabeled English word s in the word graph (i.e., X U E =  X  ). The unsupervised method, namely, BerkeleyAligner , is used to align the parallel sentences in this article (Liang, Taskar, and Klei n 2006). As an unsupervised method, it does not require us to manually collect training d ata and does not need the complex training processing, and its performance is com petitive with supervised methods. With these two advantages, we can focus more on our t ask of cross-lingual sentiment lexicon learning. Based on the word alignment der ived by BerkeleyAligner , the inter-language W A is initialized with the normalized alignment frequency. Th e English and Chinese versions 6 of WordNet are used to build the intra-language relations W e W , W T , and e W T , respectively. WordNet (Miller 1995) groups words into syn onym sets, called synsets. We collect about 117,000 synsets from the En glish WordNet and about 80,000 synsets from the Chinese WordNet. In total, we obtain 8,406 and 6,312 antonym synset pairs.
 and then determine the word sentiment polarities based on it s scores. We rank the two sets of newly labeled sentiment words according to their polarity scores. The top-ranked Chinese words are shown in Table 1. We manually la bel the top-ranked 1K sentiment words. For P@10K, we sequentially divide the to p 10K ranked list into ten equal parts. One hundred sentiment words are random ly selected from each part for labeling. Similar to the evaluation of TREC Blo g Distillation (Ounis,
Macdonald, and Soboroff 2008), all the labeled words from ea ch approach are used in the evaluation. We then evaluate the ranked lists with two me trics, Precision@K and
Recall. 30 4.2 Evaluation of the Bilingual Word Graph
In this set of experiments, we examine the influence of graph t opologies on sentiment lexicon learning.

Chinese monolingual word graph G T = ( X T , W T  X  e W T ). Because it needs labeled sen-timent words, we incorporate the English labeled sentiment words X language relation W A in the first iteration. Then we set X iterations.
 bilingual word graph. It only involves the inter-language r elation W intra-language relations W E and W T . e W E and e W T are set to be zero. language relation W A , the synonym intra-language relations W antonym intra-language relations e W E and e W T .
 approaches are shown in Figure 3. The figure shows that the app roaches based on the bilingual word graph significantly outperform the one ba sed on the monolingual word graph. The bilingual word graph can bring in more word re lations and accelerate the sentiment propagation. Besides, in the bilingual word g raph, the English sentiment seed words can continually provide accurate sentiment info rmation. Thus we observe the increase in the approaches based on the bilingual word gr aph in term of both precision and recall (Table 2). Meanwhile, we find that addin g the antonym relation in the bilingual word graph slightly enhances precision in top -ranked words and similar findings are observed in our later experiments. It appears th at the antonym relations depict word relations in a more accurate way and can refine the word sentiment scores more precisely. However, the synonym relation and word alig nment relation dominate, whereas the antonym relation accounts for only a small perce ntage of the graph. It is hard for the antonym relation to introduce new relations int o the graph and thus it cannot help to further improve recall. 4.3 Comparison with Baseline and Existing Approaches
In this set of experiments, we compare our approach with the b aseline and existing approaches.
 of a positive (negative) word are always positive (negative ), and the antonyms of a positive (negative) word are always negative (positive). F or the inter-language relation, we regard the Chinese word aligned to positive (negative) En glish words as positive (negative). If a word connects to both positive and negative English words, we regard it as objective. Based on this heuristic, we generate two sets o f sentiment words. unlabeled words based on the mean hitting time to the two sets of sentiment seed words. Given the graph G = ( X E  X  X T , W E  X  e W E  X  W T  X  e W probability from node i to node j as word j . Starting with the word i and ending with the sentiment word k  X  M , the mean hitting time h ( i | M ) can be formally defined as 32
Let M + and M  X  denote the GI positive and negative seeds. If h ( i | M
The generated positive words and negative words are then ran ked according to their polarity scores, respectively.
 adsorption algorithm (Baluja et al. 2008) by adding a new reg ularization term. In particular, besides the positive and negative labels, a dum my label is assigned to each word in the MAD approach. Two additional columns, representing the scores of the dummy label, are added into Y and F , respectively. We denote these two matrices with the dummy labels as  X  Y and  X  F . Meanwhile,  X  R is used to represent the initial dummy scores of all the words. For a word x i , the newly added columns in
Then, the predicted label  X  F i of the word x i is iteratively obtained by  X  1  X  3 and  X  are used to tune the importance of each iteration term. We set  X   X  to 10, and  X  to 0.1, which produces reasonably good results. After propa gation, and  X  f ( i , 1) are used to determine the sentiment polarity of the word x BLP and SOP , the Rule approach learns fewer sentiment words. The coverage of the
Rule approach is inevitably low because many words in the corpus a re aligned to both positive and negative words. For example, in most cases the p ositive Chinese word (or misaligned) to the negative English words, like freak . Under this situation, the word tends to be predicted as objective. In SOP , the positive and negative scores are related to the distances of the word to the positive and negative seed words, and the distance is usually coarse-grained to depict the sentiment polarity. F or example, the shortest path between the word good and the word bad in WordNet is only 5 (Kamps et al. 2004). The Rule and SOP approaches find different sentiment words. We then evaluate the learned test indicates that our approach significantly outperforms the Rule and SOP approaches.
The major difference of our approach is that the polarity inf ormation can be transferred between English and Chinese and within each language at the s ame time, whereas in the other two approaches the polarity information mainly tr ansfers from English to
Chinese and once a word gets a polarity score, it is difficult t o change or refine. The idea of the MAD approach is similar to bilingual graph label propagation, b ut the MAD approach fails to leverage the antonym intra-language rela tion. We observe that the
MAD approach can achieve a comparable result to the BLP approach. MAD can obtain a smoother label score by adding a dummy label. But the dummy la bel does not influence the sentiment labels too much because it is not used in the det ermination of the word sentiment polarity. Besides, MAD cannot deal with the antonym relation. As a result, these experiments demonstrate the overall superiority of o ur approach in cross-lingual sentiment lexicon learning. This also indicates the effect iveness of the BLP approach in
Chinese sentiment lexicon learning. 4.4 Evaluation of the Inter-Language Relation This set of experiments is to examine the ways to build the int er-language relation. and Universal Dictionary (UD). 8 From these dictionaries (both English X  X hinese and
Chinese X  X nglish dictionaries), we collect 41,034 transla tion entries between the English and Chinese words. If the English word x i can be translated to the Chinese word x UD dictionary, w A ( i , j ) and w A ( j , i ) are set to 1.

Google Translator. If the Chinese word x i can be translated to the English word x w we assume that the Chinese word is projected to each word in th e English phrase. To improve the coverage, we translate the English sentiment se ed words with three other methods; they are word collocation, coordinated phrase, an d punctuation, as mentioned in Meng et al. (2012b).
 k . As shown in Figure 5, we find that the alignment-based approa ch outperforms the dictionary-based and MT-based approaches. The reason t hat contributes to this is that we can build more inter-language relations based on wor d alignment, compared 34 with the translation entries from the dictionary and the tra nslation pairs from Google
Translator. For example, the English word move is often translated to (shift) and these word translation pairs, the word move can be also aligned to (plain sailing bon voyage) that is commonly used in Chinese greetin g texts. This translation entry is hard to find in dictionaries or by MT engines. The word s are aligned between the two parallel sentences. Sometimes the word move may be forced to be aligned to in the parallel sentences good luck and best wishes on your career move and alignment, our approach is likely to learn more sentiment wo rd candidates. It is also the reason why the dictionary-based and MT-based approache s learn fewer sentiment words than our approach, as indicated in Table 4. According t o our statistic, on average a Chinese word is connected to 2.3 and 2.1 English words if we b uild the inter-language relations with the dictionary and Google Translator, respe ctively. By building the inter-language relation with word alignment, our approach connects a Chinese word to 16.21 English words an average, which greatly increases t he coverage of the learned sentiment lexicon. 4.5 Evaluation of the Intra-Language Relation The following set of experiments reveals the influence of the intra-language relation. language relations with either English or Chinese WordNet s ynsets. Only the inter-language relation with word alignment is used to buil d the graph. That means W , e W E , W T , and e W T are defined as zero matrixes.
 intra-English relation W E , but the intra-Chinese relation W matrixes.
 intra-Chinese relation W T , but the intra-English relation W matrixes.
 relations, the precision curves of both positive and negati ve predictions increase. This indicates that adding the intra-language relations has a po sitive influence. The im-provement can be explained by the ability of the intra-langu age relations to refine the polarity scores. For example, the English word sophisticated can be aligned to the positive Chinese word (delicate) as well as the negative Chinese word (wily, wicked). In the GI lexicon, the English word sophisticated is labeled as posi-tive. In the bilingual word graph that contains only the inte r-language relations, the negative Chinese word is likely to be labeled as positive. However, with the intra-language relation, the negative Chinese word may connect to the other negative Chinese words, like (foxy); and the Chinese positive word may connect to the other positive Chinese words, like (elaborate). Thus the polarity score of the word can be refined by the intra-languag e relation in each iteration of propagation. Another advantage of the intra-language re lation is that it helps to reduce the noise introduced by the inter-language relation . For example, sometimes the Chinese positive word (help) is misaligned to the negative English word freak by the inter-language relation, but it is also connected to t he synonyms (help) and (salutary) (which are positive) by the intra-language rela tions. The polarity score of the word can be adjusted by the intra-language relation. Thus, thoug h the inter-language relation brings in certain noisy alignm ents, the intra-language re-lation can help to refine the polarity score of the word using i ts intra-language relation. 36 4.6 Sensitivity of Parameter  X  1  X  1 and  X  2 in Equation (3) tune the English and Chinese synonym intra-l anguage prop-agation, while  X  3 and  X  4 in Equation (4) adjust the English and Chinese antonym intra -language propagation. For simplicity, let  X  1 equal  X  2 and let  X   X 
Precision@1K ranges from 0.631 to 0.689 and Recall ranges fr om 0.651 to 0.729 on average. In general, we find that when 1  X   X  3,4 &lt;  X  1,2 4.7 Evaluation on Sentiment Classification
Sentiment classification is one of the most extensively stud ied tasks in the community of sentiment analysis (Pang and Lee 2008). To see whether the pe rformance improvement in lexicon learning also improves the results of sentiment c lassification, we apply the generated Chinese sentiment lexicons to sentence-level se ntiment classification. (Seki et al. 2008, 2009). We extract the Chinese sentences th at have positive, negative, or neutral labels. The numbers of extracted sentences are sh own in Table 5. The learned sentiment words in the Mono and BLP approaches are used as cla ssification features. We implement the following baselines for comparison.
 data set as features. We rank the features according to their frequencies and gradually increase the value of N for the Top-N classification features.
 total of 836 positive words and 1,254 negative words are coll ected from HowNet . tences. The accuracies over N number of features are plotted in Figure 7. Our approach achieves a very promising improvement, although the featur es and the sentences that sentiment lexicon is adaptive and qualitative enough for se ntiment classification. 5. Conclusions and Future Work
In this article, we studied the task of cross-lingual sentim ent lexicon learning. We built a bilingual word graph with the words in two languages and con nected them with the inter-language and intra-language relations. We proposed a bilingual word graph label propagation approach to transduce the sentiment informati on from English sentiment words to the words in the target language. The synonym and ant onym relations among the words in the same languages are leveraged to build the int ra-language relations.
Word alignment derived from a large parallel corpus is used t o build the inter-language relations. Experiments on Chinese sentiment lexicon learn ing demonstrate the effec-tiveness of the proposed approach. There are three main conc lusions from this work.
First, the bilingual word graph is suitable for sentiment in formation transfer and the proposed approach can iteratively improve the precision of the generated sentiment lexicon. Second, building the inter-language relations wi th the large parallel corpus can significantly improve the coverage. Third, by incorporatin g the antonym relations into the bilingual word graph, the BLP approach can achieve an imp rovement in precision.
In the future, we will explore the opportunity of expanding o r generating the sentiment lexicons for multiple languages by bootstrapping.
 Acknowledgments References 38
