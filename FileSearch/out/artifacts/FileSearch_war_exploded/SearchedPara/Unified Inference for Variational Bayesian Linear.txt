 In these models the observations v according to: where N (  X ,  X ) denotes a Gaussian with mean  X  and covariance  X  , and 0 dimensional zero vector. The observation v H . Probabilistically, the LGSSM is defined by: with p ( v implementing the VB method is performing inference over h routines appear, at first sight, not to be applicable. tational cost.
 and demonstrates the ease of applying our VB framework.  X   X  rameters is found by maximizing p ( v For the parameter priors, here we define Gaussians on the colu mns of A and B 3 : p ( A |  X ,  X  H )  X  conjugate priors for general inverse covariances  X   X  1 hyperparameters are then  X   X  = {  X ,  X  } 5 .
 Variational Bayes where H d ( x ) The key approximation in VB is q ( X  , h optimality of F , Our main concern is with the update for q ( h treatments previously presented. Optimally q ( h  X  In addition, optimally, q ( A |  X  carry out the averages in Eq. (2). The further averages over q ( X  to conjugacy. Whilst this defines the distribution q ( h distribution  X  q ( h Mean + Fluctuation Decomposition A useful decomposition is to write and similarly  X  where the parameter covariances are S  X  Inference Using an Augmented LGSSM To represent Eq. (2) as an LGSSM  X  q ( h where U decomposition of S written as an LGSSM  X  q ( h compute q ( h is returned in the mean  X  h T  X  q ( h t |  X  v 1: T ) is computed by calling the FORWARD and BACKWARD procedures. Algorithm 1 with parameters  X  A,  X  B,  X   X  parameters  X  A,  X  B  X  ,  X   X  1b (where U T Computing q ( h 3.1 Relation to Previous Approaches for suitably defined quadratic forms  X  eraging over the parameters A, B,  X  literature, whose properties have been well studied [14]. Figure 1: The structure of the LGSSM for ICA.
  X  Inference on q ( h A small modification of the mean + fluctuation decomposition f or B occurs, namely: where  X  B  X   X   X  W  X  P and S A.1 with the replacement h where U specifying  X  A  X   X  A  X  ,  X   X  constrained parameter case. 4.1 Demonstration As a simple demonstration, we used an LGSSM to generate 3 sour ces s c matrices A c ,  X  = 0 v and  X  To bias the model to find the simplest sources, we used  X  A c  X  0 Figure 2: (a) Original sources s v found with MAP LGSSM. Figure 3: (a) Original raw EEG recordings from 4 channels. (b -e) 16 sources s Bayesian LGSSM. procedure by adding a prior term to the original log-likelih ood log p ( v sources is identified. 4.2 Application to EEG Analysis W the usefulness and applicability of the VB method in a real-w orld situation. models.
 A.1 Determining q ( B |  X  By examining F , the contribution of q ( B |  X  between q ( B |  X  [ X  The mean is given by  X  B  X  = N Determining q ( A |  X  Optimally, q ( A |  X  The mean is given by  X  A  X  = N explicit updates are given below.
 Determining q ( X  For the constraint  X   X  1 Ga ( b 1 , b 2 ) [7], q (  X  ) factorizes and the optimal updates are where G Determining q ( X  Analogously, for  X   X  1 where G Acknowledgments contained herein.
 References
