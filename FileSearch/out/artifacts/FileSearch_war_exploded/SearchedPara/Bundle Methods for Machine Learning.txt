 In recent years optimization methods for convex models have seen significant progress. Starting from the active set methods described by Vapnik [17] increasingly sophisticated algorithms for solv-ing regularized risk minimization problems have been developed. Some of the most exciting recent developments are SVMPerf [5] and the Pegasos gradient descent solver [12]. The former computes gradients of the current solution at every step and adds those to the optimization problem. Joachims [5] prove an O (1 / 2 ) rate of convergence. For Pegasos Shalev-Shwartz et al. [12] prove an O (1 / ) rate of convergence, which suggests that Pegasos should be much more suitable for optimization. In this paper we extend the ideas of SVMPerf to general convex optimization problems and a much wider class of regularizers. In addition to this, we present a formulation which does not require algorithms of the SVMPerf family. Our error analysis shows that the rates achieved by this algorithm are considerably better than what was previously known for SVMPerf, namely the algorithm enjoys O (1 / ) convergence and O (log(1 / )) convergence, whenever the loss is sufficiently smooth. An important feature of our algorithm is that it automatically takes advantage of smoothness in the problem.
 Our work builds on [15], which describes the basic extension of SVMPerf to general convex prob-lems. The current paper provides a) significantly improved performance bounds which match better what can be observed in practice and which apply to a wide range of regularization terms, b) a vari-ant of the algorithm which does not require quadratic programming, yet enjoys the same fast rates of convergence, and c) experimental data comparing the speed of our solver to Pegasos and SVMPerf. Due to space constraints we relegate the proofs to an technical report [13]. Denote by x  X  X and y  X  Y patterns and labels respectively and let l ( x,y,w ) be a loss function which is convex in w  X  W , where either W = R d (linear classifier), or W is a Reproducing Kernel functional which many estimation methods strive to minimize can be written as  X  is cheap to compute and to minimize whereas the empirical risk term R emp ( w ) is computationally expensive to deal with. For instance, in the case of intractable graphical models it requires approx-imate inference methods such as sampling or semidefinite programming. To make matters worse the number of training observations m may be huge. We assume that the empirical risk R emp ( w ) is nonnegative .
 Note that there is no need for R emp [ w ] to decompose into individual losses in an additive fashion. For instance, scores, such as Precision@k [4], or SVM Ranking scores do not satisfy this property. Likewise, estimation problems which allow for an unregularized common constant offset or adaptive derivative of R emp [ w ] with respect to w no more decomposes trivially into a sum of gradients. 3.1 Subdifferential and Subgradient Before we describe the bundle method, it is necessary to clarify a key technical point. The subgradi-ent is a generalization of gradients appropriate for convex functions, including those which are not necessarily smooth. Suppose w is a point where a convex function F is finite. Then a subgradi-ent is the normal vector of any tangential supporting hyperplane of F at w . Formally  X  is called a subgradient of F at w if, and only if, singleton then, the function is said to be differentiable at w . 3.2 The Algorithm Denote by w t  X  W the values of w which are obtained by successive steps of our method, Let a R emp [ w t ] can be written as Note that we do not require R emp to be differentiable: if R emp is not differentiable at w t we simply bound, we may take their maximum to obtain that R emp ( w )  X  max t  X  a t ,w  X  + b t . Moreover, by Algorithm 1 Bundle Method( )
Initialize t = 0 ,w 0 = 0 ,a 0 = 0 ,b 0 = 0 , and J 0 ( w ) =  X   X ( w ) repeat until t  X  virtue of the fact that R emp is a non-negative function we can write the following lower bounds on R emp and J respectively: By construction R t 0  X  R t  X  R emp and J t 0  X  J t  X  J for all t 0  X  t . Define The following lemma establishes some useful properties of  X  t and t .
 is monotonically decreasing with t  X  t +1  X  J t +1 ( w t +1 )  X  J t ( w t )  X  0 . Also, t upper bounds the distance from optimality via  X  t  X  t  X  min t 0  X  t J ( w t 0 )  X  J ( w  X  ) . 3.3 Dual Problem Optimization is often considerably easier in the dual space. In fact, we will show that we need sup w  X  w, X   X  X  X   X ( w ) . If  X   X  is a so-called Legendre function [e.g. 10] the w at which the supremum differentiable and Legendre. Examples include  X   X  (  X  ) = 1 2 k  X  k 2 or  X   X  (  X  ) = P i exp[  X  ] i . Theorem 2 Let  X   X  R t , denote by A = [ a 1 ,...,a t ] the matrix whose columns are the (sub)gradients, let b = [ b 1 ,...,b t ] . The dual problem of Furthermore, the optimal w t and  X  t are related by the dual connection w t =  X   X   X  (  X   X   X  1 A X  t ) . commonly used in SVMs and Gaussian Processes. The following corollary is immediate: Corollary 3 Define Q := A &gt; A , i.e. Q uv :=  X  a u ,a v  X  . For quadratic regularization, i.e. minimize w max(0 , max t 0  X  t  X  a t 0 ,w  X  + b t 0 ) +  X  2 k w k 2 2 the dual becomes This means that for quadratic regularization the dual optimization problem is a quadratic program typically in the order of 10s to 100s, the resulting QP is very cheap to solve. In fact, we don X  X  even need to know the gradients explicitly. All that is required to define the QP are the inner products between gradient vectors  X  a u ,a v  X  . Later in this section we propose a variant which does away with the quadratic program altogether while preserving most of the appealing convergence properties of Algorithm 1. 3.4 Examples Structured Estimation Many estimation problems [14, 16] can be written in terms of a piecewise linear loss function a subdifferential of (8) is given by  X  w l ( x,y,w ) =  X  ( x,y  X  )  X   X  ( x,y ) where y  X  := argmax Since R emp is defined as a summation of loss terms, this allows us to apply Algorithm 1 directly pair and compute the composite gradient vector. This vector is then added to the convex program we have so far.
 binary loss. Effectively, by defining a joint feature map as the sum over individual feature maps and by defining a joint loss  X  as the sum over individual losses SVMPerf performs exactly the same operations as we described above. Hence, for losses of type (8) our algorithm is a direct extension of SVMPerf to structured estimation.
 Exponential Families One of the advantages of our setting is that it applies to any convex loss function, as long as there is an efficient way of computing the gradient. That is, we can use it for cases where we are interested in modeling as Gaussian Process classification and Conditional Random Fields [1]. Such settings have been in graphical models. Choosing l to be the negative log-likelihood it follows that R emp ( w ) = This means that column generation methods are therefore directly applicable to Gaussian Process estimation, a problem where large scale solvers were somewhat more difficult to find. It also shows that adding a new model becomes a matter of defining a new loss function and its corresponding gradient, rather than having to build a full solver from scratch. While Algorithm 1 is intuitively plausible, it remains to be shown that it has good rates of conver-which would make the application infeasible in practice.
 We use a duality argument similar to those put forward in [11, 16], both of which share key tech-niques with [18]. The crux of our proof argument lies in showing that t  X  t +1  X  J t +1 ( w t +1 )  X  J ( w t ) (see Theorem 4) is sufficiently bounded away from 0. In other words, since t bounds the distance from the optimality, at every step Algorithm 1 makes sufficient progress towards the op-timum. Towards this end, we first observe that by strong duality the values of the primal and dual problems (5) and (6) are equal at optimality. Hence, any progress in J t +1 can be computed in the dual.
 Next, we observe that the solution of the dual problem (6) at iteration t , denoted by  X  t , forms a feasible set of parameters for the dual problem (6) at iteration t + 1 by means of the parameterization (  X  in terms of  X  t . Note that, in general, solving the dual problem (6) results in an increase which is larger than that obtained via the line search. The line search is employed in the analysis only for analytic tractability. We aim to lower-bound t  X  t +1 in terms of t and solve the resultant difference equation.
 Depending on J ( w ) we will be able to prove two different convergence results. bounds are better. Finally we prove the convergence rates by solving the difference equation in t . This reasoning leads to the following theorem: Theorem 4 Assume that k  X  w R emp ( w ) k  X  G for all w  X  W , where W is some domain of interest for all  X   X   X   X   X  1  X  A  X   X  where  X   X   X  0 and k  X   X  k 1  X  1 . In this case we have Furthermore, if  X  2 w J ( w )  X  H , then we have Note that the error keeps on halving initially and settles for a somewhat slower rate of convergence difference in the convergence bound for differentiable and non-differentiable losses is that in the former case the gradient of the risk converges to 0 as we approach optimality, whereas in the former case, no such guarantees hold (e.g. when minimizing | x | the (sub)gradient does not vanish at the optimum).
 Two facts are worthwhile noting: a) The dual of many regularizers, e.g. squares norm, squared ` p norm, and the entropic regularizer have bounded second derivative. See e.g. [11] for a discussion and details. Thus our condition  X  2  X   X   X  (  X  )  X  H  X  is not unreasonable. b) Since the improvements decrease with the size of  X  t we may replace  X  t by t in both bounds and conditions without any ill effect (the bound only gets worse). Applying the previous result we obtain a convergence theorem for bundle methods.
 Theorem 5 Assume that J ( w )  X  0 for all w . Under the assumptions of Theorem 4 we can give the following convergence guarantee for Algorithm 1. For any &lt; 4 G 2 H  X  / X  the algorithm converges to the desired precision after steps. If furthermore the Hessian of J ( w ) is bounded, convergence to any  X  H/ 2 takes at most the following number of steps: Several observations are in order: firstly, note that the number of iterations only depends logarithmi-cally on how far the initial value J (0) is away from the optimal solution. Compare this to the result of Tsochantaridis et al. [16], where the number of iterations is linear in J (0) . case. This matches the rate of Shalev-Shwartz et al. [12]. In addition to that, the convergence is O (log(1 / )) for continuously differentiable problems.
 Note that whenever R emp ( w ) is the average over many piecewise linear functions R emp ( w ) behaves essentially like a function with bounded Hessian as long as we are taking large enough steps not to  X  X otice X  the fact that the term is actually nonsmooth.
 Remark 6 For  X ( w ) = 1 2 k w k 2 the dual Hessian is exactly H  X  = 1 . Moreover we know that H  X   X  since  X  2 w J ( w ) =  X  +  X  2 w R emp ( w ) .
 Effectively the rate of convergence of the algorithm is governed by upper bounds on the primal and dual curvature of the objective function. This acts like a condition number of the problem  X  for would have a significant influence on the convergence.
 count does increase with  X  , albeit not as badly as predicted. This is likely due to the fact that the a natural regularizer in addition to the regularization afforded by  X   X [ w ] . updates promise good rates of convergence it is tempting to replace the corresponding step in the bundle update. This can lead to considerable savings in particular for smaller problems, where the time spent in the quadratic programming solver is a substantial fraction of the total runtime. To keep matters simple, we only consider quadratic regularization  X ( w ) := 1 2 k w k 2 . Note that J  X  gradients for the update. To obtain  X  t note that we are computing R emp ( w t ) as part of the Taylor simplified algorithm has essentially the same convergence properties. In this section we show experimental results that demonstrate the merits of our algorithm and its analysis. Due to space constraints, we report results of experiments with two large datasets namely Astro-Physics (astro-ph) and Reuters-CCAT (reuters-ccat) [5, 12]. For a fair comparison with exist-ing solvers we use the quadratic regularizer  X ( w ) :=  X  2 k w k 2 , and the binary hinge loss. In our first experiment, we address the rate of convergence and its dependence on the value of  X  . In Figure 2 we plot t as a function of iterations for various values of  X  using the QP solver at every iteration to solve the dual problem (6) to optimality. Initially, we observe super-linear convergence; this is consistent with our analysis. Surprisingly, even though theory predicts sub-linear speed of convergence for non-differentiable losses like the binary hinge loss (see (11)), our solver exhibits linear rates of convergence predicted only for differentiable functions (see (12)). We conjecture that the average over many piecewise linear functions, R emp ( w ) , behaves essentially like a smooth function. As predicted, the convergence speed is inversely proportional to the value of  X  . Figure 2: We plot t as a function of the number of iterations. Note the logarithmic scale in t . Left: astro-ph; Right: reuters-ccat.
 Figure 3: Top: Objective function value as a function of time. Bottom: Objective function value as of the objective function + 0.001 .
 In our second experiment, we compare the convergence speed of two variants of the bundle method, namely, with a QP solver in the inner loop (which essentially boils down to SVMPerf) and the line search variant which we described in Section 5. We contrast these solvers with Pegasos [12] in the batch setting. Following [5] we set  X  = 10  X  4 for reuters-ccat and  X  = 2 . 10  X  4 for astro-ph. Figure 3 depicts the evolution of the primal objective function value as a function of both CPU time as well as the number of iterations. Following Shalev-Shwartz et al. [12] we investigate the time required by various solvers to reduce the objective value to within 0 . 001 of the optimum. This is depicted as a black horizontal line in our plots. As can be seen, Pegasos converges to this region quickly. Nevertheless, both variants of the bundle method converge to this value even faster (line datasets we tested on). Note that both line search and Pegasos converge to within 0.001 precision rather quickly, but they require a large number of iterations to converge to the optimum. Our work is closely related to Shalev-Shwartz and Singer [11] who prove mistake bounds for online algorithms by lower bounding the progress in the dual. Although not stated explicitly, essentially the same technique of lower bounding the dual improvement was used by Tsochantaridis et al. [16] to show polynomial time convergence of the SVMStruct algorithm. The main difference however is that Tsochantaridis et al. [16] only work with a quadratic objective function while the framework proposed by Shalev-Shwartz and Singer [11] can handle arbitrary convex functions. In both cases, a weaker analysis led to O (1 / 2 ) rates of convergence for nonsmooth loss functions. On the other hand, our results establish a O (1 / ) rate for nonsmooth loss functions and O (log(1 / )) rates for smooth loss functions under mild technical assumptions.
 Another related work is SVMPerf [5] which solves the SVM estimation problem in linear time. SVMPerf finds a solution with accuracy in O ( md/ (  X  2 )) time, where the m training patterns x i  X  R d . This bound was improved by Shalev-Shwartz et al. [12] to accuracy of with confidence 1  X   X  . Their algorithm, Pegasos, essentially performs stochastic (sub)gradient descent but projects the solution back onto the L 2 ball of radius 1 / experiments show, performing an exact line search in the dual leads to a faster decrease in the value of primal objective. Note that Pegasos also can be used in an online setting. This, however, only applies whenever the empirical risk decomposes into individual loss terms (e.g. it is not applicable to multivariate performance scores).
 The third related strand of research considers gradient descent in the primal with a line search to choose the optimal step size, see e.g. [2, Section 9.3.1]. Under assumptions of smoothness and strong convexity  X  that is, the objective function can be upper and lower bounded by quadratic func-tions  X  it can be shown that gradient descent with line search will converge to an accuracy of in O (log(1 / )) steps. The problem here is the line search in the primal, since evaluating the regular-ized risk functional might be as expensive as computing its gradient, thus rendering a line search in the primal unattractive. On the other hand, the dual objective is relatively simple to evaluate, thus making the line search in the dual, as performed by our algorithm, computationally feasible. Finally, we would like to point out connections to subgradient methods [7]. These algorithms are designed for nonsmooth functions, and essentially choose an arbitrary element of the subgradient set r centered around the minimizer of J ( w ) . By applying the analysis of Nedich and Bertsekas [7] to References
