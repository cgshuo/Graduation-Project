 While many studies have been conducted on query under-standing, there is limited understanding on why users start searches and how to predict search intent. In this paper, we propose to study this important but less explored problem. Our key intuition is that searches are triggered by di erent pre-search contexts, but the triggering relations are often hidden. For example, a user may search \bitcoin" because of a news article or an email the user just read, but the system does not know which of the pre-search contexts (the news article or the email) is the triggering source. Following this intuition, we conduct an in-depth analysis of pre-search context on a large-scale user log, which not only veri es the hidden triggering relations in the real world but also identi-es a set of important characteristics of pre-search context and their triggered queries. Since the hidden triggering rela-tions make it challenging to directly use pre-search context for intent prediction, we develop a mixture generative model to learn without any supervision how queries are triggered by di erent types of pre-search context. Further, we discuss how to apply our model to improve query prediction and query auto-completion. Our experiments on a large-scale of real-world data show that our model could accurately pre-dict user search intent with pre-search context and improve upon the state-of-the-art methods signi cantly.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Search Process Keywords: Search Context; Pre-Search Context; Search Intent; Query Auto-Completion; Query Prediction
To improve user search experience, many studies have been proposed to understand user search intent given is-sued queries ( e.g. , query suggestion and query disambigua-tion) via exploring clickthrough data, user search history, and search session context. However, there is very limited work on search intent prediction { predicting what a user is going to search even before the search task starts .
Search intent prediction is an important problem, as it will largely improve search experience. First, it could enable \query-less" search ( e.g. , Google Now) to greatly save user e orts. For example, if we could predict that a user is look-ing for Philippine Peso's value, we could directly recommend related queries ( e.g. , \Philippine Peso") or their results once the user opens a mobile search app, without any user input. In addition, it could also improve existing applications such as query auto-completion and query disambiguation. For example, when the user types \P" in a search box, we can accurately suggest query\Peso"or \Philippine Peso"instead of \Pinterest" or \Paypal", which are more likely to be sug-gested by current search engines. When the user searches \Peso" as his query, we could correctly interpret the intent as Philippine Peso instead of Mexico Peso.

Recently Cheng et al. [7] found that many searches are triggered by webpages users browsed, and proposed a model to predict queries triggered by a given browsed webpage. While this work provides a good start for search intent pre-diction, it has critical limitations in reality. In the real-world setting, searches could also be triggered by factors other than browsing, and systems often do not know if the current search will be triggered by the browsed webpage or other factors. Therefore, blindly predicting search intent solely based on browsed webpage is inadequate in practice. In this paper, we study this important problem of predicting search intent in this more realistic setting.
 A Key Insight We hypothesize that searches are triggered by di erent contexts prior to the search activities, which we call pre-search contexts . For example, a user searches \peso", because she reads a news article about \Devalua-tion of Philippine peso" and she is interested in its current value; a user searches \restaurants nearby" as she is visiting a new place and want to taste local food; and a user searches \Lady Gaga tour" as she hears about Lady Gaga's coming concert and wants to book tickets. In these examples, the news article, the new location, and the coming event are the pre-search contexts that trigger users to search. It is clear that pre-search context is very di erent from user search his-tory or search session context, which are explored by many previous studies for understanding search intent.
We emphasize that a pre-search context, by de nition, is just prior to the search but does not necessarily trigger it. This de nition re ects the hidden nature of triggering relations between pre-search context and searches in a real-world setting . That is, while many types of pre-search con-te xt ( e.g. , news articles, new locations, events) could trigger searches, the system does not observe which particular pre-search context triggers the current search. Ideally, if we can capture various types of pre-search context, and identify the particular one that triggers searches each time, we could pre-dict search intent based on the identi ed pre-search context. An In-depth Analysis In Section 3 we conduct an in-depth analysis to understand how pre-search context triggers queries. We focus on a speci c type of pre-search context { the news article a user browsed before the search, because: 1) a large number of search queries are triggered by news pre-search context daily, as we will show in the analysis; 2) it is easier to log browsing activity than other o -line activities in practice; 3) studying news pre-search context will provide guidelines for exploring other pre-search context in the future.

In the analysis, we use large-scale real-world data by join-ing user browsing logs ( i.e. , what news articles users have read) and search logs ( i.e. , what queries users have searched after reading news) from Yahoo!. We rst nd that a sig-ni cant amount of queries are triggered by news pre-search context ( i.e. , tens of millions queries per day), which veri-es the assumption of queries being triggered by pre-search context. More importantly, we discover insightful character-istics of news pre-search context and their triggered queries. Some of our interesting observations are: 1) news articles of-ten trigger new queries ( i.e. , 96% of triggered queries have never been searched by their users before), which are often dicult to predict or understand if only using history data; 2) most of the triggered queries are related to named enti-ties mentioned in the news articles; 3) triggered queries are more likely to appear in the beginning part of those arti-cles; 4) triggered queries could diverge from the main topics of the news articles, which are quite di erent from queries that lead clicks to the articles.

While this analysis shows the great potential of using news pre-search context for predicting search intent, it also vali-dates the hidden nature of triggering relations in the real-world setting as mentioned previously. Based on the data, we estimate a large percent of queries are triggered by other pre-search context instead of news articles. The system does not know if current search will be triggered from the browsed news articles or other pre-search context. This hidden na-ture poses two challenges in reality that, to the best of our knowledge, have not been addressed before: 1) how to pre-dict search intent with hidden triggering relations; 2) if/how can we learn prediction models and avoid using expensive labels for uncovering the hidden triggering relations ( i.e. , identify which pre-search context is the triggering factor). An Intent Prediction Model To address these real-world challenges, in Section 4, we develop a pre-context aware search intent model that learns how queries are triggered by di erent pre-search context with hidden triggering rela-tions without any supervision. More speci cally, we rst introduce the model as a general generative mixture model to principally captures that a search intent could be trig-gered by di erent pre-search context with unknown trigger-ing source. We then customize the expectation-maximization (EM) algorithm to learn the parameters of our model, so that we can take advantage of large-size browsing and search logs that are directly available, avoiding relying on expen-sive manually labeled data. In addition, we discuss how to instantiate our model for query prediction and query auto-completion tasks as concrete examples to validate the gen-erality of our proposed model. We also identify a set of e ective features for our model based on our analysis in Sec-tion 3.
 Comprehensive Experiments Finally, in Section 5, we carry out a comprehensive set of experiments to evaluate our model on query prediction and query suggestion tasks. We show that: 1) pre-search context is very useful in pre-dicting queries compared to other types of data ( e.g. , user search history and clickthrough data); 2) the state-of-the-art method [7] is inadequate in a real-world setting; 3) instead, our model captures pre-search context with hidden trigger-ing relations e ectively and outperforms the state-of-the-art method ( i.e. , 25% improvement) under the real-world setting; 4) our model also consistently improves over other methods in query auto-completion, and achieves larger im-provement in the most challenging cases, in which users have only typed one or two letters.
We divide information used for modeling user search in-tents into two categories { long-term history and short-term context.

The long-term history contains user behavior information, such as queries, clickthrough and browsed webpages, over a long period. It is often used to build user pro les and cap-ture users' general search interest. For example, in query auto-completion, many search engines suggest the comple-tions that have been most popular among users in the past history [1]. For personalized search, Gauch et al. [12] learn user pro les from browsing history, Speretta and Gauch [25] build pro les using search history. The short-term con-text instead provides a more direct information for infer-ring users' current search intent. We believe there exist two types of short-term contexts { pre-search context and in-search context. The pre-search context , as we de ned, is the search context that is prior to a search task and could trigger the search; in-search context is the search context during a search task, such as query reformulation and user clickthrough during a search session.
 In-Search context . There is a large body of work study-ing in-search context. In almost all of the work, in-search context is essentially used as additional information for un-derstanding search intent during a search task. Di erent models have been proposed for utilizing in-session context to improve various aspects of search, including query classi-cation [28, 27], query suggestions and auto-completion [5, 6, 4, 1, 11], document ranking [23, 29, 17], modeling search satisfaction [14, 13, 24, 22], search evaluation at session or task level [16, 15], and search personalization [26, 18, 2]. De-spite the rich types of contexts and applications covered by these studies, the kind of context that could trigger a search task ( i.e. , pre-seach context) is still relatively unexplored. Pre-Search context . There is limited work studying pre-search context. Dumais et al. [10] have a demonstration that automatically suggests queries while users are reading emails. However, details of the model used for the system are not provided, and no evaluation is performed for the sys-tem. Rahurkar et al. [20] detect if a query searched right af-ter browsing is relevant to the browsed webpage, and achieve n early perfect classi cation performance with 0.96 precision and 0.90 recall. But they do not consider modeling intent based on context. The same authors also carry out a prelim-inary study [21] that uses the browsed webpage to improve search relevance. The work aims to nd webpages similar to the browsed webpage, which may not be what the user needs: as we will show that triggered search intent may di-verge from the original browsed webpage in Section 3.3.1. Liebling et al. [19] try to predict what webpages users are likely to look for after browsing a webpage, based on the popularity of webpages that have been searched under the same browsing context. However, their approach is dicult to be generalized for fresh or less-popular webpages that do not have enough browsing history.

There are two recent studies [7, 3] that are similar to our work. Both of them try to predict/suggest what users are likely to search about a browsed webpage. Cheng et al. [7] collect searches after users browse a webpage in the search history, and then rank them as suggestions for that webpage using a learning to rank framework. Instead of relying on historic browsing and search information, Bordino et al. [3] focus on using webpage content, so that their approach can be generalized for previous unseen webpages. Our work sig-ni cantly di ers from the two studies in that: 1) we study a di erent problem. Motivated by the hidden nature of trig-gering relations in the real-world setting, we target at pre-dicting search intent based on pre-search context, which may or may not trigger the current search, while they target at predicting search intent for a given known triggering factor, which is normally unrealistic; 2) we perform an in-depth analysis. It veri es the hidden nature of triggering relations and discovers insightful characteristics of pre-search context, some of which have important implications on feature de-sign. Previous analysis [7] is limited to verifying triggering relations ; 3) our model captures hidden triggering relations and learns from large-size \free" unlabeled data, while their models require expensive human labels to identify triggering sources for learning.
In this section, we present an in-depth analysis of how pre-search context triggers users to search, focusing on browsed news articles (as explained in Section 1). Particularly, we rst verify the triggering relations and their hidden nature in a real-world setting, and then explore the characteristics of news pre-search context and its triggered queries.
To understand how pre-search context triggers queries in real world, we join browsing logs , which record user \brows-ing events" on Yahoo! news sites ( e.g. , News, Sports, Fi-nance), and search logs , which record user\search events"on Yahoo! Search, and then focus on \browsing-search pairs" in the joined logs. All the logs are from the English/US market and are anonymized. Formally, we can abstract our logs as follows.

A browsing event contains three elds f userID , times-tamp , URL g , which records the event that a user browses a URL at a speci c time;
A search event contains three elds f userID , timestamp , query g , which records the event that a user issues a query at a speci c time.

A browsing-search pair is a pair of a browsing event and a subsequent search event within a prede ned time window ( e.g. , 30 mins) in the logs.

In the rest of the paper, we refer to the webpage and the query in a browsing-search pair as browsed webpage and following query respectively. As not all the follow-ing queries are indeed triggered by the browsed webpages, we name the following queries that are triggered by the browsed webpages as triggered queries and the other fol-lowing queries as non-triggered queries .

We process 25 successive days of browsing and search logs in February 2014. In the following studies and experiments, we use the logs from the rst 24 days as history data , from which we extract statistics such as user search history and the queries that lead clicks to a certain webpage, and the logs from the last day as experiment data , which we use for our analysis and experiments. We have 1,796,313 browsing-search pairs after pre-processing and ltering ( e.g. , ltering out browsing-search pairs whose webpages are not accessi-ble due to outdated links or technical issues). We randomly sample 6,000 pairs for annotators to label. The annota-tors are asked to label whether or not the following query is triggered by the browsed webpage or whether they cannot decide. Those labeled pairs are further split into 3 di erent sets randomly, namely STUDY, TRAIN and EVALUATE.
 We use both STUDY and TRAIN for our analysis, TRAIN for model training, and EVALUATE for testing. Table 1 shows the statistics of annotated pairs in the three sets.
To empirically justify our key insight, we rst need to verify the triggering relations ( i.e. , whether queries are trig-gered by news pre-search context) in our user logs. Partic-ularly, by investigating the logs in our dataset, we rst nd that 3.6% of all the webpage browsing events are followed by immediate search events (no other events in between brows-ing and search) within 30 minutes. These\following"queries consist of 10% of all search trac. Further, from the human-annotated browsing-search pairs, shown in Table 1, we see that around 6% of these \following" queries are triggered by their browsed webpages. Thus, we estimate at least 0.6% all search queries are triggered by news pre-search context ( i.e. , browsed news articles). This actually underestimates the signi cance of news triggered searches due to that users may search in other search engines after browsing, which is not logged in our data. If we consider that there are many billions of queries everyday 1 , tens of millions of queries are triggered by news pre-search context. As an illustration, Table 2 shows some following queries, comprising triggered queries (queries triggered by the browsed webpages) and non-triggered queries for three webpages. We can clearly see that some following queries ( e.g. , \what is a bitcoin") in-deed are triggered by the corresponding browsed webpages ( e.g. , Bitcoin exchange Mt. Gox goes dark in blow to virtual currency).
Go ogle processes 3.5 billion searches daily http://www. statisticbrain.com/google-searches/ no n-triggered fa cebook, amanda knox, galaxy s5 launch no n-triggered c raigslist, amanda bynes, ali fedotowsky, ebay
W ebpage C : 6 Stars Who Have Returned to Regular Jobs no n-triggered fa cebook, free credit score
The analysis above veri es the triggering relations be-tween news pre-search context and searches. However, the triggering relations are hidden: the system does not directly observe whether queries will be triggered by news pre-search context or other pre-search context ( e.g. , locations, events). Based on Table 1, we estimate that around 94% of follow-ing queries are triggered by pre-search context other than browsed news articles, and the system does not know which pre-search context triggers the current search. This hid-den nature poses two real-world challenges that are not ad-dressed in previous studies [7, 3]: 1) how to predict searches with systems not knowing when/if searches will be triggered by the browsed webpages; 2) if/how can we learn predic-tion models and avoid using expensive labels for identifying which queries are triggered by the browsed webpages. We will propose a generative mixture model to principally ad-dress them in Section 4.
Next, to predict user search intent based on pre-search context, we need to characterize how pre-search context triggers searches. Particularly, we derive characteristics of triggered queries, which distinguish them from other follow-ing queries, from two main aspects: 1) relevance between browsed webpages and following queries and 2) types of fol-lowing queries themselves.
As it is natural to expect that triggered queries are rele-vant to the content of their browsed webpages (the page that triggered the query), we rst focus on whether triggered/non-triggered queries are actually relevant to the browsed web-pages.
 Text Matching We start with a basic question { can we distinguish triggered queries and non-triggered queries by how well they match the text content of browsed webpages ?
To answer this question, we compare triggered queries with non-triggered queries according to two text matching metrics: 1) exactly matching , which measures whether a query appears in a webpage; 2) overlapping , which measures whether a query has at least one word overlapping with the webpage, ignoring stopwords in the query. In Table 3, we show the percentages of the matched queries and the over-lapped queries among the triggered/non-triggered queries, as well as the percentages of the triggered/non-triggered queries belong to the matched and overlapped queries. We have two important observations from the results.
First, the results con rm our expectation that triggered queries are more relevant to browsed webpages than the non-triggered ones. Particularly, 48.41% (96.03%) of trig-gered queries match (overlap) with browsed webpages, while only 0.49% (8.22%) of non-triggered queries do. In fact, by examining queries in detail, we nd that 1) less than 4% of triggered queries that do not overlap browsed web-pages are semantically related to them ( e.g. , after browsing a webpage about Ford, a user searches a speci c automo-bile store, whose name does not appear in the webpage) and the 0.5% of the non-triggered queries which exactly match browsed webpages, are mostly popular navigational queries ( e.g. , \facebook", \google" and \espn").

Second, the results suggest that, unfortunately, we cannot distinguish triggered from non-triggered queries very well with text matching only. Speci cally, we can only obtain a high precision (87.14%) but low recall (48.41%) classi er or a high recall (96.03%) but low precision (44.32%) classi-er for predicting triggered/non-triggered queries with the matching or overlapping measures respectively. This implies that we should also consider other features for modeling trig-gering relations.
 Table 3: Matching and overlapping of triggered and non-triggered queries. Matc hing Positions Due to the limitations of matching and overlapping in distinguishing triggered and non-triggered queries, we next investigate whether where the matched queries of triggered/non triggered queries appear in the webpages can distinguish the classes. Our intuition is that because users read from top to bottom and news articles often position im-portant/interesting information at the top, triggered queries are likely to come from the beginning of browsed webpages.
To answer the question, we plot distributions of the doc-ument positions where triggered/non-triggered queries rst a ppear in Figure 1. In the gure, document positions from top to bottom are normalized to a value between 0 and 1 by word number. Triggered/non-triggered queries that do not appear in the news articles have no valid document po-sitions, and are thus not included in this plot. Figure 1 shows that most triggered queries appear in the beginning part of the news articles, and the non-triggered queries ran-domly appear in the beginning and the end of browsed web-pages. The results here verify our intuition above, which could be an e ective feature for distinguishing triggered and non-triggered queries.
 Figure 1: Distributions of triggered &amp; non-triggered queries' rst occurrences in their browsed webpages. T opic Relevance After measuring text similarity, we fur-ther measure how relevant the triggered and non-triggered queries of a webpage are to its main topic .

To answer this question, we compare triggered and non-triggered queries with the title of a webpage, which is a good representation of the main topic of the webpage. Speci cally, we measure the cosine similarity between queries and news article titles based on their term-frequency vectors. In ad-dition, we compute the similarity between \leading" queries of a webpage and the title of the webpage, as a baseline. Here, leading queries of a webpage are queries after which people click on the webpage, known to be relevant to the main topic of the webpage [8, 30]. Table 2 also shows some leading queries of two of the webpages.

Table 4 shows that, as we expected, triggered queries are more relevant to the topic of a browsed webpage than the non-triggered ones. However, interestingly, both triggered and non-triggered queries are much less relevant to the main topics of browsed webpages than the leading queries. This can also be illustrated by the examples in Table 2. For Webpage B, which is about the death of a lm director, the leading queries all focus on the death of Harold Ramis, while triggered queries might be about his movies, \animal house movie". It can be explained as users may be interested in any speci c information in a webpage.
 Table 4: Cosine similarity between triggered/non-triggered/leading queries and news titles.

Whi le the above results suggest relevance to the main topic could be a marginally e ective feature, the results are still valuable to us because: 1) they reveal an important fact that the triggered queries of a webpage could diverge from its main topic; and 2) they show clear di erences between leading queries and triggered queries, which motivate of our study of triggered queries from a new aspect.
We would like to understand if some particular types of queries are more likely to be triggered by a news pre-search context. Intuitively, we assume that if users issue queries because of reading articles ( e.g. , Bitcoin exchange Mt. Gox goes dark in blow to virtual currency), they are likely to look for explanations of some unknown concepts ( e.g. ,\what is bitcoin") or related information for some entities ( e.g. , \bitcoin value"). Thus we expect the triggered queries are new to the user and entity oriented . Next, we empirically verify these two assumptions.
 New assumption We verify that triggered queries are more likely to be new to the users (who issued the queries) than non-triggered ones by checking if triggered queries have been searched by the users before. We assume that a query never searched by a user before is likely to be a \new" concept to the user. We nd almost all (95.4%) of news-triggered queries have never been searched by their users (the par-ticular user who issued the triggered query) in their per-sonal history data (previous 24 days), while this percentage is much lower for non-triggered queries (78.64%) and for all queries issued by the same set of users in the same day (77.2%). This assumption can also be illustrated from the examples in Table 2, where in the rst example some query pre xes start with \what is", suggesting users are searching concepts new to them.

This result not only reveals another important character-istic of news triggered queries, but also implies that they will be much more dicult to predict than other queries if we solely depend on user search history.
 Entity oriented assumption To verify that triggered queries are more likely to be about entities mentioned in browsed webpages than are non-triggered ones, we apply an in-house named entity extractor on browsed webpages, and compare triggered/non-triggered queries with the recognized named entities. We nd that over half (55.8%) of the triggered queries contain at least one named entity recognized in the browsed webpages, and the percentage goes down to 1% for the non-triggered queries. Among the triggered queries with entities, 61.2% are exactly named entities without any other modi ers: for example, after reading Webpage C in Table 2, many users search query \susan boyle", who is a celebrity mentioned in the webpage. Another 38.8% of these queries are named entity with some modi ers: for exam-ple, users also issue the query \susan boyle bio" and \susan boyle movie" after reading that news page. This observa-tion clearly suggests that the named entities mentioned in the browsed webpages will be useful in predicting search in-tent triggered by the webpages.

We note that there could be more named entity related queries than these estimations, because 1) many named en-tities in the news articles are not recognized by our entity extractor, e.g. \bitcoin", and 2) queries may use di erent variations of the named entity: e.g. , \bit coin" and \bitcoin", \the walking dead" and \walking dead".
Though our analysis shows news pre-search context pro-vides useful information for search intent prediction, it also rec ognizes the hidden triggering relation challenge in real systems, that has not been addressed previously (Section 3.2). To address that challenge, we propose a Pre-search Context-aware Intent Model (PCIM) based on a generative mixture model to e ectively learn from a large amount of unlabeled data. This model di ers from a previous model [7] in that 1) it captures multiple types of pre-search context with un-known triggering source; and 2) it takes advantage of a large amount of unlabeled data to learn without supervision.
With the hope to improve di erent search related tasks, we rst describe PCIM as a general search intent model. Then we illustrate how this general model can be applied to two speci c applications, query prediction and query auto-completion.
We de ne a search intent model as a probability distri-bution over a set of intents, P ( I ) ; 8 I 2 I , where the set of intents I can be composed of words, phrases, or even ODP categories, depending on the speci c application. For ex-ample, in query prediction and query auto-completion, the set of intents contains all the query candidates that the user may search. Instead, if we use ODP categories as intents, then the search intent model can also be applied to classify search intent. We use this general de nition for our search intent model, so that the model can be applied to di erent search related tasks. We will apply this general model to query prediction and query auto-completion as examples in Section 4.2.
As discussed in Section 1, search intents can be triggered by di erent types of pre-search contexts C , such as browsed webpages (\Devaluation of Philippine Peso"), locations (new place visited) and events (Lady Gaga's coming concert). We also denote the set of all the pre-search context as C . Based on this, we assume that search intents are generated from di erent pre-search contexts according to the following gen-erative process: 1. A pre-search context C is selected as the triggering 2. The search intent I is generated by the given pre-According to this generative process, the mixture generative model for I can be written as where P ( C ) = C is the prior for each pre-search context fol-lowing the multinomial distribution, Multinomial ( ). While there are many di erent ways to model the intent distribu-tion P ( I j C ), we choose a standard log-linear model as the hypothesis function. Its function can be written as where Z = is a vector of features characterizing how likely will intent I be triggered by pre-search context C , and w C are the cor-responding feature weights. The details of used features are described in Section 4.2.3.
The PCIM model de ned in Equation 1 contains param-eters = f ; W g , where = f C g is the prior probabil-ities for each type of pre-search context; W = f w C g are the feature weights for each type of pre-search context in calculating P ( I j C ). To estimate , a straightforward way is to maximize the likelihood value of P ( f I i g i ) on a given training data set f I i g i =1 ; ;N , where N is the total num-ber of training instances. Assuming independence between di erent search intent examples I i , the log-likehood of the example collection is l ( ) = log Unfortunately, since log-likelihood in Equation 3 involves logarithm of a summation, there is no exact analytical solu-tion for . Therefore, we propose to use the EM algorithm for estimating instead.

In the E-step, we evaluate the posterior probability of a search intent being generated from a particular pre-search context given the previous model parameter estimation 0 = f 0 ; W 0 g :
In the M-Step, based on posterior estimates from the E-step, we maximize the lower bound of the log-likelihood: l ( ) &gt; l 0 ( ) = By taking derivative of l 0 ( ) w.r.t. and setting the deriva-tive to zero, we can easily update by C = P ( C j I i ; 0 W , we calculate the derivative according to the following function, and use gradient ascent to maximize l 0 ( ), where I i is the set of candidate search intents for the current search intent I i .
The PCIM described above is a general model that could be applied to di erent search related tasks. Now we show how to apply it to query prediction and query auto-completion as examples. In the query prediction task, our goal is to sug-gest a ranked list of queries that a user is likely to search (after the user browses a webpage, which may or may not trigger the search in the real-world setting). The query auto-completion task is similar to query prediction, except that it predicts (auto-complete) the query after the user has typed some initial characters of the query, which restrict the set of possible suggested queries to those that begin with the initial letters as pre x.

To apply PCIM to the two applications, we need to 1) specify search intent and pre-search context representations, 2) adapt the model for the chosen representation, and 3) de-sign specialized features for the speci c applications. Next, we describe each of these issues in detail.
In both query prediction and query auto-completion tasks, the basic unit of the search intent set I is a query. Hence we will use query Q instead of intent I in our speci c models. The candidate queries can be pooled from search logs and the browsed webpages. For query auto-completion, those candidate queries need to begin with the initial letters the user has typed. As an illustration, we consider only two types of pre-search context C in both tasks for the sake of feasibility and simplicity: 1) the webpage D the user just browsed before search; 2) a special background pre-search context G based on long-term search history, which we as-sume summarizes all other uncaptured pre-search context. We use this background pre-search context, because many other types of pre-search context could trigger searches, but are not accessible to the system ( e.g. , locations, events). We believe it is easy to extend our model for more types of pre-search context when they are accessible, which we leave as future work.
Given the representation, PCIM in Equation 1 can be rewritten as where, similar to Equation 2, P ( Q j D ) is the probability that webpage D triggers query Q , and is the prior for the query being triggered from D . P ( Q j G ) is the probability of the query being triggered by the background context. In our study, we will estimate P ( Q j G ) using the user's search his-tory H u and general search history H g for all the users: where P ( Q j H u ) / j Q 2 H u j , and P ( Q j H g ) / j j Q 2 H j denoting the frequency of query Q in H .

The model parameters can be estimated using the EM algorithm proposed in Section 4.1.3. After parameter esti-mation, we rank query candidates according to their proba-bility P ( Q ) as predictions/suggestions. Note that if we set = 0 and = 0, Equation 7 will only contain the P ( Q j H g part, which ranks queries based only on query frequency in the search logs. This special case is the same as the MPC model [1] used in query auto-completion task. If we only set = 0, the model becomes a personalized version of MPC, in which it jointly considers both search history from all users and the current user.
To characterize how likely it is that D will trigger query Q for P ( Q j D ), we use query-document matching and browsing-search history features as in previous work [7]. In addition, we design new features based on named entities, document position, and query freshness to the user, inspired by our in-depth analysis. All the proposed features are listed in Table 5.

Query-document matching features measure if the query is relevant to the document according to some pre-de ned similarity measures. Here we check if the query appears in di erent parts of the document by dMatch , hMatch . We also measure word overlap between query and document, calculated as j Q \ D j j Q j , where Q , D are represented by the words after removing stopwords. In addition, we use fea-tures based on history browsing-search pairs. The intuition Table 5: Pre-search context features for a webpage D triggering a search query Q .
 is that if many users issue the same query after browsing a certain webpage, then the webpage is very likely to trig-ger the particular query. Based on this, we use qf to count the frequency of a query being searched after users browse a webpage in the search history. Many popular navigational queries, such as \facebook", \google" are searched regard-less of what webpages user have just browsed. In order to demote these non-triggered queries, we use idf , which rep-resents the inverse frequency of di erent documents being browsed before users issue the query, in the same fashion as the conventional idf de nition.

As we nd named entities are important in our analysis, we use many entity-based features. We check if the query is or contains a named entity mentioned in the document by eMatch and eContain . We also use entity frequency to count the times of the query appearing as a named entity in the document by eFreq and ehFreq . Our analysis shows that users tend to search about 1) the beginning part of the a document and 2) new concepts that are fresh to the users. Therefore we use feature pos and freshness accord-ingly. Position in pos is normalized to 0 to 1 by word counts as described in Section 3.3.1, and set to 1 if the query does not appear in the news article.
In this section, we carry out a comprehensive set of exper-iments to evaluate our model on query prediction and query auto-completion. Particularly, we aim to empirically demon-strate that 1) pre-search context is useful for predicting user search intent and 2) our proposed model can e ectively cap-ture how queries are triggered with the hidden triggering relations in the real-world setting. Dataset In order to evaluate our model in real-world sce-narios, we continue to use the browsing and search logs de-scribed in Section 3.1. Here, we give a brief summary. We have two datasets: the rst one is history data , which con-sists of 24 days of search and browsing logs. We only use it to extract user search history and click history. The second one is experiment data , which consists of 6,000 browsing-search p airs. We use it to train and test models. Speci cally, we split the experiment data set into three subsets: STUDY (S), TRAIN (T), and EVALUATE (E). We use TRAIN to train models and set aside STUDY and EVALUATE for testing. Tasks We evaluate our model in two search tasks to demon-strate its e ectiveness for search intent prediction: 1) query prediction aims to predict what a user is going to search ( i.e. , her query) with the awareness of the pre-search context ( i.e. , after browsing a webpage); 2) query auto-completion aims to suggest queries after a user browses a webpage and enters several pre x characters of a new query.

For both tasks, we use browsing-search pairs to evalu-ate. Speci cally, for each browsing-search pair, we use the browsed webpage as the news pre-search context and the query issued after (within a limited time) as the correct an-swer. We emphasize that our research problem is predicting searches in this real-world setting, where the query may not actually be triggered by the browsed webpage { that is, it may be triggered by other uncaptured pre-search context such as locations and events.

To collect the candidate queries for both tasks, we pool the 100 most popular queries from the user's search history, the 100 most popular queries from search history across all users, and named entities appearing in the browsed web-page. To ensure there is an correct answer for comparing di erent models, we also include the following query in the candidate set. We believe this is a fair setting for com-parison, which would re ect prediction e ectiveness within the collected candidate set. The candidates for query auto-completion are almost the same as those for query predic-tion, except that now the query candidates are restricted to begin with the pre x characters.
 Methods To demonstrate the usefulness of pre-search con-text and the e ectiveness of our model, we compare against a wide range of baseline models as listed below.

Global Query Frequency (GQF). This method estimates the probability P ( Q ) that a query Q will be searched based on global search history, which is P ( Q j H g ) used in our model in Section 4.2.2. It is a widely used method [1] in query auto-completion. We use it as a baseline to com-pare the usefulness of the pre-search context and general search logs.

Global and User Query Frequency (GUQF). This method estimates the probability P ( Q ) that a user searches a query Q based on both global search history and user search history, which is P ( Q j G ) used in our model in Sec-tion 4.2.2. We use it as a baseline to compare the useful-ness of the pre-search context and user search history.
Leading Query Frequency (LQF). This method estimates the probability P ( Q ) that a user searches a query Q (with awareness of pre-search context) based on the frequency of Q in the leading queries of the browsed webpage in history.
Particularly, P ( Q ) / LQF ( Q; D ), where LQF ( Q; D ) is the frequency of Q being a leading query for D in history.
We use this method to demonstrate the di erence between the triggering relationship and the clicking relationship, which is widely studied in the literature [8, 30]. Ranking SVM (RSVM). Recent work [7] uses Ranking
SVM to rank queries that are likely to be triggered by a browsed webpage as predictions for user searches. As mentioned before, in the real-world setting, searches may also be triggered by pre-search context other than browsed webpages. Therefore, using this model to blindly predict searches solely based on browsed webpage could be inade-quate in the real-world setting. To test this hypothesis, we include this model as a baseline. In contrast to our model, this model requires expensive labeled triggered queries of each browsed webpages for training. In the experiments, we train the model with both the \real" triggered queries annotated by a human, denoted as RSVM-T, and pseudo triggered queries by assuming queries that appear in the browsed webpage are triggered by the webpage, denoted as RSVM-P. We use the same set of features as described in Table 5 for RSVM models.

Pre-search Context aware Intent Model (PCIM). This is our proposed model described in Section 4.2.2. The model does not need labels for trigger/non-triggered queries for training, and aims to predict search intent in real-world settings where queries may be triggered by the browsed webpages or other pre-search context.
 The parameters of all the models ( e.g. , , and w ) are trained and tuned using the TRAIN dataset.
 Metrics We use the following metrics to evaluate the e ec-tiveness of di erent models.

Mean Reciprocal Rank (MRR) . As both tasks aim to pre-dict correct queries (only one for each evaluation case), we use the standard mean reciprocal rank [9] as our main measure.

Log-Likelihood . In addition, we report how well a model \explains" the browsing-search pairs by the average of the log-likelihood of the queries in training and test data. For
LQF, GQF and GUQF, when the query frequency is zero, we smooth the probability simply by assigning probabil-ity 1 10 . For RSVM, since Ranking SVM only produces scores w T x + b for ranking and does not provide probabil-ities, to calculate its log-likelihood we estimate its query probability based on the score as P ( Q ) / exp f w T x + b For all the experiments, we also perform signi cance tests using paired t-test with 0.05 as the p-value threshold. We rst evaluate di erent models for query prediction. Table 6 shows MRR of each method on STUDY and EVAL-UATE testing dataset. Table 7 shows the average log-likelihood of each model on both testing and training datasets ( i.e. , STUDY, EVALUATE and TRAIN). Generally speaking, our model performs much better than all the baselines in terms of both MRR and log-likelihood on both test sets. The di er-ences between our model and the baselines are statistically signi cant. We note that the log-likelihood of our model in training and testing data is close, which suggests that our model is not over-tuned. Next, we analyze the results in detail.

To begin with, we compare PCIM with the baselines that explore di erent data ( i.e. , GQF, GUQF, and LQF). First, GQF performs worst in terms of MRR and the second worst in terms of log-likelihood, which clearly shows that the query frequency in general search history is not enough for query T able 6: MRR for query prediction on STUDY(S) and EVALUATE(E).
 T able 7: Log-likelihood of di erent models on STUDY(S), EVALUATE(E) and TRAIN(T).
 p rediction. Second, LQF performs worst in terms of log-likelihood and second worst in terms of MRR, which vali-dates that clicking relationships between queries and web-pages are di erent from the triggering relationships between webpages and queries and are not e ective for query predic-tion. Third, GUQF largely improves over GQF and LQF in both metrics, which suggests that user personal search his-tory provides valuable personal preference information for query prediction. PCIM, which captures the browsing pre-search context in additional to general and personal search history, performs the best and improves over the best base-line (GUQF) on MRR by 31% (on EVALUATE). It clearly demonstrates the e ectiveness of pre-search context in query prediction.

Then, we compare PCIM with the state-of-the-art meth-ods for triggered query prediction ( i.e. , RSVM-P and RSVM-T). RSVM-P, trained on pseudo triggered queries, performs much worse than PCIM. Even though RSVM-T largely im-proves over RSVM-P due to the manually labeled triggered queries used in training, PCIM still largely improves over it on MRR by 25% (on Evaluate). These observations ver-ify that RSVM models are inadequate or less e ective than PCIM under the real-world setting where queries can be triggered from di erent types of pre-search context ( i.e. , not only the browsed webpages) with the triggering source be-ing hidden. The experiment results also demonstrate that PCIM can e ectively learn from the unlabeled data to pre-dict queries based on both the browsing pre-search context and background pre-search context that summarizes other un-captured pre-search context.

To justify our assumptions, we further break down the overall results by triggered (T) and non-triggered (N) cases (by manually labeling) in Table 8. We only report MRR on EVALUATE in these experiments because of space lim-itations, noting that MRR is consistant with other mea-sures and the best aligned with our task. We have some interesting ndings. First, GUQF largely improves GQF for non-triggered queries and has limited improvement for triggered queries, which shows the need for exploring pre-search context. This also validates our analysis in Sec-tion 3 that triggered queries are new and cannot be e ec-tively predicted via exploring personal search history. Sec-ond, RSVM-P and RSVM-T perform better than PCIM for triggered queries but perform much worse than PCIM for non-triggered queries. The superior performance of RSVM models over PCIM on triggered queries implies that in the normally unrealistic setting of knowing searches are trig-gered from a speci c pre-search context, supervised models that focus on learning queries triggered from a single type of pre-search context is a better choice than our unsuper-vised mixture model that tries to capture multiple types of pre-search context. However, we emphasize that the trig-gering relations are usually hidden in a real-world setting (Section 3.2). The system could not know if the current search will be triggered by the browsed webpage, and there-fore could not apply RSVM models only on these triggered cases. The evaluation that combines both triggered and non-triggered queries in Table 6 and 7 re ects a realistic setting, in which we have shown that PCIM largely outper-forms RSVM models.
 Table 8: MRR for triggered(T) and non-triggered(N) queries.

F inally, we demonstrate the usefulness of our newly pro-posed features. Speci cally, we compare the performance with and without the new features (eMatch, eContain, eOver-lap, eFreq, ehFreq, pos, freshness, described in Table 5) on RSVM-P, RSVM-T and PCIM. The results are reported in Figure 2. We can see that for almost all the cases, exclud-ing the proposed features results in large decreases in per-formance, clearly demonstrating the utility of our proposed features.
 Figure 2: MRR for models with and without our proposed features. \full" and \part" stand for with and without proposed features respectively. \A", \T" and \N" stand for all cases, triggered cases and non-triggered cases, respectively.
In this section, we report results of di erent models on the query auto-completion task. In Table 9, we report MRR of di erent models evaluated for di erent initial lengths (the number of characters that a user has typed in).
 Table 9: MRR for query auto-completion on varying length of pre x characters.
 RS VM-T 0.1 129 0. 1671 0 .2318 0. 2533 0 .2618
Gen erally speaking, the results con rm our ndings from the previous experiment. GUQF improves over GQF, and PCIM further improves over GUQF and RSVM-T in query a uto-completion. Most importantly, the improvements of our model are consistent for any length of query pre x, and the di erences are all statistically signi cant. The re-sults again suggest that our model can e ectively model pre-search context, and consistently outperforms baseline mod-els. Further, we have some speci c ndings from this task. First, MRR for each model increases as pre x length in-creases, suggesting that the problem of query auto-completion become easier as the user types more letters. Second, the improvement for both GUQF over GQF and PCIM over GUQF decreases as initial length increases. This is because as a user types more and more letters, the additional infor-mation personal search history and pre-search context can provide becomes less and less helpful. However, from an-other point of view, this observation also suggests that pre-search context is particularly useful for the most challenging cases, in which the user has only typed one or two letters.
In this paper, we study the problem of predicting search intent based on pre-search context with hidden triggering relations, which advances the research of search intent pre-diction into a realistic setting. We make the following con-tributions to the problem.

We provide a key insight that searches could be triggered by di erent types of pre-search context, and search sys-tems often do not observe which particular one triggers each search ( i.e. , hidden triggering relations).
We conduct an in-depth analysis of how news pre-search context triggers searches based on real-world browsing and search logs. The analysis veri es the hidden nature of trig-gering relations between pre-search context and triggered queries, identi es interesting characteristics of them that lead to informative features, and provides guidelines for studying other pre-search context in the future.

To address the challenge of hidden triggering relations, we developed an unsupervised generative mixture model to learn how queries are triggered by di erent types of pre-search context by taking advantage of large-size unlabeled data. We also identify a set of e ective features which can be used in future work.

Our experiment results show that: 1) pre-search context is useful in predicting search intent; 2) in the normally unrealistic case of knowing searches are triggered from a speci c pre-search context, supervised models that fo-cus on learning queries triggered from that speci c pre-search context is more e ective than our proposed un-supervised mixture model; but 3) in the realistic setting of hidden triggering relations, our proposed unsupervised model could learn e ectively without expensive labels that are required in the supervised models, and predict search intents more accurately than them. This work was done during the rst author's internship at Yahoo! Labs. It was also supported in part by the Center for Intelligent Information Retrieval. Any opinions, ndings and conclusions or recommendations expressed in this ma-terial are those of the authors and do not necessarily re ect those of the sponsor.
