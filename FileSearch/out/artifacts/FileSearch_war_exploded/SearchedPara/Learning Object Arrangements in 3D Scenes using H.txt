 Yun Jiang yunjiang@cs.cornell.edu Marcus Lim mkl65@cornell.edu Ashutosh Saxena asaxena@cs.cornell.edu In fact, in human environments, arrangements of ob-jects are often governed by their affordances. For ex-ample, the objects in the 3D scene in Fig. 1 are ar-ranged in a particular configuration because they are meant to be used by humans for activities such as watching TV, working on laptop, etc. In another ex-ample, a keyboard is found below a monitor because it needs to be reachable by hand when the monitor is in sight. Learning such affordances for reasoning about the objects X  arrangements would be useful in several areas of scene understanding (e.g., Koppula et al., 2011) or assistive robots (e.g., Jiang et al., 2012a). In this work, we take an unsupervised learning ap-proach to this task. Given a collection of 3D scenes containing objects, we learn how an object relates to a human pose for an activity by defining a parameterized density function. While at the first blush, introducing human poses may seem to complicate the model, it actually simplifies it by making it more parsimonious. The reason for this is that the set of relevant human poses is far smaller than the collection of all objects. If we learn object to object relationships instead of learn-ing human pose to object relationships, the complexity of both model and computation grows quadratically in the number of objects. Human pose, which is the un-derlying factor connecting the objects to each other and the scene, provides a more parsimonious model for the arrangement of objects in a scene.
 In order to model the object arrangements, we use a Dirichlet process (DP) mixture model for defining the joint distribution of human poses and objects. We treat each human pose as a mixture component that models the distribution of objects, and we use DP to determine which human pose the object is generated from. This model allows different objects to be used by the same human pose (e.g., using a monitor, key-board and mouse at the same time), with different parameters in the density function. However, same objects will have same parameters across different hu-man poses. This requires that we formulate our learn-ing method as maximum likelihood estimation based on human poses sampled using DP. This variant of DP mixture model allows the same parameters to be shared by the instances from the same category across different mixture components.
 In this work, we specifically consider learning object arrangements with a robotic application in mind X  X  personal robot arranging a disorganized house. Jiang et al. (2012a;b) considered a similar problem. However ignoring the role of humans often led to unreasonable placements. Inference in our model naturally follows how we usually organize a room: from a given room, we first infer possible workspaces such as sitting on the chair and standing by the kitchen shelf; then we reason about how to place the objects for potential activities, such as placing a laptop on the desk facing the chair. In our experiments, we collected a large dataset comprised of different scenes from three categories: kitchen, living room and office. Each scene was manu-ally labeled by three to five subjects, who determined where and how to place the given objects. In total, 19 different types of object are placed. The experimental results demonstrate that our methods can find rea-sonable locations and elevations for objects in most cases. When evaluated by human subjects on real point-clouds, the average score is 4.3 out of 5, com-pared to 3.7 from the best baseline. Most previous work in vision and robotics has con-sidered the problem of scene understanding and its application to robotics using 2D images. With recent inexpensive RGB-D sensors, it is possible to obtain full 3D point-clouds and therefore reason about human poses in 3D. There are also some previous works that consider human pose and activity recognition, and we describe them below.
 Scene understanding. A number of works propose approaches to capture the relations between different parts of the object (Felzenszwalb et al., 2008) and be-tween different objects in 2D images (Heitz &amp; Koller, 2008). Some works extract 3D scene geometry from a single image for object detection (e.g., Saxena et al., 2005; Hoiem et al., 2006; Heitz et al., 2008; Lee et al., 2010; Li et al., 2010). The recent availability of RGB-D sensors provides more precise geometry of indoor scenes and enables capturing stronger context among objects (Koppula et al., 2011). The goal of these works is to find and label existing objects in a scene, while our goal is to infer arrangement of objects in the scene. Furthermore, these methods focus on learning object-object relationships and therefore do not scale well with large number of objects. By contrast, our work models the role of human poses as the underlying rea-son for object arrangements.
 Related applications. There is little work in robotic placing and arrangement of objects. Edsinger &amp; Kemp (2006) and Schuster et al. (2010) focused on finding flat clutter-free areas where an object could be placed, but did not model any form of semantic context for meaningful placing locations. Fisher et al. (2011) con-sidered the problem of finding the most visually rele-vant object to be placed in a given location. Jain et al. (2009) considered symbolic planning for arranging ob-jects such as setting a dinner table. Their work does not address finding desired object locations and is com-plementary to ours. Jiang et al. (2012b;a) employed 3D stability and geometric features to find stable and preferred placements. While geometry is an important hint for context, it alone cannot tell the difference be-tween a TV facing towards a couch and facing towards the wall. In order to learn such relations, we need to consider the role of human poses in meaningful object arrangements.
 Human pose estimation and activity detection.
 Estimating and understanding human pose in both static images and videos has attracted great attention in the computer vision community (e.g., Lee &amp; Cohen, 2006; Ly et al., 2012). Some other work uses human poses to facilitate high level understanding, such as hu-man activity detection (Sung et al., 2012). While these studies try to abstract human poses when human be-ings are present, there have been some work making use of imaginary human poses to detect objects (Grab-ner et al., 2011) and to infer human workspaces (Gupta et al., 2011). Our work, inspired by this viewpoint, uses human poses to detect the affordances of the ob-ject/scene to gain a deep understanding of the human environments. We take it further to inferring object arrangements based on the learned object affordances. We first generate a set of human poses in the scene based on certain criteria (such as reachability or usage with existing objects). We then use these human poses to estimate placements for new objects. It is the hu-man poses that link objects together. For example, a monitor on the desk could generate a sitting skeleton in front of it. Then the sitting skeleton could further sug-gest to place a mouse close to the hand and therefore at the edge of the desk. Note that objects are related by this means naturally. The interaction affects the human poses and object placements simultaneously. There are two components to this as described below: Human access and usability cost. One of the ob-jectives while arranging a room is to make it conve-nient for humans to use objects. For example, people usually prefer frequently-used objects to be placed on a table top, rather than on the floor, to reduce the access effort. A television is often facing an open area than the wall to increase its usability. Also, objects re-lated to the same human activity are often grouped to-gether, such as dishware and utensils on a dining table. In order to capture this in our model, we define a po-tential function over human poses and objects, based on a collection of features modeling human-object in-teraction, such as their distance, relative orientation and activity matching (see Section 4.1).
 Sampling of human poses. While there could be innumerable potential human poses in a scene, only a few of them are meaningful and relevant. For example, acrobatic or dancing poses are possible but rarely ap-pear in a room. Certain human poses are more likely than others (and also more relevant for object interac-tions/usage), such as sitting and standing. The poten-tial poses are also restricted by the layout of the scene, because of potential collisions, usage interactions with existing objects in the room, and their kinematic cost. For instance, in a room such as Fig. 1, a standing pose facing against the corner is less important because it does not connect with any object. We address this by sampling human poses according to the potential function using a DP (Section 4.2).
 Certain objects have similar purposes and are com-monly placed together, such as a PC setup of moni-tor/keyboard/mouse, a dishware set, or a TV and a remote control. This is because the human poses and object classes are linked via an activity. For exam-ple, standing pose and dishware/utensils are related through cooking activity. In our model, we use human activities to match relevant human poses and objects so that such objects would share similar human pose and activity and hence be placed together. In this section, we first define a potential function to quantify the relationships between human poses and object placements, and then present a DP-based al-gorithm infer poses and placements together. During training, we are given the objects in the scene, and our goal is to learn the distribution of human poses and the parameters of the potential function.
 Formally, given a new scene, denoted by E , which may contain some already placed objects G = { G 1 ,G 2 ,...,G n } , our task is to arrange more objects in the scene, O = { O 1 ,...,O m } . Each object G i or O is specified by its type, location and orientation. We also define the set of all possible human poses as H . A human pose is specified by its joint locations and activity (see Section 4.4 for more details). 4.1. Potential Function To describe the human-object relationship, we define the potential function between a human pose H and an object O (or G ) as It consists of several terms, as described below: Distance preference. Some objects are preferred to be at a certain distance from humans, such as a TV or a laptop. This preference, encoded as  X  dist ( O,H ), in-cludes how far the object should be, from what joint of the human skeleton, and how strong this bias is. The Euclidean distance between O and a designated joint of H follows log-normal distribution.
 Relative angular preference. There is a preference for objects to be located at a certain angle with respect to human poses. For example, people will sit in front of a laptop, but prefer the mouse to be on their right (or left). We use a von Mises distribution for  X  rel . Orientation preference. There is a preference for ob-jects to be oriented at a certain angle with respect to the human pose (e.g., a monitor should also be facing towards the skeleton when located in front of the skele-ton). Similarly to relative angular preference,  X  ori is also a von Mises distribution over the difference be-tween orientations of O and H .
 Height preference.  X  h is a Gaussian distribution of the object X  X  relative height to a human pose.
 Object-activity and pose-activity preference. Different objects can be related to different sets of activities, and so can human poses. We use two terms to represent the relationship between object and activity ( X  OA ) and between human pose and activity ( X  PA ). We represent them as probability tables.
 The potential function relates an object O to a hu-man pose H using the object-specific parameters  X . These parameters consist of parameters from each of the terms above, such as the shape and log-scale pa-rameter of the log-normal distribution in  X  dist and the mean and concentration parameter of the von Mises distribution in  X  rel and  X  ori . These parameters  X  are shared by objects from the same category. We now present how to sample H and learn  X  using DP. 4.2. Dirichlet Process Mixture Model In this section, we describe our formulation of the ob-ject arrangements as a DP mixture model. The infer-ence in our model is similar to the standard DP (see Teh (2010) and Neal (2000) for an overview). How-ever our learning method differs from it, as described in Section 4.3.
 Our particular problem can be viewed as a generative process (shown in Fig. 2), we treat each human pose as a mixture component that models the distribution of objects, and we use DP to determine which human pose the object is generated from. More formally, we first draw the prior distribution P ( H ) from DP ( P 0 , X  ), where P 0 is the base distribution and  X  is the concen-tration parameter. Then a human pose H is drawn from P ( H ). Finally, the object X  X  placement (i.e., loca-tion and orientation) is selected following  X ( O,H ;  X ). In this work, to estimate the distribution of O gener-ated by DP, we adopt the method of Gibbs sampling with auxiliary parameters in Neal (2000). In testing, this method iteratively samples the assignments c , hu-man poses H and placements O . Suppose there are K distinct c i for i = 1 ,...,n + m , where n + m is the to-tal number of objects in the scene. In other words, we have K human poses. We first augment the number of human poses to K + z by sampling z auxiliary human poses from the base distribution P 0 . Then, for every object in either O or G , we sample c i as, c = c | c  X  i ,O i , H X  where c  X  i denotes other assignments in c except c i , and n  X  i,c represents the number of assignments in c  X  i that take the value c . This equation shows that the dis-tribution of c i will be selected according to the poten-tial score and the  X  X opularity X  of a human pose ( n  X  i,c It also depends on the concentration parameter  X  that controls the probability of selecting a new pose. After the assignments are selected, H and O are se-lected based on their posterior:
H k |{ O i | c i = k }  X  Y By this means, we obtain a collection of sampled ob-jects, {O 1 ,..., O s } . Since they are drawn from the distribution of O , we approximate the distribution by counting the samples near it, i.e., O i  X  1 s P s j =1 I { O  X ( O i ) } , where I is the indicator function and  X  rep-resents a small neighborhood around this placement. 4.3. Learning Object-Specific Parameters During training, we are given scenes with placed ob-jects G and our goal is to learn object-specific parame-ters  X  that maximize the potential over all the scenes with latent human poses. Since  X  is invariant to the scene and human pose, we do not sample them in DP, but rather learn them using maximum likelihood esti-mation (MLE).
 In detail, we use a DP to sample poses H 1 ,...,H s as our observations. The optimal  X  is then given by, We can optimize  X  O independently based on the place-ments from the category O only. Furthermore, be-cause our potential function is the product of several components in Eq. (1), we can also optimize the pa-rameters in different terms separately. In detail, as  X  dist and  X  h are Gaussian distributions, the posterior mean and variance have closed form, and as  X  rel and  X  ori are von Mises distribution, their mean  X  and con-centration  X  can be estimated numerically. We iterate between computing  X   X  using Eq. (4) and sampling H until convergence. 4.4. Human Pose and Object Placement This section describes the set of human poses and ob-ject placements, which is used by DP to sample from. We extract human skeletons from the Kinect RGB-D dataset (Sung et al., 2012), which contains activities performed by four subjects. We then cluster the poses using k-means algorithm giving us six types of skele-tons (see Fig. 3). We sample the variants of these skeletons and check for collisions. In addition to the location, every sampled human poses is also assigned with an activity, used in  X  OA and  X  PA in Eq. (1). 1 We consider all the surface points of the environment as potential placing locations, and consider eight orien-tations evenly sampled from 0 to 2  X  . For each sample, we perform a stability check (if the point has a large enough region to support objects) and collision check (if the bounding boxes of objects overlap). For comparison in Section 6, we designed a number of baseline algorithms, including a discriminative classi-fier and a method based on a finite number of mixtures. Open-area preference. Following Schuster et al. (2010), we find a clutter-free area based on distances from already existing objects, and place objects in ori-entations closest to those in the training set. Height preference. We compute the average height of each object type X  X  placements. This helps in cases such as food or monitors that would be placed on a table (with the height of around 0 . 5m), while shoes and floor-lights are usually placed on the ground. Room -object context. This method considers the object X  X  relative location in a room. We first normal-ize the room X  X  size in the training data, and use the average relative location to place the object in testing. Object context. While our goal is to show that we can learn object arrangements only using how they relate to human poses, we also note that object -ob-ject context could be a piece of complementary con-text. As an example, a keyboard is placed in front of the monitor and utensil is often on the side of dish-ware. We learn object -object context  X  obj ( O, G ) as follows. We model the relative location/orientation to place the object as a Gaussian distribution, with pa-rameters extracted from training data. To select the reference object, we compute the variance of the rela-tive placement for every category and choose the one with the smallest variance.
 Discriminative Classifier ( X  X lass X ). Selecting a good placement for an object can also be treated as a binary classification problem. We build a logistic re-gression classifier for every category with a total of 97 features based on the values used above: the relative distance/orientation to other objects, height, relative XY location and size of its bounding boxes, etc. Finite Mixture Model. We also compare our algo-rithm with a finite mixture model, where the number of human poses is fixed. Suppose we have K human poses in a scene, denoted by H = { H 1 ,...,H K } . Simi-lar to the infinite mixture model, given the assignment c  X  X  1 ,...,K } , O i is distributed according to Eq. (3). After marginalizing out c i , we get where P ( c |H ) is the probability of choosing H c among K poses. The inference problem, finding O that max-imizes the potential is solved using an expectation-maximization (EM) algorithm. However, the M-step requires joint optimization over H and O , which we solve by iteration. In the training, we learn  X  using MLE (similar to Eq. (4)), i.e., This is also estimated using EM with an iterative M-step between  X  and H .
 Compared to our DP approach, a finite mixture model has two major drawbacks: (1) K is pre-defined, which limits the number of human poses varying across rooms with different size and layout; (2) It does not encode the prior of H , which is often informative. For instance, the probability of a human pose sitting on a chair is higher than standing up.
 Combining Object Context and Human Con-text. We additionally present another algorithm in which we combine the distribution of objects gen-erated through human poses O  X   X  human ( O,H ;  X ) with a distribution generated through object -ob-ject context O  X   X  obj ( O, G ) using a mixture model: O  X   X   X  human (  X  ) + (1  X   X  ) X  obj (  X  ). We give a compar-ison of methods of using object context only, human context only and their combination in our experiments. In this section, we extensively evaluate our algorithms from three perspectives: (1) robustness across a va-riety of different scenes and objects; (2) comparison between our method and several baselines, including an approach that models object-wise relationships; (3) different placing scenarios such as placing new objects and placing in an empty room.
 Dataset. We created a dataset consisting of 20 scenes in three categories: six living rooms, seven kitchens and seven offices. We downloaded the 3D model for each scene from Google 3D Warehouse. 2 All the scenes are commonly seen in the real world and have a large variety in space, layout, furniture, etc. We also gath-ered a collection of daily objects, such as dishware, books, fruit, lamps, computers, etc, for a total of 19 different types (listed in Table 1). Every room is as-signed a set of 10 to 30 objects, and we asked three to five subjects (not associated with the project) to manually label the placements of every object in the scene. A snapshot of our dataset is shown in Fig. 4. Results. The first experiment was performed on the 20 rooms, with 5-fold cross validation. In each fold, labels of 16 rooms were used for training and the other four rooms for testing, so that the test rooms had never been seen by the algorithm. We created two placing scenarios for test: placing new objects and placing in an empty room. In the first case, for every test room we took out the objects of the type being placed and left other types as given. In the second case, the test rooms had no object in it at all.
 We wanted to answer following questions: Is the learned density function meaningful? We visu-alize some learned density function in Fig. 5, where we put a skeleton facing right and centered at a room of 10m  X  10m. It shows a 2D distribution of placing different objects relative to the human pose. We can see that TV prefers some distance to the human posi-tion, and it has a narrow range of relative orientation, unlike the remote and decoration. The mouse, laptop and dishware all prefer a smaller distance, but have different preferred relative orientations.
 Do sampled human poses and object placements re-flect meaningful distributions? Fig. 6 shows sampled human poses and object placements for some scenes. When placing a monitor in the first scene, the existing objects caused human poses to be sampled at the cor-ner side of the desk, and further caused the monitor to be sampled near the same side. Note that the most likely location is aligned with the keyboard. In the sec-ond scene, most skeletons are sitting on the couch or standing in front of it, resulting a dense distribution near the TV stand. The third scene has two major areas X  X ear the chairs and near the sofa.
 Table 1 gives quantitative evaluation of our algorithms based on two metrics: location difference , measuring the Euclidean distance between the predicted and the labeled locations, and also height difference . In the task of placing new objects, using object context ( X  X bj X ) beat other baseline methods, especially for the laptop, monitor, keyboard and mouse types. This was because of the strong spatial relationships among these objects. However, our method based on human con-text ( X  X P X ) still outperformed the object context. By considering human poses, it improved the placement of objects that have weaker connection to others, such as book, TV, decoration and shoes.
 The task of arranging objects in an empty room is quite challenging since there is no object context for the first few placements. Not surprisingly, we found that the object-context method performed poorly, even worse than the simple height-preference rule. The performance of our methods was also affected. How-ever, the sampled human poses could still pick up hints from the furniture in the scene, using room geome-try (with no semantic labels). Our experiments also showed that the finite mixture model using human con-text performed better than other baselines, but not as well as the ones using DPs.
 In both tasks, our human-context algorithm success-fully predicted object placements within 1.6 meters on average. The average error in height was only 0.1 me-ters. By combining human-and object-context, the error was further reduced X  X ndicating that they pro-vide some complementary context.
 Results on real scenes. To demonstrate that our algorithm is also robust in real scenes, we tested on point clouds taken from five real offices/apartments from the dataset published in Jiang et al. (2012a). Similar to Jiang et. al., we evaluated the final arrange-ments by asking two human subjects (one male and one female, not associated with the project) to label the placements for each object as semantically correct or not, and also score the overall object arrangement on a scale from 0 to 5.
 Table 2 shows the results on the five real scenes. The office2 scene has only one big table in the room, therefore the number of semantically correct place-ments ( X  X o X ) is 100% for every method. The open-area method performed well in this scene because it placed objects spread around the table, unlike some baselines, which piled up objects. However, our and Jiang et al. X  X  approach both arranged objects more meaningfully, i.e., books were stacked together, while a keyboard, laptop and mouse were placed close to each other. The Apt2 scene has many different lay-ers for placing and thus some baselines could not iden-tify semantically correct placing areas for objects. The DP, however, performed much better by, for example, placing shoes at the bottom level of a shelf, while food and books are on the middle level or on a table. Jiang et al. X  X  approach sometimes put the laptop on a shelf making it difficult for human to access.
 Results on robotic placements. Finally, given the predicted arrangement of a scene, we used our PO-LAR and Kodiak PR2 robots to place the objects in simulation. The experimental details and results are provided in Jiang &amp; Saxena (2012). In this work, we applied human context to the task of arranging objects in a 3D scene. The key idea was that human poses and object placements relate strongly to each other in terms of object affordances, access effort and activity relevance. We designed potential func-tions based on spatial features to capture these rela-tions. In an unsupervised learning setting where we are given a collection of 3D scenes containing objects, we learned the distribution of human poses as well as object placements using a variant of Dirichlet pro-cess. Our extensive experiments on 20 different rooms with 19 types of object showed that the arrangements are improved by considering human context. We also tested this on a personal robot in arranging and plac-ing items.
 This research was funded by Microsoft Faculty Fellow-ship and Alfred P. Sloan Fellowship to Saxena.
