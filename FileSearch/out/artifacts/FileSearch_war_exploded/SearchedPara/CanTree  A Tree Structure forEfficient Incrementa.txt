
Since its introduction, frequent-pattern mining has been the subject of numerous studies, including incremental up-dating. Many existing incremental mining algorithms are Apriori-based, which are not easily adoptable to FP-tree based frequent-pattern mining. In this paper, we propose a novel tree structure, called CanTree ( Can onical-order Tree ) , that captures the content of the transaction database and orders tree nodes according to some canonical order. By exploiting its nice properties, the CanTree can be eas-ily maintained when database transactions are inserted, deleted, and/or modified. For example, the CanTree does not require adjustment, merging, and/or splitting of tree nodes during maintenance. No rescan of the entire updated database or reconstruction of a new tree is needed for in-cremental updating. Experimental results show the effec-tiveness of our CanTree.
Since its introduction [1], the problem of mining associ-ation rules X  X nd the more general problem of finding fre-quent patterns X  X rom large databases has been the subject of numerous studies. These studies can be broadly divided into the following two categories. (a) Functionality : The central question considered is what (kind of rules or patterns) to compute. While some studies [4, 6, 11] in this category considered the data min-ing exercise in isolation, some others explored how data mining can best interact with (i) the database management system [27, 28] or (ii) the human user. Examples of the lat-ter include constrained mining [5, 7, 12, 18, 20, 22, 25] and interactive and online mining [10, 15, 18]. (b) Performance : The central question considered is how to compute the association rules or frequent patterns as ef-ficiently as possible. Studies in this category can be fur-ther classified into several subgroups. The first subgroup consists of fast algorithms based on the levelwise Apriori framework [2]. The second subgroup focuses on perfor-mance enhancement techniques like hashing and segmenta-tion [21, 24] for speeding up Apriori-based algorithms. The third subgroup is on incremental updating .
With advances in technology, one could easily collect a large amount of data. This, in turn, poses a mainte-nance problem. Specifically, when new transactions are inserted into an existing database DB and/or when some old transactions are deleted from DB , one may need to update the collection of frequent patterns (e.g., add to the collection those patterns that were previously infrequent in the old database DB but are frequent in the updated database DB ). Algorithms such as FUP [8], FUP 2 [9], and UWEP [3] were developed to solve this problem.

In general, the above mentioned algorithms are Apriori-based, that is, they depend on a generate-and-test paradigm. They compute frequent patterns by generating candidates and checking their frequencies (i.e., support counts) against the transaction database. To improve efficiency of the min-ing process, Han et al. [13, 14] proposed an alternative framework, namely a tree-based framework. The algorithm they proposed in this framework constructs an extended prefix-tree structure, called Frequent Pattern tree (FP-tree) , to capture the content of the transaction database. Rather than employing the generate-and-test strategy of Apriori-based algorithms, such a tree-based algorithm focuses on frequent pattern growth X  X hich is a restricted test-only ap-proach (i.e., does not generate candidates, and only tests for frequency).

Since the introduction of such an FP-tree based frame-work, some studies have been proposed to improve func-tionality (e.g., interactive FP-tree based mining [19]) and performance (e.g., FP-tree based segmentation tech-niques [23]). So, how about FP-tree based incremen-tal mining? Recall that algorithms such as FUP [8], FUP 2 [9], and UWEP [3] were developed to handle incre-mental mining in the Apriori-based framework. They can-not be easily adoptable to FP-tree based incremental min-ing. Fortunately, some tree-based incremental mining algo-rithms were recently developed. For example, Cheung and Za  X   X ane [10] proposed the FELINE algorithm with the CATS tree, whereas Koh and Shieh [17] proposed the AFPIM al-gorithm. The former aims to make the CATS tree (a vari-ant of the FP-tree) compact, and the FELINE algorithm is well-suited for interactive mining where the database re-mains unchanged and only the minimum support thresh-old gets changed. So, it works well in situations that fol-low the  X  X uild once, mine man y X  principle (e.g., interactive mining), but its efficiency for incremental mining (where the database is changed frequently) is unclear. Unlike the FELINE algorithm, the AFPIM algorithm was proposed for incremental mining. Specifically, it was designed to pro-duce the FP-tree of the updated database, in some cases, by adjusting the old tree via the bubble sort. However, in many other cases, it requires rescanning the entire updated database DB in order to build the corresponding FP-tree.
To summarize, those existing Apriori-based incremen-tal mining algorithms cannot be easily adoptable to FP-tree based incremental mining. Among those FP-tree based algorithms, the FELINE algorithm with the CATS tree was mainly designed for interactive mining, where the  X  X uild once, mine many X  principle holds. However, such a principle does not necessarily hold for incremental min-ing. The AFPIM algorithm was proposed to reduce X  X ut not to eliminate X  X he possibility of rescanning the updated database. Is there any algorithm that aims for incremental mining? Is there any tree structure that is simpler but yet more powerful than the CATS tree? Can we do better than the AFPIM algorithm (i.e., can we avoid rescanning the en-tire updated database)?
The key contribution of this work is the development of a simple, but yet powerful, novel tree structure for main-taining frequent patterns found in the updated database. More specifically, we propose a novel tree structure, called CanTree (CANonical-order TREE), that aims for incremen-tal mining. The tree captures the content of the transaction database. When the database i s updated (i.e., transactions are inserted, deleted, and/or modified), our algorithm does not need to rescan the entire updated database. Experimen-tal results in Section 5 show that frequent-pattern mining with our CanTree is more efficient than that with existing algorithms or structures. Figure 1 summarizes the salient differences between our proposed CanTree and its most rel-evant work.

This paper is organized as follows. In the next sec-tion, related work is described. Section 3 introduces our CanTree for incremental mining. In Section 4, we discuss the additional benefits of CanTrees (e.g., for incremental constrained mining). Section 5 shows experimental results. Finally, conclusions are presented in Section 6.
In this section, we discuss two existing FP-tree based algorithms that handle incremental mining, namely (i) the FELINE algorithm with the CATS tree [10] and (ii) the AFPIM algorithm [17].
Cheung and Za  X   X ane [10] designed the CATS tree (Com-pressed and Arranged Transaction Sequences tree) mainly for interactive mining. The CATS tree extends the idea of the FP-tree to improve storage compression, and allows frequent-pattern mining without the generation of candidate itemsets. The aim is to build a CATS tree as compact as possible.

The idea of tree construction is as follows. It requires one database scan to build the tree. New transactions are added at the root level. At each level, items of the new transaction are compared with children (or descendant) nodes. If the same items exist in both the new transaction and the chil-dren (or descendant) nodes, the transaction is merged with the node at the highest frequency level. The remainder of the transaction is then added to the merged nodes, and this process is repeated recursively until all common items are found. Any remaining items of the transaction are added as a new branch to the last merged node. If the frequency of a node becomes higher than its ancestors, then it has to swap with the ancestors so as to ensure that its frequency is lower than or equal to the frequencies of its ancestors. Let us con-sider the following example to gain a better understanding of how the CATS tree is constructed.

Given that the above tree construction step takes only a single data scan (i.e., constructing the tree without prior knowledge of data), Cheung and Za  X   X ane admitted that their CATS tree is not guaranteed to have the maximal compres-sion. Moreover, the tree compression is sensitive to (a) the ordering of transactions within the database and (b) the or-dering of items within each transaction.
 In addition, when handling incremental updates, their FELINE algorithm (FrEquent/Large patterns mINing with CATS trEe) suffers from the problems/weaknesses de-scribed below. First, tree construction could be computa-tionally expensive, because it searches for common items and tries to merge the new transaction (the entire one or a portion of it) into an existing tree path when each transac-tion is added. It checks existing tree paths one-by-one until a mergeable one is found. Since items are arranged accord-ing to their local frequency in the path in the CATS tree, an item (e.g., e in Example 1) may appear above another item (e.g., c ) on one branch, but below it on another branch. This makes such a search-and-merge costly.

Second, a lot of computation is spent on tree construc-tion with an expectation that the tree is  X  X uilt once, mined many X  (e.g., in interactive mining where database remains unchanged and only the minimum support threshold min-sup is changed interactively). However, such a  X  X uild once, mine many X  principle does not necessarily hold for incre-mental mining. Specifically, for incremental mining, the database can be changed by insertions, deletions, and/or modifications of transactions. Hence, after a tree is built, it may be mined only once.

Third, extra cost is required for the swapping and/or merging of nodes. See Example 1.

Fourth, since items are arranged in descending local fre-quency order in the CATS tree. So, when forming projected databases (during the mining process), the FELINE algo-rithm needs to traverse both upwards and downwards to in-clude frequent items. This is different from usual FP-tree mining (e.g., using the FP-growth algorithm [13]) where only upward traversal is needed. Specifically, the CATS tree uses the local-frequency ordering (e.g., item e is above c on the left branch but is below c on the right branch in the fi-nal tree in Example 1), the downward traversal is needed for completeness (e.g., to avoid missing item c at the leave of the left branch). Consequently, it costs more to traverse both upwards and downwards! Due to the additional down-ward traversal, extra work is n eed for additional checking to ensure that infrequent items as well as those mined items are not doubly-counted when forming projected databases!
Koh and Shieh [17] developed the AFPIM algorithm (Adjusting FP-tree for Incremental Mining). The key idea of their algorithm can be described as follows. It uses the original notion of FP-trees, in which only  X  X requent X  items are kept in the tree. Here, an item is  X  X requent X  if its fre-quency is no less than a threshold called preMinsup ,which is lower than the usual user-support threshold minsup .As usual, all the  X  X requent X  items are arranged in descend-ing order of their global frequency. So, insertions, dele-tions, and/or modifications of transactions may affect the frequency of items. This, in turn, affects the ordering of items in the tree. More specifically, when the ordering is changed, items in the tree need to be adjusted. The AFPIM algorithm does so by swapping items via the bubble sort, which recursively exchanges adjacent items. This can be computationally intensive because the bubble sort needs to Figure 3. The FP-trees for DB , DB  X  db 1 ,and
DB  X  db 1  X  db 2 (for the AFPIM algorithm). apply to all the branches affected by the change in item fre-quency.

In addition to changes in the item ordering, incremen-tal updating may also lead to the introduction of new items in the tree. This occurs when a previously infrequent item becomes  X  X requent X  in the updated database. When facing this situation, the AFPIM algorithm can no longer produce an updated FP-tree by just adjusting items in the old tree. Instead, it needs to rescan the entire updated database to build a new FP-tree. This can be costly, especially for large databases. To gain a better understanding of the AFPIM algorithm, let us consider the following example.
Like the FELINE algorithm, the AFPIM algorithm also suffers from several problems/weaknesses when handling incremental updates. A problem is the amount of computa-tion spent on swapping, merging, and splitting tree nodes. Swapping is required because items arranged according to a frequency-dependent ordering (specifically, descending or-der of global frequency). So, when the database is updated (e.g., by inserting and/or deleting transactions), frequencies of items may be changed. As a result, the ordering of items needs to be adjusted. This problem is more serious (than FELINE) because it uses the bubble sort to recursively ex-change adjacent tree nodes. The bubble sort is known to be of O( h 2 ) computation, where h is the number of tree nodes involved in a tree branch. There are many branches in a tree! Furthermore, the swapping of tree nodes often leads to the merging and splitting of nodes. For instance, the insertion of transactions t 7 and t 8 in Example 2 changes the frequency order of items in the tree. Nodes c need to swap with nodes d and e . After swapping, nodes d and e in path d, e, c are split into two (i.e., c, d, e and d, e as branches of b ). At the same time, three children of b (i.e., c in paths c , c, d, e ,and c, e ) are in common, and hence the c nodes are merged and resulted in the rightmost FP-tree in Figure 3. To summarize, incremental updates to database often result in a lot of swapping, merging, and splitting of tree nodes.

Another problem of the AFPIM algorithm is its require-ment for an additional mining parameter preMinsup ,which is set to a value lower than the usual mining parameter minsup (the minimum support threshold). With this ad-ditional parameter, only the items whose frequency meets preMinsup are kept in the tree. However, it is well-known that finding an appropriate value for minsup is challenging, which explains the call for interactive mining where the user can interactively adjust or refine minsup . So, finding appro-priate values for both minsup and preMinsup can be even more challenging!
Recall from the previous section that, when han-dling incremental updates, the aforementioned tree-based algorithms X  X oth the FELINE algorithm (with the CATS tree) and the AFPIM algorithm (with the FP-tree) X  X uffer from several problems/weaknesses. These can be summa-rized as follows: (i) The FELINE algorithm requires a large amount of (ii) The AFPIM algorithm requires an additional mining (iii) Both FELINE and AFPIM algorithms need lots of In the remaining of this section, let us describe our pro-posed CanTree (CANonical-order TREE) and show how it solves the above mentioned problems. In general, the CanTree is designed for incremental mining. The construc-tion of the CanTree only requires one database scan. This is different from the construction of an FP-tree where two database scans are required (one scan for obtaining item fre-quencies, and another one for arranging items in descend-ing frequency order). In our CanTree, items are arranged according to some canonical order , which can be deter-mined by the user prior to the mining process or at run-time during the mining process. Specifically, items can be consistently arranged in lexicographic order or alpha-betical order (as in Example 3). Alternatively, items can be arranged according to some specific order depending on the item properties (e.g., their price values, their validity of some constraints). For example, items can be arranged ac-cording to prefix function order R or membership order M for constrained mining. (See Section 4 for more details on incremental constrained mining.) While the above or-derings are frequency-independent, items can also arranged according to some fixed frequency-related ordering (e.g., in descending order of the global frequency of the  X  X riginal X  database DB ). Notice that, in this case, once the ordering is determined (say, for DB ), items will follow this ordering in our CanTrees for subsequently updated databases (e.g. DB  X  db 1 , DB  X  db 1  X  db 2 , ...) even the frequency ordering of items in these updated databases is different from DB . With this setting (the canonical ordering of items), there are some nice properties, as described below.
 Property 1 The ordering of items is unaffected by the changes in frequency caused by incremental updates. Property 2 The frequency of a node in the CanTree is at least as high as the sum of frequencies of its children.
By exploiting properties of our CanTree, we note the fol-lowing. Transactions can be easily added to the CanTree without any extensive searches for mergeable paths (like those in FELINE). As canonical order is fixed, any changes in frequency caused by incremental updates (e.g., inser-tions, deletions, and/or modifications of transactions) do not affect the ordering of items in the CanTree at all. Con-sequently, swapping of tree nodes X  X hich often leads to merging and splitting of tree nodes X  X s not needed.
Once the CanTree is constructed, we can mine frequent patterns from the tree in a fashion similar to FP-growth. In other words, we employ a divide-and-conquer approach. We form projected databases by traversing the paths up-wards only . Since items are consistently arranged according to some canonical order (e.g., lexicographic order, prefix function order R , global frequency order of DB ), one can guarantee the inclusion of all frequent items using just up-ward traversals. There is no worry about possible omission or doubly-counting of items. Hence, for CanTrees, there is no need for having both upward and downward traver-sals. This significantly reduces computation by half! For example, forming { X } -projected databases (where X is a, b, c, ..., g ) requires traversals of 62 nodes in the rightmost CATS tree in Figure 2; it needs to traverse only 27 nodes in our CanTree!
To summarize, our proposed CanTree solves the prob-lems/weaknesses of the FELINE or AFPIM algorithms as follows: (i) For our CanTree, items are arranged according to (ii) The construction of our proposed CanTree is indepen-(iii) Since items are consistently ordered in our CanTree, The above shows how we solve the problems/weaknesses of the CATS tree/FELINE algorithm and the AFPIM algo-rithm by using our CanTree. To gain a better understanding, let us consider the following example.

Figure 4. The CanTree after each group of transactions is added.
In this section, we discuss two issues: (i) the applica-bility of the proposed CanTree for incremental constrained mining and (ii) efficiency and memory issues regarding our CanTrees. So far, we have shown how efficient our proposed CanTrees are for incremental mining. However, it is impor-tant to note that CanTrees also provide us with additional functionalities. For example, CanTrees can be used for in-cremental constrained mining .

Besides incremental mining, frequent-pattern mining has been generalized to many forms since its introduc-tion. These include constrained mining . The use of con-straints permits user focus and guidance, enables user ex-ploration and control, and leads to effective pruning of the search space and efficient discovery of frequent pat-terns satisfying the user-specified constraints. Over the past few years, several FP-tree based constrained mining algo-rithms have been developed to handle various classes of constraints. For example, the FIC algorithms [25] han-dle the so-called convertible constraints (e.g., C conv  X  avg ( S.P rice )  X  7 which finds frequent itemsets whose av-erage item price is at most $7). As another example, the FPS algorithm [20] supports the succinct constraints (e.g., C succ  X  max ( S.P rice )  X  30 which finds frequent item-sets whose maximum item price is at least $30). The suc-cess of these algorithms partly depends on their ability to arrange the items according to some specific order in the FP-trees. More specifically, FIC arranges items accord-ing to prefix function order R (e.g., arranges the items in ascending order of the price values for the above C conv ). Similarly, FPS arranges items according to order M spec-ifying their membership (e.g., arranges the items in such a way that mandatory items below optional items for the aforementioned C succ ). For lack of space, we do not de-scribe these algorithms further; please refer to the work of Pei et al. [25] and Leung et al. [20] for more details.
Our proposed CanTree provides the user with additional functionality to these algorithms, namely incremental con-strained mining. More precisely, these algorithms can use CanTrees (instead of FP-trees), and arrange tree items ac-cording to some canonical order (e.g., order R for the FIC algorithm, order M for the FPS algorithm). By so doing, when transactions are inserted into or deleted from the orig-inal database, the algorithms no longer need to rescan the updated database nor do they need to rebuild a new tree from scratch. In addition, no merging or splitting of tree nodes is needed.
On the surface, it appears that our CanTree may take a large amount of memory. For instance, our CanTree may not be as compact as the corresponding CATS tree. How-ever, it is important to note that CATS trees do not neces-sarily reduce computation or time (e.g., a lot of computation spent on finding mergeable paths as well as traversing paths upwards and downwards). In contrast, our CanTrees sig-nificantly reduce computation and time, because they easily find mergeable paths and require only upward path traver-sals. As a result, our proposed CanTrees provide users with efficient incremental (constrained or unconstrained) mining . Moreover, with modern technology, main memory space is no longer a big concern. This explains why, in this paper, we made the same realistic assumption as in many studies [10, 16, 26, 29] that we have enough main memory space (in the sense that the trees can fit into the memory). Regarding the tree size, our CanTree X  X ike FP-trees and CATS trees X  X s an extended prefix-tree structure that cap-tures the content of the transaction database. With the path sharing, the number of tree nodes is no more than the num-ber of items in the database.

For situations where the CanTree representing DB does not fit into memory, recursive projections and partitioning are required to break DB into smaller pieces. As a result, additional performance overhead may incur.
In the experiments, we used (i) several transaction databases generated by the program developed at IBM Al-maden Research Center [2] and (ii) some real-life databases from UC Irvine Machine Learning Depository. The results produced are consistent. So, for lack of space, we cite be-low the experimental results based on an IBM transaction database, which consists of 1M records with an average transaction length of 10 items and a domain of 1000 items.
All experiments were run in a time-sharing environment in a 1 GHz machine. The reported figures are based on the average of multiple runs. Runtime includes CPU and I/Os; it includes the time for both tree construction and frequent-pattern mining steps. In the experiments, we mainly com-pared the following algorith ms that were implemented in C: (i) the FELINE algorithms with the CATS tree, (ii) the AF-PIM algorithm (with the FP-tree), and (iii) the mining algo-rithm with our proposed CanTree.

In the first experiment, we divided the transaction database DB into the  X  X riginal database X  DB and the up-date portion db (i.e., DB = DB  X  db ). We tested how the minsup values affect the runtime of the algorithms. The y-axis of Figure 5(a) shows the runtime, and the x-axis shows minsup .When minsup decreased, the runtime increased. Note that FP-trees for the AFPIM algorithm were usually smaller than CATS trees and CanTrees, because only  X  X re-quent X  items were kept in the FP-trees. When minsup de-creased, the correspondingFP-trees became bigger and took longer to build. Moreover, the lower the minsup , the higher was the probability that (i) frequency order of items in the tree got changed (which, in turns, led to adjustment of tree nodes) and/or (ii) new items got introduced (which, in turns, led to construction of a new tree). As for both CanTrees and CATS trees, their construction was independent of min-sup because they both kept all items in every transaction. Among them, CATS trees took more time to build than CanTrees due to extra computation in (i) swapping, merg-ing, and splitting of tree nodes as well as (ii) searching of common items and mergeable tree paths in CATS trees.
As for mining, both AFPIM and our proposal traversed upwards to form projected databases (for frequent items). Among the two, the AFPIM algorithm required less traver-sal because the corresponding FP-trees were smaller. As for the FELINE algorithm, it took longer because it needed to traverse the corresponding CATS trees both upwards and downwards when forming projected databases! Hence, al-though CATS trees were slightly more compact (e.g., our CanTree was 1.2 times bigger than CATS trees) than our CanTrees, mining with our CanTrees could be faster (e.g., more than 1.2 times faster) th an the FELINE algorithm with CATS trees.
 In the second experiment, we again divided DB into DB and db so that DB be p % of DB and db be the re-maining (100  X  p )% . We varied the percentage p % from 10% to 90%. It was observed from Figure 5(b) that both CATS trees and our proposed CanTrees were not affected by the varying percentage values. However, for the AFPIM algorithm, the higher the percentage p % (i.e., larger DB and smaller db ), the bigger was the FP-tree for DB .This means a higher probability for the swapping, merging, and splitting of tree nodes (when the frequency order of items got changed due to incremental updates). However, it also means a lower probability for the introduction of new items (i.e., when some infrequent items became  X  X requent X  due to incremental updates in such a way that the old tree did not cover these items and new tree was needed). Hence, for low p % (e.g., p  X  40% ), updates caused tree rebuild; for high p % (e.g., p  X  50% ), updates required node adjustment. In the third experiment (see Figure 5(c)), we divided DB into DB and several update portions. We tested the number of incremental updates on the runtime. The higher the number of updates, the longer was the runtime for the AFPIM algorithm. This was because frequent updates led to a higher probability that (i) the item-frequency order before and after the update was different (i.e., swapping, merging, and splitting of tree nodes) and (ii) some new items were in-troduced after the update (which leads to tree rebuild). This problem can be worsened when using a database with items from a larger domain (e.g., 10,000 distinct domain items).
In the fourth experiment, we tested scalability with the number of transactions. The results in Figure 5(d) show that mining with our proposed CanTrees had linear scalability.
A key contribution of this paper is to provide the user with a simple, but yet powerful, tree structure for ef-ficient FP-tree based incremental mining. Specifically, we proposed and studied the novel structure of CanTree (CANonical-order TREE). The tree captures the content of the transaction database, and arranges tree nodes accord-ing to some canonical order that is unaffected by changes in item frequency. By exploiting its nice properties, the CanTree can be easily maintained when database transac-tions are inserted, deleted, and/or modified. Specifically, its maintenance does not require merging and/or splitting of tree nodes. It avoids the rescan of the entire updated database or the reconstruction of a new tree for incremental updating. Moreover, our proposed CanTree can also be used for efficient incremental constrained mining of frequent pat-terns.

This project is partially sponsored by Science and Engi-neering Research Canada (NSERC) and The University of Manitoba in the form of research grants.

