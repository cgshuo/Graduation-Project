 present, learning to rank has been divided into three categories in general, they are the the pairwise approach that is based on document preference pair according to a given query and the listwise approach that is based on document list w.r.t a given query[1]. 
The pointwise approach takes the feature vector of each single document as input instance, and takes the similarity between document and query as output. Here, ranking model could be regarded as regression model or classification model. The representative algorithms are McRank[2] and Pranking[3]. The disadvantage of pointwise approach is that the relative order between documents cannot be considered in learning process. Pairwise approach takes the document pair as input instance, the representative algorithms are RankNet[4] and Ranking SVM[5]. The input of listwise approach contains a group of documents that are relevant to the given query and the which measures the difference between predict labels and ground truth labels using loss function, such as ListNet[6] and ListMLE[7], the other one regards loss function as the upper bound of evaluation metric, such as SVMMAP[8]. 
The experimental results on Letor3.0 indi cate that listwise approach generally performs better than pointwise approach and pairwise approach[1]. Hence listwise approach could improve ranking performance. DCG metric is an important measure for evaluating web retrieval results, which assumes that whether user chooses some document in a ranked list only depends on position information of the document. Nevertheless, the assumption ignores the fact that whether user chooses i-th document in ranked list also depends on other factors. Therefore a new evaluation metric named Expected Reciprocal Rank(ERR)[9] is pr oposed, which is employed for results evaluation in learning to rank challenge organized by Yahoo! at 2010. Structural SVMERR method for the above reasons, which employs structural SVMs to optimize ERR metric. We expect this method may improve the retrieval performance.

This paper is organized as follows. It starts with the introduction of the new evaluation metric ERR in Section 2. Section 3 describes our approach named SVMERR. We make the experiments to test the performance of our method in Section 4. Section 5 gives the conclusion and future work. 2.1 Expected Reciprocal Rank (ERR) ERR is an improvement of DCG metric, DCG metric assumes that whether user chooses some document in ranked list only depends on position information of the list also depends on the satisfaction that user thinks of the documents less than i , this model is called cascade model, which can model the user X  X  real browsing behaviour, and ERR metric is a ranking measure that based on cascade model. 
For the given query q , we assumes that document i satisfies the user with probablity R i , then for a given set of R i , we assumes that user browses ranked list from top to bottom, the likelihood for which the user is satisfied and stops at position r is calculated as follows: which denotes the probability that the user is not satisfied with the first r -1 documents and is satisfied with the r -th document. 
The satisfied probability R i could be estimated by maximum likelihood on the click the relevance degree of the i -th document, then: gain function of DCG , R could be defined as follows: where g is the relevance degree, g max denotes that the document is extremely relevant. For example, if a 5 point scale is used, then g equals to 0 denotes that the document is irrelevant, and the most relevant document could be marked as g equals to 4. Equation (3) indicates that the probability that the user will be satisfied and stop browsing is big when the relevance degree of document is high. should meet the condition that  X  (1) equals to 1, and  X  ( r ) tends to 0 when position r tends to infinity. Given function  X  , a cascade based metric is the expectation of  X  ( r ), the variable r is the ranking position where the user finds the document that he want. 
Expected Reciprocal Rank (ERR) is defined as a cascade based metric that using function  X  ( r ) =1/ r : number of documents in the ranked list. P r is defined in equation (1). ERR criterion is generated by taking equation (1) into equation (4): 
Average the ERR ( q ) over all queries, we could get the final ERR value. 3.1 Structural SVMs Ranking is composed of three parts, the first part is ranking model w , and the second part space vector, in which d is the number of feature vectors. A score for rank y is noted as w
T  X  ( x loss function and noted as  X  ( y k , y ). We use the structural SVMs to learn a ranking model w , the optimization problem is described as follows: y ) will be very large, to meet the constraint in equation (6), the slack variable  X  k also needs to be increased. 3.2 Optimize ERR Metric Using Structural SVMs number of documents corresponding to k , which will increase the complexity of finding solution. For this reason, cutting plane algorithm is employed to optimize ERR metric, which is described in Table 1. There are four problems to solve when structural SVMs is employed to optimize is how to find the most violated constraint y* that maximums the H ( y,w ); the third one problem to solve is the definition of variation quantity generated by exchanging documents. These will be explained in the following subsections. 
In this paper, we choose the partial order feature map which is defined as follows: number of irrelevant documents; for any y  X  Y , y ij =1 if relevant document d i is ranked document d i in the document list corresponding to query k . the ranked list y will be added to the working set, and equation (6) will be optimized on the new working set. Its mathematical expression is listed below: with respect to y , equation (8) or equivalently, w
T  X  ( x is defined as follows: 
For given query k , the value of H ( x k , y; w ) will be changed when we exchange two documents. Fist, the relevant documents and irrelevant documents will be both sorted exchanging the i -th relevant document and the j -th irrelevant document, whose definition is:  X  exchange is made up of two parts, one of which is generated by the change of  X  ( y k , y ) err . Their definitions are listed in equation (13) and equation (14). Equation (14) denotes the variation quantity generated by the loss function of ERR  X  ( y k , y ) err . Comparing with the change generated by DCG, equation (14) could reflect the influence generated by exchanging documents well, because ERR metric considers that the documents ranked in front of document i will influence user X  X  document (reference to equation (1)). 
After we defined the loss function of ERR and the variation quantity generated by exchanging documents. Then ranking model w is computed by using cutting plane ranked by scores in descending order. We use OHSUMED and TD2003 dataset in LETOR3.0 released by MSRA[12] as our proposed by LETOR3.0. OHSUMED dataset contains 106 queries, 11303 irrelevant documents and 4837 relevant documents and TD2003 dataset contains 50 queries, 516 relevant documents and 48655 irrelevant documents. The two dataset are both evenly divided into five groups for five times cross validation. We perform the experiments on these dataset, and the results are compared with Regression (point-wise approach), Ranking SVM (pairw ise approach), SVMMAP, SVMNDCG and ListNet (listwise approach). 4.1 Experiment on OHSUMED Data Figure 1 shows the precision curve for each algorithm on OHSUMED dataset, from SVMNDCG algorithm get the best result at P@2, but SVMERR algorithm does not perform well and only outperforms others at the P@5 to P@10. From Figure 2, we could see that at NDCG metric level, SVMERR performs normally compared with other algorithms. 
Table 2 shows the MAP value for each algorithm, and table 3 presents the compared result on ERR value between SVMNDCG and SVMERR. From table 2, we could see that SVMERR does not make much improvement on MAP score level compared with other meth ods, and table 3 indicates that SVMNDCG method outperforms SVMERR on ERR level. 4.2 Experiment on TD2003 Data NDCG@k curve for each algorithm, where x-axis also denotes the position k and Y-axis denotes the NDCG@k value. 
From Figure 3 and Figure 4, we could observe that the performance of SVMERR is better than others at P@1, P@2 as well as NDCG@1, NDCG@3 and so on, because the metric ERR, which is based on cascade model, takes user X  X  browsing Table 4 shows the MAP values of each algorithm at each fold in TD2003 dataset, and table 5 presents the comparison of SVMERR method and SVMNDCG method on ERR metric. Table 4 presents that at MAP metric X  level, SVMERR obtains the best result on Fold1 and Fold5 compared with other methods and make a improvement at average MAP value. Table 5 shows that compared with SVMNDCG, SVMERR gets the best value on Fold2, Fold3 and Fold5, and th e average ERR value outperforms that of SVMNDCG. 
In general, SVMERR method performs normally on OHSUMED dataset but performs better on TD2003 dataset. We analyse the reason and think that the most likely reason for this phenomenon is the scale of OHSUMED dataset is small, by contrast, TD2003 dataset X  X  scale is large. Therefore experimental result on TD2003 is more obvious. Anyway, SVMERR method definitely improves the performance in information retrieval. 4.3 Experimental Analysis We analysed the experimental results and concluded the reasons as follows. relevant with given query and the output is a ranked list, however, pointwise approach traditional classification or regression problem, and pairwise approach concerns with document preference pair according to a given query. Both of which couldn X  X  consider approaches. Therefore the experimental resu lts of the algorithms that belongs to listwise approach are good, such as SVMMAP, SVMNDCG, ListNet as well as SVMERR proposed in this paper. 
Secondly, DCG metric, which is an evaluation measure of multi-level relevance degree, could model the user X  X  browsing behaviour better. Hence the SVMNDCG method performs well. 
Finally, the DCG metric, which is based on position model, does not consider the relevance of documents above the document of interest. However, ERR metric considers this factor. In a way, ERR metr ic is an improved version of DCG, and SVMERR is the method that optimizes ERR metric, therefore SVMERR performs better than other methods. 
Optimizing ERR metric brings the improvement of ranking performance, i.e. making the ranked list better and increasing the user X  X  satisfaction. Nevertheless, SVMERR improves the performance at the expense of time. More calculation will be balance the relationship between effectiveness and efficiency. In this paper, we propose an algorithm named SVMERR, which adopts structural SVMs to optimize ERR metric. We perform the experiments on LETOR3.0 dataset. Experimental results show that optimizing ERR metric could help improve ranking performance. Our contribution contains two parts as follows. Firstly, structural SVMs is employed to optimize ERR metric; secondly, we define the loss function of ERR metric and the variation quantity generated by exchanging documents. Our future work will focus on optimizing other evaluation measures. 
