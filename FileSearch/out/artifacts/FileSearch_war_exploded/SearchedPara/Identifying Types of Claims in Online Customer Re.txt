 There has been tremendous recent interest in opin-ion mining from online product reviews and it X  X  ef-fect on customer purchasing behavior. In this work, we present a novel alternative categorization of com-ments in online reviews as either being qualified claims or bald claims .

Comments in a review are claims that reviewers make about the products they purchase. A customer reads the reviews to help him/her make a purchas-ing decision. However, comments are often open to interpretation. For example, a simple comment like this camera is small is open to interpretation until qualified by more information about whether it is small in general (for example, based on a poll from a collection of people), or whether it is small compared to some other object. We call such claims bald claims . Customers hesitate to rely on such bald claims unless they identify (from the context or oth-erwise) themselves to be in a situation similar to the customer who posted the comment. The other cate-gory of claims that are not bald are qualified claims . Qualified claims such as it is small enough to fit easily in a coat pocket or purse are more precise claims as they give the reader more details, and are less open to interpretation. Our notion of qualified claims is similar to that proposed in the argumenta-tion literature by Toulmin (1958). This distinction of qualified vs. bald claims can be used to filter out bald claims that can X  X  be verified. For the quali-fied claims, the qualifier can be used in personalizing what is presented to the reader.

The main contributions of this work are: (i) an an-notation scheme that distinguishes qualified claims from bald claims in online reviews, and (ii) a super-vised machine learning approach that uses syntactic features to learn this distinction. In the remainder of the paper, we first motivate our work based on a customer behavior study. We then describe the proposed annotation scheme, followed by our su-pervised learning approach. We conclude the paper with a discussion of our results. In order to study how online product reviews are used to make purchasing decisions, we conducted a user study. The study involved 16 pair of gradu-ate students. In each pair there was a customer and an observer. The goal of the customer was to de-cide which camera he/she would purchase using a camera review blog 1 to inform his/her decision. As the customer read through the reviews, he/she was asked to think aloud and the observer recorded their observations.

The website used for this study had two types of reviews: expert and user reviews. There were mixed opinions about which type of reviews people wanted to read. About six customers could relate more with user reviews as they felt expert reviews were more like a  X  X ales pitch X . On the other hand, about five people were interested in only expert reviews as they believed them to be more practical and well rea-soned.

From this study, it was clear that the customers were sensitive to whether a claim was qualified or not. About 50% of the customers were concerned about the reliability of the comments and whether it applied to them. Half of them felt it was hard to comprehend whether the user criticizing a feature was doing so out of personal bias or if it represented a real concern applicable to everyone. The other half liked to see comments backed up with facts or ex-planations, to judge if the claim could be qualified. Two customers expressed interest in comments from users similar to themselves as they felt they could base their decision on such comments more reli-ably. Also, exaggerations in reviews were deemed untrustworthy by at least three customers. We now present the guidelines we used to distin-guish bald claims from qualified claims. A claim is called qualified if its validity or scope is limited by making the conditions of its applicability more explicit. It could be either a fact or a statement that is well-defined and attributed to some source. For example, the following comments from our data are qualified claims according to our definition, 1. The camera comes with a lexar 16mb starter 2. I sent my camera to nikon for servicing, took 3. I find this to be a great feature.
 The first example is a fact about the camera. The second example is a report of an event. The third example is a self-attributed opinion of the reviewer.
Bald claims on the other hand are non-factual claims that are open to interpretation and thus cannot be verified. A straightforward example of the dis-tinction between a bald claim and a qualified claim is a comment like the new flavor of peanut butter is being well appreciated vs. from a survey conducted among 20 people, 80% of the people liked the new flavor of peanut butter. We now present some exam-ples of bald claims. A more detailed explanation is provided in the annotation manual 2 :  X  Quantifiable gradable words such as small ,  X  Unattributed opinion or belief: A comment  X  Exaggerations: Exaggerations such as on ev-The two categories for gradable words defined above are similar to what Chen (2008) describes as vague-ness, non-objective measurability and imprecision . Initial work by Hu and Liu (2004) on the product review data that we have used in this paper focuses on the task of opinion mining. They propose an ap-proach to summarize product reviews by identifying opinionated statements about the features of a prod-uct. In our annotation scheme however, we classify all claims in a review, not restricting to comments with feature mentions alone.

Our task is related to opinion mining, but with a specific focus on categorizing statements as either bald claims that are open to interpretation and may not apply to a wide customer base, versus qualified claims that limit their scope by making some as-sumptions explicit. Research in analyzing subjec-tivity of text by Wiebe et al. (2005) involves identi-fying expression of private states that cannot be ob-jectively verified (and are therefore open to interpre-tation). However, our task differs from subjectivity analysis, since both bald as well as qualified claims can involve subjective language. Specifically, objec-tive statements are always categorized as qualified claims, but subjective statements can be either bald or qualified claims. Work by Kim and Hovy (2006) involves extracting pros and cons from customer re-views and as in the case of our task, these pros and cons can be either subjective or objective.

In supervised machine learning approaches to opinion mining, the results using longer n-grams and syntactic knowledge as features have been both pos-itive as well as negative (Gamon, 2004; Dave et al., 2003). In our work, we show that the qualified vs. bald claims distinction can benefit from using syn-tactic features. We applied our annotation scheme to the product re-view dataset 4 released by Hu and Liu (2004). We annotated the data for 3 out of 5 products. Each comment in the review is evaluated as being quali-fied or bald claim. The data has been made available for research purposes 5 .

The data was completely double coded such that each review comment received a code from the two annotators. For a total of 1 , 252 review comments, the Cohen X  X  kappa (Cohen, 1960) agreement was 0 . 465 . On a separate dataset (365 review com-ments) 6 , we evaluated our agreement after remov-ing the borderline cases (only about 14%) and there was a statistically significant improvement in kappa to 0 . 532 . Since the agreement was low, we resolved our conflict by consensus coding on the data that was used for supervised learning experiments. For our supervised machine learning experiments on automatic classification of comments as qualified or bald, we used the Support Vector Machine classifier in the MinorThird toolkit (Cohen, 2004) with the de-fault linear kernel. We report average classification accuracy and average Cohen X  X  Kappa using 10-fold cross-validation. 6.1 Features We experimented with several different features in-cluding standard lexical features such as word uni-grams and bigrams; pseudo-syntactic features such as Part-of-Speech bigrams and syntactic features such as dependency triples 7 . Finally, we also used syntactic scope relationships computed using the de-pendency triples. Use of features based on syntactic scope is motivated by the difference in how quali-fied and bald claims are expressed in language. We expect these features to capture the presence or ab-sence of qualifiers for a stated claim. For example,  X  I didn X  X  like this camera, but I suspect it will be a great camera for first timers.  X  is a qualified claim, whereas a comment like  X  It will be a great camera for first timers.  X  is not a qualified claim. Analysis of the syntactic parse of the two comments shows that in the first comment the word  X  X reat X  is in the scope of  X  X uspect X , whereas this is not the case for the sec-ond comment. We believe such distinctions can be helpful for our task.

We compute an approximation to the syntactic scope using dependency parse relations. Given the set of dependency relations of the form relation, headWord, dependentWord , such as
AMOD,camera,great , an in-scope feature is de-fined as INSCOPE headWord dependentWord (IN-SCOPE camera great). We then compute a tran-sitive closure of such in-scope features, similar to Bikel and Castelli (2008). For each in-scope feature in the entire training fold, we also create a corre-sponding not-in-scope feature which triggers when either (i) the dependent word appears in a comment, but not in the transitive-closured scope of the head word, or (ii) the head word is not contained in the comment but the dependent word is present.

We evaluate the benefit of each type of feature by adding them individually to the baseline set of unigram features. Table 1 presents the results. We use the majority classifier and unigrams-only perfor-mance as our baselines. We also experimented with using the same feature combinations to learn the opinion category as defined by Hu and Liu (2004) [HL-OP] on the same subset of data.

It can be seen from Table 1 that using purely unigram features, the accuracy obtained is not any better than the majority classifier for quali-fied vs. bald distinction. However, the Part-of-Speech bigram features and the not-in-scope fea-tures achieve a marginally significant improvement over the unigrams-only baseline.

For the opinion dimension from Hu and Liu (2004), there was no significant improvement from the type of syntactic features we experimented with. Hu and Liu (2004) X  X  opinion category covers several different types of opinions and hence finer linguis-tic distinctions that help in distinguishing qualified claims from bald claims may not apply in that case. In this work, we presented a novel approach to re-view mining by treating comments in reviews as claims that are either qualified or bald. We argued with examples and results from a user study as to why this distinction is important. We also proposed and motivated the use of syntactic scope as an ad-ditional type of syntactic feature, apart from those already used in opinion mining literature. Our eval-uation demonstrates a marginally significant posi-tive effect of a feature space that includes these and other syntactic features over the purely unigram-based feature space.
 We would like to thank Dr. Eric Nyberg for the helpful discussions and the user interface for doing the annotations. We would also like to thank all the anonymous reviewers for their helpful comments.
