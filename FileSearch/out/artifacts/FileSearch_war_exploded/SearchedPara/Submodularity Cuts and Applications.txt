 limited cardinality) has been addressed in the community [10, 13, 22]. stated below, this approach is inefficient even for moderate problem sizes. sensor placement and feature selection in text classification. examples in Sect. 5, and conclude the paper in Sect. 6. We start with a subset S objective function values are guaranteed to be not better than f ( S algorithms for solving mathematical programming problems [18, 11, 20]. 2.1 Lov  X  asz extension described in this subsection, is summarized in Table 1. Table 1: Correspondence between continu-ous and discrete. f is submodular Thm. 1  X  X  X   X  f is convex Given any real vector p  X  R n , we denote the m distinct elements of p by  X  p is not necessarily submodular, is defined as where U of f and the convexity of  X  f is given [15, 16]: if and only if  X  f is convex.
 Now, we define I a one-to-one correspondence between I set of half-spaces  X  that is, P is of the form P = { x  X  R n : A &gt; A a polytope D the analogy with the way of solving the continuous problem: max {  X  f ( x ) : x  X  D is, can we solve it and how good is the solution? 2.2 Submodularity cuts concave minimization, which rests on the following property (see, e.g., [11]). a vertex of P . I that any characteristic vector I between S ( D problem max {  X  f ( x ) : x  X  D To derive the submodularity cut, we use the following definition:  X  -extension of x in direction d  X  R n \{ 0 } (with respect to g ) where  X   X  R  X  X  X  X  : of polytope P  X  D construct a submodular cut as follows. Let v  X  V ( P ) be a vertex of P such that v = I S  X  S ( P ) , and let K = K ( v ; d linearly independent vectors d the vectors d Since the vectors d that contains y where e = (1 ,  X  X  X  , 1) T  X  R n and Y = (( y two halfspaces H  X  = { x : e T Y  X  1 x  X  1 + e T Y v } and H Obviously the point v is in the halfspace H  X  , and moreover, we have: Lemma 4 Let P  X  D that v = I H  X  -extensions of v in linearly independent directions d Proof Since P  X  K = K ( I of  X  f over R is attained at a vertex of R . Therefore, we have be given in Sect. 3.2. As v  X  S ( P  X  H  X  ) and v /  X  S ( P  X  H The submodular cut algorithm updates P  X  P  X  H algorithm is assured by the following theorem. Algorithm 1 General description of the submodularity cuts algorithm. 1. Compute a subset S 0 s.t. | S 0 | X  k , and set a lower bound  X  0 = f ( S 0 ) . 2. Set P 0  X  D 0 , stop  X  f alse , i  X  1 and S  X  = S 0 . 3. while stop=false do 4. Construct with respect to S i  X  1 , P i  X  1 and  X  i  X  1 a submodularity cut H i . 5. if S ( P i  X  1 ) = S ( P i  X  1  X  H i  X  ) then 6. stop  X  true ( S  X  is an optimal solution and  X  i  X  1 the optimal value). 7. else 9. Compute S i  X  S ( P i ) , and set P i  X  P i  X  1  X  H i 10. end if 11. end while Proof In the beginning, | S ( D 1. So, the number of iterations is finite. ming (BILP) solver. The pseudo-code of the resulting algorithm is shown in Alg. 2. 3.1 Construction of submodularity cuts Given a vertex of a polytope P  X  D independent directions d choice satisfying P  X  K can be substituted.
 If | S | &lt; k , then directions d focus on the case where | S | = k . Define a neighbor S That is, the neighbor S that I ( i  X  S , j  X  V \ S ), this computation is O ( nk ) . Suppose that S = { i and V \ S = { j S directions { d It is easy to see that d Lemma 6 For the directions d contains the polytope D  X  supplementary material (Sect. A). Algorithm 2 Pseudo-code of the submodularity cuts algorithm using BILP. 1. Compute a subset S 0 s.t. | S 0 | X  k , and set a lower bound  X  0 = f ( S 0 ) . 2. Set P 0  X  D 0 , stop  X  f alse , i  X  1 and S  X  = S 0 . 3. while stop=false do 4. Construct with respect to S i  X  1 , P i  X  1 and  X  i  X  1 a submodularity cut H . 6. if c  X   X  1 + e T Y  X  1 v i  X  1 then 7. stop  X  true ( S  X  is an optimal solution and  X  i  X  1 the optimal value). 8. else 10. Set P i  X  P i  X  1  X  H + and i  X  i + 1 . 11. end if 12. end while 3.2 Stopping criterion and next starting point the next starting subset S programming. Using Eq. (6), we obtain: Then S ( P )  X  H  X  if c  X   X  1 + e T Y  X  1 v .
 which can be used as a starting subset of the next iteration (see Fig. 1). (1) can be reformulated [17] as where  X  n  X  -optimal solution using both the lower and upper bound. Figure 3: Averaged computational time (log-scale) for com-puting exact and  X  -optimal solutions by the submodularity cut algorithm and existing algorithm by Nemhauser and Wolsey. we used the solution by a greedy algorithm as initial subset S mex function. If  X  =  X  in Eq. (5), we set  X  =  X  5.1 Artificial example maximization problem (1) with respect to the nondecreasing submodular function: where C = c C quickly while the upper bound decreases gradually. 5.2 Sensor placements where F This is because the greedy solutions are good when k is either very small or large. 5.3 Feature selection in text classification predictive power even though the number of the chosen words is very small. ments and feature selection in text classification.
 machine learning problems.
 Acknowledgments Also, we are very grateful to the reviewers for helpful comments.
