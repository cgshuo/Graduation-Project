 H.2.4 [ Systems ]: Text databases; I.5 [ Pattern Recogni-tion ]: Models and Design Methodology.
 Algorithms, Performance, Experimentation.
 k NN Text categorization, Training corpus, Redundancy.
Text Categorization is an important and extensively stud-ied problem in information retrieval area[1, 2, 3]. How-ever, we notice that in the literature text categorization re-search has been focusing on categorization methods. Few researchers have paid much attention to the training cor-pora from the point of text categorization research view. Nevertheless, it was observed that even for a specified cate-gorization method, classifiers trained with different training corpora show different classification performances, i.e., clas-sifier performance is relevant to training corpus.
In this paper, we study the redundancy of training corpus in the context of k NN text categorization, aim to explore how to judge whether a training corpus has redundancy and how to reduce the redundancy if it has. With the rapidly increasing of text documents, the sizes of training corpora are growing(Reuters-21578 has 21578 news articles, while RCV1 contains over 800K). Redundancy is an unavoidable existence in training corpora building and utilizing. Reduc-ing redundancy of training corpora can help to compact the training corpora, subsequently boost the efficiency of the training and classification processes, and even improve clas-sification performance. Note that redundancy is different from duplicate problem[4] that means similar text content , while redundancy indicates similar semantic content .
We give the definition of redundant training examples from the point of geometry view, and develop a redundancy reduction algorithm. Experiments are conducted to demon-strate the existence of redundancy in training corpora and validate the proposed algorithm.

From the point of geometry view, every document can be seen as a unit vector in document space. Intuitively, docu-ments belong to the same class are closer to each other in document space than those that are not in the same class. Thus documents in the same class form a dense hyper-cone area in document space, and each class has its own bound-ary. A training corpus corresponds to a cluster of hyper-cones each of which corresponds to a distinctive class. Cer-tainly, different hyper-cones may overlay with each other.
Examining the process of k NN text classification, we can see that outer documents or boundary documents (locating near the boundary) of each class (or document hyper-cone) play more decisive role in classification. On the contrary, inner documents or central documents (locating in the in-terior area) of each class (or document hyper-cone) are less important, because their contribution to classification de-cision can be obtained from the outer documents. In this sense, inner documents of each class can be seen as  X  redun-dant  X  documents. Here, redundant documents are those just do not tell us much about making classification decision, the job they do in informing classification decision can be done equivalently by other documents.
In what follows, we build the redundancy model step by step with five definitions.
 Definition 1 . Given a document d in training corpus D and a natural number k ,thesetof k nearest documents to d in D constitutes the k -reachability set of d , i.e., k -reachability ( d )= { d i | d i  X  D and d i  X  kNN ( d ) }
Definition 2 . Given a document d in training corpus D and a natural number k , there is a set of documents in the same class that d belongs to, in which each document X  X  k -reachability set contains d . We define this set of documents the k -coverage set of d , i.e., k -coverage ( d )= { d d  X  class ( d )and d  X  k -reachability ( d i ) } . Here, class ( d ) indicates the class to which d belongs.
 Definition 3 . Given a document d in training corpus D , if it could be correctly classified with k -reachability ( d ) based on the k NN method, in other words, the class of d is implied by k -reachability ( d ), then we say it is a potential ly redundant document in D .
 Definition 4 . Given a document d in training corpus D ,itisa critical document if one of the following con-ditions is fulfilled: 1) there is at least one document d in k -coverage ( d ) whose class can not be implied by its k -reachability ( d i ); 2) after d is pruned from D ,thereisat least one document d i in k -coverage ( d ) whose class can not be implied by its k -reachability ( d i ). Otherwise, we say the document is non-critical .

Definition 5 . Given a document d in training corpus D , it is redundant in D only if 1) it is a potentially redundant document, and 2) it is non-critical .
While doing redundancy reduction, it is worthy of point-ing out that the order of reducing the redundant examples is also crucial for the reducing of one redundant document may influence the decision on whether other documents are redundant. Intuitively, inner documents of a class in the training corpus should be reduced before outer documents for inner documents are prone to being implied by outer documents. We assign each redundant document a reduc-tion priority : 1) a document has more documents of its own class around itself has higher reduction priority; 2) a doc-ument locates closer to the center of its class has higher reduction priority; 3) a document locates further from the documents of other classes has higher reduction priority.
The algorithm of redundancy reduction for training cor-pus is as follows. Here, we assume that there is only one class in the training corpus. For multiple classes, just car-rying out the reduction process over the classes one by one. Algorithm 1 Redundancy-Reduction( T , P ) Input : training corpus T = { d i | i =1  X  X  T |} Output : result corpus P (a subset of T ) 1: P = T ; S = X (empty set); 2: For each document d in T 3: Compute k -reachability ( d ); 4: Compute k -coverage ( d ); 5: For each document d in T but not in S 6: If d is redundant (according to def. 5) and has the 7: S = S  X  X  d } ; P = P -{ d } ; 8: For each document d i in k -coverage ( d ) 9: Remove d from k -reachability ( d i ); 10: Add the k +1 st nearest neighbor d j of d i to k -11: Add d i to k -coverage ( d j ) 12: Return P .
To demonstrate the existence of redundancy in training corpora and validate the effectiveness of the proposed algo-rithm, experiments are conducted over the Reuters corpus version 3(simply Apte ) and version 4(simply PARC ). We do not use the Apte and PARC corpora directly, instead, we first remove the training and test documents that belong to two or more categories from the two corpora, and then se-lect the top 10 and 20 categories to form our own compiled Apte and PARC corpora, which are referred to as TC-Apte and TC-PARC respectively. In experiments, the training parts of TC-Apte and TC-PARC are pruned first by us-ing the redundancy reduction algorithm; the reduced result corpora are denoted as TC-Apte-reduced and TC-PARC-Performance precision 0.940 0.911 Performance precision 0.880 0.842 reduced respectively. Classifiers are trained by using TC-Apte , TC-PARC and theirs corresponding reduced corpora TC-Apte-reduced and TC-PARC-reduced .The reduction ra-tio (defined as the ratio of the number of reduced training documents over the total number of training documents in the original training corpus) and the trained classifiers X  per-formance( precision ) are measured. Experimental results are shownintable1andtable2respectively.

Examining the results in table 1 and table 2, we can see that for TC-Apte , more than 82% training documents are reduced at the cost of less than 3% classification precision degradation; while for TC-PARC ,the reduction-ratio is more than 72%, resulting in less than 4% degradation of classifi-cation precision . Although the sizes of training corpora are drastically reduced, their classification competence can be still preserved at nearly the same level as that without re-dundancy reduction. This means that redundancy does exist in training corpora, and the proposed model and algorithm can effectively reduce the redundancy hidden in training cor-pora. The reason of the minor precision degradation is that the redundancy reduction is completely based on the train-ing corpus that does not cover the testing documents. This inspires us to develop more effective model and algorithm for redundancy reduction.
We investigated redundancy of training corpus for text categorization, a model to describe redundancy and an al-gorithm to reduce redundancy are proposed. Experiments show that redundancy does exist in training corpora. This work was supported by the National Natural Science Foundation of China(NSFC) under grant 60373019, and par-tially supported by the Shuguang Scholar Program of Shang-hai Municipal Education Committee.
