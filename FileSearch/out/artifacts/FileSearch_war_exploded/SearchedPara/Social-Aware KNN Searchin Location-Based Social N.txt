 In the past few years, location-based social network services such as Foursquare 1 and Jiepang 2 , have seen increasing popularity, attracting millions of users. Sup-ported by the capabilities of smart mobile devices and location-aware technology, GPS and Wi-Fi, people can easily share their locations, activities, and other in-formation to their friends. The appearing LBSN services not only help users to strengthen their social connections, but also provide useful informations and generate appealing applications.

In this paper, we consider a new type of qu ery, searching nearest objects(points of interests) with friends X  visiting experiences provided by the LBSN system. We propose F riend based K -N earest N eighbor(F-KNN) query, which aims to find objects being spatially close to query lo cation as well as receiving high evalua-tions from the friends of the user. The application of F-KNN query is obvious. For example, if we want to search a suitable restaurant for dinner, however, we have not been familiar with the surrounding restaurants, then all the restaurants are candidates and it is better for us to pick a restaurant which is near our place and received high evaluations from our friends. Utilizing friends X  experiences in spatial objects search can benefit us fr om many aspects. First, it can guaran-tee the quality of searching results. Traditional query for k-nearest neighbors only concerns the spatial distance betw een location and objects. The quality of the provided objects can X  X  be guaranteed. On the contrary, we can retrieve high quality objects if our friends have good evaluations on those objects. Second, it provides believable results to users. The query can provide good explanations to users and improve their user experi ence. Users are more tend to believe and choose objects once showing friends X  experience or evaluations on those objects.
In this paper, we employ three essential information provided by the LBSN system. (1) The social links of a user, i.e. the friends of a user. (2) The geographic locations of the objects. (3) The user r ecord which correlates a user with an ob-ject. We use a numerical score to repre sent the correlation between a user and and an object. Numerical scores are fundamental representations of interactions between users and objects, as users often leave five-star scores on objects and most recommendation system eventually recommend their result with a gener-ated score. The F-KNN query returns cl ose objects with higher scores given by the friends of the user. One big challenge to answer F-KNN query is to achieve high efficiency as it requires to support millions of spatial objects and more user records.

As far as we known, F-KNN query is first proposed in our work. In previous studies, people are mainly focused on the work of predicting a grade when given a pair of a user and an object [1,2,3]. For example, in [1], some complex metrics are proposed by combing the information of users and objects to calculate a numeric value for recommendation. These scores can be integrated into F-KNN query and our work. The main contributions of our work are, (1) we formulate the F-KNN query. (2) We propose effective index and algorithms to support F-KNN query. Moreover, we propose two efficient refine methods to accelerate the querying process. (3) Experimenta l results show F-KNN query is effective and our methods achieve high performance.

The rest of this paper is structured as follows: section 2 defines the prelimi-nary concepts and introduces the baseline method. We introduce our index and algorithm for F-KNN query in section 3 , followed by two refine methods in sec-tion 4. Experimental studies are shown in section 5. Section 6 talks about related works. Conclusions are given in section 7. 2.1 Problem Formulation We first define the three employed essentia l information(mentioned in section 1), then formulate the F-KNN problem.

Social Links. Let u denote a user in a LBSN system, all users are marked by their unique ids. The social links of u are all of his friends, which can be denoted represent the closeness u towards u i or the confidence level of u on u i . The total weight of closeness is constrained to be one. The weight can be evaluated from many metrics, such as the number of common friends, number of communicating messages and so on. The size of S ( u ) is denoted as | S ( u ) | .

Objects. We use o to represent objects, o i is an id of an object with its geographic coordinate.

User Record. we use f u ( o j ) to represent a normalized score(with value be-tween zero and one) of o j given by user u .
 Definition 1 (Spatial Score).
 D ( l i ,l j ) is the euclidean distance between location l i and l j . The spatial score is normalized through the maximum s patial distance between objects( D max ). The closer two locations are, the lower spatial score they get.
 Definition 2 (Friends Recommendation Score). We compute the friends recommendation score of user u on objects o j by reviewing all the advices of friends of u . i.e.
 Every friend X  X  influence on u is weighted according to the closeness weight( w i ). Obviously F f ( u, o j ) is also a normalized score. Fo r consistency with the spatial distance, we assume the lower the value of the user record f u ( o j ), the higher evaluation o j gets from u i . For those objects that do not receive any evaluation from u , f u ( o j ) is set to be the maximum value one. However, our methods can be easily modified to support the case that the user record is the higher the better.
We propose a F-KNN score to combine both the spatial score and friends recommendation score to qualify the objects.
 Definition 3 (F-KNN Score).
 where q.u is the query user and q.l is his current geographic coordinate.  X  is tunable value to weight the importance of spatial score and friends recommen-dation score. Given a F-KNN query, it retrieves K objects with smallest F-KNN scores.
 Example 1. Figure 1 illustrates an example of F-KNN query. Social links, user records and spatial objects are described in the figure. Suppose the objects in the figure are all restaurants. The query combines both the user X  X ( u 1 ) location and his friends X ( u 2 and u 3 ) visiting experience to return Top-K restaurants. The spatial scores between q.l and objects are also showed in the figure. Let K=2,  X  =0 . 5, according to the F-KNN score ment ioned before, the F-KNN score of o 6 can be calculated as w the best restaurants with F ( q,o 7 )=0 . 27 and F ( q,o 6 )=0 . 33 for the query. o 1 , though locates nearest the query place, is not selected since u 2 and u 3 give it low evaluations. 2.2 Baseline Method The F-KNN problem is a special top K problem. We utilize the well-known threshold algorithm [4] as the baseline method and it X  X  much more efficient than linearly checking all the objects. The algorithm is based on two types of accesses to data: sorted access and ra ndom access. In our baseline solution, a quadtree [5] is used to get the sorted access for spatia l distance(by sequently find the nearest objects). And sorted inverted lists for u ser records are used to get sorted access for friends recommendation score. More details about the algorithm is described in [4]. To solve the F-KNN query efficiently, we propose a F riend based-Quadtree (F-Quadtree) index by incorporating the user record and spatial objects into a quadtree. Then we propose a region-based pruning method based on the index. The index and the querying process algorithm are introduced in section 3.1 and 3.2 respectively. 3.1 F-QuadTree Index Structure The F-Quadtree index is consisted of two parts, a quadtree index in memory and inverted lists in disk.
 Quadtree. We use a quadtree [5] to index geographic information of objects. A quadtree can recursively partition the objects into different regions. Inverted lists. We use inverted lists to index user records. We propose two types of inverted lists. (1) region based inverted index L g ( u ), L g ( u )storesall the r egional user scores. A regional user score is the minimum score among all the scores that the user has given to the objects in the region. Formally, we define f u ( R ) as the regional user record for u on region R ,where all the regional user scores of u are inserted into L g ( u ). If user u hasn X  X  been given scores for any object in region R i , f u ( R i ) is set as the maximum value and will not be stored in index. (2) user-object based inverted index L ( u, R ), which stores the user record of objects in R given by u .Theentryof L ( u, R )isauser u and a leaf region R . The items in the list are objects in R with receiving scores from u . The items are sorted in ascending order according to their scores. Example 2. The F-Quadtree index for example 1 is in Figure 2. For example, smallest score given to the objects( o 3 ,o 4 ,o 5 )in R 7 by u 2 . All the regional user and L g ( u 5 ) are omitted due to the space limitation. For every leaf region of quadtree, such as R 3 ,R 5 ,R 6 ,R 7 ,R 8 ,R 9 in figure 2, we store user record in re-spect inverted lists. For instance, o 3 ,o 4 ,o 5 are in R 7 , their related user records are stored in L ( u 2 ,R 7 )and L ( u 3 ,R 7 ). 3.2 Querying Process Region-Based Pruning. We can compute the lower bound of both spatial score and friends recommendation score. On one hand, given an object o i in region R , we use the minimum spatial distance from query to the region as the spatial lower bound(denoted by F s ( q.l,R )). If o i  X  R , F s ( q.l,R ) = 0; otherwise it is the minimum value among the distances from the query to the four corners and four boundaries of the region. On the other hand, the regional user score is a lower bound of friends recommendati on score. Therefore, a lower bound of F-KNN score can be computed as Algorithm 1 : Query Process( q,K ) It X  X  a weighted sum of lower spatial bound and regional user scores. For example, f query to all regions in figure 4 with F s ( q.l,R 8 ) = 0. For any objects in R ,the F-KNN score is larger than the lower bound F ( q,R ). We use the lower bound to prune unrelated regions, i.e. if the lower bound of a region is larger than the K -th smallest F-KNN value of objects, then the whole region can be pruned. Querying Process. We employ a best-first traver sal algorithm to process F-KNN query. Algorithm 1 shows the pseudo-code of the algorithm. A priority queue(line 4) is used to keep track of the nodes of quadtree and a min heap T is used to keep the temporary results. T K is the k-th smallest value in the heap T . The algorithm begins by searching the root of F-Quadtree(line 5). When deciding which node to visit next, the algorithm picks the node N with the smallest regional F-KNN score value in the priority queue(line 7). If the picked node is not a leaf node, then it computes the lower bound F ( q,R )forfour of its children, and pushes the survived children into the queue(line 9 to 12). Otherwise, the algorithm starts to search the objects in the leaf region(line 16). The whole algorithm terminates when T k is smaller than the smallest value of the lower bound of regions in the priority queue(line 6). If the picked node is a leaf node, we need to search the leaf node(line 16). The basic method to search a leaf node is simply verifying the objects in the region one by one. However, we have stored the user-object based inv erted lists of the region and user(i.e. L ( u, R )) in the F-Quadtree index. Therefore, when searching a leaf region, we can optimize the function by using the threshold algorithm [4]. The details are omitted due to the space limitation.
 Example 3. Again we consider the query in example 1. The algorithm starts by first pushing region R 1 into the priority queue with lower bound F ( q,R 1 )=0 . 1. It then executes the steps in figure 4. It w ill terminate and return T after step 6. 4.1 User Based Partition The F-Quadtree index groups objects with similar locations into spatial regions. Then the algorithm can directly prune unrelated regions during querying pro-cess. In this section, we further divide the objects in each leaf region into several partitions by consid ering social links and user reco rds. The partition can accel-erate the process of Searching Leaf Region in algorithm 1.

We divide the objects into k partitions with a clustering based method. We cluster the objects according to the user relationship and user record. The clus-tering approach takes three steps: 1. Instead of clustering objects, we first cluster the users into k partition (denoted as P 1 ,P 2 ...P k ). We cluster the users through their social links with a community detection(clustering) algorithm [6], provided by an open source tool Jung [7]. 2. For every pair of object o 3. For every o g ( o i ,P j ). Thus all objects can be clustered into K partitions.

The clustering based approach owns a good explanatory. First, the users are clustered into k social circles and we will partition the objects into those circles. g ( o i ,P j ) calculates the sum of scores o i received from the circle P j .Itcanbe viewed as the preference of circle P j to object o i . Finally the object will select the circle which gives the highest evalu ation to the object. Thus all the objects are divided into k clusters according to their preferences of circles. During the querying process, when searching every leaf region(function Searching Leaf Region in algorithm 1), we separately run the threshold algorithm in every partition. The query will benefit since when given the query user, most of his friends will be in the corresponding circle and the objects pre-ferred by the circle will be partitioned t ogether. The objects in other clusters have very low probability to become results since they get user records mainly from other circles. The initial threshold generated by the threshold algorithm in each partition can help to prune unrelated partitions. When the initial threshold of a partition is larger than T K , the whole partition is pruned.
 Example 4. See the query in example 1. If we cluster the users into two clusters, P R 8 are partitioned according to the user circles. o 1 is partitioned into P 2 since it only receives evaluation from u 2 and u 3 . o 2 and o 9 are partitioned into P 1 since they get evaluation mainly from u 4 and u 9 . We create inverted lists separately for P 1 and P 2 . During the query process, in st ep 6 of figure 4, when searching R , T K =0 . 33, o 2 and o 9 in P 1 will be skipped since the initial threshold for P (  X   X  0 . 1+(1  X   X  )  X  (0 . 8  X  0 . 8+0 . 2  X  0 . 6) = 0 . 39) is larger than T K . 4.2 Improvement With Memory Materialization All the user records are stored in the disk and loading the inverted lists into memory is the major cost of query processing. In this section, we improve the efficiency of F-Quadtree base d algorithm by materializing some user records into memory in advance, which can significa ntly reduce the cost of loading unneces-sary inverted lists.

We first discuss the  X  X pecial X  objects which will be loaded into memory. Recall the definition of regional user score f u ( R )(section 3), which is the smallest value of user record f u ( o i )in R . However, if we pick the object o i with smallest value into memory and make f u ( R ) the second smallest value, and check o i in memory before the F-Quadtree based algorithm, the querying process is still completely correct. Similarly, we can also pick obj ects with the smallest, second smallest values together (also the third one and so on) into memory, then replace the value of f u ( R ) with the remaining smallest value. This strategy can benefit us from two aspects: first, it increases the filtering ability of the quadtree nodes(regions) since it increases the lower bound of regions. Second, by checking the objects in memory first, it will provide an early smaller heap threshold, which will also help to reduce computing unrelated regions and objects. However, maintaining the Algorithm 2 : Greedy Algorithm( q,K ) objects in memory also leads to additional memory space cost and computations. The best choice is that we only pick a few effective objects into memory. Example 5. Consider o 3 in example 1 and Figure 2. In R 7 , o 3 is the smallest for u 2 and u 3 . If we pick o 3 into memory, f u 2 ( R 7 ) becomes 0.8 and f u 3 ( R 7 ) becomes 0.6. During the querying process, the F-KNN score of o 3 in memory will be checked first. Then for the rest objects, we use algorithm 1, the regional F-KNN score of R 7 becomes 0.46 in step 3 of figure 4, then the whole region can be skipped, which means the step 5 in figure 4 is not necessary anymore. Next we formally describe the approach to pick the  X  X pecial X  objects. Let L ( u, R )denotethe k -th item in the inverted list L ( u, R )and L k ( u, R ) .o is its k } , as the set contains the front k objects, from the first to the k -th objects in L ( u, R ). We formulate the pruning benefit of the set L k ( u, R )as W ( u ) represents the influence of a user. Apparently, the probability of a user appearing in a query becomes higher if h e owns more friends. Therefore, we set represents the increasing value from the k -th record score to the ( k + 1)-th one. | S ( u 2 ) | X  ( L 2 ( u 2 ,R 7 ) .s  X  L 1 ( u 2 ,R 7 ) .s )=0 . 8
We only load a limited number(denoted as N m ) of objects into memory. A greedy algorithm is utilized to choose the objects with largest pruning benefits. In every step, we greedily pick the set with largest pruning benefit, all the objects in the set will be chosen. The pseudo-code of the greedy algorithm is described in algorithm 2. We can get set L k ( u, R ) and compute  X  ( L k ( u, R )) easily by scanning the inverted lists L ( u, R ).

During the query process, the objects in memory will be check first. The records in memory are also organized as in verted lists. The entry of the inverted lists is user and the list items are objects with corresponding scores. The thresh-old algorithm is first run on the inverted lists in memory and generates some results into heap, then we use the F-Quadtree based algorithm on rest objects. In this section,we present e xperimental studies of our methods to F-KNN queries.
Experiment Setup. We implemented all the methods with C++, on a PC with a 3.2Ghz CPU and a 2GB RAM and an ubuntu 12.0 system. We used two real data sets, a data set of Gowalla 3 and a data set crawled from Twitter(tweets with location information). We simulated the user record by the number of their check-ins( N C ) with the equation 1 N number of a user checked at an object, the smaller the score was. The numerical closeness between friends were evaluate d and normalized as their common friends number. The parameters of the two data sets are in table 1. For each data set, we randomly selected 2000 user check-ins as queries.
 F-KNN Query Effectiveness. We first evaluated the effectiveness of F-KNN query. We used the real check-ins of querying user as ground truth, and compared the precision and recall of F-KNN query with the traditional S patil O nly(SO) query(i.e. return Top-K nearest objects). Figure 7 reflects the precision and recall on the two data sets with different K . The F-KNN query was far more effective than only considering spatial distance. For example, on Gowalla data, almost one of top-10 F-KNN(10%) results was in line with the real check-ins, and the top-40 F-KNN query could return 12% real check-ins.
Efficiency Comparison. We compared three methods in our experiments. (a) The baseline method, T hreshold A lgorithm(TA). (b) The F -Quadtree B ased A lgorithm(FBA) without refinements. (c) R efined F -Quadtree B ased A lgorithm (RFBA). We compared the three methods through different parameters of F-KNN query, K and  X  . Figure 8(a),8(c) evaluates the influence of K on the three algorithms when  X  is 0 . 5. Figure 8(b) and 8(d) evaluates the influence of  X  when K is 10. On both different parameters and data sets, FBA performs much better than TA. It took less than 100 microseco nds to process a F-KNN query for FBA, however, RFBA can still speeding up over FBA by almost 60%.
Evaluation on refinements. We also evaluated the performance of refine-ments in section 4 separately. We only give the results on the Gowalla data set due to the space. We set  X  =0 . 5. We separately incorporated the two refine methods, user partition(denoted as FBA+UP, partition number was set to be 20) and memory materialization(FBA+MM). To see the result clearly, we di-vided the running time into loading time and querying time. The loading time included the process of loading L g ( u )and L ( u, R ) into memory and the rest was the querying time. Figure 8(e),8(f) shows both two methods improved the performance. FBA+UP in Figure 8(e) wo rked on decreasing the number of ver-ifying objects in every region. It speeded up the querying time by almost 50%, which meant it further pruned half of the objects than the threshold algorithm in every region. Memory materialization in Figure 8(f) also greatly improved both the loading time and querying time.

Index Size. We demonstrated the index sizes in table 2. The index size of inverted lists of F-Quadtree was 3-4 times larger than simple inverted lists of TA. We also given the memory cost for RFBA. The cost was small and apparently the quadtree index could easily support millions of spatial objects in memory. Spatial Objects Search. Traditional k-nearest neighbor queries(KNN) in spa-tial databases focuses on searching object s near the query location with euclidean distance. Usually R-tree index [8] and quadtree [5] index are used to assist the KNN queries. Recently, spatial search c ombining with some other information has attracted a lot of attention from research community. Most of the studies consider spatial search with text or key word content [9,10]. Few related work consider both score and spatial information. Cong et. al. [11] and Senjuti et.al. [12] give single scores on objects. Spatial skyline [13,14] also considers numerical attributes(scores) of spatial objects. The scores are independent on users, i.e to-ward different users, objects have static scores. Obviously, F-KNN query differs from those problems. In an F-KNN que ry, an object can receives many scores from the friends of users. And those scor es are different according to different querying users. To our knowledge, there are no similar work existing in spatial objects search field.

Friend Based Location Recommendation. Recommendation objects with friends X  influence has aroused interest from data mining researches [2,1,3]. Their work focuses on predicting proper score s for different objects and users. These scores can be integrated int o F-KNN query and our method. We conclude our work in this paper. First, we address the new problem F-KNN query. Then we propose the F-Quadtree index and develope efficient algorithm to answer F-KNN query. Then we propose the user based partition and use memory materialization t o further accelerate the query process. Experimental studies on two real data sets show our methods achieve good performance. Acknowledgement. This work was partly supported by NSF of China (61272090), Tsinghua-Samsung Joint Laboratory,  X  X ExT Research Center X  funded by MDA, Singapore (WBS: R-252-300-001-490), and FDCT/106/2012/A3.
