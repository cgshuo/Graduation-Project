 Average precision and R-precision are two of the most com-monly cited measures of overall retrieval performance, but their correlation, though well-known, has defied explana-tion. We recently devised a geometric interpretation of R-precision which suggests that under a reasonable set of as-sumptions, R-precision approximates the area under the precision-recall curve, as does average precision, thus ex-plaining their correlation. In this paper, we consider these assumptions and our geometric interpretation of R-precision in order to further understand, and make reasonable use of, the information that R-precision provides. Given our geometric interpretation of R-precision, we show that R-precision is highly informative by demonstrating that it can be used to (1) accurately infer precision-recall curves, (2) ac-curately infer other measures of retrieval performance, and (3) devise new measures of retrieval performance. Through our analysis, we also state the conditions under which R-precision is informative.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software  X  Performance evaluation Theory, Measurement, Experimentation Evaluation, R-precision, Average Precision
The evaluation of information retrieval systems is an im-portant research area in information retrieval, and as a re-sult, many different evaluation measures have been proposed
We gratefully acknowledge the support provided by NSF grant CCF-0418390.
 Copyright 2005 ACM 1-59593-140-6/05/0010 ... $ 5.00. and studied. Most of the widely used evaluation measures are a function of precision (the proportion of retrieved doc-uments that are relevant) and recall (the proportion of rele-vant documents that are retrieved) [4, 14, 9]. Since different evaluation measures evaluate different aspects of retrieval behavior, many evaluation measures have come to prolif-erate, and these measures have been deeply analyzed and criticized [11, 8]. For example, Buckley and Voorhees com-pare evaluation measures based on their query stability [5], and Loose [10] proposes criteria to determine under which conditions different measures agree or disagree.

In assessing the efficacy of retrieval systems, the two pri-mary types of assessments performed are user-oriented eval-uation and system-oriented evaluation [7]. In user-oriented evaluation, the performance of a retrieval system is evalu-ated based on its value to a user [6]. On the other hand, system-oriented evaluation focuses on the overall quality of a retrieval system X  X  results. For example, in the case of web search engines, a user might primarily be interested in the top k documents of the output of a search engine. Therefore, a user-oriented measure would consider only the top k doc-uments of the output, whereas a system-oriented measure would evaluate the overall quality of the entire output.
Average precision and R-precision are two of the most commonly cited system-oriented measures of overall perfor-mance. Given the ranked list of documents returned by a re-trieval system in response to a given query, average precision is the average of the precisions at each relevant document the list, while R-precision is the precision at rank R , where R is the number of documents relevant to the query.
Empirically, average precision and R-precision are known to be highly correlated [13, 12, 1]. Table 1 shows the lin-ear correlation coefficient  X  and Kendall X  X   X  correlation be-tween R-precision and average precision for TRECs 8, 10, 12, and 13. The correlations in this table are computed over all runs, where a run corresponds to a list returned in re-sponse to a query. The strong correlation between these two measures has been considered surprising since average preci-sion effectively evaluates the area under the entire precision-recall curve whereas R-precision considers only a single pre-cision point [5]. We recently showed that under some rea-sonable assumptions, R-precision is also an approximation to the area under the precision-recall curve, thus explain-ing the high correlation between average precision and R-precision [3].
Precisions at unretrieved relevant documents are assumed to be zero, by convention. Table 1: Kendall X  X   X  and linear correlation coeffi-cient  X  between average precision and R-precision for the runs submitted to various TRECs. Figure 1: Precision-recall curve obtained by con-necting points (0 , 1) , ( rp , rp ) , (1 , 0) with straight lines. The shaded area beneath this piecewise-linear precision-recall curve is rp .
R-precision is a widely used and cited measure in the in-formation retrieval community, and it has been shown, em-pirically, to be a  X  X ood X  measure of overall retrieval per-formance. However, little theoretical justification for R-precision exists. In this paper, we extend our geometric interpretation of R-precision to provide further evidence as to why R-precision is a highly informative measure. In par-ticular, we show that by appropriately relaxing an assump-tion that the R-precision measure implicitly makes, one can obtain accurate inferences of many aspects of retrieval per-formance from the value of R-precision (and sometime R ) alone. Thus, we demonstrate how and why the value of R-precision is informative.

By definition, R-precision is the precision at cutoff R , where R is the number of documents relevant to a query. For a ranked list whose R-precision is rp , it is easy to see that at rank R , the recall and precision are both rp . The central result of Aslam et al. [3] is that R-precision can also be interpreted as the area under a piecewise-linear approx-imation to the precision-recall curve passing through the Figure 1).

Given this geometric interpretation, note that R-precision implicitly assumes that precision-recall curves start at (0 , 1), end at (1 , 0), and can be modeled in a piecewise-linear fash-ion through the point ( rp , rp ). The first two assumptions are often at least approximately true since precision-recall curves tend to have high precisions at low recall levels and low precisions at high recall levels. However, the assump-tion that precision-recall curves are piecewise-linear in the manner described almost never holds since precision-recall curves tend to be  X  X moother X  and tend not to have an abrupt change in slope at the point ( rp , rp ).

In the sections that follow, we show that by relaxing the implicit assumption that the shape of precision-recall curves is piecewise-linear and by instead fitting a simple, smooth tain strikingly accurate inferences of many aspects of re-trieval performance from the value of rp (and sometimes R ) alone, thus providing further evidence that R-precision is highly informative together with mechanisms (in the form of closed-form prediction formulae) for making use of the information that R-precision provides. Specifically, we show that (1) accurate inferences of precision-recall curves can be deduced from rp , (2) accurate inferences of average preci-sion can be deduced from rp , and (3) accurate inferences of precision-at-cutoff k can be deduced from rp and R . Given our geometric interpretation of R-precision, we further an-alyze the relative strengths and weaknesses of R-precision and average precision, and we discuss situations in which one measure or the other may more accurately reflect the  X  X rue X  overall performance of the system being evaluated. We tested our hypotheses and inferences using data from TRECs 3, 5 through 10, 12, and 13 (nine TREC collections in total). Due to space limitations, we only report results from TRECs 8 (ad hoc track), 10 (web track), 12 (robust track), and 13 (robust track), focusing primarily on the rep-resentative results from TREC8. However, we note that the results obtained were consistent over all TRECs tested.
In order to validate our geometric interpretation of R-precision and to test the information contained in the value of R-precision, we conducted two sets of experiments. First, we inferred precision-recall curves by fitting a simple, smooth pared these inferred precision-recall curves to actual precision-recall curves using TREC data. Second, using these in-ferred precision-recall curves (and sometimes R ), we fur-ther inferred other measures such as average precision and precision-at-cutoff k , and we compared these inferred mea-sure values to their actual values, again using TREC data. Our hypothesis is that (1) if R-precision is informative and (2) if our geometric interpretation of R-precision is valid, then these inferences should be accurate. The experimen-tal results described in the following sections support this hypothesis, thus validating our geometric interpretation of R-precision and providing theoretical and empirical evidence for the informativeness of R-precision. As mentioned previously, our geometric interpretation of R-precision implies that R-precision implicitly assumes that precision-recall curves start at the point (0 , 1), pass through the point ( rp , rp ), end at the point (1 , 0), and are piecewise-linear in shape. While the first three assumptions are correct or often approximately so, the last assumption is almost never true. Given the value of R-precision and our geometric interpretation, one could presumably infer a more accurate precision-recall curve by fitting a  X  X mooth X  curve through the points { (0 , 1) , ( rp , rp ) , (1 , 0) } . The domain and range of this curve should be [0 , 1] since both precision and recall are bounded in this range. Furthermore, in order to avoid Figure 2: A family of smooth curves fit through the points { (0 , 1) , ( rp , rp ) , (1 , 0) } for values of rp = 0 . 1 , 0 . 2 ,..., 0 . 9 . any implicit assumptions about whether systems tend to be precision-or recall-oriented, we assume that this curve should be symmetric about the line y = x .

A simple parameterized family of smooth curves satisfying these criteria is shown in Figure 2. Given an R-precision value rp , the precision p ( r ) at recall r is where  X  = (1 / rp  X  1) 2  X  1. This value of  X  is required in order to ensure that the curve passes through the point ( rp , rp ).

In Figure 3, we show how these smooth approximations to precision-recall curves compare with the actual precision-recall curves of system INQ602 in TREC8 for various queries. As can be seen from these figures, these smooth approxima-tions can be quite similar to actual precision-recall curves.
In order to validate our geometric interpretation of R-precision and to test the information provided by the value of R-precision, one can evaluate how the precision-recall curves inferred from R-precision compare with actual precision-recall curves. In order to do so, we calculated two differ-ent statistics: the Root Mean Squared ( RMS ) error and the Mean Absolute Error ( MAE ). RMS error measures the de-viation of estimated values from the actual values, and it is related to the standard deviation of the estimation error. MAE is effectively the area between the actual and inferred precision-recall curves.

Let ( p 1 ,...,p R ret ) be the precisions at recall levels (1 /R,...,R ret /R ), where R ret is the number of relevant doc-uments retrieved by the system, 2 and let ( e 1 ,...,e R ret the estimated precisions at the given recall levels. Then the MAE and the RMS errors can be calculated as follows: Precisions beyond recall R ret /R are assumed to be zero in TREC, by convention. Hence, they are not included in the calculation of estimation error.
 TREC 8 10 12 13 AVG RMS 0.0969 0.0918 0.1109 0.1118 0.1044 MAE 0.0728 0.0679 0.0856 0.0847 0.0777 Table 2: MAE and RMS errors for each TREC.

Table 2 shows the average MAE and RMS errors for the smooth approximations to the precision-recall curves asso-ciated with runs submitted to various TRECs. The last col-umn of the table shows the average error over all TRECs. Note that the smooth approximation consistently has rela-tively low MAE and RMS errors over all TRECs; hence it estimates the actual precision-recall curves well. In order to further validate our geometric interpretation of R-precision and test the information contained in the value of R-precision, we conducted a second set of experiments. Using the inferred precision-recall curve obtained from R-precision, we can infer the values of other measures such as average precision and precision-at-cutoff k for any k . By assessing the correlation between the values of inferred mea-sures and those of the actual measures themselves, we can further validate our geometric interpretation of R-precision and the informativeness of the value of R-precision.
It is well known that average precision approximates the area under the precision-recall curve. Hence, given the con-tinuous precision-recall curve p ( r ) (Equation 1) obtained us-ing the R-precision value rp , average precision should ap-proximately be where  X  = (1 / rp  X  1) 2  X  1. Evaluating this integral, we obtain the following inferred value of average precision: In deriving this formula, we assumed that the given retrieval system has a recall of 1, i.e., that it retrieves all relevant doc-uments. However, in TREC, retrieval systems are allowed to return at most 1,000 documents; hence, many systems do not retrieve all relevant documents. The precision-recall curves of these systems are assumed to be zero after the last relevant document retrieved, and the average precisions of those systems are actually calculated only until this final recall. Therefore, in order to test our models using TREC data, we infer the average precision of a system reported by TREC by estimating the area under the smooth precision-recall curve until its final recall.

Let R ret be the number of relevant documents in a re-turned list, and let R be the number of documents relevant to the query. Let the recall at the last retrieved relevant doc-ument be  X  , where  X  = R ret /R . Then the we can estimate and (1 , 0) for system INQ602 in TREC8. the TREC average precision value as follows: 3
Let p ( r ) be the precision-recall curve associated with an output list, and let R be the number documents relevant to the query in question. Corresponding to any rank k in this list, there is a point ( r,p ( r )) on the precision-recall curve.
At rank k , there are r  X  R relevant documents, and there-fore p ( r ) = r  X  R/k . Rearranging this equation, we obtain the relationship R/k = p ( r ) /r . Hence, given our smooth approximation to the actual precision curve (Equation 1), we obtain By solving Equation 4 for r , we obtain the recall r at rank k : Since precision-at-cutoff k is the fraction of the top k docu-ments which are relevant, and since there are r  X  R relevant documents up to cutoff k , we can calculate the precision-at-cutoff k as follows: 4
In order to test the quality of these inferences, we con-ducted two sets of experiments for all TRECs. Due to space limitations, we focus primarily on the representative results obtained for TREC8.
When rp = 1 (i.e.,  X  =  X  1), rp = 1 / 2 (i.e.,  X  = 0), and rp = 0 (i.e.,  X  =  X  ), this formula is ill-defined. By taking the limit of the given formula, it can be seen that when rp = 1, ap = 1; when rp = 1 / 2, ap =  X  (1  X   X / 2); and when rp = 0, ap = 0.
When rp = 1 (i.e.,  X  =  X  1), rp = 1 / 2 (i.e.,  X  = 0), and rp = 0 (i.e.,  X  =  X  ), this formula is ill-defined. By taking the limit of the given formula, it can be seen that when rp = 1, for k  X  R , PC ( k ) = 1 and for k&gt;R , PC ( k ) = R/k ; when rp = 1 / 2, PC ( k ) = R/ ( R + k ); and when rp = 0, PC ( k ) = 0.

In the first set of experiments, we compared the actual values of measures with inferred values obtained using the formulas given in Equations 3 and 5. In this experiment, it is important to estimate the actual values of the measures accurately. Hence, we used RMS error as the evaluation criterion.

The plots in the top row of Figure 4 show how R-precision is actually correlated with average precision, precision-at-cutoff 30 and precision-at-cutoff 100 using the data from TREC8. The cutoffs 30 and 100 were chosen since they are two of the most commonly cited cutoffs. Note that each point in these plots corresponds to a run (execution of a system on a particular query) in TREC8. All plots contain the line y = x for the purpose of comparison.

Taking the plots in the top row as a baseline, the plots in the bottom row show how the values of actual measures com-pare with the values of measures inferred from the smooth approximation to precision-recall curves using R-precision. As an example, the top left plot of the figure shows how actual R-precision and actual average precision are corre-lated, and the bottom left plot shows how the average pre-cision inferred from R-precision is correlated with actual average precision. By this comparison, one can see how much information R-precision provides. It can be seen that the predicted average precision and actual average precision are much more correlated than actual average precision and actual R-precision. Applying the same comparison to the other measures, one can see that the measures inferred from R-precision are highly correlated with the actual measures, while R-precision and the actual measures themselves are not as highly correlated. Apart from the high correlation between the actual measures and inferred measures, it can be seen that the estimations obtained from R-precision are also unbiased or nearly so.

Table 3 shows the RMS errors for estimations of the mea-sures average precision and precision-at-cutoff k , for all cut-offs k reported by TREC and for all TRECs. The table shows that on a per-run basis, the estimate of average preci-sion is highly accurate and the estimate of precision-at-cutoff k is also highly accurate for most of the cutoffs k ; however, all estimates of precisions at small cutoffs are affected by the assumption that precision-recall curves start at (0 , 1).
In the second set of experiments, using the same idea as in the previous set, we compare how the mean values of actual measures are correlated with the mean values of the inferred measures. (The mean of a measure is the average of at-cutoff 100 on a per-run basis. the measure over all queries.) Hence, we show how much one can learn about the overall behavior of a system by using the R-precision value. The results of this experiment are shown in Figure 5.

In this set of experiments, we use RMS error, linear cor-relation coefficient  X  , and Kendall X  X   X  as our evaluation cri-teria. The linear correlation coefficient evaluates the cor-relation between the actual and inferred values based on how well the actual and inferred values fit to a straight line. Given the mean values of a measure, we can convert these values to a ranking of systems by ordering the systems ac-cording to the measure. Given two different rankings of sys-tems, Kendall X  X   X  evaluates how one ordering of the systems compares with another ordering in terms of the number of adjacent swaps needed to convert one to the other. Both  X  and Kendall X  X   X  range from  X  1 (perfectly negatively corre-lated) to +1 (perfectly correlated).

As in the first set of experiments, the figures in the top row show the correlation between R-precision and the ac-tual measures, and the bottom row shows the correlation between the inferred measures and the actual measures. It can be seen that for all measures, the correlation between the inferred and actual measures is much higher than the correlation between R-precision and the actual measures. Note that the inferred mean measures are also unbiased or nearly so.

Table 4 shows the RMS errors for inferences of mean aver-age precision and mean precision-at-cutoff k , for all cutoffs k and for all TRECs. Note that the inferences obtained for the mean measures are much better than the inferences obtained on a per-run basis due to the variance reduction obtained through averaging. Even for small cutoffs, despite the as-sumption that precision-recall curves start at (0 , 1), one can obtain accurate inferences of the mean measures through R-precision. It can be seen that the inference of mean average precision and the inference of mean precision-at-cutoff k (for sufficiently large k ) are extremely accurate.
The value of average precision reported by TREC is ef-fectively the area under the precision-recall curve only out to a recall of R ret /R , where R ret is the total number of rel-evant documents retrieved in the given run (precisions be-yond recall R ret /R are defined to be zero, by convention). In the section that follows, we propose a measure that ap-proximates the area under the entire precision-recall curve and state the conditions under which this measure may be a better measure of overall retrieval performance. We further show that R-precision and the proposed measure are per-fectly correlated in terms of ranking purposes. Hence, we argue that R-precision may be a  X  X etter X  measure of overall performance than average precision under these conditions. On the other hand, we show that R-precision itself is not very informative when the number of documents relevant to the query is small; therefore average precision may be a  X  X etter X  measure than R-precision under this condition.
In TREC, retrieval systems are allowed to return at most 1,000 documents. Hence, the outputs of search engines sub-mitted to TREC are truncated. Since average precision re-ported by TREC is an approximation to the area under the precision-recall curve until the recall ( R ret /R ) of the mean precision-at-cutoff 100.
 reported by TREC for all TRECs. returned truncated list, TREC average precision is effec-tively evaluating the quality of the returned list as opposed to evaluating the quality of the underlying retrieval func-tion. The quality of the retrieval function may be better estimated by the area under the entire precision-recall curve (which we call  X  X ctual AP X ). However, computation of  X  X c-tual AP X  would require arbitrarily long lists to be submitted to TREC, and in practice,  X  X ctual AP X  and  X  X REC AP X  are quite similar for most runs and systems.

By computing the area under the entire smooth approxi-mation to the precision-recall curve up to recall 1 (using the formula in Equation 2), one can approximate the area un-der the entire precision-recall curve and thus  X  X ctual AP. X  Hence, the measure obtained by Equation 2 can be used as a new measure of performance. Given that the infer-ences of TREC average precision (TREC AP) obtained by R-precision are very close to the reported TREC average precisions, one would expect this approximation to the area under the entire precision-recall curve to be accurate as well.
TREC AP and  X  X ctual AP X  would be expected to differ when there are significant numbers of unretrieved relevant documents. Such situations mainly occur (1) for queries which have large number of relevant documents compared to the lengths of lists submitted to TREC, (2) for systems which have significant numbers of unretrieved relevant doc-uments, and (3) for systems which return  X  X hort X  lists (e.g., READWARE and READWARE2 in TREC8).

Note that the mapping from R-precision to  X  X ctual AP X  is monotonic X  X he higher one X  X  R-precision, the higher one X  X  estimate for  X  X ctual AP X  (Equation 2). Thus, on a per-run basis,  X  X ctual AP X  and R-precision would rank systems in exactly the same manner. This behavior can be seen in Figure 6 for TREC8 data. Under the circumstances stated in the previous paragraph, inferred  X  X ctual AP, X  and hence R-precision, may better reflect the true performance of the system.

In Figure 7, we show correlations between  X  X REC AP X  and inferred  X  X ctual AP X  for each run and system in TREC8. (Similar results are obtained for other TRECs.) While the inferences for  X  X ctual AP X  are not substantially different than that of  X  X REC AP X  (bottom-left plots in Figure 4 and Figure 5) for most runs and systems, the differences are significant for a number of runs and systems. The READ-WARE systems (two of the best systems in TREC8 ad hoc track), in particular, benefit from an inference of  X  X ctual AP X  since they return quite short lists. (READWARE re-by TREC for all TRECs. Figure 6: TREC8 R-precision versus inferred Ac-tual average precision for each run submitted to the conference.

Rank System Score System Score 1 READWARE2 0.469 READWARE2 0.509 2 orcl99man 0.413 READWARE 0.437 3 iit99ma1 0.410 orcl99man 0.418 4 READWARE 0.400 iit99ma1 0.417 Table 5: The top four systems in TREC8 as scored and ranked according to TREC MAP and inferred Actual MAP. Figure 7: Left plot: TREC8 average precision ver-sus inferred actual average precision for each run submitted to the conference. Right plot: TREC8 mean average precision versus inferred actual mean average precision for each system. Figure 8: Left plot: TREC AP versus inferred TREC AP for each run submitted to TREC8, for queries where R  X  20 . Right plot: TREC AP ver-sus inferred TREC AP for each run submitted to TREC8, for queries where R&gt; 20 . turned 61 documents on average per query while READ-WARE2 returned 116 documents on average per query.) In Table 5, we show the top four systems from TREC8 as scored and ranked according to mean TREC average precision and inferred mean actual average precision. Note the significant increase in the scores associated with the READWARE sys-tems and the much smaller changes associated with other systems.
Note that when R is small, R-precision does not have a great deal of discriminatory power. For example, in the case that the number of documents relevant to a query is five, there are only six different values the R-precision of a system can take, and the difference between different R-precision values is at least 0 . 2. Because of this, many systems with quite different performance will have identical R-precision values.

Since R-precision is not a discriminatory measure when R is small, it is also not so informative. Hence, one would ex-pect the inferences of measures obtained using R-precision when R is small to be worse than the inferences of measures obtained when there are sufficient number of documents rel-evant to the query. Figure 8 shows that this behavior is exhibited in TREC8 data. In this experiment, we divided the queries in TREC8 into two different sets, one for which R  X  20 and one for which R&gt; 20, and we calculated the inferences obtained using R-precision for each set. The left plot shows the correlation between TREC average precision and inferred TREC average precision using R-precision for each run in the first set ( R  X  20), and the right plot shows this correlation for each run in the second set ( R&gt; 20). As can be seen, when R is high, the inferences obtained are much closer to the actual values and have lower variance. Therefore, the plots show that R-precision captures more information when there are larger number of documents rel-evant to the query, as expected.
Similarly, we calculated the MAE and RMS errors of the estimated precision-recall curves (obtained by using Equa-tion 1) for the two different sets. The results are shown in Table 6. It can be seen that both MAE and RMS errors are consistently higher when R  X  20 than when R&gt; 20.
Since R-precision is not very informative when R is small, one might prefer to use average precision to evaluate the overall performance of retrieval systems when there are an insufficient number of relevant documents to the query.
R-precision is one of the most commonly cited measures of overall retrieval performance. Although it has been shown, empirically, that R-precision is a  X  X ood X  measure, the reason for this, theoretically, has not been clear.
 In this paper, we provide theoretical evidence as to why R-precision is a highly informative measure by showing that using a simple geometric interpretation of R-precision, one can (1) infer estimates of precision-recall curves and (2) infer estimates of other measures of retrieval performance, such as average precision and precision-at-cutoff k . We further demonstrate, empirically, that these inferences can be highly accurate. Finally, we assess the relative strengths and weak-nesses of R-precision and average precision in light of our geometric interpretation of R-precision.

While the primary aim of this paper is to provide a the-oretical foundation for (and a geometric interpretation of) R-precision, this work leaves open an number of interest-ing problems and potential applications. First, the smooth family of curves described in Equation 1 is effectively a prior on the shape of precision-recall curves. Other poten-tially more accurate priors would presumably yield even bet-ter inferences of precision-recall curves and other measures of performance, such as average precision and precision-at-cutoff k . Second, the prediction formulae themselves may be of interest and have practical applications. Predicting other measures from R-precision may seem uninteresting at first blush since in order to calculate R-precision, one must have the complete set of relevance judgments which could be used to evaluate all other measures as well. However, the recent work of Aslam et al. [2] demonstrates that R and R-precision can be simply and accurately estimated from a relatively small set of randomly chosen relevance judgments. In this case, inferring other measures from an accurate esti-mate of R-precision becomes potentially interesting. These open questions are the subject of on-going research. [1] J. Allan. HARD track overview in TREC 2003: High [2] J. A. Aslam, V. Pavlu, and E. Yilmaz. A sampling [3] J. A. Aslam, E. Yilmaz, and V. Pavlu. A geometric [4] R. Baeza-Yates and B. Ribeiro-Neto. Modern [5] C. Buckley and E. Voorhees. Evaluating evaluation [6] W. S. Cooper. On selecting a measure of retrieval [7] B. Dervin and M. S. Nilan. Information needs and use. [8] Y. Kagolovsky and J. R. Moehr. Current status of the [9] R. M. Losee. Text Retrieval and Filtering: Analytical [10] R. M. Losee. When information retrieval measures [11] V. Raghavan, P. Bollmann, and G. S. Jung. A critical [12] J. Tague-Sutcliffe and J. Blustein. A statistical [13] E. M. Voorhees and D. Harman. Overview of the [14] E. M. Voorhees and D. Harman. Overview of the
