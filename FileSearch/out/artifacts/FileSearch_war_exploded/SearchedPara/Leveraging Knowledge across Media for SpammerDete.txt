 While microblogging has emerged as an important informa-tion sharing and communication platform, it has also be-come a convenient venue for spammers to overwhelm other users with unwanted content. Currently, spammer detection in microblogging focuses on using social networking informa-tion, but little on content analysis due to the distinct nature of microblogging messages. First, label information is hard to obtain. Second, the texts in microblogging are short and noisy. As we know, spammer detection has been extensively studied for years in various media, e.g., emails, SMS and the web. Motivated by abundant resources available in the other media, we investigate whether we can take advantage of the existing resources for spammer detection in microblog-ging. While people accept that texts in microblogging are different from those in other media, there is no quantita-tive analysis to show how different they are. In this paper, we first perform a comprehensive linguistic study to com-pare spam across different media. Inspired by the findings, we present an optimization formulation that enables the de-sign of spammer detection in microblogging using knowledge from external media. We conduct experiments on real-world Twitter datasets to verify (1) whether email, SMS and web spam resources help and (2) how different media help for spammer detection in microblogging.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  Classification ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing Algorithm, Performance, Experimentation Spammer Detection, Twitter, Emails, SMS, Web, Cross-Media Mining, Social Media
Microblogging  X  a style of communicating through short-form content  X  has emerged as a popular social network-ing platform. Microblogging systems have been increasingly used for large-scale information dissemination and sharing in various fields such as marketing, journalism or public re-lations. With microblogging X  X  growing popularity, activities of spamming have become rampant in launching various at-tacks in the medium. For example, the spammers spread ads to generate sales, disseminate pornography, viruses, phish-ing, or simply to comp romise a system X  X  r eputation [3]. To improve user experience and the overall value of a system, it is essential to detect spammers in microblogging.
Existing methods for spammer detection in social me-dia [26] focus on using social networking information. These network-based methods characterize the spammers by an-alyzing the network features, e.g., social status. The as-sumption behind this strategy is that it is difficult for the spammers to establish a large number of social relations with legitimate users. Different from other social media sites, in microblogging, users can follow anyone without prior con-sent from the followee. Many users just follow back when they are followed by someone for the sake of courtesy [20]. So, the spammers can easily enhance their influence score to fool the system. In this case, content analysis could comple-ment network-based methods in spammer detection; thus, we explore the use of content information in this work.
A straightforward way to perform content-based spam-mer detection [22] is to model this task as a supervised learning problem. These methods extract effective textual features from the messages and build a classifier or a re-gressor based on the features. Given a new user, the built model can output a class label or score to determine whether it is a spammer based on microblogging messages the user posted. Content-based methods become difficult to be di-rectly applied due to the distinct features of microblogging data. First, in microblogging, it is time-consuming and la-bor intensive to obtain labeled data, which is essential in building an effective supervised spammer detection model. Given the size and dynamic nature of microblogging, a man-ual labeling process is neither scalable nor sensible. Second, the texts in microblogging are short and noisy; thus, we lack sufficient aggregated information to evaluate the given messages. These present great challenges to directly making use of existing content-based methods for effective spammer detection in microblogging.

While the problem of spamming in microblogging is rela-tively new, it has been extensively studied for years in other platforms, e.g., email communication [4], SMS [14] and the web [31]. Similarly, the spammers in these platforms un-fairly overwhelm other users by spreading unwanted infor-mation, which leads to phishing, malware, and scams [20]. Also, it has been reported in Natural Language Processing (NLP) literature that microblogging is not as noisy as was expected [2]. Although microblogging is an informal com-munication medium, it has been shown to be similar to other platforms [21] and it is seemingly possible to employ NLP tools to  X  X lean X  it [11]. Motivated by the previous findings, we explore the possibility of using knowledge learned from other platforms to facilitate spammer detection in the con-text of microblogging.

In this paper, we explore the use of resources available in other media to help spammer detection in microblogging. To study this problem, we need to answer the following ques-tions: Are the resources from other media potentially helpful for spammer detection in microblogging? How do we explic-itly model and make use of the resources from other me-dia for spammer detection? Is the knowledge learned from other media helpful for microblogging spammer detection? By answering the above questions, this paper presents the following contributions: The remainder of this paper is organized as follows. In Section 2, we conduct a quantitative study to examine the differences between spam corpora in different media from a linguistic perspective. In Section 3, we formally define the problem of leveraging knowledge across media for spammer detection in microblogging. In Section 4, we propose a novel framework for the problem we study. In Section 5, we report empirical results on real-world datasets. In Section 6, we review existing literature related to our work. In Section 7, we conclude this work and present some future work.
This work is motivated by numerous spam resources avail-able in other well-studied media, e.g., email, SMS and web. A natural question could be, given the short and noisy form of microblogging messages, how different are the texts in microblogging when compared to those in other media? Be-fore proceeding further, we also examine whether the tex-tual information from other media is potentially useful in the problem we study.
Two Twitter datasets are used in our study for exper-iment purposes, i.e., TAMU Social Honeypots and Twit-ter Suspended Spammers. In addition, three representa-tive datasets from different types of media, including Enron Email Dataset, SMS Dataset and Web Dataset, are used in the analysis. The statistics of the datasets are presented in Table 1. Now we introduce the datasets in detail.
TAMU Social Honeypots Dataset (TweetH) :Lee et al. [22] created a collection of 41,499 Twitter users with identity labels: spammers and legitimate users. The dataset was collected from December 30, 2009 to August 2, 2010 on Twitter. It consists of users, their number of followers and tweets. We filtered the non-English tweets and users with less than two tweets.
 Twitter Suspended Spammers Dataset (TweetS) : We employed a data crawling process, which is similar to [32, 34], to construct this dataset. We first crawled a Twitter dataset from July to September 2012 via the Twitter Search API. The users that were suspended by Twitter during this period are considered as the gold standard [32] of spammers in the experiment. We then randomly sampled the legiti-mate users from a publicly available Twitter dataset pro-vided by TREC 2011. 1 We filtered the non-English tweets and users with less than two tweets.

The first dataset TweetH has balanced number of spam-mers and legitimate users. To avoid effects brought by differ-ent class distribution, according to the literature of spammer detection [22], we made the two classes in TweetS imbal-anced, i.e., the number of legitimate users is much greater than that of spammers in the dataset.

Enron Email Dataset (Email) : We used a subset of a widely used Enron email dataset, 2 which is collected during the investigation of Enron corporation and contains more than 200,000 emails between its employees. The emails in this dataset are preprocessed and used as a testbed in [25] for experiments. Each email in the dataset is labeled as either  X  X pam X  or  X  X am X .

SMS Dataset(SMS) : We used the SMS spam collection provided by Almeida et al. [1] for analysis. This dataset is constructed based on two sources, Grumbletext web site 3 and NUS SMS Corpus. 4 The spam messages were manually labeled, and the ham messages were randomly sampled from the NUS SMS Corpus. To the best of our knowledge, this is the largest public SMS spam dataset.

Web Dataset (Web) : Web spam is a key challenge for internet users. Web pages which are created to deceive other users by manipulating search engine. Webb et al. [31] con-structed the Web Dataset. This is the largest publicly avail-able dataset to the best of our knowledge. We removed the web pages that have no textual content or only contain http request error information.
To evaluate the style of a language, many metrics have been proposed in literature of linguistics and communica-tion [2, 30]. In this subsection, we first introduce the metrics used in our study and then discuss lexical analysis results on the datasets from different media.

Basic Statistics : average Word Length ( WL ,incharac-ters) and average Sentence Length ( SL , in words) are used to evaluate the basic style of different datasets. In addition to those, we further employ other widely used lexical metrics in the analysis. We list the metrics below. http://trec.nist.gov/data/tweets/ http://www.isi.edu/~adibi/Enron/Enron.htm http://www.grumbletext.co.uk/ http://wing.comp.nus.edu.sg/SMSCorpus/
Type-Token Ratio (TTR) : This is a widely used metric to evaluate the difficulty (or readability) of words, sentences and documents by measuring their lexical variety [7, 33]. The basic assumption of using TTR is that difficult words are those that appear least often in a document. Given a corpus D , TTR is calculated as TTR ( D )= w  X  D Freq ( w where w means a word (token) in the corpus, Freq ( w )means word frequency of w in D ,and Size ( D ) means the number of distinct words (types) in D . In practice, a higher TTR indicates a larger amount of lexical variation and a lower score indicates relatively less lexical variation [33].
Lexical Density (LD) : We employ lexical density to further analyze the stylistic difference between different cor-pora. Lexical words [15], also known as content or infor-mation carrying words, refer to verbs, nouns, adjectives and adverbs. Similarly, given a document D , LD is defined as ical words dictionary. In general, a higher lexical density indicates that it is a more formal document, and a lower lexical density represents a more conversational one.
Out-of-Vocabulary (OOV) : This metric is to measure the ratio of out-of-vocabulary words in the corpora. We use a list of top 10,000 words with highest frequency provided by the Project Gutenberg [16] in our study. In general, a higher OOV rate indicates that the language is more infor-mal. Many NLP and IR models suffer from high OOV rates.
Experimental results of the lexical analysis are presented in Table 2. By comparing the results of different metrics, we observe the following: (1) The word lengths of different cor-pora are very similar, and the sentence lengths of TweetH, TweetS and SMS are smaller than those of more formal me-dia Email and Web. This indicates that the textual form of microblogging data is similar to SMS, and relatively different from email and web. (2) In most of the tests, microblogging data is similar to the datasets from the other media. It demonstrates that, although microblogging is considered an informal media, the language use is similar to that in other media, especially in email and SMS. We observe that the type-token ratios of microblogging are smaller than those of SMS and web. It suggests that the language used in mi-croblogging is easier than that in the other two platforms.
We further employ hypothesis testing to examine the lex-ical differences between microblogging datasets and other Table 3: Hypothesis Testing Results (P-Values) Email 0.318 0.108 0.442 0.234 0.267 0.308 SMS &lt; 0.01 0.205 0.350 &lt; 0.01 0.082 0.163
Web &lt; 0.01 0.623 0.398 0.108 0.551 0.462 datasets. For each lexical metric, we form a null hypothesis for a microblogging dataset and a dataset from the other media. The null hypothesis is: in terms of the specific lexi-cal metric, there is no difference between microblogging data and data from the other media. We test the hypotheses on all pairs of the datasets for all the three lexical metrics. In particular, to verify the difference between TweetH and Email datasets on the TTR, we construct two vectors ttr th and ttr em . Each element of the first vector ttr th is ob-tained by calculating the TTR score of a subset sampled with bootstrapping from TweetH dataset. Similarly, each element in the second vector corresponds to the TTR score of a subset sampled with bootstrapping from Email dataset. In the experiment, the two vectors contain equal number of elements. 5 Each element in the vectors corresponds to 100 data instances. We formulate a two-sample two-tail t-test on the two constructed vectors ttr th and ttr em .Weexam-ine whether there is sufficient statistical evidence to support the hypothesis that the two datasets have the same sample mean, and it is defined as follows: where H 0 is the null hypothesis, H 1 is the alternative hy-pothesis, and  X  c and  X  r represent the sample means of the two vectors, respectively. Similarly, we form the hypothesis testings for other pairs of datasets with other lexical metrics. The t-test results, p-values, are summarized in Table 3. From the table, we can observe the following: (1) With few exceptions, the results are much greater than the significance level  X  =0 . 05. It demonstrates that there is no statistical evidence to reject the null hypothesis in the tests on the two datasets. In other words, the results suggest that microblog-ging data is not significantly different from the datasets in other media. (2) In some tests, microblogging data appears more similar to Email than the other datasets.

In conclusion, while characteristics of different datasets appear different, there are no statistically significant lexical differences between them. The resources from other media are potentially useful in the task we study. Next, we formally define the problem we study and introduce the proposed learning framework for spammer detection.
Note this is the setting used for experiment purposes, and it is not a mandatory setting for a two-sample t-test.
In this section, we first present the notations and then formally define the problem we study.

Notation: lower-case bold Roman letters (e.g., a )denote column vectors, upper-case letters (e.g., A ) denote matri-ces, and lowercase letters (e.g., a) denote scalars. A ( i, j ) denotes the entry at the i th row and j th column of a ma-trix A .Let A denote the Euclidean norm, and A F the Frobenius norm of the matrix A . Specifically, A F = pose and trace of A , respectively.

Let S =[ X , Y ] be available resources from other media, with the content information X and identity label matrix Y . We use term-user matrix X  X  R m  X  d to denote con-tent information, i.e., posts written by the users, where m is the number of textual features, and d is the number of users in the other media. X = { X 1 , X 2 , ..., X r } the combination of content information from multiple me-dia, and Y  X  R d  X  c = { Y 1 , Y 2 , ..., Y r } means the combi-nation of label information from the media. For each user ( x i , y i )  X  R m + c consists of message content and identity label, where x i  X  R m is the message feature vector and y  X  R c is the spammer label vector. In this paper, we con-sider the task we study as a two-class classification problem, i.e., c = 2. For example, y i =(1 , 0) means this user is a spammer. y T i y i = 1 constrains that y i has to have one la-bel and cannot be (0 , 0) or (1 , 1). It is practical to extend this setting to a multi-class or regression problem. We use T  X  R m  X  n to denote the content information of microblog-ging users, where m is the number of textual features, and n is the number of users in microblogging. The texts from microblogging and other media share the same feature space. We now formally define the problem as follows:
We have a set of resources S from different media, with the content information X = { X 1 , X 2 , ..., X r } and identity label information Y = { Y 1 , Y 2 , ..., Y r } . Given the content information T from microblogging, our goal is to automat-ically infer the identity labels for unknown users in T as spammers or legitimate users.
We plot the work flow of our proposed framework in Fig-ure 1. From the figure, we see that there are two constraints on the learned model for spammer detection. As shown in the upper right part of the figure, the first constraint is from the lexicon information U , which is learned from the other media sources S . As shown in the lower right part of the fig-ure, the second constraint is a Laplacian regularization M learned from microblogging content information. We now introduce each part of the proposed framework in detail.
As we discussed in the last section, from a linguistic per-spective, it does not show significant difference between mi-croblogging data and other types of data. A straightforward method to make use of external information is to learn a su-pervised model based on data from the other media, and apply the learned classifier on microblogging data for spam-mer detection. However, this method yields two problems to be directly applied to our task. First, text representation models, like n-gram model, often lead to a high-dimensional Figure 1: Illustration of the Proposed Spammer De-tection Framework feature space because of the large size of data and vocabu-lary. Second, texts in the media are short, thus making the data representation very sparse [21].

To tackle the problems, instead of learning knowledge at word-level, we propose to capture the external knowl-edge from topic-level. In particular, the proposed method is built on the orthogonal nonnegative matrix tri-factorization model (ONMTF) [9]. The basic idea of the ONMTF model is to cluster data instances based on distribution of fea-tures, and cluster features according to the distribution of data instances. The principle of ONMTF is consistent with PLSI [17], in which each document is a mixture of latent topics that each word can be generated from. The ONMTF can be formulated by optimizing: where X is the content matrix, and U  X  R m  X  c + and V  X  + are nonnegative matrices indicating low-dimensional representations of words and users, respectively. m is the size of vocabulary, c is the number of classes, d is the number of users. H  X  R c  X  c + provides a condensed view of X .The orthogonal and nonnegative conditions of U and V provide a hard assignment of class label to the words and users.
With the ONMTF model, we project the original content information from the other media into a latent topic space. By adding a topic-level least squares penalty to the ONMTF, our proposed framework can be mathematically formulated as solving the following optimization problem: where W represents the weights and Y is the label matrix. In the formulation, the first term is the basic factorization model, and the second introduces label information from the other media by using a linear penalty.  X  is to control the effect of external information to the learned lexicon U ,in which each row represents the predicted label of a word.
As the problem in Eq. (3) is not convex with respect to the four variables together, there is no closed-form solution for the problem. Next, we introduce an alternative scheme to solve the optimization problem.
Following [9], we propose to optimize the objective with respect to one variable, while fixing others. The algorithm will keep updating the variables until convergence.
Computation of H : Optimizing the objective function in Eq. (3) with respect to H is equivalent to solving
Let  X  H be the Lagrange multiplier for constraint H  X  0; the Lagrange function L ( H )isdefinedasfollows: By setting the derivative  X  H L ( H ) = 0, we get
The Karush-Kuhn-Tucker complementary condition [6] for the nonnegativity constraint of H gives thus, we obtain Similar to [9], it leads to the updating rule of H ,
Computation of U : Optimizing the objective function in Eq. (3) with respect to U is equivalent to solving Let  X  U and  X  U be the Lagrange multipliers for constraints U  X  0and U T U = I , respectively; the Lagrange function L ( U )isdefinedasfollows: L ( U )= X  X  UHV T 2 F  X  Tr ( X  U U T )+ Tr ( X  U ( U T U  X  By setting the derivative  X  U L ( U ) = 0, we get
With the KKT complementary condition for the nonneg-ativity constraint of U ,wehave thus, we obtain where
Let  X  U = X  + U  X   X   X  U ,where X  + U ( i, j )=( |  X  U ( i, j ) and  X   X  U ( i, j )=( |  X  U ( i, j ) | X   X  U ( i, j )) / 2[9];weget [  X  ( XVH T + U  X   X  U )+( UHV T VH T + U  X  + U )]( i, j ) U ( i, j )=0 , which leads to the updating rule of U , Algorithm 1: Modeling Knowledge across Media Input: { X , Y , X ,I }
Output: V
Computation of V : Optimizing the objective function in Eq. (3) with respect to V is equivalent to solving
Similar to the computation of U , by introducing two La-grange multipliers  X  V and  X  V for the constraints, we get [  X  ( X T UH +  X  YW T + V  X   X  +( VH T U T UH +  X  VWW T + V  X  + V )]( i, j ) V ( i, j )=0 , which leads to the updating rule of V ,
Computation of W : Optimizing the objective function in Eq. (3) with respect to W is equivalent to solving
Similar to the computation of U , by introducing a La-grange multiplier and satisfying KKT condition, we obtain which leads to the updating rule of W ,
We summarize the algorithm of optimizing Eq. (3) in Al-gorithm 1, where I is the number of maximum iterations. In line 1, we conduct initialization for the variables. From lines 2 to 8, the four variables are updated with the updat-ing rules until convergence or until they reach the number of maximum iterations. The correctness and convergence of the updating rules can be proven with the standard auxiliary function approach [28]. In this subsection, as shown in the lower right part of Figure 1, we introduce how to model content information of microblogging data in the proposed model.
To make use of the content information of microblogging messages, we introduce a graph Laplacian [8] in the proposed model. We construct a graph based on content information of the users. In the graph, each node represents a user and each edge represents the affinity between two users. The adjacency matrix M  X  R n  X  n of the graph is defined as where u and v are nodes, and N ( u ) represents the k-nearest neighbor of the user. Content similarity is adopted to ob-tain the k-nearest neighbor in this work. Since we aim to model the mutual content similarity between two users, the adjacency matrix is symmetric.

The basic idea of of using the graph Laplacian to model the content information is that if two nodes are close in the graph, i.e., they posted similar messages, their identity la-bels should be close to each other. It can be mathematically formulated as minimizing the following loss function: This loss function will incur a penalty if two users have dif-ferent predicted labels when they are close to each other in the graph. Let D  X  R n  X  n denote a diagonal matrix, and its diagonal element is the degree of a user in the adjacency matrix M , i.e., D ( i, i )= n j =1 M ( i, j ).

Theorem 1. The formulation in Eq. (25) is equivalent to the following objective function: where the Laplacian matrix [8] L is defined as L = D  X  M . Proof. It is easy to verify that Eq. (25) can be rewritten as which completes the proof.
As illustrated in Figure 1, we employ two types of infor-mation to formulate two kinds of constraints on the learned model. By integrating knowledge learned from other media and content information from microblogging, we can per-form spammer detection by optimizing where the first term is to factorize the microblogging data into three variables, which are similar to the idea discussed in Section 4.1. The second term is to introduce content infor-mation and the third is to introduce knowledge learned from the other media. U is the lexicon learned from the other Algorithm 2: Spammer Detection in Microblogging Input: { T , U , X , X ,I }
Output: V t media by solving the problem in Eq. (3). G U  X  X  0 , 1 } m  X  m is a diagonal indicator matrix to control the impact of the learned lexicon, i.e., G U ( i, i ) = 1 represents that the i -th word contains identity information, G U ( i, i )=0otherwise.
This optimization problem is not convex with respect to the three parameters together. Following the optimization procedure to solve Eq. (3), we propose an algorithm to solve the problem in Eq. (28) and summarize it in Algorithm 2. In line 1, we construct the Laplacian matrix L . In line 2, we initialize the variables. From lines 3 to 9, we keep updating the variables with the updating rules until convergence or until the number of maximum iterations is reached.
In this section, we empirically evaluate the proposed learn-ing framework and the factors that could bring in effects to the framework. Through the experiments, we aim to answer the following two questions:
We follow a standard experiment setup used in spammer detection literature [34] to evaluate the effectiveness of our proposed framework for leveraging knowledge aC ross media for S pammer D etection ( CSD ). In particular, we compare the proposed framework CSD with different baseline meth-ods for spammer detection. To avoid bias, both TweetH and TweetS, introduced in Section 2.1, are used in the experi-ments. For email data, we consider each sender a user; For SMS and web data, we do not have user information and consider each message as sent from a distinct user. In the experiment, precision, recall and F 1 -measure are used as the performance metrics.

To evaluate the general performance of the proposed frame-work, we use all of the three datasets from different media, i.e., Email, SMS and Web datasets. In the first set of ex-periments, to be discussed in Section 5.2, we simply com-bine them together and consider them as homogeneous data sources. In the second set of experiments, to be discussed in Section 5.3, we consider their individual impact on the per-formance of spammer detection. A standard procedure for data preprocessing is used in our experiments. The unigram model is employed to construct the feature space, tf-idf is used as the feature weight.

As we discussed in Sections 4.1 and 4.3, three positive parameters are involved in the experiments, including  X  in Eq. (3), and  X  and  X  in Eq. (28).  X  is to control the effect of knowledge from other media to the learned lexicon,  X  is to control the contribution of Laplacian regularization, and  X  is to control the contribution of lexicon to the spammer detec-tion model. Since all the parameters can be tuned via cross-validation with a set of validation data, in the experiment, we empirically set  X  =0 . 1,  X  =0 . 1and  X  =0 . 1 for general experiment purposes. The effects of the parameters on the learning model will be further discussed in Section 5.4.
We compare the proposed method CSD with other meth-ods for spammer detection, accordingly answer the first ques-tion asked above. The baseline methods are listed below. Experimental results of the methods on the two datasets, TweetH and TweetS, are respectively reported in Table 4 and 5. To avoid bias brought by the sizes of the training data, 6 we conduct two sets of experiments with different numbers of training instances. In the experiments, X  X xternal Data I (50%) X  means that we randomly chose 50% from the whole training data.  X  X xternal Data II (100%) X  means that we use all the data for training. Also,  X  X ain X  represents the percentage improvement of the methods in comparison with the first baseline method Least Squares .Intheexperiment, each result denotes an average of 10 test runs. By comparing the spammer detection performance of different methods, we observe the following: (1) From the results in the tables, we can observe that our proposed method CSD consistently outperforms other baseline methods on both datasets with different sizes of training data. Our method achieves better results than the state-of-the-art method MFTr on both datasets. We ap-ply two-sample one-tail t-tests to compare CSD to the four baseline methods. The experiment results demonstrate that the proposed model performs significantly better (with sig-nificance level  X  =0 . 01) than the four methods. (2) The performance of our proposed method CSD is bet-ter than the first three baselines, which are based on differ-ent strategies of using resources from the other media. This demonstrates the excellent use of cross-media knowledge in the proposed framework for spammer detection.
Similar to the definitions in machine learning literature, training data here refers to the labeled data from the exter-nal sources, and testing data represents the unlabeled mi-croblogging data. (3) Among the baseline methods, MFTr achieves the best results. It demonstrates that the knowledge transferred from other media help the task of spammer detection in microblog-ging. Lasso performs better than Least Squares . This shows that, for high-dimensional textual data from email, SMS and web, feature selection is necessary for a supervised learning method for this task we study. (4) The method MFSD achieves the worst performance among all the baseline methods. It shows that learning based on microblogging data itself can not discriminant well between spammers and legitimate users. It further demon-strates that the knowledge learned from external sources is helpful to build an effective model to tackle the problem.
In summary, with the effective use of data from the other media, our proposed framework outperforms the baseline methods in spammer detection. Next, we investigate the effects of different resources on the spammer detection task.
In this subsection, we study the effects of the external information from the other media on our proposed frame-work, accordingly answering the second question asked in the beginning of Section 5.

We first evaluate the performance of the proposed frame-work with data from only one of the three media. In par-ticular, we learn a lexicon based on one of the three types of media, i.e., email, SMS and web, and perform spammer detection on the microblogging datasets. We do not have legitimate web pages in the original Web dataset. To build a classifier CSD Web , following the data construction pro-cedure proposed in [18], we randomly sample 20,100 web snippets with BingAPI as legitimate data. The experimen-tal results of the methods on the two microblogging datasets are plotted in Figures 2 and 3, respectively. In the figures, the first three bars represent the performance of the base-lines with one type of external information. The last is the method with all three types of external information. From the figures, we observe the following: (1) With the integration of all three types of external infor-mation, CSD consistently achieves better performance than the three baselines with only one type of information. It demonstrates that the proposed method uses beneficial in-formation to perform effective spammer detection. (2) Among the three baseline methods, CSD Email and CSD SMS achieve better performance than CSD Web .It shows that, as external resources, email and SMS data are more suitable to be used for t he spammer detection in mi-croblogging than the web data. This result is consistent with the linguistic variation analysis in Section 2.

To further explore the effects of different media sources on the performance of spammer detection in microblogging, we employ a  X  X nockout X  technique in the experiment. Knock-out has been widely used in many fields, e.g., gene func-tion analysis, to test the performance variance brought by one component when it is made inoperative in the frame-work [10]. We conduct the experiments by knocking out one type of the external information from the proposed frame-work. The results are summarized in Table 6. In the table,  X  X oss X  represents the performance decrease of the methods as compared to the setting  X  X efault X  which is learned based on data from all three media sources. The three columns in the middle are experimental settings, in which  X 0 X  means this resource is knocked out. The last two columns are the F 1 measure results under different experimental settings. From the table, we observe the following: (1) By knocking out one of the external sources, perfor-mance of the proposed framework decreases. This suggests that all the three types of external information are useful for spammer detection in microblogging. (2) Knocking out email from the resources incurs the most performance decrease among all the experimental settings. This demonstrates that email is the most effective source among the three types of information. This finding is con-sistent with our discussion above.

In summary, the use of data from the other media shows the effectiveness in spammer detection task. The superior performance of the proposed method CSD validates its ex-cellent use of knowledge from the other media.
As discussed in Section 5.1, three positive parameters, i.e.,  X  ,  X  and  X  , are involved in the proposed framework. We first examine the effects brought by  X  , which is to control the contribution of knowledge from other media to the learned lexicon. In previous subsections, for general experimental purposes, we empirically set  X  =0 . 1. We now conduct ex-periments to compare the spa mmer detection performance of the four methods introduced in Section 5.3 with differ-ent settings of  X  . The experiment results on the TweetH dataset are plotted in Figure 4. From the figure, we observe the following: (1) The general trends of the four methods are similar with the variation of different parameter settings. They achieve relatively good performance when setting  X  in the range of [0.1, 10]. (2) In most cases, performance of 0.881 (-5.09%) 0.911 (-1.86%) 0.923 (-0.57%)
Figure 4: Performance with Different  X  Settings Figure 5: Impact of Content Information (  X  )and External Information (  X  ) the proposed CSD is better than the other three methods. It demonstrates that the combination of the three resources improve the spammer detection performance.

We further examine the effects of the parameters  X  and  X  discussed in Eq. (28) on the proposed framework.  X  is to control the contribution of content information and  X  is to control the effects of external information from the other media. To understand the effects brought by the parame-ters, we compare the spammer detection performance of the proposed CSD on the Twitter datasets with different pa-rameter settings. The results on the TweetH dataset are plotted in Figure 5. From the figure, we observe that the proposed method CSD performs well when  X   X  [0 . 1 , 5] and  X   X  [0 . 1 , 1]. Generally, the performance of CSD is not quite sensitive to the parameters. The proposed framework can perform well when choosing parameter settings in a reason-able range. Similar results have been observed for the two sets of experiments on the TweetS dataset; we omit the re-sults owing to lack of space.
Significant efforts have been devoted to detecting spam-mers in various online social networks, including Facebook [5], Twitter [19, 20, 22], Renren [32], Blogosphere [24], etc. One effective way to perform spammer detection is to use the social network information. The assumption is that spam-mers cannot establish a large number of social trust relations with normal users. This assumption might not hold in many social networks. Yang et al. [32] studied the spammers in Renren, and found that spammers can have their friend re-quests accepted by many other users and thus blend into the Renren social graph. Different from Facebook-like OSNs, microblogging systems feature unidirectional user bindings because anyone can follow anyone else without prior consent from the followee. Ghosh et al. [13] show that spammers can acquire many legitimate followers. Besides the methods based on social networks, some efforts [22] have also been de-voted to study characteristics related to tweet content and user social behavior. By understanding spammer activities in social networks, features are extracted to perform effective spammer detection. These methods need a large amount of labeled data, which is hard to obtain in social media.
Spammer detection on emails [4], SMS [14] and the web [31] has been a hot topic for quite a few years. The spams are designed to corrupt the user experience by spreading ads or driving traffic to particular web sites [31]. A popular and well-developed approach for anti-spam applications is learning-based filtering. The basic idea is that we extract effective features from the labeled data and build a classi-fier. We then classify new users / messages as either spam or ham according to their content information for filtering. The attempts have been done in these areas and the abundant labeled resources are the major motivation of our work.
Some efforts have been made to employ domain adaption and transfer learning in various applications, e.g. sentiment analysis [23] and text classification [27]. Our work started the investigation of leveraging knowledge from other media for spammer detection in microblogging. Different from tra-ditional methods, based on the quantitatively linguistic vari-ation analysis, our proposed framework naturally combines knowledge learned from internal and external data sources in a unified model. In addition, some work has been done to study the linguistic challenges of social media texts. It is accepted that texts in social media are noisy, but it is also reported by researchers that the texts are not as noisy as what people expected [2]. The language used in Twitter is more like a projection of the language of formal media like news and blogs with shorter form [21], and it is possible to make use of normalization and domain adaption to  X  X lean X  it [11]. The evidence provided by linguists also motivate us to explore the language differences of spams across different media, and make use of resources from other media to help spammer detection in microblogging.
Texts in microblogging are short, noisy, and labeling pro-cessing is time-consuming and labor-intensive, which presents great challenges for spammer detection. In this paper, we first conduct a quantitative analysis to study how noisy the microblogging texts are by comparing them with spam mes-sages from other media. The results suggest that microblog-ging data is not significantly different from data from the other media. Based on the observations, a matrix factor-ization model is employed to learn lexicon information from external spam resources. By incorporating external infor-mation from other media and content information from mi-croblogging, we propose a novel framework for spammer de-tection. The experimental results demonstrate the effective-ness of our proposed model as well as the roles of different types of information in spammer detection.

This work suggests some interesting future directions. Dif-ferent types of medium resources have different effects on the spammer detection performance. It would be interesting to quantify the contributions of different types of sources to spammer detection in microblogging. This could be an im-portant support for source selection in spammer detection. We truly thank the anonymous reviewers for their perti-nent comments. This work is, in part, supported by ONR (N000141410095) and ARO (#025071). [1] T. A. Almeida, J. M. G. Hidalgo, and A. Yamakami. [2] T. Baldwin, P. Cook, M. Lui, A. MacKinlay, and [3] L. Bilge, T. Strufe, D. Balzarotti, and E. Kirda. All [4] E. Blanzieri and A. Bryl. A survey of learning-based [5] Y.Boshmaf,I.Muslukhov,K.Beznosov,and [6] S. Boyd and L. Vandenberghe. Convex optimization . [7] H. M. Breland. Word frequency and word difficulty: A [8] F. Chung. Spectral graph theory . Number 92. Amer [9] C.Ding,T.Li,andM.Jordan.Convexand [10] T. Egener, J. Granado, and M. Guitton. High [11] J. Eisenstein. What to do about bad language on the [12] J. Friedman, T. Hastie, and R. Tibshirani. The [13] S. Ghosh, B. Viswanath, F. Kooti, N. Sharma, [14] J. M. G  X  omez Hidalgo, G. C. Bringas, E. P. S  X  anz, and [15] M. A. Halliday and C. M. Matthiessen. An [16] M. Hart. Project gutenberg . Project Gutenberg, 1971. [17] T. Hofmann. Probabilistic latent semantic indexing. In [18] X. Hu, N. Sun, C. Zhang, and T.-S. Chua. Exploiting [19] X. Hu, J. Tang, and H. Liu. Online social spammer [20] X. Hu, J. Tang, Y. Zhang, and H. Liu. Social [21] X. Hu, L. Tang, J. Tang, and H. Liu. Exploiting social [22] K. Lee, J. Caverlee, and S. Webb. Uncovering social [23] T. Li, Y. Zhang, and V. Sindhwani. A non-negative [24] Y.-R. Lin, H. Sundaram, Y. Chi, J. Tatemura, and [25] V. Metsis, I. Androutsopoulos, and G. Paliouras. [26] D. O X  X allaghan, M. Harrigan, J. Carthy, and [27] S. J. Pan and Q. Yang. A survey on transfer learning. [28] D. Seung and L. Lee. Algorithms for non-negative [29] R. Tibshirani. Regression shrinkage and selection via [30] R. Wardhaugh. An introduction to sociolinguistics , [31] S. Webb, J. Caverlee, and C. Pu. Introducing the [32] Z. Yang, C. Wilson, X. Wang, T. Gao, B. Zhao, and [33] S. J. Yates. Oral and written linguistic aspects of [34] Y. Zhu, X. Wang, E. Zhong, N. Liu, H. Li, and
