 In the data stream model, data arrive at high speed, and algorithms that process them must do so under very strict constraints of space and time. Consequently, data streams pose several challenges for data mining algorithm design. First, algorithms must make use of limited resources (time and memory). Second, they must deal with data whose nature or distribution changes over time.
An important issue in data stream mining is the cost of performing the learn-ing and prediction process. As an example, it is possible to buy time and space usage from cloud computing providers [25]. Several rental cost options exist:  X  Cost per hour of usage: Amazon Elastic Compute Cloud (Amazon EC2) is  X  Cost per hour and memory used: GoGri d is a web service similar to Amazon It is crucial to find mining methods that use resources efficiently. In this spirit, we propose in this paper the Hoeffding Perceptron Tree for classification, as a faster method compared to the state-of-the-art Hoeffding Tree with naive Bayes leaves. The idea is to implement perceptron classifiers at the leaves of the Hoeffding Tree, to potentially increase accura cy, but mainly to reduce runtime.
We introduce the use of RAM-Hours as an evaluation measure of the resources used by streaming algorithms. The paper is structured as follows: related work is presented in Section 2. Hoeffding Perceptron Trees and bagging of such trees are discussed in Section 3. An experimental evaluation is conducted in Section 4. Fi-nally, conclusions and suggested items for future work are presented in Section 5. Standard decision tree learners such as ID3, C4.5, and CART [18,21] assume that all training examples can be stored simultaneously in main memory, and are thus severely limited in the number of examples they can learn from. In particular, they are not applicable to data streams, where potentially there is no bound on the number of examples and these arrive sequentially.

Domingos et al. [6,14] proposed the Hoeffding tree as an incremental, anytime decision tree induction algorithm that is capable of learning from data streams, assuming that the distribution generating examples does not change over time.
Hoeffding trees exploit the fact that a s mall sample can often suffice to choose a splitting attribute. This idea is supported by the Hoeffding bound, which quantifies the number of observations (i n our case, examples) needed to estimate some statistics within a prescribed precision (in our case, the goodness of an attribute). More precisely, the Hoeffding bound states that with probability 1  X   X  , the true mean of a random variable of range R will not differ from the estimated mean after n independent observations by more than: A theoretically appealing feature of Ho effding Trees not shared by other incre-mental decision tree learners (ID4 [22], ID5 [24]) is that it has sound guarantees of performance. Using the Hoeffding bound one can show that its output is asymptotically nearly identical to tha t of a non-incrementa l learner using in-finitely many examples. CVFDT [14] is an extension of the Hoeffding Tree to evolving data streams, but does not exhibit theoretical guarantees.
Outside the data stream world, there is prior work on using perceptrons or similar classifiers in decision trees. Utgoff [24] presented the Perceptron Decision Tree as a decision tree in which each leaf node uses the perceptron as a classifier of the input instances. Bennett et al. [2] showed that maximizing margins in perceptron decision trees can be useful t o combat overfitting. Zhou [26] proposed Hybrid Decision Trees as a hybrid learning approach combining decision trees with neural networks.
 Frank et al. [7] investigated using Model Trees for classification problems. Model trees are decision trees with linea r regression functions at the leaves. Logistic Model Trees [17] are model trees that use logistic regression instead of linear regression. They have been sh own to be very accurate and compact classifiers, but their induction is very time-consuming.

The LTree algorithm of Gama [8] embodies a general framework for learn-ing functional trees, multivariate classification or regression trees that can use combinations of attributes at decision nodes, leaf nodes, or both.

In the data streams literature, Ikonom ovska et al. [15] presented FIMT, a fast incremental model tree for regressi on on static data streams. To deal with concept drift, Ikonomovska et al. [16] proposed FIRT-DD as an adaption of the FIMT algorithm to time-changing distributions. FIMT and FIRT-DD use a perceptron learner at the leaves to perform regression. Considering classification methods for data streams, Bifet et al. [4] presented two new ensemble learning methods: one using bagging with decision trees of different size and one using ADWIN , an adaptive sliding window method that detects change and adjusts the size of the window correspondingly. We revisit the latter approach in this paper. In this section, we present t he perceptron learner we use, and the Hoeffding Per-ceptron Tree based on it. We also consider bagging trees with change detection. 3.1 Perceptron Learning We use an online version of the perceptron that employs the sigmoid activation function instead of the threshold activation function and optimizes the squared error, with one perceptron per class value.

Given a data stream x i ,y i ,where x i is an example and y i is its example class, the classifier X  X  goal is to minimize the number of misclassified examples. Let h w ( x i ) be the hypothesis function of the perceptron for instance x i .We use the mean-square error J ( w )= 1 2 ( y i  X  h w ( x i )) 2 instead of the 0-1 loss function, since it is differentiable.

The classic perceptron takes a linear com bination and thresholds it. The pre-dicted class of the perceptron is h w ( x i )=sgn( w T x i ), where a bias weight with constant input is included. Our hypothesis function h w =  X  ( w T x )in-stead uses the sigmoid function  X  ( x )=1 / (1 + e  X  x ) since it has the property  X  ( x )=  X  ( x )(1  X   X  ( x )). Thus, we can compute the gradient of the error function where for sigmoid hypothesis obtaining the following weight update rule
Because we work in a data stream scenario, rather than performing batch updates, we use stochastic gradient descent where the weight vector is updated after every example. As we deal with multi-class problems, we train one per-ceptron for each class. To cl assify an unseen instance x , we obtain the pre-dictions h w 1 ( x ) ,...,h w n ( x ) from the perceptrons, and the predicted class is arg max class h w class ( x ). The pseudocode is shown in Figure 1. 3.2 Hoeffding Perceptron Tree Hoeffding trees [6] are state-of-the-art in classification for data streams and they perform prediction by choosing the majori ty class at each leaf. Their predictive accuracy can be increased by adding naive Bayes models at the leaves of the trees. However, Holmes et al. [12] identified si tuations where the naive Bayes method outperforms the standard Hoeffding tree initially but is eventually overtaken. They propose a hybrid adaptive method that generally outperforms the two original prediction methods for both simple and complex concepts. We call this method Hoeffding Naive Bayes Tree ( hnbt ). This method works by performing a naive Bayes prediction per training instance, and comparing its prediction with the majority class. Counts are stored to measure how many times the naive Bayes prediction gets the true class co rrect as compared to the majority class. When performing a prediction on a test in stance, the leaf will only return a naive Bayes prediction if it has been more accu rate overall than the majority class, otherwise it resorts to a majority class prediction.

We adapt this methodology to deal with perceptrons rather than naive Bayes models. A Hoeffding Perceptron Tree ( hpt ) is a Hoeffding Tree that has a per-ceptron at each leaf. Similarly to hnbt , predictions by the perceptron are only used if they are more accurate on average than the majority class. It improves on hnbt in terms of runtime because it does not need to estimate the statistical distribution for numeric attributes and calculate density values based on the exponential function, and for discrete attributes it does not need to calculate divisions to estimate probabilities.

Finally, a Hoeffding Naive Bayes Perceptron Tree ( hnbpt ) is a Hoeffding Tree that has three classifiers at each leaf: a majority class, naive Bayes, and a per-ceptron. Voting is used for prediction. It is slower than the Hoeffding Perceptron Tree and the Hoeffding Naive Bayes Tree, but it combines the predictive power of the base learners. 3.3 Bagging Trees with ADWIN ADWIN [3] is a change detector and estimator that solves in a well-specified way the problem of tracking the average of a s tream of bits or real-valued numbers. ADWIN keeps a variable-length window o f recently seen items, with the prop-erty that the window has the maximal length statistically consistent with the hypothesis  X  X here has been no change in the average value inside the window X .
ADWIN is parameter-and assumption-free in the sense that it automatically detects and adapts to the current rate of change. Its only parameter is a confi-dence bound  X  , indicating how confident we want to be in the algorithm X  X  output, inherent to all algorithms dealing with random processes.

Also important for our purposes, ADWIN does not maintain the window explic-itly, but compresses it using a variant of the exponential histogram technique. This means that it keeps a window of length W using only O (log W )memory and O (log W ) processing time per item.

ADWIN Bagging is the online bagging method of Oza and Russell [19] with the addition of the ADWIN algorithm as a change detector. When a change is detected, the worst classifier of the ensemble of classifiers is removed and a new classifier is added to the ensemble. M assive O nline A nalysis (MOA) [13] is a software environment for implementing algorithms and running experiments for online learning from data streams. All algorithms evaluated in this paper were implemented in the Java programming language by extending the MOA software.

We use the experimental framework for concept drift presented in [4]. Con-sidering data streams as data generated from pure distributions, we can model a concept drift event as a weighted comb ination of two pure distributions that characterizes the target concepts before and after the drift. This framework de-fines the probability that a new instance of the stream belongs to the new concept after the drift based on the sigmoid function.
 Definition 1. Given two data streams a , b , we define c = a  X  W t stream built by joining the two data streams a and b ,where t 0 is the point of Pr[ c ( t )= a ( t )] = 1  X  Pr[ c ( t )= b ( t )] .
 In order to create a data stream with multiple concept changes, we can build new data streams joining different concept drifts, i. e. ((( a  X  W 0 t 4.1 Datasets for Concept Drift Synthetic data has several advantages  X  it is easier to reproduce and there is little cost in terms of storage and transmission. For this paper we use the data generators most commonly found in the literature.
 SEA Concepts Generator. This artificial dataset contains abrupt concept Rotating Hyperplane. This data was used as a testbed for CVFDT versus Random RBF Generator. This generator was devised to offer an alternate LED Generator. This data source originates from the CART book [5]. An im-4.2 Real-World Data The UCI machine learning repository [1] contains some real-world benchmark data for evaluating machine learning techniques. We consider three of the largest: Forest Covertype, Poker-Hand, and Electricity.
 Forest Covertype. Contains the forest cover type for 30 x 30 meter cells ob-Poker-Hand. Consists of 1 , 000 , 000 instances and 11 attributes. Each record Electricity. is another widely used dataset described by M. Harries [11] and We use normalized versions of these datasets, so that the numerical values are between 0 and 1. With the Poker-Hand dat aset, the cards are not ordered, i.e. a hand can be represented by any permutation, which makes it very hard for propositional learners, es pecially for linear ones. We use a modified version, where cards are sorted by rank and suit, and have removed duplicates. This dataset loses about 171 , 799 examples, and comes down to 829 , 201 examples.
These datasets are small compared to syn thetic datasets we consider. Another important fact is that we do not know wh en drift occurs or indeed if there is any drift. We may simulate concept drift, joining the three datasets, merging attributes, and supposing that each dataset corresponds to a different concept As all examples need to have the same number of attributes, we simply concate-nate all the attributes, and set the number of classes to the maximum number of classes of all the datasets. 4.3 Results We use the datasets explained in the previous sections for evaluation. The exper-iments were performed on a 3 GHz Intel 64-bit machine with 2 GB of memory. The evaluation methodology used was Interleaved Test-Then-Train on 10 runs: every example was used for testing the model before using it to train. This inter-leaved test followed by train procedure was carried out on 10 million examples from the hyperplane and RandomRBF datasets, and one million examples from the SEA dataset. The parameters of these streams are the following:  X  RBF ( x , v ): RandomRBF data stream with x centroids moving at speed v .  X  HYP ( x , v ): Hyperplane data stream with x attributes changing at speed v .  X  SEA ( v ): SEA dataset, with length of change v .  X  LED ( v ): LED dataset, with length of change v .
 Tables 1, 2 and 3 report the final accuracy, and speed of the classification mod-els induced on the synthetic data and the real datasets: Forest CoverType , Poker Hand , Electricity and CovPokElec . Accuracy is measured as the final percentage of examples co rrectly classified over the test/train interleaved evaluation. Time is measured in seconds, and memory in MB. The classifica-tion methods used are the following: perceptron, naive Bayes, Hoeffding Naive Bayes Tree ( hnbt ), Hoeffding Perceptron Tree ( hpt ), Hoeffding Naive Bayes Perceptron Tree ( hnbpt ), and ADWIN bagging using hnbt , hpt ,and hnbpt .
The learning curves and model growth curves for the Led dataset are plotted in Figure 2. We observe that ht and hpt are the fastest decision trees. As the trees do not need more space to compute naive Bayes predictions at the leaves, hnbt uses the same memory as ht ,and hpnbt uses the same memory as hpt .On accuracy, ht is the method that adapts more slowly to change, and during some time intervals hpt performs better than hnbt , but in other intervals performs worse. hnbpt is always the most or very close to the most accurate method as it is capable of making use of the decision of the majority class, naive Bayes and perceptron.

Table 1 shows the accuracy, speed and m emory usage of a naive Bayes learner, a perceptron with  X  = 1 and a classic Hoeffding Tree with majority class learn-ing at the leaves. As naive Bayes uses a Gaussian distribution to model numeric attributes, with different variances for each class, it does not yield a linear sep-arator as the perceptron does. In general terms, we see that the perceptron and the Hoeffding Tree are the fastest methods, but the Hoeffding Tree needs more memory. Comparing using RAM-Hours, naive Bayes needs 3 . 5timesmoreRAM-Hours than the perceptron, and the Hoeffding Tree needs 89 more RAM-Hours than naive Bayes. Note that using  X  = 1 we obtain a very fast adaptive method, but for some datasets like Poker , the results are worse than obtained using a more conservative rate like  X  =0 . 01. Choosing an optimal  X  remains an open problem for further research.

Table 2 shows, for the Hoeffding tree models, their accuracy, speed and mem-ory. We see that hpt is much faster than hnbt , and more accurate in several streams. hnbpt is more accurate than hnbt and hpt , but it needs more time. Comparing RAM-Hours, hpt needs 1 . 11 times more RAM-Hours than hnbt , and 2 . 54 more RAM-Hours than ht ,and hnbpt needs 1 . 37 more than hpt .
Table 3 reports the accuracy, speed and memory of ADWIN bagging using hnbt , hpt ,and hnbpt . ADWIN bagging using hnbpt is the most accurate method, but it uses more time and memory than the other variants. In RAM-Hours, it needs 1 . 62 times more than ADWIN bagging using hpt ,and1 . 51 times more than ADWIN bagging using hnbt . ADWIN bagging using hpt needs fewer resources than ADWIN bagging using hnbt .

Comparing hnbpt from Table 2 with the single perceptron from Table 1, we obtain a 20% better accuracy, but at a cost of 780 times the amount of RAM-Hours. Comparing ADWIN bagging using hnbpt (Table 3) with a single hnbpt (Table 2) we obtain a 3% better accuracy, at 16 . 50 times the RAM-Hours.
Concept drift is handled well by the proposed ADWIN bagging algorithms, exclud-ing the poor performance of the hpt -based classifier on CovPokElec ,whichis due to the nature of the Poker dataset. Decision trees alone do not deal as well with evolving streaming data, as they have limited capability of adaption.
A tradeoff between RAM-Hours and a ccuracy could be to use single per-ceptrons when resources are scarce, and ADWIN bagging methods when more accuracy is needed. Note that to gain an i ncrease of 24% in accuracy, we have to increase by more than 10 , 000 times the RAM-Hours needed; this is the differ-ence of magnitude between th e RAM-Hours needed for a single perceptron and for a more accurate ADWIN bagging method. We have investigated four perceptron-based methods for data streams: a single learner, a decision tree, a hybrid tree, and an ensemble method. These methods use perceptrons with a sigmoid activation function, optimizing the squared error, with one perceptron per class value. We ob serve that perceptron-based methods are competitive in accuracy and use of resources. We have introduced the use of RAM-Hours as a performance measure. Using RAM-Hours, it is easy to compare resources used by classifier algorithms.

As future work, we would like to build new methods based on the perceptron, with an adaptive learning rate. We think that in changing scenarios, using a flexible learning rate may allow us to obtain more accurate methods, without incurring large additional runtime or memory costs.

