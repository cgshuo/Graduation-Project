 Urban neighborhood classification using very high resolution (VHR) remote sensing imagery is a challenging and emerging application. A semi-supervised learning approach for identi-fying neighborhoods is presented which employs superpixel tessellation representations of VHR imagery. The image rep-resentation utilizes homogeneous and irregularly shaped re-gions termed superpixels and derives novel features based on intensity histograms, geometry, corner and superpixel den-sity and scale of tessellation. The semi-supervised learning approach uses a support vector machine (SVM) to obtain a preliminary classification which is then subsequently refined using graph Laplacian propagation. Several intermediate stages in the pipeline are presented to showcase the impor-tant features of this approach. We evaluated this approach on four different geographic settings with varying neighbor-hood types and compared it with the recent Gaussian Mul-tiple Learning algorithm. This evaluation shows several ad-vantages, including model building, accuracy, and efficiency which makes it a great choice for deployment in large scale applications like global human settlement mapping and pop-ulation distribution (e.g., LandScan), and change detection.  X  This work is partially supported by NSF IIS 1065081 c  X  H.2.8 [ Database Applications ]: Data Mining; I.5 [ Pattern Recognition ]: Models X  Statistical Remote Sensing, Segmentation, Neighborhoods
The past twenty years have seen an explosion of interest in remote sensing technologies and machine learning algo-rithms aimed at representing, categorizing and classifying land cover. With applications ranging across agriculture, space exploration and urban planning, remote sensing is a technology with a huge upside with many more applications yet to be envisaged let alone executed. The various im-provements in imaging resolution over the past two decades has led to fine-grained classification with several applica-tions; biomass monitoring, urban settlement mapping, cli-mate change projections, etc.

Remote sensing applications have certain characteristics that take them beyond routine application of machine learn-ing algorithms for classification. First, remote sensing data live on a spatio-temporal grid and this fact is fundamentally at odds with the independent and identically distributed (i.i.d.) sample-based approach adopted in present-day ma-chine learning. For this reason, it is vitally important to begin with an image representation which takes the gridded nature of the data into account. We adopt a segmentation-based representation which leverages the spatial remote sens-ing grid. Second, the range of queries in remote sensing are more complex. Instead of requiring pixel-level classification, newer applications requires us to classify homogeneous re-gions into single categories while maintaining clear-cut re-gion boundaries between classes (e.g., urban versus forest). This is a far cry from merely labeling individual samples X  the usual scenario in standard machine learning. Third, the Figure 1: An example of the high resolution aerial images obtained from Google Earth that we use in our experiments for terrain classification. The above image contains more than 1 million pixels and represents an area of about 1 sq km on the ground. underlying volume (terabytes to petabytes) and velocity (gi-gabytes to terabytes per day) of data produced by these ap-plications is very large and is responsible for carrying us into the bigdata regime. Effective processing requires low com-plexity and multi-scale algorithms that can exploit modern architectures with deep memory hierarchies. This is an im-portant challenge for existing machine learning techniques. Finally, expert interaction and the demarcation of training and test set regimes are very different in remote sensing. Ex-perts may be called upon to label whole regions, for example, rather than individual pixels. And, since the entire image is available at the time of training, a semi-supervised approach which does not arbitrarily demarcate training and testing regimes is needed. These considerations shape our work presented below. In a nutshell, we leverage a segmentation-driven image representation and perform semi-supervised la-bel propagation within this representation to achieve image classification in our remote sensing application. An example of the images that we are going to use in this paper can be seen in Figure 1:
A fundamental point of departure taken in the present work is the use of superpixel tessellation representations of remote sensing images. When previous work on remote sens-ing classification is examined in this light, we invariably find that either (i) a pixel-based classification approach is adopted with no consideration given to local region homo-geneity, or (ii) rectangular patches of pixels are used, once again with no consideration given to local homogeneity and natural shape of the patch under consideration. In sharp contrast to these previous approaches, we begin with a su-perpixel tessellation representation of remote sensing im-ages. Coherent local structures are a characteristic, com-mon to remote sensing imagery, and adequately captured by superpixel tessellations. Superpixels are local homoge-neous groupings of pixels and superpixel tessellations ensure that the image domain is covered by the superpixels with no overlap. Since region homogeneity is a function of scale, we employ a pyramid of superpixel tessellations with the coarser scales using local groupings of superpixels from finer scales. The result is a scale space of superpixel tessellations X  X  seg-mentation driven image representation, fundamental to our approach and rarely used by competing remote sensing clas-sification approaches at the present time.

We now briefly delve into the specifics of the superpixel tessellation approach deployed here. Essentially, we adapt the popular ultrametric contour map (UCM) approach to image segmentation to obtain superpixel tessellations. We briefly summarize UCM and highlight its use as a feature extractor X  X  novel aspect we have not seen in other recent work. UCM begins by obtaining scale-space image gradi-ent information at each pixel and for different orientations. Next, a graph is constructed by joining any pair of pixels which exhibit good evidence for a line segment connecting them in the actual image. Taking cues from recent develop-ments in graph partitioning and spectral clustering, the top eigenvectors of the graph Laplacian are computed and re-arranged in image space. Then, gradients are computed on the eigenvector images and combined with the original image gradients (from step one) to produce a contour descriptor at each pixel. A oriented watershed algorithm is executed on the gradient image to produce the lowest level superpixel tessellation. These superpixels are combined using group-ing heuristics to obtain the final ultrametric contour map (UCM) [2] with superpixel containment across levels. The superpixel tessellation obtained is the image representation used in this work.

Once the hierarchical image representation is in place, we can focus on superpixel classification. As mentioned earlier, we prefer to classify superpixels (at a suitably chosen level) instead of classifying individual pixels of rectangular regions (which lack local homogeneity). A semi-supervised machine learning approach is most suitable in remote sensing and in this context of superpixel tessellation representations. The remote sensing expert is tasked with labeling O (1) super-pixels (at a suitably chosen level) with the machine learning algorithm subsequently labeling all the other superpixels. Assume that the expert labels are in place. We now ex-tract features at every superpixel which are consonant with the kind of discrimination required in this application: the separation of remote sensing image data into urban, slum, forest and other labels. To this end, we discovered a novel feature stemming from our chosen UCM-based image repre-sentation. The number and type of superpixels (at a suit-ably chosen level) can function as discriminating features. It turns out that regularly shaped superpixels are better in-dicative of urban regions and the density of superpixels cor-relates well with slums. These novel features are combined with more standard color histogram-based features into a superpixel classification algorithm using standard support vector machines (SVMs). Since SVMs do not leverage the underlying spatial grid and indeed have a fundamental lim-itation as sample-based classifiers, we execute a Laplacian graph-based label propagation algorithm to improve on the result of SVM-based classification.

In this paper, we utilize the above approach for semi-supervised labeling of regions into different land-use land-cover (LULC) classes (urban, slums, forests, sea, sand) in high resolution aerial images. Our approach leverages new features as the traditionally available features such as HOG and SIFT may not be effective in discriminating among dif-ferent LULC classes pictured in aerial images.
The applications centered around very high resolution (VHR) imagery have gained huge impetus recently, primarily be-cause images with sub-meter resolution are now available easily X  X n outcome of a surge in the launch of satellites by private companies like Digital globe (e.g., WorldView-2 in late 2009). Evidently, such imagery provides exten-sively new avenues for the automatic classification of both natural regions (forest, sea, different kinds of terrain) and man-made structures (residential and commerical buildings, for instance) worldwide. We are developing various new approaches to efficiently process this imagery for monitor-ing natural and man-made infrastructures (including neigh-borhoods, nuclear and other energy infrastructures). At the Oak Ridge National Laboratory (ORNL), the Computa-tional Science and Engineering division produces the Land-Scan [21] high-resolution population database that is widely used by both government agencies and commercial entities across the world. One of the critical inputs to this model is a thematic layer consisting of different neighborhoods gen-erated from the VHR imagery that spans the globe. Gener-ating a global scale neighborhood map at a sub-meter res-olution is a daunting task. For the past several decades, ORNL is developing accurate and computationally efficient methods to process VHR imagery. Likewise, a few interna-tional agencies like European Commission (EU) Joint Re-search Center (JRC) are involved in global scale settlement mapping [26] using VHR imagery.

Despite great efforts by the research community across the globe, neighborhood mapping is a challenging task. First of all, neighborhoods are not well defined, which is reflected in the quote by Galster [12] X  X  X rban social scientists have treated  X  X eighborhood X  in much the same way as courts of law have treated pornography: a term that is hard to define precisely, but everyone knows it when they see it. X  There is no consistent nomenclature across the countries regarding neighborhoods and no consistent ground-truth, making it very difficult to build machine learning models for global scale problems. In addition, most of the neighborhoods are made up of complex objects (consisting of different types of objects, not just buildings and roads) which makes it very difficult to obtain ground-truth data from images.

Despite these problems and limitations, mapping neigh-borhoods, especially informal settlements is crucial in terms of national security and on humanitarian grounds as well. This is because these informal settlements arise unplanned and unauthorized in the most hazardous regions and lack basic services, and therefore, pose several challanges to the nations. Even though numerous studies sponsored by World Bank and United Nations emphasize the significance of poverty maps in designing better policies and interventions, man-ually mapping the slums of the world is a daunting task. The framework provided in this paper is an advancement in terms of computational efficiency and accuracy in the cur-rent technology available to detect new settlements (across the globe), and as well as characterize different neighbor-hoods which is a key input to other programs like Land-Scan [21]. In addition, neighborhood maps have several other applications, including but not limited to health [17], energy [10], and crime [16].

The rest of the paper is organized as follows. In the next section we briefly discuss the related work in classify-ing remote sensing image datasets using semi-supervised ap-proaches. Section 3 describes our approach in detail. Section 4 provides experimental validation and section 5 presents conclusions.
The major steps involved in remote sensing image classifi-cation can be abstracted into: (i) extraction of features from the image, (ii) collection of ground-truth (training/test) data for a few sample locations, (iii) building a classification model (e.g. na  X   X ve Bayes, decision trees, MLPs), and (iv) pre-dicting labels for the entire image. Most existing classifi-cation approaches work with spectral features (e.g., blue, green, red, thermal infrared) and derived features (e.g., tex-ture, band ratios like Normalized Difference Vegetation In-dex (NDVI), Histogram of Oriented Gradients (HOG)), ex-tracted at each pixel (spatial location). These classification approaches are called pixel-based or single instance learn-ing (SIL) algorithms. A review of these techniques can be found in [29, 14]. Most classification schemes model the correlations in feature space while the spatial locations of those features are often ignored. An improvement over per-pixel classification schemes is to incorporate spatial locations such as MRF [23]. This combination of the two where spa-tial correlations and feature correlations are modeled simul-taneously leads to what is known as spatial classification schemes. This results into much smoother class distribu-tions in the final classified image. However, it should be noted that spatial classification methods are also essentially single instance learners. One way to overcome the single instance limitation is to look at additional features beyond spectral features. For example, features that exploit spatial contextual information have proved quite useful in classify-ing very high-resolution images. Recent studies [29, 27, 14] show the improved performance of SIL methods when the spectral features are combined with a broad set of extended features such as morphological, texture, and edge density. Although these studies showed that the extended features which exploit spatial contextual information resulted in im-proved SIL accuracy, the underlying image complexity and interpixel relationships are still not fully exploited.
Complex object recognition requires investigation of spa-tial regions or image patches. Object based classification schemes [20, 4] exploit the spatial and spectral features in order to group the pixels into coherent regions called ob-jects. One can then use these objects to build a meta clas-sifier on the features (shape, geometry, etc.) that describe the whole object and not just a particular pixel. Another approach has also been to simply aggregate all features for all pixels belonging to a particular object into a single fea-ture vector and then apply any single instance learning algo-rithm. However, all these approaches lose important struc-tural and spatial properties in the aggregation process. In order to overcome some of the limitations of single instance learning schemes, multiple instance learning (MIL) meth-ods have been developed. The seminal work of Dietterich et al. [9], Diverse Density [18], and Citation-KNN [30] are some notable approaches for MIL. In general MIL meth-ods are shown to perform better than single instance learn-ing schemes, and therefore, have seen application in remote sensing image classification as well. For example, in [5], the authors have developed an MIL based binary classification scheme for identifying targets (landmines) in Hyperspectral (HS) imagery. The high computational cost of Citation-KNN has led to the development of an efficient Gaussian Multiple Instance (GMIL) [28] learning algorithm. Both of these algorithms are shown to perform better than most well-known SIL approaches, however leveraging them for global scale problems is difficult due to their computational complexity. We believe that our work which utilizes irregu-lar patches or superpixels (which are mainly homogeneous) along with novel and parallelizable machine learning tech-niques have the potential to address the scale requirements of target applications. In addition our approach eliminates the need for determining an appropriate grid size which im-pacts the performance X  X oth in terms of computation and accuracy.

Finally, we summarize the evolution X  X ainly in the past decade X  X f graph-based semi-supervised learning (SSL) method-ologies. Note that there is no general literature of graph-based SSL for gridded data. Early work on SSL focused on optimization [8] and relationships to transductive infer-ence [15, 25] and multi-view learning [19]. Since then, the use of graphs in SSL has become standard [13]. Graph-based SSL methods attempt to assign node labels using a weighted combination of the neighbors. Different methods use different principles to design objective functions for la-bel propagation. For example, a popular approach [31] it-erates a function of the graph affinity matrix until conver-gence and then uses the sign of the function at each node. A different method adapts the Jacobi iteration for linear systems and obtains a somewhat different weighted combi-nation subsequently used for prediction. Other influential methods [3] use regression to determine the weighted com-bination. First, they compute the graph Laplacian followed by eigenvector computation. Then a regression objective estimates a weighted combination of the principal eigenvec-tors on the training samples which is utilized for prediction at the unlabeled nodes. Other methods draw upon random walks on graphs to perform label prediction [7].

Our work mostly extends the work of [22] to color images and adds superpixel density at multiple scales of the UCM hierarchy as a new feature. This is an important augmenta-tion to the feature descriptor needed for terrain classification as we will describe in the following sections.
In this paper, we describe an efficient image representa-tion and machine learning approach for analyzing large scale remote sensing imagery to support a wide range of earth sci-ence applications. The application involves the labeling of remote sensing imagery given expert labels on a small frac-tion of data. Typically, the classes are (i) forestry, (ii) slums, (iii) urban and (iv) other (not above). The approach has the following steps: 1. Tessellation of the data into superpixels: This step con-verts the data into irregular (but coherent) patches called su-perpixels. Superpixels correspond to coherent patches or ar-eas in 2D. These coherent superpixels reduce the data com-plexity since processing is moved to the superpixel level from the pixel level. Superpixels also have a huge advantage over partitioning the image into regular patches because regular patches ignore the local variability of the underlying data w.r.t. the grid. 2. Generating multi-pixel features for each superpixel: At this step we generate features which are effective in discrimi-nating between terrains present in the spatiotemporal image data. We exploit intensity, geometry and scale of tessellation to arrive at these features. 3. Building a superpixel graph: This step constructs a su-perpixel graph with edges corresponding to the spatiotem-poral (grid). The nodes of the graph are the superpixels with the edges being the connections between them. 4. Label initialization of superpixels: Labels are only available for a small number of pixels. This information is used to derive the labels for a subset of superpixels. A clas-sifier is then built using these features to provide an initial label for all the nodes of the superpixel graph. 5. Label refinement: The rudimentary labels obtained from the previous step are further smoothed using semi-supervised learning techniques.
 These steps are described in detail in the next subsections. We will use the image in Figure 1 as a running example. The size of this image is roughly 1 million pixels.
Since the advent of normalized cuts [24] and graph-cuts [11, 6], there has been considerable interest in segmenting an image into sets of superpixels. There are several techniques available in the image processing literature. The ultramet-ric contour map (UCM) [2] is a popular method which uses local and global cues to produce a hierarchy of tessellations at different scales ranging from fine to coarse. These tes-sellations respect the containment property, that is every finer scale tessellation is contained within the next higher (or coarser) scale tessellation. We use UCM in two different ways  X  (i) The first usage is more traditional and direct in the sense that a finer scale tessellation is obtained which is used for classifying (or labeling) each superpixel (as against each individual pixel). (ii) This second usage is more sub-tle because we obtain the tessellation at a coarser scale and use it as a feature for classifying the superpixels at the finer scale selected in (i). For example, for the target application, the tessellation at a coarser scale mostly picks up promi-nent boundaries thus increasing the probability of detecting urban regions. Furthermore, the density of superpixels is greater in slum regions than in urban, forested regions mak-ing it a suitable feature for slum detection. In Figure 2, we show the superpixels obtained for the image in Figure 1 corresponding to fine and coarse scales.

The steps in superpixel estimation using UCM are as fol-lows: (i) feature extraction using oriented scale-space gradi-ents, (ii) graph construction, (iii) eigenvector computation, (iv) scale-space gradient computation on the eigenvector im-age, (v) combination of local and global information and (vi) oriented watershed transform to produce non-uniform tessellations. While this sequence is somewhat of a simpli-fication, the major steps have been highlighted. Note that the UCM approach obtains local and global contour infor-mation by combining information from the original image and weighted graph eigenvector  X  X mages. X  This perceptual grouping property is mainly responsible for obtaining good superpixel tessellations. We now detail the individual steps in the overall sequence: Step 1: Scale space feature extraction from brightness, tex-ture and wavelength channels: Oriented Gaussian derivative filters are executed at multiple scales to obtain
I where w i (  X  ) ,s (  X  ) is a set of weights that depend on the channels and scales. The dependence between the Gaus-sian filters and the scales can range from the simple to the complex. Here, we have just executed the filters at multiple scales and orientations. This results in a set of local features at different orientations which integrates information from the different color channels.
 Step 2: Weighted graph construction: A weighted graph (for the purposes of eigenvector computation) is constructed using the local filter responses above. Following the UCM strategy, pixels within a certain distance of each other are linked by a weighted edge using the relation where  X  is a constant and z ( x , y ) is any point lying on the line segment connecting x and y .
 Step 3: Eigenvector computation from the weighted graph W ( x , y ): Following the standard strategy of spectral clus-tering, the top eigenvectors of the weighted graph are com-puted. Since these eigenvectors are in location space, the result is a set { e k ( x ) } (usually rescaled using the eigenval-ues of the weighted graph).
 Step 4: Spectral information obtained from the top K eigenvectors: Since gradient information computed from the scaled eigenvectors can be expected to contain complemen-tary spectral information [2], a set of gradient operations in different orientations are computed to obtain Step 5: Combination of local and spectral information: We linearly combine the information in (1) and in (3) to obtain the final, global contour probability measure. A free param-eter is used for the combination and this needs more careful validation (which we reserve for future work).
 Step 6: Oriented watershed transform applied to the global contour measure: Since the global contour probability map may not always be closed and therefore may not divide the image into regions, we require another operation to extract closed contours. We have used the Oriented Watershed Transform (OWT) [2] to construct closed contours. Here, the orientation that maximizes the response of the contour detection approach is used to construct a set of regions and further build a hierarchical region tree from the input con-tours. Real valued weights are associated with each possible segmentation based on their likelihood to be a true bound-ary. For a specific threshold, the set of the resulting closed contours can be seen either as a segmentation or as the out-put of the super-pixellization. Further it can be seen that the uncertainty of a segmentation can be represented X  X t low thresholds, the image can be oversegmented respect-ing even very least probable boundaries and as you make the threshold higher only very strong boundaries survive (Fig. 3). This has the benefit of introducing a trade off be-tween the extreme ends of the segmentation.

The resulting tessellation for the image in Figure 1 is given in Figure 2. It shows that areas of significant variation re-quire smaller patch sizes while areas with less variations are captured by large patch sizes. We believe that this variabil-ity is one of the major strengths of the proposed approach.
Each superpixel at a finer level is described using three kinds of features X  X ntensity histograms, corner density and a binary feature derived from the coarser levels. For the intensity histograms, we quantize the grayscale intensities into 52 bins and obtain a 52 dimensional feature vector for each superpixel.

We use the Harris corner detector to obtain a density measure X  X he number of corners per unit area for each su-perpixel. Corners are an important feature for discrimi-nating between regions with buildings (for example, slums and urban area) and regions without (for example, forest and sea). To further distinguish between urban and slum dwellings, we compute the density of superpixels at a suit-able level. This feature is based on the (visual) observation that slums have a higher superpixel density relative to forest and urban settlements at the same level of the superpixel pyramid. Another key feature for distinguishing between different kinds of human settlements (slums versus urban) is the presence of stronger boundaries around the superpix-els representing the urban regions. Traditional features like dense SIFT, HOG etc. can be used to detect these regions but these suffer from the problem of determining the appro-priate scale and orientation. Further these features are also not able to pick up the prominent boundaries as detected by UCM tessellations at coarser scales. As UCM inherently in-volves a combination of dense features like textons and color histograms and only shows stronger boundaries at coarser (a) Regions overlayed with the binary urban descriptor. Re-gions overlayed as white correspond to 1 in our binary urban descriptor signifying a high probability of finding urban re-gions scales, it greatly simplifies the task of discriminating the ur-ban regions from the slums. The coarser scale UCM provides a binary feature for each finer superpixel as follows. The coarser scale UCM only keeps prominent boundaries and therefore outputs much larger superpixels. Among them, the superpixels which are smaller than a certain size thresh-old predominantly belong to urban regions. This is because urban regions are usually found with stronger boundaries and hence are more discriminating. The superpixels which are much larger than the size threshold are more likely to be a merger of several different types of smaller superpixels and are often not very discriminating. For example, these super-pixels can be a merger of slums and forests or other similar looking regions which do not have as clearly demarcating boundaries as the urban regions. We label the superpixels below the chosen size threshold with ones and the superpix-els above this threshold with zeroes in order to get binary features. These binary features are then percolated down the UCM hierarchy to the finer scale superpixels. All the finer superpixels contained in the larger superpixels get the same label as that of their larger parent superpixel.
Additionally, we also compute the density of finer level superpixels contained within each superpixel at the coarser level. This density is high in the regions corresponding to slums and low in the regions corresponding to urban and forest regions. This can be noticed in Figure 2. We choose a density threshold above which we mark all superpixels at the finer level as ones while the remaining as zeros. This approach further simplifies the task of discriminating slums from the rest of the image. Henceforth, we call these two discriminating features for urban and slum regions as binary urban descriptor and binary slum descriptor respectively. The significance of these two features is visually depicted in Figure 3 by overlaying them on top their corresponding regions of the image in Figure 1.

Further, we use average RGB values corresponding to each superpixel as another 3D feature. All these five different kind of features are then concatenated to form a 58 dimen-sional feature vector which describes each superpixel of the finer tessellation. Other features like HOG and dense SIFT can also be added to the above framework if needed.
Given the large size of the underlying datasets it is im-practical to expect that the ground truth is available except at a small number of grid points since data sets scale but experts do not. Thus, practical approaches have to be semi-supervised (as opposed to supervised or unsupervised) with the focus restricted on methods with proven scalability. In this work, we achieve semi-supervised learning through a two stage process of (i) classification using either SVM or kNN followed by (ii) graph Laplacian smoothing. Our clas-sification pipeline is similar to [1]. As mentioned above, the ground truth data is available only for a small number of superpixels as labeled by experts. We use this ground truth to train our classifier. A standard linear SVM was used for training. The model obtained from training is then used to determine preliminary labels for all other superpixels.
Because of the semi-supervised nature of the problem, the classification obtained from above is rudimentary because it is based on a classifier (SVM in our case) derived from limited ground truth data. This classification can lead to artifacts such that neighboring regions which belong to the same class may get labeled incorrectly. To correct this prob-lem and after being inspired by [1], we apply the Laplacian propagation method as detailed here.

Let f i denote the feature vector corresponding to the i th superpixel and let X i be the label that is required to be found from the Laplacian propagation. Let Y i be the ini-tial label as obtained from the first stage of either SVM or kNN. To perform Laplacian propagation, we construct a graph connecting adjacent superpixels in the spatial do-main (and not in the feature domain). The edge weight is the following objective function [1] : C ( X ) = where D ii = P N j =1 W ij . The objective in (4) is optimized separately for each category in a one versus rest fashion. Y = 1 if the superpixel belongs to the category and 0 oth-erwise. X i can take a real value and after minimizing the objective in (4) for each category, we assign each superpixel to the category which corresponds to the maximum value of X . The objective function in (4) can be directly minimized by solving a linear system of equations [1]:
The image in Figure 4a below shows the preliminary labels obtained after SVM classification and its further refinement by using the graph Laplacian to obtain the final labels in Figure 4b.
To evaluate the accuracy and the efficiency of our ap-proach, we used VHR imagery data from three different ge-ographic settings. The first five images are collected from the Google Earth from Rio, Brazil which is approximately around a million pixels corresponding to 1 square km of area. The Rio images represent two major types of neighborhoods X  formal (high-rise apartments and commercial complexes), and informal (favelas). The other two images are from Madi-son and Milwaukee suburbs from Wisconsin, USA and the last image is from Sterlings Heights city, a suburb of De-troit, Michigan, USA. These images correspond to about 4 square km with the same resolution of 1m as the rio images. The Madison image represents two distinct neighborhoods of commercial complexes and suburban residential commu-nities, while the Milwaukee image consists of downtown and residential neighborhoods. Sterling Heights is the second largest suburb of Metro Detroit and fourth largest city in the state of Michigan. The subregion chosen from Ster-ling Heights consists of commercial and residential neighbor-hoods. Apart from the major categories of neighborhoods, these images also contain forests (and isolated trees mixed with houses), grass fields and lawns, undeveloped areas (bar-ren lands and rock outcrops), water bodies, and sandy areas along the shore. As can be seen from these three study re-gions, they represent diverse set of neighborhoods. The first five Rio images and their classification results are shown in Figure 4 and Figure 5. The Madison, Milwaukee, and De-troit images are shown in Figure 6.

Prior to obtaining superpixels using the method given in [2], we converted the color images to grayscale and per-formed Gaussian smoothing on the grayscale version of our color images. The support of the Gaussian filter was chosen to be 10 pixels wide and the standard deviation was set to 15. It is important to note that this heavy smoothing of image data as mentioned above was only done for computing UCM [2]. The high resolution detail provided by the images can lead to the generation of an enormous number of superpix-els which are not needed for classification purposes. Hence, blurring the images to reduce the number of superpixels is a crucial step. Once UCM was obtained, the original images were used at every other stage of our pipeline, for example in computing features or for classification.

Grayscale intensity histograms, RGB values averaged over each superpixel at multiple levels of the UCM hierarchy, cor-ner density, textons, and descriptors derived from the UCM hierarchy were extracted for each superpixel. As explained Madison 1.01 9.82 Milwaukee 0.52 12.93 Table 1: Quantitative results for the image in Figure 1 and the 4 different images shown in Figure 5.
 Madison 6.8 3.09 Milwaukee 17.2 6.73 Table 2: Results obtained with GMIL for the first Rio image in Figure 1 and the 3 images belonging to Madison, Milwau-kee, and Detroit as shown in Figure 6. in section 3.2, simple descriptors (multilevel superpixel con-tainment density and superpixel area at a coarser UCM level) were extracted from the UCM hierarchy which can be intuitively seen to distinguish urban (residential, com-mercial) regions, and slums.

The intensity histograms were obtained by quantizing the intensities into 52 bins. The average RGB values comprised a 3-dimensional vector of average color values for each su-perpixel. The corner density feature obtained was a scalar which was multiplied by a factor of 100. The coarser scale UCM features and multilevel superpixel containment den-sity features were binary valued. These weighted features were concatenated together to obtain a 58 dimensional fea-ture vector describing each superpixel.

For training the SVM, the ground truth labels were pro-vided to only about 1% (see Table 1) of the superpixels at the finest level. Note that this was even lesser for the Milwau-kee (0.52%) and Detroit (0.26%) images. The SVM classifier was used to obtain the preliminary labels for all the other superpixels. This was then given as an initialization to the Laplacian propagation algorithm in order to obtain the final labels. The values of  X  and  X  were kept fixed to be 2 and 0.125 respectively. For all the images, the misclassification error was around 10% (Table 1). The misclassification error was computed by taking the weighted average of each mis-classified superpixel where the weights for each superpixel were the ratio of the area covered by them in the image to the total image area.

The total time required for the overall processing (includ-ing UCM) was about 20 minutes on a sequential machine.
Our proposed framework is also compared against the re-cent Gaussian Multiple Instance Learning (GMIL) [28] algo-rithm which showed good improvement over standard per-pixel based classification schemes. The accuracy estimates for these three images are summarized in Table 2. Results from our approach are comparable to GMIL, however, it should be noted that GMIL is computationally expensive and barren regions respectively.
 respectively. and also requires more ground truth training data compared to our technique.
 Careful analysis of the workflow and final classification re-sults show following key advantages of the proposed method over GMIL [28]. residential areas, commercial areas, forests, and grass, and water. Training One of the most important aspects of supervised Segment vs. Block The basic unit of learning in GMIL Computational Cost GMIL requires each image block (mil-Accuracy Accuracy of both methods are comparable (see
Based on these advantages, we are confident that the pro-posed methods will become operational in global scale ap-plications like LandScan.
We have developed a novel and scalable machine learn-ing framework for classifying neighborhoods in VHR im-ages. Accurate identification of neighborhoods is critical for many applications, including global scale high-resolution population databases (e.g., ORNL/LandScan), national se-curity, human health, and energy. To meet these challenges, we also developed features which can effectively discrimi-nate between different neighborhoods. This approach com-bines a superpixel image tessellation representation with semi-supervised label propagation (SVM followed by graph Laplacian-based propagation). The superpixel representa-tion naturally coheres with local boundary information and is a major reason for obtaining good classification. Ex-perimental evaluation on three different geographic settings showed good classification performance. The approach taken in this work is extensible to temporal data as well and will be the focus of our future work. In addition, we plan to fo-cus on improvements to superpixel tessellation-based image representations along the lines of improved computational efficiency and contextual level selection.
