 HIDEKI ISOZAKI, KATSUHITO SUDOH, HAJIME TSUKADA, and KEVIN DUH, Statistical machine translation (SMT) [Koehn 2009] works well for machine transla-tion among languages that have similar word orders. However, SMT does not work well for distant language pairs such as English and Japanese. English is a subject-verb-object (SVO) language and Japanese is a subject-object-verb (SOV) language.
Some existing methods try to solve this word-order problem in language-independent ways [Genzel 2010; Li et al. 2007; Quirk et al. 2005; Xia and McCord 2004; Yamada and Knight 2001]. They usually parse input sentences and learn reordering decisions at each node of the parse trees.

Other methods tackle this problem in language-dependent ways [Collins et al. 2005; Katz-Brown and Collins 2008; Lee et al. 2010; Nguyen and Shimazu 2006]. If we can completely reorder words by preprocessing, we will be able to greatly reduce the computational cost of SMT systems. Recently, Xu et al. [2009] and Hong et al. [2009] proposed rule-based preprocessing methods for SVO-to-SOV translation. These methods parse input sentences by dependency parsers and reorder the words using a set of hand-crafted rules to obtain SOV-like sentences. Hong et al. X  X  method [2009] is straightforward. They obtain SOV word order by moving verbs and adjectives to the ends of corresponding phrases. Xu et al. [2009] used more complicated rules. Each of their rules is composed of a part-of-speech tag, a dependency label, a rule weight, and a reordering order.

On the other hand, Lee et al. [2010] employed a constituent parser instead of de-pendency parsers. A constituent parser provides the grammatical function tag of each phrase. By using the function tags, it is easier to specify the context where a rule is applicable.

In this article, we present one general reordering rule: Head Finalization. Our idea is simple: move heads to the end of corresponding phrases or clauses because Japanese is a typical head-final language. That is, a head word usually comes after dependent words. SOV is just one aspect of head-final languages. Head-driven phrase structure grammar (HPSG [Pollard and Sag 1994]) parsers enable us to implement the rule at an abstract level. To implement this idea, we need a parser that outputs  X  X yntactic heads. X  Enju [Miyao and Jun X  X chi Tsujii 2008; Miyao and Tsujii 2005] from the University of Tokyo is such a parser 1 . In this article, we use Enju-2.4.1. We discuss other parsers in Section 5.

There is another kind of head:  X  X emantic heads. X  Hong et al. [2009] used Stanford parser [de Marneffe et al. 2006], which outputs semantic head-based dependencies; Xu et al. [2009] also used the same semantic dependencies, but they did not mention what parser they used. Enju also outputs semantic heads. Rough definitions of these heads are given as follows.
 Syntactic head: A part of a phrase that determines the syntactic role of the phrase. Semantic head: A part of a phrase that determines the main meaning of the phrase. Exact definitions may differ by researchers and parsers, but we do not discuss their definitions here. We simply accept the output of these parsers and evaluate them in terms of English-to-Japanese translation quality.

Our method simply checks whether a tree node is a syntactic head. We do not have to care about what we move or how we move it. The use of syntactic heads and the number of dependents are essential for the simplicity of Head Finalization (See Discussion). Enju outputs binary trees, which leads to simple implementation. If we use an ordinary dependency parser that does not limit the number of dependents, we need a  X  X everse X  operation just like Xu et al. [2009]. The binary tree constraint is known in SMT as  X  X nversion transduction grammar (ITG) constraints X  [Wu 1997]. Figure 1 shows Enju X  X  XML output for a simple sentence:  X  John hit a ball . X  The tag (token). Each node has a unique id . Its syntactic head is given by the node X  X   X  head  X  attribute. For instance, node c0  X  X  syntactic head is node c3 ,and c3 is a VP , or verb phrase. Thus, Enju treats not only words but also non-terminal nodes as heads. Its semantic head is given by  X  sem head  X  attribute. In this example, syntactic heads and semantic heads agree, but they sometimes disagree. We will discuss this disagreement in Section 4.2.

Enju outputs at most two child nodes for each node. One child is a syntactic head, and the other is a dependent. c3  X  X  syntactic head is c4 ,whichis VX , or a fragment of a verb phrase. c4  X  X  syntactic head is t1 or  X  hit , X  which is VBD or a past-tense verb. The left picture of Figure 2 shows the parse tree graphically. Here, thick lines indicate edges from syntactic heads.

Our Head Finalization rule simply swaps two children of each node when the head child appears before the dependent child. In the left picture of Figure 2, c3 has two children c4 and c5 . Here, c3  X  X  head c4 appears before c5 ,so c4 and c5 are swapped. The right picture shows the swapped result. Then we obtain  X  John a ball hit , X  which has the same word order as its Japanese translation  X  X on wa bohru wo utta X  except for the English determiner  X  a  X  and Japanese case markers  X  X a X  and  X  X o X . Our preliminary experiments showed that Head Final English (HFE) looks good as a first approxima-tion of Japanese word order. In addition, we can make it better by introducing some heuristic rules.
 We have to add Japanese case markers  X  X a X  (topic marker) or  X  X a X  (nominative case marker) for  X  John  X  and  X  X o X  (objective case marker) for  X  ball  X  to obtain an acceptable Japanese sentence. SMT is not good at generating appropriate case markers from English. Case marker generation has been tackled by a few research groups [Hong et al. 2009; Toutanova and Suzuki 2007].

Here, we use Enju X  X  output to generate  X  X eeds for case markers. X  As Figure 1 shows, the verb  X  hit  X  X as arg1 = "c1" and arg2 = "c5" . This indicates that c1 ("John") is the first argument (subject) of  X  hit  X  X nd c5 ("a ball") is the second argument (object) of  X  hit  X . We add seed words  X  va1  X  X fter arg1 and  X  va2  X  after arg2 . Then, we obtain  X  John va1 a ball va2 hit . X  We introduced the idea of case marker seed words in-dependently but found that this is very similar to Hong et al. [2009] for Korean. Lee et al. [2010] also used a similar method.

Figure 3 shows Enju X  X  parse tree for a more complicated sentence  X  John went to the police because Mary lost his wallet.  X  For brevity, we hide terminal nodes, and we removed non-terminal nodes X  prefix "c" .

We improved the introduction of case marker seed words in the following way.  X  For Figure 3, Japanese speakers will use  X  X on wa meari ga X  instead of  X  X on ga meari ga X  because the latter sounds like a correction of the subject. To help this, we simply rewrite  X  va1  X  X fthemainclauseas X  va0  X  in each sentence to distinguish between these two case markers. The distinction of case markers such as  X  X a X  and  X  X a X  is a delicate problem, and we rely on SMT X  X  statistical analysis for the determination of case markers.  X  By examining the output, we found that some words have two seed words. For ex-ample,  X  toy  X  X f X  He bought a toy that was popular in Japan  X  has two seed words  X  va1  X  X nd X  va2  X  because the toy is a subject in the relative clause and an object in the main clause. By considering Japanese translation, we decided to discard seed words from the relative clause.  X  X or passive sentences such as  X  John was hit by Bob , X  Enju treats  X  Bob  X  as the first argument and  X  John  X  as the second argument of  X  hit  X  because semantically Bob hit John. However, passive English sentences are often translated into passive
Japanese sentences and this argument swap for passive verbs may confuse the translation system in such cases. Therefore, we undo Enju X  X  argument swap to achieve straightforward Japanese translations. That is, we treat the subject of the passive English sentence as the first argument of the verb.

Sometimes, passive sentences are translated into active sentences, but we have to consider the focus of the story in the target language (Japanese). If we translate a passive sentence into an active sentence, its subject or the focus of the story changes.
The focus is usually used as a default subject in Japanese. Therefore, we should not change subjects without considering the context.

On the other hand, English determiners ( X  a , X   X  an , X  and  X  the  X ) are not translated into any Japanese words in most cases. Sometimes, however, they are explicitly translated ( X  the  X   X   X  kono  X ) or implicitly affect the translation ( X  a few  X  X s. X  few  X ), but we simply remove these determiners here.

Conventional rule-based machine translation (RBMT) systems swap X and Y of  X  X  because Y X  and move verbs to the end of each clause. Then we obtain  X  Mary his wallet lost because John the police to went.  X  Its word-to-word translation is a natural Japanese sentence:  X  X eari (ga) kare no saifu (wo) nakushita node jon (wa) keisatsu ni itta. X 
Our Head Finalization with case marker seed words yields a slightly differ-ent word order  X  John va0 Mary va1 his wallet va2 lost because the police to went.  X  Its word-to-word translation is  X  X on wa meari ga kare no saifu wo nakushita node keisatsu ni itta. X  This is also an acceptable Japanese sentence. This difference comes from the syntactic role of  X  because . X  In our method, Enju states that  X  because  X  X s a dependent of  X  went , X  whereas RBMT systems treat  X  because  X  as a clause conjunction. Although Head Finalization works very well for ordinary sentences, some expressions are excessively reordered. From preliminary experiments, we found that  X  A and B  X  was reordered as  X  B and A . X  Logically, they are equivalent, but sometimes their order matters. Therefore, we decided to exclude coordination nodes ( cat = "COOD" or xcat = "COOD" ) from this reordering. We call this the coordination exception rule and imple-ment it by treating the latter part B as the head of the coordination node.
Technical documents have more elements that cause excessive reordering problems. (1) Colons and semicolons. Colons and semicolons are used to concatenate (2) Patent documents often have expressions such as  X  motor 1  X  which means  X  X irst mo-(3) Parentheses: For example,  X  (1)  X  is reordered as  X  )1( . X  We can expect distance-
Therefore, we introduce a program  X  repos  X  that rewrites stepp  X  X  output as follows.  X  Rewrite POS tag CD to NN . This keeps  X  motor 1  X .  X  Rechunk short parentheses such as  X  (1)  X  and assign NN to it. This keeps  X  (1)  X .  X  Assign -LRB-to "[" and -RRB-to "]" . To show how closely our Head Finalization makes English follow Japanese word order, we measured Kendall X  X   X  [Kendall 1975], a rank correlation coefficient. We also mea-sured BLEU [Papineni et al. 2002] and other automatic evaluation scores to show that Head Finalization can actually improve the translation quality.

We used NTCIR7 PATMT X  X  Patent corpus [Fujii et al. 2008]. Its training corpus has 1.8 million sentence pairs. We used MeCab 2 to segment Japanese sentences into words. First, we examined rank correlation between HFE sentences produced by the Head Finalization rule and Japanese reference sentences. Since we do not have handcrafted word alignment data for an English-to-Japanese bilingual corpus, we used GIZA++ [Och and Ney 2003] to automatically align words.

On the basis of this automatic word alignment, we measured Kendall X  X   X  for the word order between HFE sentences and Japanese sentences. Kendall X  X   X  is a kind of rank correlation measure defined as follows. Suppose a list of integers such as L = [2, 1, 3, 4]. The number of all integer pairs in this list is 4 C 2 = 6. The number of In this case, we obtain  X  =5 / 6  X  2  X  1=0 . 667. For each sentence in the training data, we calculate  X  on the basis of a GIZA++ X  X  alignment file  X  en-ja.A3.final  X , which looks like this: John hit a ball .
 NULL ( { 3 } ) jon ( { 1 } ) wa ( {} ) bohru ( { 4 } ) wo ( {} ) utta ( { 2 } ) . ( { 5 } )
Numbers in ( {} ) indicate corresponding English words.  X  NULL  X  indicates English words without Japanese counterparts. From this alignment, we obtain an integer list [1, 4, 2, 5]. Then,  X  is 5 / 4 C 2  X  2  X  1=0 . 667. For HFE in Figure 2, we will achieve the following alignment.
 John va0 ball va2 hit .

NULL ( {} ) jon ( { 1 } ) wa ( { 2 } ) bohru ( { 3 } ) wo ( { 4 } ) utta ( { 5 } ) . ( { 6 } ) Then, we obtain [1, 2, 3, 4, 5, 6] and  X  =1 . 0. Sometimes, one Japanese word corre-sponds to an English phrase: John went to Costa Rica .
 We obtain [1, 4, 5, 3, 2, 6] from this alignment. In the following case, two occurrences of  X  of  X  correspond to one Japanese word  X  no . X  rate of change of speed NULL ( {} ) sokudo ( { 5 } ) henka ( { 3 } ) no ( { 24 } ) wariai ( { 1 } )
We excluded the discontiguously aligned words (2 4) from the calculation of  X  .We use only [5, 3, 1] and obtain  X  =  X  1 . 0. Because of this exclusion, the best value  X  =1 . 0 does not mean that we obtained the perfect word ordering, but low  X  values imply failures. Thus, even this rough calculation of  X  sheds light on the weak points of Head Finalization. Note that the rank correlation does not directly measure the quality of translation. For translation, we use Moses [Koehn 2010] with distance-based reordering. Usually, the quality of translation is measured by BLEU [Papineni et al. 2002]. However, Echizen-ya et al. [2009] showed that ROUGE-L [Lin and Och 2004] and their IM-PACT method are highly correlated to human evaluation in evaluating J-E patent translation. We also used other evaluation measures: word error rate (WER) [Su et al. 1992], position-independent error rate (PER), and translation error (or edit) rate (TER) [Snover et al. 2006].

We used the development set (915 sentences) in the NTCIR7 PATMT data as well as the formal run test set (1,381 sentences). PATMT participants used different meth-ods such as phrase-based SMT [Kumai et al. 2008], hierarchical phrase -based SMT [Watanabe et al. 2008], RBMT [Izuha et al. 2008], and example-based machine transla-tion (EBMT) [Nakazawa and Kurohashi 2008]. However, the organizers X  Moses-based baseline system denoted as  X  X oses* X  obtained the best BLEU score. First, we show  X  values to evaluate word order, and then we show BLEU and other automatic evaluation scores. The left subfigure of Figure 4 shows the distribution of  X  for the 1.8 million train-ing sentences. The average  X  (  X  ) over all training sentences was  X  =0 . 434 and the percentage of sentences with  X   X  0 . 8 was 10.1% when we used the original English sentences.
 The right subfigure shows how Head Finalization improved the distribution. Head Finalization with  X  repos  X  improved the former to 0.746 and the latter to 53.7%. The percentage of sentences with  X  =1 . 0 was 19.2%. Without repos ,  X  was slightly de-graded to 0.729. The ratio of high  X  (  X  0 . 8) sentences was 51.4% and that of sentences with  X  =1 . 0 was 7.0%. Thin bars in the right subfigure show the distribution of  X  of HFE without repos . repos increases the ratio of  X  =1 . 0 and reduces the ratio of smaller  X  .

In a preliminary experiment, we examined low  X  sentences of our method and found the following reasons for low  X  values.  X  The sentence pair is not an exact one-to-one translation. A Japanese reference sen-tence for  X  I bought the cake.  X  can be something like  X  The cake I bought.  X  X r  X  The person who bought the cake is me.  X   X  Mistakes in Enju X  X  tagging or parsing. Just like other parsers, Enju tends to make mistakes when a sentence has a comma or  X  and . X  When we used Enju-2.3.1, we often encountered severe mistakes caused by POS tag errors such as VBN/VBD (e.g., killed) and VBZ/NNS (e.g., reports). Here, we used Enju-2.4.1 X  X  -A option that takes such ambiguous POS tag cases into account.  X  Mistakes/Ambiguity of GIZA++ automatic word alignment. As we described above, discontiguously aligned words are removed from calculation of  X  . Thus, small re-ordering mistakes in other words are emphasized. As we described above, Stanford parser outputs  X  X emantic heads X  instead of  X  X yntactic heads. X  The examples in Xu et al. X  X  article [2009] imply that semantic heads are not as good as Enju X  X  syntactic heads at achieving Japanese-like word order.

Their Figure 3 shows dependency of  X  John can hit the ball . X  The root of this sen-tenceisnot X  can  X  X ut X  hit . X  Therefore, Head Finalization based on semantic heads does not move  X  can  X  to the end of the sentence, and they need an  X  X ux X  rule to move  X  can  X  X fter X  hit . X 
Their Figure 4 shows dependency of  X  Living is exciting because we don X  X  know what the future has . X  Here,  X  because  X  is just a modifier of  X  know  X  and Head Final-ization does not move  X  because  X  X otheendofthis X  because  X  clause. Therefore, they proposed a new rule in their footnote.

To justify our belief that semantic heads are not as good as syntactic heads at achiev-ing Japanese-like word order, we implemented two systems. The first system is our im-plementation of Xu et al. X  X  method [2009] with Stanford parser 1.6.2. We encountered and solved the following problems. (We describe their system in Section 5.)  X  This version of Stanford parser sometimes output cyclic dependency relationship, and we could not reach some words from the root word of the sentence.  X  Xu et al. [2009] disallowed any movement across punctuation and conjunctions. In the case of  X  X ohn saw Bob and Mary, X  we want to move  X  X ob and Mary X  together. However, it is not clear how they treated such cases.
 The second system is a Semantic Head Finalization program, which simply uses Enju X  X  semantic heads (indicated by  X  sem head  X  attribute) instead of syntactic heads. Table I compares these methods with (syntactic) HFE. According to these results, syntactic heads achieve better results than semantic heads. We also show HFE-va or HFE without case marker seed words. This also degrades the performance. In general, it is believed that translation between English and Japanese requires a large distortion limit (dl), which restricts how far a phrase can move. Some SMT researchers working on E-J or J-E translation use dl =  X 1 (unlimited) as a default value, and this takes a long translation time.

For PATMT J-E translation, Katz-Brown and Collins [2008] showed that dl = un-limited was the best in Japanese-to-English translation, but dl = unlimited took 37.2 seconds per sentence while dl = 6 took 5.0 seconds. That is, dl = unlimited was more than seven times slower than dl = 6.
 For PATMT E-J translation, Moses* performed the best in terms of single reference BLEU, and this system also used dl = unlimited. However, Kumai et al. [2008] reported that they achieved the best result  X  X hen the distortion limit was 20 instead of  X 1. X  We tried to reproduce Moses* and ran MERT with msd-bidirectional-fe and dl =  X 1. This training took 35 hours. On the other hand, MERT of the proposed method with a distance-based model dl = 5 took only seven hours.

Table II compares the output of the proposed method and that of Moses* distributed by NTCIR7 PATMT organizers. Following the PATMT overview article [Fujii et al. 2008], we segmented the output Japanese sentences with ChaSen 3 and measured the quality with Bleu Kit 4 . However, how Japanese sentences are normalized is not ex-plicitly described in the overview paper. Here, we converted hankaku (half-width) characters into zenkaku (full-width) characters.

We should pay attention to the difference of segmenters in this evaluation. Sophis-ticated MT systems tend to use in-house segmenters tuned for translation. Different segmenters give different segmentation results. One segmenter may treat  X  X ihonden-shindenwakabushikigaisha X  (Nippon Telegraph and Telephone Corporation) as one word. Another segmenter may segment it into five words. For fair evaluation, we should use the same segmenter for both reference sentences and MT outputs.
Direct output of Moses follows the segmenter used in the training phase and we should not compare it directly with reference sentences segmented with ChaSen. We simply removed spaces between Japanese words in the Moses output to get final Japanese sentences, and then applied ChaSen to the sentences.
 Table II shows that even dl = 0 (no reordering in Moses) gained better scores than Moses*. We compared proposed method (dl = 5) with Moses* by bootstrap resampling [Koehn 2004], and our proposed method won 1,000 times among 1,000 trials.
Without case marker seed words, the scores drop. However, this does not mean loss of case markers. Even without seed words, case markers are often inserted. By com-paring the output translations, we found that boundaries of adjacent verb arguments becomes explicit by the seed words, and this fact leads to better translation.
Without repos , the scores drop again. Using semantic heads is not as good as using syntactic heads.

For these ablation experiments, we applied two-tailed binomial test to automatic evaluation measures, WER, ROUGE-L, and IMPACT, which give sentence-level scores. As for dl = 0, the introduction of case marker seed words led to statistically signifi-cant improvements of these measures with p &lt; 10  X  12 . The introduction of repos also yielded statistically significant improvements of these measures with p &lt; 10  X  6 . The use of semantic heads degraded WER and IMPACT with p &lt; 0.03 but the degradation of ROUGE-L was not statistically significant ( p = 0.075).

On the other hand, position-independent word error rate (PER), which completely disregards word order, does not change very much. These facts indicate that our method improves word order, which is the most important problem in E-J translation.
In order to examine the effect of the difference of segmenters in the training phase and the evaluation phase, we also report the scores when we used MeCab as the evalu-ation segmenter. Table III shows the result. According to this table, translation quality scores did not change very much.
 Our method is closely related to tree-to-string systems [Huang et al. 2006; Liu and Gildea 2008; Wu et al. 2010]. For comparison, we used Moses X  tree-to-string model with Enju. Table IV summarizes their scores as well as our Xu-like system. We have not succeeded in showing their effectiveness in this NTCIR-7 PATMT EJ task yet. Our method used an HPSG parser, which obtains rich information but is difficult to build. It is much easier to build word dependency parsers and Penn Treebank-style parsers. Here, we describe what happened when we used these parsers. When we used these parsers, we need additional rules like Xu et al. [2009]. When we use an ordinary (syntactic head-based) word-dependency parser, we assume that the modified words are heads, and the Head Finalization rule is rephrased as  X  X ove modified words after modifiers. X 
As we described above, Xu et al. [2009] used semantic heads. Even when we use a syntactic head-based dependency parser, we encountered their  X  X xcessive movement X  problem. A straightforward application of their rules changes (0) John hit the ball but Sam threw the ball. into (1) John the ball but Sam the ball threw hit.
 Here, the two clauses are mixed up. To prevent this, they disallowed any movement across punctuation and conjunctions. We mentioned this in Section 4.2. Then they achieved a better result: (2) John the ball hit but Sam the ball threw.

When we used Enju, these clauses were not mixed up. Enju-based Head Finaliza-tion achieved the same word order as Example (2): (3) John _va1 ball _va2 hit but Sam _va0 ball _va2 throw.
 Figure 5 shows Enju X  X  parse tree. Because of our coordination exception rule,  X  Sam  X  becomes the main subject. When Head Finalization swaps the children of a mother node, the children do not move beyond the range of the mother node. Therefore, Head Finalization based on Enju does not mix up the first clause  X  John hit the ball  X  covered by Node 1 with the second clause  X  Sam threw the ball  X  covered by Node 11. Thus, non-terminal nodes in Enju X  X  output are useful to protect clauses.
On the other hand, according to our syntactic head-based word dependency parser, (1), and the two clauses become mixed up. Consequently, we need a rule like Xu et al. X  X  [2009] to avoid this excessive movement problem. We also tried Charniak-Johnson X  X  parser [Charniak and Johnson 2005]. PyInputTree 5 gives heads. Enju outputs at most two children per mother node, but Penn Treebank-style parsers do not have such a limitation on the number of children, which causes a problem.

When we use Enju,  X  This toy is popular in Japan  X  is reordered as  X  This toy va0 Japan in popular is . X  Its monotonic translation is natural:  X  X ono omocha wa nihon de ninki ga aru. X 
On the other hand, Charniak-Johnson X  X  parser outputs the following S-expression for this sentence (we added asterisks ( * ) to indicate heads). (S (NP (DT This) (NN* toy)) This VP has three children: AUX , ADJP ,and PP . Simply moving heads to the end breaks the adjacency of  X  is  X  X nd X  popular  X :  X  this toy va0 popular Japan in is . X  If we translate this monotonically,  X  Japan  X  becomes  X  popular  X  instead of  X  toy  X . We can solve this problem by using Xu et al. X  X  children reversal. This reconnects is and popular and we obtain  X  This toy va0 Japan in popular is  X  from the following reversed S-expression. (S (NP (DT This) (NN* toy)) Head Finalization achieves a good first approximation of Japanese word order in spite of its simplicity. Since it is just a rule-based method and does not use any machine learning methods, its reordering performance does not improve even when the amount of training data increases. However, Moses X  distance-based reordering helps its re-ordering performance.

Statistical machine translation depends on the assumption that we can obtain a large parallel corpus. This assumption holds for major European languages, but it is often difficult to satisfy this assumption for Asian languages. We hope Head Finaliza-tion helps building machine translation systems for such Asian languages that follow head-final word order.

One weak point of current Head Finalization is negatively translated noun modi-fiers such as  X  no  X  X r X  few  X  in long sentences. When an HFE sentence has such a word, it is difficult to translate the sentence monotonically. For instance,  X  I have no time  X  becomes  X  I va0 no time va2 have . X  Its monotonic translation is  X   X  watashi wa nai jikan wo motteiru, X  but this sentence is not acceptable. An acceptable literal trans-lation is  X (watashi niwa) jikan ga nai. X  Here, the English word  X  no  X  corresponds to  X  X ai X  at the end of the Japanese translation. When the sentence is long, we will need a large distortion limit. One solution is to rephrase the English sentence with  X  not  X  X hat modifies a verb or an auxiliary verb.

On the other hand, the advantage of Head Finalization other than simplicity is its granulaity. Some alternative preprocessing methods simply divide long sentences into short clauses, and translate each clause. These methods are also effective because short sentences do not require a large distortion limit. Kumai et al. [2008] divided sentences at commas. Sudoh et al. [2010] used Enju to parse English sentences, and removed subordinate clauses marked with  X  X  X  (footnote 1 in their p. 418) and substi-tuted them for place-holders. Then, each clause is translated independently, and recon-structed by back-substituting the place-holders for corresponding translated clauses.
However, these division methods do not work for the following article title [Nakachi et al. 2010] at all because it has neither a comma nor  X  X . X  A phase II study of induction chemotherapy with gemcitabine plus S-1 followed by chemoradiotherapy for locally advanced pancreatic cancer
Head Finalization works even in this case, and obtains the following HFE. It can be monotonically translated into Japanese. kyokusho shinko suigan ni-taisuru houshasen-kagaku-ryouhou wo heiyou-shita gemushitabin purasu S-1 deno dounyuu kagakuryouhou no dainisou shiken
Wu et al. [2010] also used Enju for English-to-Japanese translation. Their method extracts about 100 million rules on the basis of Galley et al. [2004]. Perhaps, the num-ber of rules is unnecessarily large because of parser errors and nonstandard transla-tions in the training data.

Genzel [2010] proposed a machine learning-based approach that kept the number of rules small and obtained only 40-50 rules. For Japanese, Genzel showed that his method obtained a slightly better BLEU score (0.2912) than Xu et al. X  X  [2009] manual rule set (0.2878).
 Our article clarified that even one reordering rule gives a good approximation of the English-to-Japanese reordering and even this simple rule really improves the transla-tion quality in terms of various automatic evaluation scores. To solve the word-order problem in English-to-Japanese translation, we introduced a new reordering rule called Head Finalization. We can implement word reordering in an abstract level, and the reordering rule is very simple compared to those in related work. Automatic evaluation scores showed that our method significantly improved the translation quality.

However, Head Finalization requires a sophisticated HPSG parser such as Enju, and Enju X  X  source code is not publicly available yet. We discussed the problems of other parsers and clarified that the use of syntactic heads and the binary tree constraint is essential.

Our future work is to build our own efficient and accurate dependency parser that outputs what we need in our method and to apply this method to other head-final languages such as Korean and Turkish.

