 Incremental learning ability is vital to many real world machine learning problems [8]. The common characteristics of these problems are that either the training set is too large to learn in a batched fashion, or the training cases are available as a time sequence. We need machine learning methods updating their hypothesis only with the latest cases, i.e. in incremental fashion. Much work has b een done to provide incremental learning ability for the classification problems. 
While most powerful classification methods suffer from the problem that their give interpretable, but usually less effective results. Among the latter ones are decision tree, rule induction methods, several graph based methods and ro ugh set based three incremental decision tree induction algorithms: ID5 [5], ID5R [5], and ITI [6]; rule induction methods are also efficient solutions of classification tasks and have been extended to solve incremental learning problems [9]; Galois (Concept) lattices and several extensions are data structures based on Hasse graph [1, 3] and are widely used in incremental classification and association rule induction; Rough set based methods produce a decision table of a sequence of rules for classification [9]. 
Recently, Enembreck proposed a data structure named Graph of Concepts (GC) [2] for incremental learning. GC is composed of several attribute layers each representing an attribute, and a classification layer repres enting the categories. The attribute layer is comprised of several attribute nodes mapping to the values of this attribute and the class layer is comprised of some classification nodes each mapping to a category. During the attribute layer and the classification node that the case belongs to. Then they used an entropy based method named ELA to utilize the information stored in GC for case(s). 
However, there are the following defects with these incremental methods: a) Bad memory utilization: many algorithms need to record historical cases for b) Inefficiency of updating hypothesis (decision tree methods, rule induction, Galois 
To address these problems, we design a novel incremental learning algorithm which algorithms with our special concerns on th e memory and adaptation computation costs, and the classification results are easy to understand. The rest of this paper is structured as follows: in Section 2 we give the definition of while the classification algorithm is elaborated in Section 4; in Section 5, we give a case study to evaluate the performance of our method; finally, the conclusions and the future work are given in Section 6. For each category, we construct an isomorphic structure named AttributeNet. With each AttributeNet is composed of several attribute layers comprising of attribute However, there are two significant differences between GC and AttributeNet: first, AttributeNet does not have classification layer being that each AttributeNet simply refers to only one category; second, instead of attaching the case sequence number to each node, we only save the statistical information in AttributeNet. Each node keeps a counter (node degree) to record how many cases belong to this node; for any of two nodes, another counter (link degree) is kept recording how many cases belong to both nodes. 
For explanation, we consider a simplifi ed classification problem. There are three categories, and each case has 4 attributes that have the value of either 0 or 1. For each category, an AttributeNet is constructed, i. e. there are three isomorphic AttributeNets. One of them is illustrated in Fig. 1. specific value ( node value ) of an attribute and keeps a counter ( node degree ) counting the number of cases that has this value for the specific attribute. In Fig. 1, the i th attribute of the case has the node value of ij A . composed of several nodes representing the corresponding values of this attribute. In Fig. 1, ) 4 1 (  X   X  i A i are layers, each layer is composed of two nodes: Definition 3 (Node Link) . There are links between any two nodes of different layers. If a case belongs to both nodeand ij A node gf A , the link degree between these two nodes between any two nodes of the same layer is always 0. Definition 4 (AttributeNet and AttributeNets) . An AttributeNet is composed of several represents only one category in the classification problem. With AttributeNets, we refer to the combination of these nets. complexity which makes our method suitable for online learning. 
AttributeNets memorizes statistic information of attribute values and relationships between any two of values of different attributes, with consideration of cases of only the net X  X  own category. 
When a training case of category i comes, AttributeNet i is activated while other nets attribute of the case, i.e. each layer of AttributeNet i , we increase the degree of node if this attribute has the value identical to the node value. For any two nodes of different activated by the case. 
Take the classification problem mentioned in Section 2 for example, in Table 2, there are 4 training cases of category 1, af ter training, the node degree and link degree other than 1 are not changed by these cases. 
The AttributeNets is learnt case by case and the learning result is independent of the order in which cases are learnt. When new case comes, we only need to increase the node degree of the nodes and the link degree of node links it activates. The time and AttriuteNets. section, a classification algorithm is given based on AttributeNets. 
The time complexity and space complexity of algorithm 2 are both ), ( 2 n m O  X  where m is the number of categories, n is the number of nodes in each AttriuteNet. between two nodes of AttributeNets are inve stigated, through comparing these values between layers of AttributeNets and attributes of cases. The performance of AttributeNets is a significant improvement of its counterparts. In this section we give the comparison results of AttributeNets and the related algorithms on the MONK-3 [10] classification benchmark set for the performance verification. 5.1 Performance and Robustness Evaluations of AttributeNets MONK-3 problem is a widely used benchmark data set for classification algorithms evaluation. There are two categories denoted by 0 and 1, and each case has six belongs to category 0. 
For each category, an AttributeNet is cons tructed. Therefore, there are two nets representing category 0 and category 1, respectively. For training, 150 training cases are generated randomly, 5 percent of which are noisy cases, i.e. there are 8 mislabeled different platforms: AttributeNets, ELA, and ID5R [5]. The comparison results are time cost of learning and classification. cases of the noisy training data and the scar city of training cases. The basic settings are the same as the above. First we increase the number of training cases from 25 to 175 in order to investigate the influence of training set size. Then noisy data of the percentage varying from 5 to 50 are mixed in the training set. The classification results are shown in Fig. 2(a) and (b), respectively. 
We conclude that AttributeNets is robust with noisy data (as the percentage of noisy data runs up to 30%, the precision is still as high as 87%) and it works quite well with only a small size of training set available. 5.2 Performance Discussion As Utgoff in [6] pointed out, there were 12 design principles that should be considered when designing an incremental learning cl assification system. We summarize them as the following: 1) The update cost of the method must be small 2) Input: the method should accept cases as in put described by any mix of symbolic 4) Fault tolerance: the method should be strong enough to handle noisy data and 5) Capable of handling screwed data: the method should take the possibility that the 6) Capable of handling some problems with strong relationships among several 
Our method satisfies principle 1, 3, 4, 5; pa rtly satisfies principle 2 because we have not taken continuous attribute into account. The limitation of our method is that it only relationships between more than two attributes, like MONK-2, our method does not generate results as good as Neural Networks. Incremental learning algorithms provid e new opportunities for industry whilst put forward new challenges to researchers: (1) ho w to memorize the knowledge been learnt for further updating without recording every case learnt before; (2) how to avoid (or these problems, we have designed a new data structure (AttributeNets) and algorithms for incremental learning and classification. The advantages of our algorithm are in four folds: 1. It is in itself a multi-category classifier because of multi-nets structure 3. The classification results are easy to understand 4. It is robust with the noisy data and the scarcity of training cases 
Our future work includes: first, extensions could be made to enrich AttributeNets problems, AttributeNets could be naturally extended to induct association rules, which are also important data mining problems. 
