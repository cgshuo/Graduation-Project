 Data stream classification is a major challenge to the data mining community. There are two key problems related to stream data classification. First, it is impractical to store and use all the historical data for training, since it would require infinite storage and running time. Second, there may be concept-drift in the data. The solutions to these two problems are related. If there is a concept-drift in the data, we need to refine our hypothesis to accommodate the new concept. Thus, most of the old data mus t be discarded from the training set. Therefore, one of the main issues in mini ng concept-drifting data streams is to choose the appropriate training instances to learn the evolving concept.
One approach is to select and store the training data that are most consis-tent with the current concept [1]. Some other approaches update the existing classification model when new data app ear, such as the Very Fast Decision Tree (VFDT) [2] approach. Another approach is to use an ensemble of classifiers and update the ensemble every time new data appear [3,4]. As shown in [3,4], the ensemble classifier is often more robust at handling unexpected changes and concept drifts. We propose a multi-partition, multi-chunk ensemble classification algorithm, which is a generalization over the existing ensemble methods and can improve the classificatio n accuracy significantly.

We assume that the data strea m is divided into equal sized chunks . The chunk size is chosen so that all data in a chunk may fit into the main memory. Each chunk, when labeled, is used to train classifiers. In our approach, there are three parameters that control the multi-partition, multi-chunk ensemble: v , r ,and K . Parameter v determines the number of partitions ( v =1 means single-partition ensemble), parameter r determines the number of chunks ( r = 1 means single-chunk ensemble), and parameter K controls the ensemble size. Our ensemble consists of K  X  v classifiers. This ensemble is updated whenever a new data chunk is labeled. We take the most recent labeled r consecutive data chunks and train v classifiers using v -fold partitioning of these chunks. We then update the ensemble by choosing the best (based on accuracy) K  X  v classifiers among the newly trained v classifiers and the existing K  X  v classifiers. Thus, the total number of classifiers in the ensemble is always kept constant.

It should be noted that when a new data point appears in the stream, it may not be labeled immediately. We defer the ensemble updating process until the data points in the latest data chunk have been labeled, but we keep classifying new unlabeled data using the current ense mble. For example, consider the online credit-card fraud detectio n problem. When a new credit-card transaction takes place, its class ( { fraud,authentic } ) is predicted using the current ensemble. Sup-pose a fraudulent transaction has been miss-classified as  X  X uthentic X . When the customer receives the bank statement, he will identify this error and report to the authority. In this way, the actual labels of the data points will be obtained, and the ensemble will be updated accordingly.

We have several contributions. First, we propose a generalized multi-partition, multi-chunk ensemble technique that significantly reduces the expected classifi-cation error over the existing single-partition, single-chunk ensemble methods. Second, we have theoretically justified th e effectiveness of our approach. Finally, we apply our technique on synthetically generated data as well as on real botnet traffic, and achieve better detection accura cies than other stream data classifi-cation techniques. We believe that the proposed ensemble technique provides a powerful tool for data stream classification.

The rest of the paper is organized as follo ws: section 2 discusses related works, section 3 discusses the classification al gorithm and proves its effectiveness, sec-tion 4 discusses data collection, experimental setup, evaluation techniques, and results, and section 5 concludes with directions to future work. There have been many works in stream dat a classification. There are two main approaches -single model classification, and ensemble classification. Single model classification techniques in crementally update their model with new data to cope with the evolution of stream [2,5,6,7]. These techniques usually require com-plex operations to modify the internal s tructure of the model. Besides, in these algorithms, only the most r ecent data is used to update the model. Thus, con-tributions of historical data are forgotten at a constant rate even if some of the historical data are consistent with the current concept. So, the refined model may not appropriately reflect the current concept, and its prediction accuracy may not meet the expectation.

In a non-streaming environment, ensemb le classifiers like Boosting [8] are pop-ular alternatives to single model classifiers. But these are not directly applicable to stream mining. However, several ense mble techniques for data stream mining have been proposed [3,4,9,10]. These ensemble approaches have the advantage that they can be more efficiently built than updating a single model and they observe higher accuracy than their single model counterparts [11]. Among these approaches, our ensemble approach is related to that of Wang et al [3]. Wang et al. [3] keep an ensemble of the K best classifiers. Each time a new data chunk appears, a classifier is trained from that chunk. If this classifier shows better accuracy than any of the K classifiers in the ensemble, then the new classifier replaces the old one. When classifying a n instance, weighted voting among the classifiers in the ensemble is taken, whe re the weight of a classifier is inversely proportional to its error.

There are several differences between our approach and the approach of Wang et al. First, we apply multi-partitioning of the training data to build multiple (i.e., v ) classifiers from that training data. Second, we train each classifier from r consecutive data chunks, rather than from a single chunk. Third, when we update the ensemble, the v classifiers that are removed may come from different chunks; thus, although some classifiers from a chunk may have been removed, other classifiers from that chunk may still remain in the ensemble. Whereas, in the approach of Wang et al., removal of a classifier means total removal of the knowledge obtained from one whole chunk. Finally, we use simple voting, rather than weighted voting. Thus, our multi-partition, multi-chunk ensemble approach is a generalized form of the approach of Wang et al., and users have more freedom to optimize performance by choosing the appropriate values of these two parameters (i.e., r and v ). We keep an ensemble A = { A 1 ,A 2 , ..., A K  X  v } of the most recent best K  X  v classifiers. Each time a new data chunk D n arrives, we test the data chunk with the ensemble A . We update the ensemble when D n is labeled. This ensemble training process is illustrated in figure 1 and explained in section 3.1. We will refer to our ensemble approach as the  X  X ultiple-Partition Multiple-Chunk (MPC) X  ensemble approach. The ensemble classification process uses simple majority voting. Section 3.2 explains how MPC ense mble reduces classification error over other approaches. 3.1 Ensemble Updating Algorithm Description of the algorithm (algorithm 1): Let D n be the most recent data chunk that has been labeled. In lines 1-3 of the algorithm, we compute the error of each data chunks including D n . In line 5, we randomly divide D into v equal parts = { d 1 , ..., d v with the dataset D -{ d j } . We compute the expected error of each classifier A n j on its corresponding test data d j . Finally, on line 10, we select the best K  X  v classifiers from the K  X  v + v classifiers A n  X  A . Note that any subset of the n th batch of v classifiers may take place in the new ensemble.
 Algorithm 1. Updating the classifier ensemble 3.2 Error Reduction Using Multi-Partition and Multi-Chunk As explained in algorithm 1, we build an ensemble of K  X  v classifiers A .Atest instance x is classified using a majority voting of the classifiers in the ensemble. We use simple majority voting, as opposed to weighted majority voting used in [3], since simple majority voting is theoretically proven to be the optimal choice [10]. Besides, in our experiments, we also obtain better results with simple voting. In the next few paragraphs, we show that MPC can significantly reduce the expected error in classifying concept-drifting data streams compared to other approaches that use only one data chunk for training a single classifier (i.e., r = 1, v = 1), which will be referred to henceforth as  X  X ingle-Partition Single-Chunk (SPC) X  ensemble approach.
 Given an instance x , the posterior probability distribution of class a is p ( a | x ). For a two-class classification problem, a =+ or  X  . According to Tumer and Ghosh to p ( a | x ). This is the error in addition to Bayes error and usually referred to as the  X  X dded error X . This error occurs either due to the bias of the learning algo-rithm, and/or the variance of the learned model. According to [11], the expected added error can be obtained from the following formula: Error =  X   X  p (+ | x )and p (  X  X  x ), which is independent of the learned classifier.
Let C = { C 1 , ..., C K } be an ensemble of K classifiers, where each classifier C i is trained from a single data chunk (i.e., C is an SPC ensemble). If we average the outputs of the classifiers in a K -classifier ensemble, then according to [11], the ensemble output would be: f a C = 1 K K i = i f a C is the output of the ensemble C , f a C  X  ( x ) is the average error of all classifiers, given by:  X  in the ensemble. Assuming the error variances are independent, the variance of  X  ( x )isgivenby: order to simplify the notation, we would denote  X  2  X  a
Let A be the the ensemble of K  X  v classifiers { A 1 , A 2 , ... , A K  X  v } ,where each A i is a classifier trained using r consecutive data chunks (i.e., the MPC approach). The following lemma proves that MPC reduces error over SPC by a factor of rv when the outputs of the classifiers in the ensemble are independent. Lemma 1. Let  X  2 C be the error variance of SPC. If there is no concept-drift, and the errors of the classifiers in the ensemble A are independent, then the error variance of MPC is 1 /rv times of that of SPC, i.e.,  X  2 A = 1 rv  X  2 C . Proof. Each classifier A i  X  A is trained on r consecutive data chunks. If there is no concept-drift, th en a classifier trained on r consecutive data chunks may reduce the error of the single classifiers trained on a single data chunk by a factor of r [3]. So, it follows that: where  X  2 A D chunk D j . Combining equations 1 and 2 and simplifying, we get: where  X   X  2 C common variance of  X   X  2 C However,sincewetrain v classifiers from each r consecutive data chunks, the independence assumption given above may not be valid since each pair of these v classifiers have overlapping training data. We need to consider correlation among the classifiers to compute the expected error reduction. The following lemma shows the error reduction considering error correlation.
 Lemma 2. Let  X  2 C be the error variance of SPC. If there is no concept-drift, then the error variance of MPC is ( v  X  1) /rv times of that of SPC, i.e.,  X  2 A = For example, if v =2, and r =2 then we can have an error reduction by a factor of 4. However, if there is concept-drift, then the assumption in equation 2 may not be valid. In order to analyze error in presence of concept-drift, we introduce a new term  X  X agnitude of drift X  or  X  d .
 Definition 1. Magnitude of drift or  X  d is the is the maximum error introduced to a classifier due to concept-drift. That is, every time a new data chunk appears, the error variance of a classifier is incremented  X  d times due to concept-drift. For example, let D j ,j  X  X  i, i +1 , ..., i + r  X  1 } beadatachunkinawindowof Let the actual error variance of C j in the presence of concept-drift is  X   X  2 C the error variance of C j in the absence of concept-drift is  X  2 C In other words,  X   X  2 C ence of concept-drift, when the last data chunk in the window, D i + r  X  1 appears. Our next lemma deals with error reduction in the presence of concept-drift. Lemma 3. Let  X   X  2 A be the error variance of MPC in the presence of concept-drift,  X  2 C be the error variance of SPC, and  X  d be the drifting magnitude defined Proof. Replacing  X  2 C 2, we get  X   X   X  v = Therefore, we would achieve a reduction of error provided that Where E R is the ratio of MPC error to SPC e rror in the presence of concept-drift. As we increase r and v , the relative error keep s decreasing upto a certain point. After that, it becomes flat or star ts increasing. Next, we analyze the effect of parameters r and v on error reduction, in the pr esence of concept-drift. 3.3 Upper Bounds of r and v For a given value of v , r can only be increased up to a certain value. After that, increasing r actually hurts the performance of our algorithm, because inequality 5 is violated. The upper bound of r depends on  X  d , the magnitude of drift. Although it may not be possible to know the actual value of  X  d from the data, we may determine the optimal value of r experimentally. In our experiments, we found that for smaller chunk-sizes, higher values of r work better, and vice versa. However, the best performance-cost trade-off is found for r =2 or 3. We have used r =2 in our experiments. Similarly, the upper bound of v can be found from the inequality 5 for a fixed value of r . It should be noted that if v is increased, running time also increases. From our experiments, we obtained the best performance-cost trade-off for v =5. 3.4 Time Complexity of MPC Let m bethesizeofthedatastreamand n be the total number of data chunks. Then our time complexity is O ( Kvm + nvf ( rm/n )), where f ( z ) is the time to build a classifier on a training data of size z .Since v is constant, the complexity chunk. It should be mentioned here that the time complexity of the approach by Wang et al. [3] is O ( n. ( Ks + f ( s )). Thus, the actual running time of MPC would be at most a constant factor ( rv times) higher than that of Wang et al [3]. But at the same time, we also achieve significant error reduction. We evaluate our proposed method on both synthetic data and botnet traffic gen-erated in a controlled environment, and compare with several baseline methods. 4.1 Data Sets and Experimental Setup Synthetic dataset: Synthetic data are generated with drifting concepts [3]. Concept-drifting data can b e generated with a moving h yperplane. The equation is negative, otherwise it is positive. Each example is a randomly generated d -randomly initialized with a real number in the range [0, 1]. The value of a 0 is adjusted so that roughly the same number of positive and negative examples are generated. This can be done by choosing a 0 = 1 2 d i =1 a i We also introduce noise randomly by switching the labels of p % of the examples, where p =5 is used in our experiments. There are several paramet ers that simulate concept-drift. We use the same parameters settings as in [3] to generate synthetic data. We generate a total of 250,000 records and generate four different datasets having chunk sizes 250, 500, 750, and 1000, respectively. The class distribution of these datasets is: 50% positive and 50% negative.
 Real (botnet) dataset: Botnet is a network of compromised hosts or bots , un-der the control of a human attacker known as the botmaster [13]. The botmaster can issue commands to the bots to perform malicious actions, such as launch-ing DDoS attacks, spamming, spying and so on. Thus, botnets have appeared as enormous threat to the internet community. Peer-to-Peer(P2P) is the new emerging technology of botnets. These botnets are distributed, and small. So, they are hard to detect and destroy. Examples of P2P bots are Nugache [15], Sinit [16], and Trojan.Peacomm [17].

Botnet traffic can be considered as a d atastreamhavingbothproperties: infinite length and concept-drift. So, we apply our stream classification technique to detect P2P botnet traffic. We generat e real P2P botnet traffic in a controlled environment, where we run a P2P bot named Nugache [15]. The details of the feature extraction process are disc ussed in [12]. There are 81 continuous attributes in total. The whole dataset consists of 30,000 records, representing one week X  X  worth of network traffic. We generate four different datasets having chunk sizes of 30 minutes, 60 minutes, 90 minutes, and 120 minutes, respectively. The class distribution of these datasets is: 25% positive (botnet traffic) and 75% negative (benign traffic).
 Baseline methods: For classification, we use the  X  X eka X  machine learning open source package, available at  X  X ttp: //www.cs.waikato.ac.nz/ml/weka/ X . We apply three different classifiers -J48 decision tree, Ripper, and Bayes Net. In order to compare with other techniques, we implement the followings: MPC : This is our multi-partition, multi-chunk (MPC) ensemble algorithm. BestK : This is a single-partition, single-chunk (SPC) ensemble approach, where an ensemble of the best K classifiers is used. Here K is the ensemble size. This ensemble is created by storing all the cla ssifiers seen so far, and selecting the best K of them based on expected error. An inst ance is tested using simple voting. Last : In this case, we only keep the last tra ined classifier, trained on a single data chunk. It can be considered a SPC approach with K =1.
 Wang : This is an SPC method implemented by Wang et al. [3].
 All: This is also an SPC approach. In this case, we create an ensemble of all the classifiers seen so far, and the new data chunk is tested with this ensemble by simple voting among the classifiers. 4.2 Performance Study In this section, we compare the results of all the five techniques, MPC , Wang , BestK , All and Last . As soon as a new data chunk appears, we test each of these ensembles /classifiers on the new data, a nd update its accuracy, false positive, and false negative rate. In all the results shown here, we fix the parameter values of v =5, and r =2, unless mentioned otherwise.

Figure 2(a) shows the error rates for different values of K of each method, averaged over four different chunk sizes on synthetic data, and figure 2(c) shows the same for botnet data. Here decision tree is used as the base learner. It is evident that MPC has the lowest error among all approaches. Besides, we observe that the error of MPC is lower for higher values of K .Thisisdesired because higher values of K means larger ensemble, and more error reduction. However, accuracy does not improve much after K =8. Wang and BestK also show similar characteristic. All and Last do not depend on K , so their error remains the same for any K . Figure 2(b) shows the error rates for four different chunk sizes of each method (also using d ecision tree) averaged over different values of K (2,4,6,8) on synthetic data, and figure 2(d) shows the same for botnet data. Again, MPC has the lowest error of all. Besides, the error of MPC is lower for larger chunk sizes. This is des ired because larger chunk size means more training data for a classifier.

Tables 1(a) and 1(b) report the error of decision tree and Ripper learning algorithms, respectively, on synthetic data, for different values of K and chunk sizes. We do not show the results of Bayes Net, which have the similar char-acteristics, due to space limitation. The columns denoted by M 2 , W 2 and B 2 represent MPC , Wang and BestK , respectively, for K =2. Other columns have similar interpretations. In all three tables, we see that MPC has the lowest error for all values of K (shown in bold).

Figure 3 shows the sensitivity of r and v on error and running times on synthetic data for MPC. Figure 3(a) shows the errors for different values of r for a fixed value of v (=5) and K (=8). The highest reduction in error occurs when r is increased from 1 to 2. Note that r = 1 means single chunk training. We observe no significant reduction in error for higher values of r , which follows from our analysis of parameter r on concept-drifting data in section 3.3. However, the running time keeps increasing, as shown in figure 3(c). The best trade-off between running time and error occurs for r =2. The charts in figures 3(b,d) show a similar trend for parameter v .Notethat v = 1 is the base case, i.e., the single partition ensemble approach, and v&gt; 1 is the multi-partition ensemble approach. We observe no real improvement after v = 5, although the running time keeps increasing. This result is also consistent with our analysis of the upper bounds of v , explained in section 3.3. We choose v =5 as the best trade-off between time and error.

Figure 4(a) shows the total running times of different methods on synthetic data for K =8 ,v =5and r = 2. Note that the running time of MPC is within 5 times of that of Wang . This also supports our complexity analysis that the running time of MPC would be at most rv times the running time of Wang . The running times of MPC on botnet data shown in figure 4(b) also have similar characteristics. The running times shown in figure 4 include both training and testing time. Although the total training time of MPC is higher than that of Wang , the total testing times are almost the same in both techniques. Considering that training can be done offline, we may conclude that both these techniques have the same runti me performances in classifying data streams. Besides, users have the flexibility to choose either better performance or shorter training time just by changing the parameters r and v .
 We also report the results of using equal number of classifiers in MPC and Wang by setting K =10in Wang ,and K =2, v =5, and r =1 in MPC ,whichis shown in Table 2. We observe that error of MPC is lower than that of Wang in all chunk sizes. The columns M 2 (J48), and W 10 (J48) show the error of MPC ( K =2, v =5, r =1)and Wang ( K =10), respectively, for d ecision tree algorithm. The columns M 2 (Ripper), and W 10 (Ripper) show the same for Ripper algorithm. For example, for chunk size 250, and decision tree algorithm, MPC error is 19.9%, whereas Wang error is 26.1%. We can draw two important conclusions from this result. First, if the ensemble size of Wang is simply increased v times (i.e., made equal to K  X  v ), its error does not become as low as MPC . Second, even if we use the same training set size in both these methods (i.e., r =1), error of Wang still remains higher than that of MPC . There are two possible reasons behind this performance. First, when a classifier is removed during ensemble updating in Wang , all information obtained from the corresponding chunk is forgotten, but in MPC , one or more classifiers from a chunk may survive. Thus, the ensemble updating approach in MPC tends to retain more information than that of Wang , leading to a better ensemble. Second, Wang requires at least K  X  v data chunks, whereas MPC requires at least K + r  X  1datachunkstoobtain K  X  v classifiers. Thus, Wang tends to keep much older classifiers in the ensemble than MPC , leading to some outdated classi fiers that can put a negative effect on the ensemble outcome. We have introduced a multi-partition multi-chunk ensemble method (MPC) for classifying concept-drifting data streams. Our ensemble approach keeps the best K  X  v classifiers, where a batch of v classifiers are trained with v overlapping partitions of r consecutive data chunks. It is a gen eralization over previous en-semble approaches that train a single classifier from a single data chunk. By introducing this MPC ensemble, we have reduced error significantly over the single-partition, single-chunk approach. We have proved our claims theoretically, tested our approach on both synthetic data and real botnet data, and obtained better classification accuracies compared to other approaches. In the future, we would also like to apply our technique on the classification and model evolution of other real streaming data.
 This research was funded in part by NASA grant NNX08AC35A and AFOSR under contract F A9550-06-1-0045.

