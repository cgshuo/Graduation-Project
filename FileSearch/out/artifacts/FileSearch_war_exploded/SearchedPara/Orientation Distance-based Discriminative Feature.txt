 Feature extraction is an effective step in data mining and machine learning. While many feature extraction meth-ods have been proposed for clustering, classification and re-gression, very limited work has been done on multi-class classification problems. In fact, the accuracy of multi-class classification problems relies on well-extracted features, the modeling part aside. This paper proposes a new feature extraction method, namely extracting orientation distance-based discriminative (ODD) features , which is particularly designed for multi-class classification problems. The pro-posed method works in two steps. In the first step, we extend the Fisher Discriminant idea to determine more appropri-ate kernel function and map the input data with all classes into a feature space. In the second step, the ODD features are extracted based on the one-vs-all scheme to generate discriminative features between a pattern and each hyper-plane. These newly extracted features are treated as the representative features and are further used in the subse-quent classification procedure. Substantial experiments on both UCI and real-world datasets have been conducted to in-vestigate the performance of ODD features based multi-class classification. The statistical results show that the classifi-cation accuracy based on ODD features outperforms that of the state-of-the-art feature extraction methods.
 I.2.6 [ Arti cial Intelligence ]: Learning X  Induction ; I.5.4 [ Pattern Recognition ]: Application Theory, Algorithm, Performance Multi-Class Classification; Feature Extraction
Knowledge discovery or data mining is the process of find-ing previously unknown and potentially interesting patterns and relations from data. Feature extraction, as a critical step in data mining, is to explore distinctive features from source data, in order to lead to appropriate problem-solving performance. Feature extraction also plays an important role in exploring data, by mapping the input data onto a space, which reflects the inherent structure of the original data. To date, feature extraction has attracted increased attention due to the wide potential of its application in pat-tern recognition, visualization, time series analysis, etc. [1, 2, 3, 4, 5].

Depending on the principle of the optimization models, the previous feature extraction approaches can be classified into two broad categories: (1): unsupervised feature extrac-tion [6, 7, 8, 9, 10], in which the most representative features are constructed out of the original data. In this procedure, the data label is not considered in the feature extraction pro-cedure. For example, principal component analysis (PCA) is used to represent the source data in terms of minimal mean square error between the representation and the source data, without taking the data label into account. Consequently, PCA has little to do with the discriminative features that are optimal for classification. (2): supervised feature extraction methods [11, 12, 13, 14], which map the original data onto an ideal sub-space, where the samples from different classes are considerably separated; simultaneously they incorporate the data label information into the learning procedure, such that the extracted features can yield the difference of the distinctive features.

Despite much progress made in this area, most existing methods of feature extraction are not particularly designed for multi-class classification. For example, in unsupervised feature extraction, the label information is not incorporated into the learning procedure; consequently, data from differ-ent classes may share similar extracted features. This poten-tially reduces the accuracy of multi-class classification. In supervised feature extraction methods, although the data la-bel is considered in the learning procedure, it is still difficult to choose an ideal subspace in which the projection of dif-ferent classes in the source data is distinctive. For example, in the feature space of kernel Fisher discriminant analysis (KFDA) [12], KFDA determines a canonical direction for which the data is most separated when it is projected on a line in this direction. However, when the number of classes is large, it is not easy to determine such a canonical direction so that the projections of data are well separated, even in the case that the classes are well separable in the feature space. Another important observation is that many real-world ap-plications fall into the category of multi-class classification problems, such as pattern recognition, or question classifi-cation. Therefore, it is worthwhile to explore new feature extraction methods specifically for the multi-class classifi-cation problem, such that the extracted distinctive features can contribute maximally to classification accuracy.
This paper proposes a novel supervised feature extraction method, to extract orientation distance-based discriminative (ODD) features in the kernel feature space. It is particularly designed for multi-class classification problems, to enhance the classification accuracy. The main contribution of this paper can be summarized as follows. 1. We propose the use of an extension of the Fisher dis-2. We propose an ODD feature extraction method, so 3. Finally, we conduct experiments on both UCI and real-
For clarity, the basic notions used in this paper are listed in Table 1.

The rest of the paper is organized as follows: Section 2 presents the work related to our method. Section 3 proposes our method in detail. Experiments are presented in Section 4. The conclusion and future work are discussed in Section 5.
In this Section, we review the previous works related to this paper. Because our proposed ODD feature extraction is an SVM-based technique for multi-class classification prob-lems, we first briefly recall the principle of support vector machines in Section 2.1, and then review the previous fea-ture reduction methods in Section 2.2.
Support vector machines (SVMs) [15] have been proven to be a powerful classification tool in data mining and machine learning areas. Unlike classical methods that mainly mini-mize the empirical training error, SVM seeks an optimal sep-arating hyper-plane that maximizes the margin between two classes after mapping the data into a feature space. SVM was originally designed for binary classification. They were extended later for multi-class classification by converting the multi-class problem into a set of binary class problems [15, 16, 17, 18, 19, 20]. Over the years, SVM has been success-fully applied to many real-world applications ranging from image classification, text categorization, and face recogni-tion to bioinformatics. The principle of SVM is briefly re-viewed as follows.

Let S = { ( x 1 , y 1 ) , ( x 2 , y 2 ) , . . . , ( x | S set, where x i  X  R m , y i  X  X  +1 ,  X  1 } . In the SVM [15], a non-linear mapping function  X  ( . ) is used to map a data set from the input space ( R m ) into a feature space z , where both classes are expected to be much more linearly separable. As illustrated in Figure 1, the training problem of an SVM is to find w and b to achieve the optimized hyper-plane [15]: The decision function (1) satisfies the following condition where  X  i is introduced to relax the margin constraints. To achieve SVM, we need to solve the following problem: subject to (2), where  X  is a parameter which balances clas-sifier error. By introducing the Lagrange function [15], w , b and support vectors can be determined, and the decision classifier (Equation 1) is then obtained.

For a test instance x , if D w ;b ( x ) &gt; 0, it is classified into the positive class; otherwise, it belongs to the negative class. Figure 1: A graphical view of an SVM in a two-dimensional case. M is the distance from the sepa-rator to the support vectors in the feature space.
Feature selection and feature extraction attempt to re-duce the dimensionality of data. Both have been widely used in machine learning and various application domains. Nonetheless, the principles of these two techniques are based on different objectives.

Feature Selection : Feature selection [21, 22, 23], also known as variable subset selection, is the technique of se-lecting a subset of relevant features from the original source features before doing the actual learning. Feature selection methods [24, 25, 26] are always required to find a global NP-hard optimization problem, where greedy approaches -forward selection and backward elimination -are often used to tackle the optimization problem directly.

Feature Extraction : Feature extraction aims to con-structe combinations of the source variables while still de-scribing the data with sufficient accuracy. The previous works can be broadly classified into unsupervised and su-pervised feature extraction categories.

In the unsupervised feature extraction methods, the data label is not considered in the learning procedure. PCA, KPCA [6, 27, 7], ICA and KICA [9, 28, 10] are best-known feature extraction algorithms. The transform of PCA is de-rived from eigenvectors corresponding to the largest eigen-values of the covariance matrix for data of all classes. PCA seeks to optimally represent the data in terms of minimal mean-square-error between the representation and the orig-inal data. In recent years, some variances of PCA have been proposed. For instance, Bishop [29] adopts some prior in-formation to obtain the intrinsic dimension and the optimal number of clusters for PCA in the latent variable model. Weng et al. [8] proposes an incremental principal compo-nent analysis to realize online learning for PCA. Tang et al. [30] uses traditional PCA and the nonorthogonal binary fea-ture extraction method to obtain components. In addition, Kernel methods have recently been provided to implement PCA in a nonlinear fashion in the form of kernel-PCA [7]. ICA has also been proposed as a tool to find interesting pro-jections of the data. [9, 28] maximize negentropy (divergence to a Gaussian density function) to find a subspace on which the data has the least Gaussian projection. The criterion corresponds to finding a projection of data that looks max-imally clustered. This appears to be a very useful tool for revealing non-Gaussian structures in the data. KICA [10] has been proposed for non-linear separable data by using kernel functions. However, the limitations of these methods in this category is that they are completely unsupervised with regard to the class labels of the data. Consequently, they lack the ability to enhance class separability, and have little to do with the discriminative features optimal for clas-sification.

In the supervised feature extraction, the data label is used to supervise the feature extraction procedure. In this cate-gory, Fisher discriminant analysis (FDA), also called Linear discriminant analysis (LDA), is typical [11, 13, 14]. FDA searches for the transformation by maximizing the ratio of the between-class distance to the within class distance. Un-like PCA which is not concerned with the class information, FDA takes much consideration of the label information of the data. Moreover, Kernel-based LDA (KDA) or Kernel-based FDA (KFDA) has been proposed to offer a flexible ability to handle cases where data is non-linear separable in the input space. In such case, the data is then explored in the kernel-induced feature space to find an optimal di-rection along which the separability of different classes is maximized. For this category, it is generally believed that FDA and KFDA improve the ability to enhance class separa-bility compared to unsurprised feature extraction methods. However, when the number of classes for multi-class classifi-cation problems is large, it is not easy to determine optimal direction where the projections of data are well separated, even if the classes are well separable in feature space. This potentially reduces the performance of multi-class classifi-cation problems. In addition, [31] puts forward generalized and heuristic-Free feature construction to improve accuracy.
Our proposed ODD feature extraction method is proposed to improve the performance for multi-class problems. The same as FDA and KFDA, our method is a supervised feature extraction method since the label information is fully uti-lized in the learning procedure. The difference from KFDA is that, KFDA determines an optimal direction along which the separability of different classes is maximized in the fea-ture space, whereas our ODD method directly constructs optimal hyper-planes (SVMs) in the feature space via one-vs-all scheme [15]. This strategy potentially guarantees the quality of ODD features compared to KFDA features be-cause even if the classes are well separable in the feature space after kernel mapping, there may not exist an optimal direction where the projections of data are well separated.
In addition, our proposed ODD method falls into the fea-ture extraction category, since ODD features are not the subset of source features, but the extracted features between each instance and hyper-plane. In the experiments, we will explicitly compare the ability of ODD features with those of KPCA, KICA and KFDA, since KPCA, KICA and KFDA generally outperform PCA, ICA and FDA respectively.
Feature extraction aims at to construct combinations of source variables and to reduce the high dimension of input examples while still describing the data with sufficient ac-curacy. Due to the fact that most real-world applications fall into the category of multi-class classification and pre-vious works have had difficulty with feature extraction for multi-class classification problems, it is worthwhile to ex-Algorithm 1 Outline of SVM-based ODD feature extrac-tion 1: Determine proper kernel function by the extension of 2: Construct SVM according to one-vs-all scheme in feature plore new feature extraction techniques for multi-class classi-fication problems to reduce the dimensions of data efficiently while maintaining accurate performance.

Because of its importance, we propose a supervised ori-entation distance-based discriminative (ODD) feature ex-traction technique which extracts distinctive features in the kernel space. In all, our proposed method works in two steps, as shown in Algorithm 1. In the first step, we pro-pose the extension of the Fisher discriminant-based method [32] to determine a kernel function which maps input ex-amples from the input space into a feature space in which each class is found to be distinguishable from others. In the second step, we construct SVMs according to the one-vs-all scheme to separate the classes into distinctive domains and extract the orientation distance-based discriminative (ODD) features by calculating the orientation distance between each sample and each hyper-plane. In the following, we exhibit the two steps in detail.
In the kernel method, the input data is mapped from the input space ( R m ) into another higher-dimensional feature space ( z ) via a non-linear mapping function (  X  ( . )) or kernel function ( K ( ., . )). In the feature space, the classes are ex-pected to be much more linearly separable from each other [33]. In addition, the inner products of the two vectors in the feature space can be obtained efficiently and directly from the original data items using a kernel function. RBF as shown in (4) is a typical kernel function in many real-world applications.
 Let us take a toy problem for example as illustrated in Figure 2, two classes are non-linearly separable in the input space (left panel (A)), but are linearly separably from each other after mapping them into a three-dimensional feature space via nonlinear mapping. Due to the flexible generation of the kernel method, it has demonstrated its power to en-hance the performance of many machine learning tools such as the support vector machine (SVM) [15], the kernel prin-cipal component analysis (KPCA) [34] and support vector data description [35].

In the first step of our proposed feature extraction method, the kernel method is adopted to embed the source multi-class classification data into a feature space where each class is expected to be distinguishable from the other. As for the choice of kernel function, most previous works have adopted the strategy of employing a type of kernel function first and then determining the proper parameters for the selected type of kernel function [15]. In the first step of our method, as with the previous work, we first employ a specific type of Figure 2: Illustration of kernel mapping for two-class example. (A): Both classes are non-linearly separable in the input space (B): In the feature space, both classes are linearly separable after non-linear mapping  X  ( . ) = ( z 1 , z 2 , z 3 ) = ( x 2 1 , x kernel function K ( x i , x j ) = ( x i  X  x j ) . kernel function and then propose the approach to deter-mine proper kernel parameters which maximizes the between classes distance of the classes, while minimizing the within classes scatter of the classes. For example, if RBF kernel function as in (4) is selected as the preferred function, we then need to determine kernel parameter  X  . First of all, sev-eral notions are defined for a training set S with C classes. De nition 1 . Assume the sample size of Class i is l i , and x ij  X  Class i , the class center ( m i ) and the within-class scatter of Class i ( SC i ) in the feature space are defined as In definition 1, the class center of each class is implicitly defined, SC i is denoted as the average distance between each sample and its class center which can be explicitly calculated via kernel function without knowing the actual formulation of the non-linear mapping function. In general, the smaller value is S i , the compacter is class i and more similar are instances of class i .
 De nition 2 . The class-distance between Classes i and j is denoted as d ij : In definition 2, the distance between classes i and j is defined as the distance between their class centers. As with SC i actual value can be explicitly calculated by kernel function. In general, the smaller the value of d ij , the more two classes are likely to overlap.
 De nition 3 . The discriminant function J F (  X  ) is where  X  is the parameter set in a selected type of ker-nel function. For example, in polynomial kernel function K ( x , x i ) = (1 + x  X  x i ) d , the parameter is d , and in RBF function K ( x , x i ) = exp(  X  X  X  x  X  x i  X  2 / 2  X  2 ), the parameter is  X  .

In definition 3, from the intuitive understanding and the essence of the Fisher discriminant idea, the kernel parameter can be chosen by maximizing J F (  X  ) such that the classes are found distinguishable from each other. This is because, the larger J F (  X  ) is, the larger the distance between each pair of classes ( d ij ), the compacter is each class ( S i ).
In order to maximize J F (  X  ), two typical optimization meth-ods, i.e., the gradient descent method [15] and grid search [36] can be used here. In the former, the initial value of the kernel parameter is set as  X  0 , and the step moved to where the derivative dJ F ( ) d has maximum value. However, this method always exhibits a local optimal solution. In the latter method, the kernel parameters are chosen in the pa-rameters set to maximize J F (  X  ). More specifically, assume the parameter set  X  has |  X  | parameters as follows: Suppose  X  i has pre-specified n i choices compute (8) on each choice in  X  to explore the parameters with maximum value.
 Because the grid search method has been widely used in SVM to tune hyper-parameters [36] and our ODD feature extraction is an SVM-based technique, we employ the grid search method to explore proper kernel parameters. The pseudo code of determination of the kernel parameters algo-rithm is outlined in Algorithm 2. After this, the parameters in the selected kernel function are determined, which means the kernel function K 1 ( ., . ) is confirmed.
After the determination of kernel function K 1 ( ., . ), the classes of the dataset are implicitly mapped into a feature space where the inner product of two vectors in the feature space can be explicitly calculated by K 1 ( ., . ). In the sec-ond step of our ODD feature extraction method, we first construct hyper-planes (SVMs) according to the one-vs-all scheme [15], i.e., considering one class as positive class and remaining classes as negative class, and then extract the ori-entation distance-based discriminative (ODD) features be-tween instances and each hyper-plane in the feature space to represent the source data instead of the source input fea-tures. The second step of our proposed ODD feature extrac-tion approach is detailed as follows.
First, we transfer a multi-class classification into binary problems based on the one-vs-all scheme. More specifically, for a c -class problem, we take class i as positive class and Algorithm 2 Determination of kernel parameters.
 Input : Training set S // with C classes a specific type of kernel function. // K(.,.)  X  // parameter set  X  = {  X  1 ,  X  2 , . . . ,  X  | | } preset choices for each  X  i // each  X  i has n i choices Output : kernel parameters 1: Set set  X  return to store parameters; 2: Set value = 0 as a temporal variable; 3: for (each set of choice  X  i in  X  ) do 4: Calculate the class center and within-class scatter of 5: Compute the class-distance between each pair of 6: Calculate the discriminant function according to (8); 7: if J F (  X  i ) &gt; value then 8: value = J F (  X  i ); 10: end if 11: end for 12: Return  X  return . the remaining classes as negative class to compose C binary classification problems. Suppose S i is the new formed train-ing set for the i th decomposed binary classification prob-lem. The optimal hyper-plane for the i th binary classifica-tion problem is trained as follows.
 s.t y ij ( w i  X   X  ( x ij ) + b )  X  1  X   X  ij , j = 1 , . . . , where w i is the normal vector for the i th hyper-plane,  X  ( x ) is a mapping function corresponding to K 1 ( ., . ), b i is a bias for the i th hyper-plane.

By introducing the Lagrange function [15], w i and b i can be determined, and hyper-planes D i ( x ) = 0 are then ob-tained. After this step, we obtain C hyper-planes in the the feature space.
Second, for each instance in the training set S , we can cal-culate the orientation distances between the sample x i and each hyper-plane. Firstly, we have the following Theorem. Theorem 1. For a hyper-plane D i ( x ) = w i  X   X  ( x ) + b in the feature space, the distance between an instance  X  ( x and D i ( x ) = 0 is calculated as follows. where  X  w i  X  = H i denotes the i th hyper-plane.

Proof: The formulation of SVM: D i ( x ) = w  X   X  ( x )+ b is a hyper-plane in the feature space.

As illustrated in Figure 3, assume one instance o resides on the surface of hyper-plane, that is Then the distance of instance p and D i ( x ) = 0 can be cal-Figure 3: An illustration of calculating the distance between an instance p and hyper-plane D i ( x ) = w  X   X  ( x ) + b i = 0 in feature space. culated as follows. d ( p, H i ) = According to Equation (11), we have End of Proof
After calculating the orientation distance between each sample x k and each hyper-plane, we then obtain the new C dimensional vector in the feature space, i.e., It is noted that the value of the orientation distance can be either positive or negative, which depends on the position of the sample and corresponding hyper-plane.

Let us consider the three-class problem as shown in Figure 4, where each arrowhead denotes the orientation of each hyper-plane, D 1 ( x ) = 0, D 2 ( x ) = 0, D 3 ( x ) = 0 represents the hyper-planes. An input vector is denoted as ( x , 1), where 1 represents the label of sample x . After we construct three hyper-planes based on the one-vs-all scheme and calculate the orientation distances between x and each hyper-plane, ( x , 1) is transformed into ( x new , 1): ( x , 1)  X  X  X  ( x new , 1) = (( D 1 ( x )  X  Considering the orientation of each hyper-plane, we have
The new training set S new can be transformed from the source training set S as follows.
 S = Algorithm 3 ODD features extraction.
 Input: Training set S ; // with C classes Testing set S t ; kernel function K 1 ( ., . ); // obtained from step one 1: Decompose C-class problem into C binary classification 2: for (j=1; j++;j  X  C) do 3: Training SVM for the i th binary classification prob-4: Obtain decision function D i ( x ) = 0 according to op-5: end for 6: for each instance x i in S do 7: Calculate new ODD features according to (10); 8: Put ODD features and the source label into S new ; 9: end for 10: Obtain S new in (15); 11: for each instance x i in S t do 12: Calculate new ODD features according to (10); 13: Put ODD features into S new t ; 14: end for 15: Obtain S new t in (17); 16: Return S new and S new t .  X  X  X 
In this way, the original training sample x k is represented by using a C  X  dimensional distance vector in (16) x
For the testing set S t = { x t 1 , x t 2 , . . . , x t n orientation distances between a sample and each hyper-plane is also computed, and S t is transformed into S new t as follows.
In all, the procedure of ODD feature extraction is outlined in Algorithm 3.
The computational complexity of ODD feature extraction is analyzed as follows. Binary SVM generally suffers from an O ( n ) training cost where n represents the training sample size and  X  &lt; 2. For a C  X  class problem, assume each class equally has l/C samples, the computational complexity of our ODD feature extraction according to one-vs-all scheme Figure 4: Orientation distance discriminant (ODD) feature between sample x and each hyper-plane in terms of one-vs-all scheme. is Compared to other feature extraction methods, such as KPCA, KICA and KFDA, the computational complexity of ODD is acceptable.
In order to evaluate the performance of our proposed ODD features, we conduct experiments on both UCI datasets and a real-world dataset. For comparison, three classical fea-ture extraction techniques, including unsupervised and su-pervised methods, are used here as baselines. The first is called kernel principal component analysis (KPCA). The second is referred to as kernel independent component anal-ysis (KICA). These two methods belong to unsupervised techniques without taking data label into the learning phase. The third one is kernel linear discriminant analysis (LDA or FDA), which incorporates label information into the learn-ing process.

After feature extraction by the above four methods, we employ three famous multi-class classification methods, i.e., decision three, neural network, SVMs with one-vs-one scheme, to observe the functionalities of the ODD features, KPCA features, KICA features and FDA features. In the experi-ments, RBF kernel function in (4) is used in the four kernel-based feature extraction methods.

All the experiments are conducted in Matlab environment, the test platform is a Dual 2.8GHz Intel Core2 T9600 PC with 3.45GB RAM.
The performance of classification systems is typically eval-uated in terms of F-measure [37, 38]. For the i th binary classification problem shown in Table 2, F-measure trades off precision p i and recall r i which are defined as follows.
For the C -class classification problem, the macro-average precision and recall of the category space is obtained from the overall number of instances correctly accepted and wrongly accepted Table 2: Confusion Matrix for i th binary classi ca-tion problem Predicted Dermatology 366 6 34 Pageblock 5473 5 10 The F-measure is defined as the harmonic mean of precision P av and recall R av : From this definition, it is clear that the F-measure reacts to an average effect of both precision and recall. When ei-ther of them ( P av or R av ) is small, the value will be small. Only when both are large will the F-measure exhibit large value. For the theoretical bases and practical advantages of F-measure, please refer to [38] for detail.
The datasets used in our experiments consist of eight UCI datasets and one real-world question classification dataset. The detailed information about these eight UCI datasets is illustrated in Table 6. For the real-world question classifi-cation dataset, it contains very original text questions, con-sists of 1264 questions with 6 coarse categories ( X  X ocation X ,  X  X ime X ,  X  X uman X ,  X  X bject X ,  X  X escription X  and  X  X umber X ). We extract part-of-speech (POS) features for each question.
At the pre-processing stage, all records in each dataset are normalized to [-1, 1]. The parameter  X  in (4) is searched in where  X  0 is the mean norm of the training data.
Experiments are established to compare the performance of KPCA, KICA, KFDA and ODD features. To avoid sam-pling bias, we generate 10 groups of training data and testing data for each dataset by randomly selecting 65% data as a training set and the remainder as a testing set at each time.
For each group of training and testing sets, the exper-iments consist of two steps. In the first step, we extract KPCA, KICA, KFDA, ODD features from the source data. In the second step, we perform decision three (C4.5), neural network (BP), and SVM-based one-vs-one methods on the extracted features to compare the quality of these extracted features. Specifically, in the first step, for ODD feature ex-network and SVM-based one-vs-one method traction, we perform the extension of the Fisher discriminant method on each choice of (22) to identify the parameter  X  in (4) such that the discriminant function J F in (8) achieves its maximum value. After that, one-vs-all scheme is conducted to extract ODD features between each instance and hyper-planes. In this procedure, another parameter  X  in SVM is set 1 when constructing binary SVM, since the dataset is normalized and this setting is acceptable in tuning parame-ters [39]. With respect to KPCA and KICA feature extrac-tion, we keep the extracted features on each choice of (22) because we do not know which  X  in (22) leads to a better performance of multi-class classification algorithms on them. Since there exist seven choices for  X  in (22), we obtain seven sets of KPCA features, KICA features and KFDA features respectively.

In the second step, we directly conduct the three multi-class classification algorithms on ODD features and obtain the performance. For the seven sets of KPCA features, we conduct the multi-class classification algorithm on each. We then obtain the seven highest testing accuracies and retain the most high to represent the accuracy on the KPCA fea-ture. The same conduction is performed on the seven sets of KICA and report the maximum performance.
 Finally, the testing errors on KPCA, KICA, KFDA and ODD features are shown in Figure 4 using three multi-class classification methods. The results show that, for each multi-class classification algorithm, the testing error on ODD fea-tures is always smaller than that on KPCA and KICA fea-tures. From the experiments, the results indicate that ODD features can improve the accuracy of multi-class classifica-tion algorithms. We report the average testing error and the standard deviations for KPCA, KICA, KFDA and ODD fea-tures of ten groups of training and testing sets from Table 3 to 5. It is clear that the average testing error on ODD fea-tree method.
 network method.
 based one-vs-one method.
 tures is always smaller than that of KPCA, KICA, KFDA and the standard deviation of ODD features is also compa-rable with others.
While many feature extraction methods have been pro-posed, they are often not suitable for identifying discrimi-native features for multi-class classification, and potentially result in low classification accuracy. This paper has pro-posed a novel feature extraction method, to extract orienta-tion distance-based discriminative (ODD) features, specifi-cally designed for multi-class classification problems. The proposed method works well in two steps. In the first step, the kernel function is determined by extending the Fisher discriminant idea. In the second step, ODD features are then extracted based on the one-vs-all scheme to generate discriminative features. Substantial experiments on eight UCI datasets and a real-world dataset have shown that our proposed ODD features method outperforms state-of-the-art feature extraction methods, including KPCA, KICA and KFDA.

We are extending our work in several directions. We would like to apply the ODD feature extraction method to the on-line environment, and apart from the question classification, we are trying to apply the proposed method to bioinformat-ics with multi-class classification data.
This work is sponsored in part by QCIS (Quantum Com-putation and Intelligent Systems), CMCRC (Capital Mar-kets CRC Limited), Australian Research Council Discov-ery Grants (DP1096218, DP0988016, DP0773412) and ARC Linkage Grant (LP0989721, LP0775041), as well as US NSF through grants IIS-0905215, DBI-0960443 and OISE-0968341. [1] J. Zhang and L. Gruenwald. Opening the black box of [2] W. Zuo, D. Zhang, J. Yang, and K. Wang. Bdpca plus [3] Y. Liu, L. V. Lita, R. S. Niculescu, K. Bai, P. Mitra, [4] T. Sun, S. Chen, J. Yang, and P. Shi. A novel method [5] Y. Xu, S. Furao, J. Zhao, and O. Hasegawa. To obtain [6] P. A. Devijver and J. Kittler. Pattern recognition: A [7] B. Sch  X  o lkopf, S. Mika, C. Burges, P. Knirsch, [8] J. Weng, Y. Zhang, and W. S. Hwang. Candid [9] J. Chien and B. C. Chen. A new independent [10] B. Xu, X. Jin, P. Guo, and F. Bie. Kica feature [11] M. Girolami, A. Cichocki, and S. I. Amari. A common [12] S. Mika, G. R  X  a tsch, J. Weston, B. Scho  X  o lkopf, and [13] H. Zhao, S. Sun, Z. Jing, and J. Yang. Local structure [14] J. Yang, A. F. Frangi, J. Y. Yang, D. Zhang, and [15] V. Vapnik. Statistical learning theory . Springer, 1998. [16] E. Allwein, R. Schapire, and Y. Singer. Reducing [17] B. Liu, Z. Hao, and E. Tsang. Nesting one-against-one [18] B. Liu, L. Cao, P. S. Yu, and C. Zhang.
 [19] A. Passerini, M. Pontil, and P. Frasconi. New results [20] R. Tibshirani and T. Hastie. Margin trees for [21] J. Ren, Z. Qiu, W. Fan, H. Cheng, and P. S. Yu. [22] K. Shima, M. Todoriki, and A. Suzuki. Svm-based [23] F. Pan, T. Converse, D. Ahn, F. Salvetti, and [24] J. Weston, A. Elisseeff, B. Sch  X  o lkopf, and M. Tipping. [25] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene [26] L. Song, A. Smola, A. Gretton, K. Borgwardt, and [27] F. Zhang. A polygonal line algorithm based nonlinear [28] I. Dagher and R. Nachar. Face recognition using [29] C. Bishop. Bayesian pca. In NIPS 1999 , pages [30] F. Tang, R. Crabb, and H. Tao. Representing images [31] W. Fan, E. Zhong, J. Peng, O. Verscheure, K. Zhang, [32] V. Roth and V. Steinhage. Nonlinear discriminant [33] B. Sch  X  o lkopf and A. Smola. Learning with kernels . [34] S. Mika, B. Sch  X  o lkopf, A. J. Smola, K.R. Miller, [35] D. Tax and R. Duin. Support vector data description. [36] C. Hsu. A comparison of methods for multiclass [37] S. Mika, G. R  X  a tsch, J. Weston, B. Scho  X  o lkopf, and [38] J. William and M. Shaw. On the foundation of [39] S. Keerthi. Efficient tuning of svm hyperparameters
