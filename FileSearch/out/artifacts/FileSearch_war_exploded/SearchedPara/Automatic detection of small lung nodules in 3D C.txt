 1. Introduction
Lung cancer stands out among all other types of cancer for having one of the highest incidence rates and one of the highest mortality rates. Unfortunately, diagnosis is still often made very late, affecting the treatment results. One of the hopes for changing this scenario lies in achieving a more precocious diagnosis of lung cancer through low-dose computed tomography (CT), applied as a screening method to risk groups of smokers and former smokers with high tobacco levels.
 Recent results from the National Lung Screening Trial Research
Team ( T.N.L.S.T.R. Team, 2011 ), with the participation of a high number of patients from several centers, showed that lung cancer screening in high-risk smokers with low-dose CT reduced mortal-ity by 20%, compared to the group that underwent screening through simple thorax radiography.
 basically occurs by means of the diagnosis of the lung nodule.
As thorax CT provides good-quality images, it allows detecting, quantizing and monitoring the evolution of those nodules ( Verschakelen et al., 2008 ). The automatic detection of lung nodules in tomographic exams, especially the smaller ones, is a challenging task, since the number of false positives is large. The correct detection of these nodules can signi fi cantly increase the success of the diagnosis, leading to an earlier treatment and, consequently, a higher survival rate for patients.
 odology for automatic detection of small lung nodules (between 2 mm and 10 mm in diameter). Although this method seeks to detect lung nodules, it does not provide their precise boundaries, but indicates their presence. A practical application of this meth-odology is to help in the process of detecting small nodules, which is currently performed manually by specialists. This application is adequate to track exams, since it requires searching for smaller nodules in a large number of exams.

The main contribution of this work is the use of Gaussian mixture models to segment the structures inside the lung, and Shannon's and Tsallis's Q entropy as a texture descriptor to discriminate structures as a nodule and a non-nodule. These techniques have already been used in many other contexts ( Permuter et al., 2006; Francos et al., 2003; Zivkovic, 2004;
Reynolds et al., 2000; Rodrigues and Giraldi, 2011 ), but, to our knowledge, they were not used to detect small lung nodules. In this work, we will show that these techniques are promising and can be used for this purpose. Besides these techniques, we use the
Hessian matrix to separate round structures segmented from blood vessels and bronchi, and support vector machine to reduce false positives. These topics will be detailed in the next sections.
This paper is organized as follows: in Section 2 we show some related work. Section 3 presents all the steps of the automatic segmentation of lung nodules, explaining in detail the use of techniques such as the Gaussian mixture models and Hessian matrix, and the extraction of shape and texture features from the lung nodule candidates, which compose the proposed methodol-ogy. In Section 4 we analyze all the tests performed with the application of the method. Finally, in Section 5 , we present remarks about this work. 2. Related work
The automatic segmentation of CT images is a hard task because the images contain many interfering elements, such as noise generated in the acquisition process, image features (thick-ness of the slices, dimensions of the pixels, etc.), the morphology of the nodules, their location and the presence of neighboring structures with very similar densities and directly connected.
The use of CAD/CADx systems to help physicians has been increasing day by day ( Schneider et al., 2009 ). These systems show suspect radiologic artifacts and indicate possible diagnoses, and tend to be much faster than the physician. They are especially used in screening programs, where lung nodule detection and diagnosis tasks are performed continuously and repeatedly by physicians.
The improvement in the detection and diagnosis of lung nodules is shown in various works, for example Sahiner et al. (2009) and Li et al. (2008) .

In a recent work by Nietert et al. (2011) , a study was carried out to examine the impact on the expert's con fi dence when using a
CAD system to look for small pulmonary nodules. The study showed that the con fi dence of radiologists in identifying small pulmonary nodules with CT can be improved with help of a CAD system.

The need to reach a precise diagnosis of lung nodules in order to identify cancer at an early stage (with the proceeding clinic-radiological exploration), which could lead to a cure or a longer survival, has motivated many researchers to look for new forms of computer-assisted detection and diagnosis.

The idea behind these tools is to advise the specialist physi-cians, highlighting suspect radiological artifacts, or to offer a second opinion. Classical works such as those of Jeong et al. (2007) and Reeves and Kostis (2000) have well demonstrated the tasks performed by these tools.
 A CAD system for detection of lung nodules in Computed
Tomography images is presented in Tan et al. (2011) . The CAD system was trained and tested with images from the Lung Image
Database Consortium (LIDC). Initially, a segmentation method based on nodule and vessel enhancement fi lters and a computed divergence feature was applied to locate the centers of the nodule clusters. In the subsequent classi fi cation stage, invariant features, de fi ned on a gauge coordinates system, are used to differentiate nodules from vessels. The work achieves a detection sensitivity of 87.5% with a mean of four false positives per scan, for nodules with diameter above or equal to 3 mm.

The work by Ozekes (2007) , using rule-based segmentation and template matching trained with a genetic algorithm, obtained very signi fi cant results. His system achieved 93.4% of sensitivity and 0.594 false positives per exam. Also based on template matching, but adding a genetic cellular neural network and thresholding based on fuzzy rules, Ozekes et al. (2008) achieved 100% of sensitivity and a rate of 13.37 false positives per case.
Under the same perspective, through selective enhancement fi lters and rule-based segmentation, Li et al. (2008) achieved a sensitivity of 86%, 81% and 75%, with 6.6, 3.3 and 1.6 false positives per exam, respectively, in a scheme of four sets with cross-validation.

Pu et al. (2008) used a signed distance fi eld in the whole set of images and identi fi ed the maxima found as potential nodules. These were then ranked according to the distance of the medial axis, obtained by means of clustering and the application of the marching cubes algorithm. A total of 52 tests were performed on a proprietary database containing 184 lung nodules, achieving 95.1% of sensitivity and a mean of 1200 suspect voxels per exam.
Using low-dose CT and active contour images combined with a multi-thresholding algorithm, starting at points which had max-imum resemblance to a sphere and obtained from the autovalues in each voxel, Schneider et al. (2009) managed to detect 58% of the nodules in their database, with a rate of 1.38 false positives.
The study by Ye et al. (2009) uses fi ve features, containing intensity information, shape index and 3D spatial location. Their work shows that the mean overlapping of the nodules segmented by this method is near 0.81. da Silva Sousa et al. (2010) developed a complete system for automatic segmentation in multiple stages. Each stage of the system is responsible for segmenting signi fi cant portions of the volume of CT images. The whole process is comprised of six stages: extraction of the thorax, extraction of the lung, pulmonary reconstruction, extraction of structures, elimination of tubular structures and reduction of false positives. The results obtained by this system presented a rate of 0.42 false positives and 0.15 false negatives, with sensitivity of 100%.

In the paper by Lee et al. (2008) , a set of classi fi ers, called random forest, is used in two stages, fi rst in the lung nodule detec-tion stage and then in the stage for reduction of false positives. The results obtained by their methodology were of 100% true positives, with 1.4 false positives per image.

Dolejsi et al. (2009) developed a computer-aided diagnosis system based on morphological operations and fi lters, followed by the classi fi cation of candidates by the AdaBoost classi The sensitivity achieved was of 89.62% with 12.03 false positives per image. In a later work, Dolej  X   X  and Kybic (2008) presented a classi fi er aiming to reduce the number of false positive responses of the primary detector. The new classi fi er signi fi cantly reduced the number of false positives to 2.6 per slice. de Oliveira Campo et al. (2010) developed a method based on the clustering of similar points in the image, obeying similarity standards. These standards determine the way each region of the image will be clustered (as nodule and non-nodule candidates) and are de fi ned by several descriptors. The main descriptors include variance, elongation and spherical disproportion. As a result, the combination of these three descriptors led to a sensitivity of 80.9%, with 0.23 false positives per slice.
Messay et al. (2010) combined simple image processing tech-niques, such as intensity thresholding and morphological proces-sing, to segment and detect structures that are lung nodule candidates. The authors used 245 features to determine the lung nodule candidates. Then they made a selection of these features and used them in two classi fi ers: the Fisher Linear Discriminant classi fi er and a quadratic classi fi er. The method was able to detect 92.8% of the structures as lung nodules.

Opfer and Wiemker (2007) showed that their CAD system is able to reach a detection rate of 89% of lung nodules, with average of 2 false positives per exam for nodules with diameter larger than or equal to 4 mm.

The method proposed by Xiaomin et al. (2010) is divided in three steps to detect lung nodules in CT images. First, a 2D multi-scale is used to detect lung nodule candidates in the images. Secondly, the authors discriminate nodules from non-nodules with a blob-like shape using a growing geometric restriction region. Finally, after the shape features of each region are extracted, a classi fi er based on automated rules to reduce false positives is applied.

Suarez-Cuenca et al. (2011) selected six classi fi ers: linear discri-minant analysis (LDA), quadratic discriminant analysis (QDA), arti fi cial neural network (ANN), and three types of support vector machines (SVM). They were applied separately and combined using 85 images with 110 lung nodules. The sensitivity achieved was 80%, and the number of false positives per case for each of the six classi fi ers was 6.1 for LDA, 19.9 for QDA, 8.6 for ANN, 23.7 for SVM-dot, 17.0 for SVM-poly, and 23.35 for SVM-ANOVA. When these were used in combination, the number of false positives per case was 3.4 for the majority-vote rule, 6.2 for the mean, 5.7 for the product, 9.7 for the neural network, and 28.1 for the likelihood ratio method.
 Camarlinghi et al. (2011) proposes the combination of different
CAD systems aiming to provide enhanced support for lung nodule identi fi cation. Their experiment was then compared to results of the individual systems by means of the ROC curve. The results suggest that the higher the number of CAD systems used in the detection, the higher the number of true positives, 65, and the lower the number of false positives, 139, over a database of 69 images containing 114 lung nodules.

Netto et al. (2012) propose a methodology for automatic detection of lung nodules. The proposed method consists of acquisition of computed tomography images of the lung, reduction of the volume of interest through thorax extraction techniques, lung extraction, and reconstruction of the original shape of the parenchyma. After that, growing neural gas (GNG) is applied to constrain even more the structures that are denser than the pulmonary parenchyma (nodules, blood vessels, bronchi, etc.).
The following stage is the separation of the structures resembling lung nodules from other structures, such as vessels and bronchi.
Finally, the structures are classi fi ed as either nodule or non-nodule, through shape and texture measurements together with the support vector machine. The methodology ensures that nodules of reasonable size are found with 86% sensitivity and 91% speci fi city. This results in a mean accuracy of 91% for ten training and testing experiments with a sample of 48 nodules occurring in 29 exams. The rate of false positives per exam was of 0.138, for the 29 exams analyzed.
 our method, we believe that a de fi nitive methodology that could be considered totally ef fi cient in every detection step does not currently exist. Some methodologies fail in the parenchyma extraction stage, some in the segmentation stage, some in redu-cing false positives, some require tests with more complex image databases, and some are too slow for a real-time application. In summary, all methodologies need a more conclusive validation.
For this reason, other alternatives must be investigated, and this is why we propose this work.
 methodology proposed here, which makes use of some of the techniques mentioned by them and uses their results as a basis for comparison. Our methodology follows a division of stages similar to what has been previously proposed by our team ( da Silva Sousa et al., 2010 ), but with many changes in the techniques employed in each stage. In the stage responsible for segmenting internal structures, for instance, Gaussian Mixture Models are used to these structures. In order to separate the nodule candidates, the
Hessian matrix is used to provide information about the shape of each structure in the parenchyma. Finally, the elimination of false positives employs Tsalliss and Shannons entropy measurements and Support Vector Machine. More details about the proposed methodology will be presented in the following sections. 3. Detection of lung nodules tion of lung nodules in CT images is described in detail.
This methodology comprises four stages. Acquisition of CT images, segmentation of nodule candidates, features extraction and classi-fi cation of candidates as nodules and non-nodules. Fig. 1 illus-trates these stages.
 3.1. Image acquisition
The images used during the development of this work come from an image base called Lung Image Database Consortium (LIDC) ( Armato et al., 2004 ). LIDC is a repository of low-dose helical CT images, made available over the internet by the National Cancer
Institute (NCI) for research on detection and diagnosis of lung cancer.

LIDC images are found in DICOM format and are stored as portions of longitudinal images (slices) of the patients' chests.
Grouped, these images form a three-dimensional representation of the patient's chest, containing also annotations which indicate the location of the existing carcinogenic lesions.

The images used in this work have 512 512 pixels in the x -and y -axes and a number of slices per exam varying from 101 to 505. he voxel spacing ranges from 0.65 to 1 mm for the axes x and y , while the thickness ( z -axis) varies from 1 to 6 mm. All images are quantized in 12 bits. 3.2. Segmentation of nodule candidates
In this work, the search for nodule candidates is made in four stages, and in each one of them, the region of search for nodules in the image is reduced by successive segmentation of structures that present potential to be lung nodules. In the fi rst stage, we perform the segmentation of the pulmonary parenchyma through thresh-olds and the region growing algorithm. In the second stage, a new segmentation excludes trachea and the main bronchi. The third stage segments the structures inside the lung and in the fourth and last stage, we select just the structures with shape and texture that most resemble nodules.

In the next sections we will summarize all stages for segmen-tation of the lung nodule candidates. A detailed description of the fi rst and second stages can be found in the previous works of our team ( da Silva Sousa et al., 2010; Netto et al., 2012 ). 3.2.1. Segmentation of the pulmonary parenchyma
The fi rst step for the segmentation of nodule candidates is the segmentation of the pulmonary parenchyma. In a typical thoracic CT, we fi nd two distinct groups of voxels. The fi rst group presents low values (mean of 900 HU) and mainly corresponds to the image background and low-density tissues of the parenchyma. The second group presents higher values. This group of voxels corre-sponds to the structures that have higher density, such as bones, fat and muscles, besides encompassing the structures inside the lung as well.

The segmentation of the parenchyma is divided in four sub-stages, namely: segmentation of thorax, segmentation of struc-tures inside the thorax, segmentation of the parenchyma and reconstruction of the pulmonary edge. Each one of these sub-stages is carried out separately in each slice of the exam. The processing of the segmentation in slices (2D) makes the algorithm less complex than the volumetric segmentation (3D).

In the fi rst sub-stage, the patient's thorax is separated from irrelevant parts of the image for the purpose of detection of nodules. These irrelevant parts include air, exam table, and sheets. These structures must be eliminated in order to ease the detection process. This is done by excluding all pixels with values above 500 HU. This value was determined after performing tests on the LIDC. The result of the application of this threshold is presented in Fig. 2 (image 1).

In the second sub-stage, we use the region growing algorithm to select every voxel connected to the edge of the image with null values. These voxels correspond to the region outside the thoracic cavity of the patient, mostly fi lled with air.

In the third sub-stage, we remove all the structures previously selected, obtaining just the region that belongs to the pulmonary parenchyma and air ways. The result can be seen in Fig. 2 which presents the application of all sub-stages for the segmentation of the pulmonary parenchyma. Image 1 presents the segmentation of the patient's chest to be eliminated. The segmented image 2 is related to the segmentation of the area outside the thoracic cavity.
Image 3 corresponds to the union of previous segmentations (images 1 and 2). Image 4 is related to the segmentation of the region outside the parenchyma which, after an inversion of the segmented area, leads to image 5, corresponding to the region of the segmented parenchyma, including the structures inside it. In the fourth sub-stage we perform the reconstruction of the edge of the lungs. This reconstruction is needed to reintegrate the par-enchyma and the nodules which were connected to the edge and were wrongly removed in the previous stages. Fig. 3 (image A) presents a case where the segmentation excluded a juxtapleural nodule.

The reconstruction used the rolling ball technique and described in da Silva Sousa et al. (2010) which, through morpho-logical closing operations with a circular structuring element, is able to restore the edges of the lung in order to avoid the exclusion of peripheral nodules. The radius of the structuring element used in this work was of four units. Fig. 3 (image B) presents the reconstructed parenchyma. 3.2.2. Removal of trachea and main bronchi
The segmentation performed by the previous stages is not enough to separate the pulmonary parenchyma completely. After the application of the previous stages, there are still the trachea and its bifurcation into main bronchi. The elimination of these structures is done in order to isolate the pulmonary parenchyma.
The elimination of the trachea and the main bronchi is per-formed in each slice of the exam through the selection of the two biggest regions in each one of them. These regions with larger area correspond to both lungs. 3.2.3. Segmentation of internal structures using GMM
Once the parenchyma is segmented, we perform a new seg-mentation of the structures inside the lung. The internal struc-tures present higher density in the pulmonary parenchyma. The separation of these structures is important at this moment because it further reduces the search space, allowing the search for nodules to occur only in structures with real potentiality to be nodules.
 this stage is based on the density of the voxel of the pulmonary parenchyma. For this, the distribution of the intensity value of the voxels of the image is modeled in order to distinguish two classes: internal structures and the pulmonary parenchyma. The Gaussian
Mixture Models (GMMs) are used to model this distribution. We chose GMM to perform this task because: (1) there are well-studied statistical inference techniques available; (2) there is fl exibility in choosing the component distribution; (3) it obtains a density estimation for each cluster; and (4) the  X  soft tion is available, data points are assigned to clusters with certain probabilities; (5) GMMs are able to build soft clustering bound-aries, i.e., points in space can belong to any class with a given probability, unlike K -means for example.
 f  X  x s  X  X   X  k where N  X  x s j  X  i ;  X  i  X  is a normal distribution given by
N  X  x s j  X  i ;  X  i  X  X  1  X  the chance that the voxel to belong to the class, and C i classes, where k , the number of classes, is known. This means that p i  X  p  X  x s A C i  X  ; 0 o p i o 1 ;  X  k i  X  1 p i  X  1.
 X   X  X  p where  X  i ,  X  i 2 and p i are the parameters that must be estimated. One of the most used methods for the estimation of these parameters is the Expectation-Maximization algorithm. Deeper details about the GMM and the Expectation-Maximization algorithm can be found in Permuter et al. (2006) . 3.2.4. Detection of nodule candidates performed in the previous stage represented an important reduc-tion of the search space, remaining only structures such as nodules, bronchi, bronchioles and blood vessels. The present stage intends to fi nd lung nodule candidates.

This stage is performed with basis on the shape of the structure found. A lung nodule is a spherical pulmonary opacity, circum-scribed, involved by normal pulmonary parenchyma, with dia-meter less or equal to 3 cm. The spherical opacity characteristic of lung nodules can be employed to eliminate structures which do not have this shape, such as vessels and bronchi.

The Hessian matrix is widely used to detect or emphasize disc-shaped structures and lines in 2D, as well as blob-shaped (an agglomerate of voxels) structures in 3D images. Based on the eigenvalues of the Hessian matrix in a certain location of the image, we are able to classify the pattern of this place as similar to a line, plateau or blob. This method was originally developed by
Koller et al. (1995) , which was later improved by Frangi et al. (1999) to emphasize blood vessels. Sato et al. (1997) , in turn, generalized the use of this technique to emphasize blobs.
In order to eliminate these non-round structures, we perform a search for blob-shaped structures using the Hessian matrix.
The computation of the matrix for an image generates another image in which each pixel represents the similarity between the region and a blob. The highest values are located in the center of the round structures. The use of a threshold allows the selection of these values separately and the segmentation of these structures.
The advantage of using the Hessian matrix for detection is that it allows the separation of nodules connected to structures normally found in the lung, for example, nodules connected to bronchi or blood vessels. However, the major disadvantage is that bifurca-tions between bronchi are also considered as spherical structures, generating a large number of nodule candidates. In this work we used the algorithm in Insight Segmentation and Registration Toolkit (ITK).
 Fig. 4 presents an example of the detection of circular blobs.
Image A simulated a region of a CT slice of the pulmonary parenchyma. This region contains tubular structures (bronchi and vessels, represented by the label 3) and structures in circular shape (possible nodules, labels 1 and 2). Label 2 highlights the presence of a nodule physically connected to the bronchi. Image B exempli fi es the segmentation through the Hessian matrix. Fig. 5 presents the separation of these structures in the slice of
Fig. 2 . The lung nodule is highlighted in the center, surrounded by other circular structures that correspond to the bifurcation of vessels and bronchi. Despite the fact that the Hessian matrix completely separates the nodules from the other structures, the bifurcation points of the bronchi are also separated, demanding the use of a classi fi er to determine which structures are nodules or non-nodules. 3.3. Features extraction
The segmentation of nodule candidates is capable of discon-necting the nodules from the other structures, but this stage includes many structures that do not correspond to nodules, being necessary to determine which ones are lung nodules. In this work, texture features used to differentiate nodules from non-nodules were the Shannon and Tsallis Q entropy. These measures already were used in Wei et al. (2010); Martin et al. (2004) , Mello and Schuler (2008) for other purposes with promise results, then we want experiment them in this application. de Albuquerque et al. (2004) de fi ned an expression to quantify the amount of information produced by a process. So, the entropy of a discrete source is obtained from the probability distribution, where p  X  p i is the probability of the system to be found in a certain state i . In this case, 0 r p r 1 and  X  k i  X  1 p total number of states, or possible symbols. Shannon entropy S can be described by
S  X   X  k  X 
Some physical systems (with long iterations, fractal structures, etc.) do not provide a good response to the formulation proposed by Shannon. Tsallis proposed then a new equation that generalizes the Shannon entropy and allows a non-extensive approach, adding the coef fi cient q . The equation is given by de Albuquerque et al. (2004) :
S
 X  where k is the total number of possibilities of the system and q is an entropy index which de fi nes the degree of non-extensiveness of the system.

In this work, the Shannon entropy was computed from the values of the quantized voxels for 8 bits, with the purpose of reducing the complexity of the computation.

Based on tests to compute the Tsallis entropy, we used three different values for the parameter q of the non-extensiveness, 0.5, 0.7 and 0.9. In total, six features were extracted for each nodule candidate. 3.4. Classi fi cation of nodule candidates
The classi fi cation of nodule candidate structures, that is, structures with texture features similar to nodules, raised by the previous stages, was performed by the Support Vector Machine (SVM) ( Vapnik, 1998 ). SVM is a powerful, state-of-the-art algo-rithm with strong theoretical foundations based on the Vapnik Chervonenkis theory. SVM has strong regularization properties.
Regularization refers to the generalization of the model to new data. This was the main reason for choosing this classi fi work. The accuracy of an SVM model is largely dependent on the selection of the kernel parameters such as C and  X  for Radial Basis
Function (RBF). We used the software LibSVM ( Chang and Lin, 2012 ) to estimate these two parameters.

The validation of the classi fi cation is given by number of true positive nodules (TP), number of false positive rate (FP), sensitivity (Se), speci fi city (Sp) and accuracy (Ac), and also by rate of false positives per exam (FP/exam) ( Haykin, 2001 ). 4. Results and discussion
In this work, we used 140 exams from the LIDC. These exams contain small nodules with diameters between 2 and 10 mm which were identi fi ed by specialists. For testing and validating the methodology, the image database was divided into two parts: 80% were used to estimate parameters and to train the model, corresponding to 112 exams containing 188 nodules, and 20% were used for testing, corresponding to 28 exams containing 72 nodules. 4.1. Parameter estimation
In this stage we used 112 exams with 188 nodules to estimate the GMM, and the Hessian matrix and the SVM model to validate the proposed methodology.

The GMM is constructed by estimating mean, variance and proportion of the parameters for each class using the expectation maximization algorithm. These parameters refer to the intensities of the voxels in the exams in Houns fi eld units (HU). In Table 1 we present the mean values of the parameters estimated for the images in our exam database. These are the values which de the Gaussian of the parenchyma and of the internal structures. The
GMM parameters were selected aiming to minimize the error between the volume of the detected nodules and the volumes determined by the specialists responsible for diagnosing the LIDC.
These parameters were speci fi ed experimentally so as to obtain the best results: k  X  2.7 and  X   X  1 : 9 we applied a threshold of 0.1 with the purpose of selecting only structures with shape similar to a sphere. This threshold was determined empirically.
 identi fi ed in the LIDC and 188 non-nodules (blood vessels, bronchi, etc.) identi fi ed by our specialist. The estimated parameters were
C  X  1.0 and  X   X  0 : 003. These parameters will be used in the test stage of the methodology. 4.2. Tests nodules. The segmentation of the internal structures using GMM proved to be ef fi cient; however, many isolated structures were segmented. This is due especially to the non-homogeneity of the parenchyma and to the acquisition method, which presents high-intensity structures even when not connected to internal struc-tures. With the inclusion of such structures, the process was led to include micro-structures which do not correspond to real structures. Thus, the segmentation led to a mean of 77,909 structures selected per exam, which is a very high fragmentation rate, considering that approximately 93% of the selected structures have a number of voxels below eight units. The presence of those micro-structures can be seen in Fig. 6 .
 for the minimum size that a structure must have to qualify to the following processing stages. So, all structures with less than eight voxels were discarded. This strategy does not entail the exclusion of possible nodules, since in the exam database used there are no nodules with eight pixels or less. With this strategy, about 68,973 structures per exam were eliminated, leaving about 8936 struc-tures to be better investigated in the next stages.

After eliminating all the small structures, the Hessian matrix is applied to detect nodule candidate structures based on their geometric features. For performance reasons, each slice of the exam is scanned in 80 80 80 regions for processing. This region size is enough to detect nodules with diameter smaller than 10 mm and blob-shaped structures. In this set are included both nodules and false candidates (vessel fragments and bronchial bifurcations). From the 72 nodules tested, 64 were successfully segmented and 8 were not segmented. The main reason for the failure to segment those nodules is that they were small juxta-pleural nodules.

Table 2 summarizes the results found with the proposed methodology. The fi rst row of the table presents the results of the classi fi cation performed on the training set, showing the generalization capability of the classi fi er for this set of exams.
In the second row we present the performance of the classi with the test set. The results achieved during the tests show that the classi fi er is capable of ef fi ciently discriminating lung nodules from the other segmented structures. This is proven by the sensitivity of 90.6%. On the other hand, the test pointed out a high false-positive rate (approximately 1.17 false positives per slice), which reduces the total accuracy of the method (88.4%).
As said previously, one of our goals was to evaluate the quality of the measures of Shannon's and Tsallis's Q entropy. We believe these measures produced satisfactory results, since they were able to discriminate nodules from non-nodules despite their texture similarities. However, we believe that further investigation is needed, including the combination of these measures with other ones to con fi rm their potential. 4.3. Comparison with other related works
In order to put the quality of our work into perspective, Table 3 compares the method proposed herein to other methods for detecting lung nodules in CT images. It is important to stress that a fair comparison of the cited methodologies ( Section 2 ) would only be possible if all of the works had used the same group of images. Besides, there should be some standard parameters, such as resolution and bits per pixel. Another factor that should be common to the works is the sample used, because all the methodologies should have used the same data for the training and testing stages.

We notice that the sensitivity rate found in this work is very suitable for dealing with small nodules. The mean false-positive rate estimated for the proposed methodology is of approximately 1.17 false positives per slice. This is a high value if compared to other works, but in a qualitative comparison, our methodology is intended for the detection of small nodules (smaller than 10 mm) while other works are mostly concerned with the detection of larger nodules. Detecting small nodules is much more complex than detecting large ones, because they are more dif fi cult to separate from other structures. We believe that this is the main reason for these results.

We used the methodologies of the works by da Silva Sousa et al. (2010) , de Oliveira Campo et al. (2010) and Netto et al. (2012) with the same images database used for training and testing in this paper, in order to make another evaluation of our metho-dology. These works were chosen because the stages of their methodologies are quite similar to ours: (1) segmentation of pulmonary parenchyma, (2) segmentation of structures resem-bling nodules, (3) feature extraction of the segmented structures, and (4) classi fi cation of these structures as nodule or non-nodule.
As one can notice in Table 4 , our methodology keeps having a good performance when compared to those ones. However, it is important to stress that these methodologies were not designed to use small nodules.

Previously cited works ( Tables 3 and 4 ) generally attempt to detect lung nodules (under 3 cm) without emphasizing small nodules. Some of these works report that the method's accuracy is reduced according to the size of the nodules. The main dif in detecting small nodules lies in the fact that they still lack a well-de fi ned shape and composition, can be occluded or can be mistaken for other lung structures (such as blood vessels). Recent work suggests that there is large variation even among radiologists concerning the size of a nodule ( Reeves et al., 2007 ). Our method was designed to detect lung nodules in their early stages, more precisely between 2 and 10 mm in size. Thus, we are looking for nodules with a high degree of complexity and dif fi culty but which, if detected, can increase the patient's survival chances. had been explored in other works for various purposes. However, we are not aware of any study that uses GMM aiming to segment lung nodules, especially small ones. We also never came across the use of
Shannon's and Tsallis's Q entrop y measures to characterize the structures found when discriminating nodules from non-nodules.
These techniques generated good results in our methodology and performed their tasks well.
 of the cited works with the same purpose as in our methodology, and also achieved good performance in their proposed tasks. developing a method that minimizes the problem of detecting small nodules. In the fi eld of engineering, we were able to develop a methodology that is simple, easy to replicate and promising, with techniques that are well described in the literature. 4.4. Comparison of SVM classi fi er with other classi fi ers tested other classi fi ers. In these tests, the same training and testing samples were used, as described in Table 5 . We listed all classi used by the other related works and used them as a reference for our evaluation. WEKA was used to test the classi fi ers. As can be seen, the SVM led to a better balance with respect to sensitivity, speci fi city and accuracy than the others classi fi ers. Overall, SVM had a slightly better performance than the other classi fi 4.5. Case study segmentation of lung nodules applied to some exam slices.
We selected exam number 24 from the LIDC, which contains annotations about the presence of two nodules. The fi rst nodule is more visible in slice 40 and, according to the description in the database, measures approximately 2.2 mm in diameter and has an estimated volume of 88.84 mm 3 . Fig. 7 presents the slices where these nodules appear. In Fig. 8 , we show the result of the segmentation of the pulmonary parenchyma. The result of the segmentation of internal structures is shown in Fig. 9 . Fig. 10 highlights the detection of nodule candidates in the neighborhood of each nodule. Fig. 11 shows the detected nodules.

Next, we present a case where the detection failed. In this example the reconstruction of the pulmonary parenchyma with the structuring element used did not include the region of the nodule near the parenchyma. 4.6. Discussion
The methodology proposed herein, besides being automatic, is based on simple techniques, many of them of easy implementation and, in general, fast. This makes the methodology fast and ef enough to be considered for processing large volumes of data, always taking into account the time constraints involved in any process that concerns human health. Besides, the methodology is composed of successive stages that gradually produce the fi result. The low coupling of these stages makes them easy to expand with the objective of improving results or even dealing with different cases and situations. The time spent by the methodology in 200 splits (average split in the 140 exams used) is promising. In the test set the methodology presented a mean of 3.7 min.

Despite the encouraging results, some problems found in the methodology deserve attention, such as the high false-positive rate and cases in which the detection fails, as happened with some juxtapleural nodules. These factors point to the need to expand and develop this work.

The rates discussed demonstrate that the implementation of the methodology is technically feasible. Concerning the reasons for using it, statistics related to lung cancer clearly indicate that this method can assist in early diagnosis.
 useful as a screening tool for the fi rst set of exams required, indicating suspicious cases that still need to be con fi rmed by medical analysis.
 parts of the world. It is the second most common type of cancer among men and the fourth among women, in many regions of that country. For this reason, this study is useful for early diagnosis, which represents a considerable increase in the survival chances of the patients. In this sense, the proposed methodology can be a useful tool to help specialists in identifying small nodules. network, in some places, lacks specialists and the resources needed to increase staff. Redirecting available quali fi ed profes-sionals to perform less repetitive tasks may contribute to a better use of their abilities. One possibility is to use the proposed methodology in the preliminary analysis of CT exams, leaving the physicists in charge of validating the results.
 tive solution, since it runs on simple computers, which are usually available in hospitals. 5. Conclusion of small lung nodules. The sequence of stages involved, from the acquisition of the images to the indication of the classi nodules, as well as the results, suggest that it is a promising methodology.

The results show that the methodology is capable of ef fi distinguishing lung nodules from other pulmonary structures. This is demonstrated by the sensitivity of 90.6%. On the other hand, the tests pointed out a high rate of false positives (mean of 1.17 false positives per slice), which reduced the overall accuracy of the method to 88.4%. Despite the exciting results, some problems found in the methodology deserve attention, such as the rate of false positives and some failures in detecting juxtapleural nodules, for example. This points to the need to expand and develop this work.

Due to the high rates of sensitivity per exam, this tool can be useful as triage, that is, dealing with the fi rst set of exams in order to identify suspicious cases, but requiring subsequent validation through medical analysis. Finally, the proposed methodology is also an attractive solution, since it runs on common computers. Acknowledgments The authors acknowledge CAPES, CNPq (552108/2011-1) and FAPEMA (PRONEM-01539/11) for fi nancial support.
 References
