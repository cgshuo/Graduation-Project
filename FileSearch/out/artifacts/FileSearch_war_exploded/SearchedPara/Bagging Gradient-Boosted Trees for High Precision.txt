 Recent studies have shown that boosting provides excellent predictive performance across a wide variety of tasks. In Learning-to-rank, boosted models such as RankBoost and LambdaMART have been shown to be among the best per-forming learning methods based on evaluations on public data sets. In this paper, we show how the combination of bagging as a variance reduction technique and boosting as a bias reduction technique can result in very high precision and low variance ranking models. We perform thousands of parameter tuning experiments for LambdaMART to achieve a high precision boosting model. Then we show that a bagged ensemble of such LambdaMART boosted models results in higher accuracy ranking models while also re-ducing variance as much as 50%. We report our results on three public learning-to-rank data sets using four met-rics. Bagged LamdbaMART outperforms all previously re-ported results on ten of the twelve comparisons, and bagged LambdaMART outperforms non-bagged LambdaMART on all twelve comparisons. For example, wrapping bagging around LambdaMART increases NDCG@1 from 0.4137 to 0.4200 on the MQ2007 data set; the best prior results in the literature for this data set is 0.4134 by RankBoost. H.3.3 [ Information Systems ]: Information Search and Re-trieval; H.4.m [ Information Systems ]: Miscellaneous X  Machine Learning Algorithms, Experimentation Learning-to-rank, Tree Ensembles, Bagging
The general problem of ranking, and in particular rank-ing of web search results, has received significant attention. Given the large amount of training data now available it has become possible to learn effective ranking models using machine learning. Methods that learn how to combine pre-defined features for ranking are called  X  X earning-to-rank X  methods. A large number of learning-to-rank algorithms have been proposed, such as [8, 7, 9, 21, 28, 30] (see [22] for a more complete list).

Recent studies have shown that tree models combined with ensemble techniques provide excellent predictive per-formance. In the recent Yahoo! learning-to-rank challenge [10], the top ranked teams all used tree ensemble methods. The winning entry in this competition used an ensemble of LambdaMART [32] models.

In this work, we study the effectiveness of bagged ensem-bles of ranking models to achieve higher prediction accuracy. We train multiple independent ranking models on different samples of the training data and combine the outputs of these models to get improved prediction accuracy. In our experiments, we use LambdaMART, which is a boosted en-semble of trees, as the baseline model. Thus we are forming bagged ensembles of boosted ensembles of trees. Our ap-proach is motivated by Breiman X  X  Bagging [3], so we call this method BL-MART for B agged L ambda MART . Since sub-models are independent they can be trained in parallel and the training time is not more than training time of a single LambdaMART model. In fact, because we perform sampling without replacement, each of the LambdaMART models is trained on a smaller training set and therefore the training time is less than training a single LambdaMART model on the full train set.

We summarize the main contributions of this paper as fol-lows: (a) We perform thousands of parameter tuning experiments for the LambdaMART algorithm to find near optimal pa-rameter configurations for it and also study its sensitiveness to its parameters (section 3.4). (b) We show that adding randomness during the training of LambdaMART models can improve the accuracy of these models. We introduce randomness by sub-sampling queries that are available to the algorithm during each of the train-ing iterations. We also use feature sampling as another method for increasing randomness of the algorithm (section 3.3). (c) We study the effectiveness of bagging ensembles of Lamb-daMART models to both increase the prediction accuracy of the ranking model and reduce the model variance. (d) We show that bagged ensembles of overfitted base-level models (overfitted boosted tree models) result in higher pre-diction accuracy than bagging optimally generated base-level models and we explain why this happens.
LambdaMART [32] is a ranking algorithm that uses Gra-dient boosting [15] to optimize a ranking cost function simi-lar to the LambdaRank [7] cost function. Readers can refer to [6] for details of this algorithm. However, since it is our base model, we briefly describe some of the important con-cepts that are referenced in the reminder of this paper.
Gradient boosting produces an ensemble of weak mod-els (typically regression trees) that together form a strong model. The ensemble is built in a stage-wise process by per-forming gradient descent in function space. The final model maps an input feature vector x  X  R d to a score F ( x )  X  R : where each h i is a function modeled by a single regression tree and the  X  i  X  R is the weight associated with the i  X  X h regression tree. Both the h i and the  X  i are learned during training. A given tree h i maps a given feature vector x to a real value by passing x down the tree, where the path (left or right) at a given node is determined by the value of a particular feature in the feature vector and the output is a fixed value associated with the leaf that is reached by following the path.

Gradient boosting usually requires regularization to avoid overfitting. In an overfitted model, the model X  X  generaliza-tion ability degrades because of fitting too closely to the training data. Different kinds of regularization techniques can be used to reduce overfitting in boosted trees. One com-mon regularization parameter is the number of trees in the model, M . Increasing M reduces the error on training set, but setting it too high often leads to overfitting. An optimal value of M often is selected by monitoring prediction error on a separate validation data set.

Another regularization approach is to control the com-plexity of the individual trees via a number of user-chosen parameters. For example, Max Number of Leaves per tree limits the size of individual trees thus preventing them from overfitting to the training data. Another user X  X et parameter for controlling tree size is the minimum number of observa-tions allowed in leaves. This parameter is used in the tree building process by ignoring splits that lead to nodes con-taining fewer than this number of training set observations. This prevents adding leaves that contain statistically small samples of training data.

Another important regularization technique is shrinkage which modifies the boosting update rule as follows: where parameter  X  is called the learning rate . Small learn-ing rates can dramatically improve a model X  X  generalization ability over gradient boosting without shrinkage (  X  = 1), however they result in more boosting iterations and there-fore larger models.
 According to the bias-variance decomposition of error [16], the squared error of a single example x can be decomposed into the sum of three non-negative terms: noise , bias , and variance . The first term, Noise, is the irreducible error which cannot be avoided regardless of the learning algo-rithm. In learning-to-rank domain, it can be interpreted as the noise in relevance judgments of query-url pairs. The second term, Bias, measures how closely the average pre-diction of the learning algorithm (considering all possible training sets of a fixed size) matches the optimal prediction (the Bayes rate prediction). Finally, the Variance of an al-gorithm is how much the algorithm X  X  prediction fluctuates over different possible training sets of a given size.
Ensemble methods such as Gradient Boosting [15] reduce bias by increasing the expressive power of the base learner and by forcing learning to attend to training cases that con-sistently are mispredicted. Because boosting combines the predictions of multiple trees it also can reduce variance, but boosted trees are so powerful that regularization usually is needed to prevent overfitting. Other ensemble methods such as Bagging [3] mainly reduce variance by averaging outputs of several models trained on different samples of training data. Because bagging usually does not significantly increase expressive power and often yields large reductions in vari-ance, it is a relatively safe procedure that usually does not require regularization or careful parameter tuning. In fact, bagging can be used as an effective regularization method when wrapped around other high-variance learning meth-ods that are prone to overfitting such as boosting. There has been attempts for combining bias and variance reduc-tion techniques for classification [31, 29] and regression [5, 14, 26, 27] problems. In this work, we combine bagging and boosting for improved learning-to-rank.

Similar to us, BagBoo [23] wraps Bagging around Boost-ing for improved learning-to-rank. However, there are sev-eral fundamental differences that make our work different from BagBoo. The most important difference is the size of the final model. On two public data sets that BagBoo re-ports its results, our BL-MART models are approximately 250 times smaller and also more accurate on most metrics (section 4). BagBoo is training more than a million trees on these data sets which makes its size infeasible for real time applications. In addition, the boosting algorithm that is used in BagBoo is a pointwise method, while BL-MART uses LambdaMART for boosting which is a listwise algo-rithm. Pointwise methods such as [12, 21] do not exploit relative rank information between documents, instead at-tempting to directly create a scoring function. In contrast, listwise methods such as [7, 9, 30] use lists of ranked doc-uments as instances during training, and learn a ranking model by minimizing a listwise loss function. In addition, we also show that bagging boosted ensembles that are mildy overfitted to their training data gives better results. For our experiments we work with three public data sets: TD2004 and MQ2007 from LETOR data sets [24] and the recently published MSLR-WEB10K data set from Microsoft Research [1]. Table 1 summarizes the properties of these data sets. The TD2004 and MQ2007 data sets have been used many times for evaluating new learning-to-rank algo-rithms. This provides a baseline for comparing our method with other state-of-the-art learning-to-rank algorithms. MSLR-WEB10K is a large data set which is more similar to com-mercial search data sets. Because it is larger it should pro-vide results that are more reliable. All three data sets are pre X  X olded and come with evaluation scripts that allow fair comparison of different ranking algorithms.
For model comparison we use two information retrieval metrics: Normalized Discounted Cumulative Gain (NDCG) [19] and Mean Average Precision (MAP) [2]. NDCG@k is a measure for evaluating top k positions of a ranked list using multiple levels of relevance judgment. It is defined as follows, where N  X  1 is a normalization factor chosen so that a per-fect ordering of the results will receive the score of one; r denotes the relevance level of the document ranked at the j -th position; g ( r j ) is a gain function: and d ( j ) denotes a discount function. The evaluation scripts that come with the three data sets use the following discount function:
W e use the same scripts for fair comparison of our final models with other algorithms. However in our implementa-tion of the LambdaMART algorithm and in all of our train-ing and parameter tuning experiments, we optimize for the following discount function which places stronger emphasis on higher positions: Also, based on whether we assign NDCG values of zero or one to a query where all of the documents are assigned non X  relevant labels, the scale of NDCG values will change.
In [14], Friedman proposed a modification of the gradi-ent boosting algorithm which was motivated by Breiman X  X  bagging method. He proposed that at each iteration of the algorithm, a base learner should be fit on a sub-sample of the training set drawn at random without replacement. Fried-man observed a substantial improvement in gradient boost-ing X  X  accuracy with this modification. Sub-sample size is some constant fraction s of the size of the training set. When s = 1, the algorithm is deterministic. Smaller values of s introduce randomness into the algorithm and help prevent overfitting, acting as a kind of regularization. The algorithm also becomes faster, because regression trees have to be fit to smaller data sets at each iteration.

Also similar to Random Forests [4], more randomness can be introduced by sampling features that are available to the algorithm on each tree split. On each split, the algorithm Table 2: Values used in grid search for parameter tuning Parameter Values Max Number of Leaves 2, 4, 7, 10, 15, 20, 25 Min P ercentage of Obs. per Leaf 0.12, 0.25, 0.50 Learning rate 0.05, 0.1, 0.2, 0.3 Sub-sampl ing rate 0.3, 0.5, 1.0
Feature Sampl ing rate 0.1, 0.3, 0.5, 1.0 selects the best feature from a random subset of features instead of the best overall feature.

In our experiments, we add both observation sub-sampling and feature sampling as two new parameters that need to be tuned for the LambdaMART algorithm. These parameters can take values between 0 and 1, where 1 means no sampling and values less than 1 introduce sampling randomness.
The original LambdaMART algorithm has three parame-ters that need to be tuned to achieve the best results:  X  X ax number of leaves X ,  X  X in percentage of observations per leaf X , and  X  X earning rate X . These were described in section 2. As mentioned above, to these we have added two new parame-ters:  X  X ub-sampling rate X  and  X  X eature sampling rate X . We use grid search to test 1,008 different combination of param-eters on the smaller data sets and 162 combinations on the larger data set. Table 2 shows the values we tried for each of the parameters. Since MSLR-WEB10K contains more fea-tures for each query-url pair, we need more complex trees (trees with more leaves) on this data set.

Each combination of parameters is tested on 5 folds of each data set. On each fold we use 3 different random seeds to get more accurate results. This requires 1 , 008  X  5  X  3 = 15 , 120 experiments on each of the smaller data sets and 162  X  5  X  3 = 2 , 430 experiments on MSLR-WEB10K. We used a MapReduce cluster of 40 nodes for these experiments. Map tasks receive experiments as input and compute valida-tion and test NDCG for the configuration specified by that experiment. Reduce tasks perform NDCG averaging over different folds and random seeds for each parameters com-bination. It takes about 8 hours to run this portion of our experiments once on this cluster.
 Table 3 shows the best configurations based on Validation NDCG@3 on each data set. The best performing configura-tions on all three data sets use feature sampling. The smaller data sets also get better results by sub-sampling of training queries on each iteration. We conjecture that sub-sampling queries helps when the training data is small because it helps avoid overfitting by not allowing trees to see all queries on each iteration of boosting. This adds diversity to the indi-vidual trees which is then reduced when boosting averages tree predictions. With very large data sets this is less criti-cal b ecause individual trees cannot themselves significantly overfit a large data set when tree size is limited.
Grid search for parameter tuning is computationally ex-pensive and becomes prohibitive as the data sets become large. Because of this it is useful to study the sensitivity of the LambdaMART algorithm to its parameters to have a better understanding of the number of training experiments needed on a new data set. We use the results of our pa-rameter tuning experiments to study the effect of number of experiments on improvement in NDCG. For each dataset, we create a pool of configurations that we evaluated during parameter tuning experiments. Then we randomly select different numbers of these configurations. On each random selection, we pick the config with best validation NDCG@3 and then record validation and test NDCG@3 for that con-fig. To get more accurate results, we repeat this random process 10K times and report average NDCG@3. Figure 1 shows the results.

As expected, for all three data sets, validation NDCG im-proves monotonically as we perform more experiments. On MSLR-WEB10K, the largest data set, there is less discrep-ancy between validation and test scores. The discrepancy between validation and test is largest on TD2004, the small-est data set, because the validation sets which are held aside form the training data must also be small.

Given that the parameter values we had chosen for grid search where chosen based on our experiments with Lamb-daMART on different data sets, we were expecting the pa-rameter tuning experiments to reach to the pick value on test set after trying few combinations. On TD2004, the data set with the smallest validation sets, accuracy on the test set peaks after only about 100 parameter configura-tions, and then slowly drops. Accuracy on the validation set is still rising at 100 iterations, suggesting that hyperpa-rameter optimization is overfitting to the validation sets. A similar effect is observed on MQ2007, but overfitting does not begin on this problem until about 400 parameter con-figurations have been tried. And on MSLR-WEB10K, we again observe overfitting to the validation sets after fewer than 25 configurations have been tested.

When there is randomness in the algorithm and the vali-dation sets are not infinite, as more parameter combinations are tried search begins to find parameter combinations that look better on the validation set because of this random-ness. If one is not careful, the computational power pro-vided by MapReduce Clusters is so great that it is possible to overdo parameter tuning and find parameter combina-tions that work not better than the hyperparameters that would have been found by less thorough search. One way to avoid overfitting at the hyperparameter learning stage is to use a 2nd held-out validation set to detect when parameter tuning begins to overfit and early-stop the parameter opti-mization. Holding out a 2nd validation set will reduce the size of the primary hyperparameter tuning validation sets, making overfitting more likely. But as we have seen, even large cross-validated validation sets do not completely pro-tect from overfitting when hyperparameter optimization is eventually occurs. exhaustive, so care must be exercised to prevent hyperpa-rameter optimization from becoming counterproductive. We do not directly control the best number of trees for the LambdaMART models via a user-set parameter. Instead, as iterations of boosting continue, the prediction accuracy of the model is checked on a separate validation set. Boosting continues until there has been no improvement in accuracy for 250 iterations. The algorithm then returns the number of iterations that yielded maximum accuracy on the validation set.
 Figure 2 shows two sample runs on MQ2007 and MSLR-WEB10K data sets. While NDCG@3 continues to improve on the training set, it reaches its maximum on validation sets after a few hundred iterations on both data sets. It is also interesting that we do not see significant drop in NDCG on validation sets even after 2000 iterations. This suggests that the combination of regularization techniques we use is suffi-cient to prevent models from overfitting on these datasets.
As mentioned before, for bagged LambdaMART we train many LambdaMART models in parallel and then aggregate their outputs for improved accuracy. In order to have more diverse models in the final ensemble, we randomly sample training data (without replacement) for training each of the LambdaMART sub-models. The randomization techniques discussed in section 3.3 also contribute additional diversity to the models.
For combining scores of the sub-models, one can simply take average of their scores. However, the scale and dis-tribution of the scores generated from LambdaMART al-gorithm is dependent on the training data and on smaller training data can be very different for models trained on different sub-samples. Because of this, simply averaging LambdaMART scores may not give the best results. As an example, Figure 3 shows the distributions of validation scores generated from two models that are trained on differ-ent random samples of MQ2007 and MSLR-WEB10K train-ing data. While on the larger data set, the output scores have very similar distributions, on the smaller data set we observe difference distributions and different scales of the scores. Note that even on the MSLR-WEB10K data set the output score of individual queries do not necessarily have the same scales across different models. The reason that we see a final normal-like distribution on this data set is that scores of individual queries have close to uniform distributions with different scales and result in a normal-like distribution once aggregated.

Since scores of different sub-models are on different scales, we initially tried using Borda count [18] as a rank aggrega-tion technique. Each of the LambdaMART sub-models is used for generating a ranking of the test data. For each ranking, a score of 0 is assigned to the lowest ranked result, 1 to the next-to-lowest result, and so forth; then the to-tal score of each result is computed over all the sub-models and the ensemble orders documents by this score. However, since Borda count converts real valued scores of documents to integer value ranks and then combines these ranks, the possibility of having ties increases significantly when averag-ing only have a few LambdaMART sub-models. Since tied documents would have to be ordered arbitrarily, this can sig-nificantly reduce the performance of the bagged model. Our experiments showed that Borda count works well when used to aggregate ranks of many model, but performs even worse than a single model when applied to only a few sub-models.
A better approach for combining the outputs of different sub-models is to normalize the scores of documents gener-ated by each sub-model for each query. For each query we linearly scale the scores of its corresponding documents such that the document with the highest score will have a score of 1.0 and the document with the lowest score will have a score of 0.
As mentioned in section 2, prediction error can be re-duced by reducing both bias and variance. When bagging single decision or regression trees, bagging typically results in higher accuracy if applied to unpruned trees. The rea-son is that unpruned trees are overfitted to their training data and therefore have low bias and high variance. Since bagging is a variance reduction technique, it can reduce this variance so that the final bagged model has both low bias and low variance and thus lower total prediction error. Sim-ilarly, we may get better results by bagging boosted tree the validation set.
 different random samples of training data. models that have less bias but more variance. We test the effectiveness of this idea by adding an overfitting tolerance as a new parameter to the LamdaMART algorithm.

To generate overfitted LambdaMART models, we mon-itor the prediction accuracy on a separate validation set. Without overfitting, we would select the first N trees that results in the maximum prediction accuracy on the valida-tion set. With overfitting tolerance, we add more trees af-ter this maximum point until accuracy drop is more than a threshold value. We experimented with different thresh-old values and found that 2% worked well across the three data sets. As Figure 2 shows, it is possible that even after adding hundreds of additional trees, the drop in prediction accuracy on validation set is small. To prevent the size of the overfitted model from growing excessively large in this situation we allow a maximum of 250 trees to be added after the maximum point has been reached on the validation set. It is possible that allowing more overfitting would further improve the results from bagging, but at the expense of a much larger model.
BL-MART combines LambdaMART sub-models in order to achieve higher accuracy. There is a trade-off between in-creasing accuracy and model complexity: as we add more sub-models, the gain in accuracy is offset by the final model becoming too complex. It is important to balance the num-ber of sub-models needed to achieve a reasonable gain in accuracy with the need to control complexity so that the models are still feasible to use in practice. To do this, we train BL-MART with different number of sub-models and evaluate its performance on the validation sets. We first create pools of LambdaMART models for each of the folds of the three data sets and then use these pools of mod-els during the bagging process. We used pools of size 1000 models for TD2004 and MQ2007 data sets and 50 models for MSLR-WEB10K data set. The reason is that on the smaller data sets variance is higher and bagging needs to add more models before accuracy asymptotes.

We create different pools for overfitted and non-overfitted models. Overall, we need to train 10,000 LambdaMART models on each of the smaller data sets and 500 models on MSLR-WEB10K data set. We used a MapReduce cluster for this purpose. Each mapper task is assigned a fold in one of t he data sets. It then randomly selects 67% of the training queries from that fold and generates a LambdaMART model on this random sample. Once the model is created, it evalu-ates the scores of validation and test data corresponding to that fold and passes these scores to reducer tasks. Reducer tasks just dump the scores that they receive. It takes about 18 hours to generate output scores for these models on a cluster of 40 nodes.

To study the effectiveness of using overfitted models in the bagging process, we create bagged models of different sizes from overfitted and non X  X verfitted models and then compare their MAP. Since we use a pool of models and each bagged model is created by randomly selecting a subset of models from these pools, we repeat the random process of bagged model creation 100 times and compute average validation MAP of these bagged models to have more reliable results. Figure 4 shows the results confirming that using overfitted models results in better accuracy for the final bagged en-semble. On MSLR-WEB10K data set, we need to include more models in the bagged ensemble before overfitted mod-els show better results.

We use the same results of Figure 4 for determining the number of models that should be included in the bagged ensemble. In order to have a balance between accuracy and model complexity, we picked 45 models on smaller data sets and 20 models on the larger data set.

To compare the performance of BL-MART model with the original LambdaMART model and other learning-to-rank al-gorithms, we create BL-MART models for each of the data sets. Since, we create bagged models by random selection of sub-models from our model pools, we repeat this pro-cess for 20 times and report average results. Similarly, we use 20 different random seeds for  X  X ambdaMART with ran-domization X  and report average results. The original Lamb-daMART algorithm is deterministic and we only need to run it once. Since LambdaMART code is not publicly available, we re-implemented it for our experiments.
Table 4 summarizes the results of our experiments ana-lyzing the accuracy of BL-MART models in comparison to LambdaMART and other learning-to-rank algorithms. On TD2004 and MQ2007 all prior published results are included in the table for four metrics. For the new MSLR-WEB10K data set no other results have yet been published. On TD2004 data set, BL-MART significantly outperforms LambdaMART on all four metrics. Also bagging overfitted models results in better performance on this data set. In comparison to other learning-to-rank algorithms, BL-MART performs best in terms of MAP and NDCG@5, but on NDCG@1 and NDCG@3 RankBoost and BagBoo are slightly better. It should be noted that we have limited the size of BL-MART to control complexity. For example, on this data set, BL-MART contains about 4,500 trees while BagBoo is using 1.1 million trees which is 250 times larger than our model.
On MQ2007 data set, BL-MART again significantly out-performs LambdaMART on all metrics and it also achieves the best results compared to other learning-to-rank algo-rithms which have reported their results on this data set.
On the new MSLR-WEB10K data set, BL-MART again improves LambdaMART performance on all of the metrics. However, on this data set the difference between bagging of overfitted and non X  X verfitted models is not statistically significant.

Across the three problems and four metrics, BL-MART with overfitting improves accuracy an average of 2.6% when compared to LambdaMART with randomization.

While we have reported the best results (as far as we know) compared to other ranking algorithms on TD2004 and MQ2004 data sets, it should be noted that we did not tune the LamdaMART models for bagging. We tuned a sin-gle LambdaMART model and then used the same set of best parameters in the bagged model. It might be possible to get better results by bagging LambdaMART models which are generated from different set of parameters. For example, using more complex trees in sub-models might lead to over-fitting which shows to be a desirable property for sub-models of a bagged ensemble. However, tuning the bagged model would have required much more experiments.
Variance can hurt in several ways: 1) by increasing the variance term in the bias/variance decomposition it reduces the expected accuracy of the trained models; 2) by increas-ing uncertainty it increases the number of experiments that must be run to determine which learning method and pa-rameters yield the best results; and 3) it creates risk when a final model must be selected to deploy. All other things be-ing equal (e.g. expected accuracy across multiple trials), the lower-variance learning method is prefered. For example, re-ducing variance by half reduces the number of experiments that must be run to pass a t-test by exp ected loss of the single deployed model compared to the expected loss of a typical model by a factor of 2. Because of this, learning methods that exhibit high variance can be difficult to work with, and learning methods that have lower variance are safer to deploy and easier to use for new fea-ture development. Boosting often has relatively high vari-ance. Bagging, however, is an effective variance reduction method. Thus we expect BL-MART to have significantly lower variance than LambdaMART.
 In order to compare the variance of LambdaMART and BL-MART models, we use the MSLR-WEB10K data set. We randomly select 10 samples from the training data of Fold1 of this data set. Each sample contains a random se-lection of 67% of the training queries in this fold. We then train LambdaMART and BL-MART models on each of these samples and evaluate these models on the test data of this fold.

Table 5 shows the NDCG and MAP scores and their cor-responding variance for each of the models. If we compare  X  X ambdaMART with randomization X  with  X  X L-MART with overfitting X , the following would be the reduction in variance for different metrics:
Wrapping bagging around LambdaMART boosting yields two distinct benefits: 1) it achieves accuracy comparable to trials.

LambdaMART 0.4484 18  X  10  X  6 0.4395 9 . 1  X  10  X  6 0.5640 1 . 2  X  10
LambdaMART with randomization 0.4492 22  X  10  X  6 0.4421 5 . 4  X  10
BL-MART without overfitting 0.4516 10  X  10  X  6 0.4468 7 . 8  X  10
BL-MART with overfitting 0.4528 7  X  10  X  6 0.4471 4 . 4  X  10 and often better than the current state of the art in learning-to-rank; and 2) it achives this very high accuracy while si-multaneously reducing variance. Taken individually, each of these is a reason to be interested in BL-MART. Together, however, they represent a substantial step forward.
For those interested in delivering high accuracy search re-sults at commercial search engines, the reduction in variance may be the more important result. In commercial search en-gines most gains come not from improved learning methods, but from developing new or improved features (and data) to feed into learning. Feature and data refinement requires frequent experimentation to determine if the refinement is better and should be released. Reduced learning variance makes these experiments easier and more reliable. Because bagging can easily be parallelized across multiple comput-ers, it allows one to obtain low variance experimental results with little increase in wall clock time. That these results also will have state-of-the-art accuracy makes the method even more appealing.

The final output of our BL-MART method is a high preci-sion and low variance bagged ensemble of models. However, in real-time applications such as search engines, it may not be feasible to use such large models. We are currently work-ing on compression pruning techniques that allow dropping a large fraction of trees from the final model without sig-nificantly impacting the accuracy. This is possible because we find empirically that large shrinkage is necessary to pre-vent massive overfitting in LambdaMART models. When shrinkage is high, boosting sometimes needs to generate a sequence of nearly identical trees to do what it might have accomplished with one tree without shrinkage. Yet in other places shrinkage is critical and subsequent trees are quite different from each other. By deleting and reweighting trees after the full LambdaMART ensemble has been grown, we can determine post-facto which trees are redundant and con-tribute little to the model, and which trees are critical for the model to have high accuracy. The method is currently under development, but it looks like we can also exploit the fact that bagging boosted trees generates even more reduntant trees that can safely be eliminated afterwards with little or no loss in accuracy. For example, in one set of experiments we are able to prune away 40 X 80% of the trees without sig-nificant loss in NDCG.

To run experiments, we developed a platform called jforests that not only implements LambdaMART and BL-MART, but which supports many tree-based learning methods such as bagging, boosting, regression, classification, ranking, etc. The platform is written in such a way to make it easy to code additional tree-based methods with minimum effort. For example, AdaRank and RankBoost can each be imple-mented in less than a dozen lines of new code. Our code plat-form including the implementations of LamdaMART and BL-MART will be made publicly available. The jforests platform also supports running on MapReduce environments to allow parallelization of methods such as bagging.
We present new results for LambdaMART, a state-of-the-art learning-to-rank algorithm, on three public data sets. We show that wrapping bagging around a boosting X  X ased ranking model can improve its performance while also signif-icantly reducing model variance. In our experiments, bagged LambdaMART (BL-MART) increased NDCG@1, NDCG@3, Mean NDCG, and MAP on all three test problems com-pared to un-bagged LambdaMART. Moreover, bagging re-duced variance an average of 46% across all metrics on the MSLR-WEB10K data set on which we measured variance. Most of the ideas and methods reported in this paper are general and not limited specifically to ranking. For exam-ple, our finding that overfitting boosted models that will be bagged improves accuracy can be used in other classification and regression problems to further improve performance. Authors would like to thank Amazon for a research grant that allowed us to use their MapReduce cluster. This work has been also partially supported by NSF grant OCI-074806. [1] Microsoft learning to rank datasets. [2] R. Baeza-Yates and B. Ribeiro-Neto. Modern [3] L. Breiman. Bagging predictors. Mach. Learn. , [4] L. Breiman. Random forests. Machine Learning , [5] L. Breiman. Using iterated bagging to debias [6] C. Burges. From RankNet to LambdaRank to [7] C. J. C. Burges, R. Ragno, and Q. V. Le. Learning to [8] C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, [9] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. [10] O. Chapelle, Y. Chang, and T.-Y. Liu. The Yahoo! [11] O. Chapelle and M. Wu. Gradient descent [12] K. Crammer and Y. Singer. Pranking with ranking. In [13] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An [14] J. H. Friedman. Stochastic gradient boosting. [15] J. H. Friedman. Greedy function approximation: A [16] S. Geman, E. Bienenstock, and R. Doursat. Neural [17] R. Herbrich, T. Graepel, and K. Obermayer. Large [18] T. K. Ho, J. Hull, and S. Srihari. Decision [19] K. J  X  arvelin and J. Kek  X  al  X  ainen. IR evaluation methods [20] T. Joachims. Training linear SVMs in linear time. In [21] P. Li, C. J. C. Burges, and Q. Wu. Mcrank: Learning [22] T.-Y. Liu. Learning to rank for information retrieval. [23] D. Y. Pavlov, A. Gorodilov, and C. A. Brunk. [24] T. Qin, T.-Y. Liu, J. Xu, and H. Li. LETOR: A [25] D. Sculley. Combined regression and ranking. In [26] D. Sorokina, R. Caruana, and M. Riedewald. Additive [27] Y. L. Suen, P. Melville, and R. J. Mooney. Combining [28] M.-F. Tsai, T.-Y. Liu, T. Qin, H.-H. Chen, and W.-Y. [29] G. Valentini and T. G. Dietterich. Low bias bagged [30] M. N. Volkovs and R. S. Zemel. Boltzrank: learning to [31] G. I. Webb. Multiboosting: A technique for combining [32] Q. Wu, C. Burges, K. Svore, and J. Gao. Ranking, [33] J. Xu and H. Li. AdaRank: a boosting algorithm for [34] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A
