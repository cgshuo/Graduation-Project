 This work takes an in-depth look at the factors that affect manual classifications of  X  X emporally sensitive X  information needs. We use qualitative and quantitative techniques to an-alyze 660 topics from the Text Retrieval Conference (TREC) previously used in the experimental evaluation of temporal retrieval models. Regression analysis is used to identify fac-tors in previous manual classifications. We explore potential problems with the previous classifications, considering prin-ciples and guidelines for future work on temporal retrieval models.
A growing body of information retrieval research argues that temporality should be modeled explicitly when scoring and ranking documents with respect to users X  queries. Re-searchers have explored a variety of temporal retrieval mod-els that explicitly incorporate time into document ranking [7, 2, 1]. They refer to general classes of  X  X emporal queries X  or  X  X emporal information needs. X  Models have been pro-posed for  X  X ecency queries X  [7, 2],  X  X ime-sensitive queries X  [1],  X  X mplicitly temporal queries X  [8], and  X  X emporally biased queries X  [5]. For evaluation, these studies rely on manual classifications of topics into temporal categories.
In this short paper, we take a deeper look into these manually classified topics to develop a clear understand-ing of what makes a query temporally sensitive ? Previous manual classifications combine the temporal distribution of judged-relevant documents with common-sense notions of topic temporality without a clear explanation of the crite-ria or processes used in classification. If we cannot explain the processes being modeled, use of these manually classified topics for evaluation is of limited value.
 To address this question, we analyze 660 topics from the Text Retrieval Conference (TREC) previously used in the experimental evaluation of temporal retrieval models. We employ qualitative and quantitative methods to identify topic characteristics that might affect the manual assessment of  X  X emporal-sensitivity. X  The resulting coded topics are used in a set of regression analyses to assess the relationships between these characteristics and manually assigned cate-gories. This paper X  X  main contribution is an empirical assess-ment of the complexities that underpin temporal IR. This assessment helps us understand earlier temporal IR studies, while also suggesting novel ways to incorporate time effec-tively into retrieval. Table 1: TREC topics and collections analyzed by the au-thors and their use in prior temporal retrieval studies.
In this section, we review examples of studies focused on temporal relevance . The list of topics and collections used in each of the studies are listed in Table 1.

Jones and Diaz [5] define three classes of queries based on the manual analysis of topics: temporally ambiguous (requesting multiple events), temporally unambiguous (re-questing a single event), and atemporal (having no prefer-ence). Jones and Diaz manually classify 100 TREC topics based only on their title, description, and narrative fields. They also include 2003 Novelty track topics because they include topics classified as  X  X vent X  or  X  X pinion, X  which the authors suggest correspond to the  X  X emporally unambigu-ous X  and  X  X temporal X  categories, respectively.

Efron and Golovchinsky [2] investigate models for recency queries. Topics are classified as  X  X ecency X  if at least 2/3 of the relevant documents occur after the median document time and the topic has a  X  X ona fide temporal dimension X  based on manual review, the specific criteria for which are not specified. The resulting classification consists of 61 re-cency queries.

Dakka, Gravano, and Ipeirotis [1] investigate a broad class of queries which they refer to as  X  X ime-sensitive. X  They hy-pothesize that there are queries for which more relevant doc-uments are found at specific points in time, not just recently. They manually examine the title, description and narrative of each topic and identify queries associated with specific news events. If the topic information is insufficient to make a decision, they analyze the distribution of judged-relevant documents. The resulting classification consists of 86 tem-porally sensitive queries.

Finally, Peetz, Meij, and de Rijke [9] investigate the effect of temporal bursts in estimating query models. Building on the earlier studies, they evaluate their models using the pre-vious manual classifications as well as a new collection based on TREC Blog06. As in the previous studies, the authors manually construct a subset of  X  X emporal X  queries based on topic descriptions and relevant document distributions. No specific criteria for classification are given.
Given the complex landscape described in the previous section, what in general makes a query temporally sensi-tive? Dakka et al [1] present a compelling definition. A query is temporally sensitive if  X  X he relevant documents for the query are not spread uniformly over time, but rather tend to be concentrated at restricted intervals. X  This is an essential point, since many temporal retrieval models rely on the temporal distribution of results in document scor-ing. This is also why we do not include the topics developed for the NTCIR Temporalia test collections [4]: they are pri-marily concerned with temporal topicality , i.e. queries about a certain time, in contrast to our focus on temporal sensi-tivity, which relates to the unequal occurrence of relevant documents at certain points in time. Still, the distribution of relevant documents alone is not sufficient to determine true temporality. To address this, most of the studies listed above rely on common-sense notions of temporality based on the topic content considered independently of the distri-bution of relevant documents. A primary goal of the current study is to look deeper into these common-sense criteria with the aim of providing researchers a firmer basis for assessing which queries are likely to have a temporal dimension. We use content analysis [6] to identify characteristics of TREC topics potentially associated with temporal sensitiv-ity. 660 topics were selected from the TREC Ad-hoc, Nov-elty, Blog, and Microblog tracks, all previously used by re-searchers to evaluate temporal retrieval models. The com-plete set of topics used in this study are listed in Table 1 along with the temporal constraints of each collection or sub-collection.

Two of the authors participated in the development of the codebook and subsequent coding of topics. Codes were de-fined based on characteristics of topics expected to be related to temporal sensitivity, informed by the literature. During this process, code definitions were refined and clarified. In the final coding, only topic title and description were used. Of the 660 topics, 330 were coded by both coders to allow for inter-rater consistency analysis. The final codebook is too large to publish in this short paper, but is available on-line 1 . Coding was completed using the Dedoose 2 service. After coding all 660 topics, the topic/code matrix was ex-ported for subsequent reliability and regression analysis, as described in the following sections.

An example of a coded topic from the 2004 Novelty test collection is presented in Figure 1. This topic refers to a specific event and contains place entities as well as an ex-plicit date. Topic N57 is categorized as an  X  X vent X  by the TREC topic creator and is therefore an unambiguous tem-poral topic as defined by Jones and Diaz.
 Figure 1: TREC Novelty 2004 topic N57 example annotation
In addition to coding the topics based on the defined codes, the coders assigned a temporal designation to the distribution of relevant documents for each topic. Non-parametric densities were fit to the temporal distribution of relevant documents for topics with more than 20 rele-vant documents, following Dakka et al [1]. Each coder re-viewed the relevant document distribution along with the total number of relevant documents for each topic and as-signed one of four values based on subjective impressions about the degreee to which relevant documents were tem-porally constrained: too few observations (-1), low or no temporality (0), moderate temporality (1), and high tempo-rality (2).
For this study, coder agreement is measured using Co-hen X  X   X  for the classification of the distribution of relevant documents. For the broader qualitative coding task, we use a variation of percent overlap, since coding is performed on arbitrary segments of text. We define percent overlap as: Where m is the number of excerpts assigned the same code by both coders, u 1 is the number of codes assigned to ex-cerpts only by coder 1 and u 2 is the number of codes assigned to excerpts only by coder 2. If both coders assign no codes to a topic, it is considered perfect agreement. We report the macro (calculated over all topics) and micro (calculated as a per-topic average) overlaps. Per-code overlaps are used to characterize coder agreement within each code.
In each of the four prior studies enumerated in Section 2, the authors acknowledge using the distribution of judged-relevant or pseudo-relevant documents in determining topic temporality. For this study, we use two different measures to analyze these distributions: the first-order time series auto-correlation (ACF) and the dominant power spectrum (DPS).
Jones and Diaz [5] use the ACF created by the temporal distribution of pseudo-relevant documents for a query as a predictor of query temporality. They note that queries with http://github.com/craig-willis/sigir-2016-queries/codebook http://www.dedoose.com strong inter-day dependencies will have high ACF values, indicating predictability in the time series.

Similarly, He, Chang, and Lim [3] use the DPS as a pre-dictor of the  X  X urstiness X  of temporal features for event de-tection. The DPS is the highest power spectrum, estimated using the periodogram.

In this study, both ACF and DPS measures are used to reduce the distribution of judged-relevant or pseudo-relevant documents to a single value for the regression analysis, as described in the next section.
A primary goal of this study is to determine the char-acteristics that contribute to the manual judgment of topic temporality. We use logistic regression based on the general-ized linear model (GLM) implementation in R. The predic-tors are binary presence indicators for each of the qualitative codes along with the ACF and DPS of the temporal distri-bution of true-relevant documents. The response variables are the binary temporal/non-temporal indicators manually assigned in the four studies. Model variables are selected using standard step-wise procedures based on the Akaike information criterion (AIC). Coefficients are reported using the log-odds and model fit is assessed using pseudo-R 2 .
Our qualitative analysis suggests that three broad classes of features bear on query temporality: events, named enti-ties, and explicit dates. It is intuitive that topics focused on specific and important events will have a higher degree of temporal relevance. Following the Topic Detection and Tracking definition, seminal events happen at specific times in specific places, often to individuals or other named enti-ties (e.g., organizations). Perhaps the most essential code is the  X  X pecificEvent X   X  something important that happens at a particular time and place. Related to SpecificEvent is the  X  X eriodicEvent, X  which refers to an event that recurs periodically, such as the Super Bowl, World Cup, or Hal-loween. Jones and Diaz [5] note that many of the early ad-hoc queries were temporally ambiguous, referring to multiple events. We incorporate this concept through the  X  X ener-icEvent X  code, which captures topics concerned with a class of specific events, such as earthquakes, elections, or strikes. While analyzing topics, it became apparent that some topics were likely to be inspired by a specific event, but without explicit reference in the topic description. This concept is captured through the  X  X ndirectEventReference X  code. The remaining codes are concerned with the identification of spe-cific types of named entities, which are expected to have some association with topic temporality, and explicit dates. Table 3: Logistic regression models predicting prior topic classifi-Figure 2: Percent of topics in each collection with codes assigned from the (a) entity code group and (b) events code group.

Figure 2 summarizes the percent of topics in each test col-lection with each code assigned. We can see that the Novelty and Microblog collections have a higher percentage of spe-cific events than the Blog and ad-hoc collections. The ad-hoc collections have a higher number of generic events, which supports the findings of Jones and Diaz [5]. The Blog, Nov-elty, and Microblog test collections each have larger numbers of named entities in the topic titles and descriptions.
To assess coding reliability, a total of 1,244 codes were as-signed to 330 topics by the two coders. Higher overlap indi-cates greater agreement between coders. The macro percent overlap is 0.71 and micro percent overlap is 0.83, indicating that overall our codes may be applied with good consis-tency. The per-code overlap is reported in Table 2(c). As expected, some codes have higher agreement than others. Specifically, personal names (0.94), locations (0.91), and ex-plicit dates (0.89) have very high agreement whereas indirect event references (0.19) and generic events (0.45) have lower agreement.
In this section, we report the results of the logistic regres-sion analysis, predicting the manually assigned categories for each test collection. The resulting models are reported in Table 3.

For the 2003-2004 Novelty collection, the response vari-able is the  X  X pinion X  (0) or  X  X vent X  (1) classification of each Table 4: Cohen X  X   X  for inter-coder agreement for classification of topic, which is manually assigned by the TREC organiz-ers. Following Jones and Diaz [5], we treat  X  X vent X  as the temporal category. Logistic regression analysis is performed with and without the ACF and DPS predictors, as shown in Table 3. SpecificEvent and OtherEntity are significant pre-dictors of the  X  X vent X  category ( p &lt; 0 . 01), with a pseudo-R of 0.669. Including the ACF of the true-relevant distribution is significant, with a minor improvement in model fit. The high pseudo-R 2 is unsurprising in this case, since the Speci-ficEvent code corresponds to the Novelty  X  X vent X  category. It does, however, confirm our code definition.
 Dakka et al manually classified  X  X ime-sensitive queries X  for TREC topics 301-450. As reported in Table 3, only the PlaceEntity code is a significant predictor of the manual classification. However, the pseudo-R 2 is very low (0.019). Dakka et al acknowledge examining the relevant document distributions for the LA Times and Financial Times sub col-lections. Including the DPS of the true-relevant document distribution increases the pseudo-R 2 to 0.263, suggesting that the relevant document distribution played a significant role in the manual classification.

Efron and Golovchinsky also classified topics 301-450, in this case focusing on the identification of  X  X ecency X  queries. As reported in Table 3, both PlaceEntity and OtherEntity are useful predictors of the temporal response. As with Dakka, including the DPS of the true-relevant distribution increases pseudo-R 2 from 0.181 to 0.377. This again sug-gests that the distribution of relevant documents played an important role in the determination of topic temporality.
Finally, we look at Peetz et al X  X  classification of the Blog06-08 topics 850-1050. In this case, the SpecificEvent, Peri-odicEvent, Person and Organization entities are useful pre-dictors of the temporal category (pseudo-R 2 =0.127). In-cluding DPS improves model fit (pseudo-R 2 =0.223), again suggesting that the distribution of relevant documents played a role in manual classification.
As described in Section 3.1, non-parametric densities based on the temporal distribution of true-relevant documents are manually classified by two coders into four categories. The weighted Cohen X  X   X  is calculated to assess agreement be-tween the two coders. Average Pearson X  X  correlation (  X  ) measures the correlation between these manual classifica-tions and the per-topic ACF/DPS values.

The results reported in Table 4 show moderate (0.40-0.60) to high (0.60-0.80) coder agreement and higher correlation between the ACF and the manual classifications. These find-ings show that ACF and DPS effectively capture the degree to which relevant documents are temporally constrained. In this study, we have tried to identify characteristics of TREC topics that can be used to explain manual classi-fications of  X  X emporal sensitivity. X  Other researchers have classified topics without clear definitions or criteria. We have attempted to model these classifications by proposing features believed to indicate temporality. Features include the presence of different types of named entities, classes of events, and measures of the temporal distribution of judged relevant documents.

We were successful in modeling the  X  X vent X  category in the Novelty track, based primarily on our  X  X pecificEvent X  code. Event codes were also found to be useful predictors of the classification of Peetz et al [9]. However, we were generally unable to identify characteristics that fully explain the other manual classifications. They seem to consistently conflate two different concepts: the temporal distribution of judged-relevant documents and common-sense notions of topic temporality.

Since the first-order autocorrelation of the judged-relevant document distribution is highly correlated with manual judge-ments of temporality, we recommend using the ACF or other measure of distribution  X  X urstiness X  instead of manual as-sessment. In the future, common-sense notions of temporal-ity should be clearly explicated.

If we cannot explain the process that determines the clas-sifications, it raises questions about the value of these test collections for evaluation. Specifically, how can we be clear that the queries previously identified as  X  X emporally sensi-tive X  are truly so? This ambiguity also limits the utility of previous research, since it is unclear how to select queries for which the proposed models are well-suited.
This work was supported in part by the US National Sci-ence Foundation under Grant No. 1217279. Any opin-ions, findings, conclusions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the National Science Foundation. [1] W. Dakka, L. Gravano, and P. Ipeirotis. Answering [2] M. Efron and G. Golovchinsky. Estimation methods for [3] Q. He, K. Chang, and E.-P. Lim. Analyzing feature [4] H. Joho, A. Jatowt, and R. Blanco. NTCIR Temporalia [5] R. Jones and F. Diaz. Temporal profiles of queries. [6] K. Krippendorff. Content analysis: an introduction to [7] X. Li and W. B. Croft. Time-based language models. [8] D. Metzler, R. Jones, F. Peng, and R. Zhang.
 [9] M.-H. Peetz, E. Meij, and M. de Rijke. Using temporal
