 As sensor and storage technologies continue to improve in terms of both cost and performance, increasingly rich data sets are becoming available that characterize the rhythms of human activity over time (loop-sensor data), crime statistics, email and Web access logs, and many more. Such data can be used to support a variety of different applications, such as classification of human or animal activities, detection of unusual events, or to support the broad understanding of behavior in a particular context such as the temporal patterns of Web usage.
 To ground the discussion, consider data consisting of a collection of individual or aggregated events from a single sensor, e.g., a time-stamped log recording every entry and exit from a building, or the timing and number of highway traffic accidents. For example, Figure 1 shows several days worth of data from a building log, smoothed so that the similarities in patterns are more readily visible. Of interest is the modeling of the underlying intensity of the process generating the data, where intensity here refers to the rate at which events occur. These processes are typically inhomogeneous in time (as in Figure 1), as they arise from the aggregated behavior of individuals, and thus exhibit a temporal dependence linked to the rhythms of the underlying human activity. The complexity of this temporal dependence is application-dependent and generally unknown before observing the data, suggesting that non-or semi-parametric methods (methods whose complexity is capable of growing as the number of observations increase) may be particularly appropriate.
 Formulating the underlying event generation as an inhomogeneous Poisson process is a common first estimate the time-dependent intensity function (a normalized version of the rate function; see Sec-discretization [1], and nonparametric Bayesian models [4, 5]. Among these, nonparametric Bayesian ap-proaches have a number of appealing advan-tages. First, they allow us to represent and rea-son about uncertainty in the intensity function, providing not just a single estimate but a dis-tribution over functions. Second, the Bayesian framework provides natural methods for model selection, allowing the data to be naturally ex-plained by a parsimonious set of intensity func-tions, rather than using the most complex expla-nation (though similar effects may be achieved using penalized likelihood functions [3]). Fi-nally, Bayesian methods generalize to multiple or hierarchical models, which allow informa-tion to be shared among several related but dif-fering sets of observations (e.g., multiple days of data). This second point is crucial for many problems, as we rarely obtain many observa-tions of exactly the same process under exactly the same conditions; instead, we observe mul-tiple instances which are thought to be similar, but may in fact represent any number of slightly differing circumstances. For example, behavior may be dependent on not only time of day but also day of week, type of day (weekend or weekday), unobserved factors such as the weather, or other unusual circumstances. Sharing information allows us to improve our model, but we should only do so where appropriate (itself best indicated by similarity in the data). By being Bayesian, we can remain agnostic of what data should be shared and reason over our uncertainty in this structure. In what follows we propose a non-parametric Bayesian framework for modeling intensity functions for event data over time. In particular, we describe a Dirichlet process framework for learning the functions, depending on which categories are assigned to each time-period. This allows the model to learn in a data-driven fashion what  X  X actors X  are generating the observations on a particular day, including (for example) weekday versus weekend effects as well as day-specific effects correspond-ing to unusual behavior present only on a single day. Applications to two real X  X orld data sets, a building access log and accident statistics, are used to illustrate the technique.
 We will discuss in more detail in the sections that follow how our proposed approach is related to prior work on similar topics. Broadly speaking, from the viewpoint of modeling of inhomogeneous multiple, related processes (e.g., different days). Our approach can also be viewed as an alternative groups are much more constrained than would be expected under an HDP model. events occurring within that time is Poisson with rate given by  X  = semi-parametric model for  X  ( t ) , described next.
 Let us suppose that we have a single collection of event times {  X  i } arising from a Poisson process with rate function  X  ( t ) , i.e., where  X  ( t ) is defined on t  X  [  X  X  X  ,  X  ] . We may write  X  ( t ) =  X f ( t ) , where  X  = is the intensity function , a normalized version of the rate function. A Bayesian model places prior f ( t ) , we obtain a semi-parametric prior for  X  ( t ) . Specifically, we choose where  X  is the gamma distribution, K is a kernel function (for example a Gaussian distribution) and DP is a Dirichlet process [9] with parameter  X  and base distribution G 0 . The Dirichlet process model with infinitely many components: f ( t ) = Dirichlet processes and their variations [7, 9 X 11] have gained recent attention for their ability to provide representations consisting of arbitrarily large mixture models. In particular, they have been and space X  X ime [5]. 2.1 Monte Carlo Inference time T is given by which, as T  X  X  X  (i.e., as we observe a complete data set) becomes from the mixture model distribution defined by f . As in many mixture model applications, it will be components the sample  X  i is associated. The complete data likelihood is then Inference is typically accomplished using Markov chain Monte Carlo (MCMC) sampling [9]. sampling from f is more complicated. Samples from f can be drawn in a variety of ways. One of the most common methods is the so-called  X  X hinese Restaurant Process X  (CRP, [7, 9]), in which the relative weights w j are marginalized out while drawing the assignment variables z i . Such exact sampling approaches work by exploiting the fact that only a finite number of the mixture compo-nents are occupied by the data; by treating the unoccupied clusters as a single group, the infinite number of potential associations can be treated as a finite number. The operations involved (such as of K and G than others; for example using a Gaussian kernel and normal-Wishart distribution, the necessary quantities have convenient closed forms [9].
 Another, more brute-force way around the issue of having infinitely many mixture components is to perform approximate sampling using a  X  X runcated X  Dirichlet process representation [12, 13]. As de-scribed in [12], for a given  X  , data set size N , and tolerance  X  , one can compute a maximum number of components M necessary to approximate the Dirichlet process with a Dirichlet distribution using the relation and in this manner, can work with finite numbers of mixture components. This representation will prove useful in Section 3.
 The truncated DP approximation is helpful primarily because it allows us to sample the (complete) function f ( t ) (as compared to only the  X  X ccupied X  part in the CRP formulation). Given a set of in two steps. First, we sample the occupied mixture weights, w j ( j  X  J ), and the total unoccupied weight  X  w = of Sethuraman [14].
 Note that the truncated DP approximation highlights the importance of also sampling  X  if we hope for our representation to act non-parametric in the sense that it may grow more complex as the data increase, since for a fixed  X  and  X  the number of components M is quite insensitive to N . For more details on sampling such hyper-parameters see e.g. [10]. 2.2 Finite Time Domains Our description of non-parametric Bayesian techniques for Poisson processes has so far made im-parameters  X  is no longer possible in closed form. Although methods such as Metropolis-Hastings may be used [4], they can be highly dependent on the choice of proposal density.
 Here, we take a slightly different approach, drawing truncated Gaussian kernels with parameters sampled from a truncated Normal-Wishart distribution. Specifically, we define where  X  1 ( t ) is one on [0 , 1] and zero elsewhere and NW is the normal-Wishart distribution. Sam-restrictions imposed on  X  and  X  , one can show that the normalizing quantity Z = always greater than one-third. Thus, to sample from the posterior we simply draw from the original, probability 1  X  (3 Z )  X  1 . As mentioned in the introduction, we often have several collections d = 1 . . . D of observations, {  X  are known to be identical and independent, sharing information among them is relatively easy X  X e obtain D observations N d with which to estimate  X  , and the  X  di are collectively used to estimate difficult.
 Yet it is just this situation which is most typical. Again consider Figure 1, which shows event data from ten different Mondays. Clearly, there is a great deal of consistency in both size and shape, also look at, for example, Sundays or Tuesdays (as we do in Section 4), we would see that although Sunday and Monday appear quite different and, one suspects, have little shared information, Monday and Tuesday appear relatively similar and this similarity can probably be used to improve our rate estimates for both days.
 In this example, we might reasonably assume that the category memberships are known (for ex-ample, whether a given day is a weekday or weekend, or a Monday or Tuesday), though we shall reasonable model for sharing information among categories? There are, of course, many possible choices; we use a simple additive model, described in the next section. 3.1 Additive Models underlying causes present during the period of interest. Again, we initially assume that the category memberships are known; thus, if a category is associated with a particular day, the activity profile associated with that category will be observed, along with additional activity arising from each of the other categories present.
 function of a given day d to be the sum of the rate functions of each category to which d belongs. d , we have that  X  d ( t ) = At first, this model might seem quite restrictive. However, it matches our intuition of how the data is generated, stemming from the presence or absence of a particular behavioral pattern associated with some underlying cause (such as it being a work day). In fact, we do not want a model which for example, that a day is only  X  X art X  Monday. To learn the profiles associated with a given cause (e.g., things that happen every day versus only on weekdays or only on Mondays), it makes sense to take an  X  X ll or nothing X  model where the pattern is either present, or not. This also suggests that other methods of coupling Dirichlet processes, such as the hierarchical Dirichlet process [7], may be too flexible. The HDP couples the parameters of components across levels, but only loosely relates the actual shape of the profile, since it allows components to be larger or smaller (or even additive model allows both a consistent size and shape to emerge for each category, while associating deviations from that profile to categories further down in the hierarchy.
 3.2 Sampling Membership Of course, it is frequently the case that the membership(s) of each collection of data are not known precisely. In an extreme case, we may have no idea which collections are similar and should be grouped together and wish to find profiles in an unsupervised manner. More commonly, however, we have some prior knowledge and interpretation of the profiles but do not wish to strictly enforce a known membership. For example, if we create categories with assigned meanings (weekdays, weekends, Sundays, Mondays, and so on), a day which is nominally a Monday but also happens to be a holiday, closure, or other unusual circumstances may be completely different from other behavior unique to its particular circumstances and warrant an additional category to represent it. We can accommodate both these possibilities by also sampling the values of the membership indi-let us assume we have some prior knowledge of these membership probabilities, p dc ( s dc ) ; we may then re-sample from their posterior distributions at each iteration of MCMC.
 we could easily have elected to use, for example, the CRP formulation for sampling, the association truncation depth M chosen to provide arbitrarily high precision. The likelihood of the data under these rate functions for any values of { s dc } can then be computed directly via (2) where the others, though more complex moves could also be applied. This gives the following sequence of { s Figure 2: Posterior mean estimates of rate functions for building entry log data, estimated individ-ually for each day (dotted) and learned by sharing information among multiple days (solid) for (a) Sundays, (b) Mondays, and (c) Tuesdays. Sharing information among similar days gives greatly improved estimates of the rate functions, resolving otherwise obscured features such as the decrease during and increase subsequent to lunchtime. and parameters {  X  j } . In this section we consider the application of our model to two data sets, one (mentioned previously) from the entry log of people entering a large campus building (produced by optical sensors at the about ten weeks worth of observations. In both cases, we have a plausible prior structure for and categories for  X  X ll days X ,  X  X eekends X ,  X  X eekdays X , and  X  X undays X  through  X  X aturdays X . Each of for the possibility of unusual increases in activity, we also add categories unique to each day, with lower prior probability ( p dc = . 20 ) of membership. This allows but discourages each day to add a new category if there is evidence of unusual activity. 4.1 Building Entry Data To see the improvement in the estimated rate functions when information is shared among similar days, Figure 2 shows results from three different days of the week (Sunday, Monday, Tuesday). Each panel shows the estimated profiles of each of the ten days estimated individually (using only that day X  X  observations) under a Dirichlet process mixture model (dotted lines). Superimposed in each panel is a single, black curve corresponding to the total profile for that day of week estimated using our categorical model; so, (a) shows the sum of the rate functions for  X  X ll days X ,  X  X eekends X , and  X  X undays X , while (b) shows the sum of  X  X ll days X ,  X  X eekdays X , and  X  X ondays X . We use the same prior distributions for both the individual estimates and the shared estimate. produce a much more accurate estimate of the profiles. In this case, no single day contains enough observations to be confident about the details of the rate function, so each individually X  X stimated rate function appears relatively smooth. However, when information from other days is included, the rate function begins to resolve into a clearly bi-modal shape for weekdays. This  X  X i-modal X  rate during lunchtime, and a larger, narrower second peak as most occupants return from lunch. Second, although Monday and Tuesday profiles appear similar, they also have distinct behavior, such as increased activity late Tuesday morning. This behavior too has some basis in reality, correspond-ing to a regular weekly meeting held around lunchtime over most (though not quite all) of the weeks in question. The breakdown of a particular day (the first Tuesday) into its component categories is shown in Figure 3. As we might expect, there is little consistency between weekdays and weekends, quite a bit of similarity among weekdays and among just Tuesdays, and (for this particular day) very little to set it apart from other Tuesdays.
 We can also check to see that the category memberships s dc are being used effectively. One of the Mondays in our data set fell on a holiday (the individual profile very near zero). If we average the probabilities computed during MCMC to estimate the posterior probability of the s dc for that Figure 3: Posterior mean estimates of the rate functions for each category to which the first Tuesday data might belong. For comparison, the total rate (sum of all categories) is shown as the dotted to correspond to the occupants X  return from lunch; (c) the  X  X uesday X  category has modes in the morning and afternoon, perhaps capturing regular meetings or classes; (d) the  X  X nique X  category (a category unique to this particular day) shows little or no activity. Figure 4: Profiles associated with individual-day categories in the entry log data for several days with known events (periods between dashed vertical lines). The model successfully learns which categories, and uses only the all-day and unique categories.
 We can also examine days which have high probability of requiring their own category (indicat-ing unusual activity). For this data set, we also have partial ground truth, consisting of a number of dates and times when activities were scheduled to take place in the building. Figure 4 shows corresponds well with the actual start and end time shown in the schedule (dashed vertical lines). 4.2 Vehicular Accident Data Our second data set consists of a database of ve-hicular accident times recorded by North Car-olina police departments. As we might expect of driving patterns, there is still less activity on weekends, but far more than was observed in the campus building log.
 As before, sharing information allows us to decrease our posterior uncertainty on the rate for any particular day. Figure 5 quantifies this idea by showing the posterior means and (point-wise) two-sigma confidence intervals for the rate function estimated for the same day (the first Monday in the data set) using that day X  X  data only (red curves) and using the category-based additive model (black). The additive model leverages the additional data to produce much tighter estimates of the rate profile. As with the previous example, the additional data also helps resolve detailed features of each day X  X  profile, as seen in Figure 6. For example, the weekday profiles show a tri-modal shape, with one mode corresponding to the morning commute, a small mode around noon, and another large mode Figure 6: Posterior mean estimates of rate functions for vehicular accidents, estimated individually for each day (dotted) and with sharing among multiple days (solid) for (a) Sundays, (b) Mondays, and (c) Fridays. As in Figure 2, sharing information helps resolve features which the individual days do not have enough data to reliably estimate. around the evening commute. This also helps make the pattern of deviation on Friday clear, showing (as we would expect) increased activity at night. application of statistical learning techniques. In this paper we proposed a non-parametric Bayesian approach to learning time-intensity profiles for such activity data, based on an inhomogeneous Pois-son process framework. The proposed approach allows collections of observations (e.g., days) to be grouped together by category (day of week, weekday/weekend, etc.) which in turn leverages data across different collections to yield higher quality profile estimates. When the categorization of days is not a priori certain (e.g., days that fall on a holiday or days with unusual non-recurring additional activity) the model can infer the appropriate categorization, allowing (for example) au-tomated detection of unusual events. On two large real-world data sets the model was able to infer interpretable activity profiles that correspond to real-world phenomena. Directions for further work and other exogenous phenomena, as well as modeling of multiple spatially-correlated sensors (e.g., loop sensor data for freeway traffic).

