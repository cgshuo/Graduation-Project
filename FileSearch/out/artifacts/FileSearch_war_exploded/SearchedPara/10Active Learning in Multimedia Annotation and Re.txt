 With rapid advances in storage devices, networks and compression techniques, multi-media data are increasing in an explosive way. The management of these data becomes a challenging task. The automatic understanding and efficient retrieval of these data are highly desired. Active learning is a machine learning technique that learns a model in an interactive way. In comparison with passive learning that trains models with pre-collected training data, active learning is able to select the most representative data in an iterative manner based on the model learned in each iteration. In this way, better performance can be obtained in comparison with using the precollected or randomly collected training data. Therefore, active learning is an effective approach to reducing human labeling effort (or achieving better performance with the same amount of hu-man efforts), and consequently it has been widely explored in multimedia annotation and retrieval. In this article, we consider two application domains: (1) image and video annotation; and (2) content-based image retrieval. Annotation aims to assign images and videos a set of labels that describe their content at syntactic and semantic levels [Naphade and Smith 2004b; Wang et al. 2009a, 2009b; Li and Wang 2008]. With the help of these labels, the management and manipulation of image and video data can be greatly facilitated, such as delivery, summarization and retrieval. Here we consider two annotation scenarios, namely multiclass and multilabel annotation. Multiclass annotation can be viewed as a categorization task, that is, cat-egorizing images and videos into a set of exclusive and predefined concepts. Multilabel annotation is more challenging in comparison with multiclass problem as the labels are nonexclusive and each image or video clip can be assigned multiple labels. Both multiclass annotation and multilabel annotation are typically accomplished with ma-chine learning techniques. The process is illustrated in Figure 1. A labeled training set is collected and low-level features are extracted. Models are learned with the training data, and then newly given image or video clip can be directly predicted accordingly. The difference is that multiclass annotation is naturally a multiclass classification problem, whereas multilabel annotation is usually regarded as a set of binary classi-fication problems. Given a concept, each unit is classified as  X  X ositive X  or  X  X egative X  according to whether it is associated with this concept.

Despite extensive research has been dedicated to image and video annotation, ex-isting studies show that a large training set is often needed in order to achieve a reasonable performance due to the large variety of the content of image and video data. However, manual annotation is laborious and time-consuming. For example, ex-isting studies show that typically annotating 1 hour of video with 100 concepts can take anywhere between 8 to 15 hours [Lin et al. 2003]. Therefore, several works use active learning in image and video annotation to reduce the required training data. Most of the existing commercial image retrieval systems are built based upon textual information, such as the images X  file names, ALT texts, captions and surrounding texts, but they do not directly leverage the content of images. Content-Based Image Retrieval (CBIR) has been an active research field for over 15 years, which aims to retrieve the desired images based on a user provided or synthesized query image [Rui et al. 1999; Smeulders et al. 2000]. Given the query image, the most intuitive approach is to retrieve the close images in a feature space. However, despite many different features and similarity metrics have been explored, simply retrieving images with a query example usually cannot achieve satisfying performance. Relevance feedback is an approach to addressing this difficulty [Rui et al. 1998]. As illustrated in Figure 2, it improves the retrieval performance by letting users label several returned results such that a classifier or better similarity measurement can be learned. As a consequence, active learning finds its application for its capability of providing the most informative images to users for labeling.

Many different active learning algorithms have been used in these two application domains. In this article, we present a comprehensive survey on these efforts. As we will introduce later, sample selection and model learning are two major components in an active learning scheme. We will analyze and categorize the existing sample selection approaches explored in multimedia annotation and retrieval. We also intro-duce the combination of active learning with several machine learning techniques, including semi-supervised learning, multilabel learning and multiple instance learn-ing. It is worth noting that active learning has been widely explored in many different applications, such as text categorization [Tong and Koller 2000], speech recognition [Hakkani-tur et al. 2002], information extraction [Settles et al. 2008] and dataset collection [Collins et al. 1995]. In this work we will focus on multimedia annota-tion and retrieval, and for other applications readers can refer to Settles [2009] and Olsson [2009] and the references therein. Multimedia annotation and retrieval is an important research field and here we have chosen two application domains from it, namely image/video annotation and content-based image retrieval. It is worth men-tioning that active learning has also been applied in other applications, such as content-based music retrieval [Mandel et al. 2006; Foote 1997]. Settles [2009] and Olsson [2009] have reviewed many active learning techniques, but they mainly fo-cus on the machine learning methodologies and the applications of natural language processing and bioinformatics. Huang et al. [2008] introduced the application of ac-tive learning in interactive multimedia retrieval. But they have not introduced the exploitation of active learning in multimedia annotation. In addition, Huang et al. [2008] mainly focuses on introducing several typical and exemplary active learning paradigms in multimedia retrieval, whereas there is a lack of categorization and sum-mary of the existing algorithms. This motivates us to develop a dedicated survey on active learning-based multimedia annotation and retrieval, which analyzes and cate-gorizes the existing research efforts as well as discusses several future trends in this direction.

The rest of the article is organized as follows. First, we briefly introduce active learning and a typical SVM-based active learning scheme in Section 2. In Section 3, we analyze the sample selection approaches of active learning in multimedia annotation and retrieval. In Section 4, we introduce several classification models of active learning in multimedia annotation and retrieval. Section 5 discusses some future direction of active learning in multimedia annotation and retrieval. Finally, we conclude the paper in Section 6. Active learning actually contains three different paradigms: membership query learn-ing [Angluin 1998; King et al. 2004], stream-based active learning [Cohn et al. 1994; Dagan and Engselon 1995] and pool-based active learning [Cohen et al. 1996]. In this article, we focus on pool-based active learning, in which samples are selected from an existing pool for labeling according to certain criteria. A typical active learning system is composed of two parts, that is, a learning engine and a sample selection engine. It works in an iterative way, as illustrated in Figure 3. In each round, the learning engine trains a model based on the current training set. Then, the sample selection engine selects the most informative unlabeled samples for manual labeling, and these samples are added to training set. In this way, the obtained training set is more informative than that gathered by random sampling.

It is obvious that the sample selection strategy plays a crucial role in an active learning scheme, as the learning engine can actually adopt any existing classification algorithm. Cohen et al. [1996] suggests that the optimal active learning approach should select the samples that minimize expected risk. In Cohen et al. [1996] and Roy and McCallum [2001], reduced risk is estimated with respect to each unlabeled sample, and then the most effective samples are selected. However, for most learning methods, it is infeasible to estimate the risk. Therefore, practically most active learning methods adopt an uncertainty criterion, that is, to select the samples closest to the classification boundary [Tong and Chang 2001; Tong and Koller 2000]. Wu et al. [2006] and Zhang and Chen [2003] further proposed to incorporate the density distribution of samples into the sample selection process. Brinker [2003] pointed out that the selected samples should be diverse.

Among many different classification models and sample selection methods, we first introduce SVM-based active learning as a typical scheme. More approaches will be introduced in the next sections. For a binary classification problem, given a training data set that contains l labeled samples { ( x 1 , y 1 ) , the classification hyperplane is defined as where ( . ) is a mapping from R d to a Hilbert Space H ,and in
H . Thus, the decision function f ( x )is
SVM aims to find the hyperplane with the maximum margin between the two classes, that is, the optimal hyperplane. This can be obtained by solving these quadratic opti-mization problem
In Tong and Chang [2001] and Tong and Koller [2000], an active learning approach is proposed using SVM. It is derived from the minimization of the expected size of version space. Version space is defined to be the set of consistent hypotheses [Mitchell 1982]. It means that a hypothesis f is in version space if it satisfies f ( x f ( x i ) &lt; 0if y i = X  1 for every training sample x i unbiased SVM model, that is, the separating hyperplane that passes through origin. Then, the version space can be denoted by
It is proved that selecting the samples closest to the current hyperplane in each round is an approximate approach to reducing version space in a fastest way [Tong and Chang 2001; Tong and Koller 2000]. Note that the samples closest to the hyperplane are exactly the ones with highest uncertainty measure. Therefore, this sample selection strategy can also be regarded as based on an uncertainty measurement. The detailed implementation of SVM-based active learning scheme is illustrated in Figure 4. As previously mentioned, sample selection plays a crucial role in active learning. In this section, we provide a review of typical sample selection strategies used in active learning-based multimedia annotation and retrieval. Intuitively, the optimal sample selection strategy is consistent with the aim of the learner, that is, minimizing the expected risk, which can be denoted by density function of sample distribution, and E T denotes expectation over both the conditional density P ( y | x ) and training data L . If the reduction of the expected risk of labeling each unlabeled sample can be estimated, then the optimal sample selection can be achieved.

For several models, the expected risk reduction can be estimated. For example, Hoi and Lyu [2005] and Bao et al. [2009] estimated the risk reduction after labeling an image in a graph-based semi-supervised learning algorithm and a so-called locally non-negative linear structure learning algorithm, respectively. Yan et al. [2003] proposed a method that estimates the reduced risk after labeling a sample in a multiclass setting. Qi et al. [2009] proposed a multilabel active learning method to estimate the reduced risk after labeling an image with respect to a concept. Cohen et al. [1996] decomposed the risk in Eq. (5) into three terms, that is, where E L [ . ] is an expectation over labeled data L and E [ conditional density P ( y | x ). The first term estimates the variance of the true label y given only x , the second term estimates the prediction error induced by the model itself, and the third term is a variance that estimates the mean squared error of prediction with respect to the true model. Cohen et al. [1996] then proposed a training sample selection method to minimize the variance term by estimating the reduced variance after adding each sample to training set. But unfortunately, estimating the expected variance reduction is not an easy task for most classifiers. Roy and McCallum [2001] proposed a method to empirically estimate the reduced expected risk based on Monte Carlo sampling. But this approach is computationally intensive since it needs to traverse all samples to estimate the reduced risk after adding it to training set and updating the model. In multimedia annotation and retrieval, we usually need to deal with large-scale data, and thus many efforts resort to more heuristic sample selection criteria. Here, we categorize these strategies into the following four criteria: uncertainty , diversity , density and relevance . Applying uncertainty criterion means that the most uncertain samples should be se-lected. This heuristic stems from the fact that in many learning algorithms the essential classification boundary can be preserved based solely on the nearby samples, and the sample that are far from the boundary can be regarded as redundant. For example, in a SVM model the hyperplane can be mainly constructed based on only the support vectors (i.e., the samples that lie closest to the hyperplane). A typical measure that estimates uncertainty is entropy, that is, where Pr ( y i | x ) is the estimated probability of y i given x . For a binary classification, the samples that are closest to classification boundary will be selected according to the above equation. The uncertainty criterion can also be viewed as a greedy strategy to reduce risk (without model updating, the method to reduce maximal expected risk is to select the most uncertain samples). This criterion has been widely explored for its simplicity [He et al. 2004; Tong and Chang 2001; Wu et al. 2006]. Since nearly all classification models can output prediction probabilities or confidence scores, the uncertainty-based sample selection can be easily accomplished with an O ( n ) cost, where n is the number of samples in unlabeled pool. For SVM model, uncertainty criterion will select samples that are closest to classification boundary, and it is proved that this strategy will reduce the version space in a fastest way [Tong and Chang 2001; Tong and Koller 2000]. When multiple learners exist, a widely applied strategy is selecting the samples that have the maximum disagreement amongst them [Freund et al. 1997; Seung et al. 1992]. Here the disagreement of multiple learners can also be regarded as an uncertainty measure, and thus this strategy is categorized into the uncertainty criterion as well. For multiclass annotation, Joshi et al. [2009] proposed an uncertainty estimation method that considers the posterior probabilities of the best and the second best predictions, that is, where y 1 and y 2 are the classes with the largest and second largest posterior class probabilities, respectively. If their margin is small, it means that the model is more confused on the sample and thus it is with high uncertainty. It can be easily proved that this approach is equivalent to the conventional entropy-based method in binary classification, but in multiclass setting the proposed method has shown remarkable improvements on several benchmark datasets [Joshi et al. 2009]. The diversity criterion is first investigated in batch mode active learning. In many applications, we need to select a batch of samples instead of just one in an active learning iteration. For example, sometimes the updating or retraining of a model needs extensive computation, and thus labeling just one sample each time will make the active learning process quite slow. Joshi et al. [2009] proposed that the selected samples in a batch should be diverse. Given a kernel K , the angle between two samples x x j is defined as Then, the diversity measure can be estimated as where S is the set of selected samples in a batch. Directly optimizing the selection of a batch of samples is difficult, and thus typically it is accomplished with a greedy process, that is, each time a sample is selected according to the combination of diversity and other criteria and then it is added to the batch. Several studies [Dagli et al. 2006; Gos-selin and Cord 2004; Wu et al. 2006] also show that the diversity criterion should not only be investigated in batch mode but also be considered on all labeled samples, such that the selected samples will not be constrained in a more and more restricted area. In Hoi et al. [2006], Fisher information matrix is adopted for sample selection to keep the diversity of the selected samples, and in Dagli et al. [2006] an information-theoretic diversity measure is proposed based on Shannon X  X  entropy. Given a set of points { x
Then, the sample is selected based on the criterion that maximally reduces empirical entropy [Dagli et al. 2006]. Several works indicate that the samples within the regions of high density should be selected. Wu et al. [2006] defined a  X  X epresentativeness X  measure for each sample according to its distance to nearby samples. Zhang and Chen [2003] estimated data distribution by Kernel Density Estimation (KDE) [Parzen 1962] and then explored it in sample selection. Given a set of points { x 1 , x 2 ,...,  X  p ( x ) can be estimated by where K ( x ) is a kernel function that satisfies K ( x ) the density measure can be defined by normalizing to [0, 1] as follows:
Besides that, there are also several other clustering-based methods which first group the samples and then only select the cluster centers in active learning [Nguyen and Smeulders 2004; Qi et al. 2004; Song et al. 2005]. Since cluster centers usually lie in the regions with high density, these works can also be regarded as applying a density strategy, that is, trying to select samples in dense regions. Qi et al. [2004] proposed a method that will adjust the clusters with a merging and splitting operations after each round of active learning. Relevance strategy is usually applied in multilabel image/video annotation and re-trieval. As introduced in Section 1, in these tasks samples are classified to be relevant or irrelevant according to whether they are associated with the given concept or query. Of course, the aforementioned criteria such as uncertainty can also be applied in these tasks. But in many cases it is found that the using relevance criterion, that is, di-rectly selecting the samples that have the highest probabilities to be relevant, is more effective [Ayache and Qu  X  enot 2007; Gosselin and Cord 2004; Vendrig et al. 2002]. Gosselin and Cord [2004] investigated SVM-based active learning in CBIR and showed that better performance can be achieved by selecting more relevant images. Ayache and Qu  X  enot [2007] have conducted an empirical study on different sample selection strategies for active learning on TRECVID video annotation benchmark. Experimen-tal results clearly show that for several concepts the relevance criterion can achieve comparable or even better performance than the uncertainty criterion. This can be at-tributed to the fact that positive samples are usually less than negative ones in these tasks, and the distribution of negative samples is usually in very broad domain. In addition, in these tasks ranking performance is usually adopted for evaluation, such as Average Precision (even for annotation, ranking performance is widely adopted instead of classification accuracy). This implies that finding more relevant samples with high confidence is more important than correctly classifying the samples. Therefore, the rel-evance criterion may outperform the uncertainty criterion since it is able to find more positive samples. Therefore, relevant samples should contribute more than irrelevant samples. Up to now, we have discussed five different typical sample selection criteria. It X  X  dif-ficult to directly compare the superiorities of these criteria as they will depend on specific tasks as well as the classification models. For example, simply applying rele-vance criterion may achieve the best results under several extremely unbalanced cases that positive samples are much rare. When we choose batch mode active learning for computational efficiency, integrating diversity criterion will be helpful. In many cases, these criteria are combined explicitly or implicitly. The diversity and density criteria are rarely used individually as they are independent with the classification results. Most frequently they are used to enhance the uncertainty criterion. Wang et al. [2007] have combined uncertainty , diversity , density and relevance in sample selection as
Several sample selection algorithms implicitly explore multiple criteria. [Yuan et al. 2006] adopted the uncertainty and diversity criteria in CBIR, but meanwhile, they also propose to shift the boundary such that more relevant samples can be selected. Thus, this strategy can be viewed as a combination of the uncertainty , diversity and relevance criteria. Hoi et al. [2006] proposed a batch mode sample selection approach based on the principle of maximally reducing Fisher information ratio. They show that the derived sample selection criteria will select samples that are uncertain, dissimilar to already selected samples and similar to unlabeled samples. Therefore, it is consistent with our uncertainty , diversity and density criteria. In the Appendix, there is a table illustrat-ing several research works that adopt active learning in image/video annotation or retrieval. We have illustrated the description of the applications, the adopted learning methods and sample selection criteria. As introduced in Section 2, an active learning process can be regarded as the repeat of a prediction and a sample selection steps. Therefore, with certain sample selection strategy, nearly all classification algorithms can be applied to form an active learning scheme. From the Table I in Appendix we can see that many different classification models have been explored, such as k-NN, SVM, Gaussian Mixture Model [Redner and Walker 1984], Maximum Entropy Classifier [Berger et al. 1996] and graph-based semi-supervised learning [Zhu et al. 2003a]. Figure 4 illustrates the active learning process for SVM, and actually we only need to replace SVM with other classification models and then obtain their active learning schemes. In this section, we mainly focus on the exploitation of active learning in several special machine learning techniques. More specifically, we will consider the following combinations: (1) Semi-Supervised Learn-ing + Active Learning; (2) Multilabel Learning + Active Learning; and (3) Multiple Instance Learning + Active Learning. Semi-Supervised Learning (SSL) is a technology that deals with the difficulty of train-ing data insufficiency [Chapelle et al. 2006; Zhu 2009]. While labeled data are usually limited, unlabeled data can be easily obtained. For example, nowadays we can easily collect large-scale multimedia data from Internet. In contrast to supervised learning, SSL leverages a large amount of unlabeled samples based on certain assumptions, such that the obtained models can be more accurate than those achieved by purely super-vised methods. Since SSL and active learning both involve unlabeled data, they have been exploited together in many image/video annotation and retrieval works [Wang et al. 2007; Zhu et al. 2003b; Sahbi et al. 2008]. Zhu et al. [2003b] proposed an active learning approach based on a graph-based SSL method, in which the reduction of ex-pected risk of labeling each sample can be predicted without retraining classification model. Hoi and Lyu [2005] further integrated SVM with the graph-based SSL method and applied the method to CBIR. Bao et al. [2009] adopted a similar approach based on an improved graph-based SSL method named Locally Non-negative Linear Structure Learning (NLSL). Existing research in machine learning community has also demon-strated the effectiveness of combining active learning and multiview semi-supervised learning (such as co-training [Blum and Mitchell 1998]) from both theoretical and empirical perspectives [Muslea et al. 2002; Wang and Zhou 2008]. Song et al. [2005] proposed an active learning method based on co-training in video annotation. They construct two classifiers with two different modalities and then estimate the posteriori probabilities with an independence assumption of the two modalities. In each iteration, several uncertain samples are selected for manual labeling, and the samples that are certain for one classifier are added to training set to teach the other classifier. The active learning process is as follows: (1) Train two initial complementary classifiers based on the complementary feature (2) Predict the labels of testing samples using the two classifiers. The label of each (3) Select the most uncertain samples for each classifier, and ask user to label them. (4) Take the manually labeled samples as well as several confident samples as new (5) Repeat (2) X (4) for certain number of rounds and output the final results. In multilabel image/video annotation, the concepts are not exclusive and each im-age or video clip can be associated with multiple concepts. An intuitive active learning approach in the multilabel context is to select samples considering the models of all con-cepts and then manually label these samples with respect to all the concepts. Naphade and Smith [2004a] adopted this strategy in active multilabel video annotation and showed that it introduces less labor cost in comparison with conducting active learning for each concept individually. However, this conclusion is derived by counting the num-ber of labeled samples. Existing studies show that manually labeling an image/video clip with respect to many concepts is laborious. In addition, users tend to miss relevant labels in this type of manual labeling [Volkmer et al. 2005]. Therefore, several works resort to selecting a set of samples for each individual concept or directly selecting sample-concept pairs. In these approaches, humans only need to label each sample with respect to one concept.

Wang et al. [2007] adopted a two-step process. In each iteration, first a concept is selected with the expectation of getting the highest performance gain, and a batch of suitable samples is selected to be annotated for this concept based on the combination of multiple criteria. The concept selection criterion can be viewed as optimizing the average of the annotation performance of multiple concepts. Its process is as follows: (1) Initialization. A batch of samples is selected for each concept and a classifier is (2) The concept with highest performance gain is selected. (3) Select a batch of samples according to certain criteria (more specifically, (4) Classifier is learned for the concept, and its performance gain can be predicted (5) Repeat (2) X (4) for certain number of rounds and output the final results.
Qi et al. [2007] proposed a method that selects sample-concept pairs for manual an-notation in a correlative multilabel learning approach. The sample-concept selection is derived based on the reduction of multilabel Bayesian error bound, which is formu-lated as and L ( x ) indicate the index sets of labeled and unlabeled concepts of x respectively, and given the known labeled parts y L ( x Now we can see that the sample-concept selection is based on two terms, namely timates the uncertainty of the selected pair ( x s , y s ), and this is consistent with the previously introduced uncertainty criterion. The second term actually measures the redundancy among the selected concept and the rest ones. By incorporating this term, the sample-concept selection is able to reduce the uncertain of the other concepts. Therefore, this method not only considers the uncertainty of the to-be-selected sample-concept pair but also the correlations of concepts. Zhang et al. [2009] further extended the algorithm to a multiview scenario, and the selection of label-sample pairs further takes their uncertainty over different views into consideration. Multiple Instance Learning (MIL) is a technique for problems that the label infor-mation of training data is incomplete [Dietterich et al. 1997; Maronand and Ratan 1998; Zhang and Goldman 2001; Andrews et al. 2002]. In a typical supervised learning task, every training instance is associated with a label, but in MIL the labels are only assigned to bags of instances. This technology has been widely explored in different applications such as drug activity prediction [Dietterich et al. 1997], image annotation and retrieval [Maronand and Ratan 1998; Zhang and Goldman 2001], and text catego-rization [Andrews et al. 2002]. It is particularly suitable for image/video annotation and retrieval because many concepts actually only specify part of images/videos, such as several objects. For example, an image can be viewed as a bag and it contains many in-stances where each instance is a region, but labels are usually assigned only to images.
When applying active learning with MIL, an intuitive approach is to adapt the existing criteria for supervised learning to the multiple-instance content. For example, we can adopt the uncertainty criterion by estimating the confidence scores of bag classification. However, in image/video annotation and retrieval humans can label both bags and instances. For example, users can label a whole image, and they can also label a specific region in the image. Therefore, several efforts have been dedicated to developing multi-level active MIL algorithms. A typical multi-level active MIL scheme is shown in Figure 5. Clearly, the main difficulty relies on the choice between bag labeling and instance labeling and the corresponding selection criteria.
 Settles et al. [2007] proposed an approach that labels instances in positive bags. They proposed two methods for instance selection in positive bags, one is multiple-instance uncertainty and the other is expected gradient length. Vijayanarasimhan and Grauman [2008] proposed a method that is able to label both instances and bags. They assume that labeling an instance introduces more cost than labeling a bag. This is easy to understand as labeling relevant regions will be more complex than assigning a label to a whole image. For example, Vijayanarasimhan and Grauman [2008] showed that labeling relevant regions needs about four times cost than labeling the whole image. Their proposed approach takes into account both the cost and the reduction of expected risk and then makes decision accordingly. Overall, the optimal scheme for the combination of multiple instance learning and active learning also depends on specific models and tasks, such as whether users are willing to label media instances (such as image regions) and models can integrate labeled media bags and instances. It may need to simultaneously consider sample and instance selection as well as the costs of labeling bags and instances. Extensive efforts have proved the effectiveness of active learning in multimedia anno-tation and retrieval, but there are also many open challenges. It is worth noting that, as a machine learning technology, active learning per se has many worth-studying prob-lems, such as learning rate and bound [Hanneke and Yang 2010]. But here we focus on its application in multimedia annotation and retrieval. We discuss the following two subtopics: (1) cost analysis of manual annotation; and (2) large-scale interactive mul-timedia labeling. This is because as an interactive approach, active learning involves two parts, namely human and computer. But most of the existing research efforts have been dedicated to computation algorithms, such as sample selection strategies and learning models, whereas human part receives relatively less attention. However, human also plays a very important role in active learning, and therefore we choose such two subtopics to discuss. One topic analyzes the human annotation behavior and attempts to benefit active learning-based multimedia annotation and retrieval with this analysis, and the topic attempts to leverage a large number of human labelers to annotate large-scale multimedia data. We will introduce the problems, several related works and the possible solutions. Active learning is intended to reduce humans X  manual efforts, but in most of the existing works performance evaluation is measured by counting the number of labeled samples. For example, an algorithm will be declared effective if active learning needs less labeled samples to achieve the same performance in comparison with random sample selection. But a hidden assumption here is that the cost of manually labeling a sample is identical and the sample selection criteria are derived accordingly. However, this is not the truth in multimedia annotation and retrieval. Existing studies [Volkmer et al. 2005] show that different concepts may lead to different average annotation time per sample. In addition, annotating different samples also may cost different efforts even with the same concept. For example, the time cost of annotating an image may depend upon the typicality [Tang et al. 2007] and recognizability of the objects in it (see Figure 6). Therefore, it is more reasonable to take the cost of annotating different samples into account in practical active learning approaches. For example, Settles et al. [2008] takes both the uncertainty measurements and the annotation costs into account in sample selection strategies and have shown its effectiveness.

Yan et al. [2009] first tried to model the cost of manual image annotation. They categorize manual annotation into two types, namely tagging and browsing. Tagging means users annotate each image with a chosen set of keywords and browsing means users browse images sequentially and judge each image X  X  relevance to a concept. Linear models are adopted to predict both the cost of tagging and browsing. For the tagging of an image, its expected time cost is predicted as where K l is the number of concepts associated with the image, t of entering a keyword, and t s is the initial setup time for annotation. For the browsing of a concept, its expected time cost is predicted as where L is the number of all images, L k is the number of relevant images, and t are the time costs of annotating a relevant image and an irrelevant image respectively. Based on the two models, Yan et al. [2009] proposed two methods to organize the annotation task by combining tagging and browsing, such that annotation cost can be greatly reduced.

Vijayanarasimhan and Grauman [2009] adopted a learning-based method to model the time cost of segmenting and annotating an image. They collect training data by taking advantage of Amazon X  X  Mechanical Turk system and then learn a repressor with several features that are able to indicate images X  complexity levels. The model has been investigated in a multilabel multiple-instance active learning scheme such that the sample selection will not only consider its informativeness but also the cost of its annotation.

In addition to the previously introduced manual labeling cost issue, how many sam-ples should be labeled in each round is also a problem. As previously introduced, the re-training or updating process of models may be time consuming and thus it is more reasonable to select a batch of samples in each round to reduce the rounds of model updating. In such scenario, the size of batch needs to be well established to achieve a trade-off between computational efficiency and labeling cost.

Overall, the study of manual annotation is far from comprehensiveness. Human X  X  manual annotation may be affected by many factors and many simple strategies may significantly reduce manual annotation cost. For example, in the annotation of a con-cept, we can group images into several clusters and most of the images in a cluster should share a same relevance label. Then users can assign a global label to a cluster and they only need to change the labels of few samples in the cluster, and this will be more efficient than labeling the images sequentially. In the annotation of large-scale multimedia corpus, it is not easy to achieve satisfying performance even by applying active learning. Therefore, the scale of to-be-manually-annotated data will also be very large. Therefore, a large-scale interactive multimedia annotation scheme will involve two parts, that is, large-scale active learning and large-scale manual annotation. For large-scale active learning, there are already many methods to efficiently train classifiers on large dataset [Hsieh et al. 2008; Panda et al. 2006a] and online learning is a promising approach to dealing with labeled data that are continuously increasing [Roy and McCallum 2001; Cauwenberghs and Poggio 2000]. Here, we focus on large-scale human annotation. From a global point of view, we can have two potential groups of people involved in the annotation process, that is, dedicated data labelers and grassroots users. Dedicated data labelers are experienced and they will provide high-quality annotation results. But in the Web 2.0 era, we have witnessed the great potential of Internet grassroot users. If well motivated, they are also able to contribute greatly in large-scale interactive multimedia annotation tasks. The following scenarios can be applied to attract these users to label given data. (1) Game . Designing attractive games and during playing game the players will be (2) Pay . Pay by the estimation of annotation efforts, such as the number of annotated (3) reCAPTCHA . 2 CAPTCHA is a type of challenge-response test used to determine
Figure 7 demonstrates the schematic illustration of a large-scale interactive multi-media annotation system. The task assignment module will divide and assign annota-tion tasks to different labelers through aforementioned approaches. The quality control module will control the quality of annotation results. In particular for grassroots la-belers, the annotation may contain significant noises and several users may even use robot or random algorithms to accomplish the annotation tasks. The quality control component will judge the annotation quality of different users, remove labelers with low-quality annotation and perform an annotation refinement step. This article presents a survey on the efforts of leveraging active learning in multimedia annotation and retrieval. We have briefly introduced the principle of active learning, and we categorize the existing sample selection criteria used in multimedia annotation and retrieval into five criteria: risk reduction , uncertainty , diversity , density and rel-evance . We also introduced several different classification models for active learning, including semi-supervised learning, multilabel learning and multiple instance learn-ing. We provide a discussion on several future trends of active learning in multimedia annotation and retrieval. In particular, we introduce cost analysis of human annotation and large-scale multimedia labeling.

