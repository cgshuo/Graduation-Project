 Various constrained frequent pattern mining problem for-mulations and associated algorithms have been developed that enable the user to specify various itemset-based con-straints that better capture the underlying application re-quirements and characteristics. In this paper we introduce a new class of block constraints that determine the significance of an itemset pattern by considering the dense block that is formed by the pattern X  X  items and its associated set of trans-actions. Block constraints provide a natural framework by which a number of important problems can be specified and make it possible to solve numerous problems on binary and real-valued datasets. However ,developing computationally efficient algorithms to find these block constraints poses a number of challenges as unlike the different itemset-based constraints studied earlier ,these block constraints are tough as they are neither anti-monotone ,monotone ,nor convert-ible. To overcome this problem ,we introduce a new class of pruning methods that significantly reduce the overall search space and present a computationally efficient and scalable algorithm called CBMiner to find the closed itemsets that satisfy the block constraints.
 H.2.8 [ Database Management ]: Database applications X  Data Mining
This work was supported in part by NSF CCR-9972519 ,EIA-9986042 ,ACI-9982274 ,ACI-0133464 ,and ACI-0312828; the Digital Technology Center at UMN; and by the Army HPC Research Center under the auspices of the Department of the Army ,Army Research Laboratory (ARL) under Cooperative Agreement number DAAD19-01-2-0014. The content of which does not necessarily reflect the position or the policy of the government ,and no offi-cial endorsement should be inferred. Access to research and computing facilities was provided by the Minnesota Super-computing Institute.
 Algorithms ,Theory Closed pattern ,Tough constraint ,Block constraint
Finding frequent patterns in large databases is a funda-mental data mining task with extensive applications to many areas including association ,correlation ,and causality rule discovery ,rule-based classification ,and feature-based clus-tering. As a result ,a vast amount of research has focused on this problem resulting in the development of numerous effi-cient algorithms. This research has been primarily focused on finding frequent patterns corresponding to itemsets and sequences ,but the ubiquitous nature of the problem has also resulted in the development of various algorithms that find frequent spatial ,geometric ,and topological patterns ,as well.
In recent years ,researchers have recognized that in many application areas and problem settings frequency is not the best measure to use in determining the significance of a pat-tern as it depends on a number of other parameters such as the type of items that it contains ,the length of the pattern , or various numerical attributes associated with the individ-ual items. In such cases ,even though frequent pattern dis-covery algorithms can still be used as a pre-processing step to identify a set of candidate patterns that are subsequently pruned by taking into account the additional parameters, they tend to lead to inefficient algorithms as a large number of the discovered patterns will eventually get eliminated. To address this problem ,various constrained frequent pattern mining problem formulations have been developed that en-able the user to focus on mining patterns with a rich class of constraints that capture the application semantics [14 ,25 , 19]. The key property of these itemset constraints is that they are usually (or can be converted to) anti-monotone or monotone ,making it possible to develop computationally ef-ficient algorithms to find the corresponding patterns.
In this paper we introduce a new class of constraints re-ferred to as block constraints ,which determine the signifi-cance of an itemset pattern by considering the dense block that is formed by the pattern X  X  items and its associated set of transactions. Specifically ,we focus on three different block constraints called block size , block sum ,and block similar-ity . The block size constraint applies to binary datasets ,the block sum constraint applies to datasets in which each in-stance of an item has a non-negative value associated with it that can vary across transactions ,and the block similarity constraint applies to datasets in which each transaction cor-responds to a vector-space representation of an object and the similarity between these objects is measured by the co-sine of their vectors. According to the block size constraint, a pattern is interesting if the size of its dense block (obtained by multiplying the length of the itemset and the number of its supporting transactions) is greater than a user-specified threshold. Analogously ,according to the block sum con-straint ,a pattern is interesting if the sum of the values of its dense block is greater than a user-specified threshold. Fi-nally ,according to the block similarity constraint a pattern is interesting if its dense block accounts for a certain user-specified fraction of the overall similarity between the objects in the entire dataset.

Finding patterns satisfying the above constraints has ap-plications in a number of different areas. For example ,in the context of market-basket analysis ,the block-size and block-sum constraints can be used to find the itemsets that account for a certain fraction of the overall quantities sold or rev-enue/profit generated ,respectively ,whereas in the context of document clustering ,the block similarity constraint can be used to identify the set of terms that bring a set of docu-ments together and thus correspond to thematically related words (commonly referred to as micro-concepts [11]).
Developing computationally efficient algorithms to find these block constraints is particularly challenging because unlike the different itemset-based constraints studied earlier ,these block constraints are tough as they are neither anti-monotone, monotone ,nor convertible [19]. To overcome this problem , we introduce a new class of pruning methods that can be used to significantly reduce the overall search space and make it possible to develop computationally efficient block pattern mining algorithms. Specifically ,we focus on the problem of finding the closed itemsets satisfying the proposed block con-straints and present a projection based mining framework, called CBMiner that takes advantage of a matrix-based rep-resentation of the dataset. CBMiner pushes deeply the var-ious block constraints into closed pattern mining by using three novel classes of pruning methods called column prun-ing , row pruning ,and matrix pruning that when combined lead to dramatic performance improvements. We present an extensive experimental evaluation using various datasets that shows that CBMiner not only generates more con-cise result set ,but also is much faster than the traditional frequent closed itemset mining algorithms. Moreover ,we present an interesting application in the context of docu-ment clustering that illustrates the usefulness of the block similarity constraint in micro-concept discovery.

The rest of the paper is organized as follows. Section 2 introduces some basic definitions and notations. Section 3 formulates the problem and motivates each one of the three block constraints. Section 4 describes some related work. Section 5 derives the framework for mining closed blocks, while Section 6 discusses in detail how to efficiently mine closed patterns with tough block constraints. The thorough performance study is presented in Section 7. Finally ,Sec-tion 8 provides some concluding remarks.
A transaction database is a set of transactions ,where each transaction is a 2-tuple containing a transaction id and a set of items. Let I be the complete set of distinct items and be the complete set of transactions. Any non-empty set of items is also called an itemset and any set of transactions is called a transaction set . The frequency of an itemset X (denoted as freq ( X )) is the number of transactions that con-tain all the items in X ,while the support of X is defined as  X  ( X )= freq ( X )/ |T| . For a given minimum support thresh-old  X  (0 &lt; X   X  1), X is said to be frequent if  X  ( X )  X  frequent pattern X is called closed if there exists no proper super-pattern of X with the same support as X .Anitem-set constraint C is a predicate on the power set 2 C :2 I  X  X  TRUE,FALSE } . An itemset constraint C is anti-monotone if for any itemset X that satisfies C ,allthe subsets of X also satisfy C ,and C is monotone if all the su-persets of X satisfy C . For example ,the constraint  X  ( X ) is anti-monotone ,while  X  ( X )  X   X  is monotone. An itemset constraint is tough if it is neither anti-monotone nor mono-tone ,and cannot be converted to either anti-monotone or monotone constraint.

A block is defined as a 2-tuple B =( I,T ) ,consisting of an itemset I and a transaction set T ,such that T is the supporting set of I .The size of a block B is defined as BSize ( B )= | I | X | T | .A weighted block is a block B =( I,T ) with a weight function w defined on the cross-product of the itemset and transaction set ,i.e. , w : I  X  T  X  X  + where R + is the set of positive real numbers. The sum of a weighted block B is defined as BSum ( B )= A (weighted) block B =( I,T ) is said to be a (weighted) closed block if and only if there exists no other (weighted) block B =( I ,T ) such that I  X  I and T = T .Givena (weighted) block B =( I,T ) ,a (weighted) block B =( I ,T ) is a proper superblock of B if I  X  I and T  X  T . Insucha case B is called a (weighted) proper subblock of B . We will use B  X  B to denote that B is a proper superblock of B and B  X  B to denote that B is a proper subblock of B .
A block constraint C is a predicate on 2 I  X  2 T ,i.e. , C : 2
I  X  2 T  X  X  TRUE,FALSE } .Ablock B is called a valid block for constraint C if it satisfies constraint C (i.e., C ( B ) is TRUE ). A block constraint C is a tough constraint if there is no dependency between the satisfaction/violation of a constraint by a block and the satisfaction/violation of the constraint by any of its superblocks or subblocks.

A transaction-item matrix M is a matrix where each row r represents a transaction and each column c represents an item in T such that the value of the ( r,c ) entry of the matrix, denoted by M ( r,c ) is one iff transaction r supports c ,oth-erwise M ( r,c ) is zero. Similarly a weighted transaction-item matrix M is a transaction-item matrix where for each row r and for each column c , M ( r,c )isequalto w ( r,c )(where w is a positive weight function defined on all transaction-item pairs in T ). A (weighted) block B =( I,T )canbe redefined as a (weighted) dense submatrix of the (weighted) transaction-item matrix M formed with the rows of T and columns of I such that  X  r  X  T and  X  c  X  I we have M ( r,c )= 1( M ( r,c ) &gt; 0).

Given a pre-defined ordering of the columns of M and a set p of columns in M ,a p -projected matrix w.r.t. M , M| is defined as the submatrix of M containing only the rows that support itemset p and the columns that appear after p in matrix M . For any transaction t in M| p ,its size is defined as the number of non-zero elements in its corresponding row of
M| p and will be denoted by | t | . For any column x of M| the matrix obtained by keeping only the rows of M| p that contain x is denoted as M| x p .Foreachmatrix M| p and M| x we will denote their set of corresponding transactions and items as T| p , T| x p , I| p ,and I| x p ,respectively.
Given a set of m -dimensional vectors A = { d 1 , d 2 ,..., d the composite vector of A is denoted by D and is defined to be composite vector of the block is denoted by B I and is the |I| -dimensional vector obtained as follows. For each item i  X  I ,the i th dimension of B I ,denoted by B I ( i ) ,is equal to
P a p -projected matrix M| p ,the composite vector of an item x within M| p is denoted by B x and is the |I| -dimensional vector obtained from the transactions included in T| x p such that for every i  X  X | x p , B x ( i )= i/
Given a matrix M ,the column-sum of column i in M is denoted by csum M ( i ) and is defined to be equal to the sum of the values of the column i of M ,i.e. , csum M ( i )= P by rsum M ( t ) and is defined to be equal to the sum of the values of the row t of M ,i.e. , rsum M ( t )=
In this paper we develop efficient algorithms for finding valid closed blocks that satisfy certain tough block constraints. Specifically ,we focus on three types of block constraints that are motivated and described in this section.
 Block Size Constraint In the context of market-basket analysis we are often interested in finding the set of itemsets each of which accounts for a certain fraction of the overall number of transactions that was performed during a certain period of time. Given an itemset I and its supporting set T ,the extent to which I will satisfy this constraint will de-pend on whether or not | I | X | T | is no less than the specified fraction. Finding this type of itemsets is the motivation be-hind the first block-constraint that we study ,which focuses on finding all blocks B =( I,T ) whose size is no less than a certain threshold. Specifically ,given a binary transaction database T ,the block-size constraint is defined as where 0 &lt; X   X  1and N is the total number of non-zeros in the transaction-item matrix of T ,i.e. , N =
Note that depending on the size of the itemsets associated with each valid block ,the minimum required size of the cor-responding transaction set will be different. Small itemsets will require larger transaction sets ,whereas large itemsets will lead to valid blocks with smaller transaction sets. As a result ,even if an itemset I is not part of a valid block ,an extension of I , I ,may become valid (e.g. ,cases in which the support of I does not significantly decrease compared to the support of I ). Similarly ,an itemset I which is not part of any valid block may contain subsets that are part of some valid blocks (e.g. ,cases in which the support of the subset is significantly greater than the support of I ). Consequently, the block-size constraint is a tough constraint as it is neither anti-monotone nor monotone ,and cannot be converted to either anti-monotone or monotone constraints.
 Block Sum Constraint In cases in which there is a non-negative weight associated with each individual transaction-item pair (e.g. ,sales or profit achieved by selling an item to a customer) ,in addition to finding all itemsets that satisfy a certain block-size constraint we may also be interested in finding the itemsets whose corresponding weighted blocks have a block-sum that is greater than a certain threshold. For example ,in the context of market-basket analysis ,these itemsets can be used to identify the product groups that account for a certain fraction of the overall sales ,profits , etc . Motivated by this ,the second block-constraint that we study extends the notion of the block-size constraint to weighted blocks. Formally ,given a transaction database T ,and a weight function w the block-sum constraint is defined as where 0 &lt; X   X  1and W is the sum of the weights of all the transaction-item pairs in the database ,i.e. , W = P t  X  T ,i  X  I w ( t,i ). Note that since the block-sum constraint is a generalization of the block-size constraint it also repre-sents a tough constraint.
 Block Similarity Constraint The last block constraint that we will study is motivated by the problem of find-ing groups of thematically related words in large document datasets ,each potentially describing a different micro-concept present in the collection. One way of finding such groups is to analyze the document-term matrix associated with the dataset and find sets of words that satisfy either a user spec-ified minimum support constraint or a block-size constraint (as defined earlier). However ,the limitation of these ap-proaches is that they do not account for the weights that are often associated with the various words as a result of the widely used tf-idf (term-frequency X  X nverse document-frequency) vector-space model. In general ,groups of words that have higher weights will more likely represent a the-matically coherent concept than words that have very low weights ,even if the latter groups have higher support. This often happens with words that are common in almost all the documents and will be assigned very low weight due to their high document frequency.

One way of addressing this problem is to first apply the tf-idf model on each document vector ,scale the resulting doc-ument vectors to be of the same length (e.g. ,unit length) , and then find the groups of related words by using the pre-viously defined block-sum constraint. However ,within the context of the vector-space model ,a more natural way of measuring the importance of a group of words is to look at how much they contribute to the overall similarity between the documents in the collection. In other words ,the micro-concept discovery problem can be formulated as that of find-ing all groups of words such that the removal of each group from their supporting documents will decrease the aggregate similarity between the documents by a certain fraction. In general ,groups of words that are large ,supported by many documents ,and have high weights will tend to contribute a higher fraction to the aggregate similarity and hence form better micro-concepts.

Discovering groups of words that satisfy the above prop-erty led us to develop the block-similarity constraint that is defined as follows. Let A = { d 1 ,d 2 ,...,d n } be a set of n doc-uments modeled by their unit-length tf-idf representation of the set of documents ,let m be the distinct number of terms in
A ,let B =( I,T ) be a weighted block with I being a set of words and T being its supporting set of documents ,let S be the sum of the pairwise similarities between the docu-ments in A ,and let S be the sum of the pairwise similarities between the documents in A obtained after zeroing-out the entries corresponding to block B .The similarity of the block B is defined to be the loss in the aggregate pairwise similar-ity resulting from removing B ,i.e. , BSim ( B )= S  X  S ,and the block-similarity constraint is defined as where 0 &lt; X   X  1.

In this paper ,we will measure the similarity between two documents d i and d j in A by computing the dot-product of their corresponding vectors d i and d j (i.e. ,sim( d i d  X  d j ). Since the documents in A have already been scaled to be of unit length ,this similarity measure is nothing more than the cosine of their respective vectors ,which is used widely in information retrieval. The advantage of the dot-product-based similarity measure is that it allows us to easily and efficiently compute both S and S . Specifically ,if D is the composite vector of A ,it can be shown that S = D  X  D . Similarly ,if B =( I,T )isaweightedblockof A ,and B I is its corresponding composite vector it can be shown that S =( D  X  B I )  X  ( D  X  B I ). As a result ,the similarity of a block B =( I,T )isgivenby
To simplify the presentation of the three block constraints and the associated algorithms ,in the rest of this paper we will consider the set of documents A as forming a weighted transaction-item matrix M whose rows and columns corre-spond to the documents and terms of A ,respectively. As a result ,each matrix entry M ( i,j ) will be equal to d i ( j ) (i.e., the value in the d i  X  X  vector along the j th dimension).
Efficient algorithms for finding frequent itemsets in large databases have been one of the key success stories in data mining research [2 ,5 ,3 ,9 ,29]. One of the early compu-tationally efficient algorithms was Apriori [2] ,which finds frequent itemsets of length l based on the previously mined frequent itemsets of length ( l -1). More recently ,a set of database-projection-based methods [1 ,9 ,20] have been de-veloped that significantly reduce the complexity of finding frequent long patterns. This study extends the projection-based method to mine valid sub-matrices with tough block constraints.

The frequent itemset mining algorithms usually generate a large number of frequent itemsets when the support is low. To solve this problem ,two general classes of techniques were proposed. The first is mining closed/maximal patterns. Typical examples include Max-Miner [3] ,A-close [17] ,MA-FIA [7] ,CHARM [29] ,CFP-tree [15] ,and CLOSET+ [24]. The redundant pattern pruning and column fusing methods adopted by CBMiner have been popularly used in different forms by several previous studies [3 ,28 ,21 ,7 ,29 ,24 ,15]. The second class focuses on mining constrained patterns by integrating various anti-monotone ,monotone ,or convertible constraints. The constrained association rule mining prob-lem was first considered in [23] but only for item specific constraints. Since then a number of different constrained fre-quent pattern mining algorithms have been proposed [4 ,16 , 19 ,6 ,18 ,13 ,12]. All these algorithms concentrate on con-strained itemset mining with various anti-monotone ,mono-tone ,succinct or convertible constraints.

Very recently some work [26] has been done to push ag-gregate constraints in the context of iceberg-cube computing. This algorithm mines aggregate constraints in the GROUP BY partitions of an SQL query by using a divide-and-appro-ximate strategy. The algorithm makes use of the strategy to derive a sequence of weaker anti-monotone constraints for a given non-anti-monotone constraint to prune the nodes in the search tree. Recently the LPMiner algorithm [22] was proposed to mine itemsets with length-decreasing support constraints. It uses a novel SVE property to prune the un-promising transactions of the projected databases based on the length of the transactions. Later the SVE property has been used to mine closed itemsets with length decreasing support constraints [27]. We also explore the SVE property in the context of mining closed patterns with block constraints in Section 6.2 to prune the unpromising rows of a prefix-projected matrix.
In this section we describe the ClsdPtrnMiner algo-rithm ,which forms the basis of CBMiner algorithm. Cls-dPtrnMiner follows the widely used projection-based pat-tern mining paradigm [1 ,9 ,20] ,which can be used to ef-ficiently mine the complete set of frequent patterns in a depth-first search order and as we will see later ,it can be easily adapted to mine valid closed block patterns. A key characteristic of ClsdPtrnMiner (as well as CBMiner ) is that it represents the transaction database T using the transaction-item matrix M and employs a number of effi-cient sparse matrix storage and access schemes ,allowing it to achieve high computational efficiency. For the remain-der of this section we describe the basic structure of dPtrnMiner for the problem of enumerating all patterns satisfying a constant minimum support constraint and then introduce several pruning methods to accelerate the frequent closed pattern mining. The extension of this algorithm for finding the closed blocks that satisfy the three tough block constraints described in Section 3 will be described later in Section 6.
Given a database ,the complete set of itemsets can be orga-nized into a lattice if the items are in a predefined order ,and the problem of frequent pattern mining then becomes how to traverse the lattice to find the frequent ones. The ClsdP-trnMiner algorithm adopts the depth-first search traversal and uses the downward closure property to prune the in-frequent columns from further mining. Figure 1(a) shows a database example with a minimum support 0.5. If we re-move the set of infrequent columns, { b,f,h,i,k,m } ,and sort the set of frequent columns in frequency-increasing order, then part of the lattice (i.e. ,pattern tree) formed from col-umn set { g,a,c,e,d } can be organized into the one shown in Figure 1(b). Each node in the lattice is labeled in the form p : q ,where p is a prefix itemset and q is the set of lo-cal columns appeared in the p -projected matrix, M| p .Ata certain node during the depth-first traversal of the lattice, if the corresponding prefix p is infrequent ,we stop mining the sub-tree under this node. Otherwise ,we report p as a frequent pattern ,build its projected matrix , M| p locally frequent columns in M| p and use them to grow p to get longer itemsets.

To store the various projected matrices efficiently ,we adopt the CSR sparse storage scheme [8]. The CSR format utilizes two one-dimensional arrays: the first stores the actual non-zero elements of the matrix in a row (or column) major order, Figure 1: (a) A transaction database with  X   X  0 . 5 ; (b) The pattern tree. and the second stores the indices corresponding to the be-ginning of each row (or column). To ensure that both the matrix projection as well as the column frequency count-ing are performed efficiently ,we maintain both the row-and the column-based representation of the matrix. The overall complexity of the algorithm depends on the two key steps of sorting and projecting. We used the radix sort algorithm to sort the column frequencies which has a time complexity that is linear in the number of columns being sorted ,and because of our matrix-storage scheme ,projecting the ma-trix on the column is linear on the number of non-zeros in the projected matrix. Our matrix-projection based pattern enumeration method shares some of the ideas with the re-cently developed array-projection based method [20] ,which was shown to achieve good performance ,especially for sparse datasets.
The above frequent pattern enumeration method can find the complete set of frequent itemsets. To get the set of fre-quent closed itemsets ,we need to check whether a newly found itemset is closed or not and sift out the redundant (i.e., non-closed) ones. The pattern closure checking in ClsdP-trnMiner works as follows. We maintain the set of frequent closed itemsets mined so far in a hash-table H using the sum of the transaction-IDs of the supporting transactions as the hash-key [28 ,29]. Upon getting a new itemset p ,we check against the set of already mined closed itemsets which have the same hash-key value as the one derived from p  X  X  sum of transaction-IDs ,to see if there is any itemset that is a proper superset of p with the same support. If that is the case, p is non-closed ,otherwise the union of p and the set of its local columns with the same support as p forms a closed itemset.
In the pattern enumeration process ,some prefix itemsets or columns are unpromising to generate closed itemsets and thus can be pruned. ClsdPtrnMiner adopts two pruning methods, redundant pattern pruning and column fusing [3, 28 ,21 ,7 ,29 ,24]. 1. Redundant Pattern Pruning (RPP) Once we 2. Column Fusing (CF) This optimization performs
By integrating the above optimization methods with the frequent pattern enumeration process ,we get the ClsdP-trnMiner algorithm as shown in Algorithm 5.1. It takes as input the current pattern p ,the p -projected matrix M| the given minimum support  X  ,and the current hash-table H . The algorithm initially sorts the columns of M| p and elimi-nates any infrequent columns and then proceeds to perform Column Fusing . After that it enters its main computational loop which extends p by adding each column a  X  X | p ,checks to see if p  X  X  a } can be pruned by comparing it against H ( Redundant Pattern Pruning ) ,projects M| p on a ,checks to see if p  X  X  a } is closed ,and finally calls itself recursively for pattern p  X  X  a } .

Like the traditional frequent closed pattern mining algo-rithms, ClsdPtrnMiner works under the constant support threshold framework and uses the downward closure prop-erty to prune infrequent columns. However ,with tough block constraints ,the nice properties derived from the anti-monotone (or monotone) constraints no longer hold to be used to prune search space. Designing effective pruning methods for tough block constraints is especially challeng-ing. To address this challenge we developed three classes of pruning methods ,called column-pruning , row-pruning and matrix pruning ,which eliminate the unpromising columns , rows and projected matrices from mining. The specific de-tails of these pruning methods are different for each of the three block constraints and will be described later in this section.

By incorporating these three pruning methods with the overall structure of ClsdPtrnMiner ,we can easily derive the CBMiner algorithm that mines the set of all valid closed block patterns. The pseudo code for CBMiner is shown in Algorithm 6.1. It takes as input the current pattern p ,the p -projected matrix M| p ,the hash-table H that stores the valid closed blocks that were discovered so far ,and the block-constraint C corresponding to either the block-size ,block-sum ,or block-similarity constraint. Since it is derived from ClsdPtrnMiner algorithm ,it has many steps in common and for this reason we will only describe its key differences. The first difference has to do with the pruning methods. Specifically ,instead of using the constant support-based col-umn pruning, CBMiner uses the newly proposed column-pruning , row-pruning and matrix pruning methods ,which are derived from the tough block constraints. The second difference has to do with the implementation of the column fusion optimization for the block-sum and block-similarity constraints. In the case of the block-sum constraint ,the val-ues of the fused columns correspond to the sum of the values of their constituent columns. This ensures that the resulting fused matrix contains all necessary information to correctly evaluate the constraints. In the case of the block-similarity constraint ,since the correct evaluation of the constraints re-quires access to the individual column-values ,we do not per-form any column fusion.

Following we will introduce in detail the three pruning methods, column-pruning , row-pruning and matrix pruning , in terms of the three different block constraints. Note that the details of the proofs of the Lemmas appeared in this section can be found in [10].
Given a prefix itemset p and its projected matrix M| p ,the idea behind column pruning is to identify for each column x  X  X | p a necessary condition that must be satisfied such that there is a valid block B =( p  X   X , T| p  X   X  )forwhich  X  is a subset of the columns in M| p and x  X   X  .Usingthis condition ,we can then eliminate from M| p all the columns that do not satisfy it ,as these columns cannot be part of a valid block that contain p . Note that for each column x that we eliminate ,we prevent the exploration of the sub-tree associated with the pattern p  X  X  x } ,thus ,significantly reducing the overall search space.
The necessary condition for the block-size constraint is encapsulated in the following lemma (Refer to Section 2 for a description of the notation used).

Lemma 6.1. ( Block-Size Column Pruning )Let p be a pattern and x acolumnin M| p .Theninorderfor x to be part of a valid block that satisfies the block-size constraint of Equation 1 and is obtained from extending p by adding columns from M| p , the following must hold:
For each column in M| p ,Equation 5 can be evaluated by adding up the lengths of the rows that it supports. These sums can be computed for all the columns by performing a single scan of the p -projected matrix.
The necessary condition for the block-sum constraint is similar in nature to that of the block-size constraint and is encapsulated in the following lemma.

Lemma 6.2. ( Block-Sum Column Pruning )Let p be a pattern and x acolumnof M| p .Theninorderfor x to be part of a valid block that satisfies the block-sum constraint of Equation 2 and is obtained by extending p with columns in M| p , the following must hold:
Note that the summation on the left-hand-side of Equa-tion 6 is nothing more than the sum of the non-zero elements of each row in T| x p .

The various quantities required to evaluate Equation 6 can be computed efficiently by performing a single scan of the block ( p, T| x p ) to compute the sum of each row ,and two scans of the matrix M| p . The first scan will compute the sum of the non-zero elements of each row ,and the second scan will compute the summation term in Equation 6 for each column.
Let D be the composite vector of T and consider a p -projected weighted matrix M| p . The necessary condition for the block-similarity constraint is encapsulated in the fol-lowing lemma.

Lemma 6.3. ( Block-Similarity Column Pruning )Let p be a pattern, ( p, T| p ) its corresponding block, and x acol-umn of M| p .Theninorderfor x to be part of a block that satisfies the block-similarity constraint of Equation 3 and is obtained by extending p with columns in M| p , the following must hold:
For each column of M| p ,evaluating the above equation incurs a computational cost equivalent to one scan of the p -projected matrix ,which is very costly. So ,we make use of the following lemma ,which approximates Equation 7. Lemma 6.4. ( Approximate Block-Similarity Column Pruning )Let  X  be the maximum value across the m dimen-sions of vector D and  X  be the maximum row-sum over all the rows of the p -projected matrix M| p .Theninorderfor x to be part of a block that satisfies the block-similarity constraint of Equation 3 and is obtained by extending p with columns in
M| p , the following must hold:
In a single scan of the projected matrix ,we can compute the frequency of all its columns along with the value of  X  . Hencethecomplexityisoftheorderofthesizeofthepro-jected matrix.
Given a pattern p and its projected matrix M| p ,the idea behind row pruning is to identify for each row t  X  X | necessary condition that must be satisfied such that there is a valid block B =( p  X   X , T| p  X   X  )forwhich  X   X  t .Using this condition ,we can then eliminate from M| p all the rows that do not satisfy it ,as these rows cannot be part of a valid block that contain p . By eliminating such rows we reduce the size of M p and thus reduce the amount of time required to perform subsequent projections and enhance future column pruning operations.
 To derive such conditions we make use of the Smallest Valid Extension ( SVE ) principle ,originally introduced in [22] for finding itemsets with length-decreasing support constraint. In the context of block constraints considered in this paper, the smallest valid extension of a prefix p is defined as the length of the smallest possible extension  X  to p (where  X  is a set of columns in M| p ) ,such that the resulting block B =( p  X   X , T| p  X   X  ) is valid for a given constraint C .Thatis, Knowing the SVE of a pattern ,we can then eliminate all the rows whose length is smaller than the SVE value. Note that the SVE of a pattern that already corresponds to a valid block will be by definition zero. For this reason ,the row-pruning is only applied when the pattern p does not correspond to a valid block.
 In the rest of this section we describe how to obtain such SVE -based necessary conditions for the block-size ,block-sum ,and block-similarity constraints.
The SVE of a pattern p for the block-size constraint is given by the following lemma.

Lemma 6.5. ( Block-Size Row Pruning )Let p be a pat-tern such that B =( p, T| p ) does not satisfy the block-size constraint. Then the smallest valid extension of p for the block-size constraint of Equation 1 is
The complexity of computing the SVE ( p ) is  X (1).
The SVE of a pattern p for the block-sum constraint is given by the following lemma.

Lemma 6.6. ( Block-Sum Row Pruning )Let p be a pat-tern such that B =( p, T| p ) does not satisfy the block-sum constraint, and z be the maximum column-sum over all columns of
M| p . Then the smallest valid extension of p for the block-sum constraint of Equation 2 is
The complexity of computing the SVE ( p ) is of the order of the size of the projected matrix as we need one scan of the projected matrix to compute the maximum of the column-sums.
Let D be the composite vector of T and consider a p -projected weighted matrix M| p .The column-similarity of column x in M| p is denoted by csim M | p ( x ) and is defined to be equal to Given this definition ,the SVE of a pattern p for the block-similarity constraint is given by the following lemma.
Lemma 6.7. ( Block-Similarity Row Pruning )Let p be a pattern such that B =( p, T| p ) does not satisfy the block-similarity constraint, and z is the maximum column-similarity over all columns of M| p . Then the smallest valid extension of p for the block-similarity constraint of Equation 3 is
The complexity of computing the SVE ( p )isidenticalto that for the block-sum constraint.
Given a prefix itemset p and its projected matrix M| p ,the column pruning and row pruning methods are very effective in pruning some unpromising columns and rows from M| p . However ,in many cases the whole projected matrix M| cannot be used to generate any valid block patterns and thus can be pruned. Hence we developed another class of pruning method called matrix pruning in order to further prune the search space in terms of the block size , block sum ,and block similarity constraints.
The necessary condition for the block-size constraint is encapsulated in the following lemma.

Lemma 6.8. ( Block-Size Matrix Pruning )Let p be a pattern and t a transaction in M| p .Theninorderfor M| p to be used to generate any valid block that satisfies the block-size constraint of Equation 1 and is obtained by extending p with some columns in M| p , the following must hold:
The sums in Equation 12 can be computed by a single scan of the p -projected matrix M| p .
The necessary condition for the block-sum constraint is stated in the following lemma.

Lemma 6.9. ( Block-Sum Matrix Pruning )Let p be a pattern, x acolumnin M| p ,and t a transaction in M| p . Then in order for M| p to be used to generate any valid block that satisfies the block-sum constraint of Equation 2 and is obtained by extending p with some columns in M| p ,thefol-lowing must hold:
Note that the summation on the left-hand-side of Equa-tion 13 is nothing more than the sum of the non-zero ele-ments of each row in T| p and can be computed in one scan of the p -projected matrix. Using the definition of the column-similarity introduced in Section 6.2.3 ,the necessary condition for the block-similarity constraint can be stated as follows:
Lemma 6.10. ( Block-Similarity Matrix Pruning )Let p be a pattern, x acolumnof M| p ,and csim M| p ( x ) the column-similarity of x in M| p .Theninorderfor M| p to be used to generate any valid blocks that satisfy the block-similarity constraint of Equation 3 and is obtained from ex-tending p with some columns in M| p , the following must hold:
The column-similarities of all the columns can be com-puted in a single scan of the p -projected matrix.

We evaluated the performance of CBMiner for finding blocks that satisfy the three block constraints using four real datasets ( gazelle , pumsb* , big-market ,and sports )andase-quence of synthetic datasets ( T10I4Dx ). The characteris-tics (number of transactions ,number of items and the av-erage(maximum) transaction lengths) of these datasets are shown in the Table 1. The gazelle dataset contains the click-stream data from Gazelle.com. The pumsb* dataset contains census data and big-market dataset contains the transaction information of a retail store. The sports dataset is a doc-ument dataset obtained from San Jose Mercury (TREC). The synthetic dataset series T10I4Dx were generated from IBM dataset generator ,with average transaction length of 10 ,number of distinct items of 10 ,000 ,and average frequent itemset length of 4. This dataset was used for scalability tests by varying the number of transactions from 200k to 1000k. All the experiments were performed on a 2GHz Intel P4 pro-cessor with 2GB of memory running Linux. CBMiner was implemented in C.
The experimental evaluation was performed along three different dimensions. First ,we compared the performance of the various pruning methods used by CBMiner for dif-ferent datasets and block constraints. Second ,we evaluated the scalability characteristics of CBMiner as the number of transactions increases. Third ,we compared CBMiner  X  X  per-formance against that achieved by traditional closed frequent itemset mining algorithms. The motivation behind this com-parison is twofold: (i) it allows us to verify the extent to which the closed block constraints lead to a more concise set of patterns than that produced by existing closed frequent itemset approaches ,and (ii) it provides a reference point by which to judge the underlying efficiency of CBMiner  X  X  im-plementation. Fig 2. Pruning methods (gazelle) Fig 3. Pruning methods (pumsb*)
We evaluated the effectiveness of the three newly pro-posed pruning methods, Column Pruning (CP) , Row Prun-ing (RP) ,and Matrix Pruning (MP) ,and their combination (CP+RP+MP). Fig. 2 shows these results for the BSize con-straint and dataset gazelle ,Fig. 3 shows the results for the BSum constraint and dataset pumsb* ,while Fig. 4 shows the results for the BSim constraint and dataset big-market . These results show that the combination of all the three pruning techniques is always faster than each individual prun-ing method ,and for the BSize and BSum constraints the overall ranking of the pruning effectiveness among the three methods is Column Pruning &gt; Matrix Pruning &gt; Row Prun-ing ,while for the BSim constraint Matrix Pruning is more effective than Row Pruning and Column Pruning .

Note that if we do not apply any of the three pruning meth-ods, CBMiner degenerates to ClsdPtrnMiner (denoted as No-Pruning ) and it performs poorly. For example ,without any pruning for the gazelle dataset with BSize constraint of 0.4% its runtime is 205.23 seconds ,while the corresponding runtime for CP+RP+MP is only 2 seconds (as shown in Fig. 2). For this reason we do not show the curves corresponding to No-Pruning in Figs. 2 X 4.
We used the synthetic dataset series T10I4Dx for the scal-ability test of CBMiner ,where  X  X  X  indicates the base size and varies from 200 K to 1000 K tuples. In the experiments we fixed the BSize , BSum ,and BSim threshold all at 0 . 01%. From Fig. 5 ,we can see that CBMiner has linear scalability on all the three constraints in terms of the base size.
In comparing with closed pattern mining algorithms ,we chose one of the recently developed closed itemset mining algorithms ,CLOSET+ [24] ,for our comparisons. We com-pared CBMiner with CLOSET+ by providing the minimum frequency of the valid closed block patterns generated by CBMiner as the absolute minimum support to CLOSET+. This ensures that CLOSET+ will discover all the patterns found by CBMiner . However ,CLOSET+ will find addi-tional patterns that do not satisfy the block constraints.
We performed numerous experiments to compare CBMiner with CLOSET+ for all the three block constraints and us-ing the datasets shown in Table 1. Due to limited space ,we only show part of the results. Figs. 6 X 7 show the compari-son results for the BSize constraint and dataset gazelle ,while Figs. 8 X 9 show the results for the BSum constraint and the sports dataset. The results show that in general, CBMiner is substantially faster than CLOSET+. This is primarily due to the fact that ,as it was expected ,CLOSET+ produces sig-nificantly more patterns than those produced by CBMiner . For datasets with short transactions like gazelle and big-market , CBMiner can be order(s) of magnitude faster than CLOSET+ ,and finds order(s) of magnitude fewer patterns. While for the datasets with long transactions like pumsb* , and sports ,CLOSET+ is a little faster at high block thresh-old of BSize and BSum ,but once the threshold is lowered , there is an explosive increase in the number of frequent closed itemsets (e.g. ,with BSize / BSum 0 . 2% CLOSET+ generates several orders of magnitude more patterns than CBMiner ). These results illustrate that the pruning methods used by CBMiner are indeed effective in reducing the overall search space ,leading to substantial performance improvements. Table 2: Summary of document datasets used for the application.

Finally ,we demonstrate an application for the three block constraints in the context of document clustering by show-ing that the blocks discovered by these constraints repre-sent sets of documents that have a great chance of belong-ing to the same cluster and hence can be used to identify potential cores of natural clusters in data as well as the-matically related words. For this application we chose two additional document datasets viz., LA1 and Classic in ad-dition to Sports .The LA1 dataset contains articles that appeared in LA Times news ,whereas the Classic dataset contains abstracts of technical papers. Some of the charac-teristics of these datasets are shown in Table 2. We scaled the document vectors using the well known tf-idf scaling and normalized using L 2-norm and used our closed block min-ing algorithm with block size ,block-sum and block-similarity constraints. From the patterns that were found we chose the 1000 highest ranked patterns on the basis of the constraint value. For example ,for the block sum constraint ,we se-lected the top-1000 blocks ranked on block sum and in the same way for block-size and block-similarity constraints. For each of the top-1000 blocks we computed the entropies of the documents that formed the supporting set of the block and took the average of the 1000 entropies. Similarly ,we com-puted the average block pattern frequency and average block pattern length. For comparison purposes ,we also used the CLOSET+ algorithm to find a set of frequent closed itemsets and also selected the 1000 most frequent itemsets discovered by CLOSET+. Fig. 10 shows the average entropy ,frequency , and length of the various patterns discovered by the four al-gorithms for the three datasets. Note that the CLOSET+ results are labeled as  X  X req X .

From these results we can see that the average entropy of the patterns discovered by the four schemes are quite small ,indicating that all of them do reasonably well in iden-tifying itemsets whose supporting documents are primarily from a single class. Despite that ,we can see that the block-similarity constraint outperforms the rest ,as it leads to the lowest entropies (i.e. ,purest clusters) for all datasets. This verifies our initial motivation for defining the block-similarity constraint ,as it is able to better capture the characteristics of the underlying datasets and problem ,and discover sets of words that are thematically very related. The block-size and the itemset support constraints show some inconsistency in finding good concepts as they do not account for the weights associated with the terms in the document-term matrices. On the other hand the block-sum constraint does reasonably well as it was able to take into account the differences in the terms weights provided by the L 2-norm and tf-idf scal-ing for the document vectors. Also note that the highest ranked patterns discovered by the frequent closed mining al-gorithm (CLOSET+) are in general quite short compared to the length of the patterns discovered by the block con-straints.
In this paper we studied how to mine valid closed pat-terns with tough block constraints and proposed a matrix-projection based framework called CBMiner for mining closed block patterns in transaction-item or document-term ma-tricies effectively. Under this framework we mainly discussed three typical block constraints viz., block size , block sum and block similarity . Some widely adopted properties derived from the anti-monotone or monotone constraints no longer hold to be used to prune search space for these tough block constraints. As a result ,we specifically proposed three novel pruning methods, column pruning , row pruning and matrix pruning ,which can push deeply the block constraints into pattern discovery and prune the unpromising columns ,rows , and projected matrices effectively.

The research in this paper can be extended along two dif-ferent directions. First ,the CBMiner algorithm and its pruning methods assume that the entire dataset can fit into the main memory ,which is not true for very large datasets. Extending the matrix-based projection approach along with the row-,column-,and matrix-pruning methods to a disk-based implementation is a required step for mining these datasets. Second ,we believe that the underlying principles utilized by the three pruning methods are quite general and can be used (i) by other frequent pattern mining approaches, and (ii) to prune the search space of other tough constraints. Identifying the conditions under which such extensions are possible can greatly help in extending existing algorithms and expanding the type of tough constraints that can effi-ciently be solved.
