 Modern computing systems generate large amounts of log data. System administrators or domain experts utilize the log data to understand and optimize system behaviors. Most system logs are raw textual and unstructured. One main fundamental challenge in automated log analysis is the gen-eration of system events from raw textual logs. Log messages are relatively short text messages but may have a large vo-cabulary, which often result in poor performance when ap-plying traditional text clustering techniques to the log da-ta. Other related methods have various limitations and only work well for some particular system logs. In this paper, we propose a message signature based algorithm logSig to gen-erate system events from textual log messages. By searching the most representative message signatures, logSig catego-rizes log messages into a set of event types. logSig can handle various types of log data, and is able to incorporate human X  X  domain knowledge to achieve a high performance. We conduct experiments on five real system log data. Ex-periments show that logSig outperforms other alternative algorithms in terms of the overall performance.
 I.5.4 [ Pattern Recognition ]: Applications; H.4.m [ Information Systems Applications ]: Miscellaneous; G.2.3 [ Discrete Mathematics ]: Applications Algorithms, Experimentation Event generation, Message signature, System logs
Modern computing systems generate large amounts of log data. The log data describes the status of each component and records system internal operations, such as the starting and stopping of services, detection of network connections, software configuration modifications, and execution errors. System administrators or domain experts utilize the log data to understand and optimize system behaviors.

Most system logs are raw textual and unstructured. There are two challenges in analyzing system log data. The first challenge is transforming raw textual logs into system events. The number of distinct events observed can be very large and also grows rapidly due to the large vocabulary size as well as various parameters in log generation [6]. Once raw tex-tual logs are transformed into events, the second challenge is to develop efficient algorithms to analyze or summarize the patterns from events. A lot of studies investigate the second challenge and develop many algorithms to mine sys-tem events [22] [30] [13] [15] [10] [20] [29] [14]. In this paper, we focus on the first challenge. The traditional solution to the first challenge is to develop a specialized log parser for a particular system. However, it requires users fully under-stand all kinds of log messages from the system. In practice, this is time-consuming, if not impossible given the complex-ity of current computing systems. In addition, a specialized log parser is not universal and does not work well for other types of systems.

Recent studies [6] [18] [26] apply data clustering tech-niques to automatically partition log messages into different groups. Each message group represents a particular type of events. Due to the short length and large vocabulary size of log messages [24], traditional data clustering meth-ods based on the bag-of-word model cannot perform well when applied to the log message data. Therefore, new clus-tering methods have been introduced to utilize both the for-mat and the structure information of log data [6] [18] [26]. However, these methods only work well for strictly format-ted/structured logs and their performances heavily rely on the format/structure features of the log messages. To ad-dress these limitations, this paper proposes a message sig-nature based algorithm logSig to generate system events from raw textual logs. It can handle various types of log data, and is able to incorporate human X  X  domain knowledge to achieve a high performance.
Each log message consists of a sequence of terms. Some of the terms are variables or parameters for a system even-t, such as the host name, the user name, IP address and so on. Other terms are plain text words describing seman-tic information of the event. For example, three sample log messages of the Hadoop system [3] describing one type of events about the IPC (Inter-Process Communication) sub-system are listed below:
The three messages contain many different words(or terms), such as the date, the hours, the handler name, and the port number. People can identify them as the same even-t type because they share a common subsequence:  X  INFO: org.apache.hadoop.ipc.Server: IPC Server:starting  X . Let X  X  consider how the three log messages are generated by thesystem. TheJavasourcecodeforgeneratingthemis described below: where logger is the log producer for the IPC subsystem. Us-ing different parameters, such as handlerName , the code can output different log messages. But the subsequence  X  INFO: org.apache.hadoop.ipc.Server: IPC Server : start-ing  X  is fixed in the source code. It will never change unless the source code has been modified.

Therefore, the fixed subsequence can be viewed as a sig-nature for an event type. In other words, we can check the signatures to identify the event type of a log message. Oth-er parameter terms in the log message should be ignored, since messages of the same event type can have different parameter terms. Note that some parameters, such as the handlerName in this example, consist of different numbers of terms. Consequently, the position of a message signa-ture may vary in different log messages. Hence, the string matching similarity proposed in [6] would mismatch some terms. Another method IPLoM proposed in [18] also fails to partition log messages using the term count since the length of handlerName is not fixed and three log messages have different numbers of terms.

Given an arbitrary log message, we do not know in ad-vance which item is of its signature, or which term is its parameter. That is the key challenge we aim to address in this paper.
In this paper, we first describe the drawbacks of tradition-al text clustering techniques for event generation from log messages. We show that, it is difficult for the bag-of-word model to accurately partition log messages. We also analyze that, the string kernel based approach would be inefficient when the log vocabulary size is large. In addition, we discuss some limitations of related approaches proposed in previous literatures.

Then, we propose logSig algorithm to generate system events from textual log messages. logSig algorithm tries to find k message signatures to match all given messages as much as possible, where k is specified by the user. We con-duct experiments on five real system log data. Experiments show that logSig outperforms other alternative algorithms in terms of the overall performance.

The rest of the paper is organized as follows: Section 2 de-scribes the related work about system event generation from textual logs. Then, we formulate the problem of the system events generation in Section 3. In Section 4, we present an overview of the logSig algorithm. Section 5 discusses some detailed implementation issues. Section 6 proposes t-wo approaches for incorporating the domain knowledge to improve the accuracy of the logSig algorithm. In Section 7, we present the experimental studies on five real system log data. Finally, Section 8 concludes our paper and discusses the future work.
It has been shown in [24] that log messages are relative-ly short text messages but could have a large vocabulary size. This characteristic often leads to a poor performance when using the bag-of-words model in text mining on log data. The reason is that, each single log message has only a few terms, but the vocabulary size is very large. Hence, the vector space established on sets of terms would be very sparse. The string kernel can be used to extract deep se-mantic information (e.g., the order of terms) to improve the performance of the clustering algorithm. It maps a string to a high dimensional vector to represent all possible ter-m orders. However, because of the large vocabulary size, the dimensionality of the transformed space would be very high. Although the kernel trick does not have to explicitly create those high dimensional vectors, clustering algorithms would still be influenced by the high dimensionality due to the Curse of Dimensionality [25].

The related work about the log data analysis can be broad-ly summarized into two categories. One category is on sys-tem event generation from raw log data [6] [9] [18] [26] and the other category is on analyzing patterns from system events [22] [30] [13] [15] [10] [29] [14]. Our work in this pa-per belongs to the first category. A word matching similarity measurement is introduced in [6] for clustering the log mes-sages. One problem is that, if most terms of a log message are parameter terms, then this type of log messages may not have many matched common words. This method is de-noted as StringMatch in this paper. [18] develops a 4-steps partitioning method IPLoM for clustering the log messages based on inherent characteristics of the log format. Howev-er, the method can only be useful for strictly formatted log data. The logSig algorithm proposed in this paper can han-dle various types of log data without much prior knowledge about the log format.
The goal of this paper is to identify the event type of each log message according to a set of message signatures. Given a log message and a set of signatures, we need a metric to determine which signature best matches this log message. Therefore, we propose the Match Score metric first. Notations: Let D be a set of log messages, D = { X 1 , ..., X where X i is the i th log message, i =1 , 2 , ..., N .Each X a sequence of terms, i.e., X i = w i 1 w i 2 ....w i n i . A message signature S is also a sequence of terms S = w j 1 w j 2 ....w Given a sequence X = w 1 w 2 ...w n and a term w i , w i  X  indicates w i is a term in X . X  X  X  w i } denotes a subsequence w ...w i  X  1 w i +1 ...w n . | X | denotes the length of the sequence X . LCS ( X, S )denotesthe Longest Common Subsequence between two sequences X and S .

Definition 1. ( MatchScore ) Given a log message X i and a message signature S ,the match score is computed by the function below: match ( X i ,S )= | LCS ( X i ,S ) | X  ( | S | X  X  LCS ( X i Intuitively, | LCS ( X i ,S ) | is the number of terms in X with S . | S | X  X  LCS ( X i ,S ) | is the number of terms in X matched with S . match ( X i ,S ) is the number of matched terms minus the number of not-matched terms. We illus-trate this by a simple example below:
Example 1. A log messages X = abcdef andames-sage signature S = axcey . The longest common subsequence LCS ( X, S )= ace . The matched terms are X  a  X , X  c  X , X  e  X , shown by underline words in Table 1.  X  x  X  X nd X  y  X  X n S are not matched with any term in X .Hence, match ( X,S )= | ace | X  | xy | =1 .
 Note that this score can be negative. match ( X i ,S )isused to measure the degree of the log message X i owning the signature S . If two log messages X i and X j have the same signature S , then we regard X i and X j as of the same event type. The longest common subsequence matching is a widely used similarity metric in biological data analysis [7] [19], such as RNA sequences.
If all message signatures S 1 , S 2 ,..., S k are known, identi-fying the event type of each log message in D is straight-forward. But we don X  X  know any message signature at the beginning. Therefore, we should partition log messages and find their message signatures simultaneously. The optimal result is that, within each partition, every log message match-es its signature as much as possible. This problem is formu-lated below.

Problem 1. Given a set of log messages D and an in-teger k , find k message signatures S = { S 1 , ..., S k } k -partition C 1 ,..., C k of D to maximize The objective function J ( S , D ) is the summation of all match scores. It is similar to the k -means clustering problem. The choice of k depends on the user X  X  domain knowledge to the system logs. If there is no domain knowledge, we can bor-row the idea from the method finding k for k -means [11], which plots clustering results with k . We can also display generated message signatures for k =2 , 3 ,.. until the results can be approved by experts.
 Problem 1 is similar to the classic k -means clustering prob-lem, since a message signature can be regarded as the repre-sentative of a cluster. People may ask the following question-s: Why we propose the match function to find the optimal partition? Why not use the LCS as the similarity function to do k -means clustering ? The answer for the two ques-tions is that, our goal is not to find good clusters of log messages, but to find the message signatures of all types of log messages. K -means can ensure every two messages in one cluster share a subsequence. However, it cannot guar-antee that there exists a common subsequence shared by all (or most) messages in one cluster. We illustrate this by the following example.
 Example 2. There are three log messages X 1 :  X  X bcdef, X :  X  X bghij X  and X 3 :  X  X yghef X . Clearly, LCS ( X 1 ,X 2 ) = 2 , LCS ( X 2 ,X 3 ) = 2 ,and LCS ( X 1 ,X 3 ) = 2 . However, there is no common subsequence that exists among all X 1 , X 2 and X . In our case, it means there is no message signature to describe all three log messages. Hence, it is hard to believe that they are generated by the same log message template.
Problem 1 is an NP-hard problem, even if k =1. When k = 1, we can reduce the Multiple Longest Common Sub-sequence problem to the Problem 1. The Multiple Longest Common Subsequence problem is a known NP-hard [17]. Lemma 1. Problem 1 is an NP-hard problem when k =1 . Proof: Let D = { X 1 , ..., X N } .When k =1, S = { S 1 } Construct another set of N sequences Y = { Y 1 , ..., Y N whicheachtermisuniqueinboth D and Y .Let D = D X  X  , J (
S , D )= X Let S  X  1 be the optimal message signature for D , i.e., Then, the longest common subsequence of X 1 ,..., X N must be an optimal solution S  X  1 . This can be proved by contradic-tion as follows. Let S lcs be the longest common subsequence of X 1 ,..., X N .Notethat S lcs may be an empty sequence if there is no common subsequence among all messages. Case 1: If there exists a term w i  X  S  X  1 , but w i /  X  S w i /  X  S lcs , w i is not matched with at least one message in X 1 ,..., X N .Moreover, Y 1 ,..., Y N are composed by unique terms, so w i cannot be matched with any of them. In D , the number of messages not matching w i is at least N +1, which is greater than the number of messages matching w i Therefore, which contradicts with S  X  1 =argmax Case 2: If there exists a term w i  X  S lcs , but w i /  X  S Since w i  X  S lcs , X 1 ,..., X N all match w i . The total number of messages that match w i in D is N . Then, there are N remaining messages not matching w i : Y 1 ,..., Y N . Therefore, which indicates S lcs is also an optimal solution to maximize objective function J on D .

To sum up the two cases above, if there is a polynomial time-complexity solution to find the optimal solution S  X  D ,the Multiple Longest Common Subsequence problem for X ,..., X N can be solved in polynomial time as well. How-ever, Multiple Longest Common Subsequence problem is an NP-hard problem [17].
Lemma 2. If when k = n Problem 1 is NP-hard, then when k = n +1 Problem 1 is NP-hard, where n is a positive integer.
 Proof-Sketch: This can be proved by contradiction. We can construct a message Y whose term set has no overlap to the term set of messages in D in a linear time. Suppose the optimal solution for k = n and D is C = { C 1 , ..., C k } the optimal solution for k = n +1and D X  X  Y } should be C = { C 1 , ..., C k , { Y }} . If there is a polynomial time solution for Problem 1 when k = n + 1, we could solve Problem 1 when k = n in polynomial time. In this section, we first present an approximated version of Problem 1 and then present our logSig algorithm. logSig algorithm consists of three steps. The first step is to separate every log message into several pairs of terms. The second step is to find k groups of log messages using local search strategy such that each group share common pairs as many as possible. The last step is to construct message signatures based on identified common pairs for each message group.
The first step of logSig algorithm is converting each log message into a set of term pairs. For example, there is a log message collected from FileZilla [2] client: We extract each pairwise of terms and preserve the order of two terms. Then, the converted pairs are as follows: The converted term pairs preserve the order information of message terms. On the other hand, the computation on the discrete term pairs is easier than a sequence. A similar idea was proposed in string kernel [16] for text classification. Their output is a high dimensional vector, and our output is a set of pairs.
The second step is to partition log messages into k groups based on converted term pairs. Notations: Let X be a log message, R ( X ) denotes the set of term pairs converted from X ,and | R ( X ) | denotes the number of term pairs in R ( X ).

Problem 2. Given a set of log messages D and an inte-ger k , find a k -partition C = { C 1 , ..., C k } of D to maximize objective function F ( C , D ) : Object function F ( C , D ) is the total number of common pairs over all groups. Intuitively, if a group has more common pairs, it is more likely to have a longer common subsequence. Then, the match score of that group would be higher. There-fore, maximizing function F is approximately maximizing J in Problem 1. Lemma 4 shows the average lower bound for this approximation.

Lemma 3. Given a message group C , it has n common term pairs, then the length of the longest common subse-quence of messages in C is at least Proof-sketch: Let l be the length of a longest common sub-sequence of messages in C .Let T ( l )bethenumberofterm pairs that generated by that longest common subsequence. Since each term pair has two terms, this sequence can gener-ate at most l 2 pairs. Hence, T ( l )  X  l 2 = l ( l  X  1) / 2. Note that each term pair of the longest common subsequence is a common term pair in C . Now, we already know T ( l )= n , so T ( l )= n  X  l ( l  X  1) / 2. Then, we have l  X  Lemma 4. Given a set of log messages D and a k -partition C = { C 1 , ..., C k } of D ,if F ( C , D )  X  y , y is a constant, we can find a set of message signatures S such that on average: Proof-sketch: Since F ( C , D )  X  y , on average, each group has at least y/k common pairs. Then for each group, by Lemma 3, the length of the longest common subsequence must be at least 2 y k . If we choose this longest common subsequence as the message signature, each log message can match at least 2 y k terms of the signature. As a result, the match score of each log message is at least 2 y k . D has messages. Then, we have the total match score J ( S , D )
Lemma 4 shows that, maximizing the F ( C , D ) is approx-imately maximizing the original objective function J ( S , But F ( C , D ) is easier to optimize because it deals with dis-crete pairs.
The logSig algorithm applies the local search strategy to solve Problem 2. It iteratively moves one message to another message group to increase the objective function as large as possible. However, unlike the classic local search optimiza-tion method, the movement is not explicitly determined by objective function F (  X  ). The reason is that, the value of F (  X  ) may only be updated after a bunch of movements, not just after every single movement. We illustrate this by the following example.

Example 3. Message set D is composed of 100  X  ab  X  X nd 100  X  cd  X . Now we have 2-partition C = { C 1 ,C 2 } .Each message group has 50% of each message type as shown in Table 2. The optimal 2-partition is C 1 has 100  X  ab  X  X nd C has 100  X  cd  X , or in the reverse way. However, beginning with current C 1 and C 2 , F ( C , D ) is always 0 until we move 50  X  ab  X  X rom C 2 to C 1 ,ormove50 X  cd  X  X rom C 1 to C 2 .Hence, for first 50 movements, F ( C , D ) cannot guide the local search because no matter what movement you choose, it is always 0.
 Therefore, F (  X  ) is not proper to guide the movement in the local search. The decision of every movement should consid-er the potential value of the objective function, rather than theimmediatevalue. Sowedevelopthe potential function to guide the local search instead.
 union set of term pairs from messages of C . For a term pair r  X  R ( C ), N ( r, C ) denotes the number of messages in C which contains r . p ( r, C )= N ( r, C ) / | C | is the portion of messages in C having r .
 Definition 2. Given a message group C ,thepotentialof C is defined as  X  ( C ) , The potential value indicates the overall  X  X urity X  of term pairs in C .  X  ( C ) is maximized when every term pair is contained by every message in the group. In that case, for term pairs are common pairs shared by every log message.  X  ( C ) is minimized when each term pair in R ( C )isonly contained by one message in C . In that case, for each r , N ( r, C )=1, | R ( C ) | = | C | ,  X  ( C )=1 / | C | .
Definition 3. Given a k -partition C = { C 1 , ..., C k } message set D , the overall potential of D is defined as  X ( where  X  ( C i ) is the potential of C i , i =1 , ..., k . Objective function F computes the total number of common term pairs in each group. Both  X  and F are maximized when each term pair is a common term in its corresponding message group. Let X  X  consider the average case.
 Lemma 5. Given a set of log messages D and a k -partition C = { C 1 , ..., C k } of D ,if F ( C , D )  X  y , y is a constant, then in the average case,  X ( D )  X  y  X |D| /k .
 Proof-sketch: Since F ( C , D )  X  y ,thereareatleast y com-mon term pairs distributed in message groups. For each common term pair r i ,let C i be its corresponding group. On average, | C i | = |D| /k . Note that the common pair r i pears in every message of C i ,so N ( r i ,C i )= | C i | and p ( r i ,C i ) = 1. There are at least y common term pairs, by Definition 2, we have  X ( D )  X  y  X |D| /k .

Lemma 5 implies, in the average case, if we try to increase the value of F to be at least y , we have to increase the overall potential  X  to be at least y  X |D| /k . As for the local search algorithm, we mentioned that  X  is easier to optimize than F .
 Let  X  iX  X  X  X  j  X ( D ) denote the increase of  X ( D )bymoving X  X  X  from group C i into group C j , i, j =1 , ..., k , i = j . Then, by Definition 3, where  X  ( C j  X  X  X } )  X   X  ( C j ) is the potential increase brought by inserting X to C j ,  X  ( C i )  X   X  ( C i  X  X  X } )isthepotentialloss brought by removing X from C i . Algorithm 1 is the pseu-docode of the local search algorithm in logSig . Basically, it iteratively updates every log message X  X  group according to  X  can be done.
 Algorithm 1 logSig_localsearch ( D ,k ) 1: C X  RandomSeeds ( k ) 3: Create a map G to store message X  X  group index 7: end for 8: end for 9: while C = C do 10: C  X  X  18: end if 19: end for 20: end while 21: return C Why choose this Potential Function? Given a message group C ,let g ( r )= N ( r, C )[ p ( r, C )]  X  ( C )= r  X  R ( C ) g ( r ). Since we have to consider all term pairs in C , we define  X  ( C )asthesumofall g ( r ). As for g ( r ), it should be a convex function. Figure 1 shows a curve of g ( r ) by varying the number of messages having r , i.e., N ( r, C ). The reason for why g ( r ) is convex is that, we hope to give larger awards to r when r is about to being a common term pair. That is because, if N ( r, C ) is large, then r is more likely to be a common term pair. Only when r becomes a common term pair, it can increase F (  X  ). In other words, r has more potential to increase the value of objective function F (  X  ), so the algorithm should pay more attention to r first.
In the experimental section, we will empirically compare the effectiveness of our proposed potential function  X  with the objective function function F in the local search.
The final step of logSig algorithm is to construct the message signature for each message group. Recall that a message signature is a sequence of terms that has a high match score to every message in the corresponding group. So it could be constructed by highly frequent term pairs identified in the second step.

Lemma 6. Let S be an optimal message signature for a message group C , the occurrence number of every term w j S must be equal or greater than | C | / 2 . The proof of Lemma 6 is similar to the proof of Lemma 1. If there exists a term w j  X  S only appearing in less than | C | / 2 messages, we can have: J ( { S  X  X  w j }} ,C ) &gt; J ( { S } ,C ), then S is worse than S  X  X  w j } .Thus, S is not optimal.

Lemma 6 indicates that we only need to care about those terms which appear in at least one half of the messages in a group. By scanning every message in a group, we could obtain the optimal sequence of those terms. Since log mes-sages are usually very short, there are only a few term whose occurrence number is equal or greater than | C | / 2 .There-fore, there are not many candidate sequences generated by those terms. We enumerate each one of them and select the best one to be the message signature.
In this section, we discuss some detailed issues about the implementation of the logSig algorithm.
 To efficiently compute the potential value of each message group C i  X  X  , a histogram is maintained for each group. The histogram is implemented by a hash table, whose key is a term pair and the value is the number of messages containing that term pair in the group. Then, for a term pair r , c ( r, C and p ( r, C i ) can be obtained in a constant time complexity. The straightforward way to compute  X  iX  X  X  X  j  X ( D )isenumer-ating all term pairs in C i and C j . The computation cost of finding the maximum  X  iX  X  X  X  j  X ( D )is O ( k l =1 | R ( C is the total number of term pairs in the entire data set. In log data, this number is even tens of times greater than |D| Hence, this computation cost is not affordable for each single log message.
 Our approximated solution is to consider the change of N ( r, C ), for each r  X  R ( X ). The reason is that, | C The impact to | C | by inserting or removing one message could be ignored comparing to the impact to N ( r, C ). | |
C | +1  X  X  C | X  1. So | C | can be treated as a constant in one exchange operation in local search . Then,  X  X  ( C )  X  X  ( r, C ) By utilizing the histograms, p ( r, C j )and p ( r, C i )canbe obtained in a constant time. Hence, for each log message X  X  X  , the time complexity of finding the largest  X  iX derestimate the actual change of  X ( D ), it can save a lot of computation cost. In the local search algorithm, exchanging messages X  groups only increases the overall potential  X ( D ). There is no oper-ation to decrease the overall potential.  X ( D ) is not infinite, which is less than or equal to | R ( D ) | X | D | . Therefore, the local search algorithm would converge.

Similar to other local search based optimization algorithm-s, logSig may converge into a local optima as well. However, logSig  X  X  potential function provides a more reliable heuris-tic to guide the optimization process. It is much less likely to stop at a local optima.
 The time complexity of converting log messages into term pairs is O ( | D | X  L 2 ), where L is the maximum length of log messages. For every X j  X  D , L 2  X | R ( X j ) | . For the local search, each iteration goes through every message. So the time complexity of an iteration is O ( k  X  L 2  X | D | ). Let t be the number of iterations, so the time complexity of the local search is O ( t  X  k  X  L 2  X | D | ). Our experiments shows that t is usually 3 to 12 for most system log data.

The space cost of the algorithm is mainly determined by the term pair histograms created for every message group. The total space complexity is the total number of term pairs, O ( | R ( D ) | ).
In real world applications, analyzing system behaviors mainly relies on the generated system events, so the event generation algorithm should be reliable. Current approach-es for grouping log messages into different event types are totally unsupervised. In practice, for most sorts of logs (e.g., Hadoop system logs), there are some domain knowl-edge about a fixed catalog of Java exceptions. In addition, the log generation mechanisms implicitly create some asso-ciations between the terminologies and the situations. In-corporating domain knowledge into the clustering process could improve the performance of event generation. In this section, we present two approaches for incorporating domain knowledge to improve the accuracy of logSig algorithm.
Some terms or words in log messages share some com-mon features which can help our algorithm identify the log message type. The features are domain knowledge from hu-man experts. For example,  X 2011-02-01 X  and  X 2011-01-02 X  are different terms, but they are both dates ;  X 192.168.0.12 X  and  X 202.119.23.10 X  are different terms, but they are both IP addresses . logSig algorithm allows users to provide a list of features. Each feature is described by a regular expression. An addi-tional feature layer is built to incorporate those features for representing log messages. The feature layer is created by regular expression matching. For example, we have follow-ing regular expressions 1 : Then, we could scan the log message X 1 to create the feature layer Y 1 as shown in Table 3. The feature layer Y 1 contains more semantic information. It can gather similar terms and reduce noisy terms. In addition, regular expression is easy for domain experts to write. Using a sophisticated regular language toolkit, a feature layer can be built efficiently.
The grammar of regular expressions is defined by Java Reg-ular Library
In reality, domain experts usually identify the event type of a log message by scanning keywords or phrases. For ex-ample, as for SFTP/FTP logs, they would be sensitive to those phrases:  X  Command  X ,  X  No such file  X ,  X  Error  X ,  X  failed  X  and so on. Those sensitive phrases should be in-cluded in the generated message signature to express system events. On the other hand, domain experts also know that some trivial phrases should be ignored, such as the message IDs and the timestamps. Those trivial phrases should not be included in any message signature. To sum up, those knowl-edge can be transferred as constraints on message signatures. Those constraints can help the event generation algorithm to improve its accuracy. Unlike traditional constraints in semi-supervised clustering, such as Must-Link or Cannot-Link [28], our constraints are placed in the subsequences (or phrases) of messages, not on the messages themselves. To incorporate constraints on message phrases, Problem 1 can be revised as follows:
Problem 3. Given a set of log messages D ,asetofsen-sitive phrases P S and trivial phrases P T , find k message signatures S = { S 1 , ..., S k } with a k -partition C 1 to maximize: where  X  is a user-defined parameter between 0 and 1.
The revised optimization problem can be solved by constraint-based logSig Algorithm. The basic idea of constraint-based logSig is to increase the weight of term pairs in R ( P S decrease the weight of term pairs in R ( P T ). Let w r de-note the weight of pair r . The only revised part in logSig algorithm is  X  iX  X  X  X  j  X ( D ). By multiplying the weight w becomes: and where  X   X  1 is a user-defined parameter that can be approx-imately derived from the original parameter  X  .  X  is utilized to increase(and decrease) the importance of term pairs in sensitive phrases(and trivial phrases). The choice of  X  de-pends on users X  confidence in those sensitive phrases and trivial phrases. In experiments, we let  X  =10 . 0. We show the performances of the algorithm when varying  X  from 0.0 to 20.0 and explain why we choose 10 in the experimental section.
We implement our algorithm and other comparative algo-rithms in Java 1.6 platform. Table 5 summarizes our exper-imental enviroment.

We collect log data from 5 different real systems, which are summarized in Table 6. Logs of FileZilla [2], PVFS2 [4] Apache [1] and Hadoop [3] are collected from the server machines/systems in the computer lab of a research center. Log data of ThunderBird [5] is collected from a supercom-puter in Sandia National Lab. The true categories of log messages are obtained by specialized log parsers. For in-stance, FillZilla X  X  log messages are categorized into 4 types:  X  Command  X ,  X  Status  X ,  X  Response  X ,  X  Error  X . Apache error log messages are categorized by the error type:  X  Permission denied  X ,  X  File not exist  X  X ndsoon.

The vocabulary size is an important characteristic of log data. Figure 2 exhibits the vocabulary sizes of the 5 different logs along with the data size. It can be seen that some vocabulary size could become very large if the data size is large.
We compare our algorithm with 7 alternative algorithms in this experiment. Those algorithms are described in Table 7. 6 of them are unsupervised algorithms which only look at the terms of log messages. 3 of them are semi-supervised algorithms which are able to incorporate the domain knowl-edge. IPLoM [18] and StringMatch [6] are two methods proposed in recent related literatures . VectorModel [23], Jaccard [25], StringKernel [16] are traditional methods for text clustering. VectorModel and semi-StringKernel are implemented by k -means clustering algorithm [25]. Jaccard and StringMatch are implemented by k -medoid algorithm [12], since they cannot compute the centroid point of a clus-ter. As for Jaccard , the Jaccard similarity is obtained by a hash table to accelerate the computation. VectorModel and StringKernel use Sparse Vector [23] to reduce the compu-tation and space costs. semi-logSig , semi-StringKernel and semi-Jaccard are semi-supervised versions of logSig , StringKernel and Jac-card respectively. To make a fair comparison, all those semi-supervised algorithms incorporate the same domain knowl-edge offered by users. Specifically, the 3 algorithms run on the same transformed feature layer, and the same sensitive phrases P S and trivial phrases P T . Obviously, the choices of features, P S and P T have a huge impact to the performances of semi-supervised algorithms. But we only compare a semi-supervised algorithm with other semi-supervised algorithms. Hence, they are compared under the same choice of features, P
S and P T . The approaches for those 3 algorithms to incor-porate with features, P S and P T are described as follows: Feature Layer: Replacing every log message by the trans-P
S and P T : As for semi-StringKernel , replacing Euclidean
Jaccard , StringMatch and semi-Jaccard algorithms ap-ply classic k -medoid algorithm for message clustering. The time complexity of k -medoid algorithm is very high: O ( tn [27], where t is the number of iterations, n is the number of log messages. As a result, those 3 algorithms are not capa-ble of handling large log data. Therefore, for the accuracy comparison, we split our log files into smaller files by time frame, and conduct the experiments on the small log data. The amounts of log messages, features, term pairs in P S and P
T are summarized in Table 8. In Section 7.5, larger logs are used to test the scalability.
Table 4 shows the accuracy comparison of generated sys-tem events by different algorithms. The accuracy is evalu-ated by F-measure (F1 score) [23], which is a traditional metric combining precision and recall . Since the results of k -medoid, k -means and logSig depend on the initial ran-dom seeds, we run each algorithm for 10 times , and put the average F-measures into Table 4. From this table, it can be seen that StringKernel and logSig outperform other algorithms in terms of the overall performance.

Jaccard and VectorModel apply the bag-of-word model, which ignores the order information about terms. Log mes-sages are usually short, so the information from the bag-of-word model is very limited. In addition, different log mes-sages have many identical terms, such as date , username . That X  X  the reason why the two methods cannot achieve high F-measures. IPLoM performs well in ThunderBird log da-ta, but poorly in other log data. The reason is that, the first step of IPLoM is to partition log message by the term count. One type of log message may have different num-bers of terms. For instance, in FileZilla logs, the length of Command messages depends on the type of SFTP/FTP command in the message. But for ThunderBird, most even-t types are strictly associated with one message format. Therefore, IPLoM could easily achieve the highest score. Due to the Curse of dimensionality [25], k -means based StringKernel is not easy to converge in a high dimensional space. Figure 2 shows that, 50K ThunderBird log messages contain over 30K distinct terms. As a result, the trans-formed space has over (30 K ) 2 = 900 M dimensions. It is quite sparse for 50K data points.
Generated message signatures are used as descriptors for system events, so that users can understand the meanings of those events. Due to the space limit, we cannot list all Figure 3: Average Running Time for FileZilla logs Figure 6: Varying parameter  X  message signatures. Table 9 shows generated signatures of FileZilla and Apache Error by semi-logSig ,inwhichfea-tures are indicated by italic words .

As for FileZilla log, each message signature corresponds to a message category, so that the F-measure of FileZilla could achieve 1.0. But for Apache Error log, Only 4 message sig-natures are associated with corresponding categories. The other 2 signatures are generated by two ill-partitioned mes-sage groups. They cannot be associates with any category of Apache Error logs. As a result, their  X  X ssociated Category X  in Table 9 are  X  X /A X . Therefore, the overall F-measure on Apache error log in Table 4 is only 0.7707.
All those algorithms have the parameter k ,whichisthe number of events to create. We let k be the actual number of message categories. String kernel method has an addi-tional parameter  X  , which is the decay factor of a pair of terms. We use StringKernel  X  to denote the string kernel method using decay factor  X  . In our experiments, we set up string kernel algorithms using three different decay factors:
As for the parameter  X  of our algorithm logSig ,weset  X  = 10 based on the experimental result shown by Figure 6. For each value of  X  ,werunthealgorithmfor 10 times ,and plot the average F-measure in this figure. It can be seen that, the performance becomes stable when  X  is greater than 4.
To evaluate the effectiveness of the potential function  X , we compare our proposed logSig algorithm with anoth-er logSig algorithm which uses the objective function F to guide its local search. Figure 7 shows the average F-measures of the two algorithms on each data set. Clearly, our proposed potential function  X  is more effective than F in all data sets. In addition, we find logSig algorithm using F always converges within 2 or 3 iterations. In other words, F is more likely to stop at a local optima in the local search. The reason for this has been discussed in Section 4.2.2.
Scalability is an important factor for log analysis algo-rithms. Many high performance computing systems gener-ate more than 1Mbytes log messages per second [21]. Fig-ure 3, Figure 4 and Figure 5 show the average running time comparison for all algorithms on the data sets with different sizes. We run each algorithm 3 times and plot the average running times. IPLoM is the fastest algorithm. The running times of other algorithms depend on the number of itera-tions. Clearly, k -medoid based algorithms are not capable of handling large log data. Moreover, StringKernel is not efficient even though we use Sparse Vector to implement the computation of its kernel functions. We keep track of its running process, and find out the low speed convergence is mainly due to the high dimensionality.
 Figure 8 shows the scalability of logSig algorithm on ThunderBird logs and Apache Error logs. Its actual run-ning time is approximated linear with the log data size.
In this paper, we show the drawbacks of traditional meth-ods and previous log message analyzing methods. To address the limitations of existing methods, we propose logSig ,a message signature based method for events creation from system log messages. logSig utilizes the common subse-quence information of messages to partition and describe the events generated from log messages.

As for the future work, we will integrate the structural learning techniques into our framework to capture the struc-tural information of log messages. We hope those structures could improve the performance of the logSig algorithm. The work is supported in part by NSF grants IIS-0546280 and HRD-0833093.
