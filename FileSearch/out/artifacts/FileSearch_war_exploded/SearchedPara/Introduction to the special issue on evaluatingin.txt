
Evaluation has always been a strong element of Information Retrieval (IR) research, much of our focus being on how we evaluate IR algorithms. As a research field we have benefited greatly from initiatives such as Cranfield, TREC, CLEF and INEX that have added to our knowledge of how to create test collections, the reliability of system-based evaluation criteria and our understanding of how to interpret the results of an algorithmic evaluation. In contrast, evaluations whose main focus is the user experience of searching have not yet reached the same level of maturity. Such evaluations are complex to create and assess due to the in-creased number of variables to incorporate within the study, the lack of standard tools available (for example, test collections) and the difficulty of selecting appropriate evaluation criteria for study.

In spite of the complicated nature of user-centred evaluations, this form of evaluation is necessary to under-stand the effectiveness of individual IR systems and user search interactions. The growing incorporation of users into the evaluation process reflects the changing nature of IR within society; for example, more and more people have access to IR systems through Internet search engines but have little training or guidance in how to use these systems effectively. Similarly, new types of search system and new interactive IR facilities are becom-ing available to wide groups of end-users.

In this special topic issue we present papers that tackle the methodological issues of evaluating interactive search systems. Methodologies can be presented at different levels; the papers by Blandford et al. and Petrelli present whole methodological approaches for evaluating interactive systems whereas those by Go  X  ker and
Myrhaug and Lo  X  pez Ostenero et al., consider what makes an appropriate evaluation methodological ap-proach for specific retrieval situations.

Any methodology must consider the nature of the methodological components, the instruments and pro-cesses by which we evaluate our systems. A number of papers have examined these issues in detail: Ka  X  ki and Aula focus on specific methodological issues for the evaluation of Web search interfaces, Lopatovska and
Mokros present alternate measures of retrieval success, Tenopir et al. examine the affective and cognitive ver-balisations that occur within user studies and Kelly et al. analyse questionnaires, one of the basic tools for evaluations.

The range of topics in this special issue as a whole nicely illustrates the variety and complexity by which user-centred evaluation of IR systems is undertaken.

We start the issue with a number of methodologically oriented papers. These papers show that when con-ducting user-centred evaluation the research questions must direct the evaluation design, and hence we cannot expect a single, standard evaluation solution. However, the paper by Blandford et al. titled The PRET A Rap-porter Framework: evaluating Digital Libraries from the perspective of Information work presents a framework for the planning an evaluation of Digital Libraries. The approach of the framework is of a formative HCI nature compared to the summative nature of the IR systems evaluation approaches commonly employed with-in IR. The acronym of the framework functions as the mnemonic of the steps involved: purpose of evaluation; resources and constraints; ethics, techniques for data gathering; analysis techniques; and reporting of findings.
The steps listed resemble the decision questions put forward by Tague-Sutcliffe in her famous 1992 Informa-tion Processing &amp; Management paper. This is no coincidence, as the PRET A Rapporter framework is intro-duced and discussed in relation to Tague-Suttcliffe X  X  ten decisions representing the traditional system-oriented approach to IR systems evaluation. The framework is further discussed in relation to the IIR evaluation model by Borlund. The potential of the framework for evaluation of Digital Libraries is illustrated through three case studies.
 In the paper On the Role of User-Centred Evaluation in the advancement of Interactive Information Retrieval,
Petrelli reports on the lessons learnt through a series of evaluation studies of a cross-language information retrieval system (CLIR) within the Clarity project. Hereby she demonstrates how longitudinal user-centred
IR evaluation is a powerful tool in providing a holistic and coherent picture of IR interaction. The reader is guided through the series of four evaluation studies of the CLIR system undertaken during the lifecycle of the Clarity project, and in each case the lessons learnt are reported in order to build upon.
Go  X  ker and Myrhaug, in Evaluation of a Mobile Information System in Context , also present a methodolog-ical framework based on their experiences of designing and running a number of linked evaluation studies.
Here the context is mobile information seeking and the studies are situated outside of the laboratory in real-life information seeking situations. The paper outlines the experience gained, and lessons learnt, from run-ning evaluation studies in less controlled environments and how outcomes from one study can feed into the design of future studies.

New retrieval tasks often require new evaluation methodologies and elicit new understandings of human search behaviour. In Interactive question answering: is cross-language harder than monolingual searching ?,
Lo  X  pez-Ostenero et al., consider the evaluation of question answering systems where the question and possible answers are in different languages. This task is different in nature from both single-language question answer-ing and standard cross-language retrieval. Using a specially designed interactive study as the basis for their discussion, Lo  X  pez-Ostenero et al. consider the particular nature of this retrieval task and how the character-istics of the task should influence the evaluation framework. One important conclusion is that the flexibility of human reasoning can compensate for sub-optimal performance by systems  X  humans can deduce or infer cor-rect answers where the answer is not explicit in the answer text. A second, and more pressing, argument is presented for evaluation frameworks that concentrate on the process of searching and the types and conse-quences of both human and system failure in searching.

Ka  X  ki and Aula X  X  paper Controlling the Complexity in Comparing Search User Interfaces via User Studies focuses on how to control the experimental setting, normalise for human behaviour and make search result comparable when involving users in evaluation of Web search interfaces. For this purpose three performance measures are proposed: the search speed measure; the qualified search speed measure; and the measure of immediate accuracy. Ka  X  ki and Aula share their experiences earned through several studies and present best practice on this basis.

Within any evaluation we must consider the choice of what to measure and who to measure. One of the core aspects to measure is the quality of information obtained through assessments of retrieved material. Here, relevance assessment plays a central role in IR evaluation and there are few evaluations that do not, at some point, ask a searcher or experimental subject to record which information items contain relevant information. Although relevance is an intuitive concept it is not unambiguous and there has been a long-running debate in
IR regarding how relevance should be measured within IR evaluations. In Willingness to pay and experienced utility as measures of affective value of information objects: users X  accounts , Lopatovska and Mokros construc-tively add to this discussion by investigating two new measures  X  Willingness to Pay and Experienced Utility  X  which can be used to measure a searcher X  X  emotional response to retrieved information. Lopatovska and
Mokros argue that these two measures, although highly correlated, express different parts of a searcher X  X  assessment with Willingness to Pay expressing the value of an object in completing an information task and Experienced Utility measuring the emotional reaction to information, independent of the task set. This exploration of novel measures to record human searching experience can add to our armoury of tools which with to evaluation IR systems.

Tenopir et al., in Academic users X  interactions with ScienceDirect in search tasks: affective and cognitive behaviors , also consider the emotional aspect of searching. Their study, examining academic searchers interacting with ScienceDirect, investigated both the searchers X  affective and cognitive verbalizations whilst searching. As Tenopir et al. demonstrate both these types of information are important to understand different aspects of searching. Affective responses in this study were more commonly associated with search results, particularly positive results, whereas cognitive responses were more often associated with a mixture of results and search strategies.

Finally, Kelly et al., in Questionnaire mode effects in interactive information retrieval experiments consider one of the basic tools of interactive IR evaluation namely the questionnaire. Questionnaires are widely used but can take many forms. The mode of delivery, for example, may be the traditional paper-based question-naire, interview or increasingly electronic questionnaires. Kelly et al., examine the interaction between the mode of delivery and type of question asked in the questionnaire to better understand how we should deploy questionnaires within interactive evaluations. Their study presents a number of interesting findings. Two main findings are of particular interest; firstly that experimental subjects used lower ratings to evaluate systems when being interviewed compared to completing electronic questionnaires, secondly that subjects were more efficient in answering open questions using paper-based questionnaires.

We hope that these papers selected for the special issue add constructively to the development of IR evaluation theory and practice. We would like to thank the editor-in-chief of Information Processing and
Management, Tefko Saracevic, for his support in producing this special issue and the reviewers who gave their time and expertise in reading the submitted articles: Micheline Beaulieu, Katriina Bystro  X  m, Mark Dunlop, Nigel Ford, Luanne Freund, Morten Hertzum, Jussi Karlgren, Jaana Keka  X  la  X  inen, Diane Kelly, Monica Landoni, Sharon McDonald, Gheorghe Muresan, Ragnar Nordlie, Nils Pharo, Jane Reid, Kerry Rodden, Amanda Spink, Elaine Toms, Peiling Wang, Ryen White and Mingfang Wu.

