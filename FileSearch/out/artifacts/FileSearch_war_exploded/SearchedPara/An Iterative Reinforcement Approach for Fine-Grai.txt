 In the Web2.0 era, the Internet turns from a static information media into a platform for dynamic information exchanging, on which people can ex-press their views and show their selfhood. More and more people are willing to record their feel-ings (blog), give voice to public affairs (news re-view), express their likes and dislikes on products (product review), and so on. In the face of the vol-ume of sentimental information available on the Internet continues to in crease, there is growing interest in helping people better find, filter, and manage these resources. Automatic opinion mining (Turney et al., 2003; Ku et al., 2006; Devitt et al., 2007) can play an important role in a wide variety of more flexible and dynamic information management tasks. For example, with the help of sentiment analysis sys-tem, in the field of public administration, the ad-ministrators can receive the feedbacks on one pol-icy in a timelier manner; in the field of business, manufacturers can perform more targeted updates on products to improve the consumer experience. 
The research of opinion mining began in 1997, the early research results mainly focused on the polarity of opinion words (Hatzivassiloglou et al., 1997) and treated the text-level opinion mining as a classification of either po sitive or negative on the number of positive or negative opinion words in one text (Turney et al., 2003; Pang et al., 2002; Zagibalov et al., 2008;). With the in-depth study of opinion mining, researchers committed their ef-forts for more accurate results: the research of sen-timent summarization (Philip et al., 2004; Hu et al., KDD 2004), domain transfer problem of the sen-timent analysis (Kanayama et al., 2006; Tan et al., 2007; Blitzer et al., 2007; Tan et al., 2008; An-dreevskaia et al., 2008; Tan et al., 2009) and fine-grained opinion mining (Hatzivassiloglou et al., 2000; Takamura et al., 2007; Bloom et al., 2007; Wang et al., 2008; Titov et al., 2008) are the main branches of the research of opinion mining. In this paper, we focus on the fine-grained (feature-level) opinion mining. 
For many applications (e.g. the task of public affairs review analysis and the products review analysis), simply judging the sentiment orientation of a review unit is not sufficient. Researchers (Ku-shal, 2003; Hu et al., KDD 2004; Hu et al., AAAI 2004; Popescu et al., 2005) began to work on finer-grained opinion mining which predicts the sentiment orientation rela ted to different review features. The task is known as feature-level opin-ion mining. 
In feature-level opinion mining, most of the ex-isting researches associate product features and opinion words by their explicit co-occurrence. Template extraction based method (Popescu et al., 2005) and association rule mining based method (Hu et al., AAAI 2004) are the representative ones. 
These approaches did good jobs for identifying the review features that appear explicitly in re-views, however, real reviews from customers are usually complicated. In some cases, the review features are implicit in the review sentences, but can be deduced by the opinion words in its context. The detection of such hidden sentiment association is a big challenge in feature-level opinion mining on Chinese reviews due to the nature of Chinese language (Qi et al., 2008). Obviously, neither the template extraction based method nor the associa-tion rule mining based method is effective for such cases. Moreover, in some cases, even if the review features appear explicitly in the review sentences, the co-occurrence information between review features and opinion words is too quantitatively sparse to be utilized. So we consider whether it is a more sensible way to construct or cluster review feature groups and opinion words groups to mine the implicit or hidden sentiment association in the reviews. 
The general approach will cluster the two types of objects separately, which neglects the highly interrelationship. To address this problem, in this paper, we propose an iterative reinforcement framework, under which we cluster product fea-tures and opinion words simultaneously and itera-tively by fusing both their semantic information and sentiment link informa tion. We take improved information bottleneck algorithm (Tishby, 1999) as the kernel of the proposed framework. 
The information bottleneck approach was pre-sented by Tishby (1999). The basic idea of the ap-proach is that it treats the clustering problems from the information compressing point of view, and takes this problem as a case of much more funda-mental problem: what are the features of the vari-able X that are relevant for the prediction of an-other, relevance, variable Y ? Based on the infor-mation theory, the problem can be formulated as: find a compressed representation of the variable X, denoted C, such that the mutual information be-tween C and Y is as high as possible, under a con-straint on the mutual information between X and C . For our case, take the hotel reviews as example, X is one type of objects of review features (e.g. fa-cilities, service, surrounding environment, etc) or opinion words (e.g. perfect, circumspect, quiet, etc), and Y is another one. Given some review fea-tures (or opinion words) gained from review cor-pus, we want to assemble them into categories, conserving the information about opinion words (or review features) as high as possible. 
The information bottleneck algorithm has some benefits, mainly including (1) it treats the trade-off of precision versus complexity of clustering model through the rate distortion theory, which is a sub-field of information theory ; (2) it defines the  X  X is-tance X  or  X  X imilarity X  in a well-defined way based on the mutual information. The efficiency of in-formation bottleneck algorithm (Slonim and Tishby, 2000) motivates us to take it as the kernel of our framework. As far as we know, this ap-proach has not been employed in opinion mining yet.

In traditional information bottleneck approach, the distance between two data objects is measured by the Jensen-Shannon divergence (Lin, 1991), which aims to measure the divergence between two probability distributions. We alter this meas-ure to integrate more semantic information, which will be illustrated in de tail in the following sec-tions, and the experimental result shows the effectiveness of the alteration. 
It would be worthwhile to highlight several as-pects of our work here: z We propose an iterative reinforcement z In the process of clustering, the semantic in-z The experimental results on real Chinese 2.1 The Problem In product reviews, opinion words are used to ex-press opinion, sentiment or attitude of reviewers. Although some review units may express general opinions toward a product, most review units are regarding to specific features of the product. 
A product is always reviewed under a certain feature set F . Suppose we have got a lexical list O which includes all the opinion expressions and their sentiment polarities. For the feature-level opinion mining, identifying the sentiment associa-in the whole process are as follows: z get opinion word set O (with polarity labels) z get product feature set F z identify relationships between F and O
The focus of the paper is on the latter two steps, especially for the case of hidden sentiment asso-ciation that the review features are implicit in the review sentences, but can be deduced by the opin-ion words in its context. In contrast to existing ex-plicit adjacency approaches, the proposed approach detects the sentiment association between F and O based on review feature categories and opinion word groups gained from the review corpus. 
To this end, we first consider two sets of asso-ciation objects: the set of product feature words F { o 1 ,o 2 ,...o n }. A weighted bipartite graph from F and O can be built, denoted by G = { F, O, R }. taining all the pair-wise weights between set F and O . The weight can be calculated with different co-appearance frequency of f i and o j in clause level. 
We take F and O as two random variables, and the question of constructing or clustering the ob-ject groups can be defined as finding compressed representation of each variable that reserves the information about another variable as high as pos-sible. Take F as an example, we want to find its compression, denoted as C , such that the mutual information between C and O is as high as possi-ble, under a constraint on the mutual information between F and C . 
We propose an iterative reinforcement frame-work to deal with the tasks. An improved informa-tion bottleneck algorithm is employed in this framework, which will be illustrated in detail in the following sections. 2.2 Information Bottleneck Algorithm The information bottleneck method (IB) was pre-sented by Tishby et al. (1999). According to Shan-non X  X  information theory (Cover and Thomas, 1991), for the two random variables X , Y , the mu-tual information I ( X ; Y ) between the random vari-ables X , Y is given by the symmetric functional: and the mutual information between them meas-ures the relative entropy between their joint distri-bution p ( x , y ) and the product of respective mar-ginal distributions p ( x ) p ( y ), which is the only con-sistent statistical measure of the information that variable X contains about variable Y (and vice versa). Roughly speaking, some of the mutual in-formation will be lost in the process of compres-sion, e.g. (,) ( ,) I CY I X Y  X  ( C is a compressed rep-resentation of X ). 
This representation is defined through a (possi-bly stochastic) mapping between each value x X  X  to each representative value cC  X  . Formally, this mapping can be characterized by a conditional distribution p ( c | x ), inducing a soft partitioning of X values, Specifically, each value of X with all the codebook elements ( C values), with some normalized probability. 
The IB method is based on the following simple idea. Given the empirical joint distribution of two variables, one variable is compressed so that the mutual information about the other variable is pre-served as much as possible. The method can be considered as finding a minimal sufficient partition or efficient relevant coding of one variable with respect to the other one. This problem can be solved by introducing a Lagrange multiplier  X  , and then minimizing the functional: 
This solution is given in terms of the three dis-tributions that characterize every cluster cC  X  , the prior probability for this cluster, p ( c ), its member-the relevance variable p ( y | c ). In general, the mem-x X  X  can be assigned to every cC  X  in some (normalized) probability. The information bottle-neck principle determines the distortion measure Kullback-Leibler divergence (Cover and Thomas, 1991) between the conditional distributions p ( y | x ) and p ( y | c ). Specifically, the formal optimal solu-tion is given by the following equations which must be solved together. 
Where (,) Z x  X  is a normalization factor, and the single positive (Lagrange) parameter  X  determines the  X  X oftness X  of the classi fication. Intuitively, in this procedure the information contained in X about Y  X  X queezed X  through a compact  X  X ottleneck X  vant X  part in X w.r.t to Y . 
An important special case is the  X  X ard X  cluster-That is, p ( c | x ) can only take values of zero or one, This restriction, which corresponds to the limit  X   X  X  X  in Eqs 3 meaning every x X  X  is as-signed to exactly one cluster cC  X  with a prob-ability of one and to all the others with a probabil-ity of zero. This yields a natural distance measure between distributions which can be easily imple-mented in an agglomerative hierarchical clustering procedure ( Slonim and Tishby, 1999 ). 
The algorithm starts with a trivial partitioning into | X | singleton clusters, where each cluster con-tains exactly one element of X . At each step we merge two components of the current partition into a single new component in a way that locally minimizes the loss of mutual information about the categories. Every merger, defined by the following equation: pc x p yc pyc pyc pc pc pc  X  X  X  X   X   X   X   X   X   X   X   X   X  =+  X   X   X  The decrease in the mutual information I ( C , Y ) due to this merger is defined by 
I cc IC Y IC Y  X   X  X  X  (6) When (,) tion values before and after the merger, respec-tively. After a little algebra, one can see 
I cc pc pc D pyc pyc  X   X  X  X   X + X   X  X  X  (7) Where the functional D JS is the Jensen-Shannon divergence (Lin, 1991), defined as 
D pp D p p D p p  X  X  where in our case 
By introducing the information optimization cri-terion the resulting similarity measure directly emerges from the analysis. The algorithm is now very simple. At each step we perform  X  X he best possible merger X , i.e. merge the clusters {, } which minimize (, ) 2.3 Improved Information Bottleneck Algo-In traditional information bottleneck approach, the distance between two data objects is measured by the difference of information values before and after the merger, which is measured by the Jensen-Shannon divergence. This divergence aims to measure the divergence between two probability distributions. For our case, the divergence is based on the co-occurrence inform ation between the two variables F and O . 
While the co-occurrence in corpus is usually quantitatively sparse; additi onally, Statistics based on word-occurrence loses semantic related infor-mation. To avoid such reversed effects, in the pro-posed framework we combine the co-occurrence information and semantic information as the final distance between the two types of objects. 
In equation 10, the distance between two data objects X i and X j is denoted as a linear combination of semantic distance and information value differ-ence. The parameter  X  reflects the contribution of different distances to the final distance. Input: Joint probability distribution p ( f , o ) Output: A partition of F into m clusters,  X  m  X  {1,...,| F |}, and a partition of O into n clusters  X  n  X  {1,...,| O |} 1. t  X  0 2. Repeat a. Construct CF t  X  F t b.  X  i , j =1,...,| CF t |, i &lt; j , calculate c. for m  X  | CF t |-1 to 1 d. Construct CO t  X  O t e.  X  i , j =1,...,| CO t |, i &lt; j ,calculate f. for n  X  | CO t |-1 to 1 g. t  X  t +1 3. until ( CF t = CF t-1 and CO t =CO t-1 ) 
The semantic distance can be got by the usage of lexicon, such as WordNet (Budanitsky and Hirst, 2006). In this paper, we use the Chinese lexicon HowNet 1 . 
The basic idea of the iterative reinforcement principle is to propagate the clustered results be-tween different type data objects by updating their inter-relationship spaces. The clustering process can begin from an arbitrary type of data object. The clustering results of one data object type up-date the interrelationship thus reinforce the data object categorization of another type. The process is iterative until clustering results of both object types converge. Suppose we begin the clustering process from data objects in set F , and then the steps can be expressed as Figure 1. After the itera-tion, we can get the strongest n links between product feature categories and opinion word groups. That constitutes our set of sentiment asso-ciation. In this section we describe our experiments and the data used in these experiments. 3.1 Data 
Our experiments take hotel reviews (in Chinese) as example. The corpus used in the experiments is composed by 4000 editor reviews on hotel, includ-ing 857,692 Chinese characters. They are extracted from www.ctrip.com . Each review contains a user X  X  rating represented by  X  X tars X , the number of the star denotes the user X  X  satisfaction. The de-tailed information is illustrated in Table 1 , 
Then we use ICTCLAS 2 , a Chinese word seg-mentation software to extract candidate review features and opinion words. 
Usually, adjectives are normally used to express opinions in reviews. Therefore, most of the exist-ing researches take adjectives as opinion words. In the research of Hu et al. (2004), they proposed that other components of a sentence are unlikely to be product features except for nouns and noun phrases. Some researchers (Fujii and Ishikawa, 2006) targeted nouns, noun phrases and verb phrases. The adding of verb phrases caused the identification of more possible product features, while brought lots of noises. So in this paper, we follow the points of Hu X  X , extracting nouns and noun phrases as candidate product feature words. 
Take the whole set of nouns and noun phrases as candidate features will bring some noise. In or-der to reduce such adverse effects, we use the function of Named Entity Recognition (NER) in ICTCLAS to filter out named entities, including: person, location, organization. Since the NEs have small probability of bein g product features, we prune the candidate nouns or noun phrases which have the above NE taggers. 
By pruning candidate product feature words, we get the set of product feature words F . And the set of opinion words O is composed by all the adjec-tives in reviews. The number of candidate product feature words and opinion words extracted from the corpus are shown as Table 2: 3.2 Experimental Procedure We evaluate our approach from two perspectives: construction by mutual reinforcement based clus-tering; product feature categories and opinion word groups; 4.1 Evaluation of Review Feature Category To calculate agreement between the review feature category construction results and the correct labels, we make use of the Rand index (Rand, 1971). This allows for a measure of agreement between two partitions, P 1 and P 2 , of the same data set D . Each partition is viewed as a collection of n*(n-1)/2 pair to the same cluster or to different clusters. Let a be the number of decisions where d i is in the same decisions where the two instances are placed in different clusters in both partitions. Total agree-ment can then be calculated using 
In our case, the parts of product feature words in the pre-constructed evaluation set are used to rep-resent the data set D ; a and b represent the parti-tion agreements between the pairs of any two words in the parts and in the clustering results re-spectively. 
In equation 10, the parameter  X  reflects the re-spective contribution of semantic information and co-occurrence information to the final distance. When 0  X  = or 1  X  = , the co-occurrence informa-tion or the semantic information will be utilized alone. 
In order to get the optimal combination of the two type of distance, we adjust the parameter  X  from 0 to 1(stepped by 0.2), and the accuracy of feature category construction with different  X  are shown in Figure 2: 
From this figure, we can find that the semantic information (  X  =1) contributes much more to the accuracy of review feature category construction than the co-occurrence information (  X  =0), and when  X  =0, the approach is equivalent to the tradi-tional information bottleneck approach. We con-sider this is due partly to the sparseness of the cor-pus, by enlarging the scale of the corpus or using the search engine (e.g. google etc), we can get more accurate results. 
Additionally, by a sensible adjust on the pa-rameter  X  (in this experiment, we set  X  as 0.6), we can get higher accuracy than the two baselines (  X  =0 and  X  =1), which indicates the necessity and effectiveness of the integration of semantic information and co-occurrence information in the proposed approach. 4.2 Evaluation of Sentiment Association We use precision to evaluate the performance of sentiment association. An evaluation set is con-structed manually first, in which there are not only the categories that every review feature word be-long to, but also the relationship between each category and opinion word. Then we define preci-sion as: 
A comparative result is got by the means of template-extraction based approach on the same test set. By the usage of regular expression, the nouns (phrase) and gerund (phrase) are extracted as the review features, and the nearest adjectives are extracted as the relate d opinion words. Because the modifiers of adjectives in reviews also contain rich sentiment information and express the view of customs, we extract adject ives and their modifiers simultaneously, and take them as the opinion words. Template extraction 27,683 65.89% 
Table 3 shows the advantage of our approach over the extraction by explicit adjacency. Using the same product feature categorization, our sen-timent association approach get a more accurate pair set than the direct extraction based on explicit adjacency. The precision we obtained by the itera-tive reinforcement approach is 78.90%, almost 13 points higher than the adjacency approach. This indicates that there are a large number of hidden sentiment associations in the real custom reviews, which underlines the importance and value of our work. In this paper, we propose a novel iterative rein-forcement framework based on improved informa-tion bottleneck approach to deal with the feature-level product opinion-mining problem. We alter traditional information bottleneck method by inte-gration with semantic information, and the ex-perimental result demonstrates the effectiveness of the alteration. The main contribution of our work mainly including: z We propose an iterative reinforcement in-z In the process of clustering, the semantic in-z The experimental results based on real Chi-
Although our methods for candidate product feature extraction and filtering (see in 3.1) can partly identify real product features, it may lose some data and remain some noises. We X  X l conduct deeper research in this area in future work. Addi-tionally, we plan to exploit more information, such as background knowledge, to improve the per-formance. This work was mainly supported by two funds, i.e., 0704021000 and 60803085, and one another pro-ject, i.e., 2004CB318109. 
