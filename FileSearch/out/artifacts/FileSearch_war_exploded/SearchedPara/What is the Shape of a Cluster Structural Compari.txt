 Even today, most interfaces to document clustering present clusters to users and applications as a  X  X ag of descriptive terms" X  X  tech-nique that was proposed two decades ago. Consequently, users and applications are not able to obtain sophisticated structural knowl-edge that is indeed lying hidden in document clusters. In particu-lar, the structural information about interaction of concepts that the cluster speaks about is completely missing from the bag of terms presentation. As the needs of unstructured information manage-ment increase, this shortcoming is coming into sharper focus.
To address this shortcoming, we propose a rich representation of document clusters that surfaces the concept interactions within a cluster into the representation. We show that these interactions give a  X  X hape" to the cluster. This  X  X hape" is conveniently captured using a directed, colored, vertex-weighted graph , called the shape graph or, simply, the shape of the cluster . We show that shapes convey important structural information about document clusters, and can be computed efficiently.
 Cluster Shape; Shape Graphs, Concept Interactions
Document clustering can be used in several applications in un-structured information management (UIM). However, how do you present a cluster to a user or an application? And how does the user or application gather the information they need from the clustering?
As UIM applications of clustering become more sophisticated, they will require sophisticated presentations of clusters so that they may extract the information they need from these presentations. However, in spite of the enormous research attention given to clus-tering algorithms, there is meager attention being given to the prob-lem of cluster presentation and information that can be extracted from them.

Today, clusters are usually presented to a user or application by means of a digest , which is a bag of important terms in the clus-ter, possibly along with a weight that indicates importance [10]. A standard means to compute a digest for a cluster is to rank the terms in the cluster with respect to their frequency of occurrence, and then present the user with the top-` terms and their weights as a digest. Terms in the digest serve as a proxy for the topics in the cluster.

The above technology was proposed at least two decades ago [4] and is not rich enough for today X  X  applications. To see this, consider the following two cluster digests from a 20-way clustering of the 20 Newsgroups dataset. Carefully examining both digests indicates that the respective clus-ters might be speaking of two topics each. The first about baseball and hockey and the second about two middle eastern conflicts. But are documents in these clusters comparing these two, or are they mostly disjoint discussions centered around these two? This is im-possible to tell from the digest.

The obvious shortcomings of the current digest representation of clusters include the following. 1. It is not visual. A significant proportion of users understand information far better if it is presented visually. Inspecting a list of terms is not appealing to such users. 2. It does not indicate to the user which sets of concepts (impor-tant terms) tend to co-occur within documents in the cluster. The user has no way to know what the natural sub-groups in the cluster are. For example, the first of our three example digests does not tell a user whether the documents in the cluster (a) compare two sports and their seasons, or (b) whether there is a dichotomy in the cluster, with one group of documents speaking of hockey, and an-other speaking of baseball. In the example digest of (5), without further knowledge of which concepts co-occur, there is no hope of interpreting the contents of the cluster. 3. It does not permit easy structural comparison between clus-ters. There is no way to pose, or answer the question "Is cluster A structurally similar to cluster B?" For example, each of the two clusters speaks about two topics. Are they structurally similar? 4. The inspections done above are tedious and do not scale. 5. Most cluster digests do not admit an easy interpretation. For example, the following digest, which is typical, does not admit an easy interpretation.
Our Approach . We propose a richer cluster presentation that rectifies these shortcomings. At first, it may seem that since a clus-ter is collection of documents, we may not be able to do better than display it using important terms in these documents, and/or titles of a subset of these documents. However, we may view a clus-ter as an interaction of concepts . Further, this concept interaction structure can be unearthed from each cluser using a framework that carefully records frequent itemsets in a cluster and the overlaps be-tween them. 1
Our framework yields, for each cluster, a directed, colored, vertex-weighted graph that translates the rich, concept interaction struc-ture of the cluster into the language of the graph. This is the shape graph of the cluster. The shape graph conveys structural informa-tion that is not visible in a  X  X ag of terms" digest.

Our contributions are as follows. We provide a pictorial and structurally rich presentation of individual clusters in a document clustering. Our framework enables structural comparison between clusters. Finally, we demonstrate our framework on benchmark text corpora.
Our work is related to two streams of research.
As noted, there is relatively scant research devoted specifically to clustering interfaces. [4] introduce scatter/gather, a workflow that uses clustering to navigate document corpora. The clustering inter-face consists of frequent and discriminative terms taken from clus-ters. This interface has been, more or less, retained by subsequent work. [3, 11] develop scatter/gather further, but retain the original interface. [5] compare interfaces for organizing query search re-sults by categorization and clustering. Their clustering interface uses descriptive terms taken from each cluster X  X  centroid, along with titles of a few documents from the cluster. Grouper [14] uses six phrases taken from documents in the cluster, with redundancy removed, as its interface. In addition to the above, various com-merical vendors provide their custom interfaces to clustering. All of these that we are aware of use a combination of terms, phrases, document titles, and representative documents to present a cluster.
Due to the pictorial nature of our interface, it may also be seen as a form of visualization.
 Visualizing Corpus Structure. The goal of this work is to visu-alize a dataset using clustering. [2] study unexpected relationships arising in document similarity graphs that use latent Dirichlet al-location. [8] propose a topic model for visualizing documents in a dataset using probabilistic latent semantic indexing. There is no clustering of the dataset. [12] present CLUSION, which is a re-lationship centric view of document clustering, as opposed to an object centric view that represents clusters as a set of terms. Our presentation is also relationship centric, but it is relationship cen-tric at a lower-level: at the level of terms within a cluster, whereas CLUSION presents relationships between clusters. [13] propose a visualization called  X  X alaxies" where documents and clusters are represented as a 2-D scatterplot that resembles stars in a night sky. The clusters themselves are represented by terms from a digest. DICON [1] represents clusters by means of icons that encode their high-level statistical information.

Visualization to Assist Navigation. [7] propose a topological version of probabilistic latent semantic analysis, which allows a user to navigate large text corpora by choosing a set of topics, ex-panding upon them, and iterating the above using a multi-resolution visualization using grid maps. [9] present iVisClustering: an inter-active visual clustering tool that uses topic modeling. iVisCluster-ing offers the user control over how to cluster, using various devices such as the ability to control the weights of terms in a clustering. The visualization of a cluster is, however, a set of terms and their weights.

We use frequent itemsets in order to unearth concept interactions for our representation. Frequent itemsets first arose in association
We may think of this structure as a form of conceptual skeleton of the cluster. mining [6], and the literature on them is too vast to be recounted here. However, to our knowledge, they have not been used before to construct pictorial representations of document clusters.
In this section, we clarify the setting and describe some of the techniques that we will use later.

Corpus and Processing. Our data D will be comprised of a doc-ument collection (or corpus) D = { D 1 ,...,D n } . A K -clustering C = { C 1 ,...,C K } of D is a partition of D into K disjoint non-empty subsets whose union is D . We assume a standard prepro-cessing of documents for clustering: tokenization of words, stem-ming, removal of stoplists, and removal of words occurring very infrequently (say, less than thrice) in the corpus. The resulting set of tokens will be called terms . The vocabulary of D is the set of all terms in the documents of D . Sets of terms will be referred to as itemsets . Itemsets will be denoted by F , with subscripts when nec-essary. The support of an itemset F in D , denoted by supp ( F, D ) , is the set of all documents in D in which all the terms of F co-occur. Likewise, supp ( F,C ) is the set of all documents in the cluster C that contain all the terms of F .

Frequent Itemsets. For the definitions below, C is a generic cluster in C . First we define notions of frequent, and frequent rela-tive to C .
 Definition 1. 1. Let  X  be a positive integer that is called mini-mum support . F is said to be frequent (w.r.t.  X  ) if the size of the support of F exceeds  X  . 2. A frequent itemset F is said to be frequent relative to C if supp ( F,C ) &gt;  X   X  supp ( F, D ) , where  X  is a threshold and 0 &lt;  X  &lt; 1 . 3. Then F is said to be maximal frequent relative to C if it is the maximal (w.r.t. set inclusion) itemset with the preceding property.
By  X  X requent", we henceforth mean  X  X aximal frequent relative to a cluster C ": namely, maximally large sets of co-frequent terms are considered.

Norms for Terms. Next, we need a norm for terms in clusters that takes into account the clustering structure. Let N ( t,C ) denote the number of times a term t appears, in total, in all the documents of cluster C .
 Definition 2. Let t be a term that occurs in C . The proportional L 1 norm of t with respect to C , denoted  X  ( t ) , is defined as Namely, it is the proportion of the number of times t appears in C to the total number of term occurrences in C .

We denote by C [ ` ] the ` most frequent terms in C . Equivalently, these are the ` terms having the highest proportional L 1 We denote the maximal frequent itemsets of our generic cluster C by F ( C ) = { F 1 ,...,F q } . However, we will use a more man-ageable set of frequent itemsets X  X hose whose terms are all taken Definition 3. We define F ( C [ ` ] ) as the set of frequent itemsets for C such that for all F  X  F ( C [ ` ] ) , each term in F is from C
This technique is not only justified because it computes frequent itemsets using only significant terms, but it also allows for a com-putationally inexpensive implementation. This is because the C are just the top-` terms in the cluster centroid, which is usually computed during clustering.
In this section, we describe how we associate a  X  X hape" to each cluster. We describe this process in stages, providing an intuitive justification for each stage.
First, we explain the information needed to produce a shape for each cluster. This information is described below.
 Definition 4. The cluster information for a cluster C , denoted I ( C ) , is comprised of the following data: 1. The terms in C [ ` ] , along with their proportional L 2. The maximal frequent itemsets F ( C [ ` ] ) .

We have experimented extensively with various values of ` and recommend ` = 10 or 15 since the weights of the terms after that are too low in comparison to the higher weights.
Next, we use the information about the cluster gathered thus far to construct a coherent  X  X hape" for the cluster. As mentioned ear-lier, the shape is a vertex weighted colored directed graph that cap-tures the interactions between various concepts and topics in the cluster. We denote the shape graph of cluster C k , where 1  X  k  X  K , by G k . We specify G k in three steps.
 Step I: Constructing Subgraphs for Each Concept-Class
First, for each of the concept-classes F j (represented by the fre-quent itemset) in C k , we construct the subgraph G j k of G sponding to the concept-class. This construction is described be-low. 1. The vertices of G j k are the set of concepts in F j . The color of 2. Now we need to specify the edges. Step II: Joining the Subgraphs, Associating Degrees to Vertices
Next we  X  X oin" the various subgraphs G j k , for the index j running through the concept-classes of C k . In order to do this, we simply aggregate all the incoming and outgoing edges for each concept over all the subgraphs {G j k } . Therefore, in general, there will be vertices that have multiple edges emanating from them, as well as incident upon them.

Finally, for each 1  X  i  X  ` , we attach a weight  X  i that measures concept overlaps to the i th vertex: this is the number of concept-classes the concept-classes that the concept represented by the ver-tex occurs in. Note that this weight is not the same as the concept weight of the vertex. In order to emphasize this difference, we shall call  X  the degree of the vertex.
 Step III: Coloring Vertices by Concept Weight
The mapping of concepts to weights by importance is too fine for our purposes. We would like to transform the range of this mapping to a small discrete set of  X  X olors," with each color coding a relative level of importance. The natural way to do this is to place the weights of the concepts into a small number of buckets. Definition 5. Let C [ ` ] be the ` most frequent terms in C . Let  X  be the number of colors that are desired. Let the individual colors be denoted by c 1 ,c 2 ,...c  X  . Then we assign the colors to the terms as follows. We approximate the distribution of norms of the most frequent terms to be log-linear. In this way, the curve of their logs is a straight line. We then simply equally divide this line between the start and end values into  X  segments.
 G k is the  X  X hape graph," or simply the shape of the cluster. Note: We emphasize that once the graph has been colored, the con-cept weights are no longer used (except for laying out the graph as described in  X 4.3). The vertex weight (called the degree)  X  is not the concept weight of the vertex. Figure 1: The shape graph of a cluster in N20 at K = 10 having size 505 documents.
In order to lay out (or draw) G k , we proceed in two steps. 1. The vertices are laid out by color, in levels from top to bot-2. Within a single level (equivalently, among vertices having the
We demonstrate our cluster shapes on two standard research bench-mark datasets: N20 2 comprising of articles posted to 20 newsgroups; and REU , comprising documents that appeared on the Reuters newswire in 1987. For a manageable set of clusters for our examples, we clustered at K = 10 and K = 20 using the repeat-bisect method [ ? ] to generate clusters. We used (  X , X  ) = (15 , 0 . 3) . For the purposes of pedagogy in the examples of this paper, we adopt the following reasonable approximation of Def. 5. In many clusters, the highest weighted term has a weight of around 10. Therefore, we let  X  = 4 and the colors be red, green, blue, and yellow. Then, weights above 7 will be given the red color, between 7 and 4 the green color, between 4 and 2 the blue color, and below 2 the yellow color.

For lack of space, we show three cluster shapes from K = 10 and 20 of N20 in Figs 1 to 3, move their discussion to their captions, and provide a qualitative summary of the other shapes observed. Summary . The set of shapes observed showed a rich variety. We observed line graphs, disconnected graphs, graphs with peculiar connected components, and so on. This represents the richness in variety of concept interactions that are possible in document clus-tering. We emphasize that all of this richness is lost when the clus-http://qwone.com/~jason/20Newsgroups/ Figure 2: The shape graph of a cluster in N20 at K = 10 having 1,196 documents. Figure 3: The shape graph of a cluster in N20 at K = 20 with 444 documents. ter is represented by a digest. We are currently building an enter-prise prototype that uses this rich information for data analysis.
Rich shape-based presentations of clusters provide structural in-formation that is increasingly demanded by todays UIM applica-tions. The shape of a cluster readily reveals several important char-acteristics of the cluster, notably the interaction structure of the concepts that are discussed in the cluster. The shape readily an-swers the questions posed in  X 1. The shape is widely deployable: it can replace other cluster presentations (for example, digests) in any application where richer interfaces are desired.

This work suggests the following two avenues (among others) for further research: retrieval of clusters by shapes and comparison of clusters based on their graphs (by using graph isomorphism). If an UIM application could specify a shape that it was interested in, could it be retrieved from a collection by means of isomorphism? [1] N. Cao, D. Gotz, J. Sun, and H. Qu. Dicon: Interactive visual [2] P. Crossno, A. T. Wilson, T. M. Shead, and D. M. Dunlavy. [3] D. Cutting, D. Karger, and J. Pedersen. Constant [4] D. Cutting, J. Pedersen, D. Karger, and J. Tukey.
 [5] M. A. Hearst. The use of categories and clusters for [6] J. Hipp, U. G X ntzer, and G. Nakhaeizadeh. Algorithms for [7] T. Hofmann. Probabilistic topic maps: Navigating through [8] T. Iwata, T. Yamada, and N. Ueda. Probabilistic latent [9] H. Lee, J. Kihm, J. Choo, J. T. Stasko, and H. Park. [10] C. D. Manning, P. Raghavan, and H. Schtze. Introduction to [11] P. Pirolli, P. Schank, M. Hearst, and C. Diehl. Scatter/gather [12] A. Strehl and J. Ghosh. Relationship-based clustering and [13] J. A. Wise, J. J. Thomas, K. Pennock, D. Lantrip, M. Pottier, [14] O. Zamir and O. Etzioni. Grouper: a dynamic clustering
