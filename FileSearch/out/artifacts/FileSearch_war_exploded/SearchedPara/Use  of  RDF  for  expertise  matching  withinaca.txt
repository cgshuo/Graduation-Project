
In the current economic environment, organisations have realized that effective de-velopment and management of an enterprise X  X  organisational knowledge base will be a crucial success factor in the knowledge-i ntensive markets of the current century (Abecker and Decker 1999; Davenport and Prusak 1998). An organisational mem-ory 1 is designed to store what employees have learned from the past in order for it to be reused by current employees in solving pr oblems more effectively and efficiently.
There are two kinds of retrieval in the organisational memory. One is information retrieval , which aims to provide the information required by the task at hand. How-ever, access to information only is not suffi cient, as people often need to communi-cate with each other in order to find more important information that cannot be ob-tained from explicit documentation. That is why we need another kind of retrieval X  people retrieval . The process of finding relevant individuals who have similar inter-ests is also called expertise matching. As noted by many researchers (Ackerman and Halverson 1998; Bannon et al. 1996; Bennis 1997; Bishop 2000; Cross and
Baird 2000; Gibson 1996; Stewart 1997; Wellins et al. 1993; Yimam-Seid 1999), employees learn more effectively by inter acting with other emp loyees because the tacit knowledge and expertise people possess are difficult to codify and store in a knowledge management system. There is widespread agreement that the highest value knowledge is the tacit knowledge stored in peoples heads (Horvath 2000). Con-sequently, our research emphasis is on how to support people retrieval rather than information retrieval.
 If users wish to search for information i n web pages, they can use search engines.
However, if they want to locate an individual with the required expertise, there is no existing system that provides a satisfactory result. Users have to manually check different data sources stored in the organ isational memory in order to find pieces of information relevant to an expert and then combine them manually. Considering the huge amount of information that the organisational memory stores, it is difficult for users to find experts with specific expertise. The main challenge addressed in this work is how people retrieval can be improved by extracting relevant information as-sociated with an expert from different data sources and then semantically integrating them.

This paper analyses the problem of expertise matching and presents a RDF-based solution to the problem. This approach has been tested through a case study that can assist Ph.D. applicants to the School of Computing, University of Leeds, locate the potential supervisors with the required expe rtise. The profile of each potential super-visor is created based on semantically inte grated information derived from diverse data sources; hence, the performance of e xpertise matching is enhanced, which is illustrated through the results of the experiment.

This paper is organised as follows. It begins with an analysis of the expertise matching problem in Sect. 2. Section 3 descr ibes the possible approaches to solv-ing heterogeneous problems and justifies the use of RDF to the expertise matching problem. Section 4 demonstrates expertise matching in a brokering system that has been developed at the University of Leeds t o help Ph.D. applicants locate potential supervisor(s). It also describes the rationale for the system and presents the archi-tecture. The use of the system is illustrated in Sect. 5 along with the key results.
Section 6 investigates how to extend the single disciplinary expertise matching to multidisciplines domain. Section 7 compares our work with other related projects and finally conclusions are given in Sect. 8.
There are many definitions of expertise. One definition from Webster X  X  dictionary is  X  X aving, involving, or displaying special skill or knowledge derived from train-ing or experience. X  Engestrom gave a further explanation:  X  X xpertise resides under the individual X  X  skin, in the form of exp licit or tacit knowledge, skills and cogni-tive properties that enable one to display superior performances in the given field X  (Engestrom 1992). It is also defined as  X  X he ability to apply intellectual skills and knowledge to solve problems X  (Abel et al. 1998). However, the substance of skills, knowledge and ability is a hidden variable and difficult to codify. One solution is to ask experts to express their expertise in several keywords, which are then stored in an expertise database. Examples are COS 2 , VTED 3 ,BATH REPIS 6 is distinct from these and a brief description of the system is given here.
The University of Leeds Research Expertise and Publications Information System (REPIS) is a web-based information management system. It stores information about publications and research proj ects acquired from a variety of different sources. The principal objectives of REPIS are to provid e a directory of resear ch expertise across the University so that users can locate members of staff with expertise in specific areas. The REPIS Expertise Matcher acts as a knowledge broker connecting know-ledge seekers and knowledge Providers, as shown in Fig. 1. The difference between
REPIS and other systems is that expertise is not input by the individual academics themselves but derived from their associated work outputs, in other words, their pub-lications and projects. The information o f these work outputs has already been stored in an organisational memory. The current REPIS system uses search methods em-ployed by Microsoft SQL Server to search publication and project databases in order to locate the most appropriate expert(s).
The above two kinds of approaches use DBMS techniques to store and retrieve expertise. However, there are limitations associated with DBMS techniques. First, users looking for experts in a particular field need a lot of information in order to assess if this is an appropriate person to contact. For example, users need to know the experts X  position(s), their research interests, the project(s) in which they are involved, records of activities, and they may even want to read research papers the experts have produced. Manually creating a database to store all this information is very difficult and expensive. Second, th ere is the critical problem of maintaining up-to-date information. A person X  X  expertise changes over time and it is not feasible to rely on the individual to report developments to their expertise profile and even so, the database maintenance task would be significant if many individuals were involved (for example, there are nearly 4,000 academic-related staff at the University of Leeds).

Rather than creating a new database to sto re the duplicate information, some sys-tems have been built to find up-to-date expertise information by monitoring users X  emails (such as Foner (1997), Kanfer and Schlosser (1997)), browsing behaviour (such as Cohen et al. (1998), Pikrakis et al. (1998)), organisational electronic repos-itories (such as Mattox et al. (1999), McDonald and Ackerman (2000)), and so on.
However, using email as an implicit source for expertise may raise privacy and secu-rity problems, and browsing behaviour refl ects users X  interests more than their exper-tise. Organisational electronic repositories are good sources to extract people X  X  exper-tise. For example, personal homepages, pub lications, projects can all be considered as expertise indication. However, most systems rely on a single data source only. One important issue is whether it is possible to a utomatically extract the pieces of infor-mation relevant to each expert from heteroge neous data sources in the organisational memory and then integrate them dynamically. In order to locate the right experts and present the supported information, it is n ecessary to collect more relevant informa-tion of each expert. To address this issue, we have to examine closely what kind of data source is stored in the organisational memory, including an expertise indicator.
There are a number of different data sources, varying from structured data (such as project databases) and semistructured data (such as personal web pages) to unstruc-tured data (such as technical reports) that include evidence of expertise. This het-erogeneity brings many difficulties in information integration. Heterogeneity can be classified differently (Busse et al. 2002; Seligman and Rosenthal 2001; Sheth 1998); the most common types are (1) heterogeneous interfaces, (2) heterogeneous attribute representations, (3) heterogeneous schemas, (4) heterogeneous semantics, (5) object identification .
The problems of heterogeneity have been addressed in various studies and corres-ponding techniques have been proposed. Trad itional approaches, which include stan-dards like ODBC, middleware, federated d atabase system (FDBS), and mediator-based information system, all suffer limitations. For example, middleware may be costly and may be inefficient compared with using native interfaces (Seligman and
Rosenthal 2001). FDBS is only applicable to databases, while mediator-based in-formation systems require the software developers to have a clear understanding of a variety of metadata as well as a comprehensive understanding of schematic hetero-geneity (Sheth 1998). In rule-based mediators (such as Garcia-Molina et al. (1997)), rules are mainly designed to deal with structural heterogeneity problems rather than semantic heterogeneity (Stuckenschmidt 2000). The literature on integration is more concentrated on syntax and structure, with few studies focusing on semantic inter-operability (see, for example, Fensel e t al. (1999), Stuckenschmidt (2000)). ing standard for data interchange on the web. XML has defined a neutral syntax that can transform diverse data structures into graph-structured data as nested tagged elements (Seligman and Rosenthal 2001). In this way, heterogeneous data structures can be represented in a uniform syntax X  X ML. Using XML, the three problems listed in Sect. 2 can be alleviated (heterogeneous DBMSs, heterogeneous attribute representations and heterogeneous schemas). However, XML cannot support inte-gration at the semantic level. For example, there are two expressions:
Black &lt; /Surname &gt; and &lt; Lastname &gt; Black &lt; some semantics. However, the system does not understand that Surname and Last-name mean the same thing and that they are related to another concept, for example, person. XML schema provides support for e xplicit structural cardinality and data typing constraints, but does not provide much support for the semantic knowledge necessary to integrate information (Hunter and Lagoze 2001). Again, XML does not play a very significant rol e in object identification.
 schema language for RDF) (Brickley and Guha 2000) are W3C recommendations for describing metadata on the web. They can be used to solve the semantic heteroge-neous problem. RDF provides a standard representation language for web metadata based on directed labelled graphs (Karvounarakis et al. 2000). It consists of three ob-ject types: resource, property and statement. Every resource has a uniform resource identifier (URI). The use of URIs to unambiguously denote objects and of properties to describe relationships between objects distinguish it fundamentally from XML X  X  tree-based data model (Decker et al. 2000). The same RDF tree can be expressed differently in many XML trees because th e order of elements in an XML document is very restricted. So RDF successfully avoids the problem of querying XML trees, which attempts to convert the set of all possible representations of a fact into one statement (Berners-Lee 1998).
 a shared set of terms describing the appli cation domain is needed. This is called an ontology 7 , which includes not only the definition of the terms but also the relation-ships between these terms. RDF/RDFS can be used to define instances/concepts of an ontology. Through using ontology to make the implicit meaning of their different terminologies explicit, it is then possible to dynamically locate relevant data sources based on their content and to integrate them as the need arises (Cui et al. 1999). tion, the use of RDF/RDFS in an organisational memory can now be explored. The aim is to provide a coherent and meaningful view of the integrated heterogeneous information sources associated with each expert. To better understand the issue in-volved in the design of RDF-based expertise matching, the following case study has been chosen. This case study is about a brokering system, which matches Ph.D. ap-plicants with potential supervisors in the School of Computing at the University of
Leeds.
The School of Computing in the University of Leeds is a large department and each year there are approximately 50 applicants who want to join the School as research students, but there are also many more general enquiries from individuals consider-ing making an application. Potential research students can either trawl through web pages and search databases to try and locate information about potential suitable supervisors or they may simply ask the School X  X  Ph.D. Admissions Tutor to select a suitable supervisor for them based on their proposed research topic. However, it is still very difficult for the Ph.D. Admi ssions Tutor to recall up-to-date details of all the expertise and research interest s for each academic, as individual expertise and research interests may continually ch ange and develop, especially as the rate of staff turnover is 10% a year. Furthermore, the Ph.D. Admissions Tutor may not fully understand the applicants X  intents b ecause some applicants use quite specific technical terminology. As a result, the supervisor that the Ph.D. Admissions Tutor recommends may not be the most suitable one, and there exists a real possibility that some appropriate applicants are rejected because their needs cannot be appropriately matched using such a method. In the future, we would like to make online brokering available, which would assist applicants in refining their choice of supervisor before they submit a final application.

The design of the brokering system proposed here aims to improve the process of matching supervisors and potential research students by enabling the applicant to make more informed choices about a supervisor before they formally apply to the
University, thus benefiting both the School and the applicant in the long-run.
To identify the support tasks needed in the brokering system, let us consider the following scenario, which represents a typi cal case of the problem described above:
Mary is a Masters student at the University of Manchester and plans to study for a Ph.D. She searches the web pages of several universities, including the
University of Leeds; her preferred resear ch interest is  X  X eterogeneous database systems. X  Mary first navigates the School of Computing website at the Uni-versity of Leeds and browses the homepage of each member of staff. She quickly finds that there are a large number of staff in the School and many of them are not active researchers. Th en she decides to browse the research groups in order to quickly locate a potential supervisor. She finds these web-sites are not well organised. Although she searches very carefully, she still does not find an academic who can match her requirements. She thinks that maybe there are no academics conducting research in this area and she should not apply to Leeds University.

This is not the desired outcome, as there are academics at Leeds who could supervise Mary. The scenario draws attention to the following problems involved in identifying the potential supervisor(s):
Low recall: This means that relevant people may be missed. This is mainly due to the fact that (1) there is a large number of staff in the School and it is a very time consuming task for the user to acces s each person X  X  homepage; (2) the web page of each research group does not give su fficiently detailed information on the
Low precision: This means that some of the people found are not experts in the required of the brokering system:  X  Understanding user needs/identification of expertise requirements  X  Understanding the domain knowledge in order to provide translation between  X  Providing an integrated view to the user from the multiple information sources  X  Providing up-to-date inf ormation of each researcher
The architecture of the brokering system is shown in Fig. 2 (Figure 2 also illustrates the different data sources used in our case study). The whole process of the archi-tecture can be divided into two parts, namely, (1) semantic information integration and (2) expertise management. The first p art was developed based on Vdovjak and
Houben (2001). Each component in the architecture is described in detail below:  X  Sources: Contains data sources that are relevant to identifying the expertise of  X  Wrapper: Different wrappers, such as DB-XML wrappers or HTML-XML wrap-processes are needed, such as adding me tadata in XML according to the vocab-ularies stored in the conceptual model (see Fig. 3).  X 
XML-RDF broker: Identifies the relevant concep ts in the XML sources and re-places them with the concepts in the conceptual model; the mapping rules are specified in XSLT (Clark 1999). These mapping rules are defined by the ap-plication designer 8 and can be modified if the concepts of the source change.
However, the underlying conceptual model should be stable, as it is the basis for the semantic integration; if it has to be changed, then the RDF model and the mapping rules should be modified accord ingly. This layer also creates the
RDF data from the XML instance in order to provide the actual response to a mediator X  X  query.  X 
Mediator: Maintains the conceptual model (shown in Fig. 3). This layer identifies which data sources are relevant to the query, transfers the query to subqueries and gets subresults from brokers. These subresults are input into RDFDB  X  Expertise manager: In addition to maintaining experts X  information (experts pro- X  Concept identifier: Receives a user X  X  query and pro vides the possible relevant relevant concept and sends the specified concept and the extended query to the expertise manager.  X 
User interface: Receives the query from the user and sends the results of the ranked experts together with the detailed information of the experts to the user.
The implementation of the architecture described in the previous section includes several crucial aspects, which are briefly described below. 1. Indexing and retrieval of concepts: the concepts and associated keywords are chosen from expertise domain model (combination of ACM Computing Clas-sification 10 and online computing dictionary X  X OLDOC 11 ing Classification System has roughly 100 third-level headings and it provides a relatively stable scheme that covers all research in computing (Halpern 1998). FOLDOC is a searchable dictionary of computing contributed by 1,500 people.
The dictionary has been growing since 1985 and now contains over 13,500 defini-tions, totalling nearly five megabytes of te xt. Entries are cross-referenced to each other and to related resources elsewhere on the net. The concepts and their asso-ciated keywords and supervisors are stored in a relational database. This database is connected to the Java system code via JDBC. The possible relevant concepts are retrieved based on the research interests that the user inputs. 2. Constructing the detailed information for supervisors: First, relevant information from diverse data sources is collected. T he information stored in the web pages and the REPIS database is transferred into XML form using wrappers. Some manual annotations are added for interpre ting the information stored in the un-structured data sources. Manual annotation is time consuming and it is noticed that some annotation tools, such as MnM 12 and Ontomat able to provide a degree of automation. Second, these XML files are then trans-ferred into RDF data according to the mapping rules specified in XSLT. Third, the separate RDF data is input into an RDF database X  X DFDB. Fourth, a search is conducted on RDFDB to produce the complete detailed information for each supervisor. The third and fourth steps are implemented through a Java interface for RDFDB. 3. Creating expertise profiles and ranking the expertise of potential supervisors: The integrated information of each expert is considered as one document stored in a repository. Through scanning all the documents in the repository, the keyword profile of each expert (represented as vectors of keywords) is created by the ex-pertise manager using vector space m odel techniques (TF-IDF) (Baeze-Yates and
Ribeiro-Neto 1999). In addition to keyword profiles, the expertise manager re-trieves the relevant concepts accordin g to the domain ontology, these concepts are then confirmed by experts. The confirmed concept(s) of each expert is (are) stored by an expertise manager in a re pository. The relevance of each potential supervisor is calculated through the sim ilarity between the profile of each poten-tial supervisor and the extended user query (adding the description of a concept to the original query). The weight attr ibuted to each potential supervisor is then 4. Displaying the semantically integrated information of potential supervisors: This
In order to provide high-quality informa tion about each expert, it is essential that accurate information is available. For exa mple, personal homepages and technical reports should be updated annually. In our case, the REPIS database, as a core data source, is heavily relied upon. This is because the data stored in REPIS on individual academics has been validated by the adminis trator of each department. There are also a number of automated validation processes built into REPIS. For example, one data source held in REPIS is ULRICHs 14 the authoritative serials bibliographic database providing details of title and the International Standard Serial Number (ISSN) for journals published throughout the world. If an administrator attempts to input de-tails for a publication type of academic j ournal paper and indicates an incorrect journal title and/or ISSN, then they will be automatically informed of this and pro-vided with the correct details. The other data sources (such as personal homepage and technical reports) are complementary to REPIS in order to provide a richer de-scription of each expert. If there is conflict between the REPIS database and other departmental sources, the REPIS databas e takes precedence. The duplicate informa-tion (for example, the same information a bout a paper stored in different places) will be deleted according to the predefined rul es. For example, if the two papers have been published in the same year with the same title, then it is assumed that these two papers are the same and only one paper is displayed in the final presentation.
A prototype brokering system, using the ar chitecture described above, has been built and used to match Ph.D. applicants with potential supervisors. The search for po-tential supervisor(s) follows three steps, which are summarised here and described in more detail below: 1. The user inputs a description of their preferred research interest(s) and selects 2. The user views a list of the names of acad emics working in the relevant research 3. The user views the detail of each academic and selects one as their preferred Step 1: Initially, the user inputs a brief description of their general research interests.
This description is formulated in natural language. A list of relevant research areas will then be displayed (Fig. 5). The releva nt research areas are ranked according to the number of keywords contained in the research interest field entered by the user that are relevant to each research area. Each result consists of three parts. First, the value that indicates the number of keywords that the user inputs that are relevant to the research area; second, the research area, which is displayed in upper case; third, a list of the relevant keyword stems, which are used to search all variants of the same keyword. The user can view the detailed information of each research area by clicking on  X  X how me the detail X  or they can  X  X ccept X  the resear ch area if they feel this is an area in which they would like to conduct research. They may accept as many research areas as they wish.

Step 2: The user can select any relevant research area in order to view a list of potential supervisors working in that research area (as shown in Fig. 6). The potential supervisors are ranked according to how lik ely it is that this person will be selected as the potential supervisor. The example shown in Fig. 6 indicates that there is a 64% chance of choosing Mr E. Atwell as the potential supervisor, a 23% chance for Dr D.C. Souter and a 13% chance for Dr L.W. Bod.

Step 3: The complete personal profile of the potential supervisor (as shown in Fig. 7) will be displayed if the user clicks on  X  X iew supervisor. X  The full detail page of Dr D.C. Souter appears like a standard personal homepage currently existing in the School of Computing, but in fact the information is taken from different data sources.
As shown in Fig. 7, the data is retrieved as follows: (1) The personal contact in-formation and research interests are retrieved from the personal homepage; (2) the publication section is a combination of information from the personal homepage and from a series of technical reports, w hich can be downloaded from the REPIS database. The duplicate information is del eted and the final resu lts are reorganised into a consistent format so that the user is not aware that this data comes from heterogeneous sources; (3) the project information is also retrieved from the REPIS database.
In this work, the traditional keywords s earch approach (search I) was used as the baseline against which to evaluate the brok ering system (search II). In search I, the algorithm used to calculate the similarity between the expert X  X  profile and the user X  X  query is as follows: where t ip is the weight of the i th term in an expert X  X  profile p and t of the i th term in the query q . t iq = 1ifthe i th term appears in the user X  X  query, oth-erwise t iq = 0. In search I, experts X  profiles are calculated based on the information stored in the REPIS database such as their publications and projects. the concept c ) is used instead of t iq in order to calculate the similarity between the expert X  X  profile and the concept that the user specifies. t in the concept description or the user query, otherwise t profiles are calculated based on their integrated personal detailed information; this includes their research interests, their pub lications and projects a nd technical reports.
The aim of the brokering system is to improve the expertise matching perform-ance. Therefore, the success of the brokerin g system (search II) is measured in terms of whether the brokering system achieves the following benefits when comparing with search I:  X 
Save time in locating experts;  X 
Improve the accuracy of the search results;  X  Provide richer descriptions of individual experts for selection purposes.
The accuracy of the expertise matching relie s on (1) whether the retrieved people are relevant experts in the specific area and (2) whether the ranking order of the retrieved experts is appropriate, in other words, the expert with more expertise is ranked higher than those with less expertise. Users need to have certain background knowledge in order to answer these two questions. Although Ph.D. applicants are the real users of the brokering system, it is found that they are less suitable to test the system than the current Ph.D. students. This is becau se the current Ph.D. students have more knowledge in their specific area and they know the relevant experts in the School and have more ability in judging experts X  exper tise. This is confirmed through interviews with individual Ph.D. students. Therefore, in the evaluation process, participants are current Ph.D. students rather than Ph.D. applicants.
 The experiment was conducted in the School of Computing, University of Leeds.
There are more than 100 staff in the School. Participants were asked to volunteer from the current Ph.D. students in the School. Fifty percent of all Ph.D. students attended the experiment. They ranged from first year to third year and their research interests are very varied (in fact, their res earch areas covered all the possible research areas in the School). Participants were asked to compare between two searches and they were given full instructions as well as a guided tour. Participants started with the research interests that they input in their application forms and then conducted a search for potential supervisor(s) 15 from returned experts by search I and search II.
Throughout the feedback sessions, the participants were encouraged to feedback their thoughts on the brokering system. After the feedback sessions, each participant was asked to complete an evaluation form assessing the utility and perceived usability of the system and the ways in which the brokering system performs better or worse than the traditional keyword search approach.
 search I. For example, No. 1 participant found 22 potential supervisors retrieved after he input his research interests. Among these 22, 2 were selected as relevant potential supervisors, and they were positioned 1st and 3rd on the list. The actual supervisor of the student was found and was positioned 3rd on the list.
 visors returned by the system in each cas e. The only way for the participants to evaluate the potential supervisors on t he list was to check each expert X  X  detailed information (in this case, only publication and project titles stored in the REPIS database). Participants started to lose patience after they had checked about seven or eight potential supervisors. Under this situation, ranking was very important in order to list the most relevant potential supervisors first. Unfortunately, the testing results showed that the ranking was not correct and not useful in helping participants locating the potential supervisors. From Table 1, it can be seen that 45% of actual supervisors were positioned below 10th position on the list or not found. As a con-sequence, it is no surprise that 55% of participants believed that the ranking was incorrect and not useful; 40% of participants thought that the ranking was partially useful (see Fig. 8). The precision of search I was calculated by dividing the number of accepted potential supervisors by the tota l number of potential supervisors (see
Fig. 9). The average precision of search I was 14.6%. If the number of the returned potential supervisors to limited to 10, then the precision was 22.1%.
Ta b l e 2 17 shows the results of selecting relevant experts from results returned by search II. For example, No. 1 participant found two research areas relevant to his research interests. There were four potential supervisors associated with the first re-search area and another two potential supervisors associated with the second research area (a total of six potential supervisors). After checking the detailed information of each potential supervisor, the user select ed two potential supervisors who were po-sitioned 1st and 2nd on the list for the fi rst research area and 1st for the second area. The second potential supervisor turned out to be his actual supervisor.
From Table 2, it can be seen that the number of possible supervisors returned for each participant by the system was re duced. This is because the system searched relevant research areas first, which quick ly narrowed down the possible relevant su-pervisors. The accepted potential supervisor s (relevant experts) were ranked as 1st or 2nd for each accepted research area in most cases. It is noticed that all the actual supervisors of the Ph.D. students were listed on the list (in most cases, they were ranked as 1st. Although it is not certain that the actual supervisor of each student is the most appropriate supervisor, it should be noticed that the actual supervisor of each student was selected manually and methodi cally by the students themselves or by the Ph.D. Admissions Tutor. This means that, if the names of the actual super-considered to be successful. The precision of search II was improved with an average precision of 68.7% (see Fig. 9). The ranking was more appropriate than search I, as 100% of participants believed that the ranking was correct and useful. The dif-ferences between the results obtained fro m search I and search II are significant, as shown in Fig. 10, with 95% of participants indicating the results of search II as more appropriate than those of search I.

In conclusion, search II provides bette r expertise matching performance than low.  X  Number of experts retrieved: The number of experts retrieved by search II is  X  Average time spent on searching: Users spent much less time in search II than  X  Precision: Search II provides higher precisi on than search I, which means that  X  Content of detailed personal page: It is easier to evaluate the expertise of the downloadable documents, are given in search II. All the participants were satisfied with the detailed personal information provided in search II.  X 
Ranking: The ranking in search II is more appr opriate than in search I. The reason is that the ranking in search I is based on the keywords input by the user, so some irrelevant researchers may be ranked much higher than an appropriate supervisor if they have published papers including the particular keyword. In contrast, the ranking in search II is based on the res earch area and the profile of each research area is a short document that includes more relevant keywords in this research area. This profile can better present the m eaning of the preferred research area than a short list of keywords, so the ranking results are improved.  X 
Recall: Search II provides higher recall than search I. Recall means the ratio of total number of relevant retrieved people by the total number of relevant people.
Although the number of relevant people retrieved is known, it is difficult to find all the relevant people. However, the total number of relevant people should be the same for both searches, so what is important is which search provides a larger number of relevant people. In search I, people are retrieved according to the keywords in their publication or proj ect titles, which means that these titles should include at least one of the keywords that the user inputs; the problem is that different keywords may express the same meaning and there is the possibility that relevant people are missed because they use synonyms in their publication titles. This limitation is overcome in search II as people are retrieved based on the relevant research area (concept), so all the people associated with one particular research area will be displayed together and it is less likely that relevant people will be missed. This is illustrated by the results where search II was considered to be more appropriate by participants.
The previous sections discuss how to help people locate experts and share knowledge within a single discipline. However, there is an increasing number of experts whose expertise and research interests span across more than one discipline. Thus, exper-tise matching should not only support locating single disciplinary experts but also multidisciplinary experts. Mu ltidisciplinary experts is the first step toward successful knowledge sharing between research ers from different disciplines.

Multidisciplinary 18 research implies research involving the interaction among two or more different disciplines. This interaction may range from simple communication of fields to the mutual integration of organising concepts, methodology, procedures, epistemology, terminology and data 19 . The Informatics Network (IN) at the Uni-versity of Leeds (as shown in Fig. 11) is one example in which multidisciplinary research is very common. Informatics Res earch Institute (IRI) is the hub of a grow-ing informatics network ranging across computational geography, complex systems, ecology and evolutionary biology, medical physics, health informatics and bioinfor-matics. IN offers a unique approach to the development of sophisticated computa-tional skills and their application to challenging real-world problems from a wide range of domains. Through coordinating cross-disciplinary collaboration, and thereby connecting the various informatics communities, the Informatics Network will allow ideas and techniques currently specific to individual domains to percolate through the various informatic s research enterprises 20 . In this situation, how to attract ex-perts from other domains to join IN is a critical issue. Multidisciplinary expertise matching will play an important role in locating scientists from other domains and can contribute relevant expertise to form new communities.
As stated in (Yimam-Seid 1999), there are seven domain factors in the experts find-ing systems domain model, namely (1) basis for expertise recognition, (2) expertise indicator extraction, (3) expertise models, (4) query mechanisms, (5) matching oper-ations, (6) output presentations and (7) adaptation and learning operations. Among these seven factors, items (2), (3), (4), (5) and (6) will be different when matching multidisciplinary experts rather than single disciplinary experts and these differences are discussed briefly below.  X  Expertise indicator extraction: Ideally, this should be domain-knowledge driven.  X  Expertise models: There are two kinds of expertise models. One consists of key- X  Query mechanisms: When seeking single-discipline experts, users are required to  X  Matching operations: Exact keywords matching or statistical/similar-based match-plinary experts, both experts X  profiles and users X  profiles should be grouped ac-cording to how many disciplines are involved. The matching should then be con-ducted separately and the individual matching results should be combined in an appropriate way.  X 
Output presentation: In single-discipline experts matching, the experts will be ranked according to the expertise level on a particular concep t while multidisci-plinary experts matching will have more than one criteria due to the variety of user requirements.
The expertise domain model described below is the core of our multidisciplinary expertise matching approach. While the single-discipline expertise domain model is one dimensional, the multidisciplinary expertise domain model is two or more dimensional, depending on how many disciplines are involved. Table 4 shows the expertise domain model for Geoinformatics. This can be obtained through analys-ing co-occurrence between computing and geographic concepts. There is wide ac-ceptance that concept matching results in be tter performance than keyword matching (Brasethvik and Gulla 2002). This is also illu strated through the experimental results shown in Sect. 5.2. Hence, concepts have b een used rather than keywords in the ex-pertise domain model proposed here. One dimension represents computing concepts such as C 1 ,C 2 ; the other dimension represents geographic concepts such as G
C -G j means that there is a link between the i th computing concept C geographic concept G j ; otherwise, 0 is displayed. For example, suppose C sents neural network ,G 1 represents water policy and development ,G historical geography ,C 2 -G 1 means neural network technique for water policy and development and G 1 -C 2 means water policy and development by neural network technique. The former focuses on computing technique and the latter focuses on ge-ography application. Not all the computing concepts and geography concepts can be combined. For example, th ere is no connection between neural network and histor-ical geography . This table can be seen as a central re presentation of geoinformatics expertise. The expertise model of each expe rt can be expressed as a collection of the selected items from the table, for example, {C 2 -G 1 ,C 2 pertise model can be used to support users in searching and visualizing the expertise information.
The expertise model should be expressed in two ways X  X oncepts representation and keywords representation. Basically, it is the same as for the single-discipline ex-perts matching; however, concepts are selected from the expertise domain model, for example, {C 2 -G 1 ,C 2 -G 3 ,C 3 -G 1 } and keywords are selected from both disci-plines. The concept representation is very difficult to implement automatically. The common solution is to ask the multidisciplinary experts themselves to tick the rel-evant expertise from expertise domain model. The second representation, a set of keywords, with weights {K 1 (w 1 ) ,K 2 (w 2 ) , ... ,K m experts. The weight of each keyword can be calculated through traditional IR (vec-tor model) techniques after source wrappers extract the expertise indicators from the heterogeneous information sources.

The multidisciplinary brokering system, which is the extension of the single-dis-cipline brokering system, should satisfy three kinds of user, each with a different requirement depending on their background in either the applied area or in the tech-nique.

Case 1: Users who know the computing technique and the particular applied area in which this computing technique can be used and seek experts who have expertise in both areas.

Case 2: Users who know only the applied area and seek experts with expertise in the applied area and who have also successfully used some computing techniques in solving the problem in the applied area.

Case 3: Users who know the computing technique and look for potential application areas to which they can apply this technique.
 our first attempt to the problem. The initia l studies being conducted concentrate on with how to build an expertise domain model, how to build an individual X  X  expertise model and how to rank expertise. On the one hand, topics in recent geocomputation conferences have been collected through Inte rnet searches, from which the relevant computing techniques that have been explored in solving geographic problems are extracted, for example, pattern recognition, spatial analysis, simulation, etc. On the other hand, a simple classification for the g eography-applied area (relevant to Leeds
University) has been created, which includes 2 levels and 17 items in total. Then the cooccurrence between computing con cepts and geography concepts have been calculated through analysing the informa tion (such as title, abstract, summary) of a previous 22 geoinformatics projects. The following shows one example: would determine number of houses to be built on developable sites. The model would be tested in a selected study area to see the impact of certain development to the communities.

Applied area(s): Population and migration/urban or regional geography (from geography classification).

Computing techniques: Pattern recognition, spatial analysis, simulating, mod-elling, distributed GIS environment, spatial decision making.

After building the initial expertise domain model, the expertise model of each expert is then created based on this domain model, expended with the weighted keywords (this keywords representation can be obtained using the same technique as in single-discipline expertise matching). Ranking consists of two parts: ranking on the geography applied areas and ranking on the computing techniques. The keywords in the expertise model are divided into two groups: one is geography domain, the other is computing domain. These two sets of keywords can be seen as two sets of vectors. The ranking in each domain is based on vector model. Combining these two ranking results is still an open issue.

Through investigating the multidisciplinary brokering system, ontology is found as the most important factor because it in fluences other operations such as exper-tise indicator extraction, building the expertise model, ranking experts and providing adaptability. For example, if the sufficie nt or necessary keywords for each concept were defined in the ontology, then the concepts representation of expertise model would be semiautomatically created. The better the ontology, then the better the matching. Consequently, in order to build an effective multidisciplinary brokering system, it is critical to build good-quality ontologies first.

In our initial studies, some manual work had to be done in building the multidis-ciplinary brokering system, such as analysing the computing technique(s) involved in multidisciplinary projects and the applied areas associated with these projects. These were identified by domain experts. However, it is expected that, with the develop-ment of the semantic web and annotation tools, if the research topics of each project are annotated using the controlled vocabularies in the ontologies, then the mapping between these two ontologies could be semiautomatically built up.
The work reported here is compared with several related projects concerned with finding experts. 1) Keyword-based search to locate experts: Expert Finder (Mattox et al. 1999) ex-ploits an organisation X  X  intranet documents to locate experts. The system ranks em-ployees by the number of mentions of a term or phrase and its statistical association with the employee name either in corporate communications such as newsletters or based on what they have published in their resume or document folder. Expert Rec-ommendation (Yukawa and Kashara 2001) is a system that locates engineers with a high level of expertise on a particular topic. Information source is a huge set of technical documents produced by experts. Vector space model technique is used in analysing these documents. Keywords and documents are mapped in the same multi-dimensional space through cooccurrence based thesaurus or dictionary-based concept base. Each personal profile is derived from a ssociated documents and is represented as a weighted vector. Experts are ranked acco rding to how similar are the query and the experts X  profiles.
 2) Ontology (thesaurus)-based search to locate experts: CKBS (Liao et al. 1999) system, as part of organisational memory, builds on an ontology-based model of competence fields. Experts are linked to the specific field in the ontology. In add-ition, the relevant technologies of each project have been specified. Complex heuristic inferences are used to find knowledgeable persons. Expert Locator (Clark et al. 2000) uses concepts, a large prebuilt, technical thesaurus as the initial ontology and en-hances it using simple AI techniques. This results in a reduction in the ambiguity of the single keyword problem and also exploits domain knowledge for the search. pertise matching is based on the semantic integration of heterogeneous information stored in an organisational memory rather than a single data source such as publica-tions or projects. (2) Our hybrid approach combines the advantage of flexibility of keyword search and accuracy of ontology-based search. Although an ontology-based search can quickly narrow down the relevant experts, it cannot distinguish one expert from another. In contrast, vector space model is good at ranking expertise of experts, but a syntax search may bring some irrelevant experts into the results. (3) The output presentation of experts in most experts finder systems is quite simple, only expertise identification (McDonald and Ackerman 1998) is targeted. In our system expertise selection is supported by providing high-quality in formation relevant to each expert.
An investigation of expertise matching is important, as it can be used by a knowledge broker to link people between industry and academia. Universities are now putting more emphasis on collaboration with industry than ever before in order to get finan-cial support and exploit the knowledge within the organisation. As the major part of knowledge assets in academia is people with expertise in particular fields, it is very important to match expertise within the universities to external opportunities and then facilitate tacit knowledge exchange th rough collaboration on various projects.
For example, in the KiMERA 21 project, expertise matching is used to identify a set of researchers from Leeds University with the appropriate expertise to solve specific problems.
 ganisations to form project teams, transfer best practice and so on. Expertise match-ing can help people be aware of others who are doing similar things. For example, in the University of Leeds, researchers from the School of Education, School of Com-puting and School of Psychology are conducting similar research on collaborative learning. It may bring potential benefits if these researchers knew of each other and shared expertise.
 kering system has been developed in the School of Computing, University of Leeds.
In this prototype system, RDF is used to provide a uniform underlying data model and RDFS is used to represent ontology, which acts as a semantics backbone for accessing and integrating heterogeneous in formation sources. To date, our prototype system has been tested in the School. The initial promising results indicate the ben-efits of using RDF/S in expertise matching against traditional DBMS techniques in the following areas: (1) It improves the accu racy of expertise matching by combining the advantages of keyword-based and ontol ogy-based approaches and (2) it facilitates users in the expertise selection process by providing more detailed and up-to-date information of each expert, which is separately stored in an organisational memory.
We intend to improve expertise matchin g by guiding users to find the adjacent user. To widen the application area, the initial studies on multidisciplinary exper-tise matching have been undertaken and the multidisciplinary brokering system will be built in order to satisfy the users X  requirements for sharing knowledge across disciplines.

