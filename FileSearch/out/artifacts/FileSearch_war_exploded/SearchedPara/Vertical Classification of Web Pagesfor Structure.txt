 Recent years, structured data extract ion attracts many interests and shows a promising future [1 X 3]. However, a common hypothesis behind a great portion of these methods is that the web pages should have been classified into specific verticals (e.g., books, cameras, jobs). Classification techniques can be used for this, and we call this kind of classification task to divide web pages into diverse verticals as Vertical Classification .

A lot of hierarchical classifiers [4 X 6] have emerged. The hierarchy structure of categories used by traditional hierarchical classifiers is constructed manually or given in advance. However, to build the hierarchy structure of categories man-ually is time-consuming and the results are sensitive to the subjective opinions of different persons. Moreover, most of them aim at classifying the news-like documents, which apply many sentences to convey some information, into dif-ferent topics or genres. None of them focus on classifying web pages containing structured data into verticals to our knowledge. We will give a comprehensive study on vertical classification methods in this paper.

There are some specific issues for the vert ical classification techniques which make traditional web page classifiers could X  X  be directly applied. First, the gen-eral document representation TF-IDF doesn X  X  perform well, owing to that all words in one page should function identically when more discriminative features are selected. In other words, the commo n assumption for web page classifica-tions that the feature with high term f requency in one page is considered as more important than others [7] is no longer suitable. Second, different verticals may have semantic similarities, which bring troubles to vertical classification. For instances, the four verticals Cameras, Cellphones, Tablets, Mp3players have many identical chara cteristics and are difficult to be distinguished by general web page classification.

These issues will be systematically investigated in this paper, which reports a general hierarchical vertical classification framework. Given flat datasets whose relationship among verticals is unknown or not provided, our framework can firstly makes use of agglomerative hierarchical clustering to exploit the intrin-sic hierarchical structure of relationships among verticals automatically. With a metric which is proposed to partition the hierarchical structure into levels, train-ing data is accordingly reconstructed into hierarchy, and a hierarchical classifier is learned.

We conducted a set of comparison experiments for the design of vertical classi-fiers, such as with flat vs hierarchical structure of relationships, as well as among different feature selection and classificat ion methods. Experim ental results show that the hierarchical classifiers built on the basis of the proposed framework make big improvements over the flat classifiers in classifying unseen web pages, with Support Vector Machine using Odd Ratio as the feature selection strategy performs best. Furthermore, The hierarchical classifiers require only a quarter or less number of features of the corresponding flat ones, which would reduce the training time and facilitate the memory utilizing. Web page classification has attracted much attention for a long time, and a huge amount of classifiers have been employed into the task, including naive bayes classifier [4], decision trees [8], hidden n aive bayes [9], support vector machines [5, 6, 10]. These classifiers are organized either as flat (non-hierarchy) form or hierarchical form, namely flat classifiers or hierarchical classifiers.
Comparing to flat classifiers, the hierarchical ones have many advantages. The main of them is that as the intrinsic hierarchy structure can factually describe the reality, it can be used to improve th e performance. Ceci et al. [4] applied the hierarchy of categories into all phase of text classification to construct hier-archical classifiers, and comparison experiments were done on Yahoo, ODP and RCV1 dataset. Cai et al. [6] built a SVM classifiers based on discriminant func-tions, which were structured in the way that the class hierarchy was mirrored to include the inner-class relationships, on the WIPO-alpha patent collection. Dumais et al. [5] constructed a two-leve l hierarchical SVM classifier with two different combination rules manually. Their experiments on LookSmart showed small advantages over baseline flat models. Gentile et al. [10] proposed to turn the hierarchical SVM classifier into a Bayes classifier and the method outper-formed the simple hierarchical SVM on RCV1 and OHSUMED.

In above hierarchical classification methods, the hierarchical structure of cat-egories are manually annotated or simply referred from public directories, which requires amounts of human efforts or may not be suitable for all applications. Weigend et al. [11] employed cluster analysis to suggest an implicit hierarchy structure firstly and human assignments were applied to verify the structure. Its main drawback is that it isn X  X  suitable for building large-scale heterogeneous web page classifiers automatically and can not keep pace with the explosive growth of web pages on the internet.

Moreover, existing web p age classification researc hes are mainly focusing on text classification for news-like pages, and their targets are to label the contents with different topics, taxonomies or genres, not the verticals. Vertical classifica-tion, which is assumed as a prerequisite process of structured data extraction methods [1 X 3], is not well studied. We propose a framework which can learn a hierarchical classifier from flat non-hierarchy dataset automatically. It is shown in Figure 1. By using agglomerative hierarchical clustering, the intrinsic hierarchy structure of the verticals can be constructed automatically, and then the training dataset is reconstructed into hierarchical levels. Afterwards, a classifier can be learned to distinguish a sub-level vertical from other vertic als within the same top vertical. 3.1 Find the Hierarchy of Verticals Automatically In order to automatically explore the inherent hierarchical structure of verticals, an agglomerative hierarchical clustering process ( X  X rom bottom to up X ) should be performed ahead. In our framework, we employ a vector v i =( v i 1 ,v i 2 ,  X  X  X  ,v iN ) to represent a special original vertical. The value v ij is the percentage of feature f in vertical v i and N is the total number of features. When one or more verticals make up a cluster, their vectors are grou ped to represent the cluster. In order to figure out the two closest clusters which should be merged together, a measure of dissimilarity between vertical clusters is required. The metric to evaluate the distance between two clusters of vertica ls in our proposed framework is defined as:
Distance ( A , B )= max where the vector v x and v y is a vertical of the cluster A and B respectively, and v xj and v yj is a feature of vector v x and v y . The scalar  X  j is defined as: The output of most hierarchical clustering algorithms is a nested hierarchy of graphs which can be cut at a desired dissimilarity level and form a partition (clustering) [12]. As there is no versatile approach to find the right number of clusters till now, many heuristic strategies are used in the clustering procedure. Here, we employ a threshold  X  to terminate the process of clustering to get a desired hierarchical structure, and the  X  is defined as: where v x and v y represent a pair of different verticals in the original dataset, and M is the total number of verticals, so  X  corresponds to the average distance between all pairs of verticals. Consequently, the procedure of finding the hier-archy structure of verticals automatically is shown in Algorithm 1. Given the flat non-hierarchy dataset, the algorithm can iteratively build a hierarchy tree from bottom to up by merging the two closest clusters into a larger one in each iteration, and the process continues until the shortest distance of each cluster is greater than the threshold  X  .
 All plain texts of each web page are ext racted during the preprocessing stage. The title, description and keywords fields of &lt; META &gt; tag and the content within &lt; TITLE &gt; tag provide useful information for classification and are also extracted. After that, all these texts are tokenized into words, and then are lemmatized.

Under the context of vertical classification, all words in one page function equally when selecting discriminative f eatures, because the common assumption for general web page classifications that the feature with high term frequency in one page is considered as more important than others [7] is not suitable. For instance, one of the most discriminative feature  X  X SBN X  of  X  X ooks X  vertical appears only one or two times, however the word  X  X irl X , which is one word of a special book title and is not important, may occur more than forty times in Algorithm 1. An Agglomerative Hierarchical Clustering Process to Find the Hierarchical Structure o f Verticals Automatically Amazon.com . And the general representati on TF-IDF used by common web page classifiers does not perform well. So in our framework, we regard the features in one page with diverse occurrence as equivalent and a binary vector corresponding to the presence / absence of each feature is adopted. 3.2 Learn Hierarchical Classifier In this framework, all verticals will be organized as a vertical-relational tree and each vertical node can has at most one parent. Besides, all pages can only be assigned to no more than one leaf vertical. We propose a top-down level-based approach to learn hierarchical classifiers. A classifier is built for each non-leaf node and different classifiers use different document representations. A new web page is classified by searching the vertical-relational tree top-down from root to leaves greedily. The procedure of learning the hierarchical classifier is that: (1) for each non-leaf vertical-relational node (initially the root), select discrimi-(2) learn a classifier to distinguish sub-level nodes. (3) continue the learning process on each child non-leaf node, until a leaf vertical 4.1 Feature Selection A web page is generally represented by a feature vector with thousands of unique terms. However, the high dimensionality could be problematic and expensive for computation. Many feature selection methods can reduce the dimensionality without decreasing and even increas ing the accuracy of classification. Odds Ratio (OR): The idea of OR is that the distribution of features on relevant documents is different from the distribution of features on uncorrelated documents. The features with high OR a re considered more important than others. Mladenic et al. [13] showed that the OR method performed best over all others in their experiments.
 Information Gain (IG): The IG of a feature is a metric of the expected reduction in entropy when the training data is divided into small sets owing to the feature. The entropy of a training dataset indicates the impurity of it. And, the higher IG of one feature, the more dis criminative. Yang et al. [14] declared the IG was the most competitive approach in their work. 4.2 Web Page Classifiers After more discriminative features hav e been selected, many machine learning methods can be used to build a function mapping between documents and cat-egories over a set of training data [7].
 Naive Bayes (NB): NB is one of most popular classifiers in text classification, due to its easiness to conduct and surprisingly effective [4]. NB is constructed by maximizing the posterior probability P ( c i | d ) of category c i given a document d . It assumes that all features are independent of each other given a category, which makes the posterior probability easy to be estimated.
 Hidden Naive Bayes (HNB): Although NB is simple and efficient, its hy-pothesis of feature conditional independence is always violated for real world data. In order to improve NB classifiers, the HNB [9] introduces a hidden parent for each node to combine the influences from all others. Jiang et al. [9] demon-strated that HNB significantly outperformed NB and other variants of NB, such as naive Bayes tree (NBTree), tree-augm ented naive Bayes (TAN), and averaged one-dependence estimators (AODE).
 Support Vector Machine (SVM): SVM is used for a wide rang of classifica-tion problems and often as the benchmark to evaluate other classifiers, because of its outstanding performance and generalization capability. SVM is originally designed for binary classification, as a linear SVM seeks a maximal margin hy-perplane to separate a set of positive examples from a set of negative ones. In order to extend the SVM (with binary form) to multi-class case, many algorithms have been developed. A very efficient method designed by Platt [15] employs the sequential minimal optimization (SMO) to break the large quadratic program-ming problem into a series of smaller ones, which makes it applicable for the dataset with a large amount of features. In this section, we want to know whether the performance of the proposed frame-work described in Section 3 is better than the corresponding flat ones under the context of vertical classific ation. Among the six combinations of two feature selec-tion methods ( i.e. , IG and OR) and three famous classifiers ( i.e. , HNB, NB and SVM), we are interested in which one performs best based on the framework. The dataset used in our experiments is partially derived from the structured data extraction method by [3]. The oth er is collected from some popular web-sites, and involves 3 semantically similar verticals, including Cellphones, Tablets, Mp3players . In the dataset, every v ertical has 6 websites with about 100 pages for each site. Four of them will be rando mly selected from each vertical as the training dataset, and the rest is regarded as the testing dataset, which is unseen for the learned classifiers. In addition, each page can only belong to one vertical. 5.1 Flat vs Hierarchical Classifiers On All Verticals: The most important experiment should be done is to inves-tigate the effectiveness of the hierarchical vertical classifiers with respect to flat ones. In order to achieve a fair comparis on, all datasets used to train and eval-uate the learned hierarchic al classifiers are the same as the flat ones. Besides, all experiments are conducted more than 20 times for randomly selecting the training websites and getting a more reliable results.

Figure 2 shows that all hierarchical classifiers, built on the basis of the pro-posed framework, outperform the flat classifiers significantly. Hierarchical HNB, NB, SVM get about 11%, 9.5%, 2.5% (11%, 9.5%, 3%) improvement respectively over flat non-hierarchy classifiers with IG feature selection method in precision (in F -score ). As to OR method, hierarchical HNB, NB and SVM classifiers achieve a small improvement over the flat ones respectively in both precision and F -score . Inspiringly, only 80  X  100 features are needed for hierarchical classi-fiers to obtain the best performance, no ma tter which feature selection strategy is adopted. On the contrary, the flat non-hierarchy classifiers demand more than 4  X  5 times numbers of features to achieve approximate perfo rmance. Therefore, another conclusion is drawn that the hierarchical classifiers can obtain excellent results by using a quarter or more less number of features than flat non-hierarchy classifiers, which can effectively reduce the training time and favor a memory-efficient implementation.
 On Four Similar Verticals: In order to check the effectiveness of the pro-posed framework for dealing with semantically similar verticals (e.g., Cameras, Cellphones, Tablets, Mp3players ), we compare the F -score of these verticals to find out the difference between hierarchical and flat classifiers. The results are illustrated by Figure 3.

The results show that these hierarchical classifiers outperform the correspond-ing flat ones and attain satisfactory results. Among the six combinations (of two feature selection methods and three exce llent classifiers), the biggest improve-ment is gained by the HNB classifier using IG to select features, which almost increase 40%. The second is the NB classifie r adopted IG as the feature selection method, which is about 25%. Besides, the hierarchical classifiers also outperform the corresponding flat non-hierarchy ones with the OR method, although the flat classifiers could achieve acceptable results. 5.2 Comparison between Hierarchical Classifiers In Section 5.1 we can find great advantages o f hierarchical classifiers comparing to the corresponding flat ones. In this section, we want to know which one perform best among the six combinations of the two feature selection strategies and the three classifiers used in the proposed framework for the vertical classification. From Figure 4 we can observe that the best result is won by the SVM with OR in both precision and F -score , when the number of features reaches 120. The SVM with IG gains an approximately good result when the feature number is 100. Consequently, SVM is the best classifier among the three hierarchical classifiers. As regards to HNB and NB, the former performs better than the latter, especially when using OR to sel ect features. Besides, the OR method performs better than the IG method across the three classification algorithms, which is similar to the above conclusions. This phenomenon may be caused by the fact that every vertical has some unique attributes to be distinguished from others and OR method can figure out them by only considering the present features, while the IG approach emphasizes both the attributes belonging to the special vertical and the ones not in part of it when choosing representative features. This paper describes a general framewor k to build hierarchical classifiers for vertical classification automatically from flat datasets, which contains two parts. First, exploit the intrinsic hierarchy structure of relationships among verticals automatically by using an agglomerative hierarchical clustering process. Second, learn a classifier for every non-leaf nod e in the hierarchy tree and then form a hierarchical classifier. A series of experiments demonstrate that the hierarchical classifiers built on the basis of the proposed framework outperforms the flat non-hierarchy ones, and the internal hierarch ical structure of diff erent verticals can be used to improve the effectiveness and efficiency.

In this paper we have not investigated th e effectiveness of the framework with a large number of verticals and has a deep -level hierarchy structure in nature. We will expand our framework to this kind of dataset in the future. We also have not known whether the hierarchy structure learned by our framework is better or poorer than the one built by human beings manually due to the lack of this kind of dataset currently, and we will do this in the following work. Acknowledgment. This work is funded by 973 project (Grant No. 2013CB329605), NSFC (Grant Nos. 60873237 and 61003168), Natural Science Foundation of Beijing (Grant No. 4092037) , Outstanding Young Teacher Foun-dation and Basic Research Foundation of Beijing Institute of Technology, and partially supported by Beijing Key Discipline Program.

