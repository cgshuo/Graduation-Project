 1. Introduction
During the last decade, electronic bibliographic tools have been accessed by an increasing amount of users, many of whom might be classified as novices. During this same period, the cost of scientific journals has increased exponentially, forcing many university libraries to reduce the number of journal titles or substitute paper versions with those that can be accessed electronically, thus increasing the demand for
Internet-based information access. This trend is also due to the ever-increasing availability of various and general reference information services (e.g., the Oxford English Dictionary, Encyclopaedia Britannica, or various statistics covering national or other themes) together with several bibliographic databases (e.g.,
Inspec, Biosis, Springer LINK or the ISI Web of Science). In the latter case, in order to provide effective access, various modern indexing and abstracting services (such as MEDLINE, PsychINFO, Chemical abstracts) make use of some sort of human subject analysis and indexing (Milstead, 1992), often invoking a controlled vocabulary (e.g., Library of Congress Subject Headings).

Manual indexing (Anderson &amp; P erez-Carballo, 2001a) usually relies on the use of controlled vocabu-laries in order to achieve greater consistency and to improve manual indexing quality (Svenonious, 1986).
The advantage of these authority lists is that they prescribe a uniform and invariable choice of indexing descriptors and thus help normalize orthographic variations (e.g.,  X  X  X atabase X  X  or  X  X  X ata base X  X ), lexical variants (e.g.,  X  X  X nalyzing X  X ,  X  X  X nalysis X  X ) or examine equivalent terms that are synonymous in meaning. The level of generality may be represented by hierarchical relationships (e.g.,  X  X  X ord X  X  is a  X  X  X ar X  X ), and related-term relationships (e.g.,  X  X  X ee also X  X ). However, while controlled vocabularies may increase consistency among indexers, it is more important to increase indexer X  X equester consistency, thus leading to an increase in the chance that searchers will be able to locate the information they require (Cooper, 1969).
Working with a relatively large French collection of bibliographic records, this paper compares the retrieval performance of an automatic text-word indexing with an indexing strategy based on manually assigned controlled descriptors. The remainder of this paper is organized as follows. Section 1.1 presents related work describing the usefulness of manual indexing terms for searchers while Section 1.2 reviews previous work that has compared automatic and manual indexing performance. Section 2 describes the
Amaryllis corpus and its thesaurus, along with the various search models used in this paper. Section 3 presents our evaluation methodology and compares the retrieval effectiveness of various approaches used to index and retrieve French documents. Finally, a conclusion provides an account of our study X  X  main findings. 1.1. Manually assigned descriptors and searchers
In order to verify whether or not manual indexing might be of use to searchers, various studies have analyzed the search process. When comparing search performance based on either controlled vocabularies or full texts (able to match terms in the text of the article), Tenopir (1985) found that Boolean full-text searches resulted in better recall yet less precision when compared to controlled-vocabulary searches (31 queries on the Harvard Business Review Online database containing years 1976 X 1984, and a controlled list containing around 3000 terms). Thus the use of controlled vocabularies seems to improve precision (Svenonious, 1986; Tenopir, 1985).
 In similar research using a subset of the MEDLINE collection known as OHSUMED, Hersh, Buckley,
Leone, and Hickam (1994) investigated search performance differences. When submitting queries, searchers either used terms only found in topic descriptions and in title and abstract sections of retrieved scientific papers, or they also considered the MeSH (Medical Subject Headings, a list containing around 19,000 main headings). Overall, performance differences were small and statistically insignificant, illustrating that MeSH descriptors did not offer any real advantages. However, when comparing experienced librarians with physicians, the former demonstrated a statistically significant advantage in recall, thus suggesting that with trained intermediaries, assigned descriptors manually could be worthwhile. This finding partially contra-dicts Tenopir X  X  conclusion. Blair (2002) also indicated that experienced searchers are important components in successful searches on very large systems.

In another study, Spink and Saracevic (1997) showed that using terms extracted from controlled vocabularies or other thesauri did not seem to be the most productive sources of terms when the goal was to increase the retrieval effectiveness, as compared to cases in which terms were provided by users X  search statements or by terms extracted during a relevance feedback process.

Moreover, manual indexing may also serve other purposes. For example, using the OHSUMED test-collection, French, Powell, Gey, and Perelman (2002) showed that when searching in distributed collec-tions and by selecting an appropriate number of MeSH terms (between one and three) from retrieved documents and by using these terms in an augment query, the selection procedure effectiveness could be improved. 1.2. Manual and automatic indexing
Even though assigning descriptors manually does produce mixed results, they can be useful when indexing a document. First, we must recognize that manual indexing is a current practice for various information services (Milstead, 1992). However, few studies have been conducted in order to analyze and compare the relative retrieval effectiveness of either manual or automatic indexing approaches used within various information retrieval models.

In an early work, Cleverdon (1967) reported that in the Cranfield II test context (1400 documents, 221 queries), single-word indexing was more effective than using terms extracted from a controlled vocabulary, however both indexing schemes were done by human beings. In order to analyze the performance of automatic indexing approaches, Salton (1972) compared a Boolean search system (MEDLARS) with a ranked output produced by a vector space model (SMART). Based on 45 0documents (a rather small number compared to current evaluation standards), this study showed that an automatic indexing pro-cedure was capable of retrieval performances comparable to manual indexing.

Rajashekar and Croft (1995) used the INSPEC test collection (12,684 documents, 84 queries) to evaluate retrieval effectiveness when combining various query formulations and different document representations.
In this case, the authors examined an automatic indexing procedure based on the articles X  title and abstract sections, manually assigning terms extracted from the title and abstract, and a third document represen-tation based on manually assigning terms extracted from a controlled vocabulary. This study showed that automatic indexing based on the articles X  title and the abstract performed better than any other single indexing schemes. While the controlled vocabulary terms by themselves were not effective representations, their presence as an additional source of evidence on document content might improve retrieval perfor-mance. More generally, combining multiple query formulations and/or multiple document representations usually improves the mean average precision.

The objective of this current paper is to enlarge upon these previous investigations through examining the performance achieved by 1 0different retrieval strategies and comparing several document indexing schemes. Moreover, in contrast to some earlier studies, an effort was made to place the user at the center of information retrieval evaluation, with the relevance assessments on the Amaryllis corpus being made by the same person who submitted her/his information needs (Saracevic, Kantor, Chamis, &amp; Trivison, 1988).
Finally, our evaluation was based on a large test collection of bibliographic material written in French and covering various scientific disciplines. As did Blair (2002), we too believe that retrieving information from a small database does not satisfactorily reveal all the underlying search problems faced when handling large document collection. 2. Amaryllis corpus and search models The Amaryllis corpus was created at INIST (INstitut de I X  X nformation Scientifique et Technique at
Vandoeuvre, France, a public organization of around 34 0persons), having as its mission to collect, process and distribute the results of technological and scientific research. INIST mainly provides electronic access to two bibliographic databases named FRANCIS (for arts and humanities) and PASCAL (for science, technology and medicine). Overall, the INIST collections include around 26,000 scientific journals (plus various proceedings and Ph.D. theses). The PASCAL database contains around 14.7 million records (76% of them written in English, 9% in French, and as well as other European languages), while the FRANCIS contains 2.5 million records (41% in English, 31% in French, plus some other European languages).
This section describes the overall background for our study and is organized as follows: Section 2.1 contains an overview of the Amaryllis test collection made available during the CLEF (Cross-Language Evaluation Forum) campaign. Section 2.2 describes how we constructed a stopword list and a stemmer for the French language. Finally, Section 2.3 describes the various vector space term weighting schemes used in this paper, together with the Okapi probabilistic model. 2.1. Test collection overview
The corpus used in this paper is in fact a subset of the CLEF 2002 test suite (Peters, Braschler, Gonzalo, &amp; Kluck, 2003) called the Amaryllis corpus and is composed of 148,688 scientific bibliographic records written in French. These records consist of a title (delimited by the tag &lt; have a title field due to the fact that only titles written in French are stored in this test-collection. Each article contains a set of manually assigned descriptors delimited by the tag &lt; responding descriptors written in English are delimited by the tag &lt; assigned by documentary engineers at INIST, who have a good knowledge of the domain to which the indexed article belongs. These assigned descriptors are occurrences or variants of terms extracted from the
INIST thesaurus. When the most appropriate terms cannot be found in the INIST thesaurus, the indexer may freely assign them (although this rarely happens). Table 1 contains examples of these documents, whose general structure corresponds to the examples found in other online bibliographic services.
INIST created and maintained a thesaurus that was made up available during the CLEF evaluation campaign (a part of which can be found in Table 2). It contains 173,946 entries delimited by the tags &lt; &lt; TERMFR TERMFR &gt;) and its translation into English (marked by the tag &lt; without any real interest (e.g.,  X  X 1910 X 1920 X  X  in Table 2). In 45,300 entries, the English translation is identical to the French expression (e.g.,  X  X  X quitaine X  X  in Table 2). Moreover there are 28,387 multiple entries for a given term. For example, in Table 2 are there are two entries for the expression  X  X  X ureau poste, X  X  translated as  X  X  X ost offices X  X  or  X  X  X ost office X  X . By removing non-pertinent and multiple entries, we obtain a set of 145,523 (173,946-36-28,387) unique entries.

In addition to the English translation(s) for all entries, the INIST thesaurus contains three different term relationships, namely 26,154 SYNOFRE (French synonym for 18.0% of the entries), 28,801 AUTOP (automatic expansion, available for 19.8% of the entries) and 1937 VAUSSI ( X  X  X ee also X  X , given for 1.3% of the entries). The AUTOP association is used to add automatically term(s). An example would be the term  X  X  X quitaine X  X  for which the term  X  X  X rance X  X  is automatically added. Due to a relatively small number of links between terms, this thesaurus can be viewed as a specialized bilingual dictionary or as an authority list having a rather flat structure, as compared to the MeSH thesaurus which included more links for each of its entries ( http://www.nlm.nih.gov/mesh/meshhome.html ). However, before providing access to this tool, INIST has removed some of the relationships included in their original thesaurus.

Moreover, the available INIST thesaurus does not correspond to a standard ISO thesaurus that con-forms to ISO recommendations (ISO, 1986; ISO, 1985) regarding contents, display and methods of con-struction and maintenance (entries form, abbreviations, vocabulary control, indexing terms, compound terms, basic relationships). For example, the INIST thesaurus contains orphan terms, descriptors that are not related to any other descriptors (excepted to their English translations). As described previously, the
INIST thesaurus is based mainly on the translation relationship while the synonymy, the hierarchy (broader term, narrower term) or the association (related term) relationships play only a secondary role.
Each year, an ad hoc committee decides to include new terms (together with their translations and rela-tionships with other terms) in the INIST thesaurus.

As with the TREC (Text REtrieval Conference) model, each topic was divided into three sections, namely a brief title, a one-sentence description and a narrative part identifying concepts related to the main request topic (see Table 3). Within this test collection are 25 queries written by INIST librarians, specialists in the various topic domains. Relevance assessments corresponding to each request were also made at
INIST by the same person who wrote the topic statement. Of course, we would prefer having more queries in order to ground our conclusions on more solid foundations.

In order to provide an overview of the Amaryllis test collection, in Table 4 we report certain statistics on the main characteristics of the bibliographic records and requests. As this table indicates, the test collection contains 413,262 unique indexing terms and regarding the number of relevant items per request, it shows that the mean (80.72) is greater than the median (67) and that the standard deviation is relatively large (46.0675), thus indicating that this test collection varies greatly relative to the number of pertinent articles per query.

To better depict the size of the various sections contained in a typical Amaryllis document, we included various statistics regarding the number of indexing terms compared to the whole document (under the label  X  X  X ll X  X ), or those appearing in the manually assigned sections (under the label  X  X  the title and abstract sections (under the label  X  X  TI TI of 104.617 indexing terms, while its title and abstract contains, in mean, 73.413 words. Finally, the man-ually assigned terms (both in French and English) number on average 31.205 terms or around 15 per language.
 2.2. Stopword lists and stemming procedures for the French language
In order to index and retrieve French documents, we needed to define a general stopword list for this language, made up of many words considered of no use during retrieval, but very frequently found in document content. These stopword lists were developed for two main reasons: first, we hoped that each query and a document match would be based only on pertinent indexing terms. Thus, retrieving a docu-ment just because it contains words like  X  X  X e X  X ,  X  X  X our X  X  and  X  X  X he X  X  (or their French equivalents) in the corresponding request does not constitute an intelligent search strategy. These function words represent noise, and may actually reduce retrieval performance because they do not discriminate between relevant and non-relevant articles. Second, by using them we would reduce the size of the inverted file, hopefully within the range of 30 X 50%. The stopword list used for this experiment can be found at http://www. unine.ch/info/clef/ and represents an enhanced version of one that we previously developed for this lan-guage (Savoy, 1999).

After removing high frequency words, an indexing procedure then applies the stemming algorithm, attempting to conflate word variants into the same stem or root. In developing this procedure for the
French language, our first attempt (Savoy, 1999) removed only inflectional suffixes such that singular and plural word forms or feminine and masculine forms would conflate to the same root. More sophisticated schemes have already been proposed for the English language for the removal of derivational suffixes (e.g.,  X  X -ize X  X ,  X  X -ably X  X ,  X  X -ship X  X ), such as the stemmer developed by Lovins (1968) based on a list of over 260 suffixes or Porter X  X  stemmer (1980) that looks for about 60 suffixes. In order to develop a French stemmer able to remove certain derivational suffixes, our solution (available at http://www.unine.ch/info/clef/ ) will consider a limited list of 26 derivational suffixes. Thus compared, to the two English stemmers cited pre-viously, our approach can be qualified as a  X  X  X ight X  X  stemming procedure, given that the French language involves a more complex morphology than does the English language (Savoy, 1999; Sproat, 1992). 2.3. Indexing and searching strategies
In order to obtain a broader view of the relative merit of different retrieval models, and also to compare the retrieval performance of manual and automatic indexing procedures based on various environments, we have implemented 1 0search models. The notation for these retrieval models and their corresponding weighting formulas are found in Appendix A. In the simplest case, we adopted a binary indexing scheme within which each document (or request) is represented by a set of keywords, without any weights. To measure the similarity between documents and requests, we count the number of common terms, computed according to the inner product (retrieval model denoted  X  X  X oc  X  bnn, query  X  bnn X  X  or  X  X  X nn X  X nn X  X ). For document and query indexing, however, binary logical restrictions are often too limiting. In order to weight the presence of each indexing term in a document surrogate (or in a query), we may take term occurrence frequency into account (denoted tf) thus allowing for better term distinction and increased indexing flex-ibility (retrieval model notation:  X  X  X oc  X  nnn, query  X  nnn X  X  or  X  X  X nn X  X nn X  X ).

Those terms, however, that do occur in the collection very frequently are not considered very helpful in distinguishing between relevant and non-relevant items. Thus we might count their frequency in the col-lection (denoted df), or more precisely the inverse document frequency (denoted by idf  X  ln  X  n = df  X  , with n indicating the number of documents in the collection), thus assigning more weight to sparse words and less weight to more frequent ones. Moreover, a cosine normalization could prove beneficial and each indexing weight would vary within the range of 0 X 1 (weighting scheme  X  X  X oc  X  ntc, query  X  ntc X  X ).

Other variants could also be created, especially if we consider the occurrence of a given term in a document as a rare event. Thus, it may be a good practice to give more importance to the first occurrence of this word as compared to any successive or repeating occurrences. Therefore, the term frequency com-ponent would be computed as 0.5 + 0.5  X  [tf/max tf in a document] (the term weighting scheme is denoted  X  X  X oc  X  atn X  X ). Moreover, we should consider that a term X  X  presence in a shorter document provides stronger evidence than it does in a longer document. To account for this, we integrate document length within the weighting scheme, leading to more complex formulae; for example the IR model denoted by  X  X  X oc  X  Lnu X  X  Finally, we also conducted various experiments using the Okapi probabilistic model (Robertson, Walker, &amp;
Beaulieu, 2000). 3. Evaluation
This section presents an evaluation of our experiments, and is organized as follows: Section 3.1 describes our evaluation methodology and compares the relative performance of 1 0retrieval models that access the
French corpus in response to short, medium-size or long queries. Rather than all sections included in each document, Section 3.2 evaluates the mean average precision obtained from an automatic indexing proce-dure based only on the title and abstract sections of scientific articles and also when using only the doc-ument representation based on terms extracted from a controlled vocabulary list developed by human beings. Section 3.3 investigates enhancements attainable by incorporating a pseudo-relevance feedback (or blind query expansion) procedure. Finally, by comparing manual and automatic indexing procedures,
Section 3.4 evaluates the best retrieval model using the expected search length (Cooper, 1968) in order to better assess users X  efforts, depending on whether they are more interested in precision or in recall. 3.1. Evaluation of various search models
As a retrieval effectiveness indicator, we adopted the non-interpolated average precision (computed on the basis of 1000 retrieved items per request), thus allowing both precision and recall to use a single number, an evaluation procedure applied during the TREC or CLEF evaluation campaign (Braschler &amp; Peters, 2002; Voorhees &amp; Harman, 2000). This mean average precision is computed based on the following con-sideration. For a given query q , we may compute the precision achieved after retrieving r documents, denoted by Prec r  X  q  X  , as follows: in which D r  X  q  X  is the set of retrieved documents for query q containing the first r records, and D rel set of pertinent items included in this r first retrieved documents. To define the non-interpolated average precision, the system computes this precision value after each relevant document found in the answer list, and based on this set of precision values, an average is computed for the query q . Of course, instead of using a single query q , the system performance is computed according to a set of queries (25 in our case) and, we compute the mean over all queries average precision to obtain the mean average precision or non-inter-polated average precision.

To determine whether or not a given search strategy is better than another, a decision rule is required. To achieve this, we could have applied statistical inference methods such as Wilcoxon X  X  signed rank test or the
Sign test (Hull, 1993; Salton &amp; McGill, 1983, Section 5.2) or the hypothesis test based on bootstrap methodology (Savoy, 1997). In this paper we will base our statistical validation on the bootstrap approach because this methodology does not require that the underlying distribution of the observed data be a normal one. As stated in Salton and McGill (1983) and demonstrated in Savoy (1997), the mean average precision distribution is not always a normal one and this fact may invalidate the underlying statistical test.
In our statistical testing, the null hypothesis H 0 states that both retrieval schemes produce similar mean average precision. Such a null hypothesis will be accepted if two retrieval schemes return statistically similar means, and will otherwise be rejected. Thus, as shown in the tables appearing in this paper, we have underlined statistically significant differences based on a two-sided non-parametric bootstrap test, and based on those having a significance level fixed at 5%. However, a decision to accept H equivalent of the null hypothesis H 0 being true, rather it represents the fact that  X  X  X  be false, X  X  resulting in insufficient evidence against H 0
Moreover, in the current study, we have a relatively small number of observations (25 queries). Thus, even when faced with two retrieval schemes having different retrieval performances (H tistical test cannot detect this difference in retrieval effectiveness, due to the sample size being too small.
Our evaluation results are based on queries using only the Title (T), the Title and Descriptive (TD) sections or the Title, Descriptive, and Narrative sections (TDN), as reported in Table 5. In these evalua-tions, we considered all sections of the document: title, abstract, and controlled vocabulary descriptors assigned by INIST X  X  indexers. The resulting performance can thus be viewed as the best retrieval effec-tiveness, one that can be obtained with this corpus with respect to the given retrieval model.
We can clearly see from the mean average precision depicted in Table 5 that the Okapi probabilistic model is in first position. It always produces the best mean average precision and this performance is used as the baseline from which the percentage of change is computed. In second position is the vector-space model  X  X  X oc  X  Lnu, query  X  ltc X  X  and in the third  X  X  X oc  X  atn, query  X  ntc X  X . The traditional tf-idf weighting scheme ( X  X  X oc  X  ntc, query  X  ntc X  X ) does not exhibit very satisfactory results, and the simple term-frequency weighting scheme ( X  X  X oc  X  nnn, query  X  nnn X  X ) or the simple coordinate match ( X  X  X oc  X  bnn, query  X  bnn X  X ) show poor retrieval performance. Using the Okapi as a baseline, this table also indicates how all differences in mean average precision are usually statistically significant (percentage of change underlined in Table 5 and computed according to the bootstrap statistical testing method, with a significance level of 5%).
On the other hand, when the query contained more search terms, the resulting retrieval performance increases. For example, using the Title and Descriptive (TD) sections (with a mean of 15.6 search terms), improvement is +24.6% compared to short queries (with a mean of 3.7 search terms) for the Okapi model (from 37.27 to 46.44), or of +45.3% when all sections (TDN) of the topic description (37.27 vs. 54.17) are taken into account. When computing the mean improvement over our 1 0search models, we found an average retrieval enhancement of +26.7% compared to short queries (T) with TD requests, or +53.2% when compared to short requests (T) with TDN query formulation.
 3.2. Evaluation of manual vs. automatic indexing The Amaryllis corpus does however show another interesting feature. The sections &lt; used to delimited the title and the abstract respectively of each French scientific article written by the the INIST thesaurus.

Based on the Amaryllis corpus we are therefore able to evaluate whether manually assigned descriptors resulted in better retrieval performance as compared to the scheme based on automatic indexing. To tackle this question, we evaluated the Amaryllis collection using all sections (denoted  X  X  X ll X  X  in Tables 6 and 7), using only the manually assigned terms (performance listed under the label  X  X  and abstracts from bibliographic records (under the label  X  X 
Based on short (Table 6) or medium-size (Table 7) queries, the mean average precision for the combined indexing strategy is better than both the single manual or automatic indexing schemes and these differences are usually statistically significant (difference computed with a significance level of 5% and underlined in these tables). A single exception to this rule is obtained when using the simple coordinate match (model denoted  X  X  X oc  X  bnn, query  X  bnn X  X ) for which the manual indexing scheme performs better than the combined approach (22.71 vs. 21.03 in Table 6). However, this difference is not statistically significant. When comparing retrieval effectiveness for manual (label  X  X  indexing schemes (as shown in the last column of Tables 6 and 7), we can see that for all search models, manually assigned descriptors result in better mean average precision than do automatic indexing proce-dures. However, these differences are usually not statistically significant (except for the four underlined observations). This statistical finding seems a priori counter-intuitive. For example in Table 6, the mean average precision for the Okapi model is 29.56 when using manually assigned descriptors and only 23.76 for an indexing process based on the documents X  title and abstract sections. The difference between these two runs is 19.7% (last column in Table 6) and thus the manual approach is favored. However, a query-by-query analysis reveals that the manual indexing run improved retrieval effectiveness for 15 queries out of a total of 25. For 1 0requests, however, the automatic indexing procedure depicted a better retrieval performance.
Thus, in order to find a statistically significant difference between two retrieval schemes, the performance difference between individual requests should favor one given retrieval model for a large number of queries and the difference must be significant (e.g., an improvement of 0.1% cannot be viewed as significant).
Users usually enter very short queries, however, and are more interested in the precision revealed by the
In order to obtain a more precise picture within this context, in Table 8 we reported precision results for 5, 1 0or 2 0documents retrieved using the Okapi probabilistic model. This table shows that the manual indexing scheme (labeled  X  X  MC MC &amp; KW KW  X  X ) obviously results in better performance when compared to the automatic indexing approach (labeled  X  X  TI TI &amp; AB AB documents. These differences are however not statistically significant (bootstrap testing with a significance level of 5%). We achieved the best performance from using both indexing approaches (performance de-picted under the label  X  X  X ll X  X ), resulting in differences that are usually statistically significant. Considering the expense of manual indexing, Table 8 shows that the enhancement is disappointing.
When compared to the precision after 1 0documents, manual indexing shows a precision of 54.8% as compared to 52.8% for the automatic approach. Strictly speaking, this comparison is correct. However, if an institution such as INIST decides to manually index each scientific article, it might also adopt an indexing strategy that takes into account both manually assigned descriptors and automatic indexing procedures based on the articles X  title and abstract sections. Thus, comparing the performance under the  X  X  X ll X  X  column in Table 8 with the precision shown under the  X  X  sonable approach. In this case, including a manual indexing procedure improves precision after 1 0docu-ments from 52.8% to 66.0%. Thus, in mean, we obtain 1.3 more relevant documents after 10 retrieved items when including a manually based indexing procedure. Does this improvement matter? From Lantz X  X  study (1981), we knew that when an IR system provides 6.2 relevant citations, only 3.5 documents would finally be consulted. However, this mean value varies from one discipline to another; for every 100 relevant document retrieved, medical scientists tend to read more articles (around 42) while engineers consult only a small number of papers (8). Biological, physical and social scientists form an indistinguishable group, tending to read 27 articles on average. Since this study was conducted in London, we may consider that cultural differences could invalidate, or at least attenuate, this finding. Nonetheless, manual indexing can be viewed as more important for recall-oriented users such as lawyers or medical researchers than for preci-sion-oriented users.

We must recognize, however, that, to the best of our knowledge, little research has been done regarding the impact of additional relevant documents and to what extent they will meet user information needs. The interactive track at TREC (Hersh &amp; Over, 2001; Hersh, 2003; Over, 2001) presented an interesting set of studies on various aspects of human-machine interactions, and also some more specific experiments per-taining to cross-lingual information retrieval systems were presented in (Gonzalo &amp; Oard, 2003).
Manual indexing does, however, include other advantages. For example, the Westlaw company (an online legal research service) manually indexes various court decisions. This improves online searching and also provides their users with concise statements covering entire cases, clarifying them or linking them to other particular and pertinent cases that also implicate or apply a given legal concept. Moreover, the manually versions of various legal documents may also be published, not only for manual search purposes but also to provide information for various digests and annotated legal data services. 3.3. Blind query expansion
It has been observed that pseudo-relevance feedback (blind-query expansion) can be a useful technique for enhancing retrieval effectiveness through automatically developing enhanced query formulations. In this study, we adopted Rocchio X  X  approach (Buckley et al., 1996) whereby the system was allowed to add m terms extracted from the k highest ranked documents retrieved with the original query. The new request was derived from the following formula: in which Q 0 denotes the new query built for the previous query Q , and w attached to the term T j in the document D i . In our evaluation, we fixed a  X  0 : 75, b  X  0 : 75.
We used the Okapi probabilistic model in this evaluation and enlarged the query from 1 0to 4 0terms taken from the 5 or 1 0best-ranked articles. The results depicted in Table 9 indicate that for the Amaryllis corpus the optimal parameter setting seems to be around 3 0terms and these values are very similar to those found by other studies done during the last CLEF evaluation campaign (Peters et al., 2003), based on other languages.

Using the bootstrap testing approach, Table 9 shows that differences in mean average precision are always statistically significant when using the combined or manual indexing strategies. When the system uses only the title and the abstract sections of the bibliographic records, improvements in mean average precision are only significant for the best three query expansion parameter settings.

If we compute the precision after 5, 1 0or 2 0documents using the best query expansion setting, Table 1 0 shows how Rocchio X  X  blind query expansion improves precision compared to Table 8 which shows corpus indexing using all sections or when the indexing is limited to the articles X  title and abstract sections.
However, for manual indexing, even if the mean average precision increases from 29.56 to 33.33, the precision after 5 documents decreases from 59.2% (Table 8) to 58.4% (Table 10). Thus, even though query expansion usually improves the overall performance, in some cases it may actually reduce early precision. When using the performance resulting from indexing all documents sections as a baseline as shown in Table 1 0, differences in the precision after 5, 1 0or 2 0is around 2 0% and always statistically significant.
From this table we cannot detect statistically significant differences when comparing manually assigned where only a relatively few items are retrieved, in the next section we will investigate and more precisely evaluate user effort that is required to reach a given number (or percentage) of pertinent items. 3.4. Expected search length
As a retrieval effectiveness measure, Cooper (1968) suggested computing the expected search length, defined as the mean number of non-relevant items that users must scan in order to satisfy their information need. Thus this retrieval effectiveness approach really measures users X  efforts to discard non-relevant items.
Users X  needs may be viewed according to various types, such as  X  X  X nly one relevant document is wanted, X  X  or more generally  X  X  k pertinent articles are required. X  X  When applying this retrieval measure in our context, we have developed a retrieval model that provides users with a ranked list of retrieved items instead of a weak (or set oriented) ordering, as described in Cooper X  X  paper (1968). Our current situation is thus simpler.
When evaluating our best search model using the expected search length as reported in Tables 11 and 12, we did not compute the mean but rather the median and the third quartile. These latter two values are more robust location statistics and they are less influenced by outliners or extreme observations that may dramatically change a mean value (Savoy, 1997). For example, when indexing documents based on all sections and without considering a blind query expansion approach, the first relevant item for Query #3 can be found in position 99, while for 19 requests, the first retrieved item is pertinent. The resulting mean search length is therefore 4.24, and this performance value will be 0.33 if Query #3 is ignored. Clearly, 4.24 does on such considerations, we prefer adopting the median and the third quartile (depicted in parenthesis in Tables 11 and 12).

The median values shown in Table 11 indicate that for 50% of requests, the first retrieved item is always pertinent, no matter which indexing scheme is used. As a second indicator, figures in parenthesis show the third quartile, indicating the number of non-relevant records to scan for 75% of the queries. With the retrieved item is also relevant for 75% of the requests. Based on the title and the abstract sections (last column of Table 11), for 75% of the submitted requests, on average users must discard one non-relevant item before finding a relevant document. Similar findings hold when indexing the test collection using only the manually assigned descriptors (under the label  X  X  MC MC blind query expansions (bottom part of Table 11), the conclusions drawn are similar.

When users want to find a greater number of pertinent articles from a large collection, they must anticipate scanning a large (or huge (Blair, 2002)) number of retrieved items. For example, a lawyer pre-paring to defend a client wants to find around 75% of all relevant documents (Blair &amp; Maron, 1985). In order to apply the expected search length when faced with recall-oriented users, in Table 12 we reported the median (and third quartile) number of non-relevant items to be discarded in order to retrieve a given percentage of relevant articles, their percentages varying from 10% to 75%. Clearly however it is difficult to estimate a priori the number of relevant items a lawyer will need for a given legal precedent search.
Moreover, after retrieving a given amount of pertinent documents, users cannot distinguish between the situation where additional pertinent articles do exist and the situation where the desired documents no longer exist. In addition, for a given database, there is also a difference between the objective percentage of relevant documents already extracted and the corresponding subjective percentage estimated by the user.
As mentioned by Blair and Maron (1985, p. 293),  X  X  X his meant that, on average, STAIRS could be used to retrieve only 2 0percent of the relevant documents, whereas the lawyers using the system believed they were retrieving a much higher percentage (i.e., over 75 percent). X  X  In a similar vein, recent work done by Sormunen (2001) seems to indicate that low precision in high recall searching is unavoidable when using a
Boolean search system, but this precision level may potentially be improved by considering better best-match searching.

From data shown in Table 12, we can see that the best solution is always combining both manual and automatic indexing. In our experiment, in order to obtain 75% of the pertinent articles using a blind query expansion (bottom part of Table 12), the median number of non-relevant items to be discarded is 253 and the third quartile 502. Clearly, a recall-oriented search implies a much greater effort on the part of users, even with the best solution. This strategy is obviously better than other approaches requiring greater user effort (2243 articles must be discarded with manual indexing or 3499 with only automatic indexing). Aside from the extreme case, Table 12 demonstrates that manual indexing (labeled  X  X  better retrieval effectiveness than does automatic indexing (labeled  X  X  third of the relevant records. For lower percentages, automatic indexing tends to involve less user effort. Our experiment thus confirms that for conducting an exhaustive search, manual indexing is important.
As stated by Bates (1998, p. 1196)  X  X  X owever, as the  X  X  X ore X  X  terms will probably retrieve a relatively small percentage of the relevant records (certainly under the half, in most cases), they must nonetheless tolerate sifting through lots of irrelevant ones. It is the purpose of human indexing and classification to improve this situation, to pull more records into that core than would otherwise appear here ...  X  X  4. Conclusion
Using the relatively large Amaryllis corpus, we compared the retrieval effectiveness of human controlled vocabulary based indexing to that of an automatic indexing using 1 0retrieval models. Using the title and abstract sections in French bibliographic records, Tables 6 and 7 show how manually assigned descriptors that were mainly extracted from an authority list performed better than did an automatic indexing scheme.
However, the difference between these indexing schemes is usually not statistically significant. This main conclusion was confirmed using 1 0different indexing and search procedures.

The best mean average precision is, however, always obtained when both manually assigned descriptors and automatic text-word indexing schemes are combined (see Tables 6 and 7). As additional findings, this study has shown that:  X  the Okapi probabilistic model provides the best retrieval effectiveness when considering different query formulations (Table 5) or when indexing is based on different sections of bibliographic records (Tables 6 and 7);  X  results found using a French corpus corroborate with previous studies based on a collection of Web pages written in English (Savoy &amp; Picard, 2001). Thus, the French language does not reveal any specific difficulties when known indexing and search strategies are applied;  X  when users incorporate more search keywords, the resulting retrieval effectiveness increases between 24% and 45% (see Table 5). Thus, helping users find more search terms is a valid concern for man-machine interfaces;  X  applying a blind query expansion may enhance the retrieval effectiveness by about 10% (see Table 9);  X  when comparing manually assigned descriptors with an automatic indexing approach, based on title and abstract sections, retrieval performance favors the manual approach although a statistically significant difference between these two approaches cannot always be found (see Tables 6 and 7);  X  when comparing the precision obtained after 5, 1 0or 2 0retrieved documents (see Tables 8 or 1 0), there is no real difference between these two indexing strategies;  X  when an exhaustive search is required in order to retrieve 50 X 75% of relevant records, manually assigned descriptors prove to be an attractive approach, specially when used in combination with an automatic indexing scheme (see Table 12).

Of course, these findings still need to be confirmed through submitting more queries and other test collections containing manually assigned descriptors for each stored document. While our study demon-strates that combining both indexing approaches proves to be the best retrieval performance, Anderson and
P erez-Carballo (2001a) have shown that in the  X  X  X achine vs. human indexing X  X  debate there are various, often hidden, key variables. These are important in terms of exhaustive searches (humans tend to be more selective, able to make clearer distinctions between relevant and peripheral information while computers tend to take each term occurrence into account), specificity (machines tend to index document based on words as they appear in the text while humans tend to use more generic terminology), size of document units (human indexing focuses on larger units such as complete chapters or complete monographs while automatic indexing tends to work at the paragraph level).

From the point of view of bibliographic database developers, a single automatic indexing procedure that is clearly faster and cheaper might be adopted. Such an approach is capable of interesting but not optimal retrieval performance levels for those users who want high precision after retrieving only a few documents (see Tables 8, 1 0and 11). In addition to this, and depending on the underlying costs of human indexing and clientele needs, our study has provided a general view of retrieval effectiveness concerning manual and/or automatic indexing strategies for those users who require more relevant documents or who need to conduct exhaustive searches.

Finally, it could prove worthwhile to stop viewing and treating every document as equally important. In fact, the  X  X 80-20 rule X  X  may also apply in large document collections or IR databases where around 20% of articles will provide the expected answer to 80% of needs. Along these same lines, Anderson and P erez-
Carballo (2001b) suggested developing methods that could predict which documents might be more important, and for those documents a human analysis could be applied.
 Acknowledgements
This research was supported in part by the SNSF (Swiss National Science Foundation) under grants 21-58 813.99 and 21-66 742.01. Appendix A. Weighting schemes
To assign an indexing weight w ij that reflects the importance of each single-term T might use various approaches such as shown in Table 13, where n indicates the number of documents in the collection, t the number of indexing terms, tf ij the term occurrence frequency of term T the number of documents in which the term T j appears, idf idf j  X  ln  X  n = df j  X  ), the document length (the number of unique indexing terms) of D indicates the number of indexing terms of D i , and for avdl, b , k experiments, these constants were assigned the following values, avdl  X  200, b  X  0 : 5, k and slope  X  0.2. For the Okapi weighting scheme, K represents the ratio between the length of D by dl i (sum of tf ij ) and the collection mean was noted by avdl.
 References
