 Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, MD, USA Department of Information System, University of Maryland, Baltimore County, MD, USA 1. Introduction to outliers in the data affect ing the boundaries and widths o f the discrete intervals. various locations along highways.
 Transportation Institute X  X  2007 Urba n Mobility Re port [10,22]. Specifically,  X  To compensate for congestion, companies add vehicles, hire more drivers and employees, and ex-critical in comparing various traffic patterns.
 crashes [24] can be captured in the intervals.
 data.
 1.1. Temporal dependence temporal dependence in our approach.

Specifically, in this paper we make the following contributions:  X  We present a novel approach to generate temporal neighborhoods using the robust framework of 2. Related work multi-modal density functions where variable bin-width models can be more useful. Extended SAX technique [17] (ESAX) keeps 3-tuple (min, max, mean) for each interval in order and as in SAX technique.

In addition [9,13] discuss clustering temporal data. [19] proposes a new method for unsupervised neighborhood discovery using Markov model based discretization technique and Dynamic programming to find optimal binning strategies. 3. Approach
In this paper we propose an approach to generate temporal neighborhoods by discretizing temporal distinct steps as outlined in Fig. 1: (b) Markov Modeling : We consider the equal frequency bins as the states of a Markov model. We (d) Unequal depth discretization without using Markov Models : Additionally we propose two so-3.1. Equal frequency binning Our data is a series of temporal observations. We first formally define temporal data: t binning defined as follows: X
Each temporal bin B i = { X j | t j  X  T, i.m +1 j ( i +1) m } ,  X  i =1 ...n .(when N mod n = 0 , last bin size = N  X  ( n  X  1) N n ).
 (  X   X  forms a temporal dataset. This temporal summarization now represents our equal frequency bins. 3.1.1. Distance measures follows: (from  X  0 k (  X  0 ,  X  0 ) to  X  1 k (  X  1 ,  X  1 ) )isdefinedas: Definition 3 ( KL Divergence Measure). The Kullback-Leibler divergence (differential entropy) = D divergence reduces to the following, d ( B i ,B j )= 1 2 2log e  X  j  X  Definition 4 (Mahalanobis Distance Measure). The Mahalanobis Distance Measure = D Mahalanobis (  X  (  X  that B i  X  X  (  X  i , X  2 i ) , B j  X  X  (  X  j , X  2 j ) ), D Hellinger ( B i ,B j )= d ( B i ,B j )= mensional temporal data. 3.2. Markov modeling
Now, we model the temporal summarization dataset as a Markov process. Since there is a temporal model such that S  X  = X   X  = B  X  (  X   X  , X  2  X  ) ,  X   X  =  X  1 ... X  n . can use a Markov model.
 the stochastic process Y (  X  ) becomes a Markov process.
 P ( S i | S i  X  1 ) [2].
 used for these definitions.
 Hellinger [20].
 define the initial distribution: concept of temporal window (or lag) first, i w . We next define the transition probability between any two states on our Markov model: a probability measure.
 transition probability p ij = p ij windows for each bin, thereby reducing the complexity of computation. the Markov model, which are computed from the distance measures using formula Fig. 3(d). the symmetry of the matrix. In the following merging algorithm we address this aspect. bins and w is the window size.
 Algorithm 1 InitBin: Setup Markov Model Inputs (1) X temporal : Temporal data (of size N). (2) n : Initial number of bins. (3) w : Lag or window size.
 Outputs (1) Equal-frequency temporal bins { B 1 ,...B n } . (2) Temporal summarization {  X  1 ,...  X  n } . (3) Markov Transition Probability matrix T (normalized).
 Require: (3 w n ) . 2: Divide the temporal data into n temporal bins B 1 ,B 2 ,...,B n with equal frequency. 3: if n n min then 4: exit. 5: end if 7: Compute transition probability matrix P =[ p ij ] ,where 8: Normalize P to obtain row-stochastic T ,with 3.3. Unequal depth discretization using Markov models 3.3.1. S imilarity based merg ing (SMerg) has to be carefully selected. Algorithm 2 SMerg : Generation of Unequal depth Bins using similarity based merging Inputs (1) X temporal : Temporal data (of size N). (2) n : Initial number of bins. (3) w : Lag or window size. (4) n min : Minimum number of output bins (defaults to 2 ). (5) k : Threshold factor, s.t.  X  = k n  X  1 ( k defaults to 1 ). Outputs Unequal-depth temporal bins { B 1 ,...B n Require: (2 n min n )  X  (0 . 1 k 2)  X  (3 w n ) .
 1: Call InitBin. 3: ( i , j )  X  argmax 4: while ( P ( i  X  j ) &gt; X  threshold )  X  ( n&gt;n min ) do 7: Re-compute the transition probability matrix and re-normalize. 8: n  X  n  X  1 . 10: ( i , j )  X  argmax 11: end while 13: Terminate and output final (merged) bins.
 threshold, with k starting from 0 . 1 to 2 . 0 and the results are compared. size. 3.3.2. Markov St ationary distribution based Merging (StMerg) Algorithm 3 StMerg : Generation of Unequal depth Bins using Markov Stationary Distribution Inputs (1) X temporal : Temporal data (of size N). (2) n : Initial number of bins. (3) i max :Max m iterations for stationary convergence. (4) error : Threshold for stationary convergence. (5) h : Number of DFT coefficients for the high pass filter.
 Outputs (1) Unequal-depth temporal bins { B 1 ,...B n (2) Stationary distribution  X  as approximate temporal data.
 Require: (0 error 1)  X  (0 &lt;i max 4) . 1: Call InitBin. 2:  X   X  T . 3: for iteration =0 to i max do 4:  X   X   X   X  T . 5: To check whether  X  has already been converged, compute error ( X ) = 6: if ( error ( X ) &lt; error ) then 7: break. 8: end if 9: end for 10: if  X  already converged then 12: else 14: end if 17: for i =0 to n  X  1 do 18: if (  X  i . X  i +1 &lt; 0 ) then 20: end if 21: end for 22: Merge all bins in between every two successive split points in S spl . 23: Terminate and output final (merged) bins. approach: strictly positive at some iteration of the transition matrix.
 behavior.
 O ( N + n 2 . 807 ) ). Here N is data size and n is the number of bins. Here typically w = n . 3.4. Unequal depth discretization for optimal binning or the optimal binning. We next discuss a greedy approach and an approach based on dynamic pro-maximized.
 3.4.1. Greedy approach
We can simplify the Algorithm 3 to form a greedy version without using a Markov model. The Al-focus on obtaining optimal merging which ensures a global optimum. 3.4.2. Optimal merging: Naive approach
In demarcating the neighborhoods we aim to create optimal binning such that the merging of the maximized.
 3+ ...n = n ( n  X  1) 2 =  X  ( n 2 ) , as shown in the following table. Algorithm 4 GMerg : Merging intervals and generation of temporal neighborhoods using greedy ap-proach Inputs (1) X temporal : Temporal data (of size N). (2) n : Initial number of bins. (4) n min : Minimum number of output bins (defaults to 2 ). (5)  X  threshold : Threshold on similarity (defaults to 0 . 7 ). Outputs Unequal-depth temporal bins { B 1 ,...B n 1: Call InitBin. {obtain n initial equal-depth bins} 2: for i =1 to n  X  1 do 4: end for 5: i  X  max _ element ( sim ) . {find maximum similar adjacent bins} 6: while ( n&gt;n min ) and ( sim [ i ] &gt; X  threshold ) do 7: Merge the bins ( B i ,B i+1 ) 8: n  X  n  X  1 9: Remove element i from the array sim . 10: i  X  max _ element ( sim ) . {find maximum similar adjacent bins from the rest of the bins} 11: end while j The partition will be optimal iff that for optimal partitioning we want to maximize this similarity. [ k opt +1 ,j ] , we need to find k opt , as shown in Fig. 5(c), such that
As we can see that the top-down recursive approach will be expensive in terms of time complexity since for initial bin size n , the recurrence relation T ( n ) is given by, we use dynamic programming to do the optimal merging. Here we use a bottom-up approach instead and reuse the similarities of subproblems already computed. 3.4.3. Optimal merging using dynamic programming
Figure 5 and Algorit hm 5 outlines the intuiti on on how we utilize dynami c programming to find level, merge the nodes with score greater than threshold value. 0.985 which is greater than the merge threshold of 0.8 so a merge takes place and so on. 3.5. Discussion of OptMerg We next discuss important properties of the OptMerg algorithm.
 { dissimilarities using dynamic programming 1: for i =1 to n do 2: sim [ i, i ]  X  0 3: if ( i&lt;n ) then 5: part [ i, i +1]  X  i 6: end if 7: end for 8: for l =2 to n do { length of a neighborhood} 9: for i =1 to n  X  l +1 do 10: j  X  i + l 11: sim [ i, j ]  X  X  X  1 12: max  X  X  X  1 13: for k = i to j  X  1 do 14: s  X  sim [ i, k ]+ sim [ k +1 ,j ] 15: if ( s + d ( B k ,B k +1 ) &gt; max ) then 16: sim [ i, j ]  X  s 17: max  X  s + d ( B k ,B k +1 ) 18: part [ i, j ]  X  k 19: end if 20: end for 21: end for 22: end for 23: create OPT partitions 24: CreateOPTPartTree(part, 1, n) {create OPTPart tree} Algorithm 6 CreateOPTPartTree: Create Optimal partition tree 1: CreateOPTPartTree(part, i, j, node) 2: if node == nil then 3: Create new node 4: end if 5: if ( j&gt;i ) then 6: node.interval  X  ( i, j ) 7: k  X  part [ i, j ] 8: node.score  X  e  X  d ( k,k +1) {degree of merge-ability} 9: node.left  X  CreateOPTPartTree(part, i, part[i, j], node.left) 10: node.right  X  CreateOPTPartTree(part, i, part[i, j], node.right) 11: else {if i == j } 12: node.interval  X  i 13: node.score  X  1 14: end if 15: return node proof of optimality.
 increasing dissimilarity.
 Algorithm 7 OPTMerg : Optimal Merging of temporal neighborhoods with stopping criterion Inputs (1) X temporal : Temporal data (of size N). (2) n : Initial number of bins. (4) n min : Minimum number of output bins (defaults to 2 ). Outputs Unequal-depth temporal bins { B 1 ,...B n 1: Call InitBin. {obtain n initial equal-depth bins} 2: Call OPTPart. {create the OPT partition tree} 3: Do a BFS (breadth-first-search, level-order traversal) on the OPT Partition Tree: 4: for all traversed nodes of the tree do 5: if (node.score &gt;score threshold ) then 6: Merge all the bins in node.interval 7: {no need to traverse its children} 8: end if 9: end for to  X  ( N + n log n ) .
 user externally as well.
 3.6. Temporal neighborhoods define our temporal neighborhood based on the merged bins as follows: Definition 13 (Temporal Neighborhood). Given a set of equal depth temporal bins B temporal = {
B 1 ,B 2 ,...B n } , Temporal Neighborhood NBD temporal = { NBD 1 ,..., NBD n NBD i = B temporal and NBD i  X  NBD j = X  whenever i = j such that inter neighborhood dissimilar-ity is maximized and intra neighborhood similarity is maximized. 4. Experimental results
We discuss detailed experimental results of our approach as follows:  X  Results of algorithms using Markov Models namely Smerg and STMerg .  X  Results for Optimal binning algorithms GMerg and OPTMerg .  X  Comparative results across alorithms proposed.  X  Comparing with existing approach [16]. 4.1. Datasets We use synthetic and real world datasets for these results as described below: and, (b) Gaussian: randomly generated using Box-Muller transformation [4]. The data ranged from distributions and Synthetic (3) has 3 distributions.
 US-50 West @ Church Rd West and US-50 East @ Church Rd East, (b) I-395 near Seminary Rd. Metrics for evaluation : the methods: by discussing the results of algorithms using Markov Models namely Smerg and STMerg and results for Optimal binning algorithms GMerg and OPTMerg which do not use Markov Models as a base, subsequently we discuss overall observations and comparative results. 4.2. Results with SMerg and STMerg 4.2.1. Results in the CATT Lab data observations here: powersave mode if the traffic intensity is very minimal or close to zero. 4.2.2. Results in the synthetic data SMerg and h for StMerg .
 keeping others fixed.

Number of temporal observations: As it can be seen from Figs 10 and 11 both SMerg and StMerg recall values.
 recall more than 90 percent most of the times).

Threshold value k for SMerg and h for StMerg: Accuracy of the proposed methods is dependent approximate the temporal data, using stationary Markov distribution. and precision. 4.3. Results with GMerg and OptMerg We next discuss results varying the following: (a) Impact of number of temporal observations on threshold for OptMerg .

Impact of number of temporal observations on GMerg and OptMerg: The results for change in the number of temporal observations for GMerg and OPTMerg are shown in Figs 12 and 13 respectively. large sized data.

Impact of change of threshold and initial number of bins on GMerg and OptMerg: In general it was with distance measures used. decreased from 0.8 to 0.4.

Runtime: As can be seen from the Fig 14, the algorithms SMerg and GMerg perform better than chosen as stopping criteria. 4.4. Overall observations distance measures in terms of the various settings and parameters.  X  In general SMerg and OptMerg perform the best in terms of precision and recall.  X  Mahalanobis distance measures performs best for noisy data for SMerg , OptMerg and GMerg 4.5. Comparative results comparative analysis.
 reduction via PAA) [16] with  X  c i = n N refer to this merging with SAX as SAX-Merg.

Results in the synthetic data with outliers We compare our algorithms with the approach outlined outliers in different bins (as SAX-Merg does).
 (affecting precision and recall). Extended merge keeps min and max (range) for each bin, however standard deviation is a better dispersion measure than maintaining the range. 5. Conclusion borhood discovery.
 References
