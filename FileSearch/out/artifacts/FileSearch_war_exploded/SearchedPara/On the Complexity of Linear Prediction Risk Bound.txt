 Linear prediction is the cornerstone of an extensive number of machine learning algorithms, in-margin based methods (e.g. SVMs or boosting), we seek genera lization bounds in terms of either which use linear prediction.
 the F -regularized  X  -risk. More specifically, and w . In a formulation closely related to the dual problem, we hav e: Rademacher complexities (a measure of the complexity of a fu nction class) provide a direct route to VC dimensions bounds, but they are typically much sharper and allow for distribution dependent bounds. There are a number of methods in the literature to use Rademacher complexities to obtain either generalization bounds or margin bounds. Bartlett an d Mendelson [2002] provide a general-Panchenko [2002] provide means to obtain margin bounds thro ugh Rademacher complexities. with respect to a strongly convex complexity function F (as in Equation 1). These bounds provide straints), margin bounds (including L 2 and L 1 margins, and, more generally, for L p margins), a proof of the PAC-Bayes theorem, and L 2 covering numbers (with L p norm constraints and relative this more unified methodology.
 ing algorithms  X  are rooted in convex duality (following Mei r and Zhang [2003]) and use a more provides this more detailed comparison. 1.1 Related Work A staggering number of results have focused on this problem i n varied special cases. Perhaps the son [2002] (using Rademacher complexities) and Langford an d Shawe-Taylor [2003], McAllester [2003] (using the PAC-Bayes theorem). For L 1 -margins (relevant for Boosting, winnow, etc), L 0 norm). For L 1 regularization, Ng [2004] provides generalization bounds for this case, which follow from the covering number bounds of Zhang [2002]. Howe ver, these bounds are only stated as polynomial in the relevant quantities (dependencies are not provided).
 Previous to this work, the most unified framework for providi ng generalization bounds for linear prediction stem from the covering number bounds in Zhang [20 02]. Using these covering number bounds, Zhang [2002] derives margin bounds in a variety of ca ses. However, providing sharp gen-be n  X  1 / 4 rather than n  X  1 / 2 ). We discuss this later in Section 4. X  X  Y are distributed according to some unknown distribution P . The inner product between vectors x and w is denoted by h w , x i , where w  X  S (here, S is a subset of the dual space to k w k  X  = sup {h w , x i : k x k  X  1 } . We further assume that for all x  X  X , k x k  X  X . loss. We denote the empirical loss as  X  L ( w ) = 1 n P n i =1  X  ( h w , x i i , y i ) . The restriction we make on our complexity function F is that it is a strongly convex function. In said to be  X  -strongly convex w.r.t. to kk  X  iff  X  u , v  X  S ,  X   X   X  [0 , 1] , we have See Shalev-Shwartz and Singer [2006] for more discussion on this generalized definition of strong convexity.
 Recall the definition of the Rademacher and Gaussian complex ity of a function class F , i.i.d.
  X  L ( f ) = 1 n P n i =1  X  ( f ( x i ) , y i ) is the empirical loss.
 Theorem 1. (Bartlett and Mendelson [2002]) Assume the loss  X  is Lipschitz (with respect to its probability at least 1  X   X  simultaneously for all f  X  F , we have that where R n ( F ) is the Rademacher complexity of a function class F , and n is the sample size. The second result, for binary prediction, from Koltchinski i and Panchenko [2002] provides a mar-gin bound in terms of the Rademacher complexity. The followi ng is a variant of Theorem 2 in Koltchinskii and Panchenko [2002]: Theorem 2. (Koltchinskii and Panchenko [2002]) The zero-one loss function is given by Rademacher complexities then we obtain sharp generalizati on bounds. Typically, we desire upper bounds on the Rademacher complexity that decrease with n . w  X  W} . Our main theorem bounds the complexity of F W for certain sets W .
 Theorem 3. (Complexity Bounds) Let S be a closed convex set and let F : S  X  R be  X  -strongly S : F ( w )  X  W 2  X  } . Then, we have first provide a few examples, before proving this result. 3.1 Examples where k x k p := P d j =1 | x i | p on R d w.r.t. itself. Set X , W as in Theorem 3. Then, we have (2) L  X  /L 1 norms. Let S = { w  X  R d : k w k 1 = W 1 , w j  X  0 } be the W 1 -scaled probability { w  X  S : entro ( w )  X  E } . Then, we have Note that if we take to be the uniform distribution then for any w  X  S we have that trivial upper entire scaled probability simplex. Then dimension of x to include negated copies of each coordinate. So, if we have S = { w  X  R d : In this way, even though the L 1 norm is not strongly convex (so our previous Theorem does not of the optimization problem in Equation 2. (3) Smooth norms. A norm is (2 , D ) -smooth on S if for any x , y  X  S , Let kk be a (2 , D ) -smooth norm and kk  X  be its dual. Lemma 11 in the appendix proves that kk  X  is 2 /D 2 -strongly convex w.r.t. itself. Set X , W as in Theorem 3. Then, we have (4) Bregman divergences. For a strongly convex F , define the Bregman divergence  X  F ( w k v ) := W  X  F ( k v ) inherits the strong convexity of F .
 and hence can be easily extended to infinite dimensional spac es under appropriate assumptions. 3.2 The Proof First, some background on convex duality is in order. The Fen chel conjugate of F : S  X  R is defined as: A simple consequence of this definition is Fenchel-Young ine quality, If F is  X  -strongly convex, then F  X  is differentiable and See the Appendix in Shalev-Shwartz [2007] for proof. Using t his inequality we can control the expectation of F  X  applied to a sum of independent random variables.
 F  X  ( S i )  X  iV 2 / 2  X  is a supermartingale. Furthermore, if inf nV 2 / 2  X  .
 Proof. Note that inf w  X  S F ( w ) = 0 implies F  X  ( 0 ) = 0 . Inequality (7) gives, V 2 , we get F  X  ( 0 ) = 0 .
 Like Meir and Zhang [2003] (see Section 5 therein), we begin b y using conjugate duality to bound the Rademacher complexity. To finish the proof, we exploit th e strong convexity of F by applying the above lemma.
  X  &gt; 0 . By Fenchel X  X  inequality, we have h w ,  X   X  i  X  F ( w ) + F  X  (  X   X  ) which implies Since, F ( w )  X  W 2  X  for all w  X  W , we have Taking expectation (w.r.t.  X  i  X  X ), we get V which completes the proof. 4.1 Risk Bounds stant L  X  . Based on the Rademacher generalization bound provided in t he Introduction (see Theo-rem 1) and the bounds on Rademacher complexity proved in prev ious section, we obtain the follow-ing corollaries.
 Ng [2004] provides bounds for methods which use L 1 regularization. These bounds are only stated as polynomial bounds, and, the methods used (covering numbe r techniques from Pollard [1984] and covering number bounds from Zhang [2002]) would provide rat her loose bounds (the n dependence The above argument is sharp and rather direct. 4.2 Margin Bounds now demonstrate how to get improved margin bounds using the u pper bounds for the Rademacher complexity derived in Section 3.
 Based on the Rademacher margin bound provided in the Introdu ction (see Theorem 2), we get the following corollary which will directly imply the margin bo unds we are aiming for. The bound for the p = 2 case has been used to explain the performance of SVMs. Our bou nd essentially matches the best known bound [Bartlett and Mendelson, 2002] which wa s an improvement over previous L of  X  over the sample: orem 5] and [Zhang, 2002, Theorem 7], by removing a factor of were an improvement over previous results obtained using fa t-shattering dimension estimates. Corollary 7. (Entropy Based Margins) Let X be such that for all x  X  X , k x k  X   X  X . Consider at least 1  X   X  over the sample, for all margins  X  &gt; 0 and all weight vector w  X  W , where entro ( w ) := P i | w i | k Proof. Proof is provided in the appendix. 4.3 PAC-Bayes Theorem We now show that (a form of) the PAC Bayesian theorem [McAlles ter, 1999] is a consequence of Theorem 3. In the PAC Bayesian theorem, we have a set of hypoth esis (possibly infinite) C . We  X  . Note that in this section we are considering a more general f orm of the loss. measure d X  ( ) and the loss  X  ( , x ) . This leads to the following straightforward corollary. we have that, Proof. Proof is provided in the appendix.
 p examine large n , this bound is sharper. We note that our goal was not to prove t he PAC-Bayes theorem, and we have made little attempt to optimize the cons tants. 4.4 Covering Number Bounds It is worth noting that using Sudakov X  X  minoration results w e can obtain upper bound on the L 2 corollary of the Sudakov minoration theorem for Gaussian co mplexities (Theorem 3.18, Page 80 of Ledoux and Talagrand [1991]).
 such that its L 2 covering number is bounded as follows: This bound is sharper than those that could be derived from th e N  X  covering number bounds of Zhang [2002]. risk bounds. The algorithm we consider performs the update, norms, through appropriate choices of F . See Shalev-Shwartz [2007] for discussion. For the algorithm given by the above update, the following th eorem is a bound on the cumulative in Shalev-Shwartz [2007]), applied to our linear case.
 Corollary 10. (Shalev-Shwartz and Singer [2006]) Let S be a closed convex set and let F : S  X  R W for all sequences { ( x t , y t ) } n t =1 , For completeness, we provide a direct proof in the Appendix. Interestingly, the regret above is precisely our complexity bounds (when L  X  = 1 ). Also, our risk bounds are a factor of 2 worse, essentially due to the symmetrization step used in proving T heorem 1.

