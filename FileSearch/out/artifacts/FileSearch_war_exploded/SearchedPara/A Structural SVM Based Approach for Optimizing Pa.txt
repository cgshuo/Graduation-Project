 Harikrishna Narasimhan harikrishna@csa.iisc.ernet.in Shivani Agarwal shivani@csa.iisc.ernet.in This document contains supplementary material for the paper  X  X  Structural SVM Based Approach for Op-timizing Partial AUC X , in Proceedings of the 30 th In-ternational Conference on Machine Learning , 2013. Algorithm 2 contains an outline of the cutting plane method (for optimizing partial AUC) described in Sec-tion 3 of the paper.
 Algorithm 2 Cutting Plane Method for SVM pAUC This section contains the proof for Theorem 1 in Sec-tion 4 of the paper.
 Theorem 1. The solution  X   X  to OP2 lies in  X  w m,n .  X   X  an ordering  X   X   X  in which the instances x  X  ( j  X   X  C.2. Partial AUC in [0 ,  X  ] for Protein-protein Algorithm parameters were chosen similarly to the cheminformatics data set (using a validation set). Continuous features were normalized to zero mean and unit variance; discrete features were scaled to [0 , 1]. In this application, it is natural to consider a transduc-tive setting since the set of all protein pairs is known; thus when normalizing features and filling in missing feature values, we used statistics computed from the entire data set. 2 C.3. Partial AUC in [  X ,  X  ] for Medical Algorithm parameters were chosen as for the chemin-formatics data set. All features were normalized to zero mean and unit variance (using only the training set in each case). 3 C.4. Run Time Analysis These experiments were run on an Intel Xeon (2.13 GHz) machine with 12 GB RAM; the algorithm pa-rameters C and  X  for these experiments were set to 10 and 0.1, respectively.
 Harikrishna Narasimhan harikrishna@csa.iisc.ernet.in Shivani Agarwal shivani@csa.iisc.ernet.in The receiver operating characteristic (ROC) curve plays an important role as an evaluation tool in ma-chine learning. In particular, the area under the ROC curve (AUC) is widely used to summarize the perfor-mance of a scoring function in binary classification, and is often the primary performance measure of in-terest in bipartite ranking problems ( Cortes &amp; Mohri , 2004 ; Agarwal et al. , 2005 ). In an increasing num-ber of applications, however, the performance mea-sure of interest is not the area under the full ROC curve, but instead, the partial area under the ROC curve between two specified false positive rates (FPRs) (see Figure 1 ). For example, in ranking applications where accuracy at the top is critical, one is often inter-ested in the left-most part of the ROC curve ( Rudin , 2009 ; Agarwal , 2011 ; Rakotomamonjy , 2012 ); this cor-responds to maximizing partial AUC in a false positive range of the form [0 ,  X  ]. In biometric screening, where false positives are intolerable, one is again interested in maximizing the partial AUC in a false positive range [0 ,  X  ] for some suitably small  X  . In the KDD Cup 2008 challenge on breast cancer detection, performance was measured in terms of the partial AUC in a specific false positive range [  X ,  X  ] that was deemed clinically relevant ( Rao et al. , 2008 ). 1 In this paper, we develop a general structural SVM based method for directly optimizing the partial AUC between any two given false positive rates  X  and  X  . Our approach makes use of a cutting plane solver along the lines of the structural SVM based approach for op-timizing the full AUC developed by ( Joachims , 2005 ). Each iteration of the cutting plane method involves a combinatorial search over an exponential number of orderings of the positive vs. negative training instances  X  with each ordering represented as a binary matrix  X  to find the currently most violated constraint. In the case of the AUC, this combinatorial optimization prob-lem decomposes neatly into one in which each matrix entry can be chosen independently ( Joachims , 2005 ). Unfortunately, for the partial AUC, such a straight-forward decomposition is no longer possible since the negative instances involved in the relevant false posi-tive range can be different for different orderings. We characterize the set of relevant negative instances in the optimal ordering and formulate an equivalent opti-mization problem with a restricted search space, which can then be broken down into smaller tractable opti-mization problems. For a false positive range of the form [0 ,  X  ], it turns out that following the reformula-tion, the individual matrix entries can again be opti-mized independently. For the general case [  X ,  X  ], the individual entries of the matrix cannot be chosen in-dependently, but each row of the matrix can be opti-mized separately  X  and efficiently. In both cases, the optimization procedure has the same computational complexity as in the case of Joachims X  algorithm for optimizing the usual AUC.
 Related Work. There has been much work on developing algorithms to optimize the full AUC, mostly in the context of ranking ( Herbrich et al. , 2000 ; Joachims , 2002 ; Freund et al. , 2003 ; Burges et al. , 2005 ; Joachims , 2005 ). There has also been interest in the ranking literature in optimizing measures focus-ing on the left end of the ROC curve, corresponding to maximizing accuracy at the top of the list ( Rudin , 2009 ); in particular, the recent Infinite Push algorithm ( Agarwal , 2011 ; Rakotomamonjy , 2012 ) can be viewed as maximizing the partial AUC in the range [0 , 1 where n is the number of negative training examples. The partial AUC in false positive ranges of the form [0 ,  X  ] has received some attention in the bioinfor-matics and biometrics literature ( Pepe &amp; Thompson , 2000 ; Dodd &amp; Pepe , 2003 ; Wang &amp; Chang , 2011 ; Ricamato &amp; Tortorella , 2011 ; Hsu &amp; Hsueh , 2012 ); however in most cases, the algorithms developed are heuristic in nature. The asymmetric SVM algo-rithm of ( Wu et al. , 2008 ), an extension of one-class SVM that relies on a parameter tuning procedure, also aims to maximize the partial AUC in a range [0 ,  X  ]. Some recent work on optimizing the partial AUC in general false positive ranges of the form [  X ,  X  ] includes the boosting-based algorithms pAUC-Boost ( Komori &amp; Eguchi , 2010 ) and p U -AUCBoost ( Takenouchi et al. , 2012 ).
 To our knowledge, this is the first work to develop a principled support vector method that can directly optimize the partial AUC in an arbitrary false positive range [  X ,  X  ].
 Organization. We start with some preliminaries in Section 2 . Section 3 describes our structural SVM based approach for optimizing the partial AUC. Sec-tion 4 gives efficient algorithms for finding the most vi-olated constraint for the cutting plane solver for both the special case [0 ,  X  ] and the general case [  X ,  X  ]. Sec-tion 5 gives experimental results on several real-world tasks: ranking applications; protein-protein interac-tion (PPI) prediction; and medical diagnosis. Let X be an instance space, and let D + , D  X  be prob-ability distributions on X . Given a training sam-ple S = ( S + , S  X  ) consisting of m positive instances S + = ( x + 1 , . . . , x + m )  X  X m drawn iid according to D and n negative instances S  X  = ( x  X  1 , . . . , x  X  n )  X  X n drawn iid according to D  X  , the goal is to learn a scor-ing function f : X  X  R that has good performance in terms of the partial AUC between some specified false positive rates  X  and  X  , where 0  X   X  &lt;  X   X  1, as described in more detail below.
 Partial AUC . Recall that for a scoring function f : X  X  R and threshold t  X  R , the true positive rate (TPR) of the classifier sign( f ( x )  X  t ) is the probability that it correctly classifies a random positive instance from D + as positive: 2 Similarly, the false positive rate (FPR) of the classi-fier is the probability that it misclassifies a random negative instance from D  X  as positive: The ROC curve for the scoring function f is then de-fined as the plot of TPR f ( t ) against FPR f ( t ) for dif-ferent values of t . The area under this curve can be computed as where FPR  X  1 f ( u ) = inf t  X  R | FPR f ( t )  X  u . Assuming there are no ties, it can be shown ( Cortes &amp; Mohri , 2004 ) that the AUC can be written as Our interest here is in the area under the curve be-tween FPRs  X  and  X  . The (normalized) partial AUC of f in the range [  X ,  X  ] is defined as pAUC f (  X ,  X  ) = Empirical Partial AUC. Given a sample S = ( S + , S  X  ) as above, one can plot an empirical ROC curve corresponding to a scoring function f : X  X  R ; assuming there are no ties, this is obtained by using instead of TPR f ( t ) and FPR f ( t ). Again assuming there are no ties, the area under this empirical curve is given by The (normalized) empirical partial AUC of f in the FPR range [  X ,  X  ] can then be written as ( Dodd &amp; Pepe , 2003 ) 3 \ pAUC f (  X ,  X  ) = where j  X  =  X  n X   X  , j  X  =  X  n X   X  , and x  X  ( j ) denotes the negative instance in S  X  ranked in j -th position (among negatives, in descending order of scores) by f (see Fig-ure 2 for a visual explanation of the three terms in the sum). Note that we have assumed n  X  1  X   X   X  , so that j Partial AUC vs. AUC. Before closing this section, we note that a scoring function with a high AUC value need not be optimal in terms of partial AUC in a par-ticular FPR range. This is illustrated in Table 1 and Figure 3 , which show scores assigned by two scoring functions f 1 and f 2 on a hypothetical sample of 4 pos-itive and 6 negative instances, and the corresponding ROC curves. As can be seen, while f 1 gives a higher AUC value, f 2 has higher partial AUC in the FPR range [0 . 1 , 0 . 2]. This motivates the need to design al-gorithms tailored for optimizing partial AUC. Given a training sample S = ( S + , S  X  )  X  X m  X  X n , our goal is to find a scoring function f : X  X  R that maximizes the partial AUC in an FPR range [  X ,  X  ], or equivalently, that minimizes the following empirical risk: We now cast this problem into a structural SVM framework ( Tsochantaridis et al. , 2005 ). In the follow-ing, we shall assume X  X  R d for some d  X  Z + and con-sider linear scoring functions of the form f ( x ) = w  X  x for some w  X  R d ; the approach extends to non-linear functions / non-Euclidean instance spaces using ker-nels ( Yu &amp; Joachims , 2008 ).
 For any ordering of the training instances, we can rep-resent (errors in) the relative ordering of the m positive instances in S + and n negative instances in S  X  via a matrix  X  = [  X  ij ]  X  { 0 , 1 } m  X  n as follows: Not all 2 mn matrices in { 0 , 1 } m  X  n represent a valid rel-ative ordering (due to transitivity requirements). We let  X  m,n denote the set of all matrices in { 0 , 1 } m  X  n that do correspond to valid orderings. Clearly, the correct relative ordering  X   X  has  X   X  ij = 0  X  i, j . For any  X   X   X  m,n , we can define the partial AUC loss of  X  with respect to  X   X  as  X  where ( j )  X  denotes the index of the negative instance in S  X  ranked in j -th position (among negatives) by any fixed ordering consistent with the matrix  X  (note that all such orderings yield the same value of partial AUC loss).
 Building on the approach of ( Joachims , 2005 ), we de-fine a joint feature map  X  : ( X m  X  X n )  X   X  m,n  X  R d as  X  ( S,  X  ) = The above expression evaluates to a (normalized) sum of feature vector differences over all pairs of positive-negative instances in S in which the positive instance is ordered by  X  above the negative instance. 5 This choice of  X  ( S,  X  ) ensures that for any fixed w  X  R d , maximiz-ing w  X   X  ( S,  X  ) over  X   X   X  m,n yields an ordering matrix consistent with the scoring function f ( x ) = w  X  x . The problem of optimizing the partial AUC now reduces to finding a w  X  R d for which the maximizer over  X   X   X  m,n of w  X   X  ( S,  X  ) has the highest partial AUC (or minimum partial AUC loss). This can be framed as the following convex optimization problem: s.t.  X   X   X   X  m,n : w  X   X  ( S,  X   X  )  X   X  ( S,  X  )  X   X  pAUC(  X , X  ) (  X   X  ,  X  )  X   X  , which in turn can be written as s.t.  X   X   X   X  m,n : where C &gt; 0 is an appropriate regularization param-eter. It can be shown that the slack variable  X  in the above optimization problem evaluates to an upper bound on the empirical partial AUC risk in Eq. ( 1 ) (de-tails will be provided in a longer version of the paper). For  X  = 0 and  X  = 1, the above optimization problem reduces to that associated with the full AUC consid-ered in ( Joachims , 2005 ). As we shall see, solving the above optimization problem for the partial AUC turns out to be more tricky.
 The optimization problem OP1 has an exponential number of constraints, one for each matrix  X   X   X  m,n . To solve this problem, we use the cutting plane method, which is based on the fact that for any  X  &gt; 0, a small subset of the constraints is sufficient to find an  X  -approximate solution to the problem ( Joachims , 2006 ). In particular, the cutting plane method starts with an empty constraint set C =  X  , and on each iter-ation, adds the most violated constraint to C , thereby solving a tighter relaxation of OP1 in the subsequent iteration; this continues until no constraint is violated by more than  X  (see supplementary material for an outline of this algorithm).
 It can be shown that for any fixed regularization pa-rameter C &gt; 0 and accuracy parameter  X  &gt; 0, the cut-ting plane method converges in a constant number of iterations ( Joachims , 2006 ). Since the quadratic pro-gram in each iteration of this algorithm is of constant size, the only bottleneck in the algorithm is the com-binatorial optimization over  X  m,n required to find the most violated constraint. In the following, we show how this combinatorial optimization can be performed efficiently for the partial AUC, yielding an overall time complexity that is polynomial in the size of the train-ing sample, and moreover, that matches the computa-tional complexity for the full AUC. As noted above, obtaining a polynomial-time cutting plane method for optimizing the partial AUC in the structural SVM setting of OP1 hinges on having a polynomial-time algorithm for the combinatorial opti-mization problem (over  X  m,n ) associated with finding the most violated constraint on each iteration, which can be stated explicitly as follows: where In the case of AUC, the above argmax (over an expo-nential number of matrices) can be easily computed by neatly decomposing the problem into one where each  X  ij can be chosen independently ( Joachims , 2005 ). For the case of partial AUC in an arbitrary FPR inter-val [  X ,  X  ], it is not obvious how such a decomposition is possible as the loss term in the objective Q w (  X  ) now involves different subsets of negative instances for different orderings  X   X   X  m,n . However it turns out that by working with a restricted search space of orderings in  X  m,n , one can indeed break down OP2 into smaller maximization problems over (groups of)  X  ij  X  X , and that the resulting maximization problems can then be solved with the same computational cost as that required for the usual AUC. To proceed, for any w  X  R d , let us define the set
 X  This is the set of all ordering matrices  X  in which any two negative instances that are separated by a positive instance are sorted according to w (i.e. in descending order of the scores w  X  x  X  j ). Then we have the following result (see supplementary material for a proof): Theorem 1. The solution  X   X  to OP2 lies in  X  w m,n . 6 rewrite OP2 as follows: Now note that for any  X   X   X  m,n , any ordering of the instances that is consistent with  X  yields the same value of Q w (  X  ). In particular, for any  X   X   X  w m,n , there is an ordering consistent with  X  in which all negative instances are sorted according to w (this follows from where e Q w (  X  ) = where ( j ) w refers to the index of the negative instance in S  X  ranked in j -th position by w , and where we have used the shorthand notation x  X  ij = ( x + i  X  x  X  j This allows us to rewrite OP3 as follows: This optimization problem is now easier to solve as the set of negative instances over which the loss term in the objective is computed is the same for all orderings in the search space. Below we give efficient algorithms for solving this problem both for the special case when the FPR range of interest is of the form [0 ,  X  ], and for the general case [  X ,  X  ]. 4.1. Efficient Algorithm for Special Case [0 ,  X  ] For  X  = 0, we have j  X  =  X  n X   X  = 0. In this case, e Q w (  X  ) reduces to the following: e Q w (  X  ) = We consider solving a relaxation of OP4 in which the above objective is optimized over all  X   X  { 0 , 1 } m  X  n As can be seen from Eq. ( 5 ), the objective e Q w (  X  ) de-composes into a sum of terms involving the individ-ual elements  X  i,j , and therefore the solution to OP5 is obtained by maximizing e Q w (  X  ) over each element  X  i,j  X  { 0 , 1 } separately, which clearly yields:  X   X  The following result shows that this solution to the re-laxed problem OP5 actually lies in  X  w m,n , and therefore this is also a solution to the desired problem OP4 : Theorem 2. The ordering matrix  X   X  defined by Eq. ( 6 ) Proof. (sketch) Let i  X  [ m ] and j 1 &lt; j 2 . There are 5 possibilities: (1) j 1 , j 2  X  { 1 , . . . , j  X  } ; (2) j { j  X  + 2 , . . . , n } ; (3) j 1  X  { 1 , . . . , j  X  } and j (4) j 1 = j  X  + 1 and j 2  X  { j  X  + 2 , . . . , n } ; and (5) j case, it can be verified from Eq. ( 6 ) that  X   X  i, ( j  X   X  w A straightforward implementation to compute the solution in Eq. ( 6 ) has computational complexity O ( mn + n log n ). Using a more compact representa-tion of the orderings, this can be further reduced to O (( m + n ) log( m + n )) ( Joachims , 2005 ) (details will be provided in a longer version of the paper). 4.2. Efficient Algorithm for General Case [  X ,  X  ] In the general case, where we allow FPR intervals of the form [  X ,  X  ] for  X  &gt; 0, it is no longer sufficient to solve a relaxation of OP4 over all  X   X  { 0 , 1 } m  X  n as the individual  X  ij  X  X  can no longer be chosen independently. However, it turns out that each row of the matrix,  X  i  X  { 0 , 1 } n , can still be considered separately, and moreover, that the optimization over each  X  i can be done efficiently. In particular, note that for each i , the i -th row of  X   X  essentially corresponds to an interleaving of the lone positive instance x + i with the list of negative instances sorted according to w  X  x  X  j ; thus each  X   X  i the form Algorithm 1 Find Most-Violated Constraint for some r i  X  { 0 , 1 , . . . , n } . In other words, the opti-mization over  X  i  X  { 0 , 1 } n reduces to an optimization over  X  i  X  R w i , where with | R w i | = n + 1. Clearly, we have  X  w m,n = R w 1  X  . . .  X  R w m , and therefore we can rewrite OP4 as Since the objective e Q w (  X  ) (as given by Eq. ( 4 )) de-composes into a sum of terms involving the individ-ual rows  X  i , OP6 can be solved by maximizing e Q w (  X  ) over each row  X  i  X  R w i separately. In a straightfor-ward implementation of this optimization, for each i  X  { 1 , . . . , m } , one would evaluate the term inside the sum over i in Eq. ( 4 ) for each of the n + 1 values of r i (corresponding to the n + 1 choices of  X  i  X  R w i see Eq. ( 7 )) and select the optimal among these; each such evaluation takes O ( n ) time, yielding an overall time complexity of O ( mn 2 ). It turns out, however, that one can partition the n + 1 values of r i into 3 that the optimization over r i in each of these 3 groups (after the negative instances have been sorted accord-ing to w ) can be implemented in O ( n ) time; this yields an overall time complexity of O ( mn + n log n ). A de-scription is given in Algorithm 1 . Again, using a more compact representation of the orderings, it is possible to further reduce the computational complexity of Al-gorithm 1 to O (( m + n ) log( m + n )) ( Joachims , 2005 ) (details will be provided in an extended version). Thus, for both [0 ,  X  ] and [  X ,  X  ] cases, it is possible to find the most violated constraint for the partial AUC in the same time as that required for the usual AUC. This section contains an experimental evaluation of our structural SVM based method for optimizing par-tial AUC, which we refer to as SVM pAUC , on several real-world tasks: ranking applications; protein-protein interaction (PPI) prediction; and medical diagnosis. 7 , 8 All experiments involve learning a linear function. 5.1. Partial AUC in [0 ,  X  ] for Ranking As noted in the introduction, several ranking applica-tions require optimizing performance at the top of the list, which corresponds to optimizing partial AUC in an FPR range of the form [0 ,  X  ]. We consider two such applications here.
 Cheminformatics. Here one is given examples of chemical compounds that are active or inactive against a therapeutic target, and the goal is to rank new com-pounds such that active ones appear at the top of the list. We used a virtual screening data set from ( Jorissen &amp; Gilson , 2005 ); this contains 2142 com-pounds, each represented as a 1021-bit vector using the FP2 molecular fingerprint representation as in ( Agarwal et al. , 2010 ). There are 5 sets of 50 active compounds each (active against 5 different targets), and 1892 inactive compounds. For each target, the 50 active compounds are treated as positive, and all others as negative. We considered optimizing the par-tial AUC in the FPR range [0 , 0 . 1]. The results, av-eraged over the 5 targets and 10 random 10%-90% train-test splits for each target (subject to preserving the proportion of positives) are shown in Table 2 . 9 For comparison, we also show results obtained using the SVM AUC algorithm of ( Joachims , 2005 ), as well as three existing algorithms for optimizing partial AUC: asymmetric SVM (ASVM) ( Wu et al. , 2008 ), pAUC-Boost ( Komori &amp; Eguchi , 2010 ), and a greedy heuris-tic method due to ( Ricamato &amp; Tortorella , 2011 ). Information retrieval (IR). Another ranking ap-plication where accuracy at the top is important is IR. Here one is given a certain number of training queries together with documents labeled as relevant for the query (positive) or irrelevant (negative), and the goal is to rank documents for new queries. We evaluated our algorithm on two widely used IR data sets: TD2004, which is part of the LETOR 2.0 collec-tion ( Liu et al. , 2007 ) (75 queries; total of 444 positive documents and 73726 negative documents); and the TREC10 data set (50 queries, 2892 positive documents and 203507 negative documents) used in ( Yue et al. , 2007 ). In TD2004, each document is represented us-ing 44 features, while those in TREC10 are represented using 750 features. For each data set, we used random splits containing 60% of the queries for training, 20% for validation and the remaining for testing (subject to preserving the proportion of positives). The results, averaged over 10 such random splits, are shown in Ta-ble 3 . In this case, the existing algorithms for optimiz-ing partial AUC do not apply easily as they are not designed to handle queries; we include a comparison with SVM AUC as a baseline. Since in IR it is common to focus on just a small number of documents at the top, we show performance in terms of a range of par-tial AUC values that capture this, as well as the AUC. As can be seen, optimizing partial AUC tends to give higher accuracy close to the very top of the list; where SVM AUC has similar performance, the difference is not statistically significant. 5.2. Partial AUC in [0 ,  X  ] for PPI Prediction In protein-protein interaction (PPI) prediction, given a pair of proteins, the task is to predict whether they interact or not; here again the partial AUC has been used as an evaluation measure ( Qi et al. , 2006 ). We used the PPI data for Yeast from ( Qi et al. , 2006 ), which contains 2865 protein pairs known to be inter-acting (positive) and a random set of 237384 protein pairs assumed to be non-interacting (negative). Each protein pair is represented by 162 features, but there are several missing features; we used a subset of 85 features that contained less than 25% missing values (with missing feature values replaced by mean/mode values). The results, averaged over 10 random 1%-9%-90% train-validation-test splits (subject to preserving the proportion of positives), are shown in Table 4 . Here SVM pAUC significantly outperforms all the four baseline algorithms. 5.3. Partial AUC in [  X ,  X  ] for Medical Our final evaluation is on the KDD Cup 2008 challenge on early breast cancer detection ( Rao et al. , 2008 ). Here the task is to predict whether a given region of interest (ROI) from a breast X-ray image is malignant (positive) or benign (negative). The data set is col-lected from 118 malignant patients and 1594 normal patients. Four X-ray images are available for each pa-tient; overall, there are 102294 candidate ROIs selected from these X-ray images, with each ROI represented by 117 features. In the KDD Cup challenge, perfor-mance was evaluated in terms of the partial area un-der the free-response operating characteristic (FROC) curve in a false positive range [0 . 2 , 0 . 3] deemed clini-cally relevant based on radiologist surveys. The FROC curve ( Miller , 1969 ) effectively uses a scaled version of the false positive rate; for our purposes, the corre-sponding false positive rate is obtained by re-scaling by a factor of s = 6848 / 101671 (this is the total num-ber of images divided by the total number of nega-tive ROIs). Thus, the goal in our experiments was to maximize the partial AUC in the clinically relevant FPR range [0 . 2 s, 0 . 3 s ]. The results, averaged over 10 random 5%-95% train-test splits (subject to pre-serving the proportion of positives) are shown in Ta-ble 5 . Baselines here include SVM AUC , pAUCBoost which can optimize partial AUC over FPR ranges [  X ,  X  ], and an extension of the greedy heuristic method in ( Ricamato &amp; Tortorella , 2011 ) to handle arbitrary FPR ranges. 5.4. Run Time Analysis Finally, we analyzed the training time of the proposed SVM pAUC method for different FPR intervals. We used the TREC10 data set for these experiments, fo-cusing on FPR intervals of the form [0 ,  X  ] for different values of  X  . Figure 4 shows (a) the average CPU time taken by the routine for finding the most violated con-straint (MVC) for different values of  X  , and (b) the average number of calls to this routine. As can be seen, the average time taken to find the most violated constraint is similar for all values of  X  , demonstrating as shown in Section 4 that the time complexity of this procedure for partial AUC is the same as that for full AUC (  X  = 1); on the other hand, the average number of calls to this procedure increases as  X  decreases, i.e. as the FPR interval becomes smaller. The partial AUC is increasingly used as a perfor-mance measure in several machine learning applica-tions. We have developed a structural SVM based method, termed SVM pAUC , for optimizing the par-tial AUC between any two given false positive rates with similar computational complexity as that re-quired for optimizing the usual AUC. Our empirical evaluations on several real-world tasks indicate the proposed method indeed optimizes partial AUC in the desired false positive range, performing comparable to or better than existing baseline techniques.
 Acknowledgments. Thanks to the anonymous re-viewers for helpful comments. HN thanks Microsoft Research India for a partial travel grant to attend the conference. This work is supported in part by a Ra-manujan Fellowship from DST to SA.

