 Question answering (QA) helps one go beyond traditional keywords-based querying and re trieve information in more precise form than given by a document or a list of documents. Several community-based QA (CQA) services have emerged allowing information seekers pose their information need as questions and receive answers from their fellow users. A question may receive multiple answers from multiple users and the asker or the community can choose the best answer. While the asker can thus indicate if he was satisfied with the information he received, there is no clear way of evaluating the quality of that information. We present a study to evaluate and predict the quality of an answer in a CQA setting. We chose Yahoo! Answers as such CQA service and selected a small set of questions, each with at least five answers. We asked Amazon Mechanical Turk workers to rate the quality of each answer for a given question based on 13 different criteria. Each answer was rated by five different workers. We then matched their assessments with the actual asker X  X  rating of a given answer. We show that the quality criteria we used faithfully match with asker X  X  perception of a quality answer. We furthered our investigation by extracting various features from questions, answers, and the users who posted them, and training a number of classifiers to select the best answer using those features. We demonstrate a high predictability of our trained models along with the relative mer its of each of the features for such prediction. These models support our argument that in case of CQA, contextual information such as a user X  X  profile, can be critical in evaluating and predicting content quality. H.3: INFORMATION ST ORAGE AND RETRIEVAL H.3.3: Information Search and Retrieval : Search process ; H.3.5: On-line Information Services : Web-based services Experimentation; Human Factors; Measurement. Community question answering; answer quality evaluation and prediction. Community Question Answering (CQA) sites have emerged in the past few years as an enormous market, so to speak, for the fulfillment of information needs. Estimates of the volume of questions answered are difficult to come by, but it is likely that the number of questions answered on CQA sites far exceeds the number of questions answered by library reference services [4], which until recently were one of the few institutional sources for such question answering. CQA sites make their content  X  questions and associated answers submitted on the site  X  available on the open web, and indexable by search engines, thus enabling web users to find answers provided for previously asked questions in response to new queries. Yahoo! Answers 1 and AnswerBag 2 are examples of such CQA services, the popularity of which have been increasing dramatically for the past several years. 3 The fact that CQA sites receive such a high volume of use, and that there is such a large ocean of information needs that may be fulfilled by these sites, makes it critical to establish criteria for evaluating the quality of answers provided by these sites. Li brary reference services have a long tradition of evaluation to establish the degree to which a service is meeting user needs [9]. Such evaluation is no less critical for CQA sites, and perhaps even more so, as these sites do not have a set of professional gui delines and ethics behind them, as library services do. A small but growing body of literature exists on evaluating the quality of answers provided on CQA sites. Some work has been done to predict asker satisfacti on [8], but little work exists on what factors contribute to a quality answer, beyond simply being satisfactory to the asker. As Liu et al. [8] point out, research on CQA draws on research on interactive Question Answering. One significant distinction, however, is that in studying CQA there is an actual user, while the questions used in the TREC QA track, for example, are questions submitted to the FAQ Finder system, and the found answers (by participating sites) are evaluated by trained assessors [15]. It is of course appropriate to reduce the subjectivity of evaluating answers in a large-scale evaluati on of systems like a TREC track. However, in CQA, the subjectivity of relevance assessments is http://answers.yahoo.com http://www.answerbag.com According to a March 2008 Hitwise report (http://www.hitwise.com/press-center/hitwiseHS2004/question-and-answer-websites.php), visits to CQA websites have increased 118% year-over-year since 2006. central. Furthermore, in evaluations of CQA, it becomes possible to glean relevance assessments from actual users  X  rather than trained assessors  X  possibly ev en the asker X  X  own relevance assessments. Beyond the benefit to the user of having better metrics for evaluating the quality of answers, it would benefit the management of CQA sites themselves to have such metrics. Many CQA sites have implemented reputation systems, where answerers can gain points, or advance levels, based on their participation on the site: number of questions answered, number of answers voted as best answers, etc. Having metrics for evaluating the quality of an answer would enable CQA sites to add this as a factor in building an answerer X  X  reputation. A reputation system that incorporated such quality metrics would also benefit askers: an asker coul d view the profiles of answerers who provided answers to his questi on, and see the quality of past answers provided by that answerer. Some researchers suggest that the context that gives rise to an information need is unique for every individual, and that an answer that is useful to an indi vidual in a particular context will therefore be at best only partially useful to others in other contexts [1]. Identifying the factors that contribute to the quality of answers would therefore be critical for informing web users in deciding if a previously provided answer is appropriate for answering their question. In this paper we present a novel approach to measuring the quality of answers on CQA sites, and use it to predict which of the answers to a given question the aske r will pick as the best answer. The specific problem of answer quality prediction is defined in Section 3. This is followed by our two approaches to measure the quality of the answers. In Secti on 4, we discuss how we used human assessment of different aspects of answer quality, and in Section 5, we show how we used automatically extracted features for measuring and predicting quality of answers. An overview of some of the related work is provided in the following section. Community Question Answering, according to Shah et al. [13], consists of three components: a mechanism for users to submit questions in natural language, a ve nue for users to submit answers to questions, and a community built around this exchange. Viewed in that light, online communities have performed a question answering function perhaps since the advent of Usenet and Bulletin Board Systems, so in one sense CQA is nothing new. Websites dedicated to CQA, however, have emerged on the web only within the past few years: the first CQA site was the Korean Naver Knowledge iN, launched in 2002, while the first English-language CQA site was Yahoo! Answers, launched in 2005. Despite this short history, however, CQA has already attracted a great deal of attention from res earchers investigating information seeking behaviors [5], selection of resources [3], social annotations [2], user motivations [12], comparisons with other types of question answering servi ces [14], and a range of other information-related behaviors. While CQA sites are new, they have been in existence long enough for product differentiation to begin to occur. Many CQA sites allow questions to be submitted on any topic: for example, Yahoo! Answers, WikiAnswers, An swerBag, and others. Several CQA sites, however, restrict their scope in a variety of ways. Some sites are subject-specific, such as Stack Overflow, which limits its scope to questions about programming, 4 and Math Overflow, which limits its scope to research level math questions. 5 Some sites serve a specific user community, such as HeadHunterIQ, which targets business recruiters. 6 Some sites answer only specific types of questions, such as Homework Hub, which is limited to homework help. 7 From the standpoint of user satisfaction  X  with both the answer and the site  X  it would be a benefit to CQA sites for there to be a mechanism to triage questions between sites. The t opic of a question would obviously be a factor in such a mechanism, but other factors in evaluating the quality of answers provided on the site could also be valuable for this purpose. This sort of triage is comparatively simple for a human to perform  X  and while time-consuming, is in fact commonly performed by librarians for digital reference se rvices [9,10]. The QuestionPoint reference service, 8 an application for managing a global collaborative of library reference se rvices, also performs this sort of triage automatically, by matching questions to profiles of individual libraries [6]. The level of complexity that such triage systems are currently capable of, however, pale in comparison to the complexity that a human triage can manage. Both triage and the evaluation of the quality of answers are, of course, largely classification pr oblems: in triage, the question must be placed in one and only one  X  X in X  corresponding to a specific answering service, while in evaluation it must be determined if the question does or does not meet an evaluation criterion. Most classification tasks are sufficiently complex that, by and large, humans are able to perform them more effectively  X  if not efficiently  X  than machine classification. To improve machine classification, of course, a sufficiently large dataset and a sufficiently rich set of factors are needed. The work reported here investigates two methods for developi ng sets of factors: first, a set of 13 quality metrics identified from the literature on CQA are used for human quality assessments, then a set of features are automatically extracted. CQA presents interesting problems for automatically extracting criteria to guide classification. Automatic classification must of course make use of a corpus of some kind. For CQA, this corpus consists of the questions and th eir associated answers submitted on the site, and could also include ot her data such as user profiles, answer ratings, etc. There is, however, much more going on CQA sites than just answering questi ons and reputation building. CQA sites are communities, however loos ely defined, and so there is a wealth of contextual informati on about that community, and the interpersonal dynamics within it, that could be utilized, if only we understood how to make use of it. In an article mapping out a research agenda for CQA, Shah et al. [13] identify two primary threads in this work: research on the content of CQA sites, which incl udes analyses of the content and quality of the questions and answers, and research on the community around these sites, wh ich includes analyses of the reputation systems and social nor ms on these sites. While, of http://www.stackoverflow.com http://www.mathoverflow.net http://www.headhunteriq.com http://www.stackexchangesites.com/homework-hub http://questionpoint.org course, the content of a CQA site would not exist without the community, here we focus only on the former. CQA sites may be mined to identify the information needs within a specific domain [7], but Yahoo! Answers (YA) enables questions to be asked on nearly any possible topic. This broad diversity of topics, as well as the diversity of users asking questions, makes evaluating answer s challenging. Indeed, Shah et al. [12] found that while 95% of questions that they investigated on YA received at least one answer, the quality of those answers varied widely, from correct answers to partial and unhelpful answers, to spam. Further, as in any instance of information retrieval or question answering, the quality of the answer is heavily dependent user-specific factors. One of the difficulties of studying CQA, however, is the difficulty  X  sometimes the impossibility  X  of gaining access to the members of the community themselves, askers or answerers. As a result, a range of approaches has emerged for making use of proxies for the askers themselves. Unable to contact the askers directly, Kim et al. [5] analyzed the comments provided by askers when selecting a best answer. Others have used third parties to stand in for askers: Harper et al. [3] used undergraduate students as proxies for askers, while Liu et al. [8] used both subject experts and paid workers from Amazon Mechanical Turk. 9 Both of these approaches have clear advantages: the former makes use of the asker X  X  own words and evaluation cr iteria, while the latter is able to collect more detailed evaluative data. The study presented here makes use of the latter approach. A range of approaches has also emerged for developing evaluation criteria used in studies of CQA. Liu et al. [8] had proxies evaluate answers according to the same 5-star rating system available to the asker. Kim et al. [5] let evaluation criteria emerge from the comments provided by askers when selecting a best answer. Harper et al. [3] had proxies evaluate answers according to criteria derived from the evaluation of library reference services. Perhaps the most fully developed set of evaluation criteria for answers in CQA, however, was proposed by Zhu et al. [16], who identified a set of 13 criteria from both CQA sites X  guidelines for answer ers, and the comments provided by the community on questions and answers. For our work reported here, we will use thes e 13 criteria when asking human assessors to judge the quality of an answer. The quality of an answer, or of any information content for that matter, can be subjective. A quality assessment may depend on the relevance of that content, among other factors, and relevance itself is difficult to measure in the context of CQA. We, therefore, provide our own interpretation of quality with respect to the data and the task we have on our hand. On YA, a question is considered to be resolved if either the community votes and selects one of the answers to be the best, or the asker himself chooses one as the best answer from the set of answers he received for his questi on. It is possible that multiple answers are of high quality, but only one of them gets picked as the best answer. Liu et al. [8] considered the act of an asker http://www.mturk.com choosing one as the best answer an indication of satisfaction. If the asker does not select any answer as the best one, and/or if the community votes for the best answer, the asker is assumed to be unsatisfied. For the work reported here, we will follow this notion of asker satisfaction. To extend it to indicate the quality of an answer, we will add another constrai nt that the asker has to give the chosen answer a rating of at least 3 out of 5. Thus, we consider an answer to be a hi gh quality answer, if (1) the asker chose it as the best answer, and (2 ) gave it a rating of at least 3. Given this, we define the problem of answer quality prediction to be one where we need to predict if a given answer will be selected by the asker as a high quality answ er. Thus, the goal of our work is to predict if an answer was chosen by the asker as the best answer with a high rating. In order to do that, we will evaluate each answer X  X  quality according to several measures. First, we will use the 13 criteria used by Zhu et al. [16] as different aspects of answer quality. Next, we will extract a number of features from questions, answers, and the profile s of their posters as a way to measure answer quality. For both of these approaches, we will demonstrate how we extracted necessary features and constructed fairly reliable models, and used these models to classify an answer to be in the  X  X es X  class (chosen as the best), or in the  X  X o X  class (not chosen as the best). In this section we will describe how we used the quality assessment data obtained from Amazon Mechanical Turk (MTurk) human assessors (also known as Turk workers) to build a regression model for predicting answer quality. It is important to note that in a strict sense, we are not actually trying to evaluate the quality of answers. Rather, we are trying to predict if a given answer will be selected by the asker as the best answer or not. It is possible that an answer is of good quality, but was not chosen by the asker as the best answer. Conve rsely, an asker could select an answer as the best that another (a librarian or a subject expert, for example) would not have evaluated highly. But for the purpose of our work here, we will assume that an answer declared as the best answer by the asker with a rati ng of at least 3 is a high quality answer. Yahoo! Research makes several da tasets available to university-affiliated researchers, through their Webscope X  Program. of these datasets is Yahoo! Quest, which consists of the full text of approximately 8 million questions from Yahoo! Answers, the full text of all answers to each question, which was voted the best answer, the subject of the question as chosen by the asker, and other associated data. In the Yahoo! Quest dataset, questions are categorized into four broad types: advice, informati onal, opinion, and polling. From this dataset, we randomly sampled 30 questions of each of these four types, for which there were at least five answers. For each question, we picked five answers  X  the answer voted as the best answer, and four others random ly sampled. Thus, our data consisted of 120 questions with 600 answers. Beyond being stratified into the aforementioned f our types, the questions in our http://sandbox.yahoo.com dataset were quite wide-ranging, covering a broad range of subject areas and depth of subject knowledge. As mentioned above, it is often impossible to gather evaluative data about answers from the askers themselves, and that was the case here. Users of YA may create a profile, which may include an email or IM address, but the Yahoo! Quest dataset does not include any user profile data. It was, therefore, impossible to ask the asker how they had evaluated the various answers to their question. As a proxy for the orig inal asker, we therefore used Amazon Mechanical Turk. MTurk has been used successfully to predict asker satisfaction, and i ndeed ratings from Turk workers have been shown to be more cl osely correlated with actual asker satisfaction than ratings from experts [8]. We created a Human Intelligence Task (HIT) on MTurk, in which Turk workers evaluated each of the 600 answers in our dataset according to the 13 quality criteria identified by [16] (discussed above), on a 5-point Likert scale. In particular, we presented a worker with a question-answer pair, and instructed the worker to rate the following 13 statements on scale of 1 to 5. 1. This answer provides enough information for the question. 2. This answer is polite (not offending). ( polite ) 3. This answer completely answers the whole question (rather 4. This is an easy to read answer. ( readable ) 5. This answer is relevant to the question. ( relevant ) 6. The answer is concise (not too wordy). ( brief ) 7. I trust/believe this answer. ( convincing ) 8. The answer contains enough detail . ( detailed ) 9. I believe this answer is original (not copied from another 10. I believe the answer is objective and impartial . ( objective ) 11. The answer has new ideas or concepts that made me 12. This answer is useful or helpful to address the question. 13. I believe this answer has been written by an expert . ( expert ) Since these are subjective criteria, it is likely that different people have different levels of ag reement while rating them. We, therefore, employed five different Turk workers to obtain ratings on each of these statements. T hus, we created a total of 3000 HITs. To reduce the order effect, we randomized the ordering of the above statements for each HIT. In order to ensure that the worker who completed our HIT was an actual human and not a bot, we a dded a final question:  X  X hat is two plus two, X  and provided a text field for the answer. Human workers, of course, had no trouble answering this correctly, while bots supplied nonsensical responses that were easily filtered out. Workers took about 90 seconds on average to work on a HIT (read a question-answer pair and rate 13 different criteria). As described before, each worker on MTurk rated each of the thirteen quality aspects for an answer on scale 1 to 5. To convert this rating to a two-class decision ( X  X es X  or  X  X o X  for an aspect), we assumed that a rating of 3 or higher meant  X  X es X , and a rating of 1 or 2 meant  X  X o X . Thus, for a statement  X  X his answer contains enough details. X , if a worker rated 3 or higher, we considered his decision to be  X  X es X  for the answer containing enough details to address the question. Since we had five different workers rating each answer, we used a simple voting scheme to arrive to their collective decision. We counted the votes for a factor to be  X  X es X  and normalized it by the number of raters (5). Thus, if a factor received 4  X  X es X  decisions, it had 0.8 as its weight. Using such weighted aspects for each answer, we constructed a model for our dataset using logistic regression. In this model, the independent variables were the 13 aspects, and the depe ndent variable was the asker X  X  decision about the quality of an answer. As described earlier, the dataset consisted of 120 questions with total of 600 answers. Once again, an answer was considered to be a high quality answer, if (1) the asker chose it as the best answer , and (2) gave it a rating of 3 or higher. The model constructed with logistic regression is shown in Table 1. As we can see, most factors do not contribute significantly in explaining the variability of the data. In fact, overall, pseudo R reported to be 0.0902 with ( p &gt;  X  2 )=0.0000, which means only about 9% of the variability in the data could be explained by this model. 
Table 1: Logistic regression model for 13 quality factors in This indicates the low quality of th e model. We do see that aspects  X  X ovel X ,  X  X riginal X , and  X  X eadable X  have significant impact on model X  X  ability to predict the best answer. Doing further data mining revealed that losing aspects with high p&gt;|z| values do not affect the performance of this model by mu ch. Removing aspects  X  X onvincing X  and  X  X nformative X  reduced the power of this model by only a little, with pseudo R 2 =0.0896. However, this model is not very appropriate for doing feature selection since all of the aspects are strongly correlated with each other as shown in Table 2. This may also be the reason for the model not being able to explain much of the variability in the data. Applying this model on the same data for testing, we achieved 80.33% classification accuracy. Doing a 10-fold cross-validation with this model resulted in classification accuracy of 79.50%, with given the kind of data we have, where 4 out of 5 answers are not selected as the best answers, we could achieve 80.00% classification accuracy by simply declaring every answer to be not the best answer. It should be noted that in reality, a question on YA may receive many more answers than this, and thus, constructing a classifier to identify the best answer is extremely difficult. Table 2: Pair-wise correlation of aspects. Significant relations Continuing to do data mining with this model to identify relative impact of individual aspects did not provide us any significant results. Any subset of these aspects could achieve a classification accuracy of about 80.00%, but this could simply be by chance given the nature of the data. While the classifier built with this model was not highly successful, we did find the evidence of its correct functioning when we compared the probabilities computed for the instances of both the classes ( X  X es X  and  X  X o X ). Table 3 shows the result of a two-tailed paired t -test between the probability distributions of  X  X es X  class and  X  X o X  class. In the table, they are represented by  X 1 X  and  X 0 X  group respectively. 
Table 3: Two-tailed paired t -test for probability distributions As we can see, the mean probability for  X  X o X  class instances was 0.1734, whereas for  X  X es X  class it was 0.2607. The test shows that the difference in these means was statistically significant. This indicates that even though the probabilities for class  X  X es X  were not higher than 0.5 (for their correct classification), they were significantly higher than those for class  X  X o X . In summary, we learned that it is extremely hard to pick the best answer from a set of answers for a given question. This is due to large variability in the subject and the kind of questions asked, the asker X  X  expectations, and the kind of answers given. In addition to this, only one out of many answers is selected as the best answer where not chosen answers may still be of good quality. We also realized that the 13 aspects of quality that we identified are highly correlated, but do not help us create a robust model for explaining the variability in the data, let alone predicting the best answer. We did, however, find a high leve l of agreement among the workers while rating different aspects of answer quality (Figure 1). This informs us that while people have similar understanding or perception about the quality of an answer, their collective assessment was not enough to predict the decision of the asker. Figure 1: Agreement among 5 different workers while rating aspects of answer quality. This led us to believe that there ma y be other aspects or features that we need to consider. In the ne xt section we describe how we automatically extracted such features and used them for evaluating and predicting answer quality. Now we will describe a set of e xperiments done using automatically extracted features from questions a nd/or answers. As we discussed in the previous section, we may have a common perception of what constitutes to the quality of an answ er, but in case of CQA, there are many other variables that may c ontribute to an asker choosing an answer to be the best answer for his information need. In particular, the profile of the asker as well as the answerer may play an important role in determining if the asker would find an answer appropriate for his posed question or not. In this section we will show how to use such user profile information to build a prediction model. We extracted the following features for each question and answer in our dataset:  X  Length of the question X  X  subject ( qsubject )  X  Length of the question X  X  content ( qcontent )  X  Number of answers for the question ( numanswers )  X  Number of comments for the question ( numcomments )  X  Information from the asker X  X  profile ( q_points, q_level,  X  Length of the answer X  X  content ( acontent )  X  Inclusion of references with the answer ( reference )  X  Reciprocal rank of the answer in the list of answers for the given  X  Information from the answerer X  X  profile ( a_points, a_level, A user X  X  profile (asker or answerer) contained the following information about that user: numbe r of questions asked, number of those questions resolved, number of questions answered, number of those answers chosen as the best answers, level achieved in YA (1 to 7), number of points earned, and number of stars received. Some of the data points had to be deleted because we could not obtain one or more of the features (e.g., a user X  X  profile taken down). This resulted in a total of 116 questions and 575 answers. Table 4 provides a summary of some of these features based on our dataset. 
Table 4: Summary of various features used from questions, questions_resolve d Using these 21 features, we cons tructed a model with logistic regression; similar to what we did with human assessed 13 quality aspects reported earlier. This mode l is presented in Table 5. As we can see, the model is qu ite good in terms of its power to explain the variability in the data. Since pseudo R statistical significance, we could say that more than 15% of the variability in the data can be attri buted to the factors used here. The model was successful 84.17% times in selecting the best answer on the same training set, which was better than the prediction accuracy obtained from human assessments of quality factors. Doing 10-fold cross-validation resulted in 81.74% accuracy, which was once again better than the one achieved using human rated judgments. Similar to the earlier model, we performed a two-tailed paired t -test between the probability distributions of the  X  X es X  or the  X 1 X  class and the  X  X o X  or the  X 0 X  class. The test results are reported in Table 6. Once again, we find that even though the probabilities for class  X  X es X  are not higher than 0.5 on average, they are significantly higher than those of for  X  X o X . We also found that these different features are not all significantly correlated as we found earlier with the quality aspects assessed by human workers. This indicates that not all the features extracted about questions and/or answers are geared toward predicting the quality of an answer. extracted QA features in predicting asker-rated quality of an 
Table 6: Two-tailed paired t -test for probability distributions From Table 5, it appears that num ber of comments, existence of references with an answer, and the stars received by the asker are not helping much in the prediction. Once again, data mining with this model revealed that removing these three features retains the model X  X  ability to select the best answer without much loss (pseudo R 2 =0.1404 and prediction accuracy=83.83%). Extending this further, we could get the most parsimonious model by having a_rr (reciprocal rank of an answer) only in the model (pseudo R =0.0823 and prediction accuracy=80.34%). Not very surprisingly, features extracted from questions only do not help in prediction at all (Table 7, with significance reported to be 0.9594), whereas features extracted from answers only achieve quite high power (Table 8) with pseudo R 2 =0.1386 and statistical significance. Once again, we find that the single most important feature that contributes to predicting if an answer would be selected as the best answer or not is the reciprocal rank of that answer ( a_rr ). Finally, we performed likelihood-ratio test on the models constructed with 13 human-assesse d aspects and 21 automatically extracted features. We found that both the models were significantly different (  X  2 =17.29, p= 0.0040). This shows that the latter model was successful in out performing the former one with statistical significance. extracted question features in predicting asker-rated quality extracted answer features in predicting asker-rated quality of Encouraged by the success of automatically extracted features for predicting answer quality, we extended our experiments to include another set of answers. Using YA APIs, 11 we randomly extracted 5032 answers from YA, associated with 1346 questions in different categories. We extracted all the 21 features reported earlier for each question-answer pair and constructed a logistic regression model. The model had pseudo R 2 =0.1312, and gave 84.72% classification accuracy on the same data (Figure 2). When tried with 10-fold cross-validation, the model gave 84.52% accuracy, showing its robustness for classification. When we used this model that we constructed with 5032 data-points for training and the previous model with 575 data-points for testing, the training model achieved 80.35% accuracy (Figure 3). This, once again, demonstrates the robustness of the selected features and the models constructed using them. http://developer.yahoo.com/answers/ All of the classification results reported in the present and the previous section are summarized in Table 9. It is clear that the models constructed with QA feat ures (automatically extracted) tend to give reliable results on training as well as testing data, indicating their robustness. # Model Training Testing Classification 1 Quality aspects 2 Quality aspects 3 QA features 4 QA features 5 QA features 6 QA features 7 QA features Measuring quality of content in community question-answering (CQA) sites presents a unique set of issues and challenges. As with many other IR problems, evaluating content quality is a significant challenge in CQA, and may require us to go beyond the traditional notion of  X  X elevance X  as suggested by Saracevic [11]. To address this challenge of evaluating answer quality, we used 13 different criteria to assess the overall quality of an answer on Yahoo! Answers (YA) site. We assumed that an answer is the best for a given question, if (1) the answer is chosen by the asker as the best, and (2) the asker gives it a rating of 3 or higher. This gave us a gold standard against which we could compare our models for evaluating and predicting answer quality. With the help of Amazon Mechanical Turk workers, we discovered th at people have a good understanding and high level of agreement over what constitutes as a good quality answer. We also found that different aspects of the overall quality of an answer, such as informativene ss, completeness, novelty, etc., are highly correlated. However, when these features were used for creating a model for predicting the best quality answers, we found it limiting. The human assessors were given the answers without any context (other than the question) ; they did not know who asked the question or who answered it, nor did they have any information about the conditions under which a question was asked or an answer provided. We realized that such information is critical in evaluating content quality in CQA. We, therefore, extracted several features of the questions, the answers, and the users who provided them from YA. Via model building and classifying experiments, we discovered that indeed, the answerer X  X  profile, as measured by the points earned on YA (social capital in that community), and the or der of the answer in the list of answers for a given question, as m easured by the reciprocal rank of that answer, are the most significant features for predicting the best quality answers. We demonstrated the robustness of our models by doing cross-validations and applying the trained models to a larger test set. Beyond developing models to select best answers and evaluate the quality of answers, there are several important lessons to learn here for measuring content quality in CQA. It was pointed out, above, that there is huge variety in the kind of questions and answers found on CQA services, and that a given question may receive several answers from the community. 12 Given that only one of these answers can be picked by the asker as the best answer, it is extremely hard to create a cla ssifier that outperforms random selection, based on a priori information about the data. However, with appropriate features, we c ould build models that can have significantly higher probability of identifying the best answer in the  X  X es X  class than of classifying a non-best answer in that class. CQA provides unique opportunities to consider social factors and other contextual information, wh ile evaluating its content. For instance, the placement of information, as well as the profile or social capital of its producer are some of the data that may help in better prediction and evaluation of content quality. A great many CQA sites exist, with different asking, answering, and rating mechanisms; and we believe that there are several other possible features of CQA questions and an swers worth exploring. Future work will benefit from the unique issues presented here while evaluating and predicting content quality in CQA. We are grateful to anonymous wo rkers of Amazon Mechanical Turk service for providing us valuab le human assessments for the answers that we used here. We are also thankful to Yahoo! for making their QA datasets available to us. [1] Dervin, B. (1998). Sense-making theory and practice: An [2] Gazan, R. (2008). Social annotations in digital library [3] Harper, M. F., Raban, D. R., Rafaeli, S., &amp; Konstan, J. K. Using more than a million questions from YA, we have found that on average a question receives about 6 answers. [4] Janes, J. (2003). The Global Cens us of Digital Reference. In [5] Kim, S., Oh, J-S., &amp; Oh, S. (2007). Best-Answer Selection [6] Kresh, D. N. (2000). Offering High Quality Reference [7] Lee, J. H., Downie, J. S., &amp; Cunningham, S. J. (2005). [8] Liu, Y., Bian, J., &amp; Agichtein, E. (2008). Predicting [9] Pomerantz, J. (2008). Evaluation of Online Reference [10] Pomerantz, J., Nicholson, S., Belanger, Y., &amp; Lankes, R. D. [11] Saracevic, T. (1995). Evaluation of evaluation in information [12] Shah, C., Oh, J. S., &amp; Oh, S. (2008). Exploring [13] Shah, C., Oh, S., &amp; Oh, J-S. (2009). Research Agenda for [14] Su, Q., Pavlov, D., Chow, J., &amp; Baker, W. (2007). Internet-[15] Voorhees, E. M (2003). Overview of the TREC 2003 [16] Zhu, Z., Bernhard, D., &amp; Gurevych, I. (2009). A Multi-
