 An important task in text mining is t he automatic extraction of entities and their lexical relations ; this has wide applications in natural language pr o-cessing and web search. This paper focuses on mining the hyponymy (or is -a) relation from large -scale, open -domain web documents. From the viewpoint of entity classification, the problem is to automatically assign fine -grained class labels to terms .

There have been a number of approaches (Hearst 1992; Pantel &amp; Ravichandran 2004 ; Snow et al., 2005 ; Durme &amp; Pasca, 2008 ; Talukdar et al., 2008 ) to address the problem . These methods typ i-cally exploit ed manual ly -designed or automatica l-ly -learned patterns (e.g.,  X  NP such as NP  X  ,  X  NP like NP  X ,  X  NP is a NP  X ) . Although some degree of success has been achieve d with these efforts, the results are still far from perfect , in terms of both recall and precision . As will be demonstrated in th is paper, even b y processing a large corpus of 500 million web pages with the most popular pa t-terns, we are not able to extract correct labels for many (especially rare) entities. Even for popular terms, incorrect results often appear in their label list s .

The basic philosophy in existing hyponymy e x-traction approaches (and also many other text -mining methods) is counting : count the number of supporting sentences . Here a support ing sentence of a term -label pair is a sentence from which the pair can be extrac ted via an extraction pattern. We demonstrate that the specific way of count ing has a great impact on result quality , and that the state -of -the -art counting method s are not optimal . Specif i-cally, we examine the problem from the viewpoint of probabilistic evidence combination and find that the probabili stic assumption behind simple coun t-ing is the statistical independence between the o b-servations of supporting sentences . By assuming a positive correlation between su pporting sentence observations and adoptin g properly designed no n-linear combination functions, the results precision can be improved .

It is hard to extract correct labels for rare terms from a web corpus due to the data spars eness pro b-lem . To address this issue, we propose an ev idence propagation algorithm motivated by the observ a-tion that similar terms tend t o share common h y-pernyms . For example, if we already know that 1) Helsinki and Tampere are cit ies , and 2) Porvoo is similar to Helsinki and Tampere , then Porvoo is very likely also a city. This intuition , however, does not mean that the labels of a term can always be transferred to its similar terms. For example, Mount Vesuvius and Kilimanjaro are volcano es and Lhotse is similar to them , but Lhotse is not a volcano . Therefore we should be very conservative and careful in hypernym propagation. In our pro p-agation algorithm, we first construct some pseudo supporting sentences for a term from the suppor t-ing sentences of its similar terms . Then we calc u-late label score s for terms by p erforming nonlinear evidence combination based on the (pseudo and real) supporting sentences. Such a nonlinear pro p-agation algorithm is demonstrated to perform be t-ter than linear propagation.

Experimental results on a public ly available co l-lection of 500 million web pages with hypernym labels annotated for 3 00 terms show that our no n-linear evidence fusion and propagation significan t-ly im prove the precision and coverage of the extracted hyponymy data . This is one of the tec h-nologies adopted in our semantic search and mi n-ing system NeedleSeek 2 .

In the next section, we discuss major related e f-forts and how they differ from our work . Section 3 is a brief description of the baseline approach. The probabilistic evidence combination model that we exploited is introduced in Section 4. Our main a p-proach is illustrated in Section 5 . Section 6 shows our experimental settings and results. Finally, Se c-tion 7 concludes this paper. Existing efforts for hyponymy relation extraction ha ve bee n conducted upon various types of data sources, including plain -text corpora (Hearst 1992; Pantel &amp; Ravichandran , 2004 ; Snow et al., 2005; Snow et al., 2006; Banko , et al., 200 7 ; Durme &amp; Pasca, 2008 ; Talukdar et al., 2008 ) , semi -structured web pages ( Cafarella et al., 2008; Shi n-zato &amp; Torisawa , 2004 ) , web search results ( Geraci et al., 2006 ; Kozareva et al., 2008; Wang &amp; Cohen, 2009 ) , and query logs (Pasca 2010) . Our target for optimization in this paper is the approaches that use lexico -syntactic pat terns to extract hyponymy relations from plain -text corpora. O ur future work will study the application of the proposed alg o-rithms on other types of approaches.
The probabilistic evidence combination model that we exploit here was first proposed in (Shi et al., 2009) , for com bining the page in -link evidence in building a nonlinear static -rank computation algorithm. We applied it to the hyponymy extra c-tion problem because the model takes the depen d-ency between supporting sentences into consideration and the resultant evidence fusion formulas are quite simple . In (Snow et al., 2006) , a probabilistic model was adopted to combine ev i-dence from heterogeneous relationships to jointly optimize the relationships . The independen ce of evidence was assumed in their model. In compar i-son , we show that better results will be obtained if the evidence correlation is modeled appropriately .
Our evidence propagation is basically about u s-ing term similarity information to help instance labeli ng. There have been several approaches which improv e hyponymy extraction with instance clusters built by distributional similarity. In ( Pantel &amp; Ravichandran , 2004) , labels were assigned to the committee (i.e., representative members) of a semantic class a nd used as the hypernyms of the whole class. Labels g enerated by their approach tend to be rather coarse -grained , excluding the po s-sibility of a term having its private labels (consi d-ering the case that one meaning of a term is not covered by the input semantic classes ). In contrast to their method , our label scoring and ranking a p-proach is applied to every single term rather than a semantic class . In addition, we also compute label scores in a nonlinear way , which improves results quality. In Snow et al . (2005), a supervised a p-proach was pro po sed to improve hypernym class i-fication using coordinate terms. In comparison, our approach is unsupervised. Durme &amp; Pas ca ( 2008) c lean ed the set of in stance -la bel pairs with a TF*IDF like method, by exploiting clust ers of s e-mantically related phrases. The core idea is to keep a term -label pair ( T , L ) only if the number of terms having the label L in the term T  X  s cluster is above a threshold and if L is not the label of too many clu s-ters (otherwise the pair will be discarded). In co n-trast , we are able to add new ( high -quality ) labels for a term with our evidence propagation method . On the other hand, low quality labels get smaller score gain s via propagation and are ranked lower.
L abel propagation is performed in ( Talukdar et al., 2008; Talukdar &amp; Pereira , 2010 ) based on mu l-tiple instance -label graphs . T erm similarity info r-mation was not used in their approach .
Most existing work t end s to utilize small -scale or private corpora , whereas t he corpus that we used is publicly available and much larger than most of the exist ing work . We publish ed our term set s (r e-fer to Section 6.1) and the ir corresponding user judgments so researchers work ing on similar topic s can reproduce our result s .
 Table 1 . Patterns adopted in this paper (NP: named Th e problem address ed in this paper is corpus -based is -a relation mining: extracting hypernyms (as labels) for entities from a large -scale, open -domain document corpus . The desired output is a mapping from terms to their corresponding hype r-nyms, which can naturally be represented as a weighted bipartite graph ( term -labe l graph) . Typ i-cally we are only interested in top labels of a term in the graph .

Following exi s ting efforts, we adopt pattern -matching as a basic way of extracting hype r-nym y /hyponym y relations . Two types of patterns (refer to Table 1) are employed , including the po p-ular  X  Hea r st patterns X  (Hearst, 1992) and the IsA patterns which are exploited less frequently in e x-isting hyponym mining efforts . O ne or more term -label pair s can be extracted if a pattern matches a sentence. In the baseline approach, t he weight of an edge T  X  L (from term T to hypernym label L ) in the term -label graph is computed as, where m is the number of times the pair ( T , L ) is extracted from the corpus , DF( L ) is the number of in -links of L in the graph , N is total number of terms in the graph, and IDF means the  X  inverse document frequency  X  .

A term can only keep its top -k neighbors (a c-cording to the edge weight) in the graph as its final labels.

Our pattern matching algorithm implemented in this paper uses part -of -speech (POS) tagging i n-formation , without adopting a parser or a chunker . The noun phrase boundaries (for terms and labels) are determined by a manually designed POS tag list . Here we model the hyponymy extraction problem from the probability theory point of view , aiming at estimat ing the score of a term -label pair (i.e., the score of a label w.r.t. a term) with probabili stic evidence combination . The model was studied in (Shi et al., 2009) to combine the page in -link ev i-dence in build ing a nonlinear static -rank comput a-tion algorithm .

We represent the score of a term -label pair by the probability o f the label being a correct hype r-nym of the term , and define the following events ,
A T,L : Label L is a hypernym of term T (t he a b-breviated form A is used in this paper unless it is ambiguous ) .

E i : The o bservation that ( T , L ) is extracted from a sentence S i via pattern matching (i.e., S i is a su p-porting sentence of the pair) .

Assuming that we already know m supporting sentences ( S 1 ~ S m ) , o ur problem is to compute a hypernym of term T , given evidence E 1 ~ E m . Formally, we need to fin d a function f to satisfy,
For simplicity, we first consider the case of m =2. The case of m &gt;2 is quite similar.

W e start from the simple case of independent supporting sentences . That is,
By applying Bayes rule, we get,
Then define
Here G ( A | E ) represents the log -probability -gain of A given E , with the meaning of the gain in the log -probability value of A after the evidence E is observed (or known) . It is a measure of the impact of evidence E to the probability of event A . With the definition of G ( A | E ), Formula 4. 4 can be tran s-formed to,
Therefore, if E 1 and E 2 are independent , the log -probability -gain of A given both pieces of evidence will exactly be the sum of the gains of A given ev e-ry single piece of evidence respectively. It is easy to prove (by following a similar procedure) that the above Formula holds for the case of m &gt;2 , as long as the pieces of evidence are mutually independent .
Therefore f or a term -label pair with m mutually independent supporting sentences, if we set every gain G ( A | E i ) to be a constant value g , the posterior gain score of the pair will be  X  . If the value g is the IDF of label L , the posterior gain will be,
This is exactly the Formula 3.1. By this way, we provide a probabil istic explanation of scoring the candidate labels for a term via simple counting . Table 2 . Evidence dependency estimation for intra -
In the above analysis, we assume t he statistical independence of the supporting sentence observ a-tions, which may not hold in reality. Intuitively, i f we already know one supporting sentence S 1 for a term -label pair ( T , L ), then we have more chance to find another supporting sentence than if we do not know S 1 . The reason is that, before we find S 1 , we have to estimat e the probability with the chance of discovering a supporting sentence for a random term -label pair. The probability is quite low b e-cause most term -label pairs do not have hyponymy relations. Once we have observed S 1 , however, the chance of ( T , L ) having a hyponymy relation i n-creases. Therefore the chance of observing another supporting sentence becomes larger than before.
Table 2 shows the rough estimation of as R ) , and their ratio s . The statistics are obtained by performing maximal likelihood estimation (MLE) upon our corpus and a random selection of term -label pairs from our term sets (see Section 6.1) together with their top labels 3 . The data ver i-f ies our analysis about the correlation between E 1 and E 2 (note that R =1 means independent). In add i-tion, it can be seen that the conditional indepen d-ence assumption of Formula 4.3 does not hold (because R A &gt;1). It is hence necessary t o consider the correlation between supporting sentences in the model. The esti mation of Table 2 also indicate s that, By following a similar procedure as above , with Formula s 4.2 and 4.3 replaced by 4.7, we have ,
This formula indicates that when the supporting sentences are positively correlated, the posterior score of label L w.r.t. term T ( given both the se n-tences ) is smaller than the sum of the gains caused by one sentence only. In the extreme case that se n-tence S 2 fully depends on E 1 (i.e. P ( E 2 | E 1 )=1), it is easy to prove that
It is reasonable, since event E 2 does not bring in more information than E 1 .

Formula 4. 8 cannot be used directly for comp u-ting the posterior gain. What we really need is a function h satisfying a nd
Shi et al. (2009) discussed other constraints to h and suggested the following nonlinear functions,
In the next section, w e use the above two h fun c-tions as basic building block s to compute label scores for terms . Multiple types of patterns (Table 1) can be adopted to extract term -label pairs. For two supporting se n-tence s the correlation between them may depend on whether they correspond to the same pattern . In Section 5.1, o ur nonlinear evidence fusion form u-las are constructed by making specific assumptions about the correlation between intra -pattern su p-por t ing sentences and inter -pattern ones .
Then in Section 5.2, we introduce our evidence propagation technique in which the evidence of a (T, L) pair is propagated t o the terms similar to T . 5.1 Nonlinear evidence fusion For a term -label pair ( T , L ), a ssuming K patterns are used for hyponymy extraction and the suppor t-ing sentences discovered with pattern i are, where m i is the number of supporting sentences corresponding to pattern i . Also assume the gain 
Generally speaking, supporting sentences corr e-sponding to the same pattern typically have a hig h-er correlation than the sen tences corresponding to different patterns . This can be verified by the data in Table -2. By ignoring the inter -pattern correl a-tions , we make the following simplified assum p-tion :
Assumption : Supporting sentences correspon d-ing to the same pattern are correlated, while those of different patterns are indep en dent .

According to this assumption, our label -scor ing function is,
In the simple case that ( ) , if the h function of Formula 4.12 is adopted, then,
We use an example to illustrate the above fo r-mula.

Example : For term T and label L 1 , assume the numbers of the supporting sentences corresponding 4 ), which means the number of supporting se n-tences discovered by each pattern type is 4. Also assume the supporting -sentence -count vector of label L 2 is ( 25 , 0, 0, 0, 0, 0) . If we use Formula 5.3 to compute the scores of L 1 and L 2 , we can have the following (ignoring IDF for simplicity ) , Score ( L 1 )  X  ; Score ( L 2 )  X 
One the other hand, i f we simply count the total number of supporting sentences , the score of L 2 will be larger .

The rationale implied in the formula is: For a given term T , the labels supported by multiple types of patterns tend to be more reliable than those supported by a single pattern type, if they have the same number of supporting sentences. 5.2 Evidence p ropagation According to the evidence fusion algorithm d e-scribed above, in order to extract term labels reli a-bly , it is desirable to have many supporting sentences of different types. This is a big challenge for rare terms, due to their low frequency in se n-tences (and even lower frequency in supporting sentences because not all occurrences can be co v-ered by patterns). With evidence propagation, we aim at discovering more supporting sentences for terms (especially rare terms). Evidence propag a-tion is motivated by the followin g two observ a-tions: (I ) S imilar entities or coordinate terms tend to share some common hypernyms . (II ) Large term similarity graphs are able to be built efficiently with state -of -the -art techniques ( Agirre et al., 2009 ; Pantel et al., 2009 ; Shi et al., 201 0 ) . With the graphs, we can obtain the similar i-ty between two terms without their hypernyms b e-ing available.

The first o bservation motivates us to  X  borrow  X  the supporting sentences from other terms as auxi l-iary evidence of the term. The second observation means that new information is brought with the state -of -the -art term similarity graphs (in addition to the ter m -label information discovered with the patterns of Table 1).
Our evidence propagation algorithm contains two phases . In ph ase I , some pseudo supporting sentences are constructed for a term from the su p-porting sentences of its neighbors in the similarity graph. Then we calculate the label score s for terms based on their (pseudo and real) supporting se n-tences.

Phase I : For ever y supporting sentence S and every similar term T 1 of the term T , add a pseudo supporting sentence S 1 for T 1 , with the gain score, where is the propagation factor , and ues in [0, 1] . The formula reasonably assumes that the gain score of the pseudo supporting sentence depends on the gain score of the original real su p-porting sentence , the similarity between the two terms, and the propagation factor .

Phase I I : The nonlinear evidence combination formulas in the previous subsection are adopted to c ombine the evidence of pseudo supporting se n-tences.

Term similarity graphs can be obtained by di s-tributional similarity or patterns ( Agirre et al., 200 9 ; Pantel et al., 2009; Shi et al., 2010). We call the first type of graph DS and the second type PB . DS approaches are based on the distributional h y-pothesis (Harris, 1985), which says that terms a p-pearing in analogous contexts tend to be similar. In a DS approach, a term is represented by a feature vector, with each feature corresponding to a co n-text in which the term appears. The similarity b e-tween two terms is computed as the similarity between their corresponding feature vectors. In PB approaches, a li st of carefully -designed (or aut o-matically learned) patterns is exploited and applied to a text collection, with the hypothesis that the terms extracted by applying each of the patterns to a specific piece of text tend to be similar. Two ca t-egories of patt erns have been studied in the liter a-ture (Heast 1992; Pasca 2004; Kozareva et al., 2008; Zhang et al., 2009) : sentence lexical patterns, and HTML tag patterns. An example of sentence lexical patterns is  X  T {, T}*{,} (and|or) T  X  . HTML tag patterns include H TML tables , drop -down lists , and other tag repeat patterns . In this paper, we generate the DS and PB graphs by adopting the best -performed methods studied in (Shi et al., 2010) . We will compare , by experiments, the pro p-agation performance of utilizing the two categories of graphs, and also investigate the performance of utilizing both graphs for evidence propagation. 6.1 Experimental setup Corpus We adopt a public ly available dataset in our experi ments: ClueWeb09 4 . This is a very large dataset collected by Carnegie Mellon University in early 2009 and has been used by several tracks of the Text Retrieval Conference (TREC) 5 . The whole dataset consists of 1.04 billion web pages in ten languages while only those in En glish, about 500 million pages, are used in our experiments. The reason for selecting such a dataset is twofold: First, it is a corpus large enough for conducting web -scale experiments and getting meaningful results. Second, since it is publicly available, it is possible for other researchers to reproduce the experiments in this paper.

Term sets Approaches are evaluated by using two sets of selected terms : Wiki200 , and Ext 100 . For every term in the term sets, e ach approach generates a list of hypernym labels , which are manually judged by human annotators . Wiki200 is constructed by first randomly selecting 400 Wi k-ipedia 6 titles as our candidate terms , with the pro b-ability of a title T being selected to be ( ( ) ) , where F ( T ) is the frequency of T in our data corpus . The reason of adopting such a probability formula is to balance popular terms and rare ones in our term set. Then 200 terms are manually s e-lected from the 400 candidate terms, with the pri n-ciple of maximizing the diversity of terms in terms of length (i.e., number of words) and type (person, location, organization, software, movie, song, an i-mal, plant, etc. ) . Wiki200 is further divided into two subsets: Wiki 1 00H and Wiki 100L , containing respectively the 100 high -frequency and low -freque ncy terms . Ext100 is built by first selecting 200 non -Wikipedia -title terms at random from the term -label graph generated by the baseline a p-proach (Formula 3.1) , then manually selecting 100 terms .
 Some sample terms in the term sets are list e d in Table 3 .
Annotation For each term in the term set, the top -5 results (i.e., hypernym labels) of various methods are mixed and judged by human annot a-tors . Each annotator assigns each result item a judgment of  X  X ood X ,  X  X air X  or  X  X ad X . T he annot a-tor s do not know the method by which a result item is generated . Six annotators participated in the l a-beling with a rough speed of 15 minutes per term . We also encourage the annotators to add new good results which are not discovered by any method .
The term sets and their corresponding user ann o-tations are available for download at the following links (dataset ID=data.queryset.semcat01):
Evaluation We adopt the following metrics to evaluate the hypernym list of a term generated by each method . The evaluation score on a term set is the average over all the terms .

Precision@ k : The percentage of relevant (good or fair) labels in the top -k results (labels judged as  X  X air X  are counted as 0.5)
Recall@ k : The ratio of relevant labels in the top -k results to the total number of relev ant labels
R -Precision : Precision@R where R is the total number of labels judged as  X  X ood X 
Mean average precision ( MAP ) : The average of precision values at the positions of all good or fair results
Before annotation and evaluation, the hypernym list g enera ted by each method for each term is pr e-processed to remove duplicate items . Two hype r-nyms are called duplicate items if they share the same head word (e.g.,  X  military conflict  X  and  X  co n-flict  X  ) . For dupli cate hypernyms , only the first (i.e., the highest ranked one) in the list is kept. The goal with such a preprocessing step is to partially co n-sider results diversity in evaluation and to make a more meaningful comparison among different methods . Consider two hypernym lists for  X  su b-way  X  :
There are more detailed hypernyms in the first list about  X  s ubway  X  as a restaurant or a franchise; while the second list covers a broader range of meanings for the term. It is hard to say which is better (without considering the upper -layer appl i-c a tions) . With this preprocessing step, we keep our focus on short hypernyms rather than detailed ones.
Table 4 . Performance comparison among various evidence fusion methods ( Term sets: Wiki200 and 6.2 Experimental results We first compare the evaluation results of different evidence fusion methods mentioned in Section 4.1. In Table 4 , Linear means that Formula 3.1 is used to calculate label scores, whereas Log and PNorm represent our nonlinear approach with Formulas 4.11 and 4.12 being utilized. The performance i m-provement numbers shown in the table are based on the linear version ; and the up ward pointing a r-rows indicate relative percentage improvement over the baseline. From the table, we can see that the nonlinear methods outperform the linear one s on the Wiki200 term set. It is interesting to note that the performance improvement is more si gnif i-cant on Wiki100H, the set of high frequency terms. By examining the labels and supporting sentences for the terms in each term set, we find that for many low -frequency terms (in Wiki100L) , there are only a few supporting sentences (corresponding to on e or two patterns). So the scores computed by various fusion algorithms tend to be similar. In contrast, more supporting sentences can be disco v-ered for high -frequency terms. Much information is contained in the sentences about the hypernyms of the high -fr equency terms, but the linear function of Formula 3.1 fails to make effective use of it. The two nonlinear methods achieve better perfo r-mance by appropriately modeling the dependency between supporting sentences and computing the log -probability gain in a better way.

The comparison of the linear and nonlinear methods on the Ext100 term set is shown in Table 5 . Please note that t he terms in Ext100 do not a p-pear in Wikipedia titles . Thanks to the scale of the data corpus we are using, even the baseline a p-proach achieve s reasonably good performance . Please note that the terms (r efer to Table 3) we are using are  X  X arder X  than those adopted for evalu a-tion in many existing papers. Again, the results quality is improved with the nonlinear methods , although the performance improvement is not big due to the reason that most terms in Ext100 are rare . Please note that the recall (R@1, R@5) in this paper is pseudo -recall, i.e., we treat the number of known relevant (Good or Fair) results as the total number of rele vant ones .

Table 5 . Performance comparison among various evidence fusion methods ( Term set: Ext100 ; p =2 to the degree of correlations among supporting se n tences . The linear method of Formula 3.1 corr e-sponds to t he special case of p =1 ; while p = re p-resents the case that other supporting sentences are fully correlated to the supporting sentence with the maximal log -probability gain . Figure 1 shows that, f or most of the term sets, the best performance is o b tained for [2.0, 4.0]. The reason may be that the sentence correlation s are better estimated with p values in this range.

Figure 1 . Performance curve s of PNorm with di f-
The experimental results of evidence propag a-tion are shown in Table 6 . The methods for co m-parison are, Base : The linear function without propagation.
NL : Nonlinear evidence fusion (PNorm with p =2) without propagation .

LP : Linear propagation, i.e., the linear function is used to combine the evidence of pseudo suppor t-ing sentences .

NLP : Nonlinear propagation where PNorm ( p =2) is used to combine the pseudo supporting sentences.

NL+NLP : The nonlinear method is used to combine both supporting sentences and pseudo supporting sentences .
 Table 6 . Evidence propagation results ( Term set: Wiki2 00 ; Similarity graph: PB ; Nonlinear formula: 
In this paper, we generate the DS (distributional similarity) and PB (pattern -based) graphs by adop t-ing the best -performed methods studied in (Shi et al., 2010). The performance improvement numbers (indicated by the upward pointing arrows) shown in table s 6~ 9 are relative percentage improve ment over the base approach (i.e. , linear function wit h-out propagation). The values of parameter are set to maximize the MAP value s .
 First, no performance improvement can be o b-tained with the linear propagation method (LP), while the nonli near propagation algorithm (NLP) works quite well in improving both precision and recall. The results demonstrate the high correlation between pseudo supporting sentences and the great potential of using term similarity to improve h y-pernymy extraction. The second observation is that the NL+NLP approach achieves a much larger pe r-formance improvement than NL and NLP. Similar results (omitted due to space limitation) can be observed on the Ext100 term set.

Table 7 . Combination of PB and DS graphs for evidence propagation ( Term set: Wiki200 ; Nonli n-
Table 8 . Combination of PB and DS graphs for bine the PB and DS graphs to obtain better results. As shown in Tables 7, 8, and 9 (for term sets Wiki200, Wiki1 00L, and Ext100 respectively , u s-ing the Log formula for fusion and propagation ), utilizing both graphs really yields additional pe r-formance gains. We explain this by the fact that the information in the two term similarity graphs tends to be complimentary. The performance improv e-ment over Wiki100L is especially remarkable. This is reasonable because rare t erms do not have ad e-quate information in their supporting sentences due to data sparseness. As a result, they benefit the most from the pseudo supporting sentences prop a-gated with the similarity graphs.

Table 9 . Combination of PB and DS graphs for We demonstrated that the way of aggregating su p-porting sentences has considerable impact on r e-sults quality of the hyponym extraction task using lexico -syntactic patterns, and the widely -used counting method is not optimal. We applied a s e-ries of nonlinear evidence fusion formulas to the problem and saw noticeable performance i m-provement. The data quality is improved further with the combination of nonli near evidence fusion and evidence propagation. We also introduced a new evaluation corpus with annotated hypernym labels for 300 terms , which were shared with the research community.
 We would like to thank Matt Callcut for reading through the paper . Thanks to the annotators for their efforts in judging the hypernym labels . Thanks to Yueguo Chen, Siyu Lei, and the anon y-mous reviewers for their helpful comments and suggestions. The first author is partially supported by the NSF of China (6090 3028,61070014), and Key Projects in the Tianjin Science and Technol o-gy Pillar Program.
