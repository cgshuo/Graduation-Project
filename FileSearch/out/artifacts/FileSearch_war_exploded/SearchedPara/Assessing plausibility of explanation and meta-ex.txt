 1. Introduction
Providing explanations of decisions for human users, and understanding how human agents explain their decisions, are important features of intelligent systems. A number of complex forms of human behavior is associated with attempts to provide acceptable and convincing explanations. In this paper, we propose a computational framework for treating of what we believe constitute two major explanation sources: factual explanations and structural explanations. The former source (level) is more traditional and deals with feasibility of individual claims. The latter source (level) of explanation, meta-explanation, is intro-duced in this study to formalize the explanation patterns based on structural similarity:  X  X  X  did it because people behave like this in a conflict dialog with similar structure X  X .

Importance of the explanation-aware computing has been demonstrated in multiple studies and systems ( Wheeldon, 2007; Ma et al., 2008; Elizalde et al., 2008; Dey, 2009 ). Also, Walton (2007) argued that the older model of explanations as a chain of inferences with a pragmatic and communicative model that structures an explanation as a dialog exchange. The field of argumentation is now actively contributing to such areas as legal reasoning, natural language processing and also multi-agent systems ( Dunne and Bench-Capon, 2006 ). It has been shown ( Walton, 2008 ) how the argumentation methodology implements the concept of explanation by transforming an example of an explanation into a formal dialog structure. In this study we differentiate between explaining as a chain of inference of facts mentioned in dialog, and meta-explaining as dealing with formal dialog structure represented as a graph. Both levels of explanations are implemented as argumentation: explanation operates with individual claims communicated in a dialog, and meta-explanation relies on the overall argumentation structure of scenarios.

Understanding, simulating and explaining behavior of human agents, as presented in text or other medium, is an important problem to be solved in a number of decision-making and decision support tasks (e.g. Fum et al., 2007 ). One class of the solutions to this problem involves learning argument structures from previous experience with these agents, from previous scenarios of interaction between similar agents ( Galitsky et al., 2009 ). Another class of the solutions for this problem, based on the assessment of quality and consistency of argumentation of agents, has been attracting attention of the behavior simulation community as well ( Chesn  X  evar et al., 2000 ).

In the context of agent-based decision support systems, the study of dynamics of argumentation ( Prakken and Vreeswijk, 2002 ) has proven to be a major feature for analyzing the course of interaction between conflicting agents (e.g. in argu-ment-based negotiation or in multi-agent dialogs. The issue of argumentation semantics of communicative models has also been addressed in the literature (e.g. Parsons et al., 2002 ). Case-based reasoning has been applied to learn interaction scenarios as well ( Aleven, 2003 ).

However, when there is a lack of background domain-depen-dent information to obtain a full object-level explanation, the evolution of dialogs where human agents try to explain their decisions should be taken into account in addition to the communicative actions these arguments are attached to. We are concerned with the emerging structure of such dialogs in conflict scenarios, based on inter-human interaction and refer to such structure as meta-explanation . Meta-explanation is implemented as a comparison of a given structure with similar structures for other cases to mine for relevant ones for assessing its truthfulness and assessment whether agents provide proper explanations.
In our earlier studies we proposed a concept learning technique for scenario graphs, which encode information on the sequence of communicative actions, the subjects of communicative actions, the causal ( Galitsky et al., 2005 ), and argumentation attack relation-ships between these subjects ( Galitsky et al., 2009 ). Scenario knowledge representation and learning techniques were employed in such problems as predicting an outcome of international con-flicts, assessment of an attitude of a security clearance candidate, mining emails for suspicious emotional profiles, and mining wire-less location data for suspicious behavior ( Galitsky et al., 2007 ).
In this study, we perform a comparative analysis of the two levels of explanation-related information mentioned above to assess plausibility of scenarios of interaction between agents. The meta-level of explanation is expressed via an overall structure of a scenario, which includes communicative actions and argumenta-tion attack relations. This explanation is learned from previous experience of multi-agent interactions. Scenarios are represented by directed graphs with labeled vertices (for communicative actions) and arcs (for temporal and causal relationships between these actions and their parameters) (Galitsky et al., 2005). The object-level explanation is expressed via argumentative structure of a dialog, assessing the plausibility of individual claims, which has been a subject of multiple applied and theoretical AI studies.
To observe which forms of explanations humans use in dialogs, we use the domain of customer complaints, where relevant explana-tion of problems is a main communication means to obtain customer satisfaction. We have collected and formalized thousands of banking complaints and use them as a benchmark for a number of studies of human behavior. We will use the domain of our complaint to evaluate how combination of both explanation sources contribute to complaint plausibility assessment.

To treat explanations at each level computationally, we per-form classification of complaint scenarios into the classes of plausible and implausible scenarios and observe how each expla-nation level allows us to do that.
 examples of explanations on both levels and outline explanation features we will be formalizing, introducing the domain of customer complaints and plausibility assessment of explanations in these domains (Sections 1 X 3). Meta-level explanation and machine learning of explanation structures are presented in Sections 4 and 5, followed by object-level explanation mechan-ism briefly formalized in Section 6. Evaluation of plausibility assessment is presented in Section 7, followed by the introduction of explanation phase space in Section 8. The paper is concluded with the discussion on engineering applications and related work in Section 9. 2. Explaining why vs. explaining how (example 1) demonstrate that they refer to object-level and meta-level of explanation in various degrees.
 how to build a house.
 assume that we have a 2nd grade student. The satisfactory explanation will be that first, somebody builds a foundation, then walls and finishes with building a roof. This is a natural reasoning chain with the relationship  X  X o be on top X .
 previous explanation will not be satisfactory. The explanation will have the same structure but more objects such as windows, doors, etc.
 studies a civil engineering course. Previous explanations are not satisfactory anymore, because the construction student needs to learn how he can actually make a project and build a house himself.
 context specific. In these examples, it was easy for us to under-stand that first two explanations are not satisfactory for the construction student. Often students complain that they did not get hands-on experience. This is another was to say that the explanation is not satisfactory, or not an explanation at all. In this study, we attempt to represent such context as two-level expla-nation system.
 and (2) explain  X  why to build in a particular way. In many cases, a student cannot judge whether the explanation is satisfactory if he still did not try to build a house using obtained explanations.
The explanation may miss critical details ( X  X  X now-how X  X ). However if he is asked if he has got a satisfactory explanation, he may say  X  X  X es X  X  because he simply does not know that the explanation is not complete. We call this situation illusion of explanation . Even mathematical proof can be faulty. History tells us that it may take time to discover that the proof was incomplete. The issue is that the person who understands, that the construction explanation is not satisfactory, often does not need the explanation. He poses the construction knowledge already; there is no need for reason-ing chains here.
 tion. The fact that illusion of explanation can happened is exploited in social communications such as advertisement. The customer believes that he has got a satisfactory explanation while in fact he was manipulated. The interesting research issue is how to recognize and separate real and illusory explanations for applica-tions such as recommender systems. The key idea here is in the concept of trust. If the agent, who receives the explanation, trusts the agent who provides the explanation, then the illusion of the explanation can happen. The lack of trust leads to the opposite conclusion  X  a perfect logical explanation can be rejected. In the case of high trust, if the receiving agent has a very limited knowledge base in the domain then it will increase chances for the illusion of the explanation. In the same way, the agent with the low trust and a very limited knowledge base in the domain will likely reject a legitimate explanation. These observations will be treated at meta-explanation level.

Explaining why usually requires object-level explanation, whereas how may rely on meta-explanation level only. We will further consider why  X  kind of explanation, analyzing how agents in a conflict scenario attempt to explain why they are right and their opponent are not.

If one takes a fresh look at the above examples, she can see that various cases are specific combination of object-level and meta-explanation. In the further sections, we define each level formally and outline the means to calculate the ratio between the  X  X oles X  of object-level and meta-explanation in meeting the objective of an agent trying to communicate his explanation. 3. Explaining a scenario: which levels are used? (example 2)
We first introduce a scenario as described in a blog (Rase-dezert.com 08, Fig. 1 ) and give the reader a chance to reconstruct what might have happened. This is a simple example, where one can observe how where both sources of argumentation data help to make sense of the scenario, classifying it with respect of the roles of involved agent. Our first example does not include an explicit dialog/conflict between agents.

Trying to understand whether Gonza  X  lez is a prominent crim-inal, a witness of a crime, or a well known auto race enthusiast, the reader employs two levels to explain her interpretation of this scenario, as outlined in the Introduction:
Meta-explain : Observe typical most familiar scenarios which are similar to our scenarios with respect to expected logic of events (which we call argumentation patterns in this paper).
These include: helicopter crash and accident handling, attack by criminals to eliminate witnesses or help gang members escape, and criminal run-away. A number of features of our scenario are hard to match by common/typical scenarios, such as attack of a morgue , appearance of a criminal at a public event , attack of a police car by a large group of criminals.

Explain : Take into account relevant commonsense knowledge, assess whether the main involved agent is a criminal or a crime witness , because his body was hijacked by a criminal gang, and/or an auto enthusiast , because he rented a helicopter to report the auto race.

We now show the  X  X fficial X  explanation in the press:  X  X  X aybe it was sentimental reasons, X  X  said David A. Shirk, director of the Trans-Border Institute at the University of San
Diego. The attackers, said Shirk and others, may have wanted to ensure that the man X  X  funeral was attended by his friends.  X  X  X f he was buried by authorities, they would expose themselves by coming out for any kind of public funeral, X  X  Shirk said.

Alicia Arellano was participating in a vehicle registered with the number 113, but later the authorities presented a new version where it is presumed that a member of the Arellano Felix family was actually aboard the helicopter that crashed.
On one hand, the most plausible explanation of events comes from the scenario funeral of a criminal, where friends attend without exposure to public , which is not very frequent. On the other hand, such explanation might be derived in an attempt to find an argument which attacks the statement  X  X ijack a crime witness X  and supports the argument  X  X elease a member of criminal gang X  without attacking the assertion that this member is dead at the time of release. Hence the reader observes that both meta-explaining by learning links between events from familiar scenarios (1), and finding plausible explanation (as we illustrated by defeating relationships for individual statements (2)) contri-bute to understanding scenarios and assessing its truthfulness. Hence in this example both levels of explanation are required to come up with a plausible scenario interpretation.

The goal of this paper is to estimate relative importance of these levels of explanation for the overall assessment of scenario plausibility. To do that, we build both representations, classify scenarios based on these representations, and evaluate which representation improves the classification accuracy in a higher degree.

Then we will explore the correlation between overall semantic characteristics of scenarios (such as level of competence, truthful-ness, motivation of the agent being explaining, and possible attitudes of agents being explained to) and the ratio between the above degrees of how each level contributes to classification accuracy. 4. Domain of customer complaints
A typical complaint is a report of a failure of a product or service, followed by a narrative on the customer X  X  attempts to resolve this issue. These complaints include both a description of the product or service failure and a description of t he resulting interaction process (negotiation, conflict, etc.) between the customer and the company representatives. Since it is almost impossible to verify the actual occurrence of such failures, compan y representatives must judge on the validity of complaints on the basis of the communicative actions Galitsky et al. (2009) provided by the customers in their narratives. Customers usually do th eir best to bring their points across, so that the consistency of communicative actions and the appropriateness of their arguments (represented as parameters of these actions) are major clues for the validity of their complaints.
Indeed, a complaint narrative usually describes a conflict between an unsatisfied customer and customer support representatives, in which communicated claims need to be rat ionally explained. In contrast with the almost unlimited number of possible details regarding product failures, the emerging arg umentative dialogs between cus-tomer and company can be subject to a systematic computational study ( Galitsky, 2006a, 2006b ). In this context, a major challenge in complaint processing involves distinguishing those customer com-plaints which are rationally acceptable from those which are not, so that the whole procedure of complaint handling can be better supported.

Plausible complaint scenarios are those in which the customer follows a sound argumentation line as the dialog with the company proceeds. On the contrary, invalid complaint scenarios are those, which contain some kind of ill-formed reasoning (fallacies) in the argumentation process. Several fallacies are possible (e.g., the customer performs circular reasoning, coming back to something that was already explained; or the customer contradicts himself, attacking some statement which he had previously granted as accepted.

A major challenge in complaint pr ocessing involves assessing the validity a customer complaint on the basis of the emerging dialog between a customer and a company representative. Complaint advocacy services rely on human experts for classifying complaints.
Currently, most customer complaint management solutions are limited to the use of keyword processing to relate a complaint to a certain domain-specific class (e.g. ATM transactions via credit cards for banking complaints, as reported in Cho et al. (2002) ), or to the application of knowledge management techniques in software plat-forms for workflow processing (e.g. Amadeus, 2009 ). 5. Meta-explaining agents X  behavior in dialog
We approximate an inter-human interaction scenario as a sequence of communicative actions (such as inform, agree, disagree, threaten, request ), ordered in time, with argumentation attack relation between some of the subjects of these commu-nicative language.

Scenarios are simplified to allow for effective matching by means of graphs . In such graphs, communicative actions and attack relations are the most important component to capture similarities between scenarios. Each vertex in the graph will correspond to a communicative action, which is performed by an (artificial) agent. As we are modeling dialog situations for solving a conflict, we will borrow the terms proponent and opponent from dialectical argumentation theory ( Prakken and
Vreeswijk, 2002 ) to denote such agents. An arc (oriented edge) denotes a sequence of two actions. Communicative action in the current model either carries explanation sharing, request, accep-tance or denial.
 2006a, 2006b ) communicative actions will be characterized by three parameters: (1) agent name , (2) subject (information trans-mitted, an object described, etc.), and (3) cause (motivation, explanation, etc.) for this subject. When representing scenarios as graphs, we take into account all these parameters. Different arc types denotes whether the subject stays the same or not. Thick arcs link vertices that correspond to communicative actions with the same subject , whereas thin arcs link vertices that correspond to communicative actions with different subjects. We will make explicit conflict situations in which the cause of one commu-nicative action M1  X  X  X ttacks X  X  the cause or subject of another communicative action M2 via an argumentation arc A (or argumentation link) between the vertices for these communicative actions. This attack relationship expresses that the cause of first communicative action ( X  X  X rom X  X ) defeats the subject or cause of the second communicative action ( X  X  X o X  X ). Such defeat relationship is defeasible , as it may be subject to other defeats, as we will see later.
 linked by the attack relation: a subject of the first communicative action is supported by a cause for the same (respectively, different) subjects of the second communicative action. However, we are concerned with argumentation arcs, which link non-consecutive vertices (communicative actions) as shown at Fig. 2 . representing a complaint scenario in which a client is presenting a complaint against a company because he was charged with an overdraft fee, which he considers unfair ( Fig. 2 ). We denote both parties in this complaint scenario as Pro and Con (proponent and opponent), to make clear the dialectical setting. In this text communicative actions are shown in bold . Some expressions are underlined, indicating that they are defeating earlier state-ments. Fig. 3 shows the associated graph, where straight thick and thin arcs represent temporal sequence, and curve arcs denote defeat relationships.
 comprising two vertices) are about the current transaction ( deposit ), three sentences after (and the respective sub-graph comprising three vertices) address the unfair charge, and the last sentence is probably related to both issues above. Hence the vertices of two respective subgraphs are linked with thick arcs: explain X  X onfirm and remind X  X xplain X  X isagree . It must be remarked that the underlined expressions help identify where conflict among arguments arise. Thus, the company X  X  claim as disclosed in my account information defeats the client X  X  assertion due to a bank error . Similarly, the expression I made a deposit well in advance defeats that it usually takes a day to process the deposit (makes it non-applicable). The former defeat has the intuitive meaning  X  X  existence of a rule or criterion of procedure attacks an associated claim of an error X  X  , and the latter defeat has the meaning  X  X  the rule of procedure is not applicable to this particular case X  X .
Our task is to classify (for example, by determining its plausibility) a new complaint scenario without background knowledge, having a dataset of scenarios for each class. We intend to automate the above analysis given the formal repre-sentation of the graph (obtained from a user-company interaction in the real world, filled in by the user via a special form where communicative actions and argumentation links are specified).
Formalizing complaints in terms of complaint scenarios can be helpful for automatically detecting suspicious complaint dialogs on the basis of the communicative actions involved. Indeed, complaint scenarios can help to identify subtle aspects in such dialogs, which make them ill-formed or fallacious. Thus, the complaint scenario in Fig. 3 seems ill-formed, as apparently the complainant does not understand the procedure of processing the deposit nor distinguishes it from an insufficient funds situation.
Note that the scenario itself does not have surface-level explicit inconsistencies. At the first sight, the complainant X  X  plot in terms of communicative actions seems normal, and the complainant X  X  arguments sound reasonable. Nevertheless, by looking deeper into the case (without taking into account banking knowledge), a problem becomes visible. Rather than accepting the opponent X  X  confirmation about subject S , the complainant switches from S to another subject S 0 (reminds about another transaction), and disagrees with the opponent X  X  explanation of this new subject, mixing both of them. Moreover, from the complainant X  X  perspec-tive, his opponent reacts with a denial to his disagreement. In other words, the complainant disagrees with what has already been explained but at the same time  X  X  X ttacks X  X  what has granted as confirmed, which is a suspicious argumentation pattern.
Scenario where proponents and opponents exchange explana-tions of their positions via explanations, are formalized via scenario graphs. Let us enumerate the constraints for the scenario graph: (1) All vertices are fully ordered by the temporal sequence (ear-lier X  X ater). (2) Each vertex is either assigned with the proponent (drawn on the left side in Fig. 3 ) or to the opponent (drawn on the right side). (3) Vertices denote actions either of the proponent or of the opponent. (4) The arcs of the graph are oriented from earlier vertices to later ones. (5) Thin and thick arcs point from a vertex to the subsequent one in the temporal sequence (from the proponent to the oppo-nent or vice versa). (6) Curly arcs, staying for attack relations, jump over several vertices in either direction.

Similarity between scenarios is defined by means of maximal common subscenarios. Since we describe scenarios by means of labeled graphs, we outline the definitions of labeled graphs and domination relation on them (see Kuznetsov, 1999 ). Given ordered set G of graphs ( V,E ) with vertex-and edge-labels from the sets ( L B , r and ( L E , r ). A labeled graph G from G is a quadruple of the form (( V , l ),( E , b )), where V is a set of vertices, E is a set of edges, l: V -L B is a function assigning labels to vertices, and b: E -L E is a function assigning labels to edges. The order is defined as follows: For two graphs G :  X  (( V 1 ,l 1 ),( E 1 , b 1 )) and G 2 :  X  (( V 2 , l 2 G 1 dominates G 2 or G 2 r G 1 (or G 2 is a sub-graph of G 1 exists a one-to-one mapping j :V 2 -V 1 such that it respects edges : (v,w) A E 2 ) ( j ( v ), j ( w )) A E 1 , fits under labels : l 2 ( v r l 1 ( j ( v )), ( v ,w ) A E j ( w )).

This definition allows generalization ( X  X  X eakening X  X ) of labels of matched vertices when passing from the  X  X  X arger X  X  graph G  X  X  X maller X  X  graph G 2 .

Now, generalization Z of a pair of scenario graphs X and Y (or their similarity), denoted by X n Y  X  Z , is the set of all inclusion-maximal common subgraphs of X and Y , each of them satisfying the following additional conditions:
To be matched, two vertices from graphs X and Y must denote communicative actions of the same agent.
 Each common sub-graph from Z contains at least one thick arc.
The following conditions hold when a scenario graph U is assigned to a class: (1) U is similar to (has a nonempty common scenario sub-graph (2) For any negative example R ,if U is similar to R
We now proceed to more sophisticated inductive learning algorithm to deal with explanation structures. 6. Learning similar explanation structures
To learn similar explanation structures, we use Jasmine, a logic programming machine-learning system, which is based on a learning model called JSM-method (in honor of John Stuart Mill, the English philosopher who proposed schemes of inductive reasoning in the 19th century). JSM-method ( Finn, 1991 )tobe presented in this Section implements Mill X  X  idea ( Mill, 1843 ) that similar effects are likely to follow common causes (in this sense effects are explained by causes).

The Jasmine framework consists of features (communicative actions), objects (scenarios) and targets (features to be predicted: classes of scenarios). Within a first-order language, objects are atoms, features and effects (targets) are terms which include these atoms. For a target, there are four groups of objects with respect to the evidence they provide for this target: Positive X 
Negative X  X nconsistent  X  Unknown. An inference to obtain a target feature (satisfied or not) can be represented as one in a respective four-valued logic. The predictive setting is based on building hypotheses, target(S):-feature 1 (S, y ), y ,feature n (S, y ), that separate sce-narios S, where target is to be predicted, and features 1 features are the mental actions/attitudes which are considered causes of the target.

Desired separation between the classes of explanations is based on the similarity of these explanations in terms of commu-nicative actions they include and temporal relationships between them. Similarity between a pair of explanations is a hypothetical explanation object, which obeys the common features of this pair of objects. In this work we choose anti-unification of formulas expressing features of a pair of explanations to derive a formula for similarity sub-explanation (Finn, 1991). Anti-unification, in the finite term case, was studied as the least upper bound operation in a lattice of terms. Below we will be using the predicate similar(Object1, Object2, CommonSubObject) which yields the third argument given the first and the second arguments; it is implemented as some adjustment of anti-unification to the dialog systems ( Galitsky and Kuznetsov, 2008a ).

We start with an abstract example of Jasmine setting, based on attitudes of candidates for a business partner. Our introductory example of JSM settings for unary predicate is as follows (from now on we use the conventional PROLOG notations for variables and constants), Fig. 4 .

The problem is formulated as follows: building a rule of acceptance/rejection candidates for a position based on their five features. An expert human resource agent has made the decisions in five cased below, and Jasmine X  X  task is to resolve other possible cases.

We start our presentation of reasoning procedure with the chart ( Fig. 5 ), followed by the logic program representation. Let us build a framework for predicting the target feature V of objects set by the formulas X expressing their features: unknown(X, V) .We are going to predict whether V(x 1 , y , x n ) holds or not, where x , y , x n are variables of the formulas X (in our example, X  X  [i(o1), e(o1), c(o1)], x 1  X  o1 ).

We start with the raw data, positive and negative examples, raw Pos(X, V) and rawNeg(X, V) , for the target V , where X range over formulas expressing features of objects. We form the totality of intersections for these examples (positive ones, U , that satisfy iPos(U,V) , and negative ones, W , that satisfy iNeg(W,V), not shown): iPos  X  U , V  X  : -rawPos  X  X 1 , V  X  , rawPos  X  X 2 , V  X  ,
X 1 \  X  X 2 , similar  X  X 1 , X 2 , U  X  , U \  X  X  :
To obtain the actual positive posHyp and negative negHyp hypotheses from the intersections derived above, we filter out the inconsistent hypotheses which belong to both positive and negative intersections inconsHyp(U, V) : inconsHyp  X  U , V  X  : -iPos  X  U , V  X  , iNeg  X  U , V  X 
Here U is the formula expressing the features of objects. It serves as a body of clauses for hypotheses V :-U .
 features expressed by the hypotheses are included in the features of these objects. We derive positive and negative hypotheses reprObjectsPos(X, V) and reprObjectsNeg(X, V) where X is instan-tiated with objects where V is positive and negative respectively.
The last clause (with the head reprObjectsIncons(X, V) ) implements the search for the objects to be predicted so that the features expressed by both the positive and negative hypotheses are included in the features of these objects
Finally, we approach the clauses for prediction. Two clauses above (top and middle) do not participate in prediction directly; their role is to indicate which objects deliver what kind of prediction.
For the objects with unknown targets, the system predicts that they either satisfy these targets, do not satisfy these targets, or that the fact of satisfaction is inconsistent with the raw facts. To deliver V , a positive hypothesis has to be found so that a set of features X of an object has to include the features expressed by this hypothesis and X is not from reprObjectsIncons(X, V). To deliver V , a negative hypothesis has to be found so that a set of features X of an object has to include the features expressed by this hypothesis and X is not from reprObjectsIncons(X, V). No prediction can be made for the objects with features expressed by
X from the third clause, predictIncons  X  X , V  X  predictIncons  X  X , V  X  : -unknown  X  X , V  X  , not predictPos  X  X , V  X  , not predictNeg  X  X , V  X  , not reprObjectsIncons  X  X , V  X  X  4  X 
The first clause above (shown in bold) will serve as an entry point to predict (choose) an effect of given features from the generated list of possible effects that can be obtained for the current state.
The clause below is an entry point to Jasmine if it is integrated with other applications and/or reasoning components predict _ effect _ by _ learning  X  EffectToBe Pr edicted , S  X  : -findAllPossibleEffects  X  S , As  X  , load Re quiredSamples  X  As  X  , member  X  EffectToBe Pr edicted , As  X  , predictPos  X  X , EffectToBe Pr edicted  X  , ! , X \  X   X  :
For example, for the knowledge base above, we have the following protocol and results ( Fig. 6 ):
Hence good_fit(o8) does not holds: candidate o8 is  X  X ot so good X .

Now we show how to classify the totality of candidates X  features into  X  X  X ood fit X  X ,  X  X  X ot a good fit X  X  or inconsistent prediction . 7. Explaining individual claims
In this section, we briefly outline our approach to computa-tionally treat object-level explanations in the domain of customer complaints.

To verify the truthfulness of a complainant X  X  claim, we use the special form called Interactive Argumentation Form, which assists in structuring a complaint. Use of this form enforces a user to explicitly indicate all causal and argumentation links between statements which are included in a complaint. The form is used at the object-level argumentation to assess whether a particular scenario has plausible argumentation pattern: does it contain self-attacks (explicit for the complainant).

Let us consider a typical complaint due to misunderstanding between a customer and a bank:
A customer has two mortgages and a checking account in a bank, and automated payment was set up. At some point this customer started making his monthly payments himself, and the banks continued automated withdrawal, which lead to non-sufficient fund state in his account, and consequently overdraft fees. Moreover, the bank, according to the custo-mer, does not communicate this situation in a clear way, stating that  X  X  X hat as long as Huntington had an open account for him, from which they X  X  already set up automatic with-draw, they could continue to withdraw funds for loan payment, even if the loan had already been paid by check X  X , and was threatening to repossess the mortgaged truck.
Furthermore, the customer was recommended to close his account by the bank (which is rare practice in banking industry).
 had continuing troubles. The first payment was late, due to a mistake made by Huntington, which they acknowledged.
Huntington also told him that they X  X  take the late payment off my record but it appears they never did. All in all, banking with Huntington has been a complete fiasco for this customer.

The form ( Fig. 7 ) includes eight input areas where a complai-nant presents a component-based description of a customer problem. At the beginning, the subject of the dispute is specified: an operation (or a sequence of operations) which are believed by a complainant to be performed by a company in a different manner to what was expected o Where company got confused 4 . Then the essence of the problem is described, what exactly turned out to be wrong. In the section o Company wrongdoing 4 the complainant sketches the way the company performed its duties, which caused the current complaint.
The customer X  X  perception of the damage is inputted in section o How it harmed me 4 . In the fourth section o Why I think this was wrong 4 the customer backs up his beliefs concerning the above two sections, o Where company got confused 4 and o Company wrongdoing 4 .

All possible object-level argumentation links are shown as arrows. Arrows denote the links between the sentences in the respective sections; some arrows go one way and other both ways (only the ending portion is shown in this case). If the user does not find an arrow between two sections for a pair of inputted sentences, it means that either or both of these sentences belong to a wrong section: the data needs to be modified to obey the pre-defined structure. End of each arrow is assigned by a check-box to specify if the respective link is active for a given complaint . Bold arrows denote most important links .

The role of the Interactive Argumentation Form is a visual representation of argumentation, and intuitive preliminary ana-lysis followed by the automated argumentation analysis. Since even for a typical complaint manual consideration of all argu-mentation links is rather hard, automated analysis of inter-connections between the complaint components is desired. We use the defeasible logic programming ( Garc X   X  a and Simari, 2004 ) approach to verify whether the complainant X  X  claims are plausible (cannot be defeated given the available data), concluding with the main claim, Systematic wrongdoing.

Defeasible logic program (de.l.p.) is a set of facts, strict rules P of
Let  X  ( P , D ) be a de.l.p. and L a ground literal. A defeasible derivation of L from consists of a finite sequence L 1 , L of ground literals, and each literal L i is in the sequence because: (a) L i is a fact in P ,or (b) there exists a rule R i in (strict or defeasible) with head L
Let h be a literal, and  X  ( P , D ) a de.l.p. We say that o , h 4 is an argument structure for h ,if A is a set of defeasible rules of D , such that: 1. there exists a defeasible derivation for h from  X  ( P [ ) 2. the set ( P [ ) is non-contradictory, and 3. is minimal: there is no proper subset 0 of such that 0
Hence an argument structure o , h 4 is a minimal non-contra-dictory set of defeasible rules, obtained from a defeasible deriva-tion for a given literal h .
 sub-argument o , h 4 of o 2 , h 2 4 ( D 1 ) so that h and h are inconsistent. Argumentation line is a sequence of argument structures where each element in a sequence attacks its prede-cessor. There is a number of acceptability requirements for argumentation lines (Garcia and Simari, 2004).
 us an algorithm to discover implicit self-attack relations in users X  claims. Let o 0 , h 0 4 be an argument structure from a program 1. The root of the tree is labeled with o 0 , h 0 4 2. Let N be a non-root vertex of the tree labeled o n , h labels of the path from the root to N . Let [ o 0 , q 0 4 , y , o k , q k 4 ] all attack o n , h n 4 . For each attacker with acceptable argumentation line [ L , o i , q i 4 ], we have an arc between N and its child N i .

Logic Program, and of meta-explanation by graph representation can be viewed from the standpoint of Temporal Defeasible Logic ( Riveret et al., 2006 ). This extension proved useful in modeling retroactive rules, which permit to obtain conclusions holding at a time instant that precedes the time of application of the same rules. Explanations with such conclusions can be correct as well as implausible, and Temporal Defeasible Logic allows proper treatment of such explanation cases. Time is added to Defeasible Logic in two ways, as a temporalized (literal, instant-of-time).
Secondly, rules are partitioned in persistent set and transient rules set according to whether the consequent persists until an interrupting event occurs or is co-occurring with the premises. representation of argumentation, as well as its intuitive prelimin-ary analysis. To specify supporting and defeating links for a number of statements for each section, multiple instances of these forms may be required for a given complaint. Since even for a typical complaint manual consideration of all argumentation links is rather hard, automated analysis of inter-connections between the complaint components is desired. We use the defeasible logic programming approach to verify whether the complainant X  X  claims are plausible (cannot be defeated given the available data).

In our study ( Galitsky et al., 2009 ) we provided the definition and algorithm for building dialectic trees to discover implicit self attack in a defeasible logic program, specified by the Interactive
Argumentation Form ( Fig. 7 ). 8. Evaluation of explanation plausibility via two sources
To observe the comparative contribution of explanation in object-level and meta-level, we used the database of textual complaints which were downloaded from the public website
PlanetFeedback.com. For the purpose of this evaluation, each complaint was: (1) Manually represented at a meta-level for machine-learning evaluation (There is a form ( Galitsky, 2006a, 2006b ) for specifying complaint structure). (2) Manually represented as an object level for finding self-defeating explanation claims, using Interactive Argumentation Form. (3) Manually assigned a plausibility assessment: plausible (valid, consistent) or implausible (includes faulty explanations of agents X  positions).

This complaint pre-processing resulted in 1220 complaints, divided in fourteen banks (or datasets), each of them involving 80 complaints. In each bank 40 complaints were used for training and 40 complaints for evaluation. Learning was conducted sepa-rately for each bank dataset. We refer the reader to Galitsky and
Kuznetsov (2008b) for further details on the complaint database and evaluation environment.

We performed the comparative analysis of relating scenarios to the classes of plausible/implausible taking into account (1) and (2), and combined assessment (1+2). Such an analysis sheds a light on the possibility to recognize a scenario (1) without back-ground knowledge, but reusing previous assigned argument structures, and (2) with partial background knowledge, expressed as a set of attack relations between claims ( Table 1 ). Furthermore, we evaluate a cautious approach combining (1) and (2), where scenario is plausible if (a) it is similar to a plausible one or ( b) it does not contain self-defeated claims, and implausible otherwise ( Table 2 ).

Classification results for each source include precision (true positives divided by the sum of true positives and false negatives), and recall (true positives divided by the sum of true positives and false positives). F -measure (2 n precision n recall/(precision+recall )is used to estimate overall recognition accuracy. Pearson product moment correlation value between precisions of sources 1 and sources 2 of 0.17 shows some correlation between sources 1 and 2, which is expected.

Classification results are shown in Table 1 . The columns are as follows:
First three columns on the left (in italic) : bank number, and the numbers of valid/invalid complaints as manually assessed by human experts.

The middle set of columns show the classifications results based on meta-explanation source: the number of valid (similar to valid structures), and the number of invalid (similar to invalid struc-tures) scenarios. The four columns to follow shows precision and recall calculated separately for valid and invalid scenarios. Finally, the last column in this section shows F -measure for averaged recall and precision for valid and invalid scenarios.
The right (bold italics) set of columns show the classifications results based on discovery of individual self-defeat, object-level explanation source: the number of valid complaints where such self-defeat was not found, and the number of invalid complaint where mutually inconsistent fact were found. Then, analogously to the middle section, four columns to follow shows precision and recall calculated separately for valid and invalid scenarios; the last column in this section shows F -measure for averaged recall and precision for valid and invalid scenarios.

Note that although each scenario is manually related to either class, it is not the case for similarity-based classification: some cases are close to neither. The numbers of false positives and negatives are calculated relatively to the same assessment by human experts, who were used for evaluation of both sources. Training dataset is not shown.

The reader can observe that classification based on the combination of explanation levels ( Table 2 ) gives substantial increase in recognition accuracy: F (level0)  X  76%, F (level1)  X  64%, and F (level0+level1)  X  89%.
Obviously, each bank has its own policy in handling customer complaints. In the table above, assuming we processed a statis-tically significant set of complaints, peculiarity of each bank is reflected as different contribution of object-and meta-level for scenario classification. Explaining their decisions, some banks rely more on individual facts and their policy rules, and other banks prefer references to  X  X  X ommon practice X  X , communicating their explanations. Hence for every group of scenarios involving a fixed set of agents (e.g. representatives of the same bank), one can observe a characteristic ratio between the levels of explanation.
We normalized the ratio F -measure(meta-level)/ F -measur-e(object-level) and characterize each bank with respect to impor-tance of object-level or meta-level explanations to communicate problems with this bank ( Table 3 ).

Some banks operate closer to service agreement, and therefore stick to the facts outlined in such agreements, dealing with customer service requests. In such case validity of complaint scenarios is associated with object-level explanation, most of which are references to the service agreement. Typical customer explanation for their request would be:  X  X  X ou indicated y service agreements, but this is what happened y  X  X . If a self-defeat in such scenario is found, then the complaint is invalid, and valid otherwise. The cluster of such banks is in the left part of the plot in Table 3 , where F -measure for meta-level is 50 X 60%. Commu-nication of problems for such banks refer to customer experience with other banks either rarely, or turns out to be irrelevant most of times.
 most of times, and similar banking scenarios turn out to be essential for complaint validity assessment. The cluster of banks, where F -measure for meta-level is above 70%, mostly relies on finding similar scenarios of interaction with banks in previous experience of the users of given bank.
 wider variety of behavior forms a nd how these can be characterized analogously to what we have evaluated in the banking domain. 8.1. Comparison with other approaches to assessment customer opinions plausibility of explanations in customer complaints, we generalize our domain and compare our results with the state-of-art results in communicating and explaining opinions. This domain is usually referred to as  X  X pinion mining X  and assesses positive and negative sentiments as communicated by the authors of reviews, which can be viewed as a more general forms o fcommunicationofcustomers.
Evaluation studies of customer feedback mostly arise out of academia; we outline the main differences between the industrial opinion mining settings and the ones used in most academic studies on sentiment classification and opinion mining. In industrial settings, attempt to improve th e accuracy of opinion mining is conducted in a test driven development environment, where the goal of the system is to properly extract polarity and sentiment of a manually constructed dataset. Thi s dataset and system settings are adjusted so that the accuracy approaches 100%, so that the system to provide the  X  X ug free X  code that is expected by quality assurance personnel. The resultant accuracy is therefore lower than academic-style evaluations that target maximum accuracy on the testing dataset, something that is frequently not even measured in indus-trial applications. In order to suit the quality assistance procedures of an industrial enviro nment, deterministic approaches are preferred over statistical ones because they provide more control over individual cases, which assures tha t topic and polarity extraction can be carried out on the entire dataset
Although most approaches to opinion mining focus on the overall assessment of customer reviews ( Liu et al., 2005 ), it is necessary for commercial recommendation applications to extract and determine the individual sentiment expression, its topicality, and its polarity properly so that a particular user X  X  needs or concerns may be addressed. Hence, we evaluate the accuracy of the extraction of individual reviews that are quoted to support a user decision. Obviously, assessing a group of opinion expressions as a single opinion in a given review is more accurate than assessing an individual opinion expression.
 We also compare our work to the somewhat similar domain of
U.S. Congressional floor debates ( Pang and Lee, 2008 ). The investigation addressed the possibility of determining from the transcripts whether the speeches were in support of or opposed to proposed legislation. The authors leveraged the observation that these speeches occur as part of a discussion and are backed by arguments; this fact allows them to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another, similar to our procedure for complaints.
The authors found that the incorporation of the information on discourse structures yields substantial improvements over classi-fying speeches in isolation; an accuracy of 84.25% on the training dataset and 81.07% on the testing dataset is achieved. ( Chaovalit and Zhou, 2005 ) conducted a comparison between supervised and unsupervised sentiment and topicality classification approaches using movie reviews. Their supervised machine-learning accuracy is 85.54%. The semantic orientation approach, because it is unsupervised, requires extractin g phrases containing adjectives or adverbs from the review data. Five p atterns of phrases were extracted to find their semantic orientation values based on a selected part-of-speech tagger, and 77% classification accuracy on 100 movie reviews from Movie Vault was achieved after adjusting the dividing baseline. ( Turney, 2002 ) obtained 65.83% accuracy in mining 120 movie reviews from the Epinions website. Pang et al. (2002) mined movie reviews using various machine-l earning techniques to determine whether machine-learning techniqu esareaseffectiveasothersenti-ment classification methods like movie review mining. They obtained the best classification accuracies ranging from 77.4% to 82.9% by varying input features (i.e., unigrams, bigrams, unigrams+bigrams). Most studies confirm that unsupervised results confirm our expec-tation that the machine-learning approach is more accurate but requires a significant amount of time to train the model. In contrast, the semantic orientation approach chosen in the current study is slightly less accurate but more efficient for use in real-time applications. 9. Explanation phase space
Having discussed two levels of explanation, we now intend to explore how the explanation style of individual agent and multi-agent system can be characterized in terms of degrees each of these two levels are used. Our intention here is to characterize explanation behavior by a numerical parameter. Since one can  X  X easure X  contribution of object-level and meta-explanation to sce-nario plausibility as relative accuracy , we believe this measure can serve as explanation behavior parameters, which is invariant with respect to subjects of dialog and even individual attitudes of particular scenario agents. Hence, we depict a scenario with explana-tion behavior as a point of two-dimensional space (which we call explanation phase space ).

We demonstrate that using explanation phase space ( Fig. 8 ), one can visualize the phenomenology of various forms of multi-agent behavior associated with explanation. Less plausible explanation scenarios are shown in the left-bottom corner, and fully valid ones are shown in the top-right corner. A number of epistemic states are shown in the phase space and their object-level and meta-explana-tions are described. When the trust is high, detailed causal links in explanation do not have to be provided. When object-level explana-tion is very incomplete and meta-explanation is somewhat com-plete, illusion of explanation (discussed above in example 1) may occur. An adult can say a child about somebody:  X  X  X e is happy because he is always friendly. Be fr iendly too X  X . This explanation may not be true but if the child trusts this adult, it will be accepted. This explanation has a structure close to the explanations by politicians:  X  X  X hese people do good things because they are friendly, Let X  X  be friendly people, and lets elect people who look friendly X  X .
The incompleteness at the object-level can be augmented by an agent receiving explanation by accepting meta-explanation. In
English precedent-based legal system, meta-level explanations prevails over continental statute-based legal system. A change in the behavior of agent system (demonstrated as a set of scenarios) can be shown as a trajectory in explanation phase space. 9.1. Example: explanation type-based inbox visualization
We proceed to an example of a practical application visualiz-ing explanation phase space. We consider the problem of visua-lizing of email inbox content. Usually, email are sorted by date, authors, and conversation. If, for each email, one can assess the levels of explanation (a trust view), communicated by email authors, a novel way of clustered email view in explanation phase space can be proposed. Such trust-based view would facilitate efficiency of email communication and relieve a user from some of the burden of trust management replying a manifold of emails. recipient, it turns out that each group of email authors correspond to clusters of emails with specific explanation patterns (specific ration between explanation levels). Each such cluster relies on a specific logic of explanation, so replying to emails from each cluster would occur in a cluster-specific manner.
 authors who know they are trusted well, do not have to provide as thorough explanation as the ones unacquainted with the recipient.
Email from an unknown person should contain explanation on both leveltobeacceptable(top-rightcornerofthephasespace).Astrust develops, email authors can simplify and decrease strength of explanations. Hence there is a common tendency in explanation phase space for inbox: when trust for a given email correspondent increases in time, the location on the phase space might move towards top-right (shown by an arrow in Fig. 9 ).
 advanced natural language and machine-learning techniques of automated extraction of explanation patterns. Building such information extraction is a subject of our further studies.
Obviously, the email clustering problem does not provide statis-tical significance to calculate the ration of level1/level0 recogni-tion accuracy, but the representation via explanation phase space is still relevant. 10. Results and discussions their problems in such conflict environments as customer com-plaints. We suggested how to split explanation-related behavior presented in a human language into two levels, using reasoning chains (deduction) and similarity between explanation structures (inductive learning). Relative to the former level (explanation),
Explanation for a child the latter level is an explanation of explanation structure, which we refer to as meta-explanation (explanation of explanation).
Hence, we split the explanation in multi-agent behavior into a deductive object-level and an inductive meta-level. For the first level, we use an interactive form to obtain a defeasible logic program to verify plausibility of individual claims used in expla-nation. For the meta-level, we represented scenarios as graphs and used graph-based nearest neighbor technique to determine whether given scenario is similar to plausible or implausible scenarios. 10.1. Dealing with validity of explanations in complaint management
Complaint processing ( Davidow, 2003 ) has become an impor-tant issue for customer relationship management (CRM) in large companies and organizations. Complaint management is a formal process of recording and resolving a customer complaint. Even though CRM systems in general and complaint processing systems in particular are expensive, companies can extract price-less knowledge from an appropriate handling of a complaint, with significant effects on customer retention rates and word-of-mouth recommendations. If complaints are transformed into knowledge about customers, they can provide valuable business intelligence for enterprises. To exploit this intelligence, compa-nies must design, build, operate and continuously upgrade systems for managing complaints. In the last few years, several approaches have emerged to automate complaint management such as ( Yuan and Chang, 2001 ), among others. Retailers and service providers may profit from such software services because they allow complaints to be handled faster, providing the possi-bility of feedback analysis and data mining capabilities on the basis of a complaint database.

A typical complaint is an explanation of a failure of a product or service, followed by a narrative on the customer X  X  attempts to resolve the issue. These complaints include both a description of the product or service failure as well as a description of the resulting interaction process (negotiation, conflict, etc.) between the customer and the company representatives. Because it is almost impossible for CRM personnel to verify the actual occur-rence of such failures, company representatives must judge the adequacy of a complaint on the basis of the communicative actions provided by the customers in their explanation. Custo-mers usually do their best to bring their points across, so the consistency of communicative actions of an explanation and the appropriateness of their arguments (represented as parameters of these actions) are major clues for the validity of their complaints. Indeed, a complaint narrative usually describes a conflict between an unsatisfied customer and CRM personnel, in which commu-nicated claims need to be rationally justifiable by sound argu-ments. In contrast with the almost unlimited number of possible details regarding product failures, the emerging argumentative dialogs between customer and company can be subject to a systematic computational study. In this context, a major chal-lenge in complaint processing involves distinguishing those customer complaints which are rationally acceptable from those which are not, so that the whole procedure of complaint handling can be better supported. Currently, most CRM solutions are limited to the use of keyword processing to relate a complaint to a certain domain-specific class (e.g., banking and travel complaints, as reported in this paper), or to the application of knowledge management techniques in software platforms for workflow. To the best of our knowledge, existing industrial complaint management platforms do not make use of natural language processing nor machine-learning techniques for quicker performance, quality assurance and lower sustainability costs; most complaint handling functionalities remain manual. In parti-cular, no automated solutions have been developed to assess the validity of a customer complaint on the basis of the emerging dialog between a customer and the company representatives, with the goal of better supporting the procedure of complaint handling as a part of CRM. Hence we believe a complaint validity assessment of a CRM system is expected to leverage the repre-sentations formalisms and algorithms introduced in this paper. 10.2. Related work
From the machine-learning standpoint, our approach to meta-explanation, as outlined in Sections 4 and 5, is induction. In terms of its deterministic methodology of prediction and implementa-tion via logic programming, meta-explanation as learning is close to Explanation-Based Learning (EBL, Ellman, 1989 ) class of meth-ods which are intended to derive as general expressions as possible from available data. EBL has been deployed in forming rules while observing a human expert, decision-making and advising systems in rather compact domains. One of the motiva-tions for this project is to deploy the power of EBL, expressiveness of its representations and delivery of explanation (which is essential for understanding the underlying mechanisms) for more extensive and less structured domains such as scenarios of inter-human interactions. Therefore, the proposed machine-learning mechanism is able to accommodate such noisy and scarce date as customer complaints, where a lack of an adequate domain theory, or other deficiencies such as problems with completeness, correctness and tractability occurs. To achieve this, we use more cautious prediction settings to decrease the number of false negatives and iterative application of induction-abduction proce-dure on the one hand, and provide an interactive environment to visualize explanations on the other hand. Furthermore, unlike the EBL approach, we used negative examples to falsify hypotheses that have counter-examples.

In our previous studies of argumentation in complaint scenar-ios ( Galitsky et al., 2007, 2009 ) we verified that using attack relationship in addition to communicative actions as a way to express dialog discourse indeed increases the accuracy of scenario plausibility assessment (in a similar setting to the current study).
In the current study, having showed the importance of both explanation levels, we proceeded to defining such characteristic parameter of scenarios with explanation behavior as ratio between contributions of each level to overall scenario assess-ment. We then demonstrated that using such measure a phase space can visualize scenarios with various forms of explanation activities by agents. We also showed that a number of various behaviors can be represented via the explanation phase space.
Ma et al. (2008) introduced an inferential framework for deriving logical explanations from partial temporal information.
Based on a graphical representation which allows expression of both absolute and relative temporal knowledge in incomplete forms, the system can deliver a verdict to the question if a given set of statements is temporally consistent or not, and provide understandable logical explanation rule based reasoning. The scenario graph representation proposed in this paper is a parti-cular way to encode such  X  X  X bsolute X  X  and  X  X  X elative X  X  relationships between actions.

Elizalde et al. (2008) selected statistical approach to the problem of explaining the recommendations generated by a
Markov decision process. The authors proposed an automatic explanation generation that includes, firstly, the most relevant variable given the current state is obtained, based on a factored representation (the relevant variable is defined as the factor that has the greatest impact on the utility given certain state and action). Secondly, an explanation is generated by combing the information obtained from the Markov decision process with domain knowledge represented as a frame system. The state and action are used as pointers to the knowledge base to extract the relevant information and fill X  X n the explanation template. In this way, explanations of the recommendations can be generated on X  X ine and incorporated to an intelligent assistant. In terms of the current paper, this can be viewed as automated building explanation scenarios in two levels by learning.

We observed how two levels of explanation, overall argumen-tation pattern of a scenario and explanations for individual claims, compliment each other. Comparative computational ana-lysis of scenario classification with respect to plausibility showed that assessment of both levels of explanation is essential to determine whether a scenario is plausible or not (contains misrepresentation or self-contradiction). Hence, we believe a practical explanation management system where explanation is implemented via argumentation should include scenario-oriented machine-learning capability in addition to handling argumenta-tion for individual claims.

There are examples in AI where quantitative characterization of computational complexity is rather complex and notion of phase transitions is introduced (see e.g. Selman, 1995 ). In this study we demonstrated that changing a ratio between explana-tion levels can visualize a wide range of behavior patterns related to explanations activity, which is an important part of overall activity of communicating agents. In the future studies we plan to extend our evaluation from the domain of customer complaint to a wide domain of customer relationship management ( Galitsky and de la Rosa, 2011 ), formalizing an extensive list of explana-tion-related behavior. We are also building a commercial recom-mendation system for attending events, based on social network profile. The system (zvents.com) provides explanation why a given event is recommended based on users X  Facebook profile  X  X ikes X  (object-level explanations), and also based on choices of similar users and Facebook friends (meta-level explanation, involving  X  X sers like you X ).

The novelty of this study is three-fold. Firstly, we split the general notion of explanation into two levels. Secondly, we proposed a level-specific learning method which allowed to treat these levels computationally. Finally, we proposed a representation technique for explanation patterns that allows computational treatment and visualization in a practical domain of customer complainants, and mentioned other relevant domains. We have not found any study concerned with matching the struct ured representation of explana-tion patterns or argumentation patterns as a source of  X  X  X lobal X  X  structural information about scenarios; therefore, we believe the current study is pioneering one in this respect.
 References
