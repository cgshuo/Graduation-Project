 Automatic text summarization aims at reducing the amount of text the user has to read while preserv-ing important contents, and has many applications in this age of digital information overload (Mani, 2001). In particular, query-oriented multi-document summarization is useful for helping the user satisfy his information need efficiently by gathering impor-tant pieces of information from multiple documents.
In this study, we focus on extractive summariza-tion (Liu and Liu, 2009), in particular, on sentence selection from a given set of source documents that contain relevant sentences. One well-known chal-lenge in selecting sentences relevant to the informa-tion need is the vocabulary mismatch between the query (i.e. information need representation) and the candidate sentences. Hence, to enrich the informa-tion need representation, we build a co-occurrence graph to obtain words that augment the original query terms. We call this method Query Snowball .
Another challenge in sentence selection for query-oriented multi-document summarization is how to avoid redundancy so that diverse pieces of information (i.e. nuggets (Voorhees, 2003)) can be covered. For penalizing redundancy across sen-tences, using single words as the basic unit may not always be appropriate, because different nuggets for a given information need often have many words in common. Figure 1 shows an example of this word overlap problem from the NTCIR-8 ACLIA2 Japanese question answering test collection. Here, two gold-standard nuggets for the question  X  Sen to Chihiro no Kamikakushi (Spirited Away) is a full-length animated movie from Japan. The user wants to know how it was received overseas. X  (in English translation) is shown. Each nugget represents a par-ticular award that the movie received, and the two Japanese nugget strings have as many as three words in common:  X   X  X  (review/critic) X ,  X   X  X  X  X  (ani-mation) X  and  X   X  (award). X  Thus, if we use single words as the basis for penalising redundancy in sen-tence selection, it would be difficult to cover both of these nuggets in the summary because of the word overlaps.

We therefore use word pairs as the basic unit for computing sentence scores, and then formulate the summarization problem as a Maximum Cover Prob-lem with Knapsack Constraints (MCKP) (Filatova and Hatzivassiloglou, 2004; Takamura and Oku-mura, 2009a). This problem is an optimization prob-lem that maximizes the total score of words covered by a summary under a summary length limit.
We evaluate our proposed method using Japanese complex question answering test collections from NTCIR ACLIA X  X dvanced Cross-lingual Informa-tion Access task (Mitamura et al., 2008; Mitamura et al., 2010). However, our method can easily be extended for handling other languages. Much work has been done for generic multi-document summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celiky-ilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selec-tion, which consist of document similarity and re-dundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated sum-marization as an MCKP, and they used two types of concepts in documents: single words and events (named entity pairs with a verb or a noun). While their work was for generic summarization, our method is designed specifically for query-oriented summarization.

MMR-based methods are also popular for query-oriented summarization (Jagarlamudi et al., 2005; Li et al., 2008; Hasegawa et al., 2010; Lin et al., 2010b). Moreover, graph-based methods for sum-marization and sentence retrieval are popular (Otter-bacher et al., 2005; Varadarajan and Hristidis, 2006; Bosma, 2009). Unlike existing graph-based meth-ods, our method explicitly computes indirect rela-tionships between the query and words in the docu-ments to enrich the information need representation. To this end, our method utilizes within-sentence co-occurrences of words.

The approach taken by Jagarlamudi et al. (2005) is similar to our proposed method in that it uses word co-occurrence and dependencies within sentences in order to measure relevance of words to the query. However, while their approach measures the generic relevance of each word based on Hyperspace Ana-logue to Language (Lund and Burgess, 1996) using an external corpus, our method measures the rele-vance of each word within the document contexts, and the query relevance scores are propagated recur-sively. Section 3.1 introduces the Query Snowball (QSB) method which computes the query relevance score for each word. Then, Section 3.2 describes how we formulate the summarization problem based on word pairs. 3.1 Query Snowball method (QSB) The basic idea behind QSB is to close the gap between the query (i.e. information need rep-resentation) and relevant sentences by enriching the information need representation based on co-occurrences. To this end, QSB computes a query relevance score for each word in the source docu-ments as described below.

Figure 2 shows the concept of QSB. Here, Q is the set of query terms (each represented by q ), R 1 is the set of words ( r 1 ) that co-occur with a query term in the same sentence, and R 2 is the set of words ( r 2 ) that co-occur with a word from R 1 , excluding those that are already in R 1 . The imaginary root node at the center represents the information need, and we assume that the need is propagated through this graph, where edges represent within-sentence co-occurrences. Thus, to compute sentence scores, we use not only the query terms but also the words in R 1 and R 2 .

Our first clue for computing a word score is the query-independent importance of the word. We represent this base word score by s b ( w ) = log( N/ ctf ( w )) or s b ( w ) = log( N/n ( w )) , where ctf ( w ) is the total number of occurrences of w within the corpus and n ( w ) is the document fre-quency of w , and N is the total number of docu-ments in the corpus. We will refer to these two ver-sions as itf and idf , respectively. Our second clue is the weight propagated from the center of the co-occurence graph shown in Figure 1. Below, we de-scribe how to compute the word scores for words in R 1 and then those for words in R 2 .

As Figure 2 suggests, the query relevance score for r 1  X  R 1 is computed based not only on its base word score but also on the relationship between r 1 and q  X  Q . To be more specific, let freq ( w,w 0 ) denote the within-sentence co-occurrence frequency for words w and w 0 , and let distance ( w,w 0 ) denote the minimum dependency distance between w and w : A dependency distance is the path length be-tween nodes w and w 0 within a dependency parse tree; the minimum dependency distance is the short-est path length among all dependency parse trees of source-document sentences in which w and w 0 co-occur. Then, the query relevance score for r 1 can be computed as: where sum Q = the query relevance score s r ( r 1) reflects the base word scores of both q and r 1 , as well as the co-occurrence frequency freq ( q,r 1) . Moreover, s r ( r 1) depends on distance ( q,r 1) , the minimum depen-dency distance between q and r 1 , which reflects the strength of relationship between q and r 1 . This quantity is used in one of its denominators in Eq.1 as small values of distance ( q,r 1) imply a strong re-lationship between q and r 1 . The 1 . 0 in the denom-inator avoids division by zero.

Similarly, the query relevance score for r 2  X  R 2 is computed based on the base word score of r 2 and the relationship between r 2 and r 1  X  R 1 : where sum R 1 = 3.2 Score Maximization Using Word Pairs Having determined the query relevance score, the next step is to define the summary score. To this end, we use word pairs rather than individual words as the basic unit. This is because word pairs are more in-formative for discriminating across different pieces of information than single common words. (Re-call the example mentioned in Section 1) Thus, the word pair score is simply defined as: s p ( w 1 ,w 2 ) = s ( w 1 ) s r ( w 2 ) and the summary score is computed as: where u is a textual unit, which in our case is a sentence. Our problem then is to select S to maxi-mize f QSBP ( S ) . The above function based on word pairs is still submodular, and therefore we can apply a greedy approximate algorithm with performance guarantee as proposed in previous work (Khuller et al., 1999; Takamura and Okumura, 2009a). Let l ( u ) denote the length of u . Given a set of source documents D and a length limit L for a sum-mary, Require: D,L 1: W = D,S =  X  2: while W 6 =  X  do 4: if l ( u ) + 5: S = S  X  X  u } 6: end if 7: W = W/ { u } 8: end while 10: if f ( u max ) &gt;f ( S ) then 12: else return S 13: end if where f (  X  ) is some score function such as f QSBP . We call our proposed method QSBP: Query Snow-ball with Word Pairs. 4.1 Experimental Environment
We evaluate our method using Japanese QA test collections from NTCIR-7 ACLIA1 and NTCIR-8 ACLIA2 (Mitamura et al., 2008; Mitamura et al., 2010). The collections contain complex ques-tions and their answer nuggets with weights. Ta-ble 1 shows some statistics of the data. We use the ACLIA1 development data for tuning a parameter for our baseline as shown in Section 4.2 (whereas our proposed method is parameter-free), and the ACLIA1 and ACLIA2 test data for evaluating dif-ferent methods The results for the ACLIA1 test data are omitted due to lack of space. As our aim is to answer complex questions by means of multi-document summarization, we removed factoid ques-tions from the ACLIA2 test data.

Although the ACLIA test collections were origi-nally designed for Japanese QA evaluation, we treat them as query-oriented summarization test collec-tions. We use all the candidate documents from which nuggets were extracted as input to the multi-document summarizers. That is, in our problem set-ting, the relevant documents are already given, al-though the given document sets also occasionally contain documents that were eventually never used for nugget extraction (Mitamura et al., 2008; Mita-mura et al., 2010).

We preprocessed the Japanese documents basi-cally by automatically detecting sentence bound-aries based on Japanese punctuation marks, but we also used regular-expression-based heuristics to de-tect glossary of terms in articles. As the descrip-tions of these glossaries are usually very useful for answering BIOGRAPHY and DEFINITION ques-tions, we treated each term description (generally multiple sentences) as a single sentence.

We used Mecab (Kudo et al., 2004) for morpho-logical analysis, and calculated base word scores s ( w ) using Mainichi articles from 1991 to 2005. We also used Mecab to convert each word to its base form and to filter using POS tags to extract content words. As for dependency parsing for distance com-putation, we used Cabocha (Kudo and Matsumoto, 2000). We did not use a stop word list or any other external knowledge.

Following the NTCIR-9 one click access task setting 1 , we aimed at generating summaries of Japanese 500 characters or less. To evaluate the summaries, we followed the practices at the TAC summarization tasks (Dang, 2008) and NTCIR ACLIA tasks, and computed pyramid-based preci-sion with an allowance parameter of C , recall, F X  (where  X  is 1 or 3) scores. The value of C was determined based on the average nugget length for each question type of the ACLIA2 collection (Mita-mura et al., 2010). Precision and recall are computed based on the nuggets that the summary covered as well as their weights. The first author of this paper manually evaluated whether each nugget matches a summary. The evaluation metrics are formally de-fined as follows: 4.2 Baseline MMR is a popular approach in query-oriented sum-marization. For example, at the TAC 2008 opin-ion summarization track, a top performer in terms of pyramid F score used an MMR-based method. Our own implementation of an MMR-based base-line uses an existing algorithm to maximize the fol-lowing summary set score function (Lin and Bilmes, 2010): where v D is the vector representing the source docu-ments, v Q is the vector representing the query terms, Sim is the cosine similarity, and  X  is a parameter. Thus, the first term of this function reflects how the sentences reflect the entire documents; the second term reflects the relevance of the sentences to the query; and finally the function penalizes redundant sentences. We set  X  to 0 . 8 and the scaling factor used in the algorithm to 0 . 3 based on a preliminary experiment with a part of the ACLIA1 development data. We also tried incorporating sentence position information (Radev, 2001) to our MMR baseline but this actually hurt performance in our preliminary ex-periments. 4.3 Variants of the Proposed Method To clarify the contributions of each components, the minimum dependency distance, QSB and the word pair, we also evaluated the following simplified ver-sions of QSBP. (We use the itf version by default, and will refer to the idf version as QSBP(idf). ) To examine the contribution of using minimum depen-dency distance, We remove distance ( w,w 0 ) from Eq.1 and Eq.2. We call the method QSBP(nodist). To examine the contribution of using word pairs for score maximization (see Section 3.2) on the perfor-mance of QSBP, we replaced Eq.3 with: To examine the contribution of the QSB relevance scoring (see Section 3.1) on the performance of QSBP, we replaced Eq.3 with: We will refer to this as WP. Note that this relies only on base word scores and is query-independent. 4.4 Results Tables 2 and 3 summarize our results. We used the two-tailed sign test for testing statistical signif-icance. Significant improvements over the MMR baseline are marked with a  X  (  X  =0.05) or a  X  (  X  =0.01); those over QSBP(nodist) are marked with a ] (  X  =0.05) or a ] are marked with a  X  (  X  =0.05) or a  X   X  (  X  =0.01); and those over WP are marked with a ? (  X  =0.05) or a (  X  =0.01). From Table 2, it can be observed that both QSBP and QSBP(idf) significantly outperforms QSBP(nodist), QSB, WP and the baseline in terms of all evaluation metrics. Thus, the minimum depen-dency distance, Query Snowball and the use of word pairs all contribute significantly to the performance of QSBP. Note that we are using the ACLIA data as summarization test collections and that the official QA results of ACLIA should not be compared with ours.
 QSBP and QSBP(idf) achieve 0 . 312 and 0 . 313 in F3 score, and the differences between the two are not statistically significant. Table 3 shows the F3 scores for each question type. It can be observed that QSBP is the top performer for BIO, DEF and REL questions on average, while QSBP(idf) is the top performer for EVENT and WHY questions on average. It is possible that different word scoring methods work well for different question types. We proposed the Query Snowball (QSB) method for query-oriented multi-document summarization. To enrich the information need representation of a given query, QSB obtains words that augment the original query terms from a co-occurrence graph. We then formulated the summarization problem as an MCKP based on word pairs rather than single words. Our method, QSBP, achieves a pyramid F3-score of up to 0.313 with the ACLIA2 Japanese test collection, a 36% improvement over a baseline using Maximal Marginal Relevance.

Moreover, as the principles of QSBP are basically language independent, we will investigate the effec-tiveness of QSBP in other languages. Also, we plan to extend our approach to abstractive summariza-tion.
