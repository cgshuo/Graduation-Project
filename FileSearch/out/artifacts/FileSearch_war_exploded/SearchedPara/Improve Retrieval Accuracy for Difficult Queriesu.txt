 How to improve search accuracy for difficult topics is an under-addressed, yet important research question. In this paper, we con-sider a scenario when the search results are so poor that none of the top-ranked documents is relevant to a user X  X  query, and propose to exploit negative feedback to improve retrieval accuracy for such difficult queries. Specifically, we propose to learn from a certain number of top-ranked non-relevant documents to rerank the rest un-seen documents. We propose several approaches to penalizing the documents that are similar to the known non-relevant documents in the language modeling framework. To evaluate the proposed meth-ods, we adapt standard TREC collections to construct a test col-lection containing only difficult queries. Experiment results show that the proposed approaches are effective for improving retrieval accuracy of difficult queries.
 Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Retrieval models General Terms: Algorithms Keywords: negative feedback, language modeling, difficult queries
Due to inherent limitations of current retrieval models, it is in-evitable that some queries are difficult in the sense that the search results would be poor. Indeed, some queries may be so difficult that a user can not find any relevant document in a long list of top-ranked documents even if the user has reformulated the queries several times. Clearly, how to improve the search accuracy for such difficult queries is both practically important and theoretically in-teresting.

In this paper, we consider the scenario when the search results are so poor that none of the top-ranked documents is relevant to a user X  X  query. In such a scenario, the feedback information that a user could provide, either implicitly or explicitly, is all negative. An interesting question is thus how to exploit only non-relevant infor-mation to improve search accuracy, which is referred to as negative feedback .

Although several kinds of feedback, including relevance feed-back, pseudo feedback, and implicit feedback, have been exten-sively studied in information retrieval, most existing work on feed-back relies on positive information, i.e., exploiting relevant docu-ments or documents that are assumed to be relevant. The basic idea is generally to extract useful terms from positive documents and use them to expand the original query or update the query model. When positive documents are available, they are generally more useful than negative documents [1]. As a result, how to exploit negative documents for feedback has been largely under-addressed. The only work that we are aware of is query zone [6]. But this work is in the context of document routing where many relevant docu-ments are available. In contrast, we focus on feedback based solely on negative documents in the context of ad hoc search.

Indeed, when a query is difficult, it is often impossible to ob-tain positive documents for feedback. Thus the best we could do is to exploit the negative documents to perform negative feedback. In this paper, we study negative feedback in the language mod-eling retrieval framework. Our basic idea is to identify the dis-tracting non-relevant information from the known negative exam-ple documents, and penalize unseen documents containing such information. While this idea is naturally implemented in a tradi-tional feedback method such as Rocchio [5], we show that it can-not be naturally implemented in the language modeling approach. We thus propose a heuristic implementation of this idea in the lan-guage modeling approach in a similar way to how it is implemented in Rocchio. Specifically, we would first estimate a negative topic model based on the negative example documents, and then com-bine this negative model with the original query model to penal-ize documents whose language models are similar to the negative topic model. We further propose two strategies to improve this ba-sic negative feedback method: First, we propose to down-weight or eliminate query terms in the learned negative model. The idea is so that the learned negative model would be focused on the truly distracting aspects rather than the query related aspects in a non-relevant document. Second, we propose to model multiple possible distracting negative subtopics in the negative examples documents, so that we can penalize a document as long as it is similar to one non-relevant document or one non-relevant aspect.

To evaluate the effectiveness of the proposed methods, we con-struct a test collection containing only difficult queries from TREC collections. Experiment results show that the proposed basic neg-ative feedback method is effective for improving ranking accuracy of difficult queries, and query term elimination can help further improve ranking effectiveness. However, modeling multiple nega-tive models does not show a clear gain and further investigation is needed.
Given a query Q and a document collection C , a retrieval system returns a ranked list of documents L .Weuse L i to represent the document at i -th position in the ranked list.

We focus on the scenario when a query is very difficult such that a user cannot find any relevant document in the top f ( f =10 in this study) ranked documents. In this case, the user can provide the retrieval system with negative feedback information either explic-itly or implicitly (e.g., skipping these f documents or clicking on the  X  X ext X  button). Our goal is to study how to use these negative example documents, N = { L 1 , ..., L f } , to effectively rerank the next r ( r = 1000 in this study) unseen documents in the original ranked list: U = { L f +1 , ..., L f + r } .

Our problem setup can be regarded as a special case of relevance feedback, where all the feedback information is negative. However, since no positive example is assumed to be available, our problem is much more challenging than regular relevance feedback.
In this section, we study the problem of negative feedback in the language modeling framework. We first review the language mod-eling approach and discuss the difficulty of incorporating negative feedback information in any truncated query model. To overcome this difficulty, we then propose to use a heuristic method to incor-porate negative feedback through language modeling.
In the basic language modeling approach [4], we score a doc-ument D by the likelihood of generating query Q =( q 1 , ..., q from a document language model  X  D . That is, we first estimate a multinomial distribution  X  D for D and then compute The document model  X  D needs to be smoothed and an effective method is Dirichlet smoothing [9]: where c ( w, D ) is the count of word w in D , | D | is the length of D , p ( w |C ) is the collection language model, and  X  is a Dirichlet smoothing parameter. This smoothing method is what we will use in our experiments.

The above query likelihood method is quite related to the KL-divergence retrieval model in [3]. In KL-divergence model, a query model  X  Q is also estimated for a query Q . Then a document D is ranked based on the KL-divergence between  X  Q and  X  D where V is the set of words in our vocabulary.

Using the maximum likelihood estimation of  X  Q , it can be shown that ranking based on the KL-divergence is equivalent to ranking based on the query likelihood [3]. Therefore, query likelihood can be regarded as a special case of the KL-divergence method.
Indeed, one major motivation for the KL-divergence retrieval model is to address the difficulty in incorporating feedback into the query likelihood method [8]. In the KL-divergence model, one can cast feedback as updating the query language model. Unfortu-nately, while it is easy to incorporate feedback with positive doc-uments, it cannot naturally accommodate negative feedback. The reason is because with a query model, in which every term has a non-negative probability, it is difficult to penalize a term with-out including all other terms. Specifically, if we are to penalize a term, the best we could do is to assign a very small or zero, but non-negative probability to it. Unfo rtunately, in a truncated query model (i.e., only keeping the most significant terms and assuming all others to be zero), a small non-zero probability for a distract-ing term (i.e., a term to be penalized) actually means that the term would be favored more than many non-distracting terms that aren X  X  in the truncated query model as the latter would all have zero prob-ability. In order to penalize such distracting terms, we would have to include all other terms with higher probabilities than these dis-tracting terms. Thus it is very difficult to incorporate negative infor-mation by updating query model and we need some new negative feedback methods in the language modeling approaches.
Since neither the query likelihood method nor the KL-divergence method can naturally support negative feedback, we propose a heuris-tic modification of the KL-divergence retrieval method to perform negative feedback. Intuitively, in negative feedback, we would like to push down the documents that are similar to the known nega-tive example documents. Following the spirit of language models, one way to do this would be to estimate a negative topic language model  X  N . We could then use  X  N to retrieve documents that are potentially distracting and compute a  X  X istraction score X  for each document. The distraction score of a document can then be com-bined with the original KL-divergence score of the document in such a way that we would penalize a document that has a high dis-traction score.

Specifically, let  X  Q be the estimated query model for query Q and  X  D be the estimated document model for document D .Let  X  be a negative topic model estimated based on the negative feedback documents N = { L 1 , ..., L f } . For negative feedback, we would score D with respect to Q as follows: where  X  is a parameter that controls the influence of negative feed-back. When  X  =0 , we do not perform negative feedback, and the ranking would be the same as the original ranking according to  X  We call this method the basic negative feedback model (BasicNFB) to distinguish it from some other extensions that we will propose later.

After some simple algebra transformation and ignoring those constants that do not affect ranking of documents, it is easy to show that Equation (1) can be rewritten as:
Score ( Q, D )=  X  D (  X  Q ||  X  D )+  X   X  D (  X  N ||  X  D )
In this new form of the BasicNFB scoring formula, we see that each term has a weight of [ p ( w |  X  Q )  X   X   X  p ( w |  X  which penalizes a term that has high probability in the negative topic model  X  N . Thus the BasicNFB model is essentially very sim-ilar to Rocchio in the vector space model [5] and can in some sense be regarded as the language modeling version of Rocchio. Clearly, our main question now is how to estimate  X  N , which will be dis-cussed below.
Given a set of non-relevant documents N = { L 1 , ..., L f would like to learn a negative topic language model  X  N from this set of documents. This is very similar to the case when we need to perform positive feedback with positive example documents. Thus we can use the same mixture model as used in [8] for pseudo feed-back to estimate  X  N .

Specifically, we assume that all the non-relevant documents are generated from a mixture of a unigram language model  X  N (to gen-erate non-relevant information) and a background language model (to generate common words). As usual in language modeling, we use the collection language model p ( w |C )= c ( w, C ) background model. Then the log-likelihood of the sample N is
L ( N |  X  N )= where  X  is a mixture parameter which controls the weight of the background model. Given a fixed  X  , a standard EM algorithm can then be used to estimate parameters p  X  ( w |  X  N ) : The result of the EM algorithm would give a discriminative nega-tive model  X  N which eliminates background noise.
The negative model learned above is based on the top documents returned to a query. This means all these documents may have high occurrences of query terms. As a result, the query terms would likely have high probabilities in the negative feedback model. This could make the negative feedback model biased and thus ineffec-tive to identify those truely irrelevant documents. To address this problem, we propose to eliminate the query terms from the negative model by setting their probabilities to zero and name this technique as  X  X uery term elimination. X 
While positive example documents are generally coherent, neg-ative feedback examples may be quite diverse as different negative documents may distract in completely different ways. Thus a single negative topic model may not be optimal. In this section, we pro-pose to estimate multiple negative models and use them to perform negative feedback.

A principled way of achieving multiple negative models is to learn subtopics from the negative documents. We use the modified probabilistic latent semantic analysis (PLSA) model [2] proposed in [10] to estimate k topics from N , each corresponding to a uni-gram langauge model {  X  i :1  X  i  X  k } .

To compute a distraction score of a document D with multiple negative topics,we compute the KL-divergence of  X  D and each of thenegativemodels  X  i , and choose the minimum divergence (i.e., highest similarity) as the distraction score of the document. This distraction score is then combined with the original KL-divergence as in BasicNFB. That is, Score ( Q, D )=  X  D (  X  Q ||  X  D )+  X  min { D (  X  i ||  X  D Note that a special case of this method is to have each document as a cluster and let each document define its own negative model.
Directly implementing the multiple negative models is quite ex-pensive since we have to find a minimum negative model for every document. An efficient way is to do it reversely. That is, for each negative model  X  i , we use the KL divergence model to rank all the documents in the collection. We then select the top n docu-ments for each negative model and form a pool set D P . For each document d  X  D P , we compute Score ( Q, d ) using the formula above. For any other document d  X  D P ,wesimplyscore d as  X 
D (  X  Q ||  X  d )+  X   X  c ,where c =max the query term elimination technique can also be used with multiple negative models.
We set up our experiments to simulate the following real-world scenario of difficult queries. After submitting a query to a search engine, a user would go through the top 10 ranked documents, but find that none of them is relevant, so the user would click on the  X  X ext X  button to view the second result page. Our goal is to im-prove the ranking accuracy of unseen results after the user clicks on the  X  X ext X  button. Specifically, given a baseline retrieval method, we identify the top 10 ranked documents and treat them as already seen by the user. We then exclude them and study different feed-back techniques to re-rank the remaining 1,000 unseen documents.
We use the TREC 2004 ROBUST track document collection, which contains approximately 528,000 documents [7]. Since nega-tive feedback is meant to help difficult topics, in our evaluation, we use only the relatively difficult topics from the 249 queries used in the ROBUST track of TREC 2004 [7]. Specifically, we choose the topics with low precision at 10 documents (P@10) according to a baseline method (the KL-divergence model with Dirichlet smooth-ing [9]). Based on such a criterion, we created two query sets: Hard1 : 51 queries, for which, the baseline system returned at most 1 relevant document in top 10 and at most 3 relevant documents in top 20 documents. To make the topic fit our evaluation setup, we remove all relevant documents in the top 10 results as if they were not existing in our collection.
 Hard2 : 26 queries, for which, the baseline system failed to return any relevant document in the top 10 ranked documents.

We use two sets of queries because they complement each other in the sense that Hard2 better reflects the real scenario while Hard1 has more queries to experiment with. For both query sets, we pre-process documents and queries with stemming, but without remov-ing any stopword.

We use two sets of performance measures: (1) MAP and gMAP, which serve as good measures of the overall ranking accuracy. (2) MRR, and P@10, which reflect the utility for users who only read the very top ranked documents. Due to the space limit, we only show the optimal results for different methods after tuning the pa-rameters (we ended up setting  X  =0 . 5 and  X  =0 . 8 ).
We compare the optimal performance of our proposed methods with the baseline and traditional model-based feedback method [8]. We use the following notations: BL is KL-divergence with Dirich-let smoothing [9]. PFB is the model-based feedback method [8]. BasicNFB is the proposed basic negative feedback model. QTE represents query term elimination technique. MNFB-single rep-resents the proposed multiple negative models using single docu-ments as non-relevant aspects. MNFB-cluster represents the pro-posed multiple negative models using clustering method and we set the number of clusters to 3.

The results are shown in Table 1. We can see that the observa-tions on these two query sets are mostly consistent. It is clear that traditional expansion-based method (PFB) can not improve the re-trieval performance for these difficult queries; instead, they hurt the performance substantially. This should not be surprising since PFB blindly takes non-relevant information as relevant to update query models. On the contrary, our proposed methods are more effective for negative feedback and outperform both the baseline and tradi-tional pseudo-feedback method. This confirms our hypothesis that negative feedback does help improve the accuracy. Compared with BasicNFB, multiple negative models (MNFB-single and MNFB-cluster) are more effective on the Hard2 query set but less effective on Hard1 query set. Overall, multiple negative models do not show a clear gain over the basic negative model. One possible reason is that only 10 documents can not give a reliable cluster structures due to the local maximum problem of the clustering method.
We examine the effectiveness of the proposed query term elim-ination in Table 2. For BasicNFB, it is clear that query term elim-ination helps improve the performance on both query sets for all measures except for MAP on Hard2, in which case, the MAP value of BasicNFB+QTE is lower mainly because BasicNFB+QTE did poorly on one single topic (topic 320), causing the arithmetic mean to be lower. However, for MNFB-single and MNFB-cluster, query term elimination appears to be ineffective. One possible reason is because the basic negative model is learned from all the top results of a query, which presumably have a high concentration of query terms, thus the query terms would be more salient in the learned negative model than in the case of multiple negative model meth-ods, where the negative model is learned only from one single doc-ument or subtopic/cluster.
In this paper, we show that when a query is difficulty and the search results are so poor that none of the top-ranked documents is relevant to a user X  X  query, we may exploit negative feedback to improve retrieval accuracy. Performing negative feedback with the language modeling approach is non-trivial. We propose a KL-divergence approach to negative feedback which is in spirit similar to Rocchio for the vector space model [5] with the main idea be-ing to penalize those documents that are similar to the known non-relevant documents. To evaluate the proposed methods, we adapt standard TREC collections to construct a test collection containing only difficult queries. Experiment results show that the proposed approaches are effective for improving the retrieval accuracy of dif-ficult queries.

To the best of our knowledge, our work is the first to study feed-back with only non-relevant documents. There are some natural future research directions based on this work, including investigat-ing how to automatically set the parameters and developing a more principled way to do negative feedback in the language modeling framework.
This work is in part supported by the National Science Founda-tion under award numbers IIS-0347933 and IIS-0713581.
