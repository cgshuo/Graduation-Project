 1. Introduction solving processes. However, fi nding the optimal solution for such problems is often tedious, especially in the presence of non-linearity, high dimensionality, and multi-modality. Over the last few decades, evolutionary algorithms (EAs) have shown tremen-dous success in solving complex optimization problems. Although the EA family contains a number of different algorithms, the genetic algorithm (GA) is the most popular and widely used in practice ( Goldberg, 1989 ).

Although two-parent crossover is widely used in practice, a good number of multi-parent (more than two parents) crossovers, such as unimodal distribution crossover (UNDX) ( Ono et al., 2003 ), simplex crossover (SPX) ( Tsutsui et al., 1999 ), parent centric crossover (PCX) ( Deb et al., 2002 ) and triangular crossover (TC) ( Elfeky et al., 2008 ), have also been employedfor solving optimiza-tion problems. UNDX uses multiple parents and creates offspring solutions around the center of mass of these parents, while a small probability is assigned to solutions away from the center of mass.
Although it has shown excellent performance for highly epistasis problems ( Ono et al., 2003 ), it may fail to generate offspring in some cases such as when the population size is too small relative to the search space. UNDX also has dif fi culty in fi nding an optimal solution near the boundaries of the search space ( Ono et al., 2003 ).
Simplex crossover (SPX) is a multi-parent recombination operator for real-coded GAs. The SPX operator assigns a uniform probability distribution for creating offspring within a restricted search space around the region marked by the parents. SPX uses the property of a simplex in the search space. It works well on functions having multimodality and/or epistasis with a medium number of parents, such as three parents on a low dimension function and four parents on a high dimension function ( Tsutsui et al., 1999 ).
However, it fails on functions that consist of tightly linked sub-functions ( Tsutsui et al., 1999 ). PCX allows a large probability of creating a solution near each parent, rather than near the centre of the parents, in which for each offspring one parent is selected and a difference vector is calculated between the parent and the chosen m parents. PCX applies a self adaptive approach where a new solution vector keeps moving towards the optimum ( Deb et al., 2002 ). Note that when PCX is applied with GA, it takes longer time in comparison to other crossover operators, and it fi nds dif fi culty in solving separable multimodal problems ( R X nkk X nen, 2009 ). TC uses three parents for constrained pro-blems, where two parents must be feasible and one infeasible, to generate a set of three offspring, where each offspring is generated as a linear combination of those three parents. TC works well where the optimal solution lies on the boundary of the feasible region of problems that have a single bounded feasible region in the continuous domain ( Elfeky et al., 2008 ).

In this research, we have consider the fact that, for a better performance of GA, the distribution of offspring should neither be extremely narrow nor extremely wide in comparison to their parents. If the generated offspring are distributed narrower than their parents, they may lose diversity and converge prematurely. In contrast, if the offspring are distributed extremely widely, they may be too diverse and take too long in converging to optimality ( Ono et al., 2003 ). So it should appropriately generate offspring that satis fi es a balance between exploration and exploitation. Considering these facts, in this paper, we have introduced a new three-parent crossover with GA that generates three new off-spring. Two of these offspring are to help exploitation while the third offspring is for promoting exploration. A diversity operator is also used in the algorithm that helps to escape from local optima thus avoids premature convergence. The algorithm has been tested by solving the CEC2006 benchmark problems, as well as a variety of complex real world problems, such as those concerning frequency-modulated sound waves, catalyst blend optimal control, transmission network planning, transmission pricing, antenna design, static and dynamic despatching, and spacecraft trajectory optimization. These problems are very hard in the sense that they have strong nonlinearities with many local minima. We have compared our results with other state-of-the-art algorithms and show thereby the superior performance of the proposed algo-rithm. It is worthy to mention here that this research study is different than ( Elsayed et al., 2011b ), in which a small set of test problems was used. In addition to that different constraint hand-ling techniques plus a local search were used. Furthermore, no analysis study was provided. This paper is also different from Elsayed et al. (2011a) , in which a different constraint handling method was used as well as no deep analysis was provided. This paper is organized as follows: after the introduction, Section 2 presents the proposed crossover, the diversity operator, the design of our algorithm and its similarities and differences to other EAs. Section 3 describes the used problems. The experi-mental results, and the analysis of those results, are presented in Section 4 . Finally, the conclusions are given in Section 5 . 2. GA with multi-parent crossover (GA-MPC)
In this section, fi rstly, we describe the proposed crossover and the diversity operator. The proposed algorithm is then presented, as well as the constraint handling technique that is used in this research.

To begin with, let us de fi ne the mathematical model for a constrained optimization problem (COP) min f  X  X Subject to g  X  X !  X  r 0 ; k  X  1 ; 2 ; ... ; K ; h  X  X !  X  X  0 ; e  X  1 ; 2 ; ... ; E ; L r x j r U j ; j  X  1 ; 2 ; ... ; D ; where X objective function, g k  X  X e th equality constraint, and each x j has a lower limit L upper limit U j . 2.1. The proposed crossover
In this research, we deal with real-valued encoding. We propose a multi-parent crossover (MPC) with the following steps: (1) Based on a selection rule, store the individuals that will be (2) Any duplication in the selected three individuals is removed by replacing the unwanted individual with a random individual from the selection pool. (3) Rank these three individuals from the best ( x ! 1 ) to the worst ( x ! violations. (4) Generate a random number  X  that follows a normal distribu-tion with mean value m and standard deviation s . (5) Generate three offspring  X  o i  X  o ! o ! o ! where f  X  x ! 1  X  r f  X  x ! 2  X  r f  X  x ! 3  X  .

The idea behind MPC comes from the heuristic crossover ( Wright, 1991 ), in which one offspring ( y ! ) is generated from a given pair of parents  X  x ! 1 ; x ! 2  X  , such that y !  X  X  x  X  x ! similarly to the multi-parent crossovers TC, PCX, SPX, and UNDX, as discussed earlier. In our case, the difference vectors in the above equations are not in the same order. The order in Eq. (3) is set differently from the same in Eqs. (2) and (4) . In fact, Eqs. (2) and (4) are designed to move toward better fi tness while Eq. (3) is to diversify the population. 2.2. A diversity operator
In this paper, we also propose a diversity operator to further diversify the generated offspring. While applying the diversity operator, two points should be considered: the proportion of population undergoing such operation (based on the diversity probability), and the scale of contribution of the operator (based on the information used to generate the fi nal offspring).
In implementing the proposed diversity operator, we use information from the generated offspring o i and from the archive pool ( A ) that contains the best m individuals in the current generation. The fi nal offspring o i  X f o 1 i ; o 2 i ; ... ; ated based on the diversity operator as of the following pseudo-code: For i A f 1 ; 2 ; ... PS g s For j A f 1 ; 2 ; ... D g If rand  X  0 ; 1  X  o p
Select a random individual from the archive pool ( A arch End End
End where p is a diversity probability ,o i the i th offspring, PS the fi not want to use information from a single vector as that may lead to becoming stuck in a local minima. 2.3. GA-MPC
In GA-MPC, fi rst an initial population is generated randomly, with size PS . Then an archive pool is fi lled with the best m individuals (based on their constraint violations and/or fi function). Then a tournament selection procedure with size tc takes place, from which the best individual is chosen and saved in the selection pool. For the crossover operation, with a crossover rate ( cr ), for each three consecutive individuals in the selection pool, three offspring are generated as described before.After we generate all offspring, we apply a diversity operator, with a probability p , to escape from any local minima and to visit better regions in the search space. After that we merge the individuals from the archive pool with all of the offspring, and the best PS individuals are selected as a new population for the next genera-tion. Also, to ensure more diversity, if any individual in the population is exactly same to another one, then one of them is shifted within the boundary with N  X  0 : 5 u ; 0 : 25 u  X  , where u
Table 1 shows the steps of GA-MPC. 2.4. Constraint handling which the objective function is penalized as f  X  X
The penalty value, which represents the constraints, here is equal to the sum of the individual violation of all of the violated constraints. The penalty part is handled as follows:
Penalty  X  max  X  0 ;  X  K where tol is updated according the following equation: tol  X  where initial _ tol is equal to the penalty value for the top individual and  X   X  (0.20 PS ) t is the generation number, n 1is equal to 15% of the maximum generations ( T max ), n 2 is equal to 35%
The idea is quite similar to the  X  -Constraint method ( Takahama and Sakai, 2009 ). 2.5. Similarity and differences to other EAs the design is different from other GA crossovers, in which our proposed crossover neither uses a mean-centric probability dis-tribution, such as UNDX and SPX, nor uses aparent-centric approach, such as PCX. In addition, we are not using a linear combination of parent vectors to generate the offspring as in TC. As described earlier, it is also different from the heuristic crossover.
In our case, three offspring are generated from three parents as opposed to one offspring from a pair of parents in heuristic crossover.

One may fi nd the proposed crossover is somewhat similar to the mutation strategy of differential evolution (DE) ( Storn and
Price, 1995 ). In DE, for each parent vector from the current population (target vector), a mutant vector (donor vector) is obtained. The simplest form of generating the donor vector is that three distinct parents are selected, and the difference of two of the parents multiplied by the ampli fi cation factor is added to the third vector. Note that all of the three parents must be different from the target vector. Finally an offspring is formed by combining the donor with the target vector. The resulting child of that combina-tion is called trial vector. So the similarity is that both GA-MPC and DE use a differential operation. The differences are: For each parent in DE, only one offspring is generated, while in
GA-MPC for each three distinct parents, three offspring are generated.

A recombination methodology between the target and the donor is used to generate the trial vector, while in GA-MPC a diversity operator with a probability p is used between each generated offspring and different individuals within the archive pool.

In DE, the parents used to generate the donor vector are selected randomly, must be different to each other and differ-ent to the target vector. In GA-MPC, the selection of the parents for crossover operation is based on a tournament, and they must be different to each other.

In DE, the better of the two vectors (the target and the trial vector) is selected for the next generation. In GA-MPC, the best
PS individuals from the mix of the archive pool individuals and all generated offspring are selected for the next generation. 3. Problems description
In this section, we describe the 24 well-known constrained benchmark problems, and a number of engineering optimization problems, that we have used to judge the performance of the proposed algorithm.
 3.1. Benchmark problems These benchmark problems have been taken from the 2006 IEEE Congress on Evolutionary Computation (CEC'2006) ( Liang et al., 2005 ) competition on single objective constrained problems and their details are shown in Table 2 . 3.2. Engineering optimization problems
As we mentioned earlier, a variety of engineering optimization problems are considered in this research. These problems vary in terms of mathematical properties, presence and absence of func-tional constraints, number of variables, static or dynamic in nature, and modality. For constrained problems, the number of constraints and type of constraints also vary. These problems are brie discussed below. 3.2.1. P01: Parameter estimation for frequency-modulated (FM) sound waves
The problem is to specify six parameters, a 1 ;  X  1 ; a 2 of a sound modulation model represented by the following equation: y  X  t  X  X  a 1 sin  X   X  1 t  X   X  a 2 sin  X   X  2 t  X   X  a 3 sin  X  where  X   X  X  2  X  = 100  X  . The fi tness function ( f )isde fi f  X   X  100 y  X  t  X  X  1 : 0 sin  X  5 t  X  1 : 5 sin  X  4 : 8 t  X   X  2 sin  X  4 Each parameter is in the range [ 6.4, 6.35]. This problem is a highly complex multimodal one having strong epistasis, with minimum value f n  X  0. This problem has been previously studied by a GA ( Herrera and Lozano, 2000 ). 3.2.2. P02: Lennard-Jones potential problem
This is a potential energy minimization problem that involves the minimization of molecular potential energy associated with a pure Lennard-Jones (LJ) cluster ( Hoare, 2007 ; Moloi and Ali, 2005 ).
This problem is a multi-modal optimization problem comprised of an exponential number of local minima ( Hoare, 2007 ). Most of the global minima have structures based upon the Mackay icosahe-drons and can be seen in the Cambridge Cluster Database ( http:// wwwwales.ch.cam.ac.uk/CCD.html ). An algorithm can be tested over this function for its capability to conform its molecular structure, where the atoms are organized in such a way that the molecule has minimum energy. The LJ potential energy between two atoms separated by distance r is given by LJ  X  r  X  X  1 r 12 2 r 6  X  11  X 
The total potential energy f LJ of a cluster of n p atoms is de f  X  X ; ... ; X n where X 1 A R 3 represents the coordinates of the i th atom and the norm is the Euclidean distance. 3.2.3. P03: the bifunctional catalyst blend optimal control problem
This is a hard multimodal optimal control problem which has many local optima, as many as as 300 local optima have been reported in ( Esposito and Floudas, 2000 ). The proposed problem is a chemical process which converts methylcyclopentane to ben-zene in a tubular reactor. The catalyst contains a hydrogenation component and an omerization component. The objective is to determine the mixture of the two along the length of the reactor which maximizes the concentration of the desired product in a given reaction scheme. 3.2.4. P04: optimal control of a non-linear stirred tank reactor
This is a model of a nonlinear continuous stirred tank reactor which involves two different local minima. This problem is a multimodal optimal control problem ( Ali et al., 1997 ). The optimi-zation objective is to determine a suitable value of u so that the performance index J  X  is minimized and where the initial conditions are x  X  0  X  X  X  0 : 09 0 : 09 T . Though the problem is unconstrained, the initial guess for u  X  t  X  lies in [0.0 5.0], where t A  X  t t  X  0 : 72. In this research, the integration involved in evaluation is performed using a sub-function ode 45 that is available in MATLAB with relative tolerance set to 0.1. 3.2.5. P05 Tersoff potential function minimization problem
Tersoff potential governs the interaction of silicon atoms with a strong covalent bonding. Tersoff has given two parameterizations of silicon and these are called Si(B) and Si(C), and the two sets of parameter values respectively for Si(B) and Si(C) are tabulated in Table 2 ( Ali and T X rn, 2000 ).

Although the Tersoff potential problem is N 3 dimensional in 3-dimentional space, the number of dimensions to be evaluated can be decreased in light of the fact that it depends on the relative position of each atom instead of its actual Cartesian coordinates.
This problem is dif fi cult, because the presence of a large number of local minimizers, even for a system with a small number of atoms, creates numerous regions of attraction for local searches ( Ali and T X rn, 2000 ). In this research we consider a 10 atoms problem, so the number of decision variables is 30. 3.2.6. P06: spread spectrum radar polly phase code design appropriate waveform is a key point. The problem under con-sideration is modeled as a min  X  max nonlinear non-convex opti-mization problem with continuous variables and numerous local optima, and in which the objective function is piecewise smooth ( Dukic and Dobrosavljevic, 1990 ). The problem can be considered as an NP -hard problem. The objective is to minimize the module of the biggest among the samples of the so called auto-correlation function which is related to the complex envelope of the com-pressed radar pulse at the optimal receiver output, while the variables represent symmetrized phase differences. 3.2.7. P07: transmission network expansion planning (TNEP) problem of consists in fi nding the optimal plan for an electrical system expansion, that is, it must specify the transmission lines and/or transformers that should be constructed so that the system can operate in an adequate way during a speci fi ed planning horizon ( de J Silva et al., 2005 ). This problem can be considered dif fi cult because of its nonconvex nature.
 sion planning problem without security constraints presents the following compact structure: min v  X   X  Subject to
Sf  X  g  X  d 0 r g r g 0 r n ij r n ij where n ij are integers, and f i ; j and  X  j are unbounded,  X  i .

Respectively, c ij ,  X  i ; j , n ij , n 0 ij , f i ; j and f can be added to a right-of-way i j , the susceptance of that circuit, the number of circuits added in a right-of-way i j , the number of circuits in the base case, the total power fl ow and the correspond-ing maximum power fl ow by circuit in a right-of-way i j . The variable v is the investment, S is the branch-node incidence transposed matrix of the power system, f is a vector with whose maximum value is g , d is the demand vector, n ij is the maximum number of circuits that can be added in a right-of of-way. 3.2.8. P08: large scale transmission pricing problem try have resulted in the unbundled services provided by electric utilities. Power wheeling is one of the most prevailing unbundled services which can be provided by the electric utilities. Pricing of transmission services plays a crucial role in determining whether providing transmission services is economically bene fi cial to both the wheelers and the customers ( Galiana et al., 2003 ). transmission pricing to be adopted. In ( Christie et al., 2000 ), authors proposed the principle of equivalent bilateral exchanges (EBE). That principle stated that since a solved optimal power meets the laws of Kirchhoff without violating any line fl generation limit, each generation injection fl ows without impedi-ment toward all of the demands, while each demand is fed by all of the injected generators.

Following the EBE principle above, an equivalent bilateral power exchange between the generation at bus i and the demand at bus j is de fi ned by
GD where, P sys d is the total load in the system. The net fl can be expressed in terms of equivalent bilateral power exchanges as pf
The objective is to solve the following equations min F  X  GD ij  X  X   X  2 4  X   X 
Subject to  X   X  3.2.9. P09: circular antenna array design problem
Antenna arrays have been widely used in different applications, including radar, sonar, biomedicine, communications, and imaging ( Mandal et al., 2011 ). Antenna arrays may be linear, two-dimen-sional, circular or spherical in element arrangement. A very popular type of antenna arrays is the circular array which has several advantages over other schemes. The problems are dif due to the complicated structure and overall array con fi ( Mandal et al., 2011 ).

Let us consider N antenna elements spaced on a circle of radius r in the x -y plane. The objective function is taken as
F  X j AR  X   X  sll ; I where AR is the array factor for a conventional linear array.
The fi rst component attempts to suppress the side-lobes. the angle at which the maximum side-lobe level is attained. The second component attempts to maximize directivity of the array pattern. Nowadays directivity has become a very useful fi merit for comparing array patterns. The third component strives to drive the maxima of the array pattern close to the desired maxima des , DIR is the directivity for the radiation pattern The fourth component penalizes the objective function if suf fi cient null control is not achieved, num is the number of null control research we use the following instantiation of the design pro-blems: number of elements in circular array  X  12, x 1  X  any string within bounds, null  X  [50,120] in radians (no null control), des  X  180 and distance  X  0.5. 3.2.10. P10: dynamic economic dispatch (DED) problem
Dynamic economic dispatch (DED) is one of the main functions of power generation operation and control. It determines the optimal settings of generator units with predicted load demand over a certain period of time. The objective is to operate an electric power system most economically while the system is operating within its security limits. DED is a complicated problem with nonlinear constraints.

It is a dynamic optimization problem taking into account the constraints imposed on system operation by generator ramping rate limits. The DED is not only the most accurate formulation of the economic dispatch problem but also the most dif fi cult to solve because of its large dimensionality ( Attaviriyanupap et al., 2002 ).
Normally, the DED problem can be formulated as follows. The objective function is f  X   X  n where  X  1 and  X  r are penalty parameters, n is the number of hours, N is the number of units. The penalty factors regulate the objective function such that the algorithm gives a higher cost value rather than directly judging the solutions as infeasible. The penalty term re fl ects the violation of the equality constraint and assigns a high cost penalty function. The P rlim is de fi ned by P In this research we solve two instances, with N equal 5 and 10, respectively. 3.2.11. P11: static economic load dispatch (ELD)
The economic dispatch problem concerns the question of how to distribute a power load over the units that are in service so that the total fuel cost is at a minimum.

The objective function corresponding to the production cost can be approximated as a quadratic function of the active power represented as follows: F  X   X  where f i  X  P i  X  X  a i P 2 i  X  b i P i  X  c i ; i  X  1 ; 2 ; ... ; the cost function corresponding to the i -th generating unit and a line generating units to be dispatched. The cost function for a unit with a valve point loading effect is calculated by using f  X  P i  X  X  a i P 2 i  X  b i P i  X  c i  X j e i sin  X  f i  X  P min point loading effect.

This problem is subjected to different constraints depending upon assumptions and practical implications, as shown below (1) Power balance constraints or demand constraints: this con-(2) The generator constraints: the output power of each generat-these bounds. This constraint is represented by a pair of inequality constraints as follows:
P where P min i and P max i are the lower and upper bounds for power outputs of the i th generating unit. (3) The ramp rate limits: this assumption concerns the effect whereby adjustments of the power output are instantaneous.
However, under practical circumstances, ramp rate limits restrict the operating range of all the on-line units for adjust-ing the generator operation between two operating periods.
The generation may increase or decrease with corresponding upper and downward ramp rate limits. So units are con-strained due to these ramp rate limits, such as:
P i P t 1 i r UR i , if power generation increases, and P t 1
DR i if power generation decreases, where P t 1 i is the power generation of unit i in the previous hour and UR i and DR the upper and lower ramp rate limits respectively. The inclu-sion of ramp rate limits modi fi es the generator operation constraints as follows: max  X  P min i ; UR i P i  X  r P i r min  X  P max i ; P t 1 i (4) Prohibited operating zone: the generating units may have certain ranges where operation is restricted on the grounds of physical limitations of machine components or instability, e.g. due to steam valve or vibration in shaft bearings. Conse-quently, discontinuities are produced in cost curves corre-sponding to the prohibited operating zones. Hence, there is a desire to avoid operation in these zones in order to economize production. Symbolically, for a generating unit i ,
P pz r P i r P pz  X  28  X  where P pz and P pz are the lower and upper limits of a given prohibited zone for unit i .

In this research we consider 5 cases. Case 1: 6-generating units of an IEEE 30-bus system. Case 2: 13-generating units. Case 3: 15-generating units. Case 4: 40-generating units, and case 5: 140-generating units. 3.2.12. P12: short-term hydrothermal scheduling problem
Short-term hydrothermal scheduling is one of the constrained power system optimization problems, and has complex and non-linear characteristics with various types of constraints and non-linear relationships of problem variables that make the problem of fi nding the global optimum dif fi cult using any standard optimiza-tion methods and programming techniques ( Hota et al., 2009 ). The main requirement of this problem is to generate the optimal amount of generated power for the hydro and thermal units in the system, so that they meet the load demands in the scheduling horizon of 1 day or a few days, and satisfy different constraints on the hydraulic and power system network.

The objective function ( F ) is to minimize the fuel cost of running the thermal system to meet the demand in scheduling horizon. The objective function is expressed mathematically as min F  X   X  m where f j is the cost function corresponding to the equivalent thermal unit's power generation P Tj at the i -th interval. M is the total number of intervals considered for the short term schedule.
The cost function f j can be written as f j  X  P Tj  X  X  a j P 2 Tj  X  b j P Tj  X  c j  X j e j sin  X  f i
Subject to the following various system constraints (1) Demand constraints (2) Thermal generator constraints (3) Hydro generator constraint (4) Reservoir capacity constraint (5) The water discharge constraint (6) Hydraulic continuity constraint (7) The hydro power generation quadratic cost curve without prohibited discharge zones; instance 2: quadratic cost with prohibited discharge zones; and instance 3: systems with the valve point loading effect and with prohibited operating hydro discharge zones. 3.2.13. P13 Messenger-full: spacecraft trajectory optimization problem mission to Mercury modeled as an MGA-1DSM problem. It includes the fi nal resonant fl y-bys, and seeks to minimize the on-board propellant consumption. Messenger is on its way to
Mercury. It will become the fi rst spacecraft to ever orbit around included a long planetary tour: Earth  X  Earth  X  Venus  X  Venus Mercury  X  Mercury  X  Mercury  X  Mercury ( Biscani et al., 2010 ). The
Messenger-full problem is complex with an unknown optimal solution. For more details and code, the reader is referred to http:// www.esa.int/gsp/ACT/inf/op/globopt/MessengerFull.html . 3.2.14. P14 Cassini 2: spacecraft trajectory optimization problem spacecraft Cassini. The objective function represents the sum of all
DV required to reach Saturn using an Earth  X  Venus, Venus
Jupiter  X  Saturn fl y-by sequence with deep space manoeuvres. In this model for the Cassini trajectory: deep space manoeuvres are allowed between each one of the planets. For more details the readers are referred to: http://www.esa.int/gsp/ACT/inf/op/glo bopt/edvdvdedjds.htm . The Cassini 2 problem is complex with an unknown optimal solution. 4. Experimental results and analysis
In this section, we discuss the computational results, and analyze the performance of our algorithm.

The performance of GA-MPC has been compared with the state of the art algorithms. The algorithm has been coded using visual Matlab, and has been run on a PC with a 3 Core 2 Duo processor, 3.5 G RAM, and windows XP. The detailed results are shown in Table A1 . 4.1. GA-MPC Results based on the CEC2006 Problems
We set the parameters: PS  X  90,  X   X  N (0.7, 0.1) for all test problems, and cr  X  100%. If the offspring are generated using
Eq. (2) the diversity probability ( p ) is set to 0.05, and if the offspring are generated using Eq. (3) and p  X  0.1 (the justi shown in a later section). The rational of using different values for p is provided later in this section. The tournament size is selected randomly as 2 or 3. For the number of individuals that are selected to fi ll the archive pool m  X  50% of PS , 25 independent runs were performed for each test problem, the stopping criterion was to run for up to 240,000 fi tness evaluations (FEs), and the equality constraints are handled as: j h c  X  x !  X j  X  r 0 ; forc  X  1 E is the number of equality constraints and  X   X  0.0001. Firstly, we wish to compare the proposed algorithm with other
GAs. To do this, we selected SBX and PCX, which both of them use a non-uniform mutation (NU) with probability 0.1. The SBX and
PCX parameters are set to: PS  X  100, cr  X  0.95, tournament size  X  2 or 3 randomly, index parameter  X   X  3,  X   X   X   X   X   X  0 the non-uniform mutation, the mutation probability is 0.1 and, b  X  5. The detailed results (best, median, average, worst, and standard deviation ( St. d )) are shown in Appendix A .
To start with, all algorithms could not solve g20 and 22, and thus these two problems are discarded from our analysis. Based on the obtained results, we found that the feasibility ratio (number of feasible runs = 25), for GA-MPC was 100% while SBX-
NU was 89% and PCX-NU was 83%. In regard to the best values, we found that GA-MPC was better than both SBX-NU and PCX-NU for 16 and 15 test problems, respectively, while GA-MPC was able to obtain the same results for 6 and 7 test problems, respectively. In regard to the average results, GA-MPC was superior to both SBX-
NU and PCX-NU for 17 test problems each, while GA-MPC was able to obtain the same results for 5 test problems each  X  these 5 problems are quite simple ( Fig. 1 ).
 As an example, Fig. 2 shows the convergence patterns for GA-
MPC, SBX-NU and PCX-NU. From Fig. 2 , although SBX-NU is able to converge faster in the early few generations, GA-MPC is able to converge to the optimal solution, while the other two algorithms cannot.

It is also possible, however, to study the difference between any two stochastic algorithms in a more meaningful way. To this end, we have performed statistical signi fi cance testing. We have chosen a non-parametric test, Wilcoxon Signed Rank Test ( Corder and
Foreman, 2009 ) that allows us to judge the difference between paired scores when it cannot make the assumptions required by the paired-samples t test, such as that the population should be normally distributed. The results regarding the best and average fi tness values are presented in Table 3 . As a null hypothesis, it is assumed that there is no signi fi cant difference between the best and/or mean values of two samples. Whereas the alternative hypothesis is that there is a signi fi cant difference in the best and/or mean fi tness values of the two samples, with 5% signi cance level. Based on the test results/rankings, we assign one of three signs (  X  , , and ) for the comparison of any two algorithms (shown in the last column), where the  X   X   X  sign means the fi rst algorithm is signi fi cantly better than the second, the  X   X  sign means that the fi rst algorithm is signi fi cantly worse, and the  X   X  sign means that there is no signi fi cant difference between the two algorithms. From Table 3 , GA-MPC is clearly superior to both SBX-NU and PCX-NU in regard to the best and the average solutions obtained.

To continue this analysis, we have compared GA-MPC with the state-of-the-art algorithms such as APF-GA (adaptive penalty formulation with GA by ( Tessema and Yen, 2009 )), MDE (modi differntial evolution by ( Mezura-Montes et al., 2006 )) and ECHT-
EP2 (ensemble of constraint handling techniques based on an EP algorithm by ( Mallipeddi and Suganthan, 2010 )). We must men-tion here that GA-MPC and ECHT-EP2 use 240,000 FEs, while both APF-GA and MDE use 500,000 function evaluations (FEs). Based on the best results, GA-MPC, APF-GA, and MDE and
ECHT-EP2 reached the optimal solutions for 22, 17, 20 and 19 problems, respectively. As of the average results, GA-MPC was better than APF, MDE and ECHT-EP2 for 7, 3 and 7 test problems, respectively, while GA-MPC was inferior to MDE and APF-GA for only one test problem each.

Based on the statistical test, as shown in Table 3 , GA-MPC was better than APF-GA in regard to the best and average results, GA-
MPC was superior to ECHTEP-2 in regard to the average results, while there was no signi fi cant difference between GA-MPC and MDE. 4.1.1. Further performance analysis
As of the results discussed in the previous section, GA-MPC performs very well. In this section, we have designed three set of experiments and analyzed their results to support the reasons for its superior performance. 4.2. Experiment 1
In the fi rst generation ( t  X  1), from only 3 parents, 1000 off-spring have been generated using each Eqs. (2) , (3) and (4) . Here,
Eq. (2) is based on parent 1 (P1), Eq. (3) is based on parent 2 (P2), and so on. For each equation, the percentage of individuals that are better than their parent has been calculated (i.e. how many offspring generated by Eq. (2) , were better than P1, based on section 3.4, and so on). The same calculations has been done after applying the diversity operator. From the results, we found the following: (1) For the test problems with non-separable objective functions (i.e. g02), 12% of the generated offspring, due to Eq. (2) , are much better than P1. After the diversity operator (with p  X  0.1), the percentage was 31.1%. Based on Eq. (3) , 0.35% of the generated offpsring were much better than P2, while after the diversity operator (with p  X  0.1), the percentage was 36.9% (which is a 26.5% better result than P1). Based on Eq. (4) , 34% of the generated offspring was much better than P3, while after the diversity operator (with p  X  0.1), the percentage was 85.4% (which is a 27.3% better result than P1). This is summarized in Table 4 . (2) For the test problems with separable objective functions (i.e. g10), using a diversity probablity equal to 0.1, the summary results are presented in Table 5 .

Based on points a and b , we concluded that for the offspring that were generated by Eq. (2) , it was not best to use a high diversity probability ( p  X  0.1) for test problems with a separable objective function, while p  X  0.1 was good for the test problems with non-separable objective functions. For those offspring that were generated by Eqs. (3) and (4) , p  X  0.1 was a good choice. were generated by Eq. (3) , led to a high improvement of the results. The reason for this is that visiting different search areas may lead to obtain better results after applying the diversity operator, in which case getting information from good results and mixing this information with that information of bad solutions may lead to better results. Fig. 2 shows such performance. 4.3. Experiment 2
In this experiment, we study the effect of the diversity operator on the performance of GA-MPC. Although without using the diversity operator the algorithm converged fast in the early generations, this convergence was to local optima, as it is shown in Fig. 3 .

It was also important to analyze the effect of the diversity probability. We ran GA-MPC with different values: (a) p  X  0.05 for the offspring that were generated by Eq. (2) , while p  X  0.1 was used for the offspring that were generated by Eqs. (3) and (4) (the p  X  1. We found that option (a) was the best, see Fig. 4 . Note that although the use of a high value of p for those test problems with a non-separable objective function might lead to a faster conver-gence in the beginning of the evolution process, it then becomes stuck in local optima later on.

Consequently, it is important to measure the bene fi t of how the diversity operator encourages a reasonable diversity. To do this, we calculated the diversity in each generation of GA-MPC with and without the diversity operator mechanism. Here we cacluated the diversity ( di v ) as follows: div  X  where f i is the fi tness value of individual i , f is the mean mechanism leads to better diversity, as seen in Fig. 5 . 4.4. Experiment 3
In this experiment, we calculated the distance between the individuals and the known best solution (distance1) in the initial population, after 200 generations (distance2) and after 400 gen-erations (distance3). We found that distance2 was 96% close to the optimal solution, while distance3 was more than 99.6% nearer to the golobal optimum. This can show how fast the GA-MPC is. Fig. 6 shows these distances for both g02 and g10. The reasons for such fast convergence are: (1) in Eqs. (2) and (4) , the solutions are moved toward better results, (2) instead of using the global best in each generation to guide the search, different bests are used (i.e. for each 3 generated offspring, we might use a different P1 and P3, and hence the solutions might move toward different regions), (3) visiting diverse areas, due to Eq. (3) , could help to obtain good solutions quickly after mixing with good solutions. 4.4.1. Solving real world optimization problems
The detailed results (best, median, average, worst, and standard deviation ( St. d )) are shown in Appendix B . We must mention here that for the problems that have discrete variables, those discrete variables were handled as continuous ones, and then rounded off. The performance of GA-MPC was compared against two other DE algorithms, Adaptive Differential Evolution Algorithm (ADE) ( Asafuddoula et al., 2011 ) and another algorithm that uses differ-ent DE strategies, ensemble DE algorithm (EPSDE) ( Mallipeddi and Suganthan, 2011 ). All algorithms have the same stopping criteria, 150,000 fi tness function evaluations. The detailed results are shown in Table B1 .

Based on the best fi tness values obtained, GA-MPC was able to obtain better results than IDE and EPSDE for 14 and 17 problems, respectively. GA-MPC was able to obtain the same results as IDE and EPSDE for 7 and 2 problems, respectively, while GA-MPC was inferior to both IDE and EPSDE for 1 problem each. In regard to the average fi tness values, GA-MPC was superior to IDE and EPSDE for 19 problems each. GA-MPC was able to obtain the same mean results as both IDE and EPSDE for only 1 problem each, while GA-MPC was inferior to IDE and EPSDE for 2 problems each. Finally, we have statistically analyzed the performance of GA-MPC against those two algorithms, as shown in Table 6 . From that table, it is clear that GA-MPC is superior to both IDE and EPSDE in regard to both the best and average fi tness results. 5. Conclusions
During the last few decades, many genetic algorithm variants were introduced. However, GAs was most probably inferior to other EAs, such as DE. In this paper, we showed that the ef of GA could be improved by adopting a new crossover operator with a diversity operator.

The aim of the proposed crossover was to maintain both the intensi fi cation and exploration schemes. The intensi scheme can be seen from the behavior of the directed vector the contrary, the vector difference in Eq. (2) is directed to a worse candidate. This is to maintain the diversity. The reason for using three parents, instead of one or two, is to force the algorithm to visit different sub-regions and to hence explore the landscape (increasing, not necessarily decreasing, the search space). It is worth mentioning here that the directed vector difference feature resembles the concept of gradient.

In the proposed algorithm, an initial population was generated randomly and the best m individuals were stored in an archive pool. A tournament selection with size 2 or 3 was initiated to select the best points to act as parents. Then the proposed crossover and its diversity operator were used to generate new offspring. In the proposed crossover, each three consecutive parents were able to generate three offspring. These offspring were then diversi fi ed with archive pool individuals. The generated offspring were then merged with the archive pool individual, and the best PS individuals were used as a new population for the next generation.

The proposed algorithm was analyzed by solving constrained optimization problems. Its performance was superior to two other
GAs, as well as state-of-the-art-algorithms. Furthermore, the proposed algorithm was assessed by solving several real world test problems gathered from different fi elds. In doing so, the algorithm showed a superior performance in comparison with other algorithm algorithms.

For future work, we intend to conduct further theoretical analysis of our proposed algorithm.
 Appendix A Appendix B References
