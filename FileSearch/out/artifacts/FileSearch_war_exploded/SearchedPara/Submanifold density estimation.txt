 point. In the simplest multi-dimensional KDE [3], the estim ate  X  f y where h certain conditions for f and K , assuming h error in the estimate  X  f ( y E  X  f m ( y 0 )  X  f ( y 0 ) m as definition, see, e.g., [5, 7, 1]).
 estimates the density of the data as measured in the subspace is desirable. on M , zero outside.
 n and closer to the correct value.
 estimator is pointwise consistent.
 manifold of R N ( n &lt; N ) with an induced metric g and injectivity radius r notation u M by V , and the volume form by dV .
 R and for a sample q as, where h non-negative numbers m If as m  X  X  X  .
 convergence rates of the bias and the variance. kernel becomes a  X  X elta function X  as the bandwidth gets smal ler. differentiable to second order in a neighborhood of p  X  M . Let the volume form on R n . In particular, lim Before proving this theorem, we prove some results on the rel ation between u Lemma 3.1. There exist  X  In particular, lim Proof. Let c representation of c u ( c v 0 ( s )) = k x v 0 ( s ) k and k x  X  the absolute value of the third derivative of u Thus, (7) holds with M we will pick  X  0  X  r  X  1 / p 3 M u monotonic for 0  X  r  X   X  Definition 3.2. For 0  X  r i.e., H and r Since M is assumed to be an embedded submanifold, we have H below, we will assume that all radii are smaller than r d point q  X  M such that d Lemma 3.2. H 0 such that, r  X  H Proof. H that there exists at least one point q with d Let  X  H for some M figure) In particular, H ( r,  X  gives, H r &lt;  X  H Next we show that for all small enough h , there exists some radius R with a d Lemma 3.3. For any h &lt; H q with d satisfies, In particular, lim Proof. That u will use Lemma 3.2. Let  X  ( r ) = r  X  M to-one and continuous in the interval 0  X  r  X   X   X  in this interval. From the definition of R for all h  X   X  (  X  that the third derivative of  X  is bounded in a neighborhood of 0, there exists  X   X  ( h )  X  h + M R for all h  X   X  the integral in the definition of  X  Let h solely in the patch of normal coordinates at p .
 g ( q ) = g ( y ( q )) , where g is the metric tensor of M . integration to z = y /h , we get, Thus, Letting h  X  0 , the terms (17)-(20) approach zero at the following rates: by constants as h  X  0 . In normal coordinates y , g sup k y ( p ) = 0 , for k z k X  1 , Taylor X  X  theorem gives, as h  X  0 . Since R k h  X  0 . (20): The first three terms can be bounded by constants. By Lem ma 3.3, R 1 &lt; k z k X  R p ( h ) /h is O ( R p ( h ) /h  X  1) = O ( h 2 ) as h  X  0 . Thus, the sum of the terms (17-20), is O ( h 2 ) as h  X  0 , as claimed in Theorem 3.1. Let M , f ,  X  f Lemma 4.1. Bias h  X  f Proof. We have Bias[ f follows from Theorem 3.1 with  X  replaced with f .
 Lemma 4.2. If in addition to h Proof.
 Now, and, (25) is O 1 Var[  X  f m ( p )] = O 1 mh n Proof of Theorem 2.1 Finally, since MSE h  X  f follows from Lemma 4.1 and 4.2. MSE for a range of m s. See Figure 5. K unlikely that a generic data set lives on a smooth Riemannian manifold. [5] J. Jost. Riemannian geometry and geometric analysis . Springer, 2008.
