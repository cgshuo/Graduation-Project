 The use of crowdsourcing for document relevance assess-ment has been found to be a viable alternative to corpus annotation by highly trained experts. The question of qual-ity control is a recurring challenge that is often addressed by aggregating multiple individual assessments of the same topic-document pair from independent workers. In the past, such aggregation schemes have been weighted or filtered by estimates of worker reliability based on a multitude of be-havioural features. In this paper, we propose an alterna-tive approach by relying on document information. Inspired by the clustering hypothesis of information retrieval, we as-sume textually similar documents to show similar degrees of relevance towards a given topic. Following up on this intu-ition, we propagate crowd-generated relevance judgments to similar documents, effectively smoothing the distribution of relevance labels across the similarity space.

Our experiments are based on TREC Crowdsourcing Track data and show that even simple aggregation methods utiliz-ing document similarity information significantly improve over majority voting in terms of accuracy as well as cost efficiency.
 H.5 [ Information Retrieval ]: Evaluation of retrieval re-sults X  Relevance assessment ; H.4 [ World Wide Web ]: Web applications X  Crowdsourc-ing Theory, Experimentation Crowdsourcing, Clustering Hypothesis, Relevance Assess-ment c  X  2015 ACM. ISBN 978-1-4503-3794-6/15/10 ...$15.00.
Evaluation of retrieval performance is a crucial step in the overall IR system design process. In order to ensure frequent repeatability of tests, many researchers and practi-tioners rely on static test collections with known relevance judgments for pairs of topics and documents. The creation of such resources, especially at large scale, can require con-siderable amounts of time and money as an expensive group of trained domain experts carefully judges the relevance of individual pairs [29].

Crowdsourcing is defined as the practice of obtaining con-tent from a large (typically, online) community, rather than from traditional employees. There are many ways in which individuals can be incentivized to offer their work force on crowd markets. While altruistic motives [3], community credibility [33], and entertainment [11] form valid motivat-ing factors, paid crowd labour tends to be one of the most broadly applicable schemes. Crowdsourcing platforms such as CrowdFlower or Amazon X  X  Mechanical Turk take the role of intermediaries between requesters and workers . Assign-ments are typically given out and paid for in the form of small, atomic units, so-called Human Intelligence Tasks (HITs). In the case of our IR evaluation scenario, making a single bi-nary relevance assessment between a topic and a document can be considered a HIT.

Quality control is a traditional challenge that crowdsourc-ing requesters have to address when drawing from the con-siderable power of the crowd. While most workers attempt to truthfully complete tasks, there are frequent reports of workers showing sloppy or random judging behaviour in or-der to increase their time efficiency [10]. A widely accepted way of overcoming this obstacle is to present the same HIT to multiple workers and subsequently aggregate their sub-missions. To continue with our IR test collection creation example, each query-document pair is presented to multi-ple workers and its final relevance label is determined by means of an aggregation method such as interpolation or majority voting. For many tasks, including document rel-evance assessment, this practice has effects that go beyond mere filtering of spam submissions but can also account for subjective differences in judgments across workers.
Instead of uniformly merging raw votes, much work has been dedicated into estimating worker reliability based on their past accuracy, judging behaviour, or topic affinity. Sub-sequently, this form of worker information can be used to bias the aggregation process towards the most reliable work-ers or to empower active learning schemes in which the most suitable worker for each task is to be selected. There is, how-Figure 1: Gaussian kernel density estimate of the document similarity distribution for topic 20542 ever, another, largely untapped source of information, the document X  X  content. For example, one could exploit simi-larities between documents to aggregate worker votes in an efficient manner.

Consider a cosine similarity between the tf X  X df representa-tions of documents. Figure 1 shows the distribution of sim-ilarities a) between relevant documents (inner similarities) and b) between relevant and irrelevant documents (outer similarities) for topic 20542 of the TREC Crowdsourcing Track 2011. We can clearly observe how relevant documents share much stronger commonalities than irrelevant ones. In this paper, we exploit this well-known Clustering Hypothe-sis by propagating relevance assessments to  X  X earby X  neigh-bours for the purpose of vote aggregation.

The novel contributions of this paper are threefold: 1) We present a systematic overview of the spread of crowdsourced relevance labels across a textual similarity space, support-ing the validity of the clustering hypothesis for label aggre-gation. 2) We introduce three content-aware vote aggrega-tion methods. Beginning with light and moderate modifica-tions to the standard majority voting approach, we finally describe the use of Gaussian process classification models for this purpose. 3) In a set of experiments based on his-toric submissions to the TREC 2011 Crowdsourcing Track, we demonstrate the merit of our methods in terms of cost-efficiency and accuracy in comparison to content agnostic methods.

The remainder of this paper is structured as follows: Sec-tion 2 gives an overview of related work dedicated to crowd-sourced relevance assessments and quality control mecha-nisms. Section 3 formally introduces our methods and dis-cusses relevant technical considerations. Our experimental setup and results are described in Section 4, before Section 5 concludes our investigation with a brief discussion of prac-tical implications as well as future directions inspired by our findings.
Ever since the introduction of the Cranfield experiments [28], test collections have been one of the pillars of IR system evaluation. Traditionally, such collections are created by trained professionals in controlled lab environments. In the IR community, the Text REtrieval Conference (TREC) [29] supports one of the most widely known efforts to creating such collections. In the face of constantly growing demands in terms of test collection diversity and scale, there have been numerous attempts at reducing the considerable cost involved in corpus creation and annotation. Most notably, previous work proposes designing more robust performance measures [4], selecting the right subset of documents for eval-uation [6] and inferring implicit judgments from user inter-action logs [16].

With the rise of crowdsourcing, there has been an exten-sive line of research dedicated to using this new channel for the creation and annotation of IR test collections. An early set of experiments [21, 1, 20, 12] find that aggregated la-bels of multiple untrained crowd workers can reach a quality comparable to that of a single highly trained NIST assessor.
While this alternative labour market has been shown to be time and cost efficient [11], researchers have less control over the way in which relevance judgments are created. Tra-ditionally, inaccurate judgments as well as spam submissions have been a major challenge in the crowdsourcing process. There are many effective quality control schemes, ranging from aggregating the results of independent workers to the use of honey pot questions [13, 15, 10]. Marshall et al. [24] highlight the importance of engaging HIT design on result quality. Active learning techniques have been demonstrated to greatly improve the worker-task allocation step [32].
Significant effort has been made into estimating worker reliability based on various behavioural and demographic traits. Tang and Lease [26] introduce a semi-supervised method for reliability estimation based both on labeled as well as unlabeled examples. Kazai et al. [18, 19] group work-ers into 5 different classes and study their respective judg-ment reliability and behaviour. Based on social media pro-files, Difallah et al. [9] model worker topic affinities, enabling them to assign tasks to workers with matching interest, re-sulting in significantly improved result quality. Karger et al. [17] propose a joint model for iteratively learning worker reliability and aggregating votes by means of approximate belief propagation. Blanco et al. [2] investigate the robust-ness of crowdsourced relevance assessments over time, find-ing that repeated labeling efforts produce stable results even as longer periods of time elapse.

Another prominent source of evidence can be found in the analysis of systematic judgment behaviour. Following Dawid and Skene [8], who investigated disagreements be-tween diagnoses posed by multiple individual medical doc-tors, there have been several successful attempts at harness-ing similar methods for crowdsourcing quality assurance [30, 14]. In this way, reliable workers that make occasional mis-takes can be accurately separated from spammers that se-lect answers at random or follow other, more sophisticated, cheating strategies.

Several scientific workshops have been dedicated to pur-suing how to use crowdsourcing effectively and efficiently [7, 23]. Most notably, TREC 2011 for the first time offered a dedicated crowdsourcing track [22], addressing the crowd-sourced collection of document relevance assessments.
While previous approaches have been successfully using worker demographics and behaviour in order to predict judg-ment accuracy, in this paper, we propose to investigate the content of the documents being judged. Inspired by the clus-tering hypothesis of information retrieval [27], we assume similar documents to show similar degrees of relevance to-wards a given topic. Our experiments show significantly im-proved label aggregation performance at lower overall cost, when relying on this source of evidence.
In this section, we formally introduce the problem state-ment alongside our proposed method. For each topic, there is a set documents D , that each require relevance labels. For every document d i  X  D , we can, via a crowdsourcing platform, request binary relevance judgments (votes) v Individual votes are encoded such, that v ij = 1 denotes  X  X el-evance X  and v ij = 0 denotes  X  X on-relevance X  of the given topic-document pair. To account for errors or worker sub-jectivity in the creation of individual votes, a final processing step specifies vote aggregation methods to produce overall relevance labels r i  X  X  0 , 1 } on the basis of the the collected raw votes. For practical reasons, suppose that we can query the crowd for a worker vote on any document d i at any given time. Crowdsourcing can then be represented as a process of iterative requesting of votes, where relevance labels can be aggregated at every step: Algorithm 1 Crowdsourced Relevance Assessment for k  X  1 ...K do end for
Where K delimits the number of iterations, taking the role of a budget parameter. V i denotes the set of all raw votes v requested for document d i ,  X  is the super set of all currently requested votes across D and R is the set of all final relevance labels. There are three fundamental components to the gen-eralized crowdsourcing-based relevance assessment process described in Algorithm 1. PickDocument selects the next document to request a vote for. To this end, we randomly sample among those documents that currently received the lowest overall number of votes ( d i = D (arg min( V i ))), effec-tively introducing a weighted round robin scheduling. Re-questVote details the exact procedure under which a new vote is requested (e.g., crowdsourcing platform, interface, gold standard, etc.). In this work, we rely on a large set of existing votes collected for the TREC 2011 Crowdsourcing Track [22] from which we randomly sample with replacement in order to obtain an infinite supply of votes.

Since the focus of our work resides on content-aware vote aggregation methods, we do not alter the remaining compo-nents in the course of our experiments. Please refer to Sec-tion 5 for a discussion of future work on content-aware active learning schemes that may indeed want to introduce alter-native realizations of PickDocument and RequestVote .
As stated earlier, we require aggregation methods to pro-duce relevance labels for all documents at every iteration. As a consequence, there can be ties when:
These cases can be handled similarly for all proposed ag-gregation methods. In this work, we resolve ties by se-lecting between  X  X elevance X  and  X  X on-relevance X  by tossing a fair coin . Ideally, we would want to toss a biased coin informed by the underlying probability of relevance in the collection, but for the sake of realism, we assume the aggre-gation method to only have access to the subset of votes cur-rently yielded by the crowdsourcing process. Since this as-sumption affects all proposed methods and baselines equally, we do not expect it to introduce any systematic bias towards favoring either of the methods. In fact, as we will see in Section 4, our experimental collection shows a near-uniform distribution of relevant and irrelevant documents.
As an intuitive performance baseline, we rely on the de facto standard in crowdsourcing vote aggregation [10], ma-jority voting. Let us consider the set of votes V for document d i as a realization of a sequence of Bernoulli trials. If we denote the amount of  X  X elevant X  votes received for the topic-document pair as N i ,
Therefore p i can be estimated as:
Here we use a vertical bar to denote the arithmetic mean across all votes in the set. We can now present a Major-ity Vote aggregation function in an algorithmic form, see Algorithm 2.
 Algorithm 2 MajorityVote for all i  X  I do end for
As stated earlier, the novel methods introduced in this work are based on the notion of similarity between textual documents. To this end, we represent each document d i by its tf X  X df vector T ( d i ) and define pairwise similarity between two documents d a and d b in terms of cosine similarity  X  between their vector representations: Figure 2: (Inner) Similarity between relevant docu-ments averaged across queries. where | X | denotes the Euclidean norm of a vector. Doc-uments are processed as they are, without any form of vo-cabulary pruning, stop word removal, stemming etc. To val-idate our choice of distance metric for the task of separating relevant documents from irrelevant ones, let us first test the clustering hypothesis. We apply the method which was orig-inally proposed by van Rijsbergen and Sp  X  arck Jones [27] for identifying document collections for which retrieval could yield meaningful results. We consider a set D r  X  D of doc-uments which are relevant to a particular topic and compute two sets of pairwise similarities: 1.  X  X nner X  similarities S I = {  X  ( d u ,d v ) | d u ,d v 2.  X  X uter X  similarities S O = {  X  ( d u ,d v ) | d u  X  D r
Our investigation is based on the ClueWeb09-T11Crowd collection, a subset of the full ClueWeb09 dataset [5], and uses NIST-created TREC 2011 Crowdsourcing Track top-ics and relevance judgments. We join these respective sets across all queries in the document collection and produce two histograms, where inner and outer similarities are plot-ted against their relative frequency in corresponding joined sets. The results are shown in Figures 2 and 3.

The separation between the two histograms supports the clustering hypothesis and justifies our choice of document similarity metric. In Figure 2 one can see the spike indi-cating there is a relatively high number of document pairs with similarities close to 1. Manual inspection of some ran-domly selected pairs hints that these could be Wikipedia pages redirecting to one common page, which differ only in the  X  X edirected from X  header. The presence of such near duplicates in the dataset may pose problems in performance estimation. The aggregation methods we propose attempt to use relevance judgments from similar documents when ag-gregating final estimates. For pairs of documents which are very similar to each other this strategy is likely to be very successful. Hence, the fact that the dataset contains a sig-nificant amount of document pairs that are near duplicates means that the accuracy gain may be overestimated . Figure 3: (Outer) Similarity between relevant and irrelevant documents averaged across queries.

To investigate the effect of such document pairs with ar-tificially high similarity scores, we manually inspect their distribution across the data. Queries  X 20694 X  and  X 20584 X  show the highest number of pairs close to a similarity of 1. One could argue that such topics are outliers and should be disregarded. However, even after removing these topics, there is no noticeable change to the spiking behaviour or the overall pattern. Hence, we do not consider these topics out-liers, rather, having a number of near-duplicate documents appears to be a specific population property of a sizeable Web document collection.
In the following, we will introduce three content-aware aggregation methods that serve as alternative realizations of the AggregateVotes component in Algorithm 1.
 Our first candidate method, Majority Voting with Near-est Neighbor (MVNN), is a straightforward extension of the baseline method that draws evidence from the single closest neighbor in tf X  X df space. Consider a permutation O i = { o i (1) ...o i ( | I | ) } which sorts all documents by de-creasing similarity to document d i :
While o i (1) = i refers to the document itself, o i (2) is the index of the closest neighbor of document d i . We then define a similarity threshold parameter  X  s  X  [0 , 1] to control for topical drift. Algorithm 3 details the aggregation algorithm that for every document d i merges votes V i with all votes V o i (2) requested for its single nearest neighbor d o i (2) neighbor X  X  similarity to d i is greater than  X  s . In this way, we locally smooth relevance labels across the tf X  X df space. As an extension to the previous method, we would like to use evidence from multiple neighbors instead of just one. For example, such an extension could define the number of neighbors to consider as an additional parameter. Addition-ally, we may even want to vary the amount of neighbors Algorithm 3 MajorityVoteWithNearestNeighbor
Parameters : similarity threshold  X  s for all i  X  I do end for employed in the aggregation for a particular document de-pending on the number of votes already requested. For doc-uments which did not yet receive many votes, we rely more heavily on votes in the neighborhood. We formalize this in-tuition by requiring a desired overall amount of votes per document rather than explicitly fixing the number of neigh-bors as a parameter. If the document has less votes than required, we merge votes with the closest neighbors until the required vote count is reached. This Merge Enough Votes (MEV) strategy is preferable since it ensures comparable amounts of information to be used in the label aggregation of each document, even if this requires relying on a wider neighborhood. See Algorithm 4: Algorithm 4 MergeEnoughVotes
Parameters : votes per document required C for all i  X  I do end for In this work, we aim to improve the aggregation of crowd-sourced relevance votes by utilizing the similarity between documents in content space. The previously proposed heuris-tic methods (MVNN and MEV) achieve this by incorporat-ing votes from a number of highly similar neighboring doc-uments.

Gaussian process classification [31] is a well-known dis-criminative method, in which labels are inferred by modeling  X  X imilarity X  of points in a feature space. We use this method to utilize votes available for all documents, as opposed to just immediate neighbors. To consider the GP classifica-tion method a formal extension of the proposed heuristics, we rely on the same notion of similarity, Cosine similar-ity between the tf X  X df vectors becomes our linear covariance function in tf X  X df space.

We specify the Gaussian process with a constant mean function and a linear covariance function: where  X  denotes a scalar product. We train a GP classifier on a set P of all available (document, vote) pairs, where the tf X  X df representation of a document T ( d i ) becomes our fea-ture vector, with the raw binary vote v  X  X  0 , 1 } as the class label. We then retrieve posterior probabilities of relevance b p for all documents in a topic. Algorithm 5 illustrates this process. The exact inference for the posterior Gaussian Pro-cess is not feasible, hence an approximate solution is found using an expectation propagation algorithm [25] with cu-mulative Gaussian likelihood function. The only calculated hyperparameter is the constant mean c .
 Algorithm 5 GaussianProcessAggregation
P  X   X  for all i  X  I do end for
GPClassifier.train ( P ) for all i  X  I do end for
In this section, we describe our experimental collection alongside the evaluation strategy and compare the perfor-mance of the individual methods. We show that content-aware aggregation methods attain the goal of outperforming content-agnostic approaches such as majority voting.
The TREC Crowdsourcing Track [22], hosted between 2011 and 2013, is dedicated to investigating the use of crowd-sourcing for search engine evaluation. In 2011, participating teams were offered two tasks: 1. Assessment , in which participants are to gather indi-2. Consensus , in which teams have to perform label ag-
The consensus task is highly relevant to the vote aggrega-tion problem addressed here. In 2012 and 2013 tracks, there was no explicit separation between assessment and aggre-gation. Therefore, in this paper, we adopt the data and evaluation procedure from the Crowdsourcing Track 2011 and focus on the problem setting of the consensus task. The ClueWeb09-T11Crowd collection is a subset of the full ClueWeb09 dataset [5]. Every document in the collection is a uniquely identified Web page represented by the page URL, the full page text, comparable to what a user would see when opening the page in a browser, the HTML code of the page alongside the HTML header. Out of this infor-mation, we only use the full page text. Although structural information contained in the HTML code could be poten-tially useful in the future, that sort of application would require a different choice of similarity metric.
 We use the original relevance judgments submitted by TREC 2011 participants. They were given for 30 differ-ent topics, such as  X  X ree email directory X  or  X  X rowing toma-toes X . For every topic, there is a set of approximately 100 documents. For every document there are on average 15 rel-evance judgments from individual workers, although some topic-document pairs have fewer affiliated judgments. For two topics (20644 and 20922) there were documents with as few as one single vote. Since such singleton  X  X ools X  of votes are very brittle and make for a poor representation of human knowledge, we exclude these two outlier topics from our investigation, leaving us with 28 functional ones.
In the 2011 Crowdsourcing Track evaluation was based on two benchmark annotations, expert judgments from NIST assessors as well as aggregated consensus labels gathered across all participating teams. In this work, we solely rely on data from the NIST assessors as ground truth. It con-sists of 395 relevance judgments and most topics contain ten to twenty documents with such ground truth labels. After the track participants submitted the aggregated judgments they were evaluated using the benchmark sets. For every submission precision, recall, accuracy and specificity were measured. For the available ground truth labels, both rel-evance classes have similar orders of magnitude  X  68% of 395 labels are  X  X elevant X . While the  X  X elevant X  class is larger across all topics, there is only one case in which it repre-sents as much as 80% of labels. We therefore concentrate on accuracy as a performance measure, since, in our case, it is expressive of the classifier X  X  performance.

The average differences between the ground truth judg-ment and the majority vote estimate for a document vary from topic to topic. Averaged across topics the difference is 0.15 (on a scale from -1 to 1). Intuitively, this indicates that the worker judgments aggregated with majority voting are a reasonable estimate of the true relevance and indeed pro-vide a meaningful baseline for our methods. Nevertheless, for four topics the average difference exceeds 0.30, for one of them (20922) reaching the value of 0.55. For these topics it appears less likely to attain a reasonable accuracy with any aggregation method due to the low overall agreement between crowd and experts.
Our experimental setup is based on the general iterative crowdsourcing procedure presented in Algorithm 1. At each step, one of the documents with the least amount of votes is sampled at random and a vote is requested. Since we work with a stale collection of votes, we sample with re-placement from the pool of raw votes submitted for docu-ment d i to prevent running out of votes in case of sparsely-annotated topics. We measure accuracy of aggregated rele-vance estimates at every step. Optimal settings of MVNN ( C = 0 . 5) and MEV(  X  s = 1 . 0) parameters were determined empirically in a dedicated set of experiments while Gaussian process hyperparameter c is determined within each learn-ing iteration. Figure 4 displays aggregation performance as a function of the number of votes per document within a given topic. Each plotted performance measurement repre-sents the mean accuracy across 50 sampling randomizations of the crowdsourcing process. Within each randomized run, the individual methods are evaluated on the exact same real-izations of the various sampling processes in order to ensure comparability of our findings.

There are several interesting trends to be observed. MVNN closely follows the performance curve of the majority vot-ing baseline. While less than 2 votes per document are re-quested, the neighborhood information introduces a slight performance gain that levels out and is eventually reversed as more document-specific votes are procured. GP and MEV start at a significant performance offset, yielding significant improvements when only few votes are requested. With ev-ery additional requested vote,we can note the relative ad-vantage of both methods shrinking. While MEV maintains a narrow lead, GP performance eventually dwindles. At ap-proximately three votes per document, all methods reach a stable accuracy level of approximately 0 . 75 which is not significantly improved by further votes. This finding is in line with previous observations, made, e.g., by Vuurens et al. [30], who find that requesting more than three votes on average does not improve labelling performance. We believe that the observed method ranking stems from the particu-lar scope at which neighborhood information is considered by the individual methods. MV shows the steepest perfor-mance increase, drawing new information from every new vote. MVNN has an initial advantage by using the single closest neighbor for smoothing. As sufficiently many local votes are available, this smoothing effect, however, begins to turn into noise. Gaussian processes use the widest neighbor-hood range, drawing from all available labels which results in a very strong early performance, but serves for noisy la-bels later on. MEV offers a good trade-off between strong early performance and good noise robustness since it effec-tively defaults back to local majority voting as soon as more local votes are available.

In this work, we focus especially on the extremely low-budget scenario in which aggregation effectiveness is sub-jected to tight cost-efficiency bounds. Settings like these are frequently encountered when large-scale problems are stud-ied. To get a better impression of the relative comparison between the four methods as votes are scarce, Table 1 inves-tigates aggregation performance per topic at a cut-off of 1 vote per document. It should be noted that in this partic-ular case, majority voting (MV) defaults to the case where the single requested crowd label is believed to be true for every document. Statistically significant improvements over the content-agnostic baseline MV are denoted by an aster-isk character. Methods that outperform all competitors at significance level are indicated by a hash symbol. Statisti-cal significance was tested using a Wilcoxon signed rank at  X  &lt; 0 . 05-level. Again, all presented results are mean values across 50 randomizations.

We can note that in the extremely resource-constrained setting, the wide-coverage neighborhood model used by GP performs best. Out of 28 topics, the method yields the high-est overall accuracy in 18 cases, often significantly outper-forming all competing methods. MEV is the runner-up with 8 overall best accuracies. MVNN typically introduces only mild improvements over the majority voting baseline, few of which turn out to be statistically significant. Table 1: Performance comparison in terms of accu-racy at 1 vote per document for each topic.

In this paper, we demonstrated the use of document sim-ilarity information for aggregating crowdsourced relevance assessments. Following the intuition that textually similar documents should show similar degrees of relevance towards a given query, we propagate crowdsourced relevance judge-ments across documents in order to infer the relevance of those documents that have not yet received (enough) ex-plicit votes. In a series of experiments based on the data and guidelines of the TREC 2011 Crowdsourcing Track, we show that even straight-forward methods informed by document similarity estimates significantly outperform commonly used majority voting schemes in terms of both label accuracy as well as cost efficiency.

We investigated three novel aggregation schemes relying on varying scopes of neighborhood information. Gaussian process classification considers the full available set of all votes, resulting in competitive performance in cold-start sce-narios with very few available votes. The two heuristics MVNN and MEV are more conservative in their use of neigh-borhood information, relying on fewer, highly similar neigh-bors, making them suitable for resource-rich scenarios in which many raw votes are available for every document.
A particular caveat when using methods like these lies in the danger of biasing the created labels too strongly to-wards the same intuition underlying common retrieval sys-tems (i.e., tf X  X df locality of relevant documents). While this is a valid concern, we believe that it can easily be controlled for by making only careful use of local neighborhood in-formation instead of long-distance propagation of relevance labels.

There are several exciting directions for future work. In this paper, we rely solely on document information, ignor-ing evidence of worker reliability. In the future it would be interesting to investigate to which degree these orthogo-nal sources of information can be joined to increase overall performance. Currently, our method does not include any active selection of documents to request votes for next. It would be interesting to holistically model all documents to be evaluated for a given topic and carefully select which concrete documents to require judgements for in an active learning scheme such that the entire system is benefitted most.
