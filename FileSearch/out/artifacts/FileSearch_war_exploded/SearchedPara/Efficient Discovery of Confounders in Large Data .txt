
Given a large transaction database, association analysis is concerned with the efficient identification of strongly related objects. As a core problem in data mining, association analysis has been successfully applied to various application domains, such as market-basket analysis [1], climate studies [2], public health [3], and bioinformatics [4].

Previous work mainly focuses on identifying associations at the global level [5], [6], and relatively, less attention has been paid to finding associations at a local level. We can argue that local association patterns are sometimes important because many real-world phenomena are local-ized. For example, the prevalence of a disease at a certain geographical region, and the similar preferences within a non-trivial customer segment. Nevertheless, local correlation patterns are often overlooked for a good reason: the economy of scale. For example, although technically we could divide customers into very subtle segments, we would provide each customer with a personalized service in the extreme case. However, the cost will likely exceed the benefits from the large number of lower-valued customers. A practical solution is to find global patterns first, and then find most important local patterns that are different from the global ones.
Due to the potential inconsistencies between local patterns and global patterns, the study of local correlation patterns need to address the following questions: false negatives and false positives. False negatives are patterns that are insignif-icant globally, but significant locally. Ignoring such patterns, we will be missing potential, new business opportunities [7], [8]. False positives are patterns that are significant globally, but insignificant locally. Making decisions by the false positive global patterns will lead to ineffective, sometimes harmful, business operations.

In this paper, we use  X  correlation coefficient [9] as the measure of association, and focus on finding false positive correlation patterns. In other words, a pair of items { A, B may appear correlated globally, but uncorrelated when con-trolled for a third item C . In more extreme circumstances, C may change the direction of the correlation between A and B . Here, C is called a potential confounder of item pair {
A, B } . As a pilot study, we limit our scope to local markets identified by first order constraints on a binary database. In other words, the localness of an item pair { A, B } is identified by a single item C instead of a group of items. In this problem setting, we expect to extend the study of two-way correlation analysis into three-way, and pave the way for even more complex multi-way correlation computing.
When dealing with large-scale data sets, it is challenging to identify all potential confounders in an effective and efficient way. Considering the correlation between any two items, controlled for a third item, we may need to check all possible three-item tuples, making the problem complexity as O ( n 3 ) , and so a brute-force approach will not be scalable if the number of items n is very large.

A possible solution to achieve computational efficiency is to use the idea of dynamic programming, which trades mem-ory space for time. Specially, since correlation computation is relatively more expensive, and the correlation coefficient of a pair may be required multiple times during the process of computing local associations, we save pair-wise global correlation values as intermediate results. It has been found that this way can save a lot of computation time at the cost of O ( n 2 ) memory space. However, for a data set with a large number of items, it may not be practical to handle such a huge memory demand.
In order to discover potential confounders in a more effective and efficient manner (in terms of both time and space), in this paper, we study the properties of partial correlation coefficient, and use these properties to evaluate the effect of potential confounders. Specifically, we find a necessary condition for an item to be a confounder of an item pair. Further, we derive the conditions under which no confounding factors can possibly exist. These conditions are helpful for us to design effective pruning strategies. As a result, we design a new algorithm named CONFOUND. By comparing it with brute-force as well as dynamic program-ming methods, we show that the CONFOUND algorithm can effectively and efficiently identify confounders and strikes a balance between the use of time and memory space. A. Related Work
Regarding the difference between local and global asso-ciation patterns, we find the following related concepts.
Contrast Sets [10]. Contrast sets are subgroups of the market-basket data, such that the same pattern behaves dif-ferently in different subgroups. The problem of fining group differences has been studied intensively in [11], [12], and Kralj et al. [13] has applied the contrast set mining method to analyzing brain Ischaemina data. A study by Webb et al. [14] discovers that contrast-set mining is a special case of the rule-discovery framework. However, contrast sets emphasize between-group differences, but not necessarily local versus global differences. Specially, contrast sets cannot discover local patterns showing Simpson X  X  Paradox [15]. In other words, some patterns behave differently in each local seg-ment from the global level, but they may appear similar among different local segments.

Niches [7]. Niches are defined as surprising association rules that contradict set routines, which consist of a number of dominant trends. Both the dominant trends and the niches can be captured by emerging patterns [16], which present significant differences between different classes. Although the concept of emerging patterns is closely related to contrast sets, the niche mining problem tries to discover a small num-ber of exceptions (niches) that contradict the majority (set routines), which reflect the idea of discovering significant local patterns that are different from global ones. However, the problem setting is different from ours, since it focuses on one attribute as the class label (e.g. risky customers vs. non-risky customers).

Local Linear Correlations . For a high-dimensional database, it is challenging to find a subset of features that are linearly correlated, and are supported by a large number of data points. Zhang et al. [8] designs an efficient algorithm named CARE for finding local linear correlations in high dimensional datasets. The algorithm is based on spectrum properties and prunes the search space using effective heuristic. It has been shown that even for two variables that do not appear linearly globally, local linear correlation may still exist on local intervals [17]. Due to the complexity of the problem, local linear correlation mining remains partially solved. In this paper, although we focus on binary attributes, it is potentially helpful for more general linear correlation discovery when we discretize continuous attributes effectively.
 B. Outline The rest of this paper is organized as follows. In Section II, we describe some basic notations, computational formula and research findings that are pertinent to  X  correlation coefficient. Section III introduces the problem of confounder finding. In Section IV, we illustrate relevant concepts, and identify properties that will be helpful for efficient computa-tion. Based on such properties, we develop a new algorithm named CONFOUND for efficiently identifying confounders. Section V shows the experimental results. Finally, we draw conclusions and suggest future work in Section VI.
In this section, we briefly introduce some notations, com-putational formula, examples, and major research findings in  X  correlation computing. Although confounding effects can also be observed with other association measures, such as odds ratio and relative risk, in this paper, we focus on  X  correlation coefficient [9] as a pilot study.
 A. The  X  Correlation Coefficient
Assume that we are given a large, binary market-basket database D , which contains N transactions T 1 ,T 2 ,...,T Each transaction T i , i =1 , 2 , 3 ,...,N is a collection of items being purchased. Then, for any two items A and B , we can produce a two-way contingency table, as shown in Table I.
 In Table I, we can see that among the N transactions in D , there are N AB containing both items A and B , N A  X  B containing item A but not item B , so on and so forth. With such notations, the  X  correlation coefficient between items A and B can be computed as Given a threshold  X  ,  X   X  (0 , 1) , we say items A and B are correlated if  X  AB  X   X  . Note that  X  AB is not defined if N
Let S A = N A /N , S B = N B /N . Xiong et al. [18] have derived an upper bound for  X  AB , which is Xiong et al. [18] prove that this upper bound has both 1-D and 2-D monotone property, and propose a new algorithm, named TAPER, to identify all item pairs whose correlations are above a user-specified threshold.

In a more general setting, we are interested in not only positively correlated item pairs, but also negatively correlated ones. To identify both positively and negatively correlated pairs, we can extend the TAPER algorithm by considering not only the upper bound of  X  , but also the lower bound. Specially, Xiong et al. [19] have found the lower bound for  X  AB , which is Xiong et al. [19] also identify the monotone property of this lower bound.
 B. Partial Correlation
Local correlation patterns can be evaluated by partial correlation [20]. Take the three-item scenario as an exam-ple: controlling for an item C , the partial correlation [20] between items A and B can be computed as Note that  X  AB | C is not defined if  X  AC =  X  1 or  X  BC =
When controlled for a third item, the partial correlation between two items may be different from the global one. In the following, we materialize such phenomenon with examples. Suppose we are given a database as shown in Table II, which has N =8 transactions involving four items A , B , C and D ; and the correlation threshold is  X  =0 . 3 . Evaluated globally, since N A =3 , N B =3 , and N AB =2 , by Formula (1), the correlation of item pair { A, B } will be: This indicates that items A and B are positively correlated. However, in the following examples, we can see local deviances from the global correlation.

Example 1: (False Positives) Since N C =3 , N AC =0 , and N BC =0 ,wehave Therefore, when controlling for item C , the partial correla-tion between A and B will be by which we can infer that, when controlled for an item C , items A and B are not correlated.

Example 1 shows that a globally correlated pair may be uncorrelated at a local level. Specifically, given the threshold  X  =0 . 3 , we would believe that items A and B are positively correlated, since  X  AB  X   X  . However, when controlling on item C , A and B are uncorrelated, since |  X  AB | C | &lt; X  .In this case, we say that C is a neutralizing confounder for item pair { A, B } .

Example 2: (Wrong Direction) Since N D =4 , N AD =0 , and N BD =0 ,wehave So when controlling for item D , the partial correlation between A and B will be by which we can infer that, when controlled for an item D , items A and B are negatively correlated.

Example 2 shows that the direction of the correlation at the global may be reversed at a local level. Specifically, given the threshold  X  =0 . 3 , we would believe that items A and B are positively correlated, since  X  AB  X   X  . However, when controlling on item D , items A and B becomes negatively correlated, since  X  AB | D &lt;  X   X  . In this case, we say that D is a reversing confounder for item pair { A, B } .
As illustrated in the previous section, the  X  correlation observed at a local level can be dramatically different from that observed at the global level. In this paper, we focus on first-order cardinality constraints for identifying local segmentations. Simply put, we only consider the existence of a third item C as the constraint. However, local constraints can be extended in the future work.
 A. Problem Formulation Given a correlation threshold  X  ,  X   X  (0 , 1) , items A and B are said to be (positively) correlated if  X  AB  X   X  .For a positively correlated item pair { A, B } , we can define the following types of confounders.

Definition 1: (Neutralizing Confounder) Given the threshold  X  ,  X   X  (0 , 1) , item C is called a neutralizing confounder of { A, B } ,if  X  AB  X   X  and |  X  AB | C | &lt; X  .
The implication of a neutralizing confounder is that a correlated pair observed at the global level might be un-correlated at the local market indicated by this neutralizing confounder. Take the market-basket analysis as an example, avoiding such ineffective local markets, it is possible to reduce management and operation costs, and focus on more effective parts of the market.

Definition 2: (Reversing Confounder) Given the thresh-old  X  ,  X   X  (0 , 1) , item C is called a neutralizing confounder of { A, B } ,if  X 
The implication of a reversing confounder is that a posi-tively correlated pair observed at the global level might be negatively correlated at the local market indicated by this reversing confounder. Since the direction of correlation is turned over, making decisions according to global correla-tions only will possibly have a negative impact on such local markets. In other words, reversing confounders are more important than neutralizing confounders due to the possible negative effects. Thus in this paper, we focus on find revers-ing confounders as an initial attempt towards confounder discovery. In the rest of this paper, we refer  X  X onfounders X  to  X  X eversing confounders X  only, unless otherwise specified.
In this paper, we formulate the problem of confounder discovery as follows. Suppose we have a large, binary market-basket database D = { T 1 ,T 2 ,...,T m } , where T is a transaction ( i =1 , 2 ,...,m ) . Each transaction includes a set of items from the item space I = { I 1 ,I 2 ,...,I n have T i  X  I ( i =1 , 2 ,...,m ) , and I = m i =1 T i .Soin there are m transactions involving n items. Our goal is to find all item sets in the form of { A, B | C } , such that item C is a reversing confounder of item pair { A, B } . B. Properties of Confounders In this subsection, we study the properties of confounders. Specially, for an item pair { A, B } , we identify a necessary condition for any item C to be its confounder. This condition will be useful for effective pruning in algorithm design.
Lemma 1: Given threshold  X  ( 0 &lt; X &lt; 1 ), an item C may be a confounder of { A, B } only if either where  X  AB =  X  AB  X   X  .
 we have  X  AB  X   X  .If C is a (neutralizing or reversing) confounder of { A, B } ,wehave Since  X  AC , X  BC  X  (  X  1 , 1) ,wehave Thus a necessary condition for (10) is Since  X  AC , X  BC  X  (  X  1 , 1) and  X  AB  X   X  , from (12) we know that Therefore, a necessary condition for (10) is either
Lemma 2: Given threshold  X  ( 0 &lt; X &lt; 1 ), an item C may be a reversing confounder of { A, B } only if either have  X  AB  X   X  .If C is a reversing confounder of { A, B } we have From (16) we have So Since  X  AC , X  BC  X  (  X  1 , 1) and  X  AB  X   X &gt; 0 , from (18) we have Therefore, a necessary condition for (16) is either
Corollary 1: Item C may be a reversing confounder of {
A, B } only if A , B , and C are pairwise correlated (either positively or negatively). by definition we have  X  AB  X   X  . In addition, from the necessary condition derived in Lemma 2, we have either So pairs { A, C } and { B, C } are also correlated.
Corollary 2: For any three items A , B , and C that are pairwise correlated (either positively or negatively), none of them will be a reversing confounder if  X  AB  X  AC  X  BC  X  0 . by definition we have  X  AB  X   X &gt; 0 . In addition, from the necessary condition derived in Lemma 2, we have either In either case, we have  X  AB  X  AC  X  BC &gt; 0 .

To discover all confounders in a database, the brute-force way is obviously unscalable. It has been suggested that dynamic programming is to be used. However, to compute and score O ( n 2 ) pairs is sometimes still infeasible. As a result, we look for a more effective solution, which is based on the findings we presented in the previous section.
In this section, we design an efficient and effective new algorithm, called CONFOUND, to find all confounders in the dataset. In the following, we first describe the brute-force and dynamic programming algorithms, which will serve as the baselines; and then describe the new algorithm we have designed. For each algorithm described in this section, we assume that the input variables are I , the list of items sorted by support in the non-increasing order; and  X  , the correlation threshold. The output are patterns in the format { A, B | where item C is a confounder for item pair { A, B } . A. Baseline Algorithms
In this subsection, we describe the brute-force and the dynamic programming algorithms, which will serve as the baselines for CONFOUND.

The brute-force approach is describe in Algorithm 1. The outer loops starting in Lines 1 and 3 ensure that we iterate on each possible item pair { A, B } only once. Also, since items are sorted, we have S B  X  S A .

Despite the name  X  X rute-force X , we pursue basic prunings in Algorithm 1. The first pruning happens in Line 5, which is based on the 1-D monotone property of the upper bound of  X  [18]. The second pruning is in Line 7: when A and B are not positively correlated, there is no need to check for their confounders. Otherwise, a third loop starting from Line 8 iterates on each remaining item C , and compute correlations and partial correlations by brute force.
 Obviously, the worst-case complexity of Algorithm 1 is O ( n 3 ) . Suppose the pruning ratio in Line 5 is r 1 , the pruning
Algorithm 1 : The Brute-Force Approach. ratio in Line 7 is r 2 , the average time for computing the exact correlation of an item pair is T c , and the average time for computing a partial correlation and outputting the confounder pattern (if applicable) is T p , then the total running time of Algorithm 1 will be T Notice that the computation of pairwise correlation in Algorithm 1 has lots of overlap. For instance, the correlation of  X  AC in Line 11 may have to be computed again in Line 6 at a later iteration. On the other hand, the computation of the exact value of  X  correlation proves to be expensive [18]. Thus it is desirable to store pairwise correlation for repeated use, which motivates us to design a dynamic programming based approach, as described in Algorithm 2.

Algorithm 2 tries to alleviate the computation load by making use of more storage space. Specifically, in the ini-tialization stage (Lines 1-7), the correlation of each possible item pair is computed only once, and saved for later use. Then, in the confounder search stage (Lines 8 through 19), we iterate over each possible item pair { A, B } and check against each remaining item C ,toseeif C is a confounder of { A, B } . The difference of this process from Algorithm 1 is that, instead of re-computing the  X  correlations, we simply look them up from the pre-computed results.

Obviously, the total running time of Algorithm 2 will be the sum of two parts: the time to compute all pairwise correlations and the time to search for confounders. We have Assuming the time to store and to look up a correlation is negligible, and following the notations in the running time
Algorithm 2 : The Dynamic Programming Approach. analysis of Algorithm 1, we have Obviously, when n is large, T So T DP T BF , which means that the dynamic program-ming approach is much more efficient than brute force. However, as described in Algorithm 3, the efficiency can be further improved without costing additional space.
Compared to each other, the brute-force approach requires more computation, using less space; whereas the dynamic programming method trades space for saving computation. B. The CONFOUND Algorithm In this subsection, we describe the new algorithm named CONFOUND, which is based on effective pruning of can-didates. Specially, the search space is greatly reduced by the facts we have found on the necessary conditions of confounding effects.

The CONFOUND algorithm is described in Algorithm 3. Similar to Algorithm 2, CONFOUND has two phases: correlation computing (Lines 1 through 10) and confounder searching (Lines 11 through 30).

In the correlation computing phase, we try to find all (positively or negatively) correlated pairs. This task can be done with a slight extension of the TAPER algorithm [19]In addition, we save pairwise correlations in a hash table L .For an item pair { A, B } , where A = I [ a ] and B = I [ b ] ,wesave  X  AB in L [ b ] ,if b&gt;a . Since it is likely that there are multiple
Algorithm 3 : CONFOUND, the Pruning Approach. items correlated with B , L [ b ] is an array of elements in the form ( a,  X  AB ) which are stored in the ascending order of a .
Although in Algorithm 2 we could also use a hash table to save pairwise correlations, a major difference is that in Algorithm 2 we save all pairs, whereas here we only save correlated ones. In real practise, strongly correlated pairs tend to be a small fraction of all possible pairs, especially for data sets whose rank-support follows Zipf distribution [18], [19]. This fact can save us time both in pairwise correlation computing and confounder searching, as will be shown later.
In the confounder searching phase, we iterate on the pairwise correlated triplets only, based on the necessary conditions stated in Corollaries 1 and 2. First, we iterate on the keys of L (Line 11). For key c , the iteration is skipped unless there are at least two elements in L [ c ] (Line 13). The reason is that if items A = I [ a ] , B = I [ b ] and C = I [ c ] are pairwise correlated, assuming a&lt;b&lt;c without loss of generality, due to the data structure we use, we should have at least two elements, ( a,  X  AC ) and ( b,  X  BC ) ,in L [ c ] . Then, in Line 20 we skip the iteration if  X  AB is not found in L [ b ] , which means that A and B are not correlated. Such filtering is possible due to the necessary condition stated in Corollary 1. We only search for  X  AB in L [ b ] because the loops starting in Lines 14 and 17 ensure that a&lt;b . Finally, additional pruning can be done in Line 21, which is based on Corollary 2.

Similar to Algorithm 2, the running time of CONFOUND can be divided into two parts, correlation computing time T
CF and confounder searching time T Similar to Algorithm 1, the pruning ratio of Line 5 in Algorithm 3 is still r 1 . In Line 7, the pruning ratio will be r , such that r 2  X  r 2 due to a more strict pruning condition. Assuming the storage time in Line 8 is negligible, then the total correlation computing time will be
At the end of the correlation computing phase, the number of correlated pairs stored in L will be approximately k = C items A , B and C ,  X  AB ,  X  BC , and  X  AC are all stored in L ; and so the number of unique items is no more than q =  X  2 k +1 n . Assume the pruning ratio in Line 21 is r 3 , then the total confounder search time will be So the total running time for CONFOUND is expected to be even less than that of the dynamic programming baseline.
In order to show further the effectiveness and efficiency of the CONFOUND algorithm, we compare it against the brute-force and dynamic programming baselines with real-world datasets, as shown in the next section.

A number of experiments have been conducted to eval-uate the CONFOUND algorithm. In this section, we first briefly summarize the experimental setup and then show the evaluation results. As expected, dynamic programming and CONFOUND are much more efficient than brute-force. In addition, CONFOUND is better than dynamic programming in both running time and space utility.
 A. The Experimental Setup
The CONFOUND algorithm, along with the brute-force and dynamic programming baselines, have been imple-mented for experimental evaluations. In this subsection, we describe the datasets we have used and the platform on which we performed all the evaluations.

Datasets. The Frequent Itemset Mining Implementations (FIMI) repository (http://fimi.cs.helsinki.fi/data/) provides a collection of datasets that are often used as benchmarks for evaluating frequent pattern mining algorithms. From FIMI, we have deliberatively chosen datasets with various sizes and densities, including accidents , pumsb , and retail .In addition, we use a real-world dataset called product , which is collected at a real retail store that sells a wide range of products. We use this dataset because of the availability of the exact name of each product, so that we can empirically check the validity of results and make suitable interpreta-tions. Table III lists the names and some basic statistics of these datasets.
 Platform. All the experiments were performed on a Dell Inspiron laptop, with 1.66GHz Intel Core Duo CPU and 2GB of RAM. The operating system is Microsoft Windows XP Media Center Edition. All programs are implemented with C++, and run with Cygwin.
 B. Examples of Reversed Directions
In this subsection, we show evidences of the existence of confounders with the real-world data named product . Specially, having the text labels of each product ID, we are able to recover some top ranked reversing confounders, as shown in Table IV. (The capital letters in the table are acronyms of the brand or product line names.)
Take the second pattern as an example. The ND sink skirt and the ND shower curtain have a positive global correlation ( 0 . 46 ), which makes some sense considering they are both used in the bathroom. However, when controlling for ND tub skirts, they become negatively correlated. That is, when knowing that the tub skirts are purchased (or not purchased), it is more likely that either ND sink skirt or ND shower curtain is purchased than both of them are purchased in the same transaction. Such phenomenon is known as the Simpson X  X  paradox, to which we need to pay attention for decision making, such as designing bundle sales to different customer segments.
 C. Computation Efficiency
In this subsection, we focus on the comparison of compu-tation time. We find that compared to the brute-force method, both the dynamic programming and the pruning methods can greatly reduce the running time. In addition, a de-tailed comparison of the CONFOUND algorithm versus the dynamic programming baseline reveals that CONFOUND outperforms the basic dynamic programming algorithm both in correlation computing and confounder searching.
Figure 1 shows a comparison of total running time among all three methods. Obviously, for any given  X  , the brute-force approach is much slower than the other two. Since the scale of the y-axis is overwhelmed by the brute-force time, it looks like that the running time of CONFOUND is only slightly better than that of dynamic programming.

As discussed in the previous section, the computation time of the dynamic programming and the CONFOUND algo-rithms can be divided into two steps: correlation computing and confounder searching. Since the correlation computing step is relatively more time consuming, it may dominate the total running time. As a result, in Figure 2, we split the two parts and plot the correlation computing time against the y-axis on the left, and the searching time against the y-axis on the right.

In Figure 2, we can see that for any given threshold  X  , the correlation computing time by dynamic programming is almost constant across all thresholds. However, as the threshold increases, the correlation computing time by CON-FOUND decreases accordingly. This is consistent with the findings in TAPER [19]. In addition, the searching time of CONFOUND versus the dynamic programming algorithm differs by an order of magnitude. Similar trend can be observed in various datasets.

Figure 3 shows a comparison of searching time on different datasets. It is shown that the searching time by CONFOUND is of orders of magnitude faster than the ordinary dynamic programming algorithm. Specially, for products and retail , which have larger numbers of items, the searching time with CONFOUND is almost negligible when compared to dynamic programming. This indicates that the new algorithm is much more scalable on larger numbers of items.

In summary, the experiments on running time confirm that dynamic programming can help us save substantial computation, and the new CONFOUND algorithm, which is based on effective pruning, can further improve the running time. This fact is especially useful in the case that the correlation computing step, as the initialization, is done offline, and searching is much more frequent.
 D. Space Compactness
Dynamic programming is known as a method that trades space for time. Specifically, in order to save repeated com-putations of pair-wise correlation, we have to save the corre-lation value for each pair. As a result, the space requirement is proportional to the number of all possible item pairs. In Table III, we can see that for a dataset of n items, there are n ( n +1) / 2 possible pairs.

In contrast, the new algorithm CONFOUND, uses much smaller size of space, compared to the dynamic program-ming baseline. Take the retail dataset for example, in Table V, we list the numbers of strong pairs to be saved, and the corresponding pruning ratios, which are calculated as the number of pruned pairs divided by the number of all possible pairs. For different thresholds, high pruning ratios are achieved, and similar results are found for all other datasets we have used.

In this paper, we investigated how to efficiently discover confounders attributable to local associations. Specifically, we focused on confounders that could turn positive correla-tions at a global level into negative at a local level. Along this line, we derived an upper bound by the necessary condition of confounders, which can help to prune the search space and efficiently identify local associations with various con-founding effects. The experimental results showed that the proposed CONFOUND algorithm can effectively identify confounders, and the computational performance can be an order of magnitude faster than benchmark methods.
As a matter of fact, due to the local effects, the global correlations may be changed in various ways at a local level, such as false positive, false negative, and change of directions (e.g. from positive to negative). In the future, we will design a generalized framework to explore various types of local associate patterns.
 This research was supported in part by National Science Foundation (NSF) via grant number CNS 0831186 and the Rutgers Seed Funding for Collaborative Computing Research. The views and conclusions contained in this docu-ment are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the National Science Foundation.

