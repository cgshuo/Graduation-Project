 This paper presents a study of a novel summarization problem called contrastive opinion summarization (COS). Given two sets of posi-tively and negatively opinionated sentences which are often the out-put of an existing opinion summarizer, COS aims to extract compa-rable sentences from each set of opinions and generate a compara-tive summary containing a set of contrastive sentence pairs. We for-mally formulate the problem as an optimization problem and pro-pose two general methods for generating a comparative summary using the framework, both of which rely on measuring the content similarity and contrastive similarity of two sentences. We study several strategies to compute these two similarities. We also create a test data set for evaluating such a novel summarization problem. Experiment results on this test set show that the proposed methods are effective for generating comparative summaries of contradic-tory opinions.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  text analysis ; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Linguis-tic processing Algorithms Comparative summary, Contradictory opinion, Contrastive sum-mary, Opinion summarization
With Web 2.0 technologies prevailing, people can now easily ex-press opinions on various topics through platforms such as blog spaces, forums, and dedicated opinion websites. Since there is usu-ally a large amount of opinionated text about a topic, users often find it challenging to efficiently digest all the opinions. The fact that opinionated text often contains both positive and negative opin-ions about a topic makes it even harder to accurately digest mixed opinions.

For example, some customers may say positive things about the battery life of iPhone, such as  X  X he battery life [of iPhone] has been excellent, X  but others might say the opposite, such as  X  X  can tell you that I was very disappointed with the 3G [iPhone] battery life. X  Often such contradictory opinions are not caused by poor or wrong judgments of people, but due to the different context or perspective taken to make the judgments. For example, if a positive comment is  X  X he battery life is good when I rarely use button X  and a negative comment is  X  X he battery life is bad when I use button a lot X , the two comments are really made under different conditions. When there are many such contradictory opinions expressed about a topic, a user would need to understand what the major positive opinions are, what the major negative opinions are, why these people have different opinions, and how we should interpret these contradictory opinions.

Unfortunately although there has been much work on opinion summarization (see, e.g., [12, 17] ), most existing work has gone only as far as separating positive and negative opinions about a topic.

For example, Figure 1 shows a part of a sample review sum-mary generated using a state-of-the-art feature-based opinion sum-marization technique [7, 13]. In such an opinion summary, a user can see the general sentiment distribution for each product feature, and furthermore, as shown in the figure, a user can also see a list of positive comments about a specific feature (i.e.,  X  X ase of use X ). Negative sentences are also available via another tab on the top. However, this summary cannot help a user to further digest the mixed opinions in the dimension of  X  X ase of use X . The user still has to read all the individual comments in both the positive and negative groups.

To help people digest such mixed opinions more efficiently, we propose to automatically generate a comparative summary of con-tradictory opinions. Specifically, given a set of positively opinion-ated sentences and a set of negatively opinionated sentences (which can be generated using existing techniques of opinion summariza-tion), we would like to extract comparable sentences from each set of opinions and generate a comparative summary containing a set of contrastive sentence pairs. Each contrastive sentence pair con-sists of a sentence with positive opinions and a comparable sen-tence with negative opinions, thus enabling a user to understand These sentences are real examples found by the Products Live Search portal at http://search.live.com/products/. contradictory opinions effectively. For example, if we can pair up two representative sentences with opposite opinions about the bat-tery life of iPhone, it would help a user to understand possibly dif-ferent conditions under which the specific polarity of opinions is expressed, and thus better understand why there are both positive and negative opinions about the battery life.

To the best of our knowledge, this summarization problem has not been addressed in the existing work, and we call it contrastive opinion summarization (COS). We formally formulate the COS problem as an optimization problem in which we attempt to find a list of contrastive sentence pairs that can both represent the two sets of opposite opinions well and offer interesting comparisons between positive and negative opinions about the same topical as-pect (e.g., battery life). The objective function of the optimiza-tion framework encodes two criteria to be applied to choose sen-tence pairs. One is that a chosen sentence from the set of positive (negative) sentences should represent a major positive (negative) opinion, i.e., there should be many sentences similar to the chosen sentence. We call this criterion representativeness . The other is that the two paired sentences should be comparable. That is, they should have opposite opinions about a common topical aspect. We call this criterion contrastiveness .

Intuitively we need different similarity functions to measure rep-resentativeness and contrastiveness. While we can generally use an existing sentence similarity function to measure representativeness, we need a new similarity function to measure contrastiveness. We solve this problem by excluding sentimental words from both sen-tences and then applying a regular similarity function. We also ex-plore how to leverage resources such as WordNet to accommodate matching of words that are semantically related but have different forms.

Exact solution to the optimization problem is generally intractable for realistic applications. We propose two general approximation methods to solve the problem. Both methods are greedy algo-rithms, corresponding roughly to first maximizing representative-ness and then maximizing contrastiveness, or the opposite, i.e., first maximizing contrastiveness and then maximizing representative-ness.

Because no existing data can be used directly to evaluate this new summarization task, we opted to create our own test set based on some publically available resources from the previous work [7, 8]. To test the generality of our methods, we further extended the test set by adding an additional case from a different domain.
Experiment results on this test set show that the proposed meth-ods are effective for generating comparative summaries of contra-dictory opinions.

The contributions of this paper are: 1. We propose and define a novel summarization problem (i.e., 2. We propose an optimization framework to model and solve 3. We propose specific methods to solve the optimization prob-4. We create the first test set and propose measures for evaluat-5. We run experiments to test the proposed methods and show
The rest of the paper is organized as follows. In Section 2, we define the novel problem of contrastive opinion summarization. In Section 3, we formally model the problem with an optimization framework. In Section 4 and Section 5, we then present specific methods to refine and solve the optimization problem. We present our experiment design in Section 6 and results in Section 7, and discuss related work in Section 8. Section 9 concludes the work.
As discussed in the previous section, the current opinion sum-marization techniques can separate positive sentences from nega-tive sentences about a topic (e.g., a product feature). We set up our problem as to take these sentences with different polarities as input and further generate a contrastive opinion summary to help users to digest the mixed opinions about the topic.

We thus will assume that we are given two sets of opinionated sentences about a topic, corresponding to positive and negative opinions about the topic, respectively. Our goal is to generate a list of sentence pairs with each pair containing a position sentence and a comparable negative sentence. Such a pair would allow a user to compare comparable positive and negative opinion and thus facilitate digestion of mixed opinions.

To formally define our problem, we first introduce a few basic concepts.
 opinionated sentence if it expresses either a positive or a negative opinion. For convenience, we will simply call a positively (nega-tively) opinionated sentence a positive (negative) sentence. opinionated sentences ( x, y ) is called a contrastive sentence pair if sentence x and sentence y are about the same topic aspect, but have opposite sentiment polarities.

For example, x and y may both discuss the battery life of a lap-top, but x says that that the battery life is long, while y says that it is short.

We may now define the novel problem of contrastive opinion summarization .
 Let X = { x 1 , ..., x n } be a set of positive sentences and Y = { y 1 , ..., y m } be a set of negative sentences about a common topic Q , where x i is a positive sentence and y i is a negative sentence. The task of contrastive opinion summarization (COS) is to gener-ate k contrastive sentence pairs : { ( u i , v i ) } , i = 1 , ...k , u v  X  Y , such that U = { u i } k i =1  X  X can represent the opinions in X well, and V = { v i } k i =1  X  Y can represent the opinions in Y well.

Table 1 illustrates how we may display a contrastive opinion summary in a tabular format to facilitate digestion of contradictory opinions. Each pair ( u i , v i ) summarizes a contradictory aspect. A user can use u i and/or v i as  X  X ntry points X  to navigate into relevant discussion about each side of the opinions of the corresponding contradictory aspect.

Intuitively, to generate a good contrastive summary, we would need to match sentences in X with those in Y to discover poten-tial candidate contrastive sentence pairs. At the same time, we also would like to assess which sentences can represent each polarity of opinions well. In the end, we would like to choose sentences from both X and Y that can not only form good contrastive pairs but also represent the corresponding complete set of opinions well. The problem is thus in nature an optimization problem involving multiple criteria. Below we will propose a formal optimization framework for solving COS, which would then use as a roadmap to derive several specific summarization algorithms.
In this section, we formally frame the contrastive opinion sum-marization problem as an optimization problem. Our optimization framework is based on two basic similarity measures defined on a pair of sentences. The first is to measure the content similarity of two sentences in the same group of opinions (i.e., either both are positive or both are negative). This similarity function allows us to assess which sentences are good representatives of each group. The second is to measure the contrastiveness of a positive sentence and a negative sentence. Since a good pair of contrastive sentences are generally also similar in content (but opposite in sentiment po-larity), we also call this measure a cross group similarity measure. We formally define these two functions as follows.
 two opinionated sentences s 1 and s 2 with the same polarity, the content similarity function  X  ( s 1 , s 2 )  X  [0 , 1] measures the overall content similarity of s 1 and s 2 .
 two opinionated sentences u and v with opposite polarities, the contrastive similarity function  X  ( u, v )  X  [0 , 1] measures the simi-larity of u and v excluding their difference in sentiment.
Both  X  and  X  are assumed to be symmetric, i.e.,  X  ( s 1 , s  X  ( s 2 , s 1 ) , and  X  ( u, v ) =  X  ( v, u ) .

With these two functions, we can now define two additional func-tions that can measure the representativeness and contrastiveness of a contrastive opinion summary S = { ( u i , v i ) } ( i = 1 , ...k ) of two sets of opinionated sentences X and Y .
 ness of a contrastive opinion summary S , denoted as r ( S ) , mea-sures how well the summary S represents the opinions expressed by the sentences in both X and Y . It is defined as r ( S ) = 1
Intuitively, if for every sentence x  X  X , we have at least one u with high similarity to x , our summary would represent X well. Similar reasoning can be applied to the set Y . r ( S ) is simply an aggregation over all the sentences in both X and Y .

D EFINITION 7 (C ONTRASTIVENESS ). The contrastiveness of a contrastive opinion summary S , denoted as c ( S ) , measures how well each u i matches up with v i in the summary. It is defined as the average contrastive similarity of the sentence pairs in S :
A good contrastive opinion summary should intuitively have both high representativeness and high contrastiveness, thus we may cast the problem of contrastive opinion summarization as the following optimization problem: where  X   X  (0 , 1) is a parameter to control the relative importance of representativeness and contrastiveness with a larger  X  indicating more emphasis on the representativeness.

With such an optimization framework, we see that in order to find an optimal contrastive opinion summary, we will need to solve three problems: 1. Define an appropriate content similarity function  X  . 2. Define an appropriate contrastive similarity function  X  . 3. Solve the optimization problem efficiently.

In the next two sections, we will discuss how we solve these problems.
In this section, we discuss how we implement the two similar-ity functions  X  and  X  . Our optimization framework allows us to flexibly implement them in any reasonable way as long as the two similarity functions can be normalized into the same range. This normalization is needed to ensure that the terms of these two func-tions in the objective function be comparable.

The content similarity function  X  is meant to be a normal sen-tence similarity measure applied to two sentences in the same opin-ion group. In order to consider semantic matching of terms, we define  X  ( s 1 , s 2 ) generally as: where  X  ( u, v )  X  [0 , 1] is a term similarity function and | s | s | are the total counts of words in sentences s 1 and s 2 , respec-tively.

The idea of this formula is that we would first match every word in each sentence against words in the other sentence to find its best matching score, then take a sum of all the matching scores, and finally normalize the sum by the total number of words in the two sentences. Since  X  ( u, v )  X  [0 , 1] , clearly  X  ( s 1 , s range of [0 , 1] .

Depending on how we define  X  , we can obtain several different variations of this general similarity function. In this paper, we will experiment with the following two natural variants: 1. Word Overlap (WO):  X  WO ( u, v ) = 1 iff u = v , and 2. Semantic Word Matching (SEM): :  X  SEM ( u, v ) = 1 if http://www.d.umn.edu/ tpederse/similarity.html
The contrastive similarity function  X  is meant to measure how well two sentences with opposite opinions match up with each other. Intuitively, we would like the two sentences to overlap on all words except for those sentiment related words, where they are expected to differ. Thus, we define  X  in the same way as we define  X  except that we now calculate the similarity after removing negation words and adjectives from both sentences. The rationale is that opinions are mainly expressed by adjectives and negation words. We also have two variations for  X  : WO and SEM.

Note that although in this paper we only experiment with these simple similarity measures, our optimization framework would al-low us to potentially use more sophisticated measures (e.g., [5, 21, 2, 15, 23]). A brute-force solution to the optimization problem defined in Section 3 would be to enumerate all the possible candidate sum-maries (i.e., k sentence pairs) and compute the objective function for each candidate summary. Since | X | = n and | Y | = m , there are altogether merating all of them is generally not feasible especially because computation of the value of the objective function for a candidate summary also involves additional iterations.

We now propose two heuristic strategies to find an approximate solution to the optimization problem. Our objective function con-tains two parts, corresponding to the representativeness ( r ( S ) ) and contrastiveness ( c ( S ) ) of a summary, respectively. Thus a greedy way to optimize the objective function can be to first find a subset of summaries that can score well with one of them, and then try to further select a good summary that can also score well for the other component. Depending on whether we would first optimize r ( S ) or c ( S ) , we naturally have two heuristic strategies to generate a contrastive opinion summary, called representativeness-first and contrastiveness-first, respectively.
To optimize r ( S ) means to select k sentences from each of X and Y that can best represent all the sentences in X and Y . Intu-itively, we may achieve this goal by clustering the sentences in X and Y independently to generate k clusters for each, and then take the most representative sentence from each cluster. Specifically, let { U 1 , ..., U k } be k clusters of sentences in X , and { V 1 k clusters of sentences in Y . We may assume that S = { ( u where u i  X  U i and v i  X  V i , for i = 1 , ..., k . In general, given an implementation of the content similarity function  X  , any clustering algorithm can be used. In our experiments, we used the hierar-chical agglomerative clustering algorithm and stopped it when we have obtained precisely k clusters.

Now, we may reasonably assume that the similarity of a sentence to another sentence in a different cluster is always lower than its similarity to a sentence in its own cluster. It is not hard to prove that the summary S to maximize r ( S ) is given by the centroid sentences in each cluster. That is, S = {  X  u i ,  X  v i } , and Next we would like to keep r ( S ) constant and optimize c ( S ) . Clearly c ( S ) depends on how we index the clusters of X and Y , that is, how we order { U 1 , ..., U k } and { V 1 , ..., V it depends on how we align a cluster U i with a cluster V we would like to align them so that the corresponding u i would have the highest contrastiveness similarity, i.e., to maximize  X  ( u i , v i ) . Since k is generally small, we can find the exact optimal alignment without approximation.

One may notice that the weighting parameter  X  did not matter in this strategy. Indeed, we have implicitly set  X  =  X  by first at-tempting to optimize r ( S ) and then fix it to further optimize c ( S ) . To further improve over this, we may search in each aligned clus-ter pair to find a potentially better pair of sentences that can lead to a higher objective function value than the centroid pair defined above. If we do this, we will see that  X  would affect our solution.
Specifically, let { U 1 , ..., U k } and { V 1 , ..., V k alignment of clusters. We may rewrite our objective function as g ( S ) = where g i ( u i , v i ) is given by
This objective function is now a sum over all the aligned cluster pairs, and we can now find the solution by optimizing each ( u pair independently. Formally,
Clearly g i (  X  u i ,  X  v i ) is not necessarily optimal, so we would like to search in U i and V i for a truly optimal ( u  X  i , v  X  i search has a complexity of O ( | U i || V i | ) , but we do not have to try every pair of sentences. Instead, we only need to try those pairs with a higher contrastiveness score than our centroid pair (  X  u because if a pair has a lower contrastiveness score than the cen-troid pair, it would be impossible for it to have a higher g Thus computationally, we can simply sort all the pairs in each pair of clusters in the descending order of contrastiveness scores and then go down the list to compute its g i value, until we hit the cen-troid pair. The pair that gives the highest g i would be taken as ( u summary S  X  = { ( u  X  i , v  X  i ) } .
In this strategy, we first compute  X  ( u, v ) for all u  X  X and v  X  Y . We then sort these pairs and gradually add a sentence pair to our summary starting with the pair with the highest con-trastive similarity score. If we just take the top k pairs, we would find a solution corresponding to setting  X  = 0 , i.e., solely based on contrastiveness and ignoring representativeness completely. Thus to improve our approximation, we would like to sacrifice some amount of contrastiveness score and gain more on the represen-tativeness score.

A greedy algorithm to achieve this is as follows. First, we would take the pair with the highest value of  X  as a pair in our sum-mary. Given that we have already chosen i  X  1 pairs S { ( u j , v j ) } i  X  1 j =1 , we would like to choose the next pair ( u most to our objective function, which further means to maximize the increase of both the contrastiveness and the representativeness. Given a candidate pair ( u i , v i ) , and let S i be the augmented sum-mary of S i  X  1 by adding this new pair. We want to choose ( u to maximize the following objective function : where X u i ( Y v i ) is the set of sentences in X ( Y ) that are closer to u ( v i ) than to any previously chosen u j ( v j ), j = 1 , ..., i  X  1 . That is,
Thus in our greedy algorithm, after choosing the first pair ( u we would iteratively choose ( u i , v i ) to maximize the  X  X ain func-tion X , g ( u i , v i ) given by The algorithm stops after having chosen k pairs.

Note that X u and Y v are relatively easy to compute if we remem-ber the best  X  values of all sentences given by the i  X  1 already cho-sen sentence pairs at each step because we only need to compare the remembered value with the new value of  X  given by u i
In general, we would need to consider all the remaining sentence pairs in each step. However, we can further improve efficiency by only considering the sentence pairs whose contrastiveness scores are sufficiently high (e.g., above a threshold).
There is no existing data set for evaluating our new summariza-tion task. We thus opt to create our own. Since a main assumption made in our problem definition is that we may separate positive and negative opinions about a topic using existing opinion summariza-tion methods, a natural strategy to create a test set for evaluating COS would be to leverage such separated opinion data set gener-ated by previous work. We thus obtained 14 tagged product review data sets from the previous work [7, 8] 3 , and have two human as-sessors to identify representative contrastive sentence pairs from these data. The 14 tagged review data sets contain reviews from Amazon 4 . All the sentences in these data sets have already been manually tagged with product features as well as sentiment polari-ties (i.e., positive or negative).

To show our methods can help users further understand opinions at a finer granularity level than the feature-level, we divided a prod-uct review into product-feature reviews based on the feature tags. Also, to make contrastive opinion summarization interesting, we chose reviews which are not extremely dominated by only positive (or negative) opinions using a threshold. Our assumption is that in those cases where reviews are dominantly of one polarity of opin-ions (e.g., dominantly positive), a regular summary would suffice, and we do not need to apply contrastive opinion summarization techniques. Because our data set is for evaluating the effectiveness of COS, we discarded extremely dominated sets. Based on these criteria, we obtained 12 review sets on several products and fea-tures.

To test the generality of our methods, we also prepared another non-product-review data set. We used the Yahoo! search engine to retrieve sentences about Aspartame, which is an artificial sweet-ener, and there are disputes about its safety. The constructed data set contains 50 positive and 50 negative matching sentences se-lected from the search results of the queries  X  X spartame is safe X  and  X  X spartame is dangerous X  , respectively. http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html http://www.amazon.com
Table 2 shows the data set list. For each data set, ID is assigned for convenience.

For each test case, the two assessors were asked to cluster given sentences of each polarity of sentiment and align the contrastive clusters. Among the judgments made by human evaluators, clusters that cannot be aligned are discarded.

To assess the agreement of the two assessors, we compute their clustering agreement and pairing agreement, which are 0.76 and 0.47, respectively. The clustering agreement is the percentage of agreed decisions on putting a pair of sentences of the same polarity into the same cluster or not by the two annotators. In the original sentence sets, we can make same-polarity-sentence pairs, ( u where both u i and u j are positive (negative). For all the possible pairs, check if they are in the same cluster or not based on the two evaluators X  judgments. If the judgments are same, it is an agree-ment. Then, clustering agreement is The pairing agreement is computed using the Jaccard Index. First, we generate all the ideal contrastive sentence pairs from the eval-uators X  judgements on clustering and pairing. For example, if two clusters, { u i , u j } and { v l , v m } , are paired, ( u and ( u j , v m ) would be generated as ideal pairs. Let A and B be the two sets of ideal pairs from two assessors, respectively, the Jaccard Index was calculated by the following formula
In our experiments, we use each assessor X  X  judgments separately for evaluation and then take the average of the two performance numbers. The constructed data sets are publically available
We evaluate our contrastive opinion summary with the following two measures based on the aligned cluster data set: Precision: The precision of a summary with k contrastive sentence pairs is the percentage of the k pairs that are agreed by a human an-notator. If a retrieved pair exists in an evaluator X  X  paired-cluster set, we assume that the pair is agreed by the annotator (i.e.,  X  X elevant X ). Thus precision is basically the number of such agreed pairs divided by k . Intuitively, precision tells us how contrastive the sentence pairs of our summary are. http://timan.cs.uiuc.edu/data/cos/ Aspect coverage: The aspect coverage of a summary is the per-centage of human-aligned clusters covered in the summary. If a pair of sentences appears in a human-aligned pair of clusters, we would assume that the aligned cluster is covered. Intuitively, as-pect coverage measures the representativeness of a summary.
The number k of a target summary was set heuristically by the following formula; k = 1 + log 2 ( | X | + | Y | ) . The intuition is that we will have a larger k if we have more sentences to summarize, but the growth will  X  X aturate X  as the number of sentences becomes very large. We design our experiments to answer the following questions: First, between representative-first approximation(RF) and contrastive-first approximation(CF), which optimization algorithm performs better? We can answer this question by comparing the perfor-mance of these two different approximations for various values of  X  . Second, both  X  and  X  can use semantic matching of words. Can semantic matching of words help improve performance on top of simple exact matching of words? We can answer this question by comparing the performance of methods using different semantic coefficient  X  . Third, we have hypothesized that it would be benefi-cial to exclude sentimental words when computing the contrastive similarity. Is this heuristic effective? We can answer this question by comparing performance of excluding such words with that of not excluding them (i.e., using all the words).
We first show some sample contrastive sentence pairs generated in our experiments in Table 3. Intuitively, these pairs are quite in-formative and can clearly help a user better understand the mixed opinions in different aspects. The first and second pairs show that different polarities of opinions are made from different perspec-tives. For example, from the first pair, a user would know that file transfer is fast, but you X  X l need transfer software. Similarly, the second pair shows that adjustment knob generally works well, but it is inconvenient when lowering the router. In the third pair, al-though the two sentences are classified as positive and negative, respectively, the difference is rather small, indicating that there is probably not that much disagreement here. In the fourth pair, we can learn even more details about the product. This example shows the battery life can vary depending on usage patterns even with spe-cific number of hours it can last; users can decide whether to buy this product based on their own usage style.
Next, we use the basic similarity measure (  X  WO ) to compare the two approximation strategies, i.e., representativeness-first (RF) and contrastiveness-first (CF) in Figure 2. For both methods, as-pect coverage is higher than precision, indicating that it is easier to achieve representativeness than contrastiveness. In general, we see that CF outperforms RF for almost all values of  X  , indicating that it is more important to optimize contrastiveness-first to ensure that we obtain the best contrastive alignments of sentences. We also see that the performance is sensitive to the setting of  X  , the rela-tive emphasis on the representativeness and contrastiveness of the summary. In order to examine other variables in our methods, in the following experiments, we set  X  to a reasonable value of 0 . 5 , which intuitively means that we put equal weights on the two criteria. We now look into the effectiveness of semantic term matching. Since  X  WO is a special case of  X  SEM when  X  = 0 , we can see whether semantic matching helps by varying the value of  X  . We show the results of using semantic term matching for content simi-larity and contrastive similarity respectively with two separate plots in Figure 3. In general, semantic term matching does not seem to help. Indeed, as we increase the value of  X  , the performance tends to drop. We also see that the content similarity function is more sensitive to semantic matching than the contrastive similarity func-tion. This may be because in the latter case, sentimental words are removed, so the overall influence of semantic matching would be reduced.
We hypothesized that by removing sentimental words in com-puting the contrastive similarity function we can improve matching accuracy. So finally, in order to test this hypothesis, we compare the results of using this heuristic with those of not using it (i.e., com-puting the contrastive similarity in the same way as computing the content similarity) in Table 4. We see that if we keep all these sen-timental words, the performance is consistently worse, indicating that the heuristic of removing sentimental words is effective. Table 4: Effectiveness of removing sentimental words in com-puting contrastive similarity.

Opinion summarization is an active research area because of the increased volume of opinionated data. General opinion mining was focused on finding topics among articles and clustering positive and negative opinions on topics [7, 8, 13, 9, 20, 16]. Most of the results of opinion summarization focused on showing statistics of the num-ber of positive and negative opinions. Usually people used table-shaped summary [7, 8, 16] or histogram [13]. Sometimes, each section had an extracted sentence from the article and had a link to the original one. It was not enough to show the details of the differ-ent opinions. Representative opinion summarization works mainly focused on sentiment classification on various aspects [7, 8, 13]. These studies generally relied on heuristic methods and data min-ing techniques such as association rule mining to identify aspects and sentiments of aspects. Figure 3: Effectiveness of semantic term matching for content similarity (top) and contrastive similarity (bottom)
Some opinion summarization work used probabilistic topic mod-eling methods such as probabilistic latent semantic analysis (PLSA) [6] and latent Dirichlet allocation (LDA) [3]. Topic sentiment mixture model [16] extended PLSA model with opinion priors to show pos-itive and negative aspects of topics effectively. This model finds la-tent topics as well as its associated sentiment and also reveals how opinion sentiments evolve over the time line. In [22], multi-grain topic model was proposed as an extension of LDA. This work finds ratable aspects from reviews and generates summaries for each as-pect. The proposed multi-grain LDA topic model can extract local topics which are ratable aspects written by an individual user as well as cluster local topics into global topics of objects such as the brand of a product type.

Heuristic rule-based methods have also been used in opinion summarization. Usually these methods have two steps: features ex-traction and opinion finding for each feature. In [13, 8, 9], features of products are found using supervised association rule mining and rules such as opinion features are usually noun phrases. To con-nect extracted features with opinion words, WordNet is also used. [24] focused on movie review domain. Based on domain-specific heuristics such as many features tend to be around the cast of a movie, features can be found more efficiently. Machine learning techniques [18, 11] and relaxation labeling [20] are also used for features extraction and opinion summary.

In addition to these representative probabilistic and rule-based approaches to opinion summarization, opinion integration [14] and sentiment classification [17] are also related to our work.
A main distinction of our work from these studies is that we aim at summarizing contradictory opinions. As discussed in the begin-ning of this paper, our work extends the existing work on opinion summarization to help users further digest and understand contra-dictory opinions.

There were some works on extracting comparative sentences or detecting contradiction in text. In [10], methods are proposed for detecting comparative sentences by checking signal words like  X  X han X . In [5], the authors structurally analyzed the characteris-tics of contradiction and suggested heuristic methods for detecting contradiction in text. Some work considered finding contradiction as a binary classification problem. In [21], support vector ma-chine(SVM) is applied on classification of support and oppositions in text, while in [2, 15], methods based on graph representing rela-tionship between texts or authors are used for classifying texts. Al-though these works proposed methods to find contradictions, they did not directly address the problem of summarizing contradictory opinions, for which we need to model both contrastiveness and rep-resentativeness.

In [4], the authors studied visualization of different opinions and showed various visualization methods using graph and tree. The work [23] about mining mixed opinions using topic model is also related to the current work. But none of these works can generate a comparative summary of contradictory opinions as we do.

Different sentence similarity measures are explored in [1]. The authors compared the performance of word overlap measures, TF-IDF measures, linguistic measures and combination of them over different data corpus, and found that linguistic measures perform the best in finding similar sentences, and TF-IDF measures per-form well for deciding whether input sentences are dissimilar or not. We also compared different similarity measures for the COS problem, and our results show that semantic matching appears to be not useful for this task.
In this paper, we proposed a novel summarization problem, namely contrastive opinion summarization. It aims to summarize contra-dictory or mixed opinions about a topic and generate a list of con-trastive pairs of sentences with different sentiment polarities to help users to digest contradictory opinions. We formally framed the problem as an optimization problem and proposed two approxima-tion methods to solve the optimization problem. We also explored different similarity measures in our optimization framework. We leverage existing summarization resources to create a gold standard data set for evaluating the proposed new summarization task.
Experiment results using this data set show that the proposed methods are effective for generating contrastive opinion summaries. In particular, contrastiveness-first approximation works better than representativeness-first, and the heuristic of removing sentimental words in computing contrastive similarity is effective. However, semantic term matching based on WordNet is found to be not help-ful. Sample summary results show that the generated summaries are informative and can help users digest contradictory opinions more effectively.

Our work can be extended in several directions. First, more ex-periments on additional larger data sets would be desirable. Sec-ond, we have only explored some basic semantic term matching method; it would be interesting to further explore more advanced similarity functions such as those based on sentence alignment or more in-depth semantic analysis. Finally, it should also be very interesting to further study how to develop algorithms to achieve better approximate solutions to the optimization problem using our framework. We thank the anonymous reviewers for their useful comments. This material is based upon work supported by the National Science Foundation under Grant Numbers IIS-0347933, IIS-0713581, and IIS-0713571. [1] P. Achananuparp, X. Hu, and X. Shen. The evaluation of [2] M. Bansal, C. Cardie, and L. Lee. The power of negative [3] D. M. Blei, A. Y. Ng, M. I. Jordan, and J. Lafferty. Latent [4] C. Chen, F. I. Sanjuan, E. Sanjuan, and C. Weaver. Visual [5] M. C. de Marneffe, A. N. Rafferty, and C. D. Manning. [6] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR [7] M. Hu and B. Liu. Mining and summarizing customer [8] M. Hu and B. Liu. Mining opinion features in customer [9] M. Hu and B. Liu. Opinion extraction and summarization on [10] N. Jindal and B. Liu. Identifying comparative sentences in [11] L.-W. Ku, Y.-T. Liang, and H.-H. Chen. Opinion extraction, [12] B. Liu. Web Data Mining: Exploring Hyperlinks, Contents, [13] B. Liu, M. Hu, and J. Cheng. Opinion observer: analyzing [14] Y. Lu and C. Zhai. Opinion integration through [15] R. Malouf and T. Mullen. Graph-based user classification for [16] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. Topic [17] B. Pang and L. Lee. Opinion Mining and Sentiment Analysis , [18] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?: [19] T. Pedersen, S. Patwardhan, and J. Michelizzi. Wordnet: : [20] A.-M. Popescu and O. Etzioni. Extracting Product Features [21] M. Thomas, B. Pang, and L. Lee. Get out the vote: [22] I. Titov and R. McDonald. Modeling online reviews with [23] C. Zhai, A. Velivelli, and B. Yu. A cross-collection mixture [24] L. Zhuang, F. Jing, X. Zhu, and L. Zhang. Movie review
