 To fulfill users' search needs, the search engine must have good performance, easy-to-use functionalities, and good search result quality. Se arch quality evaluation be comes challenging when users' satisfaction may not be able to judge by a single search and even within a single search judgments from various sources ar e not consistent. In this talk, I will discuss how user's satisfaction is decomposed into different components in general, and how we measure them with various means -human judgment, automatic computation with query log, and outsourcing, and their pros and cons with operational implicati ons. For an outlook, I will postulate potential evaluation approaches for a better user's satisfaction. MANAGEMENT: H.2.8 Database appl ications: Subjects: Data mining Gordon Sun has been working on algorithmic search technology since 1998 when he joined Inktomi (the leading US search engine company dur ing 1990s) as the senior scientist and architect. He also worked for two search engine companies, WiseNut and LookSmart as the director of R&amp;D during 2001 to 2003 before he joined Yahoo earl y 2004 where he was leading the Global Search Relevance team as the director of research until 2009. Gordon graduated from University of Science and Technology of China and went to th e US through the CUSPEA program (sponsored by Nobel Price Winner Prof. T.D. Lee ) in 1981 and r eceived his Ph.D. in theoretical physics from University of Iowa, 1984. He worked as the Resear ch faculty in University of Maryland for 9 years before he moved to Silicon Valle y, 1993, and worked in Communicat ion Intelligence Inc, a leading hand-writing recognition provider, as the Chief Sc ientist. He has broad work experiences, knowledge and publications in neural networks, patt ern recognition, machine learning, data mining, information retrieval, speech recognition, hand-writing rec ognition and non-linear dynamics. 
