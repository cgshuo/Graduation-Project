 In commercial search engines, a ranking function is selected for deployment mainly by comparing the relevance measure-ments over candidates. In this paper we suggest to select Web ranking functions according to both their relevance and robustness to the changes that may lead to relevance degra-dation over time. We argue that the ranking robustness can be effectively measured by taking into account the rank-ing score distribution across Web pages. We then improve NDCG with two new metrics and show their superiority in terms of stability to ranking score turbulence and stability in function selection.
 H.4 [ Information Systems Applications ]: Miscellaneous Measurement, Reliability web ranking, ranking robustness, NDCG
In Web search, a ranking function usually ranks the search result pages for a user query, by first assigning a score that measures the relevance of each page to the query, and then ranking pages in the descendent order of the ranking score.
For function selection purpose, metrics such as Mean Av-erage Precision and NDCG have been developed to measure the search result relevance. The underlying assumption for this framework is that the relevance of the selected func-tion will be consistently better over time after deployment, compared with other candidates.

However, this assumption usually does not hold for Web search. As the Web size expands, Web content updates, and rank features that the function was trained on may get updated. A deployed ranking function may not be so ro-bust in the sense that its relevance may degrade with these changes, and the relevance comparison done in function se-lection may not be valid any more. One reason for that is  X  The first two authors contribute to this paper equally. that their calculation is highly dependent on the ranking or-der of the search results, and the relevance judgment of each page, but not sensitive or smooth to the ranking score distri-bution over search result pages for a query [3]. Take NDCG as an example and consider two score lists with the same grades of the three pages are {  X  X xcellent X ,  X  X ad X ,  X  X ad X  } . Although the two lists have the identical NDCG values, ap-parently (2) is better than (1) in terms of ranking robustness with regard to potential turbulence in scores. This demon-strates the inadequacy of the NDCG in function selection when score turbulence can happen.

There are several related works in this line. [1] addresses the ranking robustness with regard to spamming. [5] de-fines a ranking robustness metric to predict query search performance, but the calculation of designed metric highly relies on a retrieval function. [3] defines a variant of NDCG (namely SoftNDCG) that is smooth to the ranking scores.
In this paper, we claim that for selecting web ranking func-tions, purely based on relevance is inadequate, and ranking robustness should also be considered and measured. We study two new variants of NDCG that combine relevance and ranking robustness measurement and compare them with NDCG. We demonstrate that the two new metrics are more stable.
NDCG [2] is a standard measure for evaluating the search relevance. For a given query, and N Web pages (each is in-dexed by j , and assigned with a human-judged grade g ( j ),  X  j  X  X  1 , 2 ,  X  X  X  , N } ), a ranking function assigns a relevance score s ( j ) to each page, and ranks the pages in a descen-dent order of the score. Let r j  X  X  0 , 1 ,  X  X  X  , N  X  1 } denote the rank of page j . Then the NDCG of the ranked list is tion that is often chosen in the form of D ( r ) = 1 / log(2 + r ) and G max is the maximum possible value of P N j =1 g ( j ) D ( r achieved when pages are optimally ordered. As defined above, the NDCG computation only relies on the order of a ranked list, and is regardless to the ranking scores.
One metric that considers ranking scores and their distri-bution across pages is SoftNDCG, introduced in [3]: where p j ( r ) is the probability of document j ranked at po-sition r . In [3], a generative process is proposed to estimate the distribution.
Another metric that can be viewed as a simplified version of SoftNDCG is rNDCG (robust NDCG), which only con-siders the ranking order changes in neighboring positions. where p r is the probability that the page in rank r may switch with the one in rank r + 1, due to score turbulence. p normalizer that controls the shape of the sigmod function. In this section, we compare the three metrics: NDCG, SoftNDCG, rNDCG in two experiments: 1) study how the relevance of a ranking function evaluated in those metrics with artificial noise; 2) study how the function selection between two candidate functions differ using different met-rics. The data set used for evaluating the three metrics con-tains 4 , 000 queries, sampled from a large commercial web search engine. All the query-URL pairs have human labels, with five relevance levels (Perfect, Excellent, Good, Fair and Bad). We train candidates functions with GBrank[4] a state-of-art ranking algorithm. The features used for training can be roughly divided into four categories: link-based features, content-based features, click-based features and others.
We compute NDCG and SoftNDCG over the ranking scores given by a ranking function, but under additive noise from a Gaussian noise model, while the NDCG and the SoftNDCG values here are averaged over all the 4 , 000 queries. To demonstrate the instability of NDCG, we implement the ex-periment ten times with the additive noise generated by ten different random seeds. The results are shown in Figure 1, showing that the SoftNDCG has smaller variance. Figure 1: The evolutions of NDCG and SoftNDCG as
This experiment is to study whether SoftNDCG and rNDCG is more reliable than NDCG in ranking function selection. Figure 2: The evolutions of mean (left subgraph) and For this purpose, We first train two ranking functions (named as f 1 and f 2 ) using GBRank with different learning parame-ters and feature sets. Then we randomly sample 1 , 000 sub-sets (each contains K queries, we also study the impact of different K ) from the test data, and calculate NDCG, soft-NDCG and rNDCG metrics on these subsets. After that, we construct Binomial distributions using the following rule: For each of the subset, a binary random variable is assigned to be 1 if the metric shows a higher and equal value (tied val-ues are very rare) for f 1 than f 2 averaged over the queries, and 0 otherwise. This way, we have a Binomial distribution for each of the metric over the 1 , 000 subsets.

Figure 2 plots the mean and variance curve for the evolu-tions of the three Binomial distributions as the subset size K increases. The mean curves of three binomial distributions converge as the subset size increases, while the variances con-verges to zero. Both NDCG metric and the two new metric indicates that f 1 is better than f 2 , when test sets are large, which means these three metrics are consistent. However, the distribution achieved from the softNDCG and rNDCG metric have smaller variances, which means they are more reliable for function evaluation.
We investigated several evaluation metrics for ranking func-tion selection, and argue that the ranking robustness with regard to ranking score turbulence should be considered in function selection. [1] R. Bhattacharjee and A. Goel. Algorithms and [2] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [3] M. Taylor, J. Guiver, S. Robertson, and T. Minka. [4] Z. Zheng, K. Chen, G. Sun, and H. Zha. A regression [5] Y. Zhou and W. B. Croft. Ranking robustness: a novel
