 such as AdaBoost [1] have shown considerable success in practice.
 and so may use large datasets much more efficiently than batch boosters. bounds on weak hypothesis error rates and combining weak hypotheses via unweighted majority probabilities. We then bound the number of rounds and examples required to achieve any target fewer assumptions. We also show that FilterBoost can use the confidence-rated predictions from weak hypotheses described by Schapire &amp; Singer [9].
 higher accuracies faster than batch boosters in many cases.
 with existing methods for both conditional probability estimation and classification. generated. The goal in classification is to choose a hypothesis h : X  X  Y which minimizes the to ( x, y ) sampled randomly from D .
 The sign of H ( x ) indicates the predicted label  X  y for x .
 Two key elements of boosting are constructing D t over S and weighting weak hypotheses. D t is generally weighted such that hypotheses with lower errors receive higher weights. 2.1 Boosting-by-Filtering example oracle, allowing it to use entirely new examples sampled i.i.d. from D on each round. D t by drawing examples from D via the oracle and reweighting them according to D t . Filtering oracle with some probability is called the filter.
 combination of h 1 , . . . , h T as a final hypothesis. The filtering setting allows the learner to estimate the error of H t to arbitrary precision by sampling from D via the oracle, so FilterBoost does this to de-cide when to stop boosting. 2.2 FilterBoost FilterBoost, given in Figure 1, is mod-eled after the aforementioned algo-rithm by Collins et al. [10] and Mada-Boost [5]. Given an example oracle, weak learner, target error  X   X  (0 , 1) , and confidence parameter  X   X  (0 , 1) upper-bounding the probability of fail-ure, it iterates until the current com-bined hypothesis H t has error  X   X  .
 On round t , FilterBoost draws m t ex-amples from the filter to train the weak learner and get h t . The number m t must be large enough to ensure h t has error t &lt; 1 / 2 with high probabil-ity. The edge of h t is  X  t = 1 / 2  X  t , and this edge is estimated by the func-tion getEdge () , discussed below, and is used to set h t  X  X  weight  X  t . The cur-rent combined hypothesis is defined as H t = sign( P The F ilter () function generates ( x, y ) from D t by repeatedly drawing ( x, y ) from the oracle, calculating the weight q ( x, y )  X  D t ( x, y ) , and accepting ( x, y ) with probability q t ( x, y ) .
 Function getEdge () uses a modifica-tion of the Nonmonotonic Adaptive Sampling method of Watanabe [11] and Domingo, Galvad ` a &amp; Watanabe [12]. Their algorithm draws an adap-tively chosen number of examples from 2.3 Analysis: Conditional Probability Estimation log likelihood of example ( x, y ) after round t as  X  ( F t +  X  t h t ) =  X  ( F +  X h ) = E  X  ln proof is in the Appendix. chooses  X  to minimize an upper bound of  X  .
 converge to the minimum of this objective when working over a finite sample. Note that Filter-Boost uses weak learners which are simple classifiers to perform regression. AdaBoost too may E[exp(  X  yF ( x ))] as the optimization objective [13]. 2.4 Analysis: Classification generated by the oracle on round t .
 Proof: bound in Lemma 2 and Theorem 5. The proofs of Lemmas 1 and 2 are in the Appendix. Lemma 1 Assume for all t that  X  t 6 = 0 and  X  t is estimated exactly. Let  X  t =  X  P ( x,y ) D ( x, y ) ln(1  X  q t ( x, y )) . Then with Lemma 1 gives the following upper bound on the required rounds T . FilterBoost runs err t  X   X ,  X  t  X  X  1 , ..., T } . Then, from Theorem 2, p t  X   X / 2 , so Lemma 1 gives Unraveling this recursion as P T t =1 (  X  t  X   X  t +1 ) =  X  1  X   X  T +1  X   X  1 gives bound follows from the first bound via the inequality 1  X  revise Lemma 1 and Theorem 3 to account for error in estimating edges. The number m t of examples the weak learner trains on must be large enough to ensure weak hy-pothesis h t has a non-zero edge and should be set according to the choice of weak learner. then err t  X   X  with probability at least 1  X   X  0 t .
 We estimate weak hypotheses X  edges  X  t using the Nonmonotonic Adaptive Sampling (NAS) algo- X   X  (0 , 1) with probability  X  1  X   X  t , the NAS algorithm uses at most 2(1+2  X  ) 2 (  X  X  examples. With this guarantee on edge estimates, we can rewrite Lemma 1 as follows:  X  =  X  P ( x,y ) D ( x, y ) ln(1  X  q t ( x, y )) . Then Boost runs rounds, then err t &lt;  X  for some t , 1  X  t  X  T .
 MadaBoost [5], which we test in our experiments, resembles FilterBoost but uses truncated ex-however, that boosting tandems result in more complicated final hypotheses. potheses return predictions whose absolute values indicate confidence. These values are chosen  X  . We also modify FilterBoost X  X  getEdge () function for efficiency. The Nonmonotonic Adaptive C of stopping once examples are depleted, and we use this  X  X ecycling X  in our tests. We compare FilterBoost against MadaBoost [5], which does not require an a priori bound on weak claims as its goal.
 learners minimize exponential loss when outputing confidence-rated predictions. We use four datasets, described in the Appendix. Briefly, we use two synthetic sets: Majority (majority vote) and Twonorm [15], and two real sets from the UCI Machine Learning Repository example quickly shrinks when the booster has seen that example many times. 3.1 Results: Conditional Probability Estimation In Section 2.3, we discussed the in-terpretation of FilterBoost and Ada-Boost as stepwise algorithms for con-ditional probability estimation. We test both algorithms and the variants discussed above on all four datasets.
 We do not test MadaBoost, as it is not clear how to use it to estimate conditional probabilities. As Figure 3 shows, both FilterBoost variants are competitive with batch algorithms when boosting decision stumps. With decision trees, all algorithms except for FilterBoost overfit badly, includ-ing FilterBoost(C-R). In each plot, we compare FilterBoost with the best of AdaBoost and AdaBoost-LOG: AdaBoost was best with decision stumps and AdaBoost-LOG with de-cisions trees. For comparison, batch logistic regression via gradient de-scent achieves RMSE 0.3489 and log (base e ) loss .4259; FilterBoost, inter-when boosting decision stumps.
 boosters must use C m on the order of 10 5 , by which point they become very inefficient. 3.2 Results: Classification predictions allow FilterBoost to outperform MadaBoost when using decision stumps but sometimes References
