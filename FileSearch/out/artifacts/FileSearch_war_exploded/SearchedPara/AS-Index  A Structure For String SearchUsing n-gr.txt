 AS-Index is a new index structure for exact string search in disk res-ident databases. It uses hashing, unlike known alternatives, whether baesd on trees or tries. It typically indexes every n -gram in the database, though non-dense indexing is possible. The hash function uses the algebraic signatures of n -grams. Use of hashing provides for constant index access time for arbitrarily long patterns, unlike other structures whose search cost is at best logarithmic. The stor-age overhead of AS-Index is basically 500 -600%, similar to that of alternatives or smaller.

We show the index structure, our use of algebraic signatures and the search algorithm. We present the theoretical and experimental performance analysis. We compare the AS-Index to main alterna-tives. We conclude that AS-Index is an attractive structure and we indicate directions for future work.
 E.1 [ Data ]: Data Structures Performance Full text indexing, Large scale indexing Databases increasingly store data of various kinds such as text, DNA records, and images. This data is at least partly unstructured, which creates the need for full text searches (or pattern match-ing) [15]. In main memory, matching a pattern P against a string S runs in O ( | S | / | P | ) at best [4]. Searching very large data sets requires an index, despite the storage overhead and possibly long index construction time.

We address the problem of searching arbitrarily long strings in external memory. We assume a database D = { R 1 , R 2 ,  X  X  X  , R of records, viewed as strings over an alphabet  X  . The database supports record insertion, deletion and updates, as well a search on any substring of the records X  contents. We call the input to the string the pattern .

Currently deployed systems for documents use almost exclusively inverted files indexing words for keyword search. However, our need for full pattern matching in records where the concept of word may not exist rules out this solution. Suffix trees and arrays form a class of indexes for pattern matching. Suffix trees work best when they fit into RAM. Attempts to create versions that work from disks are recent, experimental, and focus typically on specific applica-tions [7, 18]. The literature attributes this to bad locality of refer-ence, a necessarily complex paging scheme, and structural deteri-oration caused by inserts into the structure. A suffix array stores pointers on a list of suffixes sorted in lexicographic order, and uses binary search for pattern matching. However, a standard database architecture stores records in blocks and supports dynamic inser-tions and deletions that make difficult the maintenance of sequen-tial storage. A suffix array search needs O (log 2 N ) block accesses, where N is the size (number of characters) of D . We are not aware of a generic solution to these difficulties in the literature, and with-out one, these costs disqualify suffix arrays for our needs.
Two approaches that explicitly address disk-based indexing for full pattern-matching searches are the String B-Tree [7] and n -gram inverted index [16, 11]. The String B-Tree is basically a combina-tion of B + -Tree and Patricia Tries. The global structure is that of a B + -Tree, where keys are pointers to suffixes in the database. Each node is organized as a Patricia Trie, which helps guiding the search and insert operations. A String B-Tree finds all occurrences of a pattern P in O ( | P | + log B N ) disk accesses, where B is the block size. Another direction for text search needs are indexes inverting n -grams ( n consecutive symbols) instead of entire words. They are  X  X isk friendly X  in that they rely on fast sequential scans, provide a good locality of reference, and easily adapt to paging and parti-tioning. However, the search cost is linear, both in the size of the database and the size of the pattern.

In the present paper, we introduce a new data structure for index-based full-text search called Algebraic Signature Index (AS-Index). It follows the path of an inverted file based on n -grams. Its novel attractive property, unique at present to our knowledge, is constant disk search time, independent of the size of the database and of the length of the pattern. This results from its standard database ap-proach for large-scale indexing (namely hashing). AS-Index hash calculus is however specific. As the name suggests, it relies on al-gebraic signatures, (ASs) [13], whose algebraic properties we can exploit.

Our experiments show AS-Index to be a very fast solution to pattern searches in a database. AS-Index search only needs two disk lookups when the hash directory fits in main memory. In our experiments, it proved itself to be up to one order of magnitude faster than n -gram indexes and twice faster that String B-Trees. The basic variant of AS-Index has a storage overhead of about 5 to 6 . A variant of our scheme only indexes selective n -grams and has lower storage overhead at the costs of slower search times. All these properties should make AS-Index a practical solution for text indexing.

The paper is organized as follows. Section 2 gives a bird X  X  eye view of the AS-Index basic principles. We recall the theory of alge-braic signatures (Section 3). We then discuss the AS-Index struc-ture in Section 4 and the search algorithm in Section 5. Section 6 analyses the scheme X  X  behavior, especially collision and false pos-itive probability, as well as performance. Section 7 explains the details of our implementation of String B-Trees, n -gram index and AS-Index and experiments. We review related work in Section 8. Finally, we summarize and give future research directions in Sec-tion 9.
AS-Index is a classical hash file with variable length disk-resident buckets, (Fig. 1). Buckets are pointed to by the hash directory. Simplicity and performance of such files attracted countless appli-cations. The main advantage is constant access time, independent of file and pattern size. Constant time is not possible for a tree/trie access method.

Each bucket stores a list of entries , each indexing some n -gram in the database. The basic AS-Index is dense, indexing every n -gram. The hash function providing the bucket for an entry uses the n -gram value as the hash key. The hash function is particu-lar: it relies on algebraic signatures of n -grams, to be described in the next section. In what follows, we only deal with the static AS-Index, but the hash structure may use standard mechanisms for dynamicity, scalabilty and distribution. In overview, a search for a pattern P proceeds as follows (Fig. 1): First, we preprocess P for three signatures: (i) of the initial n -gram S 1 , (ii) of the final n -gram S 2 and (iii) of the suffix S P after S 1 . Hashing on S 1 locates the bucket with every entry e indexing an n -gram in the database with the signature of S Likewise, hashing on S 2 locates the bucket with every entry e Constr. O ( | D | X  log B | D | ) O ( | D | ) O ( | D | )
Storage O ( | D | ) O ( | D | ) O ( | D | ) (ratio) (  X  6-7) (  X  6) (  X  5-6) Preproc. none O ( | P | ) O ( | P | ) Search O ( | P | + log B | D | ) O ( | D | X | P | ) O (1) Table 1: Disk-based index structures for searching a pattern P in a database D . B is the block size. indexing an n -gram with the signature of S 2 . We only consider pairs of entries that are in the same record and at the right distance among them. We thus locate any string S matching P on its initial and terminal n -gram, at least by signature. An algebraic calculation AS ( e 1 , e 2 , S p ) determines whether S p may match the suffix of S as well. The method is probabilistic in nature, with low chances of false positives. We can avoid even a minute possibility of a false match by a symbol for symbol comparison between pattern and the relevant part of the record.

By limiting disk accesses to the two buckets associated to the first and last n -grams of the P , AS-Index search runs indepen-dently from P  X  X  size. The cost of the search procedure outlined above is reduced to that of reading two buckets. The hash directory itself can often be cached in RAM, or needs at most two additional disk accesses, as we will show. With an appropriate dynamic hash-ing mechanism that evenly distributes the entries in the structure and scales gracefully, the bucket size is expected to remain uniform enough to let the AS-Index run in constant time, independently of the database size.

Table 1 compares the analytical behavior of AS-Index with those of two competitors (String B-Trees and n -gram index) and summa-rizes its expected advantages. The size of all structures is linear in the size of the database. The ratio directly depends on the size of index entries. We mention in Table 1 the ratio obtained in our im-plementation, before any compression. The asymptotic search time in the database size is linear for n -index, logarithmic for String B-Trees and constant for AS-Index. Moreover, once the pattern P has been pre-processed, AS-Index runs independently from P  X  X , whereas n -gram index cost is linear in | P | .

In summary, AS-Index efficiently identifies matches with only two lookups, for any pattern length. This efficiency is achieved through extensive use of properties of algebraic signatures, described in the next section.
We use a Galois field GF (2 f ) of size 2 f . The elements of GF are bit strings of length f . Selecting f = 8 deals with ASCII records and f = 16 with Unicode. We recall that a Galois field is a finite set that supports addition and multiplication. These op-erations are associative, commutative and distributive, have neu-tral elements 0 and 1 , and there exist additive and multiplicative inverses. In a Galois field GF (2 f ) , addition and subtraction are implemented as the bitwise XOR. Log/antilog tables provide usu-ally the most practical method for multiplication [13]. We adopt the usual mathematical notations for the operations in what fol-lows. We use a primitive element  X  of GF (2 f ) . This means that the powers of  X  enumerate all the non-zero elements of the Galois field. It is well known that there always exist primitive elements.
Let R = r 0 r 1  X  X  X  r M  X  1 be a record with M symbols. We inter-pret R as a sequence of GF elements. Identifying the character set of the records with a Galois field provides a convenient mathemat-ical context to perform computations on record contents.
D EFINITION 1. The m -symbols signature (AS) of a record R is a vector AS m ( R ) with m coordinates ( s 1 , s 2 , . . . s 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; :
We refer the reader to [13] for more details about definitions and properties of algebraic signatures.

In our examples, we give the m -symbol AS of R as the con-catenation of the values s m ,  X  X  X  , s 1 in hexadecimal notation. For instance if s 1 = #34 and s 2 = #12 , then we write the 2-symbol AS as s 2 s 1 = #1234 .

We use different partial algebraic signatures of pattern and database records, as we now explain.
 D EFINITION 2. Let l  X  [0 , M  X  1] be any position ( offset ) in R . The Cumulative Algebraic Signature (CAS) at l , CAS m is the algebraic signature of the prefix of R ending at r CAS m ( R, l ) = AS m ( r 0 . . . r l ) .
 The Partial Algebraic Signature (PAS) from l 0 to l is the value P AS m ( R, l 0 , l ) = AS m ( r l 0 r l 0 +1  X  X  X  r l ) , with 0  X  l nally, we most often use the PAS of substrings of length n , i.e., of n -grams.

D EFINITION 3. The n -gram Algebraic Signature (NAS) of R at l is N AS m ( R, l ) = P AS m ( R, l  X  n + 1 , l ) , for l  X  n  X  1 . With other words:
In all the definitions, we may drop R whenever it is implicit for brevity X  X  sake. Figure 2 shows the respective parts of the record that define the CAS , P AS and N AS at offset l . The following simple properties of algebraic signatures, expressed for coordinate i , 1  X  i  X  m , are useful for what follows. We note i -th symbol of a CAS, as CAS m ( l ) i and proceed similarly for NAS and PAS. Properties 3 and 4 let us incrementally calculate next CAS and NAS
Record R Figure 2: Computing CAS ( l ) , P AS ( l 0 , l ) and N AS ( l ) in record R M while building the file, or preprocessing the pattern, instead of re-computing the signature entirely. This speeds up the process con-siderably. Property 5 also speeds up the pattern preprocessing, as it will appear. Property 6 finally is fundamental for the match attempt calculus.
 For 0  X  l 0 &lt; l :
Table 2 summarizes the symbols used in the paper. Our database consists of records that are made up of a Record IDentifier (RID) and some non-key field. (Extensions to databases with more than one key and/or non-key field are straight-forward.) We assume that the non-key field consists of strings of symbols from our Galois field. Our search finds all occurrences of a pattern in the non-key field of every record in the database. When we talk about offsets and algebraic signatures, we refer only to the non-key field. If R is such a field and r i a character (Galois field element) in R , then we call i the offset . An n -gram G = r l  X  n +1 sequence of n consecutive symbols in R . By extension, we then call l the offset of the n -gram.
 An AS-Index consists of entries .

D EFINITION 4. Let G be an n -gram at offset o in R . The entry indexing G , denoted E ( G ) , is a triplet ( rid, o 0 , c ) where rid is the RID of R , c is CAS 1 ( R, o ) , and o 0 is o modulo 2 f  X  1 . We assure constant size of the entries by taking the remainder. The choice of the modulus is justified by the identity  X  2 f  X  1 Galois field elements  X  .

The indexing is  X  X ense X , i.e., every n -gram in the database is in-dexed, and by a different entry. To construct the index, we process all n -grams in the database.

AS-Index is a hash file, denoted D [ 0 ..L  X  1 ], with directory length L = 2 v being a power of 2 (Fig. 3). Elements of D re-fer to buckets or lines of variable length, each containing a list of entries. Lines are of variable length to accommodate a possible un-even distribution of n -gram values. Each D [ i ] contains the address of the i -th bucket. Hash directory D
All together, the AS-Index line structure is similar to a posting list in an inverted file, except for the presence of the CAS c in each entry and a specific representation of the offset l . Since we use a hash file, lines should have a collision resolution method such as classical separate chaining that uses pointers to an overflow space. Such a technique accomodates moderate growth, but if we need to accomodate large growth, then we need a dynamic hashing method such as linear hashing.

We now describe how to calculate the index i of the line for an entry E ( G ) = ( rid, o, c ) . We calculate i from the m -symbol NAS N AS m ( G ) = ( s 1 , . . . , s m ) . The coordinates of the NAS are bit strings. By concatenating them, we obtain a bit string S = s s m  X  1 . . . s 1 that we interpret as a large, unsigned integer. The index i is: Since L = 2 v , this amounts to extracting the last v bits of S . It is easy to see that m should be such that m  X  n and m  X d v/f e . The choice of AS-Index parameters m , n and L will be further discussed in Section 6.

E XAMPLE 1. Consider a 100 GB database with byte-wide sym-bols ( f = 8 ). For the sake of example, let L = 2 30 , leading to buckets with d 10 11 / 2 30 e = 93 entries on the average. Let n = 5 and m = 4 . To calculate line index i of n -gram G we thus con-catenate s 4 ..s 1 of N AS 4 ( G ) . Then we cut lower 30 bits.
Now, consider the record with RID 73 and non-key field  X  University Paris Dauphine  X . Assume that N AS 4 (  X  X nive X  , 4) is #3456789a. Since this is the first 5-gram, CAS 1 (73 , 4) has the same value as the first component, i.e., is #9a. For subsequent 5-grams, the first coordinate of the NAS and the CAS usually differ. The entry is E = (73 , 4 , #9 a ) . Its line index is #3456789 a mod 2 30 = #3456789 a (we remove the leading 2 bits).
Let P = p 0 . . . p K  X  1 be the pattern to match. AS-Index search delivers the RID of every record in the database that contains P . The search algorithm first prepocesses P by extracting values that become then entries into AS-Index to locate matches.
The preprocessing phase computes three signatures from the pattern P : (a) the m -symbols AS of the 1st n -gram in P , called S 1 (b) the 1-symbol PAS of the suffix of P following the 1st n -gram, S p = P AS 1 ( P, n, K  X  1) . (c) the m -symbols AS of the last n -gram in P , denoted as S
There are several ways to compute these signatures. For in-stance, one may compute S 1 , then S p , then extract the m -symbol value of S 2 through property (5).

Figure 4 shows the parts of the pattern that determine the signa-tures S 1 , S 2 and S p on our running example. Recall that n = 5 and m = 4 . We preprocess the pattern P =  X  University Paris Dauphine  X  and obtain (a) S 1 = N AS 4 ( P, 4) (for 5 -gram  X  X nive X  ); (b) S 2 = N AS 4 ( P, 24) (for 5 -gram  X  X hine X  ) and (c) S p = P AS 1 ( P, 5 , 24) .
 This information is used to find the occurrences of the pattern in the database. Record R
Let i = h L ( S 1 ) and i 0 = h L ( S 2 ) . Every entry ( R, l bucket D [ i ] indexes an n -gram G in a record R whose NAS S such that h L ( S G ) = h L ( S 1 ) . Likewise, every entry ( R bucket D [ i 0 ] indexes an n -gram G 0 such that h L ( G Up to possible collisions , G and G 0 respectively equal the first and last n -grams in pattern P.

We search every pair ( G, G 0 ) able to characterize a matching string S = P . We must have R 0 = R and l 2 = ( l 1 + K  X  n ) mod (2 f  X  1) . The last component in G 0 , c 2 should match the value implied by c 1 and S p (see Figure 4). Algebraic property 6 implies that c 2 should be Hash directory D
Figure 5 illustrates the process. By hashing on S 1 = 0 Unive we retrieve the bucket D [ i ] which contains, among others, the entries indexing all occurrences of  X  X nive X  in the database. Similarly we retrieve the bucket D [ i 0 ] which contains entries for all the occurrences of the n -gram  X  X hine X  . A pair of entries [ e ( r 1 , l 1 , c 1 ) , e 0 i ( r 1 , l 4 , c 4 )] in ( D [ i ] , D [ i s of r 1 that begins with  X  X nive X  and ends with  X  X hine X  . Checking whether s matches P involves two tests: (i) we com-pute c 1 +  X  l 1 +1 .S p and compare it with c 4 to check whether the signatures of the middle parts match, and (ii) we verify as discussed
Nr. Collisions Taken Expected Table 3: Actual and expected number of collisions using a 3B signature on a dictionary of 209881 moderately sized English words. whether l 1 matches l 2 given K , i.e., S and P have the same length. If both tests are succesful, we report a probable match. The next attempt will consider ( r 3 , l 2 , c 2 ) in D [ i ] and ( r ble match on r 2 in D [ i ] . The pseudo-code of the algorithm is given below.

The selectivity of the process relies on its ability to manipulate three distinct signatures, S 1 , S 2 and S p . Therefore the pattern length must be at least n + 1 .
As hashing in general, our method is subject to collisions deliv-ering false positives. To eliminate any collisions, it is necessary to post-process AS-Search by attempting to actually find P in every R identified as a match. This requires a symbol by symbol com-parison between P and its presumed match location. It will appear however that AS-Search should typically have a negligible proba-bility of a collision. Hence post-processing may be left to presum-ably rare applications needing full assurance. Also, it should be RAM-based and therefore typically negligible with respect to the disk search time. We thus do not detail it here.
We now present a short, theoretical analysis of the expected per-formance of AS-Index.
Algebraic signature values tend to have a more uniform distri-bution than the distribution of character values due to the multi-plications by powers of  X  in their calculations. However, the total number of strings or n -grams in a dataset gives an upper bound for the number of algebraic signatures calculated from them. Biolog-ical databases often store DNA strings in an ASCII file. Only the four characters  X  X  X ,  X  X  X ,  X  X  X , and  X  X  X  appear as characters in such a file. There are only 4 6 = 4 K different 6 -grams in the file and the number of different NAS of these signatures cannot exceed this value. Increasing the number of coordinates in the NAS beyond 5 symbols is not going to achieve better uniformity. This caution applies only to files using a small alphabet.

For a different experiment, we chose a list of English words (209881 words -2.25MB) six or more characters longer taken from a world list used to perform a dictionary attack on a password file (by an administrator trying to weed out weak passwords). We cal-culated the 3B three component signature of all words with more than five characters (65536 words) and calculated the number of signatures attained by i words as well as this number for a per-fect hash function (Table 3). The  X  2 value of 0.479166 shows very close agreement. When repeating the test using 2B two compo-nent signatures, we obtained  X  2 = 0 . 21 . A much smaller set had  X  2 = 4 . 79742 , but there were less signatures attained by a high number (  X  5 ) of words. These results give experimental verifica-tion for the  X  X latness X  of signatures.
Index construction time . The properties (2) -(6) of algebraic signatures allow us to calculate all entries with a linear sweep of all records. We need to keep a pointer to the symbol just beyond the current n -gram and to the first symbol of the current n -gram. Using equations (3) and (4), we can then calculate the NAS of the next n -gram from the old one and update the running CAS of the record. Since creating the entry for an n -gram and inserting it into the index take constant time, building the index takes time linear to the size of the database.

Storage costs . The storage complexity of AS-Index is O ( N ) for N indexed n -grams. The actual size of a RID should be 3-4 bytes since 3 bytes already allow a database with 16M records. The actual storage per entry should be about 5-6 bytes, which results in a storage overhead of about (5  X  6) N . We can lower this storage overhead, e.g. to 125 %, by non-dense indexing, that we do not present here due to space limitation, at the expense of a proportional increase in search time.

E XAMPLE 2. We still consider a 100 GB database with 8b sym-bols. Assuming an average record size of 100 symbols, we have 1G records and our record identifier needs to be 4B long. We pre-viously set the size of the CAS to 1B. With the 1B offset into the record, the entry is 6B. AS-Index should use about 6 times more space than the original database.

Now assume records of 10KB each. The record identifiers can be 3B long. This gives a total entry size of 5B or a storage factor of 5.
Each element of the hash directory stores a bucket address with at least d log 2 ( L ) e bits. In the case of our large 100GB database, with L = 2 30 , choosing 4 bytes for the address leads to the required storage of 4.1GB, smaller than the current data servers standard capacity. In most cases, D is expected to fit in main memory.
Pattern preprocessing . To preprocess the pattern, we need to calculate a PAS and two NAS. We calculate both in a similar man-ner as above and obtain preprocessing times linear in the size of the pattern. Since the result depends on all symbols in the pattern, we cannot do better.

Search speed . We assume that entries are close to uniformly distributed. The search algorithm picks up two cells in the hash directory, in order to obtain the number of entries and the bucket references of, respectively, D [ i ] and D [ i 0 ] . Then the buckets them-selves must be read. Each of these accesses may incur a random disk access, hence a (constant) cost of at most four disk reads. If the hash directory resides in main memory, the cost reduces to load-ing the two buckets.
 The main memory cost is an in-RAM join of the two buckets. With l denoting the expected line length, and assuming the lines are ordered, the average complexity of this phase should be (un-der our uniformity assumption) 2 l . The worst case is O ( l case is highly unlikely, as all the entries on both lines should fit into the same record. The optional collision resolution (symbol-to-symbol) test adds O ( | P | ) . This test is typically performed in RAM and makes only a negligible contribution to the otherwise constant costs. Altogether, the search cost is where C hash represents the Hash Directory access cost, C bucket access cost, C ram the RAM processing cost and C post post-processing.

We now evaluate the actual search time that may result from the above complexity figures. We take as basis the characteristics of the current popular hardware shown in Table 4 (see also [8] for a recent analysis).
 C hash is the cost of fetching 2 elements in the hash directory. The transfer overhead is negligible, and the (worst case) cost is therefore in the range [10 , 14] ms for magnetic disks. The bucket access cost C buck is similar to C hash regarding random disk ac-cesses, but we fetch far more bytes per access. We need to transfer 2 l entries. Table 4 suggests that we can transfer up to 300 KB per ms (Flash transfer rate are similar.) Since the size of an entry is typically 5-6 bytes, each search loads 10  X  12 l. It follows that for l = 1 K , the line transfer cost is negligible. For l = 16 K , 160 to 200 KB must be transferred. The cost is about 0.5 to 0.7 ms. This is still negligible with respect to disk accesses for AS-Index on mag-netic disk, but not on solid one. In the latter case, the transfer cost is equivalent to an additional disk access.

The basic formula (average cost) for C ram , the in-RAM join of the two lines, is 2 l . In practice it is l  X  E , where E is a couple of visited entries processing cost. In detail, we have 2 RAM accesses to Rids. In this test is successful, we need 2 additional accesses to CASs, 2 to offsets o and 1 access to the log table for algebraic computations. A conservative evaluation of E = 500 ns seems fair. The cost of the in-RAM join can thus be estimated as 100  X s for l = 200 , 500  X s for l = 1 , 000 , and 8 ms for l = 16 K . The first cost is negligible whatever the storage media; next one is so for disk, but not for flash, the latter is not for either.
Finally, the postprocessing cost P can be estimated based on a unit cost of 100 ns per symbol. Even for a 1,000 symbols long pat-tern, the postprocessing costs 100  X s , which remains negligible for both magnetic disk or flash memories. Its importance also depends on the number of matches, of course.
The previous analysis leads to the following conclusions regard-ing the choice of AS-Index parameters. As a general rule of thumb, one must choose parameter L so as to maintain the hash directory D in RAM. Caching D in RAM, whenever possible, saves two disk access on four. Setting a limit on L may lead to increase the aver-age line length l , but our analysis shows that this remains beneficial even when l reaches hundreds or even thousands of entries. Under our assumptions, l should reach 16 K to add the equivalent of 1 random access to the search cost, at which point one may consider enlarging the hash directory beyond the RAM limits.

Large values for l may also be beneficial with respect to other factors. First, larger lines accommodate a larger database for a fixed n . Second, we may choose a smaller n , with a smaller minimal size of n + 1 for patterns.

Let us illustrate this latter impact. We continue with our running example of a 100 GB database with byte-wide symbols ( f = 8 ), L is 2 30 , and an average load of l = d 10 11 / 2 30 e = 93 entries per line. This implies the choice of m , which must be such that 2 mf  X  L , i.e., m  X  (log needs to consist of at least 30 bits. Correspondingly, N AS to be at least that long. Since each coordinate of N AS m 8 b, the value for m needs to be at least 4 . Each NAS then contains at least 32 bits.

The n -grams used need to contain at least m symbols. Other-wise, the range of n -gram values is smaller than L and certain lines will not contain any entries. If the n -grams are reasonably close to uniformly distributed, the range of values is 256 n and we can pick n = m. Still referring to our example with L = 2 30 , we can choose n = 4 .

However, the actual character set used is most often smaller than 256, or only a fraction of the characters appear frequently. This requires a larger n , since the range of possible n -grams must con-tain at least L values. Let v be the number of values we expect per symbol. In a simple ASCII text, the number of printable character codes is v = 96 . DNA encoding represents an extreme case with v = 4 . The n -gram size must be such that v n  X  L . With v = 96 (simple ASCII text) and L = 2 30 , n must be set to 5 , the small-est value such that 96 n  X  L , to generate all required NAS values. These parameter values were actually used for Example 1.
Consider now the case of a DNA database where only 4 of the possible 256 ASCII characters appear in records. We need to set n = 15 in order to obtain the 2 30 possible signatures. n + 1 is the minimal pattern length we allow to search for. Such limit should not be nevertheless a practical constraint for a search over a small alphabet. The need there is rather for long patterns [3]. If never-theless it was a concern, one may choose a smaller n at the price of fewer, hence longer, lines. For instance choosing n = 10 and L = 2 20 for our DNA database results in the average of 95 K en-tries in each line. The minimal pattern size decreases by five, i.e., to n + 1 = 11 symbols. We describe in this section our experimental setting and results. We implemented our AS structure as well as a String B-Tree [7] and an n -gram index, based on inverted lists [22]. The rationale for String B-Tree, discussed more in what follows, is that it ap-pears attractive for disk-based use and is among most recent pro-posals. As Section 8 discusses, the inverted list was the common basis for many variants, e.g., n-gram/2L [11]. Notice that all the discussed structures are tree/trie based. Hence, none offers the constant search performance of AS-Index. In other words, their search speed must deteriorate beyond the one of AS-Index for a sufficiently large database.

All structures are coded in C++, and we run the experiment un-der Linux on a 2.40 GHz duo-core processor with 2GB in main memory and two 80GB disks. 100 MB 403.1 (400+3.1) 605 (600+5) 737.6
We conduct our experiments on a database of records identified by a unique ID and with content consisting of a sequence of one-byte characters. We treat each character as an element in the Galois field GF (2 8 ) . The design of the database is meant to represent a large spectrum of situations ranging from many small records to large-size protein descriptions. We also consider databases of DNA sequences. All records are stored on disk.
We implemented a String B-Tree structure [7], making our best efforts to minimize the storage overhead. Each node contains a compact representation of a Patricia Trie [17]. In our implementa-tion a disk block occupies 4K, and we can store at most 550 entries in each leaf.

We build the n -gram index in two steps. First, we scan all record contents in order to extract all n -grams with their position. For each n -gram, we obtain a triple &lt; ngram, rid, offset &gt; which we insert in a temporary file. The second step sorts all triples and creates lists of 6-bytes entries. The final step builds the B+-tree which allows to access quickly to a list given an n -gram. Our simple construction in bulk is fast and creates a compact structure.

Our implementation of AS-Index is static as well. First, the n -grams are collected. For each n -gram at offset l in a record R the hash key k = h L ( N AS m ( R, l )) and the CAS c = CAS 1 are computed. The quadruplet &lt; k, c, rid, l &gt; is inserted in a temporary file. Second, the temporary file is sorted on the hash key k . This groups together entries which must be inserted into the same bucket. An entry consists of a CAS (1 byte), a record id (2 bytes) and an offset. The latter is a 1B integer obtained by taking the actual offset of the n -gram in the record, modulo 255. Using the CAS as a secondary sort key, one places the entries into the required order for insertion into the bucket.

We use three types of datasets with quite distinct characteristics: alpha , dna and text . The alpha(  X  ) type consists of syn-thetic ASCII records, with uniform distribution, ranging over an alphabet  X  which is a subset of the extended ASCII characters. We consider two alphabets:  X  26 , with only 26 characters, and  X  with all the 256 symbols that can be encoded with f = 8 bits. We call the resulting datasets alpha(26) and alpha(full) . They allow us to compare the behavior of our structure to the theoretical analysis in Section 6.

The second type, dna , consists of real DNA records ex-tracted from the UCSC database 1 . For the types alpha(full) , alpha(26) and dna , we composed datasets ranging from 10MB to 100MB. The last type, text , consists of real text records cre-ated from ASCII files of large English books. The typical size of a text record is 1-2 MB, and we created a 100MB database of text files. The n -gram size is set to 8 for DNA files, and to 4 for the other datasets. http://hgdownload.cse.ucsc.edu/ 100 MB 400.4 (400+0.4) 601 (600+1) 737.6 100 MB 402.7 (400+2.7) 602.8 (600+2.8) 843.0
The size of the AS-index is the sum of the size of cells and the directory which stores the number of entries for each bucket. The size of the n -gram index is the sum of the size of inverted lists and the size of the B+tree. The size of the String B-Tree is the size of the B+tree where each node is a serialized Patricia Trie. Tables 5, 6, and 7 give respectively the index sizes for alpha , DNA and text files.

The sizes of indexes are comparable. The size of AS-Index ben-efits from its smaller entries (4 bytes), mostly due to the 1B size of the offset. Sophisticated compression techniques [22] would ben-efit all structures. n -gram index offsets could be limited to 3B or even 2B, which would still allow the size of records to grow to up to 2 24 or 2 16 symbols. If the database has a few long records with many occurrences of the same n -gram, then we can save space by storing each rid only once in the list, followed by a possibly com-pressed list of offsets. If, on the contrary, the database consists of many, relatively small records, compression based on delta-coding can be envisaged. Both implementations use space better. The String B-Tree requires more space with 7B entries in the leaves and 11B entries for internal nodes.
 Table 8: Distribution of entries for the real datasets (100 MB files)
For real datasets, either dna or text files, the distribution is far from being uniform. Table 8 shows the distribution of the number of entries from two real 100MB databases. The average number of entries is 1,534 for the DNA database, and 372 for the text database, with an important variance. In the worst case ( text files), the largest list has 1,480,008 entries. This fully justify our choice of storing the number of entries in the directory, and of using this information to scan the smallest list during a search operation.
The building time is proportional to the size of the index. Our structures are built in bulk after sorting all n -gram entries in a tem-porary file. This leads to comparable performances. On our ma-chine, the building time for a 100 MB file is about 1,000 s, and the bulding rate is about 120 KB/s. A comparison of dynamic builds remains for future work. For AS-Index, this would reduce mostly to the standard technique of maintaining a dynamic hash file.
We performed extensively pattern searches in our databases. We Table 9: Search time in ms for 100 MB alpha(full) files extracted the patterns from the files to guarantee that at least one result is found. Pattern sizes range from 10 symbols to 500 sym-bols. To avoid initialization costs and side effects such as CPU or memory contention from other OS processes, we performed each search repeatedly until the search times stabilized. We report the average search time over a run of one hundred search operations.
We report the results on search time, in ms , in Tables 9, 10, 11 and 12, for 100MB files.

As expected, the String B-Tree and AS-Index behavior is con-stant regardless of the length of the pattern, while n -gram index performance degrades linearly with this length. For our 100MB files, the height of the String B-Tree is 3 independently of the al-phabet size, for 10 6 indexed substrings (recall that the fanout if 550, and that our bulk insertion creates full nodes). The root is always in the cache, as well as a significant part of the level below the root, depending on the indexed file size. The String B-Tree traversal is generally reduced to a single disk access for loading a leaf node. In addition, each lookup in a node requires an additional random disk access to the database in order to fetch the full string. This leads to a final cost of 4-5 physical disk accesses. The search time with String B-Tree is independent of the size of the pattern and of the size of the alphabet.

Searching with the AS-Index takes about 17 ms for alpha(full) , 30 ms for alpha(26) files, 30 ms for dna files and 18 ms for text files (real data). This is consistent with the analytical cost discussed in Section 6. The alpha datasets are uniformly generated, and this results in an almost constant number of entries per bucket. Accordingly, the search is done in few operations. The difference between alpha(full) and alpha(26) is explained by the size of the buckets which are larger for alpha(26) since the n -gram values range over the set of 26 4 possibilities compare to 256 4 for alpha(full) .
For real data (DNA and text ), buckets are likely to be larger, ei-ther because the alphabet is so small that the set of existing n -gram values is bounded and cannot fully benefit from the hash function (see Subsection 6.1 for a discussion), or because of non unifor-mity. The former case corresponds to the DNA, the latter to our Figure 6: Search result on 100MB files, for varying pattern size real text files. Table 8 shows that, on average, the number of en-tries in a bucket is larger for DNA (1,534 entries) than for text (372). The cost of DNA search is accordingly slightly higher (  X  30 ms , against  X  20 ms ). Recall that our algorithm chooses the smallest bucket for driving the search, which limits the impact of skewed datasets and the variance of search times.

Figure 6 summarizes the results discussed above, and illustrates the linearity of search times for ngram-index and the constant search times for AS-Index and String B-Tree.

Figure 7 shows the evolution of search times as the size of the database increases from 10 MB to 100 MB. Each curve represents the results for a given dataset, with patterns consisting of 10 to 200 symbols. As before, the AS-Index exhibits an almost constant be-havior (appr. 20-30 ms ), even for very large patterns (200 symbols) searched in large files (100 MB). The search time with the String B-Tree slightly increases with the size of the file (but remains con-stant with the pattern size). The tree height remains equals to 3, however the number of nodes increases with the file size. This ac-counts for a lower probability of a buffer hit during tree traversal. The logarithmic behavior of the String B-Tree is almost blurred here, because of the large node fanout (550). It would appear with larger files. Given a file size greater than 167MB for instance ( i.e. , corresponding to more than 550 3 strings), its height would increase to 4, with two (2) additional disk accesses on average for a search.
The search time evolves (sub)linearly for n -gram index, both in the size of the pattern, and in the size of the database. The slope is steeper for large files. This is explained by the necessity to scan a number of inverted lists which is proportional to the size of the pattern. In addition, larger files imply larger lists, hence the behav-ior illustrated by Figure 7. However the cost remains sublinear (the cost for 100MB is only 3 times higher than the cost for 5MB). This is due to the merge process which stops when the smallest list has been fully scanned, thereby avoiding a complete access to all lists.
Figure 8 and Figure 9 summarize the ratio of search times, giv-ing the speed-up of AS-Index over respectively the n -gram index and the String B-Tree. The alpha(full) dataset family is a spe-cial case. Because of the uniform distribution that produces a large number of n -grams ranging over all the possible values, finding the reference to a list through a traversal of the B+tree takes time. The Figure 8: Ratio of search times AS-Index/ngram-index with re-spect to pattern size number of traversals increases linearly with the pattern size, which explains the large cost with respect to the other datasets. For these datasets our algorithm clocked in as twice as fast than n -gram based search for small patterns (10 symbols) and about 30 times faster for large patterns (with hundreds of symbols). Figure 9 shows a con-stant gain of AS-Index over String B-Tree, about 1.5 with DNA and alpha(26) files and about 2.5 for text and alpha(full) files. This difference is due to the higher selectivity of n -grams for text and alpha(full) what implies smaller buckets. The String B-Tree is not affected by the selectivity of n -grams, i.e. , the alphabet size. Table 13: False-positives ratio for various alphabets and ngram size
Table 13 shows the false-positives (F/P) retrieved when search-Figure 9: Ratio of search times AS-Index/String B-Tree with respect to pattern size ing for patterns of 50 symbols in a set of files with a total size of 10MB, with respect to the alphabet and the n -gram size. As expected (see Section 6) the number of F/P highly depends on the choice of n . While 4 or 6-grams (resp 2-3 grams) generate a signifi-cant number of F/P for DNA (resp. alpha(26)), this number quickly drops to less than one on a thousand with 8-grams (resp. 4-grams). The behavior for small-size alphabets can be explained by the low number of possible n -grams ( e.g. 4 4 for DNA with n = 4 ) and thus by the low number of NAS possible signatures (see Section 6). Since AS-Index does not store the n -grams but their signature, the choice of the n has no impact on index size and performance.
Finding patterns in a large database of sets is a fundamental problem in Computer Science and its applications such as bioin-formatics. The theoretically best algorithms and data structures allow linear construction of the index in the database, have low storage overhead, and allow searches that are processed in time lin-ear on the size of the pattern. Among the many algorithms, those based on suffix trees [9] have received much attention. Recent work by Kurtz [12], Tata, Hankins, and Patel [20] among others tries to make the theoretically optimal behavior of suffix trees practical. A great part of the problem is caused by the blow-up of the index size over the database size, typically ten to twenty times [12]. Related data structures such as Manber X  X  suffix arrays [14], K X rkk X inen X  X  suffix cacti [10], or Anderson and Nilsson [1] suffix tries lower storage overhead at the prize of an increase in search time. Demen-tiev, K X rkk X inen, Mehnert, and Sanders [5] give methods to make suffix arrays effective and efficient for truly large files
Suffix arrays and suffix trees are static indexes, designed to index a single file content. If we create such an index for every record, then search times will depend on the size of the database. If we however create the index for a collection of records  X  as we ob-viously should  X  then deleting and inserting records becomes very difficult, and it is unclear how we can adapt the binary search of suffix arrays to the indirection mechanism used by the storage en-gine. Life is much simpler if the database consists of words and we restrict ourselves to word indexes that can be stored much more compactly [22].

Signatures files were proposed in [6] and shown to be inferior to inverted indexing in [23]. Some other attempts for indexing sequences are the ed-tree [19] for DNA files, and the q -gram in-dex [2]. Both focus on the specific problem of homology search in genomic databases.
Our method is predominantly based on previous work on n -gram based inverted file indexing. The technique has been advocated for string search in larger, hence naturally disk based, partly or totally unstructured files or databases (full-text, hypertext, protein, DNA). In bioinformatics, C AFE prototype uses n = 3 for protein and n = 9 for DNA string search, and is reported several times faster than previous systems [21]. All these systems used the basic n -gram index for many GB disk-resident datasets.

The latest attempt of using n-grams for a large, (hence diskbased) database, is reported in [11]. Like us, it improves storage overhead and, especially, search time, over the basic n-gram scheme. The n -Gram/2L uses a  X  X ormalized X  representation with two indexes: ( i ) one n -gram index on the subsequences of size m indexing the n-grams found in each subsequences, and ( ii ) one n -gram-index indexing the subsequences found in the files. The two indexes are smaller than the original index and though a search needs to use both indexes, it can use less look-up. If AS-Index saved storage for larger alphabets it appears to be slightly less efficient for small ones compared to n -Gram/2L However like n -gram index this structure offers a search proportional to the database size and to the query size oppositely to our constant time claim.
We have presented a novel approach to string search in databases, based on Algebraic Signatures and algebraic computations. The contribution of our paper is a simple and fast search algorithm which finds a pattern of arbitrary length in a database of arbitrary size in constant time. We showed through analysis and experiments that our technique outperforms other disk-based approaches. To our knowledge, our work is a new approach to indexing, which takes advantage of the interpretation of character as symbols in a mathematical structure to develop new computational techniques.
Future works include more complete performance studies in-cluding the study of a non-dense indexing variant and a variant that deals with skewed distribution by considering the most selective ngrams of the searched pattern. Scalable and distributed AS-Index constitute other promising research directions that we plan to in-vestigate.
 Acknowledgments. This work has been partially funded by the Advanced European Research Council grant Webdam. [1] S. N. A. Andersson. Efficient Implementation of Suffix [2] X. Cao, S. C. Li, and A. K. H. Tung. Indexing dna sequences [3] C. Charras, T. Lecroq, and J. D. Pehoushek. A Very Fast [4] M. Crochemore and M. Lecroq. Pattern Matching and Text [5] R. Dementiev, J. K X rkk X inen, J. Mehnert, and P. Sanders. [6] C. Faloutsos. Signature Files. In Information Retrieval: Data [7] P. Ferragina and R. Grossi. The String B-tree: A New Data [8] J. Gray and B. Fitzgerald. Flash Disk Opportunity for [9] D. Gusfield. Algorithms on Strings, Trees, and Sequences: [10] J. K X rkk X inen. Suffix Cactus: A Cross between Suffix Treee [11] M. S. Kim, K. Whang, J. G. Lee, and M. J. Lee. n-Gram/2L: [12] S. Kurtz. Reducing the Space Requirement of Suffix Trees. [13] W. Litwin and T. Schwarz. Algebraic Signatures for Scalable [14] U. Manber and E. W. Myers. Sufix Arrays: A New Method [15] G. Margaritis and S. V. Anastasiadis. Unleashing the Power [16] E. Miller, D. Shen, J. Liu, and C. Nicholas. Performance and [17] J. C. Na and K. Park. Simple Implementation of String [18] B. Phoophakdee and M. J. Zaki. Genome-scale disk-based [19] Z. Tan, X. Cao, B. C. Ooi, and A. K. H. Tung. The ed-tree: [20] S. Tata, R. Hankins, and J. Patel. Practical Suffix Tree [21] H. Williams and J. Zobel. Indexing and Retrieval for [22] I. Witten, A. Moffat, and T. Bell. Managing Gigabytes: [23] J. Zobel, A. Moffat, and K. Ramamohanarao. Inverted Files
